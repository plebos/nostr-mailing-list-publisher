[
    {
        "title": "[Lightning-dev] Making (some) channel limits dynamic",
        "thread_messages": [
            {
                "author": "Bastien TEINTURIER",
                "date": "2020-10-05T11:46:12",
                "message_text_only": "Good evening list,\n\nRecent discussions around channel jamming [1] have highlighted again the\nneed to think twice when\nconfiguring your channels parameters. There are currently parameters that\nare set once at channel\ncreation that would benefit a lot from being configurable throughout the\nlifetime of the channel\nto avoid closing channels when we just want to reconfigure them:\n\n* max_htlc_value_in_flight_msat\n* max_accepted_htlcs\n* htlc_minimum_msat\n* htlc_maximum_msat\n\nNodes can currently unilaterally udpate these by applying forwarding\nheuristics, but it would be\nbetter to tell our peer about the limits we want to put in place (otherwise\nwe're wasting a whole\ncycle of add/commit/revoke/fail messages for no good reason).\n\nI suggest adding tlv records in `commitment_signed` to tell our channel\npeer that we're changing\nthe values of these fields.\n\nIs someone opposed to that?\nAre there other fields you think would need to become dynamic as well?\nDo you think that needs a new message instead of using extensions of\n`commitment_signed`?\n\nCheers,\nBastien\n\n[1] https://twitter.com/joostjgr/status/1308414364911841281\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201005/f98befeb/attachment.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2020-10-06T16:14:34",
                "message_text_only": "Hello Bastien,\n\nAs a first note , I was thinking dynamic policy adjustment would be covered\nby the dynamic commitment mechanism proposed by Laolu as it presents the\nsame trade-offs, you need to stop channel HTLC processing before upgrading,\notherwise it might falsify your whole in-flight HTLC accounting.\n\n> Recent discussions around channel jamming [1] have highlighted again the\n> need to think twice when\n> configuring your channels parameters.\n\nI'm still dubious that straighter channel parameters are the best solution\nto solve channel jamming. As a routing node evaluating a HTLC, I think the\nquestion you're trying to answer is : \"Is this a _honest_ HTLC to relay ?\",\nwhere honest is defined both as paying more fees that the liquidity lock\nand with high odds of a positive settlement, otherwise you won't get paid.\n\nThe first predicate is easy to evaluate, just verify that the HTLC is\npaying more as an incoming packet that you have to send forward.\n\nOn the other hand, the second predicate is hard to evaluate. A first lead\nof a solution is to evaluate the packet forwarder instead of the packet\nitself. You may have a web-of-trust/reputation system with a one-level rank\nof trust which would be enforced at the channel opening layer, i.e don't\nopen/accept channels with random nodes. A less constraining version is\nstill a reputation system but where you statically attribute a HTLC\nforwarding policy based on counterparty reputation (~today).\n\nThe more evolved reputation system version you implicitly seem to argue for\nis adapting the forwarding policy based on counterparty past behavior, e.g\nlike relaxing channel parameters for a counterparty upstreaming a lot of\nsuccessful HTLCs. IMO, this is still presenting hurdles.\n\nIf you have 1 BTC of outgoing bandwidth but your counterparty is enforcing\na `max_htlc_value_in_flight_msat` of 0.5 BTC, it means you have a\n\"sleeping\" outgoing liquidity. Rationally, you should only open a channel\nwith a capacity somehow equivalent to what is authorized by your\ncounterparty relay policy.\n\nA lesson would be to negotiate first a policy then an opening, as of today\nthey're still bundled in one message flow. I don't think you can reduce the\ncapacity once you learn acceptor policy ? Don't overstake liquidity more\nthan you can actually gain from.\n\nThat said, if you have a dynamic policy model, at policy relaxation, you\nneed to increase channel capacity to profit from relaxation, let's say\nthrough some kind of splice-in. But now you have on-chain fees at each\npolicy/liquidity adjustment.\n\nUnder a dynamic policy model based on accumulated reputation, it sounds\nlike there is some kind of trade-off between useless off-chain liquidity\nand on-chain fees.\n\nInstead of relying on reputation, the other alternative is just to have an\nupfront payment system, where a relay node doesn't have to account for a\nHTLC issuer reputation to decide acceptance and can just forward a HTLC as\nlong it paid enough. More, I think it's better to mitigate jamming with a\nfees-based system than a web-of-trust one, less burden on network newcomers.\n\nThis doesn't prevent hybrid models where you might reward your good\nbehaving peers with a discount on your upfront payment policy.\n\nWhat's your opinion ?\n\nAntoine\n\nLe lun. 5 oct. 2020 \u00e0 07:54, Bastien TEINTURIER via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Good evening list,\n>\n> Recent discussions around channel jamming [1] have highlighted again the\n> need to think twice when\n> configuring your channels parameters. There are currently parameters that\n> are set once at channel\n> creation that would benefit a lot from being configurable throughout the\n> lifetime of the channel\n> to avoid closing channels when we just want to reconfigure them:\n>\n> * max_htlc_value_in_flight_msat\n> * max_accepted_htlcs\n> * htlc_minimum_msat\n> * htlc_maximum_msat\n>\n> Nodes can currently unilaterally udpate these by applying forwarding\n> heuristics, but it would be\n> better to tell our peer about the limits we want to put in place\n> (otherwise we're wasting a whole\n> cycle of add/commit/revoke/fail messages for no good reason).\n>\n> I suggest adding tlv records in `commitment_signed` to tell our channel\n> peer that we're changing\n> the values of these fields.\n>\n> Is someone opposed to that?\n> Are there other fields you think would need to become dynamic as well?\n> Do you think that needs a new message instead of using extensions of\n> `commitment_signed`?\n>\n> Cheers,\n> Bastien\n>\n> [1] https://twitter.com/joostjgr/status/1308414364911841281\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201006/8dc749f6/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2020-10-06T16:43:21",
                "message_text_only": "Good morning Antoine, and Bastien,\n\n\n> Instead of relying on reputation, the other alternative is just to have an upfront payment system, where a relay node doesn't have to account for a HTLC issuer reputation to decide acceptance and can just forward a HTLC as long it paid enough. More, I think it's better to mitigate jamming with a fees-based system than a web-of-trust one, less burden on network newcomers.\n\nLet us consider some of the complications here.\n\nA newcomer wants to make an outgoing payment.\nSpeculatively, it connects to some existing nodes based on some policy.\n\nNow, since forwarding is upfront, the newcomer fears that the node it connected to might not even bother forwarding the payment, and instead just fail it and claim the upfront fees.\n\nIn particular: how would the newcomer offer upfront fees to a node it is not directly channeled with?\nIn order to do that, we would have to offer the upfront fees for that node, to the node we *are* channeled with, so it can forward this as well.\n\n* We can give the upfront fee outright to the first hop, and trust that if it forwards, it will also forward the upfront fee for the next hop.\n  * The first hop would then prefer to just fail the HTLC then and there and steal all the upfront fees.\n    * After all, the offerrer is a newcomer, and might be the sybil of a hacker that is trying to tie up its liquidity.\n      The first hop would (1) avoid this risk and (2) earn more upfront fees because it does not forward those fees to later hops.\n  * This is arguably custodial and not your keys not your coins applies.\n    Thus, it returns us back to tr\\*st anyway.\n* We can require that the first hop prove *where* along the route errored.\n If it provably failed at a later hop, then the first hop can claim more as upfront fees, since it will forward the upfront fees to the later hop as well.\n  * This has to be enforcable onchain in case the channel gets dropped onchain.\n    Is there a proposal SCRIPT which can enforce this?\n  * If not enforcable onchain, then there may be onchain shenanigans possible and thus this solution might introduce an attack vector even as it fixes another.\n    * On the other hand, sub-satoshi amounts are not enforcable onchain too, and nobody cares, so...\n\nOn the other hand, a web-of-tr\\*st might not be *that* bad.\n\nOne can say that \"tr\\*st is risk\", and consider that the size and age of a channel to a peer represents your tr\\*st that that peer will behave correctly for fast and timely resolution of payments.\nAnd anyone can look at the blockchain and the network gossip to get an idea of who is generally considered tr\\*stworthy, and since that information is backed by Bitcoins locked in channels, this is reasonably hard to fake.\n\nOn the other hand, this risks centralization around existing, long-lived nodes.\n*Sigh*.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2020-10-08T12:23:40",
                "message_text_only": "Good morning Antoine and Zman,\n\nThanks for your answers!\n\nI was thinking dynamic policy adjustment would be covered by the dynamic\n> commitment mechanism proposed by Laolu\n\n\nI didn't mention this as I think we still have a long-ish way to go before\ndynamic commitments\nare spec-ed, implemented and deployed, and I think the parameters I'm\ninterested in don't require\nthat complexity to be updated.\n\nPlease forget about channel jamming, upfront fees et al and simply consider\nthe parameters I'm\nmentioning. It feels to me that these are by nature dynamic channel\nparameters (some of them are\neven present in `channel_update`, but no-one updates them yet because\ndirect peers don't take the\nupdate into account anyway). I'd like to raise `htlc_minimum_msat` on some\nbig channels because\nI'd like these channels to be used only for big-ish payments. Today I\ncan't, I have to close that\nchannel and open a new one for such a trivial configuration update, which\nis sad.\n\nThere is no need to stop the channel's operations while you're updating\nthese parameters, since\nthey can be updated unilaterally anyway. The only downside is that if you\nmake your policy stricter,\nyour peer may send you some HTLCs that you will immediately fail\nafterwards; it's only a minor\ninconvenience that won't trigger a channel closure.\n\nI'd like to know if other implementations than eclair have specificities\nthat would make this\nfeature particularly hard to implement or undesirable.\n\nThanks,\nBastien\n\nLe mar. 6 oct. 2020 \u00e0 18:43, ZmnSCPxj <ZmnSCPxj at protonmail.com> a \u00e9crit :\n\n> Good morning Antoine, and Bastien,\n>\n>\n> > Instead of relying on reputation, the other alternative is just to have\n> an upfront payment system, where a relay node doesn't have to account for a\n> HTLC issuer reputation to decide acceptance and can just forward a HTLC as\n> long it paid enough. More, I think it's better to mitigate jamming with a\n> fees-based system than a web-of-trust one, less burden on network newcomers.\n>\n> Let us consider some of the complications here.\n>\n> A newcomer wants to make an outgoing payment.\n> Speculatively, it connects to some existing nodes based on some policy.\n>\n> Now, since forwarding is upfront, the newcomer fears that the node it\n> connected to might not even bother forwarding the payment, and instead just\n> fail it and claim the upfront fees.\n>\n> In particular: how would the newcomer offer upfront fees to a node it is\n> not directly channeled with?\n> In order to do that, we would have to offer the upfront fees for that\n> node, to the node we *are* channeled with, so it can forward this as well.\n>\n> * We can give the upfront fee outright to the first hop, and trust that if\n> it forwards, it will also forward the upfront fee for the next hop.\n>   * The first hop would then prefer to just fail the HTLC then and there\n> and steal all the upfront fees.\n>     * After all, the offerrer is a newcomer, and might be the sybil of a\n> hacker that is trying to tie up its liquidity.\n>       The first hop would (1) avoid this risk and (2) earn more upfront\n> fees because it does not forward those fees to later hops.\n>   * This is arguably custodial and not your keys not your coins applies.\n>     Thus, it returns us back to tr\\*st anyway.\n> * We can require that the first hop prove *where* along the route errored.\n>  If it provably failed at a later hop, then the first hop can claim more\n> as upfront fees, since it will forward the upfront fees to the later hop as\n> well.\n>   * This has to be enforcable onchain in case the channel gets dropped\n> onchain.\n>     Is there a proposal SCRIPT which can enforce this?\n>   * If not enforcable onchain, then there may be onchain shenanigans\n> possible and thus this solution might introduce an attack vector even as it\n> fixes another.\n>     * On the other hand, sub-satoshi amounts are not enforcable onchain\n> too, and nobody cares, so...\n>\n> On the other hand, a web-of-tr\\*st might not be *that* bad.\n>\n> One can say that \"tr\\*st is risk\", and consider that the size and age of a\n> channel to a peer represents your tr\\*st that that peer will behave\n> correctly for fast and timely resolution of payments.\n> And anyone can look at the blockchain and the network gossip to get an\n> idea of who is generally considered tr\\*stworthy, and since that\n> information is backed by Bitcoins locked in channels, this is reasonably\n> hard to fake.\n>\n> On the other hand, this risks centralization around existing, long-lived\n> nodes.\n> *Sigh*.\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201008/22e78ab2/attachment-0001.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2020-10-08T15:56:33",
                "message_text_only": "> There is no need to stop the channel's operations while you're updating\nthese parameters, since\nthey can be updated unilaterally anyway\n\nI think it's just how you defne channel's operations, either emptying out\nall pending HTLCs or more a `update_fee` alike semantic. You're right that\nthe latter should be good enough for the set of parameters you're proposing.\nA lightweight `update_policy` doesn't sound to bear difficulty at first\nsight.\n\nLe jeu. 8 oct. 2020 \u00e0 08:23, Bastien TEINTURIER <bastien at acinq.fr> a \u00e9crit :\n\n> Good morning Antoine and Zman,\n>\n> Thanks for your answers!\n>\n> I was thinking dynamic policy adjustment would be covered by the dynamic\n>> commitment mechanism proposed by Laolu\n>\n>\n> I didn't mention this as I think we still have a long-ish way to go before\n> dynamic commitments\n> are spec-ed, implemented and deployed, and I think the parameters I'm\n> interested in don't require\n> that complexity to be updated.\n>\n> Please forget about channel jamming, upfront fees et al and simply\n> consider the parameters I'm\n> mentioning. It feels to me that these are by nature dynamic channel\n> parameters (some of them are\n> even present in `channel_update`, but no-one updates them yet because\n> direct peers don't take the\n> update into account anyway). I'd like to raise `htlc_minimum_msat` on some\n> big channels because\n> I'd like these channels to be used only for big-ish payments. Today I\n> can't, I have to close that\n> channel and open a new one for such a trivial configuration update, which\n> is sad.\n>\n> There is no need to stop the channel's operations while you're updating\n> these parameters, since\n> they can be updated unilaterally anyway. The only downside is that if you\n> make your policy stricter,\n> your peer may send you some HTLCs that you will immediately fail\n> afterwards; it's only a minor\n> inconvenience that won't trigger a channel closure.\n>\n> I'd like to know if other implementations than eclair have specificities\n> that would make this\n> feature particularly hard to implement or undesirable.\n>\n> Thanks,\n> Bastien\n>\n> Le mar. 6 oct. 2020 \u00e0 18:43, ZmnSCPxj <ZmnSCPxj at protonmail.com> a \u00e9crit :\n>\n>> Good morning Antoine, and Bastien,\n>>\n>>\n>> > Instead of relying on reputation, the other alternative is just to have\n>> an upfront payment system, where a relay node doesn't have to account for a\n>> HTLC issuer reputation to decide acceptance and can just forward a HTLC as\n>> long it paid enough. More, I think it's better to mitigate jamming with a\n>> fees-based system than a web-of-trust one, less burden on network newcomers.\n>>\n>> Let us consider some of the complications here.\n>>\n>> A newcomer wants to make an outgoing payment.\n>> Speculatively, it connects to some existing nodes based on some policy.\n>>\n>> Now, since forwarding is upfront, the newcomer fears that the node it\n>> connected to might not even bother forwarding the payment, and instead just\n>> fail it and claim the upfront fees.\n>>\n>> In particular: how would the newcomer offer upfront fees to a node it is\n>> not directly channeled with?\n>> In order to do that, we would have to offer the upfront fees for that\n>> node, to the node we *are* channeled with, so it can forward this as well.\n>>\n>> * We can give the upfront fee outright to the first hop, and trust that\n>> if it forwards, it will also forward the upfront fee for the next hop.\n>>   * The first hop would then prefer to just fail the HTLC then and there\n>> and steal all the upfront fees.\n>>     * After all, the offerrer is a newcomer, and might be the sybil of a\n>> hacker that is trying to tie up its liquidity.\n>>       The first hop would (1) avoid this risk and (2) earn more upfront\n>> fees because it does not forward those fees to later hops.\n>>   * This is arguably custodial and not your keys not your coins applies.\n>>     Thus, it returns us back to tr\\*st anyway.\n>> * We can require that the first hop prove *where* along the route errored.\n>>  If it provably failed at a later hop, then the first hop can claim more\n>> as upfront fees, since it will forward the upfront fees to the later hop as\n>> well.\n>>   * This has to be enforcable onchain in case the channel gets dropped\n>> onchain.\n>>     Is there a proposal SCRIPT which can enforce this?\n>>   * If not enforcable onchain, then there may be onchain shenanigans\n>> possible and thus this solution might introduce an attack vector even as it\n>> fixes another.\n>>     * On the other hand, sub-satoshi amounts are not enforcable onchain\n>> too, and nobody cares, so...\n>>\n>> On the other hand, a web-of-tr\\*st might not be *that* bad.\n>>\n>> One can say that \"tr\\*st is risk\", and consider that the size and age of\n>> a channel to a peer represents your tr\\*st that that peer will behave\n>> correctly for fast and timely resolution of payments.\n>> And anyone can look at the blockchain and the network gossip to get an\n>> idea of who is generally considered tr\\*stworthy, and since that\n>> information is backed by Bitcoins locked in channels, this is reasonably\n>> hard to fake.\n>>\n>> On the other hand, this risks centralization around existing, long-lived\n>> nodes.\n>> *Sigh*.\n>>\n>> Regards,\n>> ZmnSCPxj\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201008/4fe5af26/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2020-10-08T20:05:43",
                "message_text_only": "Good morning t-bast,\n\n> Please forget about channel jamming, upfront fees et al and simply consider the parameters I'm\n> mentioning. It feels to me that these are by nature dynamic channel parameters (some of them are\n> even present in `channel_update`, but no-one updates them yet because direct peers don't take the\n> update into account anyway). I'd like to raise `htlc_minimum_msat` on some big channels because\n> I'd like these channels to be used only for big-ish payments. Today I can't, I have to close that\n> channel and open a new one for such a trivial configuration update, which is sad.\n\nAt the risk of once more derailing the conversation: from the MPP trenches, raising the minimum payment size is another headache.\nThe general assumption with MPP is that smaller amounts are more likely to get through, but if anyone is making a significant bump up in `htlc_minimum_msat`, that assumption is upended and we have to reconsider if we may actually want to merge multiple failing splits into one, as well as considering asymmetric splits (in particular asymmetric presplits) because maybe the smaller splits will be unable to pass through the bigger channels but the bigger-side split *might*.\n\nOn the other hand: one can consider that the use of big payments as an aggregation.\nFor example: a forwarding node might support smaller `htlc_minimum_msat`, then after making multiple such forwards, find that a channel is now heavily balanced towards one side or another.\nIt can then make a single large rebalance via one of the high-`htlc_minimum_msat` channels t-bast is running.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2020-10-09T07:24:41",
                "message_text_only": "Hey Zman,\n\nraising the minimum payment size is another headache\n>\n\nIt's true that it may (depending on the algorithm) lower the success rate\nof MPP-split.\nBut it's already a parameter that node operators can configure at will (at\nchannel creation time),\nso IMO it's a complexity we have to deal with anyway. Making it dynamic\nshouldn't have a high\nimpact on MPP algorithms (apart from failures while `channel_update`s are\npropagating).\n\nTo be fully honest, my (maybe unpopular) opinion about MPP is that it's not\nnecessary on the\nnetwork's backbone, only at its edges. Once the network matures, I expect\nchannels between\n\"serious\" routing nodes to be way bigger than the size of individual\npayments. The only places\nwhere there may be small or almost-empty channels are between end-users\n(wallets) and\nrouting nodes.\nIf something like Trampoline were to be implemented, MPP would only be\nneeded to reach a\nfirst routing node (short route), that routing node would aggregate the\nparts and forward as a\nsingle HTLC to the next routing node. It would be split again once it\nreaches the other edge\nof the network (for a short route as well). In a network like this, the MPP\nroutes would only have\nto be computed on a small subset of the network, which makes brute-force\nalgorithms completely\nreasonable and the success rate higher.\n\nThis is an interesting fork of the discussion, but I don't think it's a\ngood reason to prevent these\nparameters from being updated on live channels, what do you think?\n\nBastien\n\n\nLe jeu. 8 oct. 2020 \u00e0 22:05, ZmnSCPxj <ZmnSCPxj at protonmail.com> a \u00e9crit :\n\n> Good morning t-bast,\n>\n> > Please forget about channel jamming, upfront fees et al and simply\n> consider the parameters I'm\n> > mentioning. It feels to me that these are by nature dynamic channel\n> parameters (some of them are\n> > even present in `channel_update`, but no-one updates them yet because\n> direct peers don't take the\n> > update into account anyway). I'd like to raise `htlc_minimum_msat` on\n> some big channels because\n> > I'd like these channels to be used only for big-ish payments. Today I\n> can't, I have to close that\n> > channel and open a new one for such a trivial configuration update,\n> which is sad.\n>\n> At the risk of once more derailing the conversation: from the MPP\n> trenches, raising the minimum payment size is another headache.\n> The general assumption with MPP is that smaller amounts are more likely to\n> get through, but if anyone is making a significant bump up in\n> `htlc_minimum_msat`, that assumption is upended and we have to reconsider\n> if we may actually want to merge multiple failing splits into one, as well\n> as considering asymmetric splits (in particular asymmetric presplits)\n> because maybe the smaller splits will be unable to pass through the bigger\n> channels but the bigger-side split *might*.\n>\n> On the other hand: one can consider that the use of big payments as an\n> aggregation.\n> For example: a forwarding node might support smaller `htlc_minimum_msat`,\n> then after making multiple such forwards, find that a channel is now\n> heavily balanced towards one side or another.\n> It can then make a single large rebalance via one of the\n> high-`htlc_minimum_msat` channels t-bast is running.\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201009/cdca8cc5/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2020-10-11T08:50:12",
                "message_text_only": "Good morning t-bast,\n\n> Hey Zman,\n>\n> > raising the minimum payment size is another headache\n>\n> It's true that it may (depending on the algorithm) lower the success rate of MPP-split.\n> But it's already a parameter that node operators can configure at will (at channel creation time),\n> so IMO it's a complexity we have to deal with anyway. Making it dynamic shouldn't have a high\n> impact on MPP algorithms (apart from failures while `channel_update`s are propagating).\n\nRight, it should not have much impact.\n\nFor the most part, when considering the possibility of splicing in the future, we should consider that such parameters must be made changeable largely.\n\n\n>\n> To be fully honest, my (maybe unpopular) opinion about MPP is that it's not necessary on the\n> network's backbone, only at its edges. Once the network matures, I expect channels between\n> \"serious\" routing nodes to be way bigger than the size of individual payments. The only places\n> where there may be small or almost-empty channels are between end-users (wallets) and\n> routing nodes.\n> If something like Trampoline were to be implemented, MPP would only be needed to reach a\n> first routing node (short route), that routing node would aggregate the parts and forward as a\n> single HTLC to the next routing node. It would be split again once it reaches the other edge\n> of the network (for a short route as well). In a network like this, the MPP routes would only have\n> to be computed on a small subset of the network, which makes brute-force algorithms completely\n> reasonable and the success rate higher.\n\nThis makes me wonder if we really need the onions-per-channel model we currently use.\n\nFor instance, Tor is basically two-layer: there is a lower-level TCP/IP layer where packets are sent out to specific nodes on the network and this layer is completely open about where the packet should go, but there is a higher layer where onion routing between nodes is used.\n\nWe could imitate this, with HTLC packets that openly show the next destination node, but once all parts reach the destination node, it decodes and turns out to be an onion to be sent to the next destination node, and the current destination node is just another forwarder.\n\nHTLC packets could be split arbitrarily, and later nodes could potentially merge with the lower CLTV used in subsequent hops.\n\nOr not, *shrug*.\nIt has the bad problem of being more expensive on average than purely source-based routing, and probably having worse payment latency.\n\n\nFor your proposal, how sure is the receiver that the input end of the trampoline node is \"nearer\" to the payer than itself?\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2020-10-12T07:43:25",
                "message_text_only": "Good morning,\n\nFor instance, Tor is basically two-layer: there is a lower-level TCP/IP\n> layer where packets are sent out to specific nodes on the network and this\n> layer is completely open about where the packet should go, but there is a\n> higher layer where onion routing between nodes is used.\n> We could imitate this, with HTLC packets that openly show the next\n> destination node, but once all parts reach the destination node, it decodes\n> and turns out to be an onion to be sent to the next destination node, and\n> the current destination node is just another forwarder.\n\n\nThat's an interesting comment, it may be worth exploring.\nIIUC you're suggesting that payments may look like this:\n\n* Alice wants to reach Dave by going through Bob and Carol\n* An onion encodes the route Alice -> Bob -> Carol -> Dave\n* When Bob receives that onion and discovers that Carol is the next node,\nhe finds a route to Carol\nand sends it along that route, but it's not an onion, it's \"clear-text\"\nrouting\n* When Carol receives that message, she unwraps the Alice -> Bob -> Carol\n-> Dave onion to discover\nthat Dave is the next hop and applies the same steps as Bob\n\nIt looks a lot like Trampoline, but Trampoline does onion routing between\nintermediate nodes.\nYour proposal would replace that with a potentially more efficient but less\nprivate routing scheme.\nAs long as the Trampoline route does use onion routing, it could make\nsense...\n\nFor your proposal, how sure is the receiver that the input end of the\n> trampoline node is \"nearer\" to the payer than itself?\n\n\nInvoices to the rescue!\nSince lightning payments are invoice-based, recipients would add to the\ninvoice a few nodes that\nare close to them (or a partial route, which would probably be better for\nprivacy).\n\nThanks,\nBastien\n\nLe dim. 11 oct. 2020 \u00e0 10:50, ZmnSCPxj <ZmnSCPxj at protonmail.com> a \u00e9crit :\n\n> Good morning t-bast,\n>\n> > Hey Zman,\n> >\n> > > raising the minimum payment size is another headache\n> >\n> > It's true that it may (depending on the algorithm) lower the success\n> rate of MPP-split.\n> > But it's already a parameter that node operators can configure at will\n> (at channel creation time),\n> > so IMO it's a complexity we have to deal with anyway. Making it dynamic\n> shouldn't have a high\n> > impact on MPP algorithms (apart from failures while `channel_update`s\n> are propagating).\n>\n> Right, it should not have much impact.\n>\n> For the most part, when considering the possibility of splicing in the\n> future, we should consider that such parameters must be made changeable\n> largely.\n>\n>\n> >\n> > To be fully honest, my (maybe unpopular) opinion about MPP is that it's\n> not necessary on the\n> > network's backbone, only at its edges. Once the network matures, I\n> expect channels between\n> > \"serious\" routing nodes to be way bigger than the size of individual\n> payments. The only places\n> > where there may be small or almost-empty channels are between end-users\n> (wallets) and\n> > routing nodes.\n> > If something like Trampoline were to be implemented, MPP would only be\n> needed to reach a\n> > first routing node (short route), that routing node would aggregate the\n> parts and forward as a\n> > single HTLC to the next routing node. It would be split again once it\n> reaches the other edge\n> > of the network (for a short route as well). In a network like this, the\n> MPP routes would only have\n> > to be computed on a small subset of the network, which makes brute-force\n> algorithms completely\n> > reasonable and the success rate higher.\n>\n> This makes me wonder if we really need the onions-per-channel model we\n> currently use.\n>\n> For instance, Tor is basically two-layer: there is a lower-level TCP/IP\n> layer where packets are sent out to specific nodes on the network and this\n> layer is completely open about where the packet should go, but there is a\n> higher layer where onion routing between nodes is used.\n>\n> We could imitate this, with HTLC packets that openly show the next\n> destination node, but once all parts reach the destination node, it decodes\n> and turns out to be an onion to be sent to the next destination node, and\n> the current destination node is just another forwarder.\n>\n> HTLC packets could be split arbitrarily, and later nodes could potentially\n> merge with the lower CLTV used in subsequent hops.\n>\n> Or not, *shrug*.\n> It has the bad problem of being more expensive on average than purely\n> source-based routing, and probably having worse payment latency.\n>\n>\n> For your proposal, how sure is the receiver that the input end of the\n> trampoline node is \"nearer\" to the payer than itself?\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201012/8dcb280a/attachment.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2020-10-12T18:59:37",
                "message_text_only": "> I suggest adding tlv records in `commitment_signed` to tell our channel >\n> peer that we're changing the values of these fields.\n\nI think this fits in nicely with the \"parameter re-negotiation\" portion of\nmy\nloose Dynamic commitments proposal. Note that in that paradigm, something\nlike this would be a distinct message, and also only be allowed with a\n\"clean commitment\" (as otherwise what if I reduce the number of slots to a\nvalue that is lower than the number of active slots?). With this, both sides\nwould be able to propose/accept/deny updates to the flow control parameters\nthat can be used to either increase the security of a channel, or implement\na sort of \"slow start\" protocol for any new peers that connect to you.\n\nSimilar to congestion window expansion/contraction in TCP, when a new peer\nconnects to you, you likely don't want to allow them to be able to consume\nall the newly allocated bandwidth in an outgoing direction. Instead, you may\nwant to only allow them to utilize say 10% of the available HTLC bandwidth,\nslowly increasing based on successful payments, and drastically\n(multiplicatively) decreasing when you encounter very long lived HTLCs, or\nan excessive number of failures.\n\nA dynamic HTLC bandwidth allocation mechanism would serve to mitigate\nseveral classes of attacks (supplementing any mitigations by \"channel\nacceptor\" hooks), and also give forwarding nodes more _control_ of exactly\nhow their allocated bandwidth is utilized by all connected peers.  This is\npossible to some degree today (by using an implicit value lower than\nthe negotiated values), but the implicit route doesn't give the other party\nany information, and may end up in weird re-send loops (as they _why_ an\nHTLC was rejected) wasn't communicated. Also if you end up in a half-sign\nstate, since we don't have any sort of \"unadd\", then the channel may end up\nborked if the violating party keeps retransmitting the same update upon\nreconnection.\n\n> Are there other fields you think would need to become dynamic as well?\n\nOne other value that IMO should be dynamic to protect against future\nunexpected events is the dust limit. \"It Is Known\", that this value \"doesn't\nreally change\", but we should be able to upgrade _all_ channels on the fly\nif it does for w/e reason.\n\n-- Laolu\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201012/1700eb6d/attachment.html>"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2020-10-14T08:52:24",
                "message_text_only": "Hey laolu,\n\nI think this fits in nicely with the \"parameter re-negotiation\" portion of\n> my\n> loose Dynamic commitments proposal.\n\n\nYes, maybe it's better to not offer two mechanisms and wait for dynamic\ncommitments to offer that\nflexibility.\n\nInstead, you may\n> want to only allow them to utilize say 10% of the available HTLC bandwidth,\n> slowly increasing based on successful payments, and drastically\n> (multiplicatively) decreasing when you encounter very long lived HTLCs, or\n> an excessive number of failures.\n\n\nExactly, that's the kind of heuristic I had in mind. Peers need to slowly\nbuild trust before you\ngive them access to more resources.\n\nThis is\n> possible to some degree today (by using an implicit value lower than\n> the negotiated values), but the implicit route doesn't give the other party\n> any information\n\n\nAgreed, it's easy to implement locally but it's not going to be very nice\nto your peer, who has\nno way of knowing why you're rejecting HTLCs and may end up closing the\nchannel because it sees\nweird behavior. That's why we need to offer an explicit re-negotiation of\nthese parameters, let's\nkeep this use-case in mind when designing dynamic commitments!\n\nCheers,\nBastien\n\nLe lun. 12 oct. 2020 \u00e0 20:59, Olaoluwa Osuntokun <laolu32 at gmail.com> a\n\u00e9crit :\n\n>\n> > I suggest adding tlv records in `commitment_signed` to tell our channel >\n> > peer that we're changing the values of these fields.\n>\n> I think this fits in nicely with the \"parameter re-negotiation\" portion of\n> my\n> loose Dynamic commitments proposal. Note that in that paradigm, something\n> like this would be a distinct message, and also only be allowed with a\n> \"clean commitment\" (as otherwise what if I reduce the number of slots to a\n> value that is lower than the number of active slots?). With this, both\n> sides\n> would be able to propose/accept/deny updates to the flow control parameters\n> that can be used to either increase the security of a channel, or implement\n> a sort of \"slow start\" protocol for any new peers that connect to you.\n>\n> Similar to congestion window expansion/contraction in TCP, when a new peer\n> connects to you, you likely don't want to allow them to be able to consume\n> all the newly allocated bandwidth in an outgoing direction. Instead, you\n> may\n> want to only allow them to utilize say 10% of the available HTLC bandwidth,\n> slowly increasing based on successful payments, and drastically\n> (multiplicatively) decreasing when you encounter very long lived HTLCs, or\n> an excessive number of failures.\n>\n> A dynamic HTLC bandwidth allocation mechanism would serve to mitigate\n> several classes of attacks (supplementing any mitigations by \"channel\n> acceptor\" hooks), and also give forwarding nodes more _control_ of exactly\n> how their allocated bandwidth is utilized by all connected peers.  This is\n> possible to some degree today (by using an implicit value lower than\n> the negotiated values), but the implicit route doesn't give the other party\n> any information, and may end up in weird re-send loops (as they _why_ an\n> HTLC was rejected) wasn't communicated. Also if you end up in a half-sign\n> state, since we don't have any sort of \"unadd\", then the channel may end up\n> borked if the violating party keeps retransmitting the same update upon\n> reconnection.\n>\n> > Are there other fields you think would need to become dynamic as well?\n>\n> One other value that IMO should be dynamic to protect against future\n> unexpected events is the dust limit. \"It Is Known\", that this value\n> \"doesn't\n> really change\", but we should be able to upgrade _all_ channels on the fly\n> if it does for w/e reason.\n>\n> -- Laolu\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201014/87b3dc5d/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Making (some) channel limits dynamic",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Bastien TEINTURIER",
                "Olaoluwa Osuntokun",
                "Antoine Riard",
                "ZmnSCPxj"
            ],
            "messages_count": 11,
            "total_messages_chars_count": 38008
        }
    },
    {
        "title": "[Lightning-dev] Why should funders always pay on-chain fees?",
        "thread_messages": [
            {
                "author": "Bastien TEINTURIER",
                "date": "2020-10-05T13:12:51",
                "message_text_only": "Good morning list,\n\nIt seems to me that the \"funder pays all the commit tx fees\" rule exists\nsolely for simplicity\n(which was totally reasonable). I haven't been able to find much discussion\nabout this decision\non the mailing list nor in the spec commits.\n\nAt first glance, it's true that at the beginning of the channel lifetime,\nthe funder should be\nresponsible for the fee (it's his decision to open a channel after all).\nBut as time goes by and\nboth peers earn value from this channel, this rule becomes questionable.\nWe've discovered since\nthen that there is some risk associated with having pending HTLCs\n(flood-and-loot type of attacks,\npinning, channel jamming, etc).\n\nI think that *in some cases*, fundees should be paying a portion of the\ncommit-tx on-chain fees,\notherwise we may end up with a web-of-trust network where channels would\nonly exist between peers\nthat trust each other, which is quite limiting (I'm hoping we can do\nbetter).\n\nRouting nodes may be at risk when they *receive* HTLCs. All the attacks\nthat steal funds come from\nthe fact that a routing node has paid downstream but cannot claim the\nupstream HTLCs (correct me\nif that's incorrect). Thus I'd like nodes to pay for the on-chain fees of\nthe HTLCs they offer\nwhile they're pending in the commit-tx, regardless of whether they're\nfunder or fundee.\n\nThe simplest way to do this would be to deduce the HTLC cost (172 *\nfeerate) from the offerer's\nmain output (instead of the funder's main output, while keeping the base\ncommit tx weight paid\nby the funder).\n\nA more extreme proposal would be to tie the *total* commit-tx fee to the\nchannel usage:\n\n* if there are no pending HTLCs, the funder pays all the fee\n* if there are pending HTLCs, each node pays a proportion of the fee\nproportional to the number of\nHTLCs they offered. If Alice offered 1 HTLC and Bob offered 3 HTLCs, Bob\npays 75% of the\ncommit-tx fee and Alice pays 25%. When the HTLCs settle, the fee is\nredistributed.\n\nThis model uses the on-chain fee as collateral for usage of the channel. If\nAlice wants to forward\nHTLCs through this channel (because she has something to gain - routing\nfees), she should be taking\non some of the associated risk, not Bob. Bob will be taking the same risk\ndownstream if he chooses\nto forward.\n\nI believe it also forces the fundee to care about on-chain feerates, which\nis a healthy incentive.\nIt may create a feedback loop between on-chain feerates and routing fees,\nwhich I believe is also\na good long-term thing (but it's hard to predict as there may be negative\nside-effects as well).\n\nWhat do you all think? Is this a terrible idea? Is it okay-ish, but not\nworth the additional\ncomplexity? Is it an amazing idea worth a lightning nobel? Please don't\ntake any of my claims\nfor granted and challenge them, there may be negative side-effects I'm\ncompletely missing, this is\na fragile game of incentives...\n\nSide-note: don't forget to take into account that the fees for HTLC\ntransactions (second-level txs)\nare always paid by the party that broadcasts them (which makes sense). I\nstill think this is not\nenough and can even be abused by fundees in some setups.\n\nThanks,\nBastien\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201005/521584fe/attachment.html>"
            },
            {
                "author": "darosior",
                "date": "2020-10-05T13:25:15",
                "message_text_only": "Hi Bastien,\n\n> I think that *in some cases*, fundees should be paying a portion of the commit-tx on-chain fees,\n> otherwise we may end up with a web-of-trust network where channels would only exist between peers\n> that trust each other, which is quite limiting (I'm hoping we can do better).\n\nAgreed.\nHowever in an anchor outputs future the funder only pays for the \"backbone\" fees of the channel and the fees necessary to secure the confirmation of transactions is paid in second stage by each interested party (*). It seems to me to be a reasonable middle-ground.\n\n(*) Credits to ZmnSCPxj for pointing this out to me on IRC.\n\nDarosior\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201005/1f406e21/attachment.html>"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2020-10-05T13:30:47",
                "message_text_only": "Hi darosior,\n\nThis is true, but we haven't solved yet how to estimate a good enough\n`min_relay_fee` that works\nfor end-to-end tx propagation over the network.\n\nWe've discussed this during the last two spec meetings, but it's still\nunclear whether we'll be able to solve\nthis before package-relay lands in bitcoin, so I wanted to explore this as\na potential more short-term\nsolution. But maybe it's not worth the effort and we should focus more on\nanchors and `min_relay_fee`,\nwe'll see ;)\n\nBastien\n\nLe lun. 5 oct. 2020 \u00e0 15:25, darosior <darosior at protonmail.com> a \u00e9crit :\n\n> Hi Bastien,\n>\n>\n> I think that *in some cases*, fundees should be paying a portion of the\n> commit-tx on-chain fees,\n> otherwise we may end up with a web-of-trust network where channels would\n> only exist between peers\n> that trust each other, which is quite limiting (I'm hoping we can do\n> better).\n>\n>\n> Agreed.\n> However in an anchor outputs future the funder only pays for the\n> \"backbone\" fees of the channel and the fees necessary to secure the\n> confirmation of transactions is paid in second stage by each interested\n> party (*). It seems to me to be a reasonable middle-ground.\n>\n> (*) Credits to ZmnSCPxj for pointing this out to me on IRC.\n>\n> Darosior\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201005/b987b4a9/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2020-10-05T16:06:46",
                "message_text_only": "Good morning Bastien,\n\n> Good morning list,\n>\n> It seems to me that the \"funder pays all the commit tx fees\" rule exists solely for simplicity\n> (which was totally reasonable). I haven't been able to find much discussion about this decision\n> on the mailing list nor in the spec commits.\n\n\nThis indeed seems to be one of those \"unwritten rules\", like the unwritten rule that SCRIPT opcodes can only look at the transaction they are in and cannot look at the block it got confirmed in, or previous blocks.\n\nMy rough understanding is that it prevents this kind of griefing attack:\n\n* You have a node with channels balanced just as you like it, very lucrative, earning entire satoshis of routing fees per week.\n* I have a bunch of funds lying around, much larger than your liquidity.\n* I create a temporary throwaway node and have it make a large channel to you about equal to your liquidity.\n* I send out all of the funds on the new large channel, preferably to offchain-to-onchain swaps like Loop or Boltz.\n  * This shrinks the outgoing liquidity you have available on your existing channels, making it difficult for you to route and earn routing fees -- in order to route, you need to have *both* incoming and outgoing liquidity.\n  * If I do not make another channel with my new node, then you cannot rebalance offchain.\n* Since my new large channel is now exhausted, I can delete the throwaway node forever.\n  * The `close_to` feature also allows me to arrange the funds to be closed to a cold-storage funds.\n  * Or I can just keep the necessary keys to recover funds from the unilateral channel close broadcast by you later.\n  * There is a reserve, but that is tiny, and if so I can probably afford to wait --- this is a game of chicken on who closes the channel, and since most of the channel is on your end, you are at a serious disadvantage.\n* You are now forced to unilateral close the channel so you can send it to an onchain-to-offchain swap and reload your channels into the proper balance it had before I attacked you.\n\nIf the unilateral close on the channel were paid by you, then that is adding insult to injury.\nThus, it is better if the unilateral close were paid by me, even if ultimately the unilateral close is performed by you.\nThis is \"initiator pays\" principle.\n\n--\n\nOn the other hand, a quick skim of your proposal suggests that it still respects the \"initiator pays\" principle.\nBasically, the fundee only pays fees for HTLCs they initiated, which is not relevant to the above attack (since in the above attack, my node is a dead end, you will never send out an HTLC through my channel to rebalance).\nSo it should still be acceptable.\n\nRegards,\nZmnSCPxj\n\n\n>\n> At first glance, it's true that at the beginning of the channel lifetime, the funder should be\n> responsible for the fee (it's his decision to open a channel after all). But as time goes by and\n> both peers earn value from this channel, this rule becomes questionable. We've discovered since\n> then that there is some risk associated with having pending HTLCs (flood-and-loot type of attacks,\n> pinning, channel jamming, etc).\n>\n> I think that *in some cases*, fundees should be paying a portion of the commit-tx on-chain fees,\n> otherwise we may end up with a web-of-trust network where channels would only exist between peers\n> that trust each other, which is quite limiting (I'm hoping we can do better).\n>\n> Routing nodes may be at risk when they *receive* HTLCs. All the attacks that steal funds come from\n> the fact that a routing node has paid downstream but cannot claim the upstream HTLCs (correct me\n> if that's incorrect). Thus I'd like nodes to pay for the on-chain fees of the HTLCs they offer\n> while they're pending in the commit-tx, regardless of whether they're funder or fundee.\n>\n> The simplest way to do this would be to deduce the HTLC cost (172 * feerate) from the offerer's\n> main output (instead of the funder's main output, while keeping the base commit tx weight paid\n> by the funder).\n>\n> A more extreme proposal would be to tie the *total* commit-tx fee to the channel usage:\n>\n> * if there are no pending HTLCs, the funder pays all the fee\n> * if there are pending HTLCs, each node pays a proportion of the fee proportional to the number of\n> HTLCs they offered. If Alice offered 1 HTLC and Bob offered 3 HTLCs, Bob pays 75% of the\n> commit-tx fee and Alice pays 25%. When the HTLCs settle, the fee is redistributed.\n>\n> This model uses the on-chain fee as collateral for usage of the channel. If Alice wants to forward\n> HTLCs through this channel (because she has something to gain - routing fees), she should be taking\n> on some of the associated risk, not Bob. Bob will be taking the same risk downstream if he chooses\n> to forward.\n>\n> I believe it also forces the fundee to care about on-chain feerates, which is a healthy incentive.\n> It may create a feedback loop between on-chain feerates and routing fees, which I believe is also\n> a good long-term thing (but it's hard to predict as there may be negative side-effects as well).\n>\n> What do you all think? Is this a terrible idea? Is it okay-ish, but not worth the additional\n> complexity? Is it an amazing idea worth a lightning nobel? Please don't take any of my claims\n> for granted and challenge them, there may be negative side-effects I'm completely missing, this is\n> a fragile game of incentives...\n>\n> Side-note: don't forget to take into account that the fees for HTLC transactions (second-level txs)\n> are always paid by the party that broadcasts them (which makes sense). I still think this is not\n> enough and can even be abused by fundees in some setups.\n>\n> Thanks,\n> Bastien"
            },
            {
                "author": "Antoine Riard",
                "date": "2020-10-06T16:30:31",
                "message_text_only": "Hello Bastien,\n\nI'm all in for a model where channel transactions are pre-signed with a\nreasonable minimal relay fee and the adjustment is done by the closer. The\nchannel initiator shouldn't have to pay for channel-closing as it's somehow\na liquidity allocation decision (\"My balance could be better allocated\nelsewhere than in this channel\").\n\nThat said, a channel closing might be triggered due to a security\nmechanism, like a HTLC to timeout onchain. Thus a malicious counterparty\ncan easily loop a HTLC forwarding on an honest peer. Then not cancel it\non-time to force the honest counterparty to pay onchain fees to avoid a\noffered HTLC not being claimed back on time.\n\nAFAICT, this issue is not solved by anchor outputs. A way to decentivize\nthis kind of behavior from a malicious counterparty is an upfront payment\nwhere the upholding HTLC fee * HTLC block-buffer-before-onchain is higher\nthan the cost of going onchain. It should cost higher for the counterparty\nto withhold a HTLC than paying onchain-fees to close the channel.\n\nOr can you think about another mitigation for the issue raised above ?\n\nAntoine\n\nLe lun. 5 oct. 2020 \u00e0 09:13, Bastien TEINTURIER via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Good morning list,\n>\n> It seems to me that the \"funder pays all the commit tx fees\" rule exists\n> solely for simplicity\n> (which was totally reasonable). I haven't been able to find much\n> discussion about this decision\n> on the mailing list nor in the spec commits.\n>\n> At first glance, it's true that at the beginning of the channel lifetime,\n> the funder should be\n> responsible for the fee (it's his decision to open a channel after all).\n> But as time goes by and\n> both peers earn value from this channel, this rule becomes questionable.\n> We've discovered since\n> then that there is some risk associated with having pending HTLCs\n> (flood-and-loot type of attacks,\n> pinning, channel jamming, etc).\n>\n> I think that *in some cases*, fundees should be paying a portion of the\n> commit-tx on-chain fees,\n> otherwise we may end up with a web-of-trust network where channels would\n> only exist between peers\n> that trust each other, which is quite limiting (I'm hoping we can do\n> better).\n>\n> Routing nodes may be at risk when they *receive* HTLCs. All the attacks\n> that steal funds come from\n> the fact that a routing node has paid downstream but cannot claim the\n> upstream HTLCs (correct me\n> if that's incorrect). Thus I'd like nodes to pay for the on-chain fees of\n> the HTLCs they offer\n> while they're pending in the commit-tx, regardless of whether they're\n> funder or fundee.\n>\n> The simplest way to do this would be to deduce the HTLC cost (172 *\n> feerate) from the offerer's\n> main output (instead of the funder's main output, while keeping the base\n> commit tx weight paid\n> by the funder).\n>\n> A more extreme proposal would be to tie the *total* commit-tx fee to the\n> channel usage:\n>\n> * if there are no pending HTLCs, the funder pays all the fee\n> * if there are pending HTLCs, each node pays a proportion of the fee\n> proportional to the number of\n> HTLCs they offered. If Alice offered 1 HTLC and Bob offered 3 HTLCs, Bob\n> pays 75% of the\n> commit-tx fee and Alice pays 25%. When the HTLCs settle, the fee is\n> redistributed.\n>\n> This model uses the on-chain fee as collateral for usage of the channel.\n> If Alice wants to forward\n> HTLCs through this channel (because she has something to gain - routing\n> fees), she should be taking\n> on some of the associated risk, not Bob. Bob will be taking the same risk\n> downstream if he chooses\n> to forward.\n>\n> I believe it also forces the fundee to care about on-chain feerates, which\n> is a healthy incentive.\n> It may create a feedback loop between on-chain feerates and routing fees,\n> which I believe is also\n> a good long-term thing (but it's hard to predict as there may be negative\n> side-effects as well).\n>\n> What do you all think? Is this a terrible idea? Is it okay-ish, but not\n> worth the additional\n> complexity? Is it an amazing idea worth a lightning nobel? Please don't\n> take any of my claims\n> for granted and challenge them, there may be negative side-effects I'm\n> completely missing, this is\n> a fragile game of incentives...\n>\n> Side-note: don't forget to take into account that the fees for HTLC\n> transactions (second-level txs)\n> are always paid by the party that broadcasts them (which makes sense). I\n> still think this is not\n> enough and can even be abused by fundees in some setups.\n>\n> Thanks,\n> Bastien\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201006/eb3eaa1c/attachment.html>"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2020-10-08T15:30:34",
                "message_text_only": "Thanks (again) Antoine and Zman for your answers,\n\nOn the other hand, a quick skim of your proposal suggests that it still\n> respects the \"initiator pays\" principle.\n> Basically, the fundee only pays fees for HTLCs they initiated, which is\n> not relevant to the above attack (since in the above attack, my node is a\n> dead end, you will never send out an HTLC through my channel to rebalance).\n> So it should still be acceptable.\n\n\nI agree, my proposal would have the same result as today's behavior in that\ncase.\nUnless your throw-away node waited for me to add an HTLC in its channel, in\nthat case I would pay a\npart of the fee (since I'm adding that HTLC). That leans towards the first\nof my two proposals,\nwhere the funder always pays the \"base\" fee and htlc fees are split\ndepending on who proposed the HTLC.\n\nThe channel initiator shouldn't have to pay for channel-closing as it's\n> somehow a liquidity allocation decision\n\n\nI agree 100%. Especially since mutual closing should be preferred most of\nthe time.\n\nThat said, a channel closing might be triggered due to a security\n> mechanism, like a HTLC to timeout onchain. Thus a malicious counterparty\n> can easily loop a HTLC forwarding on an honest peer. Then not cancel it\n> on-time to force the honest counterparty to pay onchain fees to avoid a\n> offered HTLC not being claimed back on time.\n\n\nYes, this is an issue, but the only way to fix it today is to never be the\nfunder, always be fundee\nand I think that creates unhealthy, assymetric incentives.\n\nThis is a scenario where the other node will only burn you once; if you\nnotice that behavior you'll\nbe forced to pay on-chain fees, but you'll ban this peer. And if he opened\nthe channel to you, he'll\nstill be paying the \"base\" fee. I don't think there's a silver bullet here\nwhere you can completely\navoid being bitten by such malicious nodes, but you can reduce exposure and\nban them after the fact.\n\nAnother note on using a minimal relay fee; in a potential future where\non-chain fees are always\nhigh and layer 1 is consistently busy, even that minimal relay fee will be\ncostly. You'll want your\npeer to pay for the HTLCs it's responsible for to split the on-chain fee\nmore fairly. So I believe\nmoving (slightly) away from the \"funder pays all\" model is desirable (or at\nleast it's worth\nexploring seriously in order to have a better reason to dismiss it than\n\"it's simpler\").\n\nDoes that make sense?\n\nThanks,\nBastien\n\nLe mar. 6 oct. 2020 \u00e0 18:30, Antoine Riard <antoine.riard at gmail.com> a\n\u00e9crit :\n\n> Hello Bastien,\n>\n> I'm all in for a model where channel transactions are pre-signed with a\n> reasonable minimal relay fee and the adjustment is done by the closer. The\n> channel initiator shouldn't have to pay for channel-closing as it's somehow\n> a liquidity allocation decision (\"My balance could be better allocated\n> elsewhere than in this channel\").\n>\n> That said, a channel closing might be triggered due to a security\n> mechanism, like a HTLC to timeout onchain. Thus a malicious counterparty\n> can easily loop a HTLC forwarding on an honest peer. Then not cancel it\n> on-time to force the honest counterparty to pay onchain fees to avoid a\n> offered HTLC not being claimed back on time.\n>\n> AFAICT, this issue is not solved by anchor outputs. A way to decentivize\n> this kind of behavior from a malicious counterparty is an upfront payment\n> where the upholding HTLC fee * HTLC block-buffer-before-onchain is higher\n> than the cost of going onchain. It should cost higher for the counterparty\n> to withhold a HTLC than paying onchain-fees to close the channel.\n>\n> Or can you think about another mitigation for the issue raised above ?\n>\n> Antoine\n>\n> Le lun. 5 oct. 2020 \u00e0 09:13, Bastien TEINTURIER via Lightning-dev <\n> lightning-dev at lists.linuxfoundation.org> a \u00e9crit :\n>\n>> Good morning list,\n>>\n>> It seems to me that the \"funder pays all the commit tx fees\" rule exists\n>> solely for simplicity\n>> (which was totally reasonable). I haven't been able to find much\n>> discussion about this decision\n>> on the mailing list nor in the spec commits.\n>>\n>> At first glance, it's true that at the beginning of the channel lifetime,\n>> the funder should be\n>> responsible for the fee (it's his decision to open a channel after all).\n>> But as time goes by and\n>> both peers earn value from this channel, this rule becomes questionable.\n>> We've discovered since\n>> then that there is some risk associated with having pending HTLCs\n>> (flood-and-loot type of attacks,\n>> pinning, channel jamming, etc).\n>>\n>> I think that *in some cases*, fundees should be paying a portion of the\n>> commit-tx on-chain fees,\n>> otherwise we may end up with a web-of-trust network where channels would\n>> only exist between peers\n>> that trust each other, which is quite limiting (I'm hoping we can do\n>> better).\n>>\n>> Routing nodes may be at risk when they *receive* HTLCs. All the attacks\n>> that steal funds come from\n>> the fact that a routing node has paid downstream but cannot claim the\n>> upstream HTLCs (correct me\n>> if that's incorrect). Thus I'd like nodes to pay for the on-chain fees of\n>> the HTLCs they offer\n>> while they're pending in the commit-tx, regardless of whether they're\n>> funder or fundee.\n>>\n>> The simplest way to do this would be to deduce the HTLC cost (172 *\n>> feerate) from the offerer's\n>> main output (instead of the funder's main output, while keeping the base\n>> commit tx weight paid\n>> by the funder).\n>>\n>> A more extreme proposal would be to tie the *total* commit-tx fee to the\n>> channel usage:\n>>\n>> * if there are no pending HTLCs, the funder pays all the fee\n>> * if there are pending HTLCs, each node pays a proportion of the fee\n>> proportional to the number of\n>> HTLCs they offered. If Alice offered 1 HTLC and Bob offered 3 HTLCs, Bob\n>> pays 75% of the\n>> commit-tx fee and Alice pays 25%. When the HTLCs settle, the fee is\n>> redistributed.\n>>\n>> This model uses the on-chain fee as collateral for usage of the channel.\n>> If Alice wants to forward\n>> HTLCs through this channel (because she has something to gain - routing\n>> fees), she should be taking\n>> on some of the associated risk, not Bob. Bob will be taking the same risk\n>> downstream if he chooses\n>> to forward.\n>>\n>> I believe it also forces the fundee to care about on-chain feerates,\n>> which is a healthy incentive.\n>> It may create a feedback loop between on-chain feerates and routing fees,\n>> which I believe is also\n>> a good long-term thing (but it's hard to predict as there may be negative\n>> side-effects as well).\n>>\n>> What do you all think? Is this a terrible idea? Is it okay-ish, but not\n>> worth the additional\n>> complexity? Is it an amazing idea worth a lightning nobel? Please don't\n>> take any of my claims\n>> for granted and challenge them, there may be negative side-effects I'm\n>> completely missing, this is\n>> a fragile game of incentives...\n>>\n>> Side-note: don't forget to take into account that the fees for HTLC\n>> transactions (second-level txs)\n>> are always paid by the party that broadcasts them (which makes sense). I\n>> still think this is not\n>> enough and can even be abused by fundees in some setups.\n>>\n>> Thanks,\n>> Bastien\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201008/45ed00b7/attachment.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2020-10-12T18:48:57",
                "message_text_only": "> It seems to me that the \"funder pays all the commit tx fees\" rule exists\n> solely for simplicity (which was totally reasonable).\n\nAt this stage, I've learned that simplicity (when doing anything that\ninvolves multi-party on-chain fee negotiating/verification/enforcement can\nreally go a long way). Just think about all the edge cases w.r.t _allocating\nenough funds to pay for fees_ we've discovered over the past few years in\nthe state machine. I fear adding a more elaborate fee splitting mechanism\nwould only blow up the number of obscure edge cases that may lead to a\nchannel temporarily or permanently being \"borked\".\n\nIf we're going to add a \"fairer\" way of splitting fees, we'll really need to\ndig down pre-deployment to ensure that we've explored any resulting edge\ncases within our solution space, as we'll only be _adding_ complexity to fee\nsplitting.\n\nIMO, anchor commitments in their \"final form\" (fixed fee rate on commitment\ntransaction, only \"emergency\" use of update_fee) significantly simplifies\nthings as it shifts from \"funding pay fees\", to \"broadcaster/confirmer pays\nfees\". However, as you note this doesn't fully distribute the worst-case\ncost of needing to go to chain with a \"fully loaded\" commitment transaction.\nEven with HTLCs, they could only be signed at 1 sat/byte from the funder's\nperspective, once again putting the burden on the broadcaster/confirmer to\nmake up the difference.\n\n-- Laolu\n\n\nOn Mon, Oct 5, 2020 at 6:13 AM Bastien TEINTURIER via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> wrote:\n\n> Good morning list,\n>\n> It seems to me that the \"funder pays all the commit tx fees\" rule exists\n> solely for simplicity\n> (which was totally reasonable). I haven't been able to find much\n> discussion about this decision\n> on the mailing list nor in the spec commits.\n>\n> At first glance, it's true that at the beginning of the channel lifetime,\n> the funder should be\n> responsible for the fee (it's his decision to open a channel after all).\n> But as time goes by and\n> both peers earn value from this channel, this rule becomes questionable.\n> We've discovered since\n> then that there is some risk associated with having pending HTLCs\n> (flood-and-loot type of attacks,\n> pinning, channel jamming, etc).\n>\n> I think that *in some cases*, fundees should be paying a portion of the\n> commit-tx on-chain fees,\n> otherwise we may end up with a web-of-trust network where channels would\n> only exist between peers\n> that trust each other, which is quite limiting (I'm hoping we can do\n> better).\n>\n> Routing nodes may be at risk when they *receive* HTLCs. All the attacks\n> that steal funds come from\n> the fact that a routing node has paid downstream but cannot claim the\n> upstream HTLCs (correct me\n> if that's incorrect). Thus I'd like nodes to pay for the on-chain fees of\n> the HTLCs they offer\n> while they're pending in the commit-tx, regardless of whether they're\n> funder or fundee.\n>\n> The simplest way to do this would be to deduce the HTLC cost (172 *\n> feerate) from the offerer's\n> main output (instead of the funder's main output, while keeping the base\n> commit tx weight paid\n> by the funder).\n>\n> A more extreme proposal would be to tie the *total* commit-tx fee to the\n> channel usage:\n>\n> * if there are no pending HTLCs, the funder pays all the fee\n> * if there are pending HTLCs, each node pays a proportion of the fee\n> proportional to the number of\n> HTLCs they offered. If Alice offered 1 HTLC and Bob offered 3 HTLCs, Bob\n> pays 75% of the\n> commit-tx fee and Alice pays 25%. When the HTLCs settle, the fee is\n> redistributed.\n>\n> This model uses the on-chain fee as collateral for usage of the channel.\n> If Alice wants to forward\n> HTLCs through this channel (because she has something to gain - routing\n> fees), she should be taking\n> on some of the associated risk, not Bob. Bob will be taking the same risk\n> downstream if he chooses\n> to forward.\n>\n> I believe it also forces the fundee to care about on-chain feerates, which\n> is a healthy incentive.\n> It may create a feedback loop between on-chain feerates and routing fees,\n> which I believe is also\n> a good long-term thing (but it's hard to predict as there may be negative\n> side-effects as well).\n>\n> What do you all think? Is this a terrible idea? Is it okay-ish, but not\n> worth the additional\n> complexity? Is it an amazing idea worth a lightning nobel? Please don't\n> take any of my claims\n> for granted and challenge them, there may be negative side-effects I'm\n> completely missing, this is\n> a fragile game of incentives...\n>\n> Side-note: don't forget to take into account that the fees for HTLC\n> transactions (second-level txs)\n> are always paid by the party that broadcasts them (which makes sense). I\n> still think this is not\n> enough and can even be abused by fundees in some setups.\n>\n> Thanks,\n> Bastien\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201012/148e00a3/attachment-0001.html>"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2020-10-14T08:24:49",
                "message_text_only": "I totally agree with the simplicity argument, I wanted to raise this\nbecause it's (IMO) an issue\ntoday because of the way we deal with on-chain fees, but it's less\nimpactful once update_fee is\nscoped to some min_relay_fee.\n\nLet's put this aside for now then and we can revisit later if needed.\n\nThanks for the feedback everyone!\nBastien\n\nLe lun. 12 oct. 2020 \u00e0 20:49, Olaoluwa Osuntokun <laolu32 at gmail.com> a\n\u00e9crit :\n\n> > It seems to me that the \"funder pays all the commit tx fees\" rule exists\n> > solely for simplicity (which was totally reasonable).\n>\n> At this stage, I've learned that simplicity (when doing anything that\n> involves multi-party on-chain fee negotiating/verification/enforcement can\n> really go a long way). Just think about all the edge cases w.r.t\n> _allocating\n> enough funds to pay for fees_ we've discovered over the past few years in\n> the state machine. I fear adding a more elaborate fee splitting mechanism\n> would only blow up the number of obscure edge cases that may lead to a\n> channel temporarily or permanently being \"borked\".\n>\n> If we're going to add a \"fairer\" way of splitting fees, we'll really need\n> to\n> dig down pre-deployment to ensure that we've explored any resulting edge\n> cases within our solution space, as we'll only be _adding_ complexity to\n> fee\n> splitting.\n>\n> IMO, anchor commitments in their \"final form\" (fixed fee rate on commitment\n> transaction, only \"emergency\" use of update_fee) significantly simplifies\n> things as it shifts from \"funding pay fees\", to \"broadcaster/confirmer pays\n> fees\". However, as you note this doesn't fully distribute the worst-case\n> cost of needing to go to chain with a \"fully loaded\" commitment\n> transaction.\n> Even with HTLCs, they could only be signed at 1 sat/byte from the funder's\n> perspective, once again putting the burden on the broadcaster/confirmer to\n> make up the difference.\n>\n> -- Laolu\n>\n>\n> On Mon, Oct 5, 2020 at 6:13 AM Bastien TEINTURIER via Lightning-dev <\n> lightning-dev at lists.linuxfoundation.org> wrote:\n>\n>> Good morning list,\n>>\n>> It seems to me that the \"funder pays all the commit tx fees\" rule exists\n>> solely for simplicity\n>> (which was totally reasonable). I haven't been able to find much\n>> discussion about this decision\n>> on the mailing list nor in the spec commits.\n>>\n>> At first glance, it's true that at the beginning of the channel lifetime,\n>> the funder should be\n>> responsible for the fee (it's his decision to open a channel after all).\n>> But as time goes by and\n>> both peers earn value from this channel, this rule becomes questionable.\n>> We've discovered since\n>> then that there is some risk associated with having pending HTLCs\n>> (flood-and-loot type of attacks,\n>> pinning, channel jamming, etc).\n>>\n>> I think that *in some cases*, fundees should be paying a portion of the\n>> commit-tx on-chain fees,\n>> otherwise we may end up with a web-of-trust network where channels would\n>> only exist between peers\n>> that trust each other, which is quite limiting (I'm hoping we can do\n>> better).\n>>\n>> Routing nodes may be at risk when they *receive* HTLCs. All the attacks\n>> that steal funds come from\n>> the fact that a routing node has paid downstream but cannot claim the\n>> upstream HTLCs (correct me\n>> if that's incorrect). Thus I'd like nodes to pay for the on-chain fees of\n>> the HTLCs they offer\n>> while they're pending in the commit-tx, regardless of whether they're\n>> funder or fundee.\n>>\n>> The simplest way to do this would be to deduce the HTLC cost (172 *\n>> feerate) from the offerer's\n>> main output (instead of the funder's main output, while keeping the base\n>> commit tx weight paid\n>> by the funder).\n>>\n>> A more extreme proposal would be to tie the *total* commit-tx fee to the\n>> channel usage:\n>>\n>> * if there are no pending HTLCs, the funder pays all the fee\n>> * if there are pending HTLCs, each node pays a proportion of the fee\n>> proportional to the number of\n>> HTLCs they offered. If Alice offered 1 HTLC and Bob offered 3 HTLCs, Bob\n>> pays 75% of the\n>> commit-tx fee and Alice pays 25%. When the HTLCs settle, the fee is\n>> redistributed.\n>>\n>> This model uses the on-chain fee as collateral for usage of the channel.\n>> If Alice wants to forward\n>> HTLCs through this channel (because she has something to gain - routing\n>> fees), she should be taking\n>> on some of the associated risk, not Bob. Bob will be taking the same risk\n>> downstream if he chooses\n>> to forward.\n>>\n>> I believe it also forces the fundee to care about on-chain feerates,\n>> which is a healthy incentive.\n>> It may create a feedback loop between on-chain feerates and routing fees,\n>> which I believe is also\n>> a good long-term thing (but it's hard to predict as there may be negative\n>> side-effects as well).\n>>\n>> What do you all think? Is this a terrible idea? Is it okay-ish, but not\n>> worth the additional\n>> complexity? Is it an amazing idea worth a lightning nobel? Please don't\n>> take any of my claims\n>> for granted and challenge them, there may be negative side-effects I'm\n>> completely missing, this is\n>> a fragile game of incentives...\n>>\n>> Side-note: don't forget to take into account that the fees for HTLC\n>> transactions (second-level txs)\n>> are always paid by the party that broadcasts them (which makes sense). I\n>> still think this is not\n>> enough and can even be abused by fundees in some setups.\n>>\n>> Thanks,\n>> Bastien\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201014/faaa0eeb/attachment-0001.html>"
            },
            {
                "author": "Johan Tor\u00e5s Halseth",
                "date": "2020-10-16T10:30:55",
                "message_text_only": "Many good thoughts here.\n\nPersonally I think we should design any changes for a package-relay\nfuture, where the commitment can be 0-fee, update_fee doesn't longer\nexist and fees are only decided upon on channel close.\n\n- johan\n\n\nOn Wed, Oct 14, 2020 at 10:25 AM Bastien TEINTURIER via Lightning-dev\n<lightning-dev at lists.linuxfoundation.org> wrote:\n>\n> I totally agree with the simplicity argument, I wanted to raise this because it's (IMO) an issue\n> today because of the way we deal with on-chain fees, but it's less impactful once update_fee is\n> scoped to some min_relay_fee.\n>\n> Let's put this aside for now then and we can revisit later if needed.\n>\n> Thanks for the feedback everyone!\n> Bastien\n>\n> Le lun. 12 oct. 2020 \u00e0 20:49, Olaoluwa Osuntokun <laolu32 at gmail.com> a \u00e9crit :\n>>\n>> > It seems to me that the \"funder pays all the commit tx fees\" rule exists\n>> > solely for simplicity (which was totally reasonable).\n>>\n>> At this stage, I've learned that simplicity (when doing anything that\n>> involves multi-party on-chain fee negotiating/verification/enforcement can\n>> really go a long way). Just think about all the edge cases w.r.t _allocating\n>> enough funds to pay for fees_ we've discovered over the past few years in\n>> the state machine. I fear adding a more elaborate fee splitting mechanism\n>> would only blow up the number of obscure edge cases that may lead to a\n>> channel temporarily or permanently being \"borked\".\n>>\n>> If we're going to add a \"fairer\" way of splitting fees, we'll really need to\n>> dig down pre-deployment to ensure that we've explored any resulting edge\n>> cases within our solution space, as we'll only be _adding_ complexity to fee\n>> splitting.\n>>\n>> IMO, anchor commitments in their \"final form\" (fixed fee rate on commitment\n>> transaction, only \"emergency\" use of update_fee) significantly simplifies\n>> things as it shifts from \"funding pay fees\", to \"broadcaster/confirmer pays\n>> fees\". However, as you note this doesn't fully distribute the worst-case\n>> cost of needing to go to chain with a \"fully loaded\" commitment transaction.\n>> Even with HTLCs, they could only be signed at 1 sat/byte from the funder's\n>> perspective, once again putting the burden on the broadcaster/confirmer to\n>> make up the difference.\n>>\n>> -- Laolu\n>>\n>>\n>> On Mon, Oct 5, 2020 at 6:13 AM Bastien TEINTURIER via Lightning-dev <lightning-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>> Good morning list,\n>>>\n>>> It seems to me that the \"funder pays all the commit tx fees\" rule exists solely for simplicity\n>>> (which was totally reasonable). I haven't been able to find much discussion about this decision\n>>> on the mailing list nor in the spec commits.\n>>>\n>>> At first glance, it's true that at the beginning of the channel lifetime, the funder should be\n>>> responsible for the fee (it's his decision to open a channel after all). But as time goes by and\n>>> both peers earn value from this channel, this rule becomes questionable. We've discovered since\n>>> then that there is some risk associated with having pending HTLCs (flood-and-loot type of attacks,\n>>> pinning, channel jamming, etc).\n>>>\n>>> I think that *in some cases*, fundees should be paying a portion of the commit-tx on-chain fees,\n>>> otherwise we may end up with a web-of-trust network where channels would only exist between peers\n>>> that trust each other, which is quite limiting (I'm hoping we can do better).\n>>>\n>>> Routing nodes may be at risk when they *receive* HTLCs. All the attacks that steal funds come from\n>>> the fact that a routing node has paid downstream but cannot claim the upstream HTLCs (correct me\n>>> if that's incorrect). Thus I'd like nodes to pay for the on-chain fees of the HTLCs they offer\n>>> while they're pending in the commit-tx, regardless of whether they're funder or fundee.\n>>>\n>>> The simplest way to do this would be to deduce the HTLC cost (172 * feerate) from the offerer's\n>>> main output (instead of the funder's main output, while keeping the base commit tx weight paid\n>>> by the funder).\n>>>\n>>> A more extreme proposal would be to tie the *total* commit-tx fee to the channel usage:\n>>>\n>>> * if there are no pending HTLCs, the funder pays all the fee\n>>> * if there are pending HTLCs, each node pays a proportion of the fee proportional to the number of\n>>> HTLCs they offered. If Alice offered 1 HTLC and Bob offered 3 HTLCs, Bob pays 75% of the\n>>> commit-tx fee and Alice pays 25%. When the HTLCs settle, the fee is redistributed.\n>>>\n>>> This model uses the on-chain fee as collateral for usage of the channel. If Alice wants to forward\n>>> HTLCs through this channel (because she has something to gain - routing fees), she should be taking\n>>> on some of the associated risk, not Bob. Bob will be taking the same risk downstream if he chooses\n>>> to forward.\n>>>\n>>> I believe it also forces the fundee to care about on-chain feerates, which is a healthy incentive.\n>>> It may create a feedback loop between on-chain feerates and routing fees, which I believe is also\n>>> a good long-term thing (but it's hard to predict as there may be negative side-effects as well).\n>>>\n>>> What do you all think? Is this a terrible idea? Is it okay-ish, but not worth the additional\n>>> complexity? Is it an amazing idea worth a lightning nobel? Please don't take any of my claims\n>>> for granted and challenge them, there may be negative side-effects I'm completely missing, this is\n>>> a fragile game of incentives...\n>>>\n>>> Side-note: don't forget to take into account that the fees for HTLC transactions (second-level txs)\n>>> are always paid by the party that broadcasts them (which makes sense). I still think this is not\n>>> enough and can even be abused by fundees in some setups.\n>>>\n>>> Thanks,\n>>> Bastien\n>>> _______________________________________________\n>>> Lightning-dev mailing list\n>>> Lightning-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            }
        ],
        "thread_summary": {
            "title": "Why should funders always pay on-chain fees?",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Antoine Riard",
                "darosior",
                "Bastien TEINTURIER",
                "Johan Tor\u00e5s Halseth",
                "Olaoluwa Osuntokun",
                "ZmnSCPxj"
            ],
            "messages_count": 9,
            "total_messages_chars_count": 40979
        }
    },
    {
        "title": "[Lightning-dev] Incremental Routing (Was: Making (some) channel limits dynamic)",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2020-10-07T17:33:38",
                "message_text_only": "Good morning Antoine, Bastien, and list,\n\n> > Instead of relying on reputation, the other alternative is just to have an upfront payment system, where a relay node doesn't have to account for a HTLC issuer reputation to decide acceptance and can just forward a HTLC as long it paid enough. More, I think it's better to mitigate jamming with a fees-based system than a web-of-trust one, less burden on network newcomers.\n>\n> Let us consider some of the complications here.\n>\n> A newcomer wants to make an outgoing payment.\n> Speculatively, it connects to some existing nodes based on some policy.\n>\n> Now, since forwarding is upfront, the newcomer fears that the node it connected to might not even bother forwarding the payment, and instead just fail it and claim the upfront fees.\n>\n> In particular: how would the newcomer offer upfront fees to a node it is not directly channeled with?\n> In order to do that, we would have to offer the upfront fees for that node, to the node we are channeled with, so it can forward this as well.\n>\n> -   We can give the upfront fee outright to the first hop, and trust that if it forwards, it will also forward the upfront fee for the next hop.\n>     -   The first hop would then prefer to just fail the HTLC then and there and steal all the upfront fees.\n>         -   After all, the offerrer is a newcomer, and might be the sybil of a hacker that is trying to tie up its liquidity.\n>             The first hop would (1) avoid this risk and (2) earn more upfront fees because it does not forward those fees to later hops.\n>\n>     -   This is arguably custodial and not your keys not your coins applies.\n>         Thus, it returns us back to tr\\*st anyway.\n>\n> -   We can require that the first hop prove where along the route errored.\n>     If it provably failed at a later hop, then the first hop can claim more as upfront fees, since it will forward the upfront fees to the later hop as well.\n>     -   This has to be enforcable onchain in case the channel gets dropped onchain.\n>         Is there a proposal SCRIPT which can enforce this?\n>\n>     -   If not enforcable onchain, then there may be onchain shenanigans possible and thus this solution might introduce an attack vector even as it fixes another.\n>         -   On the other hand, sub-satoshi amounts are not enforcable onchain too, and nobody cares, so...\n\nOne thing I have been thinking about, but have not proposed seriously yet, would be \"incremental routing\".\n\nBasically, the route of pending HTLCs also doubles as an encrypted bidirectional tunnel.\n\nLet me first describe how I imagine this \"incremental routing\" would look like.\n\nFirst, you offer an HTLC with a direct peer.\nThe data with this HTLC includes a point, which the peer will ECDH with its own privkey, to form a shared secret.\nYou can then send additional messages to that node, which it will decrypt using the shared secret as the symmetric encryption key.\nThe node can also reply to those messages, by encrypting it with the same symmetric encryption key.\nTypically this will be via a stream cipher which is XORed with the real data.\n\nOne of the messages you can send to that node (your direct peer) would be \"please send out an HTLC to this peer of yours\".\nTogether with that message, you could also bump up the value of the HTLC, and possibly the CLTV delta, you have with that node.\nThis bumping up is the forwarding fee and resolution time you have to give to that node in order to have it safely put an HTLC to the next hop.\n\nIf there is a problem on the next hop, the node replies back, saying it cannot forward the HTLC further.\nYour node can then respond by giving an alternative next hop, which that node can reply back is also not available, etc. until you say \"give up\" and that node will just fail the HTLC.\n\nHowever, suppose the next hop is online and there is enough space in the channel.\nThat node then establishes the HTLC with the next hop.\n\nAt this point, you can then send a message to the direct peer which is nothing more than \"send the rest of this message as the message to the next hop on the same HTLC, then wait for a reply and wrap it and reply to me\".\nThis is effectively onion-wrapping the message to the peer of your peer, and waiting for an onion-wrapped reply from the peer of your peer.\n\nYou can then talk to the peer of your peer (of your peer...) to incrementally build the rest of the route, until you reach the destination.\n\nHow is this related to upfront payments?\nWell, in upfront payments, on the timelock branch, instead of the entire fund value returning to the offerer of the HTLC, part of the fund value is given forward to the next peer.\nFor onchain enforcement the upfront payment is simply added to the miner fees, so at least the one offerring it does not have incentive to drop it onchain to play onchain shenanigans.\n\nNow, suppose your direct peer requires an upfront payment before it will even bother make an outgoing HTLC on the strength of your incoming one.\nSo, you bump up your HTLC, and arrange to have the timelock branch give the upfront payment to the direct peer.\n\nThen, you can now ask the peer to open an HTLC to the next peer, on the basis of that upfront payment.\n\nOf course, if the peer of the peer is not the destination, you still have to increment further.\nYou then bump up the upfront payment of the direct peer, with a message to offer an equivalent amount of upfront payment to the peer of the peer.\nYour direct peer can then pretend that communication with the peer of the peer failed, so it should get the bumped-up upfront payment, equivalent to the upfront payments of itself plus the next peer.\n\nHowever, at any one time, the upfront payment that can be stolen by your direct peer can only amount to the upfront fee of a single hop, the one that is being built out next in the incremental routing.\nThis is because it has already offered the upfront payment to the next hop.\n\nSome numbers may make this easier to understand.\nFor example, suppose all nodes require 1msat prepaid to send out to the next hop.\nAt the start, you are just offering the HTLC amount with 0 msat in upfront payment.\nThen you increase the upfront payment by 1msat, then tell the direct peer to create an HTLC to the peer of the peer, of equal HTLC value, but with 0 msats in upfront payment.\nNext, you increase the upfront payment to the direct peer to 2msat, then tell the direct peer to bump up the upfront payment of its outgoing HTLC by 1 msat as well.\n\nThen next, you increase the upfront payment to the direct peer to 3 msat.\nThe direct peer is giving out 1 msat upfront payment to the outgoing HTLC, which it is now supposed to increase to 2msat.\nAfter doing that, it sends a message to the peer of the peer.\nThe peer of the peer is giving out 0 msat upfront payment, which it is now supposed to increase to 1 msat.\nAfter doing that, it sends a message to the peer of the peer of the peer.\nThe peer of the peer of the peer has no outgoing HTLC yet, but since it is now being offered 1msat in upfront fee, it is now wlling to be instructed to send out an outgoing HTLC to another channel.\n\nAnd so on.\n\nAt any one time, an intermediate node can pretend communications to the next node in the route failed, and claim 2msat in upfront fee instead of 1msat.\nBut it cannot steal more than the equivalent of the upfront fee of one node.\n\n(I elide the fact that in this scheme, you would *also* offer a fee conditional on the HTLC actually pushing through; sort of like a \"pay half now, pay half later on completion\" deal; your imagination can fill in the rest.)\n\nThis is in contrast with upfront payments where you have to offer the upfront payment of all the nodes along the route to the direct peer.\nThe direct peer can, then and there, claim the entire upfront payment earmarked for the entire route.\n\nA nice thing about incremental routing is that the time risk of your money being locked up can be managed a little easier.\nYou can start with an HTLC with the minimum time, then also bump up the time of the first HTLC (with time bumps rippling out along the incremental route) as you build up the incremental route until you reach the destination.\n\n--\n\nAgainst incremental routing, however, is that:\n\n* It requires a lot more communication rounds and (symmetric, at least) cryptographic operations.\n* Intermediate nodes can guess the distance from the source by measuring timing of a previous response to the next message from the payer.\n  * In particular, current LN can allow an intermediate node to guess the distance to the destination of a payment (by CLTV delta bounds) but *not* the distance to the source of the payment, with incremental routing the distance to the source of the payment is guessable as well from message timings.\n* The idea is completely imaginary as of now with no peer review, and there may be actual cryptographic or practical problems with the idea.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2020-10-08T12:32:44",
                "message_text_only": "If I remember correctly, it looks very similar to how I2P establishes\ntunnels, it may be worth\ndiving in their documentation to fish for ideas.\n\nHowever in their case the goal is to establish a long-lived tunnel, which\nis why it's ok to have\na slow and costly protocol. It feels to me that for payments, this is a lot\nof messages and delays,\nI'm not sure this is feasible at a reasonable scale...\n\nCheers,\nBastien\n\nLe mer. 7 oct. 2020 \u00e0 19:34, ZmnSCPxj via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Good morning Antoine, Bastien, and list,\n>\n> > > Instead of relying on reputation, the other alternative is just to\n> have an upfront payment system, where a relay node doesn't have to account\n> for a HTLC issuer reputation to decide acceptance and can just forward a\n> HTLC as long it paid enough. More, I think it's better to mitigate jamming\n> with a fees-based system than a web-of-trust one, less burden on network\n> newcomers.\n> >\n> > Let us consider some of the complications here.\n> >\n> > A newcomer wants to make an outgoing payment.\n> > Speculatively, it connects to some existing nodes based on some policy.\n> >\n> > Now, since forwarding is upfront, the newcomer fears that the node it\n> connected to might not even bother forwarding the payment, and instead just\n> fail it and claim the upfront fees.\n> >\n> > In particular: how would the newcomer offer upfront fees to a node it is\n> not directly channeled with?\n> > In order to do that, we would have to offer the upfront fees for that\n> node, to the node we are channeled with, so it can forward this as well.\n> >\n> > -   We can give the upfront fee outright to the first hop, and trust\n> that if it forwards, it will also forward the upfront fee for the next hop.\n> >     -   The first hop would then prefer to just fail the HTLC then and\n> there and steal all the upfront fees.\n> >         -   After all, the offerrer is a newcomer, and might be the\n> sybil of a hacker that is trying to tie up its liquidity.\n> >             The first hop would (1) avoid this risk and (2) earn more\n> upfront fees because it does not forward those fees to later hops.\n> >\n> >     -   This is arguably custodial and not your keys not your coins\n> applies.\n> >         Thus, it returns us back to tr\\*st anyway.\n> >\n> > -   We can require that the first hop prove where along the route\n> errored.\n> >     If it provably failed at a later hop, then the first hop can claim\n> more as upfront fees, since it will forward the upfront fees to the later\n> hop as well.\n> >     -   This has to be enforcable onchain in case the channel gets\n> dropped onchain.\n> >         Is there a proposal SCRIPT which can enforce this?\n> >\n> >     -   If not enforcable onchain, then there may be onchain shenanigans\n> possible and thus this solution might introduce an attack vector even as it\n> fixes another.\n> >         -   On the other hand, sub-satoshi amounts are not enforcable\n> onchain too, and nobody cares, so...\n>\n> One thing I have been thinking about, but have not proposed seriously yet,\n> would be \"incremental routing\".\n>\n> Basically, the route of pending HTLCs also doubles as an encrypted\n> bidirectional tunnel.\n>\n> Let me first describe how I imagine this \"incremental routing\" would look\n> like.\n>\n> First, you offer an HTLC with a direct peer.\n> The data with this HTLC includes a point, which the peer will ECDH with\n> its own privkey, to form a shared secret.\n> You can then send additional messages to that node, which it will decrypt\n> using the shared secret as the symmetric encryption key.\n> The node can also reply to those messages, by encrypting it with the same\n> symmetric encryption key.\n> Typically this will be via a stream cipher which is XORed with the real\n> data.\n>\n> One of the messages you can send to that node (your direct peer) would be\n> \"please send out an HTLC to this peer of yours\".\n> Together with that message, you could also bump up the value of the HTLC,\n> and possibly the CLTV delta, you have with that node.\n> This bumping up is the forwarding fee and resolution time you have to give\n> to that node in order to have it safely put an HTLC to the next hop.\n>\n> If there is a problem on the next hop, the node replies back, saying it\n> cannot forward the HTLC further.\n> Your node can then respond by giving an alternative next hop, which that\n> node can reply back is also not available, etc. until you say \"give up\" and\n> that node will just fail the HTLC.\n>\n> However, suppose the next hop is online and there is enough space in the\n> channel.\n> That node then establishes the HTLC with the next hop.\n>\n> At this point, you can then send a message to the direct peer which is\n> nothing more than \"send the rest of this message as the message to the next\n> hop on the same HTLC, then wait for a reply and wrap it and reply to me\".\n> This is effectively onion-wrapping the message to the peer of your peer,\n> and waiting for an onion-wrapped reply from the peer of your peer.\n>\n> You can then talk to the peer of your peer (of your peer...) to\n> incrementally build the rest of the route, until you reach the destination.\n>\n> How is this related to upfront payments?\n> Well, in upfront payments, on the timelock branch, instead of the entire\n> fund value returning to the offerer of the HTLC, part of the fund value is\n> given forward to the next peer.\n> For onchain enforcement the upfront payment is simply added to the miner\n> fees, so at least the one offerring it does not have incentive to drop it\n> onchain to play onchain shenanigans.\n>\n> Now, suppose your direct peer requires an upfront payment before it will\n> even bother make an outgoing HTLC on the strength of your incoming one.\n> So, you bump up your HTLC, and arrange to have the timelock branch give\n> the upfront payment to the direct peer.\n>\n> Then, you can now ask the peer to open an HTLC to the next peer, on the\n> basis of that upfront payment.\n>\n> Of course, if the peer of the peer is not the destination, you still have\n> to increment further.\n> You then bump up the upfront payment of the direct peer, with a message to\n> offer an equivalent amount of upfront payment to the peer of the peer.\n> Your direct peer can then pretend that communication with the peer of the\n> peer failed, so it should get the bumped-up upfront payment, equivalent to\n> the upfront payments of itself plus the next peer.\n>\n> However, at any one time, the upfront payment that can be stolen by your\n> direct peer can only amount to the upfront fee of a single hop, the one\n> that is being built out next in the incremental routing.\n> This is because it has already offered the upfront payment to the next hop.\n>\n> Some numbers may make this easier to understand.\n> For example, suppose all nodes require 1msat prepaid to send out to the\n> next hop.\n> At the start, you are just offering the HTLC amount with 0 msat in upfront\n> payment.\n> Then you increase the upfront payment by 1msat, then tell the direct peer\n> to create an HTLC to the peer of the peer, of equal HTLC value, but with 0\n> msats in upfront payment.\n> Next, you increase the upfront payment to the direct peer to 2msat, then\n> tell the direct peer to bump up the upfront payment of its outgoing HTLC by\n> 1 msat as well.\n>\n> Then next, you increase the upfront payment to the direct peer to 3 msat.\n> The direct peer is giving out 1 msat upfront payment to the outgoing HTLC,\n> which it is now supposed to increase to 2msat.\n> After doing that, it sends a message to the peer of the peer.\n> The peer of the peer is giving out 0 msat upfront payment, which it is now\n> supposed to increase to 1 msat.\n> After doing that, it sends a message to the peer of the peer of the peer.\n> The peer of the peer of the peer has no outgoing HTLC yet, but since it is\n> now being offered 1msat in upfront fee, it is now wlling to be instructed\n> to send out an outgoing HTLC to another channel.\n>\n> And so on.\n>\n> At any one time, an intermediate node can pretend communications to the\n> next node in the route failed, and claim 2msat in upfront fee instead of\n> 1msat.\n> But it cannot steal more than the equivalent of the upfront fee of one\n> node.\n>\n> (I elide the fact that in this scheme, you would *also* offer a fee\n> conditional on the HTLC actually pushing through; sort of like a \"pay half\n> now, pay half later on completion\" deal; your imagination can fill in the\n> rest.)\n>\n> This is in contrast with upfront payments where you have to offer the\n> upfront payment of all the nodes along the route to the direct peer.\n> The direct peer can, then and there, claim the entire upfront payment\n> earmarked for the entire route.\n>\n> A nice thing about incremental routing is that the time risk of your money\n> being locked up can be managed a little easier.\n> You can start with an HTLC with the minimum time, then also bump up the\n> time of the first HTLC (with time bumps rippling out along the incremental\n> route) as you build up the incremental route until you reach the\n> destination.\n>\n> --\n>\n> Against incremental routing, however, is that:\n>\n> * It requires a lot more communication rounds and (symmetric, at least)\n> cryptographic operations.\n> * Intermediate nodes can guess the distance from the source by measuring\n> timing of a previous response to the next message from the payer.\n>   * In particular, current LN can allow an intermediate node to guess the\n> distance to the destination of a payment (by CLTV delta bounds) but *not*\n> the distance to the source of the payment, with incremental routing the\n> distance to the source of the payment is guessable as well from message\n> timings.\n> * The idea is completely imaginary as of now with no peer review, and\n> there may be actual cryptographic or practical problems with the idea.\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201008/ce30e0c4/attachment.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2020-10-08T16:17:29",
                "message_text_only": "Hi Zeeman,\n\n> * It requires a lot more communication rounds and (symmetric, at least)\ncryptographic operations.\n\nAt first sight, it sounds similar to HORNET/rendez-vous, at least in the\ngoal of achieving bidirectional communications.\n\n* Intermediate nodes can guess the distance from the source by measuring\ntiming of a previous response to the next message from the payer.\n\nYes an intermediary node servicing also a message relay one can likely\nlearn a lot from message RTT timings _and_ CLTV/routed value...\n\nNote, for the point raised about untrusted upfront payment, if your payment\npath hops are stealing upfront, just consider this as a normal routing\nfailure and downgrade them in your routing algorithm. Thus incentivizing\nthem to behave well to keep their routing fees. Of course, assigning the\nblame to the real faultive hop is likely hard without making onion errors\nreliable but I think each hop would be incentivized to sign correctly its\nfailures and police its neighbouring peers for their laziness.\n\nFor sure, upfront payments need more grinding. But I think it will also\nsolve adjacent issues like your counterparty updating a channel for nothing\nuntil you exhaust your watchtower update storage credit.\n\nBest,\nAntoine\n\nLe mer. 7 oct. 2020 \u00e0 13:33, ZmnSCPxj <ZmnSCPxj at protonmail.com> a \u00e9crit :\n\n> Good morning Antoine, Bastien, and list,\n>\n> > > Instead of relying on reputation, the other alternative is just to\n> have an upfront payment system, where a relay node doesn't have to account\n> for a HTLC issuer reputation to decide acceptance and can just forward a\n> HTLC as long it paid enough. More, I think it's better to mitigate jamming\n> with a fees-based system than a web-of-trust one, less burden on network\n> newcomers.\n> >\n> > Let us consider some of the complications here.\n> >\n> > A newcomer wants to make an outgoing payment.\n> > Speculatively, it connects to some existing nodes based on some policy.\n> >\n> > Now, since forwarding is upfront, the newcomer fears that the node it\n> connected to might not even bother forwarding the payment, and instead just\n> fail it and claim the upfront fees.\n> >\n> > In particular: how would the newcomer offer upfront fees to a node it is\n> not directly channeled with?\n> > In order to do that, we would have to offer the upfront fees for that\n> node, to the node we are channeled with, so it can forward this as well.\n> >\n> > -   We can give the upfront fee outright to the first hop, and trust\n> that if it forwards, it will also forward the upfront fee for the next hop.\n> >     -   The first hop would then prefer to just fail the HTLC then and\n> there and steal all the upfront fees.\n> >         -   After all, the offerrer is a newcomer, and might be the\n> sybil of a hacker that is trying to tie up its liquidity.\n> >             The first hop would (1) avoid this risk and (2) earn more\n> upfront fees because it does not forward those fees to later hops.\n> >\n> >     -   This is arguably custodial and not your keys not your coins\n> applies.\n> >         Thus, it returns us back to tr\\*st anyway.\n> >\n> > -   We can require that the first hop prove where along the route\n> errored.\n> >     If it provably failed at a later hop, then the first hop can claim\n> more as upfront fees, since it will forward the upfront fees to the later\n> hop as well.\n> >     -   This has to be enforcable onchain in case the channel gets\n> dropped onchain.\n> >         Is there a proposal SCRIPT which can enforce this?\n> >\n> >     -   If not enforcable onchain, then there may be onchain shenanigans\n> possible and thus this solution might introduce an attack vector even as it\n> fixes another.\n> >         -   On the other hand, sub-satoshi amounts are not enforcable\n> onchain too, and nobody cares, so...\n>\n> One thing I have been thinking about, but have not proposed seriously yet,\n> would be \"incremental routing\".\n>\n> Basically, the route of pending HTLCs also doubles as an encrypted\n> bidirectional tunnel.\n>\n> Let me first describe how I imagine this \"incremental routing\" would look\n> like.\n>\n> First, you offer an HTLC with a direct peer.\n> The data with this HTLC includes a point, which the peer will ECDH with\n> its own privkey, to form a shared secret.\n> You can then send additional messages to that node, which it will decrypt\n> using the shared secret as the symmetric encryption key.\n> The node can also reply to those messages, by encrypting it with the same\n> symmetric encryption key.\n> Typically this will be via a stream cipher which is XORed with the real\n> data.\n>\n> One of the messages you can send to that node (your direct peer) would be\n> \"please send out an HTLC to this peer of yours\".\n> Together with that message, you could also bump up the value of the HTLC,\n> and possibly the CLTV delta, you have with that node.\n> This bumping up is the forwarding fee and resolution time you have to give\n> to that node in order to have it safely put an HTLC to the next hop.\n>\n> If there is a problem on the next hop, the node replies back, saying it\n> cannot forward the HTLC further.\n> Your node can then respond by giving an alternative next hop, which that\n> node can reply back is also not available, etc. until you say \"give up\" and\n> that node will just fail the HTLC.\n>\n> However, suppose the next hop is online and there is enough space in the\n> channel.\n> That node then establishes the HTLC with the next hop.\n>\n> At this point, you can then send a message to the direct peer which is\n> nothing more than \"send the rest of this message as the message to the next\n> hop on the same HTLC, then wait for a reply and wrap it and reply to me\".\n> This is effectively onion-wrapping the message to the peer of your peer,\n> and waiting for an onion-wrapped reply from the peer of your peer.\n>\n> You can then talk to the peer of your peer (of your peer...) to\n> incrementally build the rest of the route, until you reach the destination.\n>\n> How is this related to upfront payments?\n> Well, in upfront payments, on the timelock branch, instead of the entire\n> fund value returning to the offerer of the HTLC, part of the fund value is\n> given forward to the next peer.\n> For onchain enforcement the upfront payment is simply added to the miner\n> fees, so at least the one offerring it does not have incentive to drop it\n> onchain to play onchain shenanigans.\n>\n> Now, suppose your direct peer requires an upfront payment before it will\n> even bother make an outgoing HTLC on the strength of your incoming one.\n> So, you bump up your HTLC, and arrange to have the timelock branch give\n> the upfront payment to the direct peer.\n>\n> Then, you can now ask the peer to open an HTLC to the next peer, on the\n> basis of that upfront payment.\n>\n> Of course, if the peer of the peer is not the destination, you still have\n> to increment further.\n> You then bump up the upfront payment of the direct peer, with a message to\n> offer an equivalent amount of upfront payment to the peer of the peer.\n> Your direct peer can then pretend that communication with the peer of the\n> peer failed, so it should get the bumped-up upfront payment, equivalent to\n> the upfront payments of itself plus the next peer.\n>\n> However, at any one time, the upfront payment that can be stolen by your\n> direct peer can only amount to the upfront fee of a single hop, the one\n> that is being built out next in the incremental routing.\n> This is because it has already offered the upfront payment to the next hop.\n>\n> Some numbers may make this easier to understand.\n> For example, suppose all nodes require 1msat prepaid to send out to the\n> next hop.\n> At the start, you are just offering the HTLC amount with 0 msat in upfront\n> payment.\n> Then you increase the upfront payment by 1msat, then tell the direct peer\n> to create an HTLC to the peer of the peer, of equal HTLC value, but with 0\n> msats in upfront payment.\n> Next, you increase the upfront payment to the direct peer to 2msat, then\n> tell the direct peer to bump up the upfront payment of its outgoing HTLC by\n> 1 msat as well.\n>\n> Then next, you increase the upfront payment to the direct peer to 3 msat.\n> The direct peer is giving out 1 msat upfront payment to the outgoing HTLC,\n> which it is now supposed to increase to 2msat.\n> After doing that, it sends a message to the peer of the peer.\n> The peer of the peer is giving out 0 msat upfront payment, which it is now\n> supposed to increase to 1 msat.\n> After doing that, it sends a message to the peer of the peer of the peer.\n> The peer of the peer of the peer has no outgoing HTLC yet, but since it is\n> now being offered 1msat in upfront fee, it is now wlling to be instructed\n> to send out an outgoing HTLC to another channel.\n>\n> And so on.\n>\n> At any one time, an intermediate node can pretend communications to the\n> next node in the route failed, and claim 2msat in upfront fee instead of\n> 1msat.\n> But it cannot steal more than the equivalent of the upfront fee of one\n> node.\n>\n> (I elide the fact that in this scheme, you would *also* offer a fee\n> conditional on the HTLC actually pushing through; sort of like a \"pay half\n> now, pay half later on completion\" deal; your imagination can fill in the\n> rest.)\n>\n> This is in contrast with upfront payments where you have to offer the\n> upfront payment of all the nodes along the route to the direct peer.\n> The direct peer can, then and there, claim the entire upfront payment\n> earmarked for the entire route.\n>\n> A nice thing about incremental routing is that the time risk of your money\n> being locked up can be managed a little easier.\n> You can start with an HTLC with the minimum time, then also bump up the\n> time of the first HTLC (with time bumps rippling out along the incremental\n> route) as you build up the incremental route until you reach the\n> destination.\n>\n> --\n>\n> Against incremental routing, however, is that:\n>\n> * It requires a lot more communication rounds and (symmetric, at least)\n> cryptographic operations.\n> * Intermediate nodes can guess the distance from the source by measuring\n> timing of a previous response to the next message from the payer.\n>   * In particular, current LN can allow an intermediate node to guess the\n> distance to the destination of a payment (by CLTV delta bounds) but *not*\n> the distance to the source of the payment, with incremental routing the\n> distance to the source of the payment is guessable as well from message\n> timings.\n> * The idea is completely imaginary as of now with no peer review, and\n> there may be actual cryptographic or practical problems with the idea.\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201008/b43f3096/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Incremental Routing (Was: Making (some) channel limits dynamic)",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Bastien TEINTURIER",
                "Antoine Riard",
                "ZmnSCPxj"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 29892
        }
    },
    {
        "title": "[Lightning-dev] Witness asymmetric payment channels",
        "thread_messages": [
            {
                "author": "Lloyd Fournier",
                "date": "2020-10-08T04:58:04",
                "message_text_only": "For those interested I've recently integrated the above refinements\nand tried to coherently package the whole idea together here:\nhttps://github.com/LLFourn/witness-asymmetric-channel\n\nThe main difference is that the protocol now uses what I am calling\n\"revocable signatures\" as the main primitive.\nThis allows for O(1) storage complexity in both key aggregated and\n\"multisig\" constructions.\n\nLL\n\nOn Thu, Sep 10, 2020 at 1:56 PM Lloyd Fournier <lloyd.fourn at gmail.com> wrote:\n>\n>\n>>\n>>\n>> Fortunately, I am wrong. At least in the case of non-aggregated 2-of-2 you can deterministically produce the encrypted signature by yourself for any commitment transaction as long as you use a deterministic nonce.\n>> But I think if using MuSig you would need to store each two party generated encrypted signature.\n>> Seeing as the likely way forward would be to use MuSig on an output which has a taproot which hides a discrete 2-of-2 this may not be a problem.\n>\n>\n> Upon further reflection I was missing something obvious when I came to this conclusion. You can't produce the adaptor signature for a commitment transaction deterministically without the encryption key (the other party's publication point).\n> As long as we have to store the other party's per-commitment publication point we still need O(n) storage where n is the number of commitment transactions. Sorry for the confusion.\n>\n> Fortunately I had a bit of a breakthrough which allows us to eliminate publication points altogether and enables O(1) storage when 2-of-2 multi-signatures are instantiated with or without key aggregation (i.e. MuSig or OP_CHECKMULTISIG).\n>\n> ### Eliminating Publication Points In favor of  \"revocable signatures\" (for OP_CHECKMULTISIG)\n>\n> I propose replacing the publication point with a static key that remains the same with each commitment transaction.\n> The encryption key for each commitment transaction adaptor signature is (Ra + A)  for Alice and (Rb + B) for Bob.\n> Therefore, Alice broadcasting her commitment transaction would reveal the discrete log of Ra + A (and Bob Rb + B).\n> Note that if Alice has not revealed her recvocation key (Ra) for this commitment transaction she is not in trouble since her static key A is blinded by Ra. If she has then Bob will learn her static secret key for A.\n> The intuition here is that the revocation key acts as a blinding factor for the static key in the same way a nonce blinds your secret key in a schnorr signature (more on that later).\n> If you haven't revealed your revocation key then you are free to use that signature. If you have revealed the revocation key then you have in effect \"revoked\" the signature.\n>\n> Now we need to make sure that if a party learns the secret key of the other they can efficiently punish them so make the following changes to my original proposal:\n>\n> Balance output for Alice is  2-of-2(A , Rb + B)\n> Balance output for Bob is   2-of-2(Ra + A, B)\n>\n> The implication of the above structure is that if you broadcast a commitment transaction the other party can take their balance immediately.\n> If you broadcasted a revoked commitment transaction then they can take their output and yours immediately.\n>\n> PTLC outputs and all subsequent transaction outputs then simplify to 2-of-2(A,B) on every output. Yay!\n> Consider a PTLC paying to Alice. The PTLC-success output can be 2-of-2(A,B). If Alice broadcasted it and it has been revoked then Bob knows A and B so he can punish her.\n> The converse is true for the PTLC-timeout.\n> This elegant uniformity extends to other off-chain protocols hosted in these channels e.g. DLCs\n>\n> Since A and B are static per channel and the secret keys for Ra and Rb can be incrementally derived from subsequent values (as in the present lightning spec) we have O(1) communication.\n> In practice each 2-of-2(A,B) should be randomized so they don't all look the same.\n>\n> ### Revoking key aggregated schnorr signatures (for MuSig)\n>\n> Even with the above improvement there is still O(n) storage if using key aggregation (MuSig) on the funding transaction output.\n> Key aggregation may be desirable here since you may want to not use a taproot spend to broadcast a commitment transaction.\n> Since the two party adaptor signature scheme needs randomness from both parties, you would have to store the other party's nonce and retrieve it to deterministically produce the adaptor signature so you can extract Ra + A (if Alice broadcasts) from the witness of the commitment transaction.\n>\n> Fortunately, there is a natural way to revoke a key aggregated signature you helped produce without using adaptor signatures at all: just reveal the nonce you used for it to the other party.\n> This prevents you from broadcasting it since the other party can now extract your secret key from it!\n> Explicitly, two party signature generate two Schnorr signatures for the key A + B in the form:\n>\n> sa = ra + rb' + H(Ra + Rb' || A + B || m)(a + b)\n> sb = ra' + rb + H(Ra' + Rb || A + B || m)(a + b)\n>\n> - (ra,rb) are the revocation secret keys,\n> - (ra', rb') are typical deterministically produced[1] nonces with Ra' = ra' * G etc.\n> - (a,b) are the static secret keys\n>\n> Only Alice knows sa and only Bob knows sb but they are both signatures on the same commitment transaction.\n> This is the witness asymmetry in the protocol.\n>\n> When revoking the commitment transaction associated with sa Alice reveals ra to Bob and vice versa. If Alice uses sa after that, Bob can deterministically produce rb' and ra (because each revocation key can be derived from the last) and therefore can produce:\n>\n> ra + rb' + H(Ra + Rb' || A + B || m)b\n>\n> which when subtracted from sa and divided by H(Ra + Rb' || A + B || m) will yield b (Bob's static secret key).\n>\n> The advantage of witness asymmetric channels using discrete keys over present lightning seems to boil down to a simpler transaction structure (in favour of using more complicated cryptography).\n> However, for key aggregation there is a measurable improvement in communication complexity: It halves the amount of interactive signatures that need to be computed per payment.\n> This is because each PTLC does not need to be duplicated across the asymmetric state of both parties.\n>\n> Zeeman pointed out in [2] that the number of signing rounds (which this does not improve) may be prohibitive anyway for payment applications at least so it remains to be seen whether this efficiency can be utilised  for payment PTLCs in LN.\n> Thankfully, there are still advancements being made in Schnorr key aggregated signing so the right answer to this might change over time.[3]\n>\n> [1] The caveats about not using deterministic nonces in MuSig can be avoided here since we necessarily have state in LN.\n> [2] https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-December/002375.html\n> [3] https://medium.com/blockstream/musig-dn-schnorr-multisignatures-with-verifiably-deterministic-nonces-27424b5df9d6\n>\n> Cheers,\n>\n> LL"
            }
        ],
        "thread_summary": {
            "title": "Witness asymmetric payment channels",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Lloyd Fournier"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 6962
        }
    },
    {
        "title": "[Lightning-dev] Partial LND Vulnerability Disclosure, Upgrade to 0.11.x",
        "thread_messages": [
            {
                "author": "Conner Fromknecht",
                "date": "2020-10-09T00:19:09",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\n\nHi all,\n\nWe are writing to let the Lightning community know about the existence of\nvulnerabilities that affect lnd versions 0.10.x and below. The full details of\nthese vulnerabilities will be disclosed on October 20, 2020. The circumstances\nsurrounding the discovery resulted in a compressed disclosure timeline compared\nto our usual timeframes. We will be publishing more details about this in the\ncoming weeks along with a comprehensive bug bounty program.\n\nWhile we have no reason to believe these vulnerabilities have been exploited in\nthe wild, we strongly urge the community to upgrade to lnd 0.11.0 or above ASAP.\nPlease ping us on the #lnd IRC channel, the LND Slack, or at\nsupport at lightning.engineering if you need any assistance in doing so. Upgrade\ninstructions can be found in our installation docs:\nhttps://github.com/lightningnetwork/lnd/blob/master/docs/INSTALL.md#installing-lnd.\n\nRegards,\nConner Fromknecht\n-----BEGIN PGP SIGNATURE-----\n\niQIzBAEBCAAdFiEEnI1hhop8SSADsnRO59c3tn+lkscFAl9/ozwACgkQ59c3tn+l\nkscVvBAAk21z6tlHPkOSwfj1lBE0pqc65A6Qa927WEjN5hdUpjjof4Xo2j+GzbnN\nUoj4HGZu+koakzoVpJ4mzN+vg086zAnv+K668hhl7bbPHsQu6FqA1ALiAyy0nH6H\n1yukXxpRflq53RTIVPjrEnFVdt6FCLhkCm9LuOk0a/SUf8D4b/N6OaB1Bxupeceu\nQFSCIkb9kvW/Eplwkv7PEnx/IZNGIQP9F11DaKLTAjWY5RnIxmCw/oamvlP8Mxt8\n/AqlzWVtPVqvwgJLhbMziraXNVV05naHrIXvbXrOI2Q7FZjdaxF+S4EKT4feuq1w\niW7NYSS/u5N2FP3yK8YIdoX0I/nwYQQcpsfbAv2dS4Ql2Td/dyREId4NcchmaKSV\nN3w1jByMPWrgUtinl5WEDDOJdUKS2PHkQ95t3s/1uYDFsPz1kXJR2x37a/1AVz/K\n6zQ45wFvHEopFR49hu/CV6MUvsvn4XKzPa46Ii7puaBaNqygx0RwuwlxbxCNxPNQ\nv45CaCUEq2Tj3stu7YoYGntFvrXVkxXJocn51eK6D+g0bIEXxaGlPJeTuvifKMTO\n3T3ZEEbCe9UhDUT8Ja2boP2IIi8wAyExGS59k0tndQGzMSjkzWZ0fzgYyyf+y4nt\nr3nTCGi5WWe4y1i2KpiYZTRrQkbrNkRf+fnVdlnTS4lcgEWFFiY=\n=8t9Q\n-----END PGP SIGNATURE-----"
            },
            {
                "author": "Conner Fromknecht",
                "date": "2020-10-10T00:32:47",
                "message_text_only": "Hi all,\n\nFor those looking to verify the gpg signature, please be sure the\nsupport email is formatted\ncorrectly. For example, the archive replaces \"@\" with \" at \", and\napparently google groups\ntrims \"support\" to \"sup...\". If you run into issues, please double\ncheck the plaintext matches\nverbatim with what was sent on lightning-dev.\n\nCheers,\nConner\n\n\nOn Thu, Oct 8, 2020 at 5:19 PM Conner Fromknecht\n<conner at lightning.engineering> wrote:\n>\n> -----BEGIN PGP SIGNED MESSAGE-----\n> Hash: SHA256\n>\n> Hi all,\n>\n> We are writing to let the Lightning community know about the existence of\n> vulnerabilities that affect lnd versions 0.10.x and below. The full details of\n> these vulnerabilities will be disclosed on October 20, 2020. The circumstances\n> surrounding the discovery resulted in a compressed disclosure timeline compared\n> to our usual timeframes. We will be publishing more details about this in the\n> coming weeks along with a comprehensive bug bounty program.\n>\n> While we have no reason to believe these vulnerabilities have been exploited in\n> the wild, we strongly urge the community to upgrade to lnd 0.11.0 or above ASAP.\n> Please ping us on the #lnd IRC channel, the LND Slack, or at\n> support at lightning.engineering if you need any assistance in doing so. Upgrade\n> instructions can be found in our installation docs:\n> https://github.com/lightningnetwork/lnd/blob/master/docs/INSTALL.md#installing-lnd.\n>\n> Regards,\n> Conner Fromknecht\n> -----BEGIN PGP SIGNATURE-----\n>\n> iQIzBAEBCAAdFiEEnI1hhop8SSADsnRO59c3tn+lkscFAl9/ozwACgkQ59c3tn+l\n> kscVvBAAk21z6tlHPkOSwfj1lBE0pqc65A6Qa927WEjN5hdUpjjof4Xo2j+GzbnN\n> Uoj4HGZu+koakzoVpJ4mzN+vg086zAnv+K668hhl7bbPHsQu6FqA1ALiAyy0nH6H\n> 1yukXxpRflq53RTIVPjrEnFVdt6FCLhkCm9LuOk0a/SUf8D4b/N6OaB1Bxupeceu\n> QFSCIkb9kvW/Eplwkv7PEnx/IZNGIQP9F11DaKLTAjWY5RnIxmCw/oamvlP8Mxt8\n> /AqlzWVtPVqvwgJLhbMziraXNVV05naHrIXvbXrOI2Q7FZjdaxF+S4EKT4feuq1w\n> iW7NYSS/u5N2FP3yK8YIdoX0I/nwYQQcpsfbAv2dS4Ql2Td/dyREId4NcchmaKSV\n> N3w1jByMPWrgUtinl5WEDDOJdUKS2PHkQ95t3s/1uYDFsPz1kXJR2x37a/1AVz/K\n> 6zQ45wFvHEopFR49hu/CV6MUvsvn4XKzPa46Ii7puaBaNqygx0RwuwlxbxCNxPNQ\n> v45CaCUEq2Tj3stu7YoYGntFvrXVkxXJocn51eK6D+g0bIEXxaGlPJeTuvifKMTO\n> 3T3ZEEbCe9UhDUT8Ja2boP2IIi8wAyExGS59k0tndQGzMSjkzWZ0fzgYyyf+y4nt\n> r3nTCGi5WWe4y1i2KpiYZTRrQkbrNkRf+fnVdlnTS4lcgEWFFiY=\n> =8t9Q\n> -----END PGP SIGNATURE-----"
            }
        ],
        "thread_summary": {
            "title": "Partial LND Vulnerability Disclosure, Upgrade to 0.11.x",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Conner Fromknecht"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 4129
        }
    },
    {
        "title": "[Lightning-dev] two-round MuSig less dangerous than it seems",
        "thread_messages": [
            {
                "author": "Lloyd Fournier",
                "date": "2020-10-09T04:31:28",
                "message_text_only": "Hi list,\n\ntl;dr: I think can use two round MuSig safely in the context of lightning.\n\nAs a recap, Zeeman did a good evaluation of \"purely scriptless\" lightning\nchannels after taproot/schnorr.[1]\nZ concluded that even in the most optimized case the 3 round MuSig protocol\nleads to an extra round of communication before you can forward a payment.\nI think this is correct but perhaps you could just use 2 round MuSig\n(without MuSig-DN magic[4]).\n\n=== 2 round MuSig ===\n\n(from memory) The original MuSig paper suggested the following signing\nalgorithm once two keys A and B have already been established for Alice and\nBob\n\nAlice                                           Bob\npick ra randomly\nRa = ra * G\n                          ----- Ra ---->\n                                                pick rb randomly\n                                                Rb = rb * G\n                                                let c = H(Ra + Rb || A + B\n|| m)\n                                                sb = rb + cb\n                          <--Rb, sb----\ns = sb + ra + ca\nR = Ra + Rb\nX = A + B\nverify(X, (R, s), m)?\noutput: (R,s)\n\nThis was insecure under *parallel composition* because the proof cannot\nwork (from my memory because having open signing sessions makes the\nrewinding argument in the proof incoherent) and led to actual attacks which\nhave recently improved in efficiency.[3]\n\n=== My insight ===\n\n2 round MuSig *is secure* under sequential composition and in a lightning\nchannel we are essentially arranging sequential state updates so perhaps\nwe're ok here?\n\nIt should be easy to require that you can't open another signing session\nuntil we've finished the current session.\nAlso if Alice has sent Ra there is no reason that Bob can't send multiple\n(Rb, sb) pairs for different messages for the same Ra while he wants for\nAlice to respond.\nAlice doesn't have to finish the signature yet she can just store the most\nrecent one and finish if/when she needs to.\nThis preserves the current update communication structure of lightning\nwithout breaking the security requirements of 2 round MuSig.\n\n=== How could PTLCs work ===\n\nUpdating states is more than just signing the commitment transaction. Let's\nassume we do PTLCs with both a PTLC-success and PTLC-timeout pre-signed\ntransaction (I don't see how else to do it) on *both* sides of the channel.\nThis means you have to pre-share the following:\n\n- A fresh pair of nonces for every existing PTLC in the state plus one\nextra (must be done every update)\n- A key for every existing PTLC in the state plus one extra (can be fixed\nat the start of the channel or updated as you go along).\n\nAfter pre-sharing this data Bob starts the communication with:\n\n(the postfix *-alice and *-bob means that they are in the tree of\ntransactions on alice or bob's side, not necessarily that alice or bob is\nthe one broadcasting it).\n\nBob sends:\n    - new PTLC details\n    - (Rb,sb)-success(s)-bob (uses pre-shared nonce)\n    - (Rb,sb)-success(s)-alice (uses pre-shared nonce)\n    - (Rb)-commit-bob\n    - (Rb)-timeout(s)-bob\n    - (Rb)-timeout(s)-alice\n\nAlice sends back:\n    -  (Ra,sa)-timeout(s)-bob\n    -  (Ra,sa)-timeout(s)-alice\n    -  (Ra,sa)-commit-bob\n    -  (Ra)-commit-alice\nBob sends:\n    - (Rb,sb)-commit-alice\n    - revocation key for last commitment\n\n**Note that Alice still hasn't revoked her last state but at this point she\ncan safely forward the PTLC since Bob has revoked his last state.**\n\nOther notes:\n- This is and looks convoluted but it is simpler if you use witness\nasymmetry[2] because it removes duplicating the party's transactions across\nboth sides.\n- Although you are doing parallel signing on the PTLC output key (you are\nsigning both a timeout and success) a different party is receiving the\nsignature and the nonces are communicated in reverse so it actually\nsidesteps the flaw of two round MuSig (where the adversary is always\ndeclaring his nonce second and second).\n- For protocols such as DLCs you will have to do proper three round MuSig\nbut you are not forwarding payments so it's not as time sensitive.\n- I am completely avoiding revocation mechanisms here but from my own ideas\nand what I've seen from others this is compatible.\n\n=== Claim Summary ===\n\nGiven the above, I claim there is a protocol using two-round MuSig for\nfully scriptless lightning that incurs no extra rounds of communication to\nget to the irrevocably committed state.\nIt does incur extra storage for each PTLC in the present commitment\ntransaction.\nIt includes an extra round to \"fully\" update the state between two parties\n(but this does not delay payment forwarding).\n\nI don't claim that this is the optimal path forward but just wanted to make\nthis observation to see what others thought.\n\nLL\n\n[1]\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2019-December/002375.html\n[2] https://github.com/LLFourn/witness-asymmetric-channel\n[3] https://eprint.iacr.org/2020/945.pdf (thanks @n1cklr)\n[4] https://eprint.iacr.org/2020/1057.pdf\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201009/e40df51d/attachment.html>"
            },
            {
                "author": "Lloyd Fournier",
                "date": "2020-10-10T23:47:11",
                "message_text_only": "After posting I was tapped on the shoulder and informed that it is very\nlikely that we will have an enhanced secure parallel composition 2 round\nMuSig in the future.\nThis is really good news.\nThis makes much of the post moot but I think the conclusion about being\nable to do \"pure scriptless\" lightning with the same number of rounds as\ntoday before forwarding the payment is correct.\n\nCheers,\n\nLL\n\nOn Fri, Oct 9, 2020 at 3:31 PM Lloyd Fournier <lloyd.fourn at gmail.com> wrote:\n\n> Hi list,\n>\n> tl;dr: I think can use two round MuSig safely in the context of lightning.\n>\n> As a recap, Zeeman did a good evaluation of \"purely scriptless\" lightning\n> channels after taproot/schnorr.[1]\n> Z concluded that even in the most optimized case the 3 round MuSig\n> protocol leads to an extra round of communication before you can forward a\n> payment.\n> I think this is correct but perhaps you could just use 2 round MuSig\n> (without MuSig-DN magic[4]).\n>\n> === 2 round MuSig ===\n>\n> (from memory) The original MuSig paper suggested the following signing\n> algorithm once two keys A and B have already been established for Alice and\n> Bob\n>\n> Alice                                           Bob\n> pick ra randomly\n> Ra = ra * G\n>                           ----- Ra ---->\n>                                                 pick rb randomly\n>                                                 Rb = rb * G\n>                                                 let c = H(Ra + Rb || A + B\n> || m)\n>                                                 sb = rb + cb\n>                           <--Rb, sb----\n> s = sb + ra + ca\n> R = Ra + Rb\n> X = A + B\n> verify(X, (R, s), m)?\n> output: (R,s)\n>\n> This was insecure under *parallel composition* because the proof cannot\n> work (from my memory because having open signing sessions makes the\n> rewinding argument in the proof incoherent) and led to actual attacks which\n> have recently improved in efficiency.[3]\n>\n> === My insight ===\n>\n> 2 round MuSig *is secure* under sequential composition and in a lightning\n> channel we are essentially arranging sequential state updates so perhaps\n> we're ok here?\n>\n> It should be easy to require that you can't open another signing session\n> until we've finished the current session.\n> Also if Alice has sent Ra there is no reason that Bob can't send multiple\n> (Rb, sb) pairs for different messages for the same Ra while he wants for\n> Alice to respond.\n> Alice doesn't have to finish the signature yet she can just store the most\n> recent one and finish if/when she needs to.\n> This preserves the current update communication structure of lightning\n> without breaking the security requirements of 2 round MuSig.\n>\n> === How could PTLCs work ===\n>\n> Updating states is more than just signing the commitment transaction.\n> Let's assume we do PTLCs with both a PTLC-success and PTLC-timeout\n> pre-signed transaction (I don't see how else to do it) on *both* sides of\n> the channel.\n> This means you have to pre-share the following:\n>\n> - A fresh pair of nonces for every existing PTLC in the state plus one\n> extra (must be done every update)\n> - A key for every existing PTLC in the state plus one extra (can be fixed\n> at the start of the channel or updated as you go along).\n>\n> After pre-sharing this data Bob starts the communication with:\n>\n> (the postfix *-alice and *-bob means that they are in the tree of\n> transactions on alice or bob's side, not necessarily that alice or bob is\n> the one broadcasting it).\n>\n> Bob sends:\n>     - new PTLC details\n>     - (Rb,sb)-success(s)-bob (uses pre-shared nonce)\n>     - (Rb,sb)-success(s)-alice (uses pre-shared nonce)\n>     - (Rb)-commit-bob\n>     - (Rb)-timeout(s)-bob\n>     - (Rb)-timeout(s)-alice\n>\n> Alice sends back:\n>     -  (Ra,sa)-timeout(s)-bob\n>     -  (Ra,sa)-timeout(s)-alice\n>     -  (Ra,sa)-commit-bob\n>     -  (Ra)-commit-alice\n> Bob sends:\n>     - (Rb,sb)-commit-alice\n>     - revocation key for last commitment\n>\n> **Note that Alice still hasn't revoked her last state but at this point\n> she can safely forward the PTLC since Bob has revoked his last state.**\n>\n> Other notes:\n> - This is and looks convoluted but it is simpler if you use witness\n> asymmetry[2] because it removes duplicating the party's transactions across\n> both sides.\n> - Although you are doing parallel signing on the PTLC output key (you are\n> signing both a timeout and success) a different party is receiving the\n> signature and the nonces are communicated in reverse so it actually\n> sidesteps the flaw of two round MuSig (where the adversary is always\n> declaring his nonce second and second).\n> - For protocols such as DLCs you will have to do proper three round MuSig\n> but you are not forwarding payments so it's not as time sensitive.\n> - I am completely avoiding revocation mechanisms here but from my own\n> ideas and what I've seen from others this is compatible.\n>\n> === Claim Summary ===\n>\n> Given the above, I claim there is a protocol using two-round MuSig for\n> fully scriptless lightning that incurs no extra rounds of communication to\n> get to the irrevocably committed state.\n> It does incur extra storage for each PTLC in the present commitment\n> transaction.\n> It includes an extra round to \"fully\" update the state between two parties\n> (but this does not delay payment forwarding).\n>\n> I don't claim that this is the optimal path forward but just wanted to\n> make this observation to see what others thought.\n>\n> LL\n>\n> [1]\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-December/002375.html\n> [2] https://github.com/LLFourn/witness-asymmetric-channel\n> [3] https://eprint.iacr.org/2020/945.pdf (thanks @n1cklr)\n> [4] https://eprint.iacr.org/2020/1057.pdf\n>\n>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201011/a3df0d91/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "two-round MuSig less dangerous than it seems",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Lloyd Fournier"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 11065
        }
    },
    {
        "title": "[Lightning-dev] Hold fees: 402 Payment Required for Lightning itself",
        "thread_messages": [
            {
                "author": "Joost Jager",
                "date": "2020-10-12T11:03:49",
                "message_text_only": "Hello list,\n\nMany discussions have taken place on this list on how to prevent undesired\nuse of the Lightning network. Spamming the network with HTLCs (for probing\npurposes or otherwise) or holding HTLCs to incapacitate channels can be\ndone on today's network at very little cost to an attacker. So far this\ndoesn't seem to be happening in practice, but I believe that it is only a\nmatter of time before it will become a real issue.\n\nRate limits and other limits such as the maximum number of in-flight HTLCs\nincrease the cost of an attack, but may also limit the capabilities of\nhonest users. It works as a mitigation, but it doesn't seem to be the ideal\nsolution.\n\nWe've looked at all kinds of trustless payment schemes to keep users\nhonest, but it appears that none of them is satisfactory. Maybe it is even\ntheoretically impossible to create a scheme that is trustless and has all\nthe properties that we're looking for. (A proof of that would also be\nuseful information to have.)\n\nPerhaps a small bit of trust isn't so bad. There is trust in Lightning\nalready. For example when you open a channel, you trust (or hope) that your\npeer remains well connected, keeps charging reasonable fees, doesn't\nforce-close in a bad way, etc.\n\nWhat I can see working is a system where peers charge each other a hold fee\nfor forwarded HTLCs based on the actual lock time (not the maximum lock\ntime) and the htlc value. This is just for the cost of holding and separate\nfrom the routing fee that is earned when the payment settles.\n\nThis hold fee could be: lock_time * (fee_base + fee_rate * htlc_value).\nfee_base is in there to compensate for the usage of an htlc slot, which is\na scarce resource too.\n\nI think the implementation of this is less interesting at this stage, but\nsome ideas are:\n\nA. Prepayment: node pays an amount to its channel peer (for example via\nkeysend) and the channel peer deducts the hold fees from that prepaid\nbalance until it is at zero. At that point it somehow (in the htlc fail\nmessage?) communicates Lightning's version of http 402 to ask for more\nmoney.\n\nB. Tightly integrated with the htlc add/fail/settle messages. When an htlc\nis added, the maximum cost (based on maximum lock time) for holding is\ndeducted from the sender's channel balance. When the htlc settles, a refund\nis given based on the actual lock time. An additional `update_fee`-like\nmessage is added for peers to update their hold fee parameters (fee_base\nand fee_rate).\n\nIn both cases the sender needs to trust its peer to not steal the payment\nand/or artificially delay the forwarding to inflate the hold fee. I think\nthat is acceptable given that there is a trust relation between peers\nalready anyway.\n\nA crucial thing is that these hold fees don't need to be symmetric. A new\nnode for example that opens a channel to a well-known, established routing\nnode will be forced to pay a hold fee, but won't see any traffic coming in\nanymore if it announces a hold fee itself. Nodes will need to build a\nreputation before they're able to command hold fees. Similarly, routing\nnodes that have a strong relation may decide to not charge hold fees to\neach other at all.\n\nThis asymmetry is what is supposed to prevent channel jamming attacks. The\nattacker needs to pay hold fees to send out the payment, but when it comes\nback to the attacker after traversing a circular route, they won't be able\nto charge a hold fee to cancel out the hold fee paid at the start of the\nroute. (Assuming the attacker node is not trusted.)\n\nA consequence for honest users is that payment attempts are no longer free.\nThe cost should however be negligible for fast-failing attempts. Also\nsenders will have to be a lot more selective when building a route.\nSelecting a 'black hole' hop (hop that doesn't forward nor fail) can be\ncostly.\n\nThe hold fee scheme is a bit looser compared to previously proposed schemes\n(as far as I know...). It is purely an arrangement between channel peers\nand doesn't try to exactly compensate every hop for its costs. Instead\ntrust relations that arguably exist already are leveraged to present a bill\nto the actor who deserves it.\n\nInterested to hear opinions about this proposal.\n\nI'd also like to encourage everyone to prioritize this spam/jam issue and\ndedicate more time to solving it. Obviously there is a lot more to do in\nLightning, but I am not sure if we can afford to wait for the real\nadversaries to show up on this one.\n\nCheers,\nJoost\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201012/c7943c5e/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2020-10-12T15:11:46",
                "message_text_only": "Good morning Joost,\n\nI would like some clarifications on this mechanism.\n\n\n> A. Prepayment: node pays an amount to its channel peer (for example via keysend) and the channel peer deducts the hold fees from that prepaid balance until it is at zero. At that point it somehow (in the htlc fail message?) communicates Lightning's version of http 402 to ask for more money.\n\nIf the node has already forwarded the HTLC onward, what enforcement hold does the node have on the sender of the incoming HTLC?\nPresumably the sender of the HTLC has already gotten what it wanted --- an outgoing HTLC --- so how can the forwarding node enforce this request to get more money.\n\n> B. Tightly integrated with the htlc add/fail/settle messages. When an htlc is added, the maximum cost (based on maximum lock time) for holding is deducted from the sender's channel balance. When the htlc settles, a refund is given based on the actual lock time. An additional `update_fee`-like message\u00a0is added for peers to update\u00a0their hold fee parameters (fee_base and fee_rate).\n\nIf I am a forwarding node, and I receive the preimage from the outgoing HTLC, can I deliberately defer claiming the incoming HTLC (pretending that the outgoing HTLC was taking longer than it actually took) in order to reduce the amount I have to refund?\n\n> In both cases the sender needs to trust its peer to not steal the payment and/or artificially delay the forwarding to inflate the hold fee. I think that is acceptable given that there is a trust relation between peers already anyway.\n\nI am wary of *adding* trust.\nYou might trust someone to keep an eye on your snacks while you go refill your drink, but not to keep an eye on your hardware wallet when you do the same.\n(Since consuming snacks and drinks and hardware wallets are human activities, this should show that I am in fact a human.)\n\n\nHow about this?\n\nBefore, when thinking about JIT routing, I suggested that a JIT-routing enabled forwarding node should only be willing to pay for the JIT rebalancing up to some fraction (less than 1.0) of the amount already earned from the outgoing channel, in order to protect against some attacks.\nAnd when the JIT-routing node does a rebalance in order to serve the forwarding, it should deduct the cost of the rebalance from its cumulative sum of earnings from that outgoing channel.\n\nThe effect of the above is that the already-earned forwarding fees serves as a \"level of trust\" that the rebalancing in order to serve the outgoing forward will not be wasted.\n\nPerhaps intermediate nodes should limit incoming HTLCs from peers that have not given them a lot of successful forwards and earned forwarding fees from those forwards.\ni.e. if it is a new peer, you allow HTLCs up to a certain size, then if the outgoing HTLC is claimed quickly rather than slowly and you earn a good amount of fee, you might be willing to increase the limits of incoming HTLCs.\n\nThis is effectively \"growing trust\".\n\n\nOf course, now we have to wonder about exit scams where a node manipulates you into increasing this \"trust score\" and later screwing you over when you are willing to accept larger total HTLCs....  Sigh.\nOn the other hand, if you base this on the amount of fees you earn per unit time and deducting the converted risk of having your fees locked in outgoing HTLCs, then attackers have to effectively pay you forwarding fees first before they can attack you, and once they attack, they lose the accumulated score.\n\nSo in some way, this system functions a little like pre-paid fees, except the pre-payment is in the actual pay-on-successful-forwarding of previous HTLCs you have forwarded.\nMaybe.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Joost Jager",
                "date": "2020-10-12T15:28:49",
                "message_text_only": ">\n> > A. Prepayment: node pays an amount to its channel peer (for example via\n> keysend) and the channel peer deducts the hold fees from that prepaid\n> balance until it is at zero. At that point it somehow (in the htlc fail\n> message?) communicates Lightning's version of http 402 to ask for more\n> money.\n>\n> If the node has already forwarded the HTLC onward, what enforcement hold\n> does the node have on the sender of the incoming HTLC?\n> Presumably the sender of the HTLC has already gotten what it wanted --- an\n> outgoing HTLC --- so how can the forwarding node enforce this request to\n> get more money.\n>\n\nThe idea is that the available prepaid hold fee balance is enough to cover\nthe worst case hold fee. Otherwise the forward won't happen. The main\ndifference with option B is that you pay a sum upfront which can be used to\ncover multiple forwards. And that this payment is a separate Lightning\npayment, not integrated with the add/fail/settle flow. I prefer option B,\nbut implementation effort is also a consideration.\n\n> B. Tightly integrated with the htlc add/fail/settle messages. When an\n> htlc is added, the maximum cost (based on maximum lock time) for holding is\n> deducted from the sender's channel balance. When the htlc settles, a refund\n> is given based on the actual lock time. An additional `update_fee`-like\n> message is added for peers to update their hold fee parameters (fee_base\n> and fee_rate).\n>\n> If I am a forwarding node, and I receive the preimage from the outgoing\n> HTLC, can I deliberately defer claiming the incoming HTLC (pretending that\n> the outgoing HTLC was taking longer than it actually took) in order to\n> reduce the amount I have to refund?\n>\n\nYes you can. That is the trust part, your peer trusts you not to do this.\nIf they don't trust you, they won't forward to you if you charge a (high)\nhold fee.\n\n> In both cases the sender needs to trust its peer to not steal the payment\n> and/or artificially delay the forwarding to inflate the hold fee. I think\n> that is acceptable given that there is a trust relation between peers\n> already anyway.\n>\n> I am wary of *adding* trust.\n> You might trust someone to keep an eye on your snacks while you go refill\n> your drink, but not to keep an eye on your hardware wallet when you do the\n> same.\n> (Since consuming snacks and drinks and hardware wallets are human\n> activities, this should show that I am in fact a human.)\n>\n\nSo I am arguing that there is trust already between peers. Quite\nconsiderable trust even in case of high on-chain fee conditions. The added\nrisk of being scammed out of these prepay sats may not be significant.\n\nJoost\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201012/bff0c536/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2020-10-13T10:36:36",
                "message_text_only": "Good morning Joost,\n\n> > > A. Prepayment: node pays an amount to its channel peer (for example via keysend) and the channel peer deducts the hold fees from that prepaid balance until it is at zero. At that point it somehow (in the htlc fail message?) communicates Lightning's version of http 402 to ask for more money.\n> >\n> > If the node has already forwarded the HTLC onward, what enforcement hold does the node have on the sender of the incoming HTLC?\n> > Presumably the sender of the HTLC has already gotten what it wanted --- an outgoing HTLC --- so how can the forwarding node enforce this request to get more money.\n>\n> The idea is that the available prepaid hold fee balance is enough to cover the worst case hold fee. Otherwise the forward won't happen. The main difference with option B is that you pay a sum upfront which can be used to cover multiple forwards. And that this payment is a separate Lightning payment, not integrated with the add/fail/settle flow. I prefer option B, but implementation effort is also a consideration.\n\nOkay, so basically, it becomes something like this.\n\n* There exists a network Joost <-> Rusty <-> Rene\n* I connect to Joost.\n* I owe Rene some sats.\n* First, I prepay Joost directly a bunch of sats.\n  * Joost is now willing to forward to Rusty, but Rusty is not yet willing to forward further.\n* I prepay Rusty a bunch of sats.\n  * I effectively \"spend\" some of the prepaid service that I already paid to Joost in order to pay to Rusty.\n* Finally, using the prepay to Joost and Rusty, I now can route a payment to Rene.\n\nIs that approximately how it works out?\n\nDoes it mean that Joost and Rusty have to know the source of the payment route?\nWorse, they have to keep track of prepaid fees from all nodes, not just their direct peers?\n\nIf the above is not done (i.e. if I only prepay Joost but not Rusty) then it seems to me that the below remote attack is possible:\n\n* I convince Rene to make a channel to me.\n* I connect to Joost.\n* I prepay to Joost.\n* I forward Me->Joost->Rusty->Rene->me.\n  * I am exploiting the pre-existing tr\\*st that Rusty has to Joost, and the tr\\*st that Rene has to Rusty.\n* When the HTLC reaches me, I dicker around and wait until it is about to time out before ultimately failing.\n* Rusty loses tr\\*st in Joost, and Rene loses tr\\*st in Rusty.\n\n---\n\nThinking a little more deeply: it is in principle possible to give a financial value to an amount of msat being locked for an amount of time.\nFor instance the C-Lightning `getroute` has a `riskfactor` argument which is used in this conversion.\nBasically, by being locked in an HTLC and later failing, then the forwarding node loses the expected return on investment if instead the amount were locked in an HTLC that later succeeds.\n\nNow, the cost on a forwarding node is based on the actual amount of time that its outgoing HTLC is locked.\n\nWhen we consider multi-hop payments, then we should consider that the initiator of the multi-hop payment is asking multiple nodes to put their funds at risk.\n\nThus, the initiator of a multi-hop payment should, in principle, prepay for *all* the risk of *all* the hops.\n\n\nIf we do not enforce this, then an initiator of a multi-hop payment can pay a small amount relative to the risk that *all* the hops are taking.\n\n\n\n\nSecondarily, we currently assume that forwarding nodes will, upon having their outgoing HTLC claimed, seek to claim the incoming HTLC as quickly as possible.\nThis is because the incoming HTLC would be locked and unuseable until they claim their incoming HTLC, and the liquidity would not be usable for earning more fees until the incoming HTLC is claimed and put into its pool of liquidity.\n\nHowever, if we make anything that is based on the time that a forwarding node claims its incoming HTLC, then this may incentivize the forwarding node to delay claiming the incoming HTLC.\n\n\n\n\n>\n> > > B. Tightly integrated with the htlc add/fail/settle messages. When an htlc is added, the maximum cost (based on maximum lock time) for holding is deducted from the sender's channel balance. When the htlc settles, a refund is given based on the actual lock time. An additional `update_fee`-like message\u00a0is added for peers to update\u00a0their hold fee parameters (fee_base and fee_rate).\n> >\n> > If I am a forwarding node, and I receive the preimage from the outgoing HTLC, can I deliberately defer claiming the incoming HTLC (pretending that the outgoing HTLC was taking longer than it actually took) in order to reduce the amount I have to refund?\n>\n> Yes you can. That is the trust part, your peer trusts you not to do this. If they don't trust you, they won't forward to you if you charge a (high) hold fee.\n\nWhat happens if I charge a tiny hold feerate in msats/second, but end up locking the funds for a week?\nHow does my peer know that even though I charge a tiny hold fee, I will hold their funds hostage for a week?\n\n\n> > > In both cases the sender needs to trust its peer to not steal the payment and/or artificially delay the forwarding to inflate the hold fee. I think that is acceptable given that there is a trust relation between peers already anyway.\n> >\n> > I am wary of *adding* trust.\n> > You might trust someone to keep an eye on your snacks while you go refill your drink, but not to keep an eye on your hardware wallet when you do the same.\n> > (Since consuming snacks and drinks and hardware wallets are human activities, this should show that I am in fact a human.)\n>\n> So I am arguing that there is trust already between peers. Quite considerable trust even in case of high on-chain fee conditions. The added risk of being scammed out of these prepay sats may not be significant.\n\nWell, onchain activity is fairly rare in practice, so I am uncertain about the relative size of prepays compared to relative fees of onchain activity.\nI imagine payment forwardings to be much more common than onchain activity, thus the prepays can add up and be significant compared to fees on onchain activity, thus the tr\\*st being considered may end up being a good fraction of the current tr\\*st.\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Joost Jager",
                "date": "2020-10-13T11:57:07",
                "message_text_only": ">\n> > The idea is that the available prepaid hold fee balance is enough to\n> cover the worst case hold fee. Otherwise the forward won't happen. The main\n> difference with option B is that you pay a sum upfront which can be used to\n> cover multiple forwards. And that this payment is a separate Lightning\n> payment, not integrated with the add/fail/settle flow. I prefer option B,\n> but implementation effort is also a consideration.\n>\n> If the above is not done (i.e. if I only prepay Joost but not Rusty) then\n> it seems to me that the below remote attack is possible:\n>\n\nIndeed, the above isn't done. Z only prepays Joost, not rusty.\n\n\n> * I convince Rene to make a channel to me.\n>\n\nYou may succeed, but Rene is probably not going to pay you a hold fee\nbecause you're untrusted.\n\n\n> * I connect to Joost.\n> * I prepay to Joost.\n> * I forward Me->Joost->Rusty->Rene->me.\n>   * I am exploiting the pre-existing tr\\*st that Rusty has to Joost, and\n> the tr\\*st that Rene has to Rusty.\n> * When the HTLC reaches me, I dicker around and wait until it is about to\n> time out before ultimately failing.\n> * Rusty loses tr\\*st in Joost, and Rene loses tr\\*st in Rusty.\n\n\nBut most importantly: you will have paid hold fees to Joost for the long\nlock time of the htlc. This should keep you from even trying this attack.\n\nThinking a little more deeply: it is in principle possible to give a\n> financial value to an amount of msat being locked for an amount of time.\n> For instance the C-Lightning `getroute` has a `riskfactor` argument which\n> is used in this conversion.\n> Basically, by being locked in an HTLC and later failing, then the\n> forwarding node loses the expected return on investment if instead the\n> amount were locked in an HTLC that later succeeds.\n>\n> Now, the cost on a forwarding node is based on the actual amount of time\n> that its outgoing HTLC is locked.\n>\n\nThat is indeed the proposal, to give financial value to the sats and the\nhtlc slot being locked for an amount of time.\n\n\n> When we consider multi-hop payments, then we should consider that the\n> initiator of the multi-hop payment is asking multiple nodes to put their\n> funds at risk.\n>\n> Thus, the initiator of a multi-hop payment should, in principle, prepay\n> for *all* the risk of *all* the hops.\n>\n>\n> If we do not enforce this, then an initiator of a multi-hop payment can\n> pay a small amount relative to the risk that *all* the hops are taking.\n>\n\nI understand that, but I think it might be a large enough shift in the\nincentives of the attacker.\n\n\n> Secondarily, we currently assume that forwarding nodes will, upon having\n> their outgoing HTLC claimed, seek to claim the incoming HTLC as quickly as\n> possible.\n> This is because the incoming HTLC would be locked and unuseable until they\n> claim their incoming HTLC, and the liquidity would not be usable for\n> earning more fees until the incoming HTLC is claimed and put into its pool\n> of liquidity.\n>\nHowever, if we make anything that is based on the time that a forwarding\n> node claims its incoming HTLC, then this may incentivize the forwarding\n> node to delay claiming the incoming HTLC.\n>\n\nYes, that is the trust part again.\n\n\n> > > > B. Tightly integrated with the htlc add/fail/settle messages. When\n> an htlc is added, the maximum cost (based on maximum lock time) for holding\n> is deducted from the sender's channel balance. When the htlc settles, a\n> refund is given based on the actual lock time. An additional\n> `update_fee`-like message is added for peers to update their hold fee\n> parameters (fee_base and fee_rate).\n> > >\n> > > If I am a forwarding node, and I receive the preimage from the\n> outgoing HTLC, can I deliberately defer claiming the incoming HTLC\n> (pretending that the outgoing HTLC was taking longer than it actually took)\n> in order to reduce the amount I have to refund?\n> >\n> > Yes you can. That is the trust part, your peer trusts you not to do\n> this. If they don't trust you, they won't forward to you if you charge a\n> (high) hold fee.\n>\n> What happens if I charge a tiny hold feerate in msats/second, but end up\n> locking the funds for a week?\n> How does my peer know that even though I charge a tiny hold fee, I will\n> hold their funds hostage for a week?\n>\n\nThat is the trust part also.\n\n\n> > > > In both cases the sender needs to trust its peer to not steal the\n> payment and/or artificially delay the forwarding to inflate the hold fee. I\n> think that is acceptable given that there is a trust relation between peers\n> already anyway.\n> > >\n> > > I am wary of *adding* trust.\n> > > You might trust someone to keep an eye on your snacks while you go\n> refill your drink, but not to keep an eye on your hardware wallet when you\n> do the same.\n> > > (Since consuming snacks and drinks and hardware wallets are human\n> activities, this should show that I am in fact a human.)\n> >\n> > So I am arguing that there is trust already between peers. Quite\n> considerable trust even in case of high on-chain fee conditions. The added\n> risk of being scammed out of these prepay sats may not be significant.\n>\n> Well, onchain activity is fairly rare in practice, so I am uncertain about\n> the relative size of prepays compared to relative fees of onchain activity.\n> I imagine payment forwardings to be much more common than onchain\n> activity, thus the prepays can add up and be significant compared to fees\n> on onchain activity, thus the tr\\*st being considered may end up being a\n> good fraction of the current tr\\*st.\n>\n\nFair point. If you are continuously overcharged on the hold fees, it may\noutweigh the once-in-a-channel-lifetime chain fees. But if all your\npayments through a specific node are delayed, it does get suspicious.\n\nJoost\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201013/e33007ce/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2020-10-13T12:57:43",
                "message_text_only": "Good morning Joost,\n\n\n> > * I convince Rene to make a channel to me.\n>\n> You may succeed, but Rene is probably not going to pay you a hold fee because you're untrusted.\n\nImmaterial: I am interested in damaging the Joost-Rusty and Rusty-Rene relationships, not necessarily recouping these funds.\n\n> \u00a0\n>\n> > * I connect to Joost.\n> > * I prepay to Joost.\n> > * I forward Me->Joost->Rusty->Rene->me.\n> > \u00a0 * I am exploiting the pre-existing tr\\*st that Rusty has to Joost, and the tr\\*st that Rene has to Rusty.\n> > * When the HTLC reaches me, I dicker around and wait until it is about to time out before ultimately failing.\n> > * Rusty loses tr\\*st in Joost, and Rene loses tr\\*st in Rusty.\u00a0\n>\n> But most importantly: you will have paid hold fees to Joost for the long lock time of the htlc. This should keep you from even trying this attack.\n\nBut I might be interested in paying money in order to damage your reputation with Rusty, and damaging the reputation of Rusty with Rene.\nThus my capacity to disrupt the network is increased linearly by the number of hops involved, thus my point: I think what should be paid should be for the entire route, not the first hop.\n\nFor that matter, how certain are you that Rene and Zeeman are not secretly the same person, have you seen them in the same room together?\n\n\nAs I pointed out before, the main reason to engage in these attacks is to lock up the capacity of less-capitalized competitors in the payment forwarding business.\nCases of slow payment resolution caused by intermediate nodes are more likely to be because of incompetence (crashing nodes, ISP disconnections, clumsy humans tripping on power supplies) than active malice.\nThus, the primary motivated attackers are the ends of the payment: the payer and payee, who, in this attack, are coordinating with each other to lock up the funds of multiple other lesser-capacity nodes (and which might be a single node).\n\nTo an extent, to protect against such attacks, we need to know the payer and payee and somehow judge their tr\\*stworthiness --- but we want to not have to reveal their identities, since that works against our privacy.\n\nHmmm.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Rusty Russell",
                "date": "2020-10-13T03:05:39",
                "message_text_only": "Joost Jager <joost.jager at gmail.com> writes:\n> This hold fee could be: lock_time * (fee_base + fee_rate * htlc_value).\n> fee_base is in there to compensate for the usage of an htlc slot, which is\n> a scarce resource too.\n\n...\n> \n> In both cases the sender needs to trust its peer to not steal the payment\n> and/or artificially delay the forwarding to inflate the hold fee. I think\n> that is acceptable given that there is a trust relation between peers\n> already anyway.\n>\n> A crucial thing is that these hold fees don't need to be symmetric. A new\n> node for example that opens a channel to a well-known, established routing\n> node will be forced to pay a hold fee, but won't see any traffic coming in\n> anymore if it announces a hold fee itself. Nodes will need to build a\n> reputation before they're able to command hold fees. Similarly, routing\n> nodes that have a strong relation may decide to not charge hold fees to\n> each other at all.\n\nI can still establish channels to various low-reputation nodes, and then\nuse them to grief a high-reputation node.  Not only do I get to jam up\nthe high-reputation channels, as a bonus I get the low-reputation nodes\nto pay for it!\n\nOperators of high reputation nodes can even make this profitable; doubly\nso, since they eliminate the chance of any of those low-reputation nodes\nevery getting to be high reputation (and thus competing).\n\nAFAICT any scheme which penalizes the direct peer creates a bias against\nforwarding unknown payments, thus is deanonymizing.\n\n> I'd also like to encourage everyone to prioritize this spam/jam issue and\n> dedicate more time to solving it. Obviously there is a lot more to do in\n> Lightning, but I am not sure if we can afford to wait for the real\n> adversaries to show up on this one.\n\nAgreed.  It's a classic \"it's not actually on fire *right now*\" problem,\nso it does keep getting pushed back.\n\nCheers,\nRusty."
            },
            {
                "author": "Joost Jager",
                "date": "2020-10-13T07:46:27",
                "message_text_only": ">\n> > A crucial thing is that these hold fees don't need to be symmetric. A new\n> > node for example that opens a channel to a well-known, established\n> routing\n> > node will be forced to pay a hold fee, but won't see any traffic coming\n> in\n> > anymore if it announces a hold fee itself. Nodes will need to build a\n> > reputation before they're able to command hold fees. Similarly, routing\n> > nodes that have a strong relation may decide to not charge hold fees to\n> > each other at all.\n>\n> I can still establish channels to various low-reputation nodes, and then\n> use them to grief a high-reputation node.  Not only do I get to jam up\n> the high-reputation channels, as a bonus I get the low-reputation nodes\n> to pay for it!\n>\n\nSo you're saying:\n\nATTACKER --(no hold fee)--> LOW-REP --(hold fee)--> HIGH-REP\n\nIf I were LOW-REP, I'd still charge an unknown node a hold fee. I would\nonly waive the hold fee for high-reputation nodes. In that case, the\nattacker is still paying for the attack. I may be forced to take a small\nloss on the difference, but at least the larger part of the pain is felt by\nthe attacker. The assumption is that this is sufficient enough to deter the\nattacker from even trying.\n\n\n> Operators of high reputation nodes can even make this profitable; doubly\n> so, since they eliminate the chance of any of those low-reputation nodes\n> every getting to be high reputation (and thus competing).\n>\n\n> AFAICT any scheme which penalizes the direct peer creates a bias against\n> forwarding unknown payments, thus is deanonymizing.\n>\n\nIf you're an honest but unknown sender (initiating the payment) and you\njust pay the hold fee, I don't think there is a problem? The unknown\nforward will still be carried out by a high-rep node. Also need to keep in\nmind that the hold fee for quick happy flow payments is going to be tiny\n(for example when calculating back from a desired annual return on the\nstaked channel capacity). And we can finally make these parasitic hodl\ninvoice users pay for it!\n\nI guess your concern is with trying to become a routing node? If nobody\nknows you, you'll be forced to pay hold fees but can't attract traffic if\nyou charge hold fees yourself. That indeed means that you'll need to be\nselective with whom you accept htlcs from. Put limits in place to control\nthe expenditure. Successful forwards will earn a routing fee which could\ncompensate for the loss in hold fees too.\n\nI think this mechanism can create interesting dynamics on the network and\neventually reach an equilibrium that is still healthy in terms of\ndecentralization and privacy.\n\nJoost\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201013/aede1d83/attachment.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2020-10-13T11:41:56",
                "message_text_only": "I think the mechanism can indeed create interesting dynamics, but not in\na good sense :-)\n\n>> I can still establish channels to various low-reputation nodes, and\n>> then use them to grief a high-reputation node.  Not only do I get to\n>> jam up the high-reputation channels, as a bonus I get the\n>> low-reputation nodes to pay for it!\n>\n> So you're saying:\n>\n> ATTACKER --(no hold fee)--> LOW-REP --(hold fee)--> HIGH-REP\n>\n> If I were LOW-REP, I'd still charge an unknown node a hold fee. I\n> would only waive the hold fee for high-reputation nodes. In that case,\n> the attacker is still paying for the attack. I may be forced to take a\n> small loss on the difference, but at least the larger part of the pain\n> is felt by the attacker. The assumption is that this is sufficient\n> enough to deter the attacker from even trying.\n\nThe LOW-REP node being out of pocket is the clue here: if one party\nloses funds, even a tiny bit, another party gains some funds. In this\ncase the HIGH-REP node collaborating with the ATTACKER can extract some\nfunds from the intermediate node, allowing them to dime their way to all\nof LOW-REP's funds. If an attack results in even a tiny loss for an\nintermediary and can be repeated, the intermediary's funds can be\nsyphoned by an attacker.\n\nAnother attack that is a spin on ZmnSCPxj's waiting to backpropagate the\npreimage is even worse:\n\n - Attacker node `A` charging hold fees receives HTLC from victim `V`\n - `A` does not forward the HTLC, but starts charging hold fees\n - Just before the timeout for the HTLC would force us to settle onchain\n   `A` just removes the HTLC without forwarding it or he can try to\n   forward at the last moment, potentially blaming someone else for its\n   failure to complete\n\nThis results in `A` extracting the maximum hold fee from `V`, without\nthe downstream hold fees cutting into their profits. By forwarding as\nlate as possible `A` can cause a downstream failure and look innocent,\nand the overall payment has the worst possible outcome: we waited an\neternity for what turns out to be a failed attempt.\n\nCheers,\nChristian"
            },
            {
                "author": "Joost Jager",
                "date": "2020-10-13T12:05:19",
                "message_text_only": ">\n> > If I were LOW-REP, I'd still charge an unknown node a hold fee. I\n> > would only waive the hold fee for high-reputation nodes. In that case,\n> > the attacker is still paying for the attack. I may be forced to take a\n> > small loss on the difference, but at least the larger part of the pain\n> > is felt by the attacker. The assumption is that this is sufficient\n> > enough to deter the attacker from even trying.\n>\n> The LOW-REP node being out of pocket is the clue here: if one party\n> loses funds, even a tiny bit, another party gains some funds. In this\n> case the HIGH-REP node collaborating with the ATTACKER can extract some\n> funds from the intermediate node, allowing them to dime their way to all\n> of LOW-REP's funds. If an attack results in even a tiny loss for an\n> intermediary and can be repeated, the intermediary's funds can be\n> syphoned by an attacker.\n>\n\nThe assumption is that HIGH-REP nodes won't do this :) LOW-REP will see all\nthose failed payments and small losses and start to realize that something\nstrange is happening. I know the proposal isn't fully trustless, but I\nthink it can work in practice.\n\n\n> Another attack that is a spin on ZmnSCPxj's waiting to backpropagate the\n> preimage is even worse:\n>\n>  - Attacker node `A` charging hold fees receives HTLC from victim `V`\n>  - `A` does not forward the HTLC, but starts charging hold fees\n>  - Just before the timeout for the HTLC would force us to settle onchain\n>    `A` just removes the HTLC without forwarding it or he can try to\n>    forward at the last moment, potentially blaming someone else for its\n>    failure to complete\n>\n> This results in `A` extracting the maximum hold fee from `V`, without\n> the downstream hold fees cutting into their profits. By forwarding as\n> late as possible `A` can cause a downstream failure and look innocent,\n> and the overall payment has the worst possible outcome: we waited an\n> eternity for what turns out to be a failed attempt.\n>\n\nThe idea is that an attacker node is untrusted and won't be able to charge\nhold fees.\n\n- Joost\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201013/5b59146c/attachment.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2020-10-13T12:19:20",
                "message_text_only": "Joost Jager <joost.jager at gmail.com> writes:\n>> The LOW-REP node being out of pocket is the clue here: if one party\n>> loses funds, even a tiny bit, another party gains some funds. In this\n>> case the HIGH-REP node collaborating with the ATTACKER can extract some\n>> funds from the intermediate node, allowing them to dime their way to all\n>> of LOW-REP's funds. If an attack results in even a tiny loss for an\n>> intermediary and can be repeated, the intermediary's funds can be\n>> syphoned by an attacker.\n>>\n>\n> The assumption is that HIGH-REP nodes won't do this :) LOW-REP will see all\n> those failed payments and small losses and start to realize that something\n> strange is happening. I know the proposal isn't fully trustless, but I\n> think it can work in practice.\n>\n>\n>> Another attack that is a spin on ZmnSCPxj's waiting to backpropagate the\n>> preimage is even worse:\n>>\n>>  - Attacker node `A` charging hold fees receives HTLC from victim `V`\n>>  - `A` does not forward the HTLC, but starts charging hold fees\n>>  - Just before the timeout for the HTLC would force us to settle onchain\n>>    `A` just removes the HTLC without forwarding it or he can try to\n>>    forward at the last moment, potentially blaming someone else for its\n>>    failure to complete\n>>\n>> This results in `A` extracting the maximum hold fee from `V`, without\n>> the downstream hold fees cutting into their profits. By forwarding as\n>> late as possible `A` can cause a downstream failure and look innocent,\n>> and the overall payment has the worst possible outcome: we waited an\n>> eternity for what turns out to be a failed attempt.\n>>\n>\n> The idea is that an attacker node is untrusted and won't be able to charge\n> hold fees.\n\nThe attacker controls both the sender and the HIGH-REP node. The sender\ndoesn't need to be trusted, it just initiates a payment that is used to\nextract hold fees from a forwarding node. The HIGH-REP node doesn't\nlose reputation because from what we can witness externally the payment\nfailed somewhere downstream. It does require an attacker to have a hold\nfee charging HIGH-REP node, yes, but he is not jeopardizing its\nreputation by having it fail downstream.\n\nCheers,\nChristian"
            },
            {
                "author": "Rusty Russell",
                "date": "2020-10-15T01:49:33",
                "message_text_only": "Joost Jager <joost.jager at gmail.com> writes:\n>>\n>> > A crucial thing is that these hold fees don't need to be symmetric. A new\n>> > node for example that opens a channel to a well-known, established\n>> routing\n>> > node will be forced to pay a hold fee, but won't see any traffic coming\n>> in\n>> > anymore if it announces a hold fee itself. Nodes will need to build a\n>> > reputation before they're able to command hold fees. Similarly, routing\n>> > nodes that have a strong relation may decide to not charge hold fees to\n>> > each other at all.\n>>\n>> I can still establish channels to various low-reputation nodes, and then\n>> use them to grief a high-reputation node.  Not only do I get to jam up\n>> the high-reputation channels, as a bonus I get the low-reputation nodes\n>> to pay for it!\n>\n> So you're saying:\n>\n> ATTACKER --(no hold fee)--> LOW-REP --(hold fee)--> HIGH-REP\n>\n> If I were LOW-REP, I'd still charge an unknown node a hold fee. I would\n> only waive the hold fee for high-reputation nodes. In that case, the\n> attacker is still paying for the attack. I may be forced to take a small\n> loss on the difference, but at least the larger part of the pain is felt by\n> the attacker. The assumption is that this is sufficient enough to deter the\n> attacker from even trying.\n\nNo, because HIGH-REP == ATTACKER and LOW-REP pays.\n\n> I guess your concern is with trying to become a routing node? If nobody\n> knows you, you'll be forced to pay hold fees but can't attract traffic if\n> you charge hold fees yourself. That indeed means that you'll need to be\n> selective with whom you accept htlcs from. Put limits in place to control\n> the expenditure. Successful forwards will earn a routing fee which could\n> compensate for the loss in hold fees too.\n\n\"Be selectinve with whom you accept HTLCs from\"... it always comes back\nto incentives to de-anonymize the network :(\n\n> I think this mechanism can create interesting dynamics on the network and\n> eventually reach an equilibrium that is still healthy in terms of\n> decentralization and privacy.\n\nI suspect that if you try to create a set of actual rules for nodes\nusing actual numbers, I think you'll find you enter a complexity spiral\nas you try to play whack-a-mole on all the different ways you can\nexploit it.\n\n(This is what happened every time I tried to design a peer-penalty\nsystem).\n\nCheers,\nRusty."
            },
            {
                "author": "Antoine Riard",
                "date": "2020-10-15T13:39:37",
                "message_text_only": "Hi Joost,\n\nThanks for your proposal, please find my following opinion which is\ndeliberately on a high-level as IMO defining better threats model and\nagreeing on expected network dynamics resulting from any solution\ntrade-offs sounds required before to work on any solution.\n\n> We've looked at all kinds of trustless payment schemes to keep users\n\n> honest, but it appears that none of them is satisfactory. Maybe it is\neven\n> theoretically impossible to create a scheme that is trustless and has all\n\n> the properties that we're looking for. (A proof of that would also be\n\n> useful information to have.)\n\nI don't think anyone has drawn yet a formal proof of this, but roughly a\nrouting peer Bob, aiming to prevent resource abuse at HTLC relay is seeking\nto answer the following question \"Is this payment coming from Alice and\ngoing to Caroll will compensate for my resources consumption ?\". With the\ncurrent LN system, the compensation is conditional on payment settlement\nsuccess and both Alice and Caroll are distrusted yet discretionary on\nfailure/success. Thus the underscored question is undecidable for a routing\npeer making relay decisions only on packet observation.\n\nOne way to mitigate this, is to introduce statistical observation of\nsender/receiver, namely a reputation system. It can be achieved through a\nscoring system, web-of-trust, or whatever other solution with the same\nproperties.\nBut still it must be underscored that statistical observations are only\nprobabilistic and don't provide resource consumption security to Bob, the\nrouting peer, in a deterministic way. A well-scored peer may start to\nsuddenly misbehave.\n\nIn that sense, the efficiency evaluation of a reputation-based solution to\ndeter DoS must be evaluated based based on the loss of the reputation\nbearer related to the potential damage which can be inflicted. It's just\nreputation sounds harder to compute accurately than a pure payment-based\nDoS protection system.\n\n> Perhaps a small bit of trust isn't so bad. There is trust in Lightning\n\n> already. For example when you open a channel, you trust (or hope) that\nyour\n> peer remains well connected, keeps charging reasonable fees, doesn't\n\n> force-close in a bad way, etc.\n\nThat's a good recall, obviously we should avoid getting stuck in a false\ntrust-vs-trustlessness dichotomy but always bound the discussion to a\nspecific situation. Even the base layer involved some trust assumptions,\nlike fetching your initial p2p peers from DNS seeds, all the matter is how\ndo you minimize this assumption. You might not have the same expectation\nwhen it's miners which might completely screw up the safety of your coin\nstack than routing nodes which might only make your loss a tiny routing\nfee, a minor nuisance.\n\n> What I can see working is a system where peers charge each other a hold\nfee\n> for forwarded HTLCs based on the actual lock time (not the maximum lock\n\n> time) and the htlc value. This is just for the cost of holding and\nseparate\n> from the routing fee that is earned when the payment settles\n\nYes I guess any solution will work as long as it enforces an asymmetry\nbetween the liquidity requester and a honest routing peer. This asymmetry\ncan be defined as guaranteeing that the routing peer's incoming/outgoing\nbalance is always increasing, independently of payment success. Obviously\nthis increase should be materialized by a payment, while minding it might\nbe discounted based on requester reputation (\"pay-with-your-reputation\").\nThis reputation evaluation can be fully delegated to the routing node\npolicy, without network-wise guidance.\n\nThat said, where I'm skeptical on any reputation-heavy system is on the\nlong-term implications.\n\nEither, due to the wants of a subset of actors deliberately willingly to\ntrade satoshis against discounted payment flow by buying well-scored\npubkeys, we see the emergence of a reputation market. Thus enabling\nreputation to be fungible to satoshis, but with now a weird \"reputation\"\ntoken to care about.\n\nOr, reputation is too hard to make liquid (e.g hard to disentangle pubkeys\nfrom channel ownership or export your score across routing peers) and thus\nyou now have reputation scarcity which is introducing a bias from a \"purer\"\nmarket, where agents are only routing based on advertised fees. IMO, we\nshould strive for the more liquid Lightning market we can, as it avoids\nbias towards past actors and thus may contain centralization inertia. I'm\ncurious about your opinion on this last point.\n\nMoving forward, I think t-bast is working on gathering materials to\ncheckbox the first step, establishing a fully-fledged threat model.\n\nCheers,\n\nAntoine\n\nLe lun. 12 oct. 2020 \u00e0 07:04, Joost Jager <joost.jager at gmail.com> a \u00e9crit :\n\n> Hello list,\n>\n> Many discussions have taken place on this list on how to prevent undesired\n> use of the Lightning network. Spamming the network with HTLCs (for probing\n> purposes or otherwise) or holding HTLCs to incapacitate channels can be\n> done on today's network at very little cost to an attacker. So far this\n> doesn't seem to be happening in practice, but I believe that it is only a\n> matter of time before it will become a real issue.\n>\n> Rate limits and other limits such as the maximum number of in-flight HTLCs\n> increase the cost of an attack, but may also limit the capabilities of\n> honest users. It works as a mitigation, but it doesn't seem to be the ideal\n> solution.\n>\n> We've looked at all kinds of trustless payment schemes to keep users\n> honest, but it appears that none of them is satisfactory. Maybe it is even\n> theoretically impossible to create a scheme that is trustless and has all\n> the properties that we're looking for. (A proof of that would also be\n> useful information to have.)\n>\n> Perhaps a small bit of trust isn't so bad. There is trust in Lightning\n> already. For example when you open a channel, you trust (or hope) that your\n> peer remains well connected, keeps charging reasonable fees, doesn't\n> force-close in a bad way, etc.\n>\n> What I can see working is a system where peers charge each other a hold\n> fee for forwarded HTLCs based on the actual lock time (not the maximum lock\n> time) and the htlc value. This is just for the cost of holding and separate\n> from the routing fee that is earned when the payment settles.\n>\n> This hold fee could be: lock_time * (fee_base + fee_rate * htlc_value).\n> fee_base is in there to compensate for the usage of an htlc slot, which is\n> a scarce resource too.\n>\n> I think the implementation of this is less interesting at this stage, but\n> some ideas are:\n>\n> A. Prepayment: node pays an amount to its channel peer (for example via\n> keysend) and the channel peer deducts the hold fees from that prepaid\n> balance until it is at zero. At that point it somehow (in the htlc fail\n> message?) communicates Lightning's version of http 402 to ask for more\n> money.\n>\n> B. Tightly integrated with the htlc add/fail/settle messages. When an htlc\n> is added, the maximum cost (based on maximum lock time) for holding is\n> deducted from the sender's channel balance. When the htlc settles, a refund\n> is given based on the actual lock time. An additional `update_fee`-like\n> message is added for peers to update their hold fee parameters (fee_base\n> and fee_rate).\n>\n> In both cases the sender needs to trust its peer to not steal the payment\n> and/or artificially delay the forwarding to inflate the hold fee. I think\n> that is acceptable given that there is a trust relation between peers\n> already anyway.\n>\n> A crucial thing is that these hold fees don't need to be symmetric. A new\n> node for example that opens a channel to a well-known, established routing\n> node will be forced to pay a hold fee, but won't see any traffic coming in\n> anymore if it announces a hold fee itself. Nodes will need to build a\n> reputation before they're able to command hold fees. Similarly, routing\n> nodes that have a strong relation may decide to not charge hold fees to\n> each other at all.\n>\n> This asymmetry is what is supposed to prevent channel jamming attacks. The\n> attacker needs to pay hold fees to send out the payment, but when it comes\n> back to the attacker after traversing a circular route, they won't be able\n> to charge a hold fee to cancel out the hold fee paid at the start of the\n> route. (Assuming the attacker node is not trusted.)\n>\n> A consequence for honest users is that payment attempts are no longer\n> free. The cost should however be negligible for fast-failing attempts. Also\n> senders will have to be a lot more selective when building a route.\n> Selecting a 'black hole' hop (hop that doesn't forward nor fail) can be\n> costly.\n>\n> The hold fee scheme is a bit looser compared to previously proposed\n> schemes (as far as I know...). It is purely an arrangement between channel\n> peers and doesn't try to exactly compensate every hop for its costs.\n> Instead trust relations that arguably exist already are leveraged to\n> present a bill to the actor who deserves it.\n>\n> Interested to hear opinions about this proposal.\n>\n> I'd also like to encourage everyone to prioritize this spam/jam issue and\n> dedicate more time to solving it. Obviously there is a lot more to do in\n> Lightning, but I am not sure if we can afford to wait for the real\n> adversaries to show up on this one.\n>\n> Cheers,\n> Joost\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201015/75e1091b/attachment.html>"
            },
            {
                "author": "Joost Jager",
                "date": "2020-10-18T07:24:40",
                "message_text_only": ">\n> > We've looked at all kinds of trustless payment schemes to keep users\n>\n> > honest, but it appears that none of them is satisfactory. Maybe it is\n> even\n> > theoretically impossible to create a scheme that is trustless and has\n> all\n> > the properties that we're looking for. (A proof of that would also be\n>\n> > useful information to have.)\n>\n> I don't think anyone has drawn yet a formal proof of this, but roughly a\n> routing peer Bob, aiming to prevent resource abuse at HTLC relay is seeking\n> to answer the following question \"Is this payment coming from Alice and\n> going to Caroll will compensate for my resources consumption ?\". With the\n> current LN system, the compensation is conditional on payment settlement\n> success and both Alice and Caroll are distrusted yet discretionary on\n> failure/success. Thus the underscored question is undecidable for a routing\n> peer making relay decisions only on packet observation.\n>\n> One way to mitigate this, is to introduce statistical observation of\n> sender/receiver, namely a reputation system. It can be achieved through a\n> scoring system, web-of-trust, or whatever other solution with the same\n> properties.\n> But still it must be underscored that statistical observations are only\n> probabilistic and don't provide resource consumption security to Bob, the\n> routing peer, in a deterministic way. A well-scored peer may start to\n> suddenly misbehave.\n>\n> In that sense, the efficiency evaluation of a reputation-based solution to\n> deter DoS must be evaluated based based on the loss of the reputation\n> bearer related to the potential damage which can be inflicted. It's just\n> reputation sounds harder to compute accurately than a pure payment-based\n> DoS protection system.\n>\n\nI can totally see the issues and complexity of a reputation-based system.\nWith 'trustless payment scheme' I meant indeed a trustless pure\npayment-based DoS protection system and the question whether such a system\ncan be proven to not exist. A sender would pay an up-front amount to cover\nthe maximum cost, but with the guarantee that nodes can only take a fair\npart of the deposit (based on actual lock time). Perhaps the taproot\nupgrade offers new possibilities with adaptor signatures to atomically swap\npart of the up-front payment with htlc-received-in-time-signatures from\nnodes downstream (random wild idea).\n\n\n> > What I can see working is a system where peers charge each other a hold\n> fee\n> > for forwarded HTLCs based on the actual lock time (not the maximum lock\n>\n> > time) and the htlc value. This is just for the cost of holding and\n> separate\n> > from the routing fee that is earned when the payment settles\n>\n> Yes I guess any solution will work as long as it enforces an asymmetry\n> between the liquidity requester and a honest routing peer. This asymmetry\n> can be defined as guaranteeing that the routing peer's incoming/outgoing\n> balance is always increasing, independently of payment success. Obviously\n> this increase should be materialized by a payment, while minding it might\n> be discounted based on requester reputation (\"pay-with-your-reputation\").\n> This reputation evaluation can be fully delegated to the routing node\n> policy, without network-wise guidance.\n>\n> That said, where I'm skeptical on any reputation-heavy system is on the\n> long-term implications.\n>\n> Either, due to the wants of a subset of actors deliberately willingly to\n> trade satoshis against discounted payment flow by buying well-scored\n> pubkeys, we see the emergence of a reputation market. Thus enabling\n> reputation to be fungible to satoshis, but with now a weird \"reputation\"\n> token to care about.\n>\n> Or, reputation is too hard to make liquid (e.g hard to disentangle pubkeys\n> from channel ownership or export your score across routing peers) and thus\n> you now have reputation scarcity which is introducing a bias from a \"purer\"\n> market, where agents are only routing based on advertised fees. IMO, we\n> should strive for the more liquid Lightning market we can, as it avoids\n> bias towards past actors and thus may contain centralization inertia. I'm\n> curious about your opinion on this last point.\n>\n\nI am in favor of more liquidity and less centralization, but as far as I\nknow the reality is that we don't have a good solution yet to achieve this\nwithout being vulnerable to DoS attacks. If those attacks would happen on a\nlarge scale today, what would we do?\n\nAlso peers can implement these trusted upfront payments without protocol\nchanges. Just stop forwarding when the prepaid forwarding budget is used up\nand require a top-up. It may have been implemented already in parts of the\nnetwork, I don't think there is a way to know. I've experimented a bit with\nthe fee model myself (\nhttps://twitter.com/joostjgr/status/1317546071984427009). Node operators\ndon't need to wait for permission.\n\nTo me it seems that the longer it takes to come up with a good anti-DoS\nsystem for Lightning, the further the outside world will have developed\ntheir trust-based systems and established that potential bias towards\ncentralization.\n\nThat might be another reason to prioritize this issue. Not just because we\nwant the network to be safe, but also to be able to implement the preferred\nsolution while the opportunity window is still open.\n\n- Joost\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201018/6af64b5a/attachment.html>"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2020-10-19T15:38:59",
                "message_text_only": "Good morning list,\n\nI've started summarizing proposals, attacks and threat models on github [1].\nI'm hoping it will help readers get up-to-speed and avoid falling in the\nsame pitfalls we already\nfell into with previous proposals.\n\nI've kept it very high-level for now; we can add nitty-gritty technical\ndetails as we slowly\nconverge towards acceptable solutions. I have probably missed subtleties\nfrom previous proposals;\nfeel free to contribute to correct my mistakes. I have omitted for examples\nthe details of Rusty's\nprevious proposal since he mentioned a new, better one that will be\ndescribed soon.\n\nWhile doing this exercise, I couldn't find a reason why the `reverse\nupfront payment` proposal\nwould be broken (notice that I described it using a flat amount after a\ngrace period, not an amount\nbased on the time HTLCs are held). Can someone point me to the most obvious\nattacks on it?\n\nIt feels to me that its only issue is that it still allows spamming for\ndurations smaller than the\ngrace period; my gut feeling is that if we add a smaller forward direction\nupfront payment to\ncomplement it it could be a working solution.\n\nPasting it here for completeness:\n\n### Reverse upfront payment\n\nThis proposal builds on the previous one, but reverses the flow. Nodes pay\na fee for *receiving*\nHTLCs instead of *sending* them.\n\n```text\nA -----> B -----> C -----> D\n\nB pays A to receive the HTLC.\nThen C pays B to receive the forwarded HTLC.\nThen D pays C to receive the forwarded HTLC.\n```\n\nThere must be a grace period during which no fees are paid; otherwise the\n`uncontrolled spam` attack\nallows the attacker to force all nodes in the route to pay fees while he's\nnot paying anything.\n\nThe fee cannot be the same at each hop, otherwise it's free for the\nattacker when he is at both\nends of the payment route.\n\nThis fee must increase as the HTLC travels downstream: this ensures that\nnodes that hold HTLCs\nlonger are penalized more than nodes that fail them fast, and if a node has\nto hold an HTLC for a\nlong time because it's stuck downstream, they will receive more fees than\nwhat they have to pay.\n\nThe grace period cannot be the same at each hop either, otherwise the\nattacker can force Bob to be\nthe only one to pay fees. Similarly to how we have `cltv_expiry_delta`,\nnodes must have a\n`grace_period_delta` and the `grace_period` must be bigger upstream than\ndownstream.\n\nDrawbacks:\n\n* The attacker can still lock HTLCs for the duration of the `grace_period`\nand repeat the attack\ncontinuously\n\nOpen questions:\n\n* Does the fee need to be based on the time the HTLC is held?\n* What happens when a channel closes and HTLC-timeout has to be redeemed\non-chain?\n* Can we implement this without exposing the route length to intermediate\nnodes?\n\nCheers,\nBastien\n\n[1] https://github.com/t-bast/lightning-docs/blob/master/spam-prevention.md\n\nLe dim. 18 oct. 2020 \u00e0 09:25, Joost Jager <joost.jager at gmail.com> a \u00e9crit :\n\n> > We've looked at all kinds of trustless payment schemes to keep users\n>>\n>> > honest, but it appears that none of them is satisfactory. Maybe it is\n>> even\n>> > theoretically impossible to create a scheme that is trustless and has\n>> all\n>> > the properties that we're looking for. (A proof of that would also be\n>>\n>> > useful information to have.)\n>>\n>> I don't think anyone has drawn yet a formal proof of this, but roughly a\n>> routing peer Bob, aiming to prevent resource abuse at HTLC relay is seeking\n>> to answer the following question \"Is this payment coming from Alice and\n>> going to Caroll will compensate for my resources consumption ?\". With the\n>> current LN system, the compensation is conditional on payment settlement\n>> success and both Alice and Caroll are distrusted yet discretionary on\n>> failure/success. Thus the underscored question is undecidable for a routing\n>> peer making relay decisions only on packet observation.\n>>\n>> One way to mitigate this, is to introduce statistical observation of\n>> sender/receiver, namely a reputation system. It can be achieved through a\n>> scoring system, web-of-trust, or whatever other solution with the same\n>> properties.\n>> But still it must be underscored that statistical observations are only\n>> probabilistic and don't provide resource consumption security to Bob, the\n>> routing peer, in a deterministic way. A well-scored peer may start to\n>> suddenly misbehave.\n>>\n>> In that sense, the efficiency evaluation of a reputation-based solution\n>> to deter DoS must be evaluated based based on the loss of the reputation\n>> bearer related to the potential damage which can be inflicted. It's just\n>> reputation sounds harder to compute accurately than a pure payment-based\n>> DoS protection system.\n>>\n>\n> I can totally see the issues and complexity of a reputation-based system.\n> With 'trustless payment scheme' I meant indeed a trustless pure\n> payment-based DoS protection system and the question whether such a system\n> can be proven to not exist. A sender would pay an up-front amount to cover\n> the maximum cost, but with the guarantee that nodes can only take a fair\n> part of the deposit (based on actual lock time). Perhaps the taproot\n> upgrade offers new possibilities with adaptor signatures to atomically swap\n> part of the up-front payment with htlc-received-in-time-signatures from\n> nodes downstream (random wild idea).\n>\n>\n>> > What I can see working is a system where peers charge each other a hold\n>> fee\n>> > for forwarded HTLCs based on the actual lock time (not the maximum lock\n>>\n>> > time) and the htlc value. This is just for the cost of holding and\n>> separate\n>> > from the routing fee that is earned when the payment settles\n>>\n>> Yes I guess any solution will work as long as it enforces an asymmetry\n>> between the liquidity requester and a honest routing peer. This asymmetry\n>> can be defined as guaranteeing that the routing peer's incoming/outgoing\n>> balance is always increasing, independently of payment success. Obviously\n>> this increase should be materialized by a payment, while minding it might\n>> be discounted based on requester reputation (\"pay-with-your-reputation\").\n>> This reputation evaluation can be fully delegated to the routing node\n>> policy, without network-wise guidance.\n>>\n>> That said, where I'm skeptical on any reputation-heavy system is on the\n>> long-term implications.\n>>\n>> Either, due to the wants of a subset of actors deliberately willingly to\n>> trade satoshis against discounted payment flow by buying well-scored\n>> pubkeys, we see the emergence of a reputation market. Thus enabling\n>> reputation to be fungible to satoshis, but with now a weird \"reputation\"\n>> token to care about.\n>>\n>> Or, reputation is too hard to make liquid (e.g hard to disentangle\n>> pubkeys from channel ownership or export your score across routing peers)\n>> and thus you now have reputation scarcity which is introducing a bias from\n>> a \"purer\" market, where agents are only routing based on advertised fees.\n>> IMO, we should strive for the more liquid Lightning market we can, as it\n>> avoids bias towards past actors and thus may contain centralization\n>> inertia. I'm curious about your opinion on this last point.\n>>\n>\n> I am in favor of more liquidity and less centralization, but as far as I\n> know the reality is that we don't have a good solution yet to achieve this\n> without being vulnerable to DoS attacks. If those attacks would happen on a\n> large scale today, what would we do?\n>\n> Also peers can implement these trusted upfront payments without protocol\n> changes. Just stop forwarding when the prepaid forwarding budget is used up\n> and require a top-up. It may have been implemented already in parts of the\n> network, I don't think there is a way to know. I've experimented a bit with\n> the fee model myself (\n> https://twitter.com/joostjgr/status/1317546071984427009). Node operators\n> don't need to wait for permission.\n>\n> To me it seems that the longer it takes to come up with a good anti-DoS\n> system for Lightning, the further the outside world will have developed\n> their trust-based systems and established that potential bias towards\n> centralization.\n>\n> That might be another reason to prioritize this issue. Not just because we\n> want the network to be safe, but also to be able to implement the preferred\n> solution while the opportunity window is still open.\n>\n> - Joost\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201019/93818bf0/attachment.html>"
            },
            {
                "author": "Joost Jager",
                "date": "2020-10-20T07:15:55",
                "message_text_only": "Hi Bastien,\n\nThanks for creating the summary!\n\nWhile doing this exercise, I couldn't find a reason why the `reverse\n> upfront payment` proposal\n> would be broken (notice that I described it using a flat amount after a\n> grace period, not an amount\n> based on the time HTLCs are held). Can someone point me to the most\n> obvious attacks on it?\n>\n> It feels to me that its only issue is that it still allows spamming for\n> durations smaller than the\n> grace period; my gut feeling is that if we add a smaller forward direction\n> upfront payment to\n> complement it it could be a working solution.\n>\n\nThe 'uncontrolled spamming' as you called it in your doc is pretty serious.\nIf you want to have fun, you should really try to spin up a bunch of\nthreads and keep your outgoing channels fully saturated with max length\nroutes going nowhere. I tried it on testnet and it was quite bad. All that\ntraffic is fighting for resources which makes it take even longer to unlock\nthe htlcs again.\n\nI think that any solution should definitely address this case too.\n\nYour proposal to add a small upfront payment, wouldn't that allow the\n(arbitrary) grace period to be removed? It would mean that routing nodes\nalways need to pay something for forwarding spam, but if they do it quick\nenough (honest nodes) that expense is covered by the upfront payment.\n\n- Joost\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201020/0861166d/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2020-10-21T03:21:11",
                "message_text_only": "Good morning t-bast,\n\n>\n> I've started summarizing proposals, attacks and threat models on github [1].\n> I'm hoping it will help readers get up-to-speed and avoid falling in the same pitfalls we already\n> fell into with previous proposals.\n\nWould my inane incremental routing idea also be in scope here? https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-October/002811.html\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2020-10-22T09:47:20",
                "message_text_only": "Good morning list,\n\nSorry in advance for the lengthy email, but I think it's worth detailing my\nhybrid proposal\n(bidirectional upfront payments), it feels to me like a workable solution\nthat builds on\nprevious proposals. You can safely ignore the details at the end of the\nemail and focus only on\nthe high-level mechanism at first.\n\nLet's consider the following route: A -----> B -----> C -----> D\n\nWe add a `hold_grace_period_delta` field to `channel_update` (in seconds).\nWe add two new fields in the tlv extension of `update_add_htlc`:\n\n* `hold_grace_period` (seconds)\n* `hold_fees` (msat)\n\nWe add an `outgoing_hold_grace_period` field in the onion per-hop payload.\n\nWhen nodes receive an `update_add_htlc`, they verify that:\n\n* `hold_fees` is not unreasonable large\n* `hold_grace_period` is not unreasonably small or large\n* `hold_grace_period` - `outgoing_hold_grace_period` >=\n`hold_grace_period_delta`\n\nOtherwise they immediately fail the HTLC instead of relaying it.\n\nFor the example we assume all nodes use `hold_grace_period_delta = 10`.\n\nWe add a forward upfront payment of 1 msat (fixed) that is paid\nunconditionally when offering an HTLC.\nWe add a backwards upfront payment of `hold_fees` that is paid when\nreceiving an HTLC, but refunded\nif the HTLC is settled before the `hold_grace_period` ends (see footnotes\nabout this).\n\n* A sends an HTLC to B:\n* `hold_grace_period = 100 sec`\n* `hold_fees = 5 msat`\n* `next_hold_grace_period = 90 sec`\n* forward upfront payment: 1 msat is deduced from A's main output and added\nto B's main output\n* backwards upfront payment: 5 msat are deduced from B's main output and\nadded to A's main output\n* B forwards the HTLC to C:\n* `hold_grace_period = 90 sec`\n* `hold_fees = 6 msat`\n* `next_hold_grace_period = 80 sec`\n* forward upfront payment: 1 msat is deduced from B's main output and added\nto C's main output\n* backwards upfront payment: 6 msat are deduced from C's main output and\nadded to B's main output\n* C forwards the HTLC to D:\n* `hold_grace_period = 80 sec`\n* `hold_fees = 7 msat`\n* `next_hold_grace_period = 70 sec`\n* forward upfront payment: 1 msat is deduced from C's main output and added\nto D's main output\n* backwards upfront payment: 7 msat are deduced from D's main output and\nadded to C's main output\n\n* Scenario 1: D settles the HTLC quickly:\n* all backwards upfront payments are refunded (returned to the respective\nmain outputs)\n* only the forward upfront payments have been paid (to protect against\n`uncontrolled spam`)\n\n* Scenario 2: D settles the HTLC after the grace period:\n* D's backwards upfront payment is not refunded\n* If C and B relay the settlement upstream quickly (before\n`hold_grace_period_delta`) their backwards\nupfront payments are refunded\n* all the forward upfront payments have been paid (to protect against\n`uncontrolled spam`)\n\n* Scenario 3: C delays the HTLC:\n* D settles before its `grace_period`, so its backwards upfront payment is\nrefunded by C\n* C delays before settling upstream: it can ensure B will not get refunded,\nbut C will not get\nrefunded either so B gains the difference in backwards upfront payments\n(which protects against\n`controlled spam`)\n* all the forward upfront payments have been paid (to protect against\n`uncontrolled spam`)\n\n* Scenario 4: the channel B <-> C closes:\n* D settles before its `grace_period`, so its backwards upfront payment is\nrefunded by C\n* for whatever reason (malicious or not) the B <-> C channel closes\n* this ensures that C's backwards upfront payment is paid to B\n* if C publishes an HTLC-fulfill quickly, B may have his backwards upfront\npayment refunded by A\n* if B is forced to wait for his HTLC-timeout, his backwards upfront\npayment will not be refunded\nbut it's ok because B got C's backwards upfront payment\n* all the forward upfront payments have been paid (to protect against\n`uncontrolled spam`)\n\nIf done naively, this mechanism may allow intermediate nodes to deanonymize\nsender/recipient.\nIf the base `grace_period` and `hold_fees` are randomized, I believe this\nattack vector disappears,\nbut it's worth exploring in more details.\n\nThe most painful part of this proposal will be handling the `grace_period`:\n\n* when do you start counting: when you send/receive `update_add_htlc`,\n`commit_sig` or\n`revoke_and_ack`?\n* what happens if there is a disconnection (how do you account for the\ndelay of reconnecting)?\n* what happens if the remote settles after the `grace_period`, but refunds\nhimself when sending his\n`commit_sig` (making it look like from his point of view he settled before\nthe `grace_period`)?\nI think in that case the behavior should be to give your peers some leeway\nand let them get away\nwith it, but record it. If they're doing it too often, close channels and\nban them; stealing\nupfront fees should never be worth losing channels.\n\nI chose to make the backwards upfront payment fixed instead of scaling it\nbased on the time an HTLC\nis left pending; it's slightly less penalizing for spammers, but is less\ncomplex and introduces less\npotential griefing against honest nodes. With the scaling approach, an\nhonest node that has its\nchannel unilaterally closed is too heavily penalized IMHO (because it has\nto pay for the maximum\nhold duration).\n\nI also chose to make the forward upfront payment constant (1 msat). Is it\ngoing to be a pain to\nbikeshed this constant? Do we need to add a mechanism to upgrade it? We\ndon't want to make this\nmore complex than it should.\n\nBefore we dive into the specifics (addressing the implementation concerns),\ncan you all please take\na bit of time to figure out whether the proposed mechanisms would mitigate\nspam or not, and whether\nit introduces griefing attacks against honest nodes. I think it would be a\nwaste of your time to\nbikeshed the nuts and bolts details if the proposal is fundamentally\nbroken...\n\nAnswering to previous emails below.\n\nYour proposal to add a small upfront payment, wouldn't that allow the\n> (arbitrary) grace period to be removed?\n\n\nI think we need a `grace_period` for the backwards upfront payment, to\nencourage nodes to settle\nquickly (otherwise I believe it's too easy to grief honest nodes because\nthe backwards upfront payment\nwill be bigger than the forward one - because `uncontrolled spam` is based\non volume so it doesn't\nneed a huge fee to mitigate).\n\nWould my inane incremental routing idea also be in scope here?\n\n\nIt could potentially be easier to integrate spam mitigation inside\nincremental routing, indeed.\nBut I think this proposal is unfortunately very costly in terms of\nlatency...\n\nThanks everyone,\nBastien\n\nLe mer. 21 oct. 2020 \u00e0 05:21, ZmnSCPxj <ZmnSCPxj at protonmail.com> a \u00e9crit :\n\n> Good morning t-bast,\n>\n> >\n> > I've started summarizing proposals, attacks and threat models on github\n> [1].\n> > I'm hoping it will help readers get up-to-speed and avoid falling in the\n> same pitfalls we already\n> > fell into with previous proposals.\n>\n> Would my inane incremental routing idea also be in scope here?\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-October/002811.html\n>\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201022/4a7c0856/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2020-10-22T17:56:12",
                "message_text_only": "Good morning t-bast,\n\n> Sorry in advance for the lengthy email,\n\nCome on, ZmnSCPxj writes lengthier.\n\n> but I think it's worth detailing my hybrid proposal\n> (bidirectional upfront payments), it feels to me like a workable solution that builds on\n> previous proposals. You can safely ignore the details at the end of the email and focus only on\n> the high-level mechanism at first.\n>\n> Let's consider the following route: A -----> B -----> C -----> D\n>\n> We add a `hold_grace_period_delta` field to `channel_update` (in seconds).\n> We add two new fields in the tlv extension of `update_add_htlc`:\n>\n> * `hold_grace_period` (seconds)\n> * `hold_fees` (msat)\n>\n> We add an `outgoing_hold_grace_period` field in the onion per-hop payload.\n>\n> When nodes receive an `update_add_htlc`, they verify that:\n>\n> * `hold_fees` is not unreasonable large\n> * `hold_grace_period` is not unreasonably small or large\n> * `hold_grace_period` - `outgoing_hold_grace_period` >= `hold_grace_period_delta`\n>\n> Otherwise they immediately fail the HTLC instead of relaying it.\n>\n> For the example we assume all nodes use `hold_grace_period_delta = 10`.\n>\n> We add a forward upfront payment of 1 msat (fixed) that is paid unconditionally when offering an HTLC.\n> We add a backwards upfront payment of `hold_fees` that is paid when receiving an HTLC, but refunded\n> if the HTLC is settled before the `hold_grace_period` ends (see footnotes about this).\n\nMy first instinct is that additional complications are worse in general.\n\nHowever, it looks like simpler solutions are truly not enough, so adding the complication may very well be necessary.\n\nPossibly one of the issues with previous proposals is that we considered the source as the payer of upfront fees always, without considering the possibility of the destination paying upfront fees.\n\nWe should consider *why* we did so.\nIs it only because of lack of imagination?\n\nThe succeeding text refers to HTLCs \"settling\".\nWhat does this mean exactly?\nDoes it refer to the preimage being revealed only?\nOr does it refer to *either* `update_fulfill_htlc` **OR** `update_fail_htlc`?\n\nIf \"settling\" does not include failing the HTLC, I can extract hold fees from my peers by giving them HTLCs to random hashes that with very high probability has a preimage unknown to my peer.\n\nThus, \"settling\" here must also include failing the HTLC.\n\n\nCan we arrange the HTLC as below?\n\n* The HTLC is lower by the from-destination hold fee than what it \"should\" be.\n  * So in your example, A would offer an HTLC that is 5 msat lower than what it \"should\" be, to represent the from-destination hold-fee.\n* When the HTLC is fulfilled offchain, we move the hold fee amount from offerrer to acceptor, thus \"refunding\" the hold fee.\n* If the HTLC is failed offchain, we just delete the HTLC.\n\nThe above would disincentivize dropping the channel onchain for whatever shenanigans might be possible there.\n\nNote that if hold fees are always from the \"main output\" of the acceptor, then we cannot use single-funded channel opening.\nSingle-funded channel opening is significantly lower-risk as, if somebody opens a channel to you and then goes offline forever, you would not care, none of your funds are in the channel.\n(Sure they could move funds out of the channel and *then* go offline forever, but you got paid for that by the forwarding fees in the first place.)\nDual-funding requires some amount of trust/reputation, where you would only be willing to put funds on the initiating peer if you are reasonably sure that they would remain online so that the funds you put in the channel are not uselessly locked.\n\nIf we also require that the hold fee be funded from the main output, then we cannot use single-funded channels, except perhaps with `push_msat`.\n\nOn the other hand, it might be useful to require that though, as a promise from the funder that yes, it *is* committed to making that channel work, and will thus `push_msat` at you so you can pay for hold fees.\n\n\n>\n> * A sends an HTLC to B:\n> * `hold_grace_period = 100 sec`\n> * `hold_fees = 5 msat`\n> * `next_hold_grace_period = 90 sec`\n> * forward upfront payment: 1 msat is deduced from A's main output and added to B's main output\n> * backwards upfront payment: 5 msat are deduced from B's main output and added to A's main output\n> * B forwards the HTLC to C:\n> * `hold_grace_period = 90 sec`\n> * `hold_fees = 6 msat`\n> * `next_hold_grace_period = 80 sec`\n> * forward upfront payment: 1 msat is deduced from B's main output and added to C's main output\n> * backwards upfront payment: 6 msat are deduced from C's main output and added to B's main output\n> * C forwards the HTLC to D:\n> * `hold_grace_period = 80 sec`\n> * `hold_fees = 7 msat`\n> * `next_hold_grace_period = 70 sec`\n> * forward upfront payment: 1 msat is deduced from C's main output and added to D's main output\n> * backwards upfront payment: 7 msat are deduced from D's main output and added to C's main output\n\nThe fact that the hold fees are incrementing with distance from the source means intermediate nodes now have an unreliable oracle for distance-from-source, though I guess you mention that later.\n\nIt seems to me that we have no escape here, and we really do need to leak some more information, sigh.\n\n\n>\n> * Scenario 1: D settles the HTLC quickly:\n> * all backwards upfront payments are refunded (returned to the respective main outputs)\n> * only the forward upfront payments have been paid (to protect against `uncontrolled spam`)\n>\n> * Scenario 2: D settles the HTLC after the grace period:\n> * D's backwards upfront payment is not refunded\n> * If C and B relay the settlement upstream quickly (before `hold_grace_period_delta`) their backwards\n> upfront payments are refunded\n\nAnd in this case C earns.\n\nCan C delay the refund to D to after the grace period even if D settled the HTLC quickly?\nHopefully the only way C can do that would be if it refused to respond to D giving the result of the HTLC, in which case --- would D be justified in dropping the channel onchain and hurting the connectivity of C (but also hurting its own connectivity, which might not be symmetric)?\n\n\n> * all the forward upfront payments have been paid (to protect against `uncontrolled spam`)\n>\n> * Scenario 3: C delays the HTLC:\n> * D settles before its `grace_period`, so its backwards upfront payment is refunded by C\n> * C delays before settling upstream: it can ensure B will not get refunded, but C will not get\n> refunded either so B gains the difference in backwards upfront payments (which protects against\n> `controlled spam`)\n> * all the forward upfront payments have been paid (to protect against `uncontrolled spam`)\n>\n> * Scenario 4: the channel B <-> C closes:\n> * D settles before its `grace_period`, so its backwards upfront payment is refunded by C\n> * for whatever reason (malicious or not) the B <-> C channel closes\n> * this ensures that C's backwards upfront payment is paid to B\n> * if C publishes an HTLC-fulfill quickly, B may have his backwards upfront payment refunded by A\n> * if B is forced to wait for his HTLC-timeout, his backwards upfront payment will not be refunded\n> but it's ok because B got C's backwards upfront payment\n> * all the forward upfront payments have been paid (to protect against `uncontrolled spam`)\n>\n> If done naively, this mechanism may allow intermediate nodes to deanonymize sender/recipient.\n> If the base `grace_period` and `hold_fees` are randomized, I believe this attack vector disappears,\n> but it's worth exploring in more details.\n>\n> The most painful part of this proposal will be handling the `grace_period`:\n>\n> * when do you start counting: when you send/receive `update_add_htlc`, `commit_sig` or\n> `revoke_and_ack`?\n> * what happens if there is a disconnection (how do you account for the delay of reconnecting)?\n\nYou do not: a reliable peer would not have disconnected in the first place, in which case, it is the fault of the peer for getting disconnected and having a delay in reconnecting, possibly forfeiting the hold fee because of that.\n\n> * what happens if the remote settles after the `grace_period`, but refunds himself when sending his\n> `commit_sig` (making it look like from his point of view he settled before the `grace_period`)?\n> I think in that case the behavior should be to give your peers some leeway and let them get away\n> with it, but record it. If they're doing it too often, close channels and ban them; stealing\n> upfront fees should never be worth losing channels.\n\nRight, because there is the risk that communications latency delays a response that was *sent out* at the correct time but was *received in* after the grace period.\n\n>\n> I chose to make the backwards upfront payment fixed instead of scaling it based on the time an HTLC\n> is left pending; it's slightly less penalizing for spammers, but is less complex and introduces less\n> potential griefing against honest nodes. With the scaling approach, an honest node that has its\n> channel unilaterally closed is too heavily penalized IMHO (because it has to pay for the maximum\n> hold duration).\n>\n> I also chose to make the forward upfront payment constant (1 msat). Is it going to be a pain to\n> bikeshed this constant? Do we need to add a mechanism to upgrade it? We don't want to make this\n> more complex than it should.\n\nIs 1msat going to even deter anyone?\nI can make a thousand probes and just get charged a measly satoshi, which will not even buy a single piece of fish ball at a local street vendor (we call them \"fish ball\" as a courtesy, but in reality they are \"flour disks\" made with flour that was processed in equipment that was once in contact with fish at some unknown point in the past; still delicious when deep-fried and dipped in vinegar though), would that really deter anyone?\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Joost Jager",
                "date": "2020-10-23T05:58:11",
                "message_text_only": "Hi Bastien,\n\nWe add a forward upfront payment of 1 msat (fixed) that is paid\n> unconditionally when offering an HTLC.\n> We add a backwards upfront payment of `hold_fees` that is paid when\n> receiving an HTLC, but refunded\n> if the HTLC is settled before the `hold_grace_period` ends (see footnotes\n> about this).\n>\n\nIt is interesting that the forward and backward payments are relatively\nindependent of each other. In particular the forward anti-spam payment\ncould quite easily be implemented to help protect the network. As you said,\njust transfer that fixed fee for every `update_add_htlc` message from the\nofferer to the receiver.\n\nI am wondering though what the values for the fwd and bwd fees should be. I\nagree with ZmnSCPxj that 1 msat for the fwd is probably not going to be\nenough.\n\nMaybe a way to approach it is this: suppose routing nodes are able to make\n5% per year on their committed capital. An aggressive routing node could be\nwilling to spend up to that amount to take down a competitor.\n\nSuppose the network consists only of 1 BTC, 483 slot channels. What should\nthe fwd and bwd fees be so that even an attacked routing node will still\nearn that 5% (not through forwarding fees, but through hold fees) in both\nthe controlled and the uncontrolled spam scenario?\n\n- Joost\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201023/9b24ec04/attachment.html>"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2020-10-23T09:15:10",
                "message_text_only": "Thanks for your answers,\n\nMy first instinct is that additional complications are worse in general.\n> However, it looks like simpler solutions are truly not enough, so adding\n> the complication may very well be necessary.\n\n\nI agree with both these statements ;). I'd love to find a simpler solution,\nbut this is the simplest\nI've been able to come up with for now that seems to work without adding\ngriefing vectors...\n\nThe succeeding text refers to HTLCs \"settling\".\n\n\nAs you noted, settling means getting the HTLC removed from the commitment\ntransaction.\nIt includes both fulfills and fails, otherwise the proposal indeed doesn't\npenalize spam.\n\nIf we also require that the hold fee be funded from the main output, then\n> we cannot use single-funded channels, except perhaps with `push_msat`.\n\n\nI see what you mean, the first payment cannot require a hold fee since the\nfundee doesn't have a\nmain output. I think it's ok, it's the same thing as the reserve not being\nmet initially.\n\nBut you're right that there are potentially other mechanisms to enforce the\nfee (like your suggestion\nof subtracting from the HTLC output), I chose the simplest for now but we\ncan (and will) revisit\nthat choice if we think that the overall mechanisms work!\n\nAnd in this case C earns.\n\nCan C delay the refund to D to after the grace period even if D settled the\n> HTLC quickly?\n\n\nYes C earns, but D has misbehaved. As a final recipient, D isn't dependent\non anyone downstream.\nAn honest D should settle the HTLC before the `grace_period` ends. If D\nchooses to hold the HTLC\nfor a while, then it's fair that he pays C for this.\n\nit is the fault of the peer for getting disconnected and having a delay in\n> reconnecting, possibly forfeiting the hold fee because of that.\n\n\nI think I agree with that, but we'll need to think about the pros and cons\nwhen we get to details.\n\nIs 1msat going to even deter anyone?\n\nI am wondering though what the values for the fwd and bwd fees should be. I\n> agree with ZmnSCPxj that 1 msat for the fwd is probably not going to be\n> enough.\n\n\nThese values are only chosen for the simplicity of the example's sake. If\nwe agree the proposal works\nto fight spam, we will do some calculations to figure a good value for\nthis. But I think finding the\nright base values will not be the hard part, so we'll focus on this if\nwe're convinced the proposal\nis worth exploring in full details.\n\nIt is interesting that the forward and backward payments are relatively\n> independent of each other\n\n\nTo explain this further, I think it's important to highlight that the\nforward fee is meant to fight\n`uncontrolled spam` (where the recipient is an honest node) while the\nbackward fee is meant to fight\n`controlled spam` (where the recipient also belongs to the attacker).\n\nThe reason it works is because the `uncontrolled spam` requires the\nattacker to send a large volume\nof HTLCs, so a very small forward fee gets magnified. The backward fee will\nbe much bigger because\nin `controlled spam`, the attacker doesn't need a large volume of HTLCs but\nholds them for a long\ntime. What I think is nice is that this proposal has only a tiny cost for\nhonest senders (the\nforward fee).\n\nWhat I'd really like to explore is whether there is a type of spam that I\nmissed or griefing attacks\nthat appear because of the mechanisms I introduce. TBH I think the\nimplementation details (amounts,\ngrace periods and their deltas, when to start counting, etc) are things\nwe'll be able to figure out\ncollectively later.\n\nThanks again for your time!\nBastien\n\n\nLe ven. 23 oct. 2020 \u00e0 07:58, Joost Jager <joost.jager at gmail.com> a \u00e9crit :\n\n> Hi Bastien,\n>\n> We add a forward upfront payment of 1 msat (fixed) that is paid\n>> unconditionally when offering an HTLC.\n>> We add a backwards upfront payment of `hold_fees` that is paid when\n>> receiving an HTLC, but refunded\n>> if the HTLC is settled before the `hold_grace_period` ends (see footnotes\n>> about this).\n>>\n>\n> It is interesting that the forward and backward payments are relatively\n> independent of each other. In particular the forward anti-spam payment\n> could quite easily be implemented to help protect the network. As you said,\n> just transfer that fixed fee for every `update_add_htlc` message from the\n> offerer to the receiver.\n>\n> I am wondering though what the values for the fwd and bwd fees should be.\n> I agree with ZmnSCPxj that 1 msat for the fwd is probably not going to be\n> enough.\n>\n> Maybe a way to approach it is this: suppose routing nodes are able to make\n> 5% per year on their committed capital. An aggressive routing node could be\n> willing to spend up to that amount to take down a competitor.\n>\n> Suppose the network consists only of 1 BTC, 483 slot channels. What should\n> the fwd and bwd fees be so that even an attacked routing node will still\n> earn that 5% (not through forwarding fees, but through hold fees) in both\n> the controlled and the uncontrolled spam scenario?\n>\n> - Joost\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201023/7d252577/attachment-0001.html>"
            },
            {
                "author": "Joost Jager",
                "date": "2020-10-23T10:28:34",
                "message_text_only": ">\n> It is interesting that the forward and backward payments are relatively\n>> independent of each other\n>\n>\n> To explain this further, I think it's important to highlight that the\n> forward fee is meant to fight\n> `uncontrolled spam` (where the recipient is an honest node) while the\n> backward fee is meant to fight\n> `controlled spam` (where the recipient also belongs to the attacker).\n>\n\nYes, that was clear. I just meant to say that we could choose to first only\nimplement the easier uncontrolled spam protection via the forward payment.\nNot that any type of protocol upgrade is easy...\n\nWhat I'd really like to explore is whether there is a type of spam that I\n> missed or griefing attacks\n> that appear because of the mechanisms I introduce. TBH I think the\n> implementation details (amounts,\n> grace periods and their deltas, when to start counting, etc) are things\n> we'll be able to figure out\n> collectively later.\n>\n\nI brought up the question about the amounts because it could be that\namounts high enough to thwart attacks are too high for honest users or\ncertain uses. If that is the case, we don't need to look for other\npotential weaknesses. It is just a different order to explore the\nfeasibility of the proposal.\n\nThe forward payment can indeed be small, because uncontrolled spam can only\nbe in-flight for a short time. To get to that annual return of 5% on a 1\nBTC / 483 slot channel, it needs to be approx 1 sat/hour (if I calculated\nthat correctly). Let's say the spam payment is in-flight on average 30\nseconds on a 20 route hop (60 sec at the start, 0 sec at the end). The\ntotal \"damage\" would then be 600 hop-seconds, requiring a forward payment\nof 150 msat to cover that. Still seems acceptable to me. If an honest\nuser makes a payment and needs 10 attempts, they will pay an additional 1.5\nsats for that. Might be a ux-challenge to communicate that cost to a normal\nuser for a failed payment though.\n\nBut what happens if the attacker is also on the other end of the\nuncontrolled spam payment? Not holding the payment, but still collecting\nthe forward payments?\n\nFor the backward payment the pricing is different. The max expiry of the\nhtlc is 2000 blocks, 1000 blocks on average along the route. 1000 blocks is\nabout 160 hours. So ideally the attacker at the far end of the route should\npay 20 * 160 * 1sat/hr = 3200 sat. This will also be the cost for a hold\ninvoice then, but not everybody liked them anyway. The net cost for a\nregular (fast) payment will be nothing as you described.\n\n- Joost\n\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201023/ebe76cd3/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2020-10-23T10:50:12",
                "message_text_only": "Good morning t-bast,\n\n\n> > And in this case C earns.\n>\n> > Can C delay the refund to D to after the grace period even if D settled the HTLC quickly?\n>\n> Yes C earns, but D has misbehaved. As a final recipient, D isn't dependent on anyone downstream.\n> An honest D should settle the HTLC before the `grace_period` ends. If D chooses to hold the HTLC\n> for a while, then it's fair that he pays C for this.\n\n\nOkay, now let us consider the case where the supposedly-delaying party is not the final destination.\n\nSo, suppose D indicates to C that it should fail the HTLC.\nIn this case, C cannot immediately propagate the `update_fail_htlc` upstream, since the latest commitment transaction for the C<->D channel still contains the HTLC.\n\nIn addition, our state machine is hand-over-hand, i.e. there is a small window where there are two valid commitment transactions.\nWhat happens is we sign the next commitment transaction and *then* revoke the previous one.\n\nSo I think C can only safely propagate its own upstream `update_fail_htlc` once it receives the `revoke_and_ack` from D.\n\nSo the time measured for the grace period between C and D should be from C sending `update_add_htlc` to C receiving `revoke_and_ack` from D, in case the HTLC fails.\nThis is the time period that D is allowed to consume, and if it exceeds the grace period, it is penalized.\n\n(In this situation, it is immaterial if D is the destination: C cannot know this fact.)\n\nSo let us diagram this better:\n\n     C                       D\n     |----update_add_htlc--->| ---\n     |---commitment_signed-->|  ^\n     |<----revoke_and_ack----|  |\n     |<--commitment_signed---|  |\n     |-----revoke_and_ack--->|  |\n     |                       | grace period\n     |<--update_fail_htlc----|  |\n     |<--commitment_signed---|  |\n     |-----revoke_and_ack--->|  |\n     |---commitment_signed-->|  v  <--- grief point!\n     |<----revoke_and_ack----| ---\n\n(somebody *else* better make sure my understanding of the state machine is correct!)\n\nC can trivially grief D here, making it look like D is delaying, by delaying its own `commitment_signed` containing the *removal* of the HTLC.\nD cannot send its own `revoke_and_ack` until it receives the signature for its own next commitment, as if it did so, it would lose the ability to close the channel unilaterally; it has to wait for C to send the `commitment_signed`.\n\nThus, it seems to me that C can grief D here.\n\nThe question is: does the above C-can-grief-D matter?\n\nI think D does have a defense against C griefing in the above case:\n\n* If the time between D->`update_fail_htlc`->C and the corresponding C->`commitment_signed`->D becomes too long:\n  * D drops the channel onchain.\n    * The dropped commitment tx still contains the HTLC, since it is the \"previous\" commitment that D happens to hold that has not yet had the `update_fail_htlc` committed.\n\nIf D performs the above, then C is forced to wait *even longer* (it has to wait out the HTLC timelock) before it can safely propagate the `update_fail_htlc`: D could be fooling with it and actually knows the preimage and claim it onchain, so C for its own safety *must* wait out the onchain timelock.\n\nDoes that make sense?\nDoes it sensibly protect against this griefing?\nIs it too much of a punishment and could potentially hurt D more than it hurts C if C is a heavily-connected node that will not miss the channel while D has fewer channels and opened the C<->D channel in the first place?\n\n--\n\nFor success case `update_fulfill_htlc`, I believe C can immediately propagate this back to its upstream since it can now.\nThus, in that case, we can stop the timer at the `update_fulfill_htlc`.\n\nSo at least for the *end point* of the grace period, I think the end point should be:\n\n* If the HTLC failed:\n  * When both participants have sent `revoke_and_ack`.\n* If the HTLC succeeded:\n  * When the downstream participant has sent `update_fulfill_htlc`.\n\nFor the *start point*, it seems the C->`commitment_signed`->D containing the HTLC would work as the start point.\nIn particular, it seems to me that C can also deliberately defer its own C->`revoke_and_ack`->D:\n\n     C                       D\n     |----update_add_htlc--->|\n     |---commitment_signed-->| ---\n     |<----revoke_and_ack----|  ^\n     |<--commitment_signed---|  |\n     |-----revoke_and_ack--->|  |  <--- grief point!\n     |                       | grace period\n     |<--update_fail_htlc----|  |\n     |<--commitment_signed---|  |\n     |-----revoke_and_ack--->|  |\n     |---commitment_signed-->|  v\n     |<----revoke_and_ack----| ---\n\n(If D deliberately delays, then it is penalized, so we should consider the case where C attempts to trigger the reverse case).\nD cannot safely fulfill the HTLC until after the previous commitment transactions of *both* sides have been revoked (\"irrevocably committed\" state).\nSo D can use the same defense, I think: if C is taking too long to send the `revoke_and_ack` pointed at above, it drops the channel onchain with the HTLC instantiated (which is why the *start time* has to be the C->`commitment_signed`->D that contains the new HTLC).\n\nThus the D grace period has two smaller grace periods that D imposes on C, using the threat of channel drop to protect against the C-side griefing.\n\n\n\nSorry for dropping into details already but so far this is the only griefing attack I can think of right now.\n\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2020-10-23T13:06:45",
                "message_text_only": "Hey Joost and Z,\n\nI brought up the question about the amounts because it could be that\n> amounts high enough to thwart attacks are too high for honest users or\n> certain uses.\n\n\nI don't think this is a concern for this proposal, unless there's an attack\nvector I missed.\nThe reason I claim that is that the backwards upfront payment can be made\nsomewhat big without any\nnegative impact on honest nodes. If you're an honest intermediate node,\nonly two cases are possible:\n\n* your downstream peer settled the HTLC quickly (before the grace period\nends): in that case you\nrefund him his upfront fee, and you have time to settle the HTLC upstream\nwhile still honoring\nthe grace period, so it will be refunded to you as well (unless you delay\nthe settlement upstream\nfor whatever reason, in which case you deserve to pay the hold_fee)\n* your grace period has expired, so you can't get a refund upstream: if\nthat happens, the grace\nperiod with your downstream node has also expired, so you're earning money\ndownstream and paying\nmoney upstream, and you'll usually even take a small positive spread so\neverything's good\n\nThe only node that can end up loosing money on the backwards upfront\npayment is the last node in\nthe route. But that node should always settle the HTLC quickly (or decide\nto hodl it, but in that\ncase it's normal that it pays the hold_fee).\n\nBut what happens if the attacker is also on the other end of the\n> uncontrolled spam payment? Not holding the payment, but still collecting\n> the forward payments?\n\n\nThat's what I call short-lived `controlled spam`. In that case the attacker\npays the forward fee at\nthe beginning of the route but has it refunded at the end of the route. If\nthe attacker doesn't\nwant to lose any money, he has to release the HTLC before the grace period\nends (which is going to\nbe short-lived - at least compared to block times). This gives an\nopportunity for legitimate payments\nto use the HTLC slots (but it's a race between the attacker and the\nlegitimate users).\n\nIt's not ideal, because the attacker isn't penalized...the only way I think\nwe can penalize this\nkind of attack is if the forward fee decrements at each hop, but in that\ncase it needs to be in the\nonion (to avoid probing) and the delta needs to be high enough to actually\npenalize the attacker.\nTime to bikeshed some numbers!\n\nC can trivially grief D here, making it look like D is delaying, by\n> delaying its own `commitment_signed` containing the *removal* of the HTLC.\n\n\nYou're right to dive into these, there may be something here.\nBut I think your example doesn't work, let me know if I'm mistaken.\nD is the one who decides whether he'll be refunded or not, because D is the\nfirst to send the\n`commit_sig` that removes the HTLC. I think we would extend `commit_sig`\nwith a tlv field that\nindicates \"I refunded myself for HTLC N\" to help C compute the same commit\ntx and verify sigs.\n\nI agree with you that the details of how we'll implement the grace period\nmay have griefing attacks\ndepending on how we do it, it's worth exploring further.\n\nCheers,\nBastien\n\nLe ven. 23 oct. 2020 \u00e0 12:50, ZmnSCPxj <ZmnSCPxj at protonmail.com> a \u00e9crit :\n\n> Good morning t-bast,\n>\n>\n> > > And in this case C earns.\n> >\n> > > Can C delay the refund to D to after the grace period even if D\n> settled the HTLC quickly?\n> >\n> > Yes C earns, but D has misbehaved. As a final recipient, D isn't\n> dependent on anyone downstream.\n> > An honest D should settle the HTLC before the `grace_period` ends. If D\n> chooses to hold the HTLC\n> > for a while, then it's fair that he pays C for this.\n>\n>\n> Okay, now let us consider the case where the supposedly-delaying party is\n> not the final destination.\n>\n> So, suppose D indicates to C that it should fail the HTLC.\n> In this case, C cannot immediately propagate the `update_fail_htlc`\n> upstream, since the latest commitment transaction for the C<->D channel\n> still contains the HTLC.\n>\n> In addition, our state machine is hand-over-hand, i.e. there is a small\n> window where there are two valid commitment transactions.\n> What happens is we sign the next commitment transaction and *then* revoke\n> the previous one.\n>\n> So I think C can only safely propagate its own upstream `update_fail_htlc`\n> once it receives the `revoke_and_ack` from D.\n>\n> So the time measured for the grace period between C and D should be from C\n> sending `update_add_htlc` to C receiving `revoke_and_ack` from D, in case\n> the HTLC fails.\n> This is the time period that D is allowed to consume, and if it exceeds\n> the grace period, it is penalized.\n>\n> (In this situation, it is immaterial if D is the destination: C cannot\n> know this fact.)\n>\n> So let us diagram this better:\n>\n>      C                       D\n>      |----update_add_htlc--->| ---\n>      |---commitment_signed-->|  ^\n>      |<----revoke_and_ack----|  |\n>      |<--commitment_signed---|  |\n>      |-----revoke_and_ack--->|  |\n>      |                       | grace period\n>      |<--update_fail_htlc----|  |\n>      |<--commitment_signed---|  |\n>      |-----revoke_and_ack--->|  |\n>      |---commitment_signed-->|  v  <--- grief point!\n>      |<----revoke_and_ack----| ---\n>\n> (somebody *else* better make sure my understanding of the state machine is\n> correct!)\n>\n> C can trivially grief D here, making it look like D is delaying, by\n> delaying its own `commitment_signed` containing the *removal* of the HTLC.\n> D cannot send its own `revoke_and_ack` until it receives the signature for\n> its own next commitment, as if it did so, it would lose the ability to\n> close the channel unilaterally; it has to wait for C to send the\n> `commitment_signed`.\n>\n> Thus, it seems to me that C can grief D here.\n>\n> The question is: does the above C-can-grief-D matter?\n>\n> I think D does have a defense against C griefing in the above case:\n>\n> * If the time between D->`update_fail_htlc`->C and the corresponding\n> C->`commitment_signed`->D becomes too long:\n>   * D drops the channel onchain.\n>     * The dropped commitment tx still contains the HTLC, since it is the\n> \"previous\" commitment that D happens to hold that has not yet had the\n> `update_fail_htlc` committed.\n>\n> If D performs the above, then C is forced to wait *even longer* (it has to\n> wait out the HTLC timelock) before it can safely propagate the\n> `update_fail_htlc`: D could be fooling with it and actually knows the\n> preimage and claim it onchain, so C for its own safety *must* wait out the\n> onchain timelock.\n>\n> Does that make sense?\n> Does it sensibly protect against this griefing?\n> Is it too much of a punishment and could potentially hurt D more than it\n> hurts C if C is a heavily-connected node that will not miss the channel\n> while D has fewer channels and opened the C<->D channel in the first place?\n>\n> --\n>\n> For success case `update_fulfill_htlc`, I believe C can immediately\n> propagate this back to its upstream since it can now.\n> Thus, in that case, we can stop the timer at the `update_fulfill_htlc`.\n>\n> So at least for the *end point* of the grace period, I think the end point\n> should be:\n>\n> * If the HTLC failed:\n>   * When both participants have sent `revoke_and_ack`.\n> * If the HTLC succeeded:\n>   * When the downstream participant has sent `update_fulfill_htlc`.\n>\n> For the *start point*, it seems the C->`commitment_signed`->D containing\n> the HTLC would work as the start point.\n> In particular, it seems to me that C can also deliberately defer its own\n> C->`revoke_and_ack`->D:\n>\n>      C                       D\n>      |----update_add_htlc--->|\n>      |---commitment_signed-->| ---\n>      |<----revoke_and_ack----|  ^\n>      |<--commitment_signed---|  |\n>      |-----revoke_and_ack--->|  |  <--- grief point!\n>      |                       | grace period\n>      |<--update_fail_htlc----|  |\n>      |<--commitment_signed---|  |\n>      |-----revoke_and_ack--->|  |\n>      |---commitment_signed-->|  v\n>      |<----revoke_and_ack----| ---\n>\n> (If D deliberately delays, then it is penalized, so we should consider the\n> case where C attempts to trigger the reverse case).\n> D cannot safely fulfill the HTLC until after the previous commitment\n> transactions of *both* sides have been revoked (\"irrevocably committed\"\n> state).\n> So D can use the same defense, I think: if C is taking too long to send\n> the `revoke_and_ack` pointed at above, it drops the channel onchain with\n> the HTLC instantiated (which is why the *start time* has to be the\n> C->`commitment_signed`->D that contains the new HTLC).\n>\n> Thus the D grace period has two smaller grace periods that D imposes on C,\n> using the threat of channel drop to protect against the C-side griefing.\n>\n>\n>\n> Sorry for dropping into details already but so far this is the only\n> griefing attack I can think of right now.\n>\n>\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201023/87e456f4/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2020-10-23T15:26:57",
                "message_text_only": "Good morning Bastien,\n\n> > C can trivially grief D here, making it look like D is delaying, by delaying its own `commitment_signed` containing the *removal* of the HTLC.\n>\n> You're right to dive into these, there may be something here.\n> But I think your example doesn't work, let me know if I'm mistaken.\n> D is the one who decides whether he'll be refunded or not, because D is the first to send the\n> `commit_sig` that removes the HTLC. I think we would extend `commit_sig` with a tlv field that\n> indicates \"I refunded myself for HTLC N\" to help C compute the same commit tx and verify sigs.\n\nD sending `commitment_signed` simply means C has the option to use either the previous commitment or the new one.\nC can still drop the previous commitment, which has the hold fee still owned by C.\n\nC only loses that option by sending `revoke_and_ack`, so C can still unfairly delay this, and at this point D is holding the previous commitment (which, as mentioned, has the hold fee still owned by C).\nSo C can still delay by not revoking its previous commitment (`revoke_and_ack`) and not signing the D-side next commitment (`commitment_signed`).\n\nOn the *other* hand if C can only *take* the hold fee at this point by dropping onchain, then the onchain fees and the loss of a viable channel (meaning the funds of C in that channel need to be put back into a new channel, again onchain fees) might very well dominate.\nIs this enough of a deterrent?\n\nOn the other *other* hand, rules which involve \"SHOULD/MUST fail the channel\" have classically caused headaches in interop, xref. the mass channel closes between C-Lightning and lnd nodes some years ago due to sudden onchain fee movements.\n\n-------------\n\nOn a mildly related note I have this old crap I wrote earlier this year, it might be possible to glean something from it:\n\n* https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-April/002608.html\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Joost Jager",
                "date": "2020-10-24T08:58:32",
                "message_text_only": "Hi Bastien,\n\nI brought up the question about the amounts because it could be that\n>> amounts high enough to thwart attacks are too high for honest users or\n>> certain uses.\n>\n>\n> I don't think this is a concern for this proposal, unless there's an\n> attack vector I missed.\n> The reason I claim that is that the backwards upfront payment can be made\n> somewhat big without any\n> negative impact on honest nodes.\n>\n\nYes, that makes sense.\n\n\n> But what happens if the attacker is also on the other end of the\n>> uncontrolled spam payment? Not holding the payment, but still collecting\n>> the forward payments?\n>\n>\n> That's what I call short-lived `controlled spam`. In that case the\n> attacker pays the forward fee at\n> the beginning of the route but has it refunded at the end of the route. If\n> the attacker doesn't\n> want to lose any money, he has to release the HTLC before the grace period\n> ends (which is going to\n> be short-lived - at least compared to block times). This gives an\n> opportunity for legitimate payments\n> to use the HTLC slots (but it's a race between the attacker and the\n> legitimate users).\n>\n\nI think indeed that this short-lived controlled spam also needs to be\nbrought under control. Otherwise it is still easy to jam a channel,\nalthough it would need a continuous process to do it rather than sending a\nbunch of 2000-block expiry htlcs. For the short-lived controlled spam there\nis still a multiplier possible by making loops in the route. It is a race\nwith legitimate users, but if the spammer is efficient the probability of a\nlegitimate payment coming through is low. Similar to DDoS attacks where a\nlegitimate web request could make it to the server but probably doesn't.\n\n\n> It's not ideal, because the attacker isn't penalized...the only way I\n> think we can penalize this\n> kind of attack is if the forward fee decrements at each hop, but in that\n> case it needs to be in the\n> onion (to avoid probing) and the delta needs to be high enough to actually\n> penalize the attacker.\n> Time to bikeshed some numbers!\n>\n\nSo in your proposal, an htlc that is received by a routing node has the\nfollowing properties:\n* htlc amount\n* forward up-front payment (anti-spam)\n* backward up-front payment (anti-hold)\n* grace period\n\nThe routing node forwards this to the next hop with\n* lower htlc amount (to earn routing fees when the htlc settles)\n* lower forward up-front payment (to make sure that an attacker at the\nother end loses money when failing quickly)\n* higher backward up-front payment (to make sure that an attacker at the\nother end loses money when holding)\n* shorter grace period (so that there is time to fail back and not lose the\nbackward up-front payment)\n\nOn a high level, it seems to me that this can actually work.\n\n- Joost\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201024/2d303022/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2020-10-28T01:13:29",
                "message_text_only": "Good morning Bastien, Joost, and all,\n\nAn issue with the bidirectional upfront/hold fees is related to trustless offchain-to-onchain swaps, like Boltz and Lightning Loop.\n\nAs the claiming of the offchain side is dependent on claiming of the onchain side of the trustless swap mechanism, which is *definitely* slow, the swap service will in general be forced to pay up the hold fees.\n\nIt seems to me that the hold-fees mechanism cannot be ported over in the onchain side, so even if you set a \"reasonable\" grace period at the swap service of say 1 hour (and assuming forwarding nodes are OK with that humongous grace period!), the onchain side of the swap can delay the release of onchain.\n\nTo mitigate against this, the swap service would need to issue a separate invoice to pay for the hold fee for the \"real\" swap payment.\nThe Boltz protocol supports a separate mining-fee invoice (disabled on the Boltz production servers) that is issued after the invoice is \"locked in\" at the swap service, but I think that in view of the use of hold fee, a combined mining-fee+hold-fee invoice would have to be issued at the same time as the \"real\" swap invoice.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Hold fees: 402 Payment Required for Lightning itself",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Antoine Riard",
                "Bastien TEINTURIER",
                "Joost Jager",
                "Rusty Russell",
                "ZmnSCPxj",
                "Christian Decker"
            ],
            "messages_count": 27,
            "total_messages_chars_count": 111476
        }
    },
    {
        "title": "[Lightning-dev] [RFC] Simplified (but less optimal) HTLC Negotiation",
        "thread_messages": [
            {
                "author": "Rusty Russell",
                "date": "2020-10-12T23:37:07",
                "message_text_only": "Hi all,\n\n        Our HTLC state machine is optimal, but complex[1]; the Lightning\nLabs team recently did some excellent work finding another place the spec\nis insufficient[2].  Also, the suggestion for more dynamic changes makes it\nmore difficult, usually requiring forced quiescence.\n\nThe following protocol returns to my earlier thoughts, with cost of\nlatency in some cases.\n\n1. The protocol is half-duplex, with each side taking turns; opener first.\n2. It's still the same form, but it's always one-direction so both sides\n   stay in sync.\n        update+-> commitsig-> <-revocation <-commitsig revocation->\n3. A new message pair \"turn_request\" and \"turn_reply\" let you request\n   when it's not your turn.\n4. If you get an update in reply to your turn_request, you lost the race\n   and have to defer your own updates until after peer is finished.\n5. On reconnect, you send two flags: send-in-progress (if you have\n   sent the initial commitsig but not the final revocation) and\n   receive-in-progress (if you have received the initial commitsig\n   not not received the final revocation).  If either is set,\n   the sender (as indicated by the flags) retransmits the entire\n   sequence.\n   Otherwise, (arbitrarily) opener goes first again.\n\nPros:\n1. Way simpler.  There is only ever one pair of commitment txs for any\n   given commitment index.\n2. Fee changes are now deterministic.  No worrying about the case where\n   the peer's changes are also in flight.\n3. Dynamic changes can probably happen more simply, since we always\n   negotiate both sides at once.\n\nCons:\n1. If it's not your turn, it adds 1 RTT latency.\n\nUnchanged:\n1. Database accesses are unchanged; you need to commit when you send or\n   receive a commitsig.\n2. You can use the same state machine as before, but one day (when\n   this would be compulsory) you'll be able signficantly simplify;\n   you'll need to record the index at which HTLCs were changed\n   (added/removed) in case peer wants you to rexmit though.\n\nCheers,\nRusty.\n\n[1] This is my fault; I was persuaded early on that optimality was more\n    important than simplicity in a classic nerd-snipe.\n[2] https://github.com/lightningnetwork/lightning-rfc/issues/794"
            },
            {
                "author": "Christian Decker",
                "date": "2020-10-13T11:58:49",
                "message_text_only": "I wonder if we should just go the tried-and-tested leader-based\nmechanism:\n\n 1. The node with the lexicographically lower node_id is determined to\n    be the leader.\n 2. The leader receives proposals for changes from itself and the peer\n    and orders them into a logical sequence of changes\n 3. The leader applies the changes locally and streams them to the peer.\n 4. Either node can initiate a commitment by proposing a `flush` change.\n 5. Upon receiving a `flush` the nodes compute the commitment\n    transaction and exchange signatures.\n\nThis is similar to your proposal, but does away with turn changes (it's\nalways the leader's turn), and therefore reduces the state we need to\nkeep track of (and re-negotiate on reconnect).\n\nThe downside is that we add a constant overhead to one side's\noperations, but since we pipeline changes, and are mostly synchronous\nduring the signing of the commitment tx today anyway, this comes out to\n1 RTT for each commitment.\n\nOn the other hand a token-passing approach (which I think is what you\npropose) require a synchronous token handover whenever a the direction\nof the updates changes. This is assuming I didn't misunderstand the turn\nmechanics of your proposal :-)\n\nCheers,\nChristian\n\nRusty Russell <rusty at rustcorp.com.au> writes:\n> Hi all,\n>\n>         Our HTLC state machine is optimal, but complex[1]; the Lightning\n> Labs team recently did some excellent work finding another place the spec\n> is insufficient[2].  Also, the suggestion for more dynamic changes makes it\n> more difficult, usually requiring forced quiescence.\n>\n> The following protocol returns to my earlier thoughts, with cost of\n> latency in some cases.\n>\n> 1. The protocol is half-duplex, with each side taking turns; opener first.\n> 2. It's still the same form, but it's always one-direction so both sides\n>    stay in sync.\n>         update+-> commitsig-> <-revocation <-commitsig revocation->\n> 3. A new message pair \"turn_request\" and \"turn_reply\" let you request\n>    when it's not your turn.\n> 4. If you get an update in reply to your turn_request, you lost the race\n>    and have to defer your own updates until after peer is finished.\n> 5. On reconnect, you send two flags: send-in-progress (if you have\n>    sent the initial commitsig but not the final revocation) and\n>    receive-in-progress (if you have received the initial commitsig\n>    not not received the final revocation).  If either is set,\n>    the sender (as indicated by the flags) retransmits the entire\n>    sequence.\n>    Otherwise, (arbitrarily) opener goes first again.\n>\n> Pros:\n> 1. Way simpler.  There is only ever one pair of commitment txs for any\n>    given commitment index.\n> 2. Fee changes are now deterministic.  No worrying about the case where\n>    the peer's changes are also in flight.\n> 3. Dynamic changes can probably happen more simply, since we always\n>    negotiate both sides at once.\n>\n> Cons:\n> 1. If it's not your turn, it adds 1 RTT latency.\n>\n> Unchanged:\n> 1. Database accesses are unchanged; you need to commit when you send or\n>    receive a commitsig.\n> 2. You can use the same state machine as before, but one day (when\n>    this would be compulsory) you'll be able signficantly simplify;\n>    you'll need to record the index at which HTLCs were changed\n>    (added/removed) in case peer wants you to rexmit though.\n>\n> Cheers,\n> Rusty.\n>\n> [1] This is my fault; I was persuaded early on that optimality was more\n>     important than simplicity in a classic nerd-snipe.\n> [2] https://github.com/lightningnetwork/lightning-rfc/issues/794\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2020-10-14T09:22:51",
                "message_text_only": "To be honest the current protocol can be hard to grasp at first (mostly\nbecause it's hard to reason\nabout two commit txs being constantly out of sync), but from an\nimplementation's point of view I'm\nnot sure your proposals are simpler.\n\nOne of the benefits of the current HTLC state machine is that once you\ndescribe your state as a set\nof local changes (proposed by you) plus a set of remote changes (proposed\nby them), where each of\nthese is split between proposed, signed and acked updates, the flow is\nstraightforward to implement\nand deterministic.\n\nThe only tricky part (where we've seen recurring compatibility issues) is\nwhat happens on\nreconnections. But it seems to me that the only missing requirement in the\nspec is on the order of\nmessages sent, and more specifically that if you are supposed to send a\n`revoke_and_ack`, you must\nsend that first (or at least before sending any `commit_sig`). Adding test\nscenarios in the spec\ncould help implementers get this right.\n\nIt's a bit tricky to get it right at first, but once you get it right you\ndon't need to touch that\ncode again and everything runs smoothly. We're pretty close to that state,\nso why would we want to\nstart from scratch? Or am I missing something?\n\nCheers,\nBastien\n\nLe mar. 13 oct. 2020 \u00e0 13:58, Christian Decker <decker.christian at gmail.com>\na \u00e9crit :\n\n> I wonder if we should just go the tried-and-tested leader-based\n> mechanism:\n>\n>  1. The node with the lexicographically lower node_id is determined to\n>     be the leader.\n>  2. The leader receives proposals for changes from itself and the peer\n>     and orders them into a logical sequence of changes\n>  3. The leader applies the changes locally and streams them to the peer.\n>  4. Either node can initiate a commitment by proposing a `flush` change.\n>  5. Upon receiving a `flush` the nodes compute the commitment\n>     transaction and exchange signatures.\n>\n> This is similar to your proposal, but does away with turn changes (it's\n> always the leader's turn), and therefore reduces the state we need to\n> keep track of (and re-negotiate on reconnect).\n>\n> The downside is that we add a constant overhead to one side's\n> operations, but since we pipeline changes, and are mostly synchronous\n> during the signing of the commitment tx today anyway, this comes out to\n> 1 RTT for each commitment.\n>\n> On the other hand a token-passing approach (which I think is what you\n> propose) require a synchronous token handover whenever a the direction\n> of the updates changes. This is assuming I didn't misunderstand the turn\n> mechanics of your proposal :-)\n>\n> Cheers,\n> Christian\n>\n> Rusty Russell <rusty at rustcorp.com.au> writes:\n> > Hi all,\n> >\n> >         Our HTLC state machine is optimal, but complex[1]; the Lightning\n> > Labs team recently did some excellent work finding another place the spec\n> > is insufficient[2].  Also, the suggestion for more dynamic changes makes\n> it\n> > more difficult, usually requiring forced quiescence.\n> >\n> > The following protocol returns to my earlier thoughts, with cost of\n> > latency in some cases.\n> >\n> > 1. The protocol is half-duplex, with each side taking turns; opener\n> first.\n> > 2. It's still the same form, but it's always one-direction so both sides\n> >    stay in sync.\n> >         update+-> commitsig-> <-revocation <-commitsig revocation->\n> > 3. A new message pair \"turn_request\" and \"turn_reply\" let you request\n> >    when it's not your turn.\n> > 4. If you get an update in reply to your turn_request, you lost the race\n> >    and have to defer your own updates until after peer is finished.\n> > 5. On reconnect, you send two flags: send-in-progress (if you have\n> >    sent the initial commitsig but not the final revocation) and\n> >    receive-in-progress (if you have received the initial commitsig\n> >    not not received the final revocation).  If either is set,\n> >    the sender (as indicated by the flags) retransmits the entire\n> >    sequence.\n> >    Otherwise, (arbitrarily) opener goes first again.\n> >\n> > Pros:\n> > 1. Way simpler.  There is only ever one pair of commitment txs for any\n> >    given commitment index.\n> > 2. Fee changes are now deterministic.  No worrying about the case where\n> >    the peer's changes are also in flight.\n> > 3. Dynamic changes can probably happen more simply, since we always\n> >    negotiate both sides at once.\n> >\n> > Cons:\n> > 1. If it's not your turn, it adds 1 RTT latency.\n> >\n> > Unchanged:\n> > 1. Database accesses are unchanged; you need to commit when you send or\n> >    receive a commitsig.\n> > 2. You can use the same state machine as before, but one day (when\n> >    this would be compulsory) you'll be able signficantly simplify;\n> >    you'll need to record the index at which HTLCs were changed\n> >    (added/removed) in case peer wants you to rexmit though.\n> >\n> > Cheers,\n> > Rusty.\n> >\n> > [1] This is my fault; I was persuaded early on that optimality was more\n> >     important than simplicity in a classic nerd-snipe.\n> > [2] https://github.com/lightningnetwork/lightning-rfc/issues/794\n> > _______________________________________________\n> > Lightning-dev mailing list\n> > Lightning-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201014/dcec5288/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2020-10-15T01:02:13",
                "message_text_only": "Bastien TEINTURIER <bastien at acinq.fr> writes:\n> It's a bit tricky to get it right at first, but once you get it right you\n> don't need to touch that\n> code again and everything runs smoothly. We're pretty close to that state,\n> so why would we want to\n> start from scratch? Or am I missing something?\n\nWell, if you've implemented a state-based approach then this is simply a\nsubset of that so it's simple to implement (I believe, I haven't done it\nyet!).\n\nBut with a synchronous approach like this, we can do dynamic protocol\nupdates at any time without having a special \"stop and drain\" step.\n\nFor example, you can decrease the amount of HTLCs you accept, without\nworrying about the case where there HTLCs being added right now.  This\nsolves a similar outstanding problem with update_fee.\n\nCheers,\nRusty."
            },
            {
                "author": "Rusty Russell",
                "date": "2020-10-15T01:00:12",
                "message_text_only": "Christian Decker <decker.christian at gmail.com> writes:\n> I wonder if we should just go the tried-and-tested leader-based\n> mechanism:\n>\n>  1. The node with the lexicographically lower node_id is determined to\n>     be the leader.\n>  2. The leader receives proposals for changes from itself and the peer\n>     and orders them into a logical sequence of changes\n>  3. The leader applies the changes locally and streams them to the peer.\n>  4. Either node can initiate a commitment by proposing a `flush` change.\n>  5. Upon receiving a `flush` the nodes compute the commitment\n>     transaction and exchange signatures.\n>\n> This is similar to your proposal, but does away with turn changes (it's\n> always the leader's turn), and therefore reduces the state we need to\n> keep track of (and re-negotiate on reconnect).\n\nBut now you need to be able to propose two kinds of things, which is\nactually harder to implement; update-from-you and update-from-me.  This\nis a deeper protocol change.\n\nAnd you don't get the benefit of the turn-taking approach, which is that\nyou can have a known state for fee changes.  Even if you change it to\nhave opener always the leader, it still has to handle the case where\nincoming changes are not allowed under the new fee regime (and similar\nissues for other dynamic updates).\n\n> The downside is that we add a constant overhead to one side's\n> operations, but since we pipeline changes, and are mostly synchronous\n> during the signing of the commitment tx today anyway, this comes out to\n> 1 RTT for each commitment.\n\nYeah, it adds 1RTT to every hop on the network, vs my proposal which\nadds just over 1/2 RTT on average.\n\n> On the other hand a token-passing approach (which I think is what you\n> propose) require a synchronous token handover whenever a the direction\n> of the updates changes. This is assuming I didn't misunderstand the turn\n> mechanics of your proposal :-)\n\nYes, but it alternates because that's optimal for a non-busy channel\n(since it's usually \"Alice adds htlc, Bob completes the htlc\").\n\nCheers,\nRusty."
            },
            {
                "author": "Christian Decker",
                "date": "2020-10-15T09:47:59",
                "message_text_only": "> And you don't get the benefit of the turn-taking approach, which is that\n> you can have a known state for fee changes.  Even if you change it to\n> have opener always the leader, it still has to handle the case where\n> incoming changes are not allowed under the new fee regime (and similar\n> issues for other dynamic updates).\n\nGood point, I hadn't considered that a change from one side might become\ninvalid due to a change from the other side. I think however this can only\naffect changes that result in other changes no longer being applicable,\ne.g., changing the number of HTLCs you'll allow on a channel making the\nHTLC we just added and whose update_add is still in flight invalid.\n\nI don't think fee changes are impacted here, since the non-leader only\napplies the change to its commitment once it gets back its own change.\nThe leader will have inserted your update_add into its stream after the\nfee update, and so you'll first apply the fee update, and then use the\ncorrect fee to add the HTLC to your commitment, resulting in the same\nstate.\n\nThe remaining edgecases where changes can become invalid if they are in\nflight, can be addressed by bouncing the change through the non-leader,\ntelling him that \"hey, I'd like to propose this change, if you're good\nwith it send it back to me and I'll add it to my stream\". This can be\nseen as draining the queue of in-flight changes, however the non-leader\nmay pipeline its own changes after it and take the updated parameters\ninto consideration. Think of it as a two-phase commit, alerting the peer\nwith a proposal, before committing it by adding it to the stream. It\nadds latency (about 1/2RTT over the token-passing approach since we can\nemulate it with the token-passing approach) but these synchronization\npoints are rare and not on the critical path when forwarding payments.\n\n>> The downside is that we add a constant overhead to one side's\n>> operations, but since we pipeline changes, and are mostly synchronous\n>> during the signing of the commitment tx today anyway, this comes out to\n>> 1 RTT for each commitment.\n>\n> Yeah, it adds 1RTT to every hop on the network, vs my proposal which\n> adds just over 1/2 RTT on average.\n\nDoesn't that assume a change of turns while the HTLC was in-flight?\nAdding and resolving an HTLC requires one change coming from either side\nof the channel, implying that a turn change must have been performed,\nwhich itself takes 1 RTT. Thus to add an remove an HTLC we add at least\n1RTT for each hop.\n\nWith the leader-based approach, we add 1RTT latency to the updates from\none side, but the other never has to wait for the token, resulting in\n1/2RTT per direction as well, since messages are well-balanced.\n\n> Yes, but it alternates because that's optimal for a non-busy channel\n> (since it's usually \"Alice adds htlc, Bob completes the htlc\").\n\nWhat's bothering me more about the turn-based approach is that while the\ntoken is in flight, neither endpoint can make any progress, since the\none reliquishing the token promised not to say anything and the other\none hasn't gotten the token yet. This might result in rather a lot of\ndead-air if both sides have a constant stream of changes to add. So we'd\nlikely have to add a timeout to defer giving up the token, to counter\ndead-air, further adding delay to the changes from the other end, and\nadding yet another parameter.\n\nThis is in stark contrast to the leader-based approach, where both\nparties can just keep queuing updates without silent times to\ntransferring the token from one end to the other.\n\nCheers,\nChristian"
            },
            {
                "author": "Rusty Russell",
                "date": "2020-10-20T23:40:48",
                "message_text_only": "Christian Decker <decker.christian at gmail.com> writes:\n>> And you don't get the benefit of the turn-taking approach, which is that\n>> you can have a known state for fee changes.  Even if you change it to\n>> have opener always the leader, it still has to handle the case where\n>> incoming changes are not allowed under the new fee regime (and similar\n>> issues for other dynamic updates).\n>\n> Good point, I hadn't considered that a change from one side might become\n> invalid due to a change from the other side. I think however this can only\n> affect changes that result in other changes no longer being applicable,\n> e.g., changing the number of HTLCs you'll allow on a channel making the\n> HTLC we just added and whose update_add is still in flight invalid.\n\nTo make dynamic changes in the current system, you need to make them the\nsame way we make feechanges: first remote, then local (once they ack).\n\nThis means you have to handle the cases where this causes the the commit\ntx to not meet the new restrictions.  It's all possible, it's just\nmessy.\n\n> I don't think fee changes are impacted here, since the non-leader only\n> applies the change to its commitment once it gets back its own change.\n> The leader will have inserted your update_add into its stream after the\n> fee update, and so you'll first apply the fee update, and then use the\n> correct fee to add the HTLC to your commitment, resulting in the same\n> state.\n\nSure, but we still have the (existing) problem where you propose a fee\nchange you can no longer afford, because the other side is also adding\nthings.\n\nThey can just refuse to reflect the fee in that case, though.\n\n> The remaining edgecases where changes can become invalid if they are in\n> flight, can be addressed by bouncing the change through the non-leader,\n> telling him that \"hey, I'd like to propose this change, if you're good\n> with it send it back to me and I'll add it to my stream\". This can be\n> seen as draining the queue of in-flight changes, however the non-leader\n> may pipeline its own changes after it and take the updated parameters\n> into consideration. Think of it as a two-phase commit, alerting the peer\n> with a proposal, before committing it by adding it to the stream. It\n> adds latency (about 1/2RTT over the token-passing approach since we can\n> emulate it with the token-passing approach) but these synchronization\n> points are rare and not on the critical path when forwarding payments.\n\nYou can create a protocol to reject changes, but now we're more complex\nthan the simply-alternate-leader approach.\n\n> With the leader-based approach, we add 1RTT latency to the updates from\n> one side, but the other never has to wait for the token, resulting in\n> 1/2RTT per direction as well, since messages are well-balanced.\n\nGood point.\n\n>> Yes, but it alternates because that's optimal for a non-busy channel\n>> (since it's usually \"Alice adds htlc, Bob completes the htlc\").\n>\n> What's bothering me more about the turn-based approach is that while the\n> token is in flight, neither endpoint can make any progress, since the\n> one reliquishing the token promised not to say anything and the other\n> one hasn't gotten the token yet. This might result in rather a lot of\n> dead-air if both sides have a constant stream of changes to add. So we'd\n> likely have to add a timeout to defer giving up the token, to counter\n> dead-air, further adding delay to the changes from the other end, and\n> adding yet another parameter.\n\nI originally allowed optimistically sending commitment_signed.  But it\nmeans there can be more than one commitment tx for any given height (you\nhave to assume they received the sig and might broadcast it), which\nseemed to complicate things.  OTOH this is only true if you choose to do\nthis.\n\n> This is in stark contrast to the leader-based approach, where both\n> parties can just keep queuing updates without silent times to\n> transferring the token from one end to the other.\n\nYou've swayed me, but it needs new wire msgs to indicate \"these are your\nproposals I'm reflecting to you\".\n\nOTOH they don't need to carry data, so we can probably just have:\n\nupdate_htlcs_ack:\n   * [`channel_id`:`channel_id`]\n   * [`u16`:`num_added`]\n   * [`num_added*u64`:`added`]\n   * [`u16`:`num_removed`]\n   * [`num_removed*u64`:`removed`]\n\nupdate_fee can stay the same.\n\nThoughts?\nRusty."
            },
            {
                "author": "Christian Decker",
                "date": "2020-10-26T12:46:44",
                "message_text_only": "Rusty Russell <rusty at rustcorp.com.au> writes:\n>> This is in stark contrast to the leader-based approach, where both\n>> parties can just keep queuing updates without silent times to\n>> transferring the token from one end to the other.\n>\n> You've swayed me, but it needs new wire msgs to indicate \"these are\n> your proposals I'm reflecting to you\".\n>\n> OTOH they don't need to carry data, so we can probably just have:\n>\n> update_htlcs_ack:\n>    * [`channel_id`:`channel_id`]\n>    * [`u16`:`num_added`]\n>    * [`num_added*u64`:`added`]\n>    * [`u16`:`num_removed`]\n>    * [`num_removed*u64`:`removed`]\n>\n> update_fee can stay the same.\n>\n> Thoughts?\n\nSo this would pretty much be a batch-ack, sent after a whole series of\nchanges were proposed to the leader, and referenced by their `htlc_id`,\ncorrect? This is one optimization step further than what I was thinking,\nbut it can work. My proposal would have been to either reflect the whole\nmessage (nodes need to remember proposals they've sent anyway in case of\ndisconnects, so matching incoming changes with the pending ones should\nnot be too hard), or send back individual acks, containing the hash of\nthe message if we want to safe on bytes transferred. Alternatively we\ncould also use reference the change by its htlc_id.\n\nThe latter however means that we are now tightly binding the\nlinearization protocol (in which order should the changes be applied)\nwith the internals of these changes (namely we look into the change, and\nreference the htlc_id). My goal ultimately is introduce a better\nlayering between the change proposal/commitment scheme, and the\nsemantics of the the individual changes (\"which order\" vs. \"what\").\n\nI wonder what the performance increase of the batching would be compared\nto just acking each update individually. My expectation would be that in\nmost cases we'd be acking a batch of size 1 :-)\n\nPersonally I think just reflecting the changes as a whole, interleaving\nmy updates with yours is likely the simplest protocol, with the least\nimplied state that can get out of sync, and cause nodes to drift apart\nlike we had a number of times (\"bad signature\" anyone ^^). And looking\n(much much) further it is also a feasible protocol for multiparty\nchannels with eltoo or similar constructions, where the leader\nreflecting my own changes back to me is more of a special case than the\nnorm.\n\nCheers,\nChristian"
            }
        ],
        "thread_summary": {
            "title": "Simplified (but less optimal) HTLC Negotiation",
            "categories": [
                "Lightning-dev",
                "RFC"
            ],
            "authors": [
                "Rusty Russell",
                "Bastien TEINTURIER",
                "Christian Decker"
            ],
            "messages_count": 8,
            "total_messages_chars_count": 24731
        }
    },
    {
        "title": "[Lightning-dev] Lightning-dev Digest, Vol 62, Issue 14",
        "thread_messages": [
            {
                "author": "Bitcoin Error Log",
                "date": "2020-10-14T06:17:59",
                "message_text_only": "Regarding putting trust on the table as a design solution to possible\nattacks that aren't happening, wouldn't it be wise to start with whatever\ntrust solutions common networks already use and mutate that to this\nsituation?\n\nExample: KYC, black/whitelisting, reputation scoring, permissioned/private\nsubnets, scoring/tiers\n\nOf course, none of us actually want to design formal KYC into LN, but it\nreally is the same premise: \"identifying\" your counterparty and assessing\nthe amount of risk you would like to allocate. Permissioned access, reduced\npublic listening, out-of-band credentialing, etc, etc.\n\nNetworking issues like these might not even be appropriate to be handled at\nthe LN level. Possibly better to use a multipurpose context layer (gets\ninto ID systems, namespaces, WoTs).\n\nSorry if I'm oversimplifying the topic, but it is frustrating to watch devs\nargue about the edge of an edge of a knife no one is using, and then\nbikeshed every imperfection...\n\nCryptographic punishment schemes aren't swiss army knives.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201014/c6636dc0/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Lightning-dev Digest, Vol 62, Issue 14",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Bitcoin Error Log"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1207
        }
    },
    {
        "title": "[Lightning-dev] Watchtowers cannot sweep HTLCs if option_anchor_outputs is enabled (regression)",
        "thread_messages": [
            {
                "author": "SomberNight",
                "date": "2020-10-19T20:31:15",
                "message_text_only": "Hi all,\n\nRecently I've been looking at refactoring some of the watchtower code in Electrum, and not wanting to have to do it again soon, was also considering how it should be adapted to option_anchor_outputs.\n\nThis made me realise there does not seem to be a way for a watchtower to trustlessly sweep HTLCs if option_anchor_outputs is enabled for the channel (unlike without the option).\n\nSpecifically, consider the scenario where the breacher also broadcasted second-stage HTLC transactions (HTLC-Timeout or HTLC-Success) [0], along with some old commitment tx. The watchtower would have to react within the relative timelock (CSV) and spend the output using the branch that requires revocationpubkey. Note that the watchtower cannot be given (the privkey for) revocationpubkey as then it would be able to spend the output to any address (such as their own).\n\nWithout option_anchor_outputs, the Electrum watchtower is currently given a complete pre-signed transaction that spends the (revoked) second-stage output to a user wallet address whenever the commitment tx is updated off-chain.\n\nHowever, with option_anchor_outputs enabled, the txid of the second-stage HTLC tx can no longer be predicted (due to the use of SIGHASH_SINGLE|SIGHASH_ANYONECANPAY for the signature in the input).\nAs all current sighash flags commit to the outpoint being spent (see item 4 \"outpoint\" in bip-143 [1]), this seems to imply a signature using the revocationpubkey cannot be constructed ahead of time by the client to be given to the watchtower.\n\nEven if we had sighash_noinput (or similar), some care would need to be taken, given the address-reuse between each second-stage HTLC output (and even the to_local output). Also, as noinput/anyprevout as per current proposal will only work with witness v1 outputs, the current tx template will need to be changed to use those.\n\nA watchtower that does not sweep HTLC outputs seems to be of limited utility to me: to mitigate risk, max_htlc_value_in_flight_msat would have to be set to a small fraction of the channel capacity.\n\nI was told on IRC that this regression is a \"clear\" tradeoff of using option_anchor_outputs - but as I have not seen this discussed anywhere previously and as it was not completely obvious to me at first, I just wanted to explicitly point it out for the benefit of others.\n\nghost43\n\n\n[0]: https://github.com/lightningnetwork/lightning-rfc/blob/7e8c478aef0d23a445845b7d297b0e804583697c/03-transactions.md#htlc-timeout-and-htlc-success-transactions\n[1]: https://github.com/bitcoin/bips/blob/master/bip-0143.mediawiki#specification"
            }
        ],
        "thread_summary": {
            "title": "Watchtowers cannot sweep HTLCs if option_anchor_outputs is enabled (regression)",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "SomberNight"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2587
        }
    },
    {
        "title": "[Lightning-dev] CVE-2020-26895: LND Low-S Tx-Relay Standardness",
        "thread_messages": [
            {
                "author": "Conner Fromknecht",
                "date": "2020-10-20T21:52:27",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\n\nHi all,\n\nToday we are writing to disclose the details of CVE-2020-26895 as a follow up to\nthe partial disclosure sent to lightning-dev [1].\n\n## Abstract\n\nPrior to v0.10.0-beta, a malicious peer could force an lnd node to accept a\nhigh-S ECDSA signature when updating new off-chain states. Though the signatures\nare valid according to consensus rules, the mempool policy would reject\ntransactions containing high-S values, potentially leading to loss of funds if\ntime-sensitive transactions cannot be relayed and confirmed. We have no evidence\nof the bug being exploited in the wild.\n\nIt affects all classes of lnd nodes: routing, merchant, mobile, etc.\n\nThe vulnerability was reported privately to the lnd team by Antoine Riard.\n\n## Background\n\nThe lightning-rfc specifies a fixed-width, 64-byte encoding used to transmit\nECDSA signatures in the Lightning protocol, which differs from the DER-encoding\nused at the consensus layer. For regular, on-chain transactions, signature\nserialization is handled by the btcec library's Signature.Serialize() method\n[2]. This method always normalizes signatures to their low-S variant before\nperforming the DER-encoding to ensure that the btcec library can't _produce_\nhigh-S signatures.\n\nEarly in lnd's history, however, serialization modeled off btcec was added to\nproduce DER-encoded signatures directly from the fixed-width representation,\nbypassing the conversion into big.Int representation used internally by btcec.\nIn doing so, retaining the low-S normalization behavior was overlooked, and so\nSig.ToSignatureBytes() [3] would return high-S DER signatures whenever the\nfixed-size signature was encoded with a high-S value.\n\nDuring unilateral closure, this can be exploited by an attacker to cause a\nsecond-level HTLC-success transaction from being accepted to the mempool. If the\nvictim is unable to patch before the HTLC's CLTV expires, the attacker can then\nbroadcast their HTLC-timeout transaction and recover the full value of the HTLC\nminus fees. On the other hand, lnd\u2019s cooperative close fully verifies the remote\nparty\u2019s signatures using full policy-aware verification. As a result, the only\nexploitation vector occurs during the force close scenario.\n\n## Updates to Lighting RFC\n\nAs noted by Riard during the process, the lightning-rfc is lacking in terms of\nspecifying how nodes should validate signatures accepted off-chain. Notably, the\nsignatures should be checked for conformation to both consensus _and_ tx-relay\nstandardness rules, and rejected otherwise. Riard has confirmed that he is\nplanning to submit an update to the specification incorporating these\nrecommendations.\n\n## Patch\n\nThis vulnerability was fixed in v0.10.0-beta by converting all witness\nconstruction methods in lnd to accept signatures according to the\ninput.Signature interface introduced in PR 4172 [4], which requires the passed\nobject to have a Serialize() method. lnwire.Sig does not have a Serialize()\nmethod, and so cannot satisfy the interface. As a result, the relevant call\nsites were updated to pass in a btcec.Signature, forcing witness signature\nserialization through btcec's Serialize() method which includes low-S\nnormalization.\n\nNote: A high-S signature can be converted to a low-S one manually w/o software\nchanges, or by a 3rd party, assuming one is aware of the reason for rejection.\n\nThough the above recommendation to the spec by Riard also mitigates the issue,\nthis approach was chosen because it could retroactively patch affected nodes if\nthey are upgraded before the HTLC deadline expires, as well as its covertness.\nAfter upgrading, any outstanding broadcasts would be reattempted, this time\nnormalizing any previously-persisted high-S signatures into their low-S variant.\n\nFollowing the disclosure, lnd will also introduce the full tx-relay standardness\nchecks that are to be added to the lightning-rfc, as this offers a more general\nand complete approach to ensuring lnd always adheres to standardness rules.\n\n## Timeline\n\n04/03/2020 - Initial report from Antoine Riard\n04/10/2020 - PR 4172 merged into master\n04/29/2020 - lnd v0.10.0-beta released\n08/20/2020 - lnd v0.11.0-beta released\n10/08/2020 - Partial Disclosure sent to lightning-dev and lnd mailing list [1]\n10/20/2020 - Full Disclosure sent to lightning-dev and lnd mailing list\n\n## References\n\n[1] https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-October/002819.html\n[2] https://github.com/btcsuite/btcd/blob/ba530c4abb35ea824a1d3c01d74969b5564c3b08/btcec/signature.go#L47\n[3] https://github.com/lightningnetwork/lnd/blob/0f94b8dc624cf0e96ddc8fe1b8e3bf4b3fc4c074/lnwire/signature.go#L92\n[4] https://github.com/lightningnetwork/lnd/pull/4172\n[5] https://gist.github.com/ariard/fb432a9d2cd3ba24fdc18ccc8c5c6eb4\n\nHuge thanks to Antoine Riard for the responsible disclosure and for helping to\nmake lnd more safu. More information can be found in Antoine\u2019s disclosure [5].\n\nRegards,\nConner Fromknecht\n\n-----BEGIN PGP SIGNATURE-----\n\niQIzBAEBCAAdFiEEnI1hhop8SSADsnRO59c3tn+lkscFAl+PWykACgkQ59c3tn+l\nksf7Yg//WqZjtkw4Ol1fsbK+lfAkt6UENAq6Pja4/ZatkFI+U9Of3SwCNA5aAP1p\nXSSk4tQptNJi3o8IoUSGLuFZVOzc6SB/vrU3l+MMVcAfgnSj1N5rqW1OLv3Lm/fH\nJz8eSvPtSEdtFt1JOC1VEd4ZEukjB8T6P/d4yudptUTk0tMrkLhLcN7byflxttU/\nvG2JTtLl0uO+A2rhHkNvoUYqNLEOQssoAmShEBu+8uK56xGkRJQoBWvWN+yJDWor\nMsI9awUzWrvlGU4qCD2WxVWjK7/SXK7cz4PC1XaNr0RqhucXm6lL2wn4r/G1Qqx6\nz6ztMG3rHNnLI43Y1DVn2qPiLyhx1cBe/X8U9EUmA1jDv3zCKo9MmRx1gS69DQx+\nrWBx3N6AIevs3aU6fF2IrwMfrX/0Xb0HT/fIDq1qhN46UZaxJTUuRkQkpJHNNajI\nze5mmussTg/cdPlSkYTt7/h0CgWN6DAEGVYp6hsPqGoH7jrsEB+wRtsihHRAXevX\ndcHZRoB4OG/oKs5vYZk2LIRKr1miNSdas5T0rJA7voGMMJEa5L8nKqGfzMXsudAv\nn+gVXCLPx3KjTF8yQrNu277ptGIy4AG71MQoOdxkzLtbzm2vfyDMNd14Qk3KWHVz\nNACF4EXcq+oebSIj5kkiNx2jei0hdYVJGPHp8XkewRb0KuB2yDw=\n=Sd09\n-----END PGP SIGNATURE-----"
            }
        ],
        "thread_summary": {
            "title": "CVE-2020-26895: LND Low-S Tx-Relay Standardness",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Conner Fromknecht"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 5812
        }
    },
    {
        "title": "[Lightning-dev] CVE-2020-26896: LND Invoice Preimage Extraction",
        "thread_messages": [
            {
                "author": "Conner Fromknecht",
                "date": "2020-10-20T21:52:29",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\n\nHi all,\n\nToday we are writing to disclose the details of CVE-2020-26896 as a follow up to\nthe partial disclosure sent to lightning-dev [1].\n\n## Abstract\n\nPrior to v0.11.0-beta, an lnd node could be coerced into revealing an invoice\npreimage for a forwarded HTLC with a colliding payment hash. This can be\nexploited to a) weaken the victim's receiver privacy by confirming the\ndestination of an HTLC, and/or b) under certain circumstances, result in loss of\nfunds to the victim by believing the invoice was paid when it only received\nrouting fees. We have no evidence of either case being exploited in the wild.\n\nIt affects routing nodes, i.e. any lnd node that permits HTLC forwarding and\noperates as any form of merchant node (accepts payment for goods & services);\nnodes that use rejecthtlc=1 are not affected.\n\nThe vulnerability was reported privately to the lnd team by Antoine Riard.\n\n## Background\n\nWhen resolving incoming HTLCs on-chain, the forwarding node is expected to\nsupply a valid preimage via the witness in order to claim the HTLC. Preimages\ncan be learned in two primary ways: by generating an invoice or receiving a\npreimage as the result of a successfully forwarded HTLC. Internally, lnd tracks\nthese differing preimage classes in two distinct places: an invoice database and\na preimage database.\n\nFor proper handling, a node should inspect the off-chain HTLC it received to\ndetermine whether the node was an intermediary or the final hop (indicated by an\nall zeros next_hop short channel id). Final hops are intended to consult the\ninvoice database for preimages, while intermediaries should consult the preimage\ndatabase.\n\nEarlier versions of lnd would incorrectly fallback to the invoice database if\nthe preimage database did not contain the preimage. This resulted in situations\nwhere the node would reveal an invoice preimage, even though the victim was not\nthe final hop in a route.\n\nIn coordination with a triggered channel closure, an attacker can construct a\nmalicious HTLC through the victim, divulging the target invoice preimage\non-chain when the incoming HTLCis claimed. After disclosing the preimage, an\nattacker can claim a concurrent outgoing HTLC whose CLTV has not yet expired.\nhis results in the HTLC's value being stolen, minus routing fees, since the\nvictim incorrectly believes they received the payment.\n\n## Attack Scenario\n\nAn example of the attack goes as follows:\n\nAssume a _real HTLC_ is routed to a malicious node, Malice, that has a channel\nwith sufficient outgoing bandwidth to forward to the victim, Bob, the intended\nreceiver of the payment.\n\nInstead of forwarding the real HTLC, Malice creates a new _malicious HTLC_ which\nhas the same payment hash. The first hop\u2019s amount should be equal or slightly\nlarger, the CLTV can be increased as desired up to default implementation\nlimits.\n\nMalice forwards the malicious HTLC in a circular route through Bob and back to\nherself, where she holds the malicious HTLC.\n\nWith the malicious HTLC locked in place, Malice triggers a unilateral close of\nthe Malice-Bob channel. The commitment played (from Bob\u2019s POV) has an incoming\nHTLC with the target invoice\u2019s payment hash. The unilateral closure should be\ninitiated well before any of the malicious HTLC\u2019s CLTVs expire, otherwise the\nlast channel in the route will also go to chain.\n\nWhen the unilateral close is confirmed, Bob will promptly broadcast the\nHTLC-success transaction for the malicious HTLC. Due to the bug, the preimage\nprovided to sweep the malicious HTLC is obtained from the invoice database as a\nfallback to the (forwarded) preimage database, rather than being ignored.\n\nThe victim, Bob, has already marked the invoice settled and published the\npreimage on-chain. However, the malicious HTLC is still active on Bob\u2019s\ndownstream channel (and the rest of the circular route), allowing Malice to\nsettle the malicious HTLC she holds _after_ the invoice has already been marked\npaid. This results in Bob only receiving routing fees, and the Malice\nredirecting the payment to herself, while simultaneously convincing Bob of\nreceiving the full payment.\n\nAt this point, the malicious HTLC  has been successfully pulled from Malice to\nherself in a circular route, making herself whole minus routing fees (in\naddition to chain fees if she was the initiator of the Malice-Bob channel).\nMalice then settles the real, intercepted HTLC using the same preimage to obtain\na profit.\n\n### Caveats\n\nFor the attack to succeed, Malice must intercept a legacy HTLC or a\nsingle-sharded MPP HTLC. If any other concurrent MPP shard has already reached\nBob before attempting to claim on-chain, the attack will fail due to additional\nsafety checks added to lnd [2], preventing an invoice from being settled by a\ndowngraded HTLC after it has already accepted an MPP shard with a valid payment\nsecret.\n\nIn order to directly profit from the attack, Malice must be intercepting a\npayment from an unsuspecting victim, limiting control of timing and the amount\nthat can be siphoned. Malice must also somehow infer or guess that Bob has the\ncorresponding invoice being paid.\n\nIf Malice runs the same attack without intercepting a real HTLC, she pays\nrouting fees, and possibly chain fees, in exchange for the invoice preimage and\nidentity of the receiver. However, it is possible for her to indirectly profit\nfrom this if the service provider releases tangible goods or services to anyone\nwith knowledge of the invoice preimage, which is not recommended in practice.\n\nThe upstream attacker does not need to be adjacent, they only need to know which\nchannel to target and watch for closure. Being adjacent increases the\nassuredness of pulling off an exploit, but is not strictly required.\n\nSimilarly, the downstream attacker (possibly distinct from Malice) does not need\nto be adjacent, they can settle the malicious HTLC further downstream to the\nsame effect at the cost of more routing fees.\n\n## Patch\n\nThis vulnerability was patched in lnd v0.11.0-beta, by properly isolating the\npreimage database from the invoice database according to the HTLC's next_hop\nfield in commit cf739f3f [3] of PR 4157 [4]. The isolation ensures that we can\nonly claim forwarded HTLCs as a result of learning the preimage from an outgoing\nHTLC. It also fixes the privacy leak by not revealing invoice preimages unless\nthe node is the final destination.\n\nDue to the complexities involved in describing vulnerabilities over textual\nmediums, the full nature of the issue wasn\u2019t fully understood until after\nv0.10.0-beta had been released. Additionally, the covert fix contained in the\nv0.11.0-beta release was pushed back due to a concurrent investigation into\nnetwork instabilities resulting in unexpected channel closures.\n\nNote, although the above patch fixes the issue, this issue could have also been\navoided by having receivers require payment secrets (BOLT 11 `s` field) since\nthe attackers would be unable to guess the payment secret. However, left as\noptional, the attacker can always downgrade to using malicious HTLCs that omit\nthe payment secret.\n\nFor some time we have debated flipping the switch on requiring payment secrets\nacross the three major implementations. This vulnerability is further evidence\nto the additional safety and privacy benefits. Now almost a year since the\ninitial deployment of payment secrets in lnd, the upcoming v0.12.0-beta release\nof lnd is likely to make payment secrets required by default. We would welcome\nother implementations to do the same.\n\n## Timeline\n\n04/19/2020 - Initial report from Antoine Riard\n04/29/2020 - lnd v0.10.0-beta released\n07/07/2020 - PR 4157 merged into master\n08/20/2020 - lnd v0.11.0-beta released\n10/08/2020 - Partial Disclosure sent to lightning-dev and lnd mailing list [1]\n10/20/2020 - Full Disclosure sent to lightning-dev and lnd mailing list\n\n## References\n\n[1] https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-October/002819.html\n[2] https://github.com/lightningnetwork/lnd/blob/9f32942a90bcd91cc37a4a9c6c2fb454f534a65d/invoices/update.go#L229\n[3] https://github.com/lightningnetwork/lnd/pull/4157/commits/cf739f3f87fdcb28ab45dfd48e3d18adf26e45b3\n[4] https://github.com/lightningnetwork/lnd/pull/4157\n[5] https://gist.github.com/ariard/6bdeb995565d1cc292753e1ee4ae402d\n\n\nA big thank you to Antoine for the responsible disclosure and for helping to\nmake lnd more safu. More information can be found in Antoine\u2019s disclosure [5].\n\nRegards,\nConner Fromknecht\n\n-----BEGIN PGP SIGNATURE-----\n\niQIzBAEBCAAdFiEEnI1hhop8SSADsnRO59c3tn+lkscFAl+PWzMACgkQ59c3tn+l\nkseIJw//UwswUyh6BNgmi4D8NoC6olelW0dRmecqcZF7JBQa619kVFm/D7rixp33\nJ1YsXvZC2OLTpqmaJcJ3OvBKLVcW7CxheDp3Pm0JjrfVnmOl1NGX4CSymL6Zpou7\nnFqh+nqOZ2n6o4OIv+mx0y2YANKjAVtAcr9LakubMn/3LgYzqvKKu39QGqrtz9vZ\nlYGAAPU3zlAjIjFNv56xWpF0Pj9VE2mQB27w2QmbSuNtR21feOSJhJimEvmXhk6d\nO0Ze78Fea+eaS+d1uyRkB7aaEKBRAA5WCtDKgSOwfEY+mHC7u5+LRasyegjlc8Ie\nhYBNOsjEZqVjwIgr+lqMDbQ8B5RtW4LVro/LMYGCbVRnGuF16gHu/lkDnVgz/sY7\nsbsPVG11wfVFH0U/TyJoBC8qOmeHMJoVsvGbY9I2XQiFw7yAbWxEdU+7mMhQZA2Z\nZd9pl0ATByLFPyg58gA6G4JV+F45DvYrG3jj6cdkUvL2nQST08IZtTjnDxAnkDTk\nHwnJo0fd7vsixEyssTMuSCjbGSaPDMPCkmNQg8PAhhoIK8MeKUlylCKJuM6gMWeW\nYypzGBmE6O7OtoMTOYFWysU67edVXgQTV2dD/PE6abTYOfS79gvkNekU4BvW9NDE\naf0JBywXovzNVshdqpijPBleOT8/QSyTdLvI78ev+zpJMsKMugU=\n=PY6W\n-----END PGP SIGNATURE-----"
            }
        ],
        "thread_summary": {
            "title": "CVE-2020-26896: LND Invoice Preimage Extraction",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Conner Fromknecht"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 9332
        }
    },
    {
        "title": "[Lightning-dev] Full Disclosure: CVE-2020-26896 LND \"The (un)covert channel\"",
        "thread_messages": [
            {
                "author": "Antoine Riard",
                "date": "2020-10-20T22:05:18",
                "message_text_only": "# Problem\n\nIn case of a relayed HTLC hash-and-amount collision with an expected\npayment HTLC on the same channel, LND was releasing the preimage for the\nlater while claiming onchain the former. A malicious peer could have\ndeliberately intercepted a HTLC intended for the victim node, probe the\nvictim to learn the preimage and steal the intercepted HTLC.\n\nPrior to v0.11, LND had a vulnerability in its invoice database while\nclaiming onchain a received HTLC output, it didn't verify that the\ncorresponding outgoing off-chain HTLC was already settled before releasing\nthe preimage. In case of hash-and-amount collision with an invoice, the\npreimage for an expected payment was instead released. A malicious peer\ncould have deliberately intercept a HTLC intended for the victim node,\nprobe the preimage through a colluding relayed HTLC and steal the\nintercepted HTLC.\n\nNote that MPP payments (invoices with `s` field`) aren't subject to this\nvulnerability as an attacker wouldn't have the payment secret to\nauthenticate the probe HTLC. As of today, this class of invoices isn\u2019t\nwell-deployed as it outlaws non-upgraded payers.\n\nThis vulnerability was exposing routing nodes also receiving payments (e.g\nmerchant nodes). A peer servicing as an intermediary hop of a payment path\ncould have guessed that the next hop is the final receiver by comparing the\nHTLC's `amount_msat` with any goods/services advertised by the victim node\nmerchant website. This is not affecting non-routing nodes.\n\nNote, this is a case of _indirect_ fund loss of which the exploitation\nscope depends on application logic. As the invoice state would have been\nmarked as settled, an application directly subscribing to it might have\nautomatically released the offered good/service. Otherwise, the loss may\nhave been characterized if the preimage had a direct liquid value (e.g\natomic data exchange where the preimage is a decryption key).\n\n# Solution\n\nThe current spec requirement is the following :\n\n\"A local node if it receives (or already possesses) a payment preimage for\nan unresolved HTLC output that it has been offered AND for which it has\ncommitted to an outgoing HTLC MUST resolve the output by spending it, using\nthe HTLC-Success transaction\".\n\nThis point could be clearer and precise the risk of HTLC hash-and-amount\ncollision : \"MUST NOT reveal a preimage for an incoming HTLC if it has not\nlearnt the preimage for the claiming of the outgoing HTLC\" [0]\n\nI engage the LN ecosystem to consider the mandatory deployment of\n`payment_secret`, reducing the surface for this class of bugs, among other\nbenefits.\n\n\n# Background\n\nAlice, a merchant, is sending an invoice for amount A and hash H to Bob.\n\nBob is routing the HTLC A/H to Alice through Mallory.\n\nChannel topology [0]\n\n\n                HTLC (A,H')         HTLC (A,H')          HTLC (A,H)\n             _____________       _____________          _____________\n            /             \\     /              \\       /             \\\n           V               \\   V                \\     V               \\\n      Mallet <-----------> Alice <------------> Mallory <-----------> Bob\n                             \\                                        ^\n                              \\______________________________________/\n\n                                          invoice (A,H)\n\nMallory intercepts HTLC (A,H) from Bob and doesn't relay it forward.\nMallory, knowing that Alice node pubkey is tied to a merchant node, is\nbrowsing through her goods/services index to decide if Bob's HTLC is\nintended for Alice by comparing the relayed amount with the price of an\nitem.\n\nIf there is a match, more-or-less some noise of sats, Mallory draws a new\npayment path to Mallet, a colluding node. Mallory sends a HTLC (A,H') to\nAlice, which relays it forward to Mallet.\n\nMallet holds the HTLC and doesn't do it immediately. Instead, Mallet routes\nback a new HTLC Y to Mallory through Alice. This HTLC Y won't be failed by\nMallory, thus forcing Alice to force-close the channel when HTLC Y\noff-chain settlement deadline is crossed.\n\nAlice goes onchain with her commitment for chan Alice-Mallory. She claims\nback offered output for HTLC Y and received output for HTLC H'. She reveals\npreimage P, with H'=sha256(P) equivalent to H=sha256(P).\n\nMallory learns preimage P onchain and sends it out-of-band to Mallet which\nclaims the incoming HTLC H' from Alice. Mallory claims the previously\nintercepted HTLC H from Bob.\n\nBob learns the preimage P.\n\nThe coalition Mallet and Mallory gain the value of HTLC H minus Alice's\nrouting fee for HTLC H'. Also, they may have to pay the channels\nopening/closing fees depending on who initiates.\n\nAlice reveals a preimage P which corresponds to a provided good/service\nwithout receiving the payment for it, thus being at loss.\n\nAlice might not have learnt about her loss until reconciling her merchant\ninventory with her HTLC accounting, thus this exploitation might have been\nstealth for a while.\n\n# Discovery\n\nWhile working on Rust-Lightning, I observed that the specification was\nsilent on the risk of hash collision during the onchain operations. I\nsurveyed deployed Lightning implementations on this behavior. A quick\nmanual test against LND (65f5119) confirmed this vulnerability, which has\nbeen immediately disclosed to the LND team.\n\nWe agreed for a 6-month embargo period, as at that time a LND release\n(0.10) was undergoing and it was too late to implement a covert fix in this\nrelease.\n\nIf this vulnerability has been exploited, the original sender would have\ndiscovered the preimage, according to the pre-agreed invoice but without\nthe issuer effectively being paid. In case of legal disagreement if the\ncorresponding good/service should be settled, and assuming parties were\nsubject to the same jurisdiction, it could have been an interesting case to\ndecide if the invoice/preimage pair is legally binding.\n\n# Timeline\n\n2020-04-19: Vulnerability discovered, LND team notified\n2020-04-29: lnd v0.10.0-beta released (without fix)\n2020-07-31: lnd v0.11-0-beta released (with fix)\n2020-10-08: Partial Disclosure, encouraging lnd users to upgrade to lnd\nv0.11.x ASAP\n2020-10-09: CVE assigned (\nhttps://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26896)\n200-10-15: Preventive Disclosure to c-lightning, Electrum and Eclair teams\n2020-10-20: Full Disclosure\n\n[0] See gist if it doesn\u2019t survive ML formatting:\nhttps://gist.github.com/ariard/42935dd26ccf88cea7b91505c3d59d63\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201020/483c3051/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Full Disclosure: CVE-2020-26896 LND \"The (un)covert channel\"",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Antoine Riard"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 6621
        }
    },
    {
        "title": "[Lightning-dev] Full Disclosure: CVE-2020-26895 LND \"Hodl my Shitsig\"",
        "thread_messages": [
            {
                "author": "Antoine Riard",
                "date": "2020-10-20T22:56:09",
                "message_text_only": "# Problem\n\nA lightning node must verify that its channel transactions are not only\nconsensus-valid but also tx-relay standard. The counterparty signatures are\npart of the local txn (commitment/HTLC) as provided in the\n`commitment_signed`. Verifying consensus-validity of these signatures but\nnot their tx-relay standardness, let an attacker provoke a permanent\ntx-relay failure of the victim transactions. The attacker can steal\nin-flight HTLCs once they expire as the victim can't trustlessly claim them\nonchain during the CLTV delay.\n\nBitcoin ECDSA signatures are made of the scalar pair (R, S). Since Bitcoin\nCore 0.10 [0], high-S (above curve order / 2) signatures aren't standard\nand thus any transaction containing one won't be relayed/mined on the\nregular p2p network. Note, that this check isn't part of consensus rules\neven if it has been proposed to be soft-forked [1].\n\nPrior to v0.10, LND would have accepted counterparty high-S signature and\nbroadcast tx-relay invalid local commitment/HTLC transactions. This can be\nexploited by any peer with an already open channel whatever victim\nsituation (routing node, payment-receiver, payment-sender [2]).\n\nContrary to other Lightning implementations, LND uses\n`btcd.btcec.signature` for verifying counterparty signatures at commitment\nsigned exchange. This go package relies itself on the default golang crypto\necdsa package. It differs from libsecp256k1, as the verification method\ndoesn't enforce the lower-S form of the signature, thus a libsecp256k1\nsignature validity is tighter than a golang ecdsa package one.\n\nThe `btcec` serialization code was correctly normalizing signature to low-S\nbut this step wasn\u2019t included when that code was brought into LND\u2019s\n`lnwire`.\n\nNote, that LND didn't suffer from this vulnerability on opening/closing as\nit was relying on btcd.txscript witness verification method with the\ncorrect standard flags, enforcing low-S signatures.\n\nNote that Bitcoin Core (`CPubKey::Verify`) always normalizes signatures\nbefore passing them to libsecp256k1 verification method which\nunconditionally enforces low-S (`secp256k1_ecdsa_verify`).\n\n# Solution\n\nAs ECDSA signatures are inherently malleable, even if the counterparty\nprovides a high-S signature it can be normalized by the receiver to a\ntx-relay standard one.\n\nA more proactive solution is to fail the channel at any reception of a\nhigh-S signature as it's a clear signal that your counterparty is either\nmalicious or buggy (most bitcoin softwares generates low-S signature since\na while [3]).\n\nFor now, the first solution has been adopted by the LND team. A spec change\nhas been proposed to make the second a requirement.\n\n# Background\n\nA lightning node security underlies the assumption to be always able to\nunilaterally broadcast channel transactions in the aim to timely confirm\nthem on-chain to enforce an off-chain negotiated balance.\n\nIt must be remembered that channel transactions are asymmetric, thus each\nparty owns a different version including all parties's balances/HTLCs. To\nbroadcast its version, a party must own a valid witness at any time.\n\nFor commitment transactions, the witness stack is the following :\n\n0 <localfunding_sig> <remotefunding_sig>\n\nFor HTLC-Success:\n\n0 <remotehtlc_sig> <localhtlc_sig> <payment_preimage>\n\nFor HTLC-Timeout:\n\n0 <remotehtlc_sig> <localhtlc_sig> <> (empty vector)\n\nThe <remotefunding_sig>/<remotehtlc_sig> are the ones which might have been\nmaliciously malleated by an attacker.\n\nThese signatures are provided at channels updated by a counterparty's\n`commitment signed`. Once it's accepted, the local node must release the\nrevocation secret for the previous channel state, thus relying on the\nvalidity of highest state transactions for its funds' safety.\n\nThese transactions must be unilaterally broadcasted in case of reaching the\noff-chain resolution deadline for [4]:\n* offered HTLCs for a routing/original sender\n* received HTLCs for a routing/last receiver\n\nNote, this off-chain resolution deadline even if it's expressed as block\nheight it's not equal to a HTLC absolute timelock but must always be\ninferior. It offers a block buffer for a local node to broadcast, fee-bump\nand hopefully confirm its transactions.\n\nA non-standard transaction can still be confirmed by out-of-band agreement\nwith a miner or a user intervention to correct the transaction if possible.\nIn the case of Lightning, the security model doesn't assume this kind of\nuser intervention and deployed timelocks would have been too short for a\nreasonable intervention of a node operator.\n\n# Discovery\n\nWhile working on Rust-Lightning, I observed that the implementation was\ngenerating MINIMALIF-invalid transactions due to a regression. This case\nwasn't covered by our test framework as there is no easy-to-integrate\nutility to test transaction standardness. After patching the spec, to\nrecall the MINIMALIF requirement on some channel transactions witnesses\n[5], I audited deployed Lightning implementations w.r.t to Core script\ninterpreter policy flags. A quick test against LND (65f5119) revealed this\nvulnerability.\n\nAfter informing the LND team, I also informed the c-lightning and Eclair\nteams. Even if this vulnerability is implementation specific, the tx-relay\nstandardness issues it underscored could have been failed by other\nimplementations. Thus an urgency fix by one of the team could have revealed\neasy-to-exploit weaknesses in other in-production implementations. As the\nvulnerability involves language-specific serializers and dependencies, it's\na hard task to evaluate correctness of an implementation without alerting\nthe concerned developers teams. For the future, it would be better to have\na clearer Lightning-wise coordination policy [6].\n\nNote, that we agreed with the LND team for a 6-month embargo period, the\ndouble of channel funding check CVEs. As the Lightning ecosystem is\nmaturing, the funds at stake are also growing. It should be minded how much\nLightning software is sensible compared to other security/cryptographic\nsoftware (e.g TLS). The high-stake nodes are known, channel connections are\nmostly open, they're operating on top of a public infrastructure (the\nBitcoin network) and the codebases are fully open source. Lightning node\noperators are running a Bitcoin bank in the plainsight, any failure might\nbe observed and exploited by an attacker.\n\nI'm eager to engage the wider Lightning community on what a reasonable\nvulnerability embargo should endure, both incentivizing dev teams to\nproactively fix their security vulnerabilities while minding a wide range\nof users, from business to hobbyists.\n\n# Ecosystem Implications\n\nThis vulnerability is a case of a transaction standardness malleability.\nTransaction standardness is a set of supplementary anti-DoS rules on top of\nBitcoin consensus rules. For e.g, MIN_STANDARD_TX_NONWITNESS_SIZE or\nMINIMALIF-compliant witnesses. This set of rules isn't well-defined as it's\npart of a full-node policy, which varies across implementations and\nversions.\n\nThere is no _a_ tx-relay policy but as many tx-relay policies as there are\nfull-node deployed as some checks might be tightened by node settings. This\nfact is acknowledged by LN devs since a while who are most of the time\ntesting channel transactions standardness against the latest Core release,\nfor lack of better.\n\nThis situation is concerning and sound to have been an undersight during\nLightning/payment channels protocols design. The transaction standardness\nsurface is quite wide, and any standardness fault, either accidental or\nmalicious triggered, can provoke a loss of funds for a LN node.\n\nFurther, this tx-relay standardness issue has a wider echo as other\ntime-sensitive multi-party protocols (e.g vaults/CoinSwaps) are affected.\nAll Bitcoin software implementing this class of protocols must correctly\nsanitize counterparty contribution, to avoid jeopardizing funds security or\nintroducing easy DoS (e.g a dual-funded channel with counterparty\nnon-standard input) [7]\n\nAt the Bitcoin base layer, a tighter, new tx-relay standard rule could\npotentially break the security of all these off-chain protocols and thus be\ndeployed silently without anyone realizing before it's an easy\nexploitation. As of today, tx-relay standardness fog is a systematic risk\nfor layer 2 protocols relying on time-sensitive transactions and I engage\nthe wider Bitcoin ecosystem to address it.\n\n# Timeline\n\n2020-04-03: Vulnerability discovered, LND team notified\n2020-04-29: lnd v0.10.0-beta released\n2020-07-31: lnd v0.11.0-beta released\n2020-10-08: Partial Disclosure, encourage lnd users to upgrade to lnd\nv1.11.x ASAP\n2020-10-09: CVE assigned (\nhttps://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26895)\n2020-10-15: Preventive Disclosure to c-lightning, Electrum and Eclair teams\n2020-10-20: Full Disclosure\n\n[0] https://github.com/bitcoin/bitcoin/pull/6769\n[1] https://github.com/bitcoin/bips/blob/master/bip-0147.mediawiki\n[2] A sender won't be able to timeout its offered payment on the first link\nof the payment path thus it might be claimed after the expiration with a\npreimage of which the utility is out-of-date\n[3]\nhttps://github.com/bitcoinops/scaling-book/issues/9#issuecomment-599047992\n[4] For more information, see the Flood & Loot paper (\nhttps://arxiv.org/pdf/2006.08513.pdf). The\nchannel timelocks which could have been exploited due to this current\nvulnerable are similar, even if the attack building block differs (i.e\nmalicious mempool-congestion)\n[5] https://github.com/lightningnetwork/lightning-rfc/pull/764\n[6] https://github.com/lightningnetwork/lightning-rfc/pull/772\n[7] For a wider discussion, see\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-July/018063.html\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201020/425937a8/attachment-0001.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2020-10-20T22:05:12",
                "message_text_only": "# Problem\n\nA lightning node must verify that its channel transactions are not only\nconsensus-valid but also tx-relay standard. The counterparty signatures are\npart of the local txn (commitment/HTLC) as provided in the\n`commitment_signed`. Verifying consensus-validity of these signatures but\nnot their tx-relay standardness, let an attacker provoke a permanent\ntx-relay failure of the victim transactions. The attacker can steal\nin-flight HTLCs once they expire as the victim can't trustlessly claim them\nonchain during the CLTV delay.\n\nBitcoin ECDSA signatures are made of the scalar pair (R, S). Since Bitcoin\nCore 0.10 [0], high-S (above curve order / 2) signatures aren't standard\nand thus any transaction containing one won't be relayed/mined on the\nregular p2p network. Note, that this check isn't part of consensus rules\neven if it has been proposed to be soft-forked [1].\n\nPrior to v0.10, LND would have accepted counterparty high-S signature and\nbroadcast tx-relay invalid local commitment/HTLC transactions. This can be\nexploited by any peer with an already open channel whatever victim\nsituation (routing node, payment-receiver, payment-sender [2]).\n\nContrary to other Lightning implementations, LND uses\n`btcd.btcec.signature` for verifying counterparty signatures at commitment\nsigned exchange. This go package relies itself on the default golang crypto\necdsa package. It differs from libsecp256k1, as the verification method\ndoesn't enforce the lower-S form of the signature, thus a libsecp256k1\nsignature validity is tighter than a golang ecdsa package one.\n\nThe `btcec` serialization code was correctly normalizing signature to low-S\nbut this step wasn\u2019t included when that code was brought into LND\u2019s\n`lnwire`.\n\nNote, that LND didn't suffer from this vulnerability on opening/closing as\nit was relying on btcd.txscript witness verification method with the\ncorrect standard flags, enforcing low-S signatures.\n\nNote that Bitcoin Core (`CPubKey::Verify`) always normalizes signatures\nbefore passing them to libsecp256k1 verification method which\nunconditionally enforces low-S (`secp256k1_ecdsa_verify`).\n\n# Solution\n\nAs ECDSA signatures are inherently malleable, even if the counterparty\nprovides a high-S signature it can be normalized by the receiver to a\ntx-relay standard one.\n\nA more proactive solution is to fail the channel at any reception of a\nhigh-S signature as it's a clear signal that your counterparty is either\nmalicious or buggy (most bitcoin softwares generates low-S signature since\na while [3]).\n\nFor now, the first solution has been adopted by the LND team. A spec change\nhas been proposed to make the second a requirement.\n\n# Background\n\nA lightning node security underlies the assumption to be always able to\nunilaterally broadcast channel transactions in the aim to timely confirm\nthem on-chain to enforce an off-chain negotiated balance.\n\nIt must be remembered that channel transactions are asymmetric, thus each\nparty owns a different version including all parties's balances/HTLCs. To\nbroadcast its version, a party must own a valid witness at any time.\n\nFor commitment transactions, the witness stack is the following :\n\n0 <localfunding_sig> <remotefunding_sig>\n\nFor HTLC-Success:\n\n0 <remotehtlc_sig> <localhtlc_sig> <payment_preimage>\n\nFor HTLC-Timeout:\n\n0 <remotehtlc_sig> <localhtlc_sig> <> (empty vector)\n\nThe <remotefunding_sig>/<remotehtlc_sig> are the ones which might have been\nmaliciously malleated by an attacker.\n\nThese signatures are provided at channels updated by a counterparty's\n`commitment signed`. Once it's accepted, the local node must release the\nrevocation secret for the previous channel state, thus relying on the\nvalidity of highest state transactions for its funds' safety.\n\nThese transactions must be unilaterally broadcasted in case of reaching the\noff-chain resolution deadline for [4]:\n* offered HTLCs for a routing/original sender\n* received HTLCs for a routing/last receiver\n\nNote, this off-chain resolution deadline even if it's expressed as block\nheight it's not equal to a HTLC absolute timelock but must always be\ninferior. It offers a block buffer for a local node to broadcast, fee-bump\nand hopefully confirm its transactions.\n\nA non-standard transaction can still be confirmed by out-of-band agreement\nwith a miner or a user intervention to correct the transaction if possible.\nIn the case of Lightning, the security model doesn't assume this kind of\nuser intervention and deployed timelocks would have been too short for a\nreasonable intervention of a node operator.\n\n# Discovery\n\nWhile working on Rust-Lightning, I observed that the implementation was\ngenerating MINIMALIF-invalid transactions due to a regression. This case\nwasn't covered by our test framework as there is no easy-to-integrate\nutility to test transaction standardness. After patching the spec, to\nrecall the MINIMALIF requirement on some channel transactions witnesses\n[5], I audited deployed Lightning implementations w.r.t to Core script\ninterpreter policy flags. A quick test against LND (65f5119) revealed this\nvulnerability.\n\nAfter informing the LND team, I also informed the c-lightning and Eclair\nteams. Even if this vulnerability is implementation specific, the tx-relay\nstandardness issues it underscored could have been failed by other\nimplementations. Thus an urgency fix by one of the team could have revealed\neasy-to-exploit weaknesses in other in-production implementations. As the\nvulnerability involves language-specific serializers and dependencies, it's\na hard task to evaluate correctness of an implementation without alerting\nthe concerned developers teams. For the future, it would be better to have\na clearer Lightning-wise coordination policy [6].\n\nNote, that we agreed with the LND team for a 6-month embargo period, the\ndouble of channel funding check CVEs. As the Lightning ecosystem is\nmaturing, the funds at stake are also growing. It should be minded how much\nLightning software is sensible compared to other security/cryptographic\nsoftware (e.g TLS). The high-stake nodes are known, channel connections are\nmostly open, they're operating on top of a public infrastructure (the\nBitcoin network) and the codebases are fully open source. Lightning node\noperators are running a Bitcoin bank in the plainsight, any failure might\nbe observed and exploited by an attacker.\n\nI'm eager to engage the wider Lightning community on what a reasonable\nvulnerability embargo should endure, both incentivizing dev teams to\nproactively fix their security vulnerabilities while minding a wide range\nof users, from business to hobbyists.\n\n# Ecosystem Implications\n\nThis vulnerability is a case of a transaction standardness malleability.\nTransaction standardness is a set of supplementary anti-DoS rules on top of\nBitcoin consensus rules. For e.g, MIN_STANDARD_TX_NONWITNESS_SIZE or\nMINIMALIF-compliant witnesses. This set of rules isn't well-defined as it's\npart of a full-node policy, which varies across implementations and\nversions.\n\nThere is no _a_ tx-relay policy but as many tx-relay policies as there are\nfull-node deployed as some checks might be tightened by node settings. This\nfact is acknowledged by LN devs since a while who are most of the time\ntesting channel transactions standardness against the latest Core release,\nfor lack of better.\n\nThis situation is concerning and sound to have been an undersight during\nLightning/payment channels protocols design. The transaction standardness\nsurface is quite wide, and any standardness fault, either accidental or\nmalicious triggered, can provoke a loss of funds for a LN node.\n\nFurther, this tx-relay standardness issue has a wider echo as other\ntime-sensitive multi-party protocols (e.g vaults/CoinSwaps) are affected.\nAll Bitcoin software implementing this class of protocols must correctly\nsanitize counterparty contribution, to avoid jeopardizing funds security or\nintroducing easy DoS (e.g a dual-funded channel with counterparty\nnon-standard input) [7]\n\nAt the Bitcoin base layer, a tighter, new tx-relay standard rule could\npotentially break the security of all these off-chain protocols and thus be\ndeployed silently without anyone realizing before it's an easy\nexploitation. As of today, tx-relay standardness fog is a systematic risk\nfor layer 2 protocols relying on time-sensitive transactions and I engage\nthe wider Bitcoin ecosystem to address it.\n\n# Timeline\n\n2020-04-03: Vulnerability discovered, LND team notified\n2020-04-29: lnd v0.10.0-beta released\n2020-07-31: lnd v0.11.0-beta released\n2020-10-08: Partial Disclosure, encourage lnd users to upgrade to lnd\nv1.11.x ASAP\n2020-10-09: CVE assigned (\nhttps://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26895)\n2020-10-15: Preventive Disclosure to c-lightning, Electrum and Eclair teams\n2020-10-20: Full Disclosure\n\n[0] https://github.com/bitcoin/bitcoin/pull/6769\n[1] https://github.com/bitcoin/bips/blob/master/bip-0147.mediawiki\n[2] A sender won't be able to timeout its offered payment on the first link\nof the payment path thus it might be claimed after the expiration with a\npreimage of which the utility is out-of-date\n[3]\nhttps://github.com/bitcoinops/scaling-book/issues/9#issuecomment-599047992\n[4] For more information, see the Flood & Loot paper (\nhttps://arxiv.org/pdf/2006.08513.pdf). The\nchannel timelocks which could have been exploited due to this current\nvulnerable are similar, even if the attack building block differs (i.e\nmalicious mempool-congestion)\n[5] https://github.com/lightningnetwork/lightning-rfc/pull/764\n[6] https://github.com/lightningnetwork/lightning-rfc/pull/772\n[7] For a wider discussion, see\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-July/018063.html\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20201020/06cbbba0/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Full Disclosure: CVE-2020-26895 LND \"Hodl my Shitsig\"",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Antoine Riard"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 19782
        }
    }
]