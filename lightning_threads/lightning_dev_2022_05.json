[
    {
        "title": "[Lightning-dev] Taro - Separating Taro concerns from LN token concerns",
        "thread_messages": [
            {
                "author": "John Carvalho",
                "date": "2022-05-01T05:33:47",
                "message_text_only": "For those who do not know me, I have been pioneering the concept of \"tokens\non Lightning\" for roughly three years, first by researching Liquid, then by\nsecuring funding for RGB and assisting that project for roughly 2 years,\nthen by researching and implementing OmniBOLT, etc, etc. I was pitching\ntokens on Lightning back when Ryan Gentry (LL bizdev) still worked at\nMulticoin Capital ;)\n\nMy general thinking was that if LN could scale Bitcoin, it could scale\ntokens on Bitcoin too.\n\nI am very familiar with Bitcoin sidechains and Bitcoin-anchored token\nprojects, and I think each has its own tradeoffs and arguments for\nexisting. I could easily argue the benefits of Taro over Liquid, or Liquid\nover Taro, or Omni over Taro, or Taro over Omni, etc. In my estimation,\nthere is no clear winner, and all of them could be obsoleted by future tech\nto come anyway.\n\nThat said, I believe that the correct approach to supporting \"tokens on\nLightning\" is to make it a separate concern from Taro, and that LL should\ncreate a separate BOLT proposal from the current Taro BIPs to ensure it LN\nstandards have a genericized protocol that all LN implementations would be\ninterested in supporting.\n\nTaro is not LN-native in any particular way, as it is simply a new design\nfor using Taproot and M-sum trees to establish token abstractions on-chain.\nIn practice, there is no such thing as issuing a token \"on\" Lightning, but\ninstead the requirement to add several feature concepts to LN that would\nallow tokens to interact with LN nodes and LN routing:\n\n - Making LN nodes aware of assets on other networks\n - Establishing commitments for (atomic) swapping for payments/routing\n - Supporting the ability to exchange and advertise exchange rates for\nasset pairs\n - Supporting other multi-asset routes when considering routing paths,\nbridging nodes with alternate assets\n - ...probably other stuff :)\n\nSo, I ask that Lightning Labs coordinate with the LN community to ensure\nsuch support for other networks and other assets not be dedicated only to\nTaro, and instead genericized enough so that other networks may compete\nfairly in the market, for the sake of Bitcoiners, and that LN standards by\nflexible enough to support future advances in token tech, other sidechains\nand Bitcoin layers like Omni, RGB, Rootstock, Liquid, 1WP sidechains, etc,\netc.\n\nOtherwise, we will be left with LL's advantage being that LND supports\nTaro, and weird narratives that Taro is somehow superior because LND\nspecifically added support for it, without creating a generic spec or BOLT\nthat all nodes could adopt for multi-network, multi-asset LN-as-rails use\ncases.\n\nFinally, I want to state that I do not represent any specific token\nnetworks solutions, and our company has not finalized which token networks\nit will support. If you are trying to assume where my loyalties are, you\nwill be wrong. I simply want *all* LN implementations and all current and\nfuture token solutions to get fair play and maximum interoperability.\n\nThanks!\n\n--\nJohn Carvalho\nCEO, Synonym.to <http://synonym.to/>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220501/1f7ad737/attachment.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2022-05-02T22:42:44",
                "message_text_only": "Hi John,\n\n> That said, I believe that the correct approach to supporting \"tokens on\n> Lightning\" is to make it a separate concern from Taro, and that LL should\n> create a separate BOLT proposal from the current Taro BIPs to ensure it LN\n> standards have a genericized protocol that all LN implementations would be\n> interested in supporting.\n\nThe current Taro BIPs describe just about everything needed in order to\ncreate, validate, and interact with assets on chain. Naturally, the system\nneeds to exist on-chain before any off-chain constructs can be built on top\nof it.\n\nOn the topic of a BOLT, I don't think something like Taro (particularly our\nvision for the deployment path) should exist at the _BOLT level_. Instead,\nwe aim to create a bLIP that fully specifies the _optional_ series of TLV\nextensions needed to open channels using Taro assets, and send them\noff-chain. IMO this isn't something that needs to be a BOLT as: it isn't\nintended to be 100% universal (most LN routing nodes and users will only\nknow of the core bitcoin backbone), isn't critical to the operation of the\ncore LN network, and it's something that will only initial be deployed at\nthe edges (sender+receiver).\n\nOn the BOLT side, there're a number of important upgrades/extensions being\nproposed, and imo it doesn't make sense to attempt to soak up the already\nscarce review bandwidth into something like Taro that will live purely at\nthe edges of the network. I also don't want to speak for the other LN devs,\nbut I think most would prefer to just focus on the core LN protocol and\nignore anything non-bitcoin on the sides. The implementations/developers\nthat think this is something worth implementing will be able to contribute\nto and review the bLIPs as they wish.\n\nA few implementations support LTC today, but that was mainly an exercise in\nhelping to build consensus for segwit so we could ultimately deploy LN on\nBitcoin's mainnet (iirc some implementations are in the process of even\nremoving support).  A prior version of the onion payload (now called the\nlegacy payload) had a \"realm\" field that was intended to be used for\nmulti-chain stuff. The newer modern TLV payload dropped that field as it\nwasn't being used anywhere.  IMO that was the right move as it allows us to\nkeep the core protocol simple and let other ppl be concerned w/ building\nmulti-asset stuff on top of the base protocol.\n\n> but instead the requirement to add several feature concepts to LN that\n> would allow tokens to interact with LN nodes and LN routing:\n\n>From this list of items, I gather that your vision is actually pretty\ndifferent from ours. Rather than update the core network to understand the\nexistence of the various Taro assets, instead we plan on leaving the core\nprotocol essentially unchanged, with the addition of new TLV extensions to\nallow the edges to be aware of and interact w/ the Taro assets. As an\nexample, we wouldn't need to do anything like advertise exchange rates in\nthe core network over the existing gossip protocol (which doesn't seem like\nthe best idea in any case given how quickly they can change and the existing\nchallenges we have today in ensuring speedy update propagation).\n\n> So, I ask that Lightning Labs coordinate with the LN community to ensure\n> such support for other networks and other assets not be dedicated only to\n> Taro, and instead genericized enough so that other networks may compete\n> fairly in the market,\n\nIf you're eager to create a generalized series of extensions to enable your\nvision, then of course you're welcome to pursue that. However, I don't think\nthe other LN developers will really care much about building some\ngeneralized multi-chain/multi-asset system given all the existing work we\nstill need to do to make sure the bitcoin backbone works properly and can\nscale up sufficiently. I'd also caution you against making the same mistakes\nthat Interledger did: they set out to build a generalized off-chain system\nwhich abstracts over the assets/chains entirely, but years later, and\nseveral hundred wc3 mailing list posts later, virtually nothing uses it.\nWhy? IMO, because it was overly generalized and they assumed that if they\nbuilt it, the entities that actually needed it would magically pop up\n(spoiler alert -- *SpongeBob narrator voice*: several years later, they\ndidn't).\n\n> Otherwise, we will be left with LL's advantage being that LND supports\n> Taro, and weird narratives that Taro is somehow superior because LND\n> specifically added support for it, without creating a generic spec or BOLT\n> that all nodes could adopt for multi-network, multi-asset LN-as-rails use\n> cases.\n\nGiven that all the specs so far are in the open, and we opted to first build\nout the specifications before releasing our own implementation, I don't\nforesee Taro being something that only LL or lnd implements. All the BIPs\nare public, and the bLIP will be soon as well, so any motivated individual\nor set of individuals will also be able to implement and adopt the protocol.\nIf you or anyone else reading this is interested in contributing: I'm\naccepting PRs to my fork of the BIP repo [1] (where I've already made\nseveral modifications based on feedback from the wider community, and merged\na few PRs as well), and I'm also hanging out on IRC at ##taro on Libera.\n\n[1]: https://github.com/Roasbeef/bips/tree/bip-taro\n\n-- Laolu\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220502/d9b6768e/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-05-02T23:18:56",
                "message_text_only": "Good morning John, and Laolu,\n\n> > but instead the requirement to add several feature concepts to LN that\n> > would allow tokens to interact with LN nodes and LN routing:\n>\n> From this list of items, I gather that your vision is actually pretty\n> different from ours. Rather than update the core network to understand the\n> existence of the various Taro assets, instead we plan on leaving the core\n> protocol essentially unchanged, with the addition of new TLV extensions to\n> allow the edges to be aware of and interact w/ the Taro assets. As an\n> example, we wouldn't need to do anything like advertise exchange rates in\n> the core network over the existing gossip protocol (which doesn't seem like\n> the best idea in any case given how quickly they can change and the existing\n> challenges we have today in ensuring speedy update propagation).\n\nAdding on to this, the American Call Option problem that arises when using H/PTLCs: https://lists.linuxfoundation.org/pipermail/lightning-dev/2018-December/001752.html\n\nThe above objection seems to be one reason for proposing multi-asset \"on the edge\" rather than have it widely deployed in the published Lightning Network.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "John Carvalho",
                "date": "2022-05-03T05:35:59",
                "message_text_only": "Zman,\n\nI was not arguing for moving things from the edge, nor was I arguing to\nmake Taro a BOLT. Laolu is misinterpreting my message.\n\nI was explaining that the capabilities that would allow Taro to interact\nwith LN have no special relationship to Taro alone and should be designed\nto accommodate any outside layer/network.\n\nI gave specific examples of requirements that LL is portraying as Taro\nLayer design, that are really just new features for LN nodes that do not\nneed to be network/layer-specific:\n\n - Making LN nodes aware of assets on other networks\n - Establishing commitments for (atomic) swapping for payments/routing\n - Supporting the ability to exchange and advertise exchange rates for\nasset pairs\n - Supporting other multi-asset routes when considering routing paths,\nbridging nodes with alternate assets\n\nI don't care whether this is framed as BOLT or BLIP content, as in the end\neach implementation will do what it needs to stay relevant in the market. I\ncare that this is framed and designed correctly, so we aren't locked into\none specific outside layer. You could argue the degree to which the above\nfeatures need to exist in the network, and whether to restrict such\nfeatures to the \"edge,\" but my point is that an LN node that wants to be\naware of an outside network, and extra assets in addition to Bitcoin, will\nneed such features, and such features are not Taro-specific.\n\n\nGood morning John, and Laolu,\n>\n> > > but instead the requirement to add several feature concepts to LN that\n> > > would allow tokens to interact with LN nodes and LN routing:\n> >\n> > From this list of items, I gather that your vision is actually pretty\n> > different from ours. Rather than update the core network to understand\n> the\n> > existence of the various Taro assets, instead we plan on leaving the core\n> > protocol essentially unchanged, with the addition of new TLV extensions\n> to\n> > allow the edges to be aware of and interact w/ the Taro assets. As an\n> > example, we wouldn't need to do anything like advertise exchange rates in\n> > the core network over the existing gossip protocol (which doesn't seem\n> like\n> > the best idea in any case given how quickly they can change and the\n> existing\n> > challenges we have today in ensuring speedy update propagation).\n>\n> Adding on to this, the American Call Option problem that arises when using\n> H/PTLCs:\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2018-December/001752.html\n>\n> The above objection seems to be one reason for proposing multi-asset \"on\n> the edge\" rather than have it widely deployed in the published Lightning\n> Network.\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220503/d3b236c6/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-05-03T08:25:22",
                "message_text_only": "Good morning John,\n\nThank you for clarifying.\n\n> Zman,\n> I was not arguing for moving things from the edge, nor was I arguing to make Taro a BOLT. Laolu is misinterpreting my message.\n> I was explaining that the capabilities that would allow Taro to interact with LN have no special relationship to Taro alone and should be designed to accommodate any outside layer/network.\n> I gave specific examples of requirements that LL is portraying as Taro Layer design, that are really just new features for LN nodes that do not need to be network/layer-specific:\n> - Making LN nodes aware of assets on other networks- Establishing commitments for (atomic) swapping for payments/routing- Supporting the ability to exchange and advertise exchange rates for asset pairs- Supporting other multi-asset routes when considering routing paths, bridging nodes with alternate assets\n> I don't care whether this is framed as BOLT or BLIP content, as in the end each implementation will do what it needs to stay relevant in the market. I care that this is framed and designed correctly, so we aren't locked into one specific outside layer. You could argue the degree to which the above features need to exist in the network, and whether to restrict such features to the \"edge,\" but my point is that an LN node that wants to be aware of an outside network, and extra assets in addition to Bitcoin, will need such features, and such features are not Taro-specific.\n\nMy understanding here of \"the edge\" vs \"the core\" is that the core is responsible for multi-hop routes and advertisements for channels.\nThus the below:\n\n> - Supporting the ability to exchange and advertise exchange rates for asset pairs\n> - Supporting other multi-asset routes when considering routing paths, bridging nodes with alternate assets\n\n... would be considered part of \"the core\".\n\nNotwithstanding the previously linked objection against a multi-asset Lightning Network, we can discuss these as two topics:\n\n* Advertising exchange rates.\n* Routing between channels of different asset types.\n\n### Advertising Exchange Rates\n\nWithout changing the BOLT protocol, we can define a particular odd featurebit that cross-asset exchanges can set.\nThen, odd-numbered messages can be defined, such that I can ask that node:\n\n* What assets it has on what channels.\n* Exchange rates of each asset to Bitcoin in msats (to serve as a common exchange rate to allow conversion from any one asset to any other asset, specifying only N exchange rates instead of N^2).\n  * We also need to spec out any rounding algorithm, in order to have the same calculation across implementations.\n\nBOLT is flexible enough that this does not need to be \"blessed\" until more than one LN implementation agrees on the new spec.\n\n### Routing Between Channnels Of Different Asset Types\n\nI was the one who first suggested dropping the `realm` byte.\n\nOriginally, `realm` was a 1-byte identifier for the asset type.\n\nHowever, I pointed out that `realm` was simultaneously too large and too small.\n\n* Too Large: We needed a byte in order to allow the new \"TLV\" thing to be used in routing onions. so that we could specify how many sections the TLV thing would take up, and we had already taken up all the space in a typical IP packet for the onion.\n* Too Small: If multi-asset actually materializes, it is hard to imagine that there would be only 255 of them (`realm = 0` was already for Bitcoin, so there were only 255 possible identifiers left).\n\nThe idea in my mind basically was that instead of using the `realm` byte for identifying asset, we would instead add a new type for TLV, which would have 20 bytes.\nThese 20 bytes would be, say, RIPEMD160 . SHA256 of the name of the asset.\n\nOdd TLV types are ignored, but individual onion layers are targeted to specific nodes anyway, so it should be safe to use an even TLV type instead for this.\n\n--\n\nAgain, note that this is a change in \"the core\" (and thus, pedantically, you *are* arguing for moving it from the edge, if you want these two items you specified).\nI personally think it dubious to consider, for the reason that I already linked to in the previous reply, but in any case, it is indeed possible to do.\n\nGenerally, the path towards updating the BOLT is for at least one implementation to actually implement this, then convince at least one other implementation that this makes sense (possibly via this mailing list), and *then* maybe you have a chance of it getting into the BOLT spec.\nYou may find it more useful to e.g. hire a freelancer to work on this for`lnd` and get it merged.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Taro - Separating Taro concerns from LN token concerns",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "John Carvalho",
                "Olaoluwa Osuntokun",
                "ZmnSCPxj"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 17376
        }
    },
    {
        "title": "[Lightning-dev] [bitcoin-dev] [Pre-BIP] Fee Accounts",
        "thread_messages": [
            {
                "author": "Jeremy Rubin",
                "date": "2022-05-02T15:59:49",
                "message_text_only": "Ok, got it. Won't waste anyone's time on terminology pedantism.\n\n\nThe model that I proposed above is simply what *any* correct timestamping\nservice must do. If OTS does not follow that model, then I suspect whatever\nOTS is, is provably incorrect or, in this context, unreliable, even when\nservers and clients are honest. Unreliable might mean different things to\ndifferent people, I'm happy to detail the types of unreliability issue that\narise if you do not conform to the model I presented above (of which,\nlinearizability is one way to address it, there are others that still\nimplement epoch based recommitting that could be conceptually sound without\nrequiring linearizability).\n\nDo you have any formal proof of what guarantees OTS provides against which\nthreat model? This is likely difficult to produce without a formal model of\nwhat OTS is, but perhaps you can give your best shot at producing one and\nwe can carry the conversation on productively from there.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220502/af0d2b92/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Fee Accounts",
            "categories": [
                "Lightning-dev",
                "bitcoin-dev",
                "Pre-BIP"
            ],
            "authors": [
                "Jeremy Rubin"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1151
        }
    },
    {
        "title": "[Lightning-dev] Blind Signing Considered Harmful",
        "thread_messages": [
            {
                "author": "Devrandom",
                "date": "2022-05-09T19:04:26",
                "message_text_only": "( a formatted version of this message is here:\nhttps://gitlab.com/lightning-signer/docs/-/wikis/Blind%20Signing%20Considered%20Harmful\n)\n\n# Introduction\n\nThis post discusses blind signers.  Blind signers do not put the user in\ncontrol of their funds and are subject to a long list of exploits.\n\nThis post also (re-)introduces the open-source [Validating Lightning\nSigner](https://gitlab.com/lightning-signer/docs/-/blob/master/README.md)\nProject.\n\n# Background\n\nA **Signer** is a component that performs cryptographic operations,\nseparately from a wallet. A Bitcoin hardware wallet is an example of a\nSigner, where private keys are controlled on a hardened device. There is\ncurrently no complete solution for a hardware signer for the Lightning\nnetwork.\n\nA **Blind Signer** is a signer that does not perform validation. There are\nseveral Lightning wallets and node implementations that as of today support\nonly blind signing. I believe these configurations are insecure.\n\nA **Validating Signer** performs a comprehensive set of policy checks to\nensure that the keys are not misused. For example, a validating Bitcoin\nhardware wallet checks the destination, amount and change outputs in\ncollaboration with the user.\n\nA layer-2 validating signer is significantly more complex, because of the\ncomplexity of the Lightning protocol.\n\n**While a Blind Signer is a technical step on the road to the higher\nsecurity of a Validating Signer, by itself it actually reduces security if\ndeployed in production. This is because it presents two points of attack -\nat the node and at the signer.**\n\n# The VLS Project\n\nThe [Validating Lightning Signer](\nhttps://gitlab.com/lightning-signer/docs/-/blob/master/README.md) project\naims to close the gap for securing the Lightning ecosystem. It is an\nopen-source Rust library and reference implementation. The project is\napproaching Beta, which is the point where the main goal will be met: funds\nare safe even if the node is completely compromised.\n\nThe task is relatively complex because of the complexity of the Lightning\nprotocol. There are more than [50 policies](\nhttps://gitlab.com/lightning-signer/docs/-/blob/master/policy-controls.md)\nthat must be enforced, and many of them require stateful inspection of the\nprotocol.\n\nBoth servers and consumer devices are targeted, the latter via a Rust\n`no_std` compilation mode.\n\n# Signing Configurations\n\nHere are some of the potential configurations of a Lightning node:\n\n* Monolithic node\n* Node with a separate Blind Signer\n* Node with a separate Validating Signer - the signer ensures that the\nLightning state machine ran correctly and funds are not at risk\n\n# The (In)security of Blind Signing\n\n![blind-signing-diagram-1.svg](uploads/78db1bd2b59228492e09ea272c873cf3/blind-signing-diagram-1.svg)\n\n* The monolithic case has one point of attack - at the node.\n* The blind signing case has **two points of attack** - at the node and at\nthe Signer. A blind signer will perform any signing operation the node\nrequests, so **a compromised node will still result in loss of funds**. And\nobviously, a compromised signer will also result in loss of funds. This is\nworse than a monolithic node because funds can be lost if **either** is\ncompromised.\n* The validated signing case has just one point of attack with a small\nattack surface\n\n# Wallets with Blind Signers Must Trust the Node Operator\n\nBlind signing wallets where the node is run by an LSP (Lightning Service\nProvider) are not self-custodial because the LSP can unilaterally control\nthe funds. The LSP merely has to provide the Signer with a transaction that\nsends the funds to the LSP or another destination.\n\n# Examples of Blind Signing Exploits\n\nA compromised node can unilaterally submit transactions to be signed by the\nblind Signer.  The following can result in the funds being stolen:\n\n* The node submits a mutual closing transaction which sends funds to the\nattacker's address\n* The node asks the blind signer to sign a revoked transaction which will\ncause loss of all funds when published\n* And many more ...\n\nA compromised node can also lose funds when it doesn't follow the Lightning\nprotocol. Some potential exploits include:\n\n* The node fails to validate the counter-party's revocation, and the\ncounter-party broadcasts an old commitment transaction that sends most of\nthe funds to the counter-party\n* The node fails to claim input HTLCs when routing payments, leading to the\ngradual loss of all funds\n* And many more ...\n\n# Validating Signers\n\nIn the Validating Signer case, a compromise of the Lightning node will not\nresult in the loss of funds. The security of such a setup is only dependent\non the security of the Signer. The Signer can be hardened as needed for the\nspecific use case.\n\nSome of the validation rules that a validated Signer can implement include:\n\n- Don't sign a revoked commitment transaction\n- Don't revoke a signed commitment transaction\n- Don't close a channel to an unapproved destination\n- Routed payments must have at least as much input as output value\n- Payments must claim at least as much from the input as was claimed from\nus on the output\n- And many more ...\n\n# Conclusion\n\nBlind signers reduce the security of Lightning nodes and are subject to\n[many exploits](\nhttps://gitlab.com/lightning-signer/docs/-/wikis/Potential-Exploits).\n\nValidating signers improve security by reducing the attack surface. The VLS\nproject aims to provide a library and reference implementation for\nenterprise servers and consumer devices.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220509/95d3c85d/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-05-10T00:40:26",
                "message_text_only": "Good morning devrandom,\n\nIt seems to me that a true validating Lightning signer would need to be a Bitcoin node with active mitigation against eclipse attacks, the ability to monitor the blockheight, and the ability to broadcast transactions.\n\nOtherwise, a compromised node can lie and tell the signer that the block height is much lower than it really is, letting the node peers clawback incoming HTLCs and claim outgoing HTLCs, leading to a net loss of funds in the forwarding case.\n\nLooking at the link, it seems to me that you have a \"UTXO Set Oracle\", does this inform your `lightning-signer` about block height and facilitate transaction broadcast?\nIs this intended to be a remote device from the `lightning-signer` device?\nIf so, what happens if the connection between the \"UTXO Set Oracle\" remote device and the `lightning-signer` is interrupted?\n\nIn particular:\n\n* Incoming forward arrives.\n* Compromised node accepts the incoming HTLC and offers outgoing HTLC.\n  * Presumably the `lightning-signer` signs off on this, as long as the outgoing HTLC is of lower value etc etc.\n* Compromised node stops communicating with the `lightning-signer`.\n* Outgoing HTLC times out, but compromised node and the outgoing peer do nothing.\n* Incoming HTLC times out, and the incoming peer unilaterally closes the channel, claiming the timelock branch of the HTLC onchain.\n* Outgoing peer unilaterally closes the channel, claiming the hashlock branch of the outgoing HTLC onchain.\n\nUnless the `lightning-signer` unilaterally closes the channel when the outgoing HTLC times out and actively signs and broadcasts the timelock branch for the outgoing HTLC, then this leads to funds loss.\nThis requires that the `lightning-signer` be attached to a Bitcoin node that is capable of:\n\n* Actively finding and connecting to multiple Bitcoin peers.\n* Actively checking the block header chain (acceptable at only SPV security since you really only care about blockheight, and have a UTOX Set Oracle which upgrades the rest of your security from SPV to full?).\n* Actively broadcasting unilateral closes and HTLC timelock claims for outgoing HTLCs.\n\nIs that how `lightning-signer` is designed?\n\nThis seems to be listed in: https://gitlab.com/lightning-signer/docs/-/wikis/Potential-Exploits\n\n> an HTLC is failed and removed on the input before it is removed on the output.  The output is then claimed by the counterparty, losing that amount\n\nIs there a mitigation, planned or implemented, against this exploit?\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Devrandom",
                "date": "2022-05-10T08:02:35",
                "message_text_only": "Good morning ZmnSCPxj,\n\nOn Mon, May 9, 2022 at 5:40 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning devrandom,\n>\n> It seems to me that a true validating Lightning signer would need to be a\n> Bitcoin node with active mitigation against eclipse attacks, the ability to\n> monitor the blockheight, and the ability to broadcast transactions.\n>\n\nThe UTXO Oracle(s) have some additional properties that improve on a plain\nBitcoin node:\n\n- they provide compact attestations that can be checked in an\nisolated/hardened environment, so that the signer doesn't need to be\nexposed to a network stack and can live on a consumer device or HSM\n- a UTXO Oracle can send out attestations on a broadcast medium (e.g. live\nbehind Tor, satellite, etc.), so it's harder to block or eclipse\n- the periodic attestation would cover the current block hash, and a\ncommitment for the UTXO set hash (e.g. hash of the compact filter history)\n\nBroadcast is a separate concern.  For broadcast, the intent is to have a\ndisaster recovery procedure:\n\n- the signer sends out a heartbeat if it has a quorum of non-stale oracle\nattestations and there are no upcoming safety deadlines (e.g. HTLC timeout\nor need to breach-remedy)\n- the heartbeats form a \"deadman's switch\" - if the node operator doesn't\nget them, they spring into action\n- the operator falls back to sneakernet for broadcasting a\nclosing/breach-remedy transaction if needed\n\n\n>\n> Otherwise, a compromised node can lie and tell the signer that the block\n> height is much lower than it really is, letting the node peers clawback\n> incoming HTLCs and claim outgoing HTLCs, leading to a net loss of funds in\n> the forwarding case.\n>\n\nRight, routing nodes really need to be aware of the on-chain status of\nincoming channels.\n\n\n>\n> Looking at the link, it seems to me that you have a \"UTXO Set Oracle\",\n> does this inform your `lightning-signer` about block height and facilitate\n> transaction broadcast?\n> Is this intended to be a remote device from the `lightning-signer` device?\n>\n\nIt could be a quorum of systems, remote from the signer device, some or all\nof which may be under third-party operational control.  The attestation for\na certain block is deterministic, so they should all agree.\n\n\n> If so, what happens if the connection between the \"UTXO Set Oracle\" remote\n> device and the `lightning-signer` is interrupted?\n>\n\nIf there is no quorum, heartbeats would cease, which would alert the\noperator to start disaster recovery.\n\n\n>\n> In particular:\n> [...]\n>\n\nAgreed, that's the main attack scenario on a router that doesn't properly\nchain the on-chain status of an input channel.\n\nThis seems to be listed in:\n> https://gitlab.com/lightning-signer/docs/-/wikis/Potential-Exploits\n>\n> > an HTLC is failed and removed on the input before it is removed on the\n> output.  The output is then claimed by the counterparty, losing that amount\n>\n> Is there a mitigation, planned or implemented, against this exploit?\n>\n\nYes, heartbeats would cease and the operator would manually intervene as\nabove.\n\nThat said, there is another potential mode:  if you limit the value of\nHTLCs in flight (e.g. 5% of channel value), are willing to lose that\namount, you don't do routing, and you have watchtowers, then you can live\nwithout the UTXO Oracle component.  This may be acceptable in a consumer\napplication.\n\nFinally, we could essentially embed UTXO Oracles into the network if:\n\n- we commit to compact-filters or utreexo roots in the consensus\n- we are OK with SPV-style security, where we detect an eclipse by noticing\na reduction in block rate\n\nBut it's hard to say when the first item might be plausibly deployed.\n\n--\ndevrandom\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220510/b1cb218a/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Blind Signing Considered Harmful",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "Devrandom"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 12045
        }
    },
    {
        "title": "[Lightning-dev] Invitation to test our research on probabilistic and optimal payment flows. I made it quick & easy for you (:",
        "thread_messages": [
            {
                "author": "Ren\u00e9 Pickhardt",
                "date": "2022-05-12T11:57:43",
                "message_text_only": "Dear fellow lightning developers,\n\nlast week I have started a new repository [0] where I maintain a python\npackage that can be used to test (and more importantly simulate) the\nimprovements to payment delivery that we have suggested over the last years\n(c.f. [1][2][3]). I kindly invite you to check it out.\n\nFeedback & code review will be highly appreciated if you find the spare\ntime to do so. Similarly I will be delighted if you can provide a patch or\nan extension if necessary. Note while this may already be very useful that\nthis is work in progress and there are already quite some issues open [4]\nand I am supervising several Summer of Bitcoin projects [5] that I expect\nto contribute to the repository over the next couple of weeks in various\nways.\n\nYou can easily install the **pre-alpha** package (version 0.0.0) with the\nPythonPackageIndex via:\n\n:~$ pip install pickhardtpayments\n\nExample code that shows how to use this library can be found in the\n`Readme.md` file and the `example` folder [6]. Unfortunately my\npresentation at MIT Bitcoin Expo where I first talked about this is still\nburied in a long Livestream video (Day 2 Track A starts at 1:16:19) and is\nnot yet available as a stand alone video but I will maintain a\n`Resources.md` file where I link to useful talks & articles [7] and other\nresources related to the topic.\n\nOne final and very significant way to speed up the computation of the min\ncost flow solver that I have included since my last mail [3] was achieved\nby pruning the network to remove edges that are highly unlikely to be part\nof the payment flow. The current solution is pretty ad-hoc but I am working\non doing this in a more automated / reliable way[8]. In this way I am very\npleased to report that as of now and with the given size of the zeroBaseFee\nsubnet...\n\n**...The min cost flow solver consistently takes less that 100ms to find\nclose to optimal flows!**\n\nThat being said: Please be aware that the entire runtime of the provided\ncode is currently much higher. The reason is that for mere convenience I\nhave used the very slow `networkx` library that is used to store the\nChannelGraph, UncertaintyNetwork and OracleLightningNetwork with a lot of\nmemory overhead to maintain those data structures and to copy the necessary\ndata to the min cost flow solver before starting the actual solver (c.f.\n[9]).\n\nFor safety reasons I deliberately did not provide an API to do mainnet\ntests. However to the fellow expert user - who knows what they are doing -\nit should be fairly straight forward to conduct mainnet experiments via the\nfollowing two steps:\n\n1.) Create your own wrapper to an Lightning Node that exposes the\n`send_onion` call [10] in the same way how it is currently being done in\nthe `OracleLightingNetwok` class and bring your own wrapper to the\n`SyncSimulatedPaymentSession` class instead of the oracle that we have\n\n2.) You may wish to make the payment loop [11] inside the\n`SyncSimulatedPaymentSession` async.\n\nOf course delivering production ready mainnet code will require more work\nas one needs to figure out other constraints like channel reserves, min/max\nhtlc sizes, available HTLC slots, offline peers, hanging htlcs (stuck\npayments),...\n\nAlso if you want to do simulations with non uniform distributions of\nliquidity or because your LSP has a crawled snapshot of actual Liquidity\nyou can just bring your own `OracleLightningNetwork` that encodes your\nassumed or known ground truth about the network.\n\nI hope all of this makes sense. Feel free to ask me anything that you need\nand I am curious to see if this will be useful for you. Note that there is\na notebook that has basically the same code but more documentation and in\nparticular a glossary of terms that can currently be found at [12].\n\nThanks to all people that I have been collaborating with so far and to\nNTNU, BitMEX, various anonymous donors and Patreons who all helped to\nachieve this.\n\nwith kind regards Rene Pickhardt\n\n[0]: https://github.com/renepickhardt/pickhardtpayments\n[1]:\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2021-March/002984.html\n[2]: https://arxiv.org/abs/2107.05322\n[3]:\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2022-March/003510.html\n[4]: https://github.com/renepickhardt/pickhardtpayments/issues\n[5]:\nhttps://www.summerofbitcoin.org/project-ideas-details?recordId=recJchpFa9tqSZkQ4\n[6]: https://github.com/renepickhardt/pickhardtpayments/tree/main/examples\n[7]:\nhttps://github.com/renepickhardt/pickhardtpayments/blob/main/Resources.md\n[8]: https://github.com/renepickhardt/pickhardtpayments/issues/1\n[9]: https://github.com/renepickhardt/pickhardtpayments/issues/6\n[10]:\nhttps://github.com/renepickhardt/pickhardtpayments/blob/1121b48ec6bf5fb2dee2b1793f87d489ce3149e3/pickhardtpayments/OracleLightningNetwork.py#L37\n\n[11]:\nhttps://github.com/renepickhardt/pickhardtpayments/blob/1121b48ec6bf5fb2dee2b1793f87d489ce3149e3/pickhardtpayments/SyncSimulatedPaymentSession.py#L351\n[12]:\nhttps://github.com/renepickhardt/mpp-splitter/blob/pickhardt-payments-simulation-dev/Pickhardt-Payments-Simulation.ipynb\n(includes\na glossary of terms and many explainations)\n\n-- \nhttps://ln.rene-pickhardt.de  <https://ln.rene-pickhardt.de>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220512/4eb88b86/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Invitation to test our research on probabilistic and optimal payment flows. I made it quick & easy for you (:",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Ren\u00e9 Pickhardt"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 5387
        }
    },
    {
        "title": "[Lightning-dev] #PickhardtPayments implemented in lnd-manageJ",
        "thread_messages": [
            {
                "author": "Carsten Otto",
                "date": "2022-05-15T20:01:58",
                "message_text_only": "Dear all,\n\nthe most recent version of lnd-manageJ [1] now includes basic, but usable,\nsupport for #PickhardtPayments. I kindly invite you to check out the code, give\nit a try, and use this work for upcoming experiments.\n\nTeaser with video: https://twitter.com/c_otto83/status/1525879972786749453\n\nThe problem, heavily summarized:\n\n- Sending payments in the LN often fails, especially with larger amounts.\n- Splitting a large payment into smaller shards (using MPP) is helpful,\n  as in general the smaller shards don't fail as often as a single large\n  payment would. However, the success probability also drops\n  exponentially with the number of channels included [2].\n- Finding routes through the LN is tricky, as the channels' liquidity is\n  uncertain at the time of computing the routes and a simple \"trial and\n  error\" approach might take too long.\n- There are several implementations using various heuristics, taking\n  aspects like fees, previous failures, HTLC deltas, channel age, ...\n  into account. Comparing these approaches is very hard.\n\nThe gist of #PickhardtPayments:\n\n- The issue of finding MPP routes in the LN corresponds to the\n  well-known minimum-cost flow problem in computer science (graph\n  theory) with lots of related research, results, algorithms, ...\n- As shown in the paper [3] the results are optimal, no matter which\n  \"cost\" function is used to reason about routing costs (fees) and/or\n  reliability. However, depending on the characteristics of the\n  function, actually finding optimal results can be extremely hard\n  (NP-complete in some cases). By imposing the zero base fee limitation\n  and assuming a uniform distribution of funds, fast implementations\n  (polynomial time with sub-second runtimes) can be used.\n- Assuming (!) a uniform distribution of funds in each channel and zero\n  base fee only, #PickhardtPayments offers an approach that is optimal,\n  i.e. proven perfect and computationally feasible.\n- The most basic version only considers uncertainty costs for\n  reliability, but it is possible (and implemented in lnd-manageJ) to\n  also consider routing costs (fee rates) and optimize for both features\n  to come up with reliable and cheap-ish MPPs.\n- The implementation of #PickhardtPayments in lnd-manageJ needs to\n  ignore non-zero base fee channels to avoid extremely slow\n  (NP-complete) computations. Furthermore, certain aspects are\n  approximated [4]. As such, optimality claims are limited to the zero\n  base fee subset of the LN, and real-world experiments might be tricky\n  to interpret. However, as also shown in the testnet videos [5][6],\n  first results are very promising!\n\nThe real strength of #PickhardtPayments:\n\n- Liquidity information, for example obtained by previous failures, is\n  taken into account. For each attempt, the relevant bits of information\n  are obtained and will be used to guide the following attempts.\n- As the underlying algorithm is proven to be optimal, we do not need to\n  rely on heuristics. Instead, the algorithm happily finds routes that\n  may be very long (but very probable/cheap, for whatever reason), have\n  a surprising number of shards, or rather odd amounts.\n- The underlying algorithm also deals with shared segments, i.e.\n  individual channels that are used for more than one shard of the MPP,\n  without oversaturating it.\n\nThe code in lnd-manageJ:\n\n- Highly experimental, but it's a start!\n- lnd + gRPC middleware + Java/Spring + PostgreSQL is a bit more complex\n  than necessary.\n- Only works with lnd.\n- Doesn't really work with lnd until issue #5746 [7] is fixed. I'd be\n  very happy for someone to have a look at my proposal (PR #6543 [8])!\n- The code doesn't handle all corner cases, especially the\n  less-than-usual failure codes. For example, if a node decides to raise\n  the fee rate, the corresponding channel will be ignored for a while as\n  I'm too lazy to think about how to update the information in the data\n  structure used to compute the MPP.\n- Pending shards (neither failed nor settled) just cause the whole MPP\n  to hang until something times out. Most likely this can't be fixed\n  without stuckless payments?\n\nI'd be very happy to discuss implementation details (not on this list, I\nguess?) and help with upcoming (mainnet?) benchmarks and experiments\n(Ren\u00e9 Pickhardt is also interested in helping with this). Given a fix\nfor the blocking lnd issue, I'd be happy to let my node c-otto.de take\npart in some experiments.\n\nLast but not least, a huge thank you to Ren\u00e9 Pickhardt for lots of\ninsightful discussions, proof reading, and of course (together with\nStefan Richter) the actual work of coming up with #PickhardtPayments!\n\nBest regards,\nCarsten\n\n1: https://github.com/C-Otto/lnd-manageJ/blob/main/PickhardtPayments.md\n2: https://arxiv.org/abs/2103.08576\n3: https://arxiv.org/abs/2107.05322\n4: https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-March/003510.html\n5: https://c-otto.de/pp/pp.mp4\n6: https://c-otto.de/pp/lnd.mp4\n7: https://github.com/lightningnetwork/lnd/issues/5746\n8: https://github.com/lightningnetwork/lnd/pull/6543\n-- \nDr. Carsten Otto\ncarsten at c-otto.de\nhttps://c-otto.de\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 195 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220515/d3f70c90/attachment.sig>"
            },
            {
                "author": "Matt Corallo",
                "date": "2022-05-16T20:59:05",
                "message_text_only": "Its probably worth somewhat disentangling the concept of switching to a minimum-cost flow routing \nalgorithm from the concept of \"scoring based on channel value and estimated available liquidity\".\n\nThese are two largely-unrelated concepts that are being mashed into one in this description - the \nfirst concept needs zero-base-fee to be exact, though its not clear to me that a heuristics-based \napproach won't give equivalent results in practice, given the noise in success rate compared to \ntheory here.\n\nThe second concept is something that LDK (and I believe CLN and maybe even eclair now) do already, \nthough lnd does not last I checked. For payments where MPP does not add much to success rate (i.e. \npayments where the amount is relatively \"low\" compared to available network liquidity) dijkstra's \nwith a liquidity/channel-size based scoring will give you the exact same result.\n\nFor cases where you're sending an amount which is \"high\" compared to available network liquidity, \ntaking a minimum-cost-flow algorithm becomes important, as you point out. Of course you're always \ngoing to suffer really slow payment and many retires in this case anyway.\n\nMatt\n\nOn 5/15/22 1:01 PM, Carsten Otto via Lightning-dev wrote:\n> Dear all,\n> \n> the most recent version of lnd-manageJ [1] now includes basic, but usable,\n> support for #PickhardtPayments. I kindly invite you to check out the code, give\n> it a try, and use this work for upcoming experiments.\n> \n> Teaser with video: https://twitter.com/c_otto83/status/1525879972786749453\n> \n> The problem, heavily summarized:\n> \n> - Sending payments in the LN often fails, especially with larger amounts.\n> - Splitting a large payment into smaller shards (using MPP) is helpful,\n>    as in general the smaller shards don't fail as often as a single large\n>    payment would. However, the success probability also drops\n>    exponentially with the number of channels included [2].\n> - Finding routes through the LN is tricky, as the channels' liquidity is\n>    uncertain at the time of computing the routes and a simple \"trial and\n>    error\" approach might take too long.\n> - There are several implementations using various heuristics, taking\n>    aspects like fees, previous failures, HTLC deltas, channel age, ...\n>    into account. Comparing these approaches is very hard.\n> \n> The gist of #PickhardtPayments:\n> \n> - The issue of finding MPP routes in the LN corresponds to the\n>    well-known minimum-cost flow problem in computer science (graph\n>    theory) with lots of related research, results, algorithms, ...\n> - As shown in the paper [3] the results are optimal, no matter which\n>    \"cost\" function is used to reason about routing costs (fees) and/or\n>    reliability. However, depending on the characteristics of the\n>    function, actually finding optimal results can be extremely hard\n>    (NP-complete in some cases). By imposing the zero base fee limitation\n>    and assuming a uniform distribution of funds, fast implementations\n>    (polynomial time with sub-second runtimes) can be used.\n> - Assuming (!) a uniform distribution of funds in each channel and zero\n>    base fee only, #PickhardtPayments offers an approach that is optimal,\n>    i.e. proven perfect and computationally feasible.\n> - The most basic version only considers uncertainty costs for\n>    reliability, but it is possible (and implemented in lnd-manageJ) to\n>    also consider routing costs (fee rates) and optimize for both features\n>    to come up with reliable and cheap-ish MPPs.\n> - The implementation of #PickhardtPayments in lnd-manageJ needs to\n>    ignore non-zero base fee channels to avoid extremely slow\n>    (NP-complete) computations. Furthermore, certain aspects are\n>    approximated [4]. As such, optimality claims are limited to the zero\n>    base fee subset of the LN, and real-world experiments might be tricky\n>    to interpret. However, as also shown in the testnet videos [5][6],\n>    first results are very promising!\n> \n> The real strength of #PickhardtPayments:\n> \n> - Liquidity information, for example obtained by previous failures, is\n>    taken into account. For each attempt, the relevant bits of information\n>    are obtained and will be used to guide the following attempts.\n> - As the underlying algorithm is proven to be optimal, we do not need to\n>    rely on heuristics. Instead, the algorithm happily finds routes that\n>    may be very long (but very probable/cheap, for whatever reason), have\n>    a surprising number of shards, or rather odd amounts.\n> - The underlying algorithm also deals with shared segments, i.e.\n>    individual channels that are used for more than one shard of the MPP,\n>    without oversaturating it.\n> \n> The code in lnd-manageJ:\n> \n> - Highly experimental, but it's a start!\n> - lnd + gRPC middleware + Java/Spring + PostgreSQL is a bit more complex\n>    than necessary.\n> - Only works with lnd.\n> - Doesn't really work with lnd until issue #5746 [7] is fixed. I'd be\n>    very happy for someone to have a look at my proposal (PR #6543 [8])!\n> - The code doesn't handle all corner cases, especially the\n>    less-than-usual failure codes. For example, if a node decides to raise\n>    the fee rate, the corresponding channel will be ignored for a while as\n>    I'm too lazy to think about how to update the information in the data\n>    structure used to compute the MPP.\n> - Pending shards (neither failed nor settled) just cause the whole MPP\n>    to hang until something times out. Most likely this can't be fixed\n>    without stuckless payments?\n> \n> I'd be very happy to discuss implementation details (not on this list, I\n> guess?) and help with upcoming (mainnet?) benchmarks and experiments\n> (Ren\u00e9 Pickhardt is also interested in helping with this). Given a fix\n> for the blocking lnd issue, I'd be happy to let my node c-otto.de take\n> part in some experiments.\n> \n> Last but not least, a huge thank you to Ren\u00e9 Pickhardt for lots of\n> insightful discussions, proof reading, and of course (together with\n> Stefan Richter) the actual work of coming up with #PickhardtPayments!\n> \n> Best regards,\n> Carsten\n> \n> 1: https://github.com/C-Otto/lnd-manageJ/blob/main/PickhardtPayments.md\n> 2: https://arxiv.org/abs/2103.08576\n> 3: https://arxiv.org/abs/2107.05322\n> 4: https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-March/003510.html\n> 5: https://c-otto.de/pp/pp.mp4\n> 6: https://c-otto.de/pp/lnd.mp4\n> 7: https://github.com/lightningnetwork/lnd/issues/5746\n> 8: https://github.com/lightningnetwork/lnd/pull/6543\n> \n> \n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2022-05-17T07:23:07",
                "message_text_only": "I completely agree with Matt, these two components are completely\nindependent\nand too often conflated. Scoring channels and estimating liquidity is\nsomething\nthat has been regularly discussed by implementations for the last few years,\nwhere every implementation did its own experiments over time.\n\nEclair has quite a large, configurable set of heuristics around channel\nscoring,\nalong with an A/B testing system that we've been using for a while on\nmainnet\n(see [1] for details). We've also been toying with channel liquidity\nestimation for\nmore than half a year, which you can follow in [2] and [3].\n\nThese are heuristics, and it's impossible to judge whether they work or not\nuntil\nyou've tried them on mainnet with real payments, so I strongly encourage\npeople\nto run such experiments. But when you do, you should have enough volume for\nthe result data to be statistically meaningful and you should do A/B\ntesting,\notherwise you can make the data say pretty much everything you want. What\nI believe is mostly missing is the volume, the network doesn't have enough\nreal\npayments yet IMHO for this data to accurately say that one heuristic is\nbetter\nthan another.\n\nUsing an MCF algorithm instead of dijkstra is useful when relaying large\npayments\nthat will need to be split aggressively to reach the destination. It does\nmake a lot\nof sense in that scenario. However, it's important to also take a step back\nand\nlook at whether it is economical to make such payments on lightning.\n\nFor a route with an aggregated proportional fee of 1000ppm, here is a rough\ncomparison of the fees between on-chain and lightning:\n\n* At 1 sat/byte on-chain, payments above 2mBTC cost less on-chain than\noff-chain\n* At 10 sat/byte on-chain, payments above 20mBTC cost less on-chain than\noff-chain\n* At 25 sat/byte on-chain, payments above 50mBTC cost less on-chain than\noff-chain\n* And so on (just keep multiplying)\n\nOf course, making payments on lightning has more benefits than just fees,\nthey\nalso confirm faster than on-chain payments, but I think it's important to\nkeep these\nfigures in mind.\n\nIt would be also useful to think about the shape of the network. Using an\nMCF\nalgorithm makes sense when payments are saturating channels. But if channels\nare much bigger than your payment size, this is probably overkill. If\nchannels are\nsmall \"at the edges of the network\" and bigger than payments at the \"core\nof the\nnetwork\", and we're using trampoline routing [4], it makes sense to run\ndifferent\npath-finding algorithms depending on where we are (e.g. MCF at the edges on\na small subset of the graph and dijkstra inside the core).\n\nI'm very happy that all this research is happening and helping lightning\npayments\nbecome more reliable, thanks for everyone involved! I think the design\nspace is\nstill quite large when we take everything into account, so I expect that\nwe'll see\neven more innovation in the coming years.\n\nCheers,\nBastien\n\n[1]\nhttps://github.com/ACINQ/eclair/blob/10eb9e932f9c0de06cc8926230d8ad4e2d1d9e2c/eclair-core/src/main/resources/reference.conf#L237\n[2] https://github.com/ACINQ/eclair/pull/2263\n[3] https://github.com/ACINQ/eclair/pull/2071\n[4] https://github.com/lightning/bolts/pull/829\n\n\nLe lun. 16 mai 2022 \u00e0 22:59, Matt Corallo <lf-lists at mattcorallo.com> a\n\u00e9crit :\n\n> Its probably worth somewhat disentangling the concept of switching to a\n> minimum-cost flow routing\n> algorithm from the concept of \"scoring based on channel value and\n> estimated available liquidity\".\n>\n> These are two largely-unrelated concepts that are being mashed into one in\n> this description - the\n> first concept needs zero-base-fee to be exact, though its not clear to me\n> that a heuristics-based\n> approach won't give equivalent results in practice, given the noise in\n> success rate compared to\n> theory here.\n>\n> The second concept is something that LDK (and I believe CLN and maybe even\n> eclair now) do already,\n> though lnd does not last I checked. For payments where MPP does not add\n> much to success rate (i.e.\n> payments where the amount is relatively \"low\" compared to available\n> network liquidity) dijkstra's\n> with a liquidity/channel-size based scoring will give you the exact same\n> result.\n>\n> For cases where you're sending an amount which is \"high\" compared to\n> available network liquidity,\n> taking a minimum-cost-flow algorithm becomes important, as you point out.\n> Of course you're always\n> going to suffer really slow payment and many retires in this case anyway.\n>\n> Matt\n>\n> On 5/15/22 1:01 PM, Carsten Otto via Lightning-dev wrote:\n> > Dear all,\n> >\n> > the most recent version of lnd-manageJ [1] now includes basic, but\n> usable,\n> > support for #PickhardtPayments. I kindly invite you to check out the\n> code, give\n> > it a try, and use this work for upcoming experiments.\n> >\n> > Teaser with video:\n> https://twitter.com/c_otto83/status/1525879972786749453\n> >\n> > The problem, heavily summarized:\n> >\n> > - Sending payments in the LN often fails, especially with larger amounts.\n> > - Splitting a large payment into smaller shards (using MPP) is helpful,\n> >    as in general the smaller shards don't fail as often as a single large\n> >    payment would. However, the success probability also drops\n> >    exponentially with the number of channels included [2].\n> > - Finding routes through the LN is tricky, as the channels' liquidity is\n> >    uncertain at the time of computing the routes and a simple \"trial and\n> >    error\" approach might take too long.\n> > - There are several implementations using various heuristics, taking\n> >    aspects like fees, previous failures, HTLC deltas, channel age, ...\n> >    into account. Comparing these approaches is very hard.\n> >\n> > The gist of #PickhardtPayments:\n> >\n> > - The issue of finding MPP routes in the LN corresponds to the\n> >    well-known minimum-cost flow problem in computer science (graph\n> >    theory) with lots of related research, results, algorithms, ...\n> > - As shown in the paper [3] the results are optimal, no matter which\n> >    \"cost\" function is used to reason about routing costs (fees) and/or\n> >    reliability. However, depending on the characteristics of the\n> >    function, actually finding optimal results can be extremely hard\n> >    (NP-complete in some cases). By imposing the zero base fee limitation\n> >    and assuming a uniform distribution of funds, fast implementations\n> >    (polynomial time with sub-second runtimes) can be used.\n> > - Assuming (!) a uniform distribution of funds in each channel and zero\n> >    base fee only, #PickhardtPayments offers an approach that is optimal,\n> >    i.e. proven perfect and computationally feasible.\n> > - The most basic version only considers uncertainty costs for\n> >    reliability, but it is possible (and implemented in lnd-manageJ) to\n> >    also consider routing costs (fee rates) and optimize for both features\n> >    to come up with reliable and cheap-ish MPPs.\n> > - The implementation of #PickhardtPayments in lnd-manageJ needs to\n> >    ignore non-zero base fee channels to avoid extremely slow\n> >    (NP-complete) computations. Furthermore, certain aspects are\n> >    approximated [4]. As such, optimality claims are limited to the zero\n> >    base fee subset of the LN, and real-world experiments might be tricky\n> >    to interpret. However, as also shown in the testnet videos [5][6],\n> >    first results are very promising!\n> >\n> > The real strength of #PickhardtPayments:\n> >\n> > - Liquidity information, for example obtained by previous failures, is\n> >    taken into account. For each attempt, the relevant bits of information\n> >    are obtained and will be used to guide the following attempts.\n> > - As the underlying algorithm is proven to be optimal, we do not need to\n> >    rely on heuristics. Instead, the algorithm happily finds routes that\n> >    may be very long (but very probable/cheap, for whatever reason), have\n> >    a surprising number of shards, or rather odd amounts.\n> > - The underlying algorithm also deals with shared segments, i.e.\n> >    individual channels that are used for more than one shard of the MPP,\n> >    without oversaturating it.\n> >\n> > The code in lnd-manageJ:\n> >\n> > - Highly experimental, but it's a start!\n> > - lnd + gRPC middleware + Java/Spring + PostgreSQL is a bit more complex\n> >    than necessary.\n> > - Only works with lnd.\n> > - Doesn't really work with lnd until issue #5746 [7] is fixed. I'd be\n> >    very happy for someone to have a look at my proposal (PR #6543 [8])!\n> > - The code doesn't handle all corner cases, especially the\n> >    less-than-usual failure codes. For example, if a node decides to raise\n> >    the fee rate, the corresponding channel will be ignored for a while as\n> >    I'm too lazy to think about how to update the information in the data\n> >    structure used to compute the MPP.\n> > - Pending shards (neither failed nor settled) just cause the whole MPP\n> >    to hang until something times out. Most likely this can't be fixed\n> >    without stuckless payments?\n> >\n> > I'd be very happy to discuss implementation details (not on this list, I\n> > guess?) and help with upcoming (mainnet?) benchmarks and experiments\n> > (Ren\u00e9 Pickhardt is also interested in helping with this). Given a fix\n> > for the blocking lnd issue, I'd be happy to let my node c-otto.de take\n> > part in some experiments.\n> >\n> > Last but not least, a huge thank you to Ren\u00e9 Pickhardt for lots of\n> > insightful discussions, proof reading, and of course (together with\n> > Stefan Richter) the actual work of coming up with #PickhardtPayments!\n> >\n> > Best regards,\n> > Carsten\n> >\n> > 1: https://github.com/C-Otto/lnd-manageJ/blob/main/PickhardtPayments.md\n> > 2: https://arxiv.org/abs/2103.08576\n> > 3: https://arxiv.org/abs/2107.05322\n> > 4:\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-March/003510.html\n> > 5: https://c-otto.de/pp/pp.mp4\n> > 6: https://c-otto.de/pp/lnd.mp4\n> > 7: https://github.com/lightningnetwork/lnd/issues/5746\n> > 8: https://github.com/lightningnetwork/lnd/pull/6543\n> >\n> >\n> > _______________________________________________\n> > Lightning-dev mailing list\n> > Lightning-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220517/0a3105b6/attachment-0001.html>"
            },
            {
                "author": "Vincent",
                "date": "2022-05-18T09:31:37",
                "message_text_only": "Hi all.\n\nusually I'm just a reader of the mailing list, but I'm preparing a presentation about my research and I think t-bash make exactly the point of the problem that I'm trying to solve. So I use the t-bast words because I can not do better.\n\n>These are heuristics, and it's impossible to judge whether they work or not until\nyou've tried them on mainnet with real payments, so I strongly encourage people\nto run such experiments. But when you do, you should have enough volume for\nthe result data to be statistically meaningful and you should do A/B testing,\notherwise you can make the data say pretty much everything you want. What\nI believe is mostly missing is the volume, the network doesn't have enough real\npayments yet IMHO for this data to accurately say that one heuristic is betterthan another.\n\nand\n\n>However, it's important to also take a step back and\nlook at whether it is economical to make such payments on lightning.\n\nIn particular, in the network, we can have different views and different usage, like routing node, mobile node, or daily usage node, and one of the things that I'm trying to abstract is precise \"how to make the step back, and collect all I necessary information in a common way\" so in this way the own research that each implementation makes can be shared and also verified if it is true for other implementation.\nMore importantly, if we have a way to collect metrics in the node without running a custom version of the node, we can collect data for the real environment, extract all the information that we want, and also some fancy benchmark for node performance.\n\nWell there is already a draft implementation where i'm trying to build a system to make a step back and collect some open data to make maybe some standard metrics. https://github.com/LNOpenMetrics/lnmetrics.rfc\n\nIn addition, I think that MCF is the way to go for the path finding also because it is just a way to use dijkstra in a different way (ok, I know it slow because all the basic MCF algorithm use Bellman Ford), with the only think that right now we don't know what we are trying to minimize (fee? bad channel? all together? up to the user?)\n\nCheers!\n\nVincent.\nhttps://github.com/vincenzopalazzo\n\n------- Original Message -------\nOn Tuesday, May 17th, 2022 at 9:23 AM, Bastien TEINTURIER <bastien at acinq.fr> wrote:\n\n> I completely agree with Matt, these two components are completely independent\n> and too often conflated. Scoring channels and estimating liquidity is something\n> that has been regularly discussed by implementations for the last few years,\n> where every implementation did its own experiments over time.\n>\n> Eclair has quite a large, configurable set of heuristics around channel scoring,\n> along with an A/B testing system that we've been using for a while on mainnet\n> (see [1] for details). We've also been toying with channel liquidity estimation for\n> more than half a year, which you can follow in [2] and [3].\n>\n> These are heuristics, and it's impossible to judge whether they work or not until\n> you've tried them on mainnet with real payments, so I strongly encourage people\n> to run such experiments. But when you do, you should have enough volume for\n> the result data to be statistically meaningful and you should do A/B testing,\n> otherwise you can make the data say pretty much everything you want. What\n> I believe is mostly missing is the volume, the network doesn't have enough real\n> payments yet IMHO for this data to accurately say that one heuristic is better\n> than another.\n>\n> Using an MCF algorithm instead of dijkstra is useful when relaying large payments\n> that will need to be split aggressively to reach the destination. It does make a lot\n> of sense in that scenario. However, it's important to also take a step back and\n> look at whether it is economical to make such payments on lightning.\n>\n> For a route with an aggregated proportional fee of 1000ppm, here is a rough\n> comparison of the fees between on-chain and lightning:\n>\n> * At 1 sat/byte on-chain, payments above 2mBTC cost less on-chain than off-chain\n> * At 10 sat/byte on-chain, payments above 20mBTC cost less on-chain than off-chain\n> * At 25 sat/byte on-chain, payments above 50mBTC cost less on-chain than off-chain\n> * And so on (just keep multiplying)\n>\n> Of course, making payments on lightning has more benefits than just fees, they\n> also confirm faster than on-chain payments, but I think it's important to keep these\n> figures in mind.\n>\n> It would be also useful to think about the shape of the network. Using an MCF\n> algorithm makes sense when payments are saturating channels. But if channels\n> are much bigger than your payment size, this is probably overkill. If channels are\n> small \"at the edges of the network\" and bigger than payments at the \"core of the\n> network\", and we're using trampoline routing [4], it makes sense to run different\n> path-finding algorithms depending on where we are (e.g. MCF at the edges on\n> a small subset of the graph and dijkstra inside the core).\n>\n> I'm very happy that all this research is happening and helping lightning payments\n> become more reliable, thanks for everyone involved! I think the design space is\n> still quite large when we take everything into account, so I expect that we'll see\n> even more innovation in the coming years.\n>\n> Cheers,\n> Bastien\n>\n> [1] https://github.com/ACINQ/eclair/blob/10eb9e932f9c0de06cc8926230d8ad4e2d1d9e2c/eclair-core/src/main/resources/reference.conf#L237\n> [2] https://github.com/ACINQ/eclair/pull/2263\n> [3] https://github.com/ACINQ/eclair/pull/2071\n> [4] https://github.com/lightning/bolts/pull/829\n>\n> Le lun. 16 mai 2022 \u00e0 22:59, Matt Corallo <lf-lists at mattcorallo.com> a \u00e9crit :\n>\n>> Its probably worth somewhat disentangling the concept of switching to a minimum-cost flow routing\n>> algorithm from the concept of \"scoring based on channel value and estimated available liquidity\".\n>>\n>> These are two largely-unrelated concepts that are being mashed into one in this description - the\n>> first concept needs zero-base-fee to be exact, though its not clear to me that a heuristics-based\n>> approach won't give equivalent results in practice, given the noise in success rate compared to\n>> theory here.\n>>\n>> The second concept is something that LDK (and I believe CLN and maybe even eclair now) do already,\n>> though lnd does not last I checked. For payments where MPP does not add much to success rate (i.e.\n>> payments where the amount is relatively \"low\" compared to available network liquidity) dijkstra's\n>> with a liquidity/channel-size based scoring will give you the exact same result.\n>>\n>> For cases where you're sending an amount which is \"high\" compared to available network liquidity,\n>> taking a minimum-cost-flow algorithm becomes important, as you point out. Of course you're always\n>> going to suffer really slow payment and many retires in this case anyway.\n>>\n>> Matt\n>>\n>> On 5/15/22 1:01 PM, Carsten Otto via Lightning-dev wrote:\n>>> Dear all,\n>>>\n>>> the most recent version of lnd-manageJ [1] now includes basic, but usable,\n>>> support for #PickhardtPayments. I kindly invite you to check out the code, give\n>>> it a try, and use this work for upcoming experiments.\n>>>\n>>> Teaser with video: https://twitter.com/c_otto83/status/1525879972786749453\n>>>\n>>> The problem, heavily summarized:\n>>>\n>>> - Sending payments in the LN often fails, especially with larger amounts.\n>>> - Splitting a large payment into smaller shards (using MPP) is helpful,\n>>> as in general the smaller shards don't fail as often as a single large\n>>> payment would. However, the success probability also drops\n>>> exponentially with the number of channels included [2].\n>>> - Finding routes through the LN is tricky, as the channels' liquidity is\n>>> uncertain at the time of computing the routes and a simple \"trial and\n>>> error\" approach might take too long.\n>>> - There are several implementations using various heuristics, taking\n>>> aspects like fees, previous failures, HTLC deltas, channel age, ...\n>>> into account. Comparing these approaches is very hard.\n>>>\n>>> The gist of #PickhardtPayments:\n>>>\n>>> - The issue of finding MPP routes in the LN corresponds to the\n>>> well-known minimum-cost flow problem in computer science (graph\n>>> theory) with lots of related research, results, algorithms, ...\n>>> - As shown in the paper [3] the results are optimal, no matter which\n>>> \"cost\" function is used to reason about routing costs (fees) and/or\n>>> reliability. However, depending on the characteristics of the\n>>> function, actually finding optimal results can be extremely hard\n>>> (NP-complete in some cases). By imposing the zero base fee limitation\n>>> and assuming a uniform distribution of funds, fast implementations\n>>> (polynomial time with sub-second runtimes) can be used.\n>>> - Assuming (!) a uniform distribution of funds in each channel and zero\n>>> base fee only, #PickhardtPayments offers an approach that is optimal,\n>>> i.e. proven perfect and computationally feasible.\n>>> - The most basic version only considers uncertainty costs for\n>>> reliability, but it is possible (and implemented in lnd-manageJ) to\n>>> also consider routing costs (fee rates) and optimize for both features\n>>> to come up with reliable and cheap-ish MPPs.\n>>> - The implementation of #PickhardtPayments in lnd-manageJ needs to\n>>> ignore non-zero base fee channels to avoid extremely slow\n>>> (NP-complete) computations. Furthermore, certain aspects are\n>>> approximated [4]. As such, optimality claims are limited to the zero\n>>> base fee subset of the LN, and real-world experiments might be tricky\n>>> to interpret. However, as also shown in the testnet videos [5][6],\n>>> first results are very promising!\n>>>\n>>> The real strength of #PickhardtPayments:\n>>>\n>>> - Liquidity information, for example obtained by previous failures, is\n>>> taken into account. For each attempt, the relevant bits of information\n>>> are obtained and will be used to guide the following attempts.\n>>> - As the underlying algorithm is proven to be optimal, we do not need to\n>>> rely on heuristics. Instead, the algorithm happily finds routes that\n>>> may be very long (but very probable/cheap, for whatever reason), have\n>>> a surprising number of shards, or rather odd amounts.\n>>> - The underlying algorithm also deals with shared segments, i.e.\n>>> individual channels that are used for more than one shard of the MPP,\n>>> without oversaturating it.\n>>>\n>>> The code in lnd-manageJ:\n>>>\n>>> - Highly experimental, but it's a start!\n>>> - lnd + gRPC middleware + Java/Spring + PostgreSQL is a bit more complex\n>>> than necessary.\n>>> - Only works with lnd.\n>>> - Doesn't really work with lnd until issue #5746 [7] is fixed. I'd be\n>>> very happy for someone to have a look at my proposal (PR #6543 [8])!\n>>> - The code doesn't handle all corner cases, especially the\n>>> less-than-usual failure codes. For example, if a node decides to raise\n>>> the fee rate, the corresponding channel will be ignored for a while as\n>>> I'm too lazy to think about how to update the information in the data\n>>> structure used to compute the MPP.\n>>> - Pending shards (neither failed nor settled) just cause the whole MPP\n>>> to hang until something times out. Most likely this can't be fixed\n>>> without stuckless payments?\n>>>\n>>> I'd be very happy to discuss implementation details (not on this list, I\n>>> guess?) and help with upcoming (mainnet?) benchmarks and experiments\n>>> (Ren\u00e9 Pickhardt is also interested in helping with this). Given a fix\n>>> for the blocking lnd issue, I'd be happy to let my node c-otto.de take\n>>> part in some experiments.\n>>>\n>>> Last but not least, a huge thank you to Ren\u00e9 Pickhardt for lots of\n>>> insightful discussions, proof reading, and of course (together with\n>>> Stefan Richter) the actual work of coming up with #PickhardtPayments!\n>>>\n>>> Best regards,\n>>> Carsten\n>>>\n>>> 1: https://github.com/C-Otto/lnd-manageJ/blob/main/PickhardtPayments.md\n>>> 2: https://arxiv.org/abs/2103.08576\n>>> 3: https://arxiv.org/abs/2107.05322\n>>> 4: https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-March/003510.html\n>>> 5: https://c-otto.de/pp/pp.mp4\n>>> 6: https://c-otto.de/pp/lnd.mp4\n>>> 7: https://github.com/lightningnetwork/lnd/issues/5746\n>>> 8: https://github.com/lightningnetwork/lnd/pull/6543\n>>>\n>>>\n>>> _______________________________________________\n>>> Lightning-dev mailing list\n>>> Lightning-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220518/d8e1a803/attachment-0001.html>"
            },
            {
                "author": "Carsten Otto",
                "date": "2022-05-17T10:46:08",
                "message_text_only": "Three small updates:\n\n1)\nThe most recent version of lnd-manageJ now also considers channels that\ncharge a base fee. In order to make this work, the worst-case scenario\nis assumed (which is: a channel is used for the smallest possible amount)\nand the fee rate used for the route computation is adjusted accordingly.\n\nWith the default settings (quantization 10,000sat), a channel charging a\nbase fee of Xsat and a fee rate of Yppm is considered with a virtual fee\nrate of (Y+X*100)ppm.\n\nThis penalty is linear in the quantization amount, i.e. if you use a\nquantization of 100,000sat, a base fee of Xsat only results in a\npenalty of X*10ppm.\n\n2)\nI added a Dockerfile that should help with experiments.\nhttps://github.com/C-Otto/lnd-manageJ/blob/fix-5746/Dockerfile\n\nThe only interesting requirement is that your lnd needs to be patched:\nhttps://github.com/lightningnetwork/lnd/pull/6545\n\nYou can tweak the lnd-manageJ.conf (as demonstrated in lines 10-12), see\nhttps://github.com/C-Otto/lnd-manageJ/blob/main/PickhardtPayments.md for\nthe available options.\n\nAs I was unable to figure out how to avoid TLS issues with gRPC, you\nneed to use \"--network host\" if you run lnd on localhost.\n\nThe docker image boots PostgreSQL on the default port 5432, so make sure\nit is available and not in use by your host.\n\nFor your experiments please take into account that lnd-manageJ needs to\nrequest the LN graph from lnd, which takes a few seconds. This graph is\ncached for up to two minutes and, if used, refreshed before it times out.\nAs such, please issue a test payment (to request the graph) before\nmeasuring the time it takes to send out any real payment.\n\n3)\nRoute/hop hints provided in the payment request are now considered so that\nprivate/virtual channels are used for the route computation. This only\nworks for \"route hints\" which contain a single hop (which should be\nfine?). Each \"hint\" channel is added with a capacity of 50 BTC and the\nalgorithm is made to believe that 100% of that is available. This \"known\nliquidity\" is not shown in the streaming NDJSON output you get via HTTP,\nthough, which is just a display issue which doesn't affect the route\ncomputation.\n\nHappy hacking,\nCarsten\n-- \nDr. Carsten Otto\ncarsten at c-otto.de\nhttps://c-otto.de\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 195 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220517/6f893265/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "#PickhardtPayments implemented in lnd-manageJ",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Carsten Otto",
                "Bastien TEINTURIER",
                "Matt Corallo",
                "Vincent"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 38254
        }
    },
    {
        "title": "[Lightning-dev] Gossip Propagation, Anti-spam, and Set Reconciliation",
        "thread_messages": [
            {
                "author": "Matt Corallo",
                "date": "2022-05-26T19:48:05",
                "message_text_only": "Oops, sorry, I don't really monitor the dev lists but once every few months so this fell off my plate :/\n\nOn 4/28/22 6:11 PM, Rusty Russell wrote:\n> Matt Corallo <lf-lists at mattcorallo.com> writes:\n> OK, let's step back.  Unlike Bitcoin, we can use a single sketch for\n> *all* peers.  This is because we *can* encode enough information that\n> you can get useful info from the 64 bit id, and because it's expensive\n> to create them so you can't spam.\n\nYep, makes sense.\n\n> The more boutique per-peer handling we need, the further it gets from\n> this ideal;.\n> \n>> The second potential thing I think you might have meant here I don't see as an issue at all? You can\n>> simply...let the sketch include one channel update that you ignored? See above discussion of similar\n>> rate-limits.\n> \n> No, you need to get all the ignored ones somehow?  There's so much cruft\n> in the sketch you can't decode it.  Now you need to remember the ones\n> you ratelimited, and try to match other's ratelimiting.\n\nRight, you'd end up downloading the thing you rate-limited, but only once (possibly per-peer). If \nyou use the total-sync approach you'd download it on every sync, vs a \"only updates\" approach you'd \ndo it once.\n\n>> I agree there should be *some* rough consensus, but rate-limits are a locally-enforced thing, not a\n>> global one. There will always be races and updates you reject that your peers dont, no matter the\n>> rate-limit, and while I agree we should have guidelines, we can't \"just make them the same\" - it\n>> both doesn't solve the problem and means we can't change them in the future.\n> \n> Sure it does!  It severly limits the set divergence to race conditions\n> (down to block height divergence, in practice).\n\nHuh? There's always some line you draw, if an update happens right on the line (which they almost \ncertainly often will because people want to update, and they'll update every X hours to whatever the \nrate limit is), then ~half the network will accept the update and half won't. I don't see how you \nsolve this problem.\n> Maybe.  What's a \"non-update\" based sketch?  Some huge percentage of\n> gossip is channel_update, so it's kind of the thing we want?\n\nOops, maybe we're on *very* different pages, here - I mean doing sketches based on \"the things that \nI received since the last sync, ie all the gossip updates from the last hour\" vs doing sketches \nbased on \"the things I have, ie my full gossip store\".\n\nMatt"
            },
            {
                "author": "Rusty Russell",
                "date": "2022-05-26T20:25:36",
                "message_text_only": "Matt Corallo <lf-lists at mattcorallo.com> writes:\n>>> I agree there should be *some* rough consensus, but rate-limits are a locally-enforced thing, not a\n>>> global one. There will always be races and updates you reject that your peers dont, no matter the\n>>> rate-limit, and while I agree we should have guidelines, we can't \"just make them the same\" - it\n>>> both doesn't solve the problem and means we can't change them in the future.\n>> \n>> Sure it does!  It severly limits the set divergence to race conditions\n>> (down to block height divergence, in practice).\n>\n> Huh? There's always some line you draw, if an update happens right on the line (which they almost \n> certainly often will because people want to update, and they'll update every X hours to whatever the \n> rate limit is), then ~half the network will accept the update and half won't. I don't see how you \n> solve this problem.\n\nThe update contains a block number.  Let's say we allow an update every\n100 blocks.  This must be <= current block height (and presumably, newer\nthan height - 2016).\n\nIf you send an update number 600000, and then 600100, it will propagate.\n600099 will not.\n\nIf some nodes have 600000 and others have 600099 (because you broke the\nratelimiting recommendation, *and* propagated both approx the same\ntime), then the network will split, sure.\n\nWe could be fascist and penalize nodes which do this, but that's\noverkill unless it actually happens a lot.\n\nNodes which want to keep an potential update \"up their sleeve\" will\nbackdate updates by 101 blocks (everyone should do this, in fact).\n\nAs I said, this has a problem with block height differences, but that's\nexplicitly included in the messages so you can ignore and wait if you\nwant.  Again, may not be a problem in practice.\n\n>> Maybe.  What's a \"non-update\" based sketch?  Some huge percentage of\n>> gossip is channel_update, so it's kind of the thing we want?\n>\n> Oops, maybe we're on *very* different pages, here - I mean doing sketches based on \"the things that \n> I received since the last sync, ie all the gossip updates from the last hour\" vs doing sketches \n> based on \"the things I have, ie my full gossip store\".\n\nBut that requires state.  Full store requires none, keeping it\nsuper-simple\n\nThough Alex has a idea for a \"include even the expired entries\" then\n\"regenerate every N blocks\" which avoids the problem that each change is\ntwo deltas (one remove, one add), at cost of some complexity.\n\nCheers,\nRusty."
            },
            {
                "author": "Matt Corallo",
                "date": "2022-05-26T22:19:17",
                "message_text_only": "On 5/26/22 1:25 PM, Rusty Russell wrote:\n> Matt Corallo <lf-lists at mattcorallo.com> writes:\n>>>> I agree there should be *some* rough consensus, but rate-limits are a locally-enforced thing, not a\n>>>> global one. There will always be races and updates you reject that your peers dont, no matter the\n>>>> rate-limit, and while I agree we should have guidelines, we can't \"just make them the same\" - it\n>>>> both doesn't solve the problem and means we can't change them in the future.\n>>>\n>>> Sure it does!  It severly limits the set divergence to race conditions\n>>> (down to block height divergence, in practice).\n>>\n>> Huh? There's always some line you draw, if an update happens right on the line (which they almost\n>> certainly often will because people want to update, and they'll update every X hours to whatever the\n>> rate limit is), then ~half the network will accept the update and half won't. I don't see how you\n>> solve this problem.\n> \n> The update contains a block number.  Let's say we allow an update every\n> 100 blocks.  This must be <= current block height (and presumably, newer\n> than height - 2016).\n> \n> If you send an update number 600000, and then 600100, it will propagate.\n> 600099 will not.\n\nAh, this is an additional proposal on top, and requires a gossip \"hard fork\", which means your new \nprotocol would only work for taproot channels, and any old/unupgraded channels will have to be \npropagated via the old mechanism. I'd kinda prefer to be able to rip out the old gossip sync code \nsooner than a few years from now :(.\n\n> If some nodes have 600000 and others have 600099 (because you broke the\n> ratelimiting recommendation, *and* propagated both approx the same\n> time), then the network will split, sure.\n\nRight, so what do you do in that case, though? AFAIU, in your proposed sync mechanism if a node does \nthis once, you're stuck with all of your gossip reconciliations with every peer \"wasting\" one \ndifference \"slot\" for a day or however long it takes before the peer does a sane update. In my \nproposed alternative it only appears once and then you move on (or maybe once more on startup, but \nwe can maybe be willing to take on some extra cost there?).\n\n>>> Maybe.  What's a \"non-update\" based sketch?  Some huge percentage of\n>>> gossip is channel_update, so it's kind of the thing we want?\n>>\n>> Oops, maybe we're on *very* different pages, here - I mean doing sketches based on \"the things that\n>> I received since the last sync, ie all the gossip updates from the last hour\" vs doing sketches\n>> based on \"the things I have, ie my full gossip store\".\n> \n> But that requires state.  Full store requires none, keeping it\n> super-simple\n\nHeh, I'm surprised you'd complain about this - IIUC your existing gossip storage system keeps this \nas a side-effect so it'd be a single integer for y'all :p. In any case, if it makes the protocol a \nchunk more efficient I don't see why its a big deal to keep track of the set of which invoices have \nchanged recently, you could even make it super efficient by just saying \"anything more recent than \ntimestamp X *except* a few exceptions that we got with some lag against the update timestamp\".\n\nBetter, the state is global, not per-peer, and a small fraction of the total state of the gossip \nstore anyway, so its not like its introducing some new giant or non-constant-factor blowup.\n\nMatt"
            },
            {
                "author": "Alex Myers",
                "date": "2022-05-27T03:59:14",
                "message_text_only": "> > The update contains a block number. Let's say we allow an update every\n> > 100 blocks. This must be <= current block height (and presumably, newer\n> > than height - 2016).\n> >\n> > If you send an update number 600000, and then 600100, it will propagate.\n> > 600099 will not.\n>\n>\n> Ah, this is an additional proposal on top, and requires a gossip \"hard fork\", which means your new\n> protocol would only work for taproot channels, and any old/unupgraded channels will have to be\n> propagated via the old mechanism. I'd kinda prefer to be able to rip out the old gossip sync code\n> sooner than a few years from now :(.\n\nI viewed it as a soft fork, where if you want to use set reconciliation, anything added to the set would be subject to a constricted ruleset - in this case the gossip must be accompanied by a blockheight tlv (or otherwise reference a blockheight) and it must not replace a message in the current 100 block range.\n\nIt doesn't necessarily have to reference blockheight, but that would simplify many edge cases.  The key is merely that a node is responsible for limiting it's own gossip to a predefined interval, and it must be easily verifiable for any other nodes building and reconciling sketches.  Given that we have access to a timechain, this just made the most sense.\n\n> > If some nodes have 600000 and others have 600099 (because you broke the\n> > ratelimiting recommendation, and propagated both approx the same\n> > time), then the network will split, sure.\n>\n>\n> Right, so what do you do in that case, though? AFAIU, in your proposed sync mechanism if a node does\n> this once, you're stuck with all of your gossip reconciliations with every peer \"wasting\" one\n> difference \"slot\" for a day or however long it takes before the peer does a sane update. In my\n> proposed alternative it only appears once and then you move on (or maybe once more on startup, but\n> we can maybe be willing to take on some extra cost there?).\n\nThis case may not be all that difficult. Easiest answer is you offer a spam proof to your peer.  Send both messages, signed by the offending node as proof they violated the set reconciliation rate limit, then remove both from your sketch. You may want to keep the evidence it in your data store, at least until it's superceded by the next valid update, but there's no reason it must occupy a slot of the sketch.  Meanwhile, feel free to use the message as you wish, just keep both out of the sketch. It's not perfect, but the sketch capacity is not compromised and the second incidence of spam should not propagate at all. (It may be possible to keep one, but this is the simplest answer.)\n\n> Heh, I'm surprised you'd complain about this - IIUC your existing gossip storage system keeps this\n> as a side-effect so it'd be a single integer for y'all :p. In any case, if it makes the protocol a\n> chunk more efficient I don't see why its a big deal to keep track of the set of which invoices have\n> changed recently, you could even make it super efficient by just saying \"anything more recent than\n> timestamp X except a few exceptions that we got with some lag against the update timestamp\".\n\nThe benefit of a single global sketch is less overhead in adding additional gossip peers, though looking at the numbers, sketch decoding time seems to be the more significant driving factor than rebuilding sketches (when they're incremental.) I also like maximizing the utility of the sketch by adding the full gossip store if possible.\n\nI still think getting the rate-limit responsibility to the originating node would be a win in either case. It will chew into sketch capacity regardless.\n\n-Alex\n\n\n------- Original Message -------\nOn Thursday, May 26th, 2022 at 5:19 PM, Matt Corallo <lf-lists at mattcorallo.com> wrote:\n\n\n>\n> On 5/26/22 1:25 PM, Rusty Russell wrote:\n>\n> > Matt Corallo lf-lists at mattcorallo.com writes:\n> >\n> > > > > I agree there should be some rough consensus, but rate-limits are a locally-enforced thing, not a\n> > > > > global one. There will always be races and updates you reject that your peers dont, no matter the\n> > > > > rate-limit, and while I agree we should have guidelines, we can't \"just make them the same\" - it\n> > > > > both doesn't solve the problem and means we can't change them in the future.\n> > > >\n> > > > Sure it does! It severly limits the set divergence to race conditions\n> > > > (down to block height divergence, in practice).\n> > >\n> > > Huh? There's always some line you draw, if an update happens right on the line (which they almost\n> > > certainly often will because people want to update, and they'll update every X hours to whatever the\n> > > rate limit is), then ~half the network will accept the update and half won't. I don't see how you\n> > > solve this problem.\n> >\n> > The update contains a block number. Let's say we allow an update every\n> > 100 blocks. This must be <= current block height (and presumably, newer\n> > than height - 2016).\n> >\n> > If you send an update number 600000, and then 600100, it will propagate.\n> > 600099 will not.\n>\n>\n> Ah, this is an additional proposal on top, and requires a gossip \"hard fork\", which means your new\n> protocol would only work for taproot channels, and any old/unupgraded channels will have to be\n> propagated via the old mechanism. I'd kinda prefer to be able to rip out the old gossip sync code\n> sooner than a few years from now :(.\n>\n> > If some nodes have 600000 and others have 600099 (because you broke the\n> > ratelimiting recommendation, and propagated both approx the same\n> > time), then the network will split, sure.\n>\n>\n> Right, so what do you do in that case, though? AFAIU, in your proposed sync mechanism if a node does\n> this once, you're stuck with all of your gossip reconciliations with every peer \"wasting\" one\n> difference \"slot\" for a day or however long it takes before the peer does a sane update. In my\n> proposed alternative it only appears once and then you move on (or maybe once more on startup, but\n> we can maybe be willing to take on some extra cost there?).\n>\n> > > > Maybe. What's a \"non-update\" based sketch? Some huge percentage of\n> > > > gossip is channel_update, so it's kind of the thing we want?\n> > >\n> > > Oops, maybe we're on very different pages, here - I mean doing sketches based on \"the things that\n> > > I received since the last sync, ie all the gossip updates from the last hour\" vs doing sketches\n> > > based on \"the things I have, ie my full gossip store\".\n> >\n> > But that requires state. Full store requires none, keeping it\n> > super-simple\n>\n>\n> Heh, I'm surprised you'd complain about this - IIUC your existing gossip storage system keeps this\n> as a side-effect so it'd be a single integer for y'all :p. In any case, if it makes the protocol a\n> chunk more efficient I don't see why its a big deal to keep track of the set of which invoices have\n> changed recently, you could even make it super efficient by just saying \"anything more recent than\n> timestamp X except a few exceptions that we got with some lag against the update timestamp\".\n>\n> Better, the state is global, not per-peer, and a small fraction of the total state of the gossip\n> store anyway, so its not like its introducing some new giant or non-constant-factor blowup.\n>\n> Matt"
            },
            {
                "author": "Matt Corallo",
                "date": "2022-05-27T05:12:43",
                "message_text_only": "On 5/26/22 8:59 PM, Alex Myers wrote:\n>> Ah, this is an additional proposal on top, and requires a gossip \"hard fork\", which means your new\n>> protocol would only work for taproot channels, and any old/unupgraded channels will have to be\n>> propagated via the old mechanism. I'd kinda prefer to be able to rip out the old gossip sync code\n>> sooner than a few years from now :(.\n> \n> I viewed it as a soft fork, where if you want to use set reconciliation, anything added to the set would be subject to a constricted ruleset - in this case the gossip must be accompanied by a blockheight tlv (or otherwise reference a blockheight) and it must not replace a message in the current 100 block range.\n> \n> It doesn't necessarily have to reference blockheight, but that would simplify many edge cases.  The key is merely that a node is responsible for limiting it's own gossip to a predefined interval, and it must be easily verifiable for any other nodes building and reconciling sketches.  Given that we have access to a timechain, this just made the most sense.\n\nAh, good point, you can just add it as a TLV. It still implies that \"old-gossip\" can't go away for a \nlont time - ~everyone has to upgrade, so we'll have two parallel systems. Worse, people are relying \non the old behavior and some nodes may avoid upgrading to avoid the new rate-limits :(.\n\n>>> If some nodes have 600000 and others have 600099 (because you broke the\n>>> ratelimiting recommendation, and propagated both approx the same\n>>> time), then the network will split, sure.\n>>\n>>\n>> Right, so what do you do in that case, though? AFAIU, in your proposed sync mechanism if a node does\n>> this once, you're stuck with all of your gossip reconciliations with every peer \"wasting\" one\n>> difference \"slot\" for a day or however long it takes before the peer does a sane update. In my\n>> proposed alternative it only appears once and then you move on (or maybe once more on startup, but\n>> we can maybe be willing to take on some extra cost there?).\n> \n> This case may not be all that difficult. Easiest answer is you offer a spam proof to your peer.  Send both messages, signed by the offending node as proof they violated the set reconciliation rate limit, then remove both from your sketch. You may want to keep the evidence it in your data store, at least until it's superceded by the next valid update, but there's no reason it must occupy a slot of the sketch.  Meanwhile, feel free to use the message as you wish, just keep both out of the sketch. It's not perfect, but the sketch capacity is not compromised and the second incidence of spam should not propagate at all. (It may be possible to keep one, but this is the simplest answer.)\n\nRight, well if we're gonna start adding \"spam-proofs\" we shouldn't start talking about complexity of \ntracking the changed-set :p.\n\nWorse, unlike tracking the chanaged-set as proposed this protocol is a ton of unused code to handle \nan edge case we should only rarely hit...in other words code that will almost certainly be buggy, \nuntested, and fail if people start hitting it. In general, I'm not a huge fan of protocols with any \nmore usually-unused code than is strictly necessary.\n\nThis also doesn't capture things like channel_update extensions - BOLTs today say a recipient \"MAY \nchoose NOT to for messages longer than the minimum expected length\" - so now we'd need remove that \n(and I guess have a fixed \"maximum length\" for channel updates that everyone agrees to...basically \nwe have to have exact consensus on valid channel updates across nodes.\n\n>> Heh, I'm surprised you'd complain about this - IIUC your existing gossip storage system keeps this\n>> as a side-effect so it'd be a single integer for y'all :p. In any case, if it makes the protocol a\n>> chunk more efficient I don't see why its a big deal to keep track of the set of which invoices have\n>> changed recently, you could even make it super efficient by just saying \"anything more recent than\n>> timestamp X except a few exceptions that we got with some lag against the update timestamp\".\n> \n> The benefit of a single global sketch is less overhead in adding additional gossip peers, though looking at the numbers, sketch decoding time seems to be the more significant driving factor than rebuilding sketches (when they're incremental.) I also like maximizing the utility of the sketch by adding the full gossip store if possible.\n\nNote that the alternative here does not prevent you from having a single global sketch. You can keep \na rolling global sketch that you send to all your peers at once, it would just be a bit of a \nbandwidth burst when they all request a few channel updates/announcements from you.\n\nMore generally, I'm somewhat surprised to hear a performance concern here - I can't imagine we'd be \nincluding any more entries in such a sketch than Bitcoin Core does transactions to relay to peers, \nand this is exactly the design direction they went in (because of basically the same concerns).\n\n> I still think getting the rate-limit responsibility to the originating node would be a win in either case. It will chew into sketch capacity regardless.\n\nThat's fair, though I do still very much struggle with how inflexible this proposal would be towards \nany future changes to relay policy. Basically we're locking ourselves into a fixed rate-limit that \neveryone has to agree to, with problems introduced if we ever go to change it.\n\nMatt"
            }
        ],
        "thread_summary": {
            "title": "Gossip Propagation, Anti-spam, and Set Reconciliation",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Rusty Russell",
                "Matt Corallo",
                "Alex Myers"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 20973
        }
    },
    {
        "title": "[Lightning-dev] Principle Limitations to the reliability of the Lightning Network Protocol",
        "thread_messages": [
            {
                "author": "Ren\u00e9 Pickhardt",
                "date": "2022-05-26T21:32:30",
                "message_text_only": "Dear fellow lightning developers,\n\nplease note my recent blog article titled \"Price of Anarchy from selfish\nrouting strategies on the Lightning Network\" [1] where we investigate how\nthe selfish behavior of nodes sending Bitcoin over the Lightning Network\nmay lead to higher drain on channels which in turn is expected to result in\nhigher depletion and failure rates for payments on the network. All of the\nobservations have been derived purely be looking at statistical measures\nand computations on the data that the Gossip Protocol and Bitcoin Network\nprovides about the topology of the Lightning Network. No probing or\nempirical experiments had to be conducted to derive these theoretical\nresults. All code can be found in the lnresearch repository at [2].\n\nWhile those preliminary results are only presented for some of the\nstrategies that are currently being deployed by `pay` implementations we\nhave not been able yet to study the dynamics of the entire game, secondary\neffects or to find the dominant strategies of routing and sending nodes.\nDue to the implications with respect to reliability and payment failure\nrates - which I assume many have observed in the wild - I thought I would\nalready share these early results with you.\n\nWhile routing nodes seem to be able to mitigate some of the effects we note\nin the article that it seems as if the routing nodes can hardly engage into\nselfish behavior or strategies themselves to help with flow and congestion\ncontrol. This is because it seems as if all operations that we can\ncurrently think of that routing nodes could engage in are limited (through\nprotocol design) if applied at scale. E.g:\n\n* Adopting fees (limited through gossip relay policies which prevent spam)\n* Opening / closing channels (limited through block space)\n* Pro active off chain rebalancing (limited through fees that other nodes\ncharge and the time needed for finding opportunities to conduct rebalancing\nand the additional load this put to the network )\n* Pro active on chain rebalancing (limited through block space and routing\nfees)\n\nI hope the described effects won't be too strong for the expected traffic\nand usage of the network so that the technology will work properly at the\nrequired scale. I am very happy for your thoughts, feedback, comments and\nquestions as I find it fascinating to see how the game theory of the\nLightning network will eventually play out and at least in my current\nunderstanding seems to produce limitations to the amount of traffic the\nprotocol may eventually be able to handle.\n\nwith kind Regards Rene Pickhardt\n\n[1]:\nhttps://blog.bitmex.com/price-of-anarchy-from-selfish-routing-strategies/\n[2]: https://github.com/lnresearch/Price-Of-Anarchy-in-Selfish-Routing\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220526/cdf5b9c4/attachment.html>"
            },
            {
                "author": "Gregorio Guidi",
                "date": "2022-05-27T16:28:40",
                "message_text_only": "On 5/26/22 23:32, Ren\u00e9 Pickhardt via Lightning-dev wrote:\n> Dear fellow lightning developers,\n>\n> please note my recent blog article titled \"Price of Anarchy from \n> selfish routing strategies on the Lightning Network\" [1] where we \n> investigate how the selfish behavior of nodes sending Bitcoin over the \n> Lightning Network may lead to higher drain on channels which in turn \n> is expected to result in higher depletion and failure rates for \n> payments on the network. All of the observations have been derived \n> purely be looking at statistical measures and computations on the data \n> that the Gossip Protocol and Bitcoin Network provides about the \n> topology of the Lightning Network. No probing or empirical experiments \n> had to be conducted to derive these theoretical results. All code can \n> be found in the lnresearch repository at [2].\n>\n> ...\n>\n> I hope the described effects won't be too strong for the expected \n> traffic and usage of the network so that the technology will work \n> properly at the required scale. I am very happy for your thoughts, \n> feedback, comments and questions as I find it fascinating to see how \n> the game theory of the Lightning network will eventually play out and \n> at least in my current understanding seems to produce limitations to \n> the amount of traffic the protocol may eventually be able to handle.\n>\n> with kind Regards Rene Pickhardt\n>\n> [1]: \n> https://blog.bitmex.com/price-of-anarchy-from-selfish-routing-strategies/\n> [2]: https://github.com/lnresearch/Price-Of-Anarchy-in-Selfish-Routing\n\nDear Ren\u00e9,\n\na few years ago I made a very small contribution to this list by posting \na paper on \"Modeling a Steady-State Lightning Network Economy\":\n\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2019-August/002115.html\n\nI mention it here because perhaps there are some ideas tangentially \nrelated to the research program you are conducting on routing \nstrategies. I copy below the abstract and a relevant quote (full paper \nhere: https://github.com/gr-g/ln-steady-state-model). In particular you \ncan find a link between the idea of \"drain\" you defined and the concept \nof \"demand imbalance\" in the paper.\n\nAbstract:\n\n/In this paper, we consider an idealized scenario in which the Lightning //\n//Network (or any similar payment network) has scaled to the size and //\n//volume of a self-sustained economy, meaning that the number of on-chain //\n//transactions - including channel opening and closing - has become //\n//negligible when compared to the number of off-chain transactions, and //\n//payments continuously flow across a network with relatively stable //\n//topology. We take this scenario to the extreme and model a network \nwhere //\n//the channels are fixed, so that payments form a completely closed //\n//system, and where nodes have (on a long enough timescale) stable and //\n//perfectly balanced incoming and outgoing payments (i.e. they spend //\n//exactly what they earn). We call this scenario the \"steady-state //\n//economy\" of the payment network.//\n//\n//We argue that in such scenario, in a network of n connected nodes, //\n//there is a tendency towards a state where exactly n-1 channels have //\n//perfectly balanced flows in the two directions (\"self-balancing\" //\n//channels), while all other channels are either unused, or have a //\n//permanent tendency towards imbalance: the channel balance accumulates \nat //\n//one end and the channel is only intermittently available in one //\n//direction (\"stuttering\" channels). We note that the \"self-balancing\" //\n//channels form a spanning tree of the network graph, which we call the //\n//\"core spanning tree\" of the payment network.//\n//\n//We also try to derive some practical lessons from this idealized //\n//scenario, hopefully providing some useful insight to node operators of //\n//the current (embryonic) Lightning Network.//\n//\n//At the end of the paper, we provide some remarks on the more general //\n//case in which nodes do not balance their income and expenses.//\n/\n\n From section 4:\n\n/There is general consensus on the //fact that having a large fraction \nof channels\nnot usable or barely usable in one////direction is not a healthy \npredicament for the\nnetwork, and that some form of////channel management will need to be \npracticed\nby node operators involving a mix////of rebalancing and fee fine-tuning. \nHowever,\none of the main takeaways of the////analysis of the steady-state model \nis that the\nnetwork might have a tendency////to push////most////of the channels \n(when not unused)\ntowards being chronically////unbalanced.//\n//We wonder if these two tools (rebalancing and fee management) are really//\n//enough to contrast the tendency toward imbalance. If not, it would be \nappro-//\n//priate to consider also other strategies to \u201cwork with the imbalances\u201d \ninstead//\n//of fighting them.////We refer, for example, to efficient low-latency \nmechanisms//\n//to signal when a channel becomes unusable in one direction, in order \nto limit//\n//the failure rate, together with a general robustness of the network \nagainst a//\n//pervasive and high-volume flow of information about channels that \nswitch from//\n//being available to not available and vice versa (or that switch \nbetween low fees//\n//and high fees)./\n\nKind regards,\n\nGregorio\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220527/2aa3b108/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Principle Limitations to the reliability of the Lightning Network Protocol",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Ren\u00e9 Pickhardt",
                "Gregorio Guidi"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 8379
        }
    },
    {
        "title": "[Lightning-dev] On the Routing Convergence Delay in the Lightning Network",
        "thread_messages": [
            {
                "author": "dergoegge",
                "date": "2022-05-27T11:59:30",
                "message_text_only": "Hi all,\n\nGiven the recent discussion about the gossip protocol and potential\nalternatives, we (Florian Tschorsch, Elias Rohrer and I) wanted to inform you\nabout a paper we wrote on the convergence delay of routing information in the\nLightning Network. The preprint is available here:\nhttps://arxiv.org/abs/2205.12737\n\nAs a summary, here is an excerpt from the paper stating our contributions:\n\n- We analyze the Lightning Network\u2019s gossip protocol in its current state by\n  looking at and comparing c-lightning and LND, the two most popular node\n  implementations. We measure the delay seen in the real network through a\n  passive experiment and catalog the seen gossip messages (specifically all\n  channel updates) to understand why and when gossip messages are broadcast by\n  nodes. The catalog is also useful to understand which types of channel\n  updates are potentially disruptive to payment routing.\n- We implemented a simulator capable of simulating the Lightning Network\u2019s\n  gossip protocol as well as payments in the Lightning Network. We can boot-\n  strap our simulation from historical topology data and replay recorded gossip\n  messages. We use the simulation to gain further inside into how the gossip\n  protocol operates and where its inefficiencies lie.\n- We evaluate the use of alternative message propagation mechanisms in the\n  Lightning Network. Through simulation, we compare flooding, a structured\n  broadcast utilizing the channel graph topology, inventory based gossip, as\n  well as efficient set reconciliation using Minisketch.\n\nAccording to our measurements of the convergence delay, it takes 359.9 seconds\non average until a node sees a message after it was broadcast, with 95% of\nnodes seeing messages after 753 seconds and 100% of nodes seeing messages after\n2,500 seconds.\n\nAn interesting result from dissecting samples of recorded gossip was that ~50%\nof all channel updates were keep alive updates (i.e. updates that only differ\nin the timestamp). Reducing the number of keep alives could have multiple\nbenefits for the network. Besides reducing the bandwidth usage for each node we\nshow through simulations that, due to the parameter choices of LND's staggered\nbroadcast, the convergence delay would be smaller if less keep alive updates\nwere broadcast.\n\nWe were not able to pinpoint exactly why this many keep alive updates are\ncirculating in the network and we are curious if anyone has more insights into\nthis. The following is a list of node IDs that broadcast more than 200 keep\nalive updates within 10 hours during one of our gossip recordings (30th October\n2021).\n\n```\n026db2cbf3d8ab4a4c01eed2df432d20cf0a13136402097574209d2595cb9e9d93\n0390b5d4492dc2f5318e5233ab2cebf6d48914881a33ef6a9c6bcdbb433ad986d0\n0297a77f4d1ccc55d7a10a9b137119b1103d9a9d38a5a97ffa1d0152c818fcdd0a\n03c8dfbf829eaeb0b6dab099d87fdf7f8faceb0c1b935cd243e8c1fb5af71361cf\n03e691f81f08c56fa876cc4ef5c9e8b727bd682cf35605be25d48607a802526053\n0260fab633066ed7b1d9b9b8a0fac87e1579d1709e874d28a0d171a1f5c43bb877\n0217890e3aad8d35bc054f43acc00084b25229ecff0ab68debd82883ad65ee8266\n03f3297397c8f5f685a562847611e20d15f56d6aaabc4d808a6e04e631dea6e612\n02c43a8c5dd024c4d3c5be5612347f87cf90d79e5c2417861908d25f72046354c3\n023d70f2f76d283c6c4e58109ee3a2816eb9d8feb40b23d62469060a2b2867b77f\n033d8656219478701227199cbd6f670335c8d408a92ae88b962c49d4dc0e83e025\n03ea08d787c0153d42f0aa286a1b7000de17d959771e059aadc1cf85d5f2a67e35\n02315fe3619ffdea2561bcacecada87b226723f471a59fdbfec18c4e84bcf785b2\n0242a4ae0c5bef18048fbecf995094b74bfb0f7391418d71ed394784373f41e4f3\n03c2abfa93eacec04721c019644584424aab2ba4dff3ac9bdab4e9c97007491dda\n03a503d8e30f2ff407096d235b5db63b4fcf3f89a653acb6f43d3fc492a7674019\n```\n\nIf you operate one of these nodes, we would be interested to here about your\nset up. (What implementation are you running? Are you using any automated fee\nadjusters?)\n\nIf results like these are interesting to you, then you might want to give the\nwhole a paper a read. We would be thrilled to hear your thoughts/feedback.\n\nBest regards,\nNiklas G\u00f6gge"
            }
        ],
        "thread_summary": {
            "title": "On the Routing Convergence Delay in the Lightning Network",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "dergoegge"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 4033
        }
    }
]