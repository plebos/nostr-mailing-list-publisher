[
    {
        "title": "[Lightning-dev] BOLT 11, real time micro payments, and route redundancy",
        "thread_messages": [
            {
                "author": "Rusty Russell",
                "date": "2017-09-02T03:39:09",
                "message_text_only": "Andy Schroder <info at AndySchroder.com> writes:\n> Hello,\n>\n> I'm looking through BOLT 11. I don't really see an option for a refund \n> address like is present in BIP 70. Is this intentional? If so, why do \n> you not see that people would possibly want to receive a refund?\n\nHi!\n\n        I never even thought of that requirement!\n\n        The payment latency is likely to be in the hundreds of\nmilliseconds, making it hard to match with a pump as it normally works\nAFAICT.  People don't appreciate overpaying :)\n\n> I'm trying to adapt my fuel pump \n> (http://andyschroder.com/BitcoinVendingDevices/) to use lightening and \n> it requires a refund address because their is a pre-payment required. \n> Change is then immediately returned at the end of the sale for any \n> unused credit. An alternative is for one's automobile to do real time \n> micro pre-payments, but I'm not sure that the latency of a lightening \n> payment will be low enough and the bandwidth requirement might be too \n> expensive. It would likely also require people's automobiles to measure \n> the product delivered and have an on board wallet. This would be ideal \n> long term, but I'm not sure if it is realistic at this time.\n\nIt's only a problem if the a goes down actually *during* a payment,\nwhich is a fairly narrow window.\n\nThen it's stuck, and you re-route a new payment.\n\n> Also, assuming that a real time micropayment is doable at the automobile \n> level, what happens if one of your hops goes down in the middle of the \n> product delivery? Can there be automatic alternate/redundant fail over \n> routes like happens with IP traffic? It seems like this could be \n> difficult with onion routing.\n> \n> With all that being said, even if real time micro payments can be a \n> reality, I still see many of other unrelated use cases where there may \n> be a refund desired. I think that's why they put a refund address option \n> in BIP 70.\n\nI think the logical approach is a flag in BOLT 11 which says it wants a\nrefund address, and we put the refund information in the payment onion\nitself.\n\nThe refund requires basically another complete BOLT 11 payment\nrequest, which would be only known to the final recipient.  That won't\nfit in the onion for 1.0, but there's a brainstorming item to allow for\nmore information to be crammed in there:\n\n        https://github.com/lightningnetwork/lightning-rfc/wiki/Brainstorming\n\nI've filed a feature request:\nhttps://github.com/lightningnetwork/lightning-rfc/issues/234\n\nThanks!\nRusty."
            },
            {
                "author": "Andy Schroder",
                "date": "2017-09-03T05:58:39",
                "message_text_only": "Hello,\n\nYes, it seems as though high frequency payments are not a reality. For \nhigh value products that are delivered quickly, \"micro\" payments are not \neven possible. With my fuel delivery system, the smallest volume of \nproduct that could be individually payed for would be on the order of a \nhundred mL. If I were to implement paying by the 100mL, is there any \nprotocol for doing repeated payments? Do you have to request a new \npayment request, or can you just send more to the same payment request?\n\nRegarding the payment route going down, if you can re-use the same \npayment request, when you say *during* a payment, do you mean when you \nare actually sending the payment (I think this is probably it), or \neverything related to that payment request (I don't think this is it)? \nIt seems like this could definitely be a problem for lower value \nproducts that are delivered slowly over long periods of time, such as \nwater, natural gas, electricity, internet, a parking meter, or some kind \nof digital content.\n\nRegarding the refund address. Thanks for adding that to the issue \nrequests. I guess I'm confused how this is going to work safely. If you \nput a refund request in with your payment, isn't that revealing the \npublic key of your node and then defeating the whole purpose of the \nonion routing of the payment in the first place (I'm, assuming the payee \nnode re-uses the same public key?)? It seems like rather than putting a \nflag in BOLT 11 to instruct a payer to include a refund payment request, \nshouldn't the payer just know to do that if they think they will need \nto? Or maybe they won't always?\n\nIn BOLT 11, how does a payee distinguish payments from different payers? \nIn standard bitcoin transactions, this is usually by different bitcoin \naddresses that have been presented to different payers. Is this what the \npurpose of the d and h tagged fields are?\n\nIn BOLT 11, in the examples section, for the p tagged field, it lists it \nas a \"preimage\". Is this supposed to be a \"preimage hash\"?\n\nIn BOLT 11, what's the point of tagged field n if the public key is \nimplied through the signature and the required recovery id?\n\nThanks,\n\nAndy Schroder\n\nOn 09/01/2017 11:39 PM, Rusty Russell wrote:\n> Andy Schroder <info at AndySchroder.com> writes:\n>> Hello,\n>>\n>> I'm looking through BOLT 11. I don't really see an option for a refund\n>> address like is present in BIP 70. Is this intentional? If so, why do\n>> you not see that people would possibly want to receive a refund?\n> Hi!\n>\n>          I never even thought of that requirement!\n>\n>          The payment latency is likely to be in the hundreds of\n> milliseconds, making it hard to match with a pump as it normally works\n> AFAICT.  People don't appreciate overpaying :)\n>\n>> I'm trying to adapt my fuel pump\n>> (http://andyschroder.com/BitcoinVendingDevices/) to use lightening and\n>> it requires a refund address because their is a pre-payment required.\n>> Change is then immediately returned at the end of the sale for any\n>> unused credit. An alternative is for one's automobile to do real time\n>> micro pre-payments, but I'm not sure that the latency of a lightening\n>> payment will be low enough and the bandwidth requirement might be too\n>> expensive. It would likely also require people's automobiles to measure\n>> the product delivered and have an on board wallet. This would be ideal\n>> long term, but I'm not sure if it is realistic at this time.\n> It's only a problem if the a goes down actually *during* a payment,\n> which is a fairly narrow window.\n>\n> Then it's stuck, and you re-route a new payment.\n>\n>> Also, assuming that a real time micropayment is doable at the automobile\n>> level, what happens if one of your hops goes down in the middle of the\n>> product delivery? Can there be automatic alternate/redundant fail over\n>> routes like happens with IP traffic? It seems like this could be\n>> difficult with onion routing.\n>>\n>> With all that being said, even if real time micro payments can be a\n>> reality, I still see many of other unrelated use cases where there may\n>> be a refund desired. I think that's why they put a refund address option\n>> in BIP 70.\n> I think the logical approach is a flag in BOLT 11 which says it wants a\n> refund address, and we put the refund information in the payment onion\n> itself.\n>\n> The refund requires basically another complete BOLT 11 payment\n> request, which would be only known to the final recipient.  That won't\n> fit in the onion for 1.0, but there's a brainstorming item to allow for\n> more information to be crammed in there:\n>\n>          https://github.com/lightningnetwork/lightning-rfc/wiki/Brainstorming\n>\n> I've filed a feature request:\n> https://github.com/lightningnetwork/lightning-rfc/issues/234\n>\n> Thanks!\n> Rusty.\n>"
            },
            {
                "author": "Rusty Russell",
                "date": "2017-09-04T00:34:01",
                "message_text_only": "Andy Schroder <info at AndySchroder.com> writes:\n> Hello,\n>\n> Yes, it seems as though high frequency payments are not a reality. For \n> high value products that are delivered quickly, \"micro\" payments are not \n> even possible. With my fuel delivery system, the smallest volume of \n> product that could be individually payed for would be on the order of a \n> hundred mL. If I were to implement paying by the 100mL, is there any \n> protocol for doing repeated payments? Do you have to request a new \n> payment request, or can you just send more to the same payment request?\n\nNot currently, no: paying the same payment request twice is an\ninvitation for anyone in the middle to just take your funds!\n\nWith 1.1 we're looking at changing the way payment hashes work so this\nwill be possible (kind of like bip32, except for lightning).\n\n> Regarding the payment route going down, if you can re-use the same \n> payment request, when you say *during* a payment, do you mean when you \n> are actually sending the payment (I think this is probably it), or \n> everything related to that payment request (I don't think this is it)? \n> It seems like this could definitely be a problem for lower value \n> products that are delivered slowly over long periods of time, such as \n> water, natural gas, electricity, internet, a parking meter, or some kind \n> of digital content.\n\nNo, it's only during the actualy payment.  Which looks like:\n\n A: Hi B, I'll pay you $1 for preimage of hash X.\n B: Hi C, I'll pay you $0.99 for preimage of hash X.\n C: Hi D, I'll pay you $0.98 for preimage of hash X.\n D: Thanks C, here's the preimage.\n C: Thanks B, here's the preimage.\n B: Thanks A, here's the preimage.\n\nNow, if B or C go down after receiving the offer but before either\nfailing or returning the preimage, A has to wait, unsure if they'll try\nto redeem the offer or not.  (If B or C are simply offline, A or B\nsimply fail and A gets to try again on a different route).\n\nIn practice it's a few hundred ms of exposure, but it's still possible.\nThen we slow down to blockchain speed.\n\n> Regarding the refund address. Thanks for adding that to the issue \n> requests. I guess I'm confused how this is going to work safely. If you \n> put a refund request in with your payment, isn't that revealing the \n> public key of your node and then defeating the whole purpose of the \n> onion routing of the payment in the first place (I'm, assuming the payee \n> node re-uses the same public key?)? It seems like rather than putting a \n> flag in BOLT 11 to instruct a payer to include a refund payment request, \n> shouldn't the payer just know to do that if they think they will need \n> to? Or maybe they won't always?\n\nNobody along the route (B and C in our example above) can see it.  And D\nkind of has to, since it needs to send the refund.\n\nBut there needs to be some way for D to tell A it wants a refund\naddress, hence a flag in BOLT 11.\n\n> In BOLT 11, how does a payee distinguish payments from different payers? \n> In standard bitcoin transactions, this is usually by different bitcoin \n> addresses that have been presented to different payers. Is this what the \n> purpose of the d and h tagged fields are?\n\nIn this case 'p' is the payment-specific variable.\n\n> In BOLT 11, in the examples section, for the p tagged field, it lists it \n> as a \"preimage\". Is this supposed to be a \"preimage hash\"?\n\nOops, thanks!\n\nhttps://github.com/lightningnetwork/lightning-rfc/pull/235\n\n> In BOLT 11, what's the point of tagged field n if the public key is \n> implied through the signature and the required recovery id?\n\nI like key recovery, but there were some question marks over it.  So if\nwe decide doing key recovery is a dumb idea, we can start using the n\nfield.  Since everyone should support it, it's a trivial change.\n\nCheers!\nRusty."
            },
            {
                "author": "Christian Decker",
                "date": "2017-09-04T18:42:06",
                "message_text_only": "On Mon, Sep 04, 2017 at 10:04:01AM +0930, Rusty Russell wrote:\n> Not currently, no: paying the same payment request twice is an\n> invitation for anyone in the middle to just take your funds!\n> \n> With 1.1 we're looking at changing the way payment hashes work so this\n> will be possible (kind of like bip32, except for lightning).\n\nWe could allow for amount adjustments while the payment has not been\nresolved. So let's say the sender would like to perform incremental\npayments to a recipient. The recipient issues a payment request that\nindicates support for adjustments. The sender now sends an initial\ntransfer to the recipient through a route of her chosing. The\nrecipient does not immediately claim the transfer by revealing the\npreimage, instead it serves the sender and keeps the transfer\nopen. The sender now increments the amount by sending an updated\nadd_htlc message with matching payment hash and a higher value. Nodes\nalong the route notice that this is an update to an existing HTLC, and\nforward it along the route (resetting any timeouts to unlock the\nHTLCs).\n\nThis could allow for payments similar to the simple Spillman style\npayment channels, but routed along a path or multiple hops, but it\nobviously has some pitfalls as well, e.g., it opens a new DoS vector\nwhere an attacker can lock up funds for a longer time, so we need to\nbe careful about how we implement these.\n\nCheers,\nChristian"
            },
            {
                "author": "Andy Schroder",
                "date": "2017-09-11T06:00:41",
                "message_text_only": "On 09/03/2017 08:34 PM, Rusty Russell wrote:\n> Andy Schroder <info at AndySchroder.com> writes:\n>> Hello,\n>>\n>> Yes, it seems as though high frequency payments are not a reality. For\n>> high value products that are delivered quickly, \"micro\" payments are not\n>> even possible. With my fuel delivery system, the smallest volume of\n>> product that could be individually payed for would be on the order of a\n>> hundred mL. If I were to implement paying by the 100mL, is there any\n>> protocol for doing repeated payments? Do you have to request a new\n>> payment request, or can you just send more to the same payment request?\n> Not currently, no: paying the same payment request twice is an\n> invitation for anyone in the middle to just take your funds!\n\n\nThanks for the clarification on this. So, basically, donations with \nlightening work a lot different from with blockchain donation \n(static/reused) addresses because they can't be re-used? If so, that's \nfine. I always though static donation addresses were a cool idea, but at \nthe same time are a huge privacy problem for both parties. Might be \nworth pointing this out in the BIP for newcomers.\n\n\n>\n> With 1.1 we're looking at changing the way payment hashes work so this\n> will be possible (kind of like bip32, except for lightning).\n\nThat would definitely help.\n\n>\n>> Regarding the payment route going down, if you can re-use the same\n>> payment request, when you say *during* a payment, do you mean when you\n>> are actually sending the payment (I think this is probably it), or\n>> everything related to that payment request (I don't think this is it)?\n>> It seems like this could definitely be a problem for lower value\n>> products that are delivered slowly over long periods of time, such as\n>> water, natural gas, electricity, internet, a parking meter, or some kind\n>> of digital content.\n> No, it's only during the actualy payment.  Which looks like:\n>\n>   A: Hi B, I'll pay you $1 for preimage of hash X.\n>   B: Hi C, I'll pay you $0.99 for preimage of hash X.\n>   C: Hi D, I'll pay you $0.98 for preimage of hash X.\n>   D: Thanks C, here's the preimage.\n>   C: Thanks B, here's the preimage.\n>   B: Thanks A, here's the preimage.\n>\n> Now, if B or C go down after receiving the offer but before either\n> failing or returning the preimage, A has to wait, unsure if they'll try\n> to redeem the offer or not.  (If B or C are simply offline, A or B\n> simply fail and A gets to try again on a different route).\n\n\nAre there any security holes where B or C can receive the offer, but \npretend they don't and then the payment gets re-routed some other way?\n\n\nAny way for a party to have multiple hosts for redundancy, like how you \ncan have multiple MX and NS records in DNS?\n\n\n>\n> In practice it's a few hundred ms of exposure, but it's still possible.\n> Then we slow down to blockchain speed.\n>\n>> Regarding the refund address. Thanks for adding that to the issue\n>> requests. I guess I'm confused how this is going to work safely. If you\n>> put a refund request in with your payment, isn't that revealing the\n>> public key of your node and then defeating the whole purpose of the\n>> onion routing of the payment in the first place (I'm, assuming the payee\n>> node re-uses the same public key?)? It seems like rather than putting a\n>> flag in BOLT 11 to instruct a payer to include a refund payment request,\n>> shouldn't the payer just know to do that if they think they will need\n>> to? Or maybe they won't always?\n> Nobody along the route (B and C in our example above) can see it.  And D\n> kind of has to, since it needs to send the refund.\n\n\n\nIt seems to me like this is sort of a limitation in privacy with \nlightening. With blockchain payments on my fuel pump, I could return a \nrefund back to the customer without always knowing who they are. With \nlightning, it looks like the payer will reveal their identity to the \npayee by offering a refund payment request. It's great that those along \nthe payment route don't know, but it's still bad to have the payer \nrevealed to the payee. Why does someone have to reveal their identity \njust to get a refund?\n\n\n>\n> But there needs to be some way for D to tell A it wants a refund\n> address, hence a flag in BOLT 11.\n\nI guess it won't hurt to be explicit.\n\n>> In BOLT 11, how does a payee distinguish payments from different payers?\n>> In standard bitcoin transactions, this is usually by different bitcoin\n>> addresses that have been presented to different payers. Is this what the\n>> purpose of the d and h tagged fields are?\n> In this case 'p' is the payment-specific variable.\n\n\nGotcha.\n\n>\n>> In BOLT 11, in the examples section, for the p tagged field, it lists it\n>> as a \"preimage\". Is this supposed to be a \"preimage hash\"?\n> Oops, thanks!\n>\n> https://github.com/lightningnetwork/lightning-rfc/pull/235\n>\n>> In BOLT 11, what's the point of tagged field n if the public key is\n>> implied through the signature and the required recovery id?\n> I like key recovery, but there were some question marks over it.  So if\n> we decide doing key recovery is a dumb idea, we can start using the n\n> field.  Since everyone should support it, it's a trivial change.\n\n\nMakes sense.\n\n\n>\n> Cheers!\n> Rusty.\n>"
            },
            {
                "author": "Rusty Russell",
                "date": "2017-09-15T03:49:18",
                "message_text_only": "Andy Schroder <info at AndySchroder.com> writes:\n> On 09/03/2017 08:34 PM, Rusty Russell wrote:\n>> Andy Schroder <info at AndySchroder.com> writes:\n>>> Hello,\n>>>\n>>> Yes, it seems as though high frequency payments are not a reality. For\n>>> high value products that are delivered quickly, \"micro\" payments are not\n>>> even possible. With my fuel delivery system, the smallest volume of\n>>> product that could be individually payed for would be on the order of a\n>>> hundred mL. If I were to implement paying by the 100mL, is there any\n>>> protocol for doing repeated payments? Do you have to request a new\n>>> payment request, or can you just send more to the same payment request?\n>> Not currently, no: paying the same payment request twice is an\n>> invitation for anyone in the middle to just take your funds!\n>\n>\n> Thanks for the clarification on this. So, basically, donations with \n> lightening work a lot different from with blockchain donation \n> (static/reused) addresses because they can't be re-used? If so, that's \n> fine. I always though static donation addresses were a cool idea, but at \n> the same time are a huge privacy problem for both parties. Might be \n> worth pointing this out in the BIP for newcomers.\n\nYes, lightning is an interactive protocol, whereas bitcoin on-chain\nisn't.  As you point out, that leads to problems...\n\n>>> It seems like this could definitely be a problem for lower value\n>>> products that are delivered slowly over long periods of time, such as\n>>> water, natural gas, electricity, internet, a parking meter, or some kind\n>>> of digital content.\n>> No, it's only during the actualy payment.  Which looks like:\n>>\n>>   A: Hi B, I'll pay you $1 for preimage of hash X.\n>>   B: Hi C, I'll pay you $0.99 for preimage of hash X.\n>>   C: Hi D, I'll pay you $0.98 for preimage of hash X.\n>>   D: Thanks C, here's the preimage.\n>>   C: Thanks B, here's the preimage.\n>>   B: Thanks A, here's the preimage.\n>>\n>> Now, if B or C go down after receiving the offer but before either\n>> failing or returning the preimage, A has to wait, unsure if they'll try\n>> to redeem the offer or not.  (If B or C are simply offline, A or B\n>> simply fail and A gets to try again on a different route).\n>\n>\n> Are there any security holes where B or C can receive the offer, but \n> pretend they don't and then the payment gets re-routed some other way?\n\nNo; if I've sent the committed offer, I have to assume you can redeem\nit.  Technically we send in batches (offer, offer ... commit) and\nc-lightning uses a 10 msec timer so there's a chance to get a failure\nbefore we send the commit.\n\n> Any way for a party to have multiple hosts for redundancy, like how you \n> can have multiple MX and NS records in DNS?\n\nMore than possible, but it requires multinode realtime failover, which I\ndon't think anyone has implemented yet...\n\n>>> node re-uses the same public key?)? It seems like rather than putting a\n>>> flag in BOLT 11 to instruct a payer to include a refund payment request,\n>>> shouldn't the payer just know to do that if they think they will need\n>>> to? Or maybe they won't always?\n>> Nobody along the route (B and C in our example above) can see it.  And D\n>> kind of has to, since it needs to send the refund.\n>\n> It seems to me like this is sort of a limitation in privacy with \n> lightening. With blockchain payments on my fuel pump, I could return a \n> refund back to the customer without always knowing who they are. With \n> lightning, it looks like the payer will reveal their identity to the \n> payee by offering a refund payment request. It's great that those along \n> the payment route don't know, but it's still bad to have the payer \n> revealed to the payee. Why does someone have to reveal their identity \n> just to get a refund?\n\nIndeed, it's deeply suboptimal for privacy.\n\nThere's a more complex scheme which is possible, using round-trip\npayments (I think this was originally from Christian Decker?); I make a\npayment via you and back to myself, it's just that I pay your node an\nabnormally high \"fee\".  But unfortunately for security reasons each\nencrypted hop contains the amount it expects to be sent, which doesn't\nwork if I don't know how much you're going to refund.\n\nTechnically, you can put a really small amount in there (each node only\ninsists that the amount sent is >= this amount), but this just allows\none of those return nodes to untracably steal the extra refund amount.\n\nSo, we really need to be able to include a (smaller) return onion to\nfix this properly.  I've added that to:\n\n        https://github.com/lightningnetwork/lightning-rfc/wiki/Brainstorming#refunds\n\nThanks!\nRusty."
            }
        ],
        "thread_summary": {
            "title": "BOLT 11, real time micro payments, and route redundancy",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Rusty Russell",
                "Andy Schroder",
                "Christian Decker"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 22321
        }
    },
    {
        "title": "[Lightning-dev] Route finding and route generation",
        "thread_messages": [
            {
                "author": "Johan Tor\u00e5s Halseth",
                "date": "2017-09-04T09:13:15",
                "message_text_only": "I haven\u2019t looked too closely at your proposal, but the first thing that hit me (2 weeks ago, sorry for not answering right away), is that instead of the basic flood-the-network-about-channels algorithm that currently is being used, this makes each route discovery request behave more or less like flooding (ask everybody within your local topology)?\nAlso it will be interesting to see when the network starts developing, I suspect that if you keep every node within 3 hops in your local topology, you end up storing most of the network anyway. So I think the routing algorithms will be far easier to optimize and analyze after some real world data :)\nThat being said, interesting idea, I\u2019m not dismissing it :D\n\nOn Tue, Aug 22, 2017 at 3:08, Billy Tetrud <billy.tetrud at gmail.com> wrote:\nHey Guys,\nI'm testing this mailing list out for the first time, so I'm probably gonna be doing it wrong.\nI want to talk about route discovery and route generation in the lightning network. It seems there's a couple types of things going on with routing: * Super basic flood-the-network style routing to get things up and running, as I believe is implicitly proposed here: https://github.com/lightningnetwork/lightning-rfc/blob/master/07-routing-gossip.md [https://github.com/lightningnetwork/lightning-rfc/blob/master/07-routing-gossip.md]\n   \n * More involved research projects that may not reach fruition any time soon. Eg this: http://bitfury.com/content/5-white-papers-research/whitepaper_flare_an_approach_to_routing_in_lightning_network_7_7_2016.pdf [http://bitfury.com/content/5-white-papers-research/whitepaper_flare_an_approach_to_routing_in_lightning_network_7_7_2016.pdf]\n   \n\nI'd like to discuss a near-term approach that can replace the basic flood-the-network style route discovery, but isn't so complicated that it needs a ton of study and work. This won't be the end-all solution to route discovery, but should take us a lot further than flood-the-network.\nI propose a protocol where each node knows about its own local network topology, and to find a final route, a transaction originator queries a number of its connections for routes to the intended destination. By doing this, it means that nodes are *not* receiving or storing the entire network topology, which makes route discovery a lot less taxing on the network (in terms of bandwidth and storage space).\nTo go into more detail...\nWhen a node joins the network: 1. it broadcasts its information to all its channels (pretty much as proposed here [https://github.com/lightningnetwork/lightning-rfc/blob/master/07-routing-gossip.md] ) announcing its relevant channel information 2. it requests local network topology information from all its channels for information about channels 1 hop beyond its direct connection (ie it will receive information about which addresses those channels are connected to, and their related fee info / etc) 3. it then requests topology information for channels 2 hops beyond, etc until it has filled its cache to its satisfaction (the user can choose some amount of megabytes as its limit of network topology data to store) 4. it also subscribes to topology changes for nodes at those distances (eg if a node has requested information from 3 hops away, it will receive info about any new channels or removed channels that fall within that distance)\nWhen a node receives an announcement message from a node joining the network: 1. it will store that node's info in its cache 2. it will also forward that info to any node that's subscribed to topology changes that fall within the relevant distance\nWhen a node wants to construct a route for a transaction: 1. It checks to see if it has a path to that node in its cache. If it does, it finds the cost of the cheapest path it has. 2. It asks all the channels on the edge of its cached local view for their cheapest path (however you want to define cheapest), specifying that it only care about paths with a maximum cost of the cheapest path it has already found in its cache. For example, if the node has nodes up to 3 hops away in its cache, it will *only* ask the nodes 3 hops away (it will not ask its direct connections, nor nodes 2 hops away, since it already has enough information to ignore them) 3. When it gets all its responses, it constructs a path\nWhen a node receives a path request from a node: 1. It checks its cache for its cheapest cache-only path 2. It asks nodes on the edge of its cached local view for their cheapest path, specifying that it only cares about paths with a maximum cost of either its cheapest cache-only path or the max-cost specified by the requesting node minus the channel cost between it and the requesting node (whichever is cheaper). A node on the edge of its cached local view is *not* asked for route information if the cost to that node exceeds the max-cost this relay node would specify. 3. It reports the results that satisfy the max-cost requirements of the requesting node\nAnd that's it. All the route information can be encrypted and signed so relaying nodes can't read the information inside, and so the requesting source node can verify which nodes sent that information.\nThis protocol should keep both node-announcement messages *and* route request messages highly localized.\nThoughts?\n~BT\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20170904/66bc1247/attachment-0001.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2017-09-04T20:02:02",
                "message_text_only": "Hi,\n\nWelcome to the mailing list!\n\nReplying here rather than the issue you filed on the lighting-rfc repo as\nI'd say this is a better venue to discuss matters such as this. The\nstructure of the mailing list may be a bit strange initially, but once you\nget over that hump, I think you'll find it's rather pleasant to use. Also,\nif you stick around, you'll find that most members of the Bitcoin\ndevelopment community use technical mailing lists such as this one to\nshare ideas, propose new solutions, analyze existing schemes, etc.\n\nThis list has been a bit lower traffic recently than compared to the past\nas many of us have locked ourselves in dungeons coding without pants as\nwe're rapidly approaching the first mainnet release of our various\nlightning node software. As a result, replies may have a bit of delay, but\nthat's nothing to fret about!\n\n> I propose a protocol where each node knows about its own local network\n> topology, and to find a final route, a transaction originator queries a\n> number of its connections for routes to the intended destination.\n\nFor color, I'm one of the co-authors of the Flare paper. What you've\ndescribed in this email is close to the approach, but it utilizes a blind\nall-the-links flood towards the destination, rather than a DFS search\nguided by the distance metric proposed in the paper. One thing to note is\nthat Flare utilizes HORNET to allow senders to query their beacon nodes,\nand also nodes beyond the horizon of their beacon nodes in a private\nmanner.  By using the bi-directional communication circuit, we maintain\nrequester anonymity, and don't force those performing search to reveal\ntheir identity nor intent. Also note that Flare itself also uses an\ninitial routing cache for a neighborhood of N nodes.\n\nWhat you've described here is essentially Dynamic Source Routing (DSR)[1],\nwith a mix of components from Fisheye State Routing (FSR) making it a\nhybrid protocol that combines reactive and proactive elements in order to\nachieve its goals.\n\nOur current stop-gap is a pure proactive protocol, meaning that nodes\ngather all link state data and then make forwarding and\nrouting decisions based off of that. The clear trade off (as you point\nout), is the increase in table state and bandwidth incurred due to keeping\nup with the announcements. The upside is that the latency observed when\nattempting payments to random sections of the graphs are minimized.\nAdditionally, as we have complete information, we can make more\nintelligent path finding decisions, and also ensure that we collect a\nredundant set of routes for each payment. By ensuring that we collect a\nset of routes with high path diversity, we have many options to fall back\nto in the case of a routing failure with one of the earlier routes in our\nset.\n\nHowever, protocols that have a reactive element for each circuit\nestablishment may not necessarily scale better. This is due to the\ncomputational overhead of circuit establishment. Particularly in your\nDSR+FSR combo as the flood proceeds in all directions. As a result,\ncircuit establishment may have latency in the seconds as each random\npayment may potentially need to flood out in the graph in search of the\ndestination. Without a sort of distance metric to guide the search, it\nmay wastefully explore non-relevant sections, further increasing payment\nlatency and overhead for all other nodes. Finally, one aspect to consider\nis how DoS-able schemes like this that require flooding for each circuit\nestablishment are.\n\n> When a node wants to construct a route for a transaction:\n> 2. It asks all the channels on the edge of its cached local view for\n> their cheapest path\n\nSimply _asking_ nodes for a path to a destination defeats the point of\nusing onion routing at all. If one is prepared to make that tradeoff, then\nfar more scalable routing protocols can be explored as at that point, one\nwould move to distance vector based algorithms.\n\nVery happy to see that more folks are exploring alternative\nrouting/discovery solutions! In the future we'll definitely need to scale\nup the network.\n\nOne beauty of the way the system is laid out is that multiple\nheterogeneous routing protocols can be used within the network just as\nwithin the Internet (eBGP vs iBGP), so different sub-graphs can chose\nprotocols that achieve their goals in light of the various tradeoffs. I\nthink I'll follow up this post with a general survey of potential\napproaches I've come up with and come across in the literature along with\ntheir various tradeoffs, and possible paths forward for the network as a\nwhole.\n\n\n-- Laolu\n\n\n[1]: https://arxiv.org/pdf/1507.05724.pdf\n[2]: http://www.utdallas.edu/~ksarac/Papers/DSR.pdf\n[3]:\nhttp://nrlweb.cs.ucla.edu/publication/download/203/05_75_fisheye-state-routing-in.pdf\n\n\nOn Mon, Aug 21, 2017 at 6:09 PM Billy Tetrud <billy.tetrud at gmail.com> wrote:\n\n> Hey Guys,\n>\n> I'm testing this mailing list out for the first time, so I'm probably\n> gonna be doing it wrong.\n>\n> I want to talk about route discovery and route generation in the lightning\n> network. It seems there's a couple types of things going on with routing:\n>\n>    - Super basic flood-the-network style routing to get things up and\n>    running, as I believe is implicitly proposed here:\n>    https://github.com/lightningnetwork/lightning-rfc/blob/master/07-routing-gossip.md\n>    - More involved research projects that may not reach fruition any time\n>    soon. Eg this:\n>    http://bitfury.com/content/5-white-papers-research/whitepaper_flare_an_approach_to_routing_in_lightning_network_7_7_2016.pdf\n>\n> I'd like to discuss a near-term approach that can replace the basic\n> flood-the-network style route discovery, but isn't so complicated that it\n> needs a ton of study and work. This won't be the end-all solution to route\n> discovery, but should take us a lot further than flood-the-network.\n>\n> I propose a protocol where each node knows about its own local network\n> topology, and to find a final route, a transaction originator queries a\n> number of its connections for routes to the intended destination. By doing\n> this, it means that nodes are *not* receiving or storing the entire network\n> topology, which makes route discovery a lot less taxing on the network (in\n> terms of bandwidth and storage space).\n>\n> To go into more detail...\n>\n> When a node joins the network:\n> 1. it broadcasts its information to all its channels (pretty much as\n> proposed here\n> <https://github.com/lightningnetwork/lightning-rfc/blob/master/07-routing-gossip.md>)\n> announcing its relevant channel information\n> 2. it requests local network topology information from all its channels\n> for information about channels 1 hop beyond its direct connection (ie it\n> will receive information about which addresses those channels are connected\n> to, and their related fee info / etc)\n> 3. it then requests topology information for channels 2 hops beyond, etc\n> until it has filled its cache to its satisfaction (the user can choose some\n> amount of megabytes as its limit of network topology data to store)\n> 4. it also subscribes to topology changes for nodes at those distances (eg\n> if a node has requested information from 3 hops away, it will receive info\n> about any new channels or removed channels that fall within that distance)\n>\n> When a node receives an announcement message from a node joining the\n> network:\n> 1. it will store that node's info in its cache\n> 2. it will also forward that info to any node that's subscribed to\n> topology changes that fall within the relevant distance\n>\n> When a node wants to construct a route for a transaction:\n> 1. It checks to see if it has a path to that node in its cache. If it\n> does, it finds the cost of the cheapest path it has.\n> 2. It asks all the channels on the edge of its cached local view for their\n> cheapest path (however you want to define cheapest), specifying that it\n> only care about paths with a maximum cost of the cheapest path it has\n> already found in its cache. For example, if the node has nodes up to 3 hops\n> away in its cache, it will *only* ask the nodes 3 hops away (it will not\n> ask its direct connections, nor nodes 2 hops away, since it already has\n> enough information to ignore them)\n> 3. When it gets all its responses, it constructs a path\n>\n> When a node receives a path request from a node:\n> 1. It checks its cache for its cheapest cache-only path\n> 2. It asks nodes on the edge of its cached local view for their cheapest\n> path, specifying that it only cares about paths with a maximum cost of\n> either its cheapest cache-only path or the max-cost specified by the\n> requesting node minus the channel cost between it and the requesting node\n> (whichever is cheaper). A node on the edge of its cached local view is\n> *not* asked for route information if the cost to that node exceeds the\n> max-cost this relay node would specify.\n> 3. It reports the results that satisfy the max-cost requirements of the\n> requesting node\n>\n> And that's it. All the route information can be encrypted and signed so\n> relaying nodes can't read the information inside, and so the requesting\n> source node can verify which nodes sent that information.\n>\n> This protocol should keep both node-announcement messages *and* route\n> request messages highly localized.\n>\n> Thoughts?\n>\n> ~BT\n>\n>\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20170904/fe7bbb32/attachment-0001.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2017-09-04T20:48:54",
                "message_text_only": "Thanks for the information Laolu! I'm glad people smarter than me are\nworking on this : ) . Do you have any idea when Flare might be ready for\nreal-world use?\n\n~BT\n\nOn Mon, Sep 4, 2017 at 1:02 PM, Olaoluwa Osuntokun <laolu32 at gmail.com>\nwrote:\n\n> Hi,\n>\n> Welcome to the mailing list!\n>\n> Replying here rather than the issue you filed on the lighting-rfc repo as\n> I'd say this is a better venue to discuss matters such as this. The\n> structure of the mailing list may be a bit strange initially, but once you\n> get over that hump, I think you'll find it's rather pleasant to use. Also,\n> if you stick around, you'll find that most members of the Bitcoin\n> development community use technical mailing lists such as this one to\n> share ideas, propose new solutions, analyze existing schemes, etc.\n>\n> This list has been a bit lower traffic recently than compared to the past\n> as many of us have locked ourselves in dungeons coding without pants as\n> we're rapidly approaching the first mainnet release of our various\n> lightning node software. As a result, replies may have a bit of delay, but\n> that's nothing to fret about!\n>\n> > I propose a protocol where each node knows about its own local network\n> > topology, and to find a final route, a transaction originator queries a\n> > number of its connections for routes to the intended destination.\n>\n> For color, I'm one of the co-authors of the Flare paper. What you've\n> described in this email is close to the approach, but it utilizes a blind\n> all-the-links flood towards the destination, rather than a DFS search\n> guided by the distance metric proposed in the paper. One thing to note is\n> that Flare utilizes HORNET to allow senders to query their beacon nodes,\n> and also nodes beyond the horizon of their beacon nodes in a private\n> manner.  By using the bi-directional communication circuit, we maintain\n> requester anonymity, and don't force those performing search to reveal\n> their identity nor intent. Also note that Flare itself also uses an\n> initial routing cache for a neighborhood of N nodes.\n>\n> What you've described here is essentially Dynamic Source Routing (DSR)[1],\n> with a mix of components from Fisheye State Routing (FSR) making it a\n> hybrid protocol that combines reactive and proactive elements in order to\n> achieve its goals.\n>\n> Our current stop-gap is a pure proactive protocol, meaning that nodes\n> gather all link state data and then make forwarding and\n> routing decisions based off of that. The clear trade off (as you point\n> out), is the increase in table state and bandwidth incurred due to keeping\n> up with the announcements. The upside is that the latency observed when\n> attempting payments to random sections of the graphs are minimized.\n> Additionally, as we have complete information, we can make more\n> intelligent path finding decisions, and also ensure that we collect a\n> redundant set of routes for each payment. By ensuring that we collect a\n> set of routes with high path diversity, we have many options to fall back\n> to in the case of a routing failure with one of the earlier routes in our\n> set.\n>\n> However, protocols that have a reactive element for each circuit\n> establishment may not necessarily scale better. This is due to the\n> computational overhead of circuit establishment. Particularly in your\n> DSR+FSR combo as the flood proceeds in all directions. As a result,\n> circuit establishment may have latency in the seconds as each random\n> payment may potentially need to flood out in the graph in search of the\n> destination. Without a sort of distance metric to guide the search, it\n> may wastefully explore non-relevant sections, further increasing payment\n> latency and overhead for all other nodes. Finally, one aspect to consider\n> is how DoS-able schemes like this that require flooding for each circuit\n> establishment are.\n>\n> > When a node wants to construct a route for a transaction:\n> > 2. It asks all the channels on the edge of its cached local view for\n> > their cheapest path\n>\n> Simply _asking_ nodes for a path to a destination defeats the point of\n> using onion routing at all. If one is prepared to make that tradeoff, then\n> far more scalable routing protocols can be explored as at that point, one\n> would move to distance vector based algorithms.\n>\n> Very happy to see that more folks are exploring alternative\n> routing/discovery solutions! In the future we'll definitely need to scale\n> up the network.\n>\n> One beauty of the way the system is laid out is that multiple\n> heterogeneous routing protocols can be used within the network just as\n> within the Internet (eBGP vs iBGP), so different sub-graphs can chose\n> protocols that achieve their goals in light of the various tradeoffs. I\n> think I'll follow up this post with a general survey of potential\n> approaches I've come up with and come across in the literature along with\n> their various tradeoffs, and possible paths forward for the network as a\n> whole.\n>\n>\n> -- Laolu\n>\n>\n> [1]: https://arxiv.org/pdf/1507.05724.pdf\n> [2]: http://www.utdallas.edu/~ksarac/Papers/DSR.pdf\n> [3]: http://nrlweb.cs.ucla.edu/publication/download/203/05_\n> 75_fisheye-state-routing-in.pdf\n>\n>\n> On Mon, Aug 21, 2017 at 6:09 PM Billy Tetrud <billy.tetrud at gmail.com>\n> wrote:\n>\n>> Hey Guys,\n>>\n>> I'm testing this mailing list out for the first time, so I'm probably\n>> gonna be doing it wrong.\n>>\n>> I want to talk about route discovery and route generation in the\n>> lightning network. It seems there's a couple types of things going on with\n>> routing:\n>>\n>>    - Super basic flood-the-network style routing to get things up and\n>>    running, as I believe is implicitly proposed here: https://github.com/\n>>    lightningnetwork/lightning-rfc/blob/master/07-routing-gossip.md\n>>    <https://github.com/lightningnetwork/lightning-rfc/blob/master/07-routing-gossip.md>\n>>    - More involved research projects that may not reach fruition any\n>>    time soon. Eg this: http://bitfury.com/content/5-white-papers-\n>>    research/whitepaper_flare_an_approach_to_routing_in_\n>>    lightning_network_7_7_2016.pdf\n>>    <http://bitfury.com/content/5-white-papers-research/whitepaper_flare_an_approach_to_routing_in_lightning_network_7_7_2016.pdf>\n>>\n>> I'd like to discuss a near-term approach that can replace the basic\n>> flood-the-network style route discovery, but isn't so complicated that it\n>> needs a ton of study and work. This won't be the end-all solution to route\n>> discovery, but should take us a lot further than flood-the-network.\n>>\n>> I propose a protocol where each node knows about its own local network\n>> topology, and to find a final route, a transaction originator queries a\n>> number of its connections for routes to the intended destination. By doing\n>> this, it means that nodes are *not* receiving or storing the entire network\n>> topology, which makes route discovery a lot less taxing on the network (in\n>> terms of bandwidth and storage space).\n>>\n>> To go into more detail...\n>>\n>> When a node joins the network:\n>> 1. it broadcasts its information to all its channels (pretty much as\n>> proposed here\n>> <https://github.com/lightningnetwork/lightning-rfc/blob/master/07-routing-gossip.md>)\n>> announcing its relevant channel information\n>> 2. it requests local network topology information from all its channels\n>> for information about channels 1 hop beyond its direct connection (ie it\n>> will receive information about which addresses those channels are connected\n>> to, and their related fee info / etc)\n>> 3. it then requests topology information for channels 2 hops beyond, etc\n>> until it has filled its cache to its satisfaction (the user can choose some\n>> amount of megabytes as its limit of network topology data to store)\n>> 4. it also subscribes to topology changes for nodes at those distances\n>> (eg if a node has requested information from 3 hops away, it will receive\n>> info about any new channels or removed channels that fall within that\n>> distance)\n>>\n>> When a node receives an announcement message from a node joining the\n>> network:\n>> 1. it will store that node's info in its cache\n>> 2. it will also forward that info to any node that's subscribed to\n>> topology changes that fall within the relevant distance\n>>\n>> When a node wants to construct a route for a transaction:\n>> 1. It checks to see if it has a path to that node in its cache. If it\n>> does, it finds the cost of the cheapest path it has.\n>> 2. It asks all the channels on the edge of its cached local view for\n>> their cheapest path (however you want to define cheapest), specifying that\n>> it only care about paths with a maximum cost of the cheapest path it has\n>> already found in its cache. For example, if the node has nodes up to 3 hops\n>> away in its cache, it will *only* ask the nodes 3 hops away (it will not\n>> ask its direct connections, nor nodes 2 hops away, since it already has\n>> enough information to ignore them)\n>> 3. When it gets all its responses, it constructs a path\n>>\n>> When a node receives a path request from a node:\n>> 1. It checks its cache for its cheapest cache-only path\n>> 2. It asks nodes on the edge of its cached local view for their cheapest\n>> path, specifying that it only cares about paths with a maximum cost of\n>> either its cheapest cache-only path or the max-cost specified by the\n>> requesting node minus the channel cost between it and the requesting node\n>> (whichever is cheaper). A node on the edge of its cached local view is\n>> *not* asked for route information if the cost to that node exceeds the\n>> max-cost this relay node would specify.\n>> 3. It reports the results that satisfy the max-cost requirements of the\n>> requesting node\n>>\n>> And that's it. All the route information can be encrypted and signed so\n>> relaying nodes can't read the information inside, and so the requesting\n>> source node can verify which nodes sent that information.\n>>\n>> This protocol should keep both node-announcement messages *and* route\n>> request messages highly localized.\n>>\n>> Thoughts?\n>>\n>> ~BT\n>>\n>>\n>>\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20170904/46ed51f1/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Route finding and route generation",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Olaoluwa Osuntokun",
                "Billy Tetrud",
                "Johan Tor\u00e5s Halseth"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 25607
        }
    },
    {
        "title": "[Lightning-dev] Lightning-dev Digest, Vol 25, Issue 1",
        "thread_messages": [
            {
                "author": "Billy Tetrud",
                "date": "2017-09-04T09:52:55",
                "message_text_only": "Johan,\n\n\" I suspect that if you keep every node within 3 hops in your local\ntopology\"\n\nNot every node would store 3 hops. Maybe for some nodes 2 hops is the right\nnumber. Or if you're connected directly with a huge hub, maybe you would\nonly be able to store your direct connections and the 2nd hops for only\n*some* of your connections. Or maybe the algorithm could aim for a certain\namount of topology data for each connection (eg 10MB per connection) so\nthat you'll only store a small percentage of 2nd hops for a large hub,\nwhile still be able to store 5th or 6th hops for long chains of nodes with\nfew connections.\n\n~BT\n\n\n> ---------- Forwarded message ----------\n> From: \"Johan Tor\u00e5s Halseth\" <johanth at gmail.com>\n> To: Billy Tetrud <billy.tetrud at gmail.com>\n> Cc: lightning-dev at lists.linuxfoundation.org\n> Bcc:\n> Date: Mon, 04 Sep 2017 11:13:15 +0200\n> Subject: Re: [Lightning-dev] Route finding and route generation\n> I haven\u2019t looked too closely at your proposal, but the first thing that\n> hit me (2 weeks ago, sorry for not answering right away), is that instead\n> of the basic flood-the-network-about-channels algorithm that currently is\n> being used, this makes each route discovery request behave more or less\n> like flooding (ask everybody within your local topology)?\n>\n> Also it will be interesting to see when the network starts developing, I\n> suspect that if you keep every node within 3 hops in your local topology,\n> you end up storing most of the network anyway. So I think the routing\n> algorithms will be far easier to optimize and analyze after some real world\n> data :)\n>\n> That being said, interesting idea, I\u2019m not dismissing it :D\n>\n> On Tue, Aug 22, 2017 at 3:08, Billy Tetrud <billy.tetrud at gmail.com> wrote:\n>\n> Hey Guys,\n>\n> I'm testing this mailing list out for the first time, so I'm probably\n> gonna be doing it wrong.\n>\n> I want to talk about route discovery and route generation in the lightning\n> network. It seems there's a couple types of things going on with routing:\n>\n>    - Super basic flood-the-network style routing to get things up and\n>    running, as I believe is implicitly proposed here: https://github.com/\n>    lightningnetwork/lightning-rfc/blob/master/07-routing-gossip.md\n>    <https://github.com/lightningnetwork/lightning-rfc/blob/master/07-routing-gossip.md>\n>    - More involved research projects that may not reach fruition any time\n>    soon. Eg this: http://bitfury.com/content/5-white-papers-research/\n>    whitepaper_flare_an_approach_to_routing_in_lightning_\n>    network_7_7_2016.pdf\n>    <http://bitfury.com/content/5-white-papers-research/whitepaper_flare_an_approach_to_routing_in_lightning_network_7_7_2016.pdf>\n>\n> I'd like to discuss a near-term approach that can replace the basic\n> flood-the-network style route discovery, but isn't so complicated that it\n> needs a ton of study and work. This won't be the end-all solution to route\n> discovery, but should take us a lot further than flood-the-network.\n>\n> I propose a protocol where each node knows about its own local network\n> topology, and to find a final route, a transaction originator queries a\n> number of its connections for routes to the intended destination. By doing\n> this, it means that nodes are *not* receiving or storing the entire network\n> topology, which makes route discovery a lot less taxing on the network (in\n> terms of bandwidth and storage space).\n>\n> To go into more detail...\n>\n> When a node joins the network:\n> 1. it broadcasts its information to all its channels (pretty much as\n> proposed here\n> <https://github.com/lightningnetwork/lightning-rfc/blob/master/07-routing-gossip.md>)\n> announcing its relevant channel information\n> 2. it requests local network topology information from all its channels\n> for information about channels 1 hop beyond its direct connection (ie it\n> will receive information about which addresses those channels are connected\n> to, and their related fee info / etc)\n> 3. it then requests topology information for channels 2 hops beyond, etc\n> until it has filled its cache to its satisfaction (the user can choose some\n> amount of megabytes as its limit of network topology data to store)\n> 4. it also subscribes to topology changes for nodes at those distances (eg\n> if a node has requested information from 3 hops away, it will receive info\n> about any new channels or removed channels that fall within that distance)\n>\n> When a node receives an announcement message from a node joining the\n> network:\n> 1. it will store that node's info in its cache\n> 2. it will also forward that info to any node that's subscribed to\n> topology changes that fall within the relevant distance\n>\n> When a node wants to construct a route for a transaction:\n> 1. It checks to see if it has a path to that node in its cache. If it\n> does, it finds the cost of the cheapest path it has.\n> 2. It asks all the channels on the edge of its cached local view for their\n> cheapest path (however you want to define cheapest), specifying that it\n> only care about paths with a maximum cost of the cheapest path it has\n> already found in its cache. For example, if the node has nodes up to 3 hops\n> away in its cache, it will *only* ask the nodes 3 hops away (it will not\n> ask its direct connections, nor nodes 2 hops away, since it already has\n> enough information to ignore them)\n> 3. When it gets all its responses, it constructs a path\n>\n> When a node receives a path request from a node:\n> 1. It checks its cache for its cheapest cache-only path\n> 2. It asks nodes on the edge of its cached local view for their cheapest\n> path, specifying that it only cares about paths with a maximum cost of\n> either its cheapest cache-only path or the max-cost specified by the\n> requesting node minus the channel cost between it and the requesting node\n> (whichever is cheaper). A node on the edge of its cached local view is\n> *not* asked for route information if the cost to that node exceeds the\n> max-cost this relay node would specify.\n> 3. It reports the results that satisfy the max-cost requirements of the\n> requesting node\n>\n> And that's it. All the route information can be encrypted and signed so\n> relaying nodes can't read the information inside, and so the requesting\n> source node can verify which nodes sent that information.\n>\n> This protocol should keep both node-announcement messages *and* route\n> request messages highly localized.\n>\n> Thoughts?\n>\n> ~BT\n>\n>\n>\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20170904/db2b4551/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Lightning-dev Digest, Vol 25, Issue 1",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Billy Tetrud"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 6826
        }
    },
    {
        "title": "[Lightning-dev] [MINUTES] Dev Meeting 2017-09-04",
        "thread_messages": [
            {
                "author": "Rusty Russell",
                "date": "2017-09-05T03:24:25",
                "message_text_only": "Highlights:\n\n        - No significant protocol changes.\n\n        - Rusty gets a lesson in bitcoin block endianness he somehow\n          missed previously.\n\nhttps://docs.google.com/document/d/1y_JcK66iXqfxLe1ZnlqvFv94MCNVnC6Q2LKVe6-Vu6U/edit?usp=sharing"
            }
        ],
        "thread_summary": {
            "title": "Dev Meeting 2017-09-04",
            "categories": [
                "Lightning-dev",
                "MINUTES"
            ],
            "authors": [
                "Rusty Russell"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 252
        }
    },
    {
        "title": "[Lightning-dev] [MINUTES] Dev Meeting 2017-09-18",
        "thread_messages": [
            {
                "author": "Rusty Russell",
                "date": "2017-09-19T21:03:15",
                "message_text_only": "Highlights:\n\n        - Onion error format will change\n        - Much deferred to 1.1\n\nhttps://docs.google.com/document/d/1i3rX8ZPWlqHVHuZ3DAsbAJSqxyboQidLSroYk9auFEo/edit#"
            }
        ],
        "thread_summary": {
            "title": "Dev Meeting 2017-09-18",
            "categories": [
                "Lightning-dev",
                "MINUTES"
            ],
            "authors": [
                "Rusty Russell"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 171
        }
    }
]