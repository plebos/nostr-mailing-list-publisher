[
    {
        "title": "[Lightning-dev] Routemap scaling (was: Just in Time Routing (JIT-Routing) and a channel rebalancing heuristic as an add on for improved routing success in BOLT 1.0)",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2019-04-01T02:21:53",
                "message_text_only": "Good morning Ariel,\n\n\n\n> > A good pruning heuristic is \"channel capacity\", which can be checked onchain (the value of the UTXO backing the channel is the channel capacity).\n> > It is good to keep channels with large capacity in the routemap, because such large channels are more likely to successfully route a payment than smaller channels.\n> > So it is reasonable to delete channels with low capacity when the routemap memory is becoming close to full.\n>\n> I'm generally concerned about these heuristics because any time nodes\n> can game a heuristic they will be expected to do so.\n> Gaming a heuristic can be good or bad depending on the side-effects.\n> One side effect of the \"channel capacity\" heuristic is more reliable\n> payments but another side effect is that low capacity (but possibly\n> reliable) channels are neglected\n\nThe heuristic is gameable at the cost of devoting more capacity to Lightning.\nIt is also quite incentive-compatible to source nodes with limited storage but which may need to forward arbitrary amounts to arbitrary nodes.\n\nLow capacity channels cannot be used at all if their capacity is below my payment amount, no matter how reliable they may be, unless I do multipart payments, which increases base fee paid.\n\n>\n> > It seems to me that s/aggregate-channel/high-capacity-channel/g in your idea would still work.\n> > In effect, the high-capacity channel is very likely to successfully route in either direction.\n> > But if by chance it falls into a state where it is unable to route in one direction or other, the intermediate node has other, lower-capacity channels that it can use JIT-Routing with to improve the directionality of the high-capacity channel.\n> > Nothing in the JIT-Routing idea requires that the rebalancing loop is small, only that a loop exists.\n> > Nodes still need to track their direct channels (so they are implicitly always in the routemap).\n> > But payee nodes making BOLT1 invoices could also provide `r` routes in the invoice, with the given routes being from nodes with high-capacity channels, so that even if the intermediate channels are pruned due to low capacity, it is possible to get paid.\n>\n> Without low capacity channels spontaneous payments would not work.\n\nThey would not be prevented a priori, only if all channels it has are too small to be kept in memory by typical source nodes.\n\nIf I truly wanted to help such a node, I might make a large capacity channel to it and then send my spontaneous payment.\n\n> Or\n> they would depend on asking for an invoice under the hood.\n\nWhich I think is better for spontaneous payments.\n\n> It feels to me that at some point, someone with complete knowledge of\n> the network needs to eb employed.\n\n\nIndeed.\nSee the trampoline payments thread also under discuasion.\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-04-01T03:31:31",
                "message_text_only": "Good morning Rene,\n\n>\n> Maybe I oversee something - in that case sorry for spamming the list - but I don't understand how you could know the distance from yourself if you don't know the entire topology? (unless u use it on the pruned view as you suggested)\u00a0\n\nThat is correct, and the reason that it would still lose determinism (and thus be hard to test anyway), as I lamented.\nAt least for a pruning heuristic like you proposed (i.e. probabilistically prune channels) it is possible to make deterministic with a fixed RNG seed.\n\nFor instance a node might concatenate its own public key with the short-channel-id (in some fixed byte order) of the channel, hash the concatenation, then use the hash as the seed of an RNG that gives 1 bit, and prune the channel if the bit is cleared (to remove half the channels in the routemap).\n\n>\n> On the other hand querying for a certain depth would be possible with the suggested `query ask egonetwork` in case for depth 3 one would have to peer with the nodes from the friend of a friend network.\u00a0\n\nBut for this, you would require that every node also keep every channel with depth less than your maximum query amount.\n\nPerhaps for friend-of-friend, it might be sufficient to just query a peer to give its own direct public channels, which it needs to remember anyway.\nBut that is basically only a depth of 2 (if we count depth of 1 as your direct channels).\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-04-03T04:41:44",
                "message_text_only": "Good morning list,\n\n> One way you could have both determinism and encourage a diverse distribution of network maps is to treat it as a spatial indexing problem, where the space we use is the lexicographical space of the node ids (or hashes of), borrowing some similarities from DHTs.\n>\n> If for example, we take a quadtree, you can take the 2 most-significant bits of the public key hash, which would put your node into one of 4 buckets. Nodes could advertise a feature bit indicating that they are committed to keeping the entire routemap of the bucket which matches their own public key hash, which would be all of the nodes in the same bucket - and all of the channels with one or both of their endpoints in the bucket.\n\nI would be hesitant to divide the world in such manner.\nI understand that in typical computer science, splitting your objects up into smaller parts is a long-accepted method of doing things.\n\nHowever, when it comes to finances and political power (the power to censor or disrupt), such splitting is highly undesirable.\n\nThus I am strongly allergic to things that would \"hierarchical\"ize or \"bucket\"ize or even just have a separation between \"endpoint\" and \"core\" network.\n\nI would rather propose for acceptance into BOLT, such proposals that would keep the network homogeneous.\n\nVarious nodes may have different resources (BTC, CPU, memory, bandwidth, latency).\nSuch inequalities are inevitable in this universe.\n\nBut these nodes should still, as much as we can, remain peers on the network.\n\n(I am not responding m.a.holden specifically, but am addressing the entire list)\n\n\nThus let me propose instead:\n\n1.  Nodes already have \"addresses\" - the public key that identifies the node.\n2.  We can hash this address, then treat the hash as a 256-bit signed number in two's complement notation.\n3.  From this, we can synthesize an artificial \"distance\" measure: dist(x, y) = abs(as_signed(hash(x)) - as_signed(hash(y)).\n\n\nThen, we can have the following global rule:\n\n* Given three nodes X, Y, and Z, and dist(X, Z) < dist(Y, Z), then X SHOULD be more likely to know a route from itself to Z, than Y to know a route from itself to Z.\n\nThis rule can be probabilistically fulfilled by having each node N know *at least* routes to some nodes Ls..., where for all L <- Ls, dist(N, L) < some constant.\nThe constant here can be selected by each node independently, depending on its memory and CPU capacity.\n(the node knowing more routes than that is perfectly fine)\n\n\nThus, suppose some node N is selected as trampoline.\nIt is unable to locate the next node D in the trampoline onion.\nHowever, it knows some nodes Ls in its local routemap.\nIt looks for any node L <- Ls such that dist(L, D) < dist(N, D).\nIn fact, it can just sort those nodes according to dist(L, D) and start with the lowest distance, and fail once it reaches a dist(L, D) that exceeds its own dist(N, D).\n\nThen it can delegate the routing to D to L, by rewrapping the onion to tell L to give it to D.\nIf node L itself does not know D, it does the same algorithm.\n\nThe above algorithm converges since dist(N, D) for each N that is delegated to will progressively get smaller until we reach some N that knows the destination D.\n\n\nAt the same time, nodes are equal and there are no a priori divisions/distinctions between nodes.\nAll nodes remain peers, there are no special \"flare nodes\", no special \"knows the entire map\" nodes, no special \"knows my octant\" nodes, no \"endpoint\" nodes, no hubs and spokes.\nThe network remains homogeneous and all are still peers, some might have smaller routemaps than others, but all are still equal in the network, as all things should be.\n\n\nIt does require that trampoline routing allow a trampoline to delegate searching for the next trampoline to another node.\n\n\n--\n\n\nNow, we want to be able to consider, how can each node N prune its routemap Ls such that it prioritizes keeping routes to those nodes with low dist(N, L)?\n\nI present here an algorithm inspired by mark-and-sweep garbage collection.\nImprovements in GC algorithms might be possible to adapt to this algorithm.\nFor instance, it might be useful to be able to perform the marking operation concurrently with gossiping.\n\nWe define two thresholds, T1, and T2, on the number of channels+nodes in the routemap.\n(this can be changed to the number of bytes each channel/node takes up in the routemap, plus whatever bytes are needed for whatever bookkeeping structures are needed for fast routing through the routemap, but the principle remains the same)\n\nT1 and T2 must be reasonably far from each other.\n\nEach channel and node includes a \"mark\" bit.\n\nWhile gossiping, we allocate new channels and nodes and keep track of how many are allocated.\nOnce the number reaches T2, we trigger the mark-and-sweep algorithm.\n\n1.  Clear all mark bits and set a \"number of marked objects\" variable to 0.\n2.  Sort nodes from smallest to largest dist(N, L), where N is your own node and L is the node being sorted.\n3.  While number of marked objects < T1:\n    1.  Get the next node L in the sorted sequence of nodes.\n    2.  Find a route from L to N.\n    3.  If route found:\n        1.  Mark all channels and nodes on that route.\n            Increment the number of marked objects if changing from unmarked to marked.\n    4.  If route not found:\n        1.  Mark all channels and nodes within some channel distance of L on the routemap.\n            Increment the number of marked objects if changing from unmarked to marked.\n4.  Prune all unmarked nodes and channels.\n    Set the number of allocated nodes and channels correctly from the marked nodes and channels.\n    (the node might keep at least the node addresses of unmarked nodes around so that it has a larger pool of nodes to put in a trampoline onion, but that can be kept in a flat array without pointers to its channels)\n\nThe above algorithm results in a limit on the routemap size, but allows dynamic changes in the routemap to occur.\n\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "m.a.holden",
                "date": "2019-04-04T07:08:44",
                "message_text_only": "Good morning ZmnSCPxj, thanks for the response.\n\n\n> I would be hesitant to divide the world in such manner.\n> I understand that in typical computer science, splitting your objects up into smaller parts is a long-accepted method of doing things.\n\n> However, when it comes to finances and political power (the power to censor or disrupt), such splitting is highly undesirable.\n\n> Thus I am strongly allergic to things that would \"hierarchical\"ize or \"bucket\"ize or even just have a separation between \"endpoint\" and \"core\" network.\n\n> I would rather propose for acceptance into BOLT, such proposals that would keep the network homogeneous.\n\nFirstly, I completely agree with you that we should not be splitting up the network in any way which nodes or channels can be politically targeted, or in which any node is designated special privelege or status. I have the same allergy as you and took this into considerations when proposing this, and some of your suggestions are along a simlar  line of thinking as mine.\n\nI think you might have misunderstood what I was proposing, but it's probably my fault for not expressing it well. With my suggestion, all nodes continue to be equal participants and there are no special nodes. I used the term \"endpoint\" previously to mean one of the two nodes which a regular channel belongs to, and not to mean some kind of special node. Any node can open channels to any other node - the \"buckets\" are merely a strategy for locally organizing gossip so that it can be made more efficient. The term \"buckets\" is used in the descriptions of other DHTs such as Kademlia too, and they don't refer to splitting up the global network, they merely provide a perspective for looking at subsections of it.\n\nI'm certainly interested in any solutions which keep the network permissionless because we really don't want to end up with DNS 2.0 or similar.\n\n> Various nodes may have different resources (BTC, CPU, memory, bandwidth, latency).\n> Such inequalities are inevitable in this universe.\n> But these nodes should still, as much as we can, remain peers on the network.\n\nMy suggestion accounts for the difference in computational requirements, as each node can determine its own depth in  the tree based on the approximate quantity of information it wishes to gossip. A node could filter information to whichever depth of the tree they wished, by setting the bucket size at which they spill. This also allows for the size of gossip to dynamically shrink as the network grows, and is similar to garbage collection, in which anything which isn't part of the destination bucket on spilling is purged.\n\nNodes could also pick (multiple) specific quadtree buckets to communicate all gossip about through filters they negotiate (the filter being the hash prefix of the desired bucket). It might be desirable to broadcast their filter as part of the gossip itself, so that other nodes can learn who are the better information providers, but again this would be completely optional.\n\n---\n\n> This rule can be probabilistically fulfilled by having each node N know *at least* routes to some nodes Ls..., where for all L <- Ls, dist(N, L) < some constant.\n> The constant here can be selected by each node independently, depending on its memory and CPU capacity.\n(the node knowing more routes than that is perfectly fine)\n\nThe quadtree proposes a similar idea, but instead of using some constant, N knows about routes to Ls, where each L is within the same bucket. Essentially, i < dist(N, L) <= j, where i=BUCKET_MIN and j=BUCKET_MAX. For example, if using 8-bit ids and the first two bits identify the bucket, then i=00000000, j=00111111. I guess the equivalent distance using your constant would be k=00100000, to cover the same range but without discriminating the results based on any boundaries like my suggestion does.\n\n> Then, we can have the following global rule:\n> * Given three nodes X, Y, and Z, and dist(X, Z) < dist(Y, Z), then X SHOULD be more likely to know a route from itself to Z, than Y to know a route from itself to Z, is also met for nodes which are in different buckets.\n\nThis rule is essentially the same as what I was thinking for the distance between buckets. With the autopilot suggestion of decreasing the number of channels opened as distance increases, the probability of knowing about a neighbouring bucket is increased compared with a distant one.\n\n---\n\n> It looks for any node L <- Ls such that dist(L, D) < dist(N, D).\n> In fact, it can just sort those nodes according to dist(L, D) and start with the lowest distance, and fail once it reaches a dist(L, D) that exceeds its own dist(N, D).\n> The above algorithm converges since dist(N, D) for each N that is delegated to will progressively get smaller until we reach some N that knows the destination D.\n\nThis proposal also seems very similar to how existing DHTs work, unless I'm mistaken. My problem with the approach of existing DHTs is that they are suboptimal for the number of queries which must be made to find a route - which is worst-case O(log n) for example, in Chord or Kademlia. This isn't a problem for things like file-sharing where latency isn't a major concern, but we don't really want to be waiting for a bunch of queries if we're stood in queue to pay for a coffee. (Given also that some routes may fail due to inherent constraints of the LN itself). As the network grows, the efficiency of route finding declines too.\n\nI came up with the idea of using the quadtree specifically for trying to reduce the maxmimum (or typical) route length to query where possible, at the expense of storing much more information than existing DHTs, but trying to get reasonable savings on resources. Although the worst-case query cost is still O(log n), it seems that this is unlikely to occur and O(1) seems plausible for small depth.\n\nThe other expense is that this approach will not find the most optimal routes, as it prioiritizes considering the smallest number of buckets. However, it is not possible to know the most optimal path without knowing about the entire network topology anyway, so this problem exists with the DHT too. I've optimized for reduced querying.\n\n> All nodes remain peers, there are no special \"flare nodes\", no special \"knows the entire map\" nodes, no \"knows my octant\" nodes, no \"endpoint\" nodes, no hubs and spokes.\n\nThere are no special nodes in my approach, only a commitment to maintain information about nodes (and their channels) whose PKH prefix matches your own at the depth you have chosen to gossip, regardless of whether or not they've advertized the same feature. A node expressing depth=0 would be a \"knows the entire map\" node, but it does not give them any special status.\n\n> The network remains homogeneous and all are still peers, some might have smaller routemaps than others, but all are still equal in the network, as all things should be.\n\nAll nodes are still equal, but the network isn't entirely homogeneous (in the distribution of channels) because of the autopilot suggestions which prioritize local channels and deprioritize distant channels, with the goal of improving efficiency. I guess there is some trade-off with this approach, but it's still worth considering, because \"perfectly unstructured\" isn't the end goal - there may be some middle ground between that and other approaches which tend towards centralization.\n\nIt may even be harmful to try to keep it perfectly unstructured, because with no structure, the network will effectively be \"maximally inefficient\" for those not maintaining sufficient gossip information. If by default, it is inefficient to find a route, then there will inevitably be somebody who will fill that market gap. Big players will have no problem keeping knowledge of the whole network and giving information about paths in just one query - in exchange for people's data. Like DNS.\n\nA semi-structured approach might provide enough incentive for everyone to keep as much information locally as realistic for the resources they have, and like Adam Smith's invisible hand, by doing so they not only benefit themselves, but they benefit everybody by improving the efficiency of the network overall. The network would become \"reasonably efficient\" for everyone, rather than inefficient for most except the few with sufficient resources to maintain a useful routemap.\n\n---\n\nI'll try to convey by example how I think censorship should not be cause for concern with this quadtree, and what savings might be expected. This makes some crude assumptions using the current network size, with my previous suggestions for autopilot (ie, nodes open 1/2 their channels to nodes in their own bucket, and decreasing with distance), and assumes uniform distribution of node ids (we assume there is no numeric bias in the hashes of public keys).\n\nThere are currently ~4000 nodes with an average of ~20 channels per node (~40,000 channels) which we need to keep information about at depth=0.\n\nIf we spill to depth=1, then ~1,000 nodes will go into each bucket. If each has (on average) ~10 channels to other nodes in the same bucket, then there will be ~5,000 \"intra-bucket\" channels in each bucket. Each node also has ~10 channels to nodes in other buckets (\"inter-bucket\" channels), which is another ~10,000 channels it maintains information for.\n\nNote that \"intra-bucket\" and \"inter-bucket\" channels are just plain channels - there's nothing special about them and they are merely the difference in perspective which each node will view them based on whether either node's PKH prefix is the same bucket as their own PKH prefix at the depth which they filter gossip.\n\nSince we've filtered out all channels where neither node is in our own bucket, we now only need to know about ~15,000 channels in total, compared to the original 40,000 (~37.5% of total channels in the network). We still need to know about 1,000 nodes in our own bucket, and we keep information about the nodes at the other end of the 10,000 inter-bucket channels we know about, which could still be anything up to 100% of the nodes in the global network, but nodes will be filtered if none of the 10,000 inter-bucket channels we know about belong to them, so nodes we track information about is 1000 < N <= 4000.\n\nWe don't suddenly have no fault-tolerance or a censorship threat to find a node in another bucket here. Therea are still ~3,333 known channels on average between our bucket and each of the other 3 buckets at depth=1. Since they keep all of the gossip for their own bucket, then the chance that any of them know the destination for a payment is likely - so the number of queries you would typically need to make is 1 (or several *in parallel*). The node you query should know the remaining route from himself to the destination. You may chose to query more information for potential privacy enhancement.\n\nIf we spill to depth=2, then we now have 16 total leaf-buckets, with ~250 nodes each. Each bucket would have ~1250 intra-bucket channels, and ~2500 inter-bucket channels. Information requirement per bucket is now ~3750 channels (~10% of the total network, which is a reasonable saving). Of the inter-bucket channels, 1/2 of them are to the 3 sibling buckets, making ~400 potential routes to any of the siblings, and the other 1/2 are to \"cousin\" buckets, of which there are 12, resulting in ~100 channels on average to those buckets. Still likely sufficient to have a typical query of 1, with fault-tolerance.\n\nAt depth=3 it is ~62 nodes per bucket, ~310 intra-bucket channels, ~620 inter-bucket channels and 64 possible leaf-buckets. resulting in ~100 channels to each sibling, ~13 channels to each cousin, and just 1 or 2 channels to each second-cousin (the most distant buckets).\n\nOnly at this point does querying start to become potentially expensive if you want to make the payment to a distant node. You might still have some direct routes to the second-cousin buckets, but not much fault-tolerance. However, your cousins or the siblings of your second-cousin have a higher probability of knowing about a node at that distance than local nodes will, so you still have numerous options for discovering a node but it might take multiple queries. Queries would use a similar greedy algorithm approach to the one you have suggested. It should still be significantly less than worst-case query cost.\n\nResource constrained devices might spill to these limits at the cost of requiring more queries for payments. In practice, you probably wouldn't go beyond depth=2 as above unless the global network gets sufficiently large that bandwidth requirements at depth=2 are a problem, which probably isn't going to be the case until the network is a magnitude larger than it already is - and in which case the number of \"inter-bucket\" channels will be tenfold the current amount and spilling to depth=4 or 5 may become plausible.\n\nThere is also potential for analysts to find out which buckets do not have any, or have few channels between them, and to take the opportunity to fill that gap to try and benefit from routing fees, as those new channels would be prioritized over longer routes which span multiple buckets. The autopilot suggestions are only a starting point, but eventually it will be mostly market driven.\n\n---\n\nMy idea is not fully researched, but since the topic was raised, I decided to share my incomplete thoughts about it. The choice of a quadtree itself was quite arbitrary, but it seemed reasonable after briefly considering alternative arity trees, and the potential for linking this to approximate geographic location to optimize local payments seemed like it could be valuable too.\n\nI'll give some more thought to your suggestions and reconsider my own with these new suggestions.\n\nRegards,\nMark H"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-04-05T05:50:58",
                "message_text_only": "Good morning m.a.holden,\n\n>\n> I think you might have misunderstood what I was proposing, but it's probably my fault for not expressing it well. With my suggestion, all nodes continue to be equal participants and there are no special nodes. I used the term \"endpoint\" previously to mean one of the two nodes which a regular channel belongs to, and not to mean some kind of special node. Any node can open channels to any other node - the \"buckets\" are merely a strategy for locally organizing gossip so that it can be made more efficient. The term \"buckets\" is used in the descriptions of other DHTs such as Kademlia too, and they don't refer to splitting up the global network, they merely provide a perspective for looking at subsections of it.\n\nThis \"bucket\"ization, unfortunately, allows the possibility of targeted attacks within particular buckets (which I will describe later below).\nIt is this I refer to, when I bring up the possibility of politically-motivated attacks.\n\n> I came up with the idea of using the quadtree specifically for trying to reduce the maxmimum (or typical) route length to query where possible, at the expense of storing much more information than existing DHTs, but trying to get reasonable savings on resources. Although the worst-case query cost is still O(log n), it seems that this is unlikely to occur and O(1) seems plausible for small depth.\n\nI believe that for trees, lookup is only O(log n) if perfectly balanced, as all things should be.\nFor trees, worst case is always O(n) if the possibility of unbalanced trees exists.\nAnd in a scenario where nobody controls the data that can be placed in a tree, you can always expect unbalanced trees in attacks.\n\nNow, it seems to me what you propose, is to have octrees contain octrees, and so on.\nSome node promises to keep track of the map, at some level of octree fineness.\n\nSuppose there is a node I wish to attack, for whatever reason (probably politically-motivated).\nNow, I know there are N nodes that have promised to keep track of the octant the target is on.\n\nI can overload those N nodes by generating node addresses that also lie in that octant.\nBy just iterating over scalars, about 1/8 of the generated node addresses will lie in the target octant.\nOnce I get a node address in the targeted octant, I can commit a nominal amount of Bitcoin into some 2-of-2 with an existing actual node I control and the generated node address, then synthesize some node gossip containing those node addresses.\nThis unbalances the octree such that one octant has a far lerger number of nodes inside it than the other octants.\n\nThe N nodes that promised to keep track of the routemap within the octant find that they need to take up more and more memory to store the octant data because of this targeted attack.\nEventually, the N nodes start dropping out of this responsibility since they run out of resources.\n\nOnce the number of nodes that hold that octant drops, I can then switch to targeted DDoS attacks on the remaining octant-mapping nodes, especially easy to locate them since they openly broadcast the fact that they promise to map the octant they are in.\n\nNow the octant becomes hard to access for 7/8 of the network.\nI have successfully executed a partitioning attack and disrupted the operation of an entire octant.\n\nThis is not a situation I would like to enable, especially since we can expect politically-motivated attacks on Bitcoin and Lightning.\n\nNow it is possible I misunderstand what you are proposing.\nIs the above attack scenario plausible?\n\nThis is the primary reason why I think it is dangerous to split the network, even by just considering subsections of the network.\nWhile we must at some point operate on network subsections as the network grows, I think we should hold the following position when designing:\n\n* No node shall reveal the extent of its knowledge of the network, since if it reveals that it knows the entire network, it may become a target for attack in order to degrade the network.\n   * This also implies that if a node does not know the location of some node, instead of admitting its ignorance, it should instead delegate to another node that might have better information; otherwise it would be possible to profile nodes to determine how much of the network they know.\n\nThe intent is not to have targets.\nNodes that openly broadcast that they know the nodes within the same octant they are, are nodes that want to be DDoS'ed.\n\nThus, my definition of a \"special\" node is a lot looser than you seem to define it.\nAnything that makes it possible to point to a node and say \"this is a good node to attack, in order to disrupt Lightning\" is special.\n\nI expressed, a similar opinion in the Trampoline Routing thread: https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-April/001967.html\n\n\n> There are currently ~4000 nodes with an average of ~20 channels per node (~40,000 channels) which we need to keep information about at depth=0.\n> <snip>\n\nI appreciate you did the calculations, but I would like to point out that in an attack scenario, your computations will become meaningless as the attacker can trivially manufacture fake nodes to cause whatever buckets to overflow etc.\n\n------\n\nNow let us return, to my proposal of a distance measurement.\nThis effectively maps the LN universe onto a circle.\nEach node commits itself to storing an arc of this circle, and possibly various other nodes it happens to be directly connected to that may be far from the arc near it.\nCrucially no node admits to how large an arc of this circle they map out, only that they are more likely to map points nearer to themselves.\n\nIn order to disrupt a particular node, I must place fake nodes near that node, in an attempt to force its neighbors to reduce the arc of the circle that they can map.\nHowever, I need to generate fake nodes that are nearer to that node than genuine honest nodes.\nThis means the probability of me generating such node addresses are much lower than 1/8, thus requiring that I devote more energy to generating the falsified attack nodes.\n\nFurther, in executing this attack, while I disrupt one node very well, and nearby nodes somewhat, my effect will be far less than disrupting 1/8 of the network.\n\nFinally, nodes can easily use heuristics to filter out attack patterns that have been identified.\nThis is because nodes only commit to *probabilistically* being more likely to know nodes near to them, than nodes that are not near to them.\nThey do not commit, as in your proposal, to absolutely knowing everything within their committed area.\nThis lets nodes degrade their performance more gracefully.\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "m.a.holden",
                "date": "2019-04-06T02:40:25",
                "message_text_only": "Good morning ZmnSCPxj,\n\nI'll try to clarify my proposal further, but also have some questions about yours.\n\n---\n\n> Now, it seems to me what you propose, is to have octrees contain octrees, and so on.\n\nThere's one global tree, which is the same for all users. Every node in the tree has a bucket and exactly 4 child nodes, except leaves have no children. The tree has a max depth, which each client sets itself. The theoretical maximum being HASHLEN/2 (In practice, we're likely to be concerned about <8). Note that every parent's bucket contains all of the information of all of its children's buckets - meaning the root node of the global quadtree is equivalent to the unfiltered global network. Nodes pick a depth and concern themselves with the bucket at that depth, unless it overflows, in which case they increase the depth by 1.\n\n> Now let us return, to my proposal of a distance measurement.\n> This effectively maps the LN universe onto a circle.\n> Each node commits itself to storing an arc of this circle, and possibly various other nodes it happens to be directly connected to that may be far from the arc near it.\n\nThe quadtree can also be looked at from the perspective of a circle, assuming there is a reference point P on it. Each bucket is represented by an arc of size 2pi/(4^depth), and the PKH-prefix represents a displacement of the arc from P (Eg, the bucket 00b would represent an arc from P to pi/2+P). Bucket 00b includes all gossip information for the buckets 0000b, 0001b, 0010b and 0011b, which are sub-arcs of it, and so forth. Spilling a bucket is the same as narrowing the arc to 1/4 its previous size.\n\nAlthough different from your arbitrary distance k (arc size 2*k?), they're not too dissimilar in the kind of distance covered - with the primary distinction being that mine proposes using set interval ranges (based on powers of 4), and the node picks the range which it's PKH fits into, rather than the range being centred on the node.\n\n---\n\n> I can overload those N nodes by generating node addresses that also lie in that octant.\n> By just iterating over scalars, about 1/8 of the generated node addresses will lie in the target octant.\n\nIf you target the first layer of the tree only, then once nodes spill to the second layer, they will filter out any gossip which does not concern them. It isn't sufficient to just brute force the first 2 bits, but you need to do it for 2^depth, where depth is the target's maximum they're willing to spill to (which is not shared).\n\nHowever, commodity hardware can attempt millions of ec_mult/hash per second, so getting a node into the bucket you want is trivial anyway for small depth.\n\n> In order to disrupt a particular node, I must place fake nodes near that node, in an attempt to force its neighbors to reduce the arc of the circle that they can map.\n> However, I need to generate fake nodes that are nearer to that node than genuine honest nodes.\n> This means the probability of me generating such node addresses are much lower than 1/8, thus requiring that I devote more energy to generating the falsified attack nodes.\n\nI would argue that the above is also true in your approach. It would still be trivial to brute force the prefixes which minimize the distance between themselves and the target, even with a network of tens of millions of nodes. The amount of energy which would need devoting does not seem like it would a deciding factor for this attack.\n\n> Further, in executing this attack, while I disrupt one node very well, and nearby nodes somewhat, my effect will be far less than disrupting 1/8 of the network.\n\nSince the attack needs to target the maximum depth that nodes might spill to, then the amount of the network which could be affected by the attack would be 1/4^depth. I can't imagine it being very different in your distance based approach, since we're considering the same kind of distances, from a different perspective.\n\n---\n\n> Once I get a node address in the targeted octant, I can commit a nominal amount of Bitcoin into some 2-of-2 with an existing actual node I control and the generated node address, then synthesize some node gossip containing those node addresses.\n\nThe way I see it, this potential attack affects the global network generally. Somebody could try this regardless of our proposals, and try to flood the whole network with spam.\n\nBOLT#7 only states that a node MAY forward information about nodes for which it does not know any channels. One could reconsider this approach if constrainted on resources, such as to say if depth>X, MUST NOT forward any information about nodes with no known channels, or perhaps have a variation of gossip which combines node information into channel broadcasts to ensure that such node spam can't occur. I think the vagueness of the spec on this rule is inidicative that it needs addressing.\n\nTo try and mount an attack with greater chance of success, the attacker should need to open many new channels, for which they face obvious constraints on the bitcoin network, and real costs. And since the attack can only attempt to degrade LN at best, and offers no guarantee of censoring, it seems unlikely that an attacker would exhaust significant resources to try and target somebody with a low chance of success.\n\nThe attacker would also likely use nominal amounts for the channels they're trying to attack with, as they would not want to lock up significant funding. It may be trivial to filter many tiny capacity channels as they're not likely to be useful for making payments anyway.\n\n---\n\n> This unbalances the octree such that one octant has a far lerger number of nodes inside it than the other octants.\n> The N nodes that promised to keep track of the routemap within the octant find that they need to take up more and more memory to store the octant data because of this targeted attack.\n> Eventually, the N nodes start dropping out of this responsibility since they run out of resources.\n\nEach node would treat the quadtree as a perfectly balanced one at the depth they're concerned. The buckets themselves vary in capacity, and if the gossip reaches capacity at any depth, the bucket spills to the next depth. If an attacker managed to overflow a leaf bucket, then the potential victim would need to consider another gossip approach.\n\nOn determining whether one is being attacked though, it should be possible to detect based on information size before and after spilling. If one is being specifically targeted, then the information before and after spilling will be almost the same, with little saving on gossip, where normally spilling might expect a 50%+ saving. A client could set a low threshold on the minimum saving they expect to get from spilling, and if it is not met, they might determine that they are being targeted, and take countermeasures.\n\nTo me it seems like your distance narrowing suffers the same issue. As the size of your gossip grows, you will narrow the distance k for which you concern yourself with gossip - but at some point you must have a minimum k, else it will be too narrow to have any useful information. What happens if you decrease k to some minimum, and the amount of spam still overflows the capacity limits you've set?\n\n> Once the number of nodes that hold that octant drops, I can then switch to targeted DDoS attacks on the remaining octant-mapping nodes, especially easy to locate them since they openly broadcast the fact that they promise to map the octant they are in.\n> Now the octant becomes hard to access for 7/8 of the network.\n> I have successfully executed a partitioning attack and disrupted the operation of an entire octant.\n\nThere are no \"octant-mapping nodes\". Every node knows about every channel from its own bucket to other buckets. This remains true even after spilling - although the quantity of information is reduced. Performing a DoS on a node who stores the whole network information does not affect other users, because they're not exclusive holder of information. Every single participant of the network could operate at depth=2. There does not need to exist anybody who has the full network map.\n\nAlso, if most nodes follow a reasonable autopilot strategy, then every node in a bucket also has at least some channels to other buckets - meaning the target of a DDoS would essentially be the entire bucket to try and completely isolate it. Remember that I'm suggesting half (or more) of the open channels in a bucket are to other buckets.\n\nA node doesn't necessarily need to broadcast the depth at which they gossip, but could indicate a deeper depth, so as to reduce the amount of queries made to it, and hide the truth about the full information they carry. They shouldn't indicate a smaller depth than the one at which they gossip as they'll be unable to answer to a majority of queries.\n\nIf a node is being targeted they could specify a large depth at which it would be infeasible for an attacker to target, whilst secretly maintaining information at a smaller depth, but by doing so, they're also stating that they're not going to be widely useful.\n\nI'm also not suggesting this be the only gossip strategy. At minimum I would also want friend-of-a-friend-of-a-friend type gossip in addition to this, because those channels are the ones we'll most likely be routing payments over anyway, so we should have them cached separately from this quadtree proposal. Several other gossip approaches could be used too.\n\n---\n\n> Crucially no node admits to how large an arc of this circle they map out, only that they are more likely to map points nearer to themselves.\n\n> This is because nodes only commit to *probabilistically* being more likely to know nodes near to them, than nodes that are not near to them.\n\nI can see how using a probabilistic approach might mitigate the issue to some extent, but it doesn't seem like a cure. Since the information nearest to you is more proabable to be forwarded to you, an attacker could still try to overwhelm your resources by plaing enough nodes nearby. How is the probability decided? Does your node probabilistically filter incoming information, or do you signal to your peers to probabilisitcally filter it?\n\nIn the latter case, while not necessarily admitting what you store, it seems like you'll still leak hints about it. What information are you proposing gets communicated (if any) between peers to filter gossip being sent?\n\nMy approach is based on the idea that the broadcaster will always know whether or not to forward you information based on the filter you give them, with the default filter being a bit-mask for the depth of the quadtree matching your PKH. A node will forward you all information about any channel which has one or both nodes matching the bit mask, and both of the nodes for every channel. Everything else gets filtered (unless other filters or strategies are agreed). A node forwarding you gossip you've specifically requested to be filtered could be considered malicious.\n\nThe filter a node communicates with its peer does not necessarily need to indicate the amount of the network they actually gossip about. For example, a node wishing to have all information about bucket 00b might request filters for buckets 0000b, 0001b, 0010b and 0011b from different peers. If each peer forwards information for their buckets, then the node will learn of all information in 00b without ever revealing to anybody, at the cost of some duplication.\n\n---\n\n> They do not commit, as in your proposal, to absolutely knowing everything within their committed area.\n\nUltimately, my proposal is a convention and not a hard rule. Any commitment comes with limits and it isn't to be *assumed* that a node will know everything about their local topology, only that they are very likely to know it under normal circumstances. It doesn't prevent anyone from using other techniques, or to try routing through different buckets (which one might want to try for increasing privacy).\n\nAny feature must be considered the same. Nodes could advertize that they support a feature and then make no commitment to following it.\n\n> Nodes that openly broadcast that they know the nodes within the same octant they are, are nodes that want to be DDoS'ed.\n> Thus, my definition of a \"special\" node is a lot looser than you seem to define it.\n> Anything that makes it possible to point to a node and say \"this is a good node to attack, in order to disrupt Lightning\" is special.\n\nAcknowledged, but I'm not convinced there is a DDoS motive. Disrupting specific nodes doesn't necessarily disrupt other users. Obviously, it can disrupt the friend and friend-of-a-friend of the target, but this will always be true. The gossip network specifically intends to publish friend-of-a-friend information, so targeted DDoS attacks on individuals and their friends are not necessarily avoidable.\n\n---\n\n> * No node shall reveal the extent of its knowledge of the network, since if it reveals that it knows the entire network, it may become a target for attack in order to degrade the network.\n\nMy main question about your proposal is, given that bandwidth is a key resource constraint, what strategy are you using to prevent the sending/receiving of unwanted gossip information, which also prevents leaking information about which gossip you're interested in?\n\n>   * This also implies that if a node does not know the location of some node, instead of admitting its ignorance, it should instead delegate to another node that might have better information; otherwise it would be possible to profile nodes to determine how much of the network they know.\n\nI'm not sure I agree. I'd prefer to go the opposite way and always fail fast.\n\nIt could be specified that if you've indicated that you'll answer queries for a specific bucket, and somebody requests information about a node or channel outside of the bucket, and which there exists no direct channel from within your bucket to that node, then you should automatically fail the query, even if you know the information.\n\nA query will then either return a very likely yes (on information within the bucket), or a definite no (anything not in the bucket). Nothing more than which was previously specified is leaked, even if you keep more gossip information than you have publicized. If probing can only reveal information that is already public, there is nothing to be gained.\n\n---\n\nPS, don't take any of this to be a dismissal of your proposal because I fully acknowledge your perspective and concerns.\nI think there are potential useful optimizations in my approach which I'm not sure how equivalent could be acheived with yours.\nBut if I am convinced there are unresolvable problems with the idea I'll drop it and focus on your approach.\n\nRegards,\nMark H"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-04-08T01:00:32",
                "message_text_only": "Good morning m.a.holden,\n\n\nSent with ProtonMail Secure Email.\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\nOn Saturday, April 6, 2019 10:39 AM, m.a.holden m.a.holden at protonmail.com wrote:\n\n> Good morning ZmnSCPxj,\n> I'll try to clarify my proposal further, but also have a few questions about yours.\n>\n> > Now, it seems to me what you propose, is to have octrees contain octrees, and so on.\n>\n> There's one global tree, which is the same for all users. Every node in the tree has a bucket and exactly 4 child nodes, except leaves have no children. The tree has a max depth, which each client sets itself. The theoretical maximum being HASHLEN/2 (In practice, we're likely to be concerned about <8). Note that every parent's bucket contains all of the information of all of its children's buckets - meaning the root node of the global quadtree is equivalent to the unfiltered global network. Nodes pick a depth and concern themselves with the bucket at that depth, unless it overflows, in which case they increase the depth by 1.\n\nLet me clarify: When you say \"node\" here, do you mean Lightning Network node?\nOr do you mean instead an in-memory node?\n\nIf you mean \"Lightning Network Node\", then how can lookup proceed if a node that should be looked through at some step is brought down by a targeted DDoS?\nWhat happens if a node near the root of the tree, which is handling lookup for much more of the network, is brought down?\nIn my mechanism, the answer is: you try another node, and as long as that node has a dist(that node, target node) lower than yours, the algorithm will progress.\nThis is because in my mechanism, each node does not have some fixed set of 4 other nodes that are the only nodes referred to when doing lookup.\n\nIf you mean in-memory node, then I do not see how relevant it is, the structure that some implementation uses.\nYou can use a bag, set, tree, or whatever base structure.\nBut the number of Lightning Network nodes and channels any one node can store is finite since the memory available to that node is finite.\nThe issue is how to get help from some other node(s) when the target node is not in your routemap.\n\n--\n\nAny tree structure is, looking only at the nodes and edges, indistinguishable from a hierarchy.\nAnd takeovers of hierarchies are simple: simply attack the easiest target near the top of the hierarchy / root of the tree.\nHierarchies are excessively centralized and we should avoid them on the public network in order to prevent attacks.\nThus I consider my proposal much more resilient compared to yours.\n\nOne might say, that my approach creates a circle where all nodes are equal, and where the removal of some node is survivable.\nOr: ZmnSCPxj and the nodes of the round table.\n\n> > I can overload those N nodes by generating node addresses that also lie in that octant.\n> > By just iterating over scalars, about 1/8 of the generated node addresses will lie in the target octant.\n>\n> If you target the first layer of the tree only, then once nodes spill to the second layer, they will filter out any gossip which does not concern them. It isn't sufficient to just brute force the first 2 bits, but you need to do it for 2^depth, where depth is the target's maximum they're willing to spill to (which is not shared).\n> However, commodity hardware can attempt millions of ec_mult/hash per second, so getting a node into the bucket you want is trivial anyway for small depth.\n>\n> > In order to disrupt a particular node, I must place fake nodes near that node, in an attempt to force its neighbors to reduce the arc of the circle that they can map.\n> > However, I need to generate fake nodes that are nearer to that node than genuine honest nodes.\n> > This means the probability of me generating such node addresses are much lower than 1/8, thus requiring that I devote more energy to generating the falsified attack nodes.\n>\n> I would argue that the above is also true in your approach. It would still be trivial to brute force the prefixes which minimize the distance between themselves and the target, even with a network of tens of millions of nodes. The amount of energy which would need devoting does not seem like it would a deciding factor for this attack.\n\nThe issue is not how difficult to attack a single node is.\n\nThe issue is: it should be easier to attack a single node, than to attack several nodes simultaneously.\nThis is how all things should be.\n\nAs I understand your proposal, taking over or taking down a node near the root of the tree will make it difficult to look up several nodes at once.\nThis is because the tree nature makes nodes near the root of the tree (top of the hierarchy) much more important than leaf nodes, which only worry about themselves.\n\nThe rest of your argument only answers the specific problem I brought up.\nBut the issue is simply this:\n\nWhat happens if a node near the root of the tree is brought down?\n\nIn my approach, there is no tree, and therefore each node is far less important.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "m.a.holden",
                "date": "2019-04-08T14:01:43",
                "message_text_only": "Hi ZmnSCPxj,\n\n> Let me clarify: When you say \"node\" here, do you mean Lightning Network node?\n> Or do you mean instead an in-memory node?\n\nNeither. I meant a node in a tree. I tried to use the term bucket to make the distinction between this and Lightning node.\nThe tree is not strictly in-memory. There is a purely conceptual global quadtree.\n\nEach bucket is essentially a view over the union of its 4 children. The bucket 00b is (0000b U 0001b U 0010b U 0011b). The bucket 0011b is (001100b U 001101b U 001110b 001111b), etc. Each client choses a suitable max_depth <= HASHLEN/2, and the leaves of their tree contain a set of all PKH whose prefix matches the bucket index. A client will keep, at minimum, a subtree of the global quadtree (and both nodes for all channels where at least one of the nodes is in the same subtree).\n\nAs you state, you can use any data structure for storing this in memory, but there are obvious benefits to using a quadtree-index to mirror the conceptual global one in terms of efficient querying, filtering, spilling, aggregating branch sizes.\n\nIf many clients follow the convention then the optimisation opportunities arise, because the majority of routes will be discoverable with a single (possibly parallel) query to the network. Gossip size can be reduced and unwanted gossip can be eliminated, which alleviates the most constrained resource, bandwidth. If the conventions are widely followed, the benefits are maximized for everybody. Not following convention does not break things, it just limits the potential wins.\n\n---\n\n> As I understand your proposal, taking over or taking down a node near the root of the tree will make it difficult to look up several nodes at once.\n> This is because the tree nature makes nodes near the root of the tree (top of the hierarchy) much more important than leaf nodes, which only worry about themselves.\n\nIf a node advertises itself near the top of the tree then it will be a better information provider than others, but this is never at the exclusion of others below it. All of the information a node in the parent bucket holds is held in part by the nodes in the child buckets. The parent does not know anything that other people don't know too. No nodes are \"more important\", but they might potentially be \"more optimal\".\n\nIf you know about some nodes in bucket 0011b, but none in 0001b where a payment destination is, then you could query any node in 0011b and ask about any nodes in bucket 0001b. Since the buckets are nearby, there is a greater probability that they'll know about more nodes than somebody further away. This would be similar to how your proposal operates. If somebody did advertise being in bucket 00b, then they're able to find a potentially better (shorter) path to the destination because they know more information and you don't need to find a path through multiple buckets. If they are under DDoS, it doesn't bring down the network or limit access to the child buckets - it just makes it trivially less optimal to route because it *might* (low probability) require more queries to reach the destination.\n\nWhen querying, it is expected that if you know about any nodes in the same bucket as the payment destination, then there's a high probability that they will know a route to the destination. A parent bucket of that bucket is not any more likely to know about the destination, they have the same information. I've shown that at small depth, there's a high probability that you will have knowledge about a reasonable quantity of paths to every other bucket in the global network - so the majority of payments will only need to visit one bucket outside your own. The reason to specifically query parent buckets is limited, and not ultimately necessary.\n\nThe only way I can see the problem you raise being a concern is if misconfigured clients have an unreasonably large depth for the amount of information in the network, such that there are few, or no channels from their own bucket to other buckets. In that case, they might become over-reliant on the parent's buckets to receive payments, but there are likely to be numerous parents at every depth of the tree (correctly configured clients), meaning there isn't a clear-cut target that an attacker could DDoS to try and disrupt a bucket.\n\nNodes do not need to present the real depth at which they keep gossip, but there are potentially greater fee-earning opportunities if publicly advertising their maximum information capability. Nodes could counterbalance potential DDoS risk with higher routing fees, where people might pay more to have a greater chance of finding shorter routes in fewer queries, but a frugal client would simply chose the cheapest routes, and the more expensive routes via parent buckets would be less desirable to them - meaning DDoS on nodes in parent buckets may be wasted effort because it would drive people towards saving money on fees. Also, since the opportunity cost for missed routing fees would be increased for nodes nearer the top of the tree, they are incentivized to make the efforts to maintain uptime and try to mitigate DDoS attacks against themselves.\n\nIt's hard to say for certain whether the risks would be real cause for concern without testing on real world data, which we don't yet have the scale to test, but I just can't see it being realistic that there would be so few targets which make DDoS feasible to begin with, and that if attacks against a significant number of nodes were successful, the potential degrading of the network would be trivial or even unnoticeable. As the network grows, it seems the attack vector would get even more unrealistic because the number of targets that one would need DDoS would increase.\n\n> What happens if a node near the root of the tree is brought down?\n\nBuckets closer to the root do not hold any exclusive information. If you bring down every node in the root bucket, and every node at depth=1, then nodes at depth>=2 will continue to communicate and still collectively know the entire network map, with still a good probability that any node can find any other node in the network by asking just one other node.\n\nRegards,\nMark H"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-04-09T10:32:00",
                "message_text_only": "Good morning m.a.holden,\n\n\n> > Let me clarify: When you say \"node\" here, do you mean Lightning Network node?\n> > Or do you mean instead an in-memory node?\n>\n> Neither. I meant a node in a tree. I tried to use the term bucket to make the distinction between this and Lightning node.\n> The tree is not strictly in-memory. There is a purely conceptual global quadtree.\n\nCan you then be more specific what is the behavior of a specific node?\nNote I use the term \"node\" specifically to refer to Lightning Network Node.\n\nGiven the node has space for N channels or N nodes (or whatever), how does the node prune its routemap?\nWhat gossip messages do it ignore and which ones do it check?\n\nIf that node has to pay to a node that is not in its routemap, how does it perform the routemap lookup?\n\n>\n> Each bucket is essentially a view over the union of its 4 children. The bucket 00b is (0000b U 0001b U 0010b U 0011b). The bucket 0011b is (001100b U 001101b U 001110b 001111b), etc. Each client choses a suitable max_depth <= HASHLEN/2, and the leaves of their tree contain a set of all PKH whose prefix matches the bucket index. A client will keep, at minimum, a subtree of the global quadtree (and both nodes for all channels where at least one of the nodes is in the same subtree).\n\nWhen you refer to \"sub-tree\", do you mean each node stores a tree of certain maximum height, with the root of that tree being some random node on the global tree?\n\nWhen you refer to \"bucket spilling\", what does it mean when the buvket has to spill beyond the maximum height of the tree that the node is willing to store?\nObviously some kind of pruning has to be done.\nThat is the whole point of this exercise.\n\nI still do not see the benefit of your lookup scheme if the lookup scheme is just in-memory.\nThat is not what is interesting at all.\nWhat is interesting is remote lookup, over the network.\nIt is what must be carefully designed, as the network cannot be trusted and anyone can be an attacker and anyone can be a victim.\n\nSuppose a node decides to store the prefix 0b00000000, storing the subtree at that point with a height of only 2 (for simplicity).\nWhat happens when it needs to look for a node with prefix 0b01101101?\nDoes it mean the node has to somehow store nodes that are outside of the prefix it decides to store?\nJust what data does it have to store?\nAnd why not just use a dist() measure like I proposed?\n\n> > As I understand your proposal, taking over or taking down a node near the root of the tree will make it difficult to look up several nodes at once.\n> > This is because the tree nature makes nodes near the root of the tree (top of the hierarchy) much more important than leaf nodes, which only worry about themselves.\n>\n> If a node advertises itself near the top of the tree then it will be a better information provider than others, but this is never at the exclusion of others below it. All of the information a node in the parent bucket holds is held in part by the nodes in the child buckets. The parent does not know anything that other people don't know too. No nodes are \"more important\", but they might potentially be \"more optimal\".\n\nWe should face the fact that nodes that know more about the network are \"more important\", really.\nThus, they should be protected.\nThe simplest protection is to anonymize them by never revealing how much each node actually knows of the network.\nThis enforces that all nodes are peers, with nobody being easily visible as more important than anyone else.\n\n\nInstead, each node that is used in lookup can simply silently delegate (without informing whoever is asking) the lookup to some other arbitrary node that it knows is more likely to know the destination.\nThat node might know more than this node, or less, but it is immaterial --- no node knows how much each node knows.\n\n\n>\n> If you know about some nodes in bucket 0011b, but none in 0001b where a payment destination is, then you could query any node in 0011b and ask about any nodes in bucket 0001b. Since the buckets are nearby, there is a greater probability that they'll know about more nodes than somebody further away. This would be similar to how your proposal operates.\n\nThen just use my proposal.\nIt is simpler and I already presented the algorithms used by each node in detail, including how each node limits the memory it uses for routemap.\n\nAs I mentioned, the point is to provide anonymity by not revealing how much you know.\nThere is a tradeoff between anonymity and some measure of efficiency.\nObviously, I prioritize anonymity, else I would not be ZmnSCPxj.\n\n\n> you could query any node in 0011b and ask about any nodes in bucket 0001b\n\nIn order to query a node, you need to know that node, meaning it (and at least one route to it) is stored somewhere in your (limited) memory.\n\nNote that even now in the network, some nodes do not provide a way to contact them directly --- you can only contact them over the channels they have advertised.\nThus to contact a node on a different bucket, you must store the route to the node in that bucket.\nThus my proposal, which allows a node to delegate to some other node, on the assumption that different nodes have different local views.\n\nJust use a continuous distance function rather than buckets, buckets don't buy you anything.\n\n\n> If somebody did advertise being in bucket 00b, then they're able to find a potentially better (shorter) path to the destination because they know more information and you don't need to find a path through multiple buckets. If they are under DDoS, it doesn't bring down the network or limit access to the child buckets - it just makes it trivially less optimal to route because it might (low probability) require more queries to reach the destination.\n>\n> When querying, it is expected that if you know about any nodes in the same bucket as the payment destination, then there's a high probability that they will know a route to the destination. A parent bucket of that bucket is not any more likely to know about the destination, they have the same information. I've shown that at small depth, there's a high probability that you will have knowledge about a reasonable quantity of paths to every other bucket in the global network - so the majority of payments will only need to visit one bucket outside your own. The reason to specifically query parent buckets is limited, and not ultimately necessary.\n>\n> The only way I can see the problem you raise being a concern is if misconfigured clients have an unreasonably large depth for the amount of information in the network, such that there are few, or no channels from their own bucket to other buckets. In that case, they might become over-reliant on the parent's buckets to receive payments, but there are likely to be numerous parents at every depth of the tree (correctly configured clients), meaning there isn't a clear-cut target that an attacker could DDoS to try and disrupt a bucket.\n>\n> Nodes do not need to present the real depth at which they keep gossip, but there are potentially greater fee-earning opportunities if publicly advertising their maximum information capability. Nodes could counterbalance potential DDoS risk with higher routing fees, where people might pay more to have a greater chance of finding shorter routes in fewer queries, but a frugal client would simply chose the cheapest routes, and the more expensive routes via parent buckets would be less desirable to them - meaning DDoS on nodes in parent buckets may be wasted effort because it would drive people towards saving money on fees. Also, since the opportunity cost for missed routing fees would be increased for nodes nearer the top of the tree, they are incentivized to make the efforts to maintain uptime and try to mitigate DDoS attacks against themselves.\n>\n> It's hard to say for certain whether the risks would be real cause for concern without testing on real world data, which we don't yet have the scale to test, but I just can't see it being realistic that there would be so few targets which make DDoS feasible to begin with, and that if attacks against a significant number of nodes were successful, the potential degrading of the network would be trivial or even unnoticeable. As the network grows, it seems the attack vector would get even more unrealistic because the number of targets that one would need DDoS would increase.\n>\n> > What happens if a node near the root of the tree is brought down?\n>\n> Buckets closer to the root do not hold any exclusive information. If you bring down every node in the root bucket, and every node at depth=1, then nodes at depth>=2 will continue to communicate and still collectively know the entire network map, with still a good probability that any node can find any other node in the network by asking just one other node.\n\nCan you describe how each node decides what parts of the routemap to prune and what parts to keep?\nGiven that each node has a specific number of nodes+Channels it can store?\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Routemap scaling (was: Just in Time Routing (JIT-Routing) and a channel rebalancing heuristic as an add on for improved routing success in BOLT 1.0)",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "m.a.holden",
                "ZmnSCPxj"
            ],
            "messages_count": 9,
            "total_messages_chars_count": 65550
        }
    },
    {
        "title": "[Lightning-dev] Outsourcing route computation with trampoline payments",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2019-04-01T04:04:44",
                "message_text_only": "Good morning Rene and Pierre,\n\nAn issue here (which I think also affects Rendezvous Routing) is with compatibility of the HMAC.\n\nHMAC covers the entire 1300-byte `hops_data` field.\n\nIf, in order to reach the next trampoline, more than one intermediate node is inserted, would the packet that reaches the next trampoline have a valid HMAC still?\nConsider that the filler data generated by the current trampoline to communicate with intermediate nodes may cause different filler data to be XORed with the 0x00 data added to left-shift the data at each intermediate node.\n(I am unsure if I express myself coherently here)\n\nSo for example, suppose I am the source and I trampoline to nodes B and C, and pay to destination D.\nSuppose B is a direct neighbor of mine, but I have no idea where C and D are.\n\nSuppose for simplicity that the onion packet is just 6 hops long.\nSo I initialize as:\n\n[ 00000 ] [ 00000 ] [ 00000 ] [ 00000 ] [ 00000 ] [ 00000 ]\n\nI right shift and insert destination D:\n\n[ D, here's my payment ] [ 00000 ] [ 00000 ] [ 00000 ] [ 00000 ] [ 00000 ]\n\nThen I encrypt with some shared secret known by me A and D:\n\n[ encrypt(A * D, \"D, here's my payment\") ] [ encrypt(A * D, 0) ] [ encrypt(A * D, 0) ] [ encrypt(A * D, 0) ] [ encrypt(A * D, 0) ] [ encrypt(A * D, 0) ]\n\nThen I right shift and insert trampoline C:\n\n[ C, send this to D ] [ encrypt(A * D, \"D, here's my payment\") ] [ encrypt(A * D, 0) ] [ encrypt(A * D, 0) ] [ encrypt(A * D, 0) ] [ encrypt(A * D, 0) ]\n\nThen I encrypt with some shared secret known by me A and C:\n\n[ encrypt(A * C, \"C, send this to D\") ] [ encrypt(A * C, encrypt(A * D, \"D, here's my payment\")) ] [ encrypt(A * C, encrypt(A * D, 0)) ] [ encrypt(A * C, encrypt(A * D, 0)) ] [ encrypt(A * C, encrypt(A * D, 0)) ] [ encrypt(A * C, encrypt(A * D, 0)) ]\n\nThen I right shift and insert trampoline B:\n\n[ B, send this to C ] [ encrypt(A * C, \"C, send this to D\") ] [ encrypt(A * C, encrypt(A * D, \"D, here's my payment\")) ] [ encrypt(A * C, encrypt(A * D, 0)) ] [ encrypt(A * C, encrypt(A * D, 0)) ] [ encrypt(A * C, encrypt(A * D, 0)) ]\n\nAnd encrypt with A * B:\n\n[ encrypt(A * B, \"B, send this to C\") ] [ encrypt(A * B, encrypt(A * C, \"C, send this to D\")) ] [ encrypt(A * B, encrypt(A * C, encrypt(A * D, \"D, here's my payment\"))) ] [ encrypt(A * B, encrypt(A * C, encrypt(A * D, 0))) ] [ encrypt(A * B, encrypt(A * C, encrypt(A * D, 0))) ] [ encrypt(A * B, encrypt(A * C, encrypt(A * D, 0))) ]\n\nI send this to B, who removes one layer of encryption:\n\n[ B, send this to C ] [ encrypt(A * C, \"C, send this to D\") ] [ encrypt(A * C, encrypt(A * D, \"D, here's my payment\")) ] [ encrypt(A * C, encrypt(A * D, 0)) ] [ encrypt(A * C, encrypt(A * D, 0)) ] [ encrypt(A * C, encrypt(A * D, 0)) ]\n\nNow B finds the shortest route involves nodes N and O before reaching C.\nSo B has to insert N and O, pushing out one packet from the end.\nBut the pushed-out packet will no longer be recoverable and it cannot be re-encrypted except by A.\n\n(Unless I misunderstand the onion construction)\n\n\nUnless we propose to massively change the onion packet construction...?\n\n\n> 1.) A different fee mechanism. Let us (only as a radical thought experiment) assume we drop the privacy of the final amount in routing.\n\nPlease no.\n\n> A sending node could offer a fee for successful routing. Every routing node could decide how much fee it would collect for forwarding. Nodes could try to collect larger fees than the min they announce but that lowers the probably for the payment to be successful. Even more radical: Nodes would not even have to announce min fees anymore. Turning routing and fees to a real interactive market\n\nWould this not also require that intermediate nodes know the ultimate destination of the payment?\nHow would intermediate nodes find out which next hop would be reasonable?\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-04-01T11:32:20",
                "message_text_only": "Good morning Pierre, Rene, and list,\n\nI would also like to point out that even if the trampoline point does not know the next trampoline point, then it need not fail the routing.\n(this may occur if we start pruning the routemap as the LN size grows)\n\nAs long as we can fix the issues regarding HMAC, then the trampoline point may itself delegate the routing to the next trampoline point to another trampoline point it inserts into the onion.\n\nSo what we need to somehow support, is to be able to \"left shift\" and \"right shift\" arbitrarily in the onion packet.\nIf we can handle this, then trampolining is possible and trampoline routing is feasible to delegate routing elsewhere.\n\nThis also ties with deterministic methods of pruning routemaps.\nFor instance, somebody proposed to create a false \"geographic location\" for each node, possibly derived only from the node public key being projected into some spatial volume.\n(To be clear, this does *not* mean that every node needs to register some merely-Earth-based location, which can be easily falsified anyway; instead we project the node public key to some notion of \"nearness\")\nThen a node might be expected to keep at least the nearest nodes to its \"geographic location\" in its routemap.\n\nThen if I happen to be a trampoline point, and I am unable to locate the next trampoline point in my local routemap, I could instead locate the node on my routemap that is \"nearest\" to the next trampoline point, and forward the payment there.\n\nNow, to an extent, privacy is reduced here since it is likelier than before that the \"next trampoline\" is actually the payee.\nHowever, as a source node, I only need to know the actual route to my first trampoline point, and let the trampoline point worry about how to get it to the next trampoline.\nSo I can even just prune only the channels and channel relationships (endpoints of channels), and retain only the node public keys (a cheap 32 bytes), probably pruning the node public keys in some deterministically probabilistic fashion.\nIn this case, I might still be interested in gossip about faraway channels --- I would still want to check that the channel exists (by blockchain lookup), but after I know the channel exists I can throw it away, only considering it as a proof-of-existence of a faraway node that I might use as a trampoline to improve my privacy.\n\nIn effect, this lets us give a continuum:\n\n1.  At one end, the full route and every intermediate hop to the destination is known only by the source.\n2.  At the other end, the source only knows its direct channels, and sends to some adjacent trampoline node, and asks the trampoline node to route to the destination.\n\nThe above gives a nice continuum where the amount of space dedicated to your own local routemap improves your privacy, and you can prune your routemap at the cost of privacy reduction (and probably hedging your fees by always overpaying fees).\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Pierre",
                "date": "2019-04-01T12:26:33",
                "message_text_only": "Hello ZmnSCPxj,\n\n> Unless we propose to massively change the onion packet construction...?\n\nI'm afraid we would have to make some changes. I imagine we would have\ntwo onions:\n- one for the adjacent hops (this is the onion we are currently using)\n- one for the trampoline hops\n\nThe 'trampoline onion' would be contained in the per-hop payload of\nthe final node of the 'adjacent onion'. So in your example B would:\n1) receive the htlc\n2) see that it is the last hop in the route, and extract the trampoline payload\n3) peel the trampoline onion and see that it should delegate the payment to C\n4) find a route to C and set the trampoline onion as payload for C\n\nI haven't studied PR #593 enough to tell how easy that would be\nachievable though.\n\nThere is another unrelated issue: because trampoline nodes don't know\nanything about what happened before they received the onion, they may\nunintentionnaly create overlapping routes. So we can't simply use the\npayment_hash as we currently do, we would have to use something a bit\nmore elaborate. (maybe private keys?)\n\nCheers,\n\nPierre"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-04-03T03:41:53",
                "message_text_only": "Good morning Pierre and list,\n\n>     There is another unrelated issue: because trampoline nodes don't know\n>     anything about what happened before they received the onion, they may\n>     unintentionnaly create overlapping routes. So we can't simply use the\n>     payment_hash as we currently do, we would have to use something a bit\n>     more elaborate.\n\nJust to be clear, the issue is for example with a network like:\n\n\n    A ------- B -------- C\n             / \\\n            /   \\\n           /     \\\n          /       \\\n         D ------- E\n\nThen, A creates an inner trampoline onion \"E->C\", and an outer onion \"A->B->E\".\n\nE, on receiving the inner trampoline onion \"E->C\", finds that E->B direction is low capacity, so routes over the outer onion \"E->D->B->C\" with inner trampoline onion \"C\".\n\nThis creates an overall route A->B->E->D->B->C.\n\nWhen the B->C HTLC is resolved, B can instead claim the A->B HTLC and just fail the D->B HTLC, thereby removing D and E from the route and claiming their fees, even though they participated in the route.\n\n> (maybe private keys?)\n\nDo you refer to the changing from \"H\"TLC to \"P\"TLC point-locked timelocked contracts?\ni.e. instead of payment hash / preimage, we use payment point / scalar.\n\nI think a few ideas would be improved by this.\n\n1.  Trampoline payments, as described above.\n2.  Offline vending machines\n    - Instead of storing a fixed number of invoices from the always-online payment node, store a HD parent point and derive child points for payments.\n3.  Enables payment decorrelation.\n\nPerhaps we should consider switching to payment points/scalars sometime soon.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Johan Tor\u00e5s Halseth",
                "date": "2019-04-03T08:27:13",
                "message_text_only": "Hi Pierre and list folks,\n\nI haven't looked at the technical implications of implementing this in\ndetail, but I like the high-level direction this proposal is taking :)\n\nI think it nicely ties together several concepts that have been proposed\nearlier, and with the correct design could give the sender all the\nflexibility needed to craft payments according to its own\nreliability/privacy/fee tradeoff.\n\nIn an ideal world we would have:\n- Multi-hop locks: hop decorrelation will be even more important when the\nsender no longer controls the whole payment path.\n\n- Packet switched routing: the sender can choose whether to route purely\npacket switched (only knowing the destination, no need to keep any routing\ntable) or route purely onion-based (as today), or something in between. As\nChristian mentions, this is similar to how TOR/TCP works, and I like the\nflexibility this layering allows.\n\n- Rendezvous routing: such a proposal would be nice to combine with\nrendezvous routing. This way even the sender wouldn't necessarily know if\nthe \"destination node\" is just another trampoline. Maybe maybe this concern\nshouldn't be on this layer though?\n\n- Fees: (as today) the sender would set the fees it is willing to pay\nbetween trampolines, and it could dynamically learn about fee levels needed\nto reach different parts of the network. Today we know the fee needed to\nreach the next hop, but here we could start out low (for the\ntrampoline-to-trampoline fee) and let different trampolines return\ncompeting fee offers to get to the next hop.\n\nAs mentioned, I haven't thought about the technical implications of all\nthis, and it would certainly require a lot of work to get this actually\nimplemented.\n\nCheers,\nJohan\n\nOn Wed, Apr 3, 2019 at 5:42 AM ZmnSCPxj via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> wrote:\n\n> Good morning Pierre and list,\n>\n> >     There is another unrelated issue: because trampoline nodes don't know\n> >     anything about what happened before they received the onion, they may\n> >     unintentionnaly create overlapping routes. So we can't simply use the\n> >     payment_hash as we currently do, we would have to use something a bit\n> >     more elaborate.\n>\n> Just to be clear, the issue is for example with a network like:\n>\n>\n>     A ------- B -------- C\n>              / \\\n>             /   \\\n>            /     \\\n>           /       \\\n>          D ------- E\n>\n> Then, A creates an inner trampoline onion \"E->C\", and an outer onion\n> \"A->B->E\".\n>\n> E, on receiving the inner trampoline onion \"E->C\", finds that E->B\n> direction is low capacity, so routes over the outer onion \"E->D->B->C\" with\n> inner trampoline onion \"C\".\n>\n> This creates an overall route A->B->E->D->B->C.\n>\n> When the B->C HTLC is resolved, B can instead claim the A->B HTLC and just\n> fail the D->B HTLC, thereby removing D and E from the route and claiming\n> their fees, even though they participated in the route.\n>\n> > (maybe private keys?)\n>\n> Do you refer to the changing from \"H\"TLC to \"P\"TLC point-locked timelocked\n> contracts?\n> i.e. instead of payment hash / preimage, we use payment point / scalar.\n>\n> I think a few ideas would be improved by this.\n>\n> 1.  Trampoline payments, as described above.\n> 2.  Offline vending machines\n>     - Instead of storing a fixed number of invoices from the always-online\n> payment node, store a HD parent point and derive child points for payments.\n> 3.  Enables payment decorrelation.\n>\n> Perhaps we should consider switching to payment points/scalars sometime\n> soon.\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20190403/24eb7d58/attachment.html>"
            },
            {
                "author": "Pierre",
                "date": "2019-04-03T08:41:12",
                "message_text_only": "Hello ZmnSCPxj,\n\n> Do you refer to the changing from \"H\"TLC to \"P\"TLC point-locked timelocked contracts?\n> i.e. instead of payment hash / preimage, we use payment point / scalar.\n\nYes, that is what I meant. Cryptography isn't my strong suit though,\nso I'm not able to go into much more details.\n\nCheers,\n\nPierre"
            },
            {
                "author": "Christian Decker",
                "date": "2019-04-03T09:01:50",
                "message_text_only": "On Wed, 3 Apr 2019, 05:42 ZmnSCPxj via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> wrote:\n\n> Good morning Pierre and list,\n>\n> >     There is another unrelated issue: because trampoline nodes don't know\n> >     anything about what happened before they received the onion, they may\n> >     unintentionnaly create overlapping routes. So we can't simply use the\n> >     payment_hash as we currently do, we would have to use something a bit\n> >     more elaborate.\n>\n> Just to be clear, the issue is for example with a network like:\n>\n>\n>     A ------- B -------- C\n>              / \\\n>             /   \\\n>            /     \\\n>           /       \\\n>          D ------- E\n>\n> Then, A creates an inner trampoline onion \"E->C\", and an outer onion\n> \"A->B->E\".\n>\n> E, on receiving the inner trampoline onion \"E->C\", finds that E->B\n> direction is low capacity, so routes over the outer onion \"E->D->B->C\" with\n> inner trampoline onion \"C\".\n>\n> This creates an overall route A->B->E->D->B->C.\n>\n> When the B->C HTLC is resolved, B can instead claim the A->B HTLC and just\n> fail the D->B HTLC, thereby removing D and E from the route and claiming\n> their fees, even though they participated in the route.\n>\n\nThis is not an issue. Like we discussed for the multi-part payments the\nHTLCs still resolve correctly, though node B might chose to short circuit\nthe payment it'll also clear the HTLCs through E And D (by failing them\ninstead of settling them) but the overall payment remains atomic and\nend-to-end secure. The skipped nodes (which may include the trampoline) may\nlose a bit of fees, but that is not in any way different than a failed\npayment attempt that is being retried from the sender :-)\n\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20190403/326187f0/attachment.html>"
            },
            {
                "author": "Pierre",
                "date": "2019-04-03T11:39:59",
                "message_text_only": "> This is not an issue. Like we discussed for the multi-part payments the HTLCs still resolve correctly\n\nRight! Great, missed that"
            },
            {
                "author": "Christian Decker",
                "date": "2019-04-01T13:06:28",
                "message_text_only": "Thanks Pierre for this awesome proposal, I think we're onto something\nhere. I'll add a bit more color to the proposal, since I've been\nthinking about it all weekend :-)\n\nThere are two ways we can use this:\n\n - A simple variant in which we just tell a single trampoline what the\n   intended recipient is (just a pubkey, and an amount) and it'll find a\n   route.\n - A complex variant in which a trampoline is given a next hop, and a\n   smaller onion to pass along to the next hop. The trampoline doesn't\n   learn the intended recipient, but can still route it.\n\n# Simple Variant\n\nAs the name implies it is pretty trivial to implement: the sender\ncomputes a route to some trampoline node `t` it knows in its 2- or\n3-neightborhood and creates an onion that describes this route. The\npayload for the trampoline node `t` then contains two parameters:\n`receiver` and `amount`. The trampoline node `t` then computes a route\nfrom itself to the `receiver` and creates a new onion (the old onion\nterminates at the trampoline node). Since the trampoline node generates\na new route, it also generates the shared secrets, HMACs and everything\nelse to match (no problem with matching HMACs like in the case of\nrendezvous routing).\n\nThe receiver doesn't learn anything about this being a trampoline\npayment (it doesn't even have to implement it itself), and resolution of\nthe HTLC happens like normal (with the extra caveat that the trampoline\nneeds to associate the upstream incoming HTLC with the resolution of the\ndownstream HTLC, but we do that anyway now).\n\n# Multi-trampoline routing\n\nThe more complex proposal involves nesting a smaller onion into the\nouter routing onion. For this the sender generates a small onion of, for\nexample, 10 hops whose length is only 650 bytes instead of the 20 hops\nfor the outer routing onion. The hops in the inner/smaller onion do not\nhave to be adjacent to each other, i.e., they can be picked randomly\nfrom the set of known nodes and there doesn't need to be a channel\nbetween two consecutive hops, unlike in the outer/routing onion. The\nhops in the smaller onion are called trampolines `t_1` to `t_10`.\n\nThe construction of the smaller onion can be identical to the\nconstruction of the routing onion, just needs its size adjusted. The\nsender then picks a random trampoline node `t_0` in its known\nneighborhood and generates a routing onion containing the smaller onion\nas payload to `t_0` and signaling data (final recipient, amount, inner\nonion). Upon receiving an incoming payment with trampoline instructions\na trampoline `t_i` unwraps the inner onion, which yields the next\ntrampoline `t_{i+1}` node_id. The trampoline then finds a route to\n`t_{i+1}`, serializing the inner onion (which was unwrapped and is now\ndestined for `t_{i+1}`) and creating the outer routing onion with that\nas the payload. Notice that, like in the simple variant, `t_i` generates\na new outer onion, which means we don't have any issues with shared\nsecrets and HMACs like in rendezvous routing. Resolution is also\nidentical to above.\n\nThis construction reuses all the onion primitives we already have, and\nit allows us to bounce a payment through multiple trampolines without\nthem learning their position in this nested path. The sender does\nnot have to have a total view of the network topology, just have a\nreasonable chance that two consecutive trampolines can find a route to\neach other, i.e., don't use mobile phone as trampolines :-)\n\n# Tradeoffs everywhere\n\n## Longer Routes\n\nOne potential downside is that by introducing this two-level nesting of\nan outer routing onion and an inner trampoline onion, we increase the\nmaximum length of a route to `num_outer_hops * num_inner_hops`, given\nthat each layer of the inner onion may initiate a new `num_outer_hops`\nouter route. For the example above (which is also the worst case) we\nhave a 10 inner hops, and 9 outer hops (due to the signalling overhead),\nwhich results in a maximum route length of 90 hops. This may result in\nmore funds being used to route a payment, but it may also increase\nchances of the payment succeeding.\n\n## Comparison with TCP/IP + Tor\n\nThis proposal also brings us a lot closer to the structure of Tor on the\npublic network, in which the nodes that are part of a circuit do not\nhave to be direct neighboors in the network topology since end-to-end\nreachability is guaranteed by a base routing layer (TCP/IP) whereas\nsender/receiver obfuscation is tackled at a higher layer (Tor).\n\nIn our case the outer onion serves as the base routing layer that is\nused for point-to-point communication, but unlike TCP/IP is also\nencrypted and routed anonymously, while the inner onion takes care of\nend-to-end reachability, also in encrypted fashion.\n\n## In-network retrial\n\n>From the comparison with TCP/IP and Tor might have hinted at this, but\nsince the outer onion is created from scratch at each trampoline, a\ntrampoline may actually retry a payment multiple times if an attempt\nfailed, reducing the burden on the sender, and increasing chances of the\npayment succeeding.\n\n# Conclusion\n\nOverall I'm really excited about this proposal. It decreases the need\nfor a complete network view at the endpoints, may delegate some of the\nburden of finding routes to in-network trampolines, may increase the\nsuccessrate of our payments, and increases the total length of a\npossible route (may be a negative as well).\n\nCheers,\nChristian\n\nPierre <pm+lists at acinq.fr> writes:\n> Hello List,\n>\n> I think we can use the upcoming \"Multi-frame sphinx onion format\" [1]\n> to trustlessly outsource the computation of payment routes.\n>\n> A sends a payment to an intermediate node N, and in the onion payload,\n> A provides the actual destination D of the payment and the amount. N\n> then has to find a route to D and make a payment himself. Of course D\n> may be yet another intermediate node, and so on. The fact that we can\n> make several \"trampoline hops\" preserves the privacy characteristics\n> that we already have.\n>\n> Intermediate nodes have an incentive to cooperate because they are\n> part of the route and will earn fees. As a nice side effect, it also\n> creates an incentive for \"routing nodes\" to participate in the gossip,\n> which they are lacking currently.\n>\n> This could significantly lessen the load on (lite) sending nodes both\n> on the memory/bandwidth side (they would only need to know a smallish\n> neighborhood) and on the cpu side (intermediate nodes would run the\n> actual route computation).\n>\n> As Christian pointed out, one downside is that fee computation would\n> have to be pessimistic (he also came up with the name trampoline!).\n>\n> Cheers,\n>\n> Pierre-Marie\n>\n> [1] https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-February/001875.html\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-04-01T21:30:35",
                "message_text_only": "Good morning Christian,\n\n\n>\n> There are two ways we can use this:\n>\n> -   A simple variant in which we just tell a single trampoline what the\n>     intended recipient is (just a pubkey, and an amount) and it'll find a\n>     route.\n>\n> -   A complex variant in which a trampoline is given a next hop, and a\n>     smaller onion to pass along to the next hop. The trampoline doesn't\n>     learn the intended recipient, but can still route it.\n\nThe simple variant definitely tells the trampoline who the payee is, so I would reject it.\n\n\n> Multi-trampoline routing\n> =========================\n>\n> The more complex proposal involves nesting a smaller onion into the\n> outer routing onion. For this the sender generates a small onion of, for\n> example, 10 hops whose length is only 650 bytes instead of the 20 hops\n> for the outer routing onion. The hops in the inner/smaller onion do not\n> have to be adjacent to each other, i.e., they can be picked randomly\n> from the set of known nodes and there doesn't need to be a channel\n> between two consecutive hops, unlike in the outer/routing onion. The\n> hops in the smaller onion are called trampolines `t_1` to `t_10`.\n>\n> The construction of the smaller onion can be identical to the\n> construction of the routing onion, just needs its size adjusted. The\n> sender then picks a random trampoline node `t_0` in its known\n> neighborhood and generates a routing onion containing the smaller onion\n> as payload to `t_0` and signaling data (final recipient, amount, inner\n> onion). Upon receiving an incoming payment with trampoline instructions\n> a trampoline `t_i` unwraps the inner onion, which yields the next\n> trampoline `t_{i+1}` node_id. The trampoline then finds a route to\n> `t_{i+1}`, serializing the inner onion (which was unwrapped and is now\n> destined for `t_{i+1}`) and creating the outer routing onion with that\n> as the payload. Notice that, like in the simple variant, `t_i` generates\n> a new outer onion, which means we don't have any issues with shared\n> secrets and HMACs like in rendezvous routing. Resolution is also\n> identical to above.\n>\n> This construction reuses all the onion primitives we already have, and\n> it allows us to bounce a payment through multiple trampolines without\n> them learning their position in this nested path. The sender does\n> not have to have a total view of the network topology, just have a\n> reasonable chance that two consecutive trampolines can find a route to\n> each other, i.e., don't use mobile phone as trampolines :-)\n\nNaively, would it not be possible?\n\nSuppose a mobile phone keeps only a small subset of the routemap due to memory constraints, but has high uptime because it is the precious mobile device of somebody who uses the mobile phone at all hours.\n\nWhen the mobile phone trampoline is unable to route to the next trampoline, could it not \"delegate\" this by looking for some node in its smaller routemap that it believes (by some other mechanism) to be more likely to route to the next trampoline?\nCould this be implemented by replacing only the front of the trampoline-level onion?\n(presumably with some adjustment of how the HMAC is computed for the new trampoline layer)\n\nIf we will change to use trampoline-level onions then maybe we can change things somewhat to support this usage better.\n\nOtherwise it would seem that possible trampolines would have to advertise themselves as such.\nIt would be better if a trampoline could be just \"taken out of a hat\" and selected randomly.\nAnd as long as the trampoline is able to *delegate* the routing to some other trampoline, and there is sufficient fee, payment can succeed.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-04-04T09:44:49",
                "message_text_only": "Good morning list,\n\n\n> Could this be implemented by replacing only the front of the trampoline-level onion?\n> (presumably with some adjustment of how the HMAC is computed for the new trampoline layer)\n\nI am trying to design a trampoline-level onion that would allow replacement of the first hop of the onion.\n\nBelow is what I came up with.\nAs I am neither a cryptographer nor a mathematician, I am unable to consider, whether this may have some problem in security.\n\nNow the \"normal\" onion, the first hop is encrypted to the recipient.\n\nI propose that for the \"inner\" trampoline-level onion, the first hop be sent \"in the clear\".\nI think this is still secure, as the \"inner\" trampoline-level onion will still be wrapped by the outer link-level onion, which would still encrypt it.\n\nWhen a node receives a trampoline-level onion, it checks if it is the first hop.\nIf it is, then it decrypts the rest of the onion and tries to route to the next trampoline-level node.\nIf not, then it is being delegated to, to find the trampoline.\n\nIf the node cannot find the front of the trampoline-level onion, then it can route it to another node that it suspects is more likely to know the destination (such as the mechanisms in discussion in the \"Routemap Scaling\" thread).\n\nLet me provide a concrete example.\n\nPayer Z creates a trampoline-level onion C->D->E:\n\nC | Z | encrypt(Z * C, D | encrypt(Z * D, E))\n\nThen Z routes to link-level onion A->B->C, with the payload to C being the above trampoline-level onion:\n\nencrypt(Z * A, \"link level\" | B | encrypt(Z * B, \"link level\" | C | encrypt(Z * C, \"trampoline level\" | C | Z | encrypt(Z * C, D | encrypt(Z * D, E)))))\n\nUpon reaching C, it sees it is given a trampoline-level onion, and if C is unable to find D in its local map, it can delegate it to some other node.\n\nFor example, if C thinks its neighbor M knows D, it can create:\n\nencrypt(C * M, \"link level\" | M | encrypt(C * M, \"trampoline level\" | D | Z | encrypt(Z * D, E)))\n\nM finds that it is not the first hop in the trampoline-level onion.\nSo M finds a route to D, for example via M->N->D, and gives:\n\nencrypt(M * N, \"link level\" | D | encrypt(M * D, \"trampoline level\" | D | Z | encrypt(Z * D, E)))\n\nIs this workable?\nNote that it seems to encounter the same problem as Rendezvous Routing.\nI assume it is possible to do this somehow (else how would hidden services in Tor work?), but the details, I am uncertain of.\nI only play a cryptographer on Internet.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Christian Decker",
                "date": "2019-04-04T10:39:13",
                "message_text_only": "Hi ZmnSCPzj,\n\nI think we should not try to recover from a node not finding the next\nhop in the trampoline, and rather expect trampolines to have reasonable\nuptime (required anyway) and have an up to date routing table (that's\nwhat we're paying them for after all).\n\nSo I'd rather propose reusing the existing onion construction as is and\nexpect the trampolines to fail a payment if they can't find the next\nhop.\n\nLet's take the following route for example (upper case letters represent\ntrampolines):\n\n```\na -> b -> c -> D -> e -> f -> G -> h -> i -> j\n```\n\nWith `a` being the sender, and `j` being the recipient. `D` and `G` are\ntrampolines. The sender `a` selects trampolines `D` and `G` at random\nfrom their partial (possibly outdated) routing table. It creates the\ninner onion using those two trampolines. It then computes a route to `D`\n(`a -> b -> c -> D`). The `hop_payload` for `D` is a TLV payload that\nhas a single key `t` (assuming `t` is assigned in the TLV spec) that\ncontains the inner onion. It then initiates the payment using this\nnested onion (`a -> b -> c -> D` + trampoline payload for `D`).\n\nUpon receiving the onion `D` decrypts the outer onion to find the TLV\npayload containing the `t` entry, which indicates that it should act as\na trampoline. It then decodes the inner trampoline onion and finds the\n`node_id` of `G`. `D` then computes the outer onion to the next\ntrampoline `D -> e -> f -> G`, and adds the trampoline payload for `G`\n(the inner trampoline onion we just decoded).\n\nUpon receiving the onion `G` processes the onion like normal, finding\nagain an inner trampoline onion and decrypting it. Since `j` did not\nindicate that it understands the trampoline protocol, `G` is instructed\nto downgrade the onion into a normal non-trampoline onion (don't include\na trampoline, rather include the raw payload for `j`). It then computes\nthe route to `j`, and it creates a normal outer base routing onion `G ->\nh -> i -> j`, which completes the protocol.\n\nLike mentioned above the entire job of trampolines is to provide base\nrouting capability, and we should not make special provisions for myopic\ntrampoline nodes, since routing is their entire reason for existence :-)\n\nCheers,\nChristian\n\n>> Could this be implemented by replacing only the front of the trampoline-level onion?\n>> (presumably with some adjustment of how the HMAC is computed for the new trampoline layer)\n>\n> I am trying to design a trampoline-level onion that would allow replacement of the first hop of the onion.\n>\n> Below is what I came up with.\n> As I am neither a cryptographer nor a mathematician, I am unable to consider, whether this may have some problem in security.\n>\n> Now the \"normal\" onion, the first hop is encrypted to the recipient.\n>\n> I propose that for the \"inner\" trampoline-level onion, the first hop be sent \"in the clear\".\n> I think this is still secure, as the \"inner\" trampoline-level onion will still be wrapped by the outer link-level onion, which would still encrypt it.\n>\n> When a node receives a trampoline-level onion, it checks if it is the first hop.\n> If it is, then it decrypts the rest of the onion and tries to route to the next trampoline-level node.\n> If not, then it is being delegated to, to find the trampoline.\n>\n> If the node cannot find the front of the trampoline-level onion, then it can route it to another node that it suspects is more likely to know the destination (such as the mechanisms in discussion in the \"Routemap Scaling\" thread).\n>\n> Let me provide a concrete example.\n>\n> Payer Z creates a trampoline-level onion C->D->E:\n>\n> C | Z | encrypt(Z * C, D | encrypt(Z * D, E))\n>\n> Then Z routes to link-level onion A->B->C, with the payload to C being the above trampoline-level onion:\n>\n> encrypt(Z * A, \"link level\" | B | encrypt(Z * B, \"link level\" | C | encrypt(Z * C, \"trampoline level\" | C | Z | encrypt(Z * C, D | encrypt(Z * D, E)))))\n>\n> Upon reaching C, it sees it is given a trampoline-level onion, and if C is unable to find D in its local map, it can delegate it to some other node.\n>\n> For example, if C thinks its neighbor M knows D, it can create:\n>\n> encrypt(C * M, \"link level\" | M | encrypt(C * M, \"trampoline level\" | D | Z | encrypt(Z * D, E)))\n>\n> M finds that it is not the first hop in the trampoline-level onion.\n> So M finds a route to D, for example via M->N->D, and gives:\n>\n> encrypt(M * N, \"link level\" | D | encrypt(M * D, \"trampoline level\" | D | Z | encrypt(Z * D, E)))\n>\n> Is this workable?\n> Note that it seems to encounter the same problem as Rendezvous Routing.\n> I assume it is possible to do this somehow (else how would hidden services in Tor work?), but the details, I am uncertain of.\n> I only play a cryptographer on Internet.\n>\n> Regards,\n> ZmnSCPxj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-04-04T14:48:54",
                "message_text_only": "Good morning Christian,\n\nFirst:\n\n> Like mentioned above the entire job of trampolines is to provide base\n> routing capability, and we should not make special provisions for myopic\n> trampoline nodes, since routing is their entire reason for existence :-)\n\nThe point of providing this special provision is to increase the anonymity set of full-routemap nodes.\n\nSuppose we were to require that myopic routing nodes fail if they are unable to see the next trampoline.\nThen it becomes possible to profile the network and identify full-routemap vs. myopic routing nodes.\nMyopic routing nodes are more likely to fail than full-routemap nodes.\n\nIn particular, full-routemap nodes are particularly juicy targets for attackers who wish to disrupt the Lightning Network once the LN grows in size.\n\nThis is even worse if we end up proposing that full-routemap nodes announce themselves as such on gossip.\nThen attackers do not even need to profile the network; listening to gossip is enough to identify full-routemap targets to attack.\n\nIn a world where we allow myopic routing nodes to delegate instead of fail, then it becomes much harder for a third party to profile the network.\nMyopic routing nodes would only have slightly higher failure rate than full-routemap nodes, possibly within the noise level, making it hard for third parties to definitely differentiate full-routemap nodes from myopic routing nodes.\nAnonymity loves company, and the protection of the deity Anonymous is a tremendous boon in sustaining systems from attack and censure.\n\nAnother advantage of allowing myopic routing nodes to delegate is that it allows myopia to be a relative condition rather than a binary \"either I know the whole routemap or not\".\nThat is, I can have 90%, 75%, 50%, 25%, 10$, or 1% of the routemap, and still I can participate in supporting the network by at least helping route to those nodes that I can route to, or delegating to someone else who might know that part of the network better than I do.\nThis provides more graceful degradation of service than the binary \"full-routemap or not\" you propose.\nIn particular, as the LN grows, fewer and fewer nodes will be able to have a full routemap, and they will have larger and larger targets painted on their back as points of attack.\n\nAnother point:\n\n> I think we should not try to recover from a node not finding the next\n> hop in the trampoline, and rather expect trampolines to have reasonable\n> uptime (required anyway) and have an up to date routing table\n\nBut with myopic trampoline nodes, the only requirement is high uptime; completeness of the routing table becomes unimportant.\nThis means that selecting good trampoline nodes only requires one desiderata (high uptime).\nIn particular, high uptime can be easily measured (just attempt to connect or ping), but completeness of routing table is not.\n\nThen:\n\n> (that's\n> what we're paying them for after all).\n\nThis contradicts your position in the other sub-thread of this topic:\n\n> This is not an issue. Like we discussed for the multi-part payments the HTLCs still resolve correctly, though node B might chose to short circuit the payment it'll also clear the HTLCs through E And D (by failing them instead of settling them) but the overall payment remains atomic and end-to-end secure. The skipped nodes (which may include the trampoline) may lose a bit of fees, but that is not in any way different than a failed payment attempt that is being retried from the sender :-)\n\nIn that case, E is a full-routemap node, while B may or may not be a full-routemap node, but B gets paid (more!) while E does not.\nE took the effort to find a route while all B did was pass the onion to the next.\nWhat gives?\n\nBut regardless ---\nI would like to point out that it is still incentive-compatible to support myopic trampoline nodes, and that full-routemap nodes will be paid much more than a myopic trampoline node.\n\nSuppose a trampoline node is a full-routemap node.\nIn exchange for the service of providing routes, it expects to be allocated a higher fee.\nThen the routing fee of nodes between it and the next trampoline node are deducted from the fee allocated to it.\nIn particular, it will refuse to continue payment if it does not get a higher-than-normal fee for its own hop.\nAfter all, it must be paid for this service, as you insist.\n\nSuppose a trampoline node is a myopic node, that knows the next trampoline is not in its routemap, but considers that another node (the delegatee) is more likely to know the next trampoline that it does.\nIn such a case, that myopic node would consider it more optimal to allocate only a small amount of fee for its own hop and to devote most of the fee to the delegatee.\nThis increases the chance that the delegatee, if it knows the route to the next trampoline, will accept and continue routing.\nIts alternative would be to allocate too low a fee to the delegatee, leading the delegatee to reject the routing, which means the myopic node itself earns nothing as the payment fails.\n\nThus the myopic node is paid, but the full-routemap node can demand to be paid more.\n\nThe myopic nodes are paid, not for routing, but for the ***increased anonymity set*** that protects the full-routemap node from attack.\nThe full-routemap node is paid for the actual effort of routing (once we switch to payment points/scalars to prevent short-circuits that can prevent full-routemap node from being paid).\n\n\nLet me espouse the \"peer-to-peer\" principle:\n\nNodes should treat other nodes uniformly, regardless of the resources available to that node (including memory space available for routemaps), in order to prevent divisions of the network that may be used to enable targeted attacks.\n\nOr: the nail that sticks out gets DDoS'ed down.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Outsourcing route computation with trampoline payments",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Johan Tor\u00e5s Halseth",
                "Christian Decker",
                "Pierre",
                "ZmnSCPxj"
            ],
            "messages_count": 13,
            "total_messages_chars_count": 39307
        }
    },
    {
        "title": "[Lightning-dev] Eltoo in a tree",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2019-04-01T11:09:51",
                "message_text_only": "Good morning Hossein,\n\nThis is already known.\nIndeed, this is the basis of Burchert-Decker-Wattenhofer \"Channel Factories\". https://www.tik.ee.ethz.ch/file/a20a865ce40d40c8f942cf206a7cba96/Scalable_Funding_Of_Blockchain_Micropayment_Networks%20(1).pdf\n\nSee also discussion regarding Fulgurite. https://lists.linuxfoundation.org/pipermail/lightning-dev/2018-December/001721.html\n\nIt is likely that channel factories of some form will be created after we can get Decker-Russell-Osuntokun (\"eltoo\") online.\nDecker-Russell-Osuntokun requires some kind of `SIGHASH_NOINPUT`.\n\nIn many ways, a channel is simply a type of cryptocurrency system.\nIf we were to generate some kind of hierarchical system of types:\n\n* Cryptocurrency System (abstract)\n  * Blockchain (abstract)\n    * Bitcoin (concrete) - the only blockchain that can ever exist\n  * Offchain cryptocurrency system (abstract) - requires an existing Cryptocurrency System to construct\n    * Poon-Dryja (concrete) - current Lightning Network; 2-party only\n    * Decker-Wattenhofer (concrete) - multiparty but requires long locktimes on unilateral\n    * Decker-Russell-Osuntokun (concrete) - multiparty, requires short locktimes on unilateral\n\nBurchert-Decker-Wattenhofer factories are just the realization that you can do something like instantiate a Poon-Dryja channel inside a Decker-Wattenhofer channel inside a Bitcoin blockchain.\n\nSimilarly, your NOctaHub is just another offchain cryptocurrency system, and the realization that you can nest other offchain cryptocurrency systems inside it is simply the same realization that Burchert-Decker-Wattenhofer had.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Eltoo in a tree",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "ZmnSCPxj"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1633
        }
    },
    {
        "title": "[Lightning-dev] Final expiry probing attack",
        "thread_messages": [
            {
                "author": "Joost Jager",
                "date": "2019-04-09T09:14:08",
                "message_text_only": "Hi all,\n\nIn https://github.com/lightningnetwork/lightning-rfc/pull/516,\nthe incorrect_or_unknown_payment_details failure message is extended with\nan htlc_msat field and thereby replaces the former incorrect_payment_amount\nmessage. The objective of this change is to prevent a probing attack that\nallows an intermediate node to find out the final destination of the\npayment.\n\nShouldn't the same change be applied to the cltv expiry?\n\nCurrently in lnd, we return a final_expiry_too_soon message if the htlc\nexpiry is not meeting the invoice cltv delta requirement. This can be used\nfor probing by using low expiry values, similar to how this was previously\npossible with low amounts.\n\nThe proposed change would be: when the htlc expiry doesn't meet the invoice\ncltv delta requirement, return an incorrect_or_unknown_payment_details\nfailure (extended with a new htlc_expiry field) instead\nof final_expiry_too_soon.\n\nJoost.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20190409/28a1a5d5/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Final expiry probing attack",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Joost Jager"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1104
        }
    },
    {
        "title": "[Lightning-dev] Stale Factory (and channel) problem",
        "thread_messages": [
            {
                "author": "Alejandro Ranchal Pedrosa",
                "date": "2019-04-15T23:59:07",
                "message_text_only": "Hi all,\n\nThis is the first of three related different emails on the topic, \nthrough which I will explain what, to my understanding, is the state of \nthe construction of channel factories.\n\nFirst let's have a look at a situation known as a stale factory, or \nchannel [1], as defined by Ranchal-Pedrosa et al.. For simplicity, let's \nconsider a channel first. Suppose a DMC channel Alice between Alice and \nBob. This channel must be updated through so-called refunding \ntransactions R^k_{AB}, where k refers to the state (so initial state \nR^0_{AB}, after one payment R^1_{AB}, etc.) and _{AB} refers to both A \nand B having already signed the transaction (if a dot appears instead of \nA or B, it means there's a signature missing.\n\nThe stale channel derives from the fact that either Alice or Bob needs \nto sign before their counterparty. Suppose a situation like this:\n\nAlice \u00a0\u00a0 \u00a0\u00a0 \u00a0 \u00a0 Bob\n\n \u00a0 |\u00a0\u00a0\u00a0\u00a0 \u00a0 \u00a0\u00a0 \u00a0\u00a0\u00a0 |\n\n \u00a0 |<--R^1_{.B}<--|\n\n \u00a0 |-->R^1_{AB}-->|\n\n \u00a0 ...\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ...\n\n \u00a0 |<--R^k_{.B}<--|\n\n \u00a0 |\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |\n\nIn this situation, Bob does not have a fully signed transaction for the \nlast state k, whereas Alice may have it (from the point of view of Bob). \nThat is, even if the message was lost, all Bob knows in the trustless \nmodel is that he signed for something that he did not get a fully signed \ntransaction back for. So he should believe Alice has the fully signed \ntransaction and may enforce it if necessary. At the same time though, \nBob can enforce transaction R^{k-1}_{AB}, which he must have, and \ntherefore finish with the ambiguity by publishing on-chain such state, \nshould he not be able to communicate with Alice and perform updates anymore.\n\nThe stale factory is the same situation, but affecting a new allocation \ntransaction, as defined by Decker et al.[2], rather than a new refunding \ntransaction. There are two major differences between a stale factory and \na stale channel:\n\n \u00a0\u00a0\u00a0 - In a stale factory, only one user (out of n) can already cause \nthis situation. That is, even if the remaining n-1 users are active and \nonline, with one of them not replying back, is enough for Alice to \nbelieve that there's a possibility that one of its counterparties might \nhave the fully signed new allocation transaction.\n\n \u00a0\u00a0\u00a0 - The new allocation transaction may or may not affect a particular \nchannel in the factory, but if it does then users do not even know in \nwhich channel they have their funds.\n\nWays to go around these are:\n\n \u00a0\u00a0\u00a0 - Try to create a new refunding (or allocation) transaction \nR^{k+1}_{AB}. If it fails though, now there are three possible \ntransactions. Besides, if the channel/factory follows the DMC \nconstruction, the timer reduces yet again.\n\n \u00a0\u00a0\u00a0 - Close the channel/factory by publishing it on the Blockchain.\n\nOpen for discussion about this situation and its implications. In an \nupcoming email I will explain what, to my understanding, is the biggest \nproblem associated with this situation.\n\n-- \nAlejandro Ranchal-Pedrosa\n\n[1]: Scalable Lightning Factories for Bitcoin\n\n[2]: Scalable Funding of Bitcoin MicropaymentChannel Networks"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-04-16T07:39:48",
                "message_text_only": "Good morning Alejandro,\n\nYour analyses seem to be quite spot on.\nIt does seem that factories need some work to be done further.\nAs I understand it, the proposal in \"Scalable Lightning Factories for Bitcoin\" require a new signature scheme at the blockchain layer, is that correct?\n\n\nI observe that current Lightning uses single-funded channels.\nThis was to initially simplify the protocol.\nAlthough dual-funded channels are being proposed, various issues came up.\n\nPerhaps some simplification of factories is possible.\n\n* Suppose we initially insist that factories be single-funded.\n  In this mechanism, a node may open multiple channels to multiple other nodes using a single onchain UTXO.\n* The funder of the factory is the factory operator.\n* If the other nodes on the factory wish to create some change at the factory level, they contact the factory operator only.\n  If the factory operator allows it, only will the factory-level operation be allowed.\n* The only factory-level operation allowed is a cooperative close.\n  Each channel in the factory must be in a quiescent state (no HTLCs) and the factory operator requests signatures from all nodes.\n  What happens is that the factory operator requests and distributes signatures for each individual channel closure first, before corodinating to request and distribute signatures for the factory close.\n  The operation completes on publication of the factory close.\n* The factory close is the cut-through of the allocation transaction and the individual channel close transaction, with the mining fee for the factory close transaction equal to the total mining fees on the allocation transaction and individual channel close transactions.\n  It may have fewer total outputs as the channel outputs of the factory operator can be merged into a single output.\n  If both allocation and factory close are marked as RBF, then the factory close will dominate over the allocation transaction in feerate.\n  Miners would strongly prefer the factory close, and even if the allocation transaction gets through, it does not degrade security (but unnecessarily spends block space, meaning rational miners would not prefer it especially since the factory close gives the same fee and is smaller).\n* This construction requires a simple n-of-n at the factory level, as there is no update.\n\nThis is still a \"factory\", of a sort, but with only one known-safe operation.\nThe intent is simply to provide some of the scaling boost for now, until we can determine how best to implement factory-level changes.\n\nRegards,\nZmnSCPxj\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\nOn Tuesday, April 16, 2019 7:59 AM, Alejandro Ranchal Pedrosa <a.ranchalpedrosa at gmail.com> wrote:\n\n> Hi all,\n>\n> This is the first of three related different emails on the topic,\n> through which I will explain what, to my understanding, is the state of\n> the construction of channel factories.\n>\n> First let's have a look at a situation known as a stale factory, or\n> channel [1], as defined by Ranchal-Pedrosa et al.. For simplicity, let's\n> consider a channel first. Suppose a DMC channel Alice between Alice and\n> Bob. This channel must be updated through so-called refunding\n> transactions R^k_{AB}, where k refers to the state (so initial state\n> R^0_{AB}, after one payment R^1_{AB}, etc.) and {AB} refers to both A\n> and B having already signed the transaction (if a dot appears instead of\n> A or B, it means there's a signature missing.\n> The stale channel derives from the fact that either Alice or Bob needs\n> to sign before their counterparty. Suppose a situation like this:\n> Alice \u00a0\u00a0 \u00a0\u00a0 \u00a0 \u00a0 Bob\n> \u00a0 |\u00a0\u00a0\u00a0\u00a0 \u00a0 \u00a0\u00a0 \u00a0\u00a0\u00a0 |\n> \u00a0 |<--R^1{.B}<--|\n>\n> \u00a0 |-->R^1_{AB}-->|\n>\n> \u00a0 ...\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ...\n>\n> \u00a0 |<--R^k_{.B}<--|\n>\n> \u00a0 |\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |\n>\n> In this situation, Bob does not have a fully signed transaction for the\n> last state k, whereas Alice may have it (from the point of view of Bob).\n> That is, even if the message was lost, all Bob knows in the trustless\n> model is that he signed for something that he did not get a fully signed\n> transaction back for. So he should believe Alice has the fully signed\n> transaction and may enforce it if necessary. At the same time though,\n> Bob can enforce transaction R^{k-1}{AB}, which he must have, and\n> therefore finish with the ambiguity by publishing on-chain such state,\n> should he not be able to communicate with Alice and perform updates anymore.\n> The stale factory is the same situation, but affecting a new allocation\n> transaction, as defined by Decker et al.[2], rather than a new refunding\n> transaction. There are two major differences between a stale factory and\n> a stale channel:\n> \u00a0\u00a0\u00a0 - In a stale factory, only one user (out of n) can already cause\n> this situation. That is, even if the remaining n-1 users are active and\n> online, with one of them not replying back, is enough for Alice to\n> believe that there's a possibility that one of its counterparties might\n> have the fully signed new allocation transaction.\n> \u00a0\u00a0\u00a0 - The new allocation transaction may or may not affect a particular\n> channel in the factory, but if it does then users do not even know in\n> which channel they have their funds.\n> Ways to go around these are:\n> \u00a0\u00a0\u00a0 - Try to create a new refunding (or allocation) transaction\n> R^{k+1}{AB}. If it fails though, now there are three possibletransactions. Besides, if the channel/factory follows the DMC\n> construction, the timer reduces yet again.\n>\n> \u00a0\u00a0\u00a0 - Close the channel/factory by publishing it on the Blockchain.\n>\n> Open for discussion about this situation and its implications. In an\n> upcoming email I will explain what, to my understanding, is the biggest\n> problem associated with this situation.\n>\n> ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n>\n> Alejandro Ranchal-Pedrosa\n>\n> [1]: Scalable Lightning Factories for Bitcoin\n>\n> [2]: Scalable Funding of Bitcoin MicropaymentChannel Networks\n>\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-04-17T03:39:09",
                "message_text_only": "Good morning Alejandro and list,\n\nLet me characterize the problem in detail.\n\n* Stale offchain system is the issue where one participant of a multiparticipant offchain system sends a signature that advances the protocol, but is unable to receive further signatures from one or more of the other participants to continue the protocol.\n* Often, such a stall in the protocol requires some timeout and a backoff path, aborting the protocol and performing some corrective action onchain.\n* More participants means more chances of this kind of stale offchain system disruption.\n\n* For two-participant offchain systems (\"payment channels\"), this disruption is indistinguishable from the other participant going offline.\n* For payment channels, the other participant going offline implies that future updates of the channel will not occur, thus it is always possible for participants to insist on redoing the signature exchange before performing further updates.\n  * Thus, for payment channels, this issue can be fixed by allowing the exchange of signatures (including those you believe to have sent previously) of the most recent state upon re-establishing a communication channel.\n  * BOLT already requires this.\n\n* For multiparticipant offchain systems that host other offchain systems (\"channel factories\"), this disruption is also indistinguishable from one of the participants going offline.\n* For channel factories, the remaining participants might wish to continue participating in hosted offchain systems (\"on-factory channels\") that do not involve the dropped participant.\n* However, it is unknown if the dropped participant is able to construct the new state or not.\n  * Thus it is ambiguous if on-factory channels should be rooted from the old state or the new state.\n\n--\n\nA thought comes to mind: would `SIGHASH_NOINPUT` help?\nOn-factory channels not affected by a channel reorganization operation at the factory level can continue to operate, by use of `SIGHASH_NOINPUT`.\n\nFor instance, suppose the current factory state is the channels: (A, B) 1; (B, C) 1; (A, D) 1; (C, D) 1\nSuppose A, C, and D propose a reorganization to the new state: (A, B) 1; (B, C) 1; (A, C) 0.5; (C, D) 1.5\n\nIf channel states use `SIGHASH_NOINPUT` in signatures, then (A, B) and (B, C) channels can be trivially re-rooted in both the old or the new factory state,\nAt the same time, (A, D) 1 and (C, D) 1 are both closed until the new state is completely signed, so their continued operation is stopped.\nAnd the channels (A, C) 0.5 and (C, D) 1.5 are not yet opened until the new state is completely signed, so their operation cannot be begun.\n\nThis allows unaffected channels to continue operation even if a factory-level operation is in an indterminate state.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Alejandro Ranchal Pedrosa",
                "date": "2019-04-21T02:38:23",
                "message_text_only": "Hi ZmnSCPxj,\n\nI also thought about the possibility of using 'SIGHASH_NOINPUT', it \ncertainly offers a functionality very similar to the one I suggest in \nthe paper.\n\nThe problem with 'SIGHASH_NOINPUT' as it is now is that, if I am not \nmistaken, in the example you are showing, seems like A,D and C can \nunilaterally decide to include a new participant, F. Seems to me like \nthis can affect the no-lock property of offchain layers, since they \nmight require F to release the funds in the mainchain.\n\nCertainly, with some variants to SIGHASH_NOINPUT this solution can be \nachieved. Actually, this is in some way what I propose in the Lightning \nFactories paper. Adding a non-interactive aggregate signature scheme is \njust going one step further with an optimization to save space, the same \nway Schnorr-based signatures schemes for aggregation are proposed in \nChannel Factories. But with minor variants (that are listed in the \npaper), a SIGHASH_NOINPUT would work like a charm.\n\nBest,\n\nAlejandro.\n\n\nOn 17/04/2019 13:39, ZmnSCPxj wrote:\n> Good morning Alejandro and list,\n>\n> Let me characterize the problem in detail.\n>\n> * Stale offchain system is the issue where one participant of a multiparticipant offchain system sends a signature that advances the protocol, but is unable to receive further signatures from one or more of the other participants to continue the protocol.\n> * Often, such a stall in the protocol requires some timeout and a backoff path, aborting the protocol and performing some corrective action onchain.\n> * More participants means more chances of this kind of stale offchain system disruption.\n>\n> * For two-participant offchain systems (\"payment channels\"), this disruption is indistinguishable from the other participant going offline.\n> * For payment channels, the other participant going offline implies that future updates of the channel will not occur, thus it is always possible for participants to insist on redoing the signature exchange before performing further updates.\n>    * Thus, for payment channels, this issue can be fixed by allowing the exchange of signatures (including those you believe to have sent previously) of the most recent state upon re-establishing a communication channel.\n>    * BOLT already requires this.\n>\n> * For multiparticipant offchain systems that host other offchain systems (\"channel factories\"), this disruption is also indistinguishable from one of the participants going offline.\n> * For channel factories, the remaining participants might wish to continue participating in hosted offchain systems (\"on-factory channels\") that do not involve the dropped participant.\n> * However, it is unknown if the dropped participant is able to construct the new state or not.\n>    * Thus it is ambiguous if on-factory channels should be rooted from the old state or the new state.\n>\n> --\n>\n> A thought comes to mind: would `SIGHASH_NOINPUT` help?\n> On-factory channels not affected by a channel reorganization operation at the factory level can continue to operate, by use of `SIGHASH_NOINPUT`.\n>\n> For instance, suppose the current factory state is the channels: (A, B) 1; (B, C) 1; (A, D) 1; (C, D) 1\n> Suppose A, C, and D propose a reorganization to the new state: (A, B) 1; (B, C) 1; (A, C) 0.5; (C, D) 1.5\n>\n> If channel states use `SIGHASH_NOINPUT` in signatures, then (A, B) and (B, C) channels can be trivially re-rooted in both the old or the new factory state,\n> At the same time, (A, D) 1 and (C, D) 1 are both closed until the new state is completely signed, so their continued operation is stopped.\n> And the channels (A, C) 0.5 and (C, D) 1.5 are not yet opened until the new state is completely signed, so their operation cannot be begun.\n>\n> This allows unaffected channels to continue operation even if a factory-level operation is in an indterminate state.\n>\n> Regards,\n> ZmnSCPxj\n\n-- \nAlejandro Ranchal Pedrosa"
            },
            {
                "author": "Alejandro Ranchal Pedrosa",
                "date": "2019-04-16T08:30:27",
                "message_text_only": "Hi ZmnScpXj,\n\nthat's correct, Lightning Factories require support for \"transaction\nfragments\" to be added dynamically, which are only possible when using\nnon-interactive aggregation signature schemes.\n\nThe proposal of having a factory operator is in fact quite interesting, and\nit is true it would give some scalability while the community discusses\nother more long-lasting options. Is there a more in-depth proposal of the\nprotocol you suggest, or any of the like?\n\nBest,\n\nAlejandro.\n\n\n\nAlejandro Ranchal Pedrosa\n\nOn 16 Apr 2019 at 17:39, <ZmnSCPxj <zmnscpxj at protonmail.com>> wrote:\n\nGood morning Alejandro,\n\nYour analyses seem to be quite spot on.\nIt does seem that factories need some work to be done further.\nAs I understand it, the proposal in \"Scalable Lightning Factories for\nBitcoin\" require a new signature scheme at the blockchain layer, is\nthat correct?\n\n\nI observe that current Lightning uses single-funded channels.\nThis was to initially simplify the protocol.\nAlthough dual-funded channels are being proposed, various issues came up.\n\nPerhaps some simplification of factories is possible.\n\n* Suppose we initially insist that factories be single-funded.\n  In this mechanism, a node may open multiple channels to multiple\nother nodes using a single onchain UTXO.\n* The funder of the factory is the factory operator.\n* If the other nodes on the factory wish to create some change at the\nfactory level, they contact the factory operator only.\n  If the factory operator allows it, only will the factory-level\noperation be allowed.\n* The only factory-level operation allowed is a cooperative close.\n  Each channel in the factory must be in a quiescent state (no HTLCs)\nand the factory operator requests signatures from all nodes.\n  What happens is that the factory operator requests and distributes\nsignatures for each individual channel closure first, before\ncorodinating to request and distribute signatures for the factory\nclose.\n  The operation completes on publication of the factory close.\n* The factory close is the cut-through of the allocation transaction\nand the individual channel close transaction, with the mining fee for\nthe factory close transaction equal to the total mining fees on the\nallocation transaction and individual channel close transactions.\n  It may have fewer total outputs as the channel outputs of the\nfactory operator can be merged into a single output.\n  If both allocation and factory close are marked as RBF, then the\nfactory close will dominate over the allocation transaction in\nfeerate.\n  Miners would strongly prefer the factory close, and even if the\nallocation transaction gets through, it does not degrade security (but\nunnecessarily spends block space, meaning rational miners would not\nprefer it especially since the factory close gives the same fee and is\nsmaller).\n* This construction requires a simple n-of-n at the factory level, as\nthere is no update.\n\nThis is still a \"factory\", of a sort, but with only one known-safe operation.\nThe intent is simply to provide some of the scaling boost for now,\nuntil we can determine how best to implement factory-level changes.\n\nRegards,\nZmnSCPxj\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\nOn Tuesday, April 16, 2019 7:59 AM, Alejandro Ranchal Pedrosa\n<a.ranchalpedrosa at gmail.com> wrote:\n\n> Hi all,\n>\n> This is the first of three related different emails on the topic,\n> through which I will explain what, to my understanding, is the state of\n> the construction of channel factories.\n>\n> First let's have a look at a situation known as a stale factory, or\n> channel [1], as defined by Ranchal-Pedrosa et al.. For simplicity, let's\n> consider a channel first. Suppose a DMC channel Alice between Alice and\n> Bob. This channel must be updated through so-called refunding\n> transactions R^k_{AB}, where k refers to the state (so initial state\n> R^0_{AB}, after one payment R^1_{AB}, etc.) and {AB} refers to both A\n> and B having already signed the transaction (if a dot appears instead of\n> A or B, it means there's a signature missing.\n> The stale channel derives from the fact that either Alice or Bob needs\n> to sign before their counterparty. Suppose a situation like this:\n> Alice           Bob\n>   |              |\n>   |<--R^1{.B}<--|\n>\n>   |-->R^1_{AB}-->|\n>\n>   ...            ...\n>\n>   |<--R^k_{.B}<--|\n>\n>   |              |\n>\n> In this situation, Bob does not have a fully signed transaction for the\n> last state k, whereas Alice may have it (from the point of view of Bob).\n> That is, even if the message was lost, all Bob knows in the trustless\n> model is that he signed for something that he did not get a fully signed\n> transaction back for. So he should believe Alice has the fully signed\n> transaction and may enforce it if necessary. At the same time though,\n> Bob can enforce transaction R^{k-1}{AB}, which he must have, and\n> therefore finish with the ambiguity by publishing on-chain such state,\n> should he not be able to communicate with Alice and perform updates anymore.\n> The stale factory is the same situation, but affecting a new allocation\n> transaction, as defined by Decker et al.[2], rather than a new refunding\n> transaction. There are two major differences between a stale factory and\n> a stale channel:\n>     - In a stale factory, only one user (out of n) can already cause\n> this situation. That is, even if the remaining n-1 users are active and\n> online, with one of them not replying back, is enough for Alice to\n> believe that there's a possibility that one of its counterparties might\n> have the fully signed new allocation transaction.\n>     - The new allocation transaction may or may not affect a particular\n> channel in the factory, but if it does then users do not even know in\n> which channel they have their funds.\n> Ways to go around these are:\n>     - Try to create a new refunding (or allocation) transaction\n> R^{k+1}{AB}. If it fails though, now there are three possibletransactions. Besides, if the channel/factory follows the DMC\n> construction, the timer reduces yet again.\n>\n>     - Close the channel/factory by publishing it on the Blockchain.\n>\n> Open for discussion about this situation and its implications. In an\n> upcoming email I will explain what, to my understanding, is the biggest\n> problem associated with this situation.\n>\n> ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n>\n> Alejandro Ranchal-Pedrosa\n>\n> [1]: Scalable Lightning Factories for Bitcoin\n>\n> [2]: Scalable Funding of Bitcoin MicropaymentChannel Networks\n>\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20190416/324ed866/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-04-16T11:31:23",
                "message_text_only": "Good morning Alejandro,\n\n> that's correct, Lightning Factories require support for \"transaction fragments\" to be added dynamically, which are only possible when using non-interactive aggregation signature schemes.\u00a0\n\nI understand.\nUnfortunately I am not a mathematician and cannot comment on non-interactive aggregation signature schemes.\nBut it seems good to point this to bitcoin-dev.\n\n>\n> The proposal of having a factory operator is in fact quite interesting, and it is true it would give some scalability while the community discusses other more long-lasting options. Is there a more in-depth proposal of the protocol you suggest, or any of the like?\u00a0\n\nI have been mulling this over as a possible intermediate step since the release of the original Burchert-Decker-Wattenhofer channel factories paper.\nHowever, I have only now actually published it to anybody.\n(the basic genesis of this idea was to consider a `fundmultichannel` command for C-lightning, which I have not been able to implement anyway, hence the base idea that a factory could be created from a single node doing the funding; in essence, before this kind of factory could be standardized, this would be a single transaction funding multiple channels, with the switchover to factories being \"seamless\" to users of the C-lightning API once this kind of factory is standardized)\nIn particular the tremendous CSV delays in the base Decker-Wattenhofer construction was too expensive by my opinion.\n\nAlternately, it would be possible to have a Decker-Wattenhofer factory host Poon-Dryja channels, for instance, with a small number of updates at the factory level keeping the CSV delay down.\nThe logic being that channels would need large number of updates, but the factory could remain useful even with fewer supported updates.\nStill need to adjust the CLTV of every HTLC on a route by the largest CSV-delta of the channels in the route, with plain Poon-Dryja channels having 0 CSV-delta (the CSV is after the transported contract, not before as in Decker-Wattenhofer).\n\nAfter the release of the Decker-Russell-Osuntokun (\"eltoo\") paper, I pretty much kept such alternative constructions in the back of my mind as I thought that a Decker-Russell-Osuntokun channel within a Decker-Russell-Osuntokun factory would be sufficient.\nCSV-delta would double, but still be manageable especially with the smaller CSV needed in Decker-Russell-Osuntokun.\n\nHowever, stale factory at least seems to be a \"close your factory\" kind of error.\nUnfortunately it seems to be easy to trigger by specifically disconnecting after receiving a message containing a signature.\nThus updates at the factory level are potentially breaking.\n\nThus, I revived this thought, which requires not this kind of issue.\nIt also avoids the broken factory by the simple expedient of not allowing a broken factory by not allowing factory-level operations other than cooperative closes.\n\n--\n\nA thought occurs: could it be possible to distribute the signatures via node gossip?\nFactory-level operations change the channel-level topology, thus require some gossip message anyway.\n(in particular, closure of a channel inside a factory is not visible, unlike channel closures onchain which are visible onchain, thus need to have some message to attest to the destruction of the channel inside a factory)\nThis may mitigate the effect of stale factory by allowing the receipt of the next factory state signatures via other means than direct message sending.\nOf course, if some factory participant becomes completely unresponsive before it broadcasts its signature, the factory operation cannot proceed, but the broken factory problem means that is not possible to perform factory operations without all factory participants anyway.\n(it would also be helpful since participants in the same factory may not have direct communication channels with each other, and nodes are allowed to not broadcast any method of direct contact, so we *do* need some other way to perform a one-to-many send)\n\nBasically, third parties would not consider a \"change in channels in factory\" gossip message as complete until it receives all signatures from all factory members, but would still gossip such fragments around.\nThe \"change in channels in factory\" gossip might contain the sighash of the transaction, plus the actual signatures unencrypted.\nThird parties would be able to verify the signatures as valid, and could check that the signatures match the n-of-n onchain, but would not be able to force a unilateral factory closure as they would not know the contents of the actual transaction.\nThe factory participants could lie and use a random sighash, but anything happening inside a factory is their say-so anyway and cannot be verified in detail unless one is also a participant.\nAnd if the other participants are lying about the intended next state of the factory, one need only refuse to release such a signature, which would keep the message from being considered by the rest of the network as binding on the current state of the channels inside the factory.\n\n--\n\nHow are things done on a Chaumian CoinJoin?\nIt seems to me that the same issue exists; a participant in the CoinJoin might have sent a signature authorizing their coins being mixed, but if it does not see the transaction broadcast onchain after some time, it can assume that the joining failed.\nPerhaps a similar construction can be made, obviously without broadcasting onchain.\n\nPerhaps a coordinator, who is one of the factory participants, could solicit signatures from all participants.\nI.e. this could be a fixed factory operator.\nThen when a participant validates the next factory state as valid, and sends its signature to the coordinator, it starts a timer.\nIf the coordinator is unable to provide the full set of signatures in that time, the participant assumes failure and unilaterally closes.\n\nThis may mitigate the stale factory problem without actually fixing it...\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Stale Factory (and channel) problem",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Alejandro Ranchal Pedrosa",
                "ZmnSCPxj"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 32372
        }
    },
    {
        "title": "[Lightning-dev] Broken Factory Attack",
        "thread_messages": [
            {
                "author": "Alejandro Ranchal Pedrosa",
                "date": "2019-04-16T00:14:59",
                "message_text_only": "Hi all,\n\nThis is the second of three related different emails on the topic, \nthrough which I will explain what, to my understanding, is the state of \nthe construction of channel factories.\n\nKnowing what a stale factory situation is, let's have a look at the only \noff-chain proposal that claims to solve this situation: the splice-out \nmechanism [1]. I will explain why this mechanism can be easily attacked \nin what we call the Broken Factory attack [2].\n\nSuppose a factory created between Alice, Bob, Carol and Dave, with \nchannels F_{AC}, F_{AB}, F_{BC} and F_{CD} (F for \"funding\"), for instance:\n\n \u00a0\u00a0\u00a0\u00a0 Alice\u00a0\u00a0\u00a0 Bob\u00a0\u00a0\u00a0 Carol\u00a0\u00a0\u00a0 Dave\n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \\\u00a0\u00a0\u00a0\u00a0\u00a0 \\\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 /\u00a0\u00a0\u00a0\u00a0\u00a0 /\n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \\\u00a0\u00a0\u00a0\u00a0 \\\u00a0\u00a0\u00a0\u00a0 /\u00a0\u00a0\u00a0\u00a0\u00a0 /\n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \\\u00a0\u00a0\u00a0 |\u00a0\u00a0\u00a0 |\u00a0\u00a0\u00a0 /\n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 H_{ABCD}\n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |\n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 A^0_{ABCD}\n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 /\u00a0\u00a0 |\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |\u00a0\u00a0 \\\n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 /\u00a0\u00a0\u00a0\u00a0 |\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |\u00a0\u00a0\u00a0\u00a0 \\\n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 /\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \\\n\n \u00a0\u00a0\u00a0\u00a0 F_{AC}\u00a0\u00a0\u00a0 F_{AB}\u00a0\u00a0 F_{BC}\u00a0\u00a0 F_{CD}\n\n \u00a0\u00a0\u00a0 /\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |\n\nR^1_{AC}\u00a0\u00a0\u00a0\u00a0\u00a0 R^1_{AB} R^1_{BC}\u00a0\u00a0 R^1_{CD}\n\nWhere H_{ABCD} and A^0_{ABCD} refer to the Hook and Allocation \ntransactions of the factory, the equivalents of the Funding and \nRefunding transactions of channels, respectively.\n\nSuppose now that Dave, meaning that they cannot update the channel \nfactory. If they wanted to perform a factory-level update, the \nsplice-out mechanism should let them do that by redirecting outputs of \nthe channels of the factory a new factory (sort of creating a factory \nwithin the factory):\n\n \u00a0\u00a0\u00a0\u00a0 Alice\u00a0\u00a0\u00a0 Bob\u00a0\u00a0\u00a0 Carol\u00a0\u00a0\u00a0 Dave\n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \\\u00a0\u00a0\u00a0\u00a0\u00a0 \\\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 /\u00a0\u00a0\u00a0\u00a0\u00a0 /\n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \\\u00a0\u00a0\u00a0\u00a0 \\\u00a0\u00a0\u00a0\u00a0 /\u00a0\u00a0\u00a0\u00a0\u00a0 /\n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \\\u00a0\u00a0\u00a0 |\u00a0\u00a0\u00a0 |\u00a0\u00a0\u00a0 /\n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 H_{ABCD}\n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |\n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 A^0_{ABCD}\n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 /\u00a0\u00a0 |\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |\u00a0\u00a0 \\\n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 /\u00a0\u00a0\u00a0\u00a0 |\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |\u00a0\u00a0\u00a0\u00a0 \\\n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 /\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \\\n\n \u00a0\u00a0\u00a0\u00a0 F_{AC}\u00a0\u00a0\u00a0 F_{AB}\u00a0\u00a0 F_{BC}\u00a0\u00a0 F_{CD}\n\n \u00a0\u00a0\u00a0 /\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |\n\nR^1_{AC}\u00a0\u00a0\u00a0\u00a0\u00a0 #R^1_{AB}# R^1_{BC}\u00a0\u00a0 R^1_{CD}\n \u00a0\u00a0\u00a0\u00a0\u00a0 \\\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |\u00a0\u00a0\u00a0\u00a0\u00a0 |\n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \\\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |\u00a0\u00a0\u00a0\u00a0 /\n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \\\u00a0\u00a0\u00a0\u00a0\u00a0 |\u00a0\u00a0\u00a0 /\n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *H_{ABC}*\n\n\nAnd within this new Hook, Alice Bob and Carol can reorder the funds in \nnew channels as they lack. However, notice that the new Hook (marked \nwith \"*\") takes the outputs of 2-party off-chain channels. Suppose now \nthat, after creating the new hook, Alice and Bob pay in this new factory \nto Carol, through some new channels they created. Carol believes the \nsplice-out mechanism works and so accepts these off-chain payments.\n\nRight after, Alice and Bob can update their former channel state (marked \nwith \"#\" in the above image) and invalidate the state R^1_{AB}, making \nit older than a new state R^2_{AB}. This invalidates one of the inputs \nof the new Hook H_{ABC}, invalidating the transaction and the whole new \nfactory. Therefore, Alice and Bob have stolen from Carol. This is what \nwe call a Broken Factory, and which proves that the splice-out mechanism \nis not possible, at least without a race condition.\n\n-- \nAlejandro Ranchal-Pedrosa\n\n\n[1]: Scalable Funding of Bitcoin MicropaymentChannel Networks\n  \n[2]: Scalable Lightning Factories for Bitcoin"
            },
            {
                "author": "Alejandro Ranchal Pedrosa",
                "date": "2019-04-16T04:03:50",
                "message_text_only": "Apologies, just realised there is an important typo:\n\n\"Suppose that Dave is offline/unresponsive, meaning that they cannot \nupdate the channel factory\"\n\nBest,\n\n-- \nAlejandro Ranchal Pedrosa"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-04-17T03:45:53",
                "message_text_only": "Good morning Alejandro, and list,\n\nI am uncertain if this would completely solve it, but Discrete Log Contracts has a mechanism by which an Oracle is enforced to reveal its private key, if it publishes multiple signatures signing different messages for a particular sampling.\nIt seems like a way to ensure, that a public key is used only once.\n\nCan this mechanism be somehow used, so that if Alice and Bob sign an alternate transaction spending the A,B output (thus invalidating the sub-factory transaction), they also reveal to Carol the private key?\nCarol can then punish this behavior by burning the A,B output and sending it all as fees to miners.\n\nHowever, it may be insufficient.\nIf the A,B channel is very small in capacity, Alice and Bob may be willing to sacrifice it in exchange for stealing larger amounts from Carol.\n\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Alejandro Ranchal Pedrosa",
                "date": "2019-04-21T02:33:07",
                "message_text_only": "Hi ZmnSCPxj,\n\nI suppose some variant of that proposal might mitigate the attack, but \nit would trigger a race condition between the valid state of the \nsub-factory and the new state of the channel.\n\nAlso, as you said, Alice and Bob might be interested in stealing anyways \nfrom Carol, regardless of losing the race, if the stolen funds are more.\n\nBest,\n\nAlejandro.\n\nOn 17/04/2019 13:45, ZmnSCPxj wrote:\n\n> Good morning Alejandro, and list,\n>\n> I am uncertain if this would completely solve it, but Discrete Log Contracts has a mechanism by which an Oracle is enforced to reveal its private key, if it publishes multiple signatures signing different messages for a particular sampling.\n> It seems like a way to ensure, that a public key is used only once.\n>\n> Can this mechanism be somehow used, so that if Alice and Bob sign an alternate transaction spending the A,B output (thus invalidating the sub-factory transaction), they also reveal to Carol the private key?\n> Carol can then punish this behavior by burning the A,B output and sending it all as fees to miners.\n>\n> However, it may be insufficient.\n> If the A,B channel is very small in capacity, Alice and Bob may be willing to sacrifice it in exchange for stealing larger amounts from Carol.\n>\n>\n>\n> Regards,\n> ZmnSCPxj\n\n-- \nAlejandro Ranchal Pedrosa"
            }
        ],
        "thread_summary": {
            "title": "Broken Factory Attack",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Alejandro Ranchal Pedrosa",
                "ZmnSCPxj"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 5486
        }
    },
    {
        "title": "[Lightning-dev] Lightning Factories & Non-interactive aggregation signature schemes for Bitcoin",
        "thread_messages": [
            {
                "author": "Alejandro Ranchal Pedrosa",
                "date": "2019-04-16T00:29:46",
                "message_text_only": "Hi all,\n\nThis is the last of three related different emails on the topic, through \nwhich I will explain what, to my understanding, is the state of the \nconstruction of channel factories.\n\nHaving shown the stale factory and broken factory situation, it remains \nclear one thing: Every time an update attempt starts and does not end up \nin a fully signed transactions, it is safest (and possibly even the only \noption) to close the factory (or channel), in order to finish the \nambiguity in the factory.\n\nConsidering this, seems fair to look at how fast can one do that with \nthe current proposal for a DMC factory[1]. Given a lifetime l_f, defined \nin a number of blocks at the creation of the factory, if a stale factory \narrives at state k, the time to close the factory remains l_f-k (in the \nworst-case that one of the members of the factory is in fact \nunresponsive, either because of not being online, or being malicious). \nThis is a big trade-off: the bigger l_f is, the more time one can \npotentially use the factory, but the more time one has to potentially \nwait before being able to keep using its money.\n\nThis is why we propose a Lightning Factory[2]. Lightning channels have \nconstant time to close themselves, and the lifetime is potentially \nunlimited, it is not even defined as it isn't required by the protocol. \nWe extend this idea to factories.\n\nThe problem of a construction like this is, in order to account for the \nmultiple amount of nested frauds possible in an attempt to close the \nso-called Lightning Factory, one would require a protocol that stores \noff-chain n! transactions (being n the amount of users in the factory).\n\nThis is where the biggest element of discussion lies from the series of \nemails above-mentioned: if the Bitcoin community is already considering \nSchnorr-based signature schemes, that allow for interactive aggregation \nof messages, I think we should at least consider as seriously other \nsignature schemes that allow for *NON*-interactive aggregation of messages.\n\nIn such way, instead of requiring n! transactions, one could have just \nO(n) fragments of a transaction, that can be aggregated \nnon-interactively depending on the particular nested fraud attempts \n(more details in the paper [2]) to generate a specific proof-of-fraudS.\n\nAnother motivation for such schemes is the aggregation of independent \ntransactions in Blocks, which has already been proposed, but could \nactually never take place under an interactive aggregation scheme. \nCurrent literature suggests BGLS as probably the best option to \nconsider, but virtually any non-interactive aggregation scheme should \nwork (even one based on schnorr signatures, should that even be \npotentially possible).\n\nThis discussion must be held in the community if we seriously want to \nscale Bitcoin, since DMC Factories are just too dangerous to be used \nwith a big amount of users being part of the Factory, and better \napproaches can be applied under non-interactive aggregation schemes.\n\n-- \nAlejandro Ranchal Pedrosa\n\n\n[1]:  Scalable Funding of Bitcoin Micropayment Channel Networks\n[2]:  Scalable Lightning Factories for Bitcoin"
            }
        ],
        "thread_summary": {
            "title": "Lightning Factories & Non-interactive aggregation signature schemes for Bitcoin",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Alejandro Ranchal Pedrosa"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 3138
        }
    },
    {
        "title": "[Lightning-dev] An Argument For Single-Asset Lightning Network",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2019-04-23T03:51:48",
                "message_text_only": "Good morning list,\n\nReviving an old thread, but I saw this just recently: http://diyhpl.us/wiki/transcripts/scalingbitcoin/tokyo-2018/atomic-swaps/\n\nSuppose you are a BTC to WJT exchange.\nI want to pay 1 BTC to send 1000000000 WJT to YAIjbOJA.\nI have a BTC channel to you.\nYou have a WJT channel to YAIjbOJA.\n\nIn order to create a properly-incentivized American Call Option with a premium, you insist that 10% of the WJT value be the premium that is paid if the exchange does not pull through.\n\nWe perform this ritual:\n\n1.  YAIjbOJA generates a secret x and gives h(x) to me.\n2.  On my channel to you, I get 1 BTC from my side and create an HTLC.\n    Hash is h(x) payable to you, timelock is 2 days payable to me.\n3.  On your channel to YAIjbOJA, you get 1000000000 WJT from you, and 100000000 WJT (10%, the premium) from YAIjbOJA and create an HTLC.\n    Hash is h(x) payable to YAIjbOJA, timelock is 1 days payable to you.\n\nThe above also forms an American Call Option, but with a premium if the payment does not push through.\n\nHowever, extending this to beyond one hop after the exchange node is difficult.\nProblems in communicating with the next hop may cause the current hop after the exchange node to become liable for the premium without being able to forward the liability to the final payee, which is an avenue for attack.\nAnd if the payee must be the hop after the exchange node, the exchange node now knows exactly how much and when that node receives payment, and can sell this information and/or selectively disrupt/censor some payments.\n\nPutting the premium before the exchange node is possible with an additional transaction spending the HTLC (the timelock branch is payable to a 2-of-2 with a pre-signed transaction that sends the premium to the exchange and returns the rest of the value to the payer).\nBut this is unsafe, since the exchange (and any node between the payer and the exchange) can stall the protocol deliberately and refuse to forward, extracting the premium via the timelock branch.\nThis is effectively forcing fees even in a route failure, which does not incentivize intermediate nodes to actually forward when they can do nothing and receive fees anyway for not routing.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "An Argument For Single-Asset Lightning Network",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "ZmnSCPxj"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2223
        }
    },
    {
        "title": "[Lightning-dev] Improving Payment Latency by Fast Forwards",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2019-04-24T08:32:26",
                "message_text_only": "Introduction\n============\n\nCurrently, the protocol for forwarding requires 1.5 round trips before the next node can safely forward the payment.\nThis creates much greater latency for payments, and even with the current network at a nascent stage, payments can take entire seconds to complete.\nAs the network grows and some nodes start becoming unable to store the entire routemap, remote route lookups are needed.\nRemote route lookups are likely to increase the route length, thus payment latency will increase even more.\n\nSo, we should consider ways of improving payment latency, before the increasing LN size causes further increases in payment latency.\nLike all optimizations, it is likely that we will need to run as fast as we can, just to stay in one place, like the Red Queen.\n\nSlow Forwarding\n---------------\n\nThe current protocol for a single forward looks like:\n\n      Alice                     Bob                     Carol\n        |                        |                        |\n        | ---update_add_htlc---> |                        |\n        | --commitment_signed--> |                        |\n        |                        |                        |\n        | <-commitment_signed--- |                        |\n        | <--revoke_and_ack----- |                        |\n        |                        |                        |\n        | ---revoke_and_ack----> |                        |\n        |                        | ---update_add_htlc---> |\n\nBob cannot safely forward `update_add_htlc` immediately since there is no commitment transaction that contains the HTLC yet.\nFurther, even when a new pair of commitment transactions is signed by Alice, two commitment transactions can still safely be put onchain, one of which does not contain the HTLC.\nOnly when Alice and Bob have revoked their previous commitment transaction can Bob safely forward to Carol.\n\nThe above concept is called \"irrevocably committed\" by the BOLT spec.\n\nTo reach this \"irrevocably committed\" state, requires the above 1.5 round trips.\nIf Alice and Bob are physically distant from each other, communication latency can be very large.\n\nFurther, both Alice and Bob want to reduce bandwidth usage.\nThis sometimes means that Alice and Bob will wait a short while before signing commitments and revoking previous ones, in order to keep the number of new signatures being passed around small.\nC-lightning, for example, defers signing a new commitment by 10ms after sending an update in case another forward on the same channel is requested.\nThis causes additional latency on top of the communication latency.\n\nPoon-Dryja Outputs\n------------------\n\nLet me now digress, to investigate the outputs of a Poon-Dryja commitment transaction.\n\nThere are at least two commitment transactions that are valid at any one time, one for each node in the channel.\nEach one is symmetrical, but different.\nThus, there is the concepts below:\n\n1.  \"local\" commitment transaction, is the valid commitment transaction I am holding.\n2.  \"remote\" commitment transaction, is the valid commitment transaction my counterparty is holding.\n\nEach commitment transaction has at least two outputs (although one may be elided if too small or 0).\nThus, each commitment transaction has these two outputs:\n\n1.  \"to-local\" output.\n    On the local commitment transaction, this is my \"main\" output.\n    On the remote commitment transaction, this is my counterparty \"main\" output.\n2.  \"to-remote\" output.\n    On the local commitment transaction, this is my counterparty \"main\" output.\n    On the remote commitment transaction, this is my \"main\" output.\n\nIn the original Poon-Dryja formulation, the \"to-remote\" output pays directly to a P2WPKH.\nHowever, the \"to-local\" output is encumbered by a CSV, and is revocable.\nIn the BOLT 1.0 spec, the SCRIPT is:\n\n    OP_IF\n        # Penalty transaction\n        <revocationpubkey>\n    OP_ELSE\n        `to_self_delay`\n        OP_CSV\n        OP_DROP\n        <local_delayedpubkey>\n    OP_ENDIF\n    OP_CHECKSIG\n\nOf note, is that the `revocationpubkey` is actually a combination of the local node revocation key, and a remote node key.\nIt is like a 2-of-2 that cannot be signed cooperatively by both parties, but which the local node can give entirely to the remote node so that the remote node can sign by itself (revocation).\nIt could have been implemented as a 2-of-2 multisignature, but the above formulation takes less block space.\n\nIn the recent Lightning Developer Summit in 2018, it was decided that the \"to-remote\" output will also be encumbered by a CSV.\nI will propose in this writeup, that a modification of the above script be used in both \"to-local\" and \"to-remote\".\n\nFast Forwards\n=============\n\nIdeally, we would like to be able to say that an HTLC is \"irrevocably committed\" using only a single message from Alice to Bob.\nThat way, communication latencies when forwarding payments can be reduced, which should improve payment speed over the network in general.\n\nI observe that one may consider any offchain system a specialization of an offchain transaction cut-through system.\nThus, one may model changes to the offchain system state as the creation of some transactions, followed by a cut-through of those transactions into the new state.\n\nThus, I propose that to-local outputs be encumbered with the script:\n\n    OP_IF\n        # Penalty transaction/Fast forward\n        <local_revokepubkey> OP_CHECKSIGVERIFY <remote_penaltyclaimpubkey>\n    OP_ELSE\n        `to_self_delay`\n        OP_CSV\n        OP_DROP\n        <local_delayedpubkey>\n    OP_ENDIF\n    OP_CHECKSIG\n\nThen, symmetrically, to-remote outputs are encumbered with the script:\n\n    OP_IF\n        # Penalty transaction/Fast forward\n        <local_revokepubkey> OP_CHECKSIGVERIFY <remote_penaltyclaimpubkey>\n    OP_ELSE\n        `to_self_delay`\n        OP_CSV\n        OP_DROP\n        <remote_delayedpubkey>\n    OP_ENDIF\n    OP_CHECKSIG\n\nWhen doing a `revoke_and_ack`, the sender gives the `local_revokeprivkey` to the remote side, who now knows both keys to the penalty branch and can now penalize the sender if the revoked commitment transaction is published.\n\nThen, we define a new message, `fastforward_add_htlc`.\nThis creates a pair of transactions, the fast-forward HTLC transactions, on top of the latest commitment transactions of both nodes.\nFor simplicity, they can be restricted to be sent only while one commitment transaction is valid on both sides and while no update is in-flight in the channel.\n(alternatively, a channel using fast-forwards might be restricted to *only* using fast-forwards, with updates of commitment transactions being in strong synchrony rather than the weak synchrony currently used)\n\nThe local/remote fast-forward HTLC transaction spends the to-local/to-remote output of the commitment transaction.\nIt spends the value of the HTLC being forward to the normal HTLC construction used in Lightning.\nThe remaining change is placed into \"the same\" script that was spent (with pubkeys changed).\nThis change output can now be considered the \"next\" main output (it is used to chain the next `fastforward_add_htlc` from that side).\n\nThe `fastforward_add_htlc` includes the originating node signatures for both the local and remote fast-forward HTLC transactions.\nIt also contains the signatures needed to support the revocable HTLC construction.\n\nFor example, the local fast-forward HTLC transaction spends the to-local output of the local commitment.\nThe sending node signs using the `local_revokepubkey` and includes this signature in the `fastforward_add_htlc` message.\nThe remote fast-forward HTLC transaction spends the to-remote output of the remote commitment.\nThe sending node signs using the `remote_penaltyclaimpubkey` and includes this signature in the `fastforward_add_htlc` message.\n\nThe receiver of the message can now consider this HTLC to be irrevocably committed.\nThis is because it can now spend the main output of the counterparty using the fast-forward HTLC transactions by providing the other missing signature.\nFurther, the sender cannot revoke it since it cannot double-spend that transaction until after the CSV restriction.\nThe CSV restriction is precisely how long the receiver can be offline before a successful theft can be performed, so it should not be an issue for the receiver.\n\nThus, upon receipt of the `fastforward_add_htlc`, it is now possible for Bob to immediately begin forwarding the payment onward:\n\n      Alice                     Bob                     Carol\n        |                        |                        |\n        | -fastforward_add_htlc> |                        |\n        |                        | -fastforward_add_htlc> |\n\n(if Carol does not support fast forwards, Bob can send the old `update_add_htlc` instead.)\nThen, the next commitment transaction will \"cut-through\" any built up fast-forward HTLC transactions, collapsing the HTLC outputs to the commitment transactions.\n\nUnilateral Closes\n-----------------\n\nUnfortunately, unilateral closes mean that, if the counterparty is not paying attention, they will not be able to claim any HTLC added via `fastforward_add_htlc`.\nNow, the CSV setting one selects should reflect how long one feels their node can remain offline.\nAnd as long as we are able to come back online before the CSV is reached, we can apply any fast-forward HTLC transactions and claim the HTLCs that have dropped onchain.\n\nOf course, the real world has many ways to surprise our expectations.\nThus, using fast-forwards implies higher risk for nodes that accept fast forwards and which then themselves forward immediately.\n\n\nFast Failures\n=============\n\nFast forwards are not enough: due to incomplete information, failures of individual payment routing attempts are common.\nThe directive is to simply try and try again until the payment pushes through or no routes remain or too much time has been spent trying to route.\n\nFurther, success is already fast: as soon as you receive `update_fulfill_htlc` you can immediately safely send `update_fulfill_htlc` to upstream without the commitment signing and revocation of previous commitment.\n\nHowever, failures via `update_fail_htlc` cannot be propagated immediately for a similar reason that payments via `update_add_htlc` cannot be propagated immediately: there exists a valid commitment that still has the HTLC by which the money can still be claimed onchain.\n\nI observe that the \"fast forward\" technique simply reuses the revocation path to root a new transaction.\nI also observe that the HTLC construction used by Lightning is revocable.\n\nThus, since it is possible to revoke the HTLC construction used by Lightning, we can reuse the revocation path of an HTLC as the \"fast failure\" path, using the same technique we used in fast forward.\nCare must be taken to provide signatures for failing the HTLC itself, as well as the HTLC-success and HTLC-timeout transactions.\n\nThe difference here is that failed HTLCs do not contribute back to the \"main\" output immediately.\nThe transaction used to fail the HTLC is a simple one-input one-output transaction.\nOnly when failure of the HTLCs has been put in a new commitment transaction can their value be reused for adding new HTLCs.\n\nFor example, suppose we start with 3mBTC on my side of the channel.\nI offer two different HTLCs to you, of 1mBTC each, by two `fastforward_add_htlcs`.\nHowever, both of them fail.\nThis leaves me with only 1mBTC on my main output, with two different 1mBTC outputs that have not been \"merged\" back into it yet.\nSo I can no longer forward a 2mBTC HTLC, until we have resynchronized (signed new commitments and revoked) and combined the failed HTLC outputs back to my main output.\n\nStill, this removes the commitment transaction synchronization away from the critical path in the overall Lightning try-and-try-until-you-die routing algorithm, improving payment latency overall.\nThus, this may be an acceptable tradeoff when considering payment latency.\n\nFees\n====\n\nOh no.\n\nPlease do not ask about fees.\n\nIn case of a unilateral close after a fast-forward or fast-fail, additional transactions need to be put onchain, beyond just the commitment transactions.\nThese transactions need to pay for onchain fees.\n\nThus, channels offering low-latency fast forwards need to charge higher offchain fees to offset the risk that they need to pay onchain fees.\nFurther, channels offering low-latency fast forwards also need to offset the unilateral close risk with higher fees.\n\nPerhaps the two nodes on the channel can attest that they have low-latency fast forwards.\nHowever, merely because they claim it does not make it so.\n\nNodes could make known-failing payments (generate a random payment hash, route through the \"fast forward\"-claiming channel, measure latency) to determine the truth of the fast forward.\nIn fact, if nodes do this \"in the background\" continuously, they can map out which channels have good latency (regardless of the use of fast forward or not: two nodes located physically close to each other with low-latency internet connections may very well have good enough latency even without fast forwards).\n\nFast Forwards on Decker-Russell-Osuntokun\n=========================================\n\nDecker-Russell-Osuntokun does not, in fact, need fast forwards, if we design the link-level protocol properly.\n\nEach \"add HTLC\" \"fulfill HTLC\" \"fail HTLC\" \"change fee\" update message includes the signature needed for the next update transaction and the next state transaction, that immediately has the new state.\nThen the peer should reply with the remaining signatures needed immediately.\n\nUpon receiving an \"add HTLC\", one can now construct the full next update transaction/next state transaction, and the existence of this update transaction is enough to invalidate any previous update transaction.\nSo it is safe to forward the \"add HTLC\" to the next hop immediately as soon as the node can update its local database.\nThis is different from Poon-Dryja, where the existence of the next commitment transactions does not imply that the previous commitment transaction is revoked.\n\nOf course, there is now the issue of \"how do we handle when both nodes want to update at the same time and sent conflicting 'next update' messages?\"\nPerhaps it can be left as an exercise to the reader how to do this while not requiring any round trips in the critical path of forwarding, in the typical case.\nFor instance, if one has sent an update but receives an update in return, coordinate with the counterparty, do not forward yet, and then figure out a common \"next\" update/state transaction that has both updates, then continue with forwarding the update you received."
            }
        ],
        "thread_summary": {
            "title": "Improving Payment Latency by Fast Forwards",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "ZmnSCPxj"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 14659
        }
    },
    {
        "title": "[Lightning-dev] Ptarmigan mainnet release",
        "thread_messages": [
            {
                "author": "Hiroki Gondo",
                "date": "2019-04-30T01:00:24",
                "message_text_only": "Hi, all.\n\nI\u2019m Hiroki (Nayuta team).\nWe release mainnet version (reckless version) of our implementation.\nThe name says \uff40Ptarmigan\uff40.\nhttps://github.com/nayutaco/ptarmigan\nPerhaps this is the fourth implementation of BOLT compliance!\nThis is Reckless version, so we expect user try this software with the\namount afford to lose.\nThanks.\n\nBest\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20190430/560c82d1/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-04-30T04:30:56",
                "message_text_only": "Omedetou goziemasu Hiroki-san to Nayuta.\nI hope you have good first mainnet release and fix many bug!\n\nRegards,\nZmnSCPxj\n\nSent with ProtonMail Secure Email.\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\nOn Tuesday, April 30, 2019 9:00 AM, Hiroki Gondo <hiroki.gondo at nayuta.co> wrote:\n\n> Hi, all.\n>\n> I\u2019m Hiroki (Nayuta team).\n> We release mainnet version (reckless version) of our implementation.\n> The name says \uff40Ptarmigan\uff40.\n> https://github.com/nayutaco/ptarmigan\n> Perhaps this is the fourth implementation of BOLT compliance!\n> This is Reckless version, so we expect user try this software with the amount afford to lose.\n> Thanks.\n>\n> Best"
            }
        ],
        "thread_summary": {
            "title": "Ptarmigan mainnet release",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "Hiroki Gondo"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 1159
        }
    }
]