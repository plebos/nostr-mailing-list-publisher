[
    {
        "title": "[Lightning-dev] Supporting a custodial user who wishes to withdraw all sats from the account...",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2022-09-01T02:55:21",
                "message_text_only": "Good morning Martin,\n\n> Hi folks,\n> \n> I think I've seen wallets supporting \"send max\" when a zero-amount invoice was used. So isn't it a problem with the custodial service not supporting it?\n> Whatever idea we figure out they can just refuse to implement it so we can't force them into improving and being custodial they could steal already, so that shouldn't be an issue.\n\nIndeed, this is the simplest solution.\nAnd as you point out, they could steal the whole funds outright anyway, and similarly they can always pretend that the actual routing fees paid were higher than they actually were.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Supporting a custodial user who wishes to withdraw all sats from the account...",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "ZmnSCPxj"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 613
        }
    },
    {
        "title": "[Lightning-dev] Proposal: Add support for proxying p2p connections to/from LND",
        "thread_messages": [
            {
                "author": "Alex Akselrod",
                "date": "2022-09-01T17:56:04",
                "message_text_only": "At NYDIG, we're considering ways to harden large LND deployments. Joost and I discussed that currently, when external untrusted peers make inbound connections, LND must verify the identity of the peer during the noise handshake, and it must do this before enforcing any potential key-based allow lists. This is done in the same process as the node's other critical tasks, such as monitoring the chain.\n\nTo reduce the attack area of the main node process, we'd like to propose a means to optionally separate the peer communication into a separate process: something like CLN's connectd, running separately, and the connections would be multiplexed over a single network connection initiated from the node to the proxy. The core of our current idea is demonstrated in a draft PR: https://github.com/lightningnetwork/lnd/pull/6843\n\nI'd love some early feedback on the general direction of this. If this would be interesting, I'll build it out into a fully working feature.\n\nThanks,\n\nAlex Akselrod"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2022-09-02T00:37:14",
                "message_text_only": "Hi Alex,\n\nThis is a super cool project! I've shared some thoughts here in a comment on\nthe draft PR:\nPR:\nhttps://github.com/lightningnetwork/lnd/pull/6843#issuecomment-1234933319\n\nAlso I cc'd the lnd mailing list on this reply, perhaps we can move the\ndiscussion over there (or in the issue) since this is more of an lnd\nspecific thing. In the future, the lnd mailing list is also probably a\nbetter place for lnd architecture specific proposals/discussions.\n\n-- Laolu\n\n\nOn Thu, Sep 1, 2022 at 10:56 AM Alex Akselrod via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> wrote:\n\n> At NYDIG, we're considering ways to harden large LND deployments. Joost\n> and I discussed that currently, when external untrusted peers make inbound\n> connections, LND must verify the identity of the peer during the noise\n> handshake, and it must do this before enforcing any potential key-based\n> allow lists. This is done in the same process as the node's other critical\n> tasks, such as monitoring the chain.\n>\n> To reduce the attack area of the main node process, we'd like to propose a\n> means to optionally separate the peer communication into a separate\n> process: something like CLN's connectd, running separately, and the\n> connections would be multiplexed over a single network connection initiated\n> from the node to the proxy. The core of our current idea is demonstrated in\n> a draft PR: https://github.com/lightningnetwork/lnd/pull/6843\n>\n> I'd love some early feedback on the general direction of this. If this\n> would be interesting, I'll build it out into a fully working feature.\n>\n> Thanks,\n>\n> Alex Akselrod\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220901/a7c7cf81/attachment.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2022-09-02T00:47:20",
                "message_text_only": "Hi Alex,\n\nLet's say the adversary targeting your high-value \"LiFi\" infrastructure is\na nation-state sponsored hacking-group with strong capabilities (as we're\nseeing today in the cryptocurrencies DeFi space). This hacking group avails\nhundreds of bitcoins to fund channels, is able to setup thousands of sybil\npeers across the base-layer p2p network, has built a fine-grained knowledge\nof the miner mempools, controls few Internet ASNs and has bought\nsecond-hands mining chips on the market to own limited hashing capabilities.\n\nAs of today, they would have a marked embarrassment on which attack pickup\nto target your Lightning infrastructure. They could start with an \"old\nknown\" jamming attack to permanently cut your channel links from the rest\nof the wider network topology [0].\nOr they could launch time-dilation attacks by building on BGP disruptions,\nuntil your chain view is so far in the past from the network tip that your\non-chain HTLC-claiming safety reaction timers are useless [1]. Or they\ncould exercise pinning attacks to prevent you to claim back routed HTLC, as\na malicious commitment transaction sleeps in the network mempools until\nit's too far [2].  Or they could target your pre-0.24 Bitcoin Core\nfull-node to provoke a memory crash thanks to a long-chain of low-work\nheaders [3].\n\nAnd I would say that's only a subset of the attack surface of a Lightning\nnode.\n\nConsidering all those factors, moving a LN implementation architecture from\na single, monolithic process towards a collection of processes, where the\ncritical components of the LN active defense security model are replicated\nand distributed, redundant access to the chain view guaranteed, sandboxing\naccess between processes enforced and data flow monitored to react on\nanomalies, all things your #6843 would make easier, sounds a reasonable\ndirection. I think it's needed if you aim for your infrastructure to\nsurvive strong attacks.\n\nOn the LDK-side, inheriting from the adversarial thinking and safety-first\nmindset development culture from Bitcoin Core, we've always considered that\ntype of scenarii since the early days and designed our software in\nconsequence. We have been working and we'll keep working on many\nsecurity/safety hardening: external signing [4], replicated  chain\nmonitoring [5], dynamic fee-bumping of time-sensitive transactions, various\nattack vectors mitigations [6]. All that said, we're looking forward to\ncollaborate with the wider Lightning community on reusable security modules\nacross implementations (e.g jamming mitigations) and wished\n\"fix-the-annoying-holes\" changes in Bitcoin Core (e.g package relay).\n\nCheers,\nAntoine\n\n[0] https://jamming-dev.github.io/book/\n[1] https://arxiv.org/abs/2006.01418\n[2]\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2020-June/002758.html\n[3] https://github.com/bitcoin/bitcoin/pull/25717\n[4] https://github.com/lightningdevkit/rust-lightning/pull/214\n[5] https://github.com/lightningdevkit/rust-lightning/pull/679\n[6] https://github.com/lightningdevkit/rust-lightning/pull/1009\n\nLe jeu. 1 sept. 2022 \u00e0 13:56, Alex Akselrod via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> At NYDIG, we're considering ways to harden large LND deployments. Joost\n> and I discussed that currently, when external untrusted peers make inbound\n> connections, LND must verify the identity of the peer during the noise\n> handshake, and it must do this before enforcing any potential key-based\n> allow lists. This is done in the same process as the node's other critical\n> tasks, such as monitoring the chain.\n>\n> To reduce the attack area of the main node process, we'd like to propose a\n> means to optionally separate the peer communication into a separate\n> process: something like CLN's connectd, running separately, and the\n> connections would be multiplexed over a single network connection initiated\n> from the node to the proxy. The core of our current idea is demonstrated in\n> a draft PR: https://github.com/lightningnetwork/lnd/pull/6843\n>\n> I'd love some early feedback on the general direction of this. If this\n> would be interesting, I'll build it out into a fully working feature.\n>\n> Thanks,\n>\n> Alex Akselrod\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220901/d81ca8c8/attachment-0001.html>"
            },
            {
                "author": "Pierre",
                "date": "2022-09-03T13:04:22",
                "message_text_only": "Hi Alex,\n\nThis is quite similar to eclair's cluster mode [1] so I can definitely say\nit's a great idea :-)\n\nOur rationale was as much reducing the attack surface (we didn't like our\nnode being directly accessible from the internet) as improving scalability\n(handling the constant flow of connections/disconnections from mobile\nwallets, and a ton of gossip).\n\nSome specific points on our implementation:\n- there can be N instances of proxy and their number can be adjusted\nwithout downtime\n- our proxy also does preprocessing for gossip (they keep an up-to-date\ncopy of routing table and directly answer queries, and also dedup incoming\nannouncements before forwarding to the backend, where the announcements are\nverified)\n- outgoing connections (initiated by an operator from the backend) are also\ninitiated by the proxy. Even incoming connections from Tor are directed to\nthe proxy. The backend node handles zero LN connections\n- since the front needs the node key for the secure handshake, this led us\nto introduce a separate key hierarchy for channel management [2]\n- we have considered, but not implemented, a \"lockdown mode\" where the\nfront would only allow incoming connections from known peers that already\nhave a channel\n- if your setup has a separate db server (e.g. postgres), and logs are\nproperly rotated/streamed out, then the resources that typically run out\nwhen scaling up (file descriptors, disk space, etc.) should be really under\ncontrol on your main node\n\nHope that helps,\n\nPierre\n\n\n[1] https://github.com/ACINQ/eclair/blob/master/docs/Cluster.md\n[2] https://github.com/ACINQ/eclair/pull/1584\n\n\nLe jeu. 1 sept. 2022 \u00e0 19:56, Alex Akselrod via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> At NYDIG, we're considering ways to harden large LND deployments. Joost\n> and I discussed that currently, when external untrusted peers make inbound\n> connections, LND must verify the identity of the peer during the noise\n> handshake, and it must do this before enforcing any potential key-based\n> allow lists. This is done in the same process as the node's other critical\n> tasks, such as monitoring the chain.\n>\n> To reduce the attack area of the main node process, we'd like to propose a\n> means to optionally separate the peer communication into a separate\n> process: something like CLN's connectd, running separately, and the\n> connections would be multiplexed over a single network connection initiated\n> from the node to the proxy. The core of our current idea is demonstrated in\n> a draft PR: https://github.com/lightningnetwork/lnd/pull/6843\n>\n> I'd love some early feedback on the general direction of this. If this\n> would be interesting, I'll build it out into a fully working feature.\n>\n> Thanks,\n>\n> Alex Akselrod\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220903/3d759580/attachment.html>"
            },
            {
                "author": "Pierre",
                "date": "2022-09-03T13:09:15",
                "message_text_only": "Hi Alex,\n\nThis is quite similar to eclair's cluster mode [1] so I can definitely say\nit's a great idea :-)\n\nOur rationale was as much reducing the attack surface (we didn't like our\nnode being directly accessible from the internet) as improving scalability\n(handling the constant flow of connections/disconnections from mobile\nwallets, and a ton of gossip).\n\nSome specific points on our implementation:\n- there can be N instances of proxy and their number can be adjusted\nwithout downtime\n- our proxy also does preprocessing for gossip (they keep an up-to-date\ncopy of routing table and directly answer queries, and also dedup incoming\nannouncements before forwarding to the backend, where the announcements are\nverified)\n- outgoing connections (initiated by an operator from the backend) are also\ninitiated by the proxy. Even incoming connections from Tor are directed to\nthe proxy. The backend node handles zero LN connections\n- since the front needs the node key for the secure handshake, this led us\nto introduce a separate key hierarchy for channel management [2]\n- we have considered, but not implemented, a \"lockdown mode\" where the\nfront would only allow incoming connections from known peers that already\nhave a channel\n- if your setup has a separate db server (e.g. postgres), and logs are\nproperly rotated/streamed out, then the resources that typically run out\nwhen scaling up (file descriptors, disk space, etc.) should be really under\ncontrol on your main node\n\nHope that helps,\n\nPierre\n\n\n[1] https://github.com/ACINQ/eclair/blob/master/docs/Cluster.md\n[2] https://github.com/ACINQ/eclair/pull/1584\n\nLe jeu. 1 sept. 2022 \u00e0 19:56, Alex Akselrod via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> At NYDIG, we're considering ways to harden large LND deployments. Joost\n> and I discussed that currently, when external untrusted peers make inbound\n> connections, LND must verify the identity of the peer during the noise\n> handshake, and it must do this before enforcing any potential key-based\n> allow lists. This is done in the same process as the node's other critical\n> tasks, such as monitoring the chain.\n>\n> To reduce the attack area of the main node process, we'd like to propose a\n> means to optionally separate the peer communication into a separate\n> process: something like CLN's connectd, running separately, and the\n> connections would be multiplexed over a single network connection initiated\n> from the node to the proxy. The core of our current idea is demonstrated in\n> a draft PR: https://github.com/lightningnetwork/lnd/pull/6843\n>\n> I'd love some early feedback on the general direction of this. If this\n> would be interesting, I'll build it out into a fully working feature.\n>\n> Thanks,\n>\n> Alex Akselrod\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220903/ad15459d/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Proposal: Add support for proxying p2p connections to/from LND",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Pierre",
                "Alex Akselrod",
                "Olaoluwa Osuntokun",
                "Antoine Riard"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 13846
        }
    },
    {
        "title": "[Lightning-dev] Inbound channel routing fees",
        "thread_messages": [
            {
                "author": "Owen Gunden",
                "date": "2022-09-01T21:57:03",
                "message_text_only": "On 7/1/22 08:02, Joost Jager wrote:> Any thoughts from routing node \noperators would be welcome too (or links > to previous threads).\nI'm a routing node operator and have been for over a year. I've been \nwanting this feature (plus negative fees) for a long time now.\n\nReason 1:\nFor peers that tend to be liquidity sources (i.e. the liquidity tends to \nbe all on my side), there's currently no way to keep a channel in \nbalance with fees. All I can do is set a zero outbound fee and usually \nthat's not good enough :/. A high inbound fee would discourage payments \nthrough that route.\n\nReason 2:\nNot all inbound traffic is created equally. If two different peers wish \nto route through me to the same outbound peer, I may value the two \nforwards differently depending on who the sender is. E.g.\n\nA->C where A is a great peer that I frequently route payments to\nB->C where B is a peer that I rarely route payments to, or otherwise \nwish to preserve more inbound from\n\nI'm much happier routing A->C than I am routing B->C, but there's \ncurrently no way of expressing this to the market through fees. All I \ncan do is htlc-intercept B->C and reject it, but this damages my \nreputation obviously.\n\nReason 3:\nGreater expressivity in fee-setting generally allows markets to push \nmore flows off-chain without having to loop or open new channels. While \nI think negative fees would be more impactful for this, inbound fees are \nhelpful as well. Combining negative + inbound fees is where the real \nmagic would happen IMO."
            }
        ],
        "thread_summary": {
            "title": "Inbound channel routing fees",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Owen Gunden"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1513
        }
    },
    {
        "title": "[Lightning-dev] Fee Ratecards (your gateway to negativity)",
        "thread_messages": [
            {
                "author": "lisa neigut",
                "date": "2022-09-13T21:15:21",
                "message_text_only": "Hi all,\n\nI've been talking about them for a bit[^1], now[^2], but here's a more\nformal proposal for fee ratecards.\n\nSpec PR + implementation to come.\n\n## What is a fee ratecard?\n\nA ratecard is a set of four values, positive or negative, that price different\nbands of available liquidity for a channel.\n\nA ratecard replaces the current `fee_proportional_millionths` and\n`fee_base_msat` currently used to calculate the owed fees for\nforwarding a payment on the\nlightning network.\n\nInstead, with ratecards, a channel operator may specify four different\nrates for a channel's liquidity, that will automatically be updated\ndepending on the current channel capacity. The four capacity bands are fixed\nat 0-25%, 26-50%, 51-75%, and 76-100%.\n\nCurrently, all payments are priced equally. Fee ratecards update this such that\npayments can be priced differently, depending on the current available\ncapacity in a channel. This allows node operators to dynamically price\ntheir liquidity, without needing to continually issue gossip updates as\nthe balance changes (which may leak payment flow timing).\n\n## Spec Changes + Implementation Notes\n\nTo the `channel_update` gossip message, we add a TLV with one defined\ntype, `ratecard`. The ratecard contains 4 16-bit signed integers, one\nfor each quarter of channel capacity. (0-25, 26-50, 51-75, 76-100%).\n\n```\n1. `tlv_stream`: `channel_update_tlvs`\n2. types:\n    1. type: 1 (`ratecard`)\n    2. data:\n        * [`s16`:`fee_basis_0_25`]\n        * [`s16`:`fee_basis_26_50`]\n        * [`s16`:`fee_basis_51_75`]\n        * [`s16`:`fee_basis_76_100`]\n```\n\nIf a node is advertising a `ratecard` in their `channel_update` for\nthat channel, a routing node SHOULD pick the rate to pay based on\ntheir current guess of the channel's capacity as well as priority for\nthe payment.\n\nIf a payment is high priority, it is recommended to pay at the highest feeband,\nto ensure delivery (if possible).\n\nOn payment failure, the response is the same (you return a copy of the\n`channel_update` in the onion). You MAY give an additional hint as to\nwhat the current\nacceptable feerate is for a payment.\n\nIf the feerate card entry is set, the rates there take precedence over the\n`fee_base_msat` and the `fee_proportional_millionths`.\n\nThe `feerate` card effectively removes the ability to set a `fee_base_msat`.\n\nTo minimize payment routing failure, it is recommended to set the\n`fee_proportional_millionths` to the same rate as the highest `ratecard`\nrate. (Most likely the `ratecard`.`fee_basis_0_25`).\n\nNote that `ratecards` unit is in basis points, not millionths. As basis points\nare 1/10,000, they're 100x less than the same value expressed as millionths.\n\n- A `basis_point` value of 5 is a fee of 5msat on a 10k msat payment.\n- A `fee_proportional_millionths` of 500 is a fee of 5msat on a 10k\nmsat payment.\n\nThe current expected price for sats to be pushed to the other side of the\nchannel is to be priced by the lowest ratecard capacity band that that payment\nwill push the balance into.\n\nFor example, if a payment moves 50k through a 100k channel which is currently\nat a total available capacity of 75k sats (which means it'll move the capacity\nfrom 75k to 25k), that payment will be expected to pay a rate equal to the\n0-25% band, as it'll push the available capacity into the 0-25% range.\n\nUsing this rule, HTLCs consuming significant portions of a channel's\ntotal capacity are more likely to pay higher fees.\n\n### Motivation and Rationale\nAt the last in-person spec meeting, it was noted that channel capacity is\noften of variable value depending on the amount of capacity currently\navailable in that channel.\n\nIt was also noted that we'd like to allow node operators to experiment\nwith negative fees.\n\nYou can't easily allow for negative feerates with our current gossip messages,\nas it would give gossip an economic value which may have an adverse impact\non the ability of nodes to stay up to date with the current lightning channel\ntopology; it also stands to greatly increase the volume of gossip\nupdates issued by any one node, which should scale with payment volume,\nas nodes update their gossip to change their current fees on any one channel.\n\nNote that some nodes already automatically update their channel fees based on\nthe available capacity in a channel; fee ratecards should be a more succinct\nway for node operators to express more fine grained prices of their existing\ncapacity, with a much reduced bandwidth burden.\n\n### Feerate Cards, Frequently Asked Questions\n\n#### Why four evenly spaced buckets? Why not X function?\n\n1. Expressibility\n\tFees are calculated by the node creating the route for a payment, using\n\tparameters set by the channel node operator. These paramters must be\n\tcommunicated via gossip to every node, and then used as inputs to the\n\trouting function.\n\n\tParameters need to be uniform and well understood such that\n\trouting algorithm designers can make efficient use of them.\n\t\n\tFour buckets is wide enough such that payment should\n\tbe able to find/pick a current channel capacity with good\n\tprobability, yet hopefully expressive enough for routing\n\toperators to be able to price their available liquidity\n\twith sufficient granularity.\n\n\tFour evenly spaced buckets also make it much easier to\n\treason about information leakage (binary search).\n\n2. Predictability for payment success.\n\tFeerate cards don't introduce any new risk to payment failure,\n\tas the current fee rates of a channel may currently differ\n\tfrom a node's gossip information.\n\n\tRather, feerate cards offer a way for your routing algorithm\n\tto dynamically decide on what level of capacity it estimates\n\tthe channel to be at, and its tolerance for failures.\n\n\n#### Won't this leak my channel balance?\n\nAt most, feerate buckets reduce the total number of payment attempts\nrequired to find an exact channel balance by 2.\n\nIt does, however, provide a much lower 'cost of capital' mechanism\nfor discovering which band of payment is likely to succeed.\n\nCurrently, to discover if a 100k channel has capacity for a 75k sat payment,\nyou'd need 75k sats of available liquidity in the node leading up to\nthat channel (or 25k in the opposite direction) from your probe node, in\norder to discover if that bucket is available.\n\nThis ties up available liquidity along the entire probe route.\n\nWith feerate cards, you could do a 1k sat payment at each feerate card\nfee band, and discover the approximate bucket without tying up\nas much of your own own outband capital.\n\nThis makes probing more feasible for any node that wants to make a payment,\nwhile reducing the capital tied up to discern the same amount of information.\n\n#### More data in the channel_update message! Just what we need, more gossip.\n\nIt's anticipated that feerate cards will greatly reduce the number of\n`channel_update`s issued by a node, by allowing node operators to dynamically\nprice their liquidity by available capacity via a single, static\n`channel_update`.\n\nThis should reduce or remove the need to rebroadcast a `channel_update` when a\nchannel's capacity changes.\n\n\n#### How do negative fees work?\n\nAstute readers may notice that the proposed feerate card bands can be expressed\nas negative numbers. This allows node operators to price the liquidity\navailable in their channel at a discount.\n\nHere's a good way to think about the impact a negative fee will\nhave on your channel balances.\n\nA feerate of 5 basis points means that, in order to push 10k sats from\none of your channel balances to the peer, another of yours must push\nyou 10,005 sats. This means you always need more inbound than outbound\nin order to route a payment (unless you're using zero fees, in which case\nyou'd need an equal amount).\n\nNegative fees reverse this. At -5 basis points, you'd only need to receive 9,995\non an inbound channel to cause you to push out 10k sats. This allows you\nto reallocate your inbound capacity at a slight discount to the party\nwho's responsible for moving the capital.\n\n#### What happened to payment base fees?\n\nThey're going away (they do not work with negative rates).\nHopefully ratecards will give you enough expressivity to figure out\ngood rates for your channel capacity without them.\n\n\n### Credits\n\nThis proposal is a marriage of Clara Shikleman's push to start thinking\nabout negative fees, and ZmnSCPxj's comments on the variability of\nvalue of a channel's liquidity based on current capacity.\n\n\n~niftynei\n\n[^1]: [Slides on liquidity +\nlightning](https://docs.google.com/presentation/d/1gK5_JvDl6aR8EqEKIxfdIHMR2vNIgHUS5tydP2kB_gA/edit?usp=sharing)\n[^2]: [Presentation of slides at btcpp in\nAustin](https://www.youtube.com/watch?v=voEbAeS_B5w)\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220913/4d5cd38e/attachment.html>"
            },
            {
                "author": "David A. Harding",
                "date": "2022-09-23T01:11:18",
                "message_text_only": "On 2022-09-13 11:15, lisa neigut wrote:\n> Hi all,\n\nHi Lisa,\n\nThank you for describing this idea in detail on the mailing list.\n\n> A ratecard is a set of four values, positive or negative, that price\n> different bands of available liquidity for a channel.\n\nAm I understanding correctly that this implies spenders wanting to send \nan LN payment will either need to estimate the current division of funds \nfor every hop along their projected path or will need to pay the highest \nratecard for each hop?  How are spenders supposed to make those \nestimates about the division of funds in distant channels?\n\nIf there's no easy and network-friendly way for spenders to gather that \ninformation, I would worry that will lead to the creation of services \nwhich do collect the info and which become central to LN's operation.\n\n> For example, if a payment moves 50k through a 100k channel which is\n> currently at a total available capacity of 75k sats (which means it'll \n> move the\n> capacity from 75k to 25k), that payment will be expected to pay a rate \n> equal to\n> the 0-25% band, as it'll push the available capacity into the 0-25% \n> range.\n\nShouldn't this be pro rata?  If it's not expected to be proportional at \nthe protocol level, then spender Alice can still get almost proportional \nrates by sequentially sending Bob's forwarding node fifty 1k payments \nand have 24 of them pay the 51-75 rate, 25 pay the 26-50 rate, and only \n1 pay the 0-25 rate.  Since base fees are disallowed, this costs Alice \nnothing extra but reduces network capacity by consuming HTLC slots for \nher, Bob, and every other forwarding channel.\n\nHowever, for Alice to pay proportional fees either way, she'd need a \nfairly precise estimate of the current division of funds in one of Bob's \nchannels, which brings me back to my earlier question about how Alice is \nsupposed to obtain that information in a way that's easy and friendly to \nthe network (and therefore resistant to centralization).\n\nThanks,\n\n-Dave"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-09-23T02:08:08",
                "message_text_only": "Good morning Dave,\n\n> On 2022-09-13 11:15, lisa neigut wrote:\n> \n> > Hi all,\n> \n> \n> Hi Lisa,\n> \n> Thank you for describing this idea in detail on the mailing list.\n> \n> > A ratecard is a set of four values, positive or negative, that price\n> > different bands of available liquidity for a channel.\n> \n> \n> Am I understanding correctly that this implies spenders wanting to send\n> an LN payment will either need to estimate the current division of funds\n> for every hop along their projected path or will need to pay the highest\n> ratecard for each hop? How are spenders supposed to make those\n> estimates about the division of funds in distant channels?\n> \n> If there's no easy and network-friendly way for spenders to gather that\n> information, I would worry that will lead to the creation of services\n> which do collect the info and which become central to LN's operation.\n\nAs I understand it, it is exactly the same way that is done currently: the sender optimistically tries a route with a particular feerate, and if that fails, tries another route.\n\nBasically, you can model a rate card as four separate channels between the same two nodes, with different costs each.\nIf the path at the lowest cost fails, you just try at another route that may have more hops but lower effective cost, or else try the same channel at a higher cost.\n\nIf your concern is valid, one wonders why it would not already exist now in the current network where try-and-try-again is the standard overall algorithm for payments.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "David A. Harding",
                "date": "2022-09-23T04:39:55",
                "message_text_only": "On 2022-09-22 16:08, ZmnSCPxj wrote:\n> Basically, you can model a rate card as four separate channels between\n> the same two nodes, with different costs each.\n> If the path at the lowest cost fails, you just try at another route\n> that may have more hops but lower effective cost, or else try the same\n> channel at a higher cost.\n\nThat's a very easy to understand explanation of how to use the system, \nthanks!\n\n> If your concern is valid, one wonders why it would not already exist\n> now in the current network where try-and-try-again is the standard\n> overall algorithm for payments.\n\nMy concern is about pathfinding algorithms which depend on unscalable \ndata collection (e.g. frequent whole network probing).  If such an \nalgorithm performs much better than those algorithms which depend on \nscalable data collection (e.g. receiving gossip), then the network may \ngrow to depend on the centralized entities performing the data \ncollection to the detriment of its robustness and its participants' \nindependence.\n\nTry-and-try-another-path is somewhat problematic in this regard since, \neven with no additional action like probing, entities which send many \npayments will likely perform significantly better than entities which \nsend few payments owing to the high-frequency spenders gaining more \nknowledge about which channels and amounts recently worked or did not.  \nIt seems to me that a more idealized system would only rarely have \nforwarding failures so that high frequency spenders wouldn't receive \nmuch more information than low frequency spenders.  To that regard, fee \nratecards feels like a small step in the wrong direction because \nmodeling one channel as four separate channels further normalizes \nfailure and so further moves the system towards centralized dependency.  \nThat said, as you mentioned in a previous post[1], I agree ratecards is \nbetter than frequently issuing new channel updates with modified fees.\n\nThanks,\n\n-Dave\n\n[1] \nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2022-June/003598.html"
            },
            {
                "author": "lisa neigut",
                "date": "2022-09-23T20:13:53",
                "message_text_only": "Some interesting points here. Will try to respond to some of them.\n\n> pathfinding algorithms which depend on unscalable data collection\n\nFailed payment attempts are indistinguishable from data collection probing.\nI don't think it's realistic to think that payments are going to stop\nfailing in\nan environment where we continue to prioritize privacy of channel balances.\n\nI think I pretty nicely illustrated the benefits of using payment bands in\nmy original\npost wrt to a balance between \"probe-ability\" and some obfuscation of\nbalance, along\nwith a net reduction in the bandwidth consumption required to get this\nbasically\nrequired information for payment success.\n\nRene Pickhardt has a less private, less costly, more granular, smaller scope\nproposal for sharing channel balances w/ directly connected peers; I think\nthis is worth investigating independently of this proposal.\n\n> depend on the centralized entities performing the data collection\n\nI like to think that the introduction of negative fees make channel\nbalance data a competitive advantage and will actually cause node\noperators to more closely guard their balances / the balance data\nthey've collected about peers, which should hopefully reduce the current\ntrend of sharing this information with centralized parties.\n\nNote that with the present protocol design, the network incentives are such\n that centralized efforts to collect exact balance data already exist. So\nmoving to this design has the potential to reduce the incentive to\nparticipate\nin the data collection, at the very least it does not make it worse than\ncurrent.\n\n>  this costs Alice nothing extra but reduces network capacity by consuming\n>  HTLC slots for her, Bob, and every other forwarding channel.\n\nthe network already limits the size of htlcs that are allowable with the\nhtlc_maximum_msat, which I understand is typically set at a rate below 1/4\nof channel capacity (citation needed). Which is to say that the large\nconsumption of a channel in a single HTLC is currently relatively\nprohibited.\nIt's likely this proposed change would actually encourage operators to set\ntheir htlc_maximum_msat higher, now that there's a direct financial cost\ntied to larger channel bandwidth consumption.\n\nGreat questions! Hope that gives a better picture of the current landscape.\n\nAlso h/t to ZmnSCPxj for the excellent \"four parallel channels\" analogy,\nthis is\na really elegant way to think about the proposal.\n\nOn Thu, Sep 22, 2022 at 11:39 PM David A. Harding <dave at dtrt.org> wrote:\n\n> On 2022-09-22 16:08, ZmnSCPxj wrote:\n> > Basically, you can model a rate card as four separate channels between\n> > the same two nodes, with different costs each.\n> > If the path at the lowest cost fails, you just try at another route\n> > that may have more hops but lower effective cost, or else try the same\n> > channel at a higher cost.\n>\n> That's a very easy to understand explanation of how to use the system,\n> thanks!\n>\n> > If your concern is valid, one wonders why it would not already exist\n> > now in the current network where try-and-try-again is the standard\n> > overall algorithm for payments.\n>\n> My concern is about pathfinding algorithms which depend on unscalable\n> data collection (e.g. frequent whole network probing).  If such an\n> algorithm performs much better than those algorithms which depend on\n> scalable data collection (e.g. receiving gossip), then the network may\n> grow to depend on the centralized entities performing the data\n> collection to the detriment of its robustness and its participants'\n> independence.\n>\n> Try-and-try-another-path is somewhat problematic in this regard since,\n> even with no additional action like probing, entities which send many\n> payments will likely perform significantly better than entities which\n> send few payments owing to the high-frequency spenders gaining more\n> knowledge about which channels and amounts recently worked or did not.\n> It seems to me that a more idealized system would only rarely have\n> forwarding failures so that high frequency spenders wouldn't receive\n> much more information than low frequency spenders.  To that regard, fee\n> ratecards feels like a small step in the wrong direction because\n> modeling one channel as four separate channels further normalizes\n> failure and so further moves the system towards centralized dependency.\n> That said, as you mentioned in a previous post[1], I agree ratecards is\n> better than frequently issuing new channel updates with modified fees.\n>\n> Thanks,\n>\n> -Dave\n>\n> [1]\n>\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-June/003598.html\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220923/07d0d25b/attachment.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-09-24T09:01:59",
                "message_text_only": "On Fri, Sep 23, 2022 at 03:13:53PM -0500, lisa neigut wrote:\n> Some interesting points here. Will try to respond to some of them.\n> > pathfinding algorithms which depend on unscalable data collection\n> Failed payment attempts are indistinguishable from data collection probing.\n\nEven so, data collection probing is *preferable* -- it can happen out\nof band, and doesn't need to cause latency when you're trying to finish\npaying for your coffee so you can sit down and get back to doomscrolling.\n\nIn general: if you need to know channel capacities to efficiently make\npayments, doesn't that fundamentally mean that that information should\nbe gossipped?\n\nFor instance, in a world where everyone's doing rate cards, maybe every\nchannel is advertising fees at -0.05, +0.01, +0.1, +1.0 bps because that's\njust what turns out to be \"best\". But then when you're trying to find a\nroute, it becomes critically important to know which channels are at which\ncapacity quartile. If you're not gossipping that information, then someone\nmaking a payment needs to either probe every plausible path, or subscribe\nto an information provider that is regularly probing every channel.\n\nI still think what I wrote in June applies; from [0], what you want\nto maintain is a balanced flow over time, not any particular channel\nbalance -- so collecting less fees at 25% balance than at 75% balance\nis generally a false optimisation; and from [1], having fee rate cards\nthat just depend on time of day/week is probably a much better method of\noptimising for what actually matters -- \"these are the times my channel\nis in high demand in this direction, so fees are high; these are the\ntimes demand is low, so fees are low\".\n\n[0] https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-June/003624.html\n[1] https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-June/003627.html\n\n> I like to think that the introduction of negative fees make channel\n> balance data a competitive advantage and will actually cause node\n> operators to more closely guard their balances / the balance data\n> they've collected about peers, which should hopefully reduce the current\n> trend of sharing this information with centralized parties.\n\nHaving fees depend on the channel balance makes the data a competitive\nadvantage to the people trying to use the channel; for the channel owner,\nthe optimal situation is everyone knows the balance, so that more payments\nget routed over the channel (because people don't overestimate the fee\nrate). That encourages channel owners to broadcast the information,\nnot keep it private. If they can't broadcast it, that just creates a\nmarket for centralised information brokers...\n\nCheers,\naj"
            },
            {
                "author": "Ren\u00e9 Pickhardt",
                "date": "2022-09-25T19:52:17",
                "message_text_only": "Dear Lisa and lightning developers,\n\nthank you for your contribution and ideas to the problem of increasing\nreliability to the payment deliver process by having balanced channels and\nproviding liquidity where necessary. This is at least how I understand your\nintentions of the proposal. I will just add a few notes and remarks.\n\n1. I think negative fees are certainly an interesting idea that is\nworthwhile to investigate. Mathematically speaking it is kind of strange to\ncut the routing_costs at 0 and to restrict market participants from\nselecting / offering negative fees. In particular min cost flow solvers\nshould not have any problems with negative fees. I also like the fact that\nwe would have another reason to deprecate the base fee. However I am\nuncertain if negative fees may introduce other problems and unintended side\neffects in particular with strategic behavior of routing nodes. The biggest\nissue I see is if negative fees would produce a negative cost cycle.\nObviously everyone would try to cancel such a cycle and earn some arbitrage\nby moving liquidity. While the first node may be successful I assume nodes\nwill not directly update fees / propagate gossip which would basically\ncreate a lot of traffic requests to nodes in the negative cost cycle even\nthough there is no liquidity in that fee band left.\n\n2. I like the fact that fee rate cards would produce a piece wise linear\ncost function which should also be convex (assuming the rates increase in\nevery band). Assuming we move forward with fee rate cards we should make it\nan explicit requirement that the fees in higher bands must not decrease,\nwhich seems like a very sane requirement. (Is it?) However I am not sure if\nthe virtualization of the channel into 4 smaller channels as ZmnSCPxj\nsuggested to think of it is beneficial for the network. Intuitively (I have\nno formal proof or simulation for this yet) I agree with the people who\nvoiced their concern that this may just overall increase payment latency\nand decrease reliability (because selfish senders might start with the\ncheapest or cheaper bands). My concern might be mitigated if we started to\nshare two bits of information about the liquidity in a channel (either\nnetwork wide) or as I propose in PR 780 within the friend of the friend\nnetwork [1]. In your other mail of this thread [2] you referred to PR 780\nand noted that it may be worthwhile to investigate this. I agree and I\ninvite you to help me doing so.\n\n3. After having privately discussed my recent blog article [3] to set up\nvalves via `htlc_maximum_msat` with a few node operators and having heard\ntheir thoughts I realized that the idea of fee rate cards might actually be\neven more powerful if not used to divide the channel capacity but rather\nhave various fee rate cards for various `htlc_maximum_msat` values. The\nIdea is quite simple: If you have a certain drain on a channel and install\na valve to do flow control you could still offer someone to break the valve\nopen and get their large payment through and deplete your channel if they\nwould pay premium for it. Again without formal proof or simulation it is my\nintuition that this is a more natural and less complicated design and would\nachieve two things which both seem beneficial for the network and its\nparticipants:\n\n3a) Node operators would not loose out the opportunity to route large\npayments when setting up valves. The main concern I had from some node\noperators about setting up valves was that they know that some channels\nhave not many forwarding requests but usually rather large ones. Setting up\na valve may not be in their interest as it would be too restrictive. With\nvarious fee rates for various payment sizes they could still allow large\npayments (Similar to your proposal where payments that are larger than 25%\nof the capacity also cannot be in the first band of the fees)\n\n3b) In contrast to your proposal the sending nodes would not have to have\nthe doubt in which band the channel currently is and save on guessing. If\nthe amount is small enough they would usually get the cheaper fee rate but\nif they want to move a lot of liquidity they have to offer a premium. I\nthink this would also produce a convex cost function but reduce the\nguessing game and lower failed attempts and latency. It could still happen\nthat especially for a larger amount that not enough liquidity would be in\nthe channel and even a small payment could fail (as could happen these\ndays). Both could happen within your proposal too.\n\nSo while I find your proposal interesting I would love to investigate a bit\nmore how it interacts with other proposals and if we could make your and\nother proposals even stronger by combining some of the other ideas.\n\nwith kind Regards Rene\n\n[1]: https://github.com/lightning/bolts/pull/780\n[2]:\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2022-September/003693.html\n[3]:\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2022-September/003686.html\n\nOn Tue, Sep 13, 2022 at 11:15 PM lisa neigut <niftynei at gmail.com> wrote:\n\n> Hi all,\n>\n> I've been talking about them for a bit[^1], now[^2], but here's a more formal proposal for fee ratecards.\n>\n> Spec PR + implementation to come.\n>\n> ## What is a fee ratecard?\n>\n> A ratecard is a set of four values, positive or negative, that price different\n> bands of available liquidity for a channel.\n>\n> A ratecard replaces the current `fee_proportional_millionths` and `fee_base_msat` currently used to calculate the owed fees for forwarding a payment on the\n> lightning network.\n>\n> Instead, with ratecards, a channel operator may specify four different\n> rates for a channel's liquidity, that will automatically be updated\n> depending on the current channel capacity. The four capacity bands are fixed\n> at 0-25%, 26-50%, 51-75%, and 76-100%.\n>\n> Currently, all payments are priced equally. Fee ratecards update this such that\n> payments can be priced differently, depending on the current available\n> capacity in a channel. This allows node operators to dynamically price\n> their liquidity, without needing to continually issue gossip updates as\n> the balance changes (which may leak payment flow timing).\n>\n> ## Spec Changes + Implementation Notes\n>\n> To the `channel_update` gossip message, we add a TLV with one defined type, `ratecard`. The ratecard contains 4 16-bit signed integers, one for each quarter of channel capacity. (0-25, 26-50, 51-75, 76-100%).\n>\n> ```\n> 1. `tlv_stream`: `channel_update_tlvs`\n> 2. types:\n>     1. type: 1 (`ratecard`)\n>     2. data:\n>         * [`s16`:`fee_basis_0_25`]\n>         * [`s16`:`fee_basis_26_50`]\n>         * [`s16`:`fee_basis_51_75`]\n>         * [`s16`:`fee_basis_76_100`]\n> ```\n>\n> If a node is advertising a `ratecard` in their `channel_update` for that channel, a routing node SHOULD pick the rate to pay based on their current guess of the channel's capacity as well as priority for the payment.\n>\n> If a payment is high priority, it is recommended to pay at the highest feeband,\n> to ensure delivery (if possible).\n>\n> On payment failure, the response is the same (you return a copy of the `channel_update` in the onion). You MAY give an additional hint as to what the current\n> acceptable feerate is for a payment.\n>\n> If the feerate card entry is set, the rates there take precedence over the\n> `fee_base_msat` and the `fee_proportional_millionths`.\n>\n> The `feerate` card effectively removes the ability to set a `fee_base_msat`.\n>\n> To minimize payment routing failure, it is recommended to set the\n> `fee_proportional_millionths` to the same rate as the highest `ratecard`\n> rate. (Most likely the `ratecard`.`fee_basis_0_25`).\n>\n> Note that `ratecards` unit is in basis points, not millionths. As basis points\n> are 1/10,000, they're 100x less than the same value expressed as millionths.\n>\n> - A `basis_point` value of 5 is a fee of 5msat on a 10k msat payment.\n> - A `fee_proportional_millionths` of 500 is a fee of 5msat on a 10k msat payment.\n>\n> The current expected price for sats to be pushed to the other side of the\n> channel is to be priced by the lowest ratecard capacity band that that payment\n> will push the balance into.\n>\n> For example, if a payment moves 50k through a 100k channel which is currently\n> at a total available capacity of 75k sats (which means it'll move the capacity\n> from 75k to 25k), that payment will be expected to pay a rate equal to the\n> 0-25% band, as it'll push the available capacity into the 0-25% range.\n>\n> Using this rule, HTLCs consuming significant portions of a channel's\n> total capacity are more likely to pay higher fees.\n>\n> ### Motivation and Rationale\n> At the last in-person spec meeting, it was noted that channel capacity is\n> often of variable value depending on the amount of capacity currently\n> available in that channel.\n>\n> It was also noted that we'd like to allow node operators to experiment\n> with negative fees.\n>\n> You can't easily allow for negative feerates with our current gossip messages,\n> as it would give gossip an economic value which may have an adverse impact\n> on the ability of nodes to stay up to date with the current lightning channel\n> topology; it also stands to greatly increase the volume of gossip\n> updates issued by any one node, which should scale with payment volume,\n> as nodes update their gossip to change their current fees on any one channel.\n>\n> Note that some nodes already automatically update their channel fees based on\n> the available capacity in a channel; fee ratecards should be a more succinct\n> way for node operators to express more fine grained prices of their existing\n> capacity, with a much reduced bandwidth burden.\n>\n> ### Feerate Cards, Frequently Asked Questions\n>\n> #### Why four evenly spaced buckets? Why not X function?\n>\n> 1. Expressibility\n> \tFees are calculated by the node creating the route for a payment, using\n> \tparameters set by the channel node operator. These paramters must be\n> \tcommunicated via gossip to every node, and then used as inputs to the\n> \trouting function.\n>\n> \tParameters need to be uniform and well understood such that\n> \trouting algorithm designers can make efficient use of them.\n> \t\n> \tFour buckets is wide enough such that payment should\n> \tbe able to find/pick a current channel capacity with good\n> \tprobability, yet hopefully expressive enough for routing\n> \toperators to be able to price their available liquidity\n> \twith sufficient granularity.\n>\n> \tFour evenly spaced buckets also make it much easier to\n> \treason about information leakage (binary search).\n>\n> 2. Predictability for payment success.\n> \tFeerate cards don't introduce any new risk to payment failure,\n> \tas the current fee rates of a channel may currently differ\n> \tfrom a node's gossip information.\n>\n> \tRather, feerate cards offer a way for your routing algorithm\n> \tto dynamically decide on what level of capacity it estimates\n> \tthe channel to be at, and its tolerance for failures.\n>\n>\n> #### Won't this leak my channel balance?\n>\n> At most, feerate buckets reduce the total number of payment attempts\n> required to find an exact channel balance by 2.\n>\n> It does, however, provide a much lower 'cost of capital' mechanism\n> for discovering which band of payment is likely to succeed.\n>\n> Currently, to discover if a 100k channel has capacity for a 75k sat payment,\n> you'd need 75k sats of available liquidity in the node leading up to\n> that channel (or 25k in the opposite direction) from your probe node, in\n> order to discover if that bucket is available.\n>\n> This ties up available liquidity along the entire probe route.\n>\n> With feerate cards, you could do a 1k sat payment at each feerate card\n> fee band, and discover the approximate bucket without tying up\n> as much of your own own outband capital.\n>\n> This makes probing more feasible for any node that wants to make a payment,\n> while reducing the capital tied up to discern the same amount of information.\n>\n> #### More data in the channel_update message! Just what we need, more gossip.\n>\n> It's anticipated that feerate cards will greatly reduce the number of\n> `channel_update`s issued by a node, by allowing node operators to dynamically\n> price their liquidity by available capacity via a single, static\n> `channel_update`.\n>\n> This should reduce or remove the need to rebroadcast a `channel_update` when a\n> channel's capacity changes.\n>\n>\n> #### How do negative fees work?\n>\n> Astute readers may notice that the proposed feerate card bands can be expressed\n> as negative numbers. This allows node operators to price the liquidity\n> available in their channel at a discount.\n>\n> Here's a good way to think about the impact a negative fee will\n> have on your channel balances.\n>\n> A feerate of 5 basis points means that, in order to push 10k sats from\n> one of your channel balances to the peer, another of yours must push\n> you 10,005 sats. This means you always need more inbound than outbound\n> in order to route a payment (unless you're using zero fees, in which case\n> you'd need an equal amount).\n>\n> Negative fees reverse this. At -5 basis points, you'd only need to receive 9,995\n> on an inbound channel to cause you to push out 10k sats. This allows you\n> to reallocate your inbound capacity at a slight discount to the party\n> who's responsible for moving the capital.\n>\n> #### What happened to payment base fees?\n>\n> They're going away (they do not work with negative rates).\n> Hopefully ratecards will give you enough expressivity to figure out\n> good rates for your channel capacity without them.\n>\n>\n> ### Credits\n>\n> This proposal is a marriage of Clara Shikleman's push to start thinking\n> about negative fees, and ZmnSCPxj's comments on the variability of\n> value of a channel's liquidity based on current capacity.\n>\n>\n> ~niftynei\n>\n> [^1]: [Slides on liquidity + lightning](https://docs.google.com/presentation/d/1gK5_JvDl6aR8EqEKIxfdIHMR2vNIgHUS5tydP2kB_gA/edit?usp=sharing)\n> [^2]: [Presentation of slides at btcpp in Austin](https://www.youtube.com/watch?v=voEbAeS_B5w)\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n\n\n-- \nhttps://www.rene-pickhardt.de\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220925/0307ca0c/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Fee Ratecards (your gateway to negativity)",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Anthony Towns",
                "lisa neigut",
                "David A. Harding",
                "Ren\u00e9 Pickhardt",
                "ZmnSCPxj"
            ],
            "messages_count": 7,
            "total_messages_chars_count": 36293
        }
    },
    {
        "title": "[Lightning-dev] `htlc_maximum_msat` as a valve for flow control on the Lightning Network",
        "thread_messages": [
            {
                "author": "Ren\u00e9 Pickhardt",
                "date": "2022-09-22T06:40:30",
                "message_text_only": "Good morning fellow Lightning Developers,\n\nI am pleased to share my most recent research results [1] with you. They\nmay (if at all) only have a small impact on protocol development /\nspecification but are actually mainly of concern to node operators and\nLSPs. I still thought they may be relevant for the list.\n\nWhile trying to estimate the expected liquidity distribution in depleted\nchannels due to drain via Markov Models I realized that we can exploit the\n`htlc_maxium_msat` setting to act as a control valve and regulate the\n\"pressure\" coming from the drain and mitigate the depletion of channels.\nSuch ideas are btw not novel at all and heavily used in fluid networks [2].\nThus it seems very natural that we do the same on the Lightning Network.\n\nIn the article we show within a theoretic model how expected payment\nfailure rates per channel may drop significantly by up to an order of\nmagnitude if channels set up proper asymmetric `htlc_maximum_msat` pairs.\n\nWe furthermore provide in our iPython notebook [3] two experimental\nalgorithmic ideas with which node operators can find decent\n`htlc_maximum_msat` values in a greedy fashion. One of the algorithms does\nnot even require to know the drain or payment size distribution or build\nthe Markov model but just looks at the liquidity distribution in the\nchannel at the last x routing attempts and adjusts the `htlc_maximum_msat`\nvalue if the distribution is to far away from a uniform distribution.\n\nLooking forwards for your thoughts and feedback.\n\nwith kind regards Rene\n\n\n[1]:\nhttps://blog.bitmex.com/the-power-of-htlc_maximum_msat-as-a-control-valve-for-better-flow-control-improved-reliability-and-lower-expected-payment-failure-rates-on-the-lightning-network/\n[2]: https://en.wikipedia.org/wiki/Control_valve\n[3]:\nhttps://github.com/lnresearch/Flow-Control-on-Lightning-Network-Channels-with-Drain-via-Control-Valves/blob/main/htlc_maximum_msat%20as%20a%20valve%20for%20flow%20control%20on%20the%20Lightnig%20network.ipynb\n\n-- \nhttps://ln.rene-pickhardt.de\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220922/a895d1a1/attachment.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2022-09-23T08:43:41",
                "message_text_only": "Two questions -\na) How much gossip overhead do you expect this type of protocol to generate/is there a useful \noutcome for this type of update even if you limit gossip updates to once/twice/thrice per day?\nb) What are the privacy implications of the naive \"update on drained channel\", and have you done any \nanalysis of the value of this type of gossip update at different levels of privacy?\n\nThanks,\nMatt\n\nOn 9/22/22 2:40 AM, Ren\u00e9 Pickhardt via Lightning-dev wrote:\n> Good morning fellow Lightning Developers,\n> \n> I am pleased to share my most recent research results [1] with you. They may (if at all) only have a \n> small impact on protocol development / specification but are actually mainly of concern to node \n> operators and LSPs. I still thought they may be relevant for the list.\n> \n> While trying to estimate the expected liquidity distribution in depleted channels due to drain via \n> Markov Models I realized that we can exploit the `htlc_maxium_msat` setting to act as a control \n> valve and regulate the \"pressure\" coming from the drain and mitigate the depletion of channels. Such \n> ideas are btw not novel at all and heavily used in fluid networks [2]. Thus it seems very natural \n> that we do the same on the Lightning Network.\n> \n> In the article we show within a theoretic model how expected payment failure rates per channel may \n> drop significantly by up to an order of magnitude if channels set up proper asymmetric \n> `htlc_maximum_msat` pairs.\n> \n> We furthermore provide in our iPython notebook [3] two experimental algorithmic ideas with which \n> node operators can find decent `htlc_maximum_msat` values in a greedy fashion. One of the algorithms \n> does not even require to know the drain or payment size distribution or build the Markov model but \n> just looks at the liquidity distribution in the channel at the last x routing attempts and adjusts \n> the `htlc_maximum_msat` value if the distribution is to far away from a uniform distribution.\n> \n> Looking forwards for your thoughts and feedback.\n> \n> with kind regards Rene\n> \n> \n> [1]: \n> https://blog.bitmex.com/the-power-of-htlc_maximum_msat-as-a-control-valve-for-better-flow-control-improved-reliability-and-lower-expected-payment-failure-rates-on-the-lightning-network/ <https://blog.bitmex.com/the-power-of-htlc_maximum_msat-as-a-control-valve-for-better-flow-control-improved-reliability-and-lower-expected-payment-failure-rates-on-the-lightning-network/>\n> [2]: https://en.wikipedia.org/wiki/Control_valve <https://en.wikipedia.org/wiki/Control_valve>\n> [3]: \n> https://github.com/lnresearch/Flow-Control-on-Lightning-Network-Channels-with-Drain-via-Control-Valves/blob/main/htlc_maximum_msat%20as%20a%20valve%20for%20flow%20control%20on%20the%20Lightnig%20network.ipynb <https://github.com/lnresearch/Flow-Control-on-Lightning-Network-Channels-with-Drain-via-Control-Valves/blob/main/htlc_maximum_msat%20as%20a%20valve%20for%20flow%20control%20on%20the%20Lightnig%20network.ipynb>\n> \n> -- \n> https://ln.rene-pickhardt.de <https://ln.rene-pickhardt.de>\n> \n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "Ren\u00e9 Pickhardt",
                "date": "2022-09-23T10:27:03",
                "message_text_only": "Dear Matt and Lightning developers,\n\nthose are excellent and important questions that I should probably have\naddressed more explicitly in the article / ml-post! Let me add what I\ncurrently know and begin with your second question as I think I think the\nanswer will be more objective / verifiable while my response to the gossip\nquestion is a bit more speculative at this point:\n\nOn Fri, Sep 23, 2022 at 10:43 AM Matt Corallo <lf-lists at mattcorallo.com>\nwrote:\n\nb) What are the privacy implications of the naive \"update on drained\n> channel\", and have you done any\n> analysis of the value of this type of gossip update at different levels of\n> privacy?\n>\n\nWe know from information theory that the distribution with the highest\nentropy is the uniform distribution [1] which in the context of a channel\nwith capacity of `c` means `log(c+1)` bits as shown in [2].\n\nNow if a given drain leads to a pair of `htlc_maximum_msat` one could in\ndeed also go the other way around and look at the pair of\n`htlc_maximum_msat` and estimate the *past* drain. And of course also the\n(the non uniform) liquidity distribution from the past. Note that now with\nthe better `htlc_maximum_msat` pair the distribution should be closer to\nuniform again which is actually best from an information theoretic point of\nview as this maximizes the entropy in the channel.\n\nI have just created and uploaded a small python script / notebook [3]  from\nwhich we can see for example how much information about the shape of the\npast liquidity distribution we would learn if we could induce from the\n`htlc_maximum_msat` pair that the drain was 0.75. So from the notebook I\nquote:\n\nAssume we learnt a drain of 0.75\n=================================\nEntropy of uniform: 20.00 bits.\nEntropy of the channel with drain: 19.17 bits\nInformation gain: 0.83 bits\n\n\nYes we learnt less than 1 bit of information by knowing the shape of the\ndistribution from the last payments. This much is learnt by a single probe.\nGiven that we expect our payment failure rates to drop by an order of\nmagnitude and the fact that I could probably learn the drain from the on\nchain signal or gossip anyway I personally would consider this information\nleakage to be acceptable. But of course people may not agree with me and\njust not set their `htlc_maximum_msat` flag in the proposed manner. Also it\nis obvious that higher drain values produce a more heavily depleted channel\nand we might learn more bits (in case of a 0.95 drain we learnt about 3\nbits.)\n\na) How much gossip overhead do you expect this type of protocol to\n> generate/is there a useful\n> outcome for this type of update even if you limit gossip updates to\n> once/twice/thrice per day?\n>\n\nAs written in the article I expect the setting of the valves as throtteling\ndevices to be rather stable as I would assume the drain on channels should\nnot be impacted too heavily by installing valves. This assumption is\ncertainly more reasonable if nodes already used min cost flow solvers to\nsend out payments and less reasonable if nodes still use ad-hoc splitting\nheuristics and restrict their path finding on channels with sufficiently\nlarge `htlc_maximum_msat` values for the amount of the partial payment.\n\nIn any case I expect that once the valve is in a good setting for a\nparticular channel / drain the setting can stay as long as the drain won't\nchange significantly. In general I would not expect drain on channels to\nchange heavily over time (though I certainly would not expect it to be\nstatic / stable either). Certainly fee changes in the network (and other\nfactors) may eventually produce changes in drain on channels which would\nalso have to eventually be reflected in new settings for the valve.\n\nThe experimental algorithm idea that I presented in cell 12 of the original\nnotebook (which I also used in the privacy consideration code from above)\ncomputes the histogram of liquidity of the last N routing requests (failed\nand successful ones) that a node saw on the channel and then makes a\ndecision weather to open or close the valve a bit more. Using this greedy\nstrategy we saw in the notebook that a node could find decent values in\nlogarithmic many steps . So while the setup of the valve may trigger a few\ngossip messages (but each one only after a sufficient amount of routing\nrequests have been processed) and while I expect dynamics in drain on the\nchannel I don't expect such adoptions should have to happen in real time or\neven several times per day and put unnecessary load / spam to gossip. But I\nguess we can only see this by operating nodes and probably there are some\nnodes where updates occure more frequently than with other nodes.\n\nThat being said of course if the peers of a channel work together and both\nexchange information / collaborate in finding a good `htlc_maximum_msat`\npair (similalry as our protocol for finding fees in a mutual close) they\nwill probably need less gossip messages than in the case where both nodes\nindependently try to find decent settings.\n\nI am happy if people have more insights into this and challenge my\nexpectation (because it actually really is only an expectation / intuition\nat this point). That being said: Yes even with only a few gossip messages\nper day I expect for the given reasons the setup of valves to be possible\nand useful!\n\nwith kind regards Rene\n\n[1]:\nhttps://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution#Uniform_and_piecewise_uniform_distributions\n[2]: https://arxiv.org/abs/2103.08576\n[3]:\nhttps://github.com/lnresearch/Flow-Control-on-Lightning-Network-Channels-with-Drain-via-Control-Valves/blob/main/Privacy%20Considerations%20of%20signaling%20past%20drain%20via%20%60htlc_maximum_msat%60%20pairs.ipynb\n\n>\n> Thanks,\n> Matt\n>\n> On 9/22/22 2:40 AM, Ren\u00e9 Pickhardt via Lightning-dev wrote:\n> > Good morning fellow Lightning Developers,\n> >\n> > I am pleased to share my most recent research results [1] with you. They\n> may (if at all) only have a\n> > small impact on protocol development / specification but are actually\n> mainly of concern to node\n> > operators and LSPs. I still thought they may be relevant for the list.\n> >\n> > While trying to estimate the expected liquidity distribution in depleted\n> channels due to drain via\n> > Markov Models I realized that we can exploit the `htlc_maxium_msat`\n> setting to act as a control\n> > valve and regulate the \"pressure\" coming from the drain and mitigate the\n> depletion of channels. Such\n> > ideas are btw not novel at all and heavily used in fluid networks [2].\n> Thus it seems very natural\n> > that we do the same on the Lightning Network.\n> >\n> > In the article we show within a theoretic model how expected payment\n> failure rates per channel may\n> > drop significantly by up to an order of magnitude if channels set up\n> proper asymmetric\n> > `htlc_maximum_msat` pairs.\n> >\n> > We furthermore provide in our iPython notebook [3] two experimental\n> algorithmic ideas with which\n> > node operators can find decent `htlc_maximum_msat` values in a greedy\n> fashion. One of the algorithms\n> > does not even require to know the drain or payment size distribution or\n> build the Markov model but\n> > just looks at the liquidity distribution in the channel at the last x\n> routing attempts and adjusts\n> > the `htlc_maximum_msat` value if the distribution is to far away from a\n> uniform distribution.\n> >\n> > Looking forwards for your thoughts and feedback.\n> >\n> > with kind regards Rene\n> >\n> >\n> > [1]:\n> >\n> https://blog.bitmex.com/the-power-of-htlc_maximum_msat-as-a-control-valve-for-better-flow-control-improved-reliability-and-lower-expected-payment-failure-rates-on-the-lightning-network/\n> <\n> https://blog.bitmex.com/the-power-of-htlc_maximum_msat-as-a-control-valve-for-better-flow-control-improved-reliability-and-lower-expected-payment-failure-rates-on-the-lightning-network/\n> >\n> > [2]: https://en.wikipedia.org/wiki/Control_valve <\n> https://en.wikipedia.org/wiki/Control_valve>\n> > [3]:\n> >\n> https://github.com/lnresearch/Flow-Control-on-Lightning-Network-Channels-with-Drain-via-Control-Valves/blob/main/htlc_maximum_msat%20as%20a%20valve%20for%20flow%20control%20on%20the%20Lightnig%20network.ipynb\n> <\n> https://github.com/lnresearch/Flow-Control-on-Lightning-Network-Channels-with-Drain-via-Control-Valves/blob/main/htlc_maximum_msat%20as%20a%20valve%20for%20flow%20control%20on%20the%20Lightnig%20network.ipynb\n> >\n> >\n> > --\n> > https://ln.rene-pickhardt.de <https://ln.rene-pickhardt.de>\n> >\n> > _______________________________________________\n> > Lightning-dev mailing list\n> > Lightning-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n\n\n-- \nhttps://www.rene-pickhardt.de\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220923/b0791f3f/attachment-0001.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-09-24T12:12:03",
                "message_text_only": "On Thu, Sep 22, 2022 at 08:40:30AM +0200, Ren\u00e9 Pickhardt via Lightning-dev wrote:\n> While trying to estimate the expected liquidity distribution in depleted\n> channels due to drain via Markov Models I realized that we can exploit the\n> `htlc_maxium_msat` setting to act as a control valve and regulate the\n> \"pressure\" coming from the drain and mitigate the depletion of channels.\n\nThis is really neat!\n\nI think \"channel drain\" confounds two issues (or, at least, I do when\nI think about it):\n\n 1) one is you're trying to collect as many forwarding fees as you can,\n    and since a drained channel prevents you from forwarding txs, that\n    feels like a hit on profits\n\n 2) the other is that a drained channel *can't* forward a payment even\n    for no profit, so even attempting to forward a payment over a drained\n    channel wastes everyone's time, increases payment latency, and may\n    increase payment failures if you go through too many failures without\n    finding a successful path\n\nThis seems like a great idea for solving (2) -- if you make lightning\nnodes look at htlc_max_msat and throttle their use of a channel based\non its value, then channels can set that value so that their payment\nflow is balanced on average, at which point depletion becomes rare,\nand payments usually succeed.\n\nI think a simple way of thinking about it is: suppose people are\nforwarding X BTC per hour through a channel in one direction, and 2X BTC\nthrough it in the other direction, with all payments being 1000 sats\nexactly. Then if you set htlc_max_msat to 500sats on the overloaded\ndirection, and everyone then triggers their AMP paths and sends half\ntheir payments through a slightly more expensive path, you'll be at\nX-vs-X BTC per hour, with balanced flows and stable channel balances.\n\nOTOH, it is relying on senders doing things that are slightly less optimal\nin the short term (pay higher fees) for things that benefit them only in\nthe long term (avoid payment latency/failures due to depleted channels),\nand only if most people cooperate. Perhaps there's some privacy-preserving\nway that channel operators could throttle payments based on htlc_max_msat\n(and channel depletion percentage?) as well, so that cheaters are less\nlikely to prosper?\n\n\n\nBut as far as (1) goes -- this isn't actually an improvement: instead\nof rejecting X BTC per hour from the overloaded direction because\nyour channel's depleted, you're now not even getting the opportunity\nto forward those payments and collect the corresponding fees. It's no\nworse for your profit margins, but it's not any better. (And it could\nbe worse if you're throttling both sides, and only getting 0.95*X BTC\nper hour in both directions.\n\nBut there aren't many ways you can actually do better with (1).\n\nOne way is if you have a cheap way to rebalance your channels -- in that\ncase, rebalance your channel, let it drain again, collecting fees all the\nwhile, and repeat. If rebalancing is cheaper than the fees you collect,\nthis works great!\n\nThe other way is if fees rates are expected to change -- if they're likely\nto go down later, then you might as well deplete your channel now, since\nyou'll collect more fees for it now than you would later; likewise if you\nexpect fees to up up later, then you might want to retain some balance\nnow, so you can deplete it later. But that's a very dynamic situation,\nand the profits are limited -- you can only drain your channel once while\nwaiting for fee rates to be ready to change, and your profit is going to\nbe capped by your channel capacity times the difference in the fee rates.\n\n\n\nThis approach seems *much* better than the fee rate cards idea:\n\n * you're not decreasing your channel profitability half the time in\n   order to avoid your channel depleting\n\n * you're making routing decisions *less* dependent on internal/private\n   state, rather than more\n\n * you're not adding much gossip/probing traffic -- you might need\n   to refine your htlc_max_msat a few times each time you change fees,\n   but it shouldn't be often, and this should be reducing the frequency\n   you have to change fees anyway\n\n * you're providing a way of throttling payment traffic independent of\n   fees -- since fees are competitive, they can have discontinuous effects\n   where a small change to fee can cause a large change to traffic volume;\n   but this seems like it should mostly have a proportional response,\n   with a small decrease in htlc_max_msat resulting in a small decrease in\n   payment volume, and conversely. Much better for stability/optimisation!\n\nCheers,\naj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-09-26T00:29:24",
                "message_text_only": "Good morning aj, and Rene,\n\n> * you're providing a way of throttling payment traffic independent of\n> fees -- since fees are competitive, they can have discontinuous effects\n> where a small change to fee can cause a large change to traffic volume;\n> but this seems like it should mostly have a proportional response,\n> with a small decrease in htlc_max_msat resulting in a small decrease in\n> payment volume, and conversely. Much better for stability/optimisation!\n\nThis may depend on what gets popular for sender algorithms.\n\nSenders may quantize their payments, i.e. select a \"standard\" value and divide all payments into multipath sub-payments of this value.\n\n* Simplifies the computation of base fee when using a min-cost solver.\n* Simplifies the design of splitting/merging decisions if not using a min-cost solver.\n* Improves privacy once we have PTLCs (if most senders use the same standard value, it is much harder to figure out if two sub-payments, with approximately the same standard quantum, belong to the same payment or not).\n\nIf so, then we expect a large discontinuity for the `htlc_max_msat` vs `htlcs_sent` curve around whatever selected quantum there is.\nIf you set `htlc_max_msat` below this quantum your expected number of payments forwarded will drop to near 0, but a little above that and you might very well saturate since all payments are quantized anyway.\n\nAt least fees gets you basic economics of supply and demand, and is a natural throttle in all markets, including liquidity markets.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-09-26T01:26:57",
                "message_text_only": "Good morning again aj, and Rene,\n\n> Good morning aj, and Rene,\n> \n> > * you're providing a way of throttling payment traffic independent of\n> > fees -- since fees are competitive, they can have discontinuous effects\n> > where a small change to fee can cause a large change to traffic volume;\n> > but this seems like it should mostly have a proportional response,\n> > with a small decrease in htlc_max_msat resulting in a small decrease in\n> > payment volume, and conversely. Much better for stability/optimisation!\n> \n> \n> This may depend on what gets popular for sender algorithms.\n> \n> Senders may quantize their payments, i.e. select a \"standard\" value and divide all payments into multipath sub-payments of this value.\n> \n> * Simplifies the computation of base fee when using a min-cost solver.\n> * Simplifies the design of splitting/merging decisions if not using a min-cost solver.\n> * Improves privacy once we have PTLCs (if most senders use the same standard value, it is much harder to figure out if two sub-payments, with approximately the same standard quantum, belong to the same payment or not).\n> \n> If so, then we expect a large discontinuity for the `htlc_max_msat` vs `htlcs_sent` curve around whatever selected quantum there is.\n> If you set `htlc_max_msat` below this quantum your expected number of payments forwarded will drop to near 0, but a little above that and you might very well saturate since all payments are quantized anyway.\n> \n> At least fees gets you basic economics of supply and demand, and is a natural throttle in all markets, including liquidity markets.\n\nBasically, the intuition \"small decrease in `htlc_max_msat` == small decrease in payment volume\" inherently assumes that HTLC sizes have a flat distribution across all possible sizes.\nThe `htlc_max_msat` vs `payment_volume` curve is basically the integral of the distribution of HTLC sizes.\nBut:\n\n* As above, senders might quantize, and if some standard quantum becomes popular, the distribution is really a spike around the standard quantum, and there is a massive discontinuity there.\n* Coffee or other popular everyday product may settle on a standard price, which again implies a spike around that standard price.\n\nSo the reliability of `htlc_max_msat` as a valve is dependent on market forces, and may be as non-linear as feerates, which *are* the sum total of the market force.\n\nFeerates on the other hand are always going to be something that senders optimize for, because obviously senders will have a maximum amount they will be willing to pay in fees (as before, the intuition here is that the maximum fee senders are willing to pay is equivalent to the difference in subjective value between the millisatoshis they are sending and the service/product they are purchasing).\nWhatever future sender algorithms are devised, feerates will still work consistently as a valve, whreas `htlc_max_msat` may fail in a future where sender algorihms quantize the payments around some standard quantum for privacy and ease-of-implementation purposes.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-09-26T23:26:24",
                "message_text_only": "On Mon, Sep 26, 2022 at 01:26:57AM +0000, ZmnSCPxj via Lightning-dev wrote:\n> > > * you're providing a way of throttling payment traffic independent of\n> > > fees -- since fees are competitive, they can have discontinuous effects\n> > > where a small change to fee can cause a large change to traffic volume;\n> > > but this seems like it should mostly have a proportional response,\n> > > with a small decrease in htlc_max_msat resulting in a small decrease in\n> > > payment volume, and conversely. Much better for stability/optimisation!\n\n> > This may depend on what gets popular for sender algorithms.\n> > Senders may quantize their payments, i.e. select a \"standard\" value and divide all payments into multipath sub-payments of this value.\n\nI don't think that's really the case. \n\nOne option is that you quantize based on the individual payment -- you\nwant to send $100, great, your software splits it into 50x $2 payments,\nand routes them. But that doesn't have an all or nothing effect: if you\nreject anything over $1.99, then instead of routing 1/50th of payments\nup to $100, you're routing 1/50th of payments up to $99.50.\n\nThe other approach is to quantize by some fixed value no matter what the\npayment is (maybe for better privacy?). I don't think that's a good idea\nin the first place -- it trades off maybe a small win for your privacy\nfor using up everyone else's HTLC slots -- but if it is, it'll need to be\nquite a small value so as not to force you to round up the overall payment\ntoo much, and to allow small payments in the first place. But in that case\nmost channels will have their html_max_msat well above that value anyway.\n\n> Basically, the intuition \"small decrease in `htlc_max_msat` == small decrease in payment volume\" inherently assumes that HTLC sizes have a flat distribution across all possible sizes.\n\nThe intuition is really the other way around: if you want a stable,\ndecentralised network, then you need the driving decision on routing to\nbe something other than just \"who's cheaper by 0.0001%\" -- otherwise\neveryone just chooses the same route at all times (which becomes\ncentralised towards the single provider who can best monetise forwarding\nvia something other than fees), and probably that route quickly becomes\nunusable due to being drained (which isn't stable).\n\n(But of course, I hadn't had any ideas on what such a thing could be,\notherwise I'd have suggested something like this earlier!)\n\nSo, to extend the intuition further: that means that if using\nhtlc_max_msat as a valve/throttle can fill that role, then that's a reason\nto not do weird things like force every HTLC to be 2**n msats or similar.\n\nIf there is a conflict, far better to have a lightning network that's\ndecentralised, stable, and doesn't require node operators to spy on\ntransactions to pay for their servers.\n\nIt's not quite as bad as you suggest though -- the payment sizes\ndon't need to have a flat distribution, they only need to have a\nsmooth/continuous distribution.\n\n> * Coffee or other popular everyday product may settle on a standard price, which again implies a spike around that standard price.\n\nImagine the price of coffee is $5, and you find three potential paths \nto pay for that coffee:\n\n  Z -> A -> X\n  Z -> B -> C -> X\n  Z -> B -> D -> X\n\n(I think you choose both the fee and max_msat for Z->A and Z->B hops,\nso we'll assume they're 0%/infinite, respectively)\n\nSuppose the fee on AX is 0.01%, and the total fee for BCX is 0.02%\nand the total fee for BDX is 0.1%.\n\nIf AX's max_msat is $5, they'll get the entire transaction. If it's\n$4.99, you might instead optimise fees by doing AMP: send $4.99 through\nAX and $0.01 through BCX, for a total fee rate of 0.01002%.\n\nIf everyone quantizes at 10c (500sat?) instead of 1c (50sat?) or lower\nthen that just means instead of getting maybe a 0.2% reduction in payment\nflow, AX gets a 2% reduction in payment flow.\n\nLikewise, if AX's max_msat is $1, BCX's max_msat is $3, and BDX's max_msat\nis $20, then you split your payment up as $1/$3/$1 and pay a fee of\n0.034%. Meanwhile AX's payment flow has been reduced by perhaps 80%\n(if everyone's buying $5 coffees), and BCX's by perhaps 25% (from $4 to\n$3), allowing them to maintain balanced channels.\n\n> So the reliability of `htlc_max_msat` as a valve is dependent on market forces, and may be as non-linear as feerates, which *are* the sum total of the market force.\n\nNo: without some sort of external throttle, fees have a tendency to be all\nor nothing. If there's no metric other than fees, why would I ever choose\nto pay 0.02% (let alone 0.1%!) in fees? And if a new path comes along\noffering a fee rate of 0.00999% fees, why would I continue paying 0.01%?\n\nEven if everyone does start quantizing their payments -- and does so with\nan almost 6 order of magnitude jump from 1msat to 500sats -- you're only\nimplying traffic bumps of perhaps 2% when tweaking parameters that are\nnear important thresholds, rather than 100%.\n\n> Feerates on the other hand are always going to be something that senders optimize for, [...]\n\nSure, but that can't be the end of the story, since it has a clear winner\nthat everyone immediately agrees upon, and there's no point anyone else\nparticipating in trying to route lightning transactions.\n\nThere's plenty of other possible throttling mechanisms: at the very least,\nsenders can *not* optimise for fees, and instead randomly select routes\nwithin a budget; or you can maintain external \"reliability\" scores for\nnodes and factor those in along with fee rates.\n\nWhat's novel and interesting about this approach (IMO) is it's one that\nthe channel operator can use directly (so the person who's meant to know\nthe private channel state is making the decision that depends on the\nprivate channel state), and that they don't have a particular incentive\nto lie about (with random choice, maybe you want to pretend you have many\nnodes/channels so you get selected more often; if reliability is factored\nin, maybe you're tempted to pretend to be more reliable than you are).\n\nCheers,\naj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-09-27T00:23:38",
                "message_text_only": "Good morning aj,\n\n> > Basically, the intuition \"small decrease in `htlc_max_msat` == small decrease in payment volume\" inherently assumes that HTLC sizes have a flat distribution across all possible sizes.\n> \n> \n> The intuition is really the other way around: if you want a stable,\n> decentralised network, then you need the driving decision on routing to\n> be something other than just \"who's cheaper by 0.0001%\" -- otherwise\n> everyone just chooses the same route at all times (which becomes\n> centralised towards the single provider who can best monetise forwarding\n> via something other than fees), and probably that route quickly becomes\n> unusable due to being drained (which isn't stable).\n\nAll monetisation is fee-based; the question is who pays the fees.\nCertainly gossiped feerates will work less effectively if fees are paid via another mechanism.\n\nIn particular, discussing with actual forwarding node operators reveals that most of them think that CLBOSS undercuts fees too much searching a short-term profit, quickly depleting its usable liquidity in the long term.\nIn short, they want CLBOSS modified to raise fees and preserve the liquidity supply.\nThis suggests to me that channel saturation due to being cheaper by 0.0001% is not something that will occur often, as most operators will settle to a feerate that maximizes their earnings per unit liquidity they can provide, not trying to undercut everyone.\n\nIn particular, the fact that rebalancing already exists as part of the network protocol means that anyone trying to undercut will find their liquidity being bought out by more patient operators, who are willing to sacrifice short-term profits for long-term consistent earnings.\n\nIn short, the market will fix itself once we have more rational automated actors in place (i.e. not CLBOSS).\nIndeed, price signals are always places where you should pay attention to whether you need more of a good or not.\n\nBut maybe I am just modelling everything incorrectly.\nCertainly the fact that fees can be paid by somebody else other than senders can make gossiped feerates (which are the sender-paid feerates) less effective.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-09-28T02:07:51",
                "message_text_only": "Good morning aj, Rene, and all,\n\nSo let me discuss a little more about how I model the forwarding nodes.\n\nForwarding nodes want to maximize profit.\n\nForwarding nodes sell liquidity.\n\nIf a forwarding node runs out of stock of liquidity (i.e. their channel is unbalanced against the direction a payment request fails) they earn 0 profit.\n\nIf a forwarding node finds a liquidity being sold at a lower price than they would be able to sell it, they will buy out the cheaper stock and then resell it at a higher price.\nThis is called rebalancing.\nIn particular:\n\n* Other economic activity is likely to be \"has to happen at a particular time\".\n  * The intuition here is that people are not going to pay until they become interested in purchasing a product, and only then will they actually send out an HTLC.\n* Rebalances (i.e. hostile takeover bids of cheap liquidity) are \"can happen at any time\".\n* Thus, rebalances are likely to occur \"first\", because they can happen at any time, and can be done right now, whereas payments will need to wait until somebody somewhere is interested in paying.\n* Thus, channels advertising low fees are likely to have their liquidity bought out by patient forwarding nodes.\n\nThe above implies that any \"payment size distribution\" can, and *will*, be manipulated by forwarding nodes out to buy out cheap liquidity.\nIf you introduce an artificial impediment and say \"I will only accept payment sizes below N millisats\", and then go \"I will #zerofeerouting guy\", then a forwarding node will just split their rebalance into quanta of N millisats and make a spike in the payment size distribution and drain your channel anyway, so that they can turn around and resell the liquidity at a higher price later.\n\nThis also suggests to me that fees being paid by out-of-band means (i.e. \"monetizing outside of the Lightning forwarding fees\") is likely to fail, because forwarding nodes will exploit that and do a hostile takeover of the cheap liquidity.\ni.e. #zerofeerouting will never be a reliable forwarding node, because all the other forwarding nodes will be taking their liquidity for cheap long before you think to make a payment through them.\n\nRebalances cannot be differentiated from payments unless you force publication of source and destination (and even if you forced that, people can lie about who the *real* source and *real* destination are, so why bother).\nAnd rebalances are going to target cheap liquidity and will avoid any non-fee valves you impose.\n\nInvisible hand wins, yo.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-09-28T18:58:16",
                "message_text_only": "On Tue, Sep 27, 2022 at 12:23:38AM +0000, ZmnSCPxj via Lightning-dev wrote:\n> All monetisation is fee-based; the question is who pays the fees.\n\nThis isn't true. For example, if you can successfully track the payments\nyou route, you can monetize by selling data about who's buying what\nfrom whom. (Unless you meant it in some trivial sense, I guess, like\n\"all monetisation is money-based; the question is who pays the money\")\n\n> In particular, discussing with actual forwarding node operators reveals\n> that most of them think that CLBOSS undercuts fees too much searching\n> a short-term profit, quickly depleting its usable liquidity in the\n> long term.\n> In short, they want CLBOSS modified to raise fees and preserve the\n> liquidity supply.\n> This suggests to me that channel saturation due to being cheaper by\n> 0.0001% is not something that will occur often,\n\nThat seems a bit of a backwards conclusion: \"undercutting fees depletes\nliquidity\" therefore \"channel saturation due to offering cheaper fees\nseems unlikely\" -- channel saturation *is* depleted liquidity...\n\nOn Wed, Sep 28, 2022 at 02:07:51AM +0000, ZmnSCPxj via Lightning-dev wrote:\n> Forwarding nodes sell liquidity.\n> If a forwarding node runs out of stock of liquidity (i.e. their channel is unbalanced against the direction a payment request fails) they earn 0 profit.\n\nI get what you're saying, but I don't think a \"stock of liquidity\"\nis a helpful metaphor/mental model here.\n\n\"Liquidity\" usually means \"how easy it is to exchange X for Y\" -- assets\nfor cash, etc; but for lightning, liquidity is guaranteed by being\nable to drop to chain. Likewise, \"running out of stock\" isn't usually\nsomething that gets automatically fixed by someone else coming in and\nbuying something different. \n\n(Also, you don't earn 0 profit on an imbalanced channel; you're just\nforced to stop accepting some txs. Every time you forward $1 in the\navailable direction, you become able to forward another $1 back in the\nsaturated direction; and you earn fees on both those $1s)\n\nI think it's better to think in terms of \"payment flow\" -- are you\nforwarding $5/hour in one direction, but $10/hour in the other? Is\nthat an ongoing imbalance, or something that evens itself out over time\n($120/day in both directions)?\n\nOnce you start in that direction, there's also a few other questions\nyou can ask:\n\n * can I get make more revenue by getting more payment flow at a\n   lower fee, or by charging a higher fee over less payment flow?\n\n * if I had a higher capacity channel, would that let me tolerate\n   a temporarily imbalanced flow over a longer period, allowing me\n   to forward more payments and make more fee revenue?\n\nIf you want to have a long running lightning channel, your payment flows\nwill *always* be balanced. That might be through luck, it might be through\nclever management of channel parameters, but if it's not through those,\nit'll be because your channel's saturated, and you're forced to fail\npayments.\n\nUltimately, over the *entire* lifetime of a lightning channel, the only\nimbalance you can have is to either lose the funds that you've put in,\nor gain the funds your channel partner put in.\n\nThat *is* something you could sensibly model as a stock that gets depleted\nover time, if your payment flows are reliably unbalanced in a particular\ndirection. For example, consider a channel that starts off with $100k in\nfunds and has a $5k imbalance every day: after 20 days, you'll have to\nchoose between failing that $5k imbalance (though you could still route\nthe remaining balanced flows), or between rebalancing your channels,\npossibly via on-chain transactions. Does the fee income from an additional\n$100k of imbalanced transactions justify the cost of rebalancing?\n\nYou can calculate that simply enough: if the on-chain/rebalance cost is\n$300, then if you were getting a fee rate of more than 0.3% ($300/$100k),\nthen it's worth paying for the rebalance.\n\nBut if \"lifetime drain\" is the dominant factor, you're reducing\nlightning to the same performance as one-way payment channels: you move\nthe aggregate payments up to the channel capacity, and then close the\nchannel. If you get balanced payment flows, that allows you to cancel\nout 30,000 $1 transactions against 1,000 $30 transactions, and maintain\nthe channel indefinitely, with all the off-chain scaling that implies.\n\n> If a forwarding node finds a liquidity being sold at a lower price than they would be able to sell it, they will buy out the cheaper stock and then resell it at a higher price.\n> This is called rebalancing.\n\nAll that does is move the flow imbalance to someone else's channel;\nit doesn't improve the state of the network.\n\nThere definitely are times when that makes sense:\n\n * maybe the channel's run by \"dumb money\" that will eat the fee for\n   closing on-chain, so you don't have to\n\n * maybe you have secret information about the other channel, allowing\n   you to route through it for cheaper than the general public\n\n * maybe you and they have different demand at different times of the\n   day, so time-shifting imbalances is mutually beneficial? I don't\n   see how this works out without secret information though -- the\n   people who want to route payments should be choosing the cheapest\n   link at all times of the day already\n\nBut those don't generally seem aligned with the long-term health of the\nlightning network?\n\n> * Thus, channels advertising low fees are likely to have their liquidity bought out by patient forwarding nodes.\n\nI think the \"liquidity\" metaphor isn't doing you any favours here.\nHere's what (I think) that scenario looks like under \"flow\":\n\n - overall, there's unbalanced flow: for example, people want to pay a\n   higher amount from A to B than from B to A.\n - X charges low fees to forward from A to B, so their channel is always\n   depleted in that direction.\n - Y charges high fees to forward from A to B\n - Y takes advantage of being constantly online to always be the first\n   to route their rebalance through X (Y->A->X->B->Y) when X's channel\n   clears up\n\nY's rebalancing then is limited by the legitimate payment volume going\nback through X (ie B->X->A). Because there's an unbalanced flow overall,\nthat means Y cannot rebalance to compensate for the total amount of\npayments it routes, and eventually both X and Y will become depleted in\nthe same direction, and one or both channels will have to decide whether\nto rebalance on-chain. If X rebalances on-chain, allowing Y to repeatedly\ntake advantage of their low fees, that's the \"dumb money\" situation.\n\n> If you introduce an artificial impediment and say \"I will only accept\n> payment sizes below N millisats\", and then go \"I will #zerofeerouting\n> guy\", then a forwarding node will just split their rebalance into quanta\n> of N millisats and make a spike in the payment size distribution and\n> drain your channel anyway, so that they can turn around and resell the\n> liquidity at a higher price later.\n\nOf course, it's possible that overall flows *are* balanced, and you're\njust able to take advantage of your better understanding of lightning\nrouting to charge higher fees than this \"X\" character.\n\nBut the same scenario applies in the max_msat world too, with only\nslight modifications.\n\n * instead of constantly probing the A->X->B channel so that you keep it\n   drained, you create fake traffic over A->X->B causing X to detect an\n   imbalance and lower their max_msat in order to get the flows\n   balanced.\n\n * this causes users to send more traffic through A->Y->B, giving you\n   more fee income\n\n * you can then repeatedly generate more fake traffic over A->X->B,\n   causing X to lower max_msat further, perhaps giving you most of the\n   A->B traffic\n\n * meanwhile you do regular rebalances (Y->A->X->B->Y) at values less\n   X's than max_msat\n\n * your node sees perhaps 90% of A->B payment flows, and does the same\n   volume in rebalancing\n\n * X's node sees 10% of legit A->B payment flows, and your 90% of legit\n   payment flows via your rebalancing; and also 100% of legit B->A\n   payment flows\n\n * so both nodes remain balanced, reducing payment failures; and Y can\n   rebalance constantly, allowing them to operate with a smaller\n   capacity channel making it more capital efficient\n\nI think there's a few limitations on that \"attack\":\n\n * it only works if you're willing to forego legitimate fee income\n   from B->Y->A -- but if you're competing with someone who charges \n   zero fees anyway, maybe that's fine\n\n * it only works if A->X->B is charging much lower fees so your\n   rebalancing really is cheap\n\n * you probably need to reserve capital in order to create the fake\n   payment flows -- afterall, if you try to rebalance the capital you\n   used to create the fake payment flow, that creates a payment flow\n   in the other direction, which risks undoing your work\n\n * that A->X->B is overloaded (max_msat is lowish) that's a public\n   indication that there's high demand at X's fee rate along that path,\n   which may encourage people to create additional channels. creating\n   fake payment flow for all of those channels may become prohibitively\n   capital intensive.\n\n> i.e. #zerofeerouting will never be a reliable forwarding node, because all the other forwarding nodes will be taking their liquidity for cheap long before you think to make a payment through them.\n\nI don't think it's ideal if a world that includes both:\n\n * altruists, who'll forward your payments for cheap/free\n * profiteers, who are trying to make a living offering lightning\n   services\n\nresults in \"oops, optimising for low lightning fees becomes horribly\nunreliable\".\n\n[repeating this one, to followup in a different way]\n> If you introduce an artificial impediment and say \"I will only accept\n> payment sizes below N millisats\", and then go \"I will #zerofeerouting\n> guy\", then a forwarding node will just split their rebalance into quanta\n> of N millisats\n\nI mean, that's already a fundamental problem with max_msat: why wouldn't\npayment initiators do exactly that in the first place?\n\nMaking this incentive compatible with AMP payments does seem like a\nchallenge. Why pay higher fees routing through some other channel,\ninstead of just AMPing as many max_msat payments as you need through\nthe cheapest channel? (Hmm, I guess I didn't express that concern as\nclearly as I thought I did; at least outside of any deleted drafts...)\n\nI wonder whether a (per source/dest channel?) token bucket rate limit\nmight suffice, though. Hmm, it maybe would so long as you're optimising\nfor small/frequent payments to be reliable, and aren't worried so much\nabout large/infrequent ones, which *might* be reasonable... That way\nyou start rejecting payment flow *before* your channel's depleted if it\nbecomes unusually bursty in one direction, despite you having indicated\nyou want senders to throttle. And then the early rejections mean there's\nnot so much value AMPing lots of max_msat payments through a single\ncheap channel.\n\nI suppose in a *completely* ideal world, I might imagine something like\nthis:\n\n * $100,000 a day travels from A to B; $90,000 a day travels from B to A\n * all the A->B payments are $5 each\n * onchain fees for rebalancing a channel is $200\n * X and Y run routing nodes and want to make stuff cheap, so charge\n   0.001% (10-parts-per-million)\n * Z is happy to rebalance on chain, has $50,000 committed in his\n   channel, so charges fees of $200/$50,000 = 0.4% (or, probably, more)\n\nIn order to get balanced flows, X and Y set their max_msat in the A to B\ndirection to $2.25, meaning:\n\n * 20k $5 payments from A to B gets split into $2.25 through X, $2.25\n   through Y, and $0.50 through Z\n * 90 $1000 payment from B to A get split into $500 through X and $500\n   through Y.\n\nEvery day:\n\n * X and Y see $45000 going each way, collecting $0.90 in routing fees\n   per day, and having their channel not go out of balance\n * Z sees $10,000 going from A to B, collecting $40 a day\n * Z's channel runs out after 5 days, at which point he's collected $200\n   total, and has to spend $200 rebalancing on-chain\n * each person paying $5 to B pays $0.002045 in fees; each person\n   paying $1000 to A pays $0.01 in fees\n\nZ could reduce his fee rate below 0.4% and still break even if he\nincreased his channel capacity above $50k. He can't make his channel last\nlonger by getting more B->A channel flow though, because that would just\nlead X and Y's flows to be unbalanced, causing payers to have to route\nmore flow through Z.\n\nCheers,\naj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-09-29T00:41:44",
                "message_text_only": "Good morning aj,\n\n> > Forwarding nodes sell liquidity.\n> > If a forwarding node runs out of stock of liquidity (i.e. their channel is unbalanced against the direction a payment request fails) they earn 0 profit.\n> \n> \n> I get what you're saying, but I don't think a \"stock of liquidity\"\n> is a helpful metaphor/mental model here.\n> \n> \"Liquidity\" usually means \"how easy it is to exchange X for Y\" -- assets\n> for cash, etc; but for lightning, liquidity is guaranteed by being\n> able to drop to chain. Likewise, \"running out of stock\" isn't usually\n> something that gets automatically fixed by someone else coming in and\n> buying something different.\n\nSemantics.\nYou got what I am saying anyway.\n\nSo let me invent a completely new term derived from my local `/dev/random`, \"IPpvHg\".\n\nWhen your channel is imbalanced against a particular direction, you cannot forward against the balance.\nIn that case, we say \"you have insufficient stock of IPpvHg\".\n\nA forwarding node is in the business of selling IPpvHg.\n\nIf a forwarding node sets the price of its IPpvHg too low, another forwarding node, one which is more patient, can buy out its stock of IPpvHg to add to its own stock of IPpvHg.\n\nA patient and rich forwarding node can buy out the IPpvHg stock of many cheaper nodes, and that I think is what we are mostly seeing in the network.\n\n> (Also, you don't earn 0 profit on an imbalanced channel; you're just\n> forced to stop accepting some txs. Every time you forward $1 in the\n> available direction, you become able to forward another $1 back in the\n> saturated direction; and you earn fees on both those $1s)\n\nBut that is based on the existence of a stock of IPpvHg in another channel.\n\nActual forwarding node operators classify their peers as \"mostly a source\" and \"mostly a drain\" and \"mostly balanced\", they want CLBOSS to classify peers similarly.\nTheir stock of IPpvHg is getting depleted from \"mostly a drain\" peers, and the stock of IPpvHg they get from \"mostly a source\" peers, which they get in compensation, is less valuable.\n\nWhich is the whole point: there is a price to IPpvHg, and that should be reflected in the feerates your forwarding node should publish.\n\n> I think it's better to think in terms of \"payment flow\" -- are you\n> forwarding $5/hour in one direction, but $10/hour in the other? Is\n> that an ongoing imbalance, or something that evens itself out over time\n> ($120/day in both directions)?\n\nIt is helpful to notice that the channel balance is the integral of the sum of payment flows in both directions.\nThis is why actual forwarding node operators are obsessed with channel balance.\nThey already *are* thinking in terms of payment flow, and using an analytical technique to keep track of it: the channel balance itself.\n\n> \n> Once you start in that direction, there's also a few other questions\n> you can ask:\n> \n> * can I get make more revenue by getting more payment flow at a\n> lower fee, or by charging a higher fee over less payment flow?\n\nAs I pointed out, if you sell your stock of IPpvHg at too low a price point, other forwarding nodes will snatch up the cheap IPpvHg, buying out that stock.\nThey can then form an effective cartel, selling the stock of IPpvHg at a higher price later.\n\n> * if I had a higher capacity channel, would that let me tolerate\n> a temporarily imbalanced flow over a longer period, allowing me\n> to forward more payments and make more fee revenue?\n> \n> If you want to have a long running lightning channel, your payment flows\n> will always be balanced. That might be through luck, it might be through\n> clever management of channel parameters, but if it's not through those,\n> it'll be because your channel's saturated, and you're forced to fail\n> payments.\n> \n> Ultimately, over the entire lifetime of a lightning channel, the only\n> imbalance you can have is to either lose the funds that you've put in,\n> or gain the funds your channel partner put in.\n> \n> That is something you could sensibly model as a stock that gets depleted\n> over time, if your payment flows are reliably unbalanced in a particular\n> direction. For example, consider a channel that starts off with $100k in\n> funds and has a $5k imbalance every day: after 20 days, you'll have to\n> choose between failing that $5k imbalance (though you could still route\n> the remaining balanced flows), or between rebalancing your channels,\n> possibly via on-chain transactions. Does the fee income from an additional\n> $100k of imbalanced transactions justify the cost of rebalancing?\n> \n> You can calculate that simply enough: if the on-chain/rebalance cost is\n> $300, then if you were getting a fee rate of more than 0.3% ($300/$100k),\n> then it's worth paying for the rebalance.\n> \n> But if \"lifetime drain\" is the dominant factor, you're reducing\n> lightning to the same performance as one-way payment channels: you move\n> the aggregate payments up to the channel capacity, and then close the\n> channel. If you get balanced payment flows, that allows you to cancel\n> out 30,000 $1 transactions against 1,000 $30 transactions, and maintain\n> the channel indefinitely, with all the off-chain scaling that implies.\n> \n> > If a forwarding node finds a liquidity being sold at a lower price than they would be able to sell it, they will buy out the cheaper stock and then resell it at a higher price.\n> > This is called rebalancing.\n> \n> \n> All that does is move the flow imbalance to someone else's channel;\n> it doesn't improve the state of the network.\n\nI agree.\n\n\"something you can sensibly model as a stock that gets depleted over time\" is, in fact, the channel balance that forwarding node operators are so obsessed with.\n\nThis all implies to me that the main problem might not be *local* imbalances, but *global* ones.\n\n* There are net receivers who just keep their funds in Lightning.\n\nLong ago I proposed that we add to the base protocol a mandatory global functionality to support onchain-to-offchain swaps.\n\n* A node whose channels have depleted contacts one of the payees it often pays to.\n* The first node constructs an onchain HTLC offer to the second node.\n* The second node routes back a payment over the Lightning Network to the first node.\n* The first node releases the payment preimage over Lightning.\n* The second node claims the onchain HTLC via the payment preimage.\n\nI proposed that as an alternative to splicing.\n\nSplicing involves (with current technology) one transaction (spending a 2-of-2 and creating another P2WSH) but only changes the state of a single local channel.\n\nOnchain-to-offchain swaps require two transactions, but can change the state of many channels.\nThe onchain-to-offchain swap direction also follows \"initiator pays\", since it is the initiator who takes on the risk of performing an onchain action and will pay fees in case the swap protocol does not run to completion.\n\nThe existence of such a protocol would force any net receivers to move their funds from Lightning to onchain, and thus make available any global IPpvHg stocks towards them.\n\n> \n> There definitely are times when that makes sense:\n> \n> * maybe the channel's run by \"dumb money\" that will eat the fee for\n> closing on-chain, so you don't have to\n> \n> * maybe you have secret information about the other channel, allowing\n> you to route through it for cheaper than the general public\n\nChannel balance is private information.\n\nIf you know you have a low balance on your side of a channel, that implies that you have the (private) information that forwards to it are in high demand.\n\nThus you can rationally buy any cheaply-offered IPpvHg towards the same node and resell it later at a higher price.\n\n> * maybe you and they have different demand at different times of the\n> day, so time-shifting imbalances is mutually beneficial? I don't\n> see how this works out without secret information though -- the\n> people who want to route payments should be choosing the cheapest\n> link at all times of the day already\n\nChannel balance *is* the private information you are looking for here: it is the integral of the sum of payment flows.\nKnowing this integral at any time implies knowledge of the recent time-based payment flow.\n\nSure, a remote node through which you rebalance might not share this information to you, but half-knowing is one-fourth of the battle or else either math does not work or I should never have listened to G.I. Joe.\n\n> But those don't generally seem aligned with the long-term health of the\n> lightning network?\n\nIf there is no incentive to run a forwarding node, then the Lightning Network cannot exist, and its long-term health would be moot.\nThis is similar to mining nodes on the base blockchain.\n\n> > * Thus, channels advertising low fees are likely to have their liquidity bought out by patient forwarding nodes.\n> \n> \n> I think the \"liquidity\" metaphor isn't doing you any favours here.\n> Here's what (I think) that scenario looks like under \"flow\":\n> \n> - overall, there's unbalanced flow: for example, people want to pay a\n> higher amount from A to B than from B to A.\n> - X charges low fees to forward from A to B, so their channel is always\n> depleted in that direction.\n> - Y charges high fees to forward from A to B\n> - Y takes advantage of being constantly online to always be the first\n> to route their rebalance through X (Y->A->X->B->Y) when X's channel\n> \n> clears up\n> \n> Y's rebalancing then is limited by the legitimate payment volume going\n> back through X (ie B->X->A). Because there's an unbalanced flow overall,\n> \n> that means Y cannot rebalance to compensate for the total amount of\n> payments it routes, and eventually both X and Y will become depleted in\n> the same direction, and one or both channels will have to decide whether\n> to rebalance on-chain. If X rebalances on-chain, allowing Y to repeatedly\n> take advantage of their low fees, that's the \"dumb money\" situation.\n\nYes, but again: the integral of payment flows is the channel balance, and you can replace every reference to \"flow\" here with \"channel balance\".\n\n> > If you introduce an artificial impediment and say \"I will only accept\n> > payment sizes below N millisats\", and then go \"I will #zerofeerouting\n> > guy\", then a forwarding node will just split their rebalance into quanta\n> > of N millisats and make a spike in the payment size distribution and\n> > drain your channel anyway, so that they can turn around and resell the\n> > liquidity at a higher price later.\n> \n> \n> Of course, it's possible that overall flows are balanced, and you're\n> just able to take advantage of your better understanding of lightning\n> routing to charge higher fees than this \"X\" character.\n> \n> But the same scenario applies in the max_msat world too, with only\n> slight modifications.\n> \n> * instead of constantly probing the A->X->B channel so that you keep it\n> \n> drained, you create fake traffic over A->X->B causing X to detect an\n> \n> imbalance and lower their max_msat in order to get the flows\n> balanced.\n> \n> * this causes users to send more traffic through A->Y->B, giving you\n> \n> more fee income\n> \n> * you can then repeatedly generate more fake traffic over A->X->B,\n> \n> causing X to lower max_msat further, perhaps giving you most of the\n> A->B traffic\n> \n> \n> * meanwhile you do regular rebalances (Y->A->X->B->Y) at values less\n> \n> X's than max_msat\n> \n> * your node sees perhaps 90% of A->B payment flows, and does the same\n> \n> volume in rebalancing\n> \n> * X's node sees 10% of legit A->B payment flows, and your 90% of legit\n> \n> payment flows via your rebalancing; and also 100% of legit B->A\n> \n> payment flows\n> \n> * so both nodes remain balanced, reducing payment failures; and Y can\n> rebalance constantly, allowing them to operate with a smaller\n> capacity channel making it more capital efficient\n> \n> I think there's a few limitations on that \"attack\":\n> \n> * it only works if you're willing to forego legitimate fee income\n> from B->Y->A -- but if you're competing with someone who charges\n> \n> zero fees anyway, maybe that's fine\n> \n> * it only works if A->X->B is charging much lower fees so your\n> \n> rebalancing really is cheap\n> \n> * you probably need to reserve capital in order to create the fake\n> payment flows -- afterall, if you try to rebalance the capital you\n> used to create the fake payment flow, that creates a payment flow\n> in the other direction, which risks undoing your work\n> \n> * that A->X->B is overloaded (max_msat is lowish) that's a public\n> \n> indication that there's high demand at X's fee rate along that path,\n> which may encourage people to create additional channels. creating\n> fake payment flow for all of those channels may become prohibitively\n> capital intensive.\n\nBut the same thing can be done in a world where fees are the valve, and you work directly with supply and demand instead of at one-layer-removed like `htlc_max_msat` does, which may hide more attacks: complexity is where exploits lurk.\n\n> > i.e. #zerofeerouting will never be a reliable forwarding node, because all the other forwarding nodes will be taking their liquidity for cheap long before you think to make a payment through them.\n> \n> \n> I don't think it's ideal if a world that includes both:\n> \n> * altruists, who'll forward your payments for cheap/free\n> * profiteers, who are trying to make a living offering lightning\n> services\n> \n> results in \"oops, optimising for low lightning fees becomes horribly\n> unreliable\".\n\nIn the world outside of Lightning, we usually make the stink eye at any profiteers who exploit altruists and make noises about how such profiteers are evil, and occasionally even punish them a little bit.\n\nUntil you can import such a mechanism into Lightning --- which would, I think, require that forwarding nodes provide their identity, and for payments to have proof-of-identity from both sender and receiver --- then that is the result that will indeed occur.\n\n\n> [repeating this one, to followup in a different way]\n> \n> > If you introduce an artificial impediment and say \"I will only accept\n> > payment sizes below N millisats\", and then go \"I will #zerofeerouting\n> > guy\", then a forwarding node will just split their rebalance into quanta\n> > of N millisats\n> \n> \n> I mean, that's already a fundamental problem with max_msat: why wouldn't\n> payment initiators do exactly that in the first place?\n\nIndeed, which suggests that the use of `htlc_max_msat` for a flow valve is flawed: it requires continuous payment flow / payment size curve, and the payment size can be manipulated trivially by any third parties.\n\nWhat would limit this would be a decently high and definitely non-zero `base_fee`, but that is a fee-based valve already, which is what I have been pointing out is the simply best option we have.\n\n\n> \n> Making this incentive compatible with AMP payments does seem like a\n> challenge. Why pay higher fees routing through some other channel,\n> instead of just AMPing as many max_msat payments as you need through\n> the cheapest channel? (Hmm, I guess I didn't express that concern as\n> clearly as I thought I did; at least outside of any deleted drafts...)\n> \n> I wonder whether a (per source/dest channel?) token bucket rate limit\n> might suffice, though. Hmm, it maybe would so long as you're optimising\n> for small/frequent payments to be reliable, and aren't worried so much\n> about large/infrequent ones, which might be reasonable... That way\n> you start rejecting payment flow before your channel's depleted if it\n> becomes unusually bursty in one direction, despite you having indicated\n> you want senders to throttle. And then the early rejections mean there's\n> not so much value AMPing lots of max_msat payments through a single\n> cheap channel.\n\nGiven that a published channel is a global resource, any rate limit is going to be shared amongst many users, and if you underquote the value of the IPpvHg you are providing, rebalancers are going to grab as much of the rate limit as they can.\n\n> I suppose in a completely ideal world, I might imagine something like\n> this:\n> \n> * $100,000 a day travels from A to B; $90,000 a day travels from B to A\n> * all the A->B payments are $5 each\n> \n> * onchain fees for rebalancing a channel is $200\n> * X and Y run routing nodes and want to make stuff cheap, so charge\n> 0.001% (10-parts-per-million)\n> * Z is happy to rebalance on chain, has $50,000 committed in his\n> channel, so charges fees of $200/$50,000 = 0.4% (or, probably, more)\n> \n> In order to get balanced flows, X and Y set their max_msat in the A to B\n> direction to $2.25, meaning:\n> \n> * 20k $5 payments from A to B gets split into $2.25 through X, $2.25\n> through Y, and $0.50 through Z\n> * 90 $1000 payment from B to A get split into $500 through X and $500\n> through Y.\n> \n> Every day:\n> \n> * X and Y see $45000 going each way, collecting $0.90 in routing fees\n> per day, and having their channel not go out of balance\n> * Z sees $10,000 going from A to B, collecting $40 a day\n> * Z's channel runs out after 5 days, at which point he's collected $200\n> total, and has to spend $200 rebalancing on-chain\n> * each person paying $5 to B pays $0.002045 in fees; each person\n> paying $1000 to A pays $0.01 in fees\n> \n> Z could reduce his fee rate below 0.4% and still break even if he\n> increased his channel capacity above $50k. He can't make his channel last\n> longer by getting more B->A channel flow though, because that would just\n> \n> lead X and Y's flows to be unbalanced, causing payers to have to route\n> more flow through Z.\n\nI think this implies we need to have a mechanism to move funds outside of the Lightning network, i.e. every published node should have onchain/offchain swap capability.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-09-29T03:45:39",
                "message_text_only": "On Thu, Sep 29, 2022 at 12:41:44AM +0000, ZmnSCPxj wrote:\n> > I get what you're saying, but I don't think a \"stock of liquidity\"\n> > is a helpful metaphor/mental model here.\n> > \"Liquidity\" usually means \"how easy it is to exchange X for Y\" -- assets\n> > for cash, etc; but for lightning, liquidity is guaranteed by being\n> > able to drop to chain. Likewise, \"running out of stock\" isn't usually\n> > something that gets automatically fixed by someone else coming in and\n> > buying something different.\n> Semantics.\n> You got what I am saying anyway.\n\nSemantics are important. If you choose the wrong analogies, you'll jump\nto the wrong conclusions, which I think you're doing here.\n\n> So let me invent a completely new term derived from my local `/dev/random`, \"IPpvHg\".\n\nIf you're going to make up words, at least make them pronouncable...\napt-get install pwgen; pwgen -0A maybe. But there's no need to make\nup words; these aren't completely novel concepts, and existing terms\ndescribe the concepts pretty well.\n\n> A patient and rich forwarding node can buy out the IPpvHg stock of many cheaper nodes,\n\nI just spent a lot of words explaining why I disagree with that claim.\nRestating it doesn't really seem constructive.\n\n> and that I think is what we are mostly seeing in the network.\n\nI don't really agree. I think we're seeing a combination of unbalanced\noverall flows due to an insufficiently circular economy (which would\nperhaps be eased by more custodial wallets/exchanges supporting lightning)\nand the combination of a lack of any way to limit channel flow other\nthan raising fees and an inability to dynamically change fees on a\nminute-by-minute timescale.\n\n> > (Also, you don't earn 0 profit on an imbalanced channel; you're just\n> > forced to stop accepting some txs. Every time you forward $1 in the\n> > available direction, you become able to forward another $1 back in the\n> > saturated direction; and you earn fees on both those $1s)\n> But that is based on the existence of a stock of IPpvHg in another channel.\n\nNo, it's not. It applies even if there is only one channel in the\nentire network (though I guess that channel would have to be between two\ncustodial entities, or there wouldn't be any point charging fees in the\nfirst place).\n\n> Actual forwarding node operators classify their peers as \"mostly a source\" and \"mostly a drain\" and \"mostly balanced\", they want CLBOSS to classify peers similarly.\n\n\"mostly a source\" should trigger rate limiting in one direction, \"mostly\na drain\" should trigger rate limiting in the other. Both should only be\ntrue briefly, until the rate limiting kicks in and the channel becomes\n\"mostly balanced\".\n\nThat's still the case even if the rate limiting is \"oops, one side of\nthe channel has ~0 balance\".\n\n> > I think it's better to think in terms of \"payment flow\" -- are you\n> > forwarding $5/hour in one direction, but $10/hour in the other? Is\n> > that an ongoing imbalance, or something that evens itself out over time\n> > ($120/day in both directions)?\n> It is helpful to notice that the channel balance is the integral of the sum of payment flows in both directions.\n\nThe channel balance is the sum of the initial balance and all payments,\nsure. No need to add an integral in there as well. For a successful,\nlong lasting channel, sum(incoming payments) and sum(outgoing payments)\nwill be much greater than the balance, to the point where the balance\nis just a rounding error by comparison.\n\n> This is why actual forwarding node operators are obsessed with channel balance.\n> They already *are* thinking in terms of payment flow, and using an analytical technique to keep track of it: the channel balance itself.\n\nThis is exactly backwards: you don't monitor your profits by looking at\nthe rounding errors, you monitor your profits by looking at your sales.\n\nIf you've forwarded $100,000 in one direction, and $100,200 in the other\ndirection, you care about the $200,200 total that you were charging fees\non, not the $200 net delta that it's made to your channel balance.\n\n> > Once you start in that direction, there's also a few other questions\n> > you can ask:\n> > \n> > * can I get make more revenue by getting more payment flow at a\n> > lower fee, or by charging a higher fee over less payment flow?\n> \n> As I pointed out, if you sell your stock of IPpvHg at too low a price point, other forwarding nodes will snatch up the cheap IPpvHg, buying out that stock.\n> They can then form an effective cartel, selling the stock of IPpvHg at a higher price later.\n\nNo; changing your fee rate isn't about messing with other people's\nchannels, it's about encouraging more use of lightning overall.  For\nexample, if you're charging a base_fee of 1sat per HTLC, then dropping\nthat to 0sat might reduce fees from existing traffic, but maybe it will\nallow you to forward so many micropayments or AMP payments that it's\nwortwhile anyway.\n\nThe lightning network is tiny; if you're constantly thinking about how\nto steal what little profits there are from other participants, instead\nof how to grow the ecosystem, you're going to have a really bad time.\n\n(Is AMP Atomic MultiPath, or Atomic Multipath Payment? I prefer the\nformer, and that's why I don't think I'm saying ATM machine)\n\n> This all implies to me that the main problem might not be *local* imbalances, but *global* ones.\n\nI mean, sure, that seems totally plausible. This thread is about solving\nrouting problems that will occur even when there *isn't* a global\nimbalance of payment flow, though...\n\n> * A node whose channels have depleted contacts one of the payees it often pays to.\n> * The first node constructs an onchain HTLC offer to the second node.\n> * The second node routes back a payment over the Lightning Network to the first node.\n> * The first node releases the payment preimage over Lightning.\n> * The second node claims the onchain HTLC via the payment preimage.\n> I proposed that as an alternative to splicing.\n\nIsn't this the same as Lightning Labs' Loop idea? It's a fine idea,\nbut it's not really an alternative to splicing.\n\n> Channel balance is private information.\n\nI mean, it would be nice if it was, but it's not, and the fee rate card\nproposal creates a strong incentive for everyone to try to discover\nthat information?\n\n> > * maybe you and they have different demand at different times of the\n> > day, so time-shifting imbalances is mutually beneficial? I don't\n> > see how this works out without secret information though -- the\n> > people who want to route payments should be choosing the cheapest\n> > link at all times of the day already\n> Channel balance *is* the private information you are looking for here: it is the integral of the sum of payment flows.\n\nYou have precise knowledge of the payment flow for your own channels\nalready: you run the channel and saw all the payments going each way,\nincluding the ones that failed. You don't need a proxy.\n\nWhen I say \"secret information\" I mean that you, as someone who's going\nto rebalance, needs to have information about the policies of the other\nchannel (the one you're going to rebalance over) that the rest of the\nnetwork isn't privy too.\n\n> Sure, a remote node through which you rebalance might not share this information to you,\n\nIf the rebalance is mutually beneficial (as assumed), then they'll be\nhappy to share the information with you. But in that case, they'll\nprobably make the information fully public, at which point people will\njust route payments through them instead of you anyway.\n\n> Yes, but again: the integral of payment flows is the channel balance, and you can replace every reference to \"flow\" here with \"channel balance\".\n\nSure. Work out your profits based on the $200 delta, rather than the\n$200k of flows...\n\n> > But the same scenario applies in the max_msat world too, with only\n> > slight modifications. [...]\n> But the same thing can be done in a world where fees are the valve,\n\nI mean, I was duplicating a scenario that you were deriving from a world\nwhere people only use fees, so, obviously?\n\nIf you use the other scenario -- with $5 payments, two cheap channels\nthat insist on balanced flows, one expensive one that doesn't, and an\noverall unbalanced flow -- then that doesn't work well if you only have\nfees. In that scenario:\n\n * people see two channels with 0.001% fees, pick one at random and\n   route their entire $5 transaction through it\n * if they're lucky, and a $1000 payment has recently gone the other\n   way, they succed and pay $0.00005 in tx fees\n * if they're not (10% of the time), then both the cheap channels will\n   be drained, and they'll route their $5 payment through the expensive\n   channel, suffering both latency while they find a workable path, and\n   paying much higher fees ($0.02)\n\nie, some people are paying much higher fees (400x higher than the 90%\nof lucky people, or ~10x higher than everyone in the max_msat scenario).\n\n> > I don't think it's ideal if a world that includes both:\n> > * altruists, who'll forward your payments for cheap/free\n> > * profiteers, who are trying to make a living offering lightning\n> > services\n> In the world outside of Lightning, we usually make the stink eye at any profiteers who exploit altruists and make noises about how such profiteers are evil, and occasionally even punish them a little bit.\n\nMmm. You (and CLBOSS) are the profiteer exploiting altruists with all\nyour aggressive rebalancing ideas in this scenario. Be careful what you\nwish for maybe?\n\nIn a normally functioning economy, you just find the altruists come up\nwith some way other than prices to limit who they \"sell\" too -- whether\nthat be first come first served, or charities only giving alms to the\nobviously needy, or clubs that only provide benefits to members, etc.\n\nYou could think of max_msat throttling in exactly that role. But, if it\ncan be made to work, it's more broadly useful than that, since what it's\nfundamentally about is avoiding the \"whoops, this channel's depleted,\nbetter retry routing via some other path\" cases which applies even in\na world without CLBOSS rebalancing (ie, rationing via the \"first come\nfirst served\" principle).\n\n> > I mean, that's already a fundamental problem with max_msat: why wouldn't\n> > payment initiators do exactly that in the first place?\n> Indeed, which suggests that the use of `htlc_max_msat` for a flow valve is flawed:\n\nIt was first proposed barely a week ago, of course it's flawed. The\nquestion is what the flaws are, and whether they're fundamental or\nthings that can be worked around.\n\n> it requires continuous payment flow / payment size curve, and the payment size can be manipulated trivially by any third parties.\n\nThe blog post was based on a markov model, which has discrete payment\nsteps by its nature, so I don't know what you're talking about with a\n\"continuous\" payment flow curve. Both \"all payments are at the max size\n(simplified as 1 unit)\" and \"payments are uniform between (0, max)\" were\nmodelled, the former seeming a reasonable match to the \"manipulation\"\nyou're proposing, at least for a first brush on the topic.\n\n> > I wonder whether a (per source/dest channel?) token bucket rate limit\n> > might suffice, though. [...]\n> Given that a published channel is a global resource, any rate limit is going to be shared amongst many users, and if you underquote the value of the IPpvHg you are providing, rebalancers are going to grab as much of the rate limit as they can.\n\nThe goal here isn't to stop rebalancing, it's to achieve balanced payment\nflow by reducing payment attempts, so that fewer payment attempts\nfail. (Rather than achieving balanced payment flows by having many\npayment attempts fail)\n\n> I think this implies we need to have a mechanism to move funds outside of the Lightning network, i.e. every published node should have onchain/offchain swap capability.\n\nI guess it would be appealing to a profiteer to have a way of forcing\nnodes operating competing channels to taint their wallets by sending to\nOFAC banned addresses... Sure seems like the sort of feature that should\nbe opt-in only to me though.\n\nCheers,\naj"
            }
        ],
        "thread_summary": {
            "title": "`htlc_maximum_msat` as a valve for flow control on the Lightning Network",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Anthony Towns",
                "Ren\u00e9 Pickhardt",
                "Matt Corallo",
                "ZmnSCPxj"
            ],
            "messages_count": 12,
            "total_messages_chars_count": 76424
        }
    },
    {
        "title": "[Lightning-dev] CLN Release: v0.12.1 - Web 8 init (dot one)",
        "thread_messages": [
            {
                "author": "lisa neigut",
                "date": "2022-09-23T18:45:54",
                "message_text_only": "Hi all,\n\nWe'd like to announce the 0.12.1 release[1] of core-lightning, named by\n@adi2011.\n\nThis is a point release with a few bug fixes and build improvements: it's a\nrecommended upgrade from 0.12.0, with no new features.\n\nFor a list of differences, please see [v0.12.0..v0.12.1](\nhttps://github.com/ElementsProject/lightning/compare/v0.12.0...v0.12.1).\n\n## Highlights for Users\n\n- Now with fewer bugs!\n- Topology plugin could crash when it sees duplicate private channel\nannouncements.\n- `getinfo` no longer displays unannounced addresses; shows correct port\nfor discovered IP addresses.\n- Newly created channels sometimes were inexplicably disconnected, this has\nbeen fixed!\n\n## Highlights for the Network\n\n- Bad gossip store reads and gossip propagation fixed.\n\n## Highlights for Developers\n\n- Reliance on `mako` and `mrkd` python libraries has been removed.\n\nSince v0.12.0 we've had 26 commits from 5 different contributors over 29\ndays.\n\n~ @niftynei, Christian, and Rusty\n\n[1] https://github.com/ElementsProject/lightning/releases/tag/v0.12.1\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220923/6eb274b4/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "CLN Release: v0.12.1 - Web 8 init (dot one)",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "lisa neigut"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1233
        }
    },
    {
        "title": "[Lightning-dev] Splice Pinning Prevention w/o Anchors",
        "thread_messages": [
            {
                "author": "Antoine Riard",
                "date": "2022-09-26T19:50:57",
                "message_text_only": "Hi Dustin,\n\n>From my understanding, splice pinning is problematic for channel funds\nsafety. In the sense once you have a splice floating in network mempools\nand your latest valid commitment transaction pre-signed fees isn't enough\nto replace the splice, lack of confirmation might damage the claim of HTLCs.\n\nI don't know if the current splice proposal discourages pending HTLCs\nduring the splice lifetime, this would at least downgrade the pinning\nseverity in the splicing case to a simple liquidity timevalue loss.\n\nW.r.t, about the mitigation proposed.\n\n> For \u201cancestor bulking\u201d, every `tx_add_input` proposed by a peer must be\n> included in the UTXO set. A node MUST verify the presence of a proposed\n> input before adding it to the splicing transaction.\n\nI think this mitigation requires reliable access to the UTXO set, a\nsignificant constraint for LN mobile clients relying on lightweight\nvalidation backends. While this requirement already exists in matters of\nrouting to authenticate channel announcements, on the LDK-side we have\nalternative infrastructure to offer source-based routing to such a class of\nclients, without them having to care about the UTXO set [0]. I don't\nexclude there would be infrastructure in the future to access a subset of\nthe UTXO set (e.g if utreexo is deployed on the p2p network) for\nresource-constraint clients, however as of today this is still pure\nspeculation and vaporware.\n\nIn the meantime, mobile clients might not be able to partake in splicing\noperations with their LSPs, or without a decrease in trust-minimization\n(e.g assuming your LSP doesn't initiate malicious pinnings against you).\n\n> 1) You cannot CPFP a splice transaction. All splices must be RBF\u2019d to be\n> fee-bumped. The interactive tx protocol already provides a protocol for\n> initiating an RBF, which we re-use for splicing.\n\nThe issue with RBF, it assumes interactivity with your counterparties. As\nsplicing is built on top of the interactive transaction construction\nprotocol, from my understanding you could have a high order of participants\nto coordinate with, without knowledge of their signing policies (e.g if\nthey're time-constraints) therefore any re-signing operations might have\nodds to fail. Moreover, one of these participants could be malicious and\nrefuses straightly to sign, therefore the  already splicing transactions\nstay as a pin in the network mempools.\n\nIf this concern is correct, I'm not sure we have a current good solution,\nthe WIP package RBF proposal would be limited to only 2 descendants [1],\nand here we might have 3 generations: the splice, a commitment, a CPFP.\n\n[0] https://github.com/lightningdevkit/rapid-gossip-sync-server\n[1]\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-September/020937.html\n\nLe mar. 9 ao\u00fbt 2022 \u00e0 16:15, Dustin Dettmer <dustin at koinkeep.com> a \u00e9crit :\n\n> As raised by @crypto-iq and @roasbeef, splices which permit arbitrary\n> script and input inclusion are at risk of being mempool pinned. Here we\n> present a solution to this splice pinning problem.\n>\n>\n> ## Background\n>\n> Pinning can be done by building a very large \u201cjunk\u201d transaction that\n> spends from an important pending one. There are two known pinning vectors:\n> ancestor bulking thru addition of new inputs and junk pinning via the\n> spending of outputs.\n>\n>\n> Pinning pushes transactions to the bottom of the priority list without a\n> practical way of bumping it up. It is in effect a griefing attack, but in\n> the case of lightning can risk funds loss for HTLCs that have timed out for\n> a pinned commitment transaction.\n>\n>\n> Anchor outputs were introduced to lightning to mitigate the junk pinning\n> vector; they work by adding a minimum of  `1 CSV` lock to all outputs on\n> the commitment transaction except for two \u201canchor\u201d outputs, one for each\n> channel peer. (These take advantage of a 1-tx carve-out exception to enable\n> propagation of anchors despite any junk attached to the peer\u2019s anchor).\n>\n>\n> ## Mitigation\n>\n> Splice transactions are susceptible to both junk and bulk pinning attacks.\n> Here\u2019s how we propose mitigating these for splice.\n>\n>\n> [https://i.imgur.com/ayiO1Qt.png]\n>\n>\n> For \u201cancestor bulking\u201d, every `tx_add_input` proposed by a peer must be\n> included in the UTXO set. A node MUST verify the presence of a proposed\n> input before adding it to the splicing transaction.\n>\n>\n> For \u201coutput junk\u201d, every output included directly in a splice transaction\n> MUST be a v0 P2SH witness script which begins with a minimum of `1 CSV`\n> relative timelock. No output on the splice transaction will be spendable\n> until it is included in a block. This prevents junk pinning by removing the\n> ability to propose spends of splice outputs before the transaction is\n> included in a block.\n>\n>\n> There are two side effects here.\n>\n>\n> 1) You cannot CPFP a splice transaction. All splices must be RBF\u2019d to be\n> fee-bumped. The interactive tx protocol already provides a protocol for\n> initiating an RBF, which we re-use for splicing.\n>\n> 2) Arbitrary 3rd party scriptPubKeys are not permissible directly into the\n> splice tx.\n>\n>\n> In order for this to work we need to validate that every output has a 1\n> block CSV. There are two output types to consider:\n>\n>    1. New channel outpoints\n>    2. Arbitrary splice out funds\n>\n>\n> For arbitrary splice out, funds can be included in a \u201cfan-out\u201d\n> transaction. Here standard pay to address etc outputs can live. The output\n> leading to the fan-out transaction will be a P2WSH that also begins with\n> [OP_1, OP_CHECKSEQUENCEVERIFY] (referred to from here on as \u20181 CSV\u2019). Each\n> splice party SHOULD build a fan-out transaction for all arbitrary spliced\n> outputs.\n>\n>\n> [https://i.imgur.com/40Dy3oq.png]\n>\n>\n> Splice-in transactions will not require any fan-out children as long as\n> all change goes into the channel outpoint.\n>\n>\n> For new channel outpoints, the v0 witness script should be modified to\n> start with [OP_1, OP_CHECKSEQUENCEVERIFY]. It needs to be the first item in\n> the script to allow easy validation that it is used and not hidden in a\n> false conditional. This would need to be applied to post-splice channel\n> outpoints and probably dual funding channels should add it as well so they\n> can be successfully included in splices.\n>\n>\n> ### interactive tx protocol changes\n>\n> For splices, `tx_add_output` MUST include the `witness_script` in the tlv.\n> Upon receiving outputs, nodes must validate the script matches the script\n> hash in the output and that it begins with a minimum of 1 CSV.\n>\n>\n> ## HTLC Timeouts and Splices\n>\n> Typically when this technique is used, one or two anchor outputs are added\n> to purposely allow for CPFP fee bumping. But, turns out, we already have a\n> usable anchor in the original commitment transaction! Very exciting.\n>\n>\n> The interactive tx protocol mandates that splice txs are RBF-enabled.\n> Broadcast splice proposals can be replaced out for the original commitment\n> transaction at any time. Since the original commitment transaction has\n> existing anchors, these may be used to increase fees on a force close. This\n> combined with every other output in the tree being locked behind a 1 CSV\n> means the force close will always have top mempool priority, mitigating the\n> \u201coutput junk\u201d style pin.\n>\n>\n> - Nifty & Dusty\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220926/cd8dcdf3/attachment.html>"
            },
            {
                "author": "Greg Sanders",
                "date": "2022-09-26T20:46:37",
                "message_text_only": "> I think this mitigation requires reliable access to the UTXO set\n\nIn this case, how about just setting nsequence to the value 1? UTXO may not\nexist, but maybe that's ok since it means it cannot pin the commitment tx.\n\n> If this concern is correct, I'm not sure we have a current good solution,\nthe WIP package RBF proposal would be limited to only 2 descendants [1],\nand here we might have 3 generations: the splice, a commitment, a CPFP.\n\nI maybe misunderstood the point, but if we're assuming some future V3\ntransaction update, you could certainly add anchors to the splice and CPFP\nit from there. I think the effort was to attempt to avoid waiting for such\nan update.\n\nBest,\nGreg\n\nOn Mon, Sep 26, 2022 at 3:51 PM Antoine Riard <antoine.riard at gmail.com>\nwrote:\n\n> Hi Dustin,\n>\n> From my understanding, splice pinning is problematic for channel funds\n> safety. In the sense once you have a splice floating in network mempools\n> and your latest valid commitment transaction pre-signed fees isn't enough\n> to replace the splice, lack of confirmation might damage the claim of HTLCs.\n>\n> I don't know if the current splice proposal discourages pending HTLCs\n> during the splice lifetime, this would at least downgrade the pinning\n> severity in the splicing case to a simple liquidity timevalue loss.\n>\n> W.r.t, about the mitigation proposed.\n>\n> > For \u201cancestor bulking\u201d, every `tx_add_input` proposed by a peer must be\n> > included in the UTXO set. A node MUST verify the presence of a proposed\n> > input before adding it to the splicing transaction.\n>\n> I think this mitigation requires reliable access to the UTXO set, a\n> significant constraint for LN mobile clients relying on lightweight\n> validation backends. While this requirement already exists in matters of\n> routing to authenticate channel announcements, on the LDK-side we have\n> alternative infrastructure to offer source-based routing to such a class of\n> clients, without them having to care about the UTXO set [0]. I don't\n> exclude there would be infrastructure in the future to access a subset of\n> the UTXO set (e.g if utreexo is deployed on the p2p network) for\n> resource-constraint clients, however as of today this is still pure\n> speculation and vaporware.\n>\n> In the meantime, mobile clients might not be able to partake in splicing\n> operations with their LSPs, or without a decrease in trust-minimization\n> (e.g assuming your LSP doesn't initiate malicious pinnings against you).\n>\n> > 1) You cannot CPFP a splice transaction. All splices must be RBF\u2019d to be\n> > fee-bumped. The interactive tx protocol already provides a protocol for\n> > initiating an RBF, which we re-use for splicing.\n>\n> The issue with RBF, it assumes interactivity with your counterparties. As\n> splicing is built on top of the interactive transaction construction\n> protocol, from my understanding you could have a high order of participants\n> to coordinate with, without knowledge of their signing policies (e.g if\n> they're time-constraints) therefore any re-signing operations might have\n> odds to fail. Moreover, one of these participants could be malicious and\n> refuses straightly to sign, therefore the  already splicing transactions\n> stay as a pin in the network mempools.\n>\n> If this concern is correct, I'm not sure we have a current good solution,\n> the WIP package RBF proposal would be limited to only 2 descendants [1],\n> and here we might have 3 generations: the splice, a commitment, a CPFP.\n>\n> [0] https://github.com/lightningdevkit/rapid-gossip-sync-server\n> [1]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-September/020937.html\n>\n> Le mar. 9 ao\u00fbt 2022 \u00e0 16:15, Dustin Dettmer <dustin at koinkeep.com> a\n> \u00e9crit :\n>\n>> As raised by @crypto-iq and @roasbeef, splices which permit arbitrary\n>> script and input inclusion are at risk of being mempool pinned. Here we\n>> present a solution to this splice pinning problem.\n>>\n>>\n>> ## Background\n>>\n>> Pinning can be done by building a very large \u201cjunk\u201d transaction that\n>> spends from an important pending one. There are two known pinning vectors:\n>> ancestor bulking thru addition of new inputs and junk pinning via the\n>> spending of outputs.\n>>\n>>\n>> Pinning pushes transactions to the bottom of the priority list without a\n>> practical way of bumping it up. It is in effect a griefing attack, but in\n>> the case of lightning can risk funds loss for HTLCs that have timed out for\n>> a pinned commitment transaction.\n>>\n>>\n>> Anchor outputs were introduced to lightning to mitigate the junk pinning\n>> vector; they work by adding a minimum of  `1 CSV` lock to all outputs on\n>> the commitment transaction except for two \u201canchor\u201d outputs, one for each\n>> channel peer. (These take advantage of a 1-tx carve-out exception to enable\n>> propagation of anchors despite any junk attached to the peer\u2019s anchor).\n>>\n>>\n>> ## Mitigation\n>>\n>> Splice transactions are susceptible to both junk and bulk pinning\n>> attacks. Here\u2019s how we propose mitigating these for splice.\n>>\n>>\n>> [https://i.imgur.com/ayiO1Qt.png]\n>>\n>>\n>> For \u201cancestor bulking\u201d, every `tx_add_input` proposed by a peer must be\n>> included in the UTXO set. A node MUST verify the presence of a proposed\n>> input before adding it to the splicing transaction.\n>>\n>>\n>> For \u201coutput junk\u201d, every output included directly in a splice transaction\n>> MUST be a v0 P2SH witness script which begins with a minimum of `1 CSV`\n>> relative timelock. No output on the splice transaction will be spendable\n>> until it is included in a block. This prevents junk pinning by removing the\n>> ability to propose spends of splice outputs before the transaction is\n>> included in a block.\n>>\n>>\n>> There are two side effects here.\n>>\n>>\n>> 1) You cannot CPFP a splice transaction. All splices must be RBF\u2019d to be\n>> fee-bumped. The interactive tx protocol already provides a protocol for\n>> initiating an RBF, which we re-use for splicing.\n>>\n>> 2) Arbitrary 3rd party scriptPubKeys are not permissible directly into\n>> the splice tx.\n>>\n>>\n>> In order for this to work we need to validate that every output has a 1\n>> block CSV. There are two output types to consider:\n>>\n>>    1. New channel outpoints\n>>    2. Arbitrary splice out funds\n>>\n>>\n>> For arbitrary splice out, funds can be included in a \u201cfan-out\u201d\n>> transaction. Here standard pay to address etc outputs can live. The output\n>> leading to the fan-out transaction will be a P2WSH that also begins with\n>> [OP_1, OP_CHECKSEQUENCEVERIFY] (referred to from here on as \u20181 CSV\u2019). Each\n>> splice party SHOULD build a fan-out transaction for all arbitrary spliced\n>> outputs.\n>>\n>>\n>> [https://i.imgur.com/40Dy3oq.png]\n>>\n>>\n>> Splice-in transactions will not require any fan-out children as long as\n>> all change goes into the channel outpoint.\n>>\n>>\n>> For new channel outpoints, the v0 witness script should be modified to\n>> start with [OP_1, OP_CHECKSEQUENCEVERIFY]. It needs to be the first item in\n>> the script to allow easy validation that it is used and not hidden in a\n>> false conditional. This would need to be applied to post-splice channel\n>> outpoints and probably dual funding channels should add it as well so they\n>> can be successfully included in splices.\n>>\n>>\n>> ### interactive tx protocol changes\n>>\n>> For splices, `tx_add_output` MUST include the `witness_script` in the\n>> tlv. Upon receiving outputs, nodes must validate the script matches the\n>> script hash in the output and that it begins with a minimum of 1 CSV.\n>>\n>>\n>> ## HTLC Timeouts and Splices\n>>\n>> Typically when this technique is used, one or two anchor outputs are\n>> added to purposely allow for CPFP fee bumping. But, turns out, we already\n>> have a usable anchor in the original commitment transaction! Very exciting.\n>>\n>>\n>> The interactive tx protocol mandates that splice txs are RBF-enabled.\n>> Broadcast splice proposals can be replaced out for the original commitment\n>> transaction at any time. Since the original commitment transaction has\n>> existing anchors, these may be used to increase fees on a force close. This\n>> combined with every other output in the tree being locked behind a 1 CSV\n>> means the force close will always have top mempool priority, mitigating the\n>> \u201coutput junk\u201d style pin.\n>>\n>>\n>> - Nifty & Dusty\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220926/1df8e3d0/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Splice Pinning Prevention w/o Anchors",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Greg Sanders",
                "Antoine Riard"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 16516
        }
    }
]