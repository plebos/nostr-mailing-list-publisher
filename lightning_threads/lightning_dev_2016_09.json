[
    {
        "title": "[Lightning-dev] [bitcoin-dev] New BIP: Dealing with OP_IF and OP_NOTIF malleability in P2WSH",
        "thread_messages": [
            {
                "author": "Johnson Lau",
                "date": "2016-09-01T11:39:51",
                "message_text_only": "Restriction for segwit OP_IF argument as a policy has got a few concept ACK. I would like to have more people to ACK or NACK, especially the real users of OP_IF. I think Lightning network would use that at lot.\n\nPull request: https://github.com/bitcoin/bitcoin/pull/8526\n\nmore related discussion could be found at https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-August/013036.html\n\nIt does have impact if your script uses the combination of \"OP_SIZE OP_IF\" or \"OP_DEPTH OP_IF\". With this policy/softfork, you need to use  \"OP_SIZE OP_0NOTEQUAL OP_IF\" or \"OP_DEPTH OP_0NOTEQUAL OP_IF\", or reconstruct your scripts.\n\n> \n>     On August 16, 2016 at 1:53 PM Johnson Lau via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n>     -----BEGIN PGP SIGNED MESSAGE-----\n>     Hash: SHA512\n> \n>     A new BIP is prepared to deal with OP_IF and OP_NOTIF malleability in P2WSH:\n>     https://github.com/jl2012/bips/blob/minimalif/bip-minimalif.mediawiki\n>     https://github.com/bitcoin/bitcoin/pull/8526\n> \n>     BIP: x\n>     Title: Dealing with OP_IF and OP_NOTIF malleability in P2WSH\n>     Author: Johnson Lau <jl2012 at xbt.hk>\n>     Status: Draft\n>     Type: Standards Track\n>     Created: 2016-08-17\n> \n>     Abstract\n> \n>     This document specifies proposed changes to the Bitcoin script validity rules in order to make transaction malleability related to OP_IF and OP_NOTIF impossible in pay-to-witness-script-hash (P2WSH) scripts.\n> \n>     Motivation\n> \n>     OP_IF and OP_NOTIF are flow control codes in the Bitcoin script system. The programme flow is decided by whether the top stake value is True or False. However, this behaviour opens a source of malleability as a third party may replace a True (False) stack item with any other True (False) value without invalidating the transaction.\n> \n>     The proposed rules apply only to pay-to-witness-script-hash (P2WSH) scripts described in BIP141, which has not been activated on the Bitcoin mainnet as of writing. To ensure OP_IF and OP_NOTIF transactions created before the introduction of this BIP will still be accepted by the network, the new rules are not applied to non-segregated witness scripts.\n> \n>     Specification\n> \n>     In P2WSH, the argument for OP_IF and OP_NOTIF MUST be exactly an empty vector or 0x01, or the script evaluation fails immediately.\n> \n>     This is deployed using BIP9 after segregated witness (BIP141) is activated. Details TBD.\n> \n>     Compatibility\n> \n>     This is a softfork on top of BIP141. The rules are enforced as a relay policy by the reference client since the first release of BIP141 (v0.13.1). To avoid risks of fund loss, users MUST NOT create P2WSH scripts that are incompatible with this BIP. An OP_0NOTEQUAL may be used before OP_IF or OP_NOTIF to imitate the original behaviour (which may also re-enable the malleability vector depending on the exact script).\n> \n>     Implementation\n> \n>     https://github.com/bitcoin/bitcoin/pull/8526\n> \n>     Copyright\n> \n>     This work is placed in the public domain.\n>     -----BEGIN PGP SIGNATURE-----\n>     Comment: GPGTools - https://gpgtools.org\n> \n>     iQGcBAEBCgAGBQJXs1LgAAoJEO6eVSA0viTSrJQL/A/womJKgi4FuyBTL9oykCss\n>     aBMNN9+SLtmuH7SBgEUGZ8TFxa2st+6RP6Imu+Vvn4O5sXQl3DIXV+X38X93sUYk\n>     wrjdpvdpqFFYJezPDESz6pR/6bZ1ES0aO2QqX578/8sqr8GO6L388s66vJeIGj4n\n>     0LWW8sdEypMuV3HUG/9FFdUNHgiVX1U0sS1rT3P4aN30JYtb7PQpd7r8KTMta7Rt\n>     L1VOZB+W3m2m2YZ9gB7IRmMfzzNm2QXRTPIZXt2x3mYDBuMkp+zEd5+ogA4sBpgP\n>     wp2+l/aos686v0w8QYiNUX2+9Qpe7+238qUpw75d2XJYmLzdotWFvmp4g1hP+awX\n>     HEfwe4BUM+El17LjrHkNeMWNJXMlhTtXb2i0XMj8tU5lZVHep4WpQ+LEahrNlsUl\n>     FdFsi3q8HeWh8JsGaNCL41Bgbg/rKb5hUXyF6hTRHa//E6llOrpXRnsloKgBLv8c\n>     QezgKTAPwwgdjcS6Ek0AqgLp7bCFRijCduYH9i9uaQ==\n>     =lLIZ\n>     -----END PGP SIGNATURE-----\n> \n>     _______________________________________________\n>     bitcoin-dev mailing list\n>     bitcoin-dev at lists.linuxfoundation.org\n>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20160901/8e47cd9f/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2016-09-05T01:32:19",
                "message_text_only": "Johnson Lau via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>\nwrites:\n> Restriction for segwit OP_IF argument as a policy has got a few concept ACK. I would like to have more people to ACK or NACK, especially the real users of OP_IF. I think Lightning network would use that at lot.\n\nMy current scripts use OP_IF and OP_NOTIF only after OP_EQUAL, except\nfor one place where they use OP_EQUAL ... OP_EQUAL... OP_ADD OP_IF\n(where the two OP_EQUALs are comparing against different hashes, so only\n0 or 1 of the two OP_EQUAL can return 1).\n\nSo there's no effect either way on the c-lightning implementation, at\nleast.\n\nThanks!\nRusty."
            }
        ],
        "thread_summary": {
            "title": "New BIP: Dealing with OP_IF and OP_NOTIF malleability in P2WSH",
            "categories": [
                "Lightning-dev",
                "bitcoin-dev"
            ],
            "authors": [
                "Rusty Russell",
                "Johnson Lau"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 4822
        }
    },
    {
        "title": "[Lightning-dev] [BOLT Draft] Onion Routing Spec",
        "thread_messages": [
            {
                "author": "Christian Decker",
                "date": "2016-09-02T12:08:22",
                "message_text_only": "I'd like to pick up the conversation about the onion routing protocol\nagain, since we are close to merging our implementation into the\nlightningd node.\n\nAs far as I can see we mostly agree on the spec, with some issues that\nshould be deferred until later/to other specs:\n\n - Key-rotation policies\n - Backlog of seen shared secrets\n - Payload formatting (what data to include and how it is encoded)\n\nThe last pressing issue is the use of the HMAC, specifically what the\nper-hop verifiable HMAC should include. Having it cover the entirety\nof the packet, including the payloads, has the advantage that\ncorrupted packets get dropped immediately, which prevents the route\nlength discovery attacks Rusty described and we make more efficient\nuse of our funds, i.e., drop a transfer immediately not allocating\nHTLCs for a payment that is destined to fail, and quicker retries.\n\nThe downside is the rendezvous problem, in which the recipient would\nprovide part of the onion, hence the sender cannot compute the\nHMACs. We can potentially sidestep this with a partially trusted\nreflector, have an interactive construction of the packet, or come up\nwith a new scheme ourselves. Anyway, I'm happy to shelve this aspect\nfor a future v2 of the onion routing protocol, and include the payload\nin the HMAC.\n\nI think for now we should also keep the payload sizes fixed at 20\nbytes for per-hop and 1024 byte for end-to-end payload, and we can\ndiscuss how to format those payloads in another spec. Since we seem to\nwant to add and remove bits and pieces it might be worth to make it\nflexible using a generic encoding (JSON, msgpack, ...). This\npotentially includes the \"forward X units of coin Y\" and the \"expect X\nunits\" for the endpoint. We can also address the \"last-hop corrupts\"\nproblem in the payload with an additional end-to-end secret like you\nsuggested. Having them in the per-hop payload and HMACing the payloads\nsecures them against tampering.\n\nLet me know if there is any major thing that is not/insufficiently\naddressed. BTW do we have a process in place for upgrading spec drafts\nor do we keep things informal?\n\nRegards,\nChristian\n\nOn Mon, Aug 22, 2016 at 07:47:56PM +0000, Olaoluwa Osuntokun wrote:\n> On Sun, Aug 21, 2016 at 1:46 PM Rusty Russell <rusty at rustcorp.com.au> wrote:\n> \n> > > This may not fully solve the problem, since if one presumes that the\n> > > second-to-last hop is malicious, they can re-create a new onion blob\n> > > (presuming consistent hashes for each hop, of course).\n> >\n> > Great catch.  Oops...\n> >\n> \n> During the whole payment negotiation process, the sender and receiver can\n> additionally agree on a shared secret-ish value (possibly the hash of the\n> contract) that should be included in the per-hop payload for the final hop.\n> \n> If the portion of the per-hop payload doesn't match identically with this\n> value, then the payment should be rejected as a prior node has\n> unsuccessfully attempted re-create the onion packet.\n> \n> \n> -- Laolu\n\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "Rusty Russell",
                "date": "2016-09-05T02:25:22",
                "message_text_only": "Christian Decker <decker.christian at gmail.com> writes:\n> I'd like to pick up the conversation about the onion routing protocol\n> again, since we are close to merging our implementation into the\n> lightningd node.\n>\n> As far as I can see we mostly agree on the spec, with some issues that\n> should be deferred until later/to other specs:\n>\n>  - Key-rotation policies\n\nOK, I've been thinking about the costs of key-rotation.\n\nAssumptions:\n1) We simply use a single pubkey for everything about a node, aka its ID.\n2) Medium-scale public network, 250,000 nodes and 1M channels.\n3) Every node knows the entire public network.\n\nEach node ID is 33 bytes (pubkey) each channel is 6 bytes (blocknum +\ntxnum).  You need to associate channels -> ids, say another 8 bytes per\nchannel.\n\nThat's 22.25MB each node has to keep.\n\nThe proofs are larger: to prove which IDs owns a channel, each one needs\na merkle proof (12 x 32 bytes) plus the funding tx (227 bytes, we can\nskip some though), the two pubkeys (66 bytes), and a signature of the ID\nusing those pubkeys (128 bytes, schnorr would be 64?).\n\nThat's an additional 800M each node has to download to completely\nvalidate, and of course some nodes will have to keep this so we can\ndownload it from somewhere.  That's even bigger than Pokemon Go :(\n\nChange Assumptions:\n1) We use a \"comms\" key for each node instead of its ID.\n2) Nodes send out a new comms key, signed by ID.\n\nThat's another 33 bytes each to keep, or 8.25MB.  To rotate a comms key,\nwe need the new key (33 bytes), and a signature from the id (64 bytes),\nand probably a timestamp, (4 bytes), that's 25.25MB.\n\nThat's not too bad if we rotate daily.  Probably not if we rotate\nhourly..\n\nCheers,\nRusty."
            },
            {
                "author": "Christian Decker",
                "date": "2016-09-06T11:27:01",
                "message_text_only": "On Mon, Sep 05, 2016 at 11:55:22AM +0930, Rusty Russell wrote:\n> Christian Decker <decker.christian at gmail.com> writes:\n> > I'd like to pick up the conversation about the onion routing protocol\n> > again, since we are close to merging our implementation into the\n> > lightningd node.\n> >\n> > As far as I can see we mostly agree on the spec, with some issues that\n> > should be deferred until later/to other specs:\n> >\n> >  - Key-rotation policies\n> \n> OK, I've been thinking about the costs of key-rotation.\n>\n\nI forgot that we have two potential key-rotations:\n\n - Rotating the key used in transactions that hit the Bitcoin network\n - Rotating the public key used for the DH shared secret generation\n   for the onion routing protocol\n\nFor the moment I was concentrating on the latter.\n\n> Assumptions:\n> 1) We simply use a single pubkey for everything about a node, aka its ID.\n> 2) Medium-scale public network, 250,000 nodes and 1M channels.\n> 3) Every node knows the entire public network.\n> \n> Each node ID is 33 bytes (pubkey) each channel is 6 bytes (blocknum +\n> txnum).  You need to associate channels -> ids, say another 8 bytes per\n> channel.\n> \n> That's 22.25MB each node has to keep.\n> \n> The proofs are larger: to prove which IDs owns a channel, each one needs\n> a merkle proof (12 x 32 bytes) plus the funding tx (227 bytes, we can\n> skip some though), the two pubkeys (66 bytes), and a signature of the ID\n> using those pubkeys (128 bytes, schnorr would be 64?).\n> \n> That's an additional 800M each node has to download to completely\n> validate, and of course some nodes will have to keep this so we can\n> download it from somewhere.  That's even bigger than Pokemon Go :(\n>\n> Change Assumptions:\n> 1) We use a \"comms\" key for each node instead of its ID.\n> 2) Nodes send out a new comms key, signed by ID.\n> \n> That's another 33 bytes each to keep, or 8.25MB.  To rotate a comms key,\n> we need the new key (33 bytes), and a signature from the id (64 bytes),\n> and probably a timestamp, (4 bytes), that's 25.25MB.\n> \n> That's not too bad if we rotate daily.  Probably not if we rotate\n> hourly..\n>\n\nA node's public key used for DH shared secret generation exists\nindependently of its channels. I think we probably should not bind the\nrotation of the key we use to talk to that node to one of its\nchannels. However, it does make sense to require that a node also has\nat least one active channel in order for us to care at all :-)\n\nThe comms key approach is in line with what I was thinking as well.\nWe can bind the new communication key with the channel's existence by\nshowing a derivation path from the node's (fixed) public key and the\nnew key. So a node wanting to rotate its communication key just sends\nthe following: \"I am <pubkey> (33 byte), please use key <derivation\nnumber> (~4 byte) and here is a <signature> (64 bytes) that I signed\nthis rotation off.\". The communication overhead is identical to your\nproposal, but, since you send only the new key, I think in your\nproposal we'd have to churn through all known node ids to find which\none signed the rotation, or where you also using timestamp based\nderivation?\n\nAnother case we could consider is having passive rotations: when an\nendpoint announces a channel's existence it also sends its rotation\ninterval along. Every <rotation interval> nodes simply derive the new\nkey and use that for the DH shared secret generation should they want\nto talk to this node. And nodes have a switchover window in which they\naccept both (would be necessary in the active rotation as well due to\ndelays). The passive rotation incurs no communication overhead and can\nbe bound to the node's channels, so as long as we believe one of its\nchannels to exist, we rotate its keys.\n\nPossibly a mix of active and passive would make sense, with the active\nrotation enabling emergency rotations in case a key was compromised,\nbut we're in a lot of trouble then anyway :-)\n\n> Cheers,\n> Rusty.\n\nCheers,\nChristian"
            },
            {
                "author": "Rusty Russell",
                "date": "2016-09-07T01:27:47",
                "message_text_only": "Christian Decker <decker.christian at gmail.com> writes:\n> On Mon, Sep 05, 2016 at 11:55:22AM +0930, Rusty Russell wrote:\n>> Christian Decker <decker.christian at gmail.com> writes:\n>> > I'd like to pick up the conversation about the onion routing protocol\n>> > again, since we are close to merging our implementation into the\n>> > lightningd node.\n>> >\n>> > As far as I can see we mostly agree on the spec, with some issues that\n>> > should be deferred until later/to other specs:\n>> >\n>> >  - Key-rotation policies\n>> \n>> OK, I've been thinking about the costs of key-rotation.\n>>\n>\n> I forgot that we have two potential key-rotations:\n>\n>  - Rotating the key used in transactions that hit the Bitcoin network\n\nOh, that's different, yes.\n\n>  - Rotating the public key used for the DH shared secret generation\n>    for the onion routing protocol\n>\n> For the moment I was concentrating on the latter.\n\nYes.  It's the one we need to communicate through the node.\n\n>> Change Assumptions:\n>> 1) We use a \"comms\" key for each node instead of its ID.\n>> 2) Nodes send out a new comms key, signed by ID.\n>> \n>> That's another 33 bytes each to keep, or 8.25MB.  To rotate a comms key,\n>> we need the new key (33 bytes), and a signature from the id (64 bytes),\n>> and probably a timestamp, (4 bytes), that's 25.25MB.\n>> \n>> That's not too bad if we rotate daily.  Probably not if we rotate\n>> hourly..\n\n> A node's public key used for DH shared secret generation exists\n> independently of its channels. I think we probably should not bind the\n> rotation of the key we use to talk to that node to one of its\n> channels. However, it does make sense to require that a node also has\n> at least one active channel in order for us to care at all :-)\n\nThis calculation was per-node, not per-channel.\n\n> The comms key approach is in line with what I was thinking as well.\n> We can bind the new communication key with the channel's existence by\n> showing a derivation path from the node's (fixed) public key and the\n> new key. So a node wanting to rotate its communication key just sends\n> the following: \"I am <pubkey> (33 byte), please use key <derivation\n> number> (~4 byte) and here is a <signature> (64 bytes) that I signed\n> this rotation off.\". The communication overhead is identical to your\n> proposal, but, since you send only the new key, I think in your\n> proposal we'd have to churn through all known node ids to find which\n> one signed the rotation, or where you also using timestamp based\n> derivation?\n\nHmm, do we lose forward secrecy if we use a BIP32 chain?  But we may be\nable to use another derivation method where we derive key N from key\nN-1.  I'm looking at Laolu...\n\n> Another case we could consider is having passive rotations: when an\n> endpoint announces a channel's existence it also sends its rotation\n> interval along. Every <rotation interval> nodes simply derive the new\n> key and use that for the DH shared secret generation should they want\n> to talk to this node. And nodes have a switchover window in which they\n> accept both (would be necessary in the active rotation as well due to\n> delays). The passive rotation incurs no communication overhead and can\n> be bound to the node's channels, so as long as we believe one of its\n> channels to exist, we rotate its keys.\n\nI like the zero-comms overhead!  We could in fact use block number to\nrotate; key 1 is used for the first N blocks, then key 2, etc, with old\nkeys allowed X blocks late, new keys allowed X blocks early?\n\nI think we should select some N for the moment, rather than making it\nconfigurable.  If it's too small it might take a long time for clients\nto derive the current key: 50 usecs each step is almost half a second\nfor a year-old key if we rotate every 6 blocks.  Rotation daily makes\nthat much more approachable...\n\n> Possibly a mix of active and passive would make sense, with the active\n> rotation enabling emergency rotations in case a key was compromised,\n> but we're in a lot of trouble then anyway :-)\n\nIn that case, you close the channels and start a new node?\n\nCheers,\nRusty."
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2016-09-09T23:52:41",
                "message_text_only": "Rusty Russell <rusty at rustcorp.com.au> wrote:\n\n> Each node ID is 33 bytes (pubkey) each channel is 6 bytes (blocknum +\ntxnum).\n\nUsing blocknum+txnum to identify a channel doesn't account for the\npossibility of a single transaction which opens multiple channels\nconcurrently. A possible use-case for such a transaction are coin-joined\nchannel openings which obfuscate the relationships being creating with a\nchannel opening. You'd need to tack on an extra byte (maybe two) to also\nencode the output index of the channel within the transaction. Additionally,\nidentifying channels based solely from blockheight+offsets conjures up some\nre-org safety concerns. It's also a bit less \"verifiable\" than referring via\nthe full outpoint.\n\n> The proofs are larger: to prove which IDs owns a channel, each one needs a\n> merkle proof (12 x 32 bytes) plus the funding tx (227 bytes, we can skip\n> some though), the two pubkeys (66 bytes), and a signature of the ID using\n> those pubkeys (128 bytes, schnorr would be 64?).\n\nAre those two pub keys the multi-sig pub keys, or the identity pub keys? In\neither case, only requiring two pub keys to attest-to/authenticate the state\nof a channel is insufficient. With only two signatures, then nodes can\nadvertise the same channel several times creating a non-canonical graphs\nresulting in differing network views. If we'd like to eliminate such a\npossibility, then we also need to cryptographically bind the two identities\nto the proof. Using a schnorr multi-signature generated by the four pubkeys\nwould remedy this. Validators can use pubkey recovery to extract the \"group\npubkey\" from the single signature, ensure it's the result of point addition\nof the four public keys, check the channel isn't closed, then verify the\nsignature over the advertisement as normal.\n\n\nChristian Decker <decker.christian at gmail.com> wrote:\n\n> I forgot that we have two potential key-rotations:\n> - Rotating the key used in transactions that hit the Bitcoin network\n\nIf you mean the key included within the 2-of-2 multi-sig, I assume all\nimplementations will be using a fresh key with each channel either way, so\nwe don't need any explicit rotations here?\n\nI've only been thinking of rotations for the onion pubkeys used for the DH\nshared secret derivation for the onion packets. Identity keys could also\npossibly either be rotated/revoked, or delegated with some sort of\n\"certificate\".\n\n> Another case we could consider is having passive rotations: when an\n> endpoint announces a channel's existence it also sends its rotation\n> interval along.\n\nIMO a passive rotation scheme is superior due to the bandwidth savings of\npushing the onion key rotations to the edges themselves. As you said an\nactive rotation might be used in the emergency case of a forced key rotation\ndue to key compromise.\n\n\nRusty Russell <rusty at rustcorp.com.au> wrote:\n\n> Hmm, do we lose forward secrecy if we use a BIP32 chain?  But we may be\n> able to use another derivation method where we derive key N from key\n> N-1.  I'm looking at Laolu...\n\nWell, sorta. If the master pubkey is published, and nodes use public\nderivation on the edges to do passive key rotations, then if one of the\nnon-hardened child private keys is leaked, an adversary can derive the\nmaster private key thereby gaining the ability to decrypt any\nsaved/intercepted onion packets to the compromised node.\n\nEarlier in this email chain I suggested a scheme to regain forward secrecy\nin the case of such a private key leak by requiring nodes to do some upfront\npre-computation involving an intermediate derivation point. This would still\nallow passive rotation by the edges. However, in practice those\npre-generated would likely be stored in the same location? So that kinda\nputs us back at the original exploit scenario.\n\nIf we were to involve some pairing crypto (IBE style), we would use say\nblock hashes to allow nodes to passively derive keys the \"source\" node is\nable to generate the corresponding private key to...\n\nIMO we need to more clearly state our security model/assumptions here to\nreason about the attack scenarios we'll try to guard against.\n\n\n-- Laolu\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20160909/486374b1/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2016-09-27T01:47:44",
                "message_text_only": "(Sorry for delay: travel plus laptop issues...)\n\nOlaoluwa Osuntokun <laolu32 at gmail.com> writes:\n> Rusty Russell <rusty at rustcorp.com.au> wrote:\n>\n>> Each node ID is 33 bytes (pubkey) each channel is 6 bytes (blocknum +\n> txnum).\n>\n> Using blocknum+txnum to identify a channel doesn't account for the\n> possibility of a single transaction which opens multiple channels\n> concurrently. A possible use-case for such a transaction are coin-joined\n> channel openings which obfuscate the relationships being creating with a\n> channel opening. You'd need to tack on an extra byte (maybe two) to also\n> encode the output index of the channel within the transaction.\n\nGreat point.  We should absolutely support this.\n\n> Additionally, identifying channels based solely from\n> blockheight+offsets conjures up some re-org safety concerns. It's also\n> a bit less \"verifiable\" than referring via the full outpoint.\n\nA little, but the compactness is worth it.  If you're an SPV node you'll\nneed a proof, and if you're not you'll know if it's not in the main\nchain.\n\n>> The proofs are larger: to prove which IDs owns a channel, each one needs a\n>> merkle proof (12 x 32 bytes) plus the funding tx (227 bytes, we can skip\n>> some though), the two pubkeys (66 bytes), and a signature of the ID using\n>> those pubkeys (128 bytes, schnorr would be 64?).\n>\n> Are those two pub keys the multi-sig pub keys, or the identity pub keys? In\n> either case, only requiring two pub keys to attest-to/authenticate the state\n> of a channel is insufficient. With only two signatures, then nodes can\n> advertise the same channel several times creating a non-canonical graphs\n> resulting in differing network views. If we'd like to eliminate such a\n> possibility, then we also need to cryptographically bind the two identities\n> to the proof.\n\nYeah, it's not sufficient to get the multisig keys to sign the ID.\n\n> Using a schnorr multi-signature generated by the four pubkeys\n> would remedy this. Validators can use pubkey recovery to extract the \"group\n> pubkey\" from the single signature, ensure it's the result of point addition\n> of the four public keys, check the channel isn't closed, then verify the\n> signature over the advertisement as normal.\n\nI think this only works if the on-chain keys are Schnorr, right?\nUntil then, we'd need the ID to sign the pubkeys (or anything which\nuniquely identified the channel tx).\n\n> Christian Decker <decker.christian at gmail.com> wrote:\n>\n>> I forgot that we have two potential key-rotations:\n>> - Rotating the key used in transactions that hit the Bitcoin network\n>\n> If you mean the key included within the 2-of-2 multi-sig, I assume all\n> implementations will be using a fresh key with each channel either way, so\n> we don't need any explicit rotations here?\n>\n> I've only been thinking of rotations for the onion pubkeys used for the DH\n> shared secret derivation for the onion packets. Identity keys could also\n> possibly either be rotated/revoked, or delegated with some sort of\n> \"certificate\".\n>\n>> Another case we could consider is having passive rotations: when an\n>> endpoint announces a channel's existence it also sends its rotation\n>> interval along.\n>\n> IMO a passive rotation scheme is superior due to the bandwidth savings of\n> pushing the onion key rotations to the edges themselves. As you said an\n> active rotation might be used in the emergency case of a forced key rotation\n> due to key compromise.\n\nAs you state below (and as confirmed by Adam Back and others I asked at\nBlockstream), there is no passive scheme for forward secrecy though\nunless we want pairing crypto and magic.  So might as well not rotate\nkeys in that case.\n\n> Earlier in this email chain I suggested a scheme to regain forward secrecy\n> in the case of such a private key leak by requiring nodes to do some upfront\n> pre-computation involving an intermediate derivation point. This would still\n> allow passive rotation by the edges. However, in practice those\n> pre-generated would likely be stored in the same location? So that kinda\n> puts us back at the original exploit scenario.\n>\n> If we were to involve some pairing crypto (IBE style), we would use say\n> block hashes to allow nodes to passively derive keys the \"source\" node is\n> able to generate the corresponding private key to...\n>\n> IMO we need to more clearly state our security model/assumptions here to\n> reason about the attack scenarios we'll try to guard against.\n\nIndeed.  Let me try to enumerate the different secrets we need to\nprotect, and you tell me what I missed?\n\n0 - Wallet privkey(s)\n  * Must be online for channel creation.\n  * Compromise means all not-in-channel funds can be stolen.\n\n1 - Commit signing bitcoin privkey (per peer)\n  * Must be online for channel creation, change and close (ie. always).\n  * Compromise means we can collaborate with peer to steal funds in that\n    channel. (eg. sign a revoked transaction and give all funds to peer,\n    or mutually sign a tx spending the funds to anywhere the attacker wants).\n\n2 - Identity privkey\n  * Must be online for channel publishing, fee changes.\n  * Compromise means we can advertise new rates (or channel\n    unavailability), cutting node off lightning network.\n\n3 - Onion privkey\n  * Must be online for routing (ie. always)\n  * Compromise means we can decode incoming HTLC offers, maybe send fail failure\n    messages.\n\n4 - Low-level comms symmetric key (per peer)\n  * Must be online always.\n  * Compromise means we can decode all messages to peer, and break\n    connections with them (we can't create new HTLCs or change bitcoin\n    fees without the commit-signing privkey though).\n\n5 - H-preimage (per invoice)\n  * Must be online when creating or accepting payment for invoice\n    (always, unless we're routing-only).\n  * Compromise means we can lose associated incoming amount.\n\nThe simplest scheme is that the first four keys are the same.  That's\nhorrible for node privacy, since you can see even non-lightning funds.\nSo let's assume the wallet key is separate.\n\nSeparating commit keys from identity key allows nodes to post-hoc\nannounce channels (by associating them with the public identity) and\nalso means we can segregate peer privkeys (eg. one process per peer)\nwhich is slightly nicer from a security POV.\n\nSeparating onion privkey allows rotation; only a win if we get forward\nsecrecy (not a win for this node, as much as for the network as a\nwhole).\n\nThe comms symmetric key should be rotated with forward secrecy as\nwell, for similar reasons.\n\n(We *could* derive all the secrets from a single master secret, but\nthat's an implementation issue with its own tradeoffs...).\n\nThoughts?\nRusty."
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2016-09-05T19:24:11",
                "message_text_only": "On Fri, Sep 2, 2016 at 5:08 AM Christian Decker <decker.christian at gmail.com>\nwrote:\n\n> As far as I can see we mostly agree on the spec, with some issues that\n> should be deferred until later/to other specs:\n>\n>  - Key-rotation policies\n>  - Backlog of seen shared secrets\n>  - Payload formatting (what data to include and how it is encoded)\n>\n> Sounds good to me. That's a good summarization of the current outstanding\naspects to be deferred based on our past discussions.\n\n\n> Anyway, I'm happy to shelve this aspect for a future v2 of the onion\n> routing protocol, and include the payload in the HMAC.\n>\n\nAgreed, the full specification of the rendezvous handshake can deferred to\na later v2. We may even learn of some limitations or issues with the\ncurrent format which may impact the requirements or traits we want from a\nfuture rendezvous protocol.\n\nI think for now we should also keep the payload sizes fixed at 20\n> bytes for per-hop and 1024 byte for end-to-end payload\n\n\nSure, although I have some reservations about the 1024-byte e2e payload due\nto the potential bandwidth overhead for each HTLC-add in a high frequency\nsetting. However we should gather metrics from live testing environments to\nsee if this is actually a concern in practice.\n\nSince we seem to want to add and remove bits and pieces it might be worth\n> to make it flexible using a generic encoding (JSON, msgpack, ...).\n> This potentially includes the \"forward X units of coin Y\" and the \"expect X\n> units\" for the endpoint. We can also address the \"last-hop corrupts\"\n> problem in the payload with an additional end-to-end secret like you\n> suggested. Having them in the per-hop payload and HMACing the\n> payloads secures them against tampering.\n>\n\nIt feels like targeting a generic encoding might be at odds with the space\nconstraints imposed by the size of the per-hop payload. However if we can\nfind a format that's both succinct yet generic, then that would be very\nattractive.\n\n\n> BTW do we have a process in place for upgrading spec drafts\n> or do we keep things informal?\n>\n\nHmm, I'm not sure. As far as I'm aware, none of the current specifications\nhave been elevated to anything above a \"draft\" state. Perhaps we can hammer\nout the ins-and-outs of what we'd all like to see in the process in Milan?\n\nYay! This is exciting :), I'm happy with the way the current specification\nhas turned out. I think your break down of the current outstanding issues\nto be deferred to later specs are accurate. I also welcome a PR to our\n\"lightning-onion\" repo integrating the changes you implemented in your fork\nof my original code.\n\n-- Laolu\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20160905/d2e5fe4c/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Onion Routing Spec",
            "categories": [
                "Lightning-dev",
                "BOLT Draft"
            ],
            "authors": [
                "Rusty Russell",
                "Olaoluwa Osuntokun",
                "Christian Decker"
            ],
            "messages_count": 7,
            "total_messages_chars_count": 26671
        }
    },
    {
        "title": "[Lightning-dev] Payment presentation strawmen.",
        "thread_messages": [
            {
                "author": "Rusty Russell",
                "date": "2016-09-06T00:49:24",
                "message_text_only": "Hi all,\n\n        Joseph mentioned the critical issue of how a payee tells a payer\nhow to pay.   Here's my ordered wishlist:\n\n1) Can be presented with a standard QR code (ie. 1-way comms)\n2) Minimal state required on the client.\n3) Minimal network queries required by client (ie. minimize privacy leaks).\n4) Minimal state required on the server.\n\nHere's a range of possibilities:\n\nA) Simply present amount and public node address, rely on routing protocol for\n   client to get there.\n   - 4+33 bytes, tiny.  eg. http://ozlabs.org/~rusty/images/QR-04.gif [1]\n   - Client has to know entire network, or queries for route/fee\n     information which leaks far too much.\n\nB) Present one or more chains of channels from landmarks, with fee info.\n   Channels are represented as blocknum & txnum pairs.\n   - Adds 4 + 4 + 2 bytes per hop. eg http://ozlabs.org/~rusty/images/QR-11.gif\n   - Client needs to know channel tx -> node ID mapping for all nodes.\n   - Client must either know routes to landmarks (ie. globally agreed), or\n     query for them.\n\nC) As above, but include IDs for each hop in chain.\n  - Additional 33 bytes per hop. eg http://ozlabs.org/~rusty/images/QR-21.gif\n  - Client must either know routes to landmarks, or query for them.\n\nC is more scalable, OTOH B will work for the first million or so\nnodes...\n\nCheers,\nRusty.\n[1] Assuming an average route length of 5, 3 landmarks, and 20 bytes overhead."
            },
            {
                "author": "Ryan Grant",
                "date": "2016-09-10T11:16:07",
                "message_text_only": "Payments unexpectedly fragmented into multiple LN channels are\ntrickier than transactions spending multiple UTXOs.  If Alice pays Bob\nusing multiple channels to fund one payment, then Bob's accounting\nprocedures might need time-based heuristics to join separate LN\ntransactions.\n\nWherever payments might fragment, some reassembly protocol support,\nlike BIP 70's merchant_data field, should be available.  Every wallet\nshould be assisting with this accounting.\n\nSince it's a low-level protocol, a varint should suffice.\nSince we didn't need to bring in X.509 yet, call it payment_id.\nLack of a payment_id could, if tolerated at all, be considered a\n\"don't fragment\" request."
            },
            {
                "author": "Christian Decker",
                "date": "2016-09-10T21:36:12",
                "message_text_only": "On Sat, Sep 10, 2016 at 06:16:07AM -0500, Ryan Grant wrote:\n> Payments unexpectedly fragmented into multiple LN channels are\n> trickier than transactions spending multiple UTXOs.  If Alice pays Bob\n> using multiple channels to fund one payment, then Bob's accounting\n> procedures might need time-based heuristics to join separate LN\n> transactions.\n> \n> Wherever payments might fragment, some reassembly protocol support,\n> like BIP 70's merchant_data field, should be available.  Every wallet\n> should be assisting with this accounting.\n>\n\nAt least for the implementation using the r-hash to condition the\nrelease of funds there is nothing special about splitting a\npayment. As long as the recipient knows the total amount it should be\nreceiving it can delay the release of the secret until it is\nguaranteed all funds. Collating the partial payments is done with the\nr-hash. I'm pretty sure that the private key release would also work\nthe same way.\n\nA timeout might be sensible though just to be able to safely retry\nshould a partial payment get stuck.\n\nIn theory Lightning would also support arbitrary splitting and merging\nalong the paths of the payment. Not sure if this is still possible (or\ndesirable) when adding Onion Routing to the mix though :-)\n\nCheers,\nChristian"
            },
            {
                "author": "Ryan Grant",
                "date": "2016-09-11T03:16:02",
                "message_text_only": "On Sat, Sep 10, 2016 at 4:36 PM, Christian Decker\n<decker.christian at gmail.com> wrote:\n> At least for the implementation using the r-hash to condition the\n> release of funds there is nothing special about splitting a\n> payment. As long as the recipient knows the total amount it should be\n> receiving it can delay the release of the secret until it is\n> guaranteed all funds. Collating the partial payments is done with the\n> r-hash. I'm pretty sure that the private key release would also work\n> the same way.\n\nI understand how to collect partial payments on one channel this way, but\nmy reading was that the same r-hash cannot be used safely across\nmultiple channels.  Once one channel completes, exposure of the r-hash\nto a colluding intermediary in the other channel could allow pulling\nfunds before sending them onward."
            },
            {
                "author": "Ryan Grant",
                "date": "2016-09-11T04:04:34",
                "message_text_only": "Oops, I see my mistake.  Hold onto the r-hash longer."
            }
        ],
        "thread_summary": {
            "title": "Payment presentation strawmen.",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Rusty Russell",
                "Ryan Grant",
                "Christian Decker"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 4236
        }
    },
    {
        "title": "[Lightning-dev] Atomic payment to multiple parties, and payment amount obfuscation",
        "thread_messages": [
            {
                "author": "CJP",
                "date": "2016-09-23T06:15:42",
                "message_text_only": "Hi,\n\nI just woke up with a new thought:\n\nWith source routing and explicit fee payment, you can actually\natomically pay multiple parties, by letting all but one of the\nrecipients be intermediate parties in the route, and just paying them a\nvery high transaction fee. With \"atomically\" I mean it is impossible\nthat payment to one of the recipients succeeds and payment to another\nfails.\n\nPayment amount can be obfuscated by letting yourself (the sender) be the\nfinal recipient, so none of the intermediate nodes sees the actual\namount being transferred.\n\nIt isn't rocket science (or advanced cryptography), but I hadn't\nrealized this before.\n\n- CJP"
            },
            {
                "author": "Rusty Russell",
                "date": "2016-09-27T02:03:03",
                "message_text_only": "CJP <cjp at ultimatestunts.nl> writes:\n> Hi,\n>\n> I just woke up with a new thought:\n>\n> With source routing and explicit fee payment, you can actually\n> atomically pay multiple parties, by letting all but one of the\n> recipients be intermediate parties in the route, and just paying them a\n> very high transaction fee. With \"atomically\" I mean it is impossible\n> that payment to one of the recipients succeeds and payment to another\n> fails.\n>\n> Payment amount can be obfuscated by letting yourself (the sender) be the\n> final recipient, so none of the intermediate nodes sees the actual\n> amount being transferred.\n>\n> It isn't rocket science (or advanced cryptography), but I hadn't\n> realized this before.\n\nTricky :)  And adds some interesting game theory:\n\nImagine the simple case where I pay C $4 in fees, via B:\n\n     $5     $5     $1     $1\n   A ---> B ---> C ---> B ---> A\n    4days   3days  2days  1day\n\nB can simply use the H-preimage it gets from A to fulfill the HTLC A\noffered, gaining $4 and ignoring C.  If C somehow gets the preimage\nout-of-band, it can claim the $5 from B and then B can get its $1 from\nC.\n\nThe risk (for B) is that C will wait until the C->B HTLC has expired,\n*then* use the B->C HTLC to collect $5, leaving B out-of-pocket.\n\nNow, there's nothing special about this: the game happens for normal\nfees too, especially since we don't know if two apparently-distinct\nnodes are actually identical.  It's just more tempting when the fees are\nhigh.\n\nFun!\n\nThanks,\nRusty."
            },
            {
                "author": "CJP",
                "date": "2016-09-27T18:01:35",
                "message_text_only": "You mentioned two examples of out-of-band distribution of the pre-image:\nfrom B (role 2) to B (role 1), and as a scenario assumption of C\nreceiving the pre-image out-of-band. I think there is no risk in this.\n\nI think out-of-band distribution of the pre-image is not only harmless:\nit is even desirable. If one of the intermediate nodes blocks the\nregular distribution, the other ones can commit the transaction on their\nchannels as soon as they receive the pre-image (in- or out-of-band). The\nnode on the payee-side of the blocking node can enforce being paid by\nthe HTLC mechanism, and the node on the payer-side doesn't mind not\nhaving to pay (but can still pay voluntarily). The only nodes\npotentially losing funds are the ones that don't follow the regular\nprotocol.\n\nIf you don't have out-of-band distribution of the pre-image, one\nblocking node can potentially keep all HTLCs on his payer-side locked\nfor quite some time (until their time-outs). Eventually they end up\nbeing rolled back, with the blocking node again being the only one\nlosing funds (which is good).\n\nThe advantage of having your HTLCs resolved quickly, so those funds can\nflow in the opposite direction quickly, might be a sufficient incentive\nfor non-regular distribution of the pre-image. In Amiko Pay,\npayer->payee distribution is added next to payee->payer distribution,\nbut it's a voluntary thing, and people might decide to remove it from\ntheir version of Amiko Pay, without any real harm being done.\n\nCJP\n\n\nRusty Russell schreef op di 27-09-2016 om 11:33 [+0930]:\n\n> Imagine the simple case where I pay C $4 in fees, via B:\n> \n>      $5     $5     $1     $1\n>    A ---> B ---> C ---> B ---> A\n>     4days   3days  2days  1day\n> \n> B can simply use the H-preimage it gets from A to fulfill the HTLC A\n> offered, gaining $4 and ignoring C.  If C somehow gets the preimage\n> out-of-band, it can claim the $5 from B and then B can get its $1 from\n> C.\n> \n> The risk (for B) is that C will wait until the C->B HTLC has expired,\n> *then* use the B->C HTLC to collect $5, leaving B out-of-pocket.\n> \n> Now, there's nothing special about this: the game happens for normal\n> fees too, especially since we don't know if two apparently-distinct\n> nodes are actually identical.  It's just more tempting when the fees are\n> high.\n> \n> Fun!\n> \n> Thanks,\n> Rusty."
            }
        ],
        "thread_summary": {
            "title": "Atomic payment to multiple parties, and payment amount obfuscation",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Rusty Russell",
                "CJP"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 4473
        }
    },
    {
        "title": "[Lightning-dev] Testing a Flare-like routing implementation on 2500 AWS nodes",
        "thread_messages": [
            {
                "author": "Pierre",
                "date": "2016-09-18T12:58:03",
                "message_text_only": "TLDR: It kind of works (there is a 80+% chance of finding a route to any\nother node in ~500ms).\n\nHello guys,\n\nFlare[1] is a routing protocol for the Lightning Network which combines\nlocal neighborhood maps and paths to remote beacons. It is really\ninteresting and the simulations are promising, so we wanted to give it a\ntry and actually implement it in Eclair. We ended up with a close variant\n[2], and tested it on 2500 nodes on AWS.\n\nThe main difference between our implementation and the original paper is in\nthe way nodes communicate with each other. In the paper, an assumption is\nmade that \"there exists a way for any two nodes in LN to communicate with\neach other, perhaps with some prior setup (such as a distributed hashtable\noverlay maintained among LN nodes that would store mapping of node\nidentifiers to their current IP addresses)\". In our implementation there is\nno DHT: inter-nodes communication only occurs on existing open channels,\nand all routing-related messages are onion-encrypted and routed themselves,\njust like htlc messages. So a node can't talk directly to a node it doesn't\nalready have a route to. This certainly has a cost but it should also\nimproves privacy, and overall we think that using a DHT isn't necessary.\nAlso, making the assumption that a node is reachable from the outside is\nrather strong, particularly for mobile/light wallets. Whereas by\ncommunicating over channels, a node can actively participate to the global\nrouting as soon as it has more than two channels.\n\nThe neighbor discovery/beacons selection is pretty similar to what's in the\npaper, but a little bit simplified :\n- nodes don't ack table update messages (it is probably not needed since it\noccurs between adjacent nodes and any disconnection will be noticed)\n- nodes don't tell other nodes that they have been chosen as beacon (there\nis no beacon_set message). A given node knows it is a candidate because it\nreceived a beacon_req message, but it won't know if it was eventually\nselected.\n\nWe only focused on static ranking (finding a route formed of open channels\nbetween two given nodes), so it is very possible (probable?) that a route\nis not actually usable because the channels are not balanced. Basically we\nmade the assumption that the network graph is undirected, which is not true\nand is a pretty strong assumption.\n\nThe route selection has also been simplified. Whereas Flare proposed a 3\nsteps process:\n(a) sender uses its own routing table\n(b) sender requests receiver's table and uses a combination of the two\ntables\n(c) sender iterates over nodes it knows that are close to the receiver, and\nrequest their tables\nin our test we only did (b), which is a strong tradeoff. It means that the\ntime allowed to find a route is short and predictable, but it is very\nsuboptimal in terms of probability of finding a route.\n\nBelow are the results of a few tests we ran on AWS. The procedure was as\nfollows:\n1) start a certain number of m1.medium instances with standard linux AMI\n2) after startup the instances run an init script that d/l oracle jdk and\nour jar, then run the jar\n3) from a control server, we link nodes to each other following a given\ngraph using json-rpc commands\n4) wait a few minutes for the gossip to calm down (it can be pretty intense\nwith radius=3 [3] and our implementation is not optimized)\n5) pick 1000 random routes (random sender, random receiver), and for each\nroute using json-rpc we (a) asks the receiver for a \"payment request\" (H +\nrouting_table) and then (b) asks the sender to find a route for this\npayment request (without actually making a payment).\n6) collect stats\n\nCaveats:\n- nodes are connected all at once (step 3) above), and don't periodically\nreconcile their own routing table with that of their neighbors\n(neighbor_reset in the paper). This leads to nodes not knowing all the\nnodes in their radius, and probably has a significant negative impact on\nresults; we figured this out afterwards :-s\n- there is no \"proof of channels\" (nodes are not lying to each other about\nexisting channels)\n- onion messages are not actually encrypted (although channel\ncommunications are)\n- it is a \"LAN\" setup so things wouldn't be that smooth in reality\n(although we are testing static routes so it doesn't actually matter that\nmuch)\n- smallworld graphs may seem a little bit optimistic in terms of actual\nnetwork topology\n- ...\n\nParameters and results (probability of finding a route to any given node):\ntest nodes   r    beacon_count     graph      result  t_avg\n  A  1000    2    10            sw1000_6_02     87%\n  B  2000    2    10            sw2000_6_02     76%    305ms\n  C  2500    2    10            sw2500_4_01    ~20%\n  D  2500    3    10            sw2500_4_01     43%    377ms\n  E  2500    3    10            sw2500_6_02     83%    532ms\nNote: a swX_Y_0Z graph is a smallworld graph generated using the Watts and\nStrogatz model with n=X, k=Y and beta=0.Z\n\nNetwork size (known nodes):\ntest adjacent  all\n  A\n  B    6.8    62.3\n  C\n  D    4.0    58.8\n  E    6.0    97.0\nNote: expected values for column 'all' should be D=4^3=64+ and E=6^3=216+,\nsee caveat above. The good thing is that a radius=2 might actually be\nenough, just like the paper says!\n\nBeacons:\ntest dist_avg dist_max dist_var\n  A\n  B    7.4      39      29.4\n  C\n  D    9.2      96      79.5\n  E    8.1      45      36.0\n\nRoute lengths:\ntest  len_avg len_max len_var\n  A     5.4    26.0     3.3\n  B     6.2    30.0     6.2\n  C\n  D    17.9    74.0   109.5\n  E     7.3    28.0     5.6\n\nResponse time (this includes sender requesting the receiver's routing table\nand running a dijkstra):\ntest  t_avg   t_max\n  A\n  B   305ms   5158ms\n  C\n  D   377ms   7809ms\n  E   532ms   5128ms\nNote: High max times are probably related to the use of poor-performance\ninstances that sometimes behave erratically (unfortunately we don't have\nthe variance).\n\nIn conclusion, we show that our modified version of Flare actually works on\na decent size network. Because of the bug mentioned in the caveats, the\nvalue of the radius parameter should be taken with caution. One of the main\nconcerns for us is the fact that the graph is in fact directed and that at\ncreation channels are actually one-way. This might make finding route very\ndifficult at bootstrapping.\n\n\nCheers,\n\nPierre\n\n\n[1]\nhttp://bitfury.com/content/5-white-papers-research/bitfury_whitepaper_shared_send_untangling_in_bitcoin_8_24_2016.pdf\n[2]\nhttps://github.com/ACINQ/eclair/blob/wip-flare/eclair-node/src/main/scala/fr/acinq/eclair/router/FlareRouter.scala\n[3] https://s3-eu-west-1.amazonaws.com/acinq/public/network_out.PNG\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20160918/466f17e3/attachment.html>"
            },
            {
                "author": "Fabrice Drouin",
                "date": "2016-09-20T13:07:14",
                "message_text_only": "Edit: our link to the Flare white paper should be\n[1] http://bitfury.com/content/5-white-papers-research/whitepaper_flare_an_approach_to_routing_in_lightning_network_7_7_2016.pdf\n\nFabrice\n\nOn 18 September 2016 at 14:58, Pierre <pm+lists at acinq.fr> wrote:\n> TLDR: It kind of works (there is a 80+% chance of finding a route to any\n> other node in ~500ms).\n>\n> Hello guys,\n>\n> Flare[1] is a routing protocol for the Lightning Network which combines\n> local neighborhood maps and paths to remote beacons. It is really\n> interesting and the simulations are promising, so we wanted to give it a try\n> and actually implement it in Eclair. We ended up with a close variant [2],\n> and tested it on 2500 nodes on AWS.\n>\n> The main difference between our implementation and the original paper is in\n> the way nodes communicate with each other. In the paper, an assumption is\n> made that \"there exists a way for any two nodes in LN to communicate with\n> each other, perhaps with some prior setup (such as a distributed hashtable\n> overlay maintained among LN nodes that would store mapping of node\n> identifiers to their current IP addresses)\". In our implementation there is\n> no DHT: inter-nodes communication only occurs on existing open channels, and\n> all routing-related messages are onion-encrypted and routed themselves, just\n> like htlc messages. So a node can't talk directly to a node it doesn't\n> already have a route to. This certainly has a cost but it should also\n> improves privacy, and overall we think that using a DHT isn't necessary.\n> Also, making the assumption that a node is reachable from the outside is\n> rather strong, particularly for mobile/light wallets. Whereas by\n> communicating over channels, a node can actively participate to the global\n> routing as soon as it has more than two channels.\n>\n> The neighbor discovery/beacons selection is pretty similar to what's in the\n> paper, but a little bit simplified :\n> - nodes don't ack table update messages (it is probably not needed since it\n> occurs between adjacent nodes and any disconnection will be noticed)\n> - nodes don't tell other nodes that they have been chosen as beacon (there\n> is no beacon_set message). A given node knows it is a candidate because it\n> received a beacon_req message, but it won't know if it was eventually\n> selected.\n>\n> We only focused on static ranking (finding a route formed of open channels\n> between two given nodes), so it is very possible (probable?) that a route is\n> not actually usable because the channels are not balanced. Basically we made\n> the assumption that the network graph is undirected, which is not true and\n> is a pretty strong assumption.\n>\n> The route selection has also been simplified. Whereas Flare proposed a 3\n> steps process:\n> (a) sender uses its own routing table\n> (b) sender requests receiver's table and uses a combination of the two\n> tables\n> (c) sender iterates over nodes it knows that are close to the receiver, and\n> request their tables\n> in our test we only did (b), which is a strong tradeoff. It means that the\n> time allowed to find a route is short and predictable, but it is very\n> suboptimal in terms of probability of finding a route.\n>\n> Below are the results of a few tests we ran on AWS. The procedure was as\n> follows:\n> 1) start a certain number of m1.medium instances with standard linux AMI\n> 2) after startup the instances run an init script that d/l oracle jdk and\n> our jar, then run the jar\n> 3) from a control server, we link nodes to each other following a given\n> graph using json-rpc commands\n> 4) wait a few minutes for the gossip to calm down (it can be pretty intense\n> with radius=3 [3] and our implementation is not optimized)\n> 5) pick 1000 random routes (random sender, random receiver), and for each\n> route using json-rpc we (a) asks the receiver for a \"payment request\" (H +\n> routing_table) and then (b) asks the sender to find a route for this payment\n> request (without actually making a payment).\n> 6) collect stats\n>\n> Caveats:\n> - nodes are connected all at once (step 3) above), and don't periodically\n> reconcile their own routing table with that of their neighbors\n> (neighbor_reset in the paper). This leads to nodes not knowing all the nodes\n> in their radius, and probably has a significant negative impact on results;\n> we figured this out afterwards :-s\n> - there is no \"proof of channels\" (nodes are not lying to each other about\n> existing channels)\n> - onion messages are not actually encrypted (although channel communications\n> are)\n> - it is a \"LAN\" setup so things wouldn't be that smooth in reality (although\n> we are testing static routes so it doesn't actually matter that much)\n> - smallworld graphs may seem a little bit optimistic in terms of actual\n> network topology\n> - ...\n>\n> Parameters and results (probability of finding a route to any given node):\n> test nodes   r    beacon_count     graph      result  t_avg\n>   A  1000    2    10            sw1000_6_02     87%\n>   B  2000    2    10            sw2000_6_02     76%    305ms\n>   C  2500    2    10            sw2500_4_01    ~20%\n>   D  2500    3    10            sw2500_4_01     43%    377ms\n>   E  2500    3    10            sw2500_6_02     83%    532ms\n> Note: a swX_Y_0Z graph is a smallworld graph generated using the Watts and\n> Strogatz model with n=X, k=Y and beta=0.Z\n>\n> Network size (known nodes):\n> test adjacent  all\n>   A\n>   B    6.8    62.3\n>   C\n>   D    4.0    58.8\n>   E    6.0    97.0\n> Note: expected values for column 'all' should be D=4^3=64+ and E=6^3=216+,\n> see caveat above. The good thing is that a radius=2 might actually be\n> enough, just like the paper says!\n>\n> Beacons:\n> test dist_avg dist_max dist_var\n>   A\n>   B    7.4      39      29.4\n>   C\n>   D    9.2      96      79.5\n>   E    8.1      45      36.0\n>\n> Route lengths:\n> test  len_avg len_max len_var\n>   A     5.4    26.0     3.3\n>   B     6.2    30.0     6.2\n>   C\n>   D    17.9    74.0   109.5\n>   E     7.3    28.0     5.6\n>\n> Response time (this includes sender requesting the receiver's routing table\n> and running a dijkstra):\n> test  t_avg   t_max\n>   A\n>   B   305ms   5158ms\n>   C\n>   D   377ms   7809ms\n>   E   532ms   5128ms\n> Note: High max times are probably related to the use of poor-performance\n> instances that sometimes behave erratically (unfortunately we don't have the\n> variance).\n>\n> In conclusion, we show that our modified version of Flare actually works on\n> a decent size network. Because of the bug mentioned in the caveats, the\n> value of the radius parameter should be taken with caution. One of the main\n> concerns for us is the fact that the graph is in fact directed and that at\n> creation channels are actually one-way. This might make finding route very\n> difficult at bootstrapping.\n>\n>\n> Cheers,\n>\n> Pierre\n>\n>\n> [1]\n> http://bitfury.com/content/5-white-papers-research/bitfury_whitepaper_shared_send_untangling_in_bitcoin_8_24_2016.pdf\n> [2]\n> https://github.com/ACINQ/eclair/blob/wip-flare/eclair-node/src/main/scala/fr/acinq/eclair/router/FlareRouter.scala\n> [3] https://s3-eu-west-1.amazonaws.com/acinq/public/network_out.PNG\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n\n\n\n-- \nACINQ SAS, Si\u00e8ge Social 10 rue de Penthi\u00e8vre 75008 PARIS.\nCapital 51 437 Euros, 804 203 792 RCS Paris \u2013 APE 6202A \u2013 SIRET 804\n203 792 00010 \u2013 TVA FR 34 804203792."
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2016-09-20T18:47:25",
                "message_text_only": "Hi, y'all\n\nExcellent work!!\n\nAs you noted our, Flare paper as currently written only includes a series of\nsimulations of various topologies/parameters which are then extrapolated to\nlarger network sizes. The logical next step would be to deploy a\nproto-implementation within a live testbed with real latencies, preferential\nattachment, etc. I'm thrilled that y'all went ahead getting your hands dirty\nto gauge the real-word feasibility of our scheme.\n\n> In the paper, an assumption is made that \"there exists a way for any two\n> nodes in LN to communicate with each other, perhaps with some prior setup\n> (such as a distributed hashtable overlay maintained among LN nodes that\n> would store mapping of node identifiers to their current IP addresses)\".\n\nIn reality we envisioned that a DHT would acts as a substitute for a pure\nbroadcast network, rather than to allow individual nodes to communicate with\neach other. At the time of the writing of the paper, we envisioned that\ninformation such as the current onion key for each node, identity keys,\nchannel proofs, etc would potentially be stored within a DHT.\n\nFor communications, we referenced that something akin to HORNET could be\nused to allow nodes to communicate with each other without necessarily\nknowing the IP address of each node or a node's selected beacons. As y'all\nnoted, in this scenario, nodes would only be able to directly communicate\nwith nodes that they have a direct path to.\n\nHORNET was especially attractive as as after its setup phase, there exists a\nbi-directional communication link between two nodes. This link could either\nbe used in a request/response manner to notify a node that it's a selected\nbeacon and/or to fetch routing tables, or in a more streaming manner between\nthe sender/receiver to negotiate the details of further payments (additional\nR-Hashes, etc) once the link was established. As a substitute for the first\nuse-case (request/response) the latest design of our Sphinx construction\ncould be used, with the requesting node providing the backwards path within\nthe e2e payload. It's worth noting that this substitute reveals the entire\npath to the responding node, while construction based on HORNET still\nobfuscates the backwards route from the target node. Additionally with\nHORNET, the backwards route can be distinct from the forwards route.\n\n> We only focused on static ranking (finding a route formed of open channels\n> between two given nodes), so it is very possible (probable?) that a route\nis\n> not actually usable because the channels are not balanced. Basically we\nmade\n> the assumption that the network graph is undirected, which is not true\nand is a\n> pretty strong assumption.\n\nIndeed, as channels themselves have fixed capacities, proper path finding\nstems beyond simply \"shortest path\", and begins to wonder into the realm of\nnetwork flow theory[1]. Assuming nodes are aware of the available channel\ncapacity and current fee advertised by the each node, then optimal path\n(cheapest) path can be discovered by solving the for the min-cost flow[2]\nwithin the node's subset of the network graph. Additionally, the cost\nfunction for each edge within the graph can also factor in the absolute HTLC\ntime delay between each node.\n\nOn a related note, in the past Tadge has suggested that the available\nchannel capacity that a nodes wants to advertise should be an input to a\nfunction which derives the advertised fee across the link. One potential\nstrategy would be to have the advertised fee be inversely proportional to a\nmetric which captures how balanced the channel is. So if a channel was\nmostly unbalanced to in one direction, then the advertised fee in that\ndirection would be relatively \"high\", while in the opposite (balancing flow)\ndirection the advertised fee would be proportionally lower. Fully depleted\nchannels (which should only exist right after channel creation, with no\ncommitted state transitions) would then advertise a fee of \"infinity\"\nallowing nodes to update their path finding accordingly (something Rusty\npointed out the other day). Following down this line of thinking further\nbeings to invoke the concept of \"negative fees\" which have been discussed a\nbit informally in the past.\n\n> 3) from a control server, we link nodes to each other following a given\n> graph using json-rpc commands\n\nWas the capacity of the channels created uniform? Or were the channel values\nuniformly distributed? Or perhaps drawn from a particular distribution?\n\n> 5) pick 1000 random routes (random sender, random receiver), and for each\n> route using json-rpc we (a) asks the receiver for a \"payment request\" (H +\n> routing_table) and then (b) asks the sender to find a route for this\npayment\n> request (without actually making a payment).\n\nSimilar to the question above, how were the satoshis requested within the\n\"payment request\" distributed?\n\n-- Laolu\n\n[1]: https://en.wikipedia.org/wiki/Flow_network\n[2]: https://en.wikipedia.org/wiki/Minimum-cost_flow_problem\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20160920/de0e3f77/attachment.html>"
            },
            {
                "author": "Pierre",
                "date": "2016-09-21T14:55:17",
                "message_text_only": "> Excellent work!!\nThanks!\n\n> In reality we envisioned that a DHT would acts as a substitute for a pure\n> broadcast network, rather than to allow individual nodes to communicate with\n> each other. At the time of the writing of the paper, we envisioned that\n> information such as the current onion key for each node, identity keys,\n> channel proofs, etc would potentially be stored within a DHT.\nGot it, that makes sense. The reference to ip adress was confusing I guess.\n\n> For communications, we referenced that something akin to HORNET could be\n> used to allow nodes to communicate with each other [...]\n> HORNET was especially attractive as as after its setup phase, there exists a\n> bi-directional communication link between two nodes.\nYes I remember you mentioned HORNET on this ml some time ago. I am not too\nfamiliar with it, but I think it operates at the network layer, right? I am\ncurious how you envision the relation between this layer and the application\nlayer: would an application-level link (LN channel) be *equal* to a\nnetwork-level\nlink (whatever link there is between two adjacent HORNET nodes if there is such\na thing)? Or would that be completely orthogonal, meaning if A and B\nhave a channel\nopen an htlc going from A to B could have to be routed through other nodes?\nDoes this question make sense?\n\n> Additionally with\n> HORNET, the backwards route can be distinct from the forwards route.\nThat is an amazing feature, kind of magical really!\n\n> As a substitute for the first\n> use-case (request/response) the latest design of our Sphinx construction\n> could be used, with the requesting node providing the backwards path within\n> the e2e payload. It's worth noting that this substitute reveals the entire\n> path to the responding node\nYes, that's what we did, it does leak information but we are not sure how bad\nthat really is.\n\n\n> Was the capacity of the channels created uniform? Or were the channel values\n> uniformly distributed? Or perhaps drawn from a particular distribution?\n> Similar to the question above, how were the satoshis requested within the<\n> \"payment request\" distributed?\nWell it didn't matter in this particular test, because as you pointed out we\ndidn't look at channel values (that's what we meant by 'static ranking' and\n'undirected graph'). So it could have been anything.\nChannel capacities, current values and fees will be key parameters of a future\n'dynamic ranking' test, but we didn't think too much about it yet. Maybe normal\ndistributions? If you have already researched the subject or have any insights\nplease let us know :-)\n\nCheers,\n\nPierre"
            },
            {
                "author": "Gary Mulder",
                "date": "2016-09-22T20:00:30",
                "message_text_only": "On 20 September 2016 at 19:47, Olaoluwa Osuntokun <laolu32 at gmail.com> wrote:\n\n> Hi, y'all\n>\n> Excellent work!!\n>\n> As you noted our, Flare paper as currently written only includes a series\n> of\n> simulations of various topologies/parameters which are then extrapolated to\n> larger network sizes. The logical next step would be to deploy a\n> proto-implementation within a live testbed with real latencies,\n> preferential\n> attachment, etc. I'm thrilled that y'all went ahead getting your hands\n> dirty\n> to gauge the real-word feasibility of our scheme.\n>\n\nThere's modules in Linux iptables that allows you to simulate packet loss\nand latencies. While any simulation by definition will not be as good as a\nreal-world test, a lot of negative (i.e. failure) testing can be done in\nthe cloud with much faster test / debug / fix cycle times than in a\nreal-world deployment.\n\nHappy to help with iptables config. if there is interest...\n\nRegards,\nGary\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20160922/66ebecb8/attachment.html>"
            },
            {
                "author": "Viacheslav Zhygulin",
                "date": "2016-09-23T14:10:19",
                "message_text_only": "Hey Pierre,\n\nI have a question regarding your tests.\n\nYou said:\n\"The main difference between our implementation and the original paper \nis in\nthe way nodes communicate with each other. In the paper, an assumption \nis\nmade that \"there exists a way for any two nodes in LN to communicate \nwith\neach other, perhaps with some prior setup (such as a distributed \nhashtable\noverlay maintained among LN nodes that would store mapping of node\nidentifiers to their current IP addresses)\". In our implementation there \nis\nno DHT: inter-nodes communication only occurs on existing open channels,\nand all routing-related messages are onion-encrypted and routed \nthemselves,\njust like htlc messages. So a node can't talk directly to a node it \ndoesn't\nalready have a route to\"\n\nOn the other hand, you said:\n\"The route selection has also been simplified. Whereas Flare proposed a \n3\nsteps process:\n(a) sender uses its own routing table\n(b) sender requests receiver's table and uses a combination of the two\ntables\n(c) sender iterates over nodes it knows that are close to the receiver, \nand\nrequest their tables\nin our test we only did (b), which is a strong tradeoff. It means that \nthe\ntime allowed to find a route is short and predictable, but it is very\nsuboptimal in terms of probability of finding a route.\"\n\nSo, my question is: how sender requests receiver's routing table, if \nsender cannot communicate to receiver?\n\nThank you for the answer in advance!\n\nBest,\n\n\n-- \nViacheslav Zhygulin\n\n\n-- \nTHIS COMMUNICATION AND ANY ATTACHMENTS MAY CONTAIN CONFIDENTIAL INFORMATION OF THE SENDER. ALL UNAUTHORIZED USE, DISCLOSURE OR DISTRIBUTION IS PROHIBITED. IF YOU ARE NOT THE INTENDED RECIPIENT, PLEASE NOTIFY THE SENDER IMMEDIATELY AND DESTROY ALL COPIES OF THIS COMMUNICATION. THANK YOU."
            },
            {
                "author": "Pierre",
                "date": "2016-09-27T09:07:59",
                "message_text_only": "Hi Viacheslav,\n\n> So, my question is: how sender requests receiver's routing table, if sender cannot communicate to receiver?\nIt is generally accepted that the transmission of a payment request\n(amount + H) from a payee to a payer will happen off-band, maybe\nthrough some kind of QR code on a webpage or something similar. We\nassumed that we could attach a piece of data (the payee's routing\ntable) to this request. So you are right: this can be seen as a direct\none-way communication, between the receiver and the sender.\n\nCheers,\n\nPierre"
            }
        ],
        "thread_summary": {
            "title": "Testing a Flare-like routing implementation on 2500 AWS nodes",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Fabrice Drouin",
                "Pierre",
                "Viacheslav Zhygulin",
                "Olaoluwa Osuntokun",
                "Gary Mulder"
            ],
            "messages_count": 7,
            "total_messages_chars_count": 25470
        }
    }
]