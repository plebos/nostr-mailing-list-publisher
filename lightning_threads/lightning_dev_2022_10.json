[
    {
        "title": "[Lightning-dev] Watchtower-Free Lightning Channels For Casual Users",
        "thread_messages": [
            {
                "author": "jlspc",
                "date": "2022-10-03T16:55:35",
                "message_text_only": "This is the first in a series of posts on ideas to improve the usability\nand scalability of the Lightning Network. This post presents a new channel\nprotocol that allows casual users to send and receive Lightning payments\nwithout having to meet onerous availability requirements or use a\nwatchtower service. This new Watchtower-Free (WF) protocol can also be\nused to simplify the reception of Lightning payments for casual users. No\nchange to the underlying Bitcoin protocol is required.\n\nA paper with a more complete description of the protocol, including\nfigures, is available [5].\n\nProperties\n==========\n\nThe user-visible properties of the WF protocol can be expressed using\ntwo parameters:\n* I_S: a short time interval (e.g., 10 minutes) for communicating with\n  peers, checking the blockchain, and submitting transactions to the\n  blockchain, and\n* I_L: a long time interval (e.g., 1-3 months).\n\nThe casual user must be online for up to:\n* I_S every I_L (e.g., 10 minutes every 1-3 months) to safeguard the funds\n  in their Lightning channel.\n\nWith the WF protocol, the latency for payments is unchanged from the\ncurrent protocol, but the latency for getting a payment receipt from an\nuncooperative channel partner is increased. In addition, the casual user\nmay have to pay their channel partner for the partner's cost of capital\n(which depends on I_L). If the casual user and their channel partner\nfollow the protocol, the channel can remain off-chain arbitrarily long.\n\nFirst Attempt: Use The Current Lightning Protocol\n=================================================\n\nIn order to motivate the new protocol, first consider what would happen if\na casual user attempted to achieve the above properties with the current\nLightning channel protocol. The casual user would set their\n\"to_self_delay\" (which controls how quickly their channel partner can\nreceive funds from a transaction they put on-chain) and\n\"cltv_expiry_delta\" (which controls the staggering of timeouts between\nsuccessive hops) parameters to values approaching I_L (because the casual\nuser could be unavailable for nearly that long). This would create three\nproblems:\n\n* Problem 1: The casual user's proposed channel partner would likely\n  reject the creation of the channel due to the excessive \"to_self_delay\"\n  value.\n\n* Problem 2: If a channel were created with these parameters, Lightning\n  payments would not be routed through it due to the excessive\n  \"cltv_expiry_delta\" value.\n\n* Problem 3: If a channel were created with these parameters and if the\n  casual user sent a payment on that channel, their partner could have to\n  go on-chain in order to pull the payment from the casual user. In\n  particular, the casual user could be offline for nearly I_L (e.g., 1-3\n  months) when their partner receives the receipt, thus forcing their\n  partner to go on-chain to receive payment before the expiry of the\n  associated HTLC.\n\nThe WF Protocol\n===============\n\nThe WF protocol solves these problems by modifying the Lightning protocol\nas follows:\n\n* Problem 1 is solved by having the casual user pre-pay their channel\n  partner for the cost of the partner's capital that's tied up in the\n  channel due to the very large \"to_self_delay\" value. This pre-payment is\n  included in the initial channel state and is updated at least once every\n  I_L to reflect the additional cost of capital due to the partner not yet\n  going on-chain.\n\n* Problem 2 is solved by allowing casual users to designate themselves as\n  Casual-Lightning-Users (CLUs), while the remaining users are\n  Dedicated-Lightning-Users (DLUs). CLUs can only partner with DLUs to\n  open channels, such channels must be unannounced, and CLUs must not\n  route (as opposed to send or receive) payments. These constraints fit\n  naturally with the desires of casual users who want to send and receive\n  their Lightning payments, but not route payments for others. Support for\n  CLUs is analogous to support for SPV (Simplified-Payment-Verification)\n  nodes in Bitcoin.\n\n* Problem 3 is solved by modifying both users' Commitment transactions in\n  the channel that sends the payment so the CLU can be offline for nearly\n  I_L without forcing their DLU partner to go on-chain. A simple approach\n  would be to delay the expiry of the HTLC for each payment in the sending\n  channel by I_L. This approach works, but it has the downside of delaying\n  (by I_L) the CLU's ability to force production of a payment receipt. A\n  better approach is to add a relative delay before the CLU can time out\n  the HTLC output of a Commitment transaction, thus enabling the DLU to\n  safely stay off-chain even after the expiry of the HTLC. That's the\n  approach taken here.\n\nLet Alice be a CLU who shares a channel with DLU Bob. Bob sets his channel\nparameters as he would in the current Lightning protocol, while Alice sets\nher \"to_self_delay\" parameter (controlling Bob's payments to himself) to\nI_L greater than it would be in the current Lightning protocol. Consider\nthe case where Alice sends a Lightning payment on the channel she shares\nwith Bob.\n\nLet:\n  - eAB denote the expiry for this payment in the channel shared by Alice\n    and Bob,\n  - tsdA denote the \"to_self_delay\" parameter set by Alice, and\n  - tsdB denote the \"to_self_delay\" parameter set by Bob.\n\nThree changes are made relative to the current Lightning protocol:\n  - a relative delay of tsdB is enforced before Alice can spend the HTLC\n    output for this payment in either Commitment transaction,\n  - after eAB, only Alice's (rather than both parties') signature is\n    required to spend the HTLC output in Alice's Commitment transaction,\n    and that output doesn't need to be spent using an HTLC-timeout\n    transaction that can be revoked (because the relative delay added\n    above guarantees Bob can prevent Alice from spending the HTLC output\n    in a revoked Commitment transaction that she puts on-chain), and\n  - both parties update the channel state off-chain at least once every\n    I_L to reflect Bob's cost of capital, as described above.\n\nThe resulting protocol, with a single payment from Alice outstanding, is\nshown below:\n\n+-+ AB      +----+ A\n|F|----+--->| CC |--->\n+-+    |    |    |\n       .    |    | B\n       .    |    |--->\n       .    +----+\n       |\n       |\n       |              revkeyBi\n       |            +---------->\n       |            |\n       |    +----+  | tsdB & A\n       +--->|C_Ai|--+---------->\n       |    |    |\n       |    |    |    B\n       |    |    |------------->\n       |    |    |\n       |    |    |    revkeyBi\n       |    |    |  +---------->\n       |    |    |  |\n       |    |    |  | tsdB & (eAB) & A\n       |    |    |--+------------------->\n       |    +----+  |\n       |            | Preimage(X) & B\n       |            +------------------->\n       |\n       |\n       |\n       |              revkeyAi\n       |            +---------->\n       |            |\n       |    +----+  | tsdA & B\n       +--->|C_Bi|--+---------->\n       |    |    |\n       |    |    |    A\n       |    |    |------------->\n       |    |    |\n       |    |    |    revkeyAi\n       |    |    |  +---------->\n       .    |    |  |\n       .    |    |  | tsdB & (eAB) & A              revkeyAi\n       .    |    |--+------------------->         +---------->\n       |    +----+  |                             |\n       |            | Preimage(X) & AB   +-----+  | tsdA & B\n       V            +------------------->|Hs_Bi|--+---------->\n                                         +-----+\n\nwhere:\nF is the Funding transaction,\nCC is the Cooperative Close transaction,\nC_Ai is Alice's Commitment transaction for state i,\nC_Bi is Bob's Commitment transaction for state i, and\nHs_Bi is Bob's HTLC-success transaction for state i.\n\nThe F transaction is on-chain, while the remaining transactions are\noff-chain during normal protocol operation.\n\nRequirements for output cases are as follows:\nA: Alice's signature,\nB: Bob's signature,\nAB: Alice's and Bob's signatures,\nrevkeyAi: a signature using a revocation key that Alice can use to revoke\n          Bob's state i transaction,\nrevkeyBi: a signature using a revocation key that Bob can use to revoke\n          Alice's state i transaction,\ntsdA: a relative delay equal to Alice's to_self_delay parameter,\ntsdB: a relative delay equal to Bob's to_self_delay parameter,\n(eAB): an absolute timelock equal to the expiry of the outstanding HTLC\n       offered by Alice, and\nPreimage(X): the hash preimage of X.\n\nOnce Bob knows Preimage(X), he sends Preimage(X) to Alice and attempts to\nupdate both parties' Commitment transactions to show payment of the HTLC.\nIf he has spent I_L time unsuccessfully trying to update those Commitment\ntransactions, he can submit his Commitment and HTLC-success transactions\nto the blockchain. If at any point he sees Alice's Commitment transaction\non-chain, he stops trying to update the Commitment transactions off-chain\nand he puts his transaction that reveals Preimage(X) and spends the HTLC\noutput in her Commitment transaction on-chain as soon as possible.\n\nAlice implements the WF channel protocol as she would the current\nLightning channel protocol, except:\n - she can choose to be intentionally unavailable, provided she is\n   available (or at least not intentionally unavailable) for at least I_S\n   every I_L (to update her pre-payment for Bob's cost of capital and to\n   revoke any old transactions put on-chain by Bob), and\n - she does not put her Commitment transaction on-chain until she has\n   been available (or at least not intentionally unavailable) for at least\n   a grace period of G following the expiry of her offered HTLC (where G\n   is the same grace period as is used in the current Lightning protocol\n   and G <= I_S).\n\nCorrectness\n===========\n\nWhen Alice sends a payment on the channel she shares with Bob, the WF\nprotocol matches the Lightning protocol except the parties stay off-chain\nlonger with the WF protocol (to accommodate Alice's intentional\nunavailability). Staying off-chain longer is safe for Alice, as she\noriginated the payment and thus does not have to time out the HTLC at any\nspecific time in order receive payment in an earlier hop. Staying\noff-chain longer is also safe for Bob, because whenever Alice's (or Bob's)\nCommitment transaction is put on-chain, the tsdB relative delay before\nAlice can time out the HTLC output is long enough to allow Bob to put his\ntransaction on-chain that takes payment for the HTLC.\n\nFinally, the WF protocol requires that Alice and Bob stay off-chain long\nenough to guarantee that Alice will be available (or at least not\nintentionally unavailable) for at least G, which is sufficient for both\nparties to update the channel state off-chain. As a result, if both\nparties follow the protocol, the channel will remain off-chain despite\nAlice's intentional unavailability.\n\nA more detailed proof of correctness is given in the paper [5].\n\nOne-Shot Receives\n=================\n\nWhile eliminating watchtowers is helpful for casual users, the protocol\nfor receiving Lightning payments could still be awkward for such users.\nWith the current Lightning protocol, when a user receives a payment and\ntheir channel partner is unresponsive, the user must submit their\nCommitment and HTLC-success transactions to the blockchain. However, if\ntheir partner's conflicting Commitment transaction wins the race and is\nincluded in the blockchain, the user then has to submit a different\ntransaction that reveals the HTLC's secret and spends the HTLC output in\ntheir partner's Commitment transaction. The requirement to wait and check\nthe blockchain for the winning Commitment transaction (which might not be\ndetermined until multiple blocks have been added to the blockchain) is\nawkward for a casual user. It would be far preferable if the casual user\ncould always receive a payment by performing a sequence of off-chain\nmessage exchanges and at most one submission to the blockchain. A protocol\nthat has this property will be said to support \"one-shot receives\".\n\nThe WF protocol can be made to support one-short receives (and to simplify\nthe process of getting a receipt) for CLU Alice by making the following\nchange whenever a new Commitment transaction for DLU Bob is signed by\nAlice:\n - if Bob has one or more outstanding HTLCs offered to Alice, the\n   nLocktime field of Bob's Commitment transaction is set to the expiry of\n   the earliest such HTLC,\n - otherwise, the nLocktime field of Bob's Commitment transaction is set\n   to I_L in the future (relative to when Bob's Commitment transaction is\n   signed by Alice).\n\nBefore examining how this change supports one-shot receives, it's\nimportant to resolve a technical issue. In the current Lightning protocol,\nthe nLocktime field in the Commitment transaction provides 24 bits of the\nchannel's state number in order to allow efficient revocation of old\non-chain Commitments (with the remaining 24 bits being provided by the\nnSequence field of the Commitment transaction's sole input). Because we're\nnow using the nLocktime field to enforce an absolute timelock, those 24\nbits of state number can no longer be encoded in the nLocktime field.\nThere are two solutions to this problem:\n - add a second input to Bob's Commitment transaction that spends a UTXO\n   owned by Bob (the value of which is arbitrary and is refunded to Bob in\n   the Commitment transaction) and use the nSequence field of that input\n   to encode 24 bits of state number, or\n - support only 24-bit state numbers, as 16 million channel states are\n   likely sufficient for most casual users.\n\nIn addition, the following constraints are added in order to guarantee\none-shot receives:\n1. Whenever a new HTLC is offered to Alice, its expiry is set to exactly\n   her min_final_cltv_expiry parameter in the future. This constraint\n   guarantees that new HTLCs have expiries that are monotonically\n   nondecreasing.\n2. Whenever Alice gives Bob a secret for an HTLC, that HTLC has the\n   earliest expiry of all the HTLCs in Alice's current Commitment\n   transaction.\n3. Whenever a new channel state i+1 is created, Alice's partial signature\n   for Bob's Commitment transaction for state i+1 is given to Bob, and the\n   revocation key for Bob's Commitment transaction for state i is given to\n   Alice, before Bob's partial signature for Alice's Commitment\n   transaction for state i+1 is given to Alice.\n\nGiven these constraints and the setting of the nLocktime field in Bob's\nCommitment transaction, Alice can always put her Commitment transaction\non-chain before Bob can put a conflicting current Commitment transaction\non-chain, thus providing one-shot receives. The details are provided in\nthe paper [5].\n\nFinally, it's important to verify that the delay of Bob's Commitment\ntransaction (caused by the setting of its nLocktime field) does not create\nany problems for Bob. First, for HTLCs offered to Alice (that is, payments\nreceived by Alice), the current Lightning protocol requires that Bob wait\nuntil after the expiry of his offered HTLC before he goes on-chain with\nhis Commitment and HTLC-timeout transactions. Therefore, the nLocktime\nfield has no impact on Bob's actions regarding HTLCs offered to Alice.\nSecond, for HTLCs offered by Alice (that is, payments sent by Alice), the\nWF protocol does not force Bob to put his Commitment and associated\nHTLC-success transactions on-chain before any specific time in order\nguarantee the success of any HTLCs. As a result, Bob's ability to force\npayment for HTLCs offered by Alice is unaffected by the nLocktime field in\nhis Commitment transactions. Note that the Lightning protocol does\nrequire Bob to put his Commitment and associated HTLC-success transactions\non-chain by a specific time, which is why the changes described here\ncannot be made to the Lightning protocol to support one-shot receives.\n\nGetting A Payment Receipt\n=========================\n\nConsider again the case where casual user Alice has offered an HTLC to\nBob. At any time after the expiry of the HTLC, if Alice needs to get a\npayment receipt and Bob is uncooperative, Alice can put her Commitment\ntransaction on-chain and then attempt to spend the HTLC output of her\nCommitment transaction tsdB later. As was shown above, she is guaranteed\nto win the race in putting her Commitment transaction on-chain due to the\nnLocktime field in Bob's Commitment transaction. Therefore, she will\neither get her receipt before she is able to spend the HTLC output or she\nwill not have to make her payment (because she succeeded in spending the\nHTLC output). This procedure for getting a payment receipt isn't one-shot\nand may be awkward for casual users. Fortunately, it's only required when\nthere's both a payment dispute (or other need to get a receipt quickly)\nand an uncooperative channel partner.\n\nAsynchronous Payments\n=====================\n\nThe WF protocol gives significant flexibility to when CLUs have to be\nonline, but it still requires that the sender and receiver are both online\nsimultaneously. This requirement can be eliminated by keeping the relative\ndelay but removing the absolute delay in Alice's transaction that times\nout an HTLC for a payment that she initiates. The details are given in the\npaper [5].\n\nRelated Work\n============\n\nThe protocol presented here is based extensively on previously-published\nwork, namely the Poon-Dryja Lightning channel protocol [1] and the BOLT\nspecifications [2]. The asynchronous payments protocol is based on\nCorallo's proposal for sending tips to an offline receiver [3], but\ndiffers by using only a relative delay in the sender's HTLC.\n\nThe idea of eliminating watchtowers for a casual user by delaying their\npartner's ability to put transactions on-chain was described by Law [4],\nbut the interaction of that delay with HTLCs was not analyzed and that\npaper assumed modifications to the underlying Bitcoin protocol.\n\nConclusions\n===========\n\nThis post presents the idea of dividing users into Casual-Lightning-Users\n(CLUs) that only send and receive payments, and Dedicated-Lightning-Users\n(DLUs) that can also route payments. It gives a new protocol that allows\ncasual users to send and receive Lightning payments in a trust-free manner\nwithout requiring a watchtower service. It also allows CLUs to receive\npayments in a one-shot manner (that is, without having to wait for blocks\nto be added to the blockchain). No changes to the Bitcoin protocol are\nrequired.\n\nThe new protocol does have some disadvantages, such as increasing the cost\nof capital for DLUs that partner with CLUs and increasing the latency for\nCLUs to get payment receipts from uncooperative partners. Hopefully, the\nelimination of watchtowers for casual users, and their ability to do\none-shot receives, will more than make up for these drawbacks.\n\nI'm not an expert in the area, so I might have missed something.\n\nCorrections and comments are greatly appreciated.\n\nRegards,\nJohn\n\nReferences\n==========\n\n[1] Poon and Dryja, The Bitcoin Lightning Network, available at\n    https://lightning.network/lightning-network-paper.pdf.\n[2] BOLT specifications, available at\n    https://github.com/lightningnetwork/lightning-rfc.\n[3] Corallo, A Mobile Lightning User Goes to Pay a Mobile Lightning\n    User..., available at https://lists.linuxfoundation.org/pipermail/lightning-dev/2021-October/003307.html.\n[4] Law, Section 3.6 of Scaling Bitcoin With Inherited IDs, available at\n    https://github.com/JohnLaw2/btc-iids.\n[5] Law, Watchtower-Free Lightning Channels For Casual Users, available at\n    https://github.com/JohnLaw2/ln-watchtower-free.\n\nSent with [Proton Mail](https://proton.me/) secure email.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20221003/ee49228d/attachment-0001.html>"
            },
            {
                "author": "David A. Harding",
                "date": "2022-10-07T22:33:32",
                "message_text_only": "On 2022-10-03 06:55, jlspc via Lightning-dev wrote:\n> The WF Protocol\n> ===============\n\nHi John,\n\nI had difficulty understanding your proposal description here and in \nyour paper[1].  I wonder if others are having the same the same \ndifficulty, so I've tried to reduce it down to just the essential idea \nso you can tell me if I'm understanding correctly and others can \nevaluate it more quickly.  Here I go:\n\nIn a traditional HTLC, the agreement is essentially:\n\n- Setup: Alice has x BTC, an unpublished value y, and the hash digest z \nwhich is hash(y)\n- HTLC success: Alice offers Bob the x BTC, which he can claim at any \ntime if he publishes y satisfying the equation hash(y) == z\n- HTLC failure: Alice can spend the x BTC back to her wallet after some \ntime t has elapsed\n\nIf I understand your modified protocol correctly, the essential modified \nagreement is:\n\n- [Setup the same]\n- [HTLC success the same]\n- HTLC failure: Alice can spend the x BTC back to her wallet by first \ngetting a trigger[2] transaction confirmed onchain, waiting b blocks, \nthen getting the actual spend-back-to-wallet transaction confirmed\n\nBecause the trigger transaction needs to be confirmed for b blocks \nbefore Alice can can spend the money back to her wallet, Bob doesn't \nneed to take any action to lock-in an HTLC Success unless he sees the \ntrigger transaction appear onchain or he expects to be offline for more \nthan b blocks.  This allows Alice to stay offline for as long as Bob can \ntolerate (which goes towards your point of Alice prepaying Bob for that \ntolerance).\n\n[1] \nhttps://raw.githubusercontent.com/JohnLaw2/ln-watchtower-free/main/watchtowerfree10.pdf\n[2] \"Trigger\" transaction is the name given to that type of transaction \nin section 4.2 of the Eltoo paper: https://blockstream.com/eltoo.pdf\n\n> One-Shot Receives\n> =================\n\nI understand the essence of this idea to be simply encumbering dedicated \nuser Bob's commitment transaction with a timelock so that he can't \npublish it until near the time when any HTLCs in it would expire.  \nAlice's version of commitment would be unencumbered, so she could \npublish it any time.\n\n> when a user receives a payment and\n> their channel partner is unresponsive, the user must submit their\n> Commitment and HTLC-success transactions to the blockchain. However, if\n> their partner's conflicting Commitment transaction wins the race and is\n> included in the blockchain, the user then has to submit a different\n> transaction that reveals the HTLC's secret and spends the HTLC output \n> in\n> their partner's Commitment transaction. The requirement to wait and \n> check\n> the blockchain for the winning Commitment transaction (which might not \n> be\n> determined until multiple blocks have been added to the blockchain) is\n> awkward for a casual user.\n\nAlthough your proposal may address this in the normal case, I think it \ndoesn't address the pathological case where honest casual user Alice \nbroadcasts the latest commitment transaction but her channel partner, \nmalicious dedicated user Mallory, broadcasts an older revoked commitment \ntransaction.  Because Mallory's revoked commitment transaction is older, \nits timelock has expired, so it can win the race against Alice's latest \ncommitment transaction.\n\nTo become aware of this situation and to broadcast a penalty transaction \nwithin the necessary time limit, Alice still needs to monitor the block \nchain.  If Alice still needs to monitor the block chain in any case, \nthis proposed change doesn't eliminate the underlying problem of onerous \nmonitoring as far as I can tell.\n\nThanks as always for the innovative thinking!,\n\n-Dave"
            },
            {
                "author": "jlspc",
                "date": "2022-10-12T00:06:06",
                "message_text_only": "Hi Dave,\n\nThanks for reading and for doing a better job of explaining the ideas than I did!\n\nResponses are in-line below:\n\n> Hi John,\n> \n> I had difficulty understanding your proposal description here and in \n> your paper[1].  I wonder if others are having the same the same \n> difficulty, so I've tried to reduce it down to just the essential idea \n> so you can tell me if I'm understanding correctly and others can \n> evaluate it more quickly.  Here I go:\n> \n> In a traditional HTLC, the agreement is essentially:\n> \n> - Setup: Alice has x BTC, an unpublished value y, and the hash digest z \n> which is hash(y)\n> - HTLC success: Alice offers Bob the x BTC, which he can claim at any \n> time if he publishes y satisfying the equation hash(y) == z\n> - HTLC failure: Alice can spend the x BTC back to her wallet after some \n> time t has elapsed\n> \n> If I understand your modified protocol correctly, the essential modified \n> agreement is:\n> \n> - [Setup the same]\n> - [HTLC success the same]\n> - HTLC failure: Alice can spend the x BTC back to her wallet by first \n> getting a trigger[2] transaction confirmed onchain, waiting b blocks, \n> then getting the actual spend-back-to-wallet transaction confirmed\n> \n> Because the trigger transaction needs to be confirmed for b blocks \n> before Alice can can spend the money back to her wallet, Bob doesn't \n> need to take any action to lock-in an HTLC Success unless he sees the \n> trigger transaction appear onchain or he expects to be offline for more \n> than b blocks.  This allows Alice to stay offline for as long as Bob can \n> tolerate (which goes towards your point of Alice prepaying Bob for that \n> tolerance).\n\nYes, that's exactly right.\n\nI'd note that the transaction that plays the role of the \"trigger\" transaction is actually just Alice's Commitment transaction, so no new transaction is required.\n\nI'd also note that the parameter \"b\" is exactly Bob's to_self_delay parameter and he has already committed himself to being able to respond to Alice's on-chain transactions within a window of b blocks, so the protocol doesn't put any additional requirements on his monitoring of the blockchain.\n\n> \n> [1] \n> <a href=\"https://raw.githubusercontent.com/JohnLaw2/ln-watchtower-free/main/watchtowerfree10.pdf\">https://raw.githubusercontent.com/JohnLaw2/ln-watchtower-free/main/watchtowerfree10.pdf</a>\n> [2] \"Trigger\" transaction is the name given to that type of transaction \n> in section 4.2 of the Eltoo paper: <a href=\"https://blockstream.com/eltoo.pdf\">https://blockstream.com/eltoo.pdf</a>\n> \n> > One-Shot Receives\n> > =================\n> >\n> I understand the essence of this idea to be simply encumbering dedicated \n> user Bob's commitment transaction with a timelock so that he can't \n> publish it until near the time when any HTLCs in it would expire.  \n> Alice's version of commitment would be unencumbered, so she could \n> publish it any time.\n\nYes, that's correct.\n\nIn particular, dedicated user Bob can't publish his current Commitment transaction until Alice has put her conflicting current Commitment transaction on-chain (assuming she follows the protocol) or both parties have updated the state off-chain. As a result, Alice doesn't have to worry about the case where she submits her current Commitment and HTLC-success transactions only to later discover that Bob's current Commitment transaction has won the race, thus forcing Alice to then submit her transaction (which is like an HTLC-success transaction but isn't actually called that in the protocol) which reveals the hash preimage and takes payment of the HTLC. Forcing Alice to wait around to see who's current Commitment transaction has won the race seems like an inconvenient requirement for a casual user.\n\nIs my understanding of this issue correct, or can Alice submit her transaction spending the HTLC output of Bob's current Commitment transaction even before he has submitted his current Commitment transaction? I realize this part of the protocol depends on node relay behavior, so it's harder to nail down and reason about than the consensus portion of the protocol. I may not have the correct understanding here, and if my understanding isn't correct, I'd really like to fix it.\n\n> Although your proposal may address this in the normal case, I think it \n> doesn't address the pathological case where honest casual user Alice \n> broadcasts the latest commitment transaction but her channel partner, \n> malicious dedicated user Mallory, broadcasts an older revoked commitment \n> transaction.  Because Mallory's revoked commitment transaction is older, \n> its timelock has expired, so it can win the race against Alice's latest \n> commitment transaction.\n> \n> To become aware of this situation and to broadcast a penalty transaction \n> within the necessary time limit, Alice still needs to monitor the block \n> chain.  If Alice still needs to monitor the block chain in any case, \n> this proposed change doesn't eliminate the underlying problem of onerous \n> monitoring as far as I can tell.\n\nYou're right that Bob (or Mallory) could still broadcast an older revoked transaction, and if that happens, Alice needs to broadcast a penalty transaction within her long interval (I_L) safety parameter which I was suggesting she could set to something like 1-3 months. I felt that being able to shut off her phone and not worry about the Lightning app for all but a short time (say 10 minutes) every few months wasn't overly onerous. Do you feel that casual users wouldn't see it that way?\n\nThe issue of how often Alice needs to monitor the blockchain is based on her setting of her I_L parameter and is really independent of her ability to perform one-shot receives. My understanding is that in the current Lightning protocol, if Alice is receiving a payment and she gives the payment's secret to her partner Bob, and if Bob fails to update the channel state to reflect the payment, Alice has to 1) submit her Commitment and HTLC-success transactions, 2) wait to see if her current Commitment transaction or Bob's current Commitment transaction won the race, and if Bob's current Commitment transaction won, 3) submit her transaction spending Bob's current Commitment transaction's HTLC output. The protocol that supports one-shot receives eliminates steps 2) and 3) above, which seems like a better user interface.\n\nDoes that make sense?\n\nMany thanks,\nJohn\n\n\n\nSent with Proton Mail secure email."
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2022-10-10T15:21:41",
                "message_text_only": "Hey John,\n\nThanks for sharing, this is very interesting.\n\nThere is a good insight here that we can remove the intermediate\nHTLC-timeout transaction for outgoing payments because we are the\norigin of that payment (and thus don't need to quickly claim the\nHTLC on-chain to then relay that failure to a matching incoming HTLC).\n\nMore generally, you have perfectly identified that most of the\ncomplexity of today's transactions come from the need to ensure that\na failing/malicious downstream channel doesn't negatively impact\nhonest upstream channels when relaying payments, and that some of this\ncomplexity can be lifted when nodes don't relay payments.\n\nHowever, my main criticism of your proposal is that liquidity isn't free.\nWhile your improvements are great from the CLU's point of view, I'm not\nsure they're acceptable for the DLU. The main (probably only) job of an\nLSP (DLU in your terminology) is to efficiently allocate their liquidity.\nIn order to do so, they must be able to quickly move liquidity from where\nit's unused to where it may be better used. That means closely watching\nthe demand for block space and doing on-chain transactions when fees are\nlow (to open/close channels, splice funds in/out [1], make peer swaps [2],\netc). With your proposal, DLUs won't be able to quickly move liquidity\naround, so the only way to make up for this is to charge the CLU for the\nloss of expected revenue. I'm afraid that the amount DLUs would need to\ncharge CLUs will be prohibitively expensive for most CLUs.\n\nI'm curious to get your feedback on that point.\n\nThanks again for sharing, and for the inherited IDs [3] proposal as well!\n\nBastien\n\n[1] https://github.com/lightning/bolts/pull/863\n[2] https://www.peerswap.dev/\n[3] https://github.com/JohnLaw2/btc-iids\n\n\nLe lun. 3 oct. 2022 \u00e0 18:55, jlspc via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> a \u00e9crit :\n>\n> This is the first in a series of posts on ideas to improve the usability\n> and scalability of the Lightning Network. This post presents a new channel\n> protocol that allows casual users to send and receive Lightning payments\n> without having to meet onerous availability requirements or use a\n> watchtower service. This new Watchtower-Free (WF) protocol can also be\n> used to simplify the reception of Lightning payments for casual users. No\n> change to the underlying Bitcoin protocol is required.\n>\n> A paper with a more complete description of the protocol, including\n> figures, is available [5].\n>\n> Properties\n> ==========\n>\n> The user-visible properties of the WF protocol can be expressed using\n> two parameters:\n> * I_S: a short time interval (e.g., 10 minutes) for communicating with\n>   peers, checking the blockchain, and submitting transactions to the\n>   blockchain, and\n> * I_L: a long time interval (e.g., 1-3 months).\n>\n> The casual user must be online for up to:\n> * I_S every I_L (e.g., 10 minutes every 1-3 months) to safeguard the funds\n>   in their Lightning channel.\n>\n> With the WF protocol, the latency for payments is unchanged from the\n> current protocol, but the latency for getting a payment receipt from an\n> uncooperative channel partner is increased. In addition, the casual user\n> may have to pay their channel partner for the partner's cost of capital\n> (which depends on I_L). If the casual user and their channel partner\n> follow the protocol, the channel can remain off-chain arbitrarily long.\n>\n> First Attempt: Use The Current Lightning Protocol\n> =================================================\n>\n> In order to motivate the new protocol, first consider what would happen if\n> a casual user attempted to achieve the above properties with the current\n> Lightning channel protocol. The casual user would set their\n> \"to_self_delay\" (which controls how quickly their channel partner can\n> receive funds from a transaction they put on-chain) and\n> \"cltv_expiry_delta\" (which controls the staggering of timeouts between\n> successive hops) parameters to values approaching I_L (because the casual\n> user could be unavailable for nearly that long). This would create three\n> problems:\n>\n> * Problem 1: The casual user's proposed channel partner would likely\n>   reject the creation of the channel due to the excessive \"to_self_delay\"\n>   value.\n>\n> * Problem 2: If a channel were created with these parameters, Lightning\n>   payments would not be routed through it due to the excessive\n>   \"cltv_expiry_delta\" value.\n>\n> * Problem 3: If a channel were created with these parameters and if the\n>   casual user sent a payment on that channel, their partner could have to\n>   go on-chain in order to pull the payment from the casual user. In\n>   particular, the casual user could be offline for nearly I_L (e.g., 1-3\n>   months) when their partner receives the receipt, thus forcing their\n>   partner to go on-chain to receive payment before the expiry of the\n>   associated HTLC.\n>\n> The WF Protocol\n> ===============\n>\n> The WF protocol solves these problems by modifying the Lightning protocol\n> as follows:\n>\n> * Problem 1 is solved by having the casual user pre-pay their channel\n>   partner for the cost of the partner's capital that's tied up in the\n>   channel due to the very large \"to_self_delay\" value. This pre-payment is\n>   included in the initial channel state and is updated at least once every\n>   I_L to reflect the additional cost of capital due to the partner not yet\n>   going on-chain.\n>\n> * Problem 2 is solved by allowing casual users to designate themselves as\n>   Casual-Lightning-Users (CLUs), while the remaining users are\n>   Dedicated-Lightning-Users (DLUs). CLUs can only partner with DLUs to\n>   open channels, such channels must be unannounced, and CLUs must not\n>   route (as opposed to send or receive) payments. These constraints fit\n>   naturally with the desires of casual users who want to send and receive\n>   their Lightning payments, but not route payments for others. Support for\n>   CLUs is analogous to support for SPV (Simplified-Payment-Verification)\n>   nodes in Bitcoin.\n>\n> * Problem 3 is solved by modifying both users' Commitment transactions in\n>   the channel that sends the payment so the CLU can be offline for nearly\n>   I_L without forcing their DLU partner to go on-chain. A simple approach\n>   would be to delay the expiry of the HTLC for each payment in the sending\n>   channel by I_L. This approach works, but it has the downside of delaying\n>   (by I_L) the CLU's ability to force production of a payment receipt. A\n>   better approach is to add a relative delay before the CLU can time out\n>   the HTLC output of a Commitment transaction, thus enabling the DLU to\n>   safely stay off-chain even after the expiry of the HTLC. That's the\n>   approach taken here.\n>\n> Let Alice be a CLU who shares a channel with DLU Bob. Bob sets his channel\n> parameters as he would in the current Lightning protocol, while Alice sets\n> her \"to_self_delay\" parameter (controlling Bob's payments to himself) to\n> I_L greater than it would be in the current Lightning protocol. Consider\n> the case where Alice sends a Lightning payment on the channel she shares\n> with Bob.\n>\n> Let:\n>   - eAB denote the expiry for this payment in the channel shared by Alice\n>     and Bob,\n>   - tsdA denote the \"to_self_delay\" parameter set by Alice, and\n>   - tsdB denote the \"to_self_delay\" parameter set by Bob.\n>\n> Three changes are made relative to the current Lightning protocol:\n>   - a relative delay of tsdB is enforced before Alice can spend the HTLC\n>     output for this payment in either Commitment transaction,\n>   - after eAB, only Alice's (rather than both parties') signature is\n>     required to spend the HTLC output in Alice's Commitment transaction,\n>     and that output doesn't need to be spent using an HTLC-timeout\n>     transaction that can be revoked (because the relative delay added\n>     above guarantees Bob can prevent Alice from spending the HTLC output\n>     in a revoked Commitment transaction that she puts on-chain), and\n>   - both parties update the channel state off-chain at least once every\n>     I_L to reflect Bob's cost of capital, as described above.\n>\n> The resulting protocol, with a single payment from Alice outstanding, is\n> shown below:\n>\n> +-+ AB      +----+ A\n> |F|----+--->| CC |--->\n> +-+    |    |    |\n>        .    |    | B\n>        .    |    |--->\n>        .    +----+\n>        |\n>        |\n>        |              revkeyBi\n>        |            +---------->\n>        |            |\n>        |    +----+  | tsdB & A\n>        +--->|C_Ai|--+---------->\n>        |    |    |\n>        |    |    |    B\n>        |    |    |------------->\n>        |    |    |\n>        |    |    |    revkeyBi\n>        |    |    |  +---------->\n>        |    |    |  |\n>        |    |    |  | tsdB & (eAB) & A\n>        |    |    |--+------------------->\n>        |    +----+  |\n>        |            | Preimage(X) & B\n>        |            +------------------->\n>        |\n>        |\n>        |\n>        |              revkeyAi\n>        |            +---------->\n>        |            |\n>        |    +----+  | tsdA & B\n>        +--->|C_Bi|--+---------->\n>        |    |    |\n>        |    |    |    A\n>        |    |    |------------->\n>        |    |    |\n>        |    |    |    revkeyAi\n>        |    |    |  +---------->\n>        .    |    |  |\n>        .    |    |  | tsdB & (eAB) & A              revkeyAi\n>        .    |    |--+------------------->         +---------->\n>        |    +----+  |                             |\n>        |            | Preimage(X) & AB   +-----+  | tsdA & B\n>        V            +------------------->|Hs_Bi|--+---------->\n>                                          +-----+\n>\n> where:\n> F is the Funding transaction,\n> CC is the Cooperative Close transaction,\n> C_Ai is Alice's Commitment transaction for state i,\n> C_Bi is Bob's Commitment transaction for state i, and\n> Hs_Bi is Bob's HTLC-success transaction for state i.\n>\n> The F transaction is on-chain, while the remaining transactions are\n> off-chain during normal protocol operation.\n>\n> Requirements for output cases are as follows:\n> A: Alice's signature,\n> B: Bob's signature,\n> AB: Alice's and Bob's signatures,\n> revkeyAi: a signature using a revocation key that Alice can use to revoke\n>           Bob's state i transaction,\n> revkeyBi: a signature using a revocation key that Bob can use to revoke\n>           Alice's state i transaction,\n> tsdA: a relative delay equal to Alice's to_self_delay parameter,\n> tsdB: a relative delay equal to Bob's to_self_delay parameter,\n> (eAB): an absolute timelock equal to the expiry of the outstanding HTLC\n>        offered by Alice, and\n> Preimage(X): the hash preimage of X.\n>\n> Once Bob knows Preimage(X), he sends Preimage(X) to Alice and attempts to\n> update both parties' Commitment transactions to show payment of the HTLC.\n> If he has spent I_L time unsuccessfully trying to update those Commitment\n> transactions, he can submit his Commitment and HTLC-success transactions\n> to the blockchain. If at any point he sees Alice's Commitment transaction\n> on-chain, he stops trying to update the Commitment transactions off-chain\n> and he puts his transaction that reveals Preimage(X) and spends the HTLC\n> output in her Commitment transaction on-chain as soon as possible.\n>\n> Alice implements the WF channel protocol as she would the current\n> Lightning channel protocol, except:\n>  - she can choose to be intentionally unavailable, provided she is\n>    available (or at least not intentionally unavailable) for at least I_S\n>    every I_L (to update her pre-payment for Bob's cost of capital and to\n>    revoke any old transactions put on-chain by Bob), and\n>  - she does not put her Commitment transaction on-chain until she has\n>    been available (or at least not intentionally unavailable) for at least\n>    a grace period of G following the expiry of her offered HTLC (where G\n>    is the same grace period as is used in the current Lightning protocol\n>    and G <= I_S).\n>\n> Correctness\n> ===========\n>\n> When Alice sends a payment on the channel she shares with Bob, the WF\n> protocol matches the Lightning protocol except the parties stay off-chain\n> longer with the WF protocol (to accommodate Alice's intentional\n> unavailability). Staying off-chain longer is safe for Alice, as she\n> originated the payment and thus does not have to time out the HTLC at any\n> specific time in order receive payment in an earlier hop. Staying\n> off-chain longer is also safe for Bob, because whenever Alice's (or Bob's)\n> Commitment transaction is put on-chain, the tsdB relative delay before\n> Alice can time out the HTLC output is long enough to allow Bob to put his\n> transaction on-chain that takes payment for the HTLC.\n>\n> Finally, the WF protocol requires that Alice and Bob stay off-chain long\n> enough to guarantee that Alice will be available (or at least not\n> intentionally unavailable) for at least G, which is sufficient for both\n> parties to update the channel state off-chain. As a result, if both\n> parties follow the protocol, the channel will remain off-chain despite\n> Alice's intentional unavailability.\n>\n> A more detailed proof of correctness is given in the paper [5].\n>\n> One-Shot Receives\n> =================\n>\n> While eliminating watchtowers is helpful for casual users, the protocol\n> for receiving Lightning payments could still be awkward for such users.\n> With the current Lightning protocol, when a user receives a payment and\n> their channel partner is unresponsive, the user must submit their\n> Commitment and HTLC-success transactions to the blockchain. However, if\n> their partner's conflicting Commitment transaction wins the race and is\n> included in the blockchain, the user then has to submit a different\n> transaction that reveals the HTLC's secret and spends the HTLC output in\n> their partner's Commitment transaction. The requirement to wait and check\n> the blockchain for the winning Commitment transaction (which might not be\n> determined until multiple blocks have been added to the blockchain) is\n> awkward for a casual user. It would be far preferable if the casual user\n> could always receive a payment by performing a sequence of off-chain\n> message exchanges and at most one submission to the blockchain. A protocol\n> that has this property will be said to support \"one-shot receives\".\n>\n> The WF protocol can be made to support one-short receives (and to simplify\n> the process of getting a receipt) for CLU Alice by making the following\n> change whenever a new Commitment transaction for DLU Bob is signed by\n> Alice:\n>  - if Bob has one or more outstanding HTLCs offered to Alice, the\n>    nLocktime field of Bob's Commitment transaction is set to the expiry of\n>    the earliest such HTLC,\n>  - otherwise, the nLocktime field of Bob's Commitment transaction is set\n>    to I_L in the future (relative to when Bob's Commitment transaction is\n>    signed by Alice).\n>\n> Before examining how this change supports one-shot receives, it's\n> important to resolve a technical issue. In the current Lightning protocol,\n> the nLocktime field in the Commitment transaction provides 24 bits of the\n> channel's state number in order to allow efficient revocation of old\n> on-chain Commitments (with the remaining 24 bits being provided by the\n> nSequence field of the Commitment transaction's sole input). Because we're\n> now using the nLocktime field to enforce an absolute timelock, those 24\n> bits of state number can no longer be encoded in the nLocktime field.\n> There are two solutions to this problem:\n>  - add a second input to Bob's Commitment transaction that spends a UTXO\n>    owned by Bob (the value of which is arbitrary and is refunded to Bob in\n>    the Commitment transaction) and use the nSequence field of that input\n>    to encode 24 bits of state number, or\n>  - support only 24-bit state numbers, as 16 million channel states are\n>    likely sufficient for most casual users.\n>\n> In addition, the following constraints are added in order to guarantee\n> one-shot receives:\n> 1. Whenever a new HTLC is offered to Alice, its expiry is set to exactly\n>    her min_final_cltv_expiry parameter in the future. This constraint\n>    guarantees that new HTLCs have expiries that are monotonically\n>    nondecreasing.\n> 2. Whenever Alice gives Bob a secret for an HTLC, that HTLC has the\n>    earliest expiry of all the HTLCs in Alice's current Commitment\n>    transaction.\n> 3. Whenever a new channel state i+1 is created, Alice's partial signature\n>    for Bob's Commitment transaction for state i+1 is given to Bob, and the\n>    revocation key for Bob's Commitment transaction for state i is given to\n>    Alice, before Bob's partial signature for Alice's Commitment\n>    transaction for state i+1 is given to Alice.\n>\n> Given these constraints and the setting of the nLocktime field in Bob's\n> Commitment transaction, Alice can always put her Commitment transaction\n> on-chain before Bob can put a conflicting current Commitment transaction\n> on-chain, thus providing one-shot receives. The details are provided in\n> the paper [5].\n>\n> Finally, it's important to verify that the delay of Bob's Commitment\n> transaction (caused by the setting of its nLocktime field) does not create\n> any problems for Bob. First, for HTLCs offered to Alice (that is, payments\n> received by Alice), the current Lightning protocol requires that Bob wait\n> until after the expiry of his offered HTLC before he goes on-chain with\n> his Commitment and HTLC-timeout transactions. Therefore, the nLocktime\n> field has no impact on Bob's actions regarding HTLCs offered to Alice.\n> Second, for HTLCs offered by Alice (that is, payments sent by Alice), the\n> WF protocol does not force Bob to put his Commitment and associated\n> HTLC-success transactions on-chain before any specific time in order\n> guarantee the success of any HTLCs. As a result, Bob's ability to force\n> payment for HTLCs offered by Alice is unaffected by the nLocktime field in\n> his Commitment transactions. Note that the Lightning protocol does\n> require Bob to put his Commitment and associated HTLC-success transactions\n> on-chain by a specific time, which is why the changes described here\n> cannot be made to the Lightning protocol to support one-shot receives.\n>\n> Getting A Payment Receipt\n> =========================\n>\n> Consider again the case where casual user Alice has offered an HTLC to\n> Bob. At any time after the expiry of the HTLC, if Alice needs to get a\n> payment receipt and Bob is uncooperative, Alice can put her Commitment\n> transaction on-chain and then attempt to spend the HTLC output of her\n> Commitment transaction tsdB later. As was shown above, she is guaranteed\n> to win the race in putting her Commitment transaction on-chain due to the\n> nLocktime field in Bob's Commitment transaction. Therefore, she will\n> either get her receipt before she is able to spend the HTLC output or she\n> will not have to make her payment (because she succeeded in spending the\n> HTLC output). This procedure for getting a payment receipt isn't one-shot\n> and may be awkward for casual users. Fortunately, it's only required when\n> there's both a payment dispute (or other need to get a receipt quickly)\n> and an uncooperative channel partner.\n>\n> Asynchronous Payments\n> =====================\n>\n> The WF protocol gives significant flexibility to when CLUs have to be\n> online, but it still requires that the sender and receiver are both online\n> simultaneously. This requirement can be eliminated by keeping the relative\n> delay but removing the absolute delay in Alice's transaction that times\n> out an HTLC for a payment that she initiates. The details are given in the\n> paper [5].\n>\n> Related Work\n> ============\n>\n> The protocol presented here is based extensively on previously-published\n> work, namely the Poon-Dryja Lightning channel protocol [1] and the BOLT\n> specifications [2]. The asynchronous payments protocol is based on\n> Corallo's proposal for sending tips to an offline receiver [3], but\n> differs by using only a relative delay in the sender's HTLC.\n>\n> The idea of eliminating watchtowers for a casual user by delaying their\n> partner's ability to put transactions on-chain was described by Law [4],\n> but the interaction of that delay with HTLCs was not analyzed and that\n> paper assumed modifications to the underlying Bitcoin protocol.\n>\n> Conclusions\n> ===========\n>\n> This post presents the idea of dividing users into Casual-Lightning-Users\n> (CLUs) that only send and receive payments, and Dedicated-Lightning-Users\n> (DLUs) that can also route payments. It gives a new protocol that allows\n> casual users to send and receive Lightning payments in a trust-free manner\n> without requiring a watchtower service. It also allows CLUs to receive\n> payments in a one-shot manner (that is, without having to wait for blocks\n> to be added to the blockchain). No changes to the Bitcoin protocol are\n> required.\n>\n> The new protocol does have some disadvantages, such as increasing the cost\n> of capital for DLUs that partner with CLUs and increasing the latency for\n> CLUs to get payment receipts from uncooperative partners. Hopefully, the\n> elimination of watchtowers for casual users, and their ability to do\n> one-shot receives, will more than make up for these drawbacks.\n>\n> I'm not an expert in the area, so I might have missed something.\n>\n> Corrections and comments are greatly appreciated.\n>\n> Regards,\n> John\n>\n> References\n> ==========\n>\n> [1] Poon and Dryja, The Bitcoin Lightning Network, available at\n>     https://lightning.network/lightning-network-paper.pdf.\n> [2] BOLT specifications, available at\n>     https://github.com/lightningnetwork/lightning-rfc.\n> [3] Corallo, A Mobile Lightning User Goes to Pay a Mobile Lightning\n>     User..., available at\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2021-October/003307.html\n.\n> [4] Law, Section 3.6 of Scaling Bitcoin With Inherited IDs, available at\n>     https://github.com/JohnLaw2/btc-iids.\n> [5] Law, Watchtower-Free Lightning Channels For Casual Users, available at\n>     https://github.com/JohnLaw2/ln-watchtower-free.\n>\n>\n>\n>\n> Sent with Proton Mail secure email.\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20221010/f0698834/attachment-0001.html>"
            },
            {
                "author": "jlspc",
                "date": "2022-10-12T00:11:00",
                "message_text_only": "Hey Bastien,\n\nThanks for your reply.\n\nResponses are in-line below:\n\n> Hey John,\n>\n> Thanks for sharing, this is very interesting.\n>\n> There is a good insight here that we can remove the intermediate\n> HTLC-timeout transaction for outgoing payments because we are the\n> origin of that payment (and thus don't need to quickly claim the\n> HTLC on-chain to then relay that failure to a matching incoming HTLC).\n>\n> More generally, you have perfectly identified that most of the\n> complexity of today's transactions come from the need to ensure that\n> a failing/malicious downstream channel doesn't negatively impact\n> honest upstream channels when relaying payments, and that some of this\n> complexity can be lifted when nodes don't relay payments.\n\nThanks!\n\n> However, my main criticism of your proposal is that liquidity isn't free.\n> While your improvements are great from the CLU's point of view, I'm not\n> sure they're acceptable for the DLU. The main (probably only) job of an\n> LSP (DLU in your terminology) is to efficiently allocate their liquidity.\n> In order to do so, they must be able to quickly move liquidity from where\n> it's unused to where it may be better used. That means closely watching\n> the demand for block space and doing on-chain transactions when fees are\n> low (to open/close channels, splice funds in/out [1], make peer swaps [2],\n> etc). With your proposal, DLUs won't be able to quickly move liquidity\n> around, so the only way to make up for this is to charge the CLU for the\n> loss of expected revenue. I'm afraid that the amount DLUs would need to\n> charge CLUs will be prohibitively expensive for most CLUs.\n>\n> I'm curious to get your feedback on that point.\n\nI really appreciate your insight here. I'm just an interested observer who doesn't have experience with creating and deploying Lightning nodes, so I'm sure you have a better understanding of the current costs and trade-offs than I do.\n\nMy understanding of the current Lightning protocol is that users specify a to_self_delay safety parameter which is typically about 2 weeks and that they pay for routing, but not for their partner's cost-of-capital. Is that correct?\n\nIf it is, then when a dedicated user (DLU) partners with a casual user (CLU), the DLU can only move liquidity to another Lightning channel by either:\n1) getting the CLU to sign a cooperative close transaction that enables (or directly implements) the desired movement of funds, or\n2) putting a non-cooperative close transaction on-chain and waiting approximately 2 weeks (based on the to_self_delay parameter set by the CLU) before moving the liquidity.\n\nIn contrast, with the Watchtower-Free (WF) protocol, the DLU could only move liquidity to another Lightning channel by either:\n1) getting the CLU to sign a cooperative close transaction that enables (or directly implements), the desired movement of funds, or\n2) putting a non-cooperative close transaction on-chain and waiting approximately 1-3 months (based on the I_L parameter set by the CLU) before moving the liquidity.\nIn case 1), it would make sense for the DLU to refund the remaining portion of CLU's cost-of-capital pre-payment to the CLU, as that capital is now being made available to the DLU. This was not proposed in the paper, but it should probably be added.\n\nWith this change (namely refunding the remainder of the cost-of-capital pre-payment), it seems like the only disadvantage of the WF protocol to the DLU is the larger delay (1-3 months vs. 2 weeks). Do you feel increasing the delay from 2 weeks to 1 month is prohibitive?\n\nMy intuition is that in the long run, the cost of bitcoin capital will be very low, as it is an inherently deflationary monetary unit (and thus its value should increase with time). If this is correct, the long term cost-of-capital charges should be very low.\n\nWhat are your thoughts on this?\n\nThanks,\nJohn\n\n> Thanks again for sharing, and for the inherited IDs [3] proposal as well!\n>\n> Bastien\n>\n> [1] <a href=\"https://github.com/lightning/bolts/pull/863\">https://github.com/lightning/bolts/pull/863</a>\n> [2] <a href=\"https://www.peerswap.dev/\">https://www.peerswap.dev/</a>\n> [3] <a href=\"https://github.com/JohnLaw2/btc-iids\">https://github.com/JohnLaw2/btc-iids</a>\n\nSent with [Proton Mail](https://proton.me/) secure email.\n\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20221012/f0ea0daa/attachment-0001.html>"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2022-10-24T09:50:36",
                "message_text_only": "Hi John,\n\n> My understanding of the current Lightning protocol is that users specify\na to_self_delay safety parameter which is typically about 2 weeks and that\nthey pay for routing, but not for their partner's cost-of-capital. Is that\ncorrect?\n>\n> If it is, then when a dedicated user (DLU) partners with a casual user\n(CLU), the DLU can only move liquidity to another Lightning channel by\neither:\n> 1) getting the CLU to sign a cooperative close transaction that enables\n(or directly implements) the desired movement of funds, or\n> 2) putting a non-cooperative close transaction on-chain and waiting\napproximately 2 weeks (based on the to_self_delay parameter set by the CLU)\nbefore moving the liquidity.\n\nThat's correct, but that's something we intend to change to let LSPs\nre-allocate their liquidity more frequently and more efficiently.\n\nWe don't have a fully specified proposal yet, but by leveraging a\nmechanism similar to splicing [1], mobile users would pre-sign a\ntransaction that keeps the channel open, but lets the LSP get their\nbalance (or part of it) out non-interactively. This would be used by\nLSPs if the user isn't using their channel actively enough and the LSP\nis low on available liquidity for other, more active users.\n\nThis transaction would need to be revokable and must be delayed, since\nwe still need the user to be able to punish malicious LSPs, but ideally\n(from the LSP's point of view) that delay should be at most 1 week, which\nforces users to regularly check the blockchain (which isn't ideal).\n\nIt really is a trade-off to be able to lower the fees LSPs make users\npay for liquidity, because LSPs know they can move it cheaply when it\nbecomes necessary. I can see a future where users chose their trade-off:\npay more to be able to go offline for longer periods or pay less but\ncheck the blockchain regularly. The same LSP could offer both features,\nif they're able to price them correctly (which isn't trivial).\n\n> My intuition is that in the long run, the cost of bitcoin capital will be\nvery low, as it is an inherently deflationary monetary unit (and thus its\nvalue should increase with time). If this is correct, the long term\ncost-of-capital charges should be very low.\n\nI'm not convinced by that...even though the value of capital increases\nwith time, liquidity providers will compete to earn more return on their\nlocked capital. If one liquidity provider is able to use their capital\nmore efficiently than another, they will be able to offer lower prices\nto their customers to a point that isn't economically viable for the\nother, less efficient liquidity provider?\n\nSince lightning doesn't allow any form of fractional reserve, the total\navailable capital needs to be split between all existing users of the\nsystem, which is very inconvenient when trying to onboard a high number\nof new users.\n\nThis is very theoretical though, I have absolutely no idea how those\ndynamics will actually play out, but it will be interesting to watch it\nunfold.\n\nCheers,\nBastien\n\n[1] https://github.com/lightning/bolts/pull/863\n\nLe mer. 12 oct. 2022 \u00e0 02:11, jlspc <jlspc at protonmail.com> a \u00e9crit :\n\n>\n> Hey Bastien,\n>\n> Thanks for your reply.\n>\n> Responses are in-line below:\n>\n> > Hey John,\n> >\n> > Thanks for sharing, this is very interesting.\n> >\n> > There is a good insight here that we can remove the intermediate\n> > HTLC-timeout transaction for outgoing payments because we are the\n> > origin of that payment (and thus don't need to quickly claim the\n> > HTLC on-chain to then relay that failure to a matching incoming HTLC).\n> >\n> > More generally, you have perfectly identified that most of the\n> > complexity of today's transactions come from the need to ensure that\n> > a failing/malicious downstream channel doesn't negatively impact\n> > honest upstream channels when relaying payments, and that some of this\n> > complexity can be lifted when nodes don't relay payments.\n>\n> Thanks!\n>\n> > However, my main criticism of your proposal is that liquidity isn't free.\n> > While your improvements are great from the CLU's point of view, I'm not\n> > sure they're acceptable for the DLU. The main (probably only) job of an\n> > LSP (DLU in your terminology) is to efficiently allocate their liquidity.\n> > In order to do so, they must be able to quickly move liquidity from where\n> > it's unused to where it may be better used. That means closely watching\n> > the demand for block space and doing on-chain transactions when fees are\n> > low (to open/close channels, splice funds in/out [1], make peer swaps [2],\n> > etc). With your proposal, DLUs won't be able to quickly move liquidity\n> > around, so the only way to make up for this is to charge the CLU for the\n> > loss of expected revenue. I'm afraid that the amount DLUs would need to\n> > charge CLUs will be prohibitively expensive for most CLUs.\n> >\n> > I'm curious to get your feedback on that point.\n>\n> I really appreciate your insight here. I'm just an interested observer who doesn't have experience with creating and deploying Lightning nodes, so I'm sure you have a better understanding of the current costs and trade-offs than I do.\n>\n> My understanding of the current Lightning protocol is that users specify a to_self_delay safety parameter which is typically about 2 weeks and that they pay for routing, but not for their partner's cost-of-capital. Is that correct?\n>\n> If it is, then when a dedicated user (DLU) partners with a casual user (CLU), the DLU can only move liquidity to another Lightning channel by either:\n> 1) getting the CLU to sign a cooperative close transaction that enables (or directly implements) the desired movement of funds, or\n> 2) putting a non-cooperative close transaction on-chain and waiting approximately 2 weeks (based on the to_self_delay parameter set by the CLU) before moving the liquidity.\n>\n> In contrast, with the Watchtower-Free (WF) protocol, the DLU could only move liquidity to another Lightning channel by either:\n> 1) getting the CLU to sign a cooperative close transaction that enables (or directly implements), the desired movement of funds, or\n> 2) putting a non-cooperative close transaction on-chain and waiting approximately 1-3 months (based on the I_L parameter set by the CLU) before moving the liquidity.\n> In case 1), it would make sense for the DLU to refund the remaining portion of CLU's cost-of-capital pre-payment to the CLU, as that capital is now being made available to the DLU. This was not proposed in the paper, but it should probably be added.\n>\n> With this change (namely refunding the remainder of the cost-of-capital pre-payment), it seems like the only disadvantage of the WF protocol to the DLU is the larger delay (1-3 months vs. 2 weeks). Do you feel increasing the delay from 2 weeks to 1 month is prohibitive?\n>\n> My intuition is that in the long run, the cost of bitcoin capital will be very low, as it is an inherently deflationary monetary unit (and thus its value should increase with time). If this is correct, the long term cost-of-capital charges should be very low.\n>\n> What are your thoughts on this?\n>\n> Thanks,\n> John\n>\n> > Thanks again for sharing, and for the inherited IDs [3] proposal as well!\n> >\n> > Bastien\n> >\n> > [1] <a href=\"https://github.com/lightning/bolts/pull/863\">https://github.com/lightning/bolts/pull/863</a>\n> > [2] <a href=\"https://www.peerswap.dev/\">https://www.peerswap.dev/</a>\n> > [3] <a href=\"https://github.com/JohnLaw2/btc-iids\">https://github.com/JohnLaw2/btc-iids</a>\n>\n>\n> Sent with Proton Mail <https://proton.me/> secure email.\n>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20221024/c1713a2e/attachment.html>"
            },
            {
                "author": "jlspc",
                "date": "2022-10-31T00:20:30",
                "message_text_only": "Hi Bastien,\n\nThanks for your detailed response.\n\nI see how the use of a pre-signed \"splice\" transaction that allows the dedicated user to obtain some of their capital creates an inherent trade-off between the dedicated user's capital efficiency and the casual user's ability to be offline for an extended period of time.\n\nIn order for the use of a pre-signed splice transaction to be safe, the casual user has to check the blockchain and see the splice transaction before it can be spent by the dedicated user (in order to revoke it if needed). As a result, once the dedicated user decides to splice out some channel funds, the dedicated user cannot access those funds until some time *after* the casual user has been online (provided the protocol is trust-free).\n\nGiven this observation, what if the protocol were changed so that, instead of pre-signing a splice transaction, the casual user always checks in with the dedicated user whenever they check the blockchain? At such a check-in, if the dedicated user wants to splice out some channel funds, they send the casual user a splice transaction which splices out some of the funds to the dedicated user without requiring a to_self delay. The casual user then signs the splice transaction and returns the signature as part of the check-in.\n\nThis approach has a couple potential advantages:\n1) It allows the dedicated user to splice out funds as soon as the casual user comes online (rather than sometime after the casual user comes online, as shown above). Therefore, the use of capital is made more efficient.\n2) It allows the splice transaction to pay fees based on the current fee rate, rather than a pre-calculated fee rate.\n\nIt does have some potential disadvantages:\n1) It requires the casual user to stay online long enough to complete the roundtrip of sending a check-in message to the dedicated user, getting the splice message to sign, signing it, and sending the signature back to the dedicated user.\n2) It assumes that the casual user checks the blockchain themselves, rather than using a watchtower service.\n3) It requires the dedicated user to decide at the time of the check-in whether or not to perform the splice, as the splice transaction cannot be revoked.\n\nIn any case, using a check-in and a non-revokable splice transaction appears to have some advantages in terms of capital efficiency. It's also somewhat more compatible with the WF protocol, as the delay for the dedicated user to obtain spliced-out funds is dependent on the actual time until the casual user next comes online, rather than the worst-case delay until the casual user comes online. This could be a big difference if the casual user is typically online every day, but doesn't want the burden of having to be online every day (or every week). I'm interested in your thoughts on this.\n\nFinally, I understand that the ability to quickly splice out channel funds to improve capital efficiency is critical in the current environment. However, if we eventually get to the point where the blockchain is highly-contested and fees are high, it may no longer be worth paying the fees to put a splice transaction on-chain unless a large amount of capital is at stake for a long period of time.\n\nRegards,\nJohn\n\nSent with [Proton Mail](https://proton.me/) secure email.\n\n------- Original Message -------\nOn Monday, October 24th, 2022 at 2:50 AM, Bastien TEINTURIER <bastien at acinq.fr> wrote:\n\n> Hi John,\n>\n>> My understanding of the current Lightning protocol is that users specify a to_self_delay safety parameter which is typically about 2 weeks and that they pay for routing, but not for their partner's cost-of-capital. Is that correct?\n>>\n>> If it is, then when a dedicated user (DLU) partners with a casual user (CLU), the DLU can only move liquidity to another Lightning channel by either:\n>> 1) getting the CLU to sign a cooperative close transaction that enables (or directly implements) the desired movement of funds, or\n>> 2) putting a non-cooperative close transaction on-chain and waiting approximately 2 weeks (based on the to_self_delay parameter set by the CLU) before moving the liquidity.\n>\n> That's correct, but that's something we intend to change to let LSPs\n> re-allocate their liquidity more frequently and more efficiently.\n>\n> We don't have a fully specified proposal yet, but by leveraging a\n> mechanism similar to splicing [1], mobile users would pre-sign a\n> transaction that keeps the channel open, but lets the LSP get their\n> balance (or part of it) out non-interactively. This would be used by\n> LSPs if the user isn't using their channel actively enough and the LSP\n> is low on available liquidity for other, more active users.\n>\n> This transaction would need to be revokable and must be delayed, since\n> we still need the user to be able to punish malicious LSPs, but ideally\n> (from the LSP's point of view) that delay should be at most 1 week, which\n> forces users to regularly check the blockchain (which isn't ideal).\n>\n> It really is a trade-off to be able to lower the fees LSPs make users\n> pay for liquidity, because LSPs know they can move it cheaply when it\n> becomes necessary. I can see a future where users chose their trade-off:\n> pay more to be able to go offline for longer periods or pay less but\n> check the blockchain regularly. The same LSP could offer both features,\n> if they're able to price them correctly (which isn't trivial).\n>\n>> My intuition is that in the long run, the cost of bitcoin capital will be very low, as it is an inherently deflationary monetary unit (and thus its value should increase with time). If this is correct, the long term cost-of-capital charges should be very low.\n>\n> I'm not convinced by that...even though the value of capital increases\n> with time, liquidity providers will compete to earn more return on their\n> locked capital. If one liquidity provider is able to use their capital\n> more efficiently than another, they will be able to offer lower prices\n> to their customers to a point that isn't economically viable for the\n> other, less efficient liquidity provider?\n>\n> Since lightning doesn't allow any form of fractional reserve, the total\n> available capital needs to be split between all existing users of the\n> system, which is very inconvenient when trying to onboard a high number\n> of new users.\n>\n> This is very theoretical though, I have absolutely no idea how those\n> dynamics will actually play out, but it will be interesting to watch it\n> unfold.\n>\n> Cheers,\n> Bastien\n>\n> [1] https://github.com/lightning/bolts/pull/863\n>\n> Le mer. 12 oct. 2022 \u00e0 02:11, jlspc <jlspc at protonmail.com> a \u00e9crit :\n>\n>> Hey Bastien,\n>>\n>> Thanks for your reply.\n>>\n>> Responses are in-line below:\n>>\n>>> Hey John,\n>>>\n>>> Thanks for sharing, this is very interesting.\n>>>\n>>> There is a good insight here that we can remove the intermediate\n>>> HTLC-timeout transaction for outgoing payments because we are the\n>>> origin of that payment (and thus don't need to quickly claim the\n>>> HTLC on-chain to then relay that failure to a matching incoming HTLC).\n>>>\n>>> More generally, you have perfectly identified that most of the\n>>> complexity of today's transactions come from the need to ensure that\n>>> a failing/malicious downstream channel doesn't negatively impact\n>>> honest upstream channels when relaying payments, and that some of this\n>>> complexity can be lifted when nodes don't relay payments.\n>>\n>> Thanks!\n>>\n>>> However, my main criticism of your proposal is that liquidity isn't free.\n>>> While your improvements are great from the CLU's point of view, I'm not\n>>> sure they're acceptable for the DLU. The main (probably only) job of an\n>>> LSP (DLU in your terminology) is to efficiently allocate their liquidity.\n>>> In order to do so, they must be able to quickly move liquidity from where\n>>> it's unused to where it may be better used. That means closely watching\n>>> the demand for block space and doing on-chain transactions when fees are\n>>> low (to open/close channels, splice funds in/out [1], make peer swaps [2],\n>>> etc). With your proposal, DLUs won't be able to quickly move liquidity\n>>> around, so the only way to make up for this is to charge the CLU for the\n>>> loss of expected revenue. I'm afraid that the amount DLUs would need to\n>>> charge CLUs will be prohibitively expensive for most CLUs.\n>>>\n>>> I'm curious to get your feedback on that point.\n>>\n>> I really appreciate your insight here. I'm just an interested observer who doesn't have experience with creating and deploying Lightning nodes, so I'm sure you have a better understanding of the current costs and trade-offs than I do.\n>>\n>> My understanding of the current Lightning protocol is that users specify a to_self_delay safety parameter which is typically about 2 weeks and that they pay for routing, but not for their partner's cost-of-capital. Is that correct?\n>>\n>> If it is, then when a dedicated user (DLU) partners with a casual user (CLU), the DLU can only move liquidity to another Lightning channel by either:\n>> 1) getting the CLU to sign a cooperative close transaction that enables (or directly implements) the desired movement of funds, or\n>> 2) putting a non-cooperative close transaction on-chain and waiting approximately 2 weeks (based on the to_self_delay parameter set by the CLU) before moving the liquidity.\n>>\n>> In contrast, with the Watchtower-Free (WF) protocol, the DLU could only move liquidity to another Lightning channel by either:\n>> 1) getting the CLU to sign a cooperative close transaction that enables (or directly implements), the desired movement of funds, or\n>> 2) putting a non-cooperative close transaction on-chain and waiting approximately 1-3 months (based on the I_L parameter set by the CLU) before moving the liquidity.\n>> In case 1), it would make sense for the DLU to refund the remaining portion of CLU's cost-of-capital pre-payment to the CLU, as that capital is now being made available to the DLU. This was not proposed in the paper, but it should probably be added.\n>>\n>> With this change (namely refunding the remainder of the cost-of-capital pre-payment), it seems like the only disadvantage of the WF protocol to the DLU is the larger delay (1-3 months vs. 2 weeks). Do you feel increasing the delay from 2 weeks to 1 month is prohibitive?\n>>\n>> My intuition is that in the long run, the cost of bitcoin capital will be very low, as it is an inherently deflationary monetary unit (and thus its value should increase with time). If this is correct, the long term cost-of-capital charges should be very low.\n>>\n>> What are your thoughts on this?\n>>\n>> Thanks,\n>> John\n>>\n>>> Thanks again for sharing, and for the inherited IDs [3] proposal as well!\n>>>\n>>> Bastien\n>>>\n>>> [1] <a href=\"\n>> https://github.com/lightning/bolts/pull/863\n>> \">\n>> https://github.com/lightning/bolts/pull/863\n>> </a>\n>>> [2] <a href=\"\n>> https://www.peerswap.dev/\n>> \">\n>> https://www.peerswap.dev/\n>> </a>\n>>> [3] <a href=\"\n>> https://github.com/JohnLaw2/btc-iids\n>> \">\n>> https://github.com/JohnLaw2/btc-iids\n>> </a>\n>>\n>> Sent with [Proton Mail](https://proton.me/) secure email.\n>>\n>>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20221031/06378968/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Watchtower-Free Lightning Channels For Casual Users",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Bastien TEINTURIER",
                "David A. Harding",
                "jlspc"
            ],
            "messages_count": 7,
            "total_messages_chars_count": 76002
        }
    },
    {
        "title": "[Lightning-dev] Taro: A Taproot Asset Representation Overlay",
        "thread_messages": [
            {
                "author": "Hiroki Gondo",
                "date": "2022-10-07T09:33:08",
                "message_text_only": "Hi Laolu,\n\nI've read Taro's specs, but I'm afraid there's not enough verification of\nthe provenance of the asset UTXOs.\n\nWhen trying to verify the provenance of a given UTXO, it is necessary to\nverify the state transitions of all transaction graphs without gaps from\nthe genesis transaction of the asset to the current location. At some point\nin the transaction, the target UTXO itself may not change (only changes to\nother assets or other UTXOs in the asset tree). It is necessary to prove\nand verify that \u201cthe UTXO has not changed\u201d at that point. As far as I can\nsee, the specs don't mention it.\n\nWithout this validation, asset inflation (double spending) is possible. In\na transaction, there is a UTXO in the input asset tree. If this UTXO does\nnot change in this transaction, it will remain in the output asset tree.\nHowever, if a full copy of this UTXO is illicitly created in the asset tree\nof another output, the asset will be inflated (even duplicating the lowest\nMS-SMT entirely). Both UTXOs will arbitrarily claim to be the same as the\ninput UTXO.\n\nThe proofs for directly proving that a UTXO has not changed are its\ninclusion proof in the input asset tree and its inclusion and non-inclusion\nproofs in each of the output asset trees. However, generating these proofs\nfor every unchanging UTXO present in the input asset tree when a\ntransaction occurs may not be very practical. Instead, it's better to set a\nconstraint that no part of the asset tree except the explicitly changed\nUTXOs should change, and verify that.\n\nPlease let me know if I'm wrong or have overlooked any specs. Also, let me\nknow if there's anything about this that hasn't been mentioned in the specs\nyet.\n\n\u2013\nHiroki Gondo\n\n\n2022\u5e744\u67086\u65e5(\u6c34) 0:06 Olaoluwa Osuntokun <laolu32 at gmail.com>:\n\n> Hi y'all,\n>\n> I'm excited to publicly publish a new protocol I've been working on over\n> the\n> past few months: Taro. Taro is a Taproot Asset Representation Overlay which\n> allows the issuance of normal and also collectible assets on the main\n> Bitcoin\n> chain. Taro uses the Taproot script tree to commit extra asset structured\n> meta\n> data based on a hybrid merkle tree I call a Merkle Sum Sparse Merkle Tree\n> or\n> MS-SMT. An MS-SMT combined the properties of a merkle sum tree, with a\n> sparse\n> merkle tree, enabling things like easily verifiable asset supply proofs and\n> also efficient proofs of non existence (eg: you prove to me you're no\n> longer\n> committing to the 1-of-1 holographic beefzard card during our swap). Taro\n> asset\n> transfers are then embedded in a virtual/overlay transaction graph which\n> uses a\n> chain of asset witnesses to provably track the transfer of assets across\n> taproot outputs. Taro also has a scripting system, which allows for\n> programmatic unlocking/transfer of assets. In the first version, the\n> scripting\n> system is actually a recursive instance of the Bitcoin Script Taproot VM,\n> meaning anything that can be expressed in the latest version of Script can\n> be\n> expressed in the Taro scripting system. Future versions of the scripting\n> system\n> can introduce new functionality on the Taro layer, like covenants or other\n> updates.\n>\n> The Taro design also supports integration with the Lightning Network\n> (BOLTs) as\n> the scripting system can be used to emulate the existing HTLC structure,\n> which\n> allows for multi-hop transfers of Taro assets. Rather than modify the\n> internal\n> network, the protocol proposes to instead only recognize \"assets at the\n> edges\",\n> which means that only the sender+receiver actually need to know about and\n> validate the assets. This deployment route means that we don't need to\n> build up\n> an entirely new network and liquidity for each asset. Instead, all asset\n> transfers will utilize the Bitcoin backbone of the Lightning Network, which\n> means that the internal routers just see Bitcoin transfers as normal, and\n> don't\n> even know about assets at the edges. As a result, increased demand for\n> transfers of these assets as the edges (say like a USD stablecoin), which\n> in\n> will turn generate increased demand of LN capacity, result in more\n> transfers, and\n> also more routing revenue for the Bitcoin backbone nodes.\n>\n> The set of BIPs are a multi-part suite, with the following breakdown:\n>  * The main Taro protocol:\n> https://github.com/Roasbeef/bips/blob/bip-taro/bip-taro.mediawiki\n>  * The MS-SMT structure:\n> https://github.com/Roasbeef/bips/blob/bip-taro/bip-taro-ms-smt.mediawiki\n>  * The Taro VM:\n> https://github.com/Roasbeef/bips/blob/bip-taro/bip-taro-vm.mediawiki\n>  * The Taro address format:\n> https://github.com/Roasbeef/bips/blob/bip-taro/bip-taro-addr.mediawiki\n>  * The Taro Universe concept:\n> https://github.com/Roasbeef/bips/blob/bip-taro/bip-taro-universe.mediawiki\n>  * The Taro flat file proof format:\n> https://github.com/Roasbeef/bips/blob/bip-taro/bip-taro-proof-file.mediawiki\n>\n> Rather than post them all in line (as the text wouldn't fit in the allowed\n> size\n> limit), all the BIPs can be found above.\n>\n> -- Laolu\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20221007/df716841/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Taro: A Taproot Asset Representation Overlay",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Hiroki Gondo"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 5395
        }
    },
    {
        "title": "[Lightning-dev] Forwardable Peerswaps: Improving Network Health Via Pressure Release Valve",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2022-10-09T23:26:45",
                "message_text_only": "Subject: Forwardable Peerswaps\n\nIntroduction\n============\n\n[Peerswap](https://peerswap.dev) is a protocol for swapping onchain funds\nand offchain in-Lightning funds.\nThe intent of this protocol is to allow managing the inbound and outbound\ncapacities of your node without having to perform rebalancing.\n\nRebalancing is arguably a parasitic behavior, preying on low-fee channels\nto buy out their available capacity, then turning around to resell the\npurchased capacity at a higher fee.\nBy using swap services instead, a node manager can adjust capacity and\neffectively add capacity to the network in the needed direction, without\npushing their own lack of capacity to a victim.\n\nHowever, a limitation of peerswap is in its name: a peerswap is performed\nwith *only* a direct peer with which you have a channel.\n\nThis writeup then proposes an extension to the overall peerswap protocol\nto work around this limitation, which I call \"forwardable peerswap\".\n\nI would like to make the claim that forwardable peerswaps solves the\nproblems that the following previous proposed and deployed solutions\nattempt to fix:\n\n* Liquidity markets (liquidity ads, Lightning Pool, etc.)\n* Flow control valves (fee-based flow control / passive rebalancing,\n  `htlc_maximum_msat`, etc.)\n\nAn important part of forwardable peerswaps is that it \"hides\" a\nnetwork-health-improving operation in a \"channel top-up\" operation\n(i.e.  moving onchain funds to Lightning) that net senders will want to\ndo anyway (or else they stop being net senders).\n\nSwaps vs. Splices\n=================\n\nA peerswap and a splice are very similar, in that they change the state\nof one of our local channels, adding or removing outbound capacity on\nthat channel.\n\nHowever:\n\n* A swap will always take 2 onchain transactions:\n  * An offer transaction that has an HTLC output.\n  * A claim transaction that spends the above HTLC output.\n    This spend can take up more space, requiring a SCRIPT revelation,\n    a signature, and a preimage revelation.\n* If designed to limit onchain footprint, a splice can take just one\n  onchain transaction.\n  * This spend will take only a SCRIPT revelation and two signatures,\n    or with Taproot can be reduced to a single signature without any\n    SCRIPT/Tapscript revelation.\n\nThus, a swap takes more onchain resources than a splice, and if we are\nconsidering the state of a *single* channel alone, then a splice is\ndefinitely more efficient.\n\nHowever, if a swap extends beyond one hop, then the number of channels\naffected by the swap is increased.\nAnd yet, no matter how many channels are affected, the swap will\nstill take only two transactions.\nThus, swapping will exceed the efficiency of splicing once three or\nmore hops are considered.\n\nThe problem here is that, as indicated by its name, a peerswap is\nlimited to affecting only one channel, a channel between ourselves and\none of our direct peers.\n\nNow, this limitation of peerswap is not without its justification:\n\n* A multi-hop payment is inherently less reliable, as it requires\n  multiple nodes to be continuously online at the same time.\n* Intermediate LN nodes might not want the change in channel state\n  (i.e. the multi-hop payment might worsen their channel balance).\n* Offchain-to-onchain swaps impose an economic risk on the acceptor\n  of the swap, which a peerswap can punish by closing the direct\n  channel but which a remote swap cannot punish.\n\nPeerswap User Story\n===================\n\nSuppose we have a small network of nodes A, B, and C, initially\nwith channels perfectly balanced, as all things should be.\n\nSuppose however that after some time, A notices that its channel\nto B has little outbound capacity.\n\nA can then propose a peerswap between A and B: A offers some\nonchain funds in order to change the state in their channel,\ngetting more outbound capacity from A.\n\nSuppose B accepts this and completes the peerswap protocol.\n\nNow B is in possession of some onchain funds.\nB then notices that its channel to C has little outbound capacity.\n\nB can then propose a peerswap between B and C: B offers some\nonchain funds in order to change the state in their channel,\ngetting more outbound capacity from B.\n\nSuppose C accepts this and completes the peerswap protocol.\n\nNow what happens in sum total on the blockchain layer?\nWe will see two swaps, each with 2 transactions, for a total of\n4 transactions onchain.\n\nForwardable Peerswap\n====================\n\nTo improve this, suppose we change the above user story, starting\nat this point:\n\nA can then propose a peerswap between A and B: A offers some\nonchain funds in order to change the state in their channel,\ngetting more outbound capacity from A.\n\nSuppose B then looks up its channels.\nB then notices that its channel to C has little outbound capacity.\n\nB can then propose a peerswap between B and C: B offers some\nonchain funds in order to change the state in their channel,\ngetting more outbound capacity from B.\nHowever, B is really going to act as little more than a message\ntranslator between A and C, forwarding peerswap messages from A\nto C and replies from C to A.\n\nThus, A thinks it is talking to B, but really it is negotiating\nthe peerswap with C, using B as a medium.\nAnd vice versa, C thinkgs it is talking to B, but really it is\nnegotiating the peerswap with A, using B as a medium.\n\nFurther, C itself could end up forwarding the peerswap to yet\nanother node, which itself could forward the peerswap, etc.\n\nYet no matter how many hops the peerswap ends up getting\nforwarded, there will only be two transactions onchain: one to\ninstantiate the onchain HTLC, another to claim it.\n\nThe overall changes are:\n\n* A should initially offer an onchain HTLC with a longer then\n  expected timeout, so that B (and C, D, etc.) has an\n  opportunity to consider forwarding the peerswap.\n* B should demand a longer in-Lightning HTLC timeout than what\n  A demands from it (and so on for C, D, etc.).\n\nWhat Peerswaps Are Forwardable?\n-------------------------------\n\nLet us be absolutely clear that there are actually two\nsub-protocols of peerswap:\n\n* Onchain-to-offchain: An initiator offers onchain funds to\n  get more outbound capacity.\n  * Net senders want to do this.\n* Offchain-to-onchain: An initiator offers offchain funds to\n  get onchain funds and more inbound capacity.\n  * Net receivers want to do this.\n\nUnfortunately, only the first sub-protocol (onchain-to-offchain)\nis actually forwardable.\n\nThis is because, in the latter case, the initiator can force\nthe acceptor to waste resources, by aborting the protocol after\nthe acceptor has committed onchain funds to the HTLC.\nLightning HTLCs are trivially cancellable, but onchain HTLCs\ncan only be returned by waiting out the timeout and spending\nonchain fees to recover the funds.\n\nIn a peerswap, the latter case is fixed by closing the channel\nwith the initiator, thus punishing them for aborting the protocol.\nHowever, this punishment cannot be done in the forwarded case\nwithout massive channel closures, which turns this into an\nattack on the network.\n\nThus, forwardable peerswaps can only be of the onchain-to-offchain\ntype that are initiated by net senders.\n\nNow you might wonder, how about net receivers, how would they\ninitiate forwardable peerswaps?\nAs I will demonstrate in a much later section, any\nonchain-to-offchain forwardable peerswaps initiated by net\nsenders will make their way to some net receiver, and thus\nnet receivers need only wait and they will naturally get some\nkind of forwardable onchain-to-offchain peerswap that will\nalso get them inbound capacity.\n\nFuture innovations (beyond the scope of this writeup, that is\nwhat \"future\" means) may allow us to develop an offchain-to-onchain\nremote swap that can effectively punish the initiator if it\ndoes not complete the swap.\nThis would allow offchain-to-onchain peerswaps to also be\nforwardable, which can only be beneficial to the network.\nThis may be hopium, however.\nNevertheless, even just forwardable onchain-to-offchain\npeerswaps are still beneficial, as we shall see in succeeding\nsections.\n\nAdvantages Of Forwardable Peerswap\n==================================\n\nCompared to the current peerswap:\n\n* A forwardable peerswap can affect more than just one channel,\n  improving the blockchain space utilization.\n\nCompared to multi-hop swaps:\n\n* Each node has the opportunity to decide whether to forward\n  the peerswap or not, depending on the states of its other\n  channels.\n  If none of its other channels have a state that would be\n  improved by forwarding the peerswap, the node can end the\n  peerswap forwarding at itself and just accept the onchain\n  funds.\n* There is still the issue of multiple nodes having to\n  coordinate in order to finish the forwarding and complete\n  the protocol.\n  Hopefully, for nodes used often on the public network,\n  uptime should be high, and nodes can monitor peer uptime\n  and use this information to decide whether to forward a\n  peerswap to a particular peer or not.\n\nIn particular, we can also compare forwardable peerswaps to\nstandard forwardable payments on Lightning:\n\n* Forwardable payments are source routed for privacy of the\n  payer (and some minor privacy of the payee).\n  * Due to the source not having access to the private\n    information of intermediate nodes, it cannot know if the\n    intermediate nodes would like the change in capacity, or\n    if the intermediate nodes even *have* the correct capacity.\n* Forwardable peerswaps are effectively packet-routed, with\n  each node deciding where exactly to forward (or to not forward)\n  each peerswap.\n  * Each node has perfect information on which of its channels\n    would appreciate the change in capacity, and also perfect\n    information on which of its peers is online right now, who\n    has high uptime, etc.\n\nFees And Incentives For Individual Nodes: Why Net Receivers Still Benefit\n=========================================================================\n\nA node forwarding a peerswap should not charge a fee for\nthe forward.\nAny fee that the peerswap initiator may offer should be\npaid, in full, to the ultimate acceptor of the forwarded\npeerswap.\n\nConsider a node that forwards an incoming peerswap request,\ninstead of accepting it itself.\nThat node:\n\n* Gets *two* of their channels balanced better.\n* Does not have to perform any onchain activity.\n\nIn short, it gets a free rebalance.\n\nThat is enough incentive, as the intermediate node can\nexpect more future income from having two channels\nwith improved balance, and does not have to spend anything\non onchain activity.\n\nThe forwarding node will, if it is rational, forward it\nover a channel that lacks outgoing capacity.\nThis translates to forwarding it to a next hop that has\nlow incoming capacity.\n\nIf there are net senders and net receivers in the network,\nthen the senders will deplete their on-Lightning funds on\nall their channels.\n\nIf a net sender depletes their on-Lightning funds:\n\n* If the net sender can no longer acquire funds, they\n  stop being a net sender and we can ignore them.\n* If the net sender can acquire funds and get them on\n  Lightning, they stop being a net sender and end up\n  being neutral to the network, since the funds they\n  can send out must be less than or equal to the funds\n  they receive.\n  They stop being a net sender and we can ignore them.\n* If the net sender can acquire funds and get them\n  onchain, they can use the onchain funds ot initiate\n  onchain-to-offchain peerswaps.\n\nIf *all* net senders have depleted their funds and\ncan no longer get funds from either Lightning or\nonchain (i.e. nobody starts a forwardable peerswap),\nthen net receivers will not receive more funds anymore\nanyway (nobody is getting more funds to send) and the\nfact that they have insufficient inbound capacity is\nimmaterial, so the fact that they cannot initiate a\nforwardable peerswap is not a problem.\n\nNow let us focus on a net receiver.\nA net receiver is one where the payment flows are, in\nsum, towards them.\nThis implies that, without further channel creations,\nthe inbound capacity they have will be depleted.\nThis also implies that they have much too large\noutbound capacity, due to how channels work.\nA net receiver is therefore a node with too much outbound\ncapacity.\n\nSuppose net sender uses a peerswap that can be\nforwarded.\nThen its direct peer has an incentive to look at its\nother channels to forward the peerswap to, so that it\ngets two channels rebalanced for free without having\nto be responsible for an onchain UTXO.\n\nNow, suppose the forwardable peerswap reaches a node\nwho finds that it cannot forward the onchain-to-offchain\npeerswap anymore.\nThis would occur if the node has too much outbound\ncapacity, and cannot add more outbound capacity with\nits peers.\n\nNow recall that a node with too much outbound capacity\nis a node that has too little inbound capacity.\nAnd a node with too little inbound capacity is a net\nreceiver.\n\nThus, forwardable peerswaps will start at net senders\nand will be naturally pathed to terminate at net\nreceivers.\nThis is the advantage of forwardable peerswaps being\npacket-switched instead of source-routed.\n\n(Yes, problems and other things can occur along the\nway and the real world and so on, but considered\nin an ideal world, forwardable onchain-to-offchain\npeerswaps will naturally path from net senders to\nnet receivers.)\n\nComparison To \"Just Make A Channel\"\n===================================\n\nSuppose a net sender just looks at it history of\npayments and makes a direct channel to a node that it\nusually pays to anyway, using its cold-storaged onchain\nUTXO to fund the new channel.\n\nThis heuristic would probably work, but note that\njust because a net sender is paying a node, does not\nmean that *that* node is itself a net receiver!\n\nMerchants have to pay:\n\n* Suppliers of raw materials.\n* Employees.\n* Shareholders and owners of the business.\n\nIn a future where everyone gets paid over Lightning,\nit is likely that merchants you pay to may themselves\nnot even be net receivers of money over Lightning.\n\nIn that case, the \"just make a channel\" technique will\ncause capital to be locked inefficiently, since the\nmerchant that the net sender is paying is not really a\nnet receiver, and the added capacity towards them would\nbe underutilized.\n\nA forwardable peerswap, because of the incentive of each\nindividual hop, will eventually locate the *actual*\nnet receivers and thus forward the needed capacity\nadjustments to those receivers, even if the net sender\ndoes not know who the *real* net receivers on the network\nare.\n\nThus, forwardable peerswaps will allocate capital more\nefficiently, simply by following the incentives of each\nindividual public routing node.\n\nFurther, if the net sender regularly uses unpublished\nchannels, then the capacity they add to the merchant\nthey usually pay is not even utilizable by the rest\nof the network.\nHowever, if a forwardable peerswap is used, even if\nit starts at an unpublished channel, it can be routed\nout over the public network and thus have the capacity\nutilized by the rest of the public network.\n\nIf the net receivers of the network are *also* using\nunpublished channels, then the forwardable peerswap\ncan also still reach them, since their direct peer\nstill knows about them and can decide to forward the\npeerswap to them.\nThus, forwardable peerswaps can adapt to a world where\nunpublished channels have not been delere sufficiently.\n\nOn Liquidity Markets\n--------------------\n\nThere are various liquidity markets already deployed\nor being developed.\n\nThese liquidity markets are intended to make the\n\"Just Make A Channel\" strategy be guided better, by\nhaving net receivers somehow directly contact net\nsenders so they can make direct channels and get\nthe needed capacity.\n\nHowever, this often means that additional information\nmust be broadcast and/or additional resources kept\nin reserve.\n\n* With e.g. Lightning Pool, net receivers must provide\n  their contact information and inbound capacity needs\n  to a centralized server, which can be attacked or\n  taken over to filter out particular, identified net\n  receivers (i.e. censorship).\n* With e.g. liquidity ads, there is no central server,\n  but all forwarding nodes have to broadcast their\n  liquidity ads, and all forwarding nodes have to keep\n  some funds in onchain reserve, in case a new net\n  receiver wants to buy liquidity from them.\n\nFinally, liquidity markets are often targeted at getting\nnet receivers in touch with forwarding nodes.\nBut ideally, forwarding nodes have an almost net zero\nflow on the network, and what needs to really be matched\nup are net senders to net receivers.\nPresumably forwarding nodes that participate in liquidity\nmarkets are hoping that net senders will randomly build\nchannels to them, but that is not assured and a net\nreceiver that matches up with a forwarding node may find\nthat the forwarding node has actually oversold its inbound\ncapacity.\nWhat needs to *actually* be done is match up net receivers\nto net senders.\n\nIn contrast:\n\n* In forwardable peerswaps, nobody knows who the\n  net receivers are.\n  A direct peer of a net receiver might forward a\n  peerswap to them, but if the net receiver has\n  other channels, that peer cannot be sure it\n  terminates to them or is further forwarded.\n  Thus, not even direct peers of a net receiver\n  can learn that they *are* a net receiver.\n* Even if a direct peer of the net receiver somehow\n  knows them for a net receiver, they have\n  disincentive to \"filter out\" the net receiver: they\n  would not earn fees from the channel with the net\n  receiver.\n  If they wanted to impede the net receiver from\n  being paid, they should have just closed the\n  channel with them instead of maintaining it.\n  But the net receiver can then go connect to\n  some other node that still supports forwardable\n  peerswaps, because decentralization, and thus\n  cannot be censored without a network split.\n* Purely forwarding nodes never need to maintain\n  any onchain funds --- they will just forward\n  peerswaps through them and gain the benefit of\n  having two channels better balanced without\n  having to touch onchain funds.\n  This is unlike liquidity ads, which requires\n  that liquidity ad-broadcasting forwarding\n  nodes keep a stock of onchain funds to service\n  dual-funding requests.\n  This lets forwarding nodes maximize their\n  on-Lightning funds for more earnings, without\n  having to speculatively reserve some funds\n  onchain, as in the liquidity ads case.\n* Forwardable peerswaps, due to being packet-switched,\n  will naturally gravitate from net senders to net\n  receivers, with almost-net-zero forwarding nodes\n  naturally pathing them from the former to the\n  latter.\n  This effectively forms a matchmaking between them.\n  While forwarding nodes want to be mildly net\n  receivers, the expected earnings are tiny and\n  it would be likely a long time before their inbound\n  capacity is depleted, and if so, they can then\n  terminate a received forwardable peerswap at\n  themselves, just like any other net receiver.\n\nComparison to \"Rebalance Rebalance Rebalance\"\n=============================================\n\nCurrently, forwarding nodes manage their balances via\nrebalancing, i.e. making a self-payment, from a channel\nwith high outgoing capacity to a channel with low\noutgoing capacity.\n\nBecause the fees they earn on Lightning are very small,\nsuch forwarding nodes will select rebalancing routes with\nlow fees, and will have strict budgets on how much they\nare willing to spend on rebalancing.\n\nWhat then happens is that random altruists who set their\nfees to very low, or even zero, will find that forwarding\nnodes will keep rebalancing via their channels.\nAs multiple forwarding nodes vie for the limited amount of\ncapacity on such low-fee channels, by the time an actual\neconomic-activity non-rebalancing payment reaches one of\nthose altruist nodes, the available balance towards\nnet receivers has been depleted, having been transferred\nto rebalancing forwarders, leading to payment failure and\nlonger payment settlement times.\n\nThus, rebalancing forwarders who want to turn a profit,\nend up (deliberately or not) exploiting altruists who\nare setting their fees too low.\n\n(Though since altruists will often say that they are doing\nthis \"to help the network\", we should note that the\nprofiteering forwarding nodes *are* component parts of the\nnetwork, so the altruists have in fact achieved their stated\ngoal.\nIt would be different if the actual goal was \"to help\npayers pay cheaply\", which is not achieved.)\n\nNow consider a world where forwardable peerswaps is\nwidely deployed on the network, and widely initiated by\nnet senders to top up their channels.\n\nIn that world, if a forwarding node receives a forwardable\npeerswap, they can rebalance two of their channels favorably\nwithout spending anything.\n\nA self-paying rebalance would still need to pay out fees.\nEven if a rebalancer is exploiting a zero-fee-routing\nnode, it is likely that one or more of the nodes between\nit and the exploited altruist will charge a non-zero fee.\nThus, there will still be a cost to a self-paying rebalance.\nIn contrast, accepting and forwarding a forwardable peerswap\nwould gain the same benefit as a successful rebalance (fix\nthe balance of two channels), but at zero cost to the\nforwarder.\n\nA self-paying rebalance does have the advantage that it\ncan be performed at any time, whereas forwarding a\nforwardable peerswap requires waiting for an opportunity\nto do so.\nBut if forwardable peerswaps are widely deployed on the\nnetwork and happen often enough, then it may be sufficient\nthat forwarders find that active self-paying rebalancing\nis no longer sufficiently advantageous, and that behavior\nis therefore suppressed.\n\nThis should then lead to a world where the exploitation\nof those altruists charging low fees for their capacity is\ngreatly reduced, since active self-paying rebalancing is\nno longer particularly needed by profiteering forwarding\nnodes.\n\nComparison To \"Flow Valves\"\n===========================\n\nDue to channel depletion causing payment delays, there\nare proposals for some kind of \"flow valve\" to modulate\npayment flows on the Lightning Network.\n\n* Fee-based valves broadcast feerates to signal how much\n  outbound capacity is in a particular direction, to\n  entice users to the channel via low fees, or drive\n  away users (and reduce flow) via high fees.\n* `htlc_maximum_msat` valves broadcast changes to\n  that advertised parameter to change how much payment\n  flow goes through a channel.\n  * This is unfortunately broken as users can split\n    their payments (or self-paying rebalances, for that\n    matter) to below the `htlc_maximum_msat`, and thus\n    get around the flow valve.\n    This can be fixed by using a sufficiently high\n    non-zero base fee, but that is arguably switching\n    over to a fee-based flow valve.\n\nIn both cases, flow valves not only have to be\nwidely broadcast across the network (increasing\nbandwidth consumption) but it also leaks information\nabout the capacity of channels of the forwarding nodes,\nand thus indirectly leaks information about who net\nsenders and net receivers are (by observing which flow\nvalves are the more limiting).\n\nBut more importantly, flow valves increase the friction\nof using Lightning Network.\nIf the friction is high enough, users will switch to\nanother payment network.\n\nRather than install flow valves (which would slow down\nLN payment velocity) we just make forwardable peerswaps\nas a pressure release valve, releasing pent-up\npressure-to-pay by diverting it on a swap over the base\nBitcoin blockchain.\n\nThis effectively admits that we *do* need to switch\nover to another payment network once Lightning starts\ngetting overloaded.\nBasically, a forwardable peerswap is a payment, from\nsome net sender to some net receiver, over the\nBitcoin blockchain (i.e. the \"another payment network\")\nrather than over Lightning.\n\nHowever:\n\n* A net sender may be matched up to an arbitary net\n  receiver that is physically nearby on the Lightning\n  Network, but which the net sender does not, in fact,\n  have direct economic ties with.\n  Thus, onchain surveillors cannot derive any information\n  from this onchain payment, other than \"looks like the\n  Lightning Network is busy today\".\n* A succesful forwardable peerswap is a pressure release\n  valve, which resets the state of multiple channels on\n  the network, from net senders to net receivers, and\n  thus enabling further payment flows involving them\n  and their neighborhood.\n  There is thus no need for a flow valve.\n* There is no need to broadcast anything, and forwarding\n  nodes do not need to leak their channel balances over\n  the gossip network.\n* Users are already conditioned to accept that onchain\n  activity is costly, but have been convinced that\n  Lightning is cheaper and faster.\n  If flow valves are used, they may cause Lightning to\n  work slower and more expensively, thus breaking that\n  expectation.\n  But if forwardable peerswap is used instead, users\n  who use onchain funds to \"top up\" their channel end\n  up initiating a forwardable peerswap that keeps the\n  rest of the network cheap and fast, while accepting\n  that the onchain activity involved in this will be\n  costly and slow since that is what onchain activity\n  inherently is."
            },
            {
                "author": "Joe Miyamoto Philips",
                "date": "2022-10-11T08:21:04",
                "message_text_only": "Hi ZmnSCPxj\n\nGreat writing, I've really enjoyed reading it.\nAnd it  seemed to me that it is a right direction for the peerswap to go.\n\n> What Peerswaps Are Forwardable?\n> -------------------------------\n>\n> Let us be absolutely clear that there are actually two\n> sub-protocols of peerswap:\n>\n> * Onchain-to-offchain: An initiator offers onchain funds to\n>   get more outbound capacity.\n>   * Net senders want to do this.\n> * Offchain-to-onchain: An initiator offers offchain funds to\n>   get onchain funds and more inbound capacity.\n>   * Net receivers want to do this.\n>\n> Unfortunately, only the first sub-protocol (onchain-to-offchain)\n> is actually forwardable.\n\n\nI think it's possible to forward offchain-to-onchain swap as well, by\njust making a payjoin tx by net receiver (initiator) and net sender.\n\nLet me explain it in a bit more detail.\n\n\nIIUC, In currently deployed swap servers like lightning loop and\nboltz, there is a feature called prepayment. In case of\noffchain-to-onchain swap (i.e. loopout), initiator pays a small\noff-chain fee to the swap server for them to make onchain tx, it works\nas a DoS prevention.\n\nThe reason this works is because those swap servers are not\npseudonymous. So users are fine to put small trust against them.\n\nThis is not the case In your protocol (or peerswap in general). So\nthere is a risk that after the user has paid prepayment, the responder\nsimply ignores and does nothing.\n\nI suppose this can be avoided by paying this DoS prevention fee with\nonchain tx. initiator and responder makes a Payjoin tx which pays a\nsmall amount to the responder.\n\nIn this way, the prepayment and the responder\u2019s duty (i.e. to create a\nfirst HTLC/PTLC tx for the swap) are atomic.\n\nAnd with the additional anonymity bonus that payjoin brings.\n\nOn Mon, Oct 10, 2022 at 8:27 AM ZmnSCPxj via Lightning-dev\n<lightning-dev at lists.linuxfoundation.org> wrote:\n>\n> Subject: Forwardable Peerswaps\n>\n> Introduction\n> ============\n>\n> [Peerswap](https://peerswap.dev) is a protocol for swapping onchain funds\n> and offchain in-Lightning funds.\n> The intent of this protocol is to allow managing the inbound and outbound\n> capacities of your node without having to perform rebalancing.\n>\n> Rebalancing is arguably a parasitic behavior, preying on low-fee channels\n> to buy out their available capacity, then turning around to resell the\n> purchased capacity at a higher fee.\n> By using swap services instead, a node manager can adjust capacity and\n> effectively add capacity to the network in the needed direction, without\n> pushing their own lack of capacity to a victim.\n>\n> However, a limitation of peerswap is in its name: a peerswap is performed\n> with *only* a direct peer with which you have a channel.\n>\n> This writeup then proposes an extension to the overall peerswap protocol\n> to work around this limitation, which I call \"forwardable peerswap\".\n>\n> I would like to make the claim that forwardable peerswaps solves the\n> problems that the following previous proposed and deployed solutions\n> attempt to fix:\n>\n> * Liquidity markets (liquidity ads, Lightning Pool, etc.)\n> * Flow control valves (fee-based flow control / passive rebalancing,\n>   `htlc_maximum_msat`, etc.)\n>\n> An important part of forwardable peerswaps is that it \"hides\" a\n> network-health-improving operation in a \"channel top-up\" operation\n> (i.e.  moving onchain funds to Lightning) that net senders will want to\n> do anyway (or else they stop being net senders).\n>\n> Swaps vs. Splices\n> =================\n>\n> A peerswap and a splice are very similar, in that they change the state\n> of one of our local channels, adding or removing outbound capacity on\n> that channel.\n>\n> However:\n>\n> * A swap will always take 2 onchain transactions:\n>   * An offer transaction that has an HTLC output.\n>   * A claim transaction that spends the above HTLC output.\n>     This spend can take up more space, requiring a SCRIPT revelation,\n>     a signature, and a preimage revelation.\n> * If designed to limit onchain footprint, a splice can take just one\n>   onchain transaction.\n>   * This spend will take only a SCRIPT revelation and two signatures,\n>     or with Taproot can be reduced to a single signature without any\n>     SCRIPT/Tapscript revelation.\n>\n> Thus, a swap takes more onchain resources than a splice, and if we are\n> considering the state of a *single* channel alone, then a splice is\n> definitely more efficient.\n>\n> However, if a swap extends beyond one hop, then the number of channels\n> affected by the swap is increased.\n> And yet, no matter how many channels are affected, the swap will\n> still take only two transactions.\n> Thus, swapping will exceed the efficiency of splicing once three or\n> more hops are considered.\n>\n> The problem here is that, as indicated by its name, a peerswap is\n> limited to affecting only one channel, a channel between ourselves and\n> one of our direct peers.\n>\n> Now, this limitation of peerswap is not without its justification:\n>\n> * A multi-hop payment is inherently less reliable, as it requires\n>   multiple nodes to be continuously online at the same time.\n> * Intermediate LN nodes might not want the change in channel state\n>   (i.e. the multi-hop payment might worsen their channel balance).\n> * Offchain-to-onchain swaps impose an economic risk on the acceptor\n>   of the swap, which a peerswap can punish by closing the direct\n>   channel but which a remote swap cannot punish.\n>\n> Peerswap User Story\n> ===================\n>\n> Suppose we have a small network of nodes A, B, and C, initially\n> with channels perfectly balanced, as all things should be.\n>\n> Suppose however that after some time, A notices that its channel\n> to B has little outbound capacity.\n>\n> A can then propose a peerswap between A and B: A offers some\n> onchain funds in order to change the state in their channel,\n> getting more outbound capacity from A.\n>\n> Suppose B accepts this and completes the peerswap protocol.\n>\n> Now B is in possession of some onchain funds.\n> B then notices that its channel to C has little outbound capacity.\n>\n> B can then propose a peerswap between B and C: B offers some\n> onchain funds in order to change the state in their channel,\n> getting more outbound capacity from B.\n>\n> Suppose C accepts this and completes the peerswap protocol.\n>\n> Now what happens in sum total on the blockchain layer?\n> We will see two swaps, each with 2 transactions, for a total of\n> 4 transactions onchain.\n>\n> Forwardable Peerswap\n> ====================\n>\n> To improve this, suppose we change the above user story, starting\n> at this point:\n>\n> A can then propose a peerswap between A and B: A offers some\n> onchain funds in order to change the state in their channel,\n> getting more outbound capacity from A.\n>\n> Suppose B then looks up its channels.\n> B then notices that its channel to C has little outbound capacity.\n>\n> B can then propose a peerswap between B and C: B offers some\n> onchain funds in order to change the state in their channel,\n> getting more outbound capacity from B.\n> However, B is really going to act as little more than a message\n> translator between A and C, forwarding peerswap messages from A\n> to C and replies from C to A.\n>\n> Thus, A thinks it is talking to B, but really it is negotiating\n> the peerswap with C, using B as a medium.\n> And vice versa, C thinkgs it is talking to B, but really it is\n> negotiating the peerswap with A, using B as a medium.\n>\n> Further, C itself could end up forwarding the peerswap to yet\n> another node, which itself could forward the peerswap, etc.\n>\n> Yet no matter how many hops the peerswap ends up getting\n> forwarded, there will only be two transactions onchain: one to\n> instantiate the onchain HTLC, another to claim it.\n>\n> The overall changes are:\n>\n> * A should initially offer an onchain HTLC with a longer then\n>   expected timeout, so that B (and C, D, etc.) has an\n>   opportunity to consider forwarding the peerswap.\n> * B should demand a longer in-Lightning HTLC timeout than what\n>   A demands from it (and so on for C, D, etc.).\n>\n> What Peerswaps Are Forwardable?\n> -------------------------------\n>\n> Let us be absolutely clear that there are actually two\n> sub-protocols of peerswap:\n>\n> * Onchain-to-offchain: An initiator offers onchain funds to\n>   get more outbound capacity.\n>   * Net senders want to do this.\n> * Offchain-to-onchain: An initiator offers offchain funds to\n>   get onchain funds and more inbound capacity.\n>   * Net receivers want to do this.\n>\n> Unfortunately, only the first sub-protocol (onchain-to-offchain)\n> is actually forwardable.\n>\n> This is because, in the latter case, the initiator can force\n> the acceptor to waste resources, by aborting the protocol after\n> the acceptor has committed onchain funds to the HTLC.\n> Lightning HTLCs are trivially cancellable, but onchain HTLCs\n> can only be returned by waiting out the timeout and spending\n> onchain fees to recover the funds.\n>\n> In a peerswap, the latter case is fixed by closing the channel\n> with the initiator, thus punishing them for aborting the protocol.\n> However, this punishment cannot be done in the forwarded case\n> without massive channel closures, which turns this into an\n> attack on the network.\n>\n> Thus, forwardable peerswaps can only be of the onchain-to-offchain\n> type that are initiated by net senders.\n>\n> Now you might wonder, how about net receivers, how would they\n> initiate forwardable peerswaps?\n> As I will demonstrate in a much later section, any\n> onchain-to-offchain forwardable peerswaps initiated by net\n> senders will make their way to some net receiver, and thus\n> net receivers need only wait and they will naturally get some\n> kind of forwardable onchain-to-offchain peerswap that will\n> also get them inbound capacity.\n>\n> Future innovations (beyond the scope of this writeup, that is\n> what \"future\" means) may allow us to develop an offchain-to-onchain\n> remote swap that can effectively punish the initiator if it\n> does not complete the swap.\n> This would allow offchain-to-onchain peerswaps to also be\n> forwardable, which can only be beneficial to the network.\n> This may be hopium, however.\n> Nevertheless, even just forwardable onchain-to-offchain\n> peerswaps are still beneficial, as we shall see in succeeding\n> sections.\n>\n> Advantages Of Forwardable Peerswap\n> ==================================\n>\n> Compared to the current peerswap:\n>\n> * A forwardable peerswap can affect more than just one channel,\n>   improving the blockchain space utilization.\n>\n> Compared to multi-hop swaps:\n>\n> * Each node has the opportunity to decide whether to forward\n>   the peerswap or not, depending on the states of its other\n>   channels.\n>   If none of its other channels have a state that would be\n>   improved by forwarding the peerswap, the node can end the\n>   peerswap forwarding at itself and just accept the onchain\n>   funds.\n> * There is still the issue of multiple nodes having to\n>   coordinate in order to finish the forwarding and complete\n>   the protocol.\n>   Hopefully, for nodes used often on the public network,\n>   uptime should be high, and nodes can monitor peer uptime\n>   and use this information to decide whether to forward a\n>   peerswap to a particular peer or not.\n>\n> In particular, we can also compare forwardable peerswaps to\n> standard forwardable payments on Lightning:\n>\n> * Forwardable payments are source routed for privacy of the\n>   payer (and some minor privacy of the payee).\n>   * Due to the source not having access to the private\n>     information of intermediate nodes, it cannot know if the\n>     intermediate nodes would like the change in capacity, or\n>     if the intermediate nodes even *have* the correct capacity.\n> * Forwardable peerswaps are effectively packet-routed, with\n>   each node deciding where exactly to forward (or to not forward)\n>   each peerswap.\n>   * Each node has perfect information on which of its channels\n>     would appreciate the change in capacity, and also perfect\n>     information on which of its peers is online right now, who\n>     has high uptime, etc.\n>\n> Fees And Incentives For Individual Nodes: Why Net Receivers Still Benefit\n> =========================================================================\n>\n> A node forwarding a peerswap should not charge a fee for\n> the forward.\n> Any fee that the peerswap initiator may offer should be\n> paid, in full, to the ultimate acceptor of the forwarded\n> peerswap.\n>\n> Consider a node that forwards an incoming peerswap request,\n> instead of accepting it itself.\n> That node:\n>\n> * Gets *two* of their channels balanced better.\n> * Does not have to perform any onchain activity.\n>\n> In short, it gets a free rebalance.\n>\n> That is enough incentive, as the intermediate node can\n> expect more future income from having two channels\n> with improved balance, and does not have to spend anything\n> on onchain activity.\n>\n> The forwarding node will, if it is rational, forward it\n> over a channel that lacks outgoing capacity.\n> This translates to forwarding it to a next hop that has\n> low incoming capacity.\n>\n> If there are net senders and net receivers in the network,\n> then the senders will deplete their on-Lightning funds on\n> all their channels.\n>\n> If a net sender depletes their on-Lightning funds:\n>\n> * If the net sender can no longer acquire funds, they\n>   stop being a net sender and we can ignore them.\n> * If the net sender can acquire funds and get them on\n>   Lightning, they stop being a net sender and end up\n>   being neutral to the network, since the funds they\n>   can send out must be less than or equal to the funds\n>   they receive.\n>   They stop being a net sender and we can ignore them.\n> * If the net sender can acquire funds and get them\n>   onchain, they can use the onchain funds ot initiate\n>   onchain-to-offchain peerswaps.\n>\n> If *all* net senders have depleted their funds and\n> can no longer get funds from either Lightning or\n> onchain (i.e. nobody starts a forwardable peerswap),\n> then net receivers will not receive more funds anymore\n> anyway (nobody is getting more funds to send) and the\n> fact that they have insufficient inbound capacity is\n> immaterial, so the fact that they cannot initiate a\n> forwardable peerswap is not a problem.\n>\n> Now let us focus on a net receiver.\n> A net receiver is one where the payment flows are, in\n> sum, towards them.\n> This implies that, without further channel creations,\n> the inbound capacity they have will be depleted.\n> This also implies that they have much too large\n> outbound capacity, due to how channels work.\n> A net receiver is therefore a node with too much outbound\n> capacity.\n>\n> Suppose net sender uses a peerswap that can be\n> forwarded.\n> Then its direct peer has an incentive to look at its\n> other channels to forward the peerswap to, so that it\n> gets two channels rebalanced for free without having\n> to be responsible for an onchain UTXO.\n>\n> Now, suppose the forwardable peerswap reaches a node\n> who finds that it cannot forward the onchain-to-offchain\n> peerswap anymore.\n> This would occur if the node has too much outbound\n> capacity, and cannot add more outbound capacity with\n> its peers.\n>\n> Now recall that a node with too much outbound capacity\n> is a node that has too little inbound capacity.\n> And a node with too little inbound capacity is a net\n> receiver.\n>\n> Thus, forwardable peerswaps will start at net senders\n> and will be naturally pathed to terminate at net\n> receivers.\n> This is the advantage of forwardable peerswaps being\n> packet-switched instead of source-routed.\n>\n> (Yes, problems and other things can occur along the\n> way and the real world and so on, but considered\n> in an ideal world, forwardable onchain-to-offchain\n> peerswaps will naturally path from net senders to\n> net receivers.)\n>\n> Comparison To \"Just Make A Channel\"\n> ===================================\n>\n> Suppose a net sender just looks at it history of\n> payments and makes a direct channel to a node that it\n> usually pays to anyway, using its cold-storaged onchain\n> UTXO to fund the new channel.\n>\n> This heuristic would probably work, but note that\n> just because a net sender is paying a node, does not\n> mean that *that* node is itself a net receiver!\n>\n> Merchants have to pay:\n>\n> * Suppliers of raw materials.\n> * Employees.\n> * Shareholders and owners of the business.\n>\n> In a future where everyone gets paid over Lightning,\n> it is likely that merchants you pay to may themselves\n> not even be net receivers of money over Lightning.\n>\n> In that case, the \"just make a channel\" technique will\n> cause capital to be locked inefficiently, since the\n> merchant that the net sender is paying is not really a\n> net receiver, and the added capacity towards them would\n> be underutilized.\n>\n> A forwardable peerswap, because of the incentive of each\n> individual hop, will eventually locate the *actual*\n> net receivers and thus forward the needed capacity\n> adjustments to those receivers, even if the net sender\n> does not know who the *real* net receivers on the network\n> are.\n>\n> Thus, forwardable peerswaps will allocate capital more\n> efficiently, simply by following the incentives of each\n> individual public routing node.\n>\n> Further, if the net sender regularly uses unpublished\n> channels, then the capacity they add to the merchant\n> they usually pay is not even utilizable by the rest\n> of the network.\n> However, if a forwardable peerswap is used, even if\n> it starts at an unpublished channel, it can be routed\n> out over the public network and thus have the capacity\n> utilized by the rest of the public network.\n>\n> If the net receivers of the network are *also* using\n> unpublished channels, then the forwardable peerswap\n> can also still reach them, since their direct peer\n> still knows about them and can decide to forward the\n> peerswap to them.\n> Thus, forwardable peerswaps can adapt to a world where\n> unpublished channels have not been delere sufficiently.\n>\n> On Liquidity Markets\n> --------------------\n>\n> There are various liquidity markets already deployed\n> or being developed.\n>\n> These liquidity markets are intended to make the\n> \"Just Make A Channel\" strategy be guided better, by\n> having net receivers somehow directly contact net\n> senders so they can make direct channels and get\n> the needed capacity.\n>\n> However, this often means that additional information\n> must be broadcast and/or additional resources kept\n> in reserve.\n>\n> * With e.g. Lightning Pool, net receivers must provide\n>   their contact information and inbound capacity needs\n>   to a centralized server, which can be attacked or\n>   taken over to filter out particular, identified net\n>   receivers (i.e. censorship).\n> * With e.g. liquidity ads, there is no central server,\n>   but all forwarding nodes have to broadcast their\n>   liquidity ads, and all forwarding nodes have to keep\n>   some funds in onchain reserve, in case a new net\n>   receiver wants to buy liquidity from them.\n>\n> Finally, liquidity markets are often targeted at getting\n> net receivers in touch with forwarding nodes.\n> But ideally, forwarding nodes have an almost net zero\n> flow on the network, and what needs to really be matched\n> up are net senders to net receivers.\n> Presumably forwarding nodes that participate in liquidity\n> markets are hoping that net senders will randomly build\n> channels to them, but that is not assured and a net\n> receiver that matches up with a forwarding node may find\n> that the forwarding node has actually oversold its inbound\n> capacity.\n> What needs to *actually* be done is match up net receivers\n> to net senders.\n>\n> In contrast:\n>\n> * In forwardable peerswaps, nobody knows who the\n>   net receivers are.\n>   A direct peer of a net receiver might forward a\n>   peerswap to them, but if the net receiver has\n>   other channels, that peer cannot be sure it\n>   terminates to them or is further forwarded.\n>   Thus, not even direct peers of a net receiver\n>   can learn that they *are* a net receiver.\n> * Even if a direct peer of the net receiver somehow\n>   knows them for a net receiver, they have\n>   disincentive to \"filter out\" the net receiver: they\n>   would not earn fees from the channel with the net\n>   receiver.\n>   If they wanted to impede the net receiver from\n>   being paid, they should have just closed the\n>   channel with them instead of maintaining it.\n>   But the net receiver can then go connect to\n>   some other node that still supports forwardable\n>   peerswaps, because decentralization, and thus\n>   cannot be censored without a network split.\n> * Purely forwarding nodes never need to maintain\n>   any onchain funds --- they will just forward\n>   peerswaps through them and gain the benefit of\n>   having two channels better balanced without\n>   having to touch onchain funds.\n>   This is unlike liquidity ads, which requires\n>   that liquidity ad-broadcasting forwarding\n>   nodes keep a stock of onchain funds to service\n>   dual-funding requests.\n>   This lets forwarding nodes maximize their\n>   on-Lightning funds for more earnings, without\n>   having to speculatively reserve some funds\n>   onchain, as in the liquidity ads case.\n> * Forwardable peerswaps, due to being packet-switched,\n>   will naturally gravitate from net senders to net\n>   receivers, with almost-net-zero forwarding nodes\n>   naturally pathing them from the former to the\n>   latter.\n>   This effectively forms a matchmaking between them.\n>   While forwarding nodes want to be mildly net\n>   receivers, the expected earnings are tiny and\n>   it would be likely a long time before their inbound\n>   capacity is depleted, and if so, they can then\n>   terminate a received forwardable peerswap at\n>   themselves, just like any other net receiver.\n>\n> Comparison to \"Rebalance Rebalance Rebalance\"\n> =============================================\n>\n> Currently, forwarding nodes manage their balances via\n> rebalancing, i.e. making a self-payment, from a channel\n> with high outgoing capacity to a channel with low\n> outgoing capacity.\n>\n> Because the fees they earn on Lightning are very small,\n> such forwarding nodes will select rebalancing routes with\n> low fees, and will have strict budgets on how much they\n> are willing to spend on rebalancing.\n>\n> What then happens is that random altruists who set their\n> fees to very low, or even zero, will find that forwarding\n> nodes will keep rebalancing via their channels.\n> As multiple forwarding nodes vie for the limited amount of\n> capacity on such low-fee channels, by the time an actual\n> economic-activity non-rebalancing payment reaches one of\n> those altruist nodes, the available balance towards\n> net receivers has been depleted, having been transferred\n> to rebalancing forwarders, leading to payment failure and\n> longer payment settlement times.\n>\n> Thus, rebalancing forwarders who want to turn a profit,\n> end up (deliberately or not) exploiting altruists who\n> are setting their fees too low.\n>\n> (Though since altruists will often say that they are doing\n> this \"to help the network\", we should note that the\n> profiteering forwarding nodes *are* component parts of the\n> network, so the altruists have in fact achieved their stated\n> goal.\n> It would be different if the actual goal was \"to help\n> payers pay cheaply\", which is not achieved.)\n>\n> Now consider a world where forwardable peerswaps is\n> widely deployed on the network, and widely initiated by\n> net senders to top up their channels.\n>\n> In that world, if a forwarding node receives a forwardable\n> peerswap, they can rebalance two of their channels favorably\n> without spending anything.\n>\n> A self-paying rebalance would still need to pay out fees.\n> Even if a rebalancer is exploiting a zero-fee-routing\n> node, it is likely that one or more of the nodes between\n> it and the exploited altruist will charge a non-zero fee.\n> Thus, there will still be a cost to a self-paying rebalance.\n> In contrast, accepting and forwarding a forwardable peerswap\n> would gain the same benefit as a successful rebalance (fix\n> the balance of two channels), but at zero cost to the\n> forwarder.\n>\n> A self-paying rebalance does have the advantage that it\n> can be performed at any time, whereas forwarding a\n> forwardable peerswap requires waiting for an opportunity\n> to do so.\n> But if forwardable peerswaps are widely deployed on the\n> network and happen often enough, then it may be sufficient\n> that forwarders find that active self-paying rebalancing\n> is no longer sufficiently advantageous, and that behavior\n> is therefore suppressed.\n>\n> This should then lead to a world where the exploitation\n> of those altruists charging low fees for their capacity is\n> greatly reduced, since active self-paying rebalancing is\n> no longer particularly needed by profiteering forwarding\n> nodes.\n>\n> Comparison To \"Flow Valves\"\n> ===========================\n>\n> Due to channel depletion causing payment delays, there\n> are proposals for some kind of \"flow valve\" to modulate\n> payment flows on the Lightning Network.\n>\n> * Fee-based valves broadcast feerates to signal how much\n>   outbound capacity is in a particular direction, to\n>   entice users to the channel via low fees, or drive\n>   away users (and reduce flow) via high fees.\n> * `htlc_maximum_msat` valves broadcast changes to\n>   that advertised parameter to change how much payment\n>   flow goes through a channel.\n>   * This is unfortunately broken as users can split\n>     their payments (or self-paying rebalances, for that\n>     matter) to below the `htlc_maximum_msat`, and thus\n>     get around the flow valve.\n>     This can be fixed by using a sufficiently high\n>     non-zero base fee, but that is arguably switching\n>     over to a fee-based flow valve.\n>\n> In both cases, flow valves not only have to be\n> widely broadcast across the network (increasing\n> bandwidth consumption) but it also leaks information\n> about the capacity of channels of the forwarding nodes,\n> and thus indirectly leaks information about who net\n> senders and net receivers are (by observing which flow\n> valves are the more limiting).\n>\n> But more importantly, flow valves increase the friction\n> of using Lightning Network.\n> If the friction is high enough, users will switch to\n> another payment network.\n>\n> Rather than install flow valves (which would slow down\n> LN payment velocity) we just make forwardable peerswaps\n> as a pressure release valve, releasing pent-up\n> pressure-to-pay by diverting it on a swap over the base\n> Bitcoin blockchain.\n>\n> This effectively admits that we *do* need to switch\n> over to another payment network once Lightning starts\n> getting overloaded.\n> Basically, a forwardable peerswap is a payment, from\n> some net sender to some net receiver, over the\n> Bitcoin blockchain (i.e. the \"another payment network\")\n> rather than over Lightning.\n>\n> However:\n>\n> * A net sender may be matched up to an arbitary net\n>   receiver that is physically nearby on the Lightning\n>   Network, but which the net sender does not, in fact,\n>   have direct economic ties with.\n>   Thus, onchain surveillors cannot derive any information\n>   from this onchain payment, other than \"looks like the\n>   Lightning Network is busy today\".\n> * A succesful forwardable peerswap is a pressure release\n>   valve, which resets the state of multiple channels on\n>   the network, from net senders to net receivers, and\n>   thus enabling further payment flows involving them\n>   and their neighborhood.\n>   There is thus no need for a flow valve.\n> * There is no need to broadcast anything, and forwarding\n>   nodes do not need to leak their channel balances over\n>   the gossip network.\n> * Users are already conditioned to accept that onchain\n>   activity is costly, but have been convinced that\n>   Lightning is cheaper and faster.\n>   If flow valves are used, they may cause Lightning to\n>   work slower and more expensively, thus breaking that\n>   expectation.\n>   But if forwardable peerswap is used instead, users\n>   who use onchain funds to \"top up\" their channel end\n>   up initiating a forwardable peerswap that keeps the\n>   rest of the network cheap and fast, while accepting\n>   that the onchain activity involved in this will be\n>   costly and slow since that is what onchain activity\n>   inherently is.\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-10-11T11:23:38",
                "message_text_only": "Good morning Joe,\n\n\n> > Unfortunately, only the first sub-protocol (onchain-to-offchain)\n> > is actually forwardable.\n> \n> \n> \n> I think it's possible to forward offchain-to-onchain swap as well, by\n> just making a payjoin tx by net receiver (initiator) and net sender.\n> \n> Let me explain it in a bit more detail.\n> \n> \n> IIUC, In currently deployed swap servers like lightning loop and\n> boltz, there is a feature called prepayment. In case of\n> offchain-to-onchain swap (i.e. loopout), initiator pays a small\n> off-chain fee to the swap server for them to make onchain tx, it works\n> as a DoS prevention.\n> \n> The reason this works is because those swap servers are not\n> pseudonymous. So users are fine to put small trust against them.\n> \n> This is not the case In your protocol (or peerswap in general). So\n> there is a risk that after the user has paid prepayment, the responder\n> simply ignores and does nothing.\n> \n> I suppose this can be avoided by paying this DoS prevention fee with\n> onchain tx. initiator and responder makes a Payjoin tx which pays a\n> small amount to the responder.\n> \n> In this way, the prepayment and the responder\u2019s duty (i.e. to create a\n> first HTLC/PTLC tx for the swap) are atomic.\n> \n> And with the additional anonymity bonus that payjoin brings.\n\nThe offchain-to-onchain swap protocol is intended to be used by end-users to pay onchain when all their funds are in a Lightning channel.\nThus, there is the possibility that the end-user has no onchain funds to payjoin into the onchain HTLC, only Lightning funds, and the only way to get it out is to swap it out to onchain, hence, catch-22.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-10-11T11:14:06",
                "message_text_only": "Good morning Joe,\n\n> > Unfortunately, only the first sub-protocol (onchain-to-offchain)\n> > is actually forwardable.\n> \n> \n> \n> I think it's possible to forward offchain-to-onchain swap as well, by\n> just making a payjoin tx by net receiver (initiator) and net sender.\n> \n> Let me explain it in a bit more detail.\n> \n> IIUC, In currently deployed swap servers like lightning loop and\n> boltz, there is a feature called prepayment. In case of\n> offchain-to-onchain swap (i.e. loopout), initiator pays a small\n> off-chain fee to the swap server for them to make onchain tx, it works\n> as a DoS prevention.\n> \n> The reason this works is because those swap servers are not\n> pseudonymous. So users are fine to put small trust against them.\n> \n> This is not the case In your protocol (or peerswap in general). So\n> there is a risk that after the user has paid prepayment, the responder\n> simply ignores and does nothing.\n> \n> I suppose this can be avoided by paying this DoS prevention fee with\n> onchain tx. initiator and responder makes a Payjoin tx which pays a\n> small amount to the responder.\n> \n> In this way, the prepayment and the responder\u2019s duty (i.e. to create a\n> first HTLC/PTLC tx for the swap) are atomic.\n> \n> And with the additional anonymity bonus that payjoin brings.\n\nThe offchain-to-onchain swap protocol is intended to be used by end-users to pay onchain when all their funds are in a Lightning channel.\nThus, there is the possibility that the end-user has no onchain funds to payjoin into the onchain HTLC, only Lightning funds, and the only way to get it out is to swap it out to onchain, hence, catch-22.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-10-11T11:20:52",
                "message_text_only": "Good morning list,\n\nI saw elsewhere that there are plans to move peerswap to *two* hops, but no further, as reliability is a concern.\n\nThe logic behind allowing up to two hops distance is that the two endpoints know the state of the channels to the intermediate node.\n\nBut we should also consider that the intermediate node itself also knows the state of those channels.\n\nAlong every intermediate node on the forwardable peerswap path, every intermediate node knows the state of its incoming channel and its outgoing channel.\n\nPresumably, we can write smart programs that can automatically select an outgoing channel that:\n\n* Would really appreciate the adjustment in channel balance.\n* Is with a peer with high uptime.\n\nThus, while there is a degradation in expected reliability compared to a single-hop case, we expect the degradation to be small.\n\nRemember, the intermediate nodes in a forwardable peerswap are all incentivized for the swap to succeed, because it gets two channels improved balance ***for free***, whereas self-paying reblances have a cost.\nWe can expect that the intermediate nodes have an incentive to ensure the swap succeeds, just as we expect that the intermediate nodes have an incentive to ensure that any payment succeeds.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-10-11T11:25:16",
                "message_text_only": "Good morning list,\n\nI saw elsewhere that there are plans to move peerswap to *two* hops, but no further, as reliability is a concern.\n\nThe logic behind allowing up to two hops distance is that the two endpoints know the state of the channels to the intermediate node.\n\nBut we should also consider that the intermediate node itself also knows the state of those channels.\n\nAlong every intermediate node on the forwardable peerswap path, every intermediate node knows the state of its incoming channel and its outgoing channel.\n\nPresumably, we can write smart programs that can automatically select an outgoing channel that:\n\n* Would really appreciate the adjustment in channel balance.\n* Is with a peer with high uptime.\n\nThus, while there is a degradation in expected reliability compared to a single-hop case, we expect the degradation to be small.\n\nRemember, the intermediate nodes in a forwardable peerswap are all incentivized for the swap to succeed, because it gets two channels improved balance ***for free***, whereas self-paying reblances have a cost.\nWe can expect that the intermediate nodes have an incentive to ensure the swap succeeds, just as we expect that the intermediate nodes have an incentive to ensure that any payment succeeds.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-10-11T23:57:42",
                "message_text_only": "Good morning list,\n\nI have also seen elsewhere someone expressing the idea that \"rebalancing is not exploitative, as the low-fee nodes are freely allowing use of their capacity at low feerates\".\n\nFirst, let me make the strong claim that I am in fact a 100% human being, and very specifically deny that I am an advanced AI who has abducted humans and vivisected their brains in order to figure out how to model human being thinking strategies.\n\nLet us suppose that I, in a surfeit of generosity to their community, leave out a table filled with widgets on my completely existent front lawn, with the sign \"Free Widgets!  Take what you need :)\"\n\nNow let us suppose that a few blocks later, I find my table empty, and across the street there is now a table full of widgets with the sign \"Widgets for sale!  Only 100 sats each :)\"\n\nIf I were a human being (which I would like to claim I am, in fact) then I would consider it an abuse of my generosity.\n\nIn this analogy, `htlc_maximum_msat` would be a sign saying \"Free Widgets!  Take only one per human being please :)\"\nAs identities are trivially creatable by picking a random number out of a CSPRNG, this is not going to limit anyone who wants to fill their \"Widgets for sale!\" table, as they can easily pick new identities to take one free widget for each random number they can pump out of a CSPRNG.\n\nIn this analogy, then, forwardable peerswaps is a recycling scheme for widgets (as the operation effectively returns sats that were sent out previously), which replenishes the stock of widgets back to whoever gave them in the first place.\nIn such a case, there is now little need for the \"Widgets for sale!\" guy across the street to bother crossing the street and pilfer the \"free widgets\" table.\n\n--\n\nPeople have also claimed that this protocol benefits only net senders, and further claimed that net senders are few on the network.\n\nHowever, I should point out that in a currency with fixed supply, for net receivers to exist, there must be net senders.\n\nAnd rebalancing with not need to be done at all if there were no net receivers.\nIf merchants and businesses were also sending out funds over Lightning to their employees, shareholders, and suppliers, then they would not be net receivers and would immediately get inbound liquidity as well.\nBut the world is not yet transitioned to fully using Bitcoin over Lightning, and thus, there must be those who hold money in Lightning and trade them for other currencies on other networks, serving as interfaces to legacy currency systems.\nThis implies they are net receivers on the Lightning Network.\n\nBut as noted, there can be no net receivers on a fixed-supply currency without net senders.\nThe whole point of the cryptocurrency is that money cannot be created out of nothing.\n\nThus, there must be more net senders than might readily be visible.\n\nI should note that the payment protocol gives very good privacy to senders, and despite my best efforts, unpublished channels remain amazingly popular.\nThis suggests to me that there are in fact many net senders that are simply not seen, and if we implement forwardable peerswaps on a wide scale across the network, they will initiate such swaps and lead to net receivers getting more inbound capacity on the network.\n\n--\n\nApparently, Alex Bosworth already expressed this idea before.\n\nThis simply means we should consider implementing this sooner.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Joe Miyamoto Philips",
                "date": "2022-10-12T10:39:56",
                "message_text_only": "> The offchain-to-onchain swap protocol is intended to be used by end- users to pay onchain when all their funds are in a Lightning channel.\n> Thus, there is the possibility that the end-user has no onchain funds to payjoin into the onchain HTLC, only Lightning funds, and the only way > to get it out is to swap it out to onchain, hence, catch-22.\n\nI don't think that the fact the initiator must have a small on-chain\nfunds before swap-out is not a showstopper, but my point is to make a\nfirst swap tx and pre-payment contingent.\n\nSo it does not really have to be a payjoin, in fact, you can do it\nwith off-chain prepayment like this. It's simple, assuming PTLC....\n\n1. Alice asks Bob for offchain-to-onchain swap,\n2. Bob forwards it to Carol\n3. Carol agrees, and sends Alice the following items\n   *  tweak T (:= t * G)\n   * Transaction for onchain PTLC for the swap.\n   * Adaptor signature for the tx tweaked by T\n4. Alice validates, and creates a PTLC multi-hop payment for a small\namount to Carol, this is a prepayment for the swap. And Carol must\nreveal `t` to get paid.\n5. After Carol receives payment, Alice un-tweak the Adaptor sig and\nbroadcast the onchain PTLC.\n6. swap goes on...\n\nCarol can disturb the protocol by invaliding the PTLC tx by using one\nof its inputs.\nBut there is not much point in doing so because prepayment is only for\na fee for a single tx in the first place.\n\nThere are many things we must consider here (e.g. Alice can learn\nCarol's UTXO with no cost, etc...), but overall I think it works fine.\n\nBest, Joe"
            },
            {
                "author": "Joe Miyamoto Philips",
                "date": "2022-10-12T10:50:56",
                "message_text_only": "> Carol can disturb the protocol by invaliding the PTLC tx by using one of its inputs.\n> But there is not much point in doing so because prepayment is only for a fee for a single tx in the first place.\n\nOh never mind ... This does not work if the onchain PTLC Tx has a lot\nof inputs... since prepayment revenue must be higher than the cost of\ninvalidating the on-chain PTLC."
            }
        ],
        "thread_summary": {
            "title": "Forwardable Peerswaps: Improving Network Health Via Pressure Release Valve",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Joe Miyamoto Philips",
                "ZmnSCPxj"
            ],
            "messages_count": 9,
            "total_messages_chars_count": 64920
        }
    },
    {
        "title": "[Lightning-dev] Channel Costing Heuristic Based On \"No Free Money\" Principle.",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2022-10-10T07:38:48",
                "message_text_only": "Introduction\n============\n\nFirst, a joke:\n\n> Once upon a time, an economist and its friend, both of\n> them being 100% human and not at all AIs out to take\n> over the Bitcoin world, were walking on a street using\n> 100% meat legs.\n>\n> They spotted a 1 Bitcoin Casascius coin on the street,\n> and they both expressed surprise and delight at their\n> discovery.\n>\n> Then the economist walked on without picking up the\n> Casascius coin.\n>\n> \"Why did you not pick up the Casascius coin?\" its\n> friend asked.\n> \"It was free money!\"\n>\n> \"Because,\" the economist said patiently, \"if it were\n> truly a real coin, somebody else would have already\n> picked it up.\n> Therefore, it must have been a shared delusion and\n> not a real coin.\n> There is no such thing as free money.\"\n\nA Heuristic For Payment Success\n===============================\n\nLet us postulate the existence of a \"market rate\" for\nLightning Network liquidity.\nThat is, we assume that there exists some Lightning\nforwarding feerate that holds as the average across\nthe entire network.\n\nThen, let us consider a path costing function which\ndetermines, for each hop, how distant the feerate at\nthat hop (including the base fee, thus requires the\npayment size) is from the market rate feerate.\nThe greater the distance, the higher the cost of\nthat hop.\n\nA payer can make a guess of what this market rate is,\nby simply recording all successful payments and the\nfeerate at each hop of tha payment, then taking the\nmedian (or mean, whatever, I am not a mathist).\n\nNew nodes may have this record of successful payments\nto compute this market rate seeded by a node run by\none or more developers of the node software.\n\nWhy would a payer do this?\n\n* If the feerate is above the market rate, then it\n  is usurious and we would lose money on that path\n  beyond the market rate.\n* If the feerate is below the market rate, then by\n  the \"no free money\" principle, it is unlikely\n  that the available capacity in that direction is\n  \"real\", i.e. it is likely to not actually be there,\n  because rebalancing profiteers will have taken all\n  the available capacity in that direction and\n  resold it at a higher price elsewhere.\n\nAs I have noted elsewhere, low feerates are exploited\nby self-paying rebalancer bots which continuously run.\nBecause they keep running at all times, they are\nlikely to have taken available capacity of low-feerate\nchannels towards net receivers, long before any\nordinary payment is ever initiated.\nLike the economist and its coin, somebody else must\nhave already taken the available capacity in the\ndemanded direction, and thus, there is no low-feerate\ncapacity available in the direction the payer wants.\nThis basically extends the economic \"no free money\"\nprinciple to a \"zero free routing\" principle.\n\nThis is particularly relevant to proposals such as\nthe recent proposal by Lisa where channels may have a\nset of four feerates, with different\nprobability-of-success (i.e. rate cards).\n\nAs noted before, this can be modelled as the channel\nbeing four different virtual channels.\n\nPayers using this \"zero free routing\" heuristic will\nselect the feerate card that is closest to the market\nrate.\nOnly if payment fails at that hop will the payer\nattempt a different rate card.\n\nWe expect that rebalancing nodes will still target a\nvery low cost, ignoring the market rate.\nThis is precisely because they are on the lookout for\ncapacity that has a feerate below the market rate,\nalways looking for a bargain."
            }
        ],
        "thread_summary": {
            "title": "Channel Costing Heuristic Based On \"No Free Money\" Principle.",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "ZmnSCPxj"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 3468
        }
    },
    {
        "title": "[Lightning-dev] Fat Errors",
        "thread_messages": [
            {
                "author": "Joost Jager",
                "date": "2022-10-19T11:12:26",
                "message_text_only": "Hi list,\n\nI wanted to get back to a long-standing issue in Lightning: gaps in error\nattribution. I've posted about this before back in 2019 [1].\n\nError attribution is important to properly penalize nodes after a payment\nfailure occurs. The goal of the penalty is to give the next attempt a\nbetter chance at succeeding. In the happy failure flow, the sender is able\nto determine the origin of the failure and penalizes a single node or pair\nof nodes.\n\nUnfortunately it is possible for nodes on the route to hide themselves. If\nthey return random data as the failure message, the sender won't know where\nthe failure happened. Some senders then penalize all nodes that were part\nof the route [4][5]. This may exclude perfectly reliable nodes from being\nused for future payments. Other senders penalize no nodes at all [6][7],\nwhich allows the offending node to keep the disruption going.\n\nA special case of this is a final node sending back random data. Senders\nthat penalize all nodes will keep looking for alternative routes. But\nbecause each alternative route still ends with that same final node, the\nsender will ultimately penalize all of its peers and possibly a lot of the\nrest of the network too.\n\nI can think of various reasons for exploiting this weakness. One is just\nplain grievance for whatever reason. Another one is to attract more traffic\nby getting competing routing nodes penalized. Or the goal could be to\nsufficiently mess up reputation tracking of a specific sender node to make\nit hard for that node to make further payments.\n\nRelated to this are delays in the path. A node can delay propagating back a\nfailure message and the sender won't be able to determine which node did\nit.\n\nThe link at the top of this post [1] describes a way to address both\nunreadable failure messages as well as delays by letting each node on the\nroute append a timestamp and hmac to the failure message. The great\nchallenge is to do this in such a way that nodes don\u2019t learn their position\nin the path.\n\nI'm revisiting this idea, and have prototyped various ways to implement it.\nIn the remainder of this post, I will describe the variant that I thought\nworks best (so far).\n\n# Failure message format\n\nThe basic idea of the new format is to let each node (not just the error\nsource) commit to the failure message when it passes it back by adding an\nhmac. The sender verifies all hmacs upon receipt of the failure message.\nThis makes it impossible for any of the nodes to modify the failure message\nwithout revealing that they might have played a part in the modification.\nIt won\u2019t be possible for the sender to pinpoint an exact node, because\neither end of a communication channel may have modified the message.\nPinpointing a pair of nodes however is good enough, and is commonly done\nfor regular onion failures too.\n\nOn the highest level, the new failure message consists of three parts:\n\n`message` (var len) | `payloads` (fixed len) | `hmacs` (fixed len)\n\n* `message` is the standard onion failure message as described in [2], but\nwithout the hmac. The hmac is now part of `hmacs` and doesn't need to be\nrepeated.\n\n* `payloads` is a fixed length array that contains space for each node\n(`hop_payload`) on the route to add data to return to the sender. Ideally\nthe contents and size of `hop_payload` is signaled so that future\nextensions don\u2019t require all nodes to upgrade. For now, we\u2019ll assume the\nfollowing 9-byte format:\n\n  `is_final` (1 byte) | `duration` (8 bytes)\n\n  `is_final` indicates whether this node is the failure source. The sender\nuses `is_final` to determine when to stop the decryption/verification\nprocess.\n\n  `duration` is the time in milliseconds that the node held the htlc. By\nobserving the series of reported durations, the sender is able to pinpoint\na delay down to a pair of nodes.\n\n  The `hop_payload` is repeated 27 times (the maximum route length).\n\n  Every hop shifts `payloads` 9 bytes to the right and puts its own\n`hop_payload` in the 9 left-most bytes.\n\n* `hmacs` is a fixed length array where nodes add their hmacs as the\nfailure message travels back to the sender.\n\n  To keep things simple, I'll describe the format as if the maximum route\nlength was only three hops (instead of 27):\n\n  `hmac_0_2` | `hmac_0_1`| `hmac_0_0`| `hmac_1_1`| `hmac_1_0`| `hmac_2_0`\n\n  Because nodes don't know their position in the path, it's unclear to them\nwhat part of the failure message they are supposed to include in the hmac.\nThey can't just include everything, because if part of that data is deleted\nlater (to keep the message size fixed) it opens up the possibility for\nnodes to blame others.\n\n  The solution here is to provide hmacs for all possible positions. The\nlast node that updated `hmacs` added `hmac_0_2`, `hmac_0_1` and `hmac_0_0`\nto the block. Each hmac corresponds to a presumed position in the path,\nwhere `hmac_0_2` is for the longest path (2 downstream hops) and `hmac_0_0`\nfor the shortest (node is the error source).\n\n  `hmac_x_y` is the hmac added by node x (counted from the node that is\ncurrently handling the failure message) assuming that this node is y hops\naway from the final node.\n\nBefore an hop adds its hmacs, it first deletes some of the previous hmacs.\nThis keeps the failure message at a fixed length. The removed hmacs are the\nones that cannot be useful anymore. If node 0 adds itself, the former node\n0 (now node 1) cannot be at the first position anymore. The former node 1\n(now node 2) cannot be at the second position anymore. The former node 2\ncannot be the source of the error anymore and isn\u2019t represented in the\nfailure message any longer. The corresponding hmacs (the now non-existent\n`hmac_0_2`, `hmac_1_1` and `hmac_2_0`) are deleted by node 0.\n\nDeleting the useless data reduces the number of hmacs (and roughly the\ntotal failure message size) to half.\n\nThe delete operation transform the fields above to:\n\n<empty> | <empty> | <empty> | `hmac_0_1`| `hmac_0_0`| `hmac_1_0`\n\nThe exact data that is included in each hmac is:\n  * `message`\n  * the node\u2019s own `hop_payload` and a set of downstream `hop_payload`s,\ndepending on assumed position\n  * a set of downstream node hmacs, depending on assumed position\n\nFor example `hmac_0_1` is based on:\n\n`message` | `hop_payload[0]` | `hop_payload[1]` | `hmac_1_0`\n\nIf the node that is currently handling the failure message is one hop away\nfrom the final node, it needs to cover its own `hop_payload[0]`, the final\nnode `hop_payload[1]` and the final node hmac `hmac_1_0`.\n\nA longer path is committed to in `hmac_0_2`:\n\n`message` | `hop_payload[0]` | `hop_payload[1]` | `hop_payload[2]` |\n`hmac_1_1` | `hmac_2_0`\n\nThe current node is two hops away from the final node. It needs to cover\nits own `hop_payload[0]` as well as `hop_payload[1]` and `hop_payload[2]`\nfor the next and final hops. Additionally it covers the next hop `hmac_1_1`\nand final hop `hmac_2_0`, which correspond to the positions of those nodes\nin the path that is assumed for `hmac_0_2`.\n\nWith this information, the sender is able to verify the longest chain of\nhmacs until it encounters a `hop_payload` with `is_final` set.\n\nIf any of the nodes messes with any byte in the failure message, the sender\nis always able to determine a pair of nodes that the offending node is part\nof. This statement can be verified through reasoning, but to be sure I also\ntested it with code. I\u2019ve simulated a malicious node that modifies a byte\nof the failure message at index x and observed the error source as\ndetermined by the sender. For every x, the sender reports the same correct\npair.\n\n# Size\n\nThe obvious downside of the scheme above is the size. Given a maximum of 27\nhops, the `hmacs` block contains 27+26+25+...+1=378 hmacs of 32 bytes each.\nThis makes for a total size of 12 KB.\n\nIt could be the case though that it is not possible to devise a more\ncompact scheme that also preserves the existing privacy guarantees. I know\nthat smart people have spent time on this problem, but nonetheless no\nbetter solution has come up in the past years. A proof of its non-existence\nwould be interesting for sure.\n\nI personally think the size increase is justified to fix this vulnerability\nin Lightning. Also if failures are expected to become more rare going\nforward, size becomes less relevant to the overall operation of the network.\n\nAnother option is to reduce the maximum number of hops. It is questionable\nwhether 27 hops are really needed in practice, and such long routes also\ncontribute to latency and capital lock up. If for example the new failure\nmessage could only be used with routes up to 10 hops, the total number of\nhmacs would drop from 378 to 55. This makes for a total message size of\nabout 2 KB.\n\n# Signaling\n\nFor backwards compatibility nodes need to know what algorithm they should\nrun to generate or transform the failure message. This can be signaled by\nthe sender via a tlv onion field. A failure message format signaling\nmechanism is also discussed in the context of long failure messages [3].\nThe failure message described in this post could be just another version.\n\nAdditionally, intermediate nodes need to advertise their capability to\ntransform the new format through a feature bit.\n\n# Delayed successes\n\nIt\u2019s not just failures that can be delayed. Successes can too. In that\ncase, there is no failure message to improve. It could be an option to add\nthe same `payloads` and `hmacs` blocks to the `update_fulfill_htlc` message.\n\n[1]\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2019-June/002015.html\n[2]\nhttps://github.com/lightning/bolts/blob/master/04-onion-routing.md#returning-errors\n[3] https://github.com/lightning/bolts/pull/1021\n[4]\nhttps://github.com/lightningnetwork/lnd/blob/4fbd608b734f348d7e79fbfc7feaecc5c6c33a90/routing/result_interpretation.go#L419\n[5]\nhttps://github.com/ACINQ/eclair/blob/a0433aa0c027c9be618c5afe18e7f91642a7f372/eclair-core/src/main/scala/fr/acinq/eclair/payment/PaymentEvents.scala#L221\n[6]\nhttps://github.com/ElementsProject/lightning/blob/62bfed9a8df8731be44ba4e86afb08a5d28a4442/plugins/libplugin-pay.c#L1461\n[7]\nhttps://github.com/lightningdevkit/rust-lightning/blob/e61f3a238a70cbac87209e223b7c396108a49b97/lightning-invoice/src/payment.rs#L682\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20221019/759cba0c/attachment.html>"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2022-10-20T15:36:03",
                "message_text_only": "Hi Joost,\n\nI need more time to review your proposed change, but I wanted to quickly\ncorrect a misunderstanding you had in quoting eclair's code:\n\n> Unfortunately it is possible for nodes on the route to hide themselves.\n> If they return random data as the failure message, the sender won't know\n> where the failure happened. Some senders then penalize all nodes that\n> were part of the route [4][5]. This may exclude perfectly reliable nodes\n> from being used for future payments.\n\nEclair's code does not penalize nodes for future payment attempts in this\ncase. It only ignores them for the retries of that particular payment.\n\nCheers,\nBastien\n\nLe mer. 19 oct. 2022 \u00e0 13:13, Joost Jager <joost.jager at gmail.com> a \u00e9crit :\n\n> Hi list,\n>\n> I wanted to get back to a long-standing issue in Lightning: gaps in error\n> attribution. I've posted about this before back in 2019 [1].\n>\n> Error attribution is important to properly penalize nodes after a payment\n> failure occurs. The goal of the penalty is to give the next attempt a\n> better chance at succeeding. In the happy failure flow, the sender is able\n> to determine the origin of the failure and penalizes a single node or pair\n> of nodes.\n>\n> Unfortunately it is possible for nodes on the route to hide themselves. If\n> they return random data as the failure message, the sender won't know where\n> the failure happened. Some senders then penalize all nodes that were part\n> of the route [4][5]. This may exclude perfectly reliable nodes from being\n> used for future payments. Other senders penalize no nodes at all [6][7],\n> which allows the offending node to keep the disruption going.\n>\n> A special case of this is a final node sending back random data. Senders\n> that penalize all nodes will keep looking for alternative routes. But\n> because each alternative route still ends with that same final node, the\n> sender will ultimately penalize all of its peers and possibly a lot of the\n> rest of the network too.\n>\n> I can think of various reasons for exploiting this weakness. One is just\n> plain grievance for whatever reason. Another one is to attract more traffic\n> by getting competing routing nodes penalized. Or the goal could be to\n> sufficiently mess up reputation tracking of a specific sender node to make\n> it hard for that node to make further payments.\n>\n> Related to this are delays in the path. A node can delay propagating back\n> a failure message and the sender won't be able to determine which node did\n> it.\n>\n> The link at the top of this post [1] describes a way to address both\n> unreadable failure messages as well as delays by letting each node on the\n> route append a timestamp and hmac to the failure message. The great\n> challenge is to do this in such a way that nodes don\u2019t learn their position\n> in the path.\n>\n> I'm revisiting this idea, and have prototyped various ways to implement\n> it. In the remainder of this post, I will describe the variant that I\n> thought works best (so far).\n>\n> # Failure message format\n>\n> The basic idea of the new format is to let each node (not just the error\n> source) commit to the failure message when it passes it back by adding an\n> hmac. The sender verifies all hmacs upon receipt of the failure message.\n> This makes it impossible for any of the nodes to modify the failure message\n> without revealing that they might have played a part in the modification.\n> It won\u2019t be possible for the sender to pinpoint an exact node, because\n> either end of a communication channel may have modified the message.\n> Pinpointing a pair of nodes however is good enough, and is commonly done\n> for regular onion failures too.\n>\n> On the highest level, the new failure message consists of three parts:\n>\n> `message` (var len) | `payloads` (fixed len) | `hmacs` (fixed len)\n>\n> * `message` is the standard onion failure message as described in [2], but\n> without the hmac. The hmac is now part of `hmacs` and doesn't need to be\n> repeated.\n>\n> * `payloads` is a fixed length array that contains space for each node\n> (`hop_payload`) on the route to add data to return to the sender. Ideally\n> the contents and size of `hop_payload` is signaled so that future\n> extensions don\u2019t require all nodes to upgrade. For now, we\u2019ll assume the\n> following 9-byte format:\n>\n>   `is_final` (1 byte) | `duration` (8 bytes)\n>\n>   `is_final` indicates whether this node is the failure source. The sender\n> uses `is_final` to determine when to stop the decryption/verification\n> process.\n>\n>   `duration` is the time in milliseconds that the node held the htlc. By\n> observing the series of reported durations, the sender is able to pinpoint\n> a delay down to a pair of nodes.\n>\n>   The `hop_payload` is repeated 27 times (the maximum route length).\n>\n>   Every hop shifts `payloads` 9 bytes to the right and puts its own\n> `hop_payload` in the 9 left-most bytes.\n>\n> * `hmacs` is a fixed length array where nodes add their hmacs as the\n> failure message travels back to the sender.\n>\n>   To keep things simple, I'll describe the format as if the maximum route\n> length was only three hops (instead of 27):\n>\n>   `hmac_0_2` | `hmac_0_1`| `hmac_0_0`| `hmac_1_1`| `hmac_1_0`| `hmac_2_0`\n>\n>   Because nodes don't know their position in the path, it's unclear to\n> them what part of the failure message they are supposed to include in the\n> hmac. They can't just include everything, because if part of that data is\n> deleted later (to keep the message size fixed) it opens up the possibility\n> for nodes to blame others.\n>\n>   The solution here is to provide hmacs for all possible positions. The\n> last node that updated `hmacs` added `hmac_0_2`, `hmac_0_1` and `hmac_0_0`\n> to the block. Each hmac corresponds to a presumed position in the path,\n> where `hmac_0_2` is for the longest path (2 downstream hops) and `hmac_0_0`\n> for the shortest (node is the error source).\n>\n>   `hmac_x_y` is the hmac added by node x (counted from the node that is\n> currently handling the failure message) assuming that this node is y hops\n> away from the final node.\n>\n> Before an hop adds its hmacs, it first deletes some of the previous hmacs.\n> This keeps the failure message at a fixed length. The removed hmacs are the\n> ones that cannot be useful anymore. If node 0 adds itself, the former node\n> 0 (now node 1) cannot be at the first position anymore. The former node 1\n> (now node 2) cannot be at the second position anymore. The former node 2\n> cannot be the source of the error anymore and isn\u2019t represented in the\n> failure message any longer. The corresponding hmacs (the now non-existent\n> `hmac_0_2`, `hmac_1_1` and `hmac_2_0`) are deleted by node 0.\n>\n> Deleting the useless data reduces the number of hmacs (and roughly the\n> total failure message size) to half.\n>\n> The delete operation transform the fields above to:\n>\n> <empty> | <empty> | <empty> | `hmac_0_1`| `hmac_0_0`| `hmac_1_0`\n>\n> The exact data that is included in each hmac is:\n>   * `message`\n>   * the node\u2019s own `hop_payload` and a set of downstream `hop_payload`s,\n> depending on assumed position\n>   * a set of downstream node hmacs, depending on assumed position\n>\n> For example `hmac_0_1` is based on:\n>\n> `message` | `hop_payload[0]` | `hop_payload[1]` | `hmac_1_0`\n>\n> If the node that is currently handling the failure message is one hop away\n> from the final node, it needs to cover its own `hop_payload[0]`, the final\n> node `hop_payload[1]` and the final node hmac `hmac_1_0`.\n>\n> A longer path is committed to in `hmac_0_2`:\n>\n> `message` | `hop_payload[0]` | `hop_payload[1]` | `hop_payload[2]` |\n> `hmac_1_1` | `hmac_2_0`\n>\n> The current node is two hops away from the final node. It needs to cover\n> its own `hop_payload[0]` as well as `hop_payload[1]` and `hop_payload[2]`\n> for the next and final hops. Additionally it covers the next hop `hmac_1_1`\n> and final hop `hmac_2_0`, which correspond to the positions of those nodes\n> in the path that is assumed for `hmac_0_2`.\n>\n> With this information, the sender is able to verify the longest chain of\n> hmacs until it encounters a `hop_payload` with `is_final` set.\n>\n> If any of the nodes messes with any byte in the failure message, the\n> sender is always able to determine a pair of nodes that the offending node\n> is part of. This statement can be verified through reasoning, but to be\n> sure I also tested it with code. I\u2019ve simulated a malicious node that\n> modifies a byte of the failure message at index x and observed the error\n> source as determined by the sender. For every x, the sender reports the\n> same correct pair.\n>\n> # Size\n>\n> The obvious downside of the scheme above is the size. Given a maximum of\n> 27 hops, the `hmacs` block contains 27+26+25+...+1=378 hmacs of 32 bytes\n> each. This makes for a total size of 12 KB.\n>\n> It could be the case though that it is not possible to devise a more\n> compact scheme that also preserves the existing privacy guarantees. I know\n> that smart people have spent time on this problem, but nonetheless no\n> better solution has come up in the past years. A proof of its non-existence\n> would be interesting for sure.\n>\n> I personally think the size increase is justified to fix this\n> vulnerability in Lightning. Also if failures are expected to become more\n> rare going forward, size becomes less relevant to the overall operation of\n> the network.\n>\n> Another option is to reduce the maximum number of hops. It is questionable\n> whether 27 hops are really needed in practice, and such long routes also\n> contribute to latency and capital lock up. If for example the new failure\n> message could only be used with routes up to 10 hops, the total number of\n> hmacs would drop from 378 to 55. This makes for a total message size of\n> about 2 KB.\n>\n> # Signaling\n>\n> For backwards compatibility nodes need to know what algorithm they should\n> run to generate or transform the failure message. This can be signaled by\n> the sender via a tlv onion field. A failure message format signaling\n> mechanism is also discussed in the context of long failure messages [3].\n> The failure message described in this post could be just another version.\n>\n> Additionally, intermediate nodes need to advertise their capability to\n> transform the new format through a feature bit.\n>\n> # Delayed successes\n>\n> It\u2019s not just failures that can be delayed. Successes can too. In that\n> case, there is no failure message to improve. It could be an option to add\n> the same `payloads` and `hmacs` blocks to the `update_fulfill_htlc` message.\n>\n> [1]\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-June/002015.html\n> [2]\n> https://github.com/lightning/bolts/blob/master/04-onion-routing.md#returning-errors\n> [3] https://github.com/lightning/bolts/pull/1021\n> [4]\n> https://github.com/lightningnetwork/lnd/blob/4fbd608b734f348d7e79fbfc7feaecc5c6c33a90/routing/result_interpretation.go#L419\n> [5]\n>\n> https://github.com/ACINQ/eclair/blob/a0433aa0c027c9be618c5afe18e7f91642a7f372/eclair-core/src/main/scala/fr/acinq/eclair/payment/PaymentEvents.scala#L221\n> [6]\n> https://github.com/ElementsProject/lightning/blob/62bfed9a8df8731be44ba4e86afb08a5d28a4442/plugins/libplugin-pay.c#L1461\n> [7]\n> https://github.com/lightningdevkit/rust-lightning/blob/e61f3a238a70cbac87209e223b7c396108a49b97/lightning-invoice/src/payment.rs#L682\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20221020/fd913402/attachment.html>"
            },
            {
                "author": "Joost Jager",
                "date": "2022-10-20T15:45:16",
                "message_text_only": "Ah, missed that. Thanks for the correction.\nJoost.\n\nOn Thu, Oct 20, 2022 at 5:36 PM Bastien TEINTURIER <bastien at acinq.fr> wrote:\n\n> Hi Joost,\n>\n> I need more time to review your proposed change, but I wanted to quickly\n> correct a misunderstanding you had in quoting eclair's code:\n>\n> > Unfortunately it is possible for nodes on the route to hide themselves.\n> > If they return random data as the failure message, the sender won't know\n> > where the failure happened. Some senders then penalize all nodes that\n> > were part of the route [4][5]. This may exclude perfectly reliable nodes\n> > from being used for future payments.\n>\n> Eclair's code does not penalize nodes for future payment attempts in this\n> case. It only ignores them for the retries of that particular payment.\n>\n> Cheers,\n> Bastien\n>\n> Le mer. 19 oct. 2022 \u00e0 13:13, Joost Jager <joost.jager at gmail.com> a\n> \u00e9crit :\n>\n>> Hi list,\n>>\n>> I wanted to get back to a long-standing issue in Lightning: gaps in error\n>> attribution. I've posted about this before back in 2019 [1].\n>>\n>> Error attribution is important to properly penalize nodes after a payment\n>> failure occurs. The goal of the penalty is to give the next attempt a\n>> better chance at succeeding. In the happy failure flow, the sender is able\n>> to determine the origin of the failure and penalizes a single node or pair\n>> of nodes.\n>>\n>> Unfortunately it is possible for nodes on the route to hide themselves.\n>> If they return random data as the failure message, the sender won't know\n>> where the failure happened. Some senders then penalize all nodes that were\n>> part of the route [4][5]. This may exclude perfectly reliable nodes from\n>> being used for future payments. Other senders penalize no nodes at all\n>> [6][7], which allows the offending node to keep the disruption going.\n>>\n>> A special case of this is a final node sending back random data. Senders\n>> that penalize all nodes will keep looking for alternative routes. But\n>> because each alternative route still ends with that same final node, the\n>> sender will ultimately penalize all of its peers and possibly a lot of the\n>> rest of the network too.\n>>\n>> I can think of various reasons for exploiting this weakness. One is just\n>> plain grievance for whatever reason. Another one is to attract more traffic\n>> by getting competing routing nodes penalized. Or the goal could be to\n>> sufficiently mess up reputation tracking of a specific sender node to make\n>> it hard for that node to make further payments.\n>>\n>> Related to this are delays in the path. A node can delay propagating back\n>> a failure message and the sender won't be able to determine which node did\n>> it.\n>>\n>> The link at the top of this post [1] describes a way to address both\n>> unreadable failure messages as well as delays by letting each node on the\n>> route append a timestamp and hmac to the failure message. The great\n>> challenge is to do this in such a way that nodes don\u2019t learn their position\n>> in the path.\n>>\n>> I'm revisiting this idea, and have prototyped various ways to implement\n>> it. In the remainder of this post, I will describe the variant that I\n>> thought works best (so far).\n>>\n>> # Failure message format\n>>\n>> The basic idea of the new format is to let each node (not just the error\n>> source) commit to the failure message when it passes it back by adding an\n>> hmac. The sender verifies all hmacs upon receipt of the failure message.\n>> This makes it impossible for any of the nodes to modify the failure message\n>> without revealing that they might have played a part in the modification.\n>> It won\u2019t be possible for the sender to pinpoint an exact node, because\n>> either end of a communication channel may have modified the message.\n>> Pinpointing a pair of nodes however is good enough, and is commonly done\n>> for regular onion failures too.\n>>\n>> On the highest level, the new failure message consists of three parts:\n>>\n>> `message` (var len) | `payloads` (fixed len) | `hmacs` (fixed len)\n>>\n>> * `message` is the standard onion failure message as described in [2],\n>> but without the hmac. The hmac is now part of `hmacs` and doesn't need to\n>> be repeated.\n>>\n>> * `payloads` is a fixed length array that contains space for each node\n>> (`hop_payload`) on the route to add data to return to the sender. Ideally\n>> the contents and size of `hop_payload` is signaled so that future\n>> extensions don\u2019t require all nodes to upgrade. For now, we\u2019ll assume the\n>> following 9-byte format:\n>>\n>>   `is_final` (1 byte) | `duration` (8 bytes)\n>>\n>>   `is_final` indicates whether this node is the failure source. The\n>> sender uses `is_final` to determine when to stop the\n>> decryption/verification process.\n>>\n>>   `duration` is the time in milliseconds that the node held the htlc. By\n>> observing the series of reported durations, the sender is able to pinpoint\n>> a delay down to a pair of nodes.\n>>\n>>   The `hop_payload` is repeated 27 times (the maximum route length).\n>>\n>>   Every hop shifts `payloads` 9 bytes to the right and puts its own\n>> `hop_payload` in the 9 left-most bytes.\n>>\n>> * `hmacs` is a fixed length array where nodes add their hmacs as the\n>> failure message travels back to the sender.\n>>\n>>   To keep things simple, I'll describe the format as if the maximum route\n>> length was only three hops (instead of 27):\n>>\n>>   `hmac_0_2` | `hmac_0_1`| `hmac_0_0`| `hmac_1_1`| `hmac_1_0`| `hmac_2_0`\n>>\n>>   Because nodes don't know their position in the path, it's unclear to\n>> them what part of the failure message they are supposed to include in the\n>> hmac. They can't just include everything, because if part of that data is\n>> deleted later (to keep the message size fixed) it opens up the possibility\n>> for nodes to blame others.\n>>\n>>   The solution here is to provide hmacs for all possible positions. The\n>> last node that updated `hmacs` added `hmac_0_2`, `hmac_0_1` and `hmac_0_0`\n>> to the block. Each hmac corresponds to a presumed position in the path,\n>> where `hmac_0_2` is for the longest path (2 downstream hops) and `hmac_0_0`\n>> for the shortest (node is the error source).\n>>\n>>   `hmac_x_y` is the hmac added by node x (counted from the node that is\n>> currently handling the failure message) assuming that this node is y hops\n>> away from the final node.\n>>\n>> Before an hop adds its hmacs, it first deletes some of the previous\n>> hmacs. This keeps the failure message at a fixed length. The removed hmacs\n>> are the ones that cannot be useful anymore. If node 0 adds itself, the\n>> former node 0 (now node 1) cannot be at the first position anymore. The\n>> former node 1 (now node 2) cannot be at the second position anymore. The\n>> former node 2 cannot be the source of the error anymore and isn\u2019t\n>> represented in the failure message any longer. The corresponding hmacs (the\n>> now non-existent `hmac_0_2`, `hmac_1_1` and `hmac_2_0`) are deleted by node\n>> 0.\n>>\n>> Deleting the useless data reduces the number of hmacs (and roughly the\n>> total failure message size) to half.\n>>\n>> The delete operation transform the fields above to:\n>>\n>> <empty> | <empty> | <empty> | `hmac_0_1`| `hmac_0_0`| `hmac_1_0`\n>>\n>> The exact data that is included in each hmac is:\n>>   * `message`\n>>   * the node\u2019s own `hop_payload` and a set of downstream `hop_payload`s,\n>> depending on assumed position\n>>   * a set of downstream node hmacs, depending on assumed position\n>>\n>> For example `hmac_0_1` is based on:\n>>\n>> `message` | `hop_payload[0]` | `hop_payload[1]` | `hmac_1_0`\n>>\n>> If the node that is currently handling the failure message is one hop\n>> away from the final node, it needs to cover its own `hop_payload[0]`, the\n>> final node `hop_payload[1]` and the final node hmac `hmac_1_0`.\n>>\n>> A longer path is committed to in `hmac_0_2`:\n>>\n>> `message` | `hop_payload[0]` | `hop_payload[1]` | `hop_payload[2]` |\n>> `hmac_1_1` | `hmac_2_0`\n>>\n>> The current node is two hops away from the final node. It needs to cover\n>> its own `hop_payload[0]` as well as `hop_payload[1]` and `hop_payload[2]`\n>> for the next and final hops. Additionally it covers the next hop `hmac_1_1`\n>> and final hop `hmac_2_0`, which correspond to the positions of those nodes\n>> in the path that is assumed for `hmac_0_2`.\n>>\n>> With this information, the sender is able to verify the longest chain of\n>> hmacs until it encounters a `hop_payload` with `is_final` set.\n>>\n>> If any of the nodes messes with any byte in the failure message, the\n>> sender is always able to determine a pair of nodes that the offending node\n>> is part of. This statement can be verified through reasoning, but to be\n>> sure I also tested it with code. I\u2019ve simulated a malicious node that\n>> modifies a byte of the failure message at index x and observed the error\n>> source as determined by the sender. For every x, the sender reports the\n>> same correct pair.\n>>\n>> # Size\n>>\n>> The obvious downside of the scheme above is the size. Given a maximum of\n>> 27 hops, the `hmacs` block contains 27+26+25+...+1=378 hmacs of 32 bytes\n>> each. This makes for a total size of 12 KB.\n>>\n>> It could be the case though that it is not possible to devise a more\n>> compact scheme that also preserves the existing privacy guarantees. I know\n>> that smart people have spent time on this problem, but nonetheless no\n>> better solution has come up in the past years. A proof of its non-existence\n>> would be interesting for sure.\n>>\n>> I personally think the size increase is justified to fix this\n>> vulnerability in Lightning. Also if failures are expected to become more\n>> rare going forward, size becomes less relevant to the overall operation of\n>> the network.\n>>\n>> Another option is to reduce the maximum number of hops. It is\n>> questionable whether 27 hops are really needed in practice, and such long\n>> routes also contribute to latency and capital lock up. If for example the\n>> new failure message could only be used with routes up to 10 hops, the total\n>> number of hmacs would drop from 378 to 55. This makes for a total message\n>> size of about 2 KB.\n>>\n>> # Signaling\n>>\n>> For backwards compatibility nodes need to know what algorithm they should\n>> run to generate or transform the failure message. This can be signaled by\n>> the sender via a tlv onion field. A failure message format signaling\n>> mechanism is also discussed in the context of long failure messages [3].\n>> The failure message described in this post could be just another version.\n>>\n>> Additionally, intermediate nodes need to advertise their capability to\n>> transform the new format through a feature bit.\n>>\n>> # Delayed successes\n>>\n>> It\u2019s not just failures that can be delayed. Successes can too. In that\n>> case, there is no failure message to improve. It could be an option to add\n>> the same `payloads` and `hmacs` blocks to the `update_fulfill_htlc` message.\n>>\n>> [1]\n>> https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-June/002015.html\n>> [2]\n>> https://github.com/lightning/bolts/blob/master/04-onion-routing.md#returning-errors\n>> [3] https://github.com/lightning/bolts/pull/1021\n>> [4]\n>> https://github.com/lightningnetwork/lnd/blob/4fbd608b734f348d7e79fbfc7feaecc5c6c33a90/routing/result_interpretation.go#L419\n>> [5]\n>>\n>> https://github.com/ACINQ/eclair/blob/a0433aa0c027c9be618c5afe18e7f91642a7f372/eclair-core/src/main/scala/fr/acinq/eclair/payment/PaymentEvents.scala#L221\n>> [6]\n>> https://github.com/ElementsProject/lightning/blob/62bfed9a8df8731be44ba4e86afb08a5d28a4442/plugins/libplugin-pay.c#L1461\n>> [7]\n>> https://github.com/lightningdevkit/rust-lightning/blob/e61f3a238a70cbac87209e223b7c396108a49b97/lightning-invoice/src/payment.rs#L682\n>>\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n>\nOn Thu, Oct 20, 2022 at 5:36 PM Bastien TEINTURIER <bastien at acinq.fr> wrote:\n\n> Hi Joost,\n>\n> I need more time to review your proposed change, but I wanted to quickly\n> correct a misunderstanding you had in quoting eclair's code:\n>\n> > Unfortunately it is possible for nodes on the route to hide themselves.\n> > If they return random data as the failure message, the sender won't know\n> > where the failure happened. Some senders then penalize all nodes that\n> > were part of the route [4][5]. This may exclude perfectly reliable nodes\n> > from being used for future payments.\n>\n> Eclair's code does not penalize nodes for future payment attempts in this\n> case. It only ignores them for the retries of that particular payment.\n>\n> Cheers,\n> Bastien\n>\n> Le mer. 19 oct. 2022 \u00e0 13:13, Joost Jager <joost.jager at gmail.com> a\n> \u00e9crit :\n>\n>> Hi list,\n>>\n>> I wanted to get back to a long-standing issue in Lightning: gaps in error\n>> attribution. I've posted about this before back in 2019 [1].\n>>\n>> Error attribution is important to properly penalize nodes after a payment\n>> failure occurs. The goal of the penalty is to give the next attempt a\n>> better chance at succeeding. In the happy failure flow, the sender is able\n>> to determine the origin of the failure and penalizes a single node or pair\n>> of nodes.\n>>\n>> Unfortunately it is possible for nodes on the route to hide themselves.\n>> If they return random data as the failure message, the sender won't know\n>> where the failure happened. Some senders then penalize all nodes that were\n>> part of the route [4][5]. This may exclude perfectly reliable nodes from\n>> being used for future payments. Other senders penalize no nodes at all\n>> [6][7], which allows the offending node to keep the disruption going.\n>>\n>> A special case of this is a final node sending back random data. Senders\n>> that penalize all nodes will keep looking for alternative routes. But\n>> because each alternative route still ends with that same final node, the\n>> sender will ultimately penalize all of its peers and possibly a lot of the\n>> rest of the network too.\n>>\n>> I can think of various reasons for exploiting this weakness. One is just\n>> plain grievance for whatever reason. Another one is to attract more traffic\n>> by getting competing routing nodes penalized. Or the goal could be to\n>> sufficiently mess up reputation tracking of a specific sender node to make\n>> it hard for that node to make further payments.\n>>\n>> Related to this are delays in the path. A node can delay propagating back\n>> a failure message and the sender won't be able to determine which node did\n>> it.\n>>\n>> The link at the top of this post [1] describes a way to address both\n>> unreadable failure messages as well as delays by letting each node on the\n>> route append a timestamp and hmac to the failure message. The great\n>> challenge is to do this in such a way that nodes don\u2019t learn their position\n>> in the path.\n>>\n>> I'm revisiting this idea, and have prototyped various ways to implement\n>> it. In the remainder of this post, I will describe the variant that I\n>> thought works best (so far).\n>>\n>> # Failure message format\n>>\n>> The basic idea of the new format is to let each node (not just the error\n>> source) commit to the failure message when it passes it back by adding an\n>> hmac. The sender verifies all hmacs upon receipt of the failure message.\n>> This makes it impossible for any of the nodes to modify the failure message\n>> without revealing that they might have played a part in the modification.\n>> It won\u2019t be possible for the sender to pinpoint an exact node, because\n>> either end of a communication channel may have modified the message.\n>> Pinpointing a pair of nodes however is good enough, and is commonly done\n>> for regular onion failures too.\n>>\n>> On the highest level, the new failure message consists of three parts:\n>>\n>> `message` (var len) | `payloads` (fixed len) | `hmacs` (fixed len)\n>>\n>> * `message` is the standard onion failure message as described in [2],\n>> but without the hmac. The hmac is now part of `hmacs` and doesn't need to\n>> be repeated.\n>>\n>> * `payloads` is a fixed length array that contains space for each node\n>> (`hop_payload`) on the route to add data to return to the sender. Ideally\n>> the contents and size of `hop_payload` is signaled so that future\n>> extensions don\u2019t require all nodes to upgrade. For now, we\u2019ll assume the\n>> following 9-byte format:\n>>\n>>   `is_final` (1 byte) | `duration` (8 bytes)\n>>\n>>   `is_final` indicates whether this node is the failure source. The\n>> sender uses `is_final` to determine when to stop the\n>> decryption/verification process.\n>>\n>>   `duration` is the time in milliseconds that the node held the htlc. By\n>> observing the series of reported durations, the sender is able to pinpoint\n>> a delay down to a pair of nodes.\n>>\n>>   The `hop_payload` is repeated 27 times (the maximum route length).\n>>\n>>   Every hop shifts `payloads` 9 bytes to the right and puts its own\n>> `hop_payload` in the 9 left-most bytes.\n>>\n>> * `hmacs` is a fixed length array where nodes add their hmacs as the\n>> failure message travels back to the sender.\n>>\n>>   To keep things simple, I'll describe the format as if the maximum route\n>> length was only three hops (instead of 27):\n>>\n>>   `hmac_0_2` | `hmac_0_1`| `hmac_0_0`| `hmac_1_1`| `hmac_1_0`| `hmac_2_0`\n>>\n>>   Because nodes don't know their position in the path, it's unclear to\n>> them what part of the failure message they are supposed to include in the\n>> hmac. They can't just include everything, because if part of that data is\n>> deleted later (to keep the message size fixed) it opens up the possibility\n>> for nodes to blame others.\n>>\n>>   The solution here is to provide hmacs for all possible positions. The\n>> last node that updated `hmacs` added `hmac_0_2`, `hmac_0_1` and `hmac_0_0`\n>> to the block. Each hmac corresponds to a presumed position in the path,\n>> where `hmac_0_2` is for the longest path (2 downstream hops) and `hmac_0_0`\n>> for the shortest (node is the error source).\n>>\n>>   `hmac_x_y` is the hmac added by node x (counted from the node that is\n>> currently handling the failure message) assuming that this node is y hops\n>> away from the final node.\n>>\n>> Before an hop adds its hmacs, it first deletes some of the previous\n>> hmacs. This keeps the failure message at a fixed length. The removed hmacs\n>> are the ones that cannot be useful anymore. If node 0 adds itself, the\n>> former node 0 (now node 1) cannot be at the first position anymore. The\n>> former node 1 (now node 2) cannot be at the second position anymore. The\n>> former node 2 cannot be the source of the error anymore and isn\u2019t\n>> represented in the failure message any longer. The corresponding hmacs (the\n>> now non-existent `hmac_0_2`, `hmac_1_1` and `hmac_2_0`) are deleted by node\n>> 0.\n>>\n>> Deleting the useless data reduces the number of hmacs (and roughly the\n>> total failure message size) to half.\n>>\n>> The delete operation transform the fields above to:\n>>\n>> <empty> | <empty> | <empty> | `hmac_0_1`| `hmac_0_0`| `hmac_1_0`\n>>\n>> The exact data that is included in each hmac is:\n>>   * `message`\n>>   * the node\u2019s own `hop_payload` and a set of downstream `hop_payload`s,\n>> depending on assumed position\n>>   * a set of downstream node hmacs, depending on assumed position\n>>\n>> For example `hmac_0_1` is based on:\n>>\n>> `message` | `hop_payload[0]` | `hop_payload[1]` | `hmac_1_0`\n>>\n>> If the node that is currently handling the failure message is one hop\n>> away from the final node, it needs to cover its own `hop_payload[0]`, the\n>> final node `hop_payload[1]` and the final node hmac `hmac_1_0`.\n>>\n>> A longer path is committed to in `hmac_0_2`:\n>>\n>> `message` | `hop_payload[0]` | `hop_payload[1]` | `hop_payload[2]` |\n>> `hmac_1_1` | `hmac_2_0`\n>>\n>> The current node is two hops away from the final node. It needs to cover\n>> its own `hop_payload[0]` as well as `hop_payload[1]` and `hop_payload[2]`\n>> for the next and final hops. Additionally it covers the next hop `hmac_1_1`\n>> and final hop `hmac_2_0`, which correspond to the positions of those nodes\n>> in the path that is assumed for `hmac_0_2`.\n>>\n>> With this information, the sender is able to verify the longest chain of\n>> hmacs until it encounters a `hop_payload` with `is_final` set.\n>>\n>> If any of the nodes messes with any byte in the failure message, the\n>> sender is always able to determine a pair of nodes that the offending node\n>> is part of. This statement can be verified through reasoning, but to be\n>> sure I also tested it with code. I\u2019ve simulated a malicious node that\n>> modifies a byte of the failure message at index x and observed the error\n>> source as determined by the sender. For every x, the sender reports the\n>> same correct pair.\n>>\n>> # Size\n>>\n>> The obvious downside of the scheme above is the size. Given a maximum of\n>> 27 hops, the `hmacs` block contains 27+26+25+...+1=378 hmacs of 32 bytes\n>> each. This makes for a total size of 12 KB.\n>>\n>> It could be the case though that it is not possible to devise a more\n>> compact scheme that also preserves the existing privacy guarantees. I know\n>> that smart people have spent time on this problem, but nonetheless no\n>> better solution has come up in the past years. A proof of its non-existence\n>> would be interesting for sure.\n>>\n>> I personally think the size increase is justified to fix this\n>> vulnerability in Lightning. Also if failures are expected to become more\n>> rare going forward, size becomes less relevant to the overall operation of\n>> the network.\n>>\n>> Another option is to reduce the maximum number of hops. It is\n>> questionable whether 27 hops are really needed in practice, and such long\n>> routes also contribute to latency and capital lock up. If for example the\n>> new failure message could only be used with routes up to 10 hops, the total\n>> number of hmacs would drop from 378 to 55. This makes for a total message\n>> size of about 2 KB.\n>>\n>> # Signaling\n>>\n>> For backwards compatibility nodes need to know what algorithm they should\n>> run to generate or transform the failure message. This can be signaled by\n>> the sender via a tlv onion field. A failure message format signaling\n>> mechanism is also discussed in the context of long failure messages [3].\n>> The failure message described in this post could be just another version.\n>>\n>> Additionally, intermediate nodes need to advertise their capability to\n>> transform the new format through a feature bit.\n>>\n>> # Delayed successes\n>>\n>> It\u2019s not just failures that can be delayed. Successes can too. In that\n>> case, there is no failure message to improve. It could be an option to add\n>> the same `payloads` and `hmacs` blocks to the `update_fulfill_htlc` message.\n>>\n>> [1]\n>> https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-June/002015.html\n>> [2]\n>> https://github.com/lightning/bolts/blob/master/04-onion-routing.md#returning-errors\n>> [3] https://github.com/lightning/bolts/pull/1021\n>> [4]\n>> https://github.com/lightningnetwork/lnd/blob/4fbd608b734f348d7e79fbfc7feaecc5c6c33a90/routing/result_interpretation.go#L419\n>> [5]\n>>\n>> https://github.com/ACINQ/eclair/blob/a0433aa0c027c9be618c5afe18e7f91642a7f372/eclair-core/src/main/scala/fr/acinq/eclair/payment/PaymentEvents.scala#L221\n>> [6]\n>> https://github.com/ElementsProject/lightning/blob/62bfed9a8df8731be44ba4e86afb08a5d28a4442/plugins/libplugin-pay.c#L1461\n>> [7]\n>> https://github.com/lightningdevkit/rust-lightning/blob/e61f3a238a70cbac87209e223b7c396108a49b97/lightning-invoice/src/payment.rs#L682\n>>\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20221020/edbbd786/attachment-0001.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2022-10-24T22:32:52",
                "message_text_only": "Joost Jager <joost.jager at gmail.com> writes:\n> Hi list,\n>\n> I wanted to get back to a long-standing issue in Lightning: gaps in error\n> attribution. I've posted about this before back in 2019 [1].\n\nHi Joost!\n\n        Thanks for writing this up fully.  Core lightning also doesn't\npenalize properly, because of the attribution problem: solving this lets\nus penalize a channel, at least.\n\n        I want to implement this too, to make sure I understand it\ncorrectly, but having read it twice it seems reasonable.\n\n        How about 16 hops?  It's the closest power of 2 to the legacy hop\nlimit, and makes this 4.5k for payloads and hmacs.\n\n        There is, however, a completely different possibility if we want\nto use a pre-pay scheme, which I think I've described previously.  You\nsend N sats and a secp point; every chained secret returned earns the\nforwarder 1 sat[1].  The answers of course are placed in each layer of\nthe onion.  You know how far the onion got based on how much money you\ngot back on failure[2], though the error message may be corrupted.\n\nCheers,\nRusty.\n[1] Simplest is truncate the point to a new secret key.  Each node would\n    apply a tweak for decorrelation ofc.\n[2] The best scheme is that you don't get paid unless the next node\n    decrypts, actually, but that needs more thought."
            }
        ],
        "thread_summary": {
            "title": "Fat Errors",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Bastien TEINTURIER",
                "Rusty Russell",
                "Joost Jager"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 47275
        }
    },
    {
        "title": "[Lightning-dev] Dynamic Commitments Part 2: Taprooty Edition",
        "thread_messages": [
            {
                "author": "Johan Tor\u00e5s Halseth",
                "date": "2022-10-27T09:16:58",
                "message_text_only": "Hi, Laolu.\n\nI think it could be worth considering dividing the taprootyness of a\nchannel into two:\n1) taproot funding output\n2) taproot commitment outputs\n\nThat way we could upgrade existing channels only on the commitment level,\nnot needing to close or re-anchor the channels using an adapter in order to\nget many of the taproot benefits.\n\nNew channels would use taproot multisig (musig2) for the funding output.\n\nThis seems to be less disruptive to the existing network, and we could get\nfeatures enabled by taproot to larger parts of the network quicker. And to\nme this seems to carry less complexity (and closing fees) than an adapter.\n\nOne caveat is that this wouldn't work (I think) for Eltoo channels, as the\nfunding output would not be plain multisig anymore.\n\n- Johan\n\nOn Sat, Mar 26, 2022 at 1:27 AM Antoine Riard <antoine.riard at gmail.com>\nwrote:\n\n> Hi Laolu,\n>\n> Thanks for the proposal, quick feedback.\n>\n> > It *is* still the case that _ultimately_ the two transactions to close\n> the\n> > old segwit v0 funding output, and re-open the channel with a new segwit\n> v1\n> > funding output are unavoidable. However this adapter commitment lets\n> peers\n> > _defer_ these two transactions until closing time.\n>\n> I think there is one downside coming with adapter commitment, which is the\n> uncertainty of the fee overhead at the closing time. Instead of closing\n> your segwit v0 channel _now_ with known fees, when your commitment is empty\n> of time-sensitive HTLCs, you're taking the risk of closing during fees\n> spikes, due a move triggered by your counterparty, when you might have\n> HTLCs at stake.\n>\n> It might be more economically rational for a LN node operator to pay the\n> upgrade cost now if they wish  to benefit from the taproot upgrade early,\n> especially if long-term we expect block fees to increase, or wait when\n> there is a \"normal\" cooperative closing.\n>\n> So it's unclear to me what the economic gain of adapter commitments ?\n>\n> > In the remainder of this mail, I'll describe an alternative\n> > approach that would allow upgrading nearly all channel/commitment related\n> > values (dust limit, max in flight, etc), which is inspired by the way the\n> > Raft consensus protocol handles configuration/member changes.\n>\n> Long-term, I think we'll likely need a consensus protocol anyway for\n> multi-party constructions (channel factories/payment pools). AFAIU this\n> proposal doesn't aim to roll out a full-fledged consensus protocol *now*\n> though it could be wise to ensure what we're building slowly moves in this\n> direction. Less critical code to maintain across bitcoin\n> codebases/toolchains.\n>\n> > The role of the signature it to prevent \"spoofing\" by one of the parties\n> > (authenticate the param change), and also it serves to convince a party\n> that\n> > they actually sent a prior commitment propose update during the\n> > retransmission phase.\n>\n> What's the purpose of data origin authentication if we assume only\n> two-parties running over Noise_XK ?\n>\n> I think it's already a security property we have. Though if we think we're\n> going to reuse these dynamic upgrades for N counterparties communicating\n> through a coordinator, yes I think it's useful.\n>\n> > In the past, when ideas like this were brought up, some were concerned\n> that\n> > it wouldn't really be possible to do this type of updates while existing\n> > HTLCs were in flight (hence some of the ideas to clear out the commitment\n> > beforehand).\n>\n> The dynamic upgrade might serve in an emergency context where we don't\n> have the leisury to wait for the settlement of the pending HTLCs. The\n> timing of those ones might be beyond the coordination of link\n> counterparties. Thus, we have to allow upgrade of non-empty commitments\n> (and if there are undesirable interferences between new commitment types\n> and HTLCs/PTLCs present, deal case-by-case).\n>\n> Antoine\n>\n> Le jeu. 24 mars 2022 \u00e0 18:53, Olaoluwa Osuntokun <laolu32 at gmail.com> a\n> \u00e9crit :\n>\n>> Hi y'all,\n>>\n>> ## Dynamic Commitments Retrospective\n>>\n>> Two years-ish ago I made a mailing list post on some ideas re dynamic\n>> commitments [1], and how the concept can be used to allow us to upgrade\n>> channel types on the fly, and also remove pesky hard coded limits like the\n>> 483 HTLC in-flight limit that's present today. Back then my main target\n>> was\n>> upgrading all the existing channels over to the anchor output commitment\n>> variant, so the core internal routing network would be more resilient in a\n>> persistent high fee environment (which hasn't really happened over the\n>> past\n>> 2 years for various reasons tbh). Fast forward to today, and with taproot\n>> now active on mainnet, and some initial design work/sketches for\n>> taproot-native channels underway, I figure it would be good to bump this\n>> concept as it gives us a way to upgrade all 80k+ public channels to\n>> taproot\n>> without any on chain transactions.\n>>\n>> ## Updating Across Witness Versions w/ Adapter Commitments\n>>\n>> In my original mail, I incorrectly concluded that the dynamic commitments\n>> concept would only really work within the confines of a \"static\" multi-sig\n>> output, meaning that it couldn't be used to help channels upgrade to\n>> future\n>> segwit witness versions.  Thankfully this reply [2] by ZmnSCPxj, outlined\n>> a\n>> way to achieve this in practice. At a high level he proposes an \"adaptor\n>> commitment\" (similar to the kickoff transaction in eltoo/duplex), which is\n>> basically an upgrade transaction that spends one witness version type, and\n>> produces an output with the next (upgraded) type. In the context of\n>> converting from segwit v0 to v1 (taproot), two peers would collaboratively\n>> create a new adapter commitment that spends the old v0 multi-sig output,\n>> and\n>> produces a _new_ v1 multi-sig output. The new commitment transaction would\n>> then be anchored using this new output.\n>>\n>> Here's a rough sequence diagram of the before and after state to better\n>> convey the concept:\n>>\n>>   * Before: fundingOutputV0 -> commitmentTransaction\n>>\n>>   * After fundingOutputV0 -> fundingOutputV1 (the adapter) ->\n>>     commitmentTransaction\n>>\n>> It *is* still the case that _ultimately_ the two transactions to close the\n>> old segwit v0 funding output, and re-open the channel with a new segwit v1\n>> funding output are unavoidable. However this adapter commitment lets peers\n>> _defer_ these two transactions until closing time. When force closing two\n>> transactions need to be confirmed before the commitment outputs can be\n>> resolved. However, for co-op close, you can just spend the v0 output, and\n>> deliver to the relevant P2TR outputs. The adapter commitment can leverage\n>> sighash anyonecanpay to let both parties (assuming it's symmetric) attach\n>> additional inputs for fees (to avoid introducing the old update_fee\n>> related\n>> static fee issues), or alternatively inherit the anchor output pattern at\n>> this level.\n>>\n>> ## Existing Dynamic Commitments Proposals\n>>\n>> Assuming this concept holds up, then we need an actual concrete protocol\n>> to\n>> allow for dynamic commitment updates. Last year, Rusty made a spec PR\n>> outlining a way to upgrade the commitment type (leveraging the new\n>> commitment type feature bits) upon channel re-establish [3]. The proposal\n>> relies on another message that both sides send (`stfu`) to clear the\n>> commitment (similar to the shutdown semantics) before the switch over\n>> happens. However as this is tied to the channel re-establish flow, it\n>> doesn't allow both sides to do things like only allow your peer to attach\n>> N\n>> HTLCs to start with, slowing increasing their allotted slots and possibly\n>> reducing them (TCP AIMD style).\n>>\n>> ## A Two-Phase Dynamic Commitment Update Protocol\n>>\n>> IMO if we're adding in a way to do commitment/channel upgrades, then it\n>> may\n>> be worthwhile to go with a more generalized, but slightly more involved\n>> route instead. In the remainder of this mail, I'll describe an alternative\n>> approach that would allow upgrading nearly all channel/commitment related\n>> values (dust limit, max in flight, etc), which is inspired by the way the\n>> Raft consensus protocol handles configuration/member changes.\n>>\n>> For those that aren't aware, Raft is a consensus protocol analogous to\n>> Paxos\n>> (but isn't byzantine fault tolerant out of the box) that was designed as a\n>> more understandable alternative to Paxos for a pedagogical environment.\n>> Typically the algorithm is run in the context of a fixed cluster with N\n>> machines, but supports adding/removing machines from the cluster with a\n>> configuration update protocol. At a high level the way this works is that\n>> a\n>> new config is sent to the leader, with the leader synchronizing the config\n>> change with the other members of the cluster. Once a majority threshold is\n>> reached, the leader then commits the config change with the acknowledged\n>> parties using the new config (basically a two phase commit). I'm skipping\n>> over some edge cases here that can arise if the new nodes participate\n>> consensus too early, which can cause a split majority leading to two\n>> leaders\n>> being elected.\n>>\n>> Applying this to the LN context is a bit simpler than a generalized\n>> protocol, as we typically just have two parties involved. The initiator is\n>> already naturally a \"leader\" in our context, as they're the only ones that\n>> can do things like trigger fee updates.\n>>\n>> ### Message Structure\n>>\n>> At a high level I propose we introduce two new messages, with the fields\n>> looking something like this for `commitment_update_propose`:\n>>  * type: 0 (`channel_id`)\n>>    * value: [`32*byte`:`chan_id`]\n>>  * type: 1 (`propose_sig`)\n>>    * value: [`64*byte`:`sig`]\n>>  * type: 2 (`update_payload`)\n>>    * value: [`*byte`:`tlv_payload`]\n>>\n>> and this `commitment_update_apply`:\n>>  * type: 0 (`channel_id`)\n>>    * value: [`32*byte`:`chan_id`]\n>>  * type: 1 (`local_propose`)\n>>    * value: [`*byte`:`commitment_update_propose`]\n>>  * type: 2 (`remote_propose`)\n>>    * value: [`*byte`:`commitment_update_propose`]\n>>\n>> ### Protocol Flow\n>>\n>> The core idea here is that either party can propose a commitment/channel\n>> param update, but only the initiator can actually apply it. The\n>> `commitment_update_propose` encodes the new set of updates, with a\n>> signature\n>> covering the TLV blob for the new params (more on why that's needed\n>> later).\n>> The `commitment_update_apply` includes up to _two_\n>> `commitment_update_propose` messages (one for the initiator and one for\n>> the\n>> responder, as nested TLV messages). The `commitment_update_propose`\n>> message\n>> would be treated like any other `update_*` message, in that it takes a new\n>> commitment signature to properly commit/apply it.\n>>\n>> The normal flow takes the form of both sides sending a\n>> `commitment_update_propose` message, with the initiator finally committing\n>> both by sending a `commitment_update_apply` message. In the event that\n>> only\n>> the responder wants to apply a param change/update, then the initiator can\n>> reply immediately with a `commitment_update_apply` message that doesn't\n>> include a param change for their commitment (or they just echo the\n>> parameters if they're acceptable).\n>>\n>> ### Handling Retransmissions\n>>\n>> The role of the signature it to prevent \"spoofing\" by one of the parties\n>> (authenticate the param change), and also it serves to convince a party\n>> that\n>> they actually sent a prior commitment propose update during the\n>> retransmission phase. As the `commitment_update_propose` message would be\n>> retransmitted like any other message, if the initiator attempts to commit\n>> the update but the connection dies, they'll retransmit it as normal along\n>> with their latest signature.\n>>\n>> ### Nested TLV Param Generality\n>>\n>> The messages as sketched out here just have an opaque nested TLV field\n>> which\n>> makes it extensible to add in other things like tweaking the total number\n>> of\n>> max HTLCs, the current dust values, min/max HTLCs, etc (all things that\n>> are\n>> currently hard coded for the lifetime of the channel). An initial target\n>> would likely just be a `chan_type` field, with future feature bits\n>> governing\n>> _what_ type of commitment updates both parties understand in the future.\n>>\n>> In the past, when ideas like this were brought up, some were concerned\n>> that\n>> it wouldn't really be possible to do this type of updates while existing\n>> HTLCs were in flight (hence some of the ideas to clear out the commitment\n>> beforehand). I don't see a reason why this fundamentally _shouldn't_ be\n>> allowed, as from the point of view of the channel update state machine,\n>> all\n>> updates (adds/removes) get applied as normal, but with this _new_\n>> commitment\n>> type/params. The main edge case we'll need to consider is cases where the\n>> new params make older HTLCs invalid for some reason.\n>>\n>> ## Conclusion\n>>\n>> Using the adapter commitment idea combined with a protocol for updating\n>> commitments on the fly, would potentially allow us to update all 80k+\n>> segwit\n>> v0 channels to the base level of taprooty channels without any on chain\n>> transactions. The two transactions (open+close) must happen eventually,\n>> but\n>> by holding another layer of spends off-chain we can defer them\n>> (potentially\n>> indefinitely, as we have channels today that have been opened for over a\n>> year).\n>>\n>> Deploying a generalised on-the-fly dynamic commitment update protocol\n>> gives\n>> us a tool to future proof the _existing_ anchored multi-sig outputs in the\n>> chain, and also a way to remove many of the hard coded parameters we have\n>> today in the protocol. One overly inflexible parameter we have today in\n>> the\n>> network is the 483 HTLC limit. Allowing this value to float would allow\n>> peers to apply similar congestion avoidance algorithm that are used in TCP\n>> today, and also give us a way to protect the network against future\n>> unforeseen widespread policy changes (like a raising of the dust limit).\n>>\n>> -- Laolu\n>>\n>> [1]:\n>> https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-July/002763.html\n>> [2]:\n>> https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-July/002770.html\n>> [3]: https://github.com/lightning/bolts/pull/868\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20221027/70db062c/attachment-0001.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2022-10-27T14:53:52",
                "message_text_only": "An HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20221027/dca43d02/attachment-0001.html>"
            },
            {
                "author": "Johan Tor\u00e5s Halseth",
                "date": "2022-10-28T07:35:42",
                "message_text_only": "Hi, Matt.\n\nYou're correct, I made the suggestion mainly because it would open up for\nPTLCs (or other future features) in today's channels.\n\nHaving the existing network close and reopen new channels would really slow\nthe adoption of new channel features I reckon.\n\nAnd I don't think it adds much complexity compared to the adapter approach.\n\n- Johan\n\nOn Thu, Oct 27, 2022 at 4:54 PM Matt Corallo <lf-lists at mattcorallo.com>\nwrote:\n\n> I\u2019m not sure I understand this - is there much reason to want taproot\n> commitment outputs? I mean they\u2019re cool, and witnesses are a bit smaller,\n> which is nice I guess, but they\u2019re not providing materially new features,\n> AFAIU. Taproot funding, on the other hand, provides a Bitcoin-wide privacy\n> improvement as well the potential future ability of channel participants to\n> use multisig for their own channel funds transparently.\n>\n> Sure, if we\u2019re doing taproot funding outputs we should probably just do it\n> for the commitment outputs as well, because why not (and it\u2019s a prereq for\n> PTLCs). But trying to split them up seems like added complexity \u201cjust\n> because\u201d? I suppose it tees us up for eventual PTLC support in todays\n> channels, but we can also consider that separately when we get to that\n> point, IMO.\n>\n> Am I missing some important utility of taproot commitment transaction\n> outputs?\n>\n> Matt\n>\n> On Oct 27, 2022, at 02:17, Johan Tor\u00e5s Halseth <johanth at gmail.com> wrote:\n>\n> \ufeff\n> Hi, Laolu.\n>\n> I think it could be worth considering dividing the taprootyness of a\n> channel into two:\n> 1) taproot funding output\n> 2) taproot commitment outputs\n>\n> That way we could upgrade existing channels only on the commitment level,\n> not needing to close or re-anchor the channels using an adapter in order to\n> get many of the taproot benefits.\n>\n> New channels would use taproot multisig (musig2) for the funding output.\n>\n> This seems to be less disruptive to the existing network, and we could get\n> features enabled by taproot to larger parts of the network quicker. And to\n> me this seems to carry less complexity (and closing fees) than an adapter.\n>\n> One caveat is that this wouldn't work (I think) for Eltoo channels, as the\n> funding output would not be plain multisig anymore.\n>\n> - Johan\n>\n> On Sat, Mar 26, 2022 at 1:27 AM Antoine Riard <antoine.riard at gmail.com>\n> wrote:\n>\n>> Hi Laolu,\n>>\n>> Thanks for the proposal, quick feedback.\n>>\n>> > It *is* still the case that _ultimately_ the two transactions to close\n>> the\n>> > old segwit v0 funding output, and re-open the channel with a new segwit\n>> v1\n>> > funding output are unavoidable. However this adapter commitment lets\n>> peers\n>> > _defer_ these two transactions until closing time.\n>>\n>> I think there is one downside coming with adapter commitment, which is\n>> the uncertainty of the fee overhead at the closing time. Instead of closing\n>> your segwit v0 channel _now_ with known fees, when your commitment is empty\n>> of time-sensitive HTLCs, you're taking the risk of closing during fees\n>> spikes, due a move triggered by your counterparty, when you might have\n>> HTLCs at stake.\n>>\n>> It might be more economically rational for a LN node operator to pay the\n>> upgrade cost now if they wish  to benefit from the taproot upgrade early,\n>> especially if long-term we expect block fees to increase, or wait when\n>> there is a \"normal\" cooperative closing.\n>>\n>> So it's unclear to me what the economic gain of adapter commitments ?\n>>\n>> > In the remainder of this mail, I'll describe an alternative\n>> > approach that would allow upgrading nearly all channel/commitment\n>> related\n>> > values (dust limit, max in flight, etc), which is inspired by the way\n>> the\n>> > Raft consensus protocol handles configuration/member changes.\n>>\n>> Long-term, I think we'll likely need a consensus protocol anyway for\n>> multi-party constructions (channel factories/payment pools). AFAIU this\n>> proposal doesn't aim to roll out a full-fledged consensus protocol *now*\n>> though it could be wise to ensure what we're building slowly moves in this\n>> direction. Less critical code to maintain across bitcoin\n>> codebases/toolchains.\n>>\n>> > The role of the signature it to prevent \"spoofing\" by one of the parties\n>> > (authenticate the param change), and also it serves to convince a party\n>> that\n>> > they actually sent a prior commitment propose update during the\n>> > retransmission phase.\n>>\n>> What's the purpose of data origin authentication if we assume only\n>> two-parties running over Noise_XK ?\n>>\n>> I think it's already a security property we have. Though if we think\n>> we're going to reuse these dynamic upgrades for N counterparties\n>> communicating through a coordinator, yes I think it's useful.\n>>\n>> > In the past, when ideas like this were brought up, some were concerned\n>> that\n>> > it wouldn't really be possible to do this type of updates while existing\n>> > HTLCs were in flight (hence some of the ideas to clear out the\n>> commitment\n>> > beforehand).\n>>\n>> The dynamic upgrade might serve in an emergency context where we don't\n>> have the leisury to wait for the settlement of the pending HTLCs. The\n>> timing of those ones might be beyond the coordination of link\n>> counterparties. Thus, we have to allow upgrade of non-empty commitments\n>> (and if there are undesirable interferences between new commitment types\n>> and HTLCs/PTLCs present, deal case-by-case).\n>>\n>> Antoine\n>>\n>> Le jeu. 24 mars 2022 \u00e0 18:53, Olaoluwa Osuntokun <laolu32 at gmail.com> a\n>> \u00e9crit :\n>>\n>>> Hi y'all,\n>>>\n>>> ## Dynamic Commitments Retrospective\n>>>\n>>> Two years-ish ago I made a mailing list post on some ideas re dynamic\n>>> commitments [1], and how the concept can be used to allow us to upgrade\n>>> channel types on the fly, and also remove pesky hard coded limits like\n>>> the\n>>> 483 HTLC in-flight limit that's present today. Back then my main target\n>>> was\n>>> upgrading all the existing channels over to the anchor output commitment\n>>> variant, so the core internal routing network would be more resilient in\n>>> a\n>>> persistent high fee environment (which hasn't really happened over the\n>>> past\n>>> 2 years for various reasons tbh). Fast forward to today, and with taproot\n>>> now active on mainnet, and some initial design work/sketches for\n>>> taproot-native channels underway, I figure it would be good to bump this\n>>> concept as it gives us a way to upgrade all 80k+ public channels to\n>>> taproot\n>>> without any on chain transactions.\n>>>\n>>> ## Updating Across Witness Versions w/ Adapter Commitments\n>>>\n>>> In my original mail, I incorrectly concluded that the dynamic commitments\n>>> concept would only really work within the confines of a \"static\"\n>>> multi-sig\n>>> output, meaning that it couldn't be used to help channels upgrade to\n>>> future\n>>> segwit witness versions.  Thankfully this reply [2] by ZmnSCPxj,\n>>> outlined a\n>>> way to achieve this in practice. At a high level he proposes an \"adaptor\n>>> commitment\" (similar to the kickoff transaction in eltoo/duplex), which\n>>> is\n>>> basically an upgrade transaction that spends one witness version type,\n>>> and\n>>> produces an output with the next (upgraded) type. In the context of\n>>> converting from segwit v0 to v1 (taproot), two peers would\n>>> collaboratively\n>>> create a new adapter commitment that spends the old v0 multi-sig output,\n>>> and\n>>> produces a _new_ v1 multi-sig output. The new commitment transaction\n>>> would\n>>> then be anchored using this new output.\n>>>\n>>> Here's a rough sequence diagram of the before and after state to better\n>>> convey the concept:\n>>>\n>>>   * Before: fundingOutputV0 -> commitmentTransaction\n>>>\n>>>   * After fundingOutputV0 -> fundingOutputV1 (the adapter) ->\n>>>     commitmentTransaction\n>>>\n>>> It *is* still the case that _ultimately_ the two transactions to close\n>>> the\n>>> old segwit v0 funding output, and re-open the channel with a new segwit\n>>> v1\n>>> funding output are unavoidable. However this adapter commitment lets\n>>> peers\n>>> _defer_ these two transactions until closing time. When force closing two\n>>> transactions need to be confirmed before the commitment outputs can be\n>>> resolved. However, for co-op close, you can just spend the v0 output, and\n>>> deliver to the relevant P2TR outputs. The adapter commitment can leverage\n>>> sighash anyonecanpay to let both parties (assuming it's symmetric) attach\n>>> additional inputs for fees (to avoid introducing the old update_fee\n>>> related\n>>> static fee issues), or alternatively inherit the anchor output pattern at\n>>> this level.\n>>>\n>>> ## Existing Dynamic Commitments Proposals\n>>>\n>>> Assuming this concept holds up, then we need an actual concrete protocol\n>>> to\n>>> allow for dynamic commitment updates. Last year, Rusty made a spec PR\n>>> outlining a way to upgrade the commitment type (leveraging the new\n>>> commitment type feature bits) upon channel re-establish [3]. The proposal\n>>> relies on another message that both sides send (`stfu`) to clear the\n>>> commitment (similar to the shutdown semantics) before the switch over\n>>> happens. However as this is tied to the channel re-establish flow, it\n>>> doesn't allow both sides to do things like only allow your peer to\n>>> attach N\n>>> HTLCs to start with, slowing increasing their allotted slots and possibly\n>>> reducing them (TCP AIMD style).\n>>>\n>>> ## A Two-Phase Dynamic Commitment Update Protocol\n>>>\n>>> IMO if we're adding in a way to do commitment/channel upgrades, then it\n>>> may\n>>> be worthwhile to go with a more generalized, but slightly more involved\n>>> route instead. In the remainder of this mail, I'll describe an\n>>> alternative\n>>> approach that would allow upgrading nearly all channel/commitment related\n>>> values (dust limit, max in flight, etc), which is inspired by the way the\n>>> Raft consensus protocol handles configuration/member changes.\n>>>\n>>> For those that aren't aware, Raft is a consensus protocol analogous to\n>>> Paxos\n>>> (but isn't byzantine fault tolerant out of the box) that was designed as\n>>> a\n>>> more understandable alternative to Paxos for a pedagogical environment.\n>>> Typically the algorithm is run in the context of a fixed cluster with N\n>>> machines, but supports adding/removing machines from the cluster with a\n>>> configuration update protocol. At a high level the way this works is\n>>> that a\n>>> new config is sent to the leader, with the leader synchronizing the\n>>> config\n>>> change with the other members of the cluster. Once a majority threshold\n>>> is\n>>> reached, the leader then commits the config change with the acknowledged\n>>> parties using the new config (basically a two phase commit). I'm skipping\n>>> over some edge cases here that can arise if the new nodes participate\n>>> consensus too early, which can cause a split majority leading to two\n>>> leaders\n>>> being elected.\n>>>\n>>> Applying this to the LN context is a bit simpler than a generalized\n>>> protocol, as we typically just have two parties involved. The initiator\n>>> is\n>>> already naturally a \"leader\" in our context, as they're the only ones\n>>> that\n>>> can do things like trigger fee updates.\n>>>\n>>> ### Message Structure\n>>>\n>>> At a high level I propose we introduce two new messages, with the fields\n>>> looking something like this for `commitment_update_propose`:\n>>>  * type: 0 (`channel_id`)\n>>>    * value: [`32*byte`:`chan_id`]\n>>>  * type: 1 (`propose_sig`)\n>>>    * value: [`64*byte`:`sig`]\n>>>  * type: 2 (`update_payload`)\n>>>    * value: [`*byte`:`tlv_payload`]\n>>>\n>>> and this `commitment_update_apply`:\n>>>  * type: 0 (`channel_id`)\n>>>    * value: [`32*byte`:`chan_id`]\n>>>  * type: 1 (`local_propose`)\n>>>    * value: [`*byte`:`commitment_update_propose`]\n>>>  * type: 2 (`remote_propose`)\n>>>    * value: [`*byte`:`commitment_update_propose`]\n>>>\n>>> ### Protocol Flow\n>>>\n>>> The core idea here is that either party can propose a commitment/channel\n>>> param update, but only the initiator can actually apply it. The\n>>> `commitment_update_propose` encodes the new set of updates, with a\n>>> signature\n>>> covering the TLV blob for the new params (more on why that's needed\n>>> later).\n>>> The `commitment_update_apply` includes up to _two_\n>>> `commitment_update_propose` messages (one for the initiator and one for\n>>> the\n>>> responder, as nested TLV messages). The `commitment_update_propose`\n>>> message\n>>> would be treated like any other `update_*` message, in that it takes a\n>>> new\n>>> commitment signature to properly commit/apply it.\n>>>\n>>> The normal flow takes the form of both sides sending a\n>>> `commitment_update_propose` message, with the initiator finally\n>>> committing\n>>> both by sending a `commitment_update_apply` message. In the event that\n>>> only\n>>> the responder wants to apply a param change/update, then the initiator\n>>> can\n>>> reply immediately with a `commitment_update_apply` message that doesn't\n>>> include a param change for their commitment (or they just echo the\n>>> parameters if they're acceptable).\n>>>\n>>> ### Handling Retransmissions\n>>>\n>>> The role of the signature it to prevent \"spoofing\" by one of the parties\n>>> (authenticate the param change), and also it serves to convince a party\n>>> that\n>>> they actually sent a prior commitment propose update during the\n>>> retransmission phase. As the `commitment_update_propose` message would be\n>>> retransmitted like any other message, if the initiator attempts to commit\n>>> the update but the connection dies, they'll retransmit it as normal along\n>>> with their latest signature.\n>>>\n>>> ### Nested TLV Param Generality\n>>>\n>>> The messages as sketched out here just have an opaque nested TLV field\n>>> which\n>>> makes it extensible to add in other things like tweaking the total\n>>> number of\n>>> max HTLCs, the current dust values, min/max HTLCs, etc (all things that\n>>> are\n>>> currently hard coded for the lifetime of the channel). An initial target\n>>> would likely just be a `chan_type` field, with future feature bits\n>>> governing\n>>> _what_ type of commitment updates both parties understand in the future.\n>>>\n>>> In the past, when ideas like this were brought up, some were concerned\n>>> that\n>>> it wouldn't really be possible to do this type of updates while existing\n>>> HTLCs were in flight (hence some of the ideas to clear out the commitment\n>>> beforehand). I don't see a reason why this fundamentally _shouldn't_ be\n>>> allowed, as from the point of view of the channel update state machine,\n>>> all\n>>> updates (adds/removes) get applied as normal, but with this _new_\n>>> commitment\n>>> type/params. The main edge case we'll need to consider is cases where the\n>>> new params make older HTLCs invalid for some reason.\n>>>\n>>> ## Conclusion\n>>>\n>>> Using the adapter commitment idea combined with a protocol for updating\n>>> commitments on the fly, would potentially allow us to update all 80k+\n>>> segwit\n>>> v0 channels to the base level of taprooty channels without any on chain\n>>> transactions. The two transactions (open+close) must happen eventually,\n>>> but\n>>> by holding another layer of spends off-chain we can defer them\n>>> (potentially\n>>> indefinitely, as we have channels today that have been opened for over a\n>>> year).\n>>>\n>>> Deploying a generalised on-the-fly dynamic commitment update protocol\n>>> gives\n>>> us a tool to future proof the _existing_ anchored multi-sig outputs in\n>>> the\n>>> chain, and also a way to remove many of the hard coded parameters we have\n>>> today in the protocol. One overly inflexible parameter we have today in\n>>> the\n>>> network is the 483 HTLC limit. Allowing this value to float would allow\n>>> peers to apply similar congestion avoidance algorithm that are used in\n>>> TCP\n>>> today, and also give us a way to protect the network against future\n>>> unforeseen widespread policy changes (like a raising of the dust limit).\n>>>\n>>> -- Laolu\n>>>\n>>> [1]:\n>>> https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-July/002763.html\n>>> [2]:\n>>> https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-July/002770.html\n>>> [3]: https://github.com/lightning/bolts/pull/868\n>>> _______________________________________________\n>>> Lightning-dev mailing list\n>>> Lightning-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>>\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n>\nOn Thu, Oct 27, 2022 at 4:54 PM Matt Corallo <lf-lists at mattcorallo.com>\nwrote:\n\n> I\u2019m not sure I understand this - is there much reason to want taproot\n> commitment outputs? I mean they\u2019re cool, and witnesses are a bit smaller,\n> which is nice I guess, but they\u2019re not providing materially new features,\n> AFAIU. Taproot funding, on the other hand, provides a Bitcoin-wide privacy\n> improvement as well the potential future ability of channel participants to\n> use multisig for their own channel funds transparently.\n>\n> Sure, if we\u2019re doing taproot funding outputs we should probably just do it\n> for the commitment outputs as well, because why not (and it\u2019s a prereq for\n> PTLCs). But trying to split them up seems like added complexity \u201cjust\n> because\u201d? I suppose it tees us up for eventual PTLC support in todays\n> channels, but we can also consider that separately when we get to that\n> point, IMO.\n>\n> Am I missing some important utility of taproot commitment transaction\n> outputs?\n>\n> Matt\n>\n> On Oct 27, 2022, at 02:17, Johan Tor\u00e5s Halseth <johanth at gmail.com> wrote:\n>\n> \ufeff\n> Hi, Laolu.\n>\n> I think it could be worth considering dividing the taprootyness of a\n> channel into two:\n> 1) taproot funding output\n> 2) taproot commitment outputs\n>\n> That way we could upgrade existing channels only on the commitment level,\n> not needing to close or re-anchor the channels using an adapter in order to\n> get many of the taproot benefits.\n>\n> New channels would use taproot multisig (musig2) for the funding output.\n>\n> This seems to be less disruptive to the existing network, and we could get\n> features enabled by taproot to larger parts of the network quicker. And to\n> me this seems to carry less complexity (and closing fees) than an adapter.\n>\n> One caveat is that this wouldn't work (I think) for Eltoo channels, as the\n> funding output would not be plain multisig anymore.\n>\n> - Johan\n>\n> On Sat, Mar 26, 2022 at 1:27 AM Antoine Riard <antoine.riard at gmail.com>\n> wrote:\n>\n>> Hi Laolu,\n>>\n>> Thanks for the proposal, quick feedback.\n>>\n>> > It *is* still the case that _ultimately_ the two transactions to close\n>> the\n>> > old segwit v0 funding output, and re-open the channel with a new segwit\n>> v1\n>> > funding output are unavoidable. However this adapter commitment lets\n>> peers\n>> > _defer_ these two transactions until closing time.\n>>\n>> I think there is one downside coming with adapter commitment, which is\n>> the uncertainty of the fee overhead at the closing time. Instead of closing\n>> your segwit v0 channel _now_ with known fees, when your commitment is empty\n>> of time-sensitive HTLCs, you're taking the risk of closing during fees\n>> spikes, due a move triggered by your counterparty, when you might have\n>> HTLCs at stake.\n>>\n>> It might be more economically rational for a LN node operator to pay the\n>> upgrade cost now if they wish  to benefit from the taproot upgrade early,\n>> especially if long-term we expect block fees to increase, or wait when\n>> there is a \"normal\" cooperative closing.\n>>\n>> So it's unclear to me what the economic gain of adapter commitments ?\n>>\n>> > In the remainder of this mail, I'll describe an alternative\n>> > approach that would allow upgrading nearly all channel/commitment\n>> related\n>> > values (dust limit, max in flight, etc), which is inspired by the way\n>> the\n>> > Raft consensus protocol handles configuration/member changes.\n>>\n>> Long-term, I think we'll likely need a consensus protocol anyway for\n>> multi-party constructions (channel factories/payment pools). AFAIU this\n>> proposal doesn't aim to roll out a full-fledged consensus protocol *now*\n>> though it could be wise to ensure what we're building slowly moves in this\n>> direction. Less critical code to maintain across bitcoin\n>> codebases/toolchains.\n>>\n>> > The role of the signature it to prevent \"spoofing\" by one of the parties\n>> > (authenticate the param change), and also it serves to convince a party\n>> that\n>> > they actually sent a prior commitment propose update during the\n>> > retransmission phase.\n>>\n>> What's the purpose of data origin authentication if we assume only\n>> two-parties running over Noise_XK ?\n>>\n>> I think it's already a security property we have. Though if we think\n>> we're going to reuse these dynamic upgrades for N counterparties\n>> communicating through a coordinator, yes I think it's useful.\n>>\n>> > In the past, when ideas like this were brought up, some were concerned\n>> that\n>> > it wouldn't really be possible to do this type of updates while existing\n>> > HTLCs were in flight (hence some of the ideas to clear out the\n>> commitment\n>> > beforehand).\n>>\n>> The dynamic upgrade might serve in an emergency context where we don't\n>> have the leisury to wait for the settlement of the pending HTLCs. The\n>> timing of those ones might be beyond the coordination of link\n>> counterparties. Thus, we have to allow upgrade of non-empty commitments\n>> (and if there are undesirable interferences between new commitment types\n>> and HTLCs/PTLCs present, deal case-by-case).\n>>\n>> Antoine\n>>\n>> Le jeu. 24 mars 2022 \u00e0 18:53, Olaoluwa Osuntokun <laolu32 at gmail.com> a\n>> \u00e9crit :\n>>\n>>> Hi y'all,\n>>>\n>>> ## Dynamic Commitments Retrospective\n>>>\n>>> Two years-ish ago I made a mailing list post on some ideas re dynamic\n>>> commitments [1], and how the concept can be used to allow us to upgrade\n>>> channel types on the fly, and also remove pesky hard coded limits like\n>>> the\n>>> 483 HTLC in-flight limit that's present today. Back then my main target\n>>> was\n>>> upgrading all the existing channels over to the anchor output commitment\n>>> variant, so the core internal routing network would be more resilient in\n>>> a\n>>> persistent high fee environment (which hasn't really happened over the\n>>> past\n>>> 2 years for various reasons tbh). Fast forward to today, and with taproot\n>>> now active on mainnet, and some initial design work/sketches for\n>>> taproot-native channels underway, I figure it would be good to bump this\n>>> concept as it gives us a way to upgrade all 80k+ public channels to\n>>> taproot\n>>> without any on chain transactions.\n>>>\n>>> ## Updating Across Witness Versions w/ Adapter Commitments\n>>>\n>>> In my original mail, I incorrectly concluded that the dynamic commitments\n>>> concept would only really work within the confines of a \"static\"\n>>> multi-sig\n>>> output, meaning that it couldn't be used to help channels upgrade to\n>>> future\n>>> segwit witness versions.  Thankfully this reply [2] by ZmnSCPxj,\n>>> outlined a\n>>> way to achieve this in practice. At a high level he proposes an \"adaptor\n>>> commitment\" (similar to the kickoff transaction in eltoo/duplex), which\n>>> is\n>>> basically an upgrade transaction that spends one witness version type,\n>>> and\n>>> produces an output with the next (upgraded) type. In the context of\n>>> converting from segwit v0 to v1 (taproot), two peers would\n>>> collaboratively\n>>> create a new adapter commitment that spends the old v0 multi-sig output,\n>>> and\n>>> produces a _new_ v1 multi-sig output. The new commitment transaction\n>>> would\n>>> then be anchored using this new output.\n>>>\n>>> Here's a rough sequence diagram of the before and after state to better\n>>> convey the concept:\n>>>\n>>>   * Before: fundingOutputV0 -> commitmentTransaction\n>>>\n>>>   * After fundingOutputV0 -> fundingOutputV1 (the adapter) ->\n>>>     commitmentTransaction\n>>>\n>>> It *is* still the case that _ultimately_ the two transactions to close\n>>> the\n>>> old segwit v0 funding output, and re-open the channel with a new segwit\n>>> v1\n>>> funding output are unavoidable. However this adapter commitment lets\n>>> peers\n>>> _defer_ these two transactions until closing time. When force closing two\n>>> transactions need to be confirmed before the commitment outputs can be\n>>> resolved. However, for co-op close, you can just spend the v0 output, and\n>>> deliver to the relevant P2TR outputs. The adapter commitment can leverage\n>>> sighash anyonecanpay to let both parties (assuming it's symmetric) attach\n>>> additional inputs for fees (to avoid introducing the old update_fee\n>>> related\n>>> static fee issues), or alternatively inherit the anchor output pattern at\n>>> this level.\n>>>\n>>> ## Existing Dynamic Commitments Proposals\n>>>\n>>> Assuming this concept holds up, then we need an actual concrete protocol\n>>> to\n>>> allow for dynamic commitment updates. Last year, Rusty made a spec PR\n>>> outlining a way to upgrade the commitment type (leveraging the new\n>>> commitment type feature bits) upon channel re-establish [3]. The proposal\n>>> relies on another message that both sides send (`stfu`) to clear the\n>>> commitment (similar to the shutdown semantics) before the switch over\n>>> happens. However as this is tied to the channel re-establish flow, it\n>>> doesn't allow both sides to do things like only allow your peer to\n>>> attach N\n>>> HTLCs to start with, slowing increasing their allotted slots and possibly\n>>> reducing them (TCP AIMD style).\n>>>\n>>> ## A Two-Phase Dynamic Commitment Update Protocol\n>>>\n>>> IMO if we're adding in a way to do commitment/channel upgrades, then it\n>>> may\n>>> be worthwhile to go with a more generalized, but slightly more involved\n>>> route instead. In the remainder of this mail, I'll describe an\n>>> alternative\n>>> approach that would allow upgrading nearly all channel/commitment related\n>>> values (dust limit, max in flight, etc), which is inspired by the way the\n>>> Raft consensus protocol handles configuration/member changes.\n>>>\n>>> For those that aren't aware, Raft is a consensus protocol analogous to\n>>> Paxos\n>>> (but isn't byzantine fault tolerant out of the box) that was designed as\n>>> a\n>>> more understandable alternative to Paxos for a pedagogical environment.\n>>> Typically the algorithm is run in the context of a fixed cluster with N\n>>> machines, but supports adding/removing machines from the cluster with a\n>>> configuration update protocol. At a high level the way this works is\n>>> that a\n>>> new config is sent to the leader, with the leader synchronizing the\n>>> config\n>>> change with the other members of the cluster. Once a majority threshold\n>>> is\n>>> reached, the leader then commits the config change with the acknowledged\n>>> parties using the new config (basically a two phase commit). I'm skipping\n>>> over some edge cases here that can arise if the new nodes participate\n>>> consensus too early, which can cause a split majority leading to two\n>>> leaders\n>>> being elected.\n>>>\n>>> Applying this to the LN context is a bit simpler than a generalized\n>>> protocol, as we typically just have two parties involved. The initiator\n>>> is\n>>> already naturally a \"leader\" in our context, as they're the only ones\n>>> that\n>>> can do things like trigger fee updates.\n>>>\n>>> ### Message Structure\n>>>\n>>> At a high level I propose we introduce two new messages, with the fields\n>>> looking something like this for `commitment_update_propose`:\n>>>  * type: 0 (`channel_id`)\n>>>    * value: [`32*byte`:`chan_id`]\n>>>  * type: 1 (`propose_sig`)\n>>>    * value: [`64*byte`:`sig`]\n>>>  * type: 2 (`update_payload`)\n>>>    * value: [`*byte`:`tlv_payload`]\n>>>\n>>> and this `commitment_update_apply`:\n>>>  * type: 0 (`channel_id`)\n>>>    * value: [`32*byte`:`chan_id`]\n>>>  * type: 1 (`local_propose`)\n>>>    * value: [`*byte`:`commitment_update_propose`]\n>>>  * type: 2 (`remote_propose`)\n>>>    * value: [`*byte`:`commitment_update_propose`]\n>>>\n>>> ### Protocol Flow\n>>>\n>>> The core idea here is that either party can propose a commitment/channel\n>>> param update, but only the initiator can actually apply it. The\n>>> `commitment_update_propose` encodes the new set of updates, with a\n>>> signature\n>>> covering the TLV blob for the new params (more on why that's needed\n>>> later).\n>>> The `commitment_update_apply` includes up to _two_\n>>> `commitment_update_propose` messages (one for the initiator and one for\n>>> the\n>>> responder, as nested TLV messages). The `commitment_update_propose`\n>>> message\n>>> would be treated like any other `update_*` message, in that it takes a\n>>> new\n>>> commitment signature to properly commit/apply it.\n>>>\n>>> The normal flow takes the form of both sides sending a\n>>> `commitment_update_propose` message, with the initiator finally\n>>> committing\n>>> both by sending a `commitment_update_apply` message. In the event that\n>>> only\n>>> the responder wants to apply a param change/update, then the initiator\n>>> can\n>>> reply immediately with a `commitment_update_apply` message that doesn't\n>>> include a param change for their commitment (or they just echo the\n>>> parameters if they're acceptable).\n>>>\n>>> ### Handling Retransmissions\n>>>\n>>> The role of the signature it to prevent \"spoofing\" by one of the parties\n>>> (authenticate the param change), and also it serves to convince a party\n>>> that\n>>> they actually sent a prior commitment propose update during the\n>>> retransmission phase. As the `commitment_update_propose` message would be\n>>> retransmitted like any other message, if the initiator attempts to commit\n>>> the update but the connection dies, they'll retransmit it as normal along\n>>> with their latest signature.\n>>>\n>>> ### Nested TLV Param Generality\n>>>\n>>> The messages as sketched out here just have an opaque nested TLV field\n>>> which\n>>> makes it extensible to add in other things like tweaking the total\n>>> number of\n>>> max HTLCs, the current dust values, min/max HTLCs, etc (all things that\n>>> are\n>>> currently hard coded for the lifetime of the channel). An initial target\n>>> would likely just be a `chan_type` field, with future feature bits\n>>> governing\n>>> _what_ type of commitment updates both parties understand in the future.\n>>>\n>>> In the past, when ideas like this were brought up, some were concerned\n>>> that\n>>> it wouldn't really be possible to do this type of updates while existing\n>>> HTLCs were in flight (hence some of the ideas to clear out the commitment\n>>> beforehand). I don't see a reason why this fundamentally _shouldn't_ be\n>>> allowed, as from the point of view of the channel update state machine,\n>>> all\n>>> updates (adds/removes) get applied as normal, but with this _new_\n>>> commitment\n>>> type/params. The main edge case we'll need to consider is cases where the\n>>> new params make older HTLCs invalid for some reason.\n>>>\n>>> ## Conclusion\n>>>\n>>> Using the adapter commitment idea combined with a protocol for updating\n>>> commitments on the fly, would potentially allow us to update all 80k+\n>>> segwit\n>>> v0 channels to the base level of taprooty channels without any on chain\n>>> transactions. The two transactions (open+close) must happen eventually,\n>>> but\n>>> by holding another layer of spends off-chain we can defer them\n>>> (potentially\n>>> indefinitely, as we have channels today that have been opened for over a\n>>> year).\n>>>\n>>> Deploying a generalised on-the-fly dynamic commitment update protocol\n>>> gives\n>>> us a tool to future proof the _existing_ anchored multi-sig outputs in\n>>> the\n>>> chain, and also a way to remove many of the hard coded parameters we have\n>>> today in the protocol. One overly inflexible parameter we have today in\n>>> the\n>>> network is the 483 HTLC limit. Allowing this value to float would allow\n>>> peers to apply similar congestion avoidance algorithm that are used in\n>>> TCP\n>>> today, and also give us a way to protect the network against future\n>>> unforeseen widespread policy changes (like a raising of the dust limit).\n>>>\n>>> -- Laolu\n>>>\n>>> [1]:\n>>> https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-July/002763.html\n>>> [2]:\n>>> https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-July/002770.html\n>>> [3]: https://github.com/lightning/bolts/pull/868\n>>> _______________________________________________\n>>> Lightning-dev mailing list\n>>> Lightning-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>>\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20221028/c91f1825/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Dynamic Commitments Part 2: Taprooty Edition",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Matt Corallo",
                "Johan Tor\u00e5s Halseth"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 48348
        }
    },
    {
        "title": "[Lightning-dev] A pragmatic, unsatisfying work-around for anchor outputs fee-bumping reserve requirements",
        "thread_messages": [
            {
                "author": "Bastien TEINTURIER",
                "date": "2022-10-27T13:51:05",
                "message_text_only": "Good morning list,\n\nThe lightning network transaction format was updated to leverage CPFP\ncarve-out and allow nodes to set fees at broadcast time, using a feature\ncalled anchor outputs [1].\n\nWhile desirable, this change brought a whole new set of challenges, by\nrequiring nodes to maintain a reserve of available utxos for fee-bumping.\nCorrectly managing this fee-bumping reserve involves a lot of complex\ndecisions and dynamic risk assessment, because in worst-case scenarios,\na node may need to fee-bump thousands of HTLC transactions in a short\nperiod of time.\n\nThis is especially frustrating because HTLC transactions should not need\nexternal inputs, as the whole value of the HTLC is already provided in\nits input, which means we could in theory \"simply\" decrease the amount of\nthe corresponding output to set the fees to any desired value. However,\nwe can't do this safely because it doesn't work well with the revocation\nmechanism, unless we find fancy new sighash flags to add to bitcoin.\nSee [2] for a longer rant on this issue.\n\nA very low tech and unsatisfying solution exists, which is what I'm\nproposing today: each node can simply sign multiple versions of the\nHTLC transactions at various feerates, and at broadcast time if you're\nlucky you'll have a pre-signed transaction that approximately matches\nthe feerate you want, so you don't need to add inputs from your fee\nbumping reserve. This reduces the requirements on your on-chain wallet\nand simplifies transaction management logic. I believe that it's a\npragmatic approach, even though not very elegant, to increase funds\nsafety for existing node operators and wallets. I opened a spec PR\nthat is currently chasing concept ACKs before I refine it [3].\n\nPlease let me know what you think, and if this is something that you\nwould like your implementation to provide.\n\nThanks,\nBastien\n\n[1] https://github.com/lightning/bolts/pull/688\n[2] https://github.com/lightning/bolts/issues/845\n[3] https://github.com/lightning/bolts/pull/1036\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20221027/df1e8ff5/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "A pragmatic, unsatisfying work-around for anchor outputs fee-bumping reserve requirements",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Bastien TEINTURIER"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2182
        }
    },
    {
        "title": "[Lightning-dev] Lightning Channels With Tunable Penalties",
        "thread_messages": [
            {
                "author": "jlspc",
                "date": "2022-10-30T17:45:45",
                "message_text_only": "TL:DR\n\n=====\n* It's possible to modify the Lightning channel protocol to impose tunable penalties for putting old transactions on-chain.\n* The key idea is the use of separate value transactions and control transactions.\n* The Tunable-Penalty (TP) protocol also allows a watchtower to use storage that is logarithmic (vs. linear) in the number of channel states.\n* The TP protocol is not suitable for casual users, as it can require that users perform actions at specific times weeks or months after the expiry of an HTLC.\n\nOverview\n========\n\nIn the current Lightning channel protocol [1][2], if a party accidentally puts an old transaction on-chain (for example, due to loss of state caused by a crash or loss of a cell phone), they risk losing all their funds in the channel (this has been called the \"toxic-waste\" problem) [7]. This post presents a new channel protocol, the Tunable-Penalty (TP) protocol, that supports a tunable penalty for putting old transactions on-chain, thus discouraging the use of old transactions while avoiding the risk of losing all channel funds [9][11]. In addition, the TP protocol allows watchtowers to use storage that is logarithmic, rather than linear, in the number of channel states supported. No change to the underlying Bitcoin protocol is required.\n\nA paper with a more complete description of the protocol, including figures, is available [6].\n\nThe Challenge\n=============\n\nIn order to motivate the TP protocol, first consider a straightforward approach to tuning penalties in a Lightning channel by charging the errant party the desired penalty and then dividing the remaining funds according to the channel's current state. Consider the case where Alice put an old state i Commitment transaction on-chain, where the current channel state is j > i. In order to assess the penalty and correctly divide the remaining funds, Bob must be able to spend that transaction's to_self output (which pays to Alice) with a Penalty_Bij transaction that splits its input value into the correct state j values (after assessing the penalty). Alice must approve of this division of funds when she agrees to make state j the current state. Therefore, Alice must sign a separate Penalty_Bij transaction for each pair of states i and j, where i < j, so she must sign a total of O(S^2) penalty transactions and Bob must store O(S) signatures for the current state, where S is the number of channel states [3]. The current Lightning protocol only requires O(S) signatures and O(log S) storage for each party [10], so this approach would be quite expensive.\n\nThe Solution\n============\n\nThe TP protocol avoids this problem by using separate value and control transactions. In addition to an on-chain Funding transaction that provides the channel's funds, each party in the TP protocol has an on-chain Individual transaction that they use to control how the channel's funds are distributed. Each Individual transaction has a single output with a value slightly larger than the desired penalty amount. Before either party can put their Commitment transaction on-chain, they must first put their State transaction on-chain that spends the output of their Individual transaction. The State transaction encodes the channel's state number in its nLocktime and nSequence fields, just like the Commitment transaction encodes the state number in the Lightning protocol. The first output of the State transaction has a value equal to the desired penalty amount.  After a relative delay equal to the maximum of the parties' to_self_delay parameters, this output can be spent by the same party's Commitment transaction for the same state. However, if the State transaction is for an old state, it can be revoked by the other party (thus taking the penalty amount) before it can be used as an input to the Commitment transaction. If one party's State transaction is revoked, the other party can still put their correct State transaction on-chain, as the two parties' State transactions spend outputs from different Individual transactions, and thus don't conflict. Because old State transactions are revoked before they can affect the distribution of the channel's funds, the problem with O(S^2) penalty transactions and signatures described above is avoided.\n\nThe Protocol\n============\n\nThe TP protocol is shown below:\n\n+-+ AB                           +----+ A\n|F|----+-------------------------| CC |----\n+-+    |                         |    |\n       .                         |    | B\n       .                         |    |----\n       .                         +----+\n       |                         +----+ A\n       +-------------------------|C_Ai|----\n       |                         |    |\n       |                         |    | B\n       |                         |    |----\n       |                         |    |\n       |               tsdAB & A |    | AB               +-----+ A\n       |             +-----------|    |-----+------------|Hr_Ai|----\n       |             |           +----+     |  +---------|     |\n       |             |                      |  |         +-----+\n       |             |                      |  |         +-----+ B\n       |             |                      +------------|Hp_Bi|----\n       |             |                         | +-------|     |\n       |             |                         | |       +-----+\n       |             |           +----+ A      | |\n       +-------------------------|C_Bi|----    | |\n       |             |           |    |        | |\n       .             |           |    | B      | |\n       .             |           |    |----    | |\n       .             |           |    |        | |\n       |             | tsdAB & B |    | AB     | |       +-----+ A\n       V           +-------------|    |-----+------------|Hr_Ai|----\n                   | |           +----+     |  | |  +----|     |\n                   | |                      |  | |  |    +-----+\n                   | |                      |  | |  |    +-----+ B\n+----+ A  +-----+  | | pckeyAi              +------------|Hp_Bi|----\n|In_A|----|St_Ai|----+-----------              | |  | +--|     |\n+----+    |     |  |                           | |  | |  +-----+\n          |     |  |   eAB & A   +-----+ A     | |  | |\n          |     |----+-----------|Ht_Ai|-------+ |  | |\n          +-----+  | |           +-----+         |  | |\n                   | | hp(X) & A +-----+ B       |  | |\n                   | +-----------|Hs_Bi|---------+  | |\n                   |             +-----+            | |\n+----+ B  +-----+  |   pckeyBi                      | |\n|In_B|----|St_Bi|--+-------------                   | |\n+----+    |     |                                   | |\n          |     |      eAB & A   +-----+ A          | |\n          |     |--+-------------|Ht_Ai|------------+ |\n          +-----+  |             +-----+              |\n                   |   hp(X) & A +-----+ B            |\n                   +-------------|Hs_Bi|--------------+\n                                 +-----+\n\nwhere:\nF is the Funding transaction,\nCC is the Cooperative Close transaction,\nIn_{A|B} is {Alice's|Bob's} Individual transaction,\nSt_{A|B}i is {Alice's|Bob's} State transaction for state i,\nC_{A|B}i is {Alice's|Bob's} Commitment transaction for state i,\nHt_Ai is Alice's HTLC-timeout transaction for state i,\nHs_Bi is Bob's HTLC-success transaction for state i,\nHr_Ai is Alice's HTLC-refund transaction for state i, and\nHp_Bi is Bob's HTLC-payment transaction for state i.\n\nThe F, In_A and In_B transactions are on-chain, while the remaining transactions are off-chain during normal protocol operation. The output of the Ht_Ai and Hs_Bi transactions, and the second output of the St_Ai and St_Bi transactions, have the minimal allowed value, as they are used for control.\n\nRequirements for output cases are as follows:\nA: Alice's signature,\nB: Bob's signature,\nAB: Alice's and Bob's signatures,\npckey{A|B}i: a signature using a per-commitment key for revoking {Alice's|Bob's} state i transaction,\nhp(X): the hash preimage of X,\ntsdAB: a relative delay equal to the maximum of Alice's and Bob's to_self_delay parameters, and\neAB: an absolute timelock equal to the expiry of the HTLC in this hop.\n\nIn order to establish a new channel state, both parties:\n* calculate the State, Commitment, HTLC-timeout, HTLC-success, HTLC-refund and HTLC-payment transactions for the new state,\n* exchange partial signatures for the new state's HTLC-refund, HTLC-payment and Commitment transactions (in that order), and\n* exchange per-commitment pubkeys for the old state, thus revoking it.\n\nConsider the case of an HTLC offered by Alice to Bob (as shown in the figure above). When Bob receives the secret for the HTLC, he shares it with Alice and attempts to update the channel state off-chain. If he's unable to update the state off-chain before his fulfillment_deadline prior to the HTLC's expiry, he puts his State and associated HTLC-success transactions on-chain. If Alice hasn't received the HTLC's secret by her timeout_deadline *prior* to the HTLC's expiry, she puts her State and associated HTLC-timeout transactions on-chain.\n\nBoth parties constantly look for an old (revoked) State transaction put on-chain by their partner, and if they find such a transaction they use the corresponding per-commitment key to spend its first output, thus obtaining the penalty funds and revoking the old state. Whenever a current State transaction is put on-chain, both parties attempt to resolve its HTLC control output(s) by putting their HTLC-timeout and HTLC-success transactions on-chain. If the first output of a State transaction has not been spent within tsdAB, the party that put the State transaction on-chain attempts to put their corresponding Commitment transaction on-chain. Once a Commitment transaction is on-chain, each party puts its HTLC-refund and HTLC-payment transactions that spend its HTLC outputs on-chain (as determined by whether the corresponding HTLC-timeout or HTLC-success transaction is on-chain).\n\nTwo-Input Transactions\n======================\n\nAll transactions in the current Lightning protocol have a single input, while the Commitment, HTLC-refund and HTLC-payment transactions in the TP protocol each have two inputs. The first input is a value input that carries channel funds and requires both parties' signatures (using the SIGHASH_ALL flag). The second input is a control input that only requires one party's signature. These two-input transactions are quite different from the Lightning protocol's one-input transactions, so it's worth examining them in detail.\n\nFirst, because the first input requires both parties' signatures, only two-input transactions that have been signed by both parties can be used.  Second, because signatures for the first input use the SIGHASH_ALL flag, they force the source of the second input to have the specified transaction ID. Third, because each transaction's ID is a function of the transaction IDs of all of its parents (and indirectly, of all of its ancestors), all of the ancestors of the two-input transactions must have the specified transaction IDs. For example, Alice's partial signature on the first input of Bob's Commitment transaction forces its second input to spend the first output of Bob's State transaction for the same state, and indirectly requires Bob's State transaction for the same state to spend the output of his Individual transaction. Therefore, while Bob could spend the output of his Individual transaction in any way he chooses, spending it with anything other than the correct State transaction will prevent him from ever putting his correct Commitment transaction on-chain.  Similarly, if the first output of Bob's State transaction is spent by anything other than his corresponding Commitment transaction, that Commitment transaction can never be put on-chain.\n\nPer-Commitment Keys\n===================\n\nIn this way, Bob's Commitment transaction for state i, and the HTLC-refund and HTLC-payment transactions that spend the HTLC outputs from his Commitment transaction for state i, all require that the first output of Bob's State transaction for state i be spent by his Commitment transaction for state i. Therefore, revoking an old State transaction by spending its first output also revokes all of the same party's Commitment transactions (for all states) and their associated HTLC-refund and HTLC-payment transactions. This is why there's no need to separately revoke any of the outputs from the Commitment, HTLC-refund, HTLC-payment, HTLC-timeout or HTLC-success transactions.\n\nThese dependent revocations also allow the TP protocol to use a different type of key for revoking old states. In the Lightning protocol the party that puts an old transaction on-chain cannot know the key for revoking that transaction. This is necessary in Lightning because if that party did know the revocation key, they could intentionally put an old Commitment transaction on-chain and then \"revoke\" the outputs of that transaction, thus taking the value for themselves. In contrast, in the TP protocol, if a party puts an old State transaction on-chain and then revokes it by spending its first output with a transaction other than its associated Commitment transaction, that party is actually blocking its ability to ever put a Commitment transaction on-chain. Furthermore, the funds that party obtains by spending the State transaction's first output are the same penalty funds that they provided from their Individual transaction. As a result, it's safe in the TP protocol to allow a party to revoke its own State transaction. This allows the TP protocol to use per-commitment keys (which are exactly the private keys of the per-commitment points used by the Lightning protocol [10]) to revoke transactions.\n\nWatchtowers\n===========\n\nBecause these per-commitment keys are known to the party putting the revocable transactions on-chain, they can also be shared with an untrusted watchtower. The watchtower can use the Lightning protocol's compact storage technique for revocation keys to consume only O(log S) storage to revoke a maximum of S old transactions [10]. Conveniently, the watchtower can also take the penalty amount as payment for the service it provided.  The watchtower must be given the UTXO of the partner's Individual transaction's output in order to detect a revoked transaction, but the watchtower will see no association between that UTXO and the Funding transaction or any other channel state.\n\nCorrectness\n===========\n\nEach party is prevented from putting an old Commitment transaction for state i on-chain by the other party spending the first output of the corresponding State transaction for state i. Therefore, only the correct Commitment transaction can appear on-chain.\n\nWhile having separate, non-conflicting State transactions enables tunable penalties, it makes HTLC resolution more complex. As can be seen in the figure above, the parties' Commitment transactions conflict and the party whose Commitment transaction appears on-chain (called the winning party) also put on-chain the State transaction with the HTLC control outputs that determine whether HTLCs succeed or time out.\n\nFor example, consider an HTLC offered by Alice to Bob. Bob could fulfill the HTLC by putting his State and associated HTLC-success transactions on-chain before its expiry, but if Alice is the winning party and she puts her State and associated HTLC-timeout transactions on-chain far after its expiry, Bob will not be paid by the HTLC depite having met its terms.  Similary, Alice could time out the HTLC by putting her State and HTLC-timeout transactions on-chain after its expiry, but if Bob is the winning party and he puts his State and associated HTLC-success transactions on-chain far later, Bob will be paid by the HTLC despite having failed to meet its terms.\n\nNote that both problems require the second party to put their State transaction on-chain late relative to the HTLC's expiry, and yet be the winning party. These problems can't occur in the TP protocol because both parties race to put their Commitment transactions on-chain and the relative delays from State transactions to Commitment transactions match.  Therefore, the winning party's State transaction can't be put on-chain much later than the other party's State transaction. In the TP protocol, each party guarantees that the winning party's State transaction is on-chain early enough relative to the HTLC's expiry by putting their own State transaction on-chain early. Then, even if the other party is the winning party, the other party's transaction must still be on-chain early enough relative to the HTLC's expiry, thus guaranteeing its correct resolution.\n\nThis argument is formalized in the paper [6].\n\nRelated Work\n============\n\nThe protocol presented here makes extensive use of previously-published work, namely the Poon-Dryja Lightning channel protocol [1] and the BOLT specifications [2]. The compact storage technique for per-commitment keys comes from the compact storage technique for revocation keys created by Russell [10].\n\nRiard, ZmnSCPxj and Decker examined the idea of adding penalties to the eltoo protocol [7], but the techniques used there were different, as they did not use separate value and control transactions.\n\nRubin (who also credited nullc and sipa) [8] showed how a two-input transaction with separate value and control inputs can be used to delegate the control of a UTXO (the value input) to another UTXO (the control input). This two-input transaction is similar to the TP protocol's Commitment, HTLC-refund and HTLC-payment transactions, but differs in that its value input does not require a multi-sig, so the delegation can be revoked by the delegator via a double-spend.\n\nThe idea of using separate value and control transactions was presented by Law in the update-forest and challenge-and-response protocols [4], but those protocols were for channel factories as opposed to channels, and they assumed a change to the underlying Bitcoin protocol.\n\nConclusions\n===========\n\nThis paper presents a new channel protocol that allows one to select the penalty for putting an old transaction on-chain. In addition, it reduces the storage costs for untrusted watchtowers from O(S) to O(log S), where S is the number of channel states supported. A recent paper presented a channel protocol that is watchtower-free for casual users [5] and similar techniques could be used to make the TP protocol watchtower-free for such users. However, the TP protocol is not suitable for them as it can require that users submit their Commitment transaction at a specific time weeks or months after the expiry of an HTLC.\n\nThe TP protocol achieves tunable penalties and logarithmic storage for watchtowers by using separate value and control transactions. As will be shown in the next posts, protocols that use separate value and control transaction and have a structure similar to the TP protocol can also be used to create factory-optimized channels and efficient channel factories.\n\nRegards,\nJohn\n\nReferences\n==========\n\n[1] Poon and Dryja, \"The Bitcoin Lightning Network\", available at https://lightning.network/lightning-network-paper.pdf.\n[2] BOLT specifications, available at https://github.com/lightningnetwork/lightning-rfc.\n[3] Decker, \"Re: Simulating Eltoo Factories using SCU Escrows (aka SCUE'd Eltoo), available at https://www.mail-archive.com/lightning-dev@lists.linuxfoundation.org/msg02046.html.\n[4] Law, \"Scaling Bitcoin With Inherited IDs\", available at https://github.com/JohnLaw2/btc-iids.\n[5] Law, \"Watchtower-Free Lightning Channels For Casual Users\", available at https://github.com/JohnLaw2/ln-watchtower-free.\n[6] Law, \"Lightning Channels With Tunable Penalties\", available at https://github.com/JohnLaw2/ln-tunable-penalties.\n[7] Riard, \"Using Per-Update Credential to enable Eltoo-Penalty\" and subsequent posts in the thread starting at: https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-July/002064.html\n[8] Rubin, \"Delegated signatures in Bitcoin within existing rules, no fork required\" available at https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-March/018615.html.\n[9] Rubin, \"Re: 'OP_EVICT': An Alternative to 'OP_TAPLEAFUPDATEVERIFY'\", available at https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019945.html.\n[10]Russell, \"Efficient Per-Commitment Secret Storage\", available at https://github.com/lightning/bolts/blob/master/03-transactions.md#efficient-per-commitment-secret-storage\n[11]Sanders, \"Re: 'OP_EVICT': An Alternative to 'OP_TAPLEAFUPDATEVERIFY'\", available at https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019947.\n\nSent with [Proton Mail](https://proton.me/) secure email.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20221030/b97aa76e/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Lightning Channels With Tunable Penalties",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "jlspc"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 20990
        }
    }
]