[
    {
        "title": "[Lightning-dev] [BOLT Draft] Onion Routing Spec",
        "thread_messages": [
            {
                "author": "Rusty Russell",
                "date": "2016-08-02T04:25:44",
                "message_text_only": "Olaoluwa Osuntokun <laolu32 at gmail.com> writes:\n>> A special HMAC value of 20 0x00 bytes indicates that the currently\n>> processing hop is the intended recipient and that the packet should not be\n>> forwarded. At this point the end-to-end payload is fully decrypted and the\n>> route has terminated.\n>\n> It seems that with the current construction, then the \"next hop\" address\n> will\n> also be zero bytes if a packet processor is the last hop in the route.\n> Alternatively, if the sender is aware that the receiver is actually a\n> \"virtual\n> channel\", then an additional address could be used instead of the\n> zero-address\n> to facilitate de-multiplexing at the last hop to the destination virtual\n> channel.\n\nHmm, I think we should combine the \"header\" and \"per-hop payload\" into a\nsingle 40 byte field.  They're not meaningfully distinct for lightning,\nAFAICT.\n\nAnd I think we should add/steal at least one byte for \"realm\".  0 =\nterminal.  1 = bitcoin lightning.  2 = TBA. etc.\n\nObviously we should specify the layout when type = 0 (ignore) and 1\n(hash160(nexthop-pubkey) + 8-byte-amount-to-fwd + 12-ignored).\n\nThat allows us to extend the protocol incompatibly (add a new realm) or\ncompatibly (add semantics to those ignored sections).\n\n> First, lets talk replay protection. The current draft specifies that:\n>\n>> The node MUST keep a log of previously used shared secrets. Should the\n> shared\n>> secret already be in the log it MUST abort processing the packet and\n> report a\n>> route failure, since this is likely a replay attack, otherwise the shared\n>> secret is added to the log\n>\n> This is definitely necessary, however as dictated this would require nodes\n> to\n> allocate a potentially *unbounded* amount of storage to the shared secret\n> \"seen\" log. I think we can allow nodes to periodically truncate this log by\n> adding an additional session time stamp to the mix-header, either placed\n> directly after the version byte, or within the per-hop payload.\n\nLet's tease this out a bit more; thinking about replays from the PoV of\none layer higher.  We can't simply deny multiple HTLCs with the same r\nhash, since that allows an attacker to probe the network to see where an\nHTLC went (note that in the longer run, it's in a node's short-term\neconomic interest to allow this, which is why rhash is troubling).\n\nIf we switch from hash/preimage to the privkeys with point addition\nscheme (which has other benefits) this is no longer an issue and we can\nsimply refuse to accept two HTLCs with the same pubkey.\n\nThough the idea of using a \"comms\" key signed by our \"peer\" key which we\nrotate every N minutes/hours is appealing for forward secrecy and\nminimal peer-key exposure reasons, as Laolu says.\n\nCheers,\nRusty."
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2016-08-04T18:47:34",
                "message_text_only": "> Hmm, I think we should combine the \"header\" and \"per-hop payload\" into a\n> single 40 byte field.  They're not meaningfully distinct for lightning,\n> AFAICT.\n\nIt's not clear to me what we gain by squashing the header (hop's ephemeral\nkey,\nMAC, next hop) and the per-hop payload (amount to forward, possibly an\nallowed\nCLTV range, etc) into a single blob. I think the extensibility features are\nthe\nsame regardless.\n\n> And I think we should add/steal at least one byte for \"realm\".  0 =\n> terminal.  1 = bitcoin lightning.  2 = TBA. etc.\n\nWhat does \"terminal\" represent in this context?\n\nI don't think we need an equivalent of Bitcoin's \"net magic\" bytes within\nthe\nonion blob itself, as I'd imagine that the onion blob would be encapsulated\nwithin a fixed-length field within the \"htlcAdd\" message. The \"htlcAdd\"\nmessage\nwould then have a header similar to Bitcoin's, in which the \"net magic\"\nbytes\nwould be placed.\n\n> We can't simply deny multiple HTLCs with the same r hash, since that\nallows an\n> attacker to probe the network to see where an HTLC went (note that in the\n> longer run, it's in a node's short-term economic interest to allow this,\nwhich\n> is why rhash is troubling).\n\nWe wouldn't deny multiple HTLC's with the same r hash. We'd deny a packet\nwith\na shared secret we've already seen, meaning one that we've processed\nbefore. A\nnode could still forward multiple HTLC's with identical r-hashes (perhaps\ndue\nto having to fragment a payment due to insufficient link-capacity), these\nHTLC's would have distinct (indistinguishable) onion packets.\n\n(as a side note: I think we should establish some better terminology than\nr-hash, or H)\n\n> If we switch from hash/preimage to the privkeys with point addition scheme\n> (which has other benefits) this is no longer an issue and we can simply\nrefuse\n> to accept two HTLCs with the same pubkey.\n\nThe point addition derivation (P2CH style) scheme only applies to\nrevocations.\nIt's not clear to me how one can switch to priv/pub key based HTLC's without\nmodifying Bitcoin Script. At the moment, we're unable to guarantee/force key\ndisclosure with Script's current capabilities.\n\n-- Laolu\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20160804/34ecad88/attachment-0001.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2016-08-05T00:52:07",
                "message_text_only": "Olaoluwa Osuntokun <laolu32 at gmail.com> writes:\n>> Hmm, I think we should combine the \"header\" and \"per-hop payload\" into a\n>> single 40 byte field.  They're not meaningfully distinct for lightning,\n>> AFAICT.\n>\n> It's not clear to me what we gain by squashing the header (hop's ephemeral\n> key,\n> MAC, next hop) and the per-hop payload (amount to forward, possibly an\n> allowed\n> CLTV range, etc) into a single blob. I think the extensibility features are\n> the\n> same regardless.\n\nEphemeral key and mac make sense as a header, but it's fairly easy\nto image a different next hop address format for different networks.\nSee also \"realm byte\" below.\n\nThus I'm suggesting a format like:\n\n  [ephemeral key] [mac] [realm] [per-realm-information]\n\nPer-realm-information is next-hop, amount, etc.\n\n>> And I think we should add/steal at least one byte for \"realm\".  0 =\n>> terminal.  1 = bitcoin lightning.  2 = TBA. etc.\n>\n> What does \"terminal\" represent in this context?\n\nYou're right; \"Terminus\" aka \"you're the final hop\", rather than using\nmagic HMAC value.\n\n> I don't think we need an equivalent of Bitcoin's \"net magic\" bytes within\n> the\n> onion blob itself, as I'd imagine that the onion blob would be encapsulated\n> within a fixed-length field within the \"htlcAdd\" message. The \"htlcAdd\"\n> message\n> would then have a header similar to Bitcoin's, in which the \"net magic\"\n> bytes\n> would be placed.\n\nAn explicit network byte makes sense since we could eventually have\nmultiple networks (atomic cross-chain exchange).  While node keys are\nnetwork globally unique (thus the exchange is *implied* by the next\nhop), it's nice to be explicit.\n\nWe need some flag for the terminal node anyway, so it makes sense IMHO\nto expand it.\n\n>> We can't simply deny multiple HTLCs with the same r hash, since that\n> allows an\n>> attacker to probe the network to see where an HTLC went (note that in the\n>> longer run, it's in a node's short-term economic interest to allow this,\n> which\n>> is why rhash is troubling).\n>\n> We wouldn't deny multiple HTLC's with the same r hash. We'd deny a packet\n> with\n> a shared secret we've already seen, meaning one that we've processed\n> before.\n\nSorry, I was unclear: We're agreeing.  I was clarifying why an\nalternative approach to replay avoidance (using r hash uniqueness)\ndoesn't work.\n\n> (as a side note: I think we should establish some better terminology than\n> r-hash, or H)\n\nErgh, yes!\n\nStrawman proposal:\n1) HTLC-hash and HTLC-preimage? (aha H-hash & H-preimage).\n2) committx-hash and committx-preimage (C-hash / C-preimage) for the\n   commitment transaction revocation?\n\nThat avoids R altogether, which is overloaded...\n\n>> If we switch from hash/preimage to the privkeys with point addition scheme\n>> (which has other benefits) this is no longer an issue and we can simply\n> refuse\n>> to accept two HTLCs with the same pubkey.\n>\n> The point addition derivation (P2CH style) scheme only applies to\n> revocations.\n>\n> It's not clear to me how one can switch to priv/pub key based HTLC's without\n> modifying Bitcoin Script. At the moment, we're unable to guarantee/force key\n> disclosure with Script's current capabilities.\n\nYeah, it was easy to miss; it's buried deep in a thread (see the\n\"alternative approach\"):\n\n        https://lists.linuxfoundation.org/pipermail/lightning-dev/2015-November/000344.html\n\nBasically, you force r-value reuse in signatures to reveal the private\nkey.  The script to do this would be pretty big though, involving two\nother keys, three signatures and three OP_CHECKMULTISIG.\n\nAnd AFAIK nobody has actually written such a script...\n\nCheers,\nRusty."
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2016-08-12T17:59:33",
                "message_text_only": "Rusty Russell <rusty at rustcorp.com.au> wrote:\n> Ephemeral key and mac make sense as a header, but it's fairly easy\n> to image a different next hop address format for different networks.\n> See also \"realm byte\" below.\n>\n> Thus I'm suggesting a format like:\n>\n>  [ephemeral key] [mac] [realm] [per-realm-information]\n>\n>  Per-realm-information is next-hop, amount, etc.\n\nIn order to maintain the security properties of the onion blob, each onion\nblob\nis required to be the exact same length. Therefore, the amount of bytes\nallocated for the \"next-hop\" address needs to be uniform. In my mind, the\nnext-hop\" field should just be an opaque blob (at the specification level)\nwith\nno further explicit meaning. Nodes residing on various chains will parse the\naddress accordingly (they might be using a different curve, etc).\n\nWith this said, I fail to see what we gain by making the current\nchain-boundary\nexplicit (within the onion blob's mix-header).\n\n> An explicit network byte makes sense since we could eventually have\nmultiple\n> networks (atomic cross-chain exchange).  While node keys are network\nglobally\n> unique (thus the exchange is *implied* by the next hop), it's nice to be\n> explicit.\n>\n> We need some flag for the terminal node anyway, so it makes sense IMHO\n> to expand it.\n\nSure, but the explicit network byte should be within the main p2p message\nheader rather than the header for the onion blob itself. When crossing\nchains,\nnodes will properly set the net magic in the outer message header.\n\nAlso we don't need to allocate an additional byte for the terminal node in\nany\ncase. The terminal node can either be identified by the null-MAC, or\nnull-nexthop (if that isn't being used to dispatch into virtual channels).\n\n> Strawman proposal:\n> 1) HTLC-hash and HTLC-preimage? (aha H-hash & H-preimage).\n> 2) committx-hash and committx-preimage (C-hash / C-preimage) for the\n>   commitment transaction revocation?\n>\n> That avoids R altogether, which is overloaded...\n\nSounds good to me!\n\n-- Laolu\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20160812/9368dd41/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2016-08-13T10:04:02",
                "message_text_only": "Olaoluwa Osuntokun <laolu32 at gmail.com> writes:\n> Rusty Russell <rusty at rustcorp.com.au> wrote:\n>> Ephemeral key and mac make sense as a header, but it's fairly easy\n>> to image a different next hop address format for different networks.\n>> See also \"realm byte\" below.\n>>\n>> Thus I'm suggesting a format like:\n>>\n>>  [ephemeral key] [mac] [realm] [per-realm-information]\n>>\n>>  Per-realm-information is next-hop, amount, etc.\n>\n> In order to maintain the security properties of the onion blob, each onion\n> blob\n> is required to be the exact same length. Therefore, the amount of bytes\n> allocated for the \"next-hop\" address needs to be uniform. In my mind, the\n> next-hop\" field should just be an opaque blob (at the specification level)\n> with\n> no further explicit meaning. Nodes residing on various chains will parse the\n> address accordingly (they might be using a different curve, etc).\n\nKind of.  We need some identifier to know, as nodes may straddle chains.\n\n> With this said, I fail to see what we gain by making the current\n> chain-boundary\n> explicit (within the onion blob's mix-header).\n>\n>> An explicit network byte makes sense since we could eventually have\n> multiple\n>> networks (atomic cross-chain exchange).  While node keys are network\n> globally\n>> unique (thus the exchange is *implied* by the next hop), it's nice to be\n>> explicit.\n>>\n>> We need some flag for the terminal node anyway, so it makes sense IMHO\n>> to expand it.\n>\n> Sure, but the explicit network byte should be within the main p2p message\n> header rather than the header for the onion blob itself. When crossing\n> chains,\n> nodes will properly set the net magic in the outer message header.\n\nSome node will have to straddle two chains, right?  So you'd route A ->\nB -> C as normal, and C is (say) litecoin (B straddles both).  You\nreally want the onion to be explicit that this transfer to litecoin is\nwhat the sender intended.  Or some sidechain.\n\nNow, we'd hope nobody would screw this up, but I think it's worth\nflagging since the sender really should know it's changing chains.\n\n> Also we don't need to allocate an additional byte for the terminal node in\n> any\n> case. The terminal node can either be identified by the null-MAC, or\n> null-nexthop (if that isn't being used to dispatch into virtual channels).\n\nThat implies there's a final hop in the packet which is unused?  I\nbelieve strongly we want a realm byte, so I think it makes more sense to\noverload it for this.\n\n>> Strawman proposal:\n>> 1) HTLC-hash and HTLC-preimage? (aha H-hash & H-preimage).\n>> 2) committx-hash and committx-preimage (C-hash / C-preimage) for the\n>>   commitment transaction revocation?\n>>\n>> That avoids R altogether, which is overloaded...\n>\n> Sounds good to me!\n\nI shall try to use those consistently from now on!\n\nThanks,\nRusty."
            },
            {
                "author": "Christian Decker",
                "date": "2016-08-15T12:06:47",
                "message_text_only": "On Sat, Aug 13, 2016 at 07:34:02PM +0930, Rusty Russell wrote:\n> Olaoluwa Osuntokun <laolu32 at gmail.com> writes:\n> > Rusty Russell <rusty at rustcorp.com.au> wrote:\n> >> Ephemeral key and mac make sense as a header, but it's fairly easy\n> >> to image a different next hop address format for different networks.\n> >> See also \"realm byte\" below.\n> >>\n> >> Thus I'm suggesting a format like:\n> >>\n> >>  [ephemeral key] [mac] [realm] [per-realm-information]\n> >>\n> >>  Per-realm-information is next-hop, amount, etc.\n> >\n> > In order to maintain the security properties of the onion blob, each onion\n> > blob\n> > is required to be the exact same length. Therefore, the amount of bytes\n> > allocated for the \"next-hop\" address needs to be uniform. In my mind, the\n> > next-hop\" field should just be an opaque blob (at the specification level)\n> > with\n> > no further explicit meaning. Nodes residing on various chains will parse the\n> > address accordingly (they might be using a different curve, etc).\n> \n> Kind of.  We need some identifier to know, as nodes may straddle chains.\n> \n> > With this said, I fail to see what we gain by making the current\n> > chain-boundary\n> > explicit (within the onion blob's mix-header).\n> >\n> >> An explicit network byte makes sense since we could eventually have\n> > multiple\n> >> networks (atomic cross-chain exchange).  While node keys are network\n> > globally\n> >> unique (thus the exchange is *implied* by the next hop), it's nice to be\n> >> explicit.\n> >>\n> >> We need some flag for the terminal node anyway, so it makes sense IMHO\n> >> to expand it.\n> >\n> > Sure, but the explicit network byte should be within the main p2p message\n> > header rather than the header for the onion blob itself. When crossing\n> > chains,\n> > nodes will properly set the net magic in the outer message header.\n> \n> Some node will have to straddle two chains, right?  So you'd route A ->\n> B -> C as normal, and C is (say) litecoin (B straddles both).  You\n> really want the onion to be explicit that this transfer to litecoin is\n> what the sender intended.  Or some sidechain.\n> \n> Now, we'd hope nobody would screw this up, but I think it's worth\n> flagging since the sender really should know it's changing chains.\n\nI agree that the realm byte is a sensible addition. To trigger this we\nwould need to have multiple channels, on different chains, using the\nsame identifiers between two nodes. Only in this case we'd have an\nambiguity where to transfer the funds. Assuming we have the route A ->\nB => C, where => indicates two channels, one in litecoin and one in\nbitcoin, and both channels use the same identity for C. Then the\ninstruction to forward 0.01 units to C is ambiguous, as it could be\ndenominated in either litecoin or bitcoin.\n\nWhile not dangerous it is rather unfortunate as it results in\nguesswork. It is not dangerous because if A transferred litecoin to B\nthen B will (hopefully) never forward a higher value to C using\nbitcoin, and if it were bitcoin then the final recipient would not\nsign off an inferior amount than what he expected.\n\nTo prevent this we could make it a policy to never re-use identities\nfor multiple channels, but someone will surely get it wrong at some\npoint :-)\n\nI was thinking that it'd be stored in the per-hop payload, along with\nthe instructions for the hop, which is why I did not specify it, but\nI'm happy to add it, should it make things clearer.\n\n> \n> > Also we don't need to allocate an additional byte for the terminal node in\n> > any\n> > case. The terminal node can either be identified by the null-MAC, or\n> > null-nexthop (if that isn't being used to dispatch into virtual channels).\n> \n> That implies there's a final hop in the packet which is unused?  I\n> believe strongly we want a realm byte, so I think it makes more sense to\n> overload it for this.\n\nIndeed the last hop's routing information is all zeroes and the\nper-hop payload is currently unused (as the node does not forward\nanything). If we use the terminal identifier we can shave the 40 bytes\nrouting info off the packet, provided the node checks the terminal\nbyte in the per-hop payload before attempting anything with the\nrouting info which'd be garbage, i.e., encrypted fillers by the first\nhop.\n\nCheers,\nChristian"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2016-08-16T04:54:32",
                "message_text_only": "Christian Decker <decker.christian at gmail.com> wrote:\n\n> > Now, we'd hope nobody would screw this up, but I think it's worth\n> > flagging since the sender really should know it's changing chains.\n>\n> I agree that the realm byte is a sensible addition. To trigger this we\n> would need to have multiple channels, on different chains, using the\n> same identifiers between two nodes. Only in this case we'd have an\n> ambiguity where to transfer the funds. Assuming we have the route A ->\n> B => C, where => indicates two channels, one in litecoin and one in\n> bitcoin, and both channels use the same identity for C. Then the\n> instruction to forward 0.01 units to C is ambiguous, as it could be\n> denominated in either litecoin or bitcoin.\n>\n> While not dangerous it is rather unfortunate as it results in\n> guesswork. It is not dangerous because if A transferred litecoin to B\n> then B will (hopefully) never forward a higher value to C using\n> bitcoin, and if it were bitcoin then the final recipient would not\n> sign off an inferior amount than what he expected.\n>\n> To prevent this we could make it a policy to never re-use identities\n> for multiple channels, but someone will surely get it wrong at some\n> point :-)\n>\n\nGreat example; I hadn't considered this particular case before. It's clear\nto me now that we should eliminate this potential ambiguity by making the\nchain of the target link for each hop along the route explicit.\n\n\nI was thinking that it'd be stored in the per-hop payload, along with\n> the instructions for the hop, which is why I did not specify it, but\n> I'm happy to add it, should it make things clearer.\n>\n>\nI think the byte specifying the target realm should be protected under a\nMAC, as forwarding to the correct realm may be critical in order for the\npayment to succeed. Therefore, if we retain the MAC for the per-hop payload\nthen it can be placed there, otherwise they header may need to extended by\na byte in order to place the realm information there.\n\n-- Laolu\n-- Laolu\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20160816/8bd1f611/attachment.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2016-08-16T08:10:57",
                "message_text_only": "On Tue, Aug 16, 2016 at 04:54:32AM +0000, Olaoluwa Osuntokun wrote:\n> > I was thinking that it'd be stored in the per-hop payload, along with\n> > the instructions for the hop, which is why I did not specify it, but\n> > I'm happy to add it, should it make things clearer.\n> >\n> >\n> I think the byte specifying the target realm should be protected under a\n> MAC, as forwarding to the correct realm may be critical in order for the\n> payment to succeed. Therefore, if we retain the MAC for the per-hop payload\n> then it can be placed there, otherwise they header may need to extended by\n> a byte in order to place the realm information there.\n> \n> -- Laolu\n\nGood catch! We need to make sure that the integrity of the per-hop\npayload is protected at all costs. The per-hop payloads were\nintroduced to provide intermediate hops with instructions on what to\ndo, i.e., how many coins to forward, so if we can't guarantee their\nintegrity it could result in exploits. An attacker could for example\ninstruct an intermediate hop to forward more, in the hopes of\ncollecting it further down the line.\n\nA mitigating fact may be that a node will forward at most the amount\nit received, minus its fee, limiting this to a fee-shaving attack. But\nif we can find a way to fix it, that would be great.\n\nSo it would appear we cannot drop the payloads from the MAC after all,\nwhich makes stitching routes difficult in the case of rendezvous. The\ninteractive protocol I outlined before may still works, but it is\nrather ugly as it deviates from the invoice pattern, i.e., the final\nrecipient gives the necessary information for the transfer in a single\nbundle.\n\nCheers,\nChristian"
            },
            {
                "author": "Rusty Russell",
                "date": "2016-08-17T10:23:03",
                "message_text_only": "Christian Decker <decker.christian at gmail.com> writes:\n> I agree that the realm byte is a sensible addition. To trigger this we\n> would need to have multiple channels, on different chains, using the\n> same identifiers between two nodes. Only in this case we'd have an\n> ambiguity where to transfer the funds. Assuming we have the route A ->\n> B => C, where => indicates two channels, one in litecoin and one in\n> bitcoin, and both channels use the same identity for C. Then the\n> instruction to forward 0.01 units to C is ambiguous, as it could be\n> denominated in either litecoin or bitcoin.\n>\n> While not dangerous it is rather unfortunate as it results in\n> guesswork. It is not dangerous because if A transferred litecoin to B\n> then B will (hopefully) never forward a higher value to C using\n> bitcoin, and if it were bitcoin then the final recipient would not\n> sign off an inferior amount than what he expected.\n\nWorse case: C is a charity, accepting donations.  A's software screwed\nup and didn't realize C was litecoin, not bitcoin.  B collects a huge\nfee, C gets tiny donation.\n\nCheers,\nRusty."
            },
            {
                "author": "Christian Decker",
                "date": "2016-08-18T09:06:22",
                "message_text_only": "On Wed, Aug 17, 2016 at 07:53:03PM +0930, Rusty Russell wrote:\n> Christian Decker <decker.christian at gmail.com> writes:\n> > I agree that the realm byte is a sensible addition. To trigger this we\n> > would need to have multiple channels, on different chains, using the\n> > same identifiers between two nodes. Only in this case we'd have an\n> > ambiguity where to transfer the funds. Assuming we have the route A ->\n> > B => C, where => indicates two channels, one in litecoin and one in\n> > bitcoin, and both channels use the same identity for C. Then the\n> > instruction to forward 0.01 units to C is ambiguous, as it could be\n> > denominated in either litecoin or bitcoin.\n> >\n> > While not dangerous it is rather unfortunate as it results in\n> > guesswork. It is not dangerous because if A transferred litecoin to B\n> > then B will (hopefully) never forward a higher value to C using\n> > bitcoin, and if it were bitcoin then the final recipient would not\n> > sign off an inferior amount than what he expected.\n> \n> Worse case: C is a charity, accepting donations.  A's software screwed\n> up and didn't realize C was litecoin, not bitcoin.  B collects a huge\n> fee, C gets tiny donation.\n\nTrue, that's a dangerous scenario. If the recipient does not know the\nintended amount and accepts anything then fee-shaving is very\nprofitable. In general I'm a bit concerned about rhash re-use, after\nall today it's not uncommon to just publish a bitcoin address, people\nmight be tempted to do the same in Lightning.\n\nCheers,\nChristian"
            },
            {
                "author": "Rusty Russell",
                "date": "2016-08-19T00:56:31",
                "message_text_only": "Christian Decker <decker.christian at gmail.com> writes:\n> On Wed, Aug 17, 2016 at 07:53:03PM +0930, Rusty Russell wrote:\n>> Christian Decker <decker.christian at gmail.com> writes:\n>> > I agree that the realm byte is a sensible addition. To trigger this we\n>> > would need to have multiple channels, on different chains, using the\n>> > same identifiers between two nodes. Only in this case we'd have an\n>> > ambiguity where to transfer the funds. Assuming we have the route A ->\n>> > B => C, where => indicates two channels, one in litecoin and one in\n>> > bitcoin, and both channels use the same identity for C. Then the\n>> > instruction to forward 0.01 units to C is ambiguous, as it could be\n>> > denominated in either litecoin or bitcoin.\n>> >\n>> > While not dangerous it is rather unfortunate as it results in\n>> > guesswork. It is not dangerous because if A transferred litecoin to B\n>> > then B will (hopefully) never forward a higher value to C using\n>> > bitcoin, and if it were bitcoin then the final recipient would not\n>> > sign off an inferior amount than what he expected.\n>> \n>> Worse case: C is a charity, accepting donations.  A's software screwed\n>> up and didn't realize C was litecoin, not bitcoin.  B collects a huge\n>> fee, C gets tiny donation.\n>\n> True, that's a dangerous scenario. If the recipient does not know the\n> intended amount and accepts anything then fee-shaving is very\n> profitable.\n\nWhich creates a subtle requirement: even the terminal onion should\ninclude an amount.\n\n> In general I'm a bit concerned about rhash re-use, after\n> all today it's not uncommon to just publish a bitcoin address, people\n> might be tempted to do the same in Lightning.\n\nHmm, maybe we should implement the code to steal such re-sends?  Or more\ngenerously, fail it.  That would prevent this from becoming a habit, at\nleast.\n\nRelated: I can't seem to figure out why we're so concerned with onion\nreuse?  It seems if a node were to retransmit the same onion runs this\nsame risk.\n\nThanks,\nRusty."
            },
            {
                "author": "Joseph Poon",
                "date": "2016-08-19T18:36:47",
                "message_text_only": "On Fri, Aug 19, 2016 at 10:26:31AM +0930, Rusty Russell wrote:\n> >> > While not dangerous it is rather unfortunate as it results in\n> >> > guesswork. It is not dangerous because if A transferred litecoin to B\n> >> > then B will (hopefully) never forward a higher value to C using\n> >> > bitcoin, and if it were bitcoin then the final recipient would not\n> >> > sign off an inferior amount than what he expected.\n> >> \n> >> Worse case: C is a charity, accepting donations.  A's software screwed\n> >> up and didn't realize C was litecoin, not bitcoin.  B collects a huge\n> >> fee, C gets tiny donation.\n\nYeah, for sure, I agree with y'all. By default, there should be a\nrequirement that the amount is pre-negotiated by the sender and the\nrecipient (pay-to-contract, etc.)\n\nSufficient percentages of senders to a charity should be interested in\ngetting a receipt to prove funds were sent to a charity that I don't\nthink pre-sending it without generating a proof shouldn't be a normal\ncase. By default, I don't think clients should even send funds until\nthey have a signed receipt before cross-chain is supported for safety.\n\nHowever, I'm not too concerned with cross-chain. As long as there's some\nidentifier between each hop, I suspect that should be sufficient. Is the\nintent of the realm byte to indicate protocols? I think it's reasonable\nto have some kind of byte for identifying the message (e.g. using this\nas a transport layer for other things), but I think there should already\nbe sufficient information for cross-chain, presuming pay-to-contract.\n\n> > True, that's a dangerous scenario. If the recipient does not know the\n> > intended amount and accepts anything then fee-shaving is very\n> > profitable.\n> \n> Which creates a subtle requirement: even the terminal onion should\n> include an amount.\n\nThis may not fully solve the problem, since if one presumes that the\nsecond-to-last hop is malicious, they can re-create a new onion blob\n(presuming consistent hashes for each hop, of course).\n\nThere may be a requirement from deriving the fee amount for the last\nhop, though.\n\n> > In general I'm a bit concerned about rhash re-use, after\n> > all today it's not uncommon to just publish a bitcoin address, people\n> > might be tempted to do the same in Lightning.\n> \n> Hmm, maybe we should implement the code to steal such re-sends?  Or more\n> generously, fail it.  That would prevent this from becoming a habit, at\n> least.\n\nEither way seems practical for some nodes -- I presume if a small\npercentage of nodes redeem without forwarding, then basically nobody\nwould re-use.  Not sure if \"steal\" is the right word, though.\n\n-- \nJoseph Poon"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2016-08-20T20:32:19",
                "message_text_only": "Rusty Russell <rusty at rustcorp.com.au> wrote:\n> Which creates a subtle requirement: even the terminal onion should\ninclude an\n> amount.\n\nWith the current draft specification, all hops receive a per-hop payload\n(unless we're now abandoning the payload for the final hop due to the\n\"terminal\" byte?). This behavior isn't changed for the final hop, so they\nalso\nextract a payload once they process the onion packet.\n\nChristian Decker <decker.christian at gmail.com> wrote:\n> If the recipient does not know the intended amount and accepts anything\nthen\n> fee-shaving is very profitable.\n\nIn my mind (although it seems to be handled with the current onion-packet\nformat), this is an issue which should be resolved/negotiated at a higher\nlevel. Ignoring the possibility of using the network as a general messaging\nlayer (which seems a bit ambitious, plus brings up DoS concerns), the\nsender/receiver should have already negotiated all the details of the\npayment.\nIf the receiver communicated the target r-hash, then they should also be\naware\nof the associated value to be paid out with reveal of the corresponding\nr-preimage. Joseph's description fits by current thought model.\n\nRusty Russell <rusty at rustcorp.com.au> wrote:\n> Related: I can't seem to figure out why we're so concerned with onion\nreuse?\n> It seems if a node were to retransmit the same onion runs this same risk.\n\nOne might say that the concern is a bit \"academic\" in nature. In theory,\nwithin\na mix-net (sending emails/messages, not HTLC's), an adversary shouldn't be\nable\nto re-inject an arbitrary previously processed packet thereby causing node\nto\nprocess and forward the packet as normal. If they're able to do this, then\nin\nconjunction with several nodes participating within the network, the\nadversary\nmay be able to partially (worse, even fully) re-construct the original path\nby\nobserving how the replays propagate throughout the network.\n\nHowever, practically, we're currently building something that more\nresembles an\nonion routing network rather than a mix-net. Additionally, as all\ncommunication\nlinks are end-to-end encrypted+authenticated, in reality, the only party\nable\nto attempt to replay packet within the network are nodes whom have directly\nreceived+processes the onion packet in the past. In our particular context,\nassuming the onion packet is encapsulated within the message that adds a new\nHTLC, then an attempt to replay would be foolish as if one assumes nodes\nremember all r-preimages, then the first hop would just immediately pull the\nfunds (as Rusty points out).\n\nJoseph Poon <joseph at lightning.network> wrote:\n> This may not fully solve the problem, since if one presumes that the\n> second-to-last hop is malicious, they can re-create a new onion blob\n> (presuming consistent hashes for each hop, of course).\n\nFirst, *all* hops receive a per-hop payload which ideally would includes\ndetails such as payment, time-lock value, etc. Second, this is only\npossible if\nBob (the second-to-last hop), *knows* that they are in fact, the\nsecond-to-last\nhop *and* already knows all identities following them within the route.\nOtherwise, the route will fail as Bob is unable to construct a new fully\nvalid\nonion packet.\n\nEven in the case that Bob if able to do this, it shouldn't affect the\npayment\nas a whole assuming Dave (the final receiver) knows the value he's expecting\nfor each particular r-hash (as you detailed earlier).\n\n-- Laolu\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20160820/81e0955d/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2016-08-21T20:45:06",
                "message_text_only": "Joseph Poon <joseph at lightning.network> writes:\n> On Fri, Aug 19, 2016 at 10:26:31AM +0930, Rusty Russell wrote:\n>> >> > While not dangerous it is rather unfortunate as it results in\n>> >> > guesswork. It is not dangerous because if A transferred litecoin to B\n>> >> > then B will (hopefully) never forward a higher value to C using\n>> >> > bitcoin, and if it were bitcoin then the final recipient would not\n>> >> > sign off an inferior amount than what he expected.\n>> >> \n>> >> Worse case: C is a charity, accepting donations.  A's software screwed\n>> >> up and didn't realize C was litecoin, not bitcoin.  B collects a huge\n>> >> fee, C gets tiny donation.\n>\n> Yeah, for sure, I agree with y'all. By default, there should be a\n> requirement that the amount is pre-negotiated by the sender and the\n> recipient (pay-to-contract, etc.)\n\nYes, as your point below makes clear; not agreeing on an amount is\nsusceptible to theft by prior hops:\n\n> This may not fully solve the problem, since if one presumes that the\n> second-to-last hop is malicious, they can re-create a new onion blob\n> (presuming consistent hashes for each hop, of course).\n\nGreat catch.  Oops...\n\n>> Hmm, maybe we should implement the code to steal such re-sends?  Or more\n>> generously, fail it.  That would prevent this from becoming a habit, at\n>> least.\n>\n> Either way seems practical for some nodes -- I presume if a small\n> percentage of nodes redeem without forwarding, then basically nobody\n> would re-use.  Not sure if \"steal\" is the right word, though.\n\nI also tend to use that term for the collect-all-because-you-cheated\ntransaction.  Because \"rightfully taking what is mine\" is too much of a\nmouthful :)\n\nCheers,\nRusty."
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2016-08-22T19:47:56",
                "message_text_only": "On Sun, Aug 21, 2016 at 1:46 PM Rusty Russell <rusty at rustcorp.com.au> wrote:\n\n> > This may not fully solve the problem, since if one presumes that the\n> > second-to-last hop is malicious, they can re-create a new onion blob\n> > (presuming consistent hashes for each hop, of course).\n>\n> Great catch.  Oops...\n>\n\nDuring the whole payment negotiation process, the sender and receiver can\nadditionally agree on a shared secret-ish value (possibly the hash of the\ncontract) that should be included in the per-hop payload for the final hop.\n\nIf the portion of the per-hop payload doesn't match identically with this\nvalue, then the payment should be rejected as a prior node has\nunsuccessfully attempted re-create the onion packet.\n\n\n-- Laolu\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20160822/1a545f72/attachment.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2016-08-16T04:54:28",
                "message_text_only": "Rusty Russell <rusty at rustcorp.com.au> wrote:\n\n> Some node will have to straddle two chains, right?  So you'd route A ->\n> B -> C as normal, and C is (say) litecoin (B straddles both).  You\n> really want the onion to be explicit that this transfer to litecoin is\n> what the sender intended.  Or some sidechain.\n>\n> Now, we'd hope nobody would screw this up, but I think it's worth\n> flagging since the sender really should know it's changing chains.\n>\n> Ahh, I'm starting to see your point now.\n\nAgreed that it doesn't hurt to allocate an extra byte in order to make the\nchain transitions *explicit*. Alternatively, (instead of modifying the\nheader) we can simply allocate the first byte of the per-hop payload for\nthis purpose.\n\n-- Laolu\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20160816/9a0bd76b/attachment-0001.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2016-08-04T17:05:04",
                "message_text_only": "Sending to the mailing list since Laolu pointed out I only sent my reply to\nhim. Sorry for that.\n\nOn Sat, Jul 30, 2016, 15:25 Christian Decker <decker.christian at gmail.com>\nwrote:\n\n> On Wed, Jul 27, 2016 at 8:14 PM Olaoluwa Osuntokun <laolu32 at gmail.com>\n> wrote:\n>\n>> Hi Christian, welcome to the mailing list!\n>>\n> Hi Laolu,\n>\n> glad to be here :-)\n>\n>>\n>> Excellent work! I've been meaning to re-visit my implementation since I\n>> finished it last October but have been busy tending to other lnd related\n>> items.\n>> Thanks for the cleanups and optimizations you've added to my initial\n>> Sphinx\n>> implementation. Once we've finished fleshing out the initial\n>> specification, I'd\n>> be happy to get those changes merged!\n>>\n>> I've taken a look at the diff against the existing implementation, plus\n>> the\n>> current spec draft. I'd say the current spec draft is the *clearest*\n>> description of Sphinx I've encountered, especially the padding bits. IMO,\n>> the\n>> notation within the paper describing how to construct+process the\n>> mix-header is\n>> rather opaque.\n>>\n> Actually, your implementation helped a lot in getting a clear picture of\n> how Sphinx is supposed to work, so thanks for taking the time to implement\n> the paper in the first place. Glad you like my writeup, hopefully it is as\n> clear to new implementors as well :-)\n>\n>>\n>> I see you've modified the encryption scheme of the end-to-end payload,\n>> switching from the arbitrary block-size block cipher (LIONESS) to purely a\n>> stream cipher (ChaCha20). Initially I was under the impression that this\n>> might\n>> compromise one of the security arguments of the scheme, but on closer\n>> inspection this is perfectly fine if one extends the header MAC to the\n>> entire\n>> payload as you've done. If one was staying true to the original\n>> construction,\n>> then this would eliminate the possibility of Single-Use Reply Blocks\n>> (SURB's)\n>> since the sender would be unable to construct the reply mix-header as she\n>> would\n>> be unaware of they responder's message. It's clear to me now that a MAC\n>> covering\n>> the end-to-end payload was omitted in the original version since the\n>> proof of\n>> computational indistinguishability of replies vs responses depends on the\n>> structure+processing being identical for both messages types. However I\n>> don't\n>> see us having any use for SURB's so this is an excellent change.\n>> Additionally,\n>> modifications to the end-to-end payload will instantly cause packet\n>> corruption,\n>> stopping invalid packets from propagating through the network.\n>>\n>\n> I'm going back and forth about including the payloads in the header HMAC.\n> I think we have three options here:\n>\n>  1) Include the payload in the header HMAC computation\n>  2) Include an additional set of HMACs for the payload that can be checked\n> at each hop\n>  3) Just use an encryption scheme that also verifies the integrity, e.g.,\n> ChaCha20-Poly1305, on the last hop and normal ChaCha20 on all preceeding\n> hops.\n>\n> The first option is small, and it discards tampered packets as soon as the\n> next hop checks its HMAC. The downside is indeed that we lose a lot of\n> flexibility. Besides losing the ability to provide a SURB to the final\n> recipient, we also lose the ability to do anonymous rendezvous meetings,\n> where the final recipient provides half of the route in the form of a\n> precompiled header (something that Hornet is using).\n>\n> The second option is quite costly just to have the drop-early feature. It\n> would add 400 bytes to each packet, assuming 20 byte HMACs and 20 hops. On\n> the other hand forwarding a packet to the final recipient when we could\n> discard it early is quite costly since each hop locks up some funds in the\n> form of an associated HTLC.\n>\n> The third option is just the case in which we'd forward the packet to the\n> final recipient, which can then decide whether the payload was tampered\n> with or not. Costly in terms of HTLCs being created and funds being locked\n> up, but hopefully they'd be released again immediately.\n>\n> Both the per-hop checkable schemes, combined with nodes signing the\n> packets they forward, would give us the ability to identify misbehaving\n> nodes and denounce them: if we receive a packet we check the integrity and\n> if it doesn't match then we can prove to others that the node forwarded\n> something broken with its signature, or it did not check the packet itself.\n>\n>\n>> I really like the addition of the per-hop payload! It's a change to the\n>> original construction that I've seriously considered proposing. Such a\n>> payload\n>> should prove to be very useful in the future for information such as:\n>> limits on\n>> the per-hop absolute timeout, fee information, etc.\n>>\n>> The addition of a version byte is also very welcome. This'll streamline\n>> future\n>> modifications we may make to the mix-header format in the future, such as\n>> increasing the size of the per-hop payload, or switching to an alternative\n>> format to encoding the \"next hop\" address.\n>>\n>\n> Good idea, the next hop address basically does just have to make sense to\n> the node processing the packet, so maybe we can use some form of short tag\n> or index to specify which existing channel to use, instead of using the\n> globally unique address.\n>\n>>\n>> The current draft doesn't specify the processor's action in the scenario\n>> that\n>> they're unable to locate the next hop node within their local routing\n>> table.\n>> Just to be explicit, I think a final paragraph should be inserted under\n>> the\n>> \"Packet Forwarding\" section detailing the abort procedure.\n>>\n>\n> Good catch, I'll amend the spec to specify that unroutable packets are\n> dropped and the sender signaled.\n>\n>>\n>>\n>> > However, they could easily be made variable should we decide that\n>> sending\n>> > mostly empty messages is wasteful.\n>>\n>> I strongly think we should maintain the size uniformity of the packet\n>> throughout processing, changes in payload size between hop can give away\n>> information w.r.t a node's position within the route.\n>>\n>> We might want to consider dropping the end-to-end payload altogether\n>> though. I\n>> can't really think of a clear use case for the e2e payload within our\n>> specific\n>> application.  That would save us 1k bytes, reducing the size of the full\n>> mix-header to 1234 bytes. Accounting for the additional fields within an\n>> HTLC\n>> \"add\" message, plus some additional overhead, this should keep us below\n>> typical\n>> MTU sizes, avoiding fragmentation of HTLC \"add\" messages.\n>>\n>\n> There is a tradeoff between small packets and keeping the size uniform. I\n> think we could bucketize sizes, e.g., have multiples of 32 bytes or 64\n> bytes for the fields, to have packets with similar sized payloads have the\n> same packet size. That would allow us to also drop the e2e payload by\n> setting a size of 0, and still be able to forward it, should we ever find a\n> use for it.\n>\n>>\n>> > This specification is limited to version 0 packets and the structure of\n>> > future version may change. The receiving node then splits the packet\n>> into its\n>> > fields.\n>>\n>> Packets with a non-zero version byte should be immediately rejected, as\n>> well as\n>> packets which aren't *exactly* 2258 bytes (or 1234 bytes if we drop the\n>> e2e\n>> payload).\n>>\n>\n> I don't think we need a size check: the fixed size actually allows us to\n> serialize directly on the wire, without a size prefix, so if we read less\n> than 2258 bytes then we simply continue reading, if we read more, then we\n> crossed a packet boundary and ought to split. But maybe that is mixing\n> transport layer and packet specification?\n>\n>>\n>> > The resulting HMAC is compared with the HMAC from the packet. Should the\n>> > computed HMAC and the HMAC from the packet differ then the node MUST\n>> abort\n>> > processing and report a route failure.\n>>\n>> Perhaps we should explicitly specify that the HMAC equality check MUST be\n>> performed without leaking timing information (constant time comparison)? I\n>> can't think of a precise potential vulnerability otherwise since the\n>> scheme\n>> uses an encrypt-then-MAC construction with a semantically secure\n>> encryption\n>> scheme. I don't see any clear downsides in specifying that the comparison\n>> be\n>> made in constant.\n>>\n>\n> I don't see a chance of this being used either, each secret key used in\n> the HMAC computation is used just once. But better be safe than sorry. I'll\n> add it to the spec.\n>\n>>\n>> > The sender computes a route {n_1, ..., n_{r-1}, n_r}, where n_1 is a\n>> peer of\n>> > the sender and n_r is the recipient.\n>>\n>> In order to eliminate ambiguity, I think this should be more explicit,\n>> specifying that \"n_1\" is a *direct neighbor* of the sender\n>>\n>> Amended and clarified that a peer is a direct neighbor in the overlay\n> network.\n>\n>\n>> > A special HMAC value of 20 0x00 bytes indicates that the currently\n>> > processing hop is the intended recipient and that the packet should not\n>> be\n>> > forwarded. At this point the end-to-end payload is fully decrypted and\n>> the\n>> > route has terminated.\n>>\n>> It seems that with the current construction, then the \"next hop\" address\n>> will\n>> also be zero bytes if a packet processor is the last hop in the route.\n>> Alternatively, if the sender is aware that the receiver is actually a\n>> \"virtual\n>> channel\", then an additional address could be used instead of the\n>> zero-address\n>> to facilitate de-multiplexing at the last hop to the destination virtual\n>> channel.\n>>\n>\n> Sounds good, I thought I'd use the HMAC to signal so we still have the\n> first 20 bytes free to use, adding a de-multiplexing address might be one\n> use.\n>\n>>\n>> > In the pocessing phase the secret is the node's private key...\n>>\n>> Typo here, it should read \"In the processing phase...\"\n>>\n>> I think two key onion-routing related aspects are under specified within\n>> the\n>> current draft: replay protection, and key rotation. Although we might\n>> want to\n>> place details concerning key rotation in a separate document covering the\n>> initial routing protocol as the two are closely related.\n>>\n>> First, lets talk replay protection. The current draft specifies that:\n>>\n>> > The node MUST keep a log of previously used shared secrets. Should the\n>> shared\n>> > secret already be in the log it MUST abort processing the packet and\n>> report a\n>> > route failure, since this is likely a replay attack, otherwise the\n>> shared\n>> > secret is added to the log\n>>\n>> This is definitely necessary, however as dictated this would require\n>> nodes to\n>> allocate a potentially *unbounded* amount of storage to the shared secret\n>> \"seen\" log. I think we can allow nodes to periodically truncate this log\n>> by\n>> adding an additional session time stamp to the mix-header, either placed\n>> directly after the version byte, or within the per-hop payload.\n>>\n>> With this absolute timestamp, each entry within the \"seen\" log becomes a\n>> two-tuple: the shared secret itself, and the corresponding timestamp\n>> specified\n>> within the mix-header. Before the absolute timestamp has passed, the entry\n>> within the log remains, and mix-headers received with duplicated shared\n>> secret\n>> are rejected. If we enforce an upper bound on the \"session lifetime\", then\n>> nodes can periodically prune this log, discarding obsolete shared secrets.\n>> Once an entry has been pruned, although a node may not know if a shared\n>> secret\n>> is being duplicated, they can reject expired sessions according to the\n>> timestamp achieving a similar affect.\n>>\n>> Reasonable session times may be something around 30-minutes to an hour or\n>> two.\n>>\n>> With this scheme, I think that we can achieve near perfect replay\n>> protection\n>> without unbounded storage.\n>>\n>\n> We have to be careful when using timestamps in the packet as it makes\n> individual hops collatable. One solution is again to bucketize timestamps\n> included in the packet such that we have enough packets with the same\n> timestamps to avoid having an attacker associate individual hops in a\n> route. The alternative is to have a blinded timestamp per hop, i.e., in the\n> routing info, but that automatically blows the size up to 20x. So my\n> proposal would be to include a timestamp rounded up to the closest hour and\n> have a sliding window of accepted timestamps of +/- 1 hour, remembering the\n> secrets for that period and rejecting anything that is too far in the\n> future or too far in the past. The more coarse the bigger the less likely\n> an attacker is to guess which packets belong to the same route, but the\n> more storage is required on the node's side.\n>\n>>\n>> On to the second aspect: key rotation. Ignoring the possible time-stamped\n>> log\n>> solution, the (possibly) only other way to allow nodes to prune their\n>> shared\n>> secret log is to periodically rotate keys. Once a node rotates a key, it\n>> can\n>> safely delete its *entire* previous shared secret log, as replay attacks\n>> will\n>> fail on the HMAC check. Independent of replay attack prevention, key\n>> rotation\n>> is useful in order to provide a degree of forward secrecy. Without key\n>> rotation, when a node is compromised by the adversary (assuming the node\n>> keeps\n>> *all* prior mix-headers), the adversary learns of the next-node within the\n>> route, and also the per-hop payload for the compromised node. With key\n>> rotation, assuming the prior keys are deleted, then the adversary is only\n>> able\n>> to decrypt partially ciphertexts from the current epoch.\n>>\n>> So then a question arises: how do we perform key rotation within the\n>> network\n>> globally with loose synchronization? I say loose synchronization since if\n>> rotations aren't synchronized to a degree, then the payments of source\n>> nodes\n>> may fail as an intermediate hop is unable to process the packet since it\n>> used an\n>> obsolete onion key. Therefore published onion keys should come in pairs\n>> (with\n>> overlapping lifetimes), and also be authenticated by a node's identity\n>> key.\n>>\n>> A key rotation scheme might look something like the following:\n>>     * A node publishes two keys, along with a block hash of a block\n>> beyond a\n>>       \"safe\" re-org distance, and a signature (under the identity pubkey)\n>>       covering the advertisement.\n>>     * The first key is intended for use until N blocks after the specified\n>>       block hash, with nodes switching to the second key afterwards.\n>>     * At the N/2 point, the original node publishes a new advertisement\n>> with\n>>       the second key from the original advertisement listed as the\n>> \"first\" key,\n>>       and a new fresh quasi-ephemeral onion key. The original node\n>> performs\n>>       this rotation at intervals at the mid-point of expiration of the\n>> oldest\n>>       key.\n>>     * Nodes who receive this new advertisement (aware of the previous)\n>> record\n>>       this as the node's next rotation key. Nodes who receive this\n>>       advertisement, unaware of the previous treat this as the node's\n>> initial\n>>       pair of quasi-ephemeral onion keys.\n>>\n>> With this scheme, rotations are synchronized very loosely, perhaps in the\n>> timespan of half-days, days, etc. There is a new cost however, when\n>> processing\n>> packets, a node must attempt to derive the shared secret with both active\n>> onion\n>> keys. Alternatively, instead of block hashes, we can use some other time\n>> based\n>> beacon as a switch over point in order to accommodate peers on multiple\n>> blockchains.\n>>\n>\n> I like the loosely synched approach very much, but do we need explicit\n> announcements at all? We could just use the announced key, i.e., the one\n> that participated in the channel setup, as a root key for HD key\n> derivation. The derivation path could then be based on floor(blockheight /\n> 144) so that we rotate keys every day, and don't need any additional\n> communication to announce new public keys. The switchover can be a bit\n> messy, but we could just accept both old and new keys for a period around\n> switchover and try both, or add the date in the packet, which would anyway\n> be common to all packets on that day.\n>\n> A downside of using HD key derivation is that, should the root key be\n> compromised then we cannot switch keys to new ones without a teardown, but\n> in that case we'd be in a world of pain anyway since these keys allow\n> spending the node's coins :-) Not adding the date allows us to switch keys\n> quicker and each node could announce its own rotation period, to keep local\n> storage low, but it adds some more computation as we determine the intended\n> public key by trial and error.\n>\n>>\n>> I'll take a few more passes through the current draft spec, as well are\n>> your\n>> commits in your fork of my original implementation, following up with any\n>> other\n>> questions/comments.\n>>\n>> I'm also trying to figure out how to enable intermediate nodes to reply\n> to a packet, e.g., if capacities are insufficient or the next node is\n> unreachable, by recycling the routing info. Currently we'd probably forward\n> reply along the HTLC path associated with the forward path over the\n> encrypted channel.\n>\n> So far I had no luck trying to build a return path into the header while\n> forwarding. Maybe we could continue blinding the ephemeral key on the\n> return path, and have a mechanism to tell the node the total blinding\n> factor along the path so that it can encrypt something in the routing info\n> for the return path? That would neatly combine Hornet and Sphinx,\n> eliminating the initial roundtrip to setup forwarding segments.\n>\n> Thanks again for the detailed feedback, looking forward to read your\n> thoughts on this :-)\n>\n> Cheers,\n> Christian\n>\n> -- Laolu\n>>\n>>\n>> On Mon, Jul 25, 2016 at 9:23 AM Christian Decker <\n>> decker.christian at gmail.com> wrote:\n>>\n>>> Hi all,\n>>>\n>>> I took the liberty of taking apart Olaoluwa's Sphinx implementation and\n>>> I came up with a spec draft that I'd like to propose [1]. It should roughly\n>>> be Sphinx, pinning down the various key-generation and stream generation\n>>> algorithms, and adding a per-hop payload.\n>>>\n>>> The per-hop payload is used to give instructions to individual hops,\n>>> i.e., how many coins to forward to the next hop. This means that the\n>>> end-to-end payload, i.e., the message in the Sphinx protocol, is currently\n>>> unused and could be omitted.\n>>>\n>>> The payloads are currently fixed size (20 bytes per-hop and 1024 bytes\n>>> for end-to-end payload) to avoid making messages collatable by their size.\n>>> However, they could easily be made variable should we decide that\n>>> sending mostly empty messages is wasteful.\n>>>\n>>> The spec is implemented in Go [2] and in C [3]. The Go version is an\n>>> adaptation of Olaoluwa's implementation, with some minor speedups, removing\n>>> some duplicate information, stripping the header, and switching to ChaCha20\n>>> for stream generation and encryption. I also added a small commandline tool\n>>> that allows you to write packets to stdout so that we can feed an onion\n>>> generated by the C version to the Go implementation and vice-versa :-)\n>>>\n>>> Feedback is very welcome. If people like the draft I'll create\n>>> pull-requests for the spec and the implementations, but I'd like to keep\n>>> the discussion on the mailing list :-)\n>>>\n>>> Cheers,\n>>> Christian\n>>>\n>>> [1]\n>>> https://github.com/cdecker/lightning-rfc/blob/master/bolts/onion-protocol.md\n>>> [2] https://github.com/cdecker/lightning-onion/tree/chacha20\n>>> [3] https://github.com/cdecker/lightning/tree/chacha20\n>>>\n>> _______________________________________________\n>>> Lightning-dev mailing list\n>>> Lightning-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20160804/23e60f7a/attachment-0001.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2016-08-04T18:24:25",
                "message_text_only": "> I'm going back and forth about including the payloads in the header HMAC.\nI\n> think we have three options here:\n>\n> 1) Include the payload in the header HMAC computation\n\nI'd say personally, I prefer the first option. This results in \"fail fast\"\nbehavior w.r.t packet forwarding, and additionally adds the smallest\noverhead.\n\n> we also lose the ability to do anonymous rendezvous meetings, where the\nfinal\n> recipient provides half of the route in the form of a precompiled header\n> (something that Hornet is using).\n\nIt doesn't appear that we loose the ability to do rendezvous routing if we\nfollow through with the first option. The final recipient can still provide\na\nprecompiled header which is the e2e payload sent from the source to the\nrendezvous node. As the source knows the exact nested mix-header when\nsending,\nit can still be protected under the mix-header wide MAC.\n\nAdditionally, in order to hide the next-hop after the rendezvous node from\nthe\nsource node, the destination can wrap the nested header in a layer of\nencryption, decryptable only by the rendezvous node.\n\n> Both the per-hop checkable schemes, combined with nodes signing the\npackets\n> they forward, would give us the ability to identify misbehaving nodes and\n> denounce them: if we receive a packet we check the integrity and if it\ndoesn't\n> match then we can prove to others that the node forwarded something broken\n> with its signature, or it did not check the packet itself.\n\nGreat observation. However, it seems like this is currently out of scope\n(the\nimplications of \"denouncing\" a node\") and should be re-visited at a future\ntime\nwhen we brainstorm some sort of \"reputation\" scheme.\n\n> There is a tradeoff between small packets and keeping the size uniform. I\nthink\n> we could bucketize sizes, e.g., have multiples of 32 bytes or 64 bytes\nfor the\n> fields, to have packets with similar sized payloads have the same packet\nsize.\n> That would allow us to also drop the e2e payload by setting a size of 0,\nand\n> still be able to forward it, should we ever find a use for it.\n\nGood point, we may have uses for non-uniform sizes as far as mix-headers in\nthe\nfuture. So with this, then it appears there may be 3 types of mix-header\nformats:\n  1. Regular. Meaning no e2e payload, weighing in at 1234 bytes.\n  2. Extended. Meaning bearing the e2e payload with a size of 2468 bytes.\n  3. Rendezvous. Which nests another mix-header within the end-to-end\npayload,\n     with a size which is double that of the regular.\n\nIf we like this taxonomy, then we may want to reserve the first 2 version\nbytes\nwithin the draft. A version 0 packet would encompass processing the first\ntwo\ntypes, while a version 1 packet denotes that this is a rendezvous packet.\nThe\nrendezvous case needs to be distinct as it modifies the\nprocessing/forwarding\nat the final hop.\n\nAlternatively, we can use solely a version of 0 in the initial spec, with\nthe\nfinal hop checking if the [1:34] bytes of the payload (if one is present)\nare a\npoint on the curve.  If so, this triggers the rendezvous forwarding, with\nthe\nmid-point node processing the packet again as normal, completing the\nrendezvous\nroute.\n\n> We have to be careful when using timestamps in the packet as it makes\nindividual hops collatable.\n\nExcellent observation. Assuming we roll out a reasonably efficient solution\nfor\nthe collatable HTLC R-values across hops, naively selecting timestamps would\npresent another correlation vector.\n\n> So my proposal would be to include a timestamp rounded up to the closest\nhour\n> and have a sliding window of accepted timestamps of +/- 1 hour,\nremembering the\n> secrets for that period and rejecting anything that is too far in the\nfuture or\n> too far in the past.\n\nThis seems reasonable, also the size of the sliding window can easily be\ntuned\nin the future should we find it too large or small.\n\n> The more coarse the bigger the less likely an attacker is to guess which\n> packets belong to the same route, but the more storage is required on the\n> node's side.\n\nYep, there's a clear trade off between the window size of the accepted\ntimestamps, and a node's storage overhead. We can tune this value to a\nballpark\nestimate of the number of HTLCs/sec a large node with high frequency\nbi-directional throughput may forward at peak times.\n\nLet's say a HFB (High-Frequency Bitcoin) node on the network at peak\nforwards\n5k HTLC's per second: (5000/sec * 32 bytes) * 3600 sec = 576MB, if nodes are\nrequired to wait 1 hour between log prunings, and 288MB if we use a\n30-minute\ninterval. Even with such a high throughput value, that seems reasonable.\n\n> We could just use the announced key, i.e., the one that participated in\nthe\n> channel setup, as a root key for HD key derivation. The derivation path\ncould\n> then be based on floor(blockheight / 144) so that we rotate keys every\nday, and\n> don't need any additional communication to announce new public keys.\n\nGreat suggestion! However, I think instead of directly using the key which\nparticipated in the channel setup, we'd use a new independent key as the\nroot\nfor this HD onion derivation. This new independent key would then be\nauthenticated via a signature of a schnorr multi-sign of the channel\nmulti-sig\nkey and the node's identity key (or alternatively two sigs). This safeguards\nagainst the compromise of one of the derived private keys leading to\ncompromise\nof the master root HD priv key which would allow possibly stealing a node's\ncoins. Additionally, a node can switch keys more easily, avoiding a channel\ntear down.\n\nHowever, the HD Onion Key approach can potentially cancel out the forward\nsecrecy benefits. If an attacker gains access to the root HD pubkey, along\nwith\nany of the child derived onion keys, then they can compute the root privkey.\nThis allows the attacker to derive all the child priv keys, giving them the\nability to decrypt all mix-headers encrypted since the HD Onion Key was\npublished.\n\nI think we can patch this exploit by adding some precomputation for each\nnode,\nand introducing an intermediate onion derivation point. Assuming we rotate\nevery 144+ (1 day) blocks, then using the HD Onion PrivKey, each node\npre-generates 365 (or a smaller batch size) keys. Then, generates an\nindependent \"onion derivation\" key. The OD key then combined with each of\nthe\nchild onion keys, produces the final child onion key (C_i = final onion key,\nB_i = intermediate child key, A = OD):\n    * C_i = B_i + A\n\nAfter the precomputation, the OD key (A) should be *destroyed*. If so, even\nif\nan attacker gains access to one of the intermediate child onion keys,\nthey're\nunable to derive the final child onion key as the OD key has been destroyed.\nThis safeguards the forward secrecy of the scheme in the face of the HD\nroot+child exploit. As before, in the case of a root/child compromise the\noriginal node can simply authenticate a new HD Onion Key.\n\nSo perhaps we can combine the two approaches, publishing a blockhash (buried\nunder a \"safe\" re-org depth), along with an authenticated HD root pubkey.\nWith\nthis new scheme we're able to push key rotation out to the edges in a\nnon-interactive manner. Having the blockhash as an anchor will reduce the\namount of guessing required by a node to fetch the correct onion key.\n\n> I'm also trying to figure out how to enable intermediate nodes to reply\nto a\n> packet, e.g., if capacities are insufficient or the next node is\nunreachable,\n> by recycling the routing info.\n\nYeah I've also attempted to tackle this issue a bit myself. The inclusion of\nonion routing certainly makes certain classes of failures harder to\nreconcile.\nThere has been a bit of discussion of this in the past, at the time called\n\"unrolling the onion\". In a similar vein it's also more difficult to\nascribe blame node's which directly cause a payment to fail.\n\nOne of my naive ideas was to include a \"backwards\" mix-header within each\nnode's per-hop payload (though I hadn't included the per-hop payload in my\nmental model at the time), however this would result in a quadratic blow up\nspace complexity parametrized by our max-hop limit. Essentially, we'd\ninclude\na SURB within the mix-header for each hop in the route.\n\n> Maybe we could continue blinding the ephemeral key on the return path, and\n> have a mechanism to tell the node the total blinding factor along the\npath so\n> that it can encrypt something in the routing info for the return path?\nThat\n> would neatly combine Hornet and Sphinx, eliminating the initial roundtrip\nto\n> setup forwarding segments.\n\nCould you elaborate on this a bit more? It seems that this alone is\ninsufficient to allow \"backwards\" replies to the source w/o revealing the\nsource's identity.\n\nIt seems the primary question is: how can we re-use the information present\nat\na hop, post-processing to reply to the sender without an additional round\ntrip?\nIf we can solve this, then we can greatly increase the robustness of onion\nrouting within the network. I think they may be worth spinning some cycles\non,\nalthough I don't consider it blocking w.r.t the initial specification.\n\n-- Laolu\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20160804/30f33520/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2016-08-05T04:00:54",
                "message_text_only": "Olaoluwa Osuntokun <laolu32 at gmail.com> writes:\n>> I'm going back and forth about including the payloads in the header HMAC.\n> I\n>> think we have three options here:\n>>\n>> 1) Include the payload in the header HMAC computation\n>\n> I'd say personally, I prefer the first option. This results in \"fail fast\"\n> behavior w.r.t packet forwarding, and additionally adds the smallest\n> overhead.\n\nAgreed.  I worry about forwarding corrupted packets leading to the\nability for malicious nodes to probe routes.\n\n>> we also lose the ability to do anonymous rendezvous meetings, where the\n> final\n>> recipient provides half of the route in the form of a precompiled header\n>> (something that Hornet is using).\n>\n> It doesn't appear that we loose the ability to do rendezvous routing if we\n> follow through with the first option. The final recipient can still provide\n> a\n> precompiled header which is the e2e payload sent from the source to the\n> rendezvous node. As the source knows the exact nested mix-header when\n> sending,\n> it can still be protected under the mix-header wide MAC.\n>\n> Additionally, in order to hide the next-hop after the rendezvous node from\n> the\n> source node, the destination can wrap the nested header in a layer of\n> encryption, decryptable only by the rendezvous node.\n\nIn practice, you can do this one level up: simply agree with a rendevous\nnode that a given H-hash is to be fwded to you.  Then direct the payer\nto the rendevous node.\n\nSo I don't think it's worth any complexity in the routing protocol.\n\n>> There is a tradeoff between small packets and keeping the size uniform. I\n> think\n>> we could bucketize sizes, e.g., have multiples of 32 bytes or 64 bytes\n> for the\n>> fields, to have packets with similar sized payloads have the same packet\n> size.\n>> That would allow us to also drop the e2e payload by setting a size of 0,\n> and\n>> still be able to forward it, should we ever find a use for it.\n>\n> Good point, we may have uses for non-uniform sizes as far as mix-headers in\n> the\n> future. So with this, then it appears there may be 3 types of mix-header\n> formats:\n>   1. Regular. Meaning no e2e payload, weighing in at 1234 bytes.\n>   2. Extended. Meaning bearing the e2e payload with a size of 2468 bytes.\n>   3. Rendezvous. Which nests another mix-header within the end-to-end\n> payload,\n>      with a size which is double that of the regular.\n>\n> If we like this taxonomy, then we may want to reserve the first 2 version\n> bytes\n> within the draft. A version 0 packet would encompass processing the first\n> two\n> types, while a version 1 packet denotes that this is a rendezvous packet.\n\nKeep it simple; let's just support regular for now.  Nodes will have to\nbroadcast what extensions they support, and this can be used for\nextended formats later.  Including ones we *didn't* think of yet...\n\n>> We have to be careful when using timestamps in the packet as it makes\n> individual hops collatable.\n>\n> Excellent observation. Assuming we roll out a reasonably efficient solution\n> for\n> the collatable HTLC R-values across hops, naively selecting timestamps would\n> present another correlation vector.\n>\n>> So my proposal would be to include a timestamp rounded up to the closest\n> hour\n>> and have a sliding window of accepted timestamps of +/- 1 hour,\n> remembering the\n>> secrets for that period and rejecting anything that is too far in the\n> future or\n>> too far in the past.\n>\n> This seems reasonable, also the size of the sliding window can easily be\n> tuned\n> in the future should we find it too large or small.\n>\n>> The more coarse the bigger the less likely an attacker is to guess which\n>> packets belong to the same route, but the more storage is required on the\n>> node's side.\n>\n> Yep, there's a clear trade off between the window size of the accepted\n> timestamps, and a node's storage overhead. We can tune this value to a\n> ballpark\n> estimate of the number of HTLCs/sec a large node with high frequency\n> bi-directional throughput may forward at peak times.\n>\n> Let's say a HFB (High-Frequency Bitcoin) node on the network at peak\n> forwards\n> 5k HTLC's per second: (5000/sec * 32 bytes) * 3600 sec = 576MB, if nodes are\n> required to wait 1 hour between log prunings, and 288MB if we use a\n> 30-minute\n> interval. Even with such a high throughput value, that seems reasonable.\n\nI think we're over-designing.  How about: daily key rotation (which we\nwant anywat), remember all onions for current and previous key.\n\nRemember: if we switch from C-hash to C-point, then it's simpler: we\nonly need to guard against retransmissions for *unresolved* htlcs.  If\nsomeone retransmits an HTLC for which we already know the C-point value,\nthey risk us redeeming it immediately and not forwarding at all.\n\n(We need to remember all previous HTLCs anyway, so off the top of my\nhead checking this is not too hard...).\n\n>> We could just use the announced key, i.e., the one that participated in\n> the\n>> channel setup, as a root key for HD key derivation. The derivation path\n> could\n>> then be based on floor(blockheight / 144) so that we rotate keys every\n> day, and\n>> don't need any additional communication to announce new public keys.\n>\n> Great suggestion! However, I think instead of directly using the key which\n> participated in the channel setup, we'd use a new independent key as the\n> root\n> for this HD onion derivation. This new independent key would then be\n> authenticated via a signature of a schnorr multi-sign of the channel\n> multi-sig\n> key and the node's identity key (or alternatively two sigs). This safeguards\n> against the compromise of one of the derived private keys leading to\n> compromise\n> of the master root HD priv key which would allow possibly stealing a node's\n> coins. Additionally, a node can switch keys more easily, avoiding a channel\n> tear down.\n>\n> However, the HD Onion Key approach can potentially cancel out the forward\n> secrecy benefits. If an attacker gains access to the root HD pubkey, along\n> with\n> any of the child derived onion keys, then they can compute the root privkey.\n> This allows the attacker to derive all the child priv keys, giving them the\n> ability to decrypt all mix-headers encrypted since the HD Onion Key was\n> published.\n\nBroadcasting new node keys (up to?) once a day is probably fine.\nPerhaps include a validity time range with each key, so you can spot if\nyou're missing one.  Recommend allowing 12 hours overlap or something.\n\nIt'd be great to avoid it, but that seems complex enough to push to a\nfuture spec.\n\nTo summarize the keys for each node:\n\n1. channel key: bitcoin key used to sign commitment txs.  One per channel.\n2. id key: used to tie channels together (\"I own these channels\").  Signed\n   by channel keys (or could use OP_RETURN, but that's a bit spammy), and\n   signs channel keys.\n3. comms key: rotated key for onion messages.  Signed by id key.\n4. (various ephemeral keys for inter-node comms).\n\nid and comms keys don't have to be bitcoin keys; could be Schnorr.  But\nnot much point AFIACT: the big win is making the channel keys\n(ie. bitcoin) use Schnorr so they can all compactly sign the id key.\n\n>> I'm also trying to figure out how to enable intermediate nodes to reply\n> to a\n>> packet, e.g., if capacities are insufficient or the next node is\n> unreachable,\n>> by recycling the routing info.\n>\n> Yeah I've also attempted to tackle this issue a bit myself. The inclusion of\n> onion routing certainly makes certain classes of failures harder to\n> reconcile.\n\nYeah, this one's troubling.  In particular, it'd be nice to prove that\na node is misbehaving:\n\n(1) When a node gives a fail message, we want to be able to publish it\n    to prove (eg) it's lying about its fees.  That means that the\n    failure msg needs to be tied to the request so both can be\n    published.\n\n(2) If a node corrupts a fail message on return, we want to prove that.\n\nCaveats:\n1.  We don't want to expose the source of fail message (ie. leak\n    the route).\n2.  Ideally the proof can be published in a way which minimizes data\n    exposure for the originator.\n\n> If we can solve this, then we can greatly increase the robustness of onion\n> routing within the network. I think they may be worth spinning some cycles\n> on,\n> although I don't consider it blocking w.r.t the initial specification.\n\nI look forward to what you come up with!\n\nYay!\nRusty."
            },
            {
                "author": "Christian Decker",
                "date": "2016-08-08T12:27:39",
                "message_text_only": "On Thu, Aug 4, 2016 at 8:24 PM Olaoluwa Osuntokun <laolu32 at gmail.com> wrote:\n\n> > I'm going back and forth about including the payloads in the header\n> HMAC. I\n> > think we have three options here:\n> >\n> > 1) Include the payload in the header HMAC computation\n>\n> I'd say personally, I prefer the first option. This results in \"fail fast\"\n> behavior w.r.t packet forwarding, and additionally adds the smallest\n> overhead.\n>\n> > we also lose the ability to do anonymous rendezvous meetings, where the\n> final\n> > recipient provides half of the route in the form of a precompiled header\n> > (something that Hornet is using).\n>\n> It doesn't appear that we loose the ability to do rendezvous routing if we\n> follow through with the first option. The final recipient can still\n> provide a\n> precompiled header which is the e2e payload sent from the source to the\n> rendezvous node. As the source knows the exact nested mix-header when\n> sending,\n> it can still be protected under the mix-header wide MAC.\n>\n> Additionally, in order to hide the next-hop after the rendezvous node from\n> the\n> source node, the destination can wrap the nested header in a layer of\n> encryption, decryptable only by the rendezvous node.\n>\n\nSounds good, however I'm not clear on how the final recipient can provide a\nprecompiled valid header with the HMACs that include the per-hop payloads\nand the end-to-end payload without knowing them upfront.\n\n>\n> > Both the per-hop checkable schemes, combined with nodes signing the\n> packets\n> > they forward, would give us the ability to identify misbehaving nodes and\n> > denounce them: if we receive a packet we check the integrity and if it\n> doesn't\n> > match then we can prove to others that the node forwarded something\n> broken\n> > with its signature, or it did not check the packet itself.\n>\n> Great observation. However, it seems like this is currently out of scope\n> (the\n> implications of \"denouncing\" a node\") and should be re-visited at a future\n> time\n> when we brainstorm some sort of \"reputation\" scheme.\n>\n\nHappy to shelve the idea for now, I'll add it to my future-topics list :-)\n\n>\n> > There is a tradeoff between small packets and keeping the size uniform.\n> I think\n> > we could bucketize sizes, e.g., have multiples of 32 bytes or 64 bytes\n> for the\n> > fields, to have packets with similar sized payloads have the same packet\n> size.\n> > That would allow us to also drop the e2e payload by setting a size of 0,\n> and\n> > still be able to forward it, should we ever find a use for it.\n>\n> Good point, we may have uses for non-uniform sizes as far as mix-headers\n> in the\n> future. So with this, then it appears there may be 3 types of mix-header\n> formats:\n>   1. Regular. Meaning no e2e payload, weighing in at 1234 bytes.\n>   2. Extended. Meaning bearing the e2e payload with a size of 2468 bytes.\n>   3. Rendezvous. Which nests another mix-header within the end-to-end\n> payload,\n>      with a size which is double that of the regular.\n>\n> If we like this taxonomy, then we may want to reserve the first 2 version\n> bytes\n> within the draft. A version 0 packet would encompass processing the first\n> two\n> types, while a version 1 packet denotes that this is a rendezvous packet.\n> The\n> rendezvous case needs to be distinct as it modifies the\n> processing/forwarding\n> at the final hop.\n>\n> Alternatively, we can use solely a version of 0 in the initial spec, with\n> the\n> final hop checking if the [1:34] bytes of the payload (if one is present)\n> are a\n> point on the curve.  If so, this triggers the rendezvous forwarding, with\n> the\n> mid-point node processing the packet again as normal, completing the\n> rendezvous\n> route.\n>\n\nEnumerating types of packets sounds like a good tradeoff between\nflexibility and packet size. However size and semantics are orthogonal and\nkeeping them separate might be a cleaner choice.\n\nI'd prefer having a rendezvous scheme that merges the two ends of the route\nin a seamless way, which should not be too hard to do, unless we keep the\nper-hop checkable HMACs.\n\n>\n> > We have to be careful when using timestamps in the packet as it makes\n> individual hops collatable.\n>\n> Excellent observation. Assuming we roll out a reasonably efficient\n> solution for\n> the collatable HTLC R-values across hops, naively selecting timestamps\n> would\n> present another correlation vector.\n>\n> > So my proposal would be to include a timestamp rounded up to the closest\n> hour\n> > and have a sliding window of accepted timestamps of +/- 1 hour,\n> remembering the\n> > secrets for that period and rejecting anything that is too far in the\n> future or\n> > too far in the past.\n>\n> This seems reasonable, also the size of the sliding window can easily be\n> tuned\n> in the future should we find it too large or small.\n>\n> > The more coarse the bigger the less likely an attacker is to guess which\n> > packets belong to the same route, but the more storage is required on the\n> > node's side.\n>\n> Yep, there's a clear trade off between the window size of the accepted\n> timestamps, and a node's storage overhead. We can tune this value to a\n> ballpark\n> estimate of the number of HTLCs/sec a large node with high frequency\n> bi-directional throughput may forward at peak times.\n>\n> Let's say a HFB (High-Frequency Bitcoin) node on the network at peak\n> forwards\n> 5k HTLC's per second: (5000/sec * 32 bytes) * 3600 sec = 576MB, if nodes\n> are\n> required to wait 1 hour between log prunings, and 288MB if we use a\n> 30-minute\n> interval. Even with such a high throughput value, that seems reasonable.\n>\n\nDo we need both a timestamped backlog of secrets and key-rotation? If we\nget the key rotation quick enough it's probably sufficient to simply keep\nall secrets for the current key, especially if we use bloom-filters to\nstore the seen secrets.\n\n\n> > We could just use the announced key, i.e., the one that participated in\n> the\n> > channel setup, as a root key for HD key derivation. The derivation path\n> could\n> > then be based on floor(blockheight / 144) so that we rotate keys every\n> day, and\n> > don't need any additional communication to announce new public keys.\n>\n> Great suggestion! However, I think instead of directly using the key which\n> participated in the channel setup, we'd use a new independent key as the\n> root\n> for this HD onion derivation. This new independent key would then be\n> authenticated via a signature of a schnorr multi-sign of the channel\n> multi-sig\n> key and the node's identity key (or alternatively two sigs). This\n> safeguards\n> against the compromise of one of the derived private keys leading to\n> compromise\n> of the master root HD priv key which would allow possibly stealing a node's\n> coins. Additionally, a node can switch keys more easily, avoiding a channel\n> tear down.\n>\n> However, the HD Onion Key approach can potentially cancel out the forward\n> secrecy benefits. If an attacker gains access to the root HD pubkey, along\n> with\n> any of the child derived onion keys, then they can compute the root\n> privkey.\n> This allows the attacker to derive all the child priv keys, giving them the\n> ability to decrypt all mix-headers encrypted since the HD Onion Key was\n> published.\n>\n> I think we can patch this exploit by adding some precomputation for each\n> node,\n> and introducing an intermediate onion derivation point. Assuming we rotate\n> every 144+ (1 day) blocks, then using the HD Onion PrivKey, each node\n> pre-generates 365 (or a smaller batch size) keys. Then, generates an\n> independent \"onion derivation\" key. The OD key then combined with each of\n> the\n> child onion keys, produces the final child onion key (C_i = final onion\n> key,\n> B_i = intermediate child key, A = OD):\n>     * C_i = B_i + A\n>\n> After the precomputation, the OD key (A) should be *destroyed*. If so,\n> even if\n> an attacker gains access to one of the intermediate child onion keys,\n> they're\n> unable to derive the final child onion key as the OD key has been\n> destroyed.\n> This safeguards the forward secrecy of the scheme in the face of the HD\n> root+child exploit. As before, in the case of a root/child compromise the\n> original node can simply authenticate a new HD Onion Key.\n>\n> So perhaps we can combine the two approaches, publishing a blockhash\n> (buried\n> under a \"safe\" re-org depth), along with an authenticated HD root pubkey.\n> With\n> this new scheme we're able to push key rotation out to the edges in a\n> non-interactive manner. Having the blockhash as an anchor will reduce the\n> amount of guessing required by a node to fetch the correct onion key.\n>\n\nThat's a great idea, I hadn't thought about forward secrecy. I like the\nnon-interactive nature of the scheme, since we'll be communicating enough,\neven without every node broadcasting new keys upon switch. Potentially\nthere is also a way to define your own key-rotation period with the channel\nestablishment announcement so that low-memory devices can switch at a\nhigher rate, trading memory savings for slightly higher fail rates.\n\n>\n> > I'm also trying to figure out how to enable intermediate nodes to reply\n> to a\n> > packet, e.g., if capacities are insufficient or the next node is\n> unreachable,\n> > by recycling the routing info.\n>\n> Yeah I've also attempted to tackle this issue a bit myself. The inclusion\n> of\n> onion routing certainly makes certain classes of failures harder to\n> reconcile.\n> There has been a bit of discussion of this in the past, at the time called\n> \"unrolling the onion\". In a similar vein it's also more difficult to\n> ascribe blame node's which directly cause a payment to fail.\n>\n> One of my naive ideas was to include a \"backwards\" mix-header within each\n> node's per-hop payload (though I hadn't included the per-hop payload in my\n> mental model at the time), however this would result in a quadratic blow up\n> space complexity parametrized by our max-hop limit. Essentially, we'd\n> include\n> a SURB within the mix-header for each hop in the route.\n>\n> > Maybe we could continue blinding the ephemeral key on the return path,\n> and\n> > have a mechanism to tell the node the total blinding factor along the\n> path so\n> > that it can encrypt something in the routing info for the return path?\n> That\n> > would neatly combine Hornet and Sphinx, eliminating the initial\n> roundtrip to\n> > setup forwarding segments.\n>\n> Could you elaborate on this a bit more? It seems that this alone is\n> insufficient to allow \"backwards\" replies to the source w/o revealing the\n> source's identity.\n>\n> It seems the primary question is: how can we re-use the information\n> present at\n> a hop, post-processing to reply to the sender without an additional round\n> trip?\n> If we can solve this, then we can greatly increase the robustness of onion\n> routing within the network. I think they may be worth spinning some cycles\n> on,\n> although I don't consider it blocking w.r.t the initial specification.\n>\n\nI don't think this is a high priority issue for the routing spec since we\nhave to keep the HTLC information around anyway. I was thinking along\nsending a factor along with the header that'd tell each hop that the next\ntime they see this packet it'll have the current ephemeral key blinded by\nthis factor. The hop could then compute its shared secret and write routing\ninfo into its position in the header before rotating it to the back. The\nfactor would then be divided by the blinding factor applied to the\nephemeral key before forwarding it to the next hop. On the return path the\nephemeral key is what we precomputed and we can decrypt the info we stored\nin the header before.\n\nSo far all my attempts either did not work or were leaking too much\ninformation about shared secrets or blinding factors. But then again I'm\nstuck at Crypto 101 :-)\n\nCheers,\nChristian\n\n>\n> -- Laolu\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20160808/4c15b2fb/attachment-0001.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2016-08-12T18:00:34",
                "message_text_only": "Rusty Russell <rusty at rustcorp.com.au> wrote:\n> In practice, you can do this one level up: simply agree with a rendevous\n> node that a given H-hash is to be fwded to you.  Then direct the payer\n> to the rendevous node.\n>\n> So I don't think it's worth any complexity in the routing protocol.\n\nNevermind, I forgot that the nested header (assuming its the e2e payload)\nwould be wrapped in layers on onion encryption. So the hop *after* the\nrendevous node is hidden from pre-rendevous node anyway :)\n\n> Keep it simple; let's just support regular for now.  Nodes will have to\n> broadcast what extensions they support, and this can be used for\n> extended formats later.  Including ones we *didn't* think of yet...\n\nSure. Additionally, as Christian pointed out further down in the thread,\nideally we shoud aim to seamlessly integrate rendevous routing. If possible,\nwe should keep all onion packets indistinguishable from each other.\n\n> I think we're over-designing.  How about: daily key rotation (which we\n> want anywat), remember all onions for current and previous key.\n\nYeah only daily key rotation should be sufficient. It seems that we need\neither timestamps, or key rotation, not both.\n\n> It'd be great to avoid it, but that seems complex enough to push to a\n> future spec.\n\nAgreed. When I first brought up key rotation eariler in the thread, I noted\nit\nmight be better if it were deferred to a future spec. Nevertheless, the\nI've found the resulting discussion very valuable.\n\n> id and comms keys don't have to be bitcoin keys; could be Schnorr.\n\nThe EC Schnorr construction would likely use Bitcoin's curve, so there's no\nmeaningful distinction there. We're currently constrainted to ECDSA within\nBitcoin, but can freely use EC Schnorr within the network if the space\nsavings\nare desirable (as you pointed out).\n\n\nChristian Decker <decker.christian at gmail.com> writes:\n> Sounds good, however I'm not clear on how the final recipient can provide\na\n> precompiled valid header with the HMACs that include the per-hop payloads\n> and the end-to-end payload without knowing them upfront.\n>\n> I'd prefer having a rendezvous scheme that merges the two ends of the\nroute\n> in a seamless way, which should not be too hard to do, unless we keep the\n> per-hop checkable HMACs.\n\nAhh, I see what you mean now. You're correct, it seems that we may be forced\nto drop the per-hop HMAC in order to enable seamless rendevous routing.\n\n> Do we need both a timestamped backlog of secrets and key-rotation? If we\n> get the key rotation quick enough it's probably sufficient to simply keep\n> all secrets for the current key, especially if we use bloom-filters to\n> store the seen secrets.\n\nYeah, it seems that key rotation alone is much simpler than the timestamped\nlog\nof secrets. So the key rotation alone should suffice.\n\n-- Laolu\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20160812/562993f0/attachment.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2016-08-15T12:18:35",
                "message_text_only": "On Fri, Aug 12, 2016 at 06:00:34PM +0000, Olaoluwa Osuntokun wrote:\n> Rusty Russell <rusty at rustcorp.com.au> wrote:\n> > In practice, you can do this one level up: simply agree with a rendevous\n> > node that a given H-hash is to be fwded to you.  Then direct the payer\n> > to the rendevous node.\n> >\n> > So I don't think it's worth any complexity in the routing protocol.\n> \n> Nevermind, I forgot that the nested header (assuming its the e2e payload)\n> would be wrapped in layers on onion encryption. So the hop *after* the\n> rendevous node is hidden from pre-rendevous node anyway :)\n> \n> > Keep it simple; let's just support regular for now.  Nodes will have to\n> > broadcast what extensions they support, and this can be used for\n> > extended formats later.  Including ones we *didn't* think of yet...\n> \n> Sure. Additionally, as Christian pointed out further down in the thread,\n> ideally we shoud aim to seamlessly integrate rendevous routing. If possible,\n> we should keep all onion packets indistinguishable from each other.\n> \n> > I think we're over-designing.  How about: daily key rotation (which we\n> > want anywat), remember all onions for current and previous key.\n> \n> Yeah only daily key rotation should be sufficient. It seems that we need\n> either timestamps, or key rotation, not both.\n> \n> > It'd be great to avoid it, but that seems complex enough to push to a\n> > future spec.\n> \n> Agreed. When I first brought up key rotation eariler in the thread, I noted\n> it\n> might be better if it were deferred to a future spec. Nevertheless, the\n> I've found the resulting discussion very valuable.\n> \n> > id and comms keys don't have to be bitcoin keys; could be Schnorr.\n> \n> The EC Schnorr construction would likely use Bitcoin's curve, so there's no\n> meaningful distinction there. We're currently constrainted to ECDSA within\n> Bitcoin, but can freely use EC Schnorr within the network if the space\n> savings\n> are desirable (as you pointed out).\n\nIDs only have to make sense to the receiving node, so the node may\nactually broadcast a short identifier with the channel announcement\nthat uniquely identifies the channel to it (index, establishment\ntimestamp, ...). So it could be aything besides the globally unique EC\npubkey.\n> \n> \n> Christian Decker <decker.christian at gmail.com> writes:\n> > Sounds good, however I'm not clear on how the final recipient can provide\n> a\n> > precompiled valid header with the HMACs that include the per-hop payloads\n> > and the end-to-end payload without knowing them upfront.\n> >\n> > I'd prefer having a rendezvous scheme that merges the two ends of the\n> route\n> > in a seamless way, which should not be too hard to do, unless we keep the\n> > per-hop checkable HMACs.\n> \n> Ahh, I see what you mean now. You're correct, it seems that we may be forced\n> to drop the per-hop HMAC in order to enable seamless rendevous routing.\n>\nKind of sad, I know, but we may be able to save them with an\ninteractive protocol: the endpoints of the route negotiate a\nrendezvous node, then the sender builds his half of the path, computes\nthe ephemeral key at the rendezvous point and sends it to the final\nrecipient which finalizes the route construction. But maybe I'm\noverthinking it, at this point the payload could just be sent\nout-of-band :-)\n\n> > Do we need both a timestamped backlog of secrets and key-rotation? If we\n> > get the key rotation quick enough it's probably sufficient to simply keep\n> > all secrets for the current key, especially if we use bloom-filters to\n> > store the seen secrets.\n> \n> Yeah, it seems that key rotation alone is much simpler than the timestamped\n> log\n> of secrets. So the key rotation alone should suffice.\n> \n> -- Laolu\n\nPerfect, that seems like a clean and simple solution to the shared\nsecret storage problem. Noting this does for a future spec :-)\n\nCheers,\nChristian"
            }
        ],
        "thread_summary": {
            "title": "Onion Routing Spec",
            "categories": [
                "Lightning-dev",
                "BOLT Draft"
            ],
            "authors": [
                "Rusty Russell",
                "Joseph Poon",
                "Olaoluwa Osuntokun",
                "Christian Decker"
            ],
            "messages_count": 22,
            "total_messages_chars_count": 92540
        }
    },
    {
        "title": "[Lightning-dev] Blinded channel observation",
        "thread_messages": [
            {
                "author": "Tadge Dryja",
                "date": "2016-08-08T16:17:04",
                "message_text_only": "Blinded outsourcing of channel monitoring\n\nThe big risk in LN is that an attacking node can close a channel at an old,\ninvalidated state beneficial to the attacker, and the Bitcoin network will\naccept these transactions.  The node being attacked can defend by grabbing\nboth outputs, which will probably make such attacks quite rare.  But, nodes\nhave to be online to detect this attack and defend against it, or risk\nlosing funds.\n\nOutsourcing this vigilance can further defend against attacks by allowing\nmultiple parties to mount the defense.  My initial thinking was that\nminimizing the amount of data stored on the observer would be best, but I\ndon't think there's a way to make it O(log(n)) with the way Bitcoin works\ntoday.  (Sighash_noinput, some variants of MAST, etc could make that work\nand we should keep that in mind, but that may be longer term)\n\nSo if it's going to be O(n), another way to make outsourcing better is to\nanonimize / blind it.  Of course if the observer actually sees an invalid\nclose and sends out a transaction moving the attacker's time-locked funds,\nthey will learn about how big the channel was, the txid, etc.  But 99.999%\nof the time, channels won't get closed at an invalid state, so the observer\ndoesn't actually do anything.  Most of the time channels will be closed\ncooperatively, but some times they'll be closed unilaterally because one\nnode is offline / unresponsive.  We should keep privacy in either of those\ncases.\n\nStates with in-flight HTLCs are another issue... if you want to keep the\ndata storage down, you can just not include them, and make a policy that\nthe sum of all the HTLCs should be less than either non-HTLC balance in the\nchannel.  That way the attacker still loses money if they try to attack.\nThey potentially might not lose all of it though.  You could make it\nvariable size and include HTLCs as well but that increases the data rate\nsignificantly and probably hurts anonymity in various ways.\n\nWithout HTLCs, the script I have now specifies 2 pubkeys and requires 1 of\n2 signatures- either from the timeout key or the revocable key.  The goal\nis that the observer is monitoring a channel, but even an uncooperative\nclose of that channel is undetectable, even after the output is spent and\nthe pubkeys are revealed in the p2wsh preimage.  To meet this goal the two\npubkeys have to change completely with every new state, and also the\nnon-timelocked pubkey Hash output also needs to change each time.\n\nA simple way to do it would be to have the two sides of the channel make up\nnew timeout and revocable pubkeys for each new state, but that's a lot of\ndata for them, and also an extra 66 bytes per state for the observer.\nInstead, we can use the elkrem tree not just to revoke states, but also to\nobscure the keys in each state.\n\nTo do this, the messages actually didn't change too much, but key\nderivation changes a bit.  At channel setup, A and B share \"base points\",\nwhich are their public keys which will be used in the commitment script.\nHowever these public keys are never used directly so I call them \"points\"\non the curve rather than pubkeys themselves. (Not sure what the best\nconvention is for that; I figure, call it a pubkey if you end up putting it\non the blockchain and making a signature with it; call it a point if it's\nused otherwise, including to build a pubkey.)\n\nWhen making a new state, instead of A sending B a revokable pubkey, A\ninstead sends two \"elkrem points\" specific to the state number.  These\npoints are both deterministically derived from the same elkrem hash.  For\nexample, if we're building state 9, A goes down the branches of their\nelkrem tree to node #9, and gets a 32 byte hash.  A appends ascii \"t\" for\nthe timeout point, and ascii \"r\" for the revocable point.  A then uses\nthose two hashes as private scalars and multiplies by G, creating points on\nthe curve, or public keys.  A sends those points to B as the \"elkrem\npoints\".  B then adds elkrem point R to A's base point and elkrem point T\nto B's own base point.  The former results in A's revokable pubkey, the\nlatter in B's timeout pubkey for the script.\n\nA sends elkrem points:\nelkrem point R + B base point: Revokable Pubkey\nelkrem point T + A base point: Timeout Pubkey\n\nB's elkrem point R + B's refund point: Refund Pubkey hash\nwith these points, B signs & sends, A stores.\n\n\nB sends elkrem points:\nelkrem point R + A base point: Revokable Pubkey\nelkrem point T + B base point: Timeout Pubkey\n\nA's elkrem point R + A's refund point: Refund Pubkey hash\nwith these points, A signs & sends, B stores.\n\nBefore the state is revoked, the sender knows the scalar to generate the\nelkrem points, so there's nothing hidden about the timeout and refund\npubkeys.  Those just obscure the channel.  The revokation pubkey is the\nonly one where nobody individually knows the private key for the pubkey\nuntil the hashes are revealed.\n\nOnce the hashes are revealed and put into the tree, either party can\nregenerate all the pubkeys at any previous state quickly.  That's not\nactually useful for the two nodes (why would you want to generate the\nscript of an old state?) but it's very useful for the outsourcing node.\nThey won't be able to recognize state n, but state 0 ... n-1 are\nrecognizable and can be stored efficiently.\n\nThe reason two different points are needed is that if you add the same\nelkrem point to the two base points, the observer can subtract the base\npoints from the public key seen in the witness script.\n\nA base point - timeout pubkey = elkrem\nB base point - revokable pubkey = elkrem\n\nif (A base point - timeout pubkey) == (B base point - revokable pubkey)\n\nthen that point must be the elkrem point, which would indicate that those\nbase points were used, allowing the observer to identify the channel.\n\nThose are (I think) the only changes needed in the LN messages / protocol\nitself.  The design of the observer is not quite done but I have the basic\nidea.  It's kindof interesting, scalability-wise, because it will be\nlooking for a set of txids that's potentially much larger than the whole\nblockchain!  Basically when a node wants to give the observer an old state\nto watch, it just gives a signature, a txid, and an elkrem hash.  Only the\nsignature and txid need to be stored.  Also you can drop part of the txid\nat the risk of collisions, but the cost of collisions is pretty small\n(mainly a few EC operations to build the pubkeys for the script), so 16 or\neven 8 byte txids could work.\n\nIf the observer sees a transaction in a block which matches a txid they're\nlooking for, they do some sanity checks (e.g. does this tx have a p2wsh\noutput?) then re-create the script, by using the elkrem tree they have,\ngetting the state number from the tx itself (6 bytes, encoded in nLockTime\n/ nSequence), generating the elkrem points, adding them to their stored\nbase points, hashing the script and seeing if the script matches.  If it\ndoes, , add the fixed output address, and figure out the output amount\nbased on the input amount.  Then add the signature, which should match up\nand result in a valid transaction.\n\nThis is preliminary and I wanted to send this mail about a week ago but I\nkept finding ways the observer could figure out which channel it was\nobserving, and then devising ways to stop that.\n\nThe nice part about this way of doing it is that you can share the channel\nstate data with people you don't know or trust.  Maybe they won't actually\nwatch over the channel for you, but maybe they might.  If someone wanted to\nbe really nice, they could try to get *every* channel and observe it.  All\nyou need is one or two nice people like that, and invalid closes become\nnigh impossible.\n\nI'll post a bit more once it's more finalized.  If people see any problems\nwith this method please let me know!\n\n-Tadge\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20160808/70c806f2/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2016-08-09T05:43:57",
                "message_text_only": "Tadge Dryja <tadge at lightning.network> writes:\n> Blinded outsourcing of channel monitoring\n\nBut if we don't steal HTLCs, the lack of channel utilization is a pain:\n\n> States with in-flight HTLCs are another issue... if you want to keep the\n> data storage down, you can just not include them, and make a policy that\n> the sum of all the HTLCs should be less than either non-HTLC balance in the\n> channel.  That way the attacker still loses money if they try to attack.\n> They potentially might not lose all of it though.  You could make it\n> variable size and include HTLCs as well but that increases the data rate\n> significantly and probably hurts anonymity in various ways.\n\nBut now they must *always* keep their balance greater than all the HTLCs\nthey ever previously received at once, otherwise it's worth them\ncheating.\n\nI think we really do want to capture those HTLCs, but I don't think it's\nas bad as you think: an HTLC can be described in ~40 bytes.  We need to\ndo some magic to pad it out to hide the number of HTLCs, of course.\n\nSo, my method was less ambitious.  I'll describe it here:\n\nWe send the observer the \"steal\" tx every update (not really: we only\nneed to send the to-us/to-them amounts, pubkeys, HTLCs info and sig).\nThis gets encrypted+HMAC with the txid of the commit tx (or, if that's\ntoo guessable, the SHA256() of our signature on the commit tx).\n\nI had assumed we'd tell the observer our channel funding txid: when it\nsees that spent, it tries to use those signatures to decrypt all the txs\nwe sent.  If one succeeds, it spends it.\n\nIf we want to obscure our funding tx, we can simply use a txid qualifier\nthe same way you did (and maybe use the sha256(txid) as the encryption\nkey to avoid weakening that).\n\nWhatever we do, we might want to offer our own peers to do that watching\nfor them (for a fee?).\n\nCheers!\nRusty."
            },
            {
                "author": "Joseph Poon",
                "date": "2016-08-09T19:28:14",
                "message_text_only": "Hi Rusty,\n\nOn Tue, Aug 09, 2016 at 03:13:57PM +0930, Rusty Russell wrote:\n> We send the observer the \"steal\" tx every update (not really: we only\n> need to send the to-us/to-them amounts, pubkeys, HTLCs info and sig).\n> This gets encrypted+HMAC with the txid of the commit tx (or, if that's\n> too guessable, the SHA256() of our signature on the commit tx).\n>\n> [snip]\n>\n> If we want to obscure our funding tx, we can simply use a txid qualifier\n> the same way you did (and maybe use the sha256(txid) as the encryption\n> key to avoid weakening that).\n\nI think it may be necessary to identify when the transaction occurs as\nan index for outsourcing services, so the key can't be dervied directly\nfrom the txid with a single HMAC/sha256. It's possible there are\nmillions of transactions to compare, and an index based on txid is\nnecessary. The two options I can see are:\n\n1. Take the txid and use two salts (globally for all users).\nHMAC(txid+salt1) and HMAC(txid+salt2). Give the result of\nHMAC(txid+salt1) and the encrypted blob to the outsourcer. The first is\nused for identifying the txid, the second salt is for the secret key and\ncan only be derived when the actual txid is seen. When a transaction is\nreceived, do HMAC(txid+salt1) and see if there's any matches to decrypt\n& broadcast.\n\n2. HMAC the transaction itself (not txid) as the secret key (or anything\npart of the transaction, as long as it isn't SHA256(tx) for obvious\nreasons). I like something along these lines better than option #1.\nWhatever computational cost there is will be extremely low, as the\noperations are constrained by block size.\n\nI do like encrypting the outsourcing blobs best as an approach so far,\nas it gives maximum flexibility in terms of implementation (individual\nclient behavior can be upgraded in the future without modifying the\noutsourcing code/nodes in most instances).\n\n-- \nJoseph Poon"
            },
            {
                "author": "Rusty Russell",
                "date": "2016-08-09T21:06:11",
                "message_text_only": "Joseph Poon <joseph at lightning.network> writes:\n> Hi Rusty,\n>\n> On Tue, Aug 09, 2016 at 03:13:57PM +0930, Rusty Russell wrote:\n>> We send the observer the \"steal\" tx every update (not really: we only\n>> need to send the to-us/to-them amounts, pubkeys, HTLCs info and sig).\n>> This gets encrypted+HMAC with the txid of the commit tx (or, if that's\n>> too guessable, the SHA256() of our signature on the commit tx).\n>>\n>> [snip]\n>>\n>> If we want to obscure our funding tx, we can simply use a txid qualifier\n>> the same way you did (and maybe use the sha256(txid) as the encryption\n>> key to avoid weakening that).\n>\n> I think it may be necessary to identify when the transaction occurs as\n> an index for outsourcing services, so the key can't be dervied directly\n> from the txid with a single HMAC/sha256. It's possible there are\n> millions of transactions to compare, and an index based on txid is\n> necessary. The two options I can see are:\n\nThis is fun!\n\nYes, I think we agree some \"filter hint\" is needed to avoid a crazy\namount of outsourcing work (eg. first 8/16 bytes of txid).  I don't\nthink an HMAC check per registered commitment is quite fast enough.\n\nBut there's a problem with most naive filters, if you can guess\ncommitment tx N-1 from commitment tx N.  If the outsourcing service sees\nan old commit they can guess at previous commitment txs using that.\nProbably unroll the whole channel history if they can guess enough\nHTLCs.\n\n> 2. HMAC the transaction itself (not txid) as the secret key (or anything\n> part of the transaction, as long as it isn't SHA256(tx) for obvious\n> reasons). I like something along these lines better than option #1.\n> Whatever computational cost there is will be extremely low, as the\n> operations are constrained by block size.\n\nIf we include the witness in that HMAC we risk reintroducing\nmalleability.  If we don't, we risk txs being predictable.\n\nI can think of a few fixes: insert some randomness in the tx (OP_RETURN?\nDifferent addresses each time?), or try to extract the input signature\nfrom the witness, which is unguessable, as our filter?\n\nWhat's simplest?\nRusty."
            },
            {
                "author": "Joseph Poon",
                "date": "2016-08-09T22:29:38",
                "message_text_only": "On Wed, Aug 10, 2016 at 06:36:11AM +0930, Rusty Russell wrote:\n> Yes, I think we agree some \"filter hint\" is needed to avoid a crazy\n> amount of outsourcing work (eg. first 8/16 bytes of txid).  I don't\n> think an HMAC check per registered commitment is quite fast enough.\n\nFor #1, the HMAC would be pre-computed by the outsourcer. So the flow\nlooks like:\n\n1. The outsourcer takes the txid, HMAC(txid+salt1) and encrypts the blob\n2. The outsourcer gives the 32-byte hmac and blob to the watcher\n3. The watcher adds the 32-byte hmac and the blob to a key-value store\n\t(the watcher can optionally truncate or whatever)\n4. When the watcher receives a new block, they HMAC(txid+salt1) all\ntransactions and compare against the key-value store\n\nThis method does not require significant computation upon receiving a\nnew block and checking against the datastore. I forgot to note, that the\nsalt2 is sort of unnecessary, it can just be the pure txid as the key\nbut was there for superstition and aid in understanding what's going on.\nI was just making a point that there needs to be some kind of \"hint\"/key\nto look for.\n\n> > 2. HMAC the transaction itself (not txid) as the secret key (or anything\n> > part of the transaction, as long as it isn't SHA256(tx) for obvious\n> > reasons). I like something along these lines better than option #1.\n> > Whatever computational cost there is will be extremely low, as the\n> > operations are constrained by block size.\n> \n> If we include the witness in that HMAC we risk reintroducing\n> malleability.  If we don't, we risk txs being predictable.\n\nI was referring to the non-witness txid when making that comment, but\nthere should be sufficient entropy from the revocation hash (whose P2WSH\nis part of the Commitment outputs).\n\n> I can think of a few fixes: insert some randomness in the tx (OP_RETURN?\n> Different addresses each time?), or try to extract the input signature\n> from the witness, which is unguessable, as our filter?\n\nYeah! I like this idea to use one's own input sig as the key for the\nencrypted blob too. If Alice is the one outsourcing the Commitment which\nBob can broadcast, Bob can only broadcast it using the sig Alice gave\nBob as it's spending from a 2-of-2. If Alice is outsourcing Bob's\nCommitment broadcasts, a hash of her input signature is a solid way to\nderive a key as well without malleability concerns.\n\nI also like that it \"encourages\" more nodes to download witness data;\nignoring witnesses is a concern of mine which this helps with :^)\n\n-- \nJoseph Poon"
            },
            {
                "author": "Tadge Dryja",
                "date": "2016-08-10T01:34:43",
                "message_text_only": ">\n> 4. When the watcher receives a new block, they HMAC(txid+salt1) all\n> transactions and compare against the key-value store\n>\n> This method does not require significant computation upon receiving a\n> new block and checking against the datastore. I forgot to note, that the\n>\n\n\nIf you're only watching 1 channel, this is OK as you're just doing one HMAC\noperation per incoming tx.  This works when the number of states per\nchannel is high but the number of channels watched is low.\nIf you're watching 1M channels with 1K states each, this requires 1M HMAC\noperations for every incoming tx.\n\nBy giving the whole (or a truncated part) of the txid, the observing node\ncan put all txids being watched for into the same tree and have seek access\ntime independent of the number of separate channels being watched.  Whether\nit's 1 channel with 1 billion states, or 100M channels with 10 states each,\nyou have the same cpu and I/O cost per incoming tx.\n\nI'm not clear on the advantages of sending encrypted state information to\nthe observer.  Most information the observer needs to construct a punishing\ntransaction is available in the observed commitment tx; the only per-state\ninformation which needs to be stored is the signature (and in the case of\nHTLCs, the preimage).  Reconstructing messages from hashes and / or\nsignatures isn't possible as long as there's sufficient unknown data in the\npreimage or message.  This is achievable by changing the pubkeys used in\nthe commit tx script and pubkey hash each state, without needing additional\ndata in an OP_RETURN.\n\n-Tadge\n\nOn Tue, Aug 9, 2016 at 6:29 PM, Joseph Poon <joseph at lightning.network>\nwrote:\n\n> On Wed, Aug 10, 2016 at 06:36:11AM +0930, Rusty Russell wrote:\n> > Yes, I think we agree some \"filter hint\" is needed to avoid a crazy\n> > amount of outsourcing work (eg. first 8/16 bytes of txid).  I don't\n> > think an HMAC check per registered commitment is quite fast enough.\n>\n> For #1, the HMAC would be pre-computed by the outsourcer. So the flow\n> looks like:\n>\n> 1. The outsourcer takes the txid, HMAC(txid+salt1) and encrypts the blob\n> 2. The outsourcer gives the 32-byte hmac and blob to the watcher\n> 3. The watcher adds the 32-byte hmac and the blob to a key-value store\n>         (the watcher can optionally truncate or whatever)\n> 4. When the watcher receives a new block, they HMAC(txid+salt1) all\n> transactions and compare against the key-value store\n>\n> This method does not require significant computation upon receiving a\n> new block and checking against the datastore. I forgot to note, that the\n> salt2 is sort of unnecessary, it can just be the pure txid as the key\n> but was there for superstition and aid in understanding what's going on.\n> I was just making a point that there needs to be some kind of \"hint\"/key\n> to look for.\n>\n> > > 2. HMAC the transaction itself (not txid) as the secret key (or\n> anything\n> > > part of the transaction, as long as it isn't SHA256(tx) for obvious\n> > > reasons). I like something along these lines better than option #1.\n> > > Whatever computational cost there is will be extremely low, as the\n> > > operations are constrained by block size.\n> >\n> > If we include the witness in that HMAC we risk reintroducing\n> > malleability.  If we don't, we risk txs being predictable.\n>\n> I was referring to the non-witness txid when making that comment, but\n> there should be sufficient entropy from the revocation hash (whose P2WSH\n> is part of the Commitment outputs).\n>\n> > I can think of a few fixes: insert some randomness in the tx (OP_RETURN?\n> > Different addresses each time?), or try to extract the input signature\n> > from the witness, which is unguessable, as our filter?\n>\n> Yeah! I like this idea to use one's own input sig as the key for the\n> encrypted blob too. If Alice is the one outsourcing the Commitment which\n> Bob can broadcast, Bob can only broadcast it using the sig Alice gave\n> Bob as it's spending from a 2-of-2. If Alice is outsourcing Bob's\n> Commitment broadcasts, a hash of her input signature is a solid way to\n> derive a key as well without malleability concerns.\n>\n> I also like that it \"encourages\" more nodes to download witness data;\n> ignoring witnesses is a concern of mine which this helps with :^)\n>\n> --\n> Joseph Poon\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20160809/0485a6e7/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2016-08-10T10:39:50",
                "message_text_only": "Tadge Dryja <tadge at lightning.network> writes:\n>>\n>> 4. When the watcher receives a new block, they HMAC(txid+salt1) all\n>> transactions and compare against the key-value store\n>>\n>> This method does not require significant computation upon receiving a\n>> new block and checking against the datastore. I forgot to note, that the\n>\n> If you're only watching 1 channel, this is OK as you're just doing one HMAC\n> operation per incoming tx.  This works when the number of states per\n> channel is high but the number of channels watched is low.\n> If you're watching 1M channels with 1K states each, this requires 1M HMAC\n> operations for every incoming tx.\n>\n> By giving the whole (or a truncated part) of the txid, the observing node\n> can put all txids being watched for into the same tree and have seek access\n> time independent of the number of separate channels being watched.  Whether\n> it's 1 channel with 1 billion states, or 100M channels with 10 states each,\n> you have the same cpu and I/O cost per incoming tx.\n\nYes, I think I agree.\n\n> I'm not clear on the advantages of sending encrypted state information to\n> the observer.  Most information the observer needs to construct a punishing\n> transaction is available in the observed commitment tx; the only per-state\n> information which needs to be stored is the signature (and in the case of\n> HTLCs, the preimage).\n\nYes.  To be precise, watcher needs preimage, pubkeys, a signature,\nOP_CSV delay amounts for each side, and HTLCs.\n\nIf the pubkeys and OP_CSV delay amounts are fixed, you need a signature\nand preimage (required to steal the to-them output as well as htlcs)\nevery new commit tx (ie. every new watch request).  And you'll add a new\nHTLC information on average every second commit tx.\n\nBut I think you're better attaching every relevant HTLC to every watch\nrequest; otherwise, you're exposing all previous HTLCs.  I recalculated:\nthey're only 24 bytes each (ripemd hash + expiry).\n\nIf we really want to optimize that,maybe we can come up with a cleverer\nscheme, where you optionally include the key to the previous watch\nrequest with the encrypted information, allowing the sender to control\nthe privacy/data tradeoff?  Then the watcher would gather HTLCs from\nthose previous requests as well, and see which ones are required?\n{Insert bikeshed here}.  {No, here}.\n\n> Reconstructing messages from hashes and / or\n> signatures isn't possible as long as there's sufficient unknown data in the\n> preimage or message.  This is achievable by changing the pubkeys used in\n> the commit tx script and pubkey hash each state, without needing additional\n> data in an OP_RETURN.\n\nFWIW, I take back the idea about using the signature as a key:\n1) While the crypto people tell me it's probably OK, they can't prove it.\n2) If we go to Schnorr, I think it falls apart as both sigs are combined.\n\nOTOH, changing pubkeys every step isn't required if we go for Laolu's \nkey-revocation scheme instead of shachain/elkrem.  I think? (Roasbeef,\nthis is your cue to describe it in detail!)\n\nCheers,\nRusty,"
            },
            {
                "author": "Tadge Dryja",
                "date": "2016-08-10T14:52:13",
                "message_text_only": "The method of using a revocation key is compatible with shachain/elkrem so\nhas log(n) storage; I'll describe what I developed which omits hashing in\nthe commit script and uses only signature verification.  If Laolu has made\na different key revocation scheme I'm not aware, but please do post if so.\n\nThe script is:\n\nDUP\n[Revocable Pubkey]\nCHECKSIG\nNOTIF\n[Timeout Pubkey]\nCHECKSIGVERIFY\n[timeout period]\nCHECKSEQUENCEVERIFY\nENDIF\n\nThis is a little ugly as the less likely checksig (the revocable one)\nhappens first.  Saves space in the script though and we don't pay any more\nfor a checksig than any other 1 byte opcode.\n\nAs an if statement, it ends up being:\nif (revocable sig) || (timeout sig && CSV > timeout)\n\nTo build the revocable pubkey, Alice takes their elkrem sender hash from\nstate n, which we'll call EHn.  Alice multiples EHn * G, getting a point\nEPn. (Elkrem point n)  Alice sends EPn to Bob, who adds their commitment\npubkey (BP, which is never seen raw) to EPn.  The result, (RPub n = BP +\nEPn), is the revocable pubkey for state n.  At state n+1, Alice sends Bob\nEHn.  Bob can then compute the private key for Rpub n, as it's just their\ncommitment private key plus EHn, modulo the order of the curve.\n\nA similar procedure is used for the timeout key; Alice adds a point to\ntheir own timeout key, which seems kindof pointless because they know both\nscalars.  It obscures the commitment script by making both pubkeys\ndifferent each state, as they're all generated from the hash tree.  Bob\nonly needs to keep track of the most recent \"elkrem points\" and the hash\ntree itself.\n\nHope this is clear and sorry if I'm describing something completely\ndifferent than what you're asking about!\n\n-Tadge\n\u200b\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20160810/54c17813/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2016-08-11T01:55:36",
                "message_text_only": "Tadge Dryja <tadge at lightning.network> writes:\n> The method of using a revocation key is compatible with shachain/elkrem so\n> has log(n) storage; I'll describe what I developed which omits hashing in\n> the commit script and uses only signature verification.  If Laolu has made\n> a different key revocation scheme I'm not aware, but please do post if so.\n\nOh, I blamed Laolu because he told me about it; sorry for misattribution.\n\n> The script is:\n>\n> DUP\n> [Revocable Pubkey]\n> CHECKSIG\n> NOTIF\n> [Timeout Pubkey]\n> CHECKSIGVERIFY\n> [timeout period]\n> CHECKSEQUENCEVERIFY\n> ENDIF\n\nOK, so far so good.\n\n> To build the revocable pubkey, Alice takes their elkrem sender hash from\n> state n, which we'll call EHn.  Alice multiples EHn * G, getting a point\n> EPn. (Elkrem point n)  Alice sends EPn to Bob, who adds their commitment\n> pubkey (BP, which is never seen raw) to EPn.\n\n\"never seen raw on-chain\" I assume, since Bob will send it to Alice in\nsetup?\n\n> The result, (RPub n = BP + EPn), is the revocable pubkey for state n.\n> At state n+1, Alice sends Bob EHn.  Bob can then compute the private\n> key for Rpub n, as it's just their commitment private key plus EHn,\n> modulo the order of the curve.\n\nSo, AFACIT this scheme gives us a slightly smaller script and makes\nprevious commit transactions underivable.\n\nThe property I was *hoping* for was the ability for Alice (and Bob) to\nindependently predict each others' future revocation hashes/pubkeys.\nThat would neatly allow an arbitrary number of commitment transactions\nin flight each way at once.  Naively, seems like that should be\npossible.\n\n> A similar procedure is used for the timeout key; Alice adds a point to\n> their own timeout key, which seems kindof pointless because they know both\n> scalars.  It obscures the commitment script by making both pubkeys\n> different each state, as they're all generated from the hash tree.  Bob\n> only needs to keep track of the most recent \"elkrem points\" and the hash\n> tree itself.\n\nI think changing the timeout key is harmless, but gratuitous; changing\nthe revocation key is sufficient for each commitment script unguessably\ndifferent from the last one, no?\n\n> Hope this is clear and sorry if I'm describing something completely\n> different than what you're asking about!\n\nIt's all good; this is a big space and sometimes I don't even know what\nI don't know...\n\nThanks!\nRusty."
            },
            {
                "author": "Tadge Dryja",
                "date": "2016-08-11T03:12:59",
                "message_text_only": "Hi, this is very interesting idea that I hadn't thought of!\n\n\n> Oh, I blamed Laolu because he told me about it; sorry for misattribution.\n\nHeh not at all, I don't know who's coming up with what either.\n\n\n> > To build the revocable pubkey, Alice takes their elkrem sender hash from\n> > state n, which we'll call EHn.  Alice multiples EHn * G, getting a point\n> > EPn. (Elkrem point n)  Alice sends EPn to Bob, who adds their commitment\n> > pubkey (BP, which is never seen raw) to EPn.\n>\n> \"never seen raw on-chain\" I assume, since Bob will send it to Alice in\n> setup?\n\n\nRight; Alice and Bob share these base points when they're funding the\nchannel but if never shows up on-chain.\n\n\n> So, AFACIT this scheme gives us a slightly smaller script and makes\n> previous commit transactions underivable.\n>\n> The property I was *hoping* for was the ability for Alice (and Bob) to\n> independently predict each others' future revocation hashes/pubkeys.\n> That would neatly allow an arbitrary number of commitment transactions\n> in flight each way at once.  Naively, seems like that should be\n> possible.\n>\n\nThat... is a very cool idea which I had not considered.  Sketching it out,\nit seems like you'd need some kind of bi-directional trap door function.\nRSA comes to mind where repeated exponentiation to the e can be reversed by\nexponentiation to the d.  But in RSA \"private keys\" (d) and \"public keys\"\n(e) are the same thing (scalars) but here they're different.\n\nMy gut feeling is that with secp256k1 it's not possible though.  Setting up\na homomorphic relationship such that Pubkey0 lets you compute Pubkey1 seems\nlike it would also allow you to compute from Scalar0 to Scalar1 -- just\nperform whatever additions and multiplications you did to the point on the\nscalar.  Definitely something to think about though, because it'd be very\ncool.  (In practice it doesn't really affect storage, and reduces data on\nthe wire a somewhat, so not a huge difference scalability-wise, but still\nwould be really cool if possible)\n\n\n> I think changing the timeout key is harmless, but gratuitous; changing\n> the revocation key is sufficient for each commitment script unguessably\n> different from the last one, no?\n\n\nYeah, I didn't alter the timeout key initially, but I put it in because it\nhelps keep the channel private from the observer in the case where you just\nsend them signatures.  If the timeout key is unchanging, they'd see it when\nthe channel close un-cooperatively and recognize the channel they were\nmonitoring.\n\nFor privacy, the points you add to the timeout key and revocable key also\nneed to be *different* points.  If you add the same point to both, an\nobserver who knows the base points can try subtracting the timeout and\nrevocable base points from the observed pubkeys.  If the result for both is\nthe same, they'll know they've found the channel they're looking for.\n\nThanks for the new idea to stew over on the train!\n-Tadge\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20160810/5cf6e730/attachment.html>"
            },
            {
                "author": "Joseph Poon",
                "date": "2016-08-11T07:49:26",
                "message_text_only": "On Thu, Aug 11, 2016 at 11:25:36AM +0930, Rusty Russell wrote:\n> Tadge Dryja <tadge at lightning.network> writes:\n> > The method of using a revocation key is compatible with shachain/elkrem so\n> > has log(n) storage; I'll describe what I developed which omits hashing in\n> > the commit script and uses only signature verification.  If Laolu has made\n> > a different key revocation scheme I'm not aware, but please do post if so.\n> \n> Oh, I blamed Laolu because he told me about it; sorry for misattribution.\n\nI came up with it a long time ago, and worked out the\ndetails/optimizations /w Laolu more recently (I think he told you that\nnight when everything was finalized). I mentioned the general\nconstruction to you a LONG time ago too when you were in the Bay Area,\nbut I probably didn't explain it properly (I was comparing with\nVanitygen, if that helps). I think Tadge was the first to implement it\nthough, not sure.\n\n> The property I was *hoping* for was the ability for Alice (and Bob) to\n> independently predict each others' future revocation hashes/pubkeys.\n> That would neatly allow an arbitrary number of commitment transactions\n> in flight each way at once.  Naively, seems like that should be\n> possible.\n\nI'm not inclined to think an increase in complexity is helpful (if this\nis necessary), but there are probably some things you can do if you're\nlooking down these paths.\n\nIt's possible to get the same *bandwidth* optimization you want, except\nopposite. The idea with \"predicting the future revocation\nhashes/pubkeys\" is you only need to send revocations. Instead, it's\npossible to only send revocation hashes/pubkeys and not send\nrevocations. In other words, instead of predicting each others' future\nrevocation hashes/pubkeys, it's possible to revoke as *part of* giving\nthe next revocation hash/pubkey.\n\nYou can arrange something similar to a hashchain (shachain/elkrem is an\noptimization of this, ignore optimizations for a second). We treat\nprivkey->pubkey as an elaborate hash function. I think if you\npre-compute it so that privkey -> pubkey, and then take the pubkey\noutput to create a privkey, you have a reversed list of items (let's say\nyou do this 100,000 times). The final privkey -> pubkey computation is\nthe first \"revocation keypair\" used. The pubkey->privkey step can do\nanything you want, including hash functions if it makes you feel better\n(this is the point where one can optimize).\n\n(Note: I really mean EC point here, but it's simpler to understand it as\na pubkey)\n\nIf you want multiple in-flight, just have multiple parallel chains\n(minor increase in permanent storage of counterparty's revocations). I\ndon't see any need for more than a handful in-flight. Note that this\nexplicitly breaks doing multiple in-flight on a single chain, since\ndisclosure of a pubkey is disclosure of all prior revocation states.\n\nEssentially, what happens is when you disclose a pubkey, you are\nproviding the next pubkey AND revoking the prior one at the same time.\nThis construction is also possible using hashtree like structures if\nyou're using revocation hashes instead of revocation pubkeys.\n\nFor the pubkey revocation, a nested chain of privkey->pubkeys are needed\ninstead of hashes since you can't have a usable pubkey point without\nalso getting the corresponding private key.\n\nNot sure how useful this is, though. Seems a lot of complexity for some\nsmall bandwidth savings, not really that interested in doing all that,\nbut it's the closest I can get to what you want. This is off the top of\nmy head/memory, I didn't write any notes on this, so parts of this (or\nentirety) might be wrong...\n\n-- \nJoseph Poon"
            },
            {
                "author": "Rusty Russell",
                "date": "2016-08-12T03:24:52",
                "message_text_only": "Joseph Poon <joseph at lightning.network> writes:\n> On Thu, Aug 11, 2016 at 11:25:36AM +0930, Rusty Russell wrote:\n>> Tadge Dryja <tadge at lightning.network> writes:\n>> > The method of using a revocation key is compatible with shachain/elkrem so\n>> > has log(n) storage; I'll describe what I developed which omits hashing in\n>> > the commit script and uses only signature verification.  If Laolu has made\n>> > a different key revocation scheme I'm not aware, but please do post if so.\n>> \n>> Oh, I blamed Laolu because he told me about it; sorry for misattribution.\n>\n> I came up with it a long time ago, and worked out the\n> details/optimizations /w Laolu more recently (I think he told you that\n> night when everything was finalized). I mentioned the general\n> construction to you a LONG time ago too when you were in the Bay Area,\n> but I probably didn't explain it properly (I was comparing with\n> Vanitygen, if that helps). I think Tadge was the first to implement it\n> though, not sure.\n>\n>> The property I was *hoping* for was the ability for Alice (and Bob) to\n>> independently predict each others' future revocation hashes/pubkeys.\n>> That would neatly allow an arbitrary number of commitment transactions\n>> in flight each way at once.  Naively, seems like that should be\n>> possible.\n>\n> I'm not inclined to think an increase in complexity is helpful (if this\n> is necessary), but there are probably some things you can do if you're\n> looking down these paths.\n>\n> It's possible to get the same *bandwidth* optimization you want, except\n> opposite. The idea with \"predicting the future revocation\n> hashes/pubkeys\" is you only need to send revocations. Instead, it's\n> possible to only send revocation hashes/pubkeys and not send\n> revocations. In other words, instead of predicting each others' future\n> revocation hashes/pubkeys, it's possible to revoke as *part of* giving\n> the next revocation hash/pubkey.\n\nYeah, I do that already.  We give the N+1th hash when we send the\nrevocation for N:\n\n        // Complete the update.\n        message update_revocation {\n          // Hash preimage which revokes old commitment tx.\n          required sha256_hash revocation_preimage = 1;\n          // Revocation hash for my next commit transaction\n          required sha256_hash next_revocation_hash = 2;\n        }\n\nThe trivial extension is to make in the N+Mth revocation hash, and\ninstead of sending the first two revocation hashes on establishment,\nsend the first M+1.\n\nI just wanted to make sure there wasn't some clever alternative I was\nmissing.  Greg Maxwell and Pieter Wuille tell me there isn't an obvious\none.\n\n> Not sure how useful this is, though. Seems a lot of complexity for some\n> small bandwidth savings, not really that interested in doing all that,\n> but it's the closest I can get to what you want. This is off the top of\n> my head/memory, I didn't write any notes on this, so parts of this (or\n> entirety) might be wrong...\n\nYeah, agreed.  Hash trees are nice and simple, so unless we get a\nsignficiant win, let's stick with that?\n\nCheers,\nRusty."
            },
            {
                "author": "Joseph Poon",
                "date": "2016-08-12T21:28:53",
                "message_text_only": "On Fri, Aug 12, 2016 at 12:54:52PM +0930, Rusty Russell wrote:\n> Yeah, I do that already.  We give the N+1th hash when we send the\n> revocation for N:\n> \n>         // Complete the update.\n>         message update_revocation {\n>           // Hash preimage which revokes old commitment tx.\n>           required sha256_hash revocation_preimage = 1;\n>           // Revocation hash for my next commit transaction\n>           required sha256_hash next_revocation_hash = 2;\n>         }\n\nYeah, the way I described would (optimally over-the-wire) be a tree of\nchained preimages *AND* hashes as a part of the tree itself (so the\nchain looks like preimage->hash->preimage->hash instead of\npreimage->preimage). That way if you're willing to forgo the ability to\nhave multiple \"next revocation hashes\" in-flight (by instead having\nmultiple trees to achieve multiple in-flight), it's possible to only\nsend the \"next revocation hash\", which will automatically reveal the\nprior \"revocation preimage\". In other words, the wire message could only\nbe sending next_revocation_hash. If one were to use pubkey revocations,\nthen that construction *requires* using privkeys+pubkeys instead of\npreimages+hashes, which ups the cost/complexity -- which is why I said\nit probably wasn't worth it. \n\n> Yeah, agreed.  Hash trees are nice and simple, so unless we get a\n> signficiant win, let's stick with that?\n\nFor sure.\n\n-- \nJoseph Poon"
            },
            {
                "author": "Rusty Russell",
                "date": "2016-08-10T02:03:46",
                "message_text_only": "Joseph Poon <joseph at lightning.network> writes:\n> On Wed, Aug 10, 2016 at 06:36:11AM +0930, Rusty Russell wrote:\n>> Yes, I think we agree some \"filter hint\" is needed to avoid a crazy\n>> amount of outsourcing work (eg. first 8/16 bytes of txid).  I don't\n>> think an HMAC check per registered commitment is quite fast enough.\n>\n> For #1, the HMAC would be pre-computed by the outsourcer. So the flow\n> looks like:\n>\n> 1. The outsourcer takes the txid, HMAC(txid+salt1) and encrypts the blob\n> 2. The outsourcer gives the 32-byte hmac and blob to the watcher\n> 3. The watcher adds the 32-byte hmac and the blob to a key-value store\n> \t(the watcher can optionally truncate or whatever)\n> 4. When the watcher receives a new block, they HMAC(txid+salt1) all\n> transactions and compare against the key-value store\n\nAh OK!  I am in sync now, sorry for the noise!\n\n>> If we include the witness in that HMAC we risk reintroducing\n>> malleability.  If we don't, we risk txs being predictable.\n>\n> I was referring to the non-witness txid when making that comment, but\n> there should be sufficient entropy from the revocation hash (whose P2WSH\n> is part of the Commitment outputs).\n\nUnfortunately, watcher knows revocation preimage N, so it can figure out\nsome or all previous revocation preimages (and thus hashes).  That's\ntrue with shachain, and I think elkrem has similar properties (it's kind\nof what we were after!).\n\n>> I can think of a few fixes: insert some randomness in the tx (OP_RETURN?\n>> Different addresses each time?), or try to extract the input signature\n>> from the witness, which is unguessable, as our filter?\n>\n> Yeah! I like this idea to use one's own input sig as the key for the\n> encrypted blob too. If Alice is the one outsourcing the Commitment which\n> Bob can broadcast, Bob can only broadcast it using the sig Alice gave\n> Bob as it's spending from a 2-of-2. If Alice is outsourcing Bob's\n> Commitment broadcasts, a hash of her input signature is a solid way to\n> derive a key as well without malleability concerns.\n\nBut it rests on the assumption that there are no unknown malleability\nissues on signatures, which I believe makes crypto people nervous.  I've\nasked some, though, as that's above my pay grade!\n\nIt also assumes they can't set up the witness such that our sig is not\n2nd or 3rd in the witness element.  I think that's true...\n\n> I also like that it \"encourages\" more nodes to download witness data;\n> ignoring witnesses is a concern of mine which this helps with :^)\n\nGood point!\n\nCheers,\nRusty."
            },
            {
                "author": "Joseph Poon",
                "date": "2016-08-11T04:16:26",
                "message_text_only": "On Wed, Aug 10, 2016 at 11:33:46AM +0930, Rusty Russell wrote:\n> Unfortunately, watcher knows revocation preimage N, so it can figure out\n> some or all previous revocation preimages (and thus hashes). \n\nIf you take the results then HMAC it as the final step in\nshachain/elkrem (to establish a single leaf), should be fine even if\nrevocation hashes are used in lieu of a revocation pubkey.\n\n> But it rests on the assumption that there are no unknown malleability\n> issues on signatures, which I believe makes crypto people nervous.  I've\n> asked some, though, as that's above my pay grade!\n> \n> It also assumes they can't set up the witness such that our sig is not\n> 2nd or 3rd in the witness element.  I think that's true...\n\nYeah, good point. Perhaps it could be better to keep it simple and just\nuse an HMAC of the non-witness transaction. There shouldn't be stuff\nthat's easily mutatable, and the exposure is not expanded (since that\nwould break LN's child transactions anyway).\n\n-- \nJoseph Poon"
            },
            {
                "author": "Rusty Russell",
                "date": "2016-08-12T03:47:52",
                "message_text_only": "Joseph Poon <joseph at lightning.network> writes:\n> On Wed, Aug 10, 2016 at 11:33:46AM +0930, Rusty Russell wrote:\n>> Unfortunately, watcher knows revocation preimage N, so it can figure out\n>> some or all previous revocation preimages (and thus hashes). \n>\n> If you take the results then HMAC it as the final step in\n> shachain/elkrem (to establish a single leaf), should be fine even if\n> revocation hashes are used in lieu of a revocation pubkey.\n\nSure, or just SHA the leaf again.  But such schemes prevent the watcher\nfrom using the elkrem/shachain space-savings themselves, which is kind\nof painful.  (Though see below...)\n\nLet's look at space & comms requirements, per new commitment tx, for\ndifferent schemes.  Let's say we have N htlcs in the current HTLC.\n\n                        Bytes           Bytes\n                        Communicated    Stored (per tx)\nNaive (whole steal tx)  ~300 * (N+1)    ~300 * (N+1)\nrhash+sig+HTLCdata      32 + 32 + 24*N  32 + 32 + 24*N\n+shachain/elkrem        32 + 32 + 24*N  32 + 24*N\n+SIG_NOINPUT            32 + 24*N       24*N\n+Dual-scriptpubkey MAST 32              0\n\nThe ultimate is shachain/elkrem, SIG_NOINPUT and a MAST scheme which\nplaces two scripthashes into the scriptpubkey, either of which would\nallow spending.  In that case, we'd simply send a series of SIG_NOINPUT\nsigned steal txs with 1, 2, 3 ... inputs at the beginning, and then send\neach new revocation preimage as we learned it.  The storage overhead is\nbasically constant (technically, log(N) for the shachain).\n\nInterestingly, MAST by itself buys us nothing (bytewise): the 32 bytes\nwe'd need to send for the other branch of MAST is larger than the 24\nbytes for the HTLC description.\n\nRevealing the preimages seems a win, BUT if we give them unencrypted to\nthe watcher we need some other way of avoiding guessable prior\ncommitment txs.  Which implies we change at least one key in some way,\nwhich means we need to send that key to the watcher to store, which\ncosts as much as they save using shachain/elkrem!\n\nThe other win is sharing HTLCs across transactions somehow (which\nreduces N to the \"number of new HTLCs\" instead of \"number of HTLCs\").\nI can't come up with anything very good here, though :(\n\n>> But it rests on the assumption that there are no unknown malleability\n>> issues on signatures, which I believe makes crypto people nervous.  I've\n>> asked some, though, as that's above my pay grade!\n>> \n>> It also assumes they can't set up the witness such that our sig is not\n>> 2nd or 3rd in the witness element.  I think that's true...\n>\n> Yeah, good point. Perhaps it could be better to keep it simple and just\n> use an HMAC of the non-witness transaction. There shouldn't be stuff\n> that's easily mutatable, and the exposure is not expanded (since that\n> would break LN's child transactions anyway).\n\nI still don't understand why use an HMAC-of-tx instead of just the txid?\nWhat does it gain?\n\nThanks!\nRusty."
            },
            {
                "author": "Joseph Poon",
                "date": "2016-08-12T21:20:34",
                "message_text_only": "Hi Rusty,\n\nYeah, interesting thoughts!\n\nOn Fri, Aug 12, 2016 at 01:17:52PM +0930, Rusty Russell wrote:\n> I still don't understand why use an HMAC-of-tx instead of just the txid?\n> What does it gain?\n\nI'm presuming you need to index by txid to see if there needs to be a\npenalty transaction broadcast, so you can't use that as a secret key.\n\n-- \nJoseph Poon"
            },
            {
                "author": "Rusty Russell",
                "date": "2016-08-13T10:30:33",
                "message_text_only": "Joseph Poon <joseph at lightning.network> writes:\n> Hi Rusty,\n>\n> Yeah, interesting thoughts!\n>\n> On Fri, Aug 12, 2016 at 01:17:52PM +0930, Rusty Russell wrote:\n>> I still don't understand why use an HMAC-of-tx instead of just the txid?\n>> What does it gain?\n>\n> I'm presuming you need to index by txid to see if there needs to be a\n> penalty transaction broadcast, so you can't use that as a secret key.\n\nBut SHA256(txid) would be sufficent, no?\n\n(Unless we're happy with 128 bit keys, then just use the upper bits for\nhint and lower bits for key).\n\nSo, here's a strawman spec (written in cat > style, so includes bugs):\n\n1) We change the protocol slightly to use shachain/elkrem with an\n   additional SHA256() to get the preimage.  On-wire we reveal the leaf\n   node, which you SHA256() to get the preimage.  This makes commit\n   N-1 unguessable even if you know commit N.\n\n2) Format of message-to-watcher is:\n   [8-byte-txid-prefix-hint]\n   [chacha20poly1305 blob, key is SHA256(txid):\n    [txid-of-previous-commit-or-zero]\n    [bitcoin signature]\n    [revocation preimage]\n    [htlc #1 H-hash ripemd160]\n    [htlc #1 expiry]\n    [htlc #2 H-hash ripemd160]\n    [htlc #2 expiry]\n    ...\n    [arbitrary zero padding]\n   ]\n\n3) Implementations should pad this out to some reasonable amount to\n   cover expected HTLCs and avoid revealing too much using size.\n   Handwave.\n\n4) Implementations should set the txid-of-previous-commit field\n   sparingly: it saves space for HTLCs which are in multiple commits,\n   but leaks information.  Handwave.\n\n5) Initially watcher would be given commit and timeout keys for both\n   sides (which I'm assuming are static).\n\n6) Upon seeing a txid prefix match, watcher tries to decrypt.  If\n   success, decrypts previous message-to-watchers as possible using\n   txid-of-previous-commit-or-zero fields and walking back.  Accumulates\n   all the HTLCs, calculates the wscripts for them, sees which match\n   outputs.  Also look for the one the cheater paid to-self.  Generates\n   transaction that spends all the outputs it can match, checks [bitcoin\n   signature] is valid, sends tx.\n\nWhat have I missed?\nRusty."
            },
            {
                "author": "Tadge Dryja",
                "date": "2016-08-15T15:18:24",
                "message_text_only": "There's two approaches with encrypted vs non-encrypted: the non-encrypted\ndesign which I kindof like, is to make all the information given to the\nobserver not mean anything on its own.  With encrypted, you achieve the\nsame result, but have some decryption key stuffed somewhere in the observed\ntransaction to reveal meaningful data which identifies the channel, but is\nencrypted.\n\nNon-encrypted can be more efficient, because it's hard to squeeze down\ncompact encrypted data (though see below for an attempt!).  But most things\nin the channel states can be obfuscated such that even if you tell\neverything to the observer, they don't learn anything.  (Even in the case\nwhere the observer is watching both sides of the channel, they shouldn't be\nable to match them... well other than timing, which is admittedly a very\neffective way to do it!)\n\nI skipped over HTLCs though because they didn't fit with this model.  And\nthey really don't -- unlike the updating pubkeys in the commit tx, HTLC's\nare passed though multiple nodes, so information about them can get to the\nobserver pretty easily.  So I think HTLCs would need to be in some kind of\nencrypted blob to send to the observer.\n\nI really like txid[0:16] as the truncated txid for the observer and\ntxid[16:32] as the decryption key because it's simple and quite fast.  This\nwould allow constant-time lookups into the observer's database regarless of\nhow many channels it's watching, which HMAC'ing the txid doesn't have.\n (You could hash txid[16:32] again for the decryption key if you want 32\nbytes.)\n\nThe non-HTLC data can be sent unencrypted -- it's pretty much just a\nsignature and hash from the tree.  If there is a new HTLC (or a few) added\nin that state, the node can elect to send that to the observer as well.  I\nthink the format can be something like:\n\nhtlc #1 expiry (4 bytes)\nhtlc #1 preimage (20 bytes)\nhtlc #2 expiry (4 bytes)\nhtlc #2 preimage (20 bytes)\noffset to previous blob (2 bytes)\ndecrypt key for previous blob (16 bytes)\n\nhaving pointers to previous states can save a lot of space if HTLCs are\nadded incrementally.  The \"blobs\" can be kept in a separate data store\nindexed by state number, so it's quick to see that, e.g, state 471 also has\nan HTLC from state 465, which has HTLCs from state 442.  This chained\ndecryption may end up revealing more HTLCs than are needed (which are quick\nfor the observer to detect and discard) but if the fraud has occurred then\nanonymity is gone anyway and it's no big deal if the observer learns a\nlittle more -- they already learned all the important stuff.\n\nI *think* 2 bytes is enough; it's not that an HTLC can't last more than 65K\nstates, it's that an HTLC can't persist > 65K states with no other HTLCs\nbeing added during that period.  A long-lived HTLC wouldn't be referenced\ndirectly; instead later states which still had it would point to a previous\nstate that also pointed to it.  It's a bit more work for the observer, who\nmight end up with hundreds of extra preimages, but I think optimizing for\nspace savings at the cost of CPU time when the fraud occurs is a good\ntrade-off.  (The fraud occurs almost never, while the state data transfers\nand storage happen always)\n\nThis would also allow nodes to omit or include HTLCs to the observer as\nthey see fit, which seems useful for micropayments which might outstrip the\nabilities of the observer.\n\nAlso, yeah, padding (handwave) and timing are what make hiding the channel\nvery tricky, especially for HTLCs.  With non-HTLC updates, it can be hard\nto know when 2 nodes are updating a channel state, but with HTLCs there are\nmore nodes in the mix with more points for data to leak out to the\nobserver.  That's another reason you might want to omit sending out some\nportion of HTLC recovery data.\n\nI will try coding some of this and see, because it seems to work in my head\nbut that's no indication it'll work on the computer :)\n\n-Tadge\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20160815/4eb6ee12/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2016-08-18T06:47:49",
                "message_text_only": "Tadge Dryja <tadge at lightning.network> writes:\n> There's two approaches with encrypted vs non-encrypted: the non-encrypted\n> design which I kindof like, is to make all the information given to the\n> observer not mean anything on its own.  With encrypted, you achieve the\n> same result, but have some decryption key stuffed somewhere in the observed\n> transaction to reveal meaningful data which identifies the channel, but is\n> encrypted.\n>\n> Non-encrypted can be more efficient, because it's hard to squeeze down\n> compact encrypted data (though see below for an attempt!).  But most things\n> in the channel states can be obfuscated such that even if you tell\n> everything to the observer, they don't learn anything.  (Even in the case\n> where the observer is watching both sides of the channel, they shouldn't be\n> able to match them... well other than timing, which is admittedly a very\n> effective way to do it!)\n>\n> I skipped over HTLCs though because they didn't fit with this model.  And\n> they really don't -- unlike the updating pubkeys in the commit tx, HTLC's\n> are passed though multiple nodes, so information about them can get to the\n> observer pretty easily.  So I think HTLCs would need to be in some kind of\n> encrypted blob to send to the observer.\n\nYeah, I think that makes the non-encrypted idea a non-starter; we need\nto steal those HTLCs without introducing nasty restrictions.\n\n> I really like txid[0:16] as the truncated txid for the observer and\n> txid[16:32] as the decryption key because it's simple and quite fast.  This\n> would allow constant-time lookups into the observer's database regarless of\n> how many channels it's watching, which HMAC'ing the txid doesn't have.\n>  (You could hash txid[16:32] again for the decryption key if you want 32\n> bytes.)\n\nYeah; I'd just hash the whole txid again though.  I mean, why not?\n\n> The non-HTLC data can be sent unencrypted -- it's pretty much just a\n> signature and hash from the tree.  If there is a new HTLC (or a few) added\n> in that state, the node can elect to send that to the observer as well.  I\n> think the format can be something like:\n\nYou can't send hash in the clear unless you're using variable keys in\ncommit txs (since that hash is what makes the previous txids\nunguessable).  But that's OK, encrypting it doesn't hurt us AFAICT.\n\n> htlc #1 expiry (4 bytes)\n> htlc #1 preimage (20 bytes)\n> htlc #2 expiry (4 bytes)\n> htlc #2 preimage (20 bytes)\n> offset to previous blob (2 bytes)\n> decrypt key for previous blob (16 bytes)\n>\n> having pointers to previous states can save a lot of space if HTLCs are\n> added incrementally.  The \"blobs\" can be kept in a separate data store\n> indexed by state number, so it's quick to see that, e.g, state 471 also has\n> an HTLC from state 465, which has HTLCs from state 442.  This chained\n> decryption may end up revealing more HTLCs than are needed (which are quick\n> for the observer to detect and discard) but if the fraud has occurred then\n> anonymity is gone anyway and it's no big deal if the observer learns a\n> little more -- they already learned all the important stuff.\n\nHmm, this is more sophisticated that my suggestion, by allowing\nreference to *any* prev blob.\n\nWould that win much though?  If HTLCs have been added then removed, it's\npossible, but I'm not sure how much it saves.\n\neg.\n                HTLCs\nCommit tx:\n1               1\n2               1       2\n3                       2 \n4                       2       3       4       5\n5                                               5\n6                                                       6       7\n7                                                       6               8\n8                                                       6\n\nUsing a heuristic says \"don't ever leak more than <current-htlcs>\nunnessary HTLCs\" gives:\n\n#1: HTLC1\n#2: (references #1) HTLC2\n#3: (references #2)\n#4: HTLC2 HTLC3 HTLC4 HTLC5\n#5: HTLC5\n#6: HTLC6 HTLC7\n#7: (references #6) HTLC8\n#8: (refefences #6) HTLC6\n\nAssuming 24 bytes per HTLC, and 18 bytes per backref, the naive encoding\nwould be 14 * 24 = 336, backref = 11 * 24 + 3 * 18 = 318 bytes.  Not\nmuch.\n\nOK, let me write a quick simulator, with exponential distribution times\n(because bitcoin!).  Hmm, attached below.  It only references the\nprevious HTLC, not any previous, but it does show that it's hard to make\nthe savings more than 50% (reduce the chance of an HTLC expiring to 1%,\nfor example).\n\nSee horrible hacked-up code below.\n\n> Also, yeah, padding (handwave) and timing are what make hiding the channel\n> very tricky, especially for HTLCs.  With non-HTLC updates, it can be hard\n> to know when 2 nodes are updating a channel state, but with HTLCs there are\n> more nodes in the mix with more points for data to leak out to the\n> observer.  That's another reason you might want to omit sending out some\n> portion of HTLC recovery data.\n\nYes, this is the hard part.\n\n> I will try coding some of this and see, because it seems to work in my head\n> but that's no indication it'll work on the computer :)\n\nFor sure!  Here's my hacky simulation results for 1M commitment txs\nwith the mean and stddev in the brackets:\n\nNum htlcs:              0-24(3.99+/-2.5)\nRaw-encoding bytes:     0-576(95+/-60)\nBackref-encoding bytes: 16-520(40+/-34)\nBest-encoding bytes:    0-528(65+/-51)\n\nCheers,\nRusty.\n\n// 2>/dev/null; set -e; OUT=/tmp/`basename $0 .c`; if [ ! -f \"$OUT\" ] || [ \"$OUT\" -ot \"$0\" ]; then gcc -g -Wall -o \"$OUT\".$$ $0 -lm && mv \"$OUT\".$$ \"$OUT\"; fi; exec \"$OUT\" \"$@\"\n#include <stdio.h>\n#include <math.h>\n#include <stdlib.h>\n#include <assert.h>\n#include <string.h>\n\n/* For convenience, avoid allocation */\n#define MAX_HTLCS 1000\n\n/* Percentage chance of HTLC being removed. */\n#define REMOVE_CHANCE 25\n/* Percentage chance of HTLC being added. */\n#define ADD_CHANCE 50\n\n#define BYTES_PER_HTLC 24\n#define BACKREF_BYTES 16\n\nstatic void add_some_htlcs(unsigned int htlcs[], size_t *n_htlcs,\n\t\t\t   size_t *counter)\n{\n\t/* Exponential distribution again. */\n\twhile (random() < (RAND_MAX * (long long)ADD_CHANCE / 100)) {\n\t\tassert(*n_htlcs < MAX_HTLCS);\n\t\thtlcs[*n_htlcs] = *counter;\n\t\t(*counter)++;\n\t\t(*n_htlcs)++;\n\t}\n}\n\nstatic size_t calc_encoding(const unsigned int old_htlcs[], size_t n_old,\n\t\t\t    const unsigned int new_htlcs[], size_t n_new,\n\t\t\t    size_t prev_leaks)\n{\n\tsize_t o, n, num_new = 0, num_old = 0;\n\tsize_t raw_bytes, bytes_using_backref;\n\n\t/* Figure out number only in new, only in old. */\n\tfor (o = n = 0; o < n_old || n < n_new;) {\n\t\tif (o == n_old) {\n\t\t\tnum_new++;\n\t\t\tn++;\n\t\t} else if (n == n_new) {\n\t\t\tnum_old++;\n\t\t\to++;\n\t\t} else if (old_htlcs[o] == new_htlcs[n]) {\n\t\t\to++;\n\t\t\tn++;\n\t\t} else if (old_htlcs[o] < new_htlcs[n]) {\n\t\t\tnum_old++;\n\t\t\to++;\n\t\t} else {\n\t\t\tnum_new++;\n\t\t\tn++;\n\t\t}\t\n\t}\n\n\traw_bytes = n_new * BYTES_PER_HTLC;\n\tbytes_using_backref = num_new * BYTES_PER_HTLC + BACKREF_BYTES;\n\n\t/* Num htlcs, raw-encoding, backref-encoding, encoding-used */\n\tprintf(\"%zu,%zu,%zu,\", n_new, raw_bytes, bytes_using_backref);\n\n\t/* Don't use backref if we would leak more than #current. */\n\tif (!num_old || num_old + prev_leaks > n_new) {\n\t\tprintf(\"%zu\\n\", raw_bytes);\n\t\treturn 0;\n\t} else {\n\t\tprintf(\"%zu\\n\", bytes_using_backref);\n\t\treturn num_old + prev_leaks;\n\t}\n}\n\nstatic void iterate(unsigned int htlcs[], size_t *n_htlcs, size_t *counter,\n\t\t    size_t *leaks)\n{\n\tsize_t i, num;\n\tunsigned int tmp_htlcs[MAX_HTLCS];\n\n\t/* REMOVE_CHANCE percentage chance of HTLC being closed. */\n\tfor (i = 0, num = 0; i < *n_htlcs; i++) {\n\t\tif (random() < (RAND_MAX * (long long)REMOVE_CHANCE / 100))\n\t\t\tcontinue;\n\t\ttmp_htlcs[num++] = htlcs[i];\n\t}\n\tadd_some_htlcs(tmp_htlcs, &num, counter);\n\n\t*leaks = calc_encoding(htlcs, *n_htlcs, tmp_htlcs, num, *leaks);\n\tmemcpy(htlcs, tmp_htlcs, num * sizeof(htlcs[0]));\n\t*n_htlcs = num;\n}\n\nint main(int argc, char *argv[])\n{\n\tsize_t i, n_iter;\n\tunsigned int htlcs[MAX_HTLCS];\n\tsize_t n_htlcs = 0, counter = 0, leaks = 0;\n\n\tprintf(\"Num htlcs, raw-encoding bytes, backref-encoding bytes, best-encoding bytes\\n\");\n\tn_iter = argv[1] ? atoi(argv[1]) : 100;\n\tfor (i = 0; i < n_iter; i++)\n\t\titerate(htlcs, &n_htlcs, &counter, &leaks);\n\n\treturn 0;\n}"
            }
        ],
        "thread_summary": {
            "title": "Blinded channel observation",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Rusty Russell",
                "Tadge Dryja",
                "Joseph Poon"
            ],
            "messages_count": 20,
            "total_messages_chars_count": 60499
        }
    },
    {
        "title": "[Lightning-dev] c-lightning 0.4: Wright's Cryptographic Proof (https://git.io/v6Pzv)",
        "thread_messages": [
            {
                "author": "Rusty Russell",
                "date": "2016-08-19T02:03:13",
                "message_text_only": "Hi all!\n\n        This release can actually restart, and reconnect without\nforgetting everything (sqlite3), and does simple routing.  It adds\n\"accept-payment\" and \"pay\" JSON commands, but the channel setup is still\nwoefully primitive.\n\nI am setting up a couple of public nodes on testnet now, will follow up\nwith detailed usage instructions.  The next version will have IRC-based\nautodetect of nodes, though, so that will be a far nicer experience!\n\nCheers,\nRusty.\n\nChangelog:\n- Shachain saving for receiver: thanks sstone!\n- Fix inter-node encryption as per latest BOLT#1: thanks CJP!\n- add-route config option (or cmdline), and add-route json command.\n- Support routing through of HTLCs (unencrypted \"onion\" for now)\n- Drop support for uncompressed keys.\n- Reconnect support, with retransmission as required.\n- Database support, to save state at important points.\n- Switch HTLCs to states, rather than keeping explicit queues.\n- Misc changes to match latest BOLT specs.\n- Many minor fixes and cleanups."
            }
        ],
        "thread_summary": {
            "title": "c-lightning 0.4: Wright's Cryptographic Proof (https://git.io/v6Pzv)",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Rusty Russell"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1003
        }
    }
]