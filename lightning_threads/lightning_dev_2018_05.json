[
    {
        "title": "[Lightning-dev] eltoo: A Simplified update Mechanism for Lightning and Off-Chain Contracts",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-01T05:07:54",
                "message_text_only": "Good morning Christian,\n\nThis is very interesting indeed!\n\nI have started skimming through the paper.\n\nI am uncertain if the below text is correct?\n\n> Throughout this paper we will use the terms *input script* to refer to `witnessProgram` and `scriptPubKey`, and *output script* to refer to the `witness` or `scriptSig`.\n\nFigure 2 contains to what looks to me like a `witnessProgram`, `scriptPubKey`, or `redeemScript` but refers to it as an \"output script\":\n\n>    OP_IF\n>        10 OP_CSV\n>        2 As Bs 2 OP_CHECKMULTISIGVERIFY\n>    OP_ELSE\n>        2 Au Bu 2 OP_CHECKMULTISIGVERIFY\n>    OP_ENDIF\n>\n> Figure 2: The output script used by the on-chain update transactions.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Christian Decker",
                "date": "2018-05-01T11:38:12",
                "message_text_only": "ZmnSCPxj <ZmnSCPxj at protonmail.com> writes:\n> Good morning Christian,\n>\n> This is very interesting indeed!\n>\n> I have started skimming through the paper.\n>\n> I am uncertain if the below text is correct?\n>\n>> Throughout this paper we will use the terms *input script* to refer to `witnessProgram` and `scriptPubKey`, and *output script* to refer to the `witness` or `scriptSig`.\n>\n> Figure 2 contains to what looks to me like a `witnessProgram`, `scriptPubKey`, or `redeemScript` but refers to it as an \"output script\":\n>\n>>    OP_IF\n>>        10 OP_CSV\n>>        2 As Bs 2 OP_CHECKMULTISIGVERIFY\n>>    OP_ELSE\n>>        2 Au Bu 2 OP_CHECKMULTISIGVERIFY\n>>    OP_ENDIF\n>>\n>> Figure 2: The output script used by the on-chain update transactions.\n>\n> Regards,\n> ZmnSCPxj\n\nDarn last minute changes! Yes, you are right, I seem to have flipped the\ntwo definitions. I'll fix that up and push a new version."
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-01T07:12:35",
                "message_text_only": "Hi Christian,\n\nLet me know if I have summarized the paper accurately below:\n\n1.  SIGHASH_NOINPUT removes all inputs of the transaction copy before signing/verifying.\n\n2.  SIGHASH_NOINPUT can be combined with SIGHASH_SINGLE.  It is dangerous to combine it with SIGHASH_NONE (as this also deletes all outputs of the transaction copy, so the signature only commits to nLockTime) and possibly pointless to combine it with SIGHASH_ANYONECANPAY (which deletes other inputs, but since there are no inputs after SIGHASH_NOINPUT, there is nothing else to delete).  The BIP only mentions combining it with SIGHASH_SINGLE, in any case.\n\n3.  We have these kinds transactions: (1) funding transaction paying out to an ordinary N-of-N multisig (confirmed onchain); (2) offchain trigger transaction spending a funding transaction and paying out to the \"update transaction script\"; (3) offchain update transaction spending a trigger or update transaction and paying out to the \"update transaction script\" (the same script template as in trigger transaction); (4) offchain settlement transaction spending a trigger or update transaction and paying out to the counterparties according to the final agreed distribution of funds.  Update and settlement transactions are signed with SIGHASH_NOINPUT flags.\n\n4.  The update transaction script has two branches: a CSV-encumbered N-of-N \"settlement\" branch, and a CLTV-encumbered N-of-N \"update\" branch.  Crucially, we use past Unix timestamps for the CLTV encumberance on the update.  The CLTV-encumberance starts with a minimum time value on the trigger transaction, and increments by 1 for every update transaction.\n\n5.  The CLTV-encumberance ensures that if a past update transaction is confirmed, we can spend it using any later update transaction, since CLTV uses `stackTop <= nLockTime`.  The actual `nLockTime` we use is a past Unix timestamp so they are not actually time-encumbered, but the `OP_CLTV` still ensures that any later update transaction can be used to spend from any earlier update transaction, but not vice versa.  This is actually quite clever.\n\n6.  The pubkeys on the settlement branch of each update transaction are different, and are derived using any hierarchical derivation method (trivially we can simply use the CLTV-encumberance value as the derivation index).  This ensures that each settlement transaction can only spend a particular update transaction.  Or in other words: There is a one-to-one correspondence between update and settlement transaction, and a settlement transaction can only confirm if the corresponding update transaction can be confirmed deeply enough without having its output re-spent.\n\n7.  The pubkeys on the update branch are all the same in all update transactions.  This lets any update transaction replace any other update transaction, as long as the CLTV-encumberance (which is used for ordering rather than actual absolute locktime) is respected; together they mean that any later update transaction can replace any earlier update transaction.\n\n8.  If an old update transaction is confirmed, the settlement transaction corresponding to it is still encumbered by CSV and cannot be confirmed immediately.  During this time, a later update transaction can spend it and be confirmed.  If that update transaction is still not the latest available, it can be further replaced with any, later transaction (since the CLTV-encumberance and the `nLockTime` increase in lockstep with each other) until the latest update transaction is confirmed.\n\n9.  When the latest update transaction gets confirmed, no other update transaction can successfully spend its \"update\" branch (as their `nLockTime` is less than the CLTV-encumberance on that branch).  This lets the CSV-encumbered settlement transaction be confirmable after some blocks.\n\n10.  Update transactions pay no fees!  Instead, for update transactions (but not settlement transactions) we additionally give SIGHASH_SINGLE: thus, no matter what transaction they end up in, the signatures for the update transaction will always be verified against only the update transaction output that pays out to the update transaction script.  We join update transactions with a spend of some onchain UTXO we control, which pays for the fees of the joined the update transactions, and may have a second output that is the remainder of the fund after paying the fees.\n\n11.  Settlement transactions can carry contracts (in much the same way that Poon-Dryja commitment transactions); however contracts that contain absolute timelock components will be affected by the CSV-encumberance of the settlement transaction.\n\n---\n\nSome pros and cons relative to Poon-Dryja (LN-penalty) channels:\n\n- Requires more transactions in the worst-case: trigger, update, settlement.  Compare to Poon-Dryja: commitment, claim.  Decker-Russell-Osuntokun channels can be trigger-settlement but only in the degenerate case where the channel was never updated (indeed for implementation simplicity we might rather prefer to make an initial update transaction at the start, instead of starting with a trigger-settlement).\n\n- Dropping unilaterally onchain requires the party doing the drop to have some onchain funds it controls completely, since update transactions do not pay fees by themselves.  We cannot have an autopilot that puts all onchain funds on channels, we need to reserve some small amount for paying fees of unilateral closes.\n\n+ The above is counterbalanced by the fact that we can easily adjust onchain fees for update transactions, unlike the case for Poon-Dryja commitment transactions which once signed cannot have fees updated.\n\n= The onchain reserve (for paying fees for unilaterally dropped channels) is roughly equivalent to the channel reserve under Poon-Dryja; such a channel reserve no longer exists under Decker-Russell-Osuntukun channels, but is replaced by the onchain fee-paying reserve.  Possibly the fee-paying reserve here might be feasibly smaller than the channel reserve under Poon-Dryja.\n\n- The CSV-encumberance on settlement transactions, which are the ones which carry the contracts in the channel, affects all absolute-timelocked contracts transported on the channel.  Compare to Poon-Dryja, where commitment transactions themselves are unencumbered by CSV, and we simply insert the revocation to spends of the contracts being transported (i.e. the reason why we have HTLC-success and HTLC-timeout transactions in BOLT spec).\n\n+ Like invalidation trees, we can use N-party funds rather than just 2-party funds (channels).  We can even use M-of-N, so if there is at least some amount of trust you have with a group of entities (family, close friends, conglomeration partners, ZmnSCPxj, politicians...) it would be possible to have funds that require say only a majority of its owners to be updated.\n\nPlease inform me if there are important points I missed.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Christian Decker",
                "date": "2018-05-01T12:04:51",
                "message_text_only": "Excellent summary ZmnSCPxj, I'll try to address the points inline (if\nthere is anything to add that is):\n\nZmnSCPxj <ZmnSCPxj at protonmail.com> writes:\n> Hi Christian,\n>\n> Let me know if I have summarized the paper accurately below:\n>\n> 1.  SIGHASH_NOINPUT removes all inputs of the transaction copy before\n>     signing/verifying.\n\nIt sets them to a known constant, in this case we just blank\nthem. Removing could entail more costly serialization depending on the\nimplementation. bitcoind serializes into a hash accumulator so it'd not\nmake a difference there, but others may do it differently.\n\n> 2.  SIGHASH_NOINPUT can be combined with SIGHASH_SINGLE.  It is\n> dangerous to combine it with SIGHASH_NONE (as this also deletes all\n> outputs of the transaction copy, so the signature only commits to\n> nLockTime) and possibly pointless to combine it with\n> SIGHASH_ANYONECANPAY (which deletes other inputs, but since there are\n> no inputs after SIGHASH_NOINPUT, there is nothing else to delete).\n> The BIP only mentions combining it with SIGHASH_SINGLE, in any case.\n\n`SIGHASH_SINGLE` really is only singled out (no pun intended) because it\nis usefull for the fee feature that Rusty came up with. It's not our\nintention to exclude other uses, but you're right, it's hard to come up\nwith a use-case for `SIGHASH_NOINPUT` in combination with\n`SIGHASH_NONE` or `SIGHASH_ANYONECANPAY` :-)\n\nIn particular we don't want to exclude potential future sighash types,\nbut those will have to be dealt with when they come up.\n\n> 3.  We have these kinds transactions: (1) funding transaction paying\n> out to an ordinary N-of-N multisig (confirmed onchain); (2) offchain\n> trigger transaction spending a funding transaction and paying out to\n> the \"update transaction script\"; (3) offchain update transaction\n> spending a trigger or update transaction and paying out to the \"update\n> transaction script\" (the same script template as in trigger\n> transaction); (4) offchain settlement transaction spending a trigger\n> or update transaction and paying out to the counterparties according\n> to the final agreed distribution of funds.  Update and settlement\n> transactions are signed with SIGHASH_NOINPUT flags.\n>\n> 4.  The update transaction script has two branches: a CSV-encumbered\n> N-of-N \"settlement\" branch, and a CLTV-encumbered N-of-N \"update\"\n> branch.  Crucially, we use past Unix timestamps for the CLTV\n> encumberance on the update.  The CLTV-encumberance starts with a\n> minimum time value on the trigger transaction, and increments by 1 for\n> every update transaction.\n\nReally any monotonically increasing sequence of increments will work,\nbut incrementing by 1 maximizes the number of updates we can perform,\nyes.\n\n> 5.  The CLTV-encumberance ensures that if a past update transaction is\n> confirmed, we can spend it using any later update transaction, since\n> CLTV uses `stackTop <= nLockTime`.  The actual `nLockTime` we use is a\n> past Unix timestamp so they are not actually time-encumbered, but the\n> `OP_CLTV` still ensures that any later update transaction can be used\n> to spend from any earlier update transaction, but not vice versa.\n> This is actually quite clever.\n\nCorrect, the only reason to use `nLocktime` here is that we need the\nsignature of the update transaction to commit to the state number, so we\ncan't put it in the script itself.\n\n> 6.  The pubkeys on the settlement branch of each update transaction\n> are different, and are derived using any hierarchical derivation\n> method (trivially we can simply use the CLTV-encumberance value as the\n> derivation index).  This ensures that each settlement transaction can\n> only spend a particular update transaction.  Or in other words: There\n> is a one-to-one correspondence between update and settlement\n> transaction, and a settlement transaction can only confirm if the\n> corresponding update transaction can be confirmed deeply enough\n> without having its output re-spent.\n\nOther bindings are possible, e.g., compare the `nLocktime` again, but\nbinding through pubkeys minimizes the script size.\n\n> 7.  The pubkeys on the update branch are all the same in all update\n> transactions.  This lets any update transaction replace any other\n> update transaction, as long as the CLTV-encumberance (which is used\n> for ordering rather than actual absolute locktime) is respected;\n> together they mean that any later update transaction can replace any\n> earlier update transaction.\n>\n> 8.  If an old update transaction is confirmed, the settlement\n> transaction corresponding to it is still encumbered by CSV and cannot\n> be confirmed immediately.  During this time, a later update\n> transaction can spend it and be confirmed.  If that update transaction\n> is still not the latest available, it can be further replaced with\n> any, later transaction (since the CLTV-encumberance and the\n> `nLockTime` increase in lockstep with each other) until the latest\n> update transaction is confirmed.\n\nNotice that the update transactions are actually paying their keep,\ni.e., they have fees attached, so we think that participants will simply\njump to the latest state and not waste fees on things that have no\nchance of getting them closer to getting an old state enforced.\n\n> 9.  When the latest update transaction gets confirmed, no other update\n> transaction can successfully spend its \"update\" branch (as their\n> `nLockTime` is less than the CLTV-encumberance on that branch).  This\n> lets the CSV-encumbered settlement transaction be confirmable after\n> some blocks.\n>\n> 10.  Update transactions pay no fees!  Instead, for update\n> transactions (but not settlement transactions) we additionally give\n> SIGHASH_SINGLE: thus, no matter what transaction they end up in, the\n> signatures for the update transaction will always be verified against\n> only the update transaction output that pays out to the update\n> transaction script.  We join update transactions with a spend of some\n> onchain UTXO we control, which pays for the fees of the joined the\n> update transactions, and may have a second output that is the\n> remainder of the fund after paying the fees.\n>\n> 11.  Settlement transactions can carry contracts (in much the same way\n> that Poon-Dryja commitment transactions); however contracts that\n> contain absolute timelock components will be affected by the\n> CSV-encumberance of the settlement transaction.\n\nExactly, the timeouts need to be chosen high enough in order to allow an\norderly resolution on-chain, should one party become uncooperative.\n\n> ---\n>\n> Some pros and cons relative to Poon-Dryja (LN-penalty) channels:\n>\n> - Requires more transactions in the worst-case: trigger, update,\n> settlement.  Compare to Poon-Dryja: commitment, claim.\n> Decker-Russell-Osuntokun channels can be trigger-settlement but only\n> in the degenerate case where the channel was never updated (indeed for\n> implementation simplicity we might rather prefer to make an initial\n> update transaction at the start, instead of starting with a\n> trigger-settlement).\n\nActually the trigger can also be replaced in the cooperative case,\nmeaning that the settlement in that case is just a single transaction,\nidentical to LN-penalty. It is true that we've split the unilateral\ncommitment from LN-penalty into two transactions, but we removed the\nneed for a claim transaction, since funds directly go to the recipient\nand we have no our_unilateral/to_us or their_unilateral/to_them delay\nthat needs to be sweeped (technically also not necessary, but all\nimplementation do that if I'm not mistaken). Even if funds are not\nsweeped, the UTXO state is larger due to the bigger script that our\nsimple outputs for the settlement.\n\n> - Dropping unilaterally onchain requires the party doing the drop to\n> have some onchain funds it controls completely, since update\n> transactions do not pay fees by themselves.  We cannot have an\n> autopilot that puts all onchain funds on channels, we need to reserve\n> some small amount for paying fees of unilateral closes.\n\nThat is true, and I think this falls into the same category as the\nreserve funds in the LN-penalty mechanism, i.e., funds that are there\nbut can't be used. There may be a clever way to build a penalty on top\nof eltoo and then combine the fees and the penalties. Then again, a\nmisbehaving participant risking their on-chain fees for an outdated\nupdate transaction may be a sufficient deterrent.\n\n> + The above is counterbalanced by the fact that we can easily adjust\n> onchain fees for update transactions, unlike the case for Poon-Dryja\n> commitment transactions which once signed cannot have fees updated.\n>\n> = The onchain reserve (for paying fees for unilaterally dropped\n> channels) is roughly equivalent to the channel reserve under\n> Poon-Dryja; such a channel reserve no longer exists under\n> Decker-Russell-Osuntukun channels, but is replaced by the onchain\n> fee-paying reserve.  Possibly the fee-paying reserve here might be\n> feasibly smaller than the channel reserve under Poon-Dryja.\n\nThe one downside that this has is that the channel participants have\nvery little control over the fees that a misbehaving node may get\npunished with. As one endpoint I can no longer impose an arbitrary\nminimum loss in case of breach.\n\n> - The CSV-encumberance on settlement transactions, which are the ones\n> which carry the contracts in the channel, affects all\n> absolute-timelocked contracts transported on the channel.  Compare to\n> Poon-Dryja, where commitment transactions themselves are unencumbered\n> by CSV, and we simply insert the revocation to spends of the contracts\n> being transported (i.e. the reason why we have HTLC-success and\n> HTLC-timeout transactions in BOLT spec).\n\nTrue, but as I argued in another mail, this is a fixed offset, that is\nin the same range as today's CLTV deltas for some nodes. So for the\nnetwork of today using eltoo is roughly equivalent of adding another hop\nto our path :-)\n\n> + Like invalidation trees, we can use N-party funds rather than just\n> 2-party funds (channels).  We can even use M-of-N, so if there is at\n> least some amount of trust you have with a group of entities (family,\n> close friends, conglomeration partners, ZmnSCPxj, politicians...) it\n> would be possible to have funds that require say only a majority of\n> its owners to be updated.\n\nThe trust really comes down to a tradeoff between group size and\nliveness: the bigger the group and the more churn you have in that\ngroup, the less reliable the system becomes. Loss of liveness means that\nthe funds in the contract become unavailable for a know period of time."
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-01T23:55:41",
                "message_text_only": "Good morning Christian,\n\nThank you very much!\n\n> > Hi Christian,\n> > \n> > Let me know if I have summarized the paper accurately below:\n> > \n> > 1.  SIGHASH_NOINPUT removes all inputs of the transaction copy before\n> >     \n> >     signing/verifying.\n> >     \n> \n> It sets them to a known constant, in this case we just blank\n> \n> them. Removing could entail more costly serialization depending on the\n> \n> implementation. bitcoind serializes into a hash accumulator so it'd not\n> \n> make a difference there, but others may do it differently.\n\nDoes this then mean that if the transaction has two inputs, we are still committing to the fact that there are two inputs?  If so, it is probably useable together with SIGHASH_ANYONECANPAY.\n\nIn fact, it seems, if we are using this to provide additional fees to update transactions, we should also use SIGHASH_ANYONECANPAY on the update UTXO path so that we can add new inputs to the transaction that will pay for the fee.\n\n> > Some pros and cons relative to Poon-Dryja (LN-penalty) channels:\n> > \n> > -   Requires more transactions in the worst-case: trigger, update,\n> >     \n> >     settlement. Compare to Poon-Dryja: commitment, claim.\n> >     \n> >     Decker-Russell-Osuntokun channels can be trigger-settlement but only\n> >     \n> >     in the degenerate case where the channel was never updated (indeed for\n> >     \n> >     implementation simplicity we might rather prefer to make an initial\n> >     \n> >     update transaction at the start, instead of starting with a\n> >     \n> >     trigger-settlement).\n> >     \n> \n> Actually the trigger can also be replaced in the cooperative case,\n> \n> meaning that the settlement in that case is just a single transaction,\n> \n> identical to LN-penalty.\n\nYes, but the intent of this second list is to contrast it to Poon-Dryja/LN-penalty channels, so I skipped over similarities (well there is the onchannel reserve vs the onchain fee-paying reserve, but I would argue they are congruent and not exactly identical).  One can argue that any viable channel-update-mechanism should be cooperatively collapsible to a single transaction on top of the funding transaction (because neither side has a good chance of contesting what has been agreed on, and collapsing them all to a single closing transaction is always an improvement due to having less data hit the blockchain), so if you did your job right in eltoo (and it looks to me like you all have) then that is already a given.\n\n> It is true that we've split the unilateral\n> \n> commitment from LN-penalty into two transactions, but we removed the\n> \n> need for a claim transaction, since funds directly go to the recipient\n> \n> and we have no our_unilateral/to_us or their_unilateral/to_them delay\n> \n> that needs to be sweeped (technically also not necessary, but all\n> \n> implementation do that if I'm not mistaken). Even if funds are not\n> \n> sweeped, the UTXO state is larger due to the bigger script that our\n> \n> simple outputs for the settlement.\n\nAh, that is another difference I had not noticed, thank you.\n\n> > -   The CSV-encumberance on settlement transactions, which are the ones\n> >     \n> >     which carry the contracts in the channel, affects all\n> >     \n> >     absolute-timelocked contracts transported on the channel. Compare to\n> >     \n> >     Poon-Dryja, where commitment transactions themselves are unencumbered\n> >     \n> >     by CSV, and we simply insert the revocation to spends of the contracts\n> >     \n> >     being transported (i.e. the reason why we have HTLC-success and\n> >     \n> >     HTLC-timeout transactions in BOLT spec).\n> >     \n> \n> True, but as I argued in another mail, this is a fixed offset, that is\n> \n> in the same range as today's CLTV deltas for some nodes. So for the\n> \n> network of today using eltoo is roughly equivalent of adding another hop\n> \n> to our path :-)\n\nThis may complicate routing decisions, however, in a mixed network where some channels are Poon-Dryja and some channels are Decker-Russell-Osuntokun; and if we deploy this as a back-compatible extension to the Lightning Network rather than a completely new network (and we should) then we will have such a mixed network.\n\nA payer routing through such a network finding two routes with identical fees and cltv-deltas, but one going through only Poon-Dryja channels and the other with one channel going through Decker-Russell-Osuntokun (and the rest still Poon-Dryja) would prefer the purely Poon-Dryja one, which does not have any additional csv-delta to impose on the route.\n\nFurther, once a payer can find no purely Poon-Dryja routes, then it no longer matters how many Decker-Russell-Osuntokun channels a route hops, because csv-deltas are not additive.  Unfortunately most standard routing I know of assume additive costs, meaning we would either have a two-phase routing (first we try to find only purely Poon-Dryja routes, then if there are none we accept Decker-Russell-Osuntokun channels) or hack around with the routing algo to try and make things fit.\n\nFinally, it seems to me that it is very unlikely that we would accept some centrally-imposed csv-delta.  Instead this would be negotiated between peers during channel setup (probably similarly to the current closing fee negotiation), and would vary across the network.  Indeed in such a system, Poon-Dryja channels would be gossiped as having a csv-delta=0.  But now the routing decision on whether to pass through a channel would depend on whether the csv-delta is greater than the current route csv-delta or not.  Hmm.\n\n--\n\nIn any case, it seems to me that we could introduce this as a local feature.\n\n`channel_update` would have to be extended with a `csv_delta` parameter.  When gossiping with a peer that does not yet understand `csv_delta` we would only give it `channel_update` with `csv_delta=0`.  Or maybe we should have a new `channel_update` message entirely, since they need to be signed: use the current `channel_update` for Poon-Dryja channels, use the new version for Decker-Russell-Osuntokun channels.  This way, nodes that are unaware of this new channel type just build routemaps of Poon-Dryja channels, which is important as they do not know that they need to give a larger margin for Decker-Russell-Osuntokun channels.\n\nIf a peer does not support Decker-Russell-Osuntokun channels, then it is simply impossible to fund such with them.  If it *does* support Decker-Russell-Osuntokun channels, then we have a choice to fund either that or Poon-Dryja channels.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-02T12:16:29",
                "message_text_only": "Good morning Christian,\n\n> > > Hi Christian,\n> > > \n> > > Let me know if I have summarized the paper accurately below:\n> > > \n> > > 1.  SIGHASH_NOINPUT removes all inputs of the transaction copy before\n> > >     \n> > >     signing/verifying.\n> > >     \n> > \n> > It sets them to a known constant, in this case we just blank\n> > \n> > them. Removing could entail more costly serialization depending on the\n> > \n> > implementation. bitcoind serializes into a hash accumulator so it'd not\n> > \n> > make a difference there, but others may do it differently.\n> \n> Does this then mean that if the transaction has two inputs, we are still committing to the fact that there are two inputs? If so, it is probably useable together with SIGHASH_ANYONECANPAY.\n> \n> In fact, it seems, if we are using this to provide additional fees to update transactions, we should also use SIGHASH_ANYONECANPAY on the update UTXO path so that we can add new inputs to the transaction that will pay for the fee.\n\nHa, no, looking at the detailed `SIGHASH_NOINPUT` spec, `hashPrevouts`, which normally commits to the other inputs, is blanked, so we do not commit to them either.  This means that `SIGHASH_NOINPUT` implicitly has a `SIGHASH_ANYONECANPAY`.\n\n(the BIP `SIGHASH_NOINPUT` in the eltoo pdf does not mention `hashSequence`, but it seems you managed to add that to your github BIP repository)\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Christian Decker",
                "date": "2018-05-03T10:28:53",
                "message_text_only": "ZmnSCPxj <ZmnSCPxj at protonmail.com> writes:\n> Ha, no, looking at the detailed `SIGHASH_NOINPUT` spec, `hashPrevouts`, which normally commits to the other inputs, is blanked, so we do not commit to them either.  This means that `SIGHASH_NOINPUT` implicitly has a `SIGHASH_ANYONECANPAY`.\n>\n> (the BIP `SIGHASH_NOINPUT` in the eltoo pdf does not mention `hashSequence`, but it seems you managed to add that to your github BIP repository)\n\nYeah, Russell O'Connor pointed that difference out and I saw no point in\ncommitting to the hashSequence, so I amended both the paper and the\nBIP. The paper will get updated in a few days with the typos people have\nfound, but I thought that the BIP discussion was more urgent to keep up\nto date, so I pushed that directly :-)"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-02T04:12:28",
                "message_text_only": "Good morning Christian,\n\nIt seems to me we can remove the trigger transaction.\n\nThe observation is that `nSequence`-encumbered transactions are only settlement transactions and not any update transactions.\n\nThus, it is not needed for a trigger transaction to exist, if we can make update transactions behave as the trigger transaction for its settlement transaction or any later update transaction.\n\nSo, let me propose, that the funding transaction outpoint could have the below SCRIPT:\n\n    OP_IF\n        # Settlement branch\n        OP_FALSE\n    OP_ELSE\n        # Update branch. 500,000,000 is the minimum state number under Decker-Russell-Osuntokun.\n        500000000 OP_CHECKLOCKTIMEVERIFY\n        2 Au Bu 2 OP_CHECKMULTISIGVERIFY\n    OP_ENDIF\n\nThus, the funding outpoint can be spent directly by any update transaction, which has `<sigAu> <sigBu> OP_FALSE` at its `witness` script.\n\nWhen creating a mutual close transaction, we simply use the update branch of the funding transaction above, again signing with `<sigAu> <sigBu> OP_FALSE`.  This does have the drawback that the mutual close transaction can be RBFed away by an update transaction (requiring more code to handle this case); but the latest update transaction can still be published in that case and we simply devolve down to the usual unilateral close branch.\n\nThe drawback is that the mutual close transaction increases by 1 weight unit (the `OP_FALSE` in the `witness` script) plus the size of the more complicated funding transaction SCRIPT, which is no longer a regular 2-of-2 P2WSH and indelibly marking it as a Decker-Russel-Osuntokun mutual close.  Taproot would help there by implicitly including a plain 2-of-2 fallback.\n\nFinally, we could argue that the mutual close transaction is expected to be the more common case, so increasing its size by even a small amount may outweigh the size reduction in the much rarer unilateral close case.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Carsten Otto",
                "date": "2018-05-03T09:00:23",
                "message_text_only": "Hi ZmnSCPxj, Christian, list,\n\nthe paper is a bit confusing regarding the setup transaction, as it is\nnot described formally. There also seems to be a mixup of \"setup\ntransaction\" and \"funding transaction\", also named T_{u,0} without\nshowing it in the diagrams.\n\nIn 3.1 the funding transaction is described as funding \"to a multisig\naddress\". In the description of trigger transactions the change is\ndescribed as \"The output from the setup transaction is changed into a\nsimple 2-of-2 multisig output\" - which it already is?\n\nAs far as I understand the situation, the the trigger transaction is\nneeded because the broadcasted initial/funding/setup transaction\nincludes an OP_CLV (possibly enforcing premature settlement). Removing\nOP_CLV, i.e. by changing it to a simple multisig output, seems to solve\nthis issue.\n\nCould you (Christian?) explain how the \"setup transaction\" is supposed\nto look like without the changes described in section 4.2?\n\nI like the idea proposed by ZmnSCPxj, but I'm not able to weigh the\npro/cons of both approaches. For a direct unilateral close both peers\nwould need to know the first update transaction and an attached\nsettlement transaction, which is comparable to the approach presented in\nthe paper (trigger transaction and settlement transaction).\n\nThe main advantage of getting rid of the trigger transaction seems to be\nthat only two transactions (latest update and settlement) have to be\ncommitted to the blockchain in the unilateral case, compared to three\n(trigger, latest update, settlement) in the approach presented in the\npaper.\n\nBye,\nCarsten\n-- \nandrena objects ag\nGanghoferstra\u00dfe 70\n80339 M\u00fcnchen\n\nhttp://www.andrena.de\n\nVorstand: Hagen Buchwald, Dr. Dieter Kuhn, Stefan Sch\u00fcrle\nAufsichtsratsvorsitzender: Rolf Hetzelberger\n\nSitz der Gesellschaft: Karlsruhe\nAmtsgericht Mannheim, HRB 109694\nUSt-IdNr. DE174314824\n\nBitte beachten Sie auch unsere anstehenden Veranstaltungen:\nhttp://www.andrena.de/events\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 195 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180503/571cf9bf/attachment.sig>"
            },
            {
                "author": "Carsten Otto",
                "date": "2018-05-03T09:03:51",
                "message_text_only": "Hi ZmnSCPxj, Christian, list,\n\nthe paper is a bit confusing regarding the setup transaction, as it is\nnot described formally. There also seems to be a mixup of \"setup\ntransaction\" and \"funding transaction\", also named T_{u,0} without\nshowing it in the diagrams.\n\nIn 3.1 the funding transaction is described as funding \"to a multisig\naddress\". In the description of trigger transactions the change is\ndescribed as \"The output from the setup transaction is changed into a\nsimple 2-of-2 multisig output\" - which it already is?\n\nAs far as I understand the situation, the trigger transaction is needed\nbecause the broadcasted initial/funding/setup transaction includes an\nOP_CLV, which then starts the timer and could lead to premature\nsettlement. Removing OP_CLV (and having in a transaction that is only\npublished later when it is needed), i.e. by changing it to a simple\nmultisig output, seems to solve this issue.\n\nCould you (Christian?) explain how the \"setup transaction\" is supposed\nto look like without the changes described in section 4.2?\n\nI like the idea proposed by ZmnSCPxj, but I'm not able to weigh the\npro/cons of both approaches. For a direct unilateral close both peers\nwould need to know the first update transaction and an attached\nsettlement transaction, which is comparable to the approach presented in\nthe paper (trigger transaction and settlement transaction).\n\nThe main advantage of getting rid of the trigger transaction seems to be\nthat only two transactions (latest update and settlement) have to be\ncommitted to the blockchain in the unilateral case, compared to three\n(trigger, latest update, settlement) in the approach presented in the\npaper. As a minor advantage the peers do not need to remember the\ntrigger transaction.\n\nBye,\nCarsten\n-- \nandrena objects ag\nGanghoferstra\u00dfe 70\n80339 M\u00fcnchen\n\nhttp://www.andrena.de\n\nVorstand: Hagen Buchwald, Dr. Dieter Kuhn, Stefan Sch\u00fcrle\nAufsichtsratsvorsitzender: Rolf Hetzelberger\n\nSitz der Gesellschaft: Karlsruhe\nAmtsgericht Mannheim, HRB 109694\nUSt-IdNr. DE174314824\n\nBitte beachten Sie auch unsere anstehenden Veranstaltungen:\nhttp://www.andrena.de/events\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 195 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180503/118d50e1/attachment.sig>"
            },
            {
                "author": "Christian Decker",
                "date": "2018-05-03T10:51:01",
                "message_text_only": "Carsten Otto <carsten.otto at andrena.de> writes:\n> the paper is a bit confusing regarding the setup transaction, as it is\n> not described formally. There also seems to be a mixup of \"setup\n> transaction\" and \"funding transaction\", also named T_{u,0} without\n> showing it in the diagrams.\n\nThe setup transaction is simply a transaction that spends some funds and\ncreates a single output, which has the script from Figure 2, but since\nthat would be a forward reference, I decided to handwave and call it a\nmultisig. A simple fix would be to change the setup phase bullet point\nat the beginning of section 3, would that be sufficient?\n\n> In 3.1 the funding transaction is described as funding \"to a multisig\n> address\". In the description of trigger transactions the change is\n> described as \"The output from the setup transaction is changed into a\n> simple 2-of-2 multisig output\" - which it already is?\n\nIf instead of calling it a multisig we call it a multiparty output and\nreference the script in Figure 2, that'd be addressed as well.\n\n> As far as I understand the situation, the trigger transaction is needed\n> because the broadcasted initial/funding/setup transaction includes an\n> OP_CLV, which then starts the timer and could lead to premature\n> settlement. Removing OP_CLV (and having in a transaction that is only\n> published later when it is needed), i.e. by changing it to a simple\n> multisig output, seems to solve this issue.\n>\n> Could you (Christian?) explain how the \"setup transaction\" is supposed\n> to look like without the changes described in section 4.2?\n\nWell, it has arbitrary inputs, and a single output with the script from\nFigure 2, in the non-trigger case, and in the trigger case it'd be just\na `2 A B 2 OP_CMSV`."
            },
            {
                "author": "Carsten Otto",
                "date": "2018-05-03T16:17:28",
                "message_text_only": "Hey Christian,\n\nOn Thu, May 03, 2018 at 12:51:01PM +0200, Christian Decker wrote:\n> The setup transaction is simply a transaction that spends some funds and\n> creates a single output, which has the script from Figure 2, but since\n> that would be a forward reference, I decided to handwave and call it a\n> multisig. A simple fix would be to change the setup phase bullet point\n> at the beginning of section 3, would that be sufficient?\n\nA first clarification, which just regards the naming, would be to use\njust one of \"setup transaction\" and \"funding transaction\". The symbol\nT_{u,0} is only used once, on page 6. I'd either remove it, or used it\nin other places (Fig. 1?), too.\n\nWithout your mail (thanks!) I did not see that the setup transaction\noutput is complicated (has branches) as shown in Fig. 2. A clarification\nat the named bullet point would indeed help (although I see the issue of\nforward references).\n\nOne idea (just to get my idea across, I'm sure you can do better):\n old: \"to a 2-of-2 multisig address\"\n new: \"to a script making use of 2-of-2 multisig\"\n\n> If instead of calling it a multisig we call it a multiparty output and\n> reference the script in Figure 2, that'd be addressed as well.\n\nI agree.\n\n> Well, it has arbitrary inputs, and a single output with the script from\n> Figure 2, in the non-trigger case, and in the trigger case it'd be just\n> a `2 A B 2 OP_CMSV`.\n\nGot it. Follow up question: would a second output (change) be OK? I see\nsome advantages, but I'm not sure if it would work.\n\nThanks\nCarsten\n-- \nandrena objects ag\nGanghoferstra\u00dfe 70\n80339 M\u00fcnchen\n\nhttp://www.andrena.de\n\nVorstand: Hagen Buchwald, Dr. Dieter Kuhn, Stefan Sch\u00fcrle\nAufsichtsratsvorsitzender: Rolf Hetzelberger\n\nSitz der Gesellschaft: Karlsruhe\nAmtsgericht Mannheim, HRB 109694\nUSt-IdNr. DE174314824\n\nBitte beachten Sie auch unsere anstehenden Veranstaltungen:\nhttp://www.andrena.de/events\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 195 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180503/e1f0770c/attachment.sig>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-04T07:20:57",
                "message_text_only": "Good morning Carsten,\n\n> > The setup transaction is simply a transaction that spends some funds and\n> > \n> > creates a single output, which has the script from Figure 2, but since\n> > \n> > that would be a forward reference, I decided to handwave and call it a\n> > \n> > multisig. A simple fix would be to change the setup phase bullet point\n> > \n> > at the beginning of section 3, would that be sufficient?\n> \n> A first clarification, which just regards the naming, would be to use\n> \n> just one of \"setup transaction\" and \"funding transaction\".\n\nCalling it \"funding transaction\" is my fault, as I prefer that term since it matches in spirit the funding transaction in LN-penalty (Poon-Dryja).\n\n> The symbol\n> \n> T_{u,0} is only used once, on page 6. I'd either remove it, or used it\n> \n> in other places (Fig. 1?), too.\n> \n> Without your mail (thanks!) I did not see that the setup transaction\n> \n> output is complicated (has branches) as shown in Fig. 2. A clarification\n> \n> at the named bullet point would indeed help (although I see the issue of\n> \n> forward references).\n> \n> One idea (just to get my idea across, I'm sure you can do better):\n> \n> old: \"to a 2-of-2 multisig address\"\n> \n> new: \"to a script making use of 2-of-2 multisig\"\n\nNote, that when we add the trigger transaction later in the paper, the funding transaction output becomes an ordinary 2-of-2 multisig address.  The complex script becomes relegated to the output of the trigger transaction.\n\nSo in the final design of eltoo, the funding transaction output is indeed a 2-of-2 multisig.\n\nThis is useful as the mutual close transaction (what the paper calls a \"final settlement\" transaction) just spends an ordinary 2-of-2 multisig and thus has a greater anonymity set, i.e. it prevents blockchain analysis from realizing that it was actually a channel (unless, of course, it was also tracking gossip from LN).\n\n> \n> > If instead of calling it a multisig we call it a multiparty output and\n> > \n> > reference the script in Figure 2, that'd be addressed as well.\n> \n> I agree.\n> \n> > Well, it has arbitrary inputs, and a single output with the script from\n> > \n> > Figure 2, in the non-trigger case, and in the trigger case it'd be just\n> > \n> > a `2 A B 2 OP_CMSV`.\n> \n> Got it. Follow up question: would a second output (change) be OK? I see\n> \n> some advantages, but I'm not sure if it would work.\n\nYes, it definitely would be OK.  In fact, it is good to talk about funding outpoints rather than funding transactions.  It is possible for a single Bitcoin transaction to serve as an anchor for several channels at once, and the BOLT spec supports this (but I know of no implementation that actually implements this).  Basically, the funding outpoint is some transaction output that pays to a 2-of-2 multisig and which will be spent using some offchain mechanism, or a mutual close transaction --- this is what I call the \"funding transaction pattern\".\n\nRegards,\nZmnSCPxj\n\n> \n> Thanks\n> \n> Carsten\n> \n> \n> -------------------------------------------------------------------------------------------------------------------------------------------------\n> \n> andrena objects ag\n> \n> Ganghoferstra\u00dfe 70\n> \n> 80339 M\u00fcnchen\n> \n> http://www.andrena.de\n> \n> Vorstand: Hagen Buchwald, Dr. Dieter Kuhn, Stefan Sch\u00fcrle\n> \n> Aufsichtsratsvorsitzender: Rolf Hetzelberger\n> \n> Sitz der Gesellschaft: Karlsruhe\n> \n> Amtsgericht Mannheim, HRB 109694\n> \n> USt-IdNr. DE174314824\n> \n> Bitte beachten Sie auch unsere anstehenden Veranstaltungen:\n> \n> http://www.andrena.de/events"
            },
            {
                "author": "Corn\u00e9 Plooy",
                "date": "2018-05-15T13:22:44",
                "message_text_only": "Hi Christian, ZmnSCPxj ,\n\n\n>> - The CSV-encumberance on settlement transactions, which are the ones\n>> which carry the contracts in the channel, affects all\n>> absolute-timelocked contracts transported on the channel.  Compare to\n>> Poon-Dryja, where commitment transactions themselves are unencumbered\n>> by CSV, and we simply insert the revocation to spends of the contracts\n>> being transported (i.e. the reason why we have HTLC-success and\n>> HTLC-timeout transactions in BOLT spec).\n> True, but as I argued in another mail, this is a fixed offset, that is\n> in the same range as today's CLTV deltas for some nodes. So for the\n> network of today using eltoo is roughly equivalent of adding another hop\n> to our path :-)\n\nLet me think how this is supposed to work (I can't find that other mail\nyou're referring to):\n\nI assume the HTLC outputs are in the settlement transaction. In case of\na unilateral close while there are HTLCs present, the following sequence\nhappens:\n1. The trigger transaction is broadcasted.\n2. One or more update transactions are broadcasted. These are not\naffected by the CSV condition and can be confirmed immediately. Once the\nlast update transaction gets confirmed, its CSV clock for the settlement\ntransaction starts.\n3. The settlement transaction (containing HTLCs) can be broadcasted, but\nit will not be confirmed before the CSV time-out of the last update\ntransaction.\n4. At this point, the HTLC conditions become relevant. Before the HTLC\ntime-out, the receiving side can access the funds with the preimage.\nAfter the timeout, the sending side can access the funds without the\npreimage.\n\nIf the HTLC time-out is before the start of step 4, then the sending\nside can try to get the funds back, even if the preimage was already\nreleased much earlier. For the safety of the receiving side, this\nsituation must be avoided. So, if T is the (absolute) HTLC time-out\ntime, S is the CSV time-out period and C is the expected worst-case time\ninvolved in confirming everything, then the receiving side of a HTLC\nmust drop the channel on-chain before T - S - C. Obviously, HTLCs for\nwhich this time has already passed on creation must be rejected.\nDropping on-chain is no longer needed if the HTLC is removed by a\nchannel update before T - S - C.\n\nWhat happens in case of a transaction time-out? Let's number the nodes\n0, 1, ... on the route, and Tn the HTLC time-out between nodes n and\nn+1. For simplicity, assume the same S+C for all channels. If node n\nbecomes unresponsive after accepting its incoming HTLC, then two moments\nin time are relevant:\n* T(n-1)\n* T(n-2) - S - C\nIn case T(n-1) happens first, node n-1 knows it doesn't have to pay\noutgoing funds, so it can cooperatively cancel the incoming HTLC. This\nwill propagate back immediately, keeping all channels open.\nIn case T(n-2) - S - C happens first, node n-1 has to close its channel\nwith n-2. On-chain, the HTLC conditions continue to apply, so for the\nrest, nothing changes. As soon as T(n-1) is reached, node n-1 knows the\ntransaction has timed out, but it no longer has a way to propagate this\nknowledge to node n-2. For node n-2, two times are relevant:\n* T(n-2)\n* T(n-3) - S - C\n... and the same steps are repeated. In this case, a transaction\ntime-out leads to closing all channels on the route. This is a major DoS\nattack vector, and must be prevented.\n\nTherefore, we must set T(n-1) > T(n-2) - S - C, so the HTLC time-out\nincrement must be at least S+C.\n\nTheoretically, it seems workable. What would be a practical value for S\n(the CSV time-out)? In the absence of watch towers, you'd want to set it\nto at least a couple of weeks, to allow yourself to go off-line on\nholiday without worrying about computer issues. However, a time-out\nincrement of a couple of weeks *per hop* for the HTLCs adds up really\nquickly to unpractical values. Maybe the conclusion simply is that we\nreally really need watchtowers? Otherwise, only professional parties\nthat can manage near-100% uptime can safely perform transaction routing.\n\nCJP"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-15T13:41:45",
                "message_text_only": "Good morning Corne,\n\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n\nOn May 15, 2018 9:22 PM, Corn\u00e9 Plooy <corne at bitonic.nl> wrote:\n\n> Hi Christian, ZmnSCPxj ,\n> \n> > > -   The CSV-encumberance on settlement transactions, which are the ones\n> > >     \n> > >     which carry the contracts in the channel, affects all\n> > >     \n> > >     absolute-timelocked contracts transported on the channel. Compare to\n> > >     \n> > >     Poon-Dryja, where commitment transactions themselves are unencumbered\n> > >     \n> > >     by CSV, and we simply insert the revocation to spends of the contracts\n> > >     \n> > >     being transported (i.e. the reason why we have HTLC-success and\n> > >     \n> > >     HTLC-timeout transactions in BOLT spec).\n> > >     \n> > >     True, but as I argued in another mail, this is a fixed offset, that is\n> > >     \n> > >     in the same range as today's CLTV deltas for some nodes. So for the\n> > >     \n> > >     network of today using eltoo is roughly equivalent of adding another hop\n> > >     \n> > >     to our path :-)\n> > >     \n> \n> Let me think how this is supposed to work (I can't find that other mail\n> \n> you're referring to):\n\nI believe it is on bitcoin-dev: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-May/015916.html\n\nChristian argues that the CSV delay (S in your treatment) is shared across all nodes in the route, but is not additive unlike the CLTV delay.  In a heterogeneous network, the S is the maximum S among nodes along the route.\n\nI argue elsewhere (https://lists.linuxfoundation.org/pipermail/lightning-dev/2018-May/001230.html) that even if the above is true, it does not mesh well with current routing algorithms; most current \"shortest-path\" algorithms do not handle well anything that is not additive-cost, and S is effectively the max S along the route.\n\nC-lightning is considering to use a \"find all paths\" algorithm instead, which would simply filter out during route construction any routes that exceed maximum total delay and maximum forwarding fee.  This probably works better with \"try another route\" technique imposed by Lightning forwarding, and would seamlessly support extensions like the above max S or the older \"negative fees\" idea better than \"shortest-path\" algorithms; this is done by the simple technique of not actually needing the shortest path (improving privacy by not using the shortest path (we currently add virtual randomized cost multipliers in routing) is already what we do anyway, which is another reason to switch from a \"shortest-path\" algorithm).\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "eltoo: A Simplified update Mechanism for Lightning and Off-Chain Contracts",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Carsten Otto",
                "Corn\u00e9 Plooy",
                "Christian Decker",
                "ZmnSCPxj"
            ],
            "messages_count": 15,
            "total_messages_chars_count": 48315
        }
    },
    {
        "title": "[Lightning-dev] Mitigations for loop attacks",
        "thread_messages": [
            {
                "author": "Jim Posen",
                "date": "2018-05-02T06:59:16",
                "message_text_only": "I have been thinking a lot lately about attacks on lightning routing nodes,\nthe worst of which is the so-called Loop Attack [1]. See the mailing list\nthread for more details, but the basic idea is that a sender and receiver\ncollude to create a long circuit and refuse to settle or fail the HTLC at\nthe end until the last possible moment. The attack is particularly\ninsidious for a few reasons. Firstly, an attack on a multi-hop route gives\nthe attacker leverage, so that their funds locked in one channel cause\nfunds to be locked up in every channel on the route. Second, the onion\nrouting makes it difficult to attribute blame to a single node and defend\nagainst a repeated attack.\n\nFirst, I'll note that the case where a sender and recipient collude is a\nmore active variant on an attack that any node in a route can perform. Even\nif a node does not initiate the payment, it can just accept offered HTLCs\nin the route, not forward them, and wait until the offered HTLC almost\nexpires before failing. As noted above, the prior node in the route cannot\ndetermine whether the attacking node is at fault or one downstream from it.\nEven the sender may not be able to determine where in the route the payment\nfailed because of the issue where the obfuscated error reason is not MAC'ed\non every hop.\n\nThere are two directions of solutions I have heard: 1) protocol support for\ndecrypting the onion route if the HTLC is kept in-flight for too long 2)\nrequiring fees even if the payment fails as a cost to the attacker 3) some\nsort of reputation system for nodes.\n\nOption 1 I'm afraid may be quite complex. Say this mechanism kicks in and\nall nodes in the circuit deobfuscate the route and are able to see the\ndelays at each hop. The outcome we hope for is that there is one node\nclearly to blame and the prior hop in the route fails all channels with\nthem. However, the attacker can of course control multiple successive hops\nin the route, one that looks innocent in front of one that looks guilty,\nthen keep the channel alive and try again. So then all nodes need to keep a\nrecord of the full circuits and iteratively shift blame up the chain if bad\nHTLCs keep going through those channels.\n\nOption 2 is also problematic because it only protects against the case\nwhere the sender is colluding with the receiver, and not where a routing\nnode is opportunistically delaying payments. This would, however, likely be\nsuccessful against nodes being annoying and sending tons of payments with\nrandomly generated payment hashes in order to \"ping\" a circuit.\n\nOption 3 has become my preferred solution. The idea is that that for each\nnode that one has channels with, it only forwards payments through them if\nthey have a good history, otherwise it fails the payment. Notably, we\nignore whether the downstream hop is directly responsible for delaying a\npayment or whether they are simply willing to forward to another node that\nis intentionally delaying -- both should be considered bad behavior. In my\nopinion, this type of solution fits best into the Lightning model of\nindependent, linked channels where each node has private contracts with its\ndirect peers. It also is the simplest in the context of onion routing\nbecause if you are offered an HTLC to route, the only decision you can make\nis whether to forward it or fail it based on the amount, previous hop, and\nnext hop. When I refer to \"reputation\" hereafter, I do not mean a global\nreputation that is gossiped about -- just a local view of a peer's history.\n\nFor a Sybil-resistant reputation system, we can use money as a way of\nmeasuring reputation. Fees collected through routing payments across\nchannels raise the reputation of the channel peer. You lose reputation by\naccepting an HTLC and not failing or fulfilling it quickly, proportional to\nthe value of the HTLC and time to respond. What this essentially means is\nthat if an attacker wants people to send them HTLCs to delay, they pay a\ncertain amount in fees over time. So each node will track for each peer 1)\ntotal fees collected from forwarding on routes where that peer is the prior\nhop 2) total fees collect from forwarding on routes where that peer is the\nnext hop 3) total value times time of money locked in offered HTLCs on\nchannels to the peer. To track the third, as soon as an offered HTLC is\nlocked in by the peer, the node starts a timer. When the HTLC is settled or\nfailed or handled on-chain, the timer stops and the node registers the\ntotal elapsed time multiplied by the value of the HTLC.\n\nA very simple strategy would be to have two factors, R_inbound and\nR_outbound and calculate reputation per peer as R_inbound * total inbound\nfees + R_outbound * total outbound fees - total Bitcoin-seconds locked in\nan HTLC. When forwarding a payment, you calculate the worst case reputation\nloss, HTLC value * (CLTV - current block height) * 10 min / block, and fail\nthe payment if that value is greater than their current score. Effectively,\nreputation scores are the maximum number of Bitcoin-seconds on could waste\nin HTLCs as the downstream node in a channel. If you think, for example,\nhaving 1 BTC for 1 hour is worth 1 satoshi, you might set R_inbound =\nR_outbound = 10^8. Honest nodes should earn reputation passively by\nforwarding payments and failing/fulfilling quickly, while an attacker would\nlose reputation and have to pay fees to re-earn it. Furthermore, high\nreputation nodes should be able to earn reputation faster since peers will\nbe willing to forward higher value HTLCs through them.\n\nWe want an additional property from this system though, which is that an\nattacker be limited in their ability to disrupt network connectivity and\ninduce throttling on other channels. The above scheme has the problem that\nfor every unit of reputation the attacker loses, each upstream hop also\nloses approximately the same amount -- they effectively get leverage on the\nability to reduce reputation. So instead, consider a modification: each hop\nloses a quantity of reputation units equal to the *total* amount of\nBitcoin-seconds lost upstream. So say there's a payment A -> B -> C -> D,\nwhere the CLTV decrements by 12 and the amount decrements by 10 satoshis at\neach hop, with a terminal CLTV of 12 and a terminal amount of 100. So A\ntells B \"Here's an HTLC for 120 satoshis, CLTV is in 36 blocks, and you\nwill lose reputation at a rate of 120 satoshi-seconds per second of delay\".\nB calculates the maximum he could lose by forwarding to C as 120 * 36\nblocks * 10 min / block. Assuming C has a high enough reputation score, B\nwill offer an HTLC of 110 satoshis, CLTV of 24 blocks, reputation rate of\n120 + 110 = 230 satoshi-seconds per second. And finally C requires D to\nstake reputation at a rate of 230 + 100 = 330 satoshi-seconds / second. The\neffect is that if any node withholds the preimage, they lose an amount of\nreputation equal to all of the reputation lost upstream combined, removing\nany attack leverage.\n\nThis strategy has some downsides, namely in terms of centralization and\nprivacy. Telling the downstream node the rate at which they must stake\nreputation may allow them to determine the number of upstream hops in the\ncircuit. To obfuscate this, the sender may require an addition reputation\nstake, but takes a higher risk of the payment failing somewhere downstream.\nThe more obvious issue is centralization -- if payments are getting\nthrottled by reputation, there is even more of an incentive to route\nthrough a small number of channels and decrease the length of payment\ncircuits. While this is a concern, the degree to while throttling happens\nin controlled by the R parameters (the ratio at which fees contribute to\nreputation). If R is high and reputation is abundant, then there will be\nlittle throttling. But if a node finds itself under attack, it can raise\nits R parameters and start throttling bad peers.\n\nIn summary, I think a simple local reputation mechanism of this form could\nbe implemented with only minor changes to the BOLT spec and could provide\ndecent resistance against DOS/loop attacks. Sorry for the excessively long\nemail.\n\n[1] https://lists.linuxfoundation.org/pipermail/lightning-dev/2015-August/\n000135.html\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180501/35997f70/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2018-05-09T06:50:27",
                "message_text_only": "Jim Posen <jim.posen at gmail.com> writes:\n> There are two directions of solutions I have heard: 1) protocol support for\n> decrypting the onion route if the HTLC is kept in-flight for too long 2)\n> requiring fees even if the payment fails as a cost to the attacker 3) some\n> sort of reputation system for nodes.\n>\n> Option 1 I'm afraid may be quite complex. Say this mechanism kicks in and\n> all nodes in the circuit deobfuscate the route and are able to see the\n> delays at each hop. The outcome we hope for is that there is one node\n> clearly to blame and the prior hop in the route fails all channels with\n> them. However, the attacker can of course control multiple successive hops\n> in the route, one that looks innocent in front of one that looks guilty,\n> then keep the channel alive and try again. So then all nodes need to keep a\n> record of the full circuits and iteratively shift blame up the chain if bad\n> HTLCs keep going through those channels.\n\nYou missed the vital detail: that you must prove channel closure if you\ncan't unpeel the onion further.  That *will* hit an unresponsive party\nwith a penalty.[1]\n\n> Option 2 is also problematic because it only protects against the case\n> where the sender is colluding with the receiver, and not where a routing\n> node is opportunistically delaying payments. This would, however, likely be\n> successful against nodes being annoying and sending tons of payments with\n> randomly generated payment hashes in order to \"ping\" a circuit.\n\nThe models we tried in Milan all created an incentive to fail payments,\nwhich is a non-starter.\n\n> Option 3 has become my preferred solution. The idea is that that for each\n> node that one has channels with, it only forwards payments through them if\n> they have a good history, otherwise it fails the payment. Notably, we\n> ignore whether the downstream hop is directly responsible for delaying a\n> payment or whether they are simply willing to forward to another node that\n> is intentionally delaying -- both should be considered bad behavior. In my\n> opinion, this type of solution fits best into the Lightning model of\n> independent, linked channels where each node has private contracts with its\n> direct peers. It also is the simplest in the context of onion routing\n> because if you are offered an HTLC to route, the only decision you can make\n> is whether to forward it or fail it based on the amount, previous hop, and\n> next hop. When I refer to \"reputation\" hereafter, I do not mean a global\n> reputation that is gossiped about -- just a local view of a peer's history.\n\nThis seems like we'd need some serious evaluation to show that this\nworks, because the risks are very high.\n\nI can destroy your node's reputation by routing crap through you; even\nif it costs me marginaly more reputation than it does you, that just\nmeans that the largest players can force failure upon smaller players,\ncentralizing the network.  And I think trying to ensure that it costs me\nmore reputation than the sum of downstream reputation loss leaks too\nmuch information\n\nIf the system doesn't work perfectly, it creates incentive to\nde-anonymize payments so you can determine which are likely to fail, and\nalso means nodes are safest not forwarding payments, lest they ruin\ntheir reputatation.  But nodes which don't forward have much less\nprivacy.\n\nBut I agree, all solutions I've seen to this problem are bad in\ndifferent ways.\n\nCheers,\nRusty.\n\n[1] Proving that you've committed to a particular HTLC in a channel is\n    difficult, because of dust HTLCs[2] and partially-committed ones[3].\n\n[2] We could have an OP_RETURN output which was a merkle tree of trimmed\n    HTLCs to cover the dust case.\n\n[3] You committed to an HTLC but the peer didn't respond; you have no\n    proof.  So you also need to present a merkle tree of uncommitted\n    HTLCs (limited, say, to 16) otherwise you can create 1M HTLCs, then\n    claim they were all in that one channel close."
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-09T07:31:56",
                "message_text_only": "Good morning Rusty, Jim, and list,\n\n> I can destroy your node's reputation by routing crap through you; even\n> \n> if it costs me marginaly more reputation than it does you, that just\n> \n> means that the largest players can force failure upon smaller players,\n> \n> centralizing the network.\n\nMy understanding of the proposal was that reputation loss would occur only if the reply (`update_htlc_fail` or `update_htlc_success`) is delayed; this means that for you to force me to lose reputation, you need to somehow make me delay my reply.  In particular if you do simple things like give me an invalid onion, or make me forward to a payee who does not know the preimage, I do not lose reputation by replying very quickly with an `update_htlc_fail`.\n\nOf course, a large player could force reputation loss by delaying reply when they receive, and having patsy nodes route to them.  So for instance if it is Jim -> ZmnSCPxj -> Rusty, and Rusty activates the Blockstream-takes-over-the-world Apocalypse program, the Rusty node would then delay for a long time before replying, which makes my reputation suffer.  But it also makes Rusty reputation suffer even more and my reaction would be that, the next time Jim hands me an HTLC that forwards to Rusty, I would instead quickly `update_htlc_fail` back to Jim (which does not lose me significant reputation due to my quick response) than risk forwarding it to you, since you have a reputation for being slow and unresponsive.\n\nIndeed, another aspect of Jim proposal is that it is extremely local: if Jim has no channel to Rusty, then Jim has no opinion about Rusty, only about ZmnSCPxj.  However, ZmnSCPxj does have an opinion about Rusty, as ZmnSCPxj has channel with Rusty.  If I suffer too much reputation loss due to Rusty, my opinion of Rusty drops even faster, and I decide to `update_htlc_fail` in order to prevent Jim opinion of me from dropping too much that Jim decides not to forward to me (if I have other channels with more reasonable nodes).\n\nBut it also looks more and more like a policy of \"just `update_htlc_fail`\" keeps our reputation high: indeed never accepting a forwarding attempt would ensure reputation.\n\nHowever, earning via fees should help provide incentive against \"Just `update_htlc_fail`\" always.  If the goal is \"how do I earn money fastest\" then there is some optimal threshhold of risk-of-reputation-loss vs. fee-earnings-if-I-forward that is unlikely to be near the \"Just fail it\" spectrum, but somewhere in between.  We hope.\n\n> And I think trying to ensure that it costs me\n> \n> more reputation than the sum of downstream reputation loss leaks too\n> \n> much information\n\nYes, this is a major drawback of the proposal.  The rate at which the sender of the HTLC threatens me with reputation loss lets me estimate my distance from the ultimate sender of the funds.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Jim Posen",
                "date": "2018-05-09T17:23:38",
                "message_text_only": "Thanks for the thoughtful responses.\n\n> You missed the vital detail: that you must prove channel closure if you\n> can't unpeel the onion further.  That *will* hit an unresponsive party\n> with a penalty.\n\nAh, that is a good point. I still find the proposal overall worryingly\ncomplex in terms of communication overhead, time it takes to prove channel\nclosure, all of your points in [1], [2], [3], etc. Furthermore, this\nmandates that immediate channel closure is the only allowed reaction to a\nparty delaying an HTLC for a time period above a threshold -- the node\nreputation approach gives more discretion to the preceding hop.\nDeobfuscating the route may turn out to be the right option, but I think\nthe reputation system has certain advantages over this.\n\n> The models we tried in Milan all created an incentive to fail payments,\n> which is a non-starter.\n\nDo you mind elaborating or summarizing the reasons? The way I'm analyzing\nit, even if there's a nominal spam fee paid to routing nodes that fail\npayments, as long as it's low enough (say 2-5% for arguments sake), the\nnodes still have more to gain by forwarding the payment and earning the\nfull fee on a completed payment, and possibly the reputation boost\nassociated with completing a payment if that system was in effect.\nMoreover, a node that constantly fails payments will be blacklisted by the\nsender eventually and stop receiving HTLCs from them at all. Overall, I\ndon't think this is a profitable strategy. Furthermore, I think it works\nquite well in combination with the reputation system.\n\n> This seems like we'd need some serious evaluation to show that this\n> works, because the risks are very high.\n\nI agree that it needs to be evaluated. I may start working on some network\nsimulations to test various DOS mitigation strategies.\n\n> I can destroy your node's reputation by routing crap through you; even\n> if it costs me marginaly more reputation than it does you, that just\n> means that the largest players can force failure upon smaller players,\n> centralizing the network.  And I think trying to ensure that it costs me\n> more reputation than the sum of downstream reputation loss leaks too\n> much information\n\nI will add to ZmnSCPxj's response, which is mostly on point. The key here\nis that the only way to lose significant reputation is to delay a payment\nyourself or forward to a malicious downstream that delays -- neither of\nthese can be forced by the sender alone. This amounts to a system where you\nare on the hook for any malicious behavior of your downstream peers, which\nis why you must keep a reputation score for each which they earn over time.\nThis should keep all links in the network high quality and quickly\ndisconnect off delaying nodes if the incentives are right.\n\nWhile I agree that a lot of reputation is leaked by aggregating the losses\nalong the route, this serves exactly to prevent large nodes with high\nreputation from ruining links elsewhere. There are two things a node\nlooking to cause reputation loss could do. 1) Identify a node (not itself)\nit thinks will delay a payment and send to them. This locks up funds on\ntheir behalf, but is actually good behavior because it identifies a faulty\nnode and rightfully forces a loss in their reputation, eventually resulting\nin them being booted from the network. Everyone upstream loses some\nreputation for having connectivity to them, but less because of the loss\naggregation along the route. 2) Delay a payment oneself and force upstream\nreputation loss. This is why I think it's important that the reputation\nloss aggregate so that the malicious party loses the most.\n\nAs for the amount of information leaked, yes, it helps determine the number\nof upstream hops in a route. However, the CLTV values help determine the\nnumber of downstream hops in a route in exactly the same way. I see these\nas symmetric in a sense.\n\nTo address ZmnSCPxj's point:\n\n> But it also looks more and more like a policy of \"just\n`update_htlc_fail`\" keeps our reputation high: indeed never accepting a\nforwarding attempt would ensure reputation.\n> However, earning via fees should help provide incentive against \"Just\n`update_htlc_fail`\" always.  If the goal is \"how do I earn money fastest\"\nthen there is some optimal threshhold > of risk-of-reputation-loss vs.\nfee-earnings-if-I-forward that is unlikely to be near the \"Just fail it\"\nspectrum, but somewhere in between.  We hope.\n\nThis is exactly the question that your local view of peer reputations helps\nsolve: are the potential fees here worth the risk of forwarding this\npayment to this downstream? If their reputation is high, then you will want\nto forward because you think there's a low chance of you incurring\nreputation loss. If their reputation is low and the HTLC value is too high,\nyou will fail it. So I disagree that \"just `update_htlc_fail`\" is an\noptimal strategy. Consider as well that all fees you earn on successful\npayments are profit to you as well as a reputation boost in the view of\nboth of your peers. So in order to earn reputation, you have to forward\npayments. The key is not forwarding through malicious peers.\n\n-jimpo\n\nOn Wed, May 9, 2018 at 12:31 AM, ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Rusty, Jim, and list,\n>\n> > I can destroy your node's reputation by routing crap through you; even\n> >\n> > if it costs me marginaly more reputation than it does you, that just\n> >\n> > means that the largest players can force failure upon smaller players,\n> >\n> > centralizing the network.\n>\n> My understanding of the proposal was that reputation loss would occur only\n> if the reply (`update_htlc_fail` or `update_htlc_success`) is delayed; this\n> means that for you to force me to lose reputation, you need to somehow make\n> me delay my reply.  In particular if you do simple things like give me an\n> invalid onion, or make me forward to a payee who does not know the\n> preimage, I do not lose reputation by replying very quickly with an\n> `update_htlc_fail`.\n>\n> Of course, a large player could force reputation loss by delaying reply\n> when they receive, and having patsy nodes route to them.  So for instance\n> if it is Jim -> ZmnSCPxj -> Rusty, and Rusty activates the\n> Blockstream-takes-over-the-world Apocalypse program, the Rusty node would\n> then delay for a long time before replying, which makes my reputation\n> suffer.  But it also makes Rusty reputation suffer even more and my\n> reaction would be that, the next time Jim hands me an HTLC that forwards to\n> Rusty, I would instead quickly `update_htlc_fail` back to Jim (which does\n> not lose me significant reputation due to my quick response) than risk\n> forwarding it to you, since you have a reputation for being slow and\n> unresponsive.\n>\n> Indeed, another aspect of Jim proposal is that it is extremely local: if\n> Jim has no channel to Rusty, then Jim has no opinion about Rusty, only\n> about ZmnSCPxj.  However, ZmnSCPxj does have an opinion about Rusty, as\n> ZmnSCPxj has channel with Rusty.  If I suffer too much reputation loss due\n> to Rusty, my opinion of Rusty drops even faster, and I decide to\n> `update_htlc_fail` in order to prevent Jim opinion of me from dropping too\n> much that Jim decides not to forward to me (if I have other channels with\n> more reasonable nodes).\n>\n> But it also looks more and more like a policy of \"just `update_htlc_fail`\"\n> keeps our reputation high: indeed never accepting a forwarding attempt\n> would ensure reputation.\n>\n> However, earning via fees should help provide incentive against \"Just\n> `update_htlc_fail`\" always.  If the goal is \"how do I earn money fastest\"\n> then there is some optimal threshhold of risk-of-reputation-loss vs.\n> fee-earnings-if-I-forward that is unlikely to be near the \"Just fail it\"\n> spectrum, but somewhere in between.  We hope.\n>\n> > And I think trying to ensure that it costs me\n> >\n> > more reputation than the sum of downstream reputation loss leaks too\n> >\n> > much information\n>\n> Yes, this is a major drawback of the proposal.  The rate at which the\n> sender of the HTLC threatens me with reputation loss lets me estimate my\n> distance from the ultimate sender of the funds.\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180509/1cec1f17/attachment-0001.html>"
            },
            {
                "author": "Jim Posen",
                "date": "2018-05-09T17:41:56",
                "message_text_only": "One more point in terms of information leakage is that noise can be added\nto the \"this is the rate that you'll lose reputation at\" field to help\nobfuscate the number of upstream hops. I proposed setting it to \"this is\nthe upstream rate that I'm losing reputation at\" + downstream HTLC value,\nbut a node can decide to add noise. If they make it too low however,\nthere's a risk of insufficiently punishing bad nodes and if they make it\ntoo high, there's a heightened risk that the payment fails because the\ndownstream reputation is insufficient along the route.\n\nThis is why I say it's kind of symmetric to the CLTV value: if the delta is\ntoo low, there's risk of loss of funds, if the delta is too high, someone\nmight decide to fail the payment instead of taking the delay risk.\n\nOn Wed, May 9, 2018 at 10:23 AM, Jim Posen <jim.posen at gmail.com> wrote:\n\n> Thanks for the thoughtful responses.\n>\n> > You missed the vital detail: that you must prove channel closure if you\n> > can't unpeel the onion further.  That *will* hit an unresponsive party\n> > with a penalty.\n>\n> Ah, that is a good point. I still find the proposal overall worryingly\n> complex in terms of communication overhead, time it takes to prove channel\n> closure, all of your points in [1], [2], [3], etc. Furthermore, this\n> mandates that immediate channel closure is the only allowed reaction to a\n> party delaying an HTLC for a time period above a threshold -- the node\n> reputation approach gives more discretion to the preceding hop.\n> Deobfuscating the route may turn out to be the right option, but I think\n> the reputation system has certain advantages over this.\n>\n> > The models we tried in Milan all created an incentive to fail payments,\n> > which is a non-starter.\n>\n> Do you mind elaborating or summarizing the reasons? The way I'm analyzing\n> it, even if there's a nominal spam fee paid to routing nodes that fail\n> payments, as long as it's low enough (say 2-5% for arguments sake), the\n> nodes still have more to gain by forwarding the payment and earning the\n> full fee on a completed payment, and possibly the reputation boost\n> associated with completing a payment if that system was in effect.\n> Moreover, a node that constantly fails payments will be blacklisted by the\n> sender eventually and stop receiving HTLCs from them at all. Overall, I\n> don't think this is a profitable strategy. Furthermore, I think it works\n> quite well in combination with the reputation system.\n>\n> > This seems like we'd need some serious evaluation to show that this\n> > works, because the risks are very high.\n>\n> I agree that it needs to be evaluated. I may start working on some network\n> simulations to test various DOS mitigation strategies.\n>\n> > I can destroy your node's reputation by routing crap through you; even\n> > if it costs me marginaly more reputation than it does you, that just\n> > means that the largest players can force failure upon smaller players,\n> > centralizing the network.  And I think trying to ensure that it costs me\n> > more reputation than the sum of downstream reputation loss leaks too\n> > much information\n>\n> I will add to ZmnSCPxj's response, which is mostly on point. The key here\n> is that the only way to lose significant reputation is to delay a payment\n> yourself or forward to a malicious downstream that delays -- neither of\n> these can be forced by the sender alone. This amounts to a system where you\n> are on the hook for any malicious behavior of your downstream peers, which\n> is why you must keep a reputation score for each which they earn over time.\n> This should keep all links in the network high quality and quickly\n> disconnect off delaying nodes if the incentives are right.\n>\n> While I agree that a lot of reputation is leaked by aggregating the losses\n> along the route, this serves exactly to prevent large nodes with high\n> reputation from ruining links elsewhere. There are two things a node\n> looking to cause reputation loss could do. 1) Identify a node (not itself)\n> it thinks will delay a payment and send to them. This locks up funds on\n> their behalf, but is actually good behavior because it identifies a faulty\n> node and rightfully forces a loss in their reputation, eventually resulting\n> in them being booted from the network. Everyone upstream loses some\n> reputation for having connectivity to them, but less because of the loss\n> aggregation along the route. 2) Delay a payment oneself and force upstream\n> reputation loss. This is why I think it's important that the reputation\n> loss aggregate so that the malicious party loses the most.\n>\n> As for the amount of information leaked, yes, it helps determine the\n> number of upstream hops in a route. However, the CLTV values help determine\n> the number of downstream hops in a route in exactly the same way. I see\n> these as symmetric in a sense.\n>\n> To address ZmnSCPxj's point:\n>\n> > But it also looks more and more like a policy of \"just\n> `update_htlc_fail`\" keeps our reputation high: indeed never accepting a\n> forwarding attempt would ensure reputation.\n> > However, earning via fees should help provide incentive against \"Just\n> `update_htlc_fail`\" always.  If the goal is \"how do I earn money fastest\"\n> then there is some optimal threshhold > of risk-of-reputation-loss vs.\n> fee-earnings-if-I-forward that is unlikely to be near the \"Just fail it\"\n> spectrum, but somewhere in between.  We hope.\n>\n> This is exactly the question that your local view of peer reputations\n> helps solve: are the potential fees here worth the risk of forwarding this\n> payment to this downstream? If their reputation is high, then you will want\n> to forward because you think there's a low chance of you incurring\n> reputation loss. If their reputation is low and the HTLC value is too high,\n> you will fail it. So I disagree that \"just `update_htlc_fail`\" is an\n> optimal strategy. Consider as well that all fees you earn on successful\n> payments are profit to you as well as a reputation boost in the view of\n> both of your peers. So in order to earn reputation, you have to forward\n> payments. The key is not forwarding through malicious peers.\n>\n> -jimpo\n>\n> On Wed, May 9, 2018 at 12:31 AM, ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n>\n>> Good morning Rusty, Jim, and list,\n>>\n>> > I can destroy your node's reputation by routing crap through you; even\n>> >\n>> > if it costs me marginaly more reputation than it does you, that just\n>> >\n>> > means that the largest players can force failure upon smaller players,\n>> >\n>> > centralizing the network.\n>>\n>> My understanding of the proposal was that reputation loss would occur\n>> only if the reply (`update_htlc_fail` or `update_htlc_success`) is delayed;\n>> this means that for you to force me to lose reputation, you need to somehow\n>> make me delay my reply.  In particular if you do simple things like give me\n>> an invalid onion, or make me forward to a payee who does not know the\n>> preimage, I do not lose reputation by replying very quickly with an\n>> `update_htlc_fail`.\n>>\n>> Of course, a large player could force reputation loss by delaying reply\n>> when they receive, and having patsy nodes route to them.  So for instance\n>> if it is Jim -> ZmnSCPxj -> Rusty, and Rusty activates the\n>> Blockstream-takes-over-the-world Apocalypse program, the Rusty node\n>> would then delay for a long time before replying, which makes my reputation\n>> suffer.  But it also makes Rusty reputation suffer even more and my\n>> reaction would be that, the next time Jim hands me an HTLC that forwards to\n>> Rusty, I would instead quickly `update_htlc_fail` back to Jim (which does\n>> not lose me significant reputation due to my quick response) than risk\n>> forwarding it to you, since you have a reputation for being slow and\n>> unresponsive.\n>>\n>> Indeed, another aspect of Jim proposal is that it is extremely local: if\n>> Jim has no channel to Rusty, then Jim has no opinion about Rusty, only\n>> about ZmnSCPxj.  However, ZmnSCPxj does have an opinion about Rusty, as\n>> ZmnSCPxj has channel with Rusty.  If I suffer too much reputation loss due\n>> to Rusty, my opinion of Rusty drops even faster, and I decide to\n>> `update_htlc_fail` in order to prevent Jim opinion of me from dropping too\n>> much that Jim decides not to forward to me (if I have other channels with\n>> more reasonable nodes).\n>>\n>> But it also looks more and more like a policy of \"just\n>> `update_htlc_fail`\" keeps our reputation high: indeed never accepting a\n>> forwarding attempt would ensure reputation.\n>>\n>> However, earning via fees should help provide incentive against \"Just\n>> `update_htlc_fail`\" always.  If the goal is \"how do I earn money fastest\"\n>> then there is some optimal threshhold of risk-of-reputation-loss vs.\n>> fee-earnings-if-I-forward that is unlikely to be near the \"Just fail it\"\n>> spectrum, but somewhere in between.  We hope.\n>>\n>> > And I think trying to ensure that it costs me\n>> >\n>> > more reputation than the sum of downstream reputation loss leaks too\n>> >\n>> > much information\n>>\n>> Yes, this is a major drawback of the proposal.  The rate at which the\n>> sender of the HTLC threatens me with reputation loss lets me estimate my\n>> distance from the ultimate sender of the funds.\n>>\n>> Regards,\n>> ZmnSCPxj\n>>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180509/1894d9e0/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-10T04:40:19",
                "message_text_only": "Good morning Jim,\n\n> One more point in terms of information leakage is that noise can be added to the \"this is the rate that you'll lose reputation at\" field to help obfuscate the number of upstream hops. I proposed setting it to \"this is the upstream rate that I'm losing reputation at\" + downstream HTLC value, but a node can decide to add noise. If they make it too low however, there's a risk of insufficiently punishing bad nodes and if they make it too high, there's a heightened risk that the payment fails because the downstream reputation is insufficient along the route.\n\nI was about to propose this.\n\nIndeed, I intend to implement CLTV-delay randomization for payments in C-lightning, i.e. \"shadow routes\", since that is a way to obfuscate the intermediate node distance from the payee.  Randomizing the reputation-loss-rate is similar.\n\nThe concern however is that the CLTV already partly leaks the distance from the payee, whereas the reputation-loss-rate leaks distance from the payer.  It is often not interesting to know that some entity is getting paid, but it probably far more interesting to know WHO paid WHO, so leaking both distances simultaneously is more than twice as worse as leaking just one distance.\n\nRegards,\nZmnSCPxj\n\n> This is why I say it's kind of symmetric to the CLTV value: if the delta is too low, there's risk of loss of funds, if the delta is too high, someone might decide to fail the payment instead of taking the delay risk.\n>\n> On Wed, May 9, 2018 at 10:23 AM, Jim Posen <jim.posen at gmail.com> wrote:\n>\n>> Thanks for the thoughtful responses.\n>>> You missed the vital detail: that you must prove channel closure if you\n>>> can't unpeel the onion further.  That *will* hit an unresponsive party\n>>> with a penalty.\n>>\n>> Ah, that is a good point. I still find the proposal overall worryingly complex in terms of communication overhead, time it takes to prove channel closure, all of your points in [1], [2], [3], etc. Furthermore, this mandates that immediate channel closure is the only allowed reaction to a party delaying an HTLC for a time period above a threshold -- the node reputation approach gives more discretion to the preceding hop. Deobfuscating the route may turn out to be the right option, but I think the reputation system has certain advantages over this.\n>>> The models we tried in Milan all created an incentive to fail payments,\n>>> which is a non-starter.\n>>\n>> Do you mind elaborating or summarizing the reasons? The way I'm analyzing it, even if there's a nominal spam fee paid to routing nodes that fail payments, as long as it's low enough (say 2-5% for arguments sake), the nodes still have more to gain by forwarding the payment and earning the full fee on a completed payment, and possibly the reputation boost associated with completing a payment if that system was in effect. Moreover, a node that constantly fails payments will be blacklisted by the sender eventually and stop receiving HTLCs from them at all. Overall, I don't think this is a profitable strategy. Furthermore, I think it works quite well in combination with the reputation system.\n>>> This seems like we'd need some serious evaluation to show that this\n>>> works, because the risks are very high.\n>>\n>> I agree that it needs to be evaluated. I may start working on some network simulations to test various DOS mitigation strategies.\n>>\n>>> I can destroy your node's reputation by routing crap through you; even\n>>> if it costs me marginaly more reputation than it does you, that just\n>>> means that the largest players can force failure upon smaller players,\n>>> centralizing the network.  And I think trying to ensure that it costs me\n>>> more reputation than the sum of downstream reputation loss leaks too\n>>> much information\n>>\n>> I will add to ZmnSCPxj's response, which is mostly on point. The key here is that the only way to lose significant reputation is to delay a payment yourself or forward to a malicious downstream that delays -- neither of these can be forced by the sender alone. This amounts to a system where you are on the hook for any malicious behavior of your downstream peers, which is why you must keep a reputation score for each which they earn over time. This should keep all links in the network high quality and quickly disconnect off delaying nodes if the incentives are right.\n>>\n>> While I agree that a lot of reputation is leaked by aggregating the losses along the route, this serves exactly to prevent large nodes with high reputation from ruining links elsewhere. There are two things a node looking to cause reputation loss could do. 1) Identify a node (not itself) it thinks will delay a payment and send to them. This locks up funds on their behalf, but is actually good behavior because it identifies a faulty node and rightfully forces a loss in their reputation, eventually resulting in them being booted from the network. Everyone upstream loses some reputation for having connectivity to them, but less because of the loss aggregation along the route. 2) Delay a payment oneself and force upstream reputation loss. This is why I think it's important that the reputation loss aggregate so that the malicious party loses the most.\n>>\n>> As for the amount of information leaked, yes, it helps determine the number of upstream hops in a route. However, the CLTV values help determine the number of downstream hops in a route in exactly the same way. I see these as symmetric in a sense.\n>>\n>> To address ZmnSCPxj's point:\n>>> But it also looks more and more like a policy of \"just `update_htlc_fail`\" keeps our reputation high: indeed never accepting a forwarding attempt would ensure reputation.\n>>> However, earning via fees should help provide incentive against \"Just `update_htlc_fail`\" always.  If the goal is \"how do I earn money fastest\" then there is some optimal threshhold > of risk-of-reputation-loss vs. fee-earnings-if-I-forward that is unlikely to be near the \"Just fail it\" spectrum, but somewhere in between.  We hope.\n>>\n>> This is exactly the question that your local view of peer reputations helps solve: are the potential fees here worth the risk of forwarding this payment to this downstream? If their reputation is high, then you will want to forward because you think there's a low chance of you incurring reputation loss. If their reputation is low and the HTLC value is too high, you will fail it. So I disagree that \"just `update_htlc_fail`\" is an optimal strategy. Consider as well that all fees you earn on successful payments are profit to you as well as a reputation boost in the view of both of your peers. So in order to earn reputation, you have to forward payments. The key is not forwarding through malicious peers.\n>>\n>> -jimpo\n>>\n>> On Wed, May 9, 2018 at 12:31 AM, ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n>>\n>>> Good morning Rusty, Jim, and list,\n>>>\n>>>> I can destroy your node's reputation by routing crap through you; even\n>>>>\n>>>> if it costs me marginaly more reputation than it does you, that just\n>>>>\n>>>> means that the largest players can force failure upon smaller players,\n>>>>\n>>>> centralizing the network.\n>>>\n>>> My understanding of the proposal was that reputation loss would occur only if the reply (`update_htlc_fail` or `update_htlc_success`) is delayed; this means that for you to force me to lose reputation, you need to somehow make me delay my reply.  In particular if you do simple things like give me an invalid onion, or make me forward to a payee who does not know the preimage, I do not lose reputation by replying very quickly with an `update_htlc_fail`.\n>>>\n>>> Of course, a large player could force reputation loss by delaying reply when they receive, and having patsy nodes route to them.  So for instance if it is Jim -> ZmnSCPxj -> Rusty, and Rusty activates the Blockstream-takes-over-the-world Apocalypse program, the Rusty node would then delay for a long time before replying, which makes my reputation suffer.  But it also makes Rusty reputation suffer even more and my reaction would be that, the next time Jim hands me an HTLC that forwards to Rusty, I would instead quickly `update_htlc_fail` back to Jim (which does not lose me significant reputation due to my quick response) than risk forwarding it to you, since you have a reputation for being slow and unresponsive.\n>>>\n>>> Indeed, another aspect of Jim proposal is that it is extremely local: if Jim has no channel to Rusty, then Jim has no opinion about Rusty, only about ZmnSCPxj.  However, ZmnSCPxj does have an opinion about Rusty, as ZmnSCPxj has channel with Rusty.  If I suffer too much reputation loss due to Rusty, my opinion of Rusty drops even faster, and I decide to `update_htlc_fail` in order to prevent Jim opinion of me from dropping too much that Jim decides not to forward to me (if I have other channels with more reasonable nodes).\n>>>\n>>> But it also looks more and more like a policy of \"just `update_htlc_fail`\" keeps our reputation high: indeed never accepting a forwarding attempt would ensure reputation.\n>>>\n>>> However, earning via fees should help provide incentive against \"Just `update_htlc_fail`\" always.  If the goal is \"how do I earn money fastest\" then there is some optimal threshhold of risk-of-reputation-loss vs. fee-earnings-if-I-forward that is unlikely to be near the \"Just fail it\" spectrum, but somewhere in between.  We hope.\n>>>\n>>>> And I think trying to ensure that it costs me\n>>>>\n>>>> more reputation than the sum of downstream reputation loss leaks too\n>>>>\n>>>> much information\n>>>\n>>> Yes, this is a major drawback of the proposal.  The rate at which the sender of the HTLC threatens me with reputation loss lets me estimate my distance from the ultimate sender of the funds.\n>>>\n>>> Regards,\n>>> ZmnSCPxj\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180510/a41b665e/attachment-0001.html>"
            },
            {
                "author": "Chris Gough",
                "date": "2018-05-10T21:56:38",
                "message_text_only": "hello, I'm a curious lurker trying to follow this conversation:\n\nOn Thu, 10 May 2018, 2:40 pm ZmnSCPxj via Lightning-dev, <\nlightning-dev at lists.linuxfoundation.org> wrote:\n\n>\n> The concern however is that the CLTV already partly leaks the distance\n> from the payee, whereas the reputation-loss-rate leaks distance from the\n> payer.  It is often not interesting to know that some entity is getting\n> paid, but it probably far more interesting to know WHO paid WHO, so leaking\n> both distances simultaneously is more than twice as worse as leaking just\n> one distance.\n>\n\nConsider an asymetrically-resourced malevolent node that wants the ability\nto harm a specific small nodes without aquiring a bad reputation (and is\nwilling to pay for it). In preparation, this bad boss node directs normal\ntraffic to sacrificial nodes they control, while understating the\nreputation-risk (truthfully as it turns out, because they have out of band\ninfluence over the node). When the time comes, the sacrificial node\ninflicts delay on the victim node and they both suffer, while the boss\nkeeps her nose clean.\n\nIs it the case that understating risk of legitimate traffic from boss node\nto sacrificial node effectively allows transfer of reputation to the\nsacrificial node in preparation for attack, while at the same time\nobscuring their association?\n\nChris Gough\n\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180510/0808ff75/attachment.html>"
            },
            {
                "author": "Jim Posen",
                "date": "2018-05-10T23:18:08",
                "message_text_only": "Hmm, I'm not quite following the situation. What do you mean by \"directs\nnormal traffic\"? Since the sender constructs the entire circuit, routing\nnodes do not get any discretion over which nodes to forward a payment to,\nonly whether to forward or fail. What an attacker could do is perform a\nloop attack and send a payment to another node that they control and delay\nthe payment on the receiving end. Note that the sending node loses no\nreputation, only the receiving node. Since the hops being attacked are the\nones in the middle and they are faithfully enforcing the reputation\nprotocol, the receiving node's reputation should be penalized properly,\nmaking it unlikely the attack will succeed in a second attempt.\n\nOn Thu, May 10, 2018 at 2:56 PM, Chris Gough <christopher.d.gough at gmail.com>\nwrote:\n\n> hello, I'm a curious lurker trying to follow this conversation:\n>\n> On Thu, 10 May 2018, 2:40 pm ZmnSCPxj via Lightning-dev, <\n> lightning-dev at lists.linuxfoundation.org> wrote:\n>\n>>\n>> The concern however is that the CLTV already partly leaks the distance\n>> from the payee, whereas the reputation-loss-rate leaks distance from the\n>> payer.  It is often not interesting to know that some entity is getting\n>> paid, but it probably far more interesting to know WHO paid WHO, so leaking\n>> both distances simultaneously is more than twice as worse as leaking just\n>> one distance.\n>>\n>\n> Consider an asymetrically-resourced malevolent node that wants the ability\n> to harm a specific small nodes without aquiring a bad reputation (and is\n> willing to pay for it). In preparation, this bad boss node directs normal\n> traffic to sacrificial nodes they control, while understating the\n> reputation-risk (truthfully as it turns out, because they have out of band\n> influence over the node). When the time comes, the sacrificial node\n> inflicts delay on the victim node and they both suffer, while the boss\n> keeps her nose clean.\n>\n> Is it the case that understating risk of legitimate traffic from boss node\n> to sacrificial node effectively allows transfer of reputation to the\n> sacrificial node in preparation for attack, while at the same time\n> obscuring their association?\n>\n> Chris Gough\n>\n>>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180510/e6ddf135/attachment.html>"
            },
            {
                "author": "Chris Gough",
                "date": "2018-05-11T00:17:22",
                "message_text_only": "On Fri, May 11, 2018 at 9:18 AM, Jim Posen <jim.posen at gmail.com> wrote:\n> Hmm, I'm not quite following the situation. What do you mean by \"directs\n> normal traffic\"? Since the sender constructs the entire circuit, routing\n> nodes do not get any discretion over which nodes to forward a payment to,\n\n<facepalm>\n\n> only whether to forward or fail. What an attacker could do is perform a loop\n> attack and send a payment to another node that they control and delay the\n> payment on the receiving end. Note that the sending node loses no\n> reputation, only the receiving node. Since the hops being attacked are the\n> ones in the middle and they are faithfully enforcing the reputation\n> protocol, the receiving node's reputation should be penalized properly,\n> making it unlikely the attack will succeed in a second attempt.\n\nSo the attacker can purchase reputation for the sacrificial node by\nsending them legitimate payments, and then spend that reputation (at\nthe mutual expense of hops) by delaying targeted transactions. But my\nquestion about obscuring the collusion by artificially lowering\nreported risk was nonsense based on misunderstanding. Thanks for\nhelping me understand.\n\nChris Gough"
            },
            {
                "author": "Rusty Russell",
                "date": "2018-05-14T03:40:19",
                "message_text_only": "Jim Posen <jim.posen at gmail.com> writes:\n> Thanks for the thoughtful responses.\n>\n>> You missed the vital detail: that you must prove channel closure if you\n>> can't unpeel the onion further.  That *will* hit an unresponsive party\n>> with a penalty.\n>\n> Ah, that is a good point. I still find the proposal overall worryingly\n> complex in terms of communication overhead, time it takes to prove channel\n> closure, all of your points in [1], [2], [3], etc. Furthermore, this\n> mandates that immediate channel closure is the only allowed reaction to a\n> party delaying an HTLC for a time period above a threshold -- the node\n> reputation approach gives more discretion to the preceding hop.\n> Deobfuscating the route may turn out to be the right option, but I think\n> the reputation system has certain advantages over this.\n\nAgreed, it's a tradeoff.\n\n>> The models we tried in Milan all created an incentive to fail payments,\n>> which is a non-starter.\n>\n> Do you mind elaborating or summarizing the reasons? The way I'm analyzing\n> it, even if there's a nominal spam fee paid to routing nodes that fail\n> payments, as long as it's low enough (say 2-5% for arguments sake), the\n> nodes still have more to gain by forwarding the payment and earning the\n> full fee on a completed payment, and possibly the reputation boost\n> associated with completing a payment if that system was in effect.\n\nYou're forgetting the failure cases, where now I can profit.\n\nIf I disconnect from another node, I now have a disincentive to tell\nothers.  At the moment we send an update disabling the channel (though\nwe should give a few seconds for reconnect first, but whatever).\n\nSimilarly, the rewards aren't proportional: being cheaper than other\nroutes gets you all the traffic, but now you profit even if you can't\nservice the payments.  In fact, once a channel becomes hard to use (low\ncapacity, transient disconnect, whatever), I *should* advertize it as\ncheaper route than anyone else: free money!\n\nI'm sure there are other ways to game it, but the underlying reason is\nclear: it misaligns user and node incentives.\n\n> Moreover, a node that constantly fails payments will be blacklisted by the\n> sender eventually and stop receiving HTLCs from them at all. Overall, I\n> don't think this is a profitable strategy. Furthermore, I think it works\n> quite well in combination with the reputation system.\n\nIf the system is sufficiently decentralized, managing to cheat everyone\nonce is very profitable though.\n\n>> This seems like we'd need some serious evaluation to show that this\n>> works, because the risks are very high.\n>\n> I agree that it needs to be evaluated. I may start working on some network\n> simulations to test various DOS mitigation strategies.\n>\n>> I can destroy your node's reputation by routing crap through you; even\n>> if it costs me marginaly more reputation than it does you, that just\n>> means that the largest players can force failure upon smaller players,\n>> centralizing the network.  And I think trying to ensure that it costs me\n>> more reputation than the sum of downstream reputation loss leaks too\n>> much information\n>\n> I will add to ZmnSCPxj's response, which is mostly on point. The key here\n> is that the only way to lose significant reputation is to delay a payment\n> yourself or forward to a malicious downstream that delays -- neither of\n> these can be forced by the sender alone. This amounts to a system where you\n> are on the hook for any malicious behavior of your downstream peers, which\n> is why you must keep a reputation score for each which they earn over time.\n> This should keep all links in the network high quality and quickly\n> disconnect off delaying nodes if the incentives are right.\n\nBut I can make you look like a delaying node whenever I want.  The only\nway to ensure that I lose more reputation than you do is to leak\ninformation about route length for *everyone*.  And even then, it's just\na matter of numbers.  I can make successful payments to myself through\nthe same peers (but not you!) to stay above their threshold so my\nreputation is intact.\n\nSo it's basically a question of how expensive is it for me to throw you\noff the network?  You have to tune that number carefully.\n\n> While I agree that a lot of reputation is leaked by aggregating the losses\n> along the route, this serves exactly to prevent large nodes with high\n> reputation from ruining links elsewhere. There are two things a node\n> looking to cause reputation loss could do. 1) Identify a node (not itself)\n> it thinks will delay a payment and send to them. This locks up funds on\n> their behalf, but is actually good behavior because it identifies a faulty\n> node and rightfully forces a loss in their reputation, eventually resulting\n> in them being booted from the network. Everyone upstream loses some\n> reputation for having connectivity to them, but less because of the loss\n> aggregation along the route. 2) Delay a payment oneself and force upstream\n> reputation loss. This is why I think it's important that the reputation\n> loss aggregate so that the malicious party loses the most.\n\nBut we're busy trying to remove all the methods of deanonymizing the\nnetwork, and you seem to be adding a new one, *and* providing an\nincentive to deanonymize.\n\n> As for the amount of information leaked, yes, it helps determine the number\n> of upstream hops in a route. However, the CLTV values help determine the\n> number of downstream hops in a route in exactly the same way. I see these\n> as symmetric in a sense.\n\nYes, which is why we have mitigations in place (which are still probably\ninsufficient).  I really don't want to add another vector.\n\n> This is exactly the question that your local view of peer reputations helps\n> solve: are the potential fees here worth the risk of forwarding this\n> payment to this downstream?\n\nSo now I'll try to deanonymize all payments so I can determine this.\nThose who do this best will be rewarded, and those who don't try will be\nknocked off the network, probably by those who can!\n\nSo I'd like to see a real design of the reputation system.  If it's\npractical (which is a significant hurdle), we then need to carefully\nevaluate whether we're creating significant disincentives to\nneighbourliness.\n\nCheers,\nRusty."
            },
            {
                "author": "Jim Posen",
                "date": "2018-05-15T18:43:07",
                "message_text_only": ">\n> You're forgetting the failure cases, where now I can profit.\n>\n> If I disconnect from another node, I now have a disincentive to tell\n> others.  At the moment we send an update disabling the channel (though\n> we should give a few seconds for reconnect first, but whatever).\n>\n> Similarly, the rewards aren't proportional: being cheaper than other\n> routes gets you all the traffic, but now you profit even if you can't\n> service the payments.  In fact, once a channel becomes hard to use (low\n> capacity, transient disconnect, whatever), I *should* advertize it as\n> cheaper route than anyone else: free money!\n>\n> I'm sure there are other ways to game it, but the underlying reason is\n> clear: it misaligns user and node incentives.\n\n\nIf the system is sufficiently decentralized, managing to cheat everyone\n> once is very profitable though.\n>\n\nI still don't agree with the conclusion.\n\nWhat are the incentives currently for a node to advertise disabled\nchannels? Firstly, it reduces computational effort of performing an\nunnecessary channel update. The main incentive though is that it is the\nrecommended spec compliant behavior, and makes the link appear more\nreliable to the sender. I think this has to be analyzed as a repeated game,\nwhere senders will prefer links that not only advertise lower fees, but\nhave a history of being reliable (or at least don't have a history of being\nunreliable). So with a fee of 0 on failed payments, we think that there's\nprobably sufficient reason for nodes to advertise when channels are\ndisabled. I'll grant, obviously, that if that there is high fee on failed\npayments that it would create incentive problems, but not if the\nunconditional fee is low enough. Note also the unconditional fee is also\nvisible to senders, who will choose not to route through such links for\nthis exact reason. So your example of a node advertising cheap fees on\ndisabled links, it would only make sense to lower the success-conditional\nfee, not the unconditional fee. Ultimately, I think this will reach an\nequilibrium assuming senders have good route selection algorithms.\n\nNow, that said, if this unconditional fee were the only (or even main)\nprotection against loop attacks, it would probably require them to be so\nhigh as to create incentive problems. I am proposing the unconditional fee\nas a spam fee to punish senders who just send a bunch of failing payments\nin order to collect routing information, and put load on the network in the\nprocess for no good reason. With this as the purpose, I think this\nunconditional spam fee can be low enough to avoid the issues you raise.\n\nOne of my beliefs in structuring incentives it that it's far more\nacceptable for senders to get cheated by nodes they route through than for\nrouters to be attacked. The reason is that senders collect a lot more\ninformation in failure cases and can avoid repeated occurrences by\nselecting routes differently. Routers, on the other hand, have far more\nlimited information and choices and their operation is critical to the\nefficiency and decentralization of the system, so they should be protected\nby the protocol to a greater degree.\n\n\n> But I can make you look like a delaying node whenever I want.  The only\n> way to ensure that I lose more reputation than you do is to leak\n> information about route length for *everyone*.  And even then, it's just\n> a matter of numbers.  I can make successful payments to myself through\n> the same peers (but not you!) to stay above their threshold so my\n> reputation is intact.\n>\n> So it's basically a question of how expensive is it for me to throw you\n> off the network?  You have to tune that number carefully.\n>\n\nRemember, reputation is local to each node, so the only way you can raise\nyour reputation with me is by participating in successful circuits *with\nme*. If you route successful payments on circuits with other peers, you may\nraise your reputation with them, but will do nothing to convince me to send\nmore payments through you. So it is not true that you can make me look like\na delaying node whenever you want; after a few bad interactions, I will\nstop routing through you until you participate in enough successful\npayments *with me* as the downstream to earn back your reputation.\n\nI agree this number needs to be tuned carefully, but ideally this (the\nreputation rate loss) is something that can be tuned individually by\nrouters according to their risk tolerance and history of attacks on the\nnetwork. I think it's far preferable to have a number that can be tuned in\nthe face of DoS attacks than being caught with no protection whatsoever.\n\nBut we're busy trying to remove all the methods of deanonymizing the\n> network, and you seem to be adding a new one, *and* providing an\n> incentive to deanonymize.\n>\n> Yes, which is why we have mitigations in place (which are still probably\n> insufficient).  I really don't want to add another vector.\n>\n\nThis gets to the heart of the tradeoff between network efficiency and\nprivacy. I admit that my proposal sacrifices on privacy, and I personally\nfind it acceptable given the added protection against DoS attacks, but I\nunderstand the counterargument as well.\n\nOne additional thing to consider, though, is that if DoS attacks are\nproblematic and there is no protection, many nodes may stop routing\npayments entirely. Onion routing through a network with very few possible\ncircuits certainly is less private than revealing additional circuit\ninformation on a decentralized routing network.\n\n\n> So now I'll try to deanonymize all payments so I can determine this.\n> Those who do this best will be rewarded, and those who don't try will be\n> knocked off the network, probably by those who can!\n>\n\nI'm not following, how are you deanonymizing payments? Having more\ninformation about downstream hops can help one make better routing\ndecisions, but if the reputation system works properly, you should only\nneed to care about the reputation of the next hop.\n\n\n> So I'd like to see a real design of the reputation system.  If it's\n> practical (which is a significant hurdle), we then need to carefully\n> evaluate whether we're creating significant disincentives to\n> neighbourliness.\n>\n\nI tried to mostly describe the design in the first email, but it's kind of\ndisorganized. I'll try to sketch it out here more succinctly.\n\nEach node has a local configuration of its \"reputation loss rate\" per\nchannel in units of Hz. This configured rate is not explicitly advertised\nto peers. Each update_add_htlc has an additional field the \"reputation loss\nrate\" in units of satoshis per second, calculated as the reputation loss\nrate of the upstream HTLC plus the local reputation loss rate times the\noffered HTLC value. This value can be obfuscated somewhat by the origin of\nthe payment and at each hop with some amount of noise, subject to the\nconcerns raised in my discussion with ZmnSCPxj. When forwarding an HTLC,\nthe upstream hop measures the elapsed time between delivery of a commitment\nsignature on the add and receipt of the fail/update.\n\nFor each peer node, keep a reputation score in units of satoshis. For each\npeer, when a payment completes where they are the upstream or downstream\nhop, add the amount collected in fees to their reputation score. For each\npayment where they are the downstream hop, subtract from their reputation\nscore the reputation loss rate on the offered HTLC times its elapsed time.\nReputation score is not explicitly shared between peered nodes, but can be\nestimated to within differences in elapsed time measurements.\n\nWhen you receive an HTLC to be forwarded to some downstream node, compare\ntheir current reputation score with the offered reputation loss rate times\n10 minutes times the CLTV on the HTLC. If their reputation is sufficient,\nforward the HTLC and place a hold on their total reputation for that amount\n(so that concurrently forwarded HTLCs can't exceed their reputation\n\"balance\"). New, unknown, peers should get some initial reputation,\npresumably greater for channels that one initiates vs channels one receives.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180515/35dedefc/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-16T00:04:44",
                "message_text_only": "Good morning,\n\n>> But I can make you look like a delaying node whenever I want.  The only\n>> way to ensure that I lose more reputation than you do is to leak\n>> information about route length for *everyone*.  And even then, it's just\n>> a matter of numbers.  I can make successful payments to myself through\n>> the same peers (but not you!) to stay above their threshold so my\n>> reputation is intact.\n>>\n>> So it's basically a question of how expensive is it for me to throw you\n>> off the network?  You have to tune that number carefully.\n>\n> Remember, reputation is local to each node, so the only way you can raise your reputation with me is by participating in successful circuits *with me*. If you route successful payments on circuits with other peers, you may raise your reputation with them, but will do nothing to convince me to send more payments through you. So it is not true that you can make me look like a delaying node whenever you want; after a few bad interactions, I will stop routing through you until you participate in enough successful payments *with me* as the downstream to earn back your reputation.\n>\n> I agree this number needs to be tuned carefully, but ideally this (the reputation rate loss) is something that can be tuned individually by routers according to their risk tolerance and history of attacks on the network. I think it's far preferable to have a number that can be tuned in the face of DoS attacks than being caught with no protection whatsoever.\n\nConsider the below network:\n\n    .Rusty1\n    .  |\n    .  v\n    .ZmnSCPxj->Jim\n    .  |      /\n    .  v     L\n    .  Rusty2\n\nRusty1 can make me think highly of Rusty2, by routing many payments to Rusty2 which Rusty2 immediately accepts/forwards.  Then Rusty1 can make me think lowly of Jim, by forwarding payments via Jim to Rusty2, and having Rusty2 delay the payment so that Jim loses reputation badly.  Then in the future unrelated forwards that would have passed through ZmnSCPxj->Jim will be rejected by me because I think Jim has low reputation.\n\nThis holds even if local reputation is used.\n\nJim argues that this is unimportant, since (assuming all HTLC values are almost equal) Jim will stop forwarding to Rusty2 (due to imposing a higher \"reputation loss rate\" on Rusty2 than I impose in Jim) before I stop forwarding to Jim.\n\nThis can still be manipulated if Rusty1 opens a direct channel to Jim.  Then Rusty1 can route payments Rusty1->Jim->Rusty2 that succeed quickly, then route payments Rusty1->ZmnSCPxj->Jim->Rusty2 that stall.  Thus Rusty2 can have the Jim->Rusty2 reputation boosted, while alternating with reputation losses that make ZmnSCPxj->Jim reputation go down.  Since local reputation is used, ZmnSCPxj and Jim will not talk to each other about how Rusty2 seems to stall when not routing a payment from Rusty1.  Rusty1 can now manipulate the reputation view of ZmnSCPxj and Jim of each other while keeping Rusty2 reputation somewhat high.\n\n>> But we're busy trying to remove all the methods of deanonymizing the\n>> network, and you seem to be adding a new one, *and* providing an\n>> incentive to deanonymize.\n>>\n>> Yes, which is why we have mitigations in place (which are still probably\n>> insufficient).  I really don't want to add another vector.\n>\n> This gets to the heart of the tradeoff between network efficiency and privacy. I admit that my proposal sacrifices on privacy, and I personally find it acceptable given the added protection against DoS attacks, but I understand the counterargument as well.\n>\n> One additional thing to consider, though, is that if DoS attacks are problematic and there is no protection, many nodes may stop routing payments entirely. Onion routing through a network with very few possible circuits certainly is less private than revealing additional circuit information on a decentralized routing network.\n>\n>> So now I'll try to deanonymize all payments so I can determine this.\n>> Those who do this best will be rewarded, and those who don't try will be\n>> knocked off the network, probably by those who can!\n>\n> I'm not following, how are you deanonymizing payments? Having more information about downstream hops can help one make better routing decisions, but if the reputation system works properly, you should only need to care about the reputation of the next hop.\n\nAs mentioned, the CLTV total leaks information on how far the payee is.  I might decide to keep track of a reputation score, not of the local peers I have, but on the entire network.  If the CLTV total at my outgoing is high, then if the outgoing HTLC takes a long time to respond, I will distribute a small reputation loss to a large number of nodes that are accessible from the outgoing channel; if the CLTV total at my outgoing is low, I will distribute a large reputation loss to a small number of nodes that are accessible from the outgoing channel.  I now have the incentive to make this estimation even more accurate in the future.\n\n>\n>\n>> So I'd like to see a real design of the reputation system.  If it's\n>> practical (which is a significant hurdle), we then need to carefully\n>> evaluate whether we're creating significant disincentives to\n>> neighbourliness.\n>\n> I tried to mostly describe the design in the first email, but it's kind of disorganized. I'll try to sketch it out here more succinctly.\n>\n> Each node has a local configuration of its \"reputation loss rate\" per channel in units of Hz. This configured rate is not explicitly advertised to peers. Each update_add_htlc has an additional field the \"reputation loss rate\" in units of satoshis per second, calculated as the reputation loss rate of the upstream HTLC plus the local reputation loss rate times the offered HTLC value. This value can be obfuscated somewhat by the origin of the payment and at each hop with some amount of noise, subject to the concerns raised in my discussion with ZmnSCPxj. When forwarding an HTLC, the upstream hop measures the elapsed time between delivery of a commitment signature on the add and receipt of the fail/update.\n>\n> For each peer node, keep a reputation score in units of satoshis. For each peer, when a payment completes where they are the upstream or downstream hop, add the amount collected in fees to their reputation score. For each payment where they are the downstream hop, subtract from their reputation score the reputation loss rate on the offered HTLC times its elapsed time. Reputation score is not explicitly shared between peered nodes, but can be estimated to within differences in elapsed time measurements.\n\nPlease describe the below:\n\n1.  Behavior if payment succeeds after T time.\n2.  Behavior if payment fails after T time.\n\nIt seems you only described \"Behavior if payment succeeds after T time\".\n\nRegards,\nZmnSCPxj\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180515/67ca3cc4/attachment.html>"
            },
            {
                "author": "Jim Posen",
                "date": "2018-05-16T01:48:33",
                "message_text_only": ">\n> This can still be manipulated if Rusty1 opens a direct channel to Jim.\n> Then Rusty1 can route payments Rusty1->Jim->Rusty2 that succeed quickly,\n> then route payments Rusty1->ZmnSCPxj->Jim->Rusty2 that stall.  Thus Rusty2\n> can have the Jim->Rusty2 reputation boosted, while alternating with\n> reputation losses that make ZmnSCPxj->Jim reputation go down.  Since local\n> reputation is used, ZmnSCPxj and Jim will not talk to each other about how\n> Rusty2 seems to stall when not routing a payment from Rusty1.  Rusty1 can\n> now manipulate the reputation view of ZmnSCPxj and Jim of each other while\n> keeping Rusty2 reputation somewhat high.\n>\n\nYes, you are correct that in scenarios like this an attacker can pay to\ndegrade the reputation of one of its peers (or even nodes further away).\nThe key point is that doing so should be costly to the attacker because\nthey must pay the victim node to continue making itself vulnerable to\npayment delays. But if the node is getting compensated, is that really an\nattack then? This system is designed with the assumption that the best way\nto defend an anonymous/decentralized network that allows sybils is by\npricing resource utilization appropriately. In a similar way, the Bitcoin\nblockchain is \"vulnerable\" to spam attacks in the sense that attackers can\npay to fill up block space.\n\n\n> As mentioned, the CLTV total leaks information on how far the payee is.  I\n> might decide to keep track of a reputation score, not of the local peers I\n> have, but on the entire network.  If the CLTV total at my outgoing is high,\n> then if the outgoing HTLC takes a long time to respond, I will distribute a\n> small reputation loss to a large number of nodes that are accessible from\n> the outgoing channel; if the CLTV total at my outgoing is low, I will\n> distribute a large reputation loss to a small number of nodes that are\n> accessible from the outgoing channel.  I now have the incentive to make\n> this estimation even more accurate in the future.\n>\n\nOne could do this today. I'd even argue that they are incentivized to\nalready as a protection against loop attacks/payment delays. But it's\nlikely a pretty ineffective strategy depending on the number of channels\nthat the possible downstream hops have open.\n\nPlease describe the below:\n>\n> 1.  Behavior if payment succeeds after T time.\n> 2.  Behavior if payment fails after T time.\n>\n> It seems you only described \"Behavior if payment succeeds after T time\".\n>\n\nAh, sorry if I didn't make that clear. The reputation is increased in the\ncase of successful payments by the fee collected. The reputation is\ndecreased on the downstream peer proportional to time T *regardless* of\nwhether the payment succeeds or fails. If a payment succeeds quickly, the\nincrease should outweigh the decrease, but if the payment succeeds after a\nlong time, the change in reputation may be net negative. If the payment\nfails, the upstream peer's reputation does not change and the downstream\npeer's reputation always decreases proportional to time T.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180515/755f6eaf/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-16T09:44:35",
                "message_text_only": "Good morning Jim,\n\n>> This can still be manipulated if Rusty1 opens a direct channel to Jim.  Then Rusty1 can route payments Rusty1->Jim->Rusty2 that succeed quickly, then route payments Rusty1->ZmnSCPxj->Jim->Rusty2 that stall.  Thus Rusty2 can have the Jim->Rusty2 reputation boosted, while alternating with reputation losses that make ZmnSCPxj->Jim reputation go down.  Since local reputation is used, ZmnSCPxj and Jim will not talk to each other about how Rusty2 seems to stall when not routing a payment from Rusty1.  Rusty1 can now manipulate the reputation view of ZmnSCPxj and Jim of each other while keeping Rusty2 reputation somewhat high.\n>\n> Yes, you are correct that in scenarios like this an attacker can pay to degrade the reputation of one of its peers (or even nodes further away). The key point is that doing so should be costly to the attacker because they must pay the victim node to continue making itself vulnerable to payment delays. But if the node is getting compensated, is that really an attack then? This system is designed with the assumption that the best way to defend an anonymous/decentralized network that allows sybils is by pricing resource utilization appropriately. In a similar way, the Bitcoin blockchain is \"vulnerable\" to spam attacks in the sense that attackers can pay to fill up block space.\n\nHmm, you are indeed correct.  Rusty brings up that a rich node can do this to victimize poorer nodes, though: even if the poorer node is compensated for the loss of reputation, the rich node may arrange the network such that it is the only one that can practically earn from forwarding fees, with everyone else suspicious of everyone except the rich node.\n\nThere may not be a way to solve this?\n\n>> Please describe the below:\n>>\n>> 1.  Behavior if payment succeeds after T time.\n>> 2.  Behavior if payment fails after T time.\n>>\n>> It seems you only described \"Behavior if payment succeeds after T time\".\n>\n> Ah, sorry if I didn't make that clear. The reputation is increased in the case of successful payments by the fee collected. The reputation is decreased on the downstream peer proportional to time T regardless of whether the payment succeeds or fails. If a payment succeeds quickly, the increase should outweigh the decrease, but if the payment succeeds after a long time, the change in reputation may be net negative. If the payment fails, the upstream peer's reputation does not change and the downstream peer's reputation always decreases proportional to time T.\n\nThank you.  So:\n\n1.  If payment succeeds after T time, upstream reputation is increased by fee earned, downstream reputation is increased by fee earned, downstream reputation is decreased by T * reputation_loss_rate.\n2.  If payment fails after T time, downstream reputation is decreased by T * reputation_loss_rate.\n\nRegards,\nZmnSCPxj\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180516/39a69a1e/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2018-05-18T05:27:15",
                "message_text_only": "ZmnSCPxj <ZmnSCPxj at protonmail.com> writes:\n>>> Please describe the below:\n>>>\n>>> 1.  Behavior if payment succeeds after T time.\n>>> 2.  Behavior if payment fails after T time.\n>>>\n>>> It seems you only described \"Behavior if payment succeeds after T time\".\n>>\n>> Ah, sorry if I didn't make that clear. The reputation is increased in the case of successful payments by the fee collected. The reputation is decreased on the downstream peer proportional to time T regardless of whether the payment succeeds or fails. If a payment succeeds quickly, the increase should outweigh the decrease, but if the payment succeeds after a long time, the change in reputation may be net negative. If the payment fails, the upstream peer's reputation does not change and the downstream peer's reputation always decreases proportional to time T.\n>\n> Thank you.  So:\n>\n> 1.  If payment succeeds after T time, upstream reputation is increased by fee earned, downstream reputation is increased by fee earned, downstream reputation is decreased by T * reputation_loss_rate.\n> 2.  If payment fails after T time, downstream reputation is decreased by T * reputation_loss_rate.\n\nSo, to be clear, there's no *financial* fee on failure, just reputation\nfee?\n\nAlso, you talked about reputation_loss_rate as being a private per-node\nthing, and being an explicit thing in the HTLC.  I'm ignoring the\nformer, and assuming the latter.\n\nOK, let's consider a 1000 satoshi HTLC, paying a 10ppm fee and with a\n1ppm-per-second reputation_loss_rate.\n\nIf it succeeds in 1 second (measuring reputation in millisatoshi,\nbecause its cute):\n\n        reputation[source] += 10\n        reputation[destination] += 10 - 1\n\nIf it succeeds in 10 seconds:\n\n        reputation[source] += 10\n        reputation[destination] += 10 - 10\n\nIf it fails in 10 seconds:\n\n        reputation[destination] -= 10\n\nAnd if it falls onto the chain and fails after 144 blocks:\n\n        reputation[destination] -= 86400\n\nIn this scheme, it seems that I can destroy N reputation points in 18\nnodes for cost N:\n\n        Mallory1 -> Node1 -> Node2 -> ..... -> Node18 -> Mallory2\n                                                  \\\n                                               Mallory3\n\nMallory2 pays a crapload of fees to Node18 to pay Mallory3, getting a\ngreat reputation.  Then Mallory1 sends a payment to Mallory2, which\ntakes a day to resolve.\n\nOf course, this is why you want ramped reputation_loss_rate, say 1ppm\nfor Mallory1->Node1, 2ppm ->Node2.... 19ppm for Mallory2.  That just\nmeans Mallory can destroy 9*N total reputation points, not 18*N.\n\nYou can fix this by making it reputation_loss_rate exponential, but I\ndon't think Node1 could ever get the 262144x reputation to make the\npayment in the first place.\n\nWhat am I missing?\n\nThanks,\nRusty."
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-18T23:38:26",
                "message_text_only": "Good morning Rusty,\n\n\n> Also, you talked about reputation_loss_rate as being a private per-node\n> \n> thing, and being an explicit thing in the HTLC. I'm ignoring the\n> \n> former, and assuming the latter.\n\nReputation (the score) is a private per-node thing, while the `reputation_loss_rate` is explicit in the HTLC.\n\nI am uncertain how that changes your analysis, though.  In a line network like you showed, the reputation \"bins\" are Node1->Node2 and Node2->Node1 and so on.  It may be more useful to think of the reputation bins as assigned to half-chans than to nodes.\n\nSo the initial Mallory3->Node18->Mallory2 gives high reputation to half-chans Node18->Mallory3 and Node18->Mallory2, then sacrifices the Node18->Mallory3 reputation to destroy the NodeN -> NodeN+1 and NodeN+1->NodeN reputations.\n\nAlong that line, reputation lost is higher as N increases. Graphing reputation lost along that line, we form a triangle, and the area of the triangle is the total reputation destroyed.  Only the length of one edge of the triangle is what is lost by the Node18->Mallory3 reputation score.  So yes, it seems you are correct here.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Jim Posen",
                "date": "2018-05-19T00:54:49",
                "message_text_only": "Yes Rusty, you are correct that an attacker still gets leverage on their\nability to destroy reputation unless the loss rates increase exponentially.\nAnd I agree that would be a very steep increase, serving do decrease\ncircuit lengths dramatically. The reasoning for the linear increase in\nreputation loss comes from viewing reputation loss as compensation for\ntime-value lost in locked HTLCs. In a sense, every node pays in reputation\nfor all of the time-value locked in HTLCs upstream from them in proportion\nto the value assigned to it by the owners of the funds.\n\nI'm not sure whether this would be an acceptable way of pricing payment\ndelays, though I do think it is important to make attackers pay for the\nresources they are wasting on the network in some form.\n\nOn Fri, May 18, 2018 at 4:38 PM, ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Rusty,\n>\n>\n> > Also, you talked about reputation_loss_rate as being a private per-node\n> >\n> > thing, and being an explicit thing in the HTLC. I'm ignoring the\n> >\n> > former, and assuming the latter.\n>\n> Reputation (the score) is a private per-node thing, while the\n> `reputation_loss_rate` is explicit in the HTLC.\n>\n> I am uncertain how that changes your analysis, though.  In a line network\n> like you showed, the reputation \"bins\" are Node1->Node2 and Node2->Node1\n> and so on.  It may be more useful to think of the reputation bins as\n> assigned to half-chans than to nodes.\n>\n> So the initial Mallory3->Node18->Mallory2 gives high reputation to\n> half-chans Node18->Mallory3 and Node18->Mallory2, then sacrifices the\n> Node18->Mallory3 reputation to destroy the NodeN -> NodeN+1 and\n> NodeN+1->NodeN reputations.\n>\n> Along that line, reputation lost is higher as N increases. Graphing\n> reputation lost along that line, we form a triangle, and the area of the\n> triangle is the total reputation destroyed.  Only the length of one edge of\n> the triangle is what is lost by the Node18->Mallory3 reputation score.  So\n> yes, it seems you are correct here.\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180518/f341c9c4/attachment.html>"
            },
            {
                "author": "Corn\u00e9 Plooy",
                "date": "2018-05-22T15:47:48",
                "message_text_only": "> You missed the vital detail: that you must prove channel closure if you\n> can't unpeel the onion further.  That *will* hit an unresponsive party\n> with a penalty.[1]\nIs this specified in a BOLT somewhere? I tried to find it several times,\nwithout success.\n\n\nCJP"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-22T22:50:02",
                "message_text_only": "Good morning Corne,\n\nI think onion unpeeling never made it into the BOLT spec precisely due to the problems with it.  I think the unpeeling in question is essentially a hop node (rather than the ultimate payer/source) unpeeling the onion in order to find out who was being slow. Perhaps the discussion regarding it is archived elsewhere; I do not know myself.\n\nRegards,\nZmnSCPxj\n\n\u200bSent with ProtonMail Secure Email.\u200b\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n\nOn May 22, 2018 11:47 PM, Corn\u00e9 Plooy via Lightning-dev <lightning-dev at lists.linuxfoundation.org> wrote:\n\n> > You missed the vital detail: that you must prove channel closure if you\n> > \n> > can't unpeel the onion further. That will hit an unresponsive party\n> > \n> > with a penalty.[1]\n> \n> Is this specified in a BOLT somewhere? I tried to find it several times,\n> \n> without success.\n> \n> CJP\n> \n> Lightning-dev mailing list\n> \n> Lightning-dev at lists.linuxfoundation.org\n> \n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "Jim Posen",
                "date": "2018-05-23T07:41:25",
                "message_text_only": "Only place I see the onion peeling discussed is here:\nhttps://github.com/lightningnetwork/lightning-rfc/issues/182.\n\nOn Tue, May 22, 2018 at 3:50 PM, ZmnSCPxj via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> wrote:\n\n> Good morning Corne,\n>\n> I think onion unpeeling never made it into the BOLT spec precisely due to\n> the problems with it.  I think the unpeeling in question is essentially a\n> hop node (rather than the ultimate payer/source) unpeeling the onion in\n> order to find out who was being slow. Perhaps the discussion regarding it\n> is archived elsewhere; I do not know myself.\n>\n> Regards,\n> ZmnSCPxj\n>\n> \u200bSent with ProtonMail Secure Email.\u200b\n>\n> \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n>\n> On May 22, 2018 11:47 PM, Corn\u00e9 Plooy via Lightning-dev <\n> lightning-dev at lists.linuxfoundation.org> wrote:\n>\n> > > You missed the vital detail: that you must prove channel closure if you\n> > >\n> > > can't unpeel the onion further. That will hit an unresponsive party\n> > >\n> > > with a penalty.[1]\n> >\n> > Is this specified in a BOLT somewhere? I tried to find it several times,\n> >\n> > without success.\n> >\n> > CJP\n> >\n> > Lightning-dev mailing list\n> >\n> > Lightning-dev at lists.linuxfoundation.org\n> >\n> > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180523/698b6d09/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Mitigations for loop attacks",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Corn\u00e9 Plooy",
                "Chris Gough",
                "Rusty Russell",
                "Jim Posen",
                "ZmnSCPxj"
            ],
            "messages_count": 20,
            "total_messages_chars_count": 85107
        }
    },
    {
        "title": "[Lightning-dev] eltoo Trustless WatchTowers",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-02T10:43:22",
                "message_text_only": "Good morning list,\n\nI started considering, what a trustless, blinded WatchTower for eltoo channels would look like.  Unfortunately, I cannot derive a good one yet, sorry.\n\nMy basis, is the same \"encrypted blob\" we use for Poon-Dryja channels.\n\nNow an issue is that update transactions in Decker-Russell-Osuntokun channels have no txid that can be used as key for the encrypted blob.\n\nOne thing we can use is the trigger transaction, which has a stable txid since its sole input is an ordinary 2-of-2 that is signed with normal `SIGHASH_ALL`.\n\nHowever, this has the drawback that each update of a single channel will generate a `(txid[:16], blob)` pair that has the same `txid[:16]` key, letting the WatchTower correlate the timing and number of updates of each of your channels.  Compare to the case where a Poon-Dryja uses the same encoding: the WatchTower knows each `(txid[:16], blob)` pair is *some* update to *one* of your channels, but cannot know if any two pairs you submit are for the same channel or two different channels.  Indeed even if a theft attempt occurs the WatchTower can only know that some `txid[:16]` belongs to a particular channel, but not whether the other `txid[:16]` you published to it are to the same channel, or another channel you happen to be interested in.\n\nIt seems desirable to at least retain the property that the WatchTower cannot correlate our updates to our channels.\n\nAnother property that the Poon-Dryja encrypted-blob has is that we can distribute our updates for a single channel to two or more WatchTowers, to improve our privacy slightly (if we can somehow identify that two different WatchTowers are not somehow comparing notes; but simple use of Tor and similar can obscure that the same node is the source of communications).  It also seems desirable to retain this property, but under Decker-Russell-Osuntokun channels, when a new update is made, every WatchTower who is watching should be informed of that update (since only the latest update transaction is valid).  Under Poon-Dryja, if I have watching service contracts from several WatchTowers, when I get another revocation, I can distribute the revocation data to a subset of all my serving WatchTowers; under Decker-Russell-Osuntokun I would have to contact all of them to give the latest update transaction.\n\n---\n\nSo let me propose my half-baked idea for encrypted-blob, slightly-blinded Decker-Russell-Osuntokun WatchTowers:\n\n1.  We observe that for each update transaction, the signatures are malleable (sign malleability, also one signature is from the attacker and the attacker can pick a new R), but the message they are signing (a copy of the transaction that has been modified in particular ways) is not.\n2.  We observe that under SegWit the txid is simply an unusual form of the hash of the message (transaction) that is getting signed under a `SIGHASH_ALL` signature.  Thus if `txid[:16]` is acceptable as a key, then `hash[:16]` of the message signed in the update transactions should also be acceptable.\n3.  Thus we generalize our WatchTowers pairs to `(hash[:16], blob)`, where `hash` is the message that is signed in the witness program, not just the `txid`.\n4.  The same framework can be used for Poon-Dryja and Decker-Russell-Osuntokun WatchTowers.  Under Poon-Dryja the `hash` is the reverse of the `txid` (or rather, the `txid` is properly the reverse of the hash) being watched for.\n5.  The main loop of the WatchTower (which can watch either Poon-Dryja or Decker-Russell-Osuntokun channels) is now something like: When a block comes in, verify each transaction.  If during signature validation, we match the hash of the message being signed to a watched blob `hash[:16]`, we decrypt the blob and proceed to justice depending on the type of the decrypted blob.\n6.  Notice an important point: the same WatchTower, if it supported both Poon-Dryja and Decker-Russell-Osuntokun channels, would not know if a `(hash[:16], blob)` pair even belongs to a Poon-Dryja or a Decker-Russell-Osuntokun channel, unless a theft attempt was made! This is important as it lets us retain high anonymity sets while transitioning the network from Poon-Dryja to Decker-Russell-Osuntokun.\n7.  We handle Poon-Dryja channel blobs as with current Poon-Dryja techniques that I ahd Connor described last month.\n8.  For Decker-Russell-Osuntokun channels, the blob would contain: state number (`nLockTime`), `Au` and `Bu` pubkeys, the basepoints for `As,i` and `Bs,i`, and the corresponding signatures for the update transaction, the corresponding settlement transaction, and the settlement details (how much value each side, HTLCs in flight, CSV-delta, etc) as well as how the WatchTower bounty gets paid out.\n9.  The blob does not contain the latest channel update!  Instead, the WatchTower would iterate the state number repeatedly, and generate each possible future update transaction (without signatures).  This is because the message commits to constant data, an `nLockTime` we are iterating, a satoshi amount that is the same since update transactions pay no fees from the funding transaction, and a P2WSH that is derivable completely from `Au`, `Bu`, `As,i` and `Bs,i` basepoints, the current `nLockTime`, and the CSV-delta setting.  This yields new messages that are hashed and compared to other `hash[:16]` the WatchTower has.  If it matches another one, the WatchTower knows that at least one future update transaction exists: the WatchTower simply iterates until it finds the last channel update or reaches the maximum state number.\n10.  When the WatchTower finds the last channel update for the channel, and it is different from the message that triggered us, then it knows that the current published update transaction is not the latest, it knows the latest update, and it can now publish the latest update transaction and eventually its corresponding settlement transaction, in order to provide justice.\n\nThe above retains the decorrelation property (the WatchTower, given two `(hash[:16], blob)` pairs, cannot know if the two pairs refer to the same channel, or different channels), but only up to a theft attempt.  If a theft attempt occurs, then the WatchTower can now correlate the blobs (it might have kept records of when the blobs were given to it, and the decrypted blobs certainly give it history of how much money was allocated to both sides of the channel at that time) to a specific channel; hopefully theft attempts are rare enough that this is an acceptable amount of lost privacy.  If a node is served by many WatchTowers, it might elect to not give all updates to a single WatchTower (to reduce the privacy leakage in case of a theft attempt), so the WatchTower should also be prepared in that its latest known update might not actually be the latest.\n\nit also requires quite a bit of grinding on the WatchTower, as it has to generate a large number (possibly up to a billion) of possible future update transactions.\n\nRegards,\nZmnSCPxj\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180502/6347b578/attachment-0001.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-05-09T22:51:55",
                "message_text_only": "> However, this has the drawback that each update of a single channel will\n> generate a `(txid[:16], blob)` pair that has the same `txid[:16]` key,\n> letting the WatchTower correlate the timing and number of updates of each\nof\n> your channels.\n\nOn the other hand, going with the \"swap this blob\" approach is much, much\nmore scalable as each client of the watchtower only consumes a bounded\n(could say constant) amount of space. If you pad out the blob to the max\nsize accounting for HTLC limits (which further obfuscates the details of the\nupdate), then watchtowers can easily plan out the storage requirements as a\nfunction of the number of open clients.\n\nIn order to mitigate the timing leaks, clients don't need to _immediately_\nupdate their blob. Instead, they can add a level of randomization via a\ntimer drawn from a particular distribution. The downside of this is that\nthere's a window of opportunity wherein a new state is at play, yet the\nwatch tower hasn't yet been updated. Even with asymmetric commitments, one\nwould still want to introduce such randomization in order to de-correlate\ntheir updates (and also batch several updates into a single message).\n\n> It seems desirable to at least retain the property that the WatchTower\n> cannot correlate our updates to our channels.\n\nIs the scalability tradeoff worth it though?\n\n> since only the latest update transaction is valid\n\nAll update transactions are valid, just some are more valid than others ;)\n\n> Under Poon-Dryja, if I have watching service contracts from several\n> WatchTowers, when I get another revocation, I can distribute the\nrevocation\n> data to a subset of all my serving WatchTowers; under\n> Decker-Russell-Osuntokun I would have to contact all of them to give the\n> latest update transaction.\n\nNot necessarily. Due to the nature of the update transactions, several\nwatchtowers could even progressively advance the update transaction towards\nthe final most up to date state. Your constraint of \"i need to update them\nall each time\" seems arbitrary and isn't inherent to either of the schemes.\n\n--Laolu\n\n\nOn Wed, May 2, 2018 at 3:43 AM ZmnSCPxj via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> wrote:\n\n> Good morning list,\n>\n> I started considering, what a trustless, blinded WatchTower for eltoo\n> channels would look like.  Unfortunately, I cannot derive a good one yet,\n> sorry.\n>\n> My basis, is the same \"encrypted blob\" we use for Poon-Dryja channels.\n>\n> Now an issue is that update transactions in Decker-Russell-Osuntokun\n> channels have no txid that can be used as key for the encrypted blob.\n>\n> One thing we can use is the trigger transaction, which has a stable txid\n> since its sole input is an ordinary 2-of-2 that is signed with normal\n> `SIGHASH_ALL`.\n>\n> However, this has the drawback that each update of a single channel will\n> generate a `(txid[:16], blob)` pair that has the same `txid[:16]` key,\n> letting the WatchTower correlate the timing and number of updates of each\n> of your channels.  Compare to the case where a Poon-Dryja uses the same\n> encoding: the WatchTower knows each `(txid[:16], blob)` pair is *some*\n> update to *one* of your channels, but cannot know if any two pairs you\n> submit are for the same channel or two different channels.  Indeed even if\n> a theft attempt occurs the WatchTower can only know that some `txid[:16]`\n> belongs to a particular channel, but not whether the other `txid[:16]` you\n> published to it are to the same channel, or another channel you happen to\n> be interested in.\n>\n> It seems desirable to at least retain the property that the WatchTower\n> cannot correlate our updates to our channels.\n>\n> Another property that the Poon-Dryja encrypted-blob has is that we can\n> distribute our updates for a single channel to two or more WatchTowers, to\n> improve our privacy slightly (if we can somehow identify that two different\n> WatchTowers are not somehow comparing notes; but simple use of Tor and\n> similar can obscure that the same node is the source of communications).\n> It also seems desirable to retain this property, but under\n> Decker-Russell-Osuntokun channels, when a new update is made, every\n> WatchTower who is watching should be informed of that update (since only\n> the latest update transaction is valid).  Under Poon-Dryja, if I have\n> watching service contracts from several WatchTowers, when I get another\n> revocation, I can distribute the revocation data to a subset of all my\n> serving WatchTowers; under Decker-Russell-Osuntokun I would have to contact\n> all of them to give the latest update transaction.\n>\n> ---\n>\n> So let me propose my half-baked idea for encrypted-blob, slightly-blinded\n> Decker-Russell-Osuntokun WatchTowers:\n>\n> 1.  We observe that for each update transaction, the signatures are\n> malleable (sign malleability, also one signature is from the attacker and\n> the attacker can pick a new R), but the message they are signing (a copy of\n> the transaction that has been modified in particular ways) is not.\n> 2.  We observe that under SegWit the txid is simply an unusual form of the\n> hash of the message (transaction) that is getting signed under a\n> `SIGHASH_ALL` signature.  Thus if `txid[:16]` is acceptable as a key, then\n> `hash[:16]` of the message signed in the update transactions should also be\n> acceptable.\n> 3.  Thus we generalize our WatchTowers pairs to `(hash[:16], blob)`, where\n> `hash` is the message that is signed in the witness program, not just the\n> `txid`.\n> 4.  The same framework can be used for Poon-Dryja and\n> Decker-Russell-Osuntokun WatchTowers.  Under Poon-Dryja the `hash` is the\n> reverse of the `txid` (or rather, the `txid` is properly the reverse of the\n> hash) being watched for.\n> 5.  The main loop of the WatchTower (which can watch either Poon-Dryja or\n> Decker-Russell-Osuntokun channels) is now something like: When a block\n> comes in, verify each transaction.  If during signature validation, we\n> match the hash of the message being signed to a watched blob `hash[:16]`,\n> we decrypt the blob and proceed to justice depending on the type of the\n> decrypted blob.\n> 6.  Notice an important point: the same WatchTower, if it supported both\n> Poon-Dryja and Decker-Russell-Osuntokun channels, would not know if a\n> `(hash[:16], blob)` pair even belongs to a Poon-Dryja or a\n> Decker-Russell-Osuntokun channel, unless a theft attempt was made! This is\n> important as it lets us retain high anonymity sets while transitioning the\n> network from Poon-Dryja to Decker-Russell-Osuntokun.\n> 7.  We handle Poon-Dryja channel blobs as with current Poon-Dryja\n> techniques that I ahd Connor described last month.\n> 8.  For Decker-Russell-Osuntokun channels, the blob would contain: state\n> number (`nLockTime`), `Au` and `Bu` pubkeys, the basepoints for `As,i` and\n> `Bs,i`, and the corresponding signatures for the update transaction, the\n> corresponding settlement transaction, and the settlement details (how much\n> value each side, HTLCs in flight, CSV-delta, etc) as well as how the\n> WatchTower bounty gets paid out.\n> 9.  The blob does not contain the latest channel update!  Instead, the\n> WatchTower would iterate the state number repeatedly, and generate each\n> possible future update transaction (without signatures).  This is because\n> the message commits to constant data, an `nLockTime` we are iterating, a\n> satoshi amount that is the same since update transactions pay no fees from\n> the funding transaction, and a P2WSH that is derivable completely from\n> `Au`, `Bu`, `As,i` and `Bs,i` basepoints, the current `nLockTime`, and the\n> CSV-delta setting.  This yields new messages that are hashed and compared\n> to other `hash[:16]` the WatchTower has.  If it matches another one, the\n> WatchTower knows that at least one future update transaction exists: the\n> WatchTower simply iterates until it finds the last channel update or\n> reaches the maximum state number.\n> 10.  When the WatchTower finds the last channel update for the channel,\n> and it is different from the message that triggered us, then it knows that\n> the current published update transaction is not the latest, it knows the\n> latest update, and it can now publish the latest update transaction and\n> eventually its corresponding settlement transaction, in order to provide\n> justice.\n>\n> The above retains the decorrelation property (the WatchTower, given two\n> `(hash[:16], blob)` pairs, cannot know if the two pairs refer to the same\n> channel, or different channels), but only up to a theft attempt.  If a\n> theft attempt occurs, then the WatchTower can now correlate the blobs (it\n> might have kept records of when the blobs were given to it, and the\n> decrypted blobs certainly give it history of how much money was allocated\n> to both sides of the channel at that time) to a specific channel; hopefully\n> theft attempts are rare enough that this is an acceptable amount of lost\n> privacy.  If a node is served by many WatchTowers, it might elect to not\n> give all updates to a single WatchTower (to reduce the privacy leakage in\n> case of a theft attempt), so the WatchTower should also be prepared in that\n> its latest known update might not actually be the latest.\n>\n> it also requires quite a bit of grinding on the WatchTower, as it has to\n> generate a large number (possibly up to a billion) of possible future\n> update transactions.\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180509/8d8029a6/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-10T04:10:49",
                "message_text_only": "Good morning Laolu,\n\n>> However, this has the drawback that each update of a single channel will\n>> generate a `(txid[:16], blob)` pair that has the same `txid[:16]` key,\n>> letting the WatchTower correlate the timing and number of updates of each of\n>> your channels.\n>\n> On the other hand, going with the \"swap this blob\" approach is much, much\n> more scalable as each client of the watchtower only consumes a bounded\n> (could say constant) amount of space. If you pad out the blob to the max\n> size accounting for HTLC limits (which further obfuscates the details of the\n> update), then watchtowers can easily plan out the storage requirements as a\n> function of the number of open clients.\n\nIt would also mean that WatchTowers would need to identify their clients, a further privacy leak.  This is because if the WatchTower simply allows \"swap this blob\", the attacker can identify the WatchTowers the victim uses, and they know which txid they will use to attack.  They can then give a \"swap this blob\" with a blob that cannot be decrypted to a valid blob, disabling the WatchTower.\n\nPerhaps it could be made to work if the WatchTower demands a public key + signature.  If this is the first time the `txid[:16]` is used, then the WatchTower accepts the public key.  For future times (i.e. replacement requests) the WatchTower checks the signature and public key of the replacement vs the public key of the first time, and only accepts if the public key matches and the signature is verified for the public key.  But this can still be attacked: prior to signing the transaction, the attacker determines the txid that will be monitored by the WatchTower, and before sending the signature to the victim to start the channel, the attacker gives a `(txid[:16], blob)` with invalid blob to the WatchTower.  The victim receives the signatures, then creates the proper blob for the first state, and then gives the valid blob to the WatchTower --- who rejects it, because an existing blob from the attacker is already reserved to the attacker public key.\n\nThis can be prevented if the WatchTower knows a stable identity (possibly pseudonymous) of the victim and attacker.  See: https://lists.linuxfoundation.org/pipermail/lightning-dev/2018-April/001203.html for my ruminations on this topic.  In that post, the WatchTower charges for each `(txid[:16], blob)` pair it stores and monitors, and will store multiple pairs with the same `txid[:16]` key if they are from different tickets.  Charging for the service reduces DoS attacks, and storing multiple pairs with the same key prevents key-reservation attacks like the above I described.  It also lets the service be used completely anonymously, without even a pseudonymous public key.  The ticket scheme lets the WatchTower correlate updates coming from a single ticket, but this is limited since each ticket purchased has a finite number of uses.\n\nAn enhancement to the above would be to restructure things so that WatchTowers sell ticketbooks of multiple tickets.  Each ticket is valid for storing one `(txid[:16], blob)` pair, and WatchTowers will store multiple `(txid[:16], blob)` pairs with the same txid.  Then the WatchTower can operate as Chaumian bank on the tickets they sell, so that clients in theory can swap tickets from the same WatchTower with each other to decorrelate uses of tickets within a ticketbook.\n\n> In order to mitigate the timing leaks, clients don't need to _immediately_\n> update their blob. Instead, they can add a level of randomization via a\n> timer drawn from a particular distribution. The downside of this is that\n> there's a window of opportunity wherein a new state is at play, yet the\n> watch tower hasn't yet been updated. Even with asymmetric commitments, one\n> would still want to introduce such randomization in order to de-correlate\n> their updates (and also batch several updates into a single message).\n>\n>> It seems desirable to at least retain the property that the WatchTower\n>> cannot correlate our updates to our channels.\n>\n> Is the scalability tradeoff worth it though?\n\nI think not.\n\n>> since only the latest update transaction is valid\n>\n> All update transactions are valid, just some are more valid than others ;)\n\nHaha, yes.\n\n>> Under Poon-Dryja, if I have watching service contracts from several\n>> WatchTowers, when I get another revocation, I can distribute the revocation\n>> data to a subset of all my serving WatchTowers; under\n>> Decker-Russell-Osuntokun I would have to contact all of them to give the\n>> latest update transaction.\n>\n> Not necessarily. Due to the nature of the update transactions, several\n> watchtowers could even progressively advance the update transaction towards\n> the final most up to date state. Your constraint of \"i need to update them\n> all each time\" seems arbitrary and isn't inherent to either of the schemes.\n\nYou are right and this is my mistake.\n\nRegards,\nZmnSCPxj\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180510/f0a38387/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "eltoo Trustless WatchTowers",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Olaoluwa Osuntokun",
                "ZmnSCPxj"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 22016
        }
    },
    {
        "title": "[Lightning-dev] Receiving via unpublished channels",
        "thread_messages": [
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-05-07T19:39:43",
                "message_text_only": "AFAIK, all the other implementations already do this (lnd does at least\n[1]).  As otherwise, it wouldn't be possible to properly utilize routing\nhints.\n\n> I want to ask the other LN implementations (lnd, eclair, ucoin, lit)\n\nAs an side, what's \"ucoin\"? Searched for a bit and didn't find anything\nnotable.\n\n[1]:\nhttps://github.com/lightningnetwork/lnd/blob/master/discovery/gossiper.go#L1747\n\nOn Thu, Apr 26, 2018 at 4:35 PM ZmnSCPxj via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> wrote:\n\n> Good morning list,\n>\n> While implementing support for `r` field in invoices, I stumbled upon some\n> issues regarding *creating* invoices with `r` fields.\n>\n> In order to receive via an unpublished channel, we need to know what\n> onLightning fees the other side of that channel wants to charge.  We cannot\n> use our own onLightning fees because our fees apply if we were forwarding\n> to the other side.\n>\n> However, in case of an unpublished channel, we do not send\n> channel_announcement, and in that case we do not send channel_update.  So\n> the other side of the channel never informs us of the onLightning fees they\n> want to charge if we would receive funds by this channel.\n>\n> An idea we want to consider is to simply send `channel_update` as soon as\n> we lock in the channel:\n> https://github.com/ElementsProject/lightning/pull/1330#issuecomment-383931817\n>\n> I want to ask the other LN implementations (lnd, eclair, ucoin, lit) if we\n> should consider standardizing this behavior (i.e. send `channel_update`\n> after lockin  regardless of published/unpublished state).  It seems\n> back-compatible: software which does not expect this behavior will simply\n> drop the `channel_update` (as they do not follow a `channel_announcement`).\n>\n> In any case, what was the intended way to get the onLightning fee rates to\n> put into invoice `r` fields for private routes?\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180507/0c332d7c/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-07T22:39:43",
                "message_text_only": "Good morning Olauluwa\n\nIt is Ptarmigan Lightning Network project from Nayuta Ueno/Nayutaco: https://github.com/nayutaco/ptarmigan\n\nName of daemon is ucoind, CLI is ucoincli.\n\nRegards,\nZmnSCPxj\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180507/d22cd3cd/attachment.html>"
            },
            {
                "author": "ueno at nayuta.co",
                "date": "2018-05-08T06:33:53",
                "message_text_only": "Hi, ZmnSCPxj.\n\n> I want to ask the other LN implementations (lnd, eclair, ucoin, lit) if we should consider standardizing this behavior (i.e. send `channel_update`\n> after lockin  regardless of published/unpublished state).\n\nucoin/ptarmigan does not support `r` field yet(just decode and discards it).\n\n\nRegards,\nnayuta-ueno\n\n(https://github.com/nayutaco/ptarmigan)\n\nOn 2018/04/27 8:35, ZmnSCPxj via Lightning-dev wrote:\n> Good morning list,\n> \n> While implementing support for `r` field in invoices, I stumbled upon some issues regarding *creating* invoices with `r` fields.\n> \n> In order to receive via an unpublished channel, we need to know what onLightning fees the other side of that channel wants to charge.\u00a0 We cannot use \n> our own onLightning fees because our fees apply if we were forwarding to the other side.\n> \n> However, in case of an unpublished channel, we do not send channel_announcement, and in that case we do not send channel_update.\u00a0 So the other side of \n> the channel never informs us of the onLightning fees they want to charge if we would receive funds by this channel.\n> \n> An idea we want to consider is to simply send `channel_update` as soon as we lock in the channel: \n> https://github.com/ElementsProject/lightning/pull/1330#issuecomment-383931817\n> \n> I want to ask the other LN implementations (lnd, eclair, ucoin, lit) if we should consider standardizing this behavior (i.e. send `channel_update` \n> after lockin\u00a0 regardless of published/unpublished state).\u00a0 It seems back-compatible: software which does not expect this behavior will simply drop the \n> `channel_update` (as they do not follow a `channel_announcement`).\n> \n> In any case, what was the intended way to get the onLightning fee rates to put into invoice `r` fields for private routes?\n> \n> Regards,\n> ZmnSCPxj\n> \n> \n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>"
            },
            {
                "author": "Hiroshi UENO",
                "date": "2018-05-09T04:49:40",
                "message_text_only": "Hi, ZmnSCPxj.\n\n > An idea we want to consider is to simply send `channel_update` as soon as we lock in the channel:\n > https://github.com/ElementsProject/lightning/pull/1330#issuecomment-383931817\n\nOur node(ptarmigan) discard `channel_update` without `channel_announcement`.\nWe want to implement it.\n\nI have some questions.\n   * Does `channel_update` send only for `announce_channel == 0`(in` open_channel.channel_flags`) ?\n   * Does `channel_update` send every time after `funding_locked` sending(established and reconnection) ?\n\n\nRegards,\nnayuta-ueno\n\nOn 2018/04/27 8:35, ZmnSCPxj via Lightning-dev wrote:\n> Good morning list,\n> \n> While implementing support for `r` field in invoices, I stumbled upon some issues regarding *creating* invoices with `r` fields.\n> \n> In order to receive via an unpublished channel, we need to know what onLightning fees the other side of that channel wants to charge.\u00a0 We cannot use \n> our own onLightning fees because our fees apply if we were forwarding to the other side.\n> \n> However, in case of an unpublished channel, we do not send channel_announcement, and in that case we do not send channel_update.\u00a0 So the other side of \n> the channel never informs us of the onLightning fees they want to charge if we would receive funds by this channel.\n> \n> An idea we want to consider is to simply send `channel_update` as soon as we lock in the channel: \n> https://github.com/ElementsProject/lightning/pull/1330#issuecomment-383931817\n> \n> I want to ask the other LN implementations (lnd, eclair, ucoin, lit) if we should consider standardizing this behavior (i.e. send `channel_update` \n> after lockin\u00a0 regardless of published/unpublished state).\u00a0 It seems back-compatible: software which does not expect this behavior will simply drop the \n> `channel_update` (as they do not follow a `channel_announcement`).\n> \n> In any case, what was the intended way to get the onLightning fee rates to put into invoice `r` fields for private routes?\n> \n> Regards,\n> ZmnSCPxj\n> \n> \n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n> \n\n-- \n//\u682a\u5f0f\u4f1a\u793eNayuta\n//  \u4e0a\u91ce\u3000\u5bdb(Hiroshi Ueno)\n//     email: ueno at nayuta.co"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-09T05:17:26",
                "message_text_only": "Good morning Nayuta-ueno,\n\n> I have some questions.\n> \n> -   Does `channel_update` send only for `announce_channel == 0`(in`open_channel.channel_flags`) ?\n\nNo, C-lightning always send it.\n\n> -   Does `channel_update` send every time after `funding_locked` sending(established and reconnection) ?\n\nFor C-lightning, the temporary ones are sent after `funding_locked` has been sent by both sides, as well as reconnection (only if both sides have sent `funding_locked`).  The only time they are not, is if the channel is shutting down (and so any fee updates would be pointless, as the channel will no longer be useable for forwarding, sending, or receiving) or the channel has already achieved the announcement depth.\n\nAfter announcement depth, \"normal\" `channel_update` should get sent after a `channel_announcement`, and not the temporary ones.\n\nRegards,\nZmnSCPxj\n\n>     \n>     Regards,\n>     \n>     nayuta-ueno\n>     \n>     On 2018/04/27 8:35, ZmnSCPxj via Lightning-dev wrote:\n>     \n> \n> > Good morning list,\n> > \n> > While implementing support for `r` field in invoices, I stumbled upon some issues regarding creating invoices with `r` fields.\n> > \n> > In order to receive via an unpublished channel, we need to know what onLightning fees the other side of that channel wants to charge.\u00a0 We cannot use\n> > \n> > our own onLightning fees because our fees apply if we were forwarding to the other side.\n> > \n> > However, in case of an unpublished channel, we do not send channel_announcement, and in that case we do not send channel_update.\u00a0 So the other side of\n> > \n> > the channel never informs us of the onLightning fees they want to charge if we would receive funds by this channel.\n> > \n> > An idea we want to consider is to simply send `channel_update` as soon as we lock in the channel:\n> > \n> > https://github.com/ElementsProject/lightning/pull/1330#issuecomment-383931817\n> > \n> > I want to ask the other LN implementations (lnd, eclair, ucoin, lit) if we should consider standardizing this behavior (i.e. send `channel_update`\n> > \n> > after lockin\u00a0 regardless of published/unpublished state).\u00a0 It seems back-compatible: software which does not expect this behavior will simply drop the\n> > \n> > `channel_update` (as they do not follow a `channel_announcement`).\n> > \n> > In any case, what was the intended way to get the onLightning fee rates to put into invoice `r` fields for private routes?\n> > \n> > Regards,\n> > \n> > ZmnSCPxj\n> > \n> > Lightning-dev mailing list\n> > \n> > Lightning-dev at lists.linuxfoundation.org\n> > \n> > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n> \n> --\n> \n> //\u682a\u5f0f\u4f1a\u793eNayuta\n> \n> // \u4e0a\u91ce\u3000\u5bdb(Hiroshi Ueno)\n> \n> // email: ueno at nayuta.co\n> \n> Lightning-dev mailing list\n> \n> Lightning-dev at lists.linuxfoundation.org\n> \n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "Hiroshi UENO",
                "date": "2018-05-09T05:55:04",
                "message_text_only": "Hi, ZmnSCPxj.\n\nThank you for your reply and explanation.\nI will implement it like c-lightning to ptarmigan.\n\n\nRegards,\nnayuta-ueno\n\nOn 2018/05/09 14:17, ZmnSCPxj wrote:\n> Good morning Nayuta-ueno,\n> \n>> I have some questions.\n>>\n>> -   Does `channel_update` send only for `announce_channel == 0`(in`open_channel.channel_flags`) ?\n> \n> No, C-lightning always send it.\n> \n>> -   Does `channel_update` send every time after `funding_locked` sending(established and reconnection) ?\n> \n> For C-lightning, the temporary ones are sent after `funding_locked` has been sent by both sides, as well as reconnection (only if both sides have sent `funding_locked`).  The only time they are not, is if the channel is shutting down (and so any fee updates would be pointless, as the channel will no longer be useable for forwarding, sending, or receiving) or the channel has already achieved the announcement depth.\n> \n> After announcement depth, \"normal\" `channel_update` should get sent after a `channel_announcement`, and not the temporary ones.\n> \n> Regards,\n> ZmnSCPxj\n> \n>>      \n>>      Regards,\n>>      \n>>      nayuta-ueno\n>>      \n>>      On 2018/04/27 8:35, ZmnSCPxj via Lightning-dev wrote:\n>>      \n>>\n>>> Good morning list,\n>>>\n>>> While implementing support for `r` field in invoices, I stumbled upon some issues regarding creating invoices with `r` fields.\n>>>\n>>> In order to receive via an unpublished channel, we need to know what onLightning fees the other side of that channel wants to charge.\u00a0 We cannot use\n>>>\n>>> our own onLightning fees because our fees apply if we were forwarding to the other side.\n>>>\n>>> However, in case of an unpublished channel, we do not send channel_announcement, and in that case we do not send channel_update.\u00a0 So the other side of\n>>>\n>>> the channel never informs us of the onLightning fees they want to charge if we would receive funds by this channel.\n>>>\n>>> An idea we want to consider is to simply send `channel_update` as soon as we lock in the channel:\n>>>\n>>> https://github.com/ElementsProject/lightning/pull/1330#issuecomment-383931817\n>>>\n>>> I want to ask the other LN implementations (lnd, eclair, ucoin, lit) if we should consider standardizing this behavior (i.e. send `channel_update`\n>>>\n>>> after lockin\u00a0 regardless of published/unpublished state).\u00a0 It seems back-compatible: software which does not expect this behavior will simply drop the\n>>>\n>>> `channel_update` (as they do not follow a `channel_announcement`).\n>>>\n>>> In any case, what was the intended way to get the onLightning fee rates to put into invoice `r` fields for private routes?\n>>>\n>>> Regards,\n>>>\n>>> ZmnSCPxj\n>>>\n>>> Lightning-dev mailing list\n>>>\n>>> Lightning-dev at lists.linuxfoundation.org\n>>>\n>>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n>> --\n>>\n>> //\u682a\u5f0f\u4f1a\u793eNayuta\n>>\n>> // \u4e0a\u91ce\u3000\u5bdb(Hiroshi Ueno)\n>>\n>> // email: ueno at nayuta.co\n>>\n>> Lightning-dev mailing list\n>>\n>> Lightning-dev at lists.linuxfoundation.org\n>>\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n> \n> \n\n-- \n//\u682a\u5f0f\u4f1a\u793eNayuta\n//  \u4e0a\u91ce\u3000\u5bdb(Hiroshi Ueno)\n//     email: ueno at nayuta.co"
            },
            {
                "author": "Hiroshi Ueno",
                "date": "2018-05-15T14:04:15",
                "message_text_only": "Hello ZmnSCPxj,\n\nI'm implementing `r` field now to `ptarmigan`.\nI have more question.\n\n`r` field doesn't contain signature like `channel_update`.\nDo c-lightning check something in the `r` field from payee's invoice?\n\n\nRegards,\nnayuta-ueno\n\nOn 2018/05/09 14:17, ZmnSCPxj wrote:\n> Good morning Nayuta-ueno,\n> \n>> I have some questions.\n>>\n>> -   Does `channel_update` send only for `announce_channel == 0`(in`open_channel.channel_flags`) ?\n> \n> No, C-lightning always send it.\n> \n>> -   Does `channel_update` send every time after `funding_locked` sending(established and reconnection) ?\n> \n> For C-lightning, the temporary ones are sent after `funding_locked` has been sent by both sides, as well as reconnection (only if both sides have sent `funding_locked`).  The only time they are not, is if the channel is shutting down (and so any fee updates would be pointless, as the channel will no longer be useable for forwarding, sending, or receiving) or the channel has already achieved the announcement depth.\n> \n> After announcement depth, \"normal\" `channel_update` should get sent after a `channel_announcement`, and not the temporary ones.\n> \n> Regards,\n> ZmnSCPxj\n> \n>>      \n>>      Regards,\n>>      \n>>      nayuta-ueno\n>>      \n>>      On 2018/04/27 8:35, ZmnSCPxj via Lightning-dev wrote:\n>>      \n>>\n>>> Good morning list,\n>>>\n>>> While implementing support for `r` field in invoices, I stumbled upon some issues regarding creating invoices with `r` fields.\n>>>\n>>> In order to receive via an unpublished channel, we need to know what onLightning fees the other side of that channel wants to charge.\u00a0 We cannot use\n>>>\n>>> our own onLightning fees because our fees apply if we were forwarding to the other side.\n>>>\n>>> However, in case of an unpublished channel, we do not send channel_announcement, and in that case we do not send channel_update.\u00a0 So the other side of\n>>>\n>>> the channel never informs us of the onLightning fees they want to charge if we would receive funds by this channel.\n>>>\n>>> An idea we want to consider is to simply send `channel_update` as soon as we lock in the channel:\n>>>\n>>> https://github.com/ElementsProject/lightning/pull/1330#issuecomment-383931817\n>>>\n>>> I want to ask the other LN implementations (lnd, eclair, ucoin, lit) if we should consider standardizing this behavior (i.e. send `channel_update`\n>>>\n>>> after lockin\u00a0 regardless of published/unpublished state).\u00a0 It seems back-compatible: software which does not expect this behavior will simply drop the\n>>>\n>>> `channel_update` (as they do not follow a `channel_announcement`).\n>>>\n>>> In any case, what was the intended way to get the onLightning fee rates to put into invoice `r` fields for private routes?\n>>>\n>>> Regards,\n>>>\n>>> ZmnSCPxj\n>>>\n>>> Lightning-dev mailing list\n>>>\n>>> Lightning-dev at lists.linuxfoundation.org\n>>>\n>>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-15T14:23:06",
                "message_text_only": "Good morning nayuto-ueno,\n\n> `r` field doesn't contain signature like `channel_update`.\n> \n> Do c-lightning check something in the `r` field from payee's invoice?\n> \n\nNo.  Invoice has signature for whole invoice.  Of course, only payee signature (fee of this channel is fee from the peer of the payee, e.g. hub).\n\nPayee has incentive to give accurate information about the channel.  Otherwise payment may fail if the channel fees or CLTV-delta, is not same as wanted by the peer of the payee.  So only payee signature should be enough; if payee gives bad information about this channel, payee loses on getting paid.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Hiroshi Ueno",
                "date": "2018-05-15T14:54:08",
                "message_text_only": "Hello ZmnSCPxj,\n\n > Payee has incentive to give accurate information about the channel.\nI understand.\nThank you very much.\n\n\nRegards,\nnayuta-ueno\n\nOn 2018/05/15 23:23, ZmnSCPxj wrote:\n> Good morning nayuto-ueno,\n> \n>> `r` field doesn't contain signature like `channel_update`.\n>>\n>> Do c-lightning check something in the `r` field from payee's invoice?\n>>\n> \n> No.  Invoice has signature for whole invoice.  Of course, only payee signature (fee of this channel is fee from the peer of the payee, e.g. hub).\n> \n> Payee has incentive to give accurate information about the channel.  Otherwise payment may fail if the channel fees or CLTV-delta, is not same as wanted by the peer of the payee.  So only payee signature should be enough; if payee gives bad information about this channel, payee loses on getting paid.\n> \n> Regards,\n> ZmnSCPxj\n>"
            }
        ],
        "thread_summary": {
            "title": "Receiving via unpublished channels",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Hiroshi UENO",
                "ueno at nayuta.co",
                "Hiroshi Ueno",
                "Olaoluwa Osuntokun",
                "ZmnSCPxj"
            ],
            "messages_count": 9,
            "total_messages_chars_count": 17276
        }
    },
    {
        "title": "[Lightning-dev] BIP sighash_noinput",
        "thread_messages": [
            {
                "author": "Christian Decker",
                "date": "2018-05-07T19:40:46",
                "message_text_only": "Given the general enthusiasm, and lack of major criticism, for the\n`SIGHASH_NOINPUT` proposal, I'd like to formally ask the BBEs (benevolent\nBIP editors) to be assigned a BIP number. I have hacked together a\nsimple implementation of the hashing implementation in Bitcoin Core [1]\nthough I think it's unlikely to sail through review, and given the lack\nof ground-work on witness V1 scripts, I can't really test it now, and\nonly the second commit is part of the implementation itself.\n\nOne issue that was raised off list was that some fork coins have used\nsighash 0x40 as FORKID. This does not conflict with this proposal since\nthe proposal only applies to segwit transactions, which the fork coins\nhave explicitly disabled :-)\n\nI'm looking forward to discussing how to we can move forward to\nimplementing this proposal, and how we can combine multiple proposals\ninto the next soft-fork.\n\nCheers,\nChristian\n\n[1] https://github.com/cdecker/bitcoin/tree/noinput"
            }
        ],
        "thread_summary": {
            "title": "BIP sighash_noinput",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Christian Decker"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 957
        }
    },
    {
        "title": "[Lightning-dev] [bitcoin-dev] BIP sighash_noinput",
        "thread_messages": [
            {
                "author": "Bram Cohen",
                "date": "2018-05-07T20:51:11",
                "message_text_only": "A technical point about SIGHASH_NOINPUT: It seems like a more general and\ntechnically simpler to implement idea would be to have a boolean specifying\nwhether the inputs listed must be all of them (the way it works normally)\nor a subset of everything. It feels like a similar boolean should be made\nfor outputs as well. Or maybe a single boolean should apply to both. In any\ncase, one could always use SIGHASH_SUBSET and not specify any inputs and\nthat would have the same effect as SIGHASH_NOINPUT.\n\nOn Mon, May 7, 2018 at 12:40 PM, Christian Decker via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Given the general enthusiasm, and lack of major criticism, for the\n> `SIGHASH_NOINPUT` proposal, I'd like to formally ask the BBEs (benevolent\n> BIP editors) to be assigned a BIP number. I have hacked together a\n> simple implementation of the hashing implementation in Bitcoin Core [1]\n> though I think it's unlikely to sail through review, and given the lack\n> of ground-work on witness V1 scripts, I can't really test it now, and\n> only the second commit is part of the implementation itself.\n>\n> One issue that was raised off list was that some fork coins have used\n> sighash 0x40 as FORKID. This does not conflict with this proposal since\n> the proposal only applies to segwit transactions, which the fork coins\n> have explicitly disabled :-)\n>\n> I'm looking forward to discussing how to we can move forward to\n> implementing this proposal, and how we can combine multiple proposals\n> into the next soft-fork.\n>\n> Cheers,\n> Christian\n>\n> [1] https://github.com/cdecker/bitcoin/tree/noinput\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180507/6a2c236a/attachment.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2018-05-08T14:40:21",
                "message_text_only": "On Mon, May 07, 2018 at 09:40:46PM +0200, Christian Decker via bitcoin-dev wrote:\n> Given the general enthusiasm, and lack of major criticism, for the\n> `SIGHASH_NOINPUT` proposal, [...]\n\nSo first, I'm not sure if I'm actually criticising or playing devil's\nadvocate here, but either way I think criticism always helps produce\nthe best proposal, so....\n\nThe big concern I have with _NOINPUT is that it has a huge failure\ncase: if you use the same key for multiple inputs and sign one of them\nwith _NOINPUT, you've spent all of them. The current proposal kind-of\nlimits the potential damage by still committing to the prevout amount,\nbut it still seems a big risk for all the people that reuse addresses,\nwhich seems to be just about everyone.\n\nI wonder if it wouldn't be ... I'm not sure better is the right word,\nbut perhaps \"more realistic\" to have _NOINPUT be a flag to a signature\nfor a hypothetical \"OP_CHECK_SIG_FOR_SINGLE_USE_KEY\" opcode instead,\nso that it's fundamentally not possible to trick someone who regularly\nreuses keys to sign something for one input that accidently authorises\nspends of other inputs as well.\n\nIs there any reason why an OP_CHECKSIG_1USE (or OP_CHECKMULTISIG_1USE)\nwouldn't be equally effective for the forseeable usecases? That would\nensure that a _NOINPUT signature is only ever valid for keys deliberately\nintended to be single use, rather than potentially valid for every key.\n\nIt would be ~34 witness bytes worse than being able to spend a Schnorr\naggregate key directly, I guess; but that's not worse than the normal\ntaproot tradeoff: you spend the aggregate key directly in the normal,\ncooperative case; and reserve the more expensive/NOINPUT case for the\nunusual, uncooperative cases. I believe that works fine for eltoo: in\nthe cooperative case you just do a SIGHASH_ALL spend of the original\ntransaction, and _NOINPUT isn't needed.\n\nMaybe a different opcode maybe makes sense at a \"philosophical\" level:\nnormal signatures are signing a spend of a particular \"coin\" (in the\nUTXO sense), while _NOINPUT signatures are in some sense signing a spend\nof an entire \"wallet\" (all the coins spendable by a particular key, or\nmore accurately for the current proposal, all the coins of a particular\nvalue spendable by a particular key). Those are different intentions,\nso maybe it's reasonable to encode them in different addresses, which\nin turn could be done by having a new opcode for _NOINPUT.\n\nA new opcode has the theoretical advantage that it could be deployed\ninto the existing segwit v0 address space, rather than waiting for segwit\nv1. Not sure that's really meaningful, though.\n\nCheers,\naj"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-05-09T23:01:39",
                "message_text_only": "> The current proposal kind-of limits the potential damage by still\ncommitting\n> to the prevout amount, but it still seems a big risk for all the people\nthat\n> reuse addresses, which seems to be just about everyone.\n\nThe typical address re-use doesn't apply here as this is a sighash flag that\nwould only really be used for doing various contracts on Bitcoin. I don't\nsee any reason why \"regular\" wallets would update to use this sighash flag.\nWe've also seen first hand with segwit that wallet authors are slow to pull\nin the latest and greatest features available, even if they solve nuisance\nissues like malleability and can result in lower fees.\n\nIMO, sighash_none is an even bigger footgun that already exists in the\nprotocol today.\n\n-- Laolu\n\n\nOn Tue, May 8, 2018 at 7:41 AM Anthony Towns via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Mon, May 07, 2018 at 09:40:46PM +0200, Christian Decker via bitcoin-dev\n> wrote:\n> > Given the general enthusiasm, and lack of major criticism, for the\n> > `SIGHASH_NOINPUT` proposal, [...]\n>\n> So first, I'm not sure if I'm actually criticising or playing devil's\n> advocate here, but either way I think criticism always helps produce\n> the best proposal, so....\n>\n> The big concern I have with _NOINPUT is that it has a huge failure\n> case: if you use the same key for multiple inputs and sign one of them\n> with _NOINPUT, you've spent all of them. The current proposal kind-of\n> limits the potential damage by still committing to the prevout amount,\n> but it still seems a big risk for all the people that reuse addresses,\n> which seems to be just about everyone.\n>\n> I wonder if it wouldn't be ... I'm not sure better is the right word,\n> but perhaps \"more realistic\" to have _NOINPUT be a flag to a signature\n> for a hypothetical \"OP_CHECK_SIG_FOR_SINGLE_USE_KEY\" opcode instead,\n> so that it's fundamentally not possible to trick someone who regularly\n> reuses keys to sign something for one input that accidently authorises\n> spends of other inputs as well.\n>\n> Is there any reason why an OP_CHECKSIG_1USE (or OP_CHECKMULTISIG_1USE)\n> wouldn't be equally effective for the forseeable usecases? That would\n> ensure that a _NOINPUT signature is only ever valid for keys deliberately\n> intended to be single use, rather than potentially valid for every key.\n>\n> It would be ~34 witness bytes worse than being able to spend a Schnorr\n> aggregate key directly, I guess; but that's not worse than the normal\n> taproot tradeoff: you spend the aggregate key directly in the normal,\n> cooperative case; and reserve the more expensive/NOINPUT case for the\n> unusual, uncooperative cases. I believe that works fine for eltoo: in\n> the cooperative case you just do a SIGHASH_ALL spend of the original\n> transaction, and _NOINPUT isn't needed.\n>\n> Maybe a different opcode maybe makes sense at a \"philosophical\" level:\n> normal signatures are signing a spend of a particular \"coin\" (in the\n> UTXO sense), while _NOINPUT signatures are in some sense signing a spend\n> of an entire \"wallet\" (all the coins spendable by a particular key, or\n> more accurately for the current proposal, all the coins of a particular\n> value spendable by a particular key). Those are different intentions,\n> so maybe it's reasonable to encode them in different addresses, which\n> in turn could be done by having a new opcode for _NOINPUT.\n>\n> A new opcode has the theoretical advantage that it could be deployed\n> into the existing segwit v0 address space, rather than waiting for segwit\n> v1. Not sure that's really meaningful, though.\n>\n> Cheers,\n> aj\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180509/ba4835de/attachment-0001.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2018-05-09T23:04:58",
                "message_text_only": "Anthony Towns via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> writes:\n> On Mon, May 07, 2018 at 09:40:46PM +0200, Christian Decker via bitcoin-dev wrote:\n>> Given the general enthusiasm, and lack of major criticism, for the\n>> `SIGHASH_NOINPUT` proposal, [...]\n>\n> So first, I'm not sure if I'm actually criticising or playing devil's\n> advocate here, but either way I think criticism always helps produce\n> the best proposal, so....\n>\n> The big concern I have with _NOINPUT is that it has a huge failure\n> case: if you use the same key for multiple inputs and sign one of them\n> with _NOINPUT, you've spent all of them. The current proposal kind-of\n> limits the potential damage by still committing to the prevout amount,\n> but it still seems a big risk for all the people that reuse addresses,\n> which seems to be just about everyone.\n\nIf I can convince you to sign with SIGHASH_NONE, it's already a problem\ntoday.\n\n> I wonder if it wouldn't be ... I'm not sure better is the right word,\n> but perhaps \"more realistic\" to have _NOINPUT be a flag to a signature\n> for a hypothetical \"OP_CHECK_SIG_FOR_SINGLE_USE_KEY\" opcode instead,\n> so that it's fundamentally not possible to trick someone who regularly\n> reuses keys to sign something for one input that accidently authorises\n> spends of other inputs as well.\n\nThat was also suggested by Mark Friedenbach, but I think we'll end up\nwith more \"magic key\" a-la Schnorr/taproot/graftroot and less script in\nfuture.\n\nThat means we'd actually want a different Segwit version for\n\"NOINPUT-can-be-used\", which seems super ugly.\n\n> Maybe a different opcode maybe makes sense at a \"philosophical\" level:\n> normal signatures are signing a spend of a particular \"coin\" (in the\n> UTXO sense), while _NOINPUT signatures are in some sense signing a spend\n> of an entire \"wallet\" (all the coins spendable by a particular key, or\n> more accurately for the current proposal, all the coins of a particular\n> value spendable by a particular key). Those are different intentions,\n> so maybe it's reasonable to encode them in different addresses, which\n> in turn could be done by having a new opcode for _NOINPUT.\n\nIn a world where SIGHASH_NONE didn't exist, this might be an argument :)\n\nCheers,\nRusty."
            },
            {
                "author": "Anthony Towns",
                "date": "2018-05-14T09:23:29",
                "message_text_only": "On Thu, May 10, 2018 at 08:34:58AM +0930, Rusty Russell wrote:\n> > The big concern I have with _NOINPUT is that it has a huge failure\n> > case: if you use the same key for multiple inputs and sign one of them\n> > with _NOINPUT, you've spent all of them. The current proposal kind-of\n> > limits the potential damage by still committing to the prevout amount,\n> > but it still seems a big risk for all the people that reuse addresses,\n> > which seems to be just about everyone.\n> If I can convince you to sign with SIGHASH_NONE, it's already a problem\n> today.\n\nSo, I don't find that very compelling: \"there's already a way to lose\nyour money, so it's fine to add other ways to lose your money\". And\nagain, I think NOINPUT is worse here, because a SIGHASH_NONE signature\nonly lets others take the coin you're trying to spend, messing up when\nusing NOINPUT can cause you to lose other coins as well (with caveats).\n\n> [...]\n> In a world where SIGHASH_NONE didn't exist, this might be an argument :)\n\nI could see either dropping support for SIGHASH_NONE for segwit\nv1 addresses, or possibly limiting SIGHASH_NONE in a similar way to\nlimiting SIGHASH_NOINPUT. Has anyone dug through the blockchain to see\nif SIGHASH_NONE is actually used/useful?\n\n> That was also suggested by Mark Friedenbach, but I think we'll end up\n> with more \"magic key\" a-la Schnorr/taproot/graftroot and less script in\n> future.\n\nTaproot and graftroot aren't \"less script\" at all -- if anything they're\nthe opposite in that suddenly every address can have a script path.\nI think NOINPUT has pretty much the same tradeoffs as taproot/graftroot\nscripts: in the normal case for both you just use a SIGHASH_ALL\nsignature to spend your funds; in the abnormal case for NOINPUT, you use\na SIGHASH_NOINPUT (multi)sig for unilateral eltoo closes or watchtower\npenalties, in the abnormal case for taproot/graftroot you use a script.\n\n> That means we'd actually want a different Segwit version for\n> \"NOINPUT-can-be-used\", which seems super ugly.\n\nThat's backwards. If you introduce a new opcode, you can use the existing\nsegwit version, rather than needing segwit v1. You certainly don't need\nv1 segwit for regular coins and v2 segwit for NOINPUT coins, if that's\nwhere you were going?\n\nFor segwit v0, that would mean your addresses for a key \"X\", might be:\n\n   [pubkey]  X    \n    - not usable with NOINPUT\n   [script]  2 X Y 2 CHECKMULTISIG\n    - not usable with NOINPUT\n   [script]  2 X Y 2 CHECKMULTISIG_1USE_VERIFY\n    - usable with NOINPUT (or SIGHASH_ALL)\n\nCHECKMULTISIG_1USE_VERIFY being soft-forked in by replacing an OP_NOP,\nof course. Any output spendable via a NOINPUT signature would then have\nhad to have been deliberately created as being spendable by NOINPUT.\n\nFor a new segwit version with taproot that likewise includes an opcode,\nthat might be:\n\n   [taproot]  X\n    - not usable with NOINPUT\n   [taproot]  X or: X CHECKSIG_1USE\n    - usable with NOINPUT\n\nIf you had two UTXOs (with the same value), then if you construct\na taproot witness script for the latter address it will look like:\n\n    X [X CHECKSIG_1USE] [sig_X_NOINPUT]\n\nand that signature can't be used for addresses that were just intending\nto pay to X, because the NOINPUT sig/sighash simply isn't supported\nwithout a taproot path that includes the CHECKSIG_1USE opcode.\n\nIn essence, with the above construction there's two sorts of addresses\nyou generate from a public key X: addresses where you spend each coin\nindividually, and different addresses where you spend the wallet of\ncoins with that public key (and value) at once; and that remains the\nsame even if you use a single key for both.\n\nI think it's slightly more reasonable to worry about signing with NOINPUT\ncompared to signing with SIGHASH_NONE: you could pretty reasonably setup\nyour (light) bitcoin wallet to not be able to sign (or verify) with\nSIGHASH_NONE ever; but if you want to use lightning v2, it seems pretty\nlikely your wallet will be signing things with SIGHASH_NOINPUT. From\nthere, it's a matter of having a bug or a mistake cause you to\ncross-contaminate keys into your lightning subsystem, and not be\nsufficiently protected by other measures (eg, muSig versus checkmultisig).\n\n(For me the Debian ssh key generation bug from a decade ago is sufficient\nevidence that people you'd think are smart and competent do make really\nstupid mistakes in real life; so defense in depth here makes sense even\nthough you'd have to do really stupid things to get a benefit from it)\n\nThe other benefit of a separate opcode is support can be soft-forked in\nindependently of a new segwit version (either earlier or later).\n\nI don't think the code has to be much more complicated with a separate\nopcode; passing an extra flag to TransactionSignatureChecker::CheckSig()\nis probably close to enough. Some sort of flag remains needed anyway\nsince v0 and pre-segwit signatures won't support NOINPUT.\n\nCheers,\naj"
            },
            {
                "author": "Christian Decker",
                "date": "2018-05-15T14:28:22",
                "message_text_only": "Anthony Towns <aj at erisian.com.au> writes:\n\n> On Thu, May 10, 2018 at 08:34:58AM +0930, Rusty Russell wrote:\n>> > The big concern I have with _NOINPUT is that it has a huge failure\n>> > case: if you use the same key for multiple inputs and sign one of them\n>> > with _NOINPUT, you've spent all of them. The current proposal kind-of\n>> > limits the potential damage by still committing to the prevout amount,\n>> > but it still seems a big risk for all the people that reuse addresses,\n>> > which seems to be just about everyone.\n>> If I can convince you to sign with SIGHASH_NONE, it's already a problem\n>> today.\n>\n> So, I don't find that very compelling: \"there's already a way to lose\n> your money, so it's fine to add other ways to lose your money\". And\n> again, I think NOINPUT is worse here, because a SIGHASH_NONE signature\n> only lets others take the coin you're trying to spend, messing up when\n> using NOINPUT can cause you to lose other coins as well (with caveats).\n\n`SIGHASH_NOINPUT` is a rather powerful tool, but has to be used\nresponsibly, which is why we always mention that it shouldn't be used\nlightly. Then again all sighash flags can be dangerous if not well\nunderstood. Think for example `SIGHASH_SINGLE` with it's pitfall when\nthe input has no matching output, or the already mentioned SIGHASH_NONE.\n\n>From a technical, and risk, point of view I don't think there is much\ndifference between a new opcode or a new sighash flag, with the\nactivation being the one exception. I personally believe that a segwit\nscript bump has cleaner semantics than soft-forking in a new opcode\n(which has 90% overlap with the existing checksig and checkmultisig\nopcodes).\n\n>> [...]\n>> In a world where SIGHASH_NONE didn't exist, this might be an argument :)\n>\n> I could see either dropping support for SIGHASH_NONE for segwit\n> v1 addresses, or possibly limiting SIGHASH_NONE in a similar way to\n> limiting SIGHASH_NOINPUT. Has anyone dug through the blockchain to see\n> if SIGHASH_NONE is actually used/useful?\n\nThat's a good point, I'll try looking for it once I get back to my full\nnode :-) And yes, `SIGHASH_NONE` should also come with all the warning\nsigns about not using it without a very good reason.\n\n>> That was also suggested by Mark Friedenbach, but I think we'll end up\n>> with more \"magic key\" a-la Schnorr/taproot/graftroot and less script in\n>> future.\n>\n> Taproot and graftroot aren't \"less script\" at all -- if anything they're\n> the opposite in that suddenly every address can have a script path.\n> I think NOINPUT has pretty much the same tradeoffs as taproot/graftroot\n> scripts: in the normal case for both you just use a SIGHASH_ALL\n> signature to spend your funds; in the abnormal case for NOINPUT, you use\n> a SIGHASH_NOINPUT (multi)sig for unilateral eltoo closes or watchtower\n> penalties, in the abnormal case for taproot/graftroot you use a script.\n\nThat's true for today's uses of `SIGHASH_NOINPUT` and others, but there\nmight be other uses that we don't know about in which noinput isn't just\nused for the contingency, handwavy I know. That's probably not the case\nfor graftroot/taproot, but I'm happy to be corrected on that one.\n\nStill, these opcodes and hash flags being mainly used for contingencies,\ndoesn't remove the need for these contingency options to be enforced\non-chain.\n\n>> That means we'd actually want a different Segwit version for\n>> \"NOINPUT-can-be-used\", which seems super ugly.\n>\n> That's backwards. If you introduce a new opcode, you can use the existing\n> segwit version, rather than needing segwit v1. You certainly don't need\n> v1 segwit for regular coins and v2 segwit for NOINPUT coins, if that's\n> where you were going?\n>\n> For segwit v0, that would mean your addresses for a key \"X\", might be:\n>\n>    [pubkey]  X    \n>     - not usable with NOINPUT\n>    [script]  2 X Y 2 CHECKMULTISIG\n>     - not usable with NOINPUT\n>    [script]  2 X Y 2 CHECKMULTISIG_1USE_VERIFY\n>     - usable with NOINPUT (or SIGHASH_ALL)\n>\n> CHECKMULTISIG_1USE_VERIFY being soft-forked in by replacing an OP_NOP,\n> of course. Any output spendable via a NOINPUT signature would then have\n> had to have been deliberately created as being spendable by NOINPUT.\n\nThe main reason I went for the sighash flag instead of an opcode is that\nit has clean semantics, allows for it to be bundled with a number of\nother upgrades, and doesn't use up NOP-codes, which I was lectured\nfor my normalized tx BIP (BIP140) is a rare resource that should be used\nsparingly. The `SIGHASH_NOINPUT` proposal is minimal, since it enhances\n4 existing opcodes. If we were to do that with new opcodes we'd either\nwant a multisig and a singlesig variant, potentially with a verify\nvariant each. That's a lot of opcodes.\n\nThe proposal being minimal should also help against everybody trying to\nget their favorite feature added, and hopefully streamline the\ndiscussion.\n\n> For a new segwit version with taproot that likewise includes an opcode,\n> that might be:\n>\n>    [taproot]  X\n>     - not usable with NOINPUT\n>    [taproot]  X or: X CHECKSIG_1USE\n>     - usable with NOINPUT\n>\n> If you had two UTXOs (with the same value), then if you construct\n> a taproot witness script for the latter address it will look like:\n>\n>     X [X CHECKSIG_1USE] [sig_X_NOINPUT]\n>\n> and that signature can't be used for addresses that were just intending\n> to pay to X, because the NOINPUT sig/sighash simply isn't supported\n> without a taproot path that includes the CHECKSIG_1USE opcode.\n>\n> In essence, with the above construction there's two sorts of addresses\n> you generate from a public key X: addresses where you spend each coin\n> individually, and different addresses where you spend the wallet of\n> coins with that public key (and value) at once; and that remains the\n> same even if you use a single key for both.\n>\n> I think it's slightly more reasonable to worry about signing with NOINPUT\n> compared to signing with SIGHASH_NONE: you could pretty reasonably setup\n> your (light) bitcoin wallet to not be able to sign (or verify) with\n> SIGHASH_NONE ever; but if you want to use lightning v2, it seems pretty\n> likely your wallet will be signing things with SIGHASH_NOINPUT. From\n> there, it's a matter of having a bug or a mistake cause you to\n> cross-contaminate keys into your lightning subsystem, and not be\n> sufficiently protected by other measures (eg, muSig versus checkmultisig).\n\nI think the same can be addressed by simply having the wallet use a\ndifferent derivation path for keys that it is willing to sign with\nNOINPUT. I sort of dislike having a direct dependency on taproot, i.e.,\nallowing noinput only in taproot scripts, since that isn't a done deal\neither. Without that direct dependency, having the noinput path and the\nsighash_all path be differentiated in the script leaks the details\non-chain, bloating the UTXO set, and leaking details about our contract.\n\nAlso isn't the same issue true for a separate opcode?\n\n> (For me the Debian ssh key generation bug from a decade ago is sufficient\n> evidence that people you'd think are smart and competent do make really\n> stupid mistakes in real life; so defense in depth here makes sense even\n> though you'd have to do really stupid things to get a benefit from it)\n\nTotally agree, however one could argue that increased code complexity\nis a major contributor to security issues, and I'm still convinced that\nthe hashflag is the simplest and cleanest approach to getting this\nfeature implemented.\n\nThat being said, I think the soft-forked opcode is also a good option,\nif we can get agreement on the details in a reasonable amount of time.\n\n> The other benefit of a separate opcode is support can be soft-forked in\n> independently of a new segwit version (either earlier or later).\n\nThat can both be a positive as well as a negative, since a bundle of\ncomplementing features likely is easier to get reviewed and activated.\n\n> I don't think the code has to be much more complicated with a separate\n> opcode; passing an extra flag to TransactionSignatureChecker::CheckSig()\n> is probably close to enough. Some sort of flag remains needed anyway\n> since v0 and pre-segwit signatures won't support NOINPUT.\n\nThat's moving the fanout for sighash_all vs sighash_none from the opcode\nup to the interpreter, right.\n\nCheers,\nChristian"
            }
        ],
        "thread_summary": {
            "title": "BIP sighash_noinput",
            "categories": [
                "Lightning-dev",
                "bitcoin-dev"
            ],
            "authors": [
                "Anthony Towns",
                "Rusty Russell",
                "Bram Cohen",
                "Olaoluwa Osuntokun",
                "Christian Decker"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 24037
        }
    },
    {
        "title": "[Lightning-dev] Scriptless Scripts with ECDSA",
        "thread_messages": [
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-05-07T23:57:39",
                "message_text_only": "Hi Pedro,\n\nVery cool stuff! When I originally discovered the Lindell's technique, my\nimmediate thought was the we could phase this in as a way to _immediately_\n(no\nadditional Script upgrades required), replace the regular 2-of-2 mulit-sig\nwith\na single p2wkh. The immediate advantages of this would: be lower fees for\nopening/closing channels (as the public key script, and witness are\nsmaller),\nopenings and cooperative close transactions would blend in with the\nanonymity\nset of regular p2wkh transactions, and finally the htlc timeout+success\ntransactions can be made smaller as we can remove the multi-sig. The second\nbenefit is nerfed a bit if the channel are advertised, but non-advertised\nchannels would be able to take advantage of this \"stealth\" feature.\n\nThe upside of the original application I hand in mind is that it wouldn't\nrequire any end-to-end changes, as it would only be a link level change\n(diff\noutput for the funding transaction). If we wanted to allow these styles of\nchannels to be used outside of non-advertised channels, then we would need\nto\nupdate the way channels are verified in the gossip layer.\n\nApplying this to the realm of allowing us to use randomized payment\nidentifiers\nacross the route is obviously much, much doper. So then the question would\nbe\nwhat the process of integrating the scheme into the existing protocol would\nlook like. The primary thing we'd need to account for is the additional\ncryptographic overhead this scheme would add if integrated. Re-reviewing the\npaper, there's an initial setup and verification phase (which was omitted\nfrom\ny'alls note for brevity) where both parties need to complete before the\nactually signing process can take place. Ideally, we can piggy-back this\nsetup\non top of the existing accept_channel/open_channel dance both sides need to\ngo\nthrough in order to advance the channel negotiation process today.\n\nConner actually started to implement this when we first discovered the\nscheme,\nso we have a pretty good feel w.r.t the implementation of the initial set of\nproofs. The three proofs required for the set up phase are:\n\n  1. A proof that that the Paillier public key is well formed. In the paper\n  they only execute this step for the party that wishes to _obtain_ the\n  signature. In our case, since we'll need to sign for HTLCs in both\n  directions, but parties will need to execute this step.\n\n  2. A dlog proof for the signing keys themselves. We already do this more\nor\n  less, as if the remote party isn't able to sign with their target key,\nthen\n  we won't be able to update the channel, or even create a valid commitment\nin\n  the first place.\n\n  3. A proof that value encrypted (the Paillier ciphertext) is actually the\n  dlog of the public key to be used for signing. (as an aside this is the\npart\n  of the protocol that made me do a double take when first reading it:\nusing one\n  cryptosystem to encrypt the private key of another cryptosystem in order\nto\n  construct a 2pc to allow signing in the latter cryptosystem! soo clever!)\n\nFirst, we'll examine the initial proof. This only needs to be done once by\nboth\nparties AFAICT. As a result, we may be able to piggyback this onto the\ninitial\nchannel funding steps. Reviewing the paper cited on the Lindell paper [1],\nit\nappears this would take 1 RTT, so this shouldn't result in any additional\nround\ntrips during the funding process. We should be able to use a Paillier\nmodulos\nof 2048 bits, so nothing too crazy. This would just result in a slightly\nbigger\nopening message.\n\nSkipping the second proofs as it's pretty standard.\n\nThe third proof as described (Section 6 of the Lindell paper) is\ninteractive.\nIt also contains a ZK range proof as a sub-protocol which as described in\nAppendix A is also interactive. However, it was pointed out to us by Omer\nShlomovits on the lnd slack, that we can actually replace their custom range\nproofs with Bulletproofs. This would make this section non-interactive,\nallowing the proof itself to take 1.5 RTT AFAICT. Additionally, this would\nonly\nneed to be done once at the start, as AFIACT, we can re-use the encryption\nof\nthe secp256k1 private key of both parties.\n\nThe current channel opening process requires 2 RTT, so it seems that we'd be\nable to easily piggy back all the opening proofs on top of the existing\nfunding\nprotocol. The main cost would be the increased size of these opening\nmessages,\nand also the additional computational cost of operations within the Paillier\nmodulus and the new range proof.\n\nThe additional components that would need to be modified are the process of\nadding+settling an HTLC, and also the onion payload that drops off the point\nwhose dlog is r_1*alpha. Within the current protocol, adding and settling an\nHTLC are more or less non-interactive, we have a single message for each,\nwhich\nis then staged to be committed in new commitments for both parties. With\nthis\nnew scheme (if I follow it correctly), adding an HTLC now requires N RTT:\n  1. Alice sends A = G*alpha to Bob. Here alpha is the payment secret.\n  2. Bob sends R_3 = (G*alpha)*r_2 (along w/ a proof of knowledge of r_2 and\n  relation to A)\n  3. Alice sends R_3' = (G*alpha)*r_3 (along with a similar proof as above)\n4.\n  Bob then computes c3 (the encrypted partial sig which when completed will\n  reveal a) to Alice.\n  5. Alice decrypts c3 to get the plaintext partial sig (s'), then finalizes\n  the set up by sending s'' to Bob.\n\nThis process takes 2.5 RTT, and would require re-working the state machine\nslightly to only actually commit an HTLC after step 5 has been completed.\nWhen\nBob obtains a from the next party in the path, we Alice can then then over\nthe\nsignature, from which Alice can extract alpha. So adding HTLCs is now a bit\nmore interactive, but settling them is the same a before.\n\nFinally, the onion payload would need to be re-interpreted in order to\nencode\nG*alpha which takes 33 bytes. We can shave this down to 32 by selecting the\nx\ncoordinate (at the sender) to always be either even or odd. Currently, we\nhave\n12 unused bytes in the onion payload. The HMAC is currently 32 bytes. One\npath\nwould be to allocate a portion of HMAC space to encoding this point. A\n16-byte\nHMAC would probably have been enough in the beginning, so we can drop down\nto\nthat. However, that still leaves 4 bytes somewhere that has to give...one\ncould\neither obtain these extra bytes from the CLTV and Amount fields, or just\nhave\neach hop consume an extra payload. The latter path would mean that the new\nupper hop limit is actually 10.\n\nHowever, given that we would need need a new global feature bit in order to\nroll this out, it may make sense to re-work the onion format all together\nwhich\nwould mean that we wouldn't need to hack the old format a bit to accommodate\nthis additional data. One aspect of introducing a new end-to-end contract\ntype\nwhich I hadn't considered before is that each new type effectively\npartitions\nthe network. This is due to the fact that these HTLCs will now only be able\nto\nbe carried along paths that understand this new feature. As a result,\nplausible\npath diversity takes as we can no longer utilize all channels on the network\nfor routing. This would suggest that introducing new end to end contract\ntypes\n(if one wishes to use them widely across arbitrary channels and not for\nspecific contract protocols) may be a strong point of synchronization w.r.t\nupdates across the network. As a result, we may need to be a bit more\ndiscerning w.r.t new candidates for e2e contracts given the coordination\ncosts.\n\nSo the takeaways are:\n  * we can probably piggy back the extra proofs onto the channel opening\n    process\n      * one of the subproofs can use bulletproofs to make the proof shorter\nand\n        also non-interactive\n  * adding an HTLC would take 2.5 RTT's, but settling is just as quick as\n    before\n  * the onion payload would either need to be hacked, or extended to support\n    packaging the point.\n  * the utility of the scheme won't shine until all/most of the network\nuses it\n  * we could start w/ just the introduction of the OG 2PC scheme as a\nmulti-sig\n    replacement\n\n\n[1]: https://eprint.iacr.org/2011/494.pdf (section 3.3)\n\n-- Laolu\n\n\nOn Fri, Apr 27, 2018 at 11:42 AM Pedro Moreno Sanchez <pmorenos at purdue.edu>\nwrote:\n\n> Hello guys,\n>\n> as some of you already know, I am working on some cryptographic\n> constructions that might be of interest and useful for the Lightning\n> Network.\n>\n> Recently, I have come up with a scriptless version of the adaptor\n> signatures and the contract required in the Lighting Network using only\n> 2-party ECDSA signatures. The main advantage is that, instead of waiting\n> for Schnorr signatures to be deployed in Bitcoin so that Poelstra's\n> scriptless scripts can be used, I believe that this ECDSA-version of the\n> scriptless scripts can be directly applied today.\n>\n> Details are in the attached PDF. I am looking forward to hearing your\n> comments and suggestions.\n>\n> Cheers,\n> Pedro.\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180507/32b51086/attachment-0001.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-05-08T00:14:21",
                "message_text_only": "Actually, just thought about this a bit more and I think it's possible to\ndeploy this in unison with (or after) any sort of SS based on schnorr\nbecomes\npossible in Bitcoin. My observation is that since both techniques are based\non\nthe same underlying technique (revealing a secret value in a signature) and\nthey center around leveraging the onion payload to drop off a payment point\n(G*a, or G*a_1*a_2*a_3, etc), then the disclosure within the _links_ can be\nheterogeneous, as the same secret is still revealed in an end-to-end matter.\n\nAs an illustration, consider: A <-> B <-> C. The A <-> B link could use the\n2pc\npailier technique, while the B <-> C link could use the OG SS technique\nbased\non schnorr. If i'm correct, then this would mean that we can deploy both\ntechniques, without worrying about fragmenting the network due to the\nexistence\nof two similar but incompatible e2e payment routing schemes!\n\n-- Laolu\n\n\nOn Mon, May 7, 2018 at 4:57 PM Olaoluwa Osuntokun <laolu32 at gmail.com> wrote:\n\n> Hi Pedro,\n>\n> Very cool stuff! When I originally discovered the Lindell's technique, my\n> immediate thought was the we could phase this in as a way to _immediately_\n> (no\n> additional Script upgrades required), replace the regular 2-of-2 mulit-sig\n> with\n> a single p2wkh. The immediate advantages of this would: be lower fees for\n> opening/closing channels (as the public key script, and witness are\n> smaller),\n> openings and cooperative close transactions would blend in with the\n> anonymity\n> set of regular p2wkh transactions, and finally the htlc timeout+success\n> transactions can be made smaller as we can remove the multi-sig. The second\n> benefit is nerfed a bit if the channel are advertised, but non-advertised\n> channels would be able to take advantage of this \"stealth\" feature.\n>\n> The upside of the original application I hand in mind is that it wouldn't\n> require any end-to-end changes, as it would only be a link level change\n> (diff\n> output for the funding transaction). If we wanted to allow these styles of\n> channels to be used outside of non-advertised channels, then we would need\n> to\n> update the way channels are verified in the gossip layer.\n>\n> Applying this to the realm of allowing us to use randomized payment\n> identifiers\n> across the route is obviously much, much doper. So then the question would\n> be\n> what the process of integrating the scheme into the existing protocol would\n> look like. The primary thing we'd need to account for is the additional\n> cryptographic overhead this scheme would add if integrated. Re-reviewing\n> the\n> paper, there's an initial setup and verification phase (which was omitted\n> from\n> y'alls note for brevity) where both parties need to complete before the\n> actually signing process can take place. Ideally, we can piggy-back this\n> setup\n> on top of the existing accept_channel/open_channel dance both sides need\n> to go\n> through in order to advance the channel negotiation process today.\n>\n> Conner actually started to implement this when we first discovered the\n> scheme,\n> so we have a pretty good feel w.r.t the implementation of the initial set\n> of\n> proofs. The three proofs required for the set up phase are:\n>\n>   1. A proof that that the Paillier public key is well formed. In the paper\n>   they only execute this step for the party that wishes to _obtain_ the\n>   signature. In our case, since we'll need to sign for HTLCs in both\n>   directions, but parties will need to execute this step.\n>\n>   2. A dlog proof for the signing keys themselves. We already do this more\n> or\n>   less, as if the remote party isn't able to sign with their target key,\n> then\n>   we won't be able to update the channel, or even create a valid\n> commitment in\n>   the first place.\n>\n>   3. A proof that value encrypted (the Paillier ciphertext) is actually the\n>   dlog of the public key to be used for signing. (as an aside this is the\n> part\n>   of the protocol that made me do a double take when first reading it:\n> using one\n>   cryptosystem to encrypt the private key of another cryptosystem in order\n> to\n>   construct a 2pc to allow signing in the latter cryptosystem! soo clever!)\n>\n> First, we'll examine the initial proof. This only needs to be done once by\n> both\n> parties AFAICT. As a result, we may be able to piggyback this onto the\n> initial\n> channel funding steps. Reviewing the paper cited on the Lindell paper [1],\n> it\n> appears this would take 1 RTT, so this shouldn't result in any additional\n> round\n> trips during the funding process. We should be able to use a Paillier\n> modulos\n> of 2048 bits, so nothing too crazy. This would just result in a slightly\n> bigger\n> opening message.\n>\n> Skipping the second proofs as it's pretty standard.\n>\n> The third proof as described (Section 6 of the Lindell paper) is\n> interactive.\n> It also contains a ZK range proof as a sub-protocol which as described in\n> Appendix A is also interactive. However, it was pointed out to us by Omer\n> Shlomovits on the lnd slack, that we can actually replace their custom\n> range\n> proofs with Bulletproofs. This would make this section non-interactive,\n> allowing the proof itself to take 1.5 RTT AFAICT. Additionally, this would\n> only\n> need to be done once at the start, as AFIACT, we can re-use the encryption\n> of\n> the secp256k1 private key of both parties.\n>\n> The current channel opening process requires 2 RTT, so it seems that we'd\n> be\n> able to easily piggy back all the opening proofs on top of the existing\n> funding\n> protocol. The main cost would be the increased size of these opening\n> messages,\n> and also the additional computational cost of operations within the\n> Paillier\n> modulus and the new range proof.\n>\n> The additional components that would need to be modified are the process of\n> adding+settling an HTLC, and also the onion payload that drops off the\n> point\n> whose dlog is r_1*alpha. Within the current protocol, adding and settling\n> an\n> HTLC are more or less non-interactive, we have a single message for each,\n> which\n> is then staged to be committed in new commitments for both parties. With\n> this\n> new scheme (if I follow it correctly), adding an HTLC now requires N RTT:\n>   1. Alice sends A = G*alpha to Bob. Here alpha is the payment secret.\n>   2. Bob sends R_3 = (G*alpha)*r_2 (along w/ a proof of knowledge of r_2\n> and\n>   relation to A)\n>   3. Alice sends R_3' = (G*alpha)*r_3 (along with a similar proof as\n> above) 4.\n>   Bob then computes c3 (the encrypted partial sig which when completed will\n>   reveal a) to Alice.\n>   5. Alice decrypts c3 to get the plaintext partial sig (s'), then\n> finalizes\n>   the set up by sending s'' to Bob.\n>\n> This process takes 2.5 RTT, and would require re-working the state machine\n> slightly to only actually commit an HTLC after step 5 has been completed.\n> When\n> Bob obtains a from the next party in the path, we Alice can then then over\n> the\n> signature, from which Alice can extract alpha. So adding HTLCs is now a bit\n> more interactive, but settling them is the same a before.\n>\n> Finally, the onion payload would need to be re-interpreted in order to\n> encode\n> G*alpha which takes 33 bytes. We can shave this down to 32 by selecting\n> the x\n> coordinate (at the sender) to always be either even or odd. Currently, we\n> have\n> 12 unused bytes in the onion payload. The HMAC is currently 32 bytes. One\n> path\n> would be to allocate a portion of HMAC space to encoding this point. A\n> 16-byte\n> HMAC would probably have been enough in the beginning, so we can drop down\n> to\n> that. However, that still leaves 4 bytes somewhere that has to give...one\n> could\n> either obtain these extra bytes from the CLTV and Amount fields, or just\n> have\n> each hop consume an extra payload. The latter path would mean that the new\n> upper hop limit is actually 10.\n>\n> However, given that we would need need a new global feature bit in order to\n> roll this out, it may make sense to re-work the onion format all together\n> which\n> would mean that we wouldn't need to hack the old format a bit to\n> accommodate\n> this additional data. One aspect of introducing a new end-to-end contract\n> type\n> which I hadn't considered before is that each new type effectively\n> partitions\n> the network. This is due to the fact that these HTLCs will now only be\n> able to\n> be carried along paths that understand this new feature. As a result,\n> plausible\n> path diversity takes as we can no longer utilize all channels on the\n> network\n> for routing. This would suggest that introducing new end to end contract\n> types\n> (if one wishes to use them widely across arbitrary channels and not for\n> specific contract protocols) may be a strong point of synchronization w.r.t\n> updates across the network. As a result, we may need to be a bit more\n> discerning w.r.t new candidates for e2e contracts given the coordination\n> costs.\n>\n> So the takeaways are:\n>   * we can probably piggy back the extra proofs onto the channel opening\n>     process\n>       * one of the subproofs can use bulletproofs to make the proof\n> shorter and\n>         also non-interactive\n>   * adding an HTLC would take 2.5 RTT's, but settling is just as quick as\n>     before\n>   * the onion payload would either need to be hacked, or extended to\n> support\n>     packaging the point.\n>   * the utility of the scheme won't shine until all/most of the network\n> uses it\n>   * we could start w/ just the introduction of the OG 2PC scheme as a\n> multi-sig\n>     replacement\n>\n>\n> [1]: https://eprint.iacr.org/2011/494.pdf (section 3.3)\n>\n> -- Laolu\n>\n>\n> On Fri, Apr 27, 2018 at 11:42 AM Pedro Moreno Sanchez <pmorenos at purdue.edu>\n> wrote:\n>\n>> Hello guys,\n>>\n>> as some of you already know, I am working on some cryptographic\n>> constructions that might be of interest and useful for the Lightning\n>> Network.\n>>\n>> Recently, I have come up with a scriptless version of the adaptor\n>> signatures and the contract required in the Lighting Network using only\n>> 2-party ECDSA signatures. The main advantage is that, instead of waiting\n>> for Schnorr signatures to be deployed in Bitcoin so that Poelstra's\n>> scriptless scripts can be used, I believe that this ECDSA-version of the\n>> scriptless scripts can be directly applied today.\n>>\n>> Details are in the attached PDF. I am looking forward to hearing your\n>> comments and suggestions.\n>>\n>> Cheers,\n>> Pedro.\n>>\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180508/04e449be/attachment-0001.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-05-08T05:01:49",
                "message_text_only": "FWIW, Conner pointed out that the initial ZK Proof for the correctness of\nthe\nPaillier params (even w/ usage of bulletproofs) has multiple rounds of\ninteraction,\niirc up to 5+ (with additional pipelining) rounds of interaction.\n\n-- Laolu\n\nOn Mon, May 7, 2018 at 5:14 PM Olaoluwa Osuntokun <laolu32 at gmail.com> wrote:\n\n> Actually, just thought about this a bit more and I think it's possible to\n> deploy this in unison with (or after) any sort of SS based on schnorr\n> becomes\n> possible in Bitcoin. My observation is that since both techniques are\n> based on\n> the same underlying technique (revealing a secret value in a signature) and\n> they center around leveraging the onion payload to drop off a payment point\n> (G*a, or G*a_1*a_2*a_3, etc), then the disclosure within the _links_ can be\n> heterogeneous, as the same secret is still revealed in an end-to-end\n> matter.\n>\n> As an illustration, consider: A <-> B <-> C. The A <-> B link could use\n> the 2pc\n> pailier technique, while the B <-> C link could use the OG SS technique\n> based\n> on schnorr. If i'm correct, then this would mean that we can deploy both\n> techniques, without worrying about fragmenting the network due to the\n> existence\n> of two similar but incompatible e2e payment routing schemes!\n>\n> -- Laolu\n>\n>\n> On Mon, May 7, 2018 at 4:57 PM Olaoluwa Osuntokun <laolu32 at gmail.com>\n> wrote:\n>\n>> Hi Pedro,\n>>\n>> Very cool stuff! When I originally discovered the Lindell's technique, my\n>> immediate thought was the we could phase this in as a way to\n>> _immediately_ (no\n>> additional Script upgrades required), replace the regular 2-of-2\n>> mulit-sig with\n>> a single p2wkh. The immediate advantages of this would: be lower fees for\n>> opening/closing channels (as the public key script, and witness are\n>> smaller),\n>> openings and cooperative close transactions would blend in with the\n>> anonymity\n>> set of regular p2wkh transactions, and finally the htlc timeout+success\n>> transactions can be made smaller as we can remove the multi-sig. The\n>> second\n>> benefit is nerfed a bit if the channel are advertised, but non-advertised\n>> channels would be able to take advantage of this \"stealth\" feature.\n>>\n>> The upside of the original application I hand in mind is that it wouldn't\n>> require any end-to-end changes, as it would only be a link level change\n>> (diff\n>> output for the funding transaction). If we wanted to allow these styles of\n>> channels to be used outside of non-advertised channels, then we would\n>> need to\n>> update the way channels are verified in the gossip layer.\n>>\n>> Applying this to the realm of allowing us to use randomized payment\n>> identifiers\n>> across the route is obviously much, much doper. So then the question\n>> would be\n>> what the process of integrating the scheme into the existing protocol\n>> would\n>> look like. The primary thing we'd need to account for is the additional\n>> cryptographic overhead this scheme would add if integrated. Re-reviewing\n>> the\n>> paper, there's an initial setup and verification phase (which was omitted\n>> from\n>> y'alls note for brevity) where both parties need to complete before the\n>> actually signing process can take place. Ideally, we can piggy-back this\n>> setup\n>> on top of the existing accept_channel/open_channel dance both sides need\n>> to go\n>> through in order to advance the channel negotiation process today.\n>>\n>> Conner actually started to implement this when we first discovered the\n>> scheme,\n>> so we have a pretty good feel w.r.t the implementation of the initial set\n>> of\n>> proofs. The three proofs required for the set up phase are:\n>>\n>>   1. A proof that that the Paillier public key is well formed. In the\n>> paper\n>>   they only execute this step for the party that wishes to _obtain_ the\n>>   signature. In our case, since we'll need to sign for HTLCs in both\n>>   directions, but parties will need to execute this step.\n>>\n>>   2. A dlog proof for the signing keys themselves. We already do this\n>> more or\n>>   less, as if the remote party isn't able to sign with their target key,\n>> then\n>>   we won't be able to update the channel, or even create a valid\n>> commitment in\n>>   the first place.\n>>\n>>   3. A proof that value encrypted (the Paillier ciphertext) is actually\n>> the\n>>   dlog of the public key to be used for signing. (as an aside this is the\n>> part\n>>   of the protocol that made me do a double take when first reading it:\n>> using one\n>>   cryptosystem to encrypt the private key of another cryptosystem in\n>> order to\n>>   construct a 2pc to allow signing in the latter cryptosystem! soo\n>> clever!)\n>>\n>> First, we'll examine the initial proof. This only needs to be done once\n>> by both\n>> parties AFAICT. As a result, we may be able to piggyback this onto the\n>> initial\n>> channel funding steps. Reviewing the paper cited on the Lindell paper\n>> [1], it\n>> appears this would take 1 RTT, so this shouldn't result in any additional\n>> round\n>> trips during the funding process. We should be able to use a Paillier\n>> modulos\n>> of 2048 bits, so nothing too crazy. This would just result in a slightly\n>> bigger\n>> opening message.\n>>\n>> Skipping the second proofs as it's pretty standard.\n>>\n>> The third proof as described (Section 6 of the Lindell paper) is\n>> interactive.\n>> It also contains a ZK range proof as a sub-protocol which as described in\n>> Appendix A is also interactive. However, it was pointed out to us by Omer\n>> Shlomovits on the lnd slack, that we can actually replace their custom\n>> range\n>> proofs with Bulletproofs. This would make this section non-interactive,\n>> allowing the proof itself to take 1.5 RTT AFAICT. Additionally, this\n>> would only\n>> need to be done once at the start, as AFIACT, we can re-use the\n>> encryption of\n>> the secp256k1 private key of both parties.\n>>\n>> The current channel opening process requires 2 RTT, so it seems that we'd\n>> be\n>> able to easily piggy back all the opening proofs on top of the existing\n>> funding\n>> protocol. The main cost would be the increased size of these opening\n>> messages,\n>> and also the additional computational cost of operations within the\n>> Paillier\n>> modulus and the new range proof.\n>>\n>> The additional components that would need to be modified are the process\n>> of\n>> adding+settling an HTLC, and also the onion payload that drops off the\n>> point\n>> whose dlog is r_1*alpha. Within the current protocol, adding and settling\n>> an\n>> HTLC are more or less non-interactive, we have a single message for each,\n>> which\n>> is then staged to be committed in new commitments for both parties. With\n>> this\n>> new scheme (if I follow it correctly), adding an HTLC now requires N RTT:\n>>   1. Alice sends A = G*alpha to Bob. Here alpha is the payment secret.\n>>   2. Bob sends R_3 = (G*alpha)*r_2 (along w/ a proof of knowledge of r_2\n>> and\n>>   relation to A)\n>>   3. Alice sends R_3' = (G*alpha)*r_3 (along with a similar proof as\n>> above) 4.\n>>   Bob then computes c3 (the encrypted partial sig which when completed\n>> will\n>>   reveal a) to Alice.\n>>   5. Alice decrypts c3 to get the plaintext partial sig (s'), then\n>> finalizes\n>>   the set up by sending s'' to Bob.\n>>\n>> This process takes 2.5 RTT, and would require re-working the state machine\n>> slightly to only actually commit an HTLC after step 5 has been\n>> completed.  When\n>> Bob obtains a from the next party in the path, we Alice can then then\n>> over the\n>> signature, from which Alice can extract alpha. So adding HTLCs is now a\n>> bit\n>> more interactive, but settling them is the same a before.\n>>\n>> Finally, the onion payload would need to be re-interpreted in order to\n>> encode\n>> G*alpha which takes 33 bytes. We can shave this down to 32 by selecting\n>> the x\n>> coordinate (at the sender) to always be either even or odd. Currently, we\n>> have\n>> 12 unused bytes in the onion payload. The HMAC is currently 32 bytes. One\n>> path\n>> would be to allocate a portion of HMAC space to encoding this point. A\n>> 16-byte\n>> HMAC would probably have been enough in the beginning, so we can drop\n>> down to\n>> that. However, that still leaves 4 bytes somewhere that has to give...one\n>> could\n>> either obtain these extra bytes from the CLTV and Amount fields, or just\n>> have\n>> each hop consume an extra payload. The latter path would mean that the new\n>> upper hop limit is actually 10.\n>>\n>> However, given that we would need need a new global feature bit in order\n>> to\n>> roll this out, it may make sense to re-work the onion format all together\n>> which\n>> would mean that we wouldn't need to hack the old format a bit to\n>> accommodate\n>> this additional data. One aspect of introducing a new end-to-end contract\n>> type\n>> which I hadn't considered before is that each new type effectively\n>> partitions\n>> the network. This is due to the fact that these HTLCs will now only be\n>> able to\n>> be carried along paths that understand this new feature. As a result,\n>> plausible\n>> path diversity takes as we can no longer utilize all channels on the\n>> network\n>> for routing. This would suggest that introducing new end to end contract\n>> types\n>> (if one wishes to use them widely across arbitrary channels and not for\n>> specific contract protocols) may be a strong point of synchronization\n>> w.r.t\n>> updates across the network. As a result, we may need to be a bit more\n>> discerning w.r.t new candidates for e2e contracts given the coordination\n>> costs.\n>>\n>> So the takeaways are:\n>>   * we can probably piggy back the extra proofs onto the channel opening\n>>     process\n>>       * one of the subproofs can use bulletproofs to make the proof\n>> shorter and\n>>         also non-interactive\n>>   * adding an HTLC would take 2.5 RTT's, but settling is just as quick as\n>>     before\n>>   * the onion payload would either need to be hacked, or extended to\n>> support\n>>     packaging the point.\n>>   * the utility of the scheme won't shine until all/most of the network\n>> uses it\n>>   * we could start w/ just the introduction of the OG 2PC scheme as a\n>> multi-sig\n>>     replacement\n>>\n>>\n>> [1]: https://eprint.iacr.org/2011/494.pdf (section 3.3)\n>>\n>> -- Laolu\n>>\n>>\n>> On Fri, Apr 27, 2018 at 11:42 AM Pedro Moreno Sanchez <\n>> pmorenos at purdue.edu> wrote:\n>>\n>>> Hello guys,\n>>>\n>>> as some of you already know, I am working on some cryptographic\n>>> constructions that might be of interest and useful for the Lightning\n>>> Network.\n>>>\n>>> Recently, I have come up with a scriptless version of the adaptor\n>>> signatures and the contract required in the Lighting Network using only\n>>> 2-party ECDSA signatures. The main advantage is that, instead of waiting\n>>> for Schnorr signatures to be deployed in Bitcoin so that Poelstra's\n>>> scriptless scripts can be used, I believe that this ECDSA-version of the\n>>> scriptless scripts can be directly applied today.\n>>>\n>>> Details are in the attached PDF. I am looking forward to hearing your\n>>> comments and suggestions.\n>>>\n>>> Cheers,\n>>> Pedro.\n>>>\n>>> _______________________________________________\n>>> Lightning-dev mailing list\n>>> Lightning-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180508/23f2dfc3/attachment.html>"
            },
            {
                "author": "Benjamin Mord",
                "date": "2018-05-08T12:44:41",
                "message_text_only": "If I'm not mistaken, the scriptless scripts concept (as currently\nformulated) falls to Schor's algorithm, and at present there is no\nalternative implementation of the concept to fall back on. Correct? Lest we\nbuild a house of cards, I'd strongly urge everyone to not depend on\nfunctional concepts whose underlying cryptographic primitives cannot be\nswapped in an emergency.\n\nSure, we use ecdsa for example (which is also vulnerable to Schor's\nalgorithm), but in contrast to scriptless scripts we have a variety of\nbackup primitives at our disposal that fulfill the same functional\nobjective.\n\nIf scriptless scripts are found possible under lattice-based cryptography\nfor example, that would be something I suppose. The functional concept of\nscriptless scripts is indeed very awesome - we just need to add some\ncryptographic conservatism before we build on it.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180508/e565d776/attachment.html>"
            },
            {
                "author": "Greg Sanders",
                "date": "2018-05-08T12:58:33",
                "message_text_only": ">From what I understand talking to folks, the linear properties of these\nsignature tricks are maintained under a number of post-quantum schemes.\n\nOn Tue, May 8, 2018 at 8:44 AM, Benjamin Mord <ben at mord.family> wrote:\n\n>\n> If I'm not mistaken, the scriptless scripts concept (as currently\n> formulated) falls to Schor's algorithm, and at present there is no\n> alternative implementation of the concept to fall back on. Correct? Lest we\n> build a house of cards, I'd strongly urge everyone to not depend on\n> functional concepts whose underlying cryptographic primitives cannot be\n> swapped in an emergency.\n>\n> Sure, we use ecdsa for example (which is also vulnerable to Schor's\n> algorithm), but in contrast to scriptless scripts we have a variety of\n> backup primitives at our disposal that fulfill the same functional\n> objective.\n>\n> If scriptless scripts are found possible under lattice-based cryptography\n> for example, that would be something I suppose. The functional concept of\n> scriptless scripts is indeed very awesome - we just need to add some\n> cryptographic conservatism before we build on it.\n>\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180508/5b5bfa6b/attachment.html>"
            },
            {
                "author": "Benjamin Mord",
                "date": "2018-05-08T13:09:05",
                "message_text_only": "That would be awesome. Do you have a reference?\n\nAs pertains to the whole of asymmetric cryptography, I believe there are\nnot a variety of post quantum schemes, there is only one*: lattice-based\ncryptography. (Which scares me, because it is not all that different from\nthe others.)\n\n(* Actually, in contexts where time can be used for asymmetry, as in TESLA,\nwe can then use hash functions to create something like asymmetric\nsignatures as well. But the functional context has to be compatible with\ndelayed verification.)\n\n(But I do not mean to focus exclusively on Schor's algorithm, the history\nof even pre-quantum cryptanalysis shows that primitives tend to have finite\nlifespan. Redundancy of any sort of good, even when not focused\nspecifically on quantum risks.)\n\nOn Tue, May 8, 2018, 8:58 AM Greg Sanders <gsanders87 at gmail.com> wrote:\n\n> From what I understand talking to folks, the linear properties of these\n> signature tricks are maintained under a number of post-quantum schemes.\n>\n> On Tue, May 8, 2018 at 8:44 AM, Benjamin Mord <ben at mord.family> wrote:\n>\n>>\n>> If I'm not mistaken, the scriptless scripts concept (as currently\n>> formulated) falls to Schor's algorithm, and at present there is no\n>> alternative implementation of the concept to fall back on. Correct? Lest we\n>> build a house of cards, I'd strongly urge everyone to not depend on\n>> functional concepts whose underlying cryptographic primitives cannot be\n>> swapped in an emergency.\n>>\n>> Sure, we use ecdsa for example (which is also vulnerable to Schor's\n>> algorithm), but in contrast to scriptless scripts we have a variety of\n>> backup primitives at our disposal that fulfill the same functional\n>> objective.\n>>\n>> If scriptless scripts are found possible under lattice-based cryptography\n>> for example, that would be something I suppose. The functional concept of\n>> scriptless scripts is indeed very awesome - we just need to add some\n>> cryptographic conservatism before we build on it.\n>>\n>>\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180508/45b1c742/attachment.html>"
            },
            {
                "author": "Benjamin Mord",
                "date": "2018-05-08T13:31:51",
                "message_text_only": "Sorry, I do not wish to spam the list, but I need to correct a rather\nserious error in my last email. We must never call something\n\"post-quantum\", absent mathematical proof. (And good luck with that.) I\napologise for my mistake in doing so myself.\n\nI should not even refer to lattice based cryptography as a post-quantum\nalgorithm, I should at best call it a Shor's algorithm-resistant scheme. At\nleast, it is not (yet) known how Shor's algorithm could be used to break\nit. Not in public circles, anyhow.\n\nA cursory glance at the history of cryptanalysis shows that primitives\ngenerally have finite life, which makes it odd that systems seldom use\nredundant primitives, and seldom provide for their rapid and safe swap. A\nsystem whose entire security rests on nothing but cryptography, ought to\ntake particular care! The spectre of quantum computers may render the\nfinite life of certain primitives more salient than others, but we must not\nsuppose that, absent Shor's algorithm, there would be no need to plan for\ncryptographic failures. The question is not if, but when, Bitcoin and\nlightning will contend with broken primitives, whether due to classical or\nquantum cryptanalysis. Likely, both will come into play at various times,\nand one must plan accordingly.\n\nOn Tue, May 8, 2018, 9:09 AM Benjamin Mord <ben at mord.io> wrote:\n\n>\n> That would be awesome. Do you have a reference?\n>\n> As pertains to the whole of asymmetric cryptography, I believe there are\n> not a variety of post quantum schemes, there is only one*: lattice-based\n> cryptography. (Which scares me, because it is not all that different from\n> the others.)\n>\n> (* Actually, in contexts where time can be used for asymmetry, as in\n> TESLA, we can then use hash functions to create something like asymmetric\n> signatures as well. But the functional context has to be compatible with\n> delayed verification.)\n>\n> (But I do not mean to focus exclusively on Schor's algorithm, the history\n> of even pre-quantum cryptanalysis shows that primitives tend to have finite\n> lifespan. Redundancy of any sort of good, even when not focused\n> specifically on quantum risks.)\n>\n> On Tue, May 8, 2018, 8:58 AM Greg Sanders <gsanders87 at gmail.com> wrote:\n>\n>> From what I understand talking to folks, the linear properties of these\n>> signature tricks are maintained under a number of post-quantum schemes.\n>>\n>> On Tue, May 8, 2018 at 8:44 AM, Benjamin Mord <ben at mord.family> wrote:\n>>\n>>>\n>>> If I'm not mistaken, the scriptless scripts concept (as currently\n>>> formulated) falls to Schor's algorithm, and at present there is no\n>>> alternative implementation of the concept to fall back on. Correct? Lest we\n>>> build a house of cards, I'd strongly urge everyone to not depend on\n>>> functional concepts whose underlying cryptographic primitives cannot be\n>>> swapped in an emergency.\n>>>\n>>> Sure, we use ecdsa for example (which is also vulnerable to Schor's\n>>> algorithm), but in contrast to scriptless scripts we have a variety of\n>>> backup primitives at our disposal that fulfill the same functional\n>>> objective.\n>>>\n>>> If scriptless scripts are found possible under lattice-based\n>>> cryptography for example, that would be something I suppose. The functional\n>>> concept of scriptless scripts is indeed very awesome - we just need to add\n>>> some cryptographic conservatism before we build on it.\n>>>\n>>>\n>>> _______________________________________________\n>>> Lightning-dev mailing list\n>>> Lightning-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>>\n>>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180508/664c63d0/attachment-0001.html>"
            },
            {
                "author": "Jim Posen",
                "date": "2018-05-08T15:54:58",
                "message_text_only": "Benjamin,\n\nI don't agree that quantum resistance should be a blocker to deployment of\nscriptless scripts on lightning because 1) it is a layer-2 solution and 2)\nit already critically depends on the security of DL.\n\nThere are arguments against making certain protocol changes to the base\nBitcoin blockchain for the reason that they may not be quantum resistant.\nThe most notable is Confidential Transactions. There reason is that the\nworst case attack is much worse: an attacker could print money freely and\nwithout detection. In Bitcoin today, a DL break would compromise funds in\nany addresses for which the pubkey has been revealed and it's not even\nclear what to do about the remaining funds on chain. Compromising the\nfundamental security of the blockchain is a valid cause for concern.\n\nIn the case of Lightning, the attack scenario on scriptless scripts is that\na peer is going to use a quantum computer to steal all live payments routed\nthrough them from their senders before they get to the recipient. This\nwould be bad, but not catastrophic, and once it is recognized that the\nattack is possible, insecure channels could be closed.\n\nBut furthermore, an attacker with a quantum computer could just steal the\nmultisig funding output directly instead of attacking scriptless scripts.\nSo additional protocol changes relying on the DL assumption don't bother me\nin the least.\n\n\n\nOn Tue, May 8, 2018, 6:32 AM Benjamin Mord <ben at mord.io> wrote:\n\n>\n> Sorry, I do not wish to spam the list, but I need to correct a rather\n> serious error in my last email. We must never call something\n> \"post-quantum\", absent mathematical proof. (And good luck with that.) I\n> apologise for my mistake in doing so myself.\n>\n> I should not even refer to lattice based cryptography as a post-quantum\n> algorithm, I should at best call it a Shor's algorithm-resistant scheme. At\n> least, it is not (yet) known how Shor's algorithm could be used to break\n> it. Not in public circles, anyhow.\n>\n> A cursory glance at the history of cryptanalysis shows that primitives\n> generally have finite life, which makes it odd that systems seldom use\n> redundant primitives, and seldom provide for their rapid and safe swap. A\n> system whose entire security rests on nothing but cryptography, ought to\n> take particular care! The spectre of quantum computers may render the\n> finite life of certain primitives more salient than others, but we must not\n> suppose that, absent Shor's algorithm, there would be no need to plan for\n> cryptographic failures. The question is not if, but when, Bitcoin and\n> lightning will contend with broken primitives, whether due to classical or\n> quantum cryptanalysis. Likely, both will come into play at various times,\n> and one must plan accordingly.\n>\n> On Tue, May 8, 2018, 9:09 AM Benjamin Mord <ben at mord.io> wrote:\n>\n>>\n>> That would be awesome. Do you have a reference?\n>>\n>> As pertains to the whole of asymmetric cryptography, I believe there are\n>> not a variety of post quantum schemes, there is only one*: lattice-based\n>> cryptography. (Which scares me, because it is not all that different from\n>> the others.)\n>>\n>> (* Actually, in contexts where time can be used for asymmetry, as in\n>> TESLA, we can then use hash functions to create something like asymmetric\n>> signatures as well. But the functional context has to be compatible with\n>> delayed verification.)\n>>\n>> (But I do not mean to focus exclusively on Schor's algorithm, the history\n>> of even pre-quantum cryptanalysis shows that primitives tend to have finite\n>> lifespan. Redundancy of any sort of good, even when not focused\n>> specifically on quantum risks.)\n>>\n>> On Tue, May 8, 2018, 8:58 AM Greg Sanders <gsanders87 at gmail.com> wrote:\n>>\n>>> From what I understand talking to folks, the linear properties of these\n>>> signature tricks are maintained under a number of post-quantum schemes.\n>>>\n>>> On Tue, May 8, 2018 at 8:44 AM, Benjamin Mord <ben at mord.family> wrote:\n>>>\n>>>>\n>>>> If I'm not mistaken, the scriptless scripts concept (as currently\n>>>> formulated) falls to Schor's algorithm, and at present there is no\n>>>> alternative implementation of the concept to fall back on. Correct? Lest we\n>>>> build a house of cards, I'd strongly urge everyone to not depend on\n>>>> functional concepts whose underlying cryptographic primitives cannot be\n>>>> swapped in an emergency.\n>>>>\n>>>> Sure, we use ecdsa for example (which is also vulnerable to Schor's\n>>>> algorithm), but in contrast to scriptless scripts we have a variety of\n>>>> backup primitives at our disposal that fulfill the same functional\n>>>> objective.\n>>>>\n>>>> If scriptless scripts are found possible under lattice-based\n>>>> cryptography for example, that would be something I suppose. The functional\n>>>> concept of scriptless scripts is indeed very awesome - we just need to add\n>>>> some cryptographic conservatism before we build on it.\n>>>>\n>>>>\n>>>> _______________________________________________\n>>>> Lightning-dev mailing list\n>>>> Lightning-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>>>\n>>>>\n>>> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180508/f3ebdf1a/attachment.html>"
            },
            {
                "author": "Benjamin Mord",
                "date": "2018-05-08T22:02:46",
                "message_text_only": "Good evening Jim,\n\n\n> I don't agree that quantum resistance should be a blocker to deployment of\n> scriptless scripts on lightning\n>\n\nI don't mean to speak narrowly about quantum cryptanalysis, but more\ngenerally about the need for backups to every primitive we use. DL is no\nexception, but for DL signatures, we do have lattices.\n\n\n> because 1) it is a layer-2 solution\n>\n\nIf this becomes global financial infrastructure, then layer 2 security will\nmatter enough to merit some cryptographic conservatism. What if lightning\nsucceeds fabulously? I think, it might.\n\n\n> and 2) it already critically depends on the security of DL.\n>\n\nDoes it? In specific form, sure - but in a pinch, could lattice-based\nalgorithms not be swapped on relatively short notice, without changing the\nconceptual architecture? Perhaps I'm overlooking something, but I think\nprobably, yes.\n\n\n> There are arguments against making certain protocol changes to the base\n> Bitcoin blockchain for the reason that they may not be quantum resistant.\n> The most notable is Confidential Transactions. There reason is that the\n> worst case attack is much worse: an attacker could print money freely and\n> without detection.\n>\n\nYes, that risk is entirely unacceptable, I agree - even at a mere $150\nbillion market cap, and much more so if this one day surpasses USD M3. Of\ncourse, if one knew the attack, it would be tempting to advocate for the\nweakness. I'm not making such an accusation here of course, I'm just saying\nthat cryptographic conservatism helps demonstrate and maintain alignment of\ninterests, and to demonstrate such alignment also to skeptical observers.\n\n\n> In Bitcoin today, a DL break would compromise funds in any addresses for\n> which the pubkey has been revealed and it's not even clear what to do about\n> the remaining funds on chain. Compromising the fundamental security of the\n> blockchain is a valid cause for concern.\n>\n\nYes, bitcoin is wise to at least hash the pub key until use. Granted,\nlightning (necessarily?) risks public key exposure, but in a pinch there\nare other signature algorithms for lightning to move to.\n\n\n> In the case of Lightning, the attack scenario on scriptless scripts is\n> that a peer is going to use a quantum computer to steal all live payments\n> routed through them from their senders before they get to the recipient.\n> This would be bad, but not catastrophic, and once it is recognized that the\n> attack is possible, insecure channels could be closed.\n>\n\nAll channels would become insecure, the very premise of lightning would\nthus break, which is only a problem if the world came to depend on it. But\nthen why try a thing, unless you plan to maybe succeed? Also, we don't know\nthat a quantum computer is necessary. SHA-1 was secure, until it wasn't, no\nquantum computer was needed to break it.\n\n\n> But furthermore, an attacker with a quantum computer could just steal the\n> multisig funding output directly instead of attacking scriptless scripts.\n>\n\nAbsolutely, and today there is no redundant signature of different\nalgorithm, in the code. (That would be better.) But even so, how hard would\nit be to swap one signature algorithm for another? Then users \"just\" move\ntheir funds to multisig addresses under the new algorithm.\n\n\n> So additional protocol changes relying on the DL assumption don't bother\n> me in the least.\n>\n\nI don't follow the logic. If today we would have a frantic scramble in\nevent of sudden DL weakness, as indeed seems probable, it does not then\nfollow that we might as well design DL weakness to become a fundamentally\nunsurvivable problem. DL signatures bother me less because lattice\ncryptography can serve as backup. Scriptless scripts worry me because I\njust don't know what the backup is, when (not if) DL falls. Perhaps\nscriptless scripts can be done with lattices(?), in which case I am simply\nunaware - but some such backup should be identified, at least at a\nconceptual level, prior to use.\n\nIf this is just a toy, then never mind. If we don't expect the world to\never depend on this for anything important, then there is no need to fret\nover the finite lifespan of primitives. Or maybe we can just hope, that\nthis time will be unlike all the others in the history of cryptography.\n\nOtherwise, consider for example the history of DES, or SHA-1. Those are\nscenarios where we saw the problems coming far enough in advance to\ntransition. Sure, it would be better if we had redundant primitives for\nevery function in the actual code itself - just in case of either a sudden,\nor else a secret, break. In fact, we should aim to one day get there. But\nfor now, let's at least have functionally equivalent backups for every\nfunction, even if only on paper. When exciting new functionality is\ninvented but based on a mathematically unproven assumption, then let's wait\nto build on it until after at least one or two mathematically dissimilar\nassumptions have been found as alternative backup foundation. The global\neconomy deserves such careful hands, I think, or else we do not deserve it.\n\nThanks,\nBenjamin Mord\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180508/efb40998/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-09T03:59:01",
                "message_text_only": "Good morning Benjamin,\n\nYour caution is laudable, I think.\n\n> Yes, bitcoin is wise to at least hash the pub key until use. Granted, lightning (necessarily?) risks public key exposure, but in a pinch there are other signature algorithms for lightning to move to.\n\nLightning cannot *quickly* move to a new signature algorithm.  At the minimum you need to wait for the signature algorithm to get widely deployed in the base-layer blockchain, then LN implementations will need to scramble to implement the new signature algorithm.  Then all LN users need to update, close existing channels, and reopen new ones.\n\nAnother issue is that the message transport is encrypted using a shared key derived from the node-identity public keys.  If a break lets attackers derive private keys from public keys, then it is possible for any LN node to have its communications spoofed, meaning any mitigation may very well be obviated: channels need to be re-anchored cooperatively, but how do you know you are cooperating with the other node in the channel rather than the attacker, if the attacker can derive the private key from the other node public key?\n\nThe sudden influx of close followed by open transactions will probably massively load the blockchain layer, too.\n\nIn that case, perhaps a concrete proposal would be to prepare a new message protocol for reanchoring transactions:\n\n1.  A new \"signing algo\" concept, which is effectively an enumeration that will be extended later, e.g. 0 => ECDSA on secp256k1, 1->255 => reserved.\n2.  `open_channel` would need to provide a `signing_algo` that underlies the commitment structure at a lower level.\n3.  A new `reopen_channel` to move a channel from one signing algorithm to another, plus a reply to accept the switch and provide new commitment transactions for both sides.  This is effectively a `shutdown` followed by the `closing_signed` negotiation followed by a new `open_channel`, but with the resulting transaction cutting through a close and a funding transaction (in order to reduce blockspace competition).\n4.  A new `reopen_channel_rbf`, possibly including a proof that an existing reopen channel has replaced (e.g. sending the actual transaction that spends the funding outpoint and bids a higher feerate than the last re-open transaction), to RBF the re-open transaction that moves from one signing algorithm to another; better to lose the channel to miners as fees then to let a thief succeed (i.e. scorched earth). This is complicated by the fact that the re-open has to be signed cooperatively by two parties whereas a thief can be singular and thus faster in replacing transactions.  But at least better to make an effort than to just give up!\n\n>\n>\n>> In the case of Lightning, the attack scenario on scriptless scripts is that a peer is going to use a quantum computer to steal all live payments routed through them from their senders before they get to the recipient. This would be bad, but not catastrophic, and once it is recognized that the attack is possible, insecure channels could be closed.\n>\n> All channels would become insecure, the very premise of lightning would thus break, which is only a problem if the world came to depend on it. But then why try a thing, unless you plan to maybe succeed? Also, we don't know that a quantum computer is necessary. SHA-1 was secure, until it wasn't, no quantum computer was needed to break it.\n>\n>> But furthermore, an attacker with a quantum computer could just steal the multisig funding output directly instead of attacking scriptless scripts.\n>\n> Absolutely, and today there is no redundant signature of different algorithm, in the code. (That would be better.) But even so, how hard would it be to swap one signature algorithm for another? Then users \"just\" move their funds to multisig addresses under the new algorithm.\n>\n>> So additional protocol changes relying on the DL assumption don't bother me in the least.\n>\n> I don't follow the logic. If today we would have a frantic scramble in event of sudden DL weakness, as indeed seems probable, it does not then follow that we might as well design DL weakness to become a fundamentally unsurvivable problem. DL signatures bother me less because lattice cryptography can serve as backup. Scriptless scripts worry me because I just don't know what the backup is, when (not if) DL falls. Perhaps scriptless scripts can be done with lattices(?), in which case I am simply unaware - but some such backup should be identified, at least at a conceptual level, prior to use.\n\nAt least on Lightning, Scriptless Script can only be used for payment forwarding.  Thus the vulnerability is time-bounded.  Further, while Scriptless Script enables new applications such as within-path decorrelation (privacy boost) and multi-path payments with proof-of-payment (functionality boost), we *can* fall back to the simple hashlocking we use now, which degrades functionality and privacy (but only to the level that we have today!).  Hashlocking can have the exact hash function changed reasonably easily, modulo blockchain-layer infrastructure change (i.e. the base layer needs to implement the replacement before upper layers can use them), plus the need to update invoice formats to use a different hash function.\n\n> If this is just a toy, then never mind. If we don't expect the world to ever depend on this for anything important, then there is no need to fret over the finite lifespan of primitives. Or maybe we can just hope, that this time will be unlike all the others in the history of cryptography.\n>\n> Otherwise, consider for example the history of DES, or SHA-1. Those are scenarios where we saw the problems coming far enough in advance to transition. Sure, it would be better if we had redundant primitives for every function in the actual code itself - just in case of either a sudden, or else a secret, break. In fact, we should aim to one day get there. But for now, let's at least have functionally equivalent backups for every function, even if only on paper. When exciting new functionality is invented but based on a mathematically unproven assumption, then let's wait to build on it until after at least one or two mathematically dissimilar assumptions have been found as alternative backup foundation. The global economy deserves such careful hands, I think, or else we do not deserve it.\n\nNaively, it seems to me that there are already multiple ways to do Scriptless.\n\n(I am not a mathematician and not a cryptographer; I am merely a dilettante, and probably massively misunderstand this)\n\n1.  The classic one which takes advantage of Bellare-Neven signature linearity, where you provide an adaptor signature that can have magical mathematic operations with the actual signature to reveal the private key.\n2.  Nonce reuse, where you provide a different message signed using the private key and a specific nonce, and commit that the actual transaction gets signed with the same private key and the same nonce used in the first message, which in combination with magical mathematic operations will reveal the private key.\n3.  The Paillier cryptosystem thing that the ECDSA Scriptless Script uses, where you encrypt the private key in the Paillier cryptosystem, provide it, and you can have magical mathematic operations reveal the private key when you sign with it.\n\nAll of the above still depend on the Discrete Log Problem being Hard.  I am uncertain if they can be adapted in a Lattice-based signing algorithm; the first one almost certainly cannot, I am unsure for the second and third.\n\nRegards,\nZmnSCPxj\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180508/5b582653/attachment.html>"
            },
            {
                "author": "Pedro Moreno Sanchez",
                "date": "2018-05-24T16:20:57",
                "message_text_only": "Hello,\n\nI know it is kind of a late reply, but my co-authors and I have been\nworking hard to get ready an extended version of the paper for this\nwork. The paper is now available at https://eprint.iacr.org/2018/472\n\nIn this paper, we describe in detail the scriptless script (SS) ECDSA\nconstruction and formally prove its security and privacy guarantees.\nAdditionally, we describe several other constructions of interest for\nthe LN:\n\n - The SS Schnorr, initially proposed by A. Poelstra. We formally\ndescribe the protocol and prove its security and privacy guarantees\n\n - Interestingly, we show that it is possible to combine SS ECDSA and SS\nSchnorr without losing security or privacy. This allows interoperability\nbetween different implementations.\n\n - A framework to combine script-based cryptographic locks using\npartially homomorphic one-way functions.\n\n - Possible applications. For instance, SS ECDSA could be used today in\nBitcoin to perform atomic swaps where the resulting transaction no\nlonger reveals the cryptographic condition. Instead, it is embedded in a\nregular ECDSA signature. This provides several advantages such as\nreduced transaction size and better privacy/fungibility among others.\n\nPlease, let us know any comment/feedback that you might have.\n\nThanks,\nPedro.\n\nOn 5/8/18 11:59 PM, ZmnSCPxj via Lightning-dev wrote:\n> Good morning Benjamin,\n> \n> Your caution is laudable, I think.\n> \n> \n>> Yes, bitcoin is wise to at least hash the pub key until use. Granted,\n>> lightning (necessarily?) risks public key exposure, but in a pinch\n>> there are other signature algorithms for lightning to move to.\n>>\n> \n> Lightning cannot *quickly* move to a new signature algorithm.\u00a0 At the\n> minimum you need to wait for the signature algorithm to get widely\n> deployed in the base-layer blockchain, then LN implementations will need\n> to scramble to implement the new signature algorithm.\u00a0 Then all LN users\n> need to update, close existing channels, and reopen new ones.\n> \n> Another issue is that the message transport is encrypted using a shared\n> key derived from the node-identity public keys.\u00a0 If a break lets\n> attackers derive private keys from public keys, then it is possible for\n> any LN node to have its communications spoofed, meaning any mitigation\n> may very well be obviated: channels need to be re-anchored\n> cooperatively, but how do you know you are cooperating with the other\n> node in the channel rather than the attacker, if the attacker can derive\n> the private key from the other node public key?\n> \n> The sudden influx of close followed by open transactions will probably\n> massively load the blockchain layer, too.\n> \n> In that case, perhaps a concrete proposal would be to prepare a new\n> message protocol for reanchoring transactions:\n> \n> 1.\u00a0 A new \"signing algo\" concept, which is effectively an enumeration\n> that will be extended later, e.g. 0 => ECDSA on secp256k1, 1->255 =>\n> reserved.\n> 2.\u00a0 `open_channel` would need to provide a `signing_algo` that underlies\n> the commitment structure at a lower level.\n> 3.\u00a0 A new `reopen_channel` to move a channel from one signing algorithm\n> to another, plus a reply to accept the switch and provide new commitment\n> transactions for both sides.\u00a0 This is effectively a `shutdown` followed\n> by the `closing_signed` negotiation followed by a new `open_channel`,\n> but with the resulting transaction cutting through a close and a funding\n> transaction (in order to reduce blockspace competition).\n> 4.\u00a0 A new `reopen_channel_rbf`, possibly including a proof that an\n> existing reopen channel has replaced (e.g. sending the actual\n> transaction that spends the funding outpoint and bids a higher feerate\n> than the last re-open transaction), to RBF the re-open transaction that\n> moves from one signing algorithm to another; better to lose the channel\n> to miners as fees then to let a thief succeed (i.e. scorched earth).\n> This is complicated by the fact that the re-open has to be signed\n> cooperatively by two parties whereas a thief can be singular and thus\n> faster in replacing transactions.\u00a0 But at least better to make an effort\n> than to just give up!\n> \n>>\n>> \u00a0\n>>\n>>\n>>     In the case of Lightning, the attack scenario on scriptless\n>>     scripts is that a peer is going to use a quantum computer to steal\n>>     all live payments routed through them from their senders before\n>>     they get to the recipient. This would be bad, but not\n>>     catastrophic, and once it is recognized that the attack is\n>>     possible, insecure channels could be closed.\n>>\n>>\n>> All channels would become insecure, the very premise of lightning\n>> would thus break, which is only a problem if the world came to depend\n>> on it. But then why try a thing, unless you plan to maybe succeed?\n>> Also, we don't know that a quantum computer is necessary. SHA-1 was\n>> secure, until it wasn't, no quantum computer was needed to break it.\n>> \u00a0\n>>\n>>     But furthermore, an attacker with a quantum computer could just\n>>     steal the multisig funding output directly instead of attacking\n>>     scriptless scripts.\n>>\n>>\n>> Absolutely, and today there is no redundant signature of different\n>> algorithm, in the code. (That would be better.) But even so, how hard\n>> would it be to swap one signature algorithm for another? Then users\n>> \"just\" move their funds to multisig addresses under the new algorithm.\n>> \u00a0\n>>\n>>     So additional protocol changes relying on the DL assumption don't\n>>     bother me in the least.\n>>\n>>\n>> I don't follow the logic. If today we would have a frantic scramble in\n>> event of sudden DL weakness, as indeed seems probable, it does not\n>> then follow that we might as well design DL weakness to become a\n>> fundamentally unsurvivable problem. DL signatures bother me less\n>> because lattice cryptography can serve as backup. Scriptless scripts\n>> worry me because I just don't know what the backup is, when (not if)\n>> DL falls. Perhaps scriptless scripts can be done with lattices(?), in\n>> which case I am simply unaware - but some such backup should be\n>> identified, at least at a conceptual level, prior to use.\n> \n> At least on Lightning, Scriptless Script can only be used for payment\n> forwarding.\u00a0 Thus the vulnerability is time-bounded.\u00a0 Further, while\n> Scriptless Script enables new applications such as within-path\n> decorrelation (privacy boost) and multi-path payments with\n> proof-of-payment (functionality boost), we *can* fall back to the simple\n> hashlocking we use now, which degrades functionality and privacy (but\n> only to the level that we have today!).\u00a0 Hashlocking can have the exact\n> hash function changed reasonably easily, modulo blockchain-layer\n> infrastructure change (i.e. the base layer needs to implement the\n> replacement before upper layers can use them), plus the need to update\n> invoice formats to use a different hash function.\n> \n>> If this is just a toy, then never mind. If we don't expect the world\n>> to ever depend on this for anything important, then there is no need\n>> to fret over the finite lifespan of primitives. Or maybe we can just\n>> hope, that this time will be unlike all the others in the history of\n>> cryptography.\n>>\n>> Otherwise, consider for example the history of DES, or SHA-1. Those\n>> are scenarios where we saw the problems coming far enough in advance\n>> to transition. Sure, it would be better if we had redundant primitives\n>> for every function in the actual code itself - just in case of either\n>> a sudden, or else a secret, break. In fact, we should aim to one day\n>> get there. But for now, let's at least have functionally equivalent\n>> backups for every function, even if only on paper. When exciting new\n>> functionality is invented but based on a mathematically unproven\n>> assumption, then let's wait to build on it until after at least one or\n>> two mathematically dissimilar assumptions have been found as\n>> alternative backup foundation. The global economy deserves such\n>> careful hands, I think, or else we do not deserve it.\n> \n> Naively, it seems to me that there are already multiple ways to do\n> Scriptless.\n> \n> (I am not a mathematician and not a cryptographer; I am merely a\n> dilettante, and probably massively misunderstand this)\n> \n> 1.\u00a0 The classic one which takes advantage of Bellare-Neven signature\n> linearity, where you provide an adaptor signature that can have magical\n> mathematic operations with the actual signature to reveal the private key.\n> 2.\u00a0 Nonce reuse, where you provide a different message signed using the\n> private key and a specific nonce, and commit that the actual transaction\n> gets signed with the same private key and the same nonce used in the\n> first message, which in combination with magical mathematic operations\n> will reveal the private key.\n> 3.\u00a0 The Paillier cryptosystem thing that the ECDSA Scriptless Script\n> uses, where you encrypt the private key in the Paillier cryptosystem,\n> provide it, and you can have magical mathematic operations reveal the\n> private key when you sign with it.\n> \n> All of the above still depend on the Discrete Log Problem being Hard.\u00a0 I\n> am uncertain if they can be adapted in a Lattice-based signing\n> algorithm; the first one almost certainly cannot, I am unsure for the\n> second and third.\n> \n> Regards,\n> ZmnSCPxj\n> \n> \n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n> \n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 819 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180524/01971226/attachment.sig>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-05-07T23:58:54",
                "message_text_only": "> It is also not clear to me how well B-N signature aggregation can work for\n> Lightning use-cases; certainly onchain claims of unilateral closes can be\n> made smaller with signature aggregation, but for mutual closes, there is\n> only one input, unless we support close aggregation somehow\n\n>From the PoV, the two-stage HTLCs, depending on how cross-input sigagg was\nimplemented, this would allow us (using sighash_single+anyone_can_pay) to\ncoalesce all the second-layer HTLCs for a particularly state+party into a\nsingle transaction with a single signature spending all HTLCs to the second\nlayer/stage.\n\n-- Laolu\n\n\nOn Sun, Apr 29, 2018 at 9:23 PM ZmnSCPxj via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> wrote:\n\n> Good morning Pedro,\n>\n> This is certainly of great interest to me; unfortunately I am not a\n> mathematician and probably cannot review if the math is correct or not.  In\n> particular it seems to me, naively, to be able to implement my AMP idea\n> which supports both path decorrelation and proof-of-payment, which is based\n> on SS and HD.\n>\n> The Lightning BOLT 1.0 spec is mostly frozen and we have good\n> inter-implementation working of HTLCs.  Supporting SS, whether on top of\n> ECDSA or Bellare-Neven, will be a large effort, and it is not clear to me\n> if it is easy to switch between ECDSA and Bellare-Neven dynamically (i.e.\n> if one hop supports ECDSA SS and the next hop supports Bellare-Neven SS).\n>\n> It is also not clear to me how well B-N signature aggregation can work for\n> Lightning use-cases; certainly onchain claims of unilateral closes can be\n> made smaller with signature aggregation, but for mutual closes, there is\n> only one input, unless we support close aggregation somehow (involving more\n> than two parties, so much more effort).  A 2-of-2 with a single signature\n> (which I believe is the basis of your SS work?) would let the mutual close\n> and commitment transactions be smaller by one signature and one pubkey,\n> though.\n>\n> At the Lightning BOLT spec level:\n>\n> 1.  We need a new global feature bit, `option_support_scriptless`, which\n> would support routing of scriptless-script conditional payments.  Paying\n> via SS can only be done if the entire route supports this option, which may\n> hamper adoption and complicate routing implementations (cannot route an SS\n> payment through nodes that do not  support SS).\n>\n> 2.  Depending on how easy it would be to translate between ECDSA and\n> Bellare-Neven SS, maybe only a local-level feature bit for\n> `option_support_scriptless_ecdsa` and `option_support_scriptless_bn`?\n>\n> 3.  Also affects BOLT11 as we would have to support both `SHA256(secret)`\n> and `secret * G` in invoices, with the latter being used for SS payments.\n>\n> 4.  We may want intra-path decorrelation (indeed, aside from AMP, this is\n> the other use of SS on Lightning).  This requires passing a blinding secret\n> to each layer of the onion in the onion routes, I think (?).\n>\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180507/58d3211e/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Scriptless Scripts with ECDSA",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Greg Sanders",
                "Pedro Moreno Sanchez",
                "Benjamin Mord",
                "Jim Posen",
                "Olaoluwa Osuntokun",
                "ZmnSCPxj"
            ],
            "messages_count": 12,
            "total_messages_chars_count": 71871
        }
    },
    {
        "title": "[Lightning-dev] Why do we need fee estimation in the protocol?",
        "thread_messages": [
            {
                "author": "CJP",
                "date": "2018-05-12T20:45:15",
                "message_text_only": "Hi,\n\nMaybe this is a stupid question, and it is late so maybe I'm\noverlooking something, but I don't want to lose a potentially good\nidea, so here it goes:\n\nRight now, BOLT#3 imposes a certain algorithm for fee estimation. This\nalgorithm is likely to be sub-optimal: fee estimation is a difficult\nsubject, and may involve subjective situation-specific considerations\nof participants. I guess it's only there to achieve some kind of\nconsensus between the peers.\n\nBut why do we need consensus at all? There are two versions of each\ncommitment transaction: one to be used for unilateral close by one peer\n(A), and one to be used by the other (B). Peer A has an interest in\n\"commit transaction A\", so I'd consider it fair to let peer A pay the\ntransaction fee for that commit tx (subtracted from its part of the\nchannel funds), and also to let peer A determine the amount of that\nfee. If A wants a different fee for whatever reason, it should simply\nbe able to ask B for a signature on an updated \"commit transaction A\".\nB shouldn't care about that fee, as long as its own funds, HTLCs etc.\nare OK.\n\nI guess, when only changing the fee, you don't even need to use a new\nrevocation secret. Your peer may have different versions which only\ndiffer in how much fees your peer pays, and you 'll never care which of\nthem will be used by your peer. Not using a new revocation secret may\nor may not be a premature optimization though.\n\nIf a peer doesn't have enough funds to pay a reasonably effective tx\nfee, normally that shouldn't be a problem, because then it doesn't have\na significant financial interest in having a usable commit tx either.\nThe only exception is if there are significant HTLCs. Is this where the\nidea breaks down? As far as I can see, this is an exceptional (but\nimportant) case, but during normal operation, peers should be free to\nchoose the fee for their own commit tx.\n\nYou'd have to deal with any follow-up transactions as well: changing a\nfee changes the txID, so you need to update follow-up transactions.\n\nregards,\nCJP"
            },
            {
                "author": "Rusty Russell",
                "date": "2018-05-14T03:51:13",
                "message_text_only": "CJP <cjp at ultimatestunts.nl> writes:\n> Hi,\n>\n> Maybe this is a stupid question, and it is late so maybe I'm\n> overlooking something, but I don't want to lose a potentially good\n> idea, so here it goes:\n>\n> Right now, BOLT#3 imposes a certain algorithm for fee estimation. This\n> algorithm is likely to be sub-optimal: fee estimation is a difficult\n> subject, and may involve subjective situation-specific considerations\n> of participants. I guess it's only there to achieve some kind of\n> consensus between the peers.\n>\n> But why do we need consensus at all? There are two versions of each\n> commitment transaction: one to be used for unilateral close by one peer\n> (A), and one to be used by the other (B). Peer A has an interest in\n> \"commit transaction A\", so I'd consider it fair to let peer A pay the\n> transaction fee for that commit tx (subtracted from its part of the\n> channel funds), and also to let peer A determine the amount of that\n> fee. If A wants a different fee for whatever reason, it should simply\n> be able to ask B for a signature on an updated \"commit transaction A\".\n> B shouldn't care about that fee, as long as its own funds, HTLCs etc.\n> are OK.\n\nIt was an attempt at simplficiation: since A always pays the fee, it\nalways sets the level.  This means, of course, that B needs to approve\nit, particularly since HTLCs pay their own fees (oops!), so actually B\ndoes care about fee levels!\n\nAnd that got us into this mess.\n\nWe *can* make them independent (with some twists for the initial case\nwhere A holds all the funds and must of necessity pay for B's fee too),\nbut the asynchronous nature of updates means it's actually quite a\ndance.  I was bracing myself to write propose that in the spec but:\n\nEven fixing this is insufficient: neither side may end up being happy\nwith the fees when it comes to use the commitment tx.  Thus Laolu\nproposal of always having an \"OP_TRUE\" output and minimal fees, to\nbasically require you to use CPFP to push the tx.  That costs some\nonchain bloat for unilateral closes, but greatly simplfied the fee\nproblems.\n\nNote that Eltoo works around this by always being a single input/output\npair with SIGHASH_SINGLE|SIGHASH_ANYONECANPAY (and relying on\nSIGHASH_NOINPUT) so you can attach fees later without this bloat issue.\n\nCheers,\nRusty."
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-15T01:03:34",
                "message_text_only": "Good morning CJP, Rusty, and list,\n\n> > But why do we need consensus at all? There are two versions of each\n> > \n> > commitment transaction: one to be used for unilateral close by one peer\n> > \n> > (A), and one to be used by the other (B). Peer A has an interest in\n> > \n> > \"commit transaction A\", so I'd consider it fair to let peer A pay the\n> > \n> > transaction fee for that commit tx (subtracted from its part of the\n> > \n> > channel funds), and also to let peer A determine the amount of that\n> > \n> > fee. If A wants a different fee for whatever reason, it should simply\n> > \n> > be able to ask B for a signature on an updated \"commit transaction A\".\n> > \n> > B shouldn't care about that fee, as long as its own funds, HTLCs etc.\n> > \n> > are OK.\n\nI was thinking this myself.\n\n> We can make them independent (with some twists for the initial case\n> \n> where A holds all the funds and must of necessity pay for B's fee too),\n\nPerhaps the twisting could be made relatively minimal: at initial funding, assuming `push_msat=0`, then B does not in fact require a commitment transaction: it could just completely forget the channel ever existed, as it has no stake in the channel.\n\nAnd in the case `push_msat!=0`, then B *does* have funds in the channel with which to pay the onchain fee.\n\n> Even fixing this is insufficient: neither side may end up being happy\n> \n> with the fees when it comes to use the commitment tx.\n\nYes, this is indeed an issue...\n\n\n> \n> Note that Eltoo works around this by always being a single input/output\n> \n> pair with SIGHASH_SINGLE|SIGHASH_ANYONECANPAY (and relying on\n> \n> SIGHASH_NOINPUT) so you can attach fees later without this bloat issue.\n> \n\nThis works for Decker-Russell-Osuntokun trigger and update transactions, but not for settlement transactions.  But then settlement transactions do not have their outputs encumbered with a CSV (it is the settlement transaction itself which is so encumbered), hence allowing direct CPFP.\n\nThe same technique cannot be used in Poon-Dryja even with `SIGHASH_NOINPUT`, since commitment transactions often have multiple outputs.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Why do we need fee estimation in the protocol?",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Rusty Russell",
                "ZmnSCPxj",
                "CJP"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 6456
        }
    },
    {
        "title": "[Lightning-dev] May 18-19: Layer 2 Event in Boston, MA",
        "thread_messages": [
            {
                "author": "Alin S. Dragos",
                "date": "2018-05-13T21:01:31",
                "message_text_only": "Hello everyone,\n\nJoin the MIT Digital Currency Initiative and Fidelity Labs on May 18-19 for a two-day event on Layer 2. Attendees will hear from leading experts in the field, and have a chance to develop applications during an all-day hackathon at the MIT Media Lab. For more details, check this website: http://www.bostonblockchaincommunity.com/\n\nDay 1\nWhere: Fidelity Labs\nWhen: 5/18 1:00pm-6:00pm\nWhat: Lively talks and panels followed by a reception\n\nDay 2\nWhere: Media Lab, 6th Floor\nWhen: 5/19 9:00am-8:00pm\nWhat: Hackathon, with breakfast, lunch and a reception included\n\nThis event is free and open to all! Register here: https://www.eventbrite.com/e/l2-summit-tickets-45235002109\n\nWe hope to see you there!\n\nAlin S. Dragos\nDigital Currency Initiative\nMIT Media Lab,\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180513/bbde135e/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "May 18-19: Layer 2 Event in Boston, MA",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Alin S. Dragos"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 960
        }
    },
    {
        "title": "[Lightning-dev] Preventing delay abuse in a Lightning-based peer-to-peer exchange",
        "thread_messages": [
            {
                "author": "Corn\u00e9 Plooy",
                "date": "2018-05-22T14:59:08",
                "message_text_only": "Hi,\n\n\nLately I've been thinking about de-centralized crypto/crypto exchange\n(\"atomic swap\") on the Lightning network. In my view, the main problem\nis that participants can delay transactions (up to HTLC time-out, which\ncan be quite long), in order to speculate on exchange rate changes. I've\nbeen looking for several approaches to deal with this problem.\n\n\nThe first approach[1] is not really interesting anymore in my opinion,\nsince I now favor the second approach. In essence, the first approach\nlet a latency monitor service monitor latencies in transactions; this\ndata can then be used as a source of information for building reputation\nof market participants. Dealing with reputation in a community of\neasily-created pseudonyms has its own set of issues. I did some analysis\non this and identified some problems.\n\n\nThe second approach[2] doesn't need a reputation network: the trading\nparties delegate the job of executing the Lightning transaction to a\ntrusted third party. The trading parties don't need to trust each other,\nand they only need to trust the third party to not cooperate with the\nother trading party to perform the delay attack. This is not a perfectly\ntrust-free decentralized design, but there is no need for the trusted\nparty to have a monopoly, and compared to traditional exchanges, a lot\nis gained: the trusted service provider can not steal customers' funds,\nand unless it puts special requirements on performing its service, its\ncustomers can remain anonymous, and it doesn't know which asset is traded.\n\n\nIt may not be needed to let a single Lightning node have channels for\nthe different cryptocurrencies: you can also have one node for each\ncurrency, and let higher-level exchange software do the forwarding. In\nthat case, you wouldn't gossip the cross-currency link on Lightning\neither: the exchange market has its own gossip mechanisms (the details\nare still not worked out).\n\n\nI doubt decentralized Lightning-based exchange will be competitive for\nvery high-frequency or high-volume needs, but for casual use by people\nwho are already on Lightning it might have some use. At least it gets\nrid of the need for exchanges with large amounts of stored crypto\nassets, which are a very attractive target for hackers.\n\n\nCJP\n\n\n[1] A trusted latency monitor service, for preventing abuse in a\nLightning-based peer-to-peer exchange,\nhttps://bitonic.nl/public/latencymonitor.pdf\n\n[2] Preventing transaction delays with a Lightning routing service, for\npreventing abuse in a Lightning-based peer-to-peer exchange,\nhttps://bitonic.nl/public/slowdown_prevention.pdf"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-22T22:45:38",
                "message_text_only": "Good morning Corne,\n\nIt seems to me that exchange delay abuse and the loop attack in the other thread have the same attack vector, namely delaying up to just before the delay period before responding.  So mitigations for one should apply as mitigations of the other.\n\nRegards,\nZmnSCPxj\n\n\n\u200bSent with ProtonMail Secure Email.\u200b\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n\nOn May 22, 2018 10:59 PM, Corn\u00e9 Plooy via Lightning-dev <lightning-dev at lists.linuxfoundation.org> wrote:\n\n> Hi,\n> \n> Lately I've been thinking about de-centralized crypto/crypto exchange\n> \n> (\"atomic swap\") on the Lightning network. In my view, the main problem\n> \n> is that participants can delay transactions (up to HTLC time-out, which\n> \n> can be quite long), in order to speculate on exchange rate changes. I've\n> \n> been looking for several approaches to deal with this problem.\n> \n> The first approach[1] is not really interesting anymore in my opinion,\n> \n> since I now favor the second approach. In essence, the first approach\n> \n> let a latency monitor service monitor latencies in transactions; this\n> \n> data can then be used as a source of information for building reputation\n> \n> of market participants. Dealing with reputation in a community of\n> \n> easily-created pseudonyms has its own set of issues. I did some analysis\n> \n> on this and identified some problems.\n> \n> The second approach[2] doesn't need a reputation network: the trading\n> \n> parties delegate the job of executing the Lightning transaction to a\n> \n> trusted third party. The trading parties don't need to trust each other,\n> \n> and they only need to trust the third party to not cooperate with the\n> \n> other trading party to perform the delay attack. This is not a perfectly\n> \n> trust-free decentralized design, but there is no need for the trusted\n> \n> party to have a monopoly, and compared to traditional exchanges, a lot\n> \n> is gained: the trusted service provider can not steal customers' funds,\n> \n> and unless it puts special requirements on performing its service, its\n> \n> customers can remain anonymous, and it doesn't know which asset is traded.\n> \n> It may not be needed to let a single Lightning node have channels for\n> \n> the different cryptocurrencies: you can also have one node for each\n> \n> currency, and let higher-level exchange software do the forwarding. In\n> \n> that case, you wouldn't gossip the cross-currency link on Lightning\n> \n> either: the exchange market has its own gossip mechanisms (the details\n> \n> are still not worked out).\n> \n> I doubt decentralized Lightning-based exchange will be competitive for\n> \n> very high-frequency or high-volume needs, but for casual use by people\n> \n> who are already on Lightning it might have some use. At least it gets\n> \n> rid of the need for exchanges with large amounts of stored crypto\n> \n> assets, which are a very attractive target for hackers.\n> \n> CJP\n> \n> [1] A trusted latency monitor service, for preventing abuse in a\n> \n> Lightning-based peer-to-peer exchange,\n> \n> https://bitonic.nl/public/latencymonitor.pdf\n> \n> [2] Preventing transaction delays with a Lightning routing service, for\n> \n> preventing abuse in a Lightning-based peer-to-peer exchange,\n> \n> https://bitonic.nl/public/slowdown_prevention.pdf\n> \n> Lightning-dev mailing list\n> \n> Lightning-dev at lists.linuxfoundation.org\n> \n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            }
        ],
        "thread_summary": {
            "title": "Preventing delay abuse in a Lightning-based peer-to-peer exchange",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Corn\u00e9 Plooy",
                "ZmnSCPxj"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 5980
        }
    },
    {
        "title": "[Lightning-dev] Imposing minimum 253 sat/ksipa feerate?",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-29T06:15:55",
                "message_text_only": "Hi all, but most especially non-c-lightning developers,\n\nSome time ago, C-lightning imposed a minimum 253 sat/ksipa feerate: https://github.com/ElementsProject/lightning/pull/1251\n\nThe reason is that the BOLT spec specifies a fee computation that is not identical to how bitcoind computes fees.\n\nThus, the minimum 250 sat/ksipa feerate, if computed using the BOLT spec, will result in a fee which bitcoind will compute as less than the minimum 250 sat/ksipa it imposes (due to difference in how BOLT and bitcoind compute feerates).\n\nNow C-lightning will not accept an onchain feerate (from `update_fee`) of less than 253 sat/ksipa, precisely because of the above issue with the divergence in how BOLT and bitcoind compute fees: anything less than 253 sat/ksipa, computed using the BOLT spec, will be rejected by bitcoind.  This results in a few issues in C-lightning where we close unilaterally when the counterparty proposes a 250sat/ksipa feerate:\n\n* https://github.com/ElementsProject/lightning/issues/1351\n* https://github.com/ElementsProject/lightning/issues/1529\n\n(C-lightning has increased the ranges recently, but the 253sat/ksipa limit is a hard limit and will still cause C-lightning to unilaterally close if the counterparty gives an `update_fee` of <253)\n\nRecently, Eclair discovered this same issue (i.e. bitcoind will not broadcast a 250 sat/ksipa feerate tx when computed using the BOLT spec algorithm): https://github.com/ACINQ/eclair/issues/602\n\nEclair appears to have also imposed the same solution as C-lightning: https://github.com/ACINQ/eclair/commit/8981d45dd52c52abe60d5c00411d638dd2124b6f\n\nucoin (nayutaco/ptarmigan) also has 253 in a constant somewhere: https://github.com/nayutaco/ptarmigan/blob/6fe9db418ec962bf1d9282bb5271750b7c5764c2/ucoin/include/ln.h#L73 https://github.com/nayutaco/ptarmigan/blob/315e49785aa3fa19d1291b4d40bfc6951f988cda/ucoind/monitoring.c#L147\n\nI am wondering whether lnd and lit have ever encountered issues with 250 sat/ksipa transactions getting propagated on the Bitcoin-level network.  I cannot find \"253\" in either codebase, suggesting that this minimum is not imposed by lnd or lit.\n\nRegards,\nZmnSCPxj\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20180529/2c66a68d/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Imposing minimum 253 sat/ksipa feerate?",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "ZmnSCPxj"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2344
        }
    }
]