[
    {
        "title": "[Lightning-dev] LN question - real life transaction bundling",
        "thread_messages": [
            {
                "author": "Ron OHara",
                "date": "2016-07-10T08:35:21",
                "message_text_only": "Hi .... forgive me if I have missed something obvious.\n\nIn the LN whitepaper (like many others) the discussion revolves around\nAlice and Bob interacting ... fine.\n\nIF Alice and Bob interact many times during an interval, there is clear\nchance to optimize that to a single 'settlement' on the block chain.\n\n\nMy question is about how likely it is that Alice and Bob are going to\ninteract many times per interval.\n\nTake the Point Of Sale case - just using myself as an example. I might\nmake a number of purchases, but that will be with a number of shops in\nany given day.  So there is no optimization possible there. Lots of\nsingle A-->B interactions ... but many different 'B' entities.\n\nIf the interval is a month ... then since I am fairly predictable...  ,\nI purchase from the same shops many times in that month... that could be\noptimized. BUT will the merchants be happy with (up to) a months worth\nof revenue still pending inside LN?  I dont think so. Visa via the\nbanks, allows merchants access to the pending funds, with the proviso\nthat they may be reversed in the future. Cashflow is vital to merchants.\n\nOK - that is for the Alice and Bob case of interactions.  Now for the\nother little problem I see here - which makes things even worse.\n\nWith Bitcoin it is NOT 'Alice transacting with Bob'.\nIt is Address(1) transacting with Address(2) .... and if both parties\nare following the recommended practice of not re-using addresses, then\ntheir next interaction is Address(3) transacting with Address(4) -\nremoving any possibility of optimization.\n\nAs far as I can tell, long running channels, are by definition identical\nto address re-use for the period they are open. That makes them very\nvulnerable to traffic analysis and thus have lower security that native\nBitcoin transactions. That is probably acceptable for many use cases,\nbut it is a tradeoff to gain performance.\n\nI have some other questions for Rusty about routing and location\nbrokerage too - but that can wait.\n\nCheers\nRon OHara\n\n\n-- \nTalent hits a target no one else can hit, genius hits a target no one\nelse can see\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 490 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20160710/e855b356/attachment.sig>"
            },
            {
                "author": "Rusty Russell",
                "date": "2016-07-18T01:25:42",
                "message_text_only": "Ron OHara <ron.ohara54 at gmail.com> writes:\n> Hi .... forgive me if I have missed something obvious.\n>\n> In the LN whitepaper (like many others) the discussion revolves around\n> Alice and Bob interacting ... fine.\n>\n> IF Alice and Bob interact many times during an interval, there is clear\n> chance to optimize that to a single 'settlement' on the block chain.\n\n\nYes, that's a simple payment channel, which have existed for some time.\nThey're very limited.\n\nLightning adds the ability to trustlessly chain channels, so Alice can\npay Carol via Bob.\n\n> If the interval is a month ... then since I am fairly predictable...  ,\n> I purchase from the same shops many times in that month... that could be\n> optimized. BUT will the merchants be happy with (up to) a months worth\n> of revenue still pending inside LN?  I dont think so. Visa via the\n> banks, allows merchants access to the pending funds, with the proviso\n> that they may be reversed in the future. Cashflow is vital to merchants.\n\nChannels are just bitcoins held by a 2 of 2 signature, with a way of\ncashing out (with some delay!) if the other side vanishes.\n\nA recipient doesn't have to actually hold that many bitcoins though,\nsince they mainly receive payments.\n\n(Now, there's another question about whether stores will actually do\nthis themselves, or outsource to Coinbase etc, just like bitcoin...)\n\n> OK - that is for the Alice and Bob case of interactions.  Now for the\n> other little problem I see here - which makes things even worse.\n>\n> With Bitcoin it is NOT 'Alice transacting with Bob'.\n> It is Address(1) transacting with Address(2) .... and if both parties\n> are following the recommended practice of not re-using addresses, then\n> their next interaction is Address(3) transacting with Address(4) -\n> removing any possibility of optimization.\n>\n> As far as I can tell, long running channels, are by definition identical\n> to address re-use for the period they are open. That makes them very\n> vulnerable to traffic analysis and thus have lower security that native\n> Bitcoin transactions. That is probably acceptable for many use cases,\n> but it is a tradeoff to gain performance.\n\nKind of.  It's better, and worse.  If Alice only has one channel, and\nit's to Bob, Bob can see all the amounts Alice spends.  It's fairly easy\nto make sure Bob can't see the final destination (just the next hop),\nbut he gets an idea of the amounts.  Nobody else can see it unless Bob\nshows them though, so it's not quite the same as on-chain.\n\nHaving three channels is a good idea; it makes the whole system more\nrobust, it spreads the information around, *and* because Bob can never\nknow then if Alice is actually routing a payment for someone else.\n\nHope that helps!\nRusty."
            },
            {
                "author": "Ron OHara",
                "date": "2016-07-18T12:17:17",
                "message_text_only": "Thanks for your feedback, but I think there is a lot more to this. \n\nOn 18/07/16 02:25, Rusty Russell wrote:\n> Ron OHara <ron.ohara54 at gmail.com> writes:\n>> Hi .... forgive me if I have missed something obvious.\n>>\n>> In the LN whitepaper (like many others) the discussion revolves around\n>> Alice and Bob interacting ... fine.\n>>\n>> IF Alice and Bob interact many times during an interval, there is clear\n>> chance to optimize that to a single 'settlement' on the block chain.\n>\n> Yes, that's a simple payment channel, which have existed for some time.\n> They're very limited.\n\nAck on that  - very limited. I can not see any 'end user' use case (P2P)\nwhere there is a good probability of optimization here.\n\nFor B2B, there ARE many repeated interactions that could be optimized,\nso that use case might support switching to LN rather than using BTC\ndirectly, but not where one of the parties is a normal consumer.\n\n>\n> Lightning adds the ability to trustlessly chain channels, so Alice can\n> pay Carol via Bob.\n>\n>> If the interval is a month ... then since I am fairly predictable...  ,\n>> I purchase from the same shops many times in that month... that could be\n>> optimized. BUT will the merchants be happy with (up to) a months worth\n>> of revenue still pending inside LN?  I dont think so. Visa via the\n>> banks, allows merchants access to the pending funds, with the proviso\n>> that they may be reversed in the future. Cashflow is vital to merchants.\n> Channels are just bitcoins held by a 2 of 2 signature, with a way of\n> cashing out (with some delay!) if the other side vanishes.\n>\n> A recipient doesn't have to actually hold that many bitcoins though,\n> since they mainly receive payments.\n>\n> (Now, there's another question about whether stores will actually do\n> this themselves, or outsource to Coinbase etc, just like bitcoin...)\n\nAs I understand it, the recipient is still 'pending' settlement to the\nblockchain for any funds they receive. That means inwards cashflow (cash\nreceivable) is unavailable for period of time. This would a big negative\nfor actual businesses. They could obviate that by outsourcing as you\nsay, but that outsourcer effectively becomes a bank credit provider to\nthem if they are given access to the cashflow prior to settlement to the\nblockchain.\n\nEven with LN hubs, the sender side (client) does need to tie up funds.\nIf you want to get optimization, then LN needs to encompass lots of\ntransactions per client (and receiver) - otherwise it just resolves as\nnear one-to-one settlement to the blockchain.\n\nWhat is unclear to me, is which use cases (involving end users), will\nhave the volume of Tx per user, to justify them reserving funds. Even\nthough as a user you are not relying on a 3rd party to hold your funds,\nthose funds are still reserved for your channel or channels, and\nunavailable for other usage. That is like saying to my bank (sort of a\nhub?) 'even though I have $100, prevent me withdrawing that, just in\ncase I want to use my Visa card for Tx this month' .... I just do not\nbelieve that people would tolerate that. They have been conditioned by\nthe current system to expect to be given all sort of tolerance for bad\nfinancial planning. Zero cost overdrafts, with nasty fees if you exceed\nthe agreed limit, rather than prudent cash planning.\n\n>\n>> OK - that is for the Alice and Bob case of interactions.  Now for the\n>> other little problem I see here - which makes things even worse.\n>>\n>> With Bitcoin it is NOT 'Alice transacting with Bob'.\n>> It is Address(1) transacting with Address(2) .... and if both parties\n>> are following the recommended practice of not re-using addresses, then\n>> their next interaction is Address(3) transacting with Address(4) -\n>> removing any possibility of optimization.\n>>\n>> As far as I can tell, long running channels, are by definition identical\n>> to address re-use for the period they are open. That makes them very\n>> vulnerable to traffic analysis and thus have lower security that native\n>> Bitcoin transactions. That is probably acceptable for many use cases,\n>> but it is a tradeoff to gain performance.\n> Kind of.  It's better, and worse.  If Alice only has one channel, and\n> it's to Bob, Bob can see all the amounts Alice spends.  It's fairly easy\n> to make sure Bob can't see the final destination (just the next hop),\n> but he gets an idea of the amounts.  Nobody else can see it unless Bob\n> shows them though, so it's not quite the same as on-chain.\nTraffic analysis is a lot more powerful than you seem to realize. Even\nin a huge maze of convoluted transactions with many parties involved,\ntraffic analysis of a system that does not deliberately/randomly delay\ninteractions easily detects correlations - even when the content is\nencrypted. That is precisely how Bletchley Park worked during WWII for\nat least half of the information it gleaned. Breaking Enigma was not the\nonly tactic those guys used.\n\nLike I said, this appears to be an inherent vulnerability. That is not\nan issue, as long as it is a known and accepted tradeoff, but that\naspect will reduce the number of use cases where LN is seen as a good fit.\n\n>\n> Having three channels is a good idea; it makes the whole system more\n> robust, it spreads the information around, *and* because Bob can never\n> know then if Alice is actually routing a payment for someone else.\n>\n> Hope that helps!\n> Rusty.\n\nI appreciate the feedback, and want to give you some support in\nbelieving the technical aspects of this can be solved. Why do I believe\nthat? Because, back in the 1980's I architected and wrote a lot of a\nsystem (TEXAS) that tackled a surprisingly similar scenario, for\nTelstra.  It end up being the 4th largest transaction processing system\nthey had in operation, so I know the technical issues can be dealt with.\n\nI am more concerned about the bootstrap problem LN faces for whatever\nuse cases are a good fit.\n\nAs I see it, LN with hubs (with routing) really only starts to gain\nmajor traffic optimization wins, when it has a lot of channels and\nparticipants..\n\nBut how do you get there? A chicken and egg business problem.\n\nRon\n\n\n-- \nTalent hits a target no one else can hit, genius hits a target no one\nelse can see"
            },
            {
                "author": "Rusty Russell",
                "date": "2016-07-20T00:57:30",
                "message_text_only": "Ron OHara <ron.ohara54 at gmail.com> writes:\n>> (Now, there's another question about whether stores will actually do\n>> this themselves, or outsource to Coinbase etc, just like bitcoin...)\n>\n> As I understand it, the recipient is still 'pending' settlement to the\n> blockchain for any funds they receive.\n\nOnly a sense that all bitcoin funds are \"pending settlement to the\nblockchain\".\n\nie. They're immediately available to use on the lightning network, and\nonly require a counter-signature to make normal on-chain bitcoin\ntransactions.\n\nTo make this concrete: we've discussed a simple protocol extension where\nyou would agree with your counterparty to split the funding transaction\nin order to pay some of it to a given bitcoin address.  That makes it no\nworse than a normal bitcoin transfer from a wallet.\n\n(With the usual caveat, that if your channel counterparty is\nunresponsive, you will suffer a delay).\n\n>> Kind of.  It's better, and worse.  If Alice only has one channel, and\n>> it's to Bob, Bob can see all the amounts Alice spends.  It's fairly easy\n>> to make sure Bob can't see the final destination (just the next hop),\n>> but he gets an idea of the amounts.  Nobody else can see it unless Bob\n>> shows them though, so it's not quite the same as on-chain.\n\n> Traffic analysis is a lot more powerful than you seem to realize. Even\n> in a huge maze of convoluted transactions with many parties involved,\n> traffic analysis of a system that does not deliberately/randomly delay\n> interactions easily detects correlations - even when the content is\n> encrypted. That is precisely how Bletchley Park worked during WWII for\n> at least half of the information it gleaned. Breaking Enigma was not the\n> only tactic those guys used.\n\nYes, I am very aware of the power of traffic analysis, especially if\nyou're considering an adversary with global view.  Timing analysis will\nbe an ongoing battle, IMHO.\n\nBut bear in mind that we're comparing this with bitcoin, which sets a\npretty low bar.\n\n> As I see it, LN with hubs (with routing) really only starts to gain\n> major traffic optimization wins, when it has a lot of channels and\n> participants..\n>\n> But how do you get there? A chicken and egg business problem.\n\nI think instant payments have great value.  But only if there is someone\nto receive the payments :)\n\nI expect LN service to bootstrap slowly along the same lines as bitcoin,\npossibly with a killer app to accelerate growth (lightningdice? I have\nno idea).\n\nCheers,\nRusty."
            },
            {
                "author": "Luke Dashjr",
                "date": "2016-07-18T15:56:18",
                "message_text_only": "On Sunday, July 10, 2016 8:35:21 AM Ron OHara wrote:\n> With Bitcoin it is NOT 'Alice transacting with Bob'.\n> It is Address(1) transacting with Address(2) .... and if both parties\n> are following the recommended practice of not re-using addresses, then\n> their next interaction is Address(3) transacting with Address(4) -\n> removing any possibility of optimization.\n\nThis is wrong. Addresses only receive, never send.\n\nIt'd make sense* (but only at a low level) if you used \"Key\" instead of \n\"Address\", but even that doesn't reflect on what is actually going on in \nBitcoin. There is simply a database update that is consuming N tokens (all of \nwhich are authenticated by satisfying their respective scripts), and producing \nM new tokens with defined scripts to authenticate future attempts to spend \nthem.\n\nAt a high level, you have two wallets transacting, but those wallets remain \nthe same regardless of address reuse. That is, Wallet(1) is transacting with \nWallet(2) for every interaction, there is no Wallet(3) or Wallet(4). And of \ncourse, the blockchain cannot see anything about these Wallets today.\n\nLuke\n\n* To be picky, note that the next interaction might be Key(2) with Key(3) if \nit is spending the output created by the initial interaction. But that's \nbeside the point."
            }
        ],
        "thread_summary": {
            "title": "LN question - real life transaction bundling",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Rusty Russell",
                "Ron OHara",
                "Luke Dashjr"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 15075
        }
    },
    {
        "title": "[Lightning-dev] [BOLT Draft] Onion Routing Spec",
        "thread_messages": [
            {
                "author": "Christian Decker",
                "date": "2016-07-25T16:23:15",
                "message_text_only": "Hi all,\n\nI took the liberty of taking apart Olaoluwa's Sphinx implementation and I\ncame up with a spec draft that I'd like to propose [1]. It should roughly\nbe Sphinx, pinning down the various key-generation and stream generation\nalgorithms, and adding a per-hop payload.\n\nThe per-hop payload is used to give instructions to individual hops, i.e.,\nhow many coins to forward to the next hop. This means that the end-to-end\npayload, i.e., the message in the Sphinx protocol, is currently unused and\ncould be omitted.\n\nThe payloads are currently fixed size (20 bytes per-hop and 1024 bytes for\nend-to-end payload) to avoid making messages collatable by their size.\nHowever, they could easily be made variable should we decide that sending\nmostly empty messages is wasteful.\n\nThe spec is implemented in Go [2] and in C [3]. The Go version is an\nadaptation of Olaoluwa's implementation, with some minor speedups, removing\nsome duplicate information, stripping the header, and switching to ChaCha20\nfor stream generation and encryption. I also added a small commandline tool\nthat allows you to write packets to stdout so that we can feed an onion\ngenerated by the C version to the Go implementation and vice-versa :-)\n\nFeedback is very welcome. If people like the draft I'll create\npull-requests for the spec and the implementations, but I'd like to keep\nthe discussion on the mailing list :-)\n\nCheers,\nChristian\n\n[1]\nhttps://github.com/cdecker/lightning-rfc/blob/master/bolts/onion-protocol.md\n[2] https://github.com/cdecker/lightning-onion/tree/chacha20\n[3] https://github.com/cdecker/lightning/tree/chacha20\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20160725/db97ec02/attachment.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2016-07-27T18:13:55",
                "message_text_only": "Hi Christian, welcome to the mailing list!\n\nExcellent work! I've been meaning to re-visit my implementation since I\nfinished it last October but have been busy tending to other lnd related\nitems.\nThanks for the cleanups and optimizations you've added to my initial Sphinx\nimplementation. Once we've finished fleshing out the initial specification,\nI'd\nbe happy to get those changes merged!\n\nI've taken a look at the diff against the existing implementation, plus the\ncurrent spec draft. I'd say the current spec draft is the *clearest*\ndescription of Sphinx I've encountered, especially the padding bits. IMO,\nthe\nnotation within the paper describing how to construct+process the\nmix-header is\nrather opaque.\n\nI see you've modified the encryption scheme of the end-to-end payload,\nswitching from the arbitrary block-size block cipher (LIONESS) to purely a\nstream cipher (ChaCha20). Initially I was under the impression that this\nmight\ncompromise one of the security arguments of the scheme, but on closer\ninspection this is perfectly fine if one extends the header MAC to the\nentire\npayload as you've done. If one was staying true to the original\nconstruction,\nthen this would eliminate the possibility of Single-Use Reply Blocks\n(SURB's)\nsince the sender would be unable to construct the reply mix-header as she\nwould\nbe unaware of they responder's message. It's clear to me now that a MAC\ncovering\nthe end-to-end payload was omitted in the original version since the proof\nof\ncomputational indistinguishability of replies vs responses depends on the\nstructure+processing being identical for both messages types. However I\ndon't\nsee us having any use for SURB's so this is an excellent change.\nAdditionally,\nmodifications to the end-to-end payload will instantly cause packet\ncorruption,\nstopping invalid packets from propagating through the network.\n\nI really like the addition of the per-hop payload! It's a change to the\noriginal construction that I've seriously considered proposing. Such a\npayload\nshould prove to be very useful in the future for information such as:\nlimits on\nthe per-hop absolute timeout, fee information, etc.\n\nThe addition of a version byte is also very welcome. This'll streamline\nfuture\nmodifications we may make to the mix-header format in the future, such as\nincreasing the size of the per-hop payload, or switching to an alternative\nformat to encoding the \"next hop\" address.\n\nThe current draft doesn't specify the processor's action in the scenario\nthat\nthey're unable to locate the next hop node within their local routing table.\nJust to be explicit, I think a final paragraph should be inserted under the\n\"Packet Forwarding\" section detailing the abort procedure.\n\n\n> However, they could easily be made variable should we decide that sending\n> mostly empty messages is wasteful.\n\nI strongly think we should maintain the size uniformity of the packet\nthroughout processing, changes in payload size between hop can give away\ninformation w.r.t a node's position within the route.\n\nWe might want to consider dropping the end-to-end payload altogether\nthough. I\ncan't really think of a clear use case for the e2e payload within our\nspecific\napplication.  That would save us 1k bytes, reducing the size of the full\nmix-header to 1234 bytes. Accounting for the additional fields within an\nHTLC\n\"add\" message, plus some additional overhead, this should keep us below\ntypical\nMTU sizes, avoiding fragmentation of HTLC \"add\" messages.\n\n> This specification is limited to version 0 packets and the structure of\n> future version may change. The receiving node then splits the packet into\nits\n> fields.\n\nPackets with a non-zero version byte should be immediately rejected, as\nwell as\npackets which aren't *exactly* 2258 bytes (or 1234 bytes if we drop the e2e\npayload).\n\n> The resulting HMAC is compared with the HMAC from the packet. Should the\n> computed HMAC and the HMAC from the packet differ then the node MUST abort\n> processing and report a route failure.\n\nPerhaps we should explicitly specify that the HMAC equality check MUST be\nperformed without leaking timing information (constant time comparison)? I\ncan't think of a precise potential vulnerability otherwise since the scheme\nuses an encrypt-then-MAC construction with a semantically secure encryption\nscheme. I don't see any clear downsides in specifying that the comparison be\nmade in constant.\n\n> The sender computes a route {n_1, ..., n_{r-1}, n_r}, where n_1 is a peer\nof\n> the sender and n_r is the recipient.\n\nIn order to eliminate ambiguity, I think this should be more explicit,\nspecifying that \"n_1\" is a *direct neighbor* of the sender\n\n> A special HMAC value of 20 0x00 bytes indicates that the currently\n> processing hop is the intended recipient and that the packet should not be\n> forwarded. At this point the end-to-end payload is fully decrypted and the\n> route has terminated.\n\nIt seems that with the current construction, then the \"next hop\" address\nwill\nalso be zero bytes if a packet processor is the last hop in the route.\nAlternatively, if the sender is aware that the receiver is actually a\n\"virtual\nchannel\", then an additional address could be used instead of the\nzero-address\nto facilitate de-multiplexing at the last hop to the destination virtual\nchannel.\n\n> In the pocessing phase the secret is the node's private key...\n\nTypo here, it should read \"In the processing phase...\"\n\nI think two key onion-routing related aspects are under specified within the\ncurrent draft: replay protection, and key rotation. Although we might want\nto\nplace details concerning key rotation in a separate document covering the\ninitial routing protocol as the two are closely related.\n\nFirst, lets talk replay protection. The current draft specifies that:\n\n> The node MUST keep a log of previously used shared secrets. Should the\nshared\n> secret already be in the log it MUST abort processing the packet and\nreport a\n> route failure, since this is likely a replay attack, otherwise the shared\n> secret is added to the log\n\nThis is definitely necessary, however as dictated this would require nodes\nto\nallocate a potentially *unbounded* amount of storage to the shared secret\n\"seen\" log. I think we can allow nodes to periodically truncate this log by\nadding an additional session time stamp to the mix-header, either placed\ndirectly after the version byte, or within the per-hop payload.\n\nWith this absolute timestamp, each entry within the \"seen\" log becomes a\ntwo-tuple: the shared secret itself, and the corresponding timestamp\nspecified\nwithin the mix-header. Before the absolute timestamp has passed, the entry\nwithin the log remains, and mix-headers received with duplicated shared\nsecret\nare rejected. If we enforce an upper bound on the \"session lifetime\", then\nnodes can periodically prune this log, discarding obsolete shared secrets.\nOnce an entry has been pruned, although a node may not know if a shared\nsecret\nis being duplicated, they can reject expired sessions according to the\ntimestamp achieving a similar affect.\n\nReasonable session times may be something around 30-minutes to an hour or\ntwo.\n\nWith this scheme, I think that we can achieve near perfect replay protection\nwithout unbounded storage.\n\nOn to the second aspect: key rotation. Ignoring the possible time-stamped\nlog\nsolution, the (possibly) only other way to allow nodes to prune their shared\nsecret log is to periodically rotate keys. Once a node rotates a key, it can\nsafely delete its *entire* previous shared secret log, as replay attacks\nwill\nfail on the HMAC check. Independent of replay attack prevention, key\nrotation\nis useful in order to provide a degree of forward secrecy. Without key\nrotation, when a node is compromised by the adversary (assuming the node\nkeeps\n*all* prior mix-headers), the adversary learns of the next-node within the\nroute, and also the per-hop payload for the compromised node. With key\nrotation, assuming the prior keys are deleted, then the adversary is only\nable\nto decrypt partially ciphertexts from the current epoch.\n\nSo then a question arises: how do we perform key rotation within the network\nglobally with loose synchronization? I say loose synchronization since if\nrotations aren't synchronized to a degree, then the payments of source nodes\nmay fail as an intermediate hop is unable to process the packet since it\nused an\nobsolete onion key. Therefore published onion keys should come in pairs\n(with\noverlapping lifetimes), and also be authenticated by a node's identity key.\n\nA key rotation scheme might look something like the following:\n    * A node publishes two keys, along with a block hash of a block beyond a\n      \"safe\" re-org distance, and a signature (under the identity pubkey)\n      covering the advertisement.\n    * The first key is intended for use until N blocks after the specified\n      block hash, with nodes switching to the second key afterwards.\n    * At the N/2 point, the original node publishes a new advertisement with\n      the second key from the original advertisement listed as the \"first\"\nkey,\n      and a new fresh quasi-ephemeral onion key. The original node performs\n      this rotation at intervals at the mid-point of expiration of the\noldest\n      key.\n    * Nodes who receive this new advertisement (aware of the previous)\nrecord\n      this as the node's next rotation key. Nodes who receive this\n      advertisement, unaware of the previous treat this as the node's\ninitial\n      pair of quasi-ephemeral onion keys.\n\nWith this scheme, rotations are synchronized very loosely, perhaps in the\ntimespan of half-days, days, etc. There is a new cost however, when\nprocessing\npackets, a node must attempt to derive the shared secret with both active\nonion\nkeys. Alternatively, instead of block hashes, we can use some other time\nbased\nbeacon as a switch over point in order to accommodate peers on multiple\nblockchains.\n\nI'll take a few more passes through the current draft spec, as well are your\ncommits in your fork of my original implementation, following up with any\nother\nquestions/comments.\n\n-- Laolu\n\n\nOn Mon, Jul 25, 2016 at 9:23 AM Christian Decker <decker.christian at gmail.com>\nwrote:\n\n> Hi all,\n>\n> I took the liberty of taking apart Olaoluwa's Sphinx implementation and I\n> came up with a spec draft that I'd like to propose [1]. It should roughly\n> be Sphinx, pinning down the various key-generation and stream generation\n> algorithms, and adding a per-hop payload.\n>\n> The per-hop payload is used to give instructions to individual hops, i.e.,\n> how many coins to forward to the next hop. This means that the end-to-end\n> payload, i.e., the message in the Sphinx protocol, is currently unused and\n> could be omitted.\n>\n> The payloads are currently fixed size (20 bytes per-hop and 1024 bytes for\n> end-to-end payload) to avoid making messages collatable by their size.\n> However, they could easily be made variable should we decide that sending\n> mostly empty messages is wasteful.\n>\n> The spec is implemented in Go [2] and in C [3]. The Go version is an\n> adaptation of Olaoluwa's implementation, with some minor speedups, removing\n> some duplicate information, stripping the header, and switching to ChaCha20\n> for stream generation and encryption. I also added a small commandline tool\n> that allows you to write packets to stdout so that we can feed an onion\n> generated by the C version to the Go implementation and vice-versa :-)\n>\n> Feedback is very welcome. If people like the draft I'll create\n> pull-requests for the spec and the implementations, but I'd like to keep\n> the discussion on the mailing list :-)\n>\n> Cheers,\n> Christian\n>\n> [1]\n> https://github.com/cdecker/lightning-rfc/blob/master/bolts/onion-protocol.md\n> [2] https://github.com/cdecker/lightning-onion/tree/chacha20\n> [3] https://github.com/cdecker/lightning/tree/chacha20\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20160727/b804a493/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Onion Routing Spec",
            "categories": [
                "Lightning-dev",
                "BOLT Draft"
            ],
            "authors": [
                "Olaoluwa Osuntokun",
                "Christian Decker"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 13994
        }
    }
]