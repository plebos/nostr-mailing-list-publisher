[
    {
        "title": "[Lightning-dev] Revisiting Link-level payment splitting via intermediary rendezvous nodes",
        "thread_messages": [
            {
                "author": "Gijs van Dam",
                "date": "2021-08-07T08:44:10",
                "message_text_only": "Hi All,\n\n\nI would like to revisit a topic that has been discussed earlier between\nZmnSCPxj and Christian Decker. [1]\n\nI think it's important to revisit it, because not only does ZmnSCPxj\npropose a cool trick, but it can also be used to mitigate certain attacks.\n(More on that later)\n\nZmnSCPxj wrote:\n\n> consider this below graph:\n>\n>       E<---D--->C<---B\n>            ^  /\n>            | /\n>            |L\n>            A\n>\n> In the above, B requests a route from B->C->D->E.\n>\n> However, C cannot send to D, since the channel direction is saturated in\nfavor of D.\n>\n> Alternately, C can route to D via A instead.  It holds the (encrypted)\nroute from D to E.  It can take that sub-route and treat it as a partial\nroute-to-payee under rendez-vous routing, as long as node A supports\nrendez-vous routing.\n>\n> This can allow re-routing or payment splitting over multiple hops.\n\nChristian replied by explaining this is not possible because \"with this\nscheme it is not possible for C to find an ephemeral key that would end up\nidentical to the one that D would require to decrypt the onion correctly.\"\n\nChristian is correct but I think this problem can be solved with a simple\nchange to the setup. Consider the same graph, but now it's not node A that\nsupports rendezvous routing but it is node D.\n\nTo circumvent the saturated channel D-C, C creates the route C->A->D, where\nnode D supports rendezvous routing. C can create a sub-route from D to E\nand treat it as a partial route-to-payee under rendezvous routing by using\nthe hop payload found when unwrapping the onion of the original route\nB->C->D->E . Because every node in a route is able to create the ephemeral\nkey for the next node by tweaking it with its own shared secret, C is also\nable to create the ephemeral key for D. C passes that ephemeral key into\nthe payload of the rendezvous node D in the alternate route, signaling to D\nit needs to swap out the key. D, upon unwrapping its onion sees that it\nneeds to swap ephemeral keys, does so, and goes on with the route to E.\n\nUnder these changed circumstances I don't think Christian's critique still\nholds, but I'd love to hear your feedback.\n\nNow as to why I think this is useful:\n\n1. ZmnSCPxj proposed this as kind of atomic JIT rebalancing. The\nrebalancing only happens if the payment happens.\n2. It can be used to offload HTLC's to another channel, mitigating channel\njamming.\n3. It can be used to mitigate Balance Disclosure Attacks, because an\nattacker wouldn't necessarily be sure which channels are used for routing.\nI think Rusty calls this \"sloppy routing\".\n\nThanks for your attention,\n\n\nGijs van Dam\n\n[1]:\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2018-November/001573.html\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210807/28d4e9bf/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-08-10T00:51:01",
                "message_text_only": "Good morning Gijs,\n\n> To circumvent the saturated channel D-C, C creates the route C->A->D,\n> where node D supports rendezvous routing. C can create a sub-route\n> from D to E and treat it as a partial route-to-payee under rendezvous\n> routing by using the hop payload found when unwrapping the onion of\n> the original route B->C->D->E . Because every node in a route is able\n> to create the ephemeral key for the next node by tweaking it with its\n> own shared secret, C is also able to create the ephemeral key for D.\n> C passes that ephemeral key into the payload of the rendezvous node D\n> in the alternate route, signaling to D it needs to swap out the key.\n> D, upon unwrapping its onion sees that it needs to swap ephemeral\n> keys, does so, and goes on with the route to E.\n\nI confess that I only have a very vague understanding of this bit (Christian understands the math involved better than me), but my vague understanding suggests this is correct.\n\nHowever, a practical problem here is that the incoming HTLC B->C has some time limit.\nPresumably, the payer B allocates every time limit for the individual HTLCs D->E, C->D, and B->C so that the time limit is the minimum advertised by the receiver.\n\nThus, if C decides to route via C->A->D, it has to ask C->A and/or A->D to give a lower time limit, or else risk its own time limit (i.e. its outgoing C->A has a time limit that is too near to the incoming B->C time limit, or even possibly exceed its incoming time limit).\n\nThus:\n\n* For JIT rebalancing, the risk is that the payment ends up failing at some later point, and C paid for a rebalance without actually benefiting from it.\n* For the link-level splitting, the risk is that C has to give a larger time limit for the reroute via A, risking its own time limit if something has to drop onchain.\n\nThe risks are more extreme with link-level splitting --- it is far less likely to occur (the risk only really happens if things have to drop onchain, but if things remain offchain and everyone just acts in good faith, then nothing bad happens) but the consequences are more dire (C potentially loses the entire payment amount, whereas with JIT rebalancing, C only risks the fee to rebalance).\n\nIf C has some special assurance with D and/or A that reduces its risk of dropping onchain (maybe some contract or agreement?) then it may be useful to continue this development, as it trades off one kind of risk for another.\n\n\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Revisiting Link-level payment splitting via intermediary rendezvous nodes",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Gijs van Dam",
                "ZmnSCPxj"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 5342
        }
    },
    {
        "title": "[Lightning-dev] Removing the Dust Limit",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-08-08T18:52:55",
                "message_text_only": "We should remove the dust limit from Bitcoin. Five reasons:\n\n1) it's not our business what outputs people want to create\n2) dust outputs can be used in various authentication/delegation smart\ncontracts\n3) dust sized htlcs in lightning (\nhttps://bitcoin.stackexchange.com/questions/46730/can-you-send-amounts-that-would-typically-be-considered-dust-through-the-light)\nforce channels to operate in a semi-trusted mode which has implications\n(AFAIU) for the regulatory classification of channels in various\njurisdictions; agnostic treatment of fund transfers would simplify this\n(like getting a 0.01 cent dividend check in the mail)\n4) thinly divisible colored coin protocols might make use of sats as value\nmarkers for transactions.\n5) should we ever do confidential transactions we can't prevent it without\ncompromising privacy / allowed transfers\n\nThe main reasons I'm aware of not allow dust creation is that:\n\n1) dust is spam\n2) dust fingerprinting attacks\n\n1 is (IMO) not valid given the 5 reasons above, and 2 is preventable by\nwell behaved wallets to not redeem outputs that cost more in fees than they\nare worth.\n\ncheers,\n\njeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210808/c022237a/attachment.html>"
            },
            {
                "author": "fiatjaf",
                "date": "2021-08-08T19:40:34",
                "message_text_only": "For the Lightning point, even if the dust limit was removed Lightning\nwould still be trimming any HTLCs below the amount they cost to redeem\nin fees, so that wouldn't make any difference.\n\nNonetheless I think reason 1 should be enough.\n\n2021-08-08 11:52 (GMT-07:00), Jeremy <jlrubin at mit.edu> said:\n> We should remove the dust limit from Bitcoin. Five reasons:\n> 1) it's not our business what outputs people want to create\n> 2) dust outputs can be used in various authentication/delegation smart\n> contracts\n> 3) dust sized htlcs in lightning (\n> https://bitcoin.stackexchange.com/questions/46730/can-you-send-amounts-that-would-typically-be-considered-dust-through-the-light\n> ) force channels to operate in a semi-trusted mode which has implications\n> (AFAIU) for the regulatory classification of channels in various jurisdictions;\n> agnostic treatment of fund transfers would simplify this (like getting a 0.01\n> cent dividend check in the mail)\n> 4) thinly divisible colored coin protocols might make use of sats as value\n> markers for transactions.\n> 5) should we ever do confidential transactions we can't prevent it without\n> compromising privacy / allowed transfers\n> The main reasons I'm aware of not allow dust creation is that:\n> 1) dust is spam\n> 2) dust fingerprinting attacks\n> 1 is (IMO) not valid given the 5 reasons above, and 2 is preventable by well\n> behaved wallets to not redeem outputs that cost more in fees than they are\n> worth.\n> cheers,\n> jeremy\n> --\n> @JeremyRubin_______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>"
            },
            {
                "author": "David A. Harding",
                "date": "2021-08-08T21:51:01",
                "message_text_only": "On Sun, Aug 08, 2021 at 11:52:55AM -0700, Jeremy wrote:\n> We should remove the dust limit from Bitcoin. Five reasons:\n\nJeremy knows this, but to be clear for other readers, the dust limit is\na policy in Bitcoin Core (and other software) where it refuses by\ndefault to relay or mine transactions with outputs below a certain\namount.  If nodes or miners running with non-default policy choose to\nrelay or mine those transactions, they are not penalized (not directly,\nat least; there's BIP152 to consider).\n\nQuestion for Jeremy: would you also allow zero-value outputs?  Or would\nyou just move the dust limit down to a fixed 1-sat?\n\nI think the dust limit is worth keeping:\n\n> 1) it's not our business what outputs people want to create\n\nEvery additional output added to the UTXO set increases the amount of\nwork full nodes need to do to validate new transactions.  For miners\nfor whom fast validation of new blocks can significantly affect their\nrevenue, larger UTXO sets increase their costs and so contributes\ntowards centralization of mining.\n\nAllowing 0-value or 1-sat outputs minimizes the cost for polluting the\nUTXO set during periods of low feerates.\n\nIf your stuff is going to slow down my node and possibly reduce my\ncensorship resistance, how is that not my business?\n\n> 2) dust outputs can be used in various authentication/delegation smart\n> contracts\n\nAll of which can also use amounts that are economically rational to\nspend on their own.  If you're gonna use the chain for something besides\nvalue transfer, and you're already wiling to pay X in fees per onchain\nuse, why is it not reasonable for us to ask you to put up something on\nthe order of X as a bond that you'll actually clean up your mess when\nyou're no longer interested in your thing?\n\n> 3) dust sized htlcs in lightning (\n> https://bitcoin.stackexchange.com/questions/46730/can-you-send-amounts-that-would-typically-be-considered-dust-through-the-light)\n> force channels to operate in a semi-trusted mode \n\nNope, nothing is forced.  Any LN node can simply refuse to accept/route\nHTLCs below the dust limit.\n\n> which has implications\n> (AFAIU) for the regulatory classification of channels in various\n> jurisdictions\n\nSucks for the people living there.  They should change their laws.  If\nthey can't do that, they should change their LN node policies not to\nroute uneconomic HTLCs.  We shouldn't make Bitcoin worse to make\ncomplying with regulations easier.\n\nI also doubt your proposed solution fixes the problem.  Any LN node that\naccepts an uneconomic HTLC cannot recover that value, so the money is\nlost either way.  Any sane regulation would treat losing value to\ntransaction fees the same as losing value to uneconomical conditions.\n\nFinally, if LN nodes start polluting the UTXO set with no economic way\nto clean up their mess, I think that's going to cause tension between\nfull node operators and LN node operators.\n\n> agnostic treatment of fund transfers would simplify this\n> (like getting a 0.01 cent dividend check in the mail)\n\nI'm not sure I understand this point.  It sounds to me like you're\ncomparing receiving an uneconomic output to receiving a check that isn't\nworth the time to cash.  But the costs of checks are borne only by the\npeople who send, receive, and process them.  The costs of uneconomic\noutputs polluting the UTXO set are borne by every full node forever (or\nfor every archival full node forever if non-archival nodes end up using\nsomething like utreexo).\n\n> 4) thinly divisible colored coin protocols might make use of sats as value\n> markers for transactions.\n\nI'm not exactly sure what you're talking about, but if Alice wants to\ncommunicate the number n onchain, she can do:\n\n    if n < dust:\n      nSequence = 0x0000 + n  # should probably check endianess\n    else:\n      nValue = n\n\nThere's at least 15 bits of nSequence currently without consensus or\npolicy meaning, and the dust limits are currently in the hundreds of\nsat, so there's plenty of space.\n\nAlice could probably also communicate the same thing by grinding her\noutput script's hash or pubkey; again, with dust limits just being\nhundreds of sats, that's not too much grinding.\n\n> 5) should we ever do confidential transactions we can't prevent it without\n> compromising privacy / allowed transfers\n\nI'm not an expert, but it seems to me that you can do that with range\nproofs.  The range proof for >dust doesn't need to become part of the\nblock chain, it can be relay only.\n\nI haven't looked since they upgraded to bulletproofs, but ISTR the\noriginal CT implementation leaked the most significant digits or\nsomething (that kept down the byte size of the proofs), so maybe it was\nalready possible to know what was certainly not dust and what might be\ndust.\n\nIn short, it's my opinion that the dust limit is not creating any real\nproblems, so it should be kept for its contribution to keeping full\nnodes faster, cheaper, and more efficient.\n\n-Dave\n\nP.S. As I prepared to send this, Matt's email arrived about \"If it\nweren't for the implications in changing standardness here, I think we\nshould consider increasing the dust limit instead.\"  I'm in agreement\nwith both parts of that statement.\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210808/3feb232d/attachment-0001.sig>"
            },
            {
                "author": "Jeremy",
                "date": "2021-08-08T22:46:26",
                "message_text_only": "Under no circumstances do I think we should *increase* the dust limit. That\nwould have a mildly confiscatory effect on current Lightning Channel\noperators, among others.\n\nGenerally, the UTXO set will grow. We should work to accommodate the worst\ncase scenario under current consensus rules. I think this points to using\nthings like Utreexo or similar rather than meddling in the user's business.\n\nI am skeptical that 0 value outputs are a real spam problem given the cost\nto create. Generally one creates an output when one either believes it\nwould make sense to redeem it in the future. So surely this is a market\nproblem, if people want them they can pay what it is worth for them to have\nit. Again, it's not my business.\n\nMatt proposes that people might use a nominal amount of bitcoin on a zero\nvalue input so that it doesn't look like dust. What Matt is asking for is\nthat in any protocol you pay for your space not via fees, but instead via\nan assurance bond that you will eventually redeem it and clean the state\nup. In my opinion, this is worse than just allowing a zero value input\nsince then you might accrue the need for an additional change output to\nwhich the bond's collateral be returned.\n\nWith respect to the check in the mail analogy, cutting down trees for paper\nis bad for everyone and shipping things using fossil fuels contributes to\nclimate change. Therefore it's a cost borne by society in some respects.\nStill, if someone else decides it's worth sending a remittance of whichever\nvalue, it is still not my business.\n\nWith respect to CT and using the range proofs to exclude dust, I'm aware\nthat can be done (hence compromising allowed transfers). Again, I don't\nthink it's quite our business what people do, but on a technical level,\nthis would have the impact of shrinking the anonymity set so is also\nsuspect to me.\n\n---------------\n\nIf we really want to create incentives for state clean up, I think it's a\ndecent design space to consider.\n\ne.g., we could set up a bottle deposit program whereby miners contribute an\namount of funds from fee revenue from creating N outputs to a \"rolling\nutxo\" (e.g., a coinbase utxo that gets spent each block op_true to op_true\nunder some miner rules) and the rolling utxo can either disperse funds to\nthe miner reward or soak up funds from the fees in order to encourage\nblocks which have a better ratio of inputs to outputs than the mean. Miners\ncan then apply this rule in the mempool to prioritize transactions that\nhelp their block's ratio. This is all without directly interfering with the\nuser's intent to create whatever outputs they want, it just provides a way\nof paying miners to clean up the public common.\n\nGas Token by Daian et al comes to mind, from Eth, w.r.t. many pitfalls\narbing these state space freeing return curves, but it's worth thinking\nthrough nonetheless.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210808/8fcb4331/attachment.html>"
            },
            {
                "author": "Jeremy",
                "date": "2021-08-08T23:07:27",
                "message_text_only": "some additional answers/clarifications\n\n\n\n> Question for Jeremy: would you also allow zero-value outputs?  Or would\n> you just move the dust limit down to a fixed 1-sat?\n>\n\nI would remove it entirely -- i don't think there's a difference between\nthe two realistically.\n\n\n\n>\n> Allowing 0-value or 1-sat outputs minimizes the cost for polluting the\n> UTXO set during periods of low feerates.\n>\n>\nMaybe that incentivizes people to make better use of the low\nfeerate periods to do more important work like consolidations so that\nothers do not have the opportunity to pollute (therefore eliminating the\nlow fee period ;)\n\n\n\n> If your stuff is going to slow down my node and possibly reduce my\n> censorship resistance, how is that not my business?\n>\n\nYou don't know that's what I'm doing, it's a guess as to my future behavior.\n\nIf it weren't worth it to me, I wouldn't be doing it. Market will solve\nwhat is worth v.s. not worth.\n\n\n\n>\n> > 2) dust outputs can be used in various authentication/delegation smart\n> > contracts\n>\n> All of which can also use amounts that are economically rational to\n> spend on their own.  If you're gonna use the chain for something besides\n> value transfer, and you're already wiling to pay X in fees per onchain\n> use, why is it not reasonable for us to ask you to put up something on\n> the order of X as a bond that you'll actually clean up your mess when\n> you're no longer interested in your thing?\n>\n\nThese authentication/delegation smart contracts can be a part of value\ntransfer e.g. some type of atomic swaps or other escrowed payment.\n\nA bond to clean it up is a fair reason; but perhaps in a protocol it might\nnot make sense to clean up the utxo otherwise and so you're creating a\ncleanup transaction (potentially has to be presigned in a way it can't be\ndone as a consolidation) and then some future consolidation to make the\ndusts+eps aggregately convenient to spend. So you'd be trading a decent\namount more chainspace v.s. just ignoring the output and writing it to disk\nand maybe eventually into a utreexo (e.g. imagine utreexo where the last N\nyears of outputs are held in memory, but eventually things get tree'd up)\nso the long term costs need not be entirely bourne in permanent storage.\n\n\n>\n> Nope, nothing is forced.  Any LN node can simply refuse to accept/route\n> HTLCs below the dust limit.\n>\n\nI'd love to hear some broad thoughts on the impact of this on routing (cc\nTarun who thinks about these things a decent amount) as this means for\nthings like multipath routes you have much stricter constraints on which\nnodes you can route payments through. The impact on capacity from every\nuser's pov might be not insubstantial.\n\n\n\n>\n> I also doubt your proposed solution fixes the problem.  Any LN node that\n> accepts an uneconomic HTLC cannot recover that value, so the money is\n> lost either way.  Any sane regulation would treat losing value to\n> transaction fees the same as losing value to uneconomical conditions.\n>\n> Finally, if LN nodes start polluting the UTXO set with no economic way\n> to clean up their mess, I think that's going to cause tension between\n> full node operators and LN node operators.\n>\n\n\n\nMy anticipation is that the LN operators would stick the uneconomic HTLCs\naggregately into a fan out utxo and try to cooperate, but failing that only\npollute the chain by O(1) for O(n) non economic HTLCs. There is a\ndifference between losing money and knowing exactly where it is but not\nclaiming it.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210808/b39f4a28/attachment.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2021-08-09T13:22:28",
                "message_text_only": "I'm pretty conservative about increasing the standard dust limit in any\nway. This would convert a higher percentage of LN channels capacity into\ndust, which is coming with a lowering of funds safety [0]. Of course, we\ncan adjust the LN security model around dust handling to mitigate the\nsafety risk in case of adversarial settings, but ultimately the standard\ndust limit creates a  \"hard\" bound, and as such it introduces a trust\nvector in the reliability of your peer to not goes\nonchain with a commitment heavily-loaded with dust-HTLC you own.\n\nLN node operators might be willingly to compensate this \"dust\" trust vector\nby relying on side-trust model, such as PKI to authenticate their peers or\nAPI tokens (LSATs, PoW tokens), probably not free from consequences for the\n\"openness\" of the LN topology...\n\nFurther, I think any authoritative setting of the dust limit presents the\nrisk of becoming ill-adjusted  w.r.t to market realities after a few months\nor years, and would need periodic reevaluations. Those reevaluations, if\nnot automated, would become a vector of endless dramas and bikeshedding as\nthe L2s ecosystems grow bigger...\n\nNote, this would also constrain the design space of newer fee schemes. Such\nas negotiated-with-mining-pool and discounted consolidation during low\nfeerate periods deployed by such producers of low-value outputs.\n`\nMoreover as an operational point, if we proceed to such an increase on the\nbase-layer, e.g to 20 sat/vb, we're going to severely damage the\npropagation of any LN transaction, where a commitment transaction is built\nwith less than 20 sat/vb outputs. Of course, core's policy deployment on\nthe base layer is gradual, but we should first give a time window for the\nLN ecosystem to upgrade and as of today we're still devoid of the mechanism\nto do it cleanly and asynchronously (e.g dynamic upgrade or quiescence\nprotocol [1]).\n\nThat said, as raised by other commentators, I don't deny we have a\nlong-term tension between L2 nodes and full-nodes operators about the UTXO\nset growth, but for now I would rather solve this with smarter engineering\nsuch as utreexo on the base-layer side or multi-party shared-utxo or\ncompressed colored coins/authentication smart contracts (e.g\nopentimestamp's merkle tree in OP_RETURN) on the upper layers rather than\naltering the current equilibrium.\n\nI think the status quo is good enough for now, and I believe we would be\nbetter off to learn from another development cycle before tweaking the dust\nlimit in any sense.\n\nAntoine\n\n[0]\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2020-May/002714.html\n[1] https://github.com/lightningnetwork/lightning-rfc/pull/869\n\nLe dim. 8 ao\u00fbt 2021 \u00e0 14:53, Jeremy <jlrubin at mit.edu> a \u00e9crit :\n\n> We should remove the dust limit from Bitcoin. Five reasons:\n>\n> 1) it's not our business what outputs people want to create\n> 2) dust outputs can be used in various authentication/delegation smart\n> contracts\n> 3) dust sized htlcs in lightning (\n> https://bitcoin.stackexchange.com/questions/46730/can-you-send-amounts-that-would-typically-be-considered-dust-through-the-light)\n> force channels to operate in a semi-trusted mode which has implications\n> (AFAIU) for the regulatory classification of channels in various\n> jurisdictions; agnostic treatment of fund transfers would simplify this\n> (like getting a 0.01 cent dividend check in the mail)\n> 4) thinly divisible colored coin protocols might make use of sats as value\n> markers for transactions.\n> 5) should we ever do confidential transactions we can't prevent it without\n> compromising privacy / allowed transfers\n>\n> The main reasons I'm aware of not allow dust creation is that:\n>\n> 1) dust is spam\n> 2) dust fingerprinting attacks\n>\n> 1 is (IMO) not valid given the 5 reasons above, and 2 is preventable by\n> well behaved wallets to not redeem outputs that cost more in fees than they\n> are worth.\n>\n> cheers,\n>\n> jeremy\n>\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n> <https://twitter.com/JeremyRubin>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210809/83985422/attachment.html>"
            },
            {
                "author": "David A. Harding",
                "date": "2021-08-10T06:14:41",
                "message_text_only": "On Mon, Aug 09, 2021 at 09:22:28AM -0400, Antoine Riard wrote:\n> I'm pretty conservative about increasing the standard dust limit in any\n> way. This would convert a higher percentage of LN channels capacity into\n> dust, which is coming with a lowering of funds safety [0]. \n\nI think that reasoning is incomplete.  There are two related things here:\n\n- **Uneconomical outputs:** outputs that would cost more to spend than\n  the value they contain.\n\n- **Dust limit:** an output amount below which Bitcoin Core (and other\n  nodes) will not relay the transaction containing that output.\n\nAlthough raising the dust limit can have the effect you describe, \nincreases in the minimum necessary feerate to get a transaction\nconfirmed in an appropriate amount of time also \"converts a higher\npercentage of LN channel capacity into dust\".  As developers, we have no\ncontrol over prevailing feerates, so this is a problem LN needs to deal\nwith regardless of Bitcoin Core's dust limit.\n\n(Related to your linked thread, that seems to be about the risk of\n\"burning funds\" by paying them to a miner who may be a party to the\nattack.  There's plenty of other alternative ways to burn funds that can\nchange the risk profile.)\n\n> the standard dust limit [...] introduces a trust vector \n\nMy point above is that any trust vector is introduced not by the dust\nlimit but by the economics of outputs being worth less than they cost to\nspend.\n\n> LN node operators might be willingly to compensate this \"dust\" trust vector\n> by relying on side-trust model\n\nThey could also use trustless probabalistic payments, which have been\ndiscussed in the context of LN for handling the problem of payments too\nsmall to be represented onchain since early 2016:\nhttps://docs.google.com/presentation/d/1G4xchDGcO37DJ2lPC_XYyZIUkJc2khnLrCaZXgvDN0U/edit?pref=2&pli=1#slide=id.g85f425098_0_178\n\n(Probabalistic payments were discussed in the general context of Bitcoin\nwell before LN was proposed, and Elements even includes an opcode for\ncreating them.)\n\n> smarter engineering such as utreexo on the base-layer side \n\nUtreexo doesn't solve this problem.  Many nodes (such as miners) will\nstill want to store the full UTXO set and access it quickly,  Utreexo\nproofs will grow in size with UTXO set size (though, at best, only\nlog(n)), so full node operators will still not want their bandwidth\nwasted by people who create UTXOs they have no reason to spend.\n\n> I think the status quo is good enough for now\n\nI agree.\n\n-Dave\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210809/24595273/attachment.sig>"
            },
            {
                "author": "Antoine Riard",
                "date": "2021-08-10T22:37:48",
                "message_text_only": ">  As developers, we have no\ncontrol over prevailing feerates, so this is a problem LN needs to deal\nwith regardless of Bitcoin Core's dust limit.\n\nRight, as of today, we're going to trim-to-dust any commitment output of\nwhich the value is inferior to the transaction owner's\n`dust_limit_satoshis` plus the HTLC-claim (either success/timeout) fee at\nthe agreed on feerate. So the feerate is the most significant variable in\ndefining what's a LN *uneconomical output*.\n\nIMO this approach presents annoying limitations. First, you still need to\ncome with an agreement among channel operators on the mempools feerate.\nSuch agreement might be problematic to find, as on one side you would like\nto let your counterparty free to pick up a feerate gauged as efficient for\nthe confirmation of their transactions but at the same time not too high to\nburn to fees your low-values HTLCs that *your* fee-estimator judged as sane\nto claim.\n\nSecondly, the trim-to-dust evaluation doesn't correctly match the lifetime\nof the HTLC. A HTLC might be considered as dust at block 100, at which\nmempools are full. Though its expiration only occurs at block 200, at which\nmempools are empty and this HTLC is fine to claim again. I think this\ninaccuracy will even become worse with a wider deployment of long-lived\nrouted packets over LN, such as DLCs or hodl invoices.\n\nAll this to say, if for those reasons LN devs remove feerate negotiation\nfrom the trim-to-dust definition to a static feerate, it would likely put a\nhigher pressure on the full-nodes operators, as the number of uneconomical\noutputs might increase.\n\n(From a LN viewpoint, I would say we're trying to solve a price discovery\nissue, namely the cost to write on the UTXO set, in a distributed system,\nwhere any deviation from the \"honest\" price means you trust more your LN\ncounterparty)\n\n> They could also use trustless probabalistic payments, which have been\ndiscussed in the context of LN for handling the problem of payments too\nsmall to be represented onchain since early 2016:\nhttps://docs.google.com/presentation/d/1G4xchDGcO37DJ2lPC_XYyZIUkJc2khnLrCaZXgvDN0U/edit?pref=2&pli=1#slide=id.g85f425098\n\nThanks to bringing to the surface probabilistic payments, yes that's a\nworthy alternative approach for low-value payments to keep in mind.\n\nLe mar. 10 ao\u00fbt 2021 \u00e0 02:15, David A. Harding <dave at dtrt.org> a \u00e9crit :\n\n> On Mon, Aug 09, 2021 at 09:22:28AM -0400, Antoine Riard wrote:\n> > I'm pretty conservative about increasing the standard dust limit in any\n> > way. This would convert a higher percentage of LN channels capacity into\n> > dust, which is coming with a lowering of funds safety [0].\n>\n> I think that reasoning is incomplete.  There are two related things here:\n>\n> - **Uneconomical outputs:** outputs that would cost more to spend than\n>   the value they contain.\n>\n> - **Dust limit:** an output amount below which Bitcoin Core (and other\n>   nodes) will not relay the transaction containing that output.\n>\n> Although raising the dust limit can have the effect you describe,\n> increases in the minimum necessary feerate to get a transaction\n> confirmed in an appropriate amount of time also \"converts a higher\n> percentage of LN channel capacity into dust\".  As developers, we have no\n> control over prevailing feerates, so this is a problem LN needs to deal\n> with regardless of Bitcoin Core's dust limit.\n>\n> (Related to your linked thread, that seems to be about the risk of\n> \"burning funds\" by paying them to a miner who may be a party to the\n> attack.  There's plenty of other alternative ways to burn funds that can\n> change the risk profile.)\n>\n> > the standard dust limit [...] introduces a trust vector\n>\n> My point above is that any trust vector is introduced not by the dust\n> limit but by the economics of outputs being worth less than they cost to\n> spend.\n>\n> > LN node operators might be willingly to compensate this \"dust\" trust\n> vector\n> > by relying on side-trust model\n>\n> They could also use trustless probabalistic payments, which have been\n> discussed in the context of LN for handling the problem of payments too\n> small to be represented onchain since early 2016:\n>\n> https://docs.google.com/presentation/d/1G4xchDGcO37DJ2lPC_XYyZIUkJc2khnLrCaZXgvDN0U/edit?pref=2&pli=1#slide=id.g85f425098_0_178\n>\n> (Probabalistic payments were discussed in the general context of Bitcoin\n> well before LN was proposed, and Elements even includes an opcode for\n> creating them.)\n>\n> > smarter engineering such as utreexo on the base-layer side\n>\n> Utreexo doesn't solve this problem.  Many nodes (such as miners) will\n> still want to store the full UTXO set and access it quickly,  Utreexo\n> proofs will grow in size with UTXO set size (though, at best, only\n> log(n)), so full node operators will still not want their bandwidth\n> wasted by people who create UTXOs they have no reason to spend.\n>\n> > I think the status quo is good enough for now\n>\n> I agree.\n>\n> -Dave\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210810/c82980c2/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Removing the Dust Limit",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "fiatjaf",
                "David A. Harding",
                "Jeremy",
                "Antoine Riard"
            ],
            "messages_count": 8,
            "total_messages_chars_count": 27485
        }
    },
    {
        "title": "[Lightning-dev] [bitcoin-dev] Removing the Dust Limit",
        "thread_messages": [
            {
                "author": "Matt Corallo",
                "date": "2021-08-08T21:14:23",
                "message_text_only": "If it weren't for the implications in changing standardness here, I think we should consider increasing the dust limit \ninstead.\n\nThe size of the UTXO set is a fundamental scalability constraint of the system. In fact, with proposals like \nassume-utxo/background history sync it is arguably *the* fundamental scalability constraint of the system. Today's dust \nlimit is incredibly low - its based on a feerate of only 3 sat/vByte in order for claiming the UTXO to have *any* value, \nnot just having enough value to be worth bothering. As feerates have gone up over time, and as we expect them to go up \nfurther, we should be considering drastically increasing the 3 sat/vByte basis to something more like 20 sat/vB.\n\nMatt\n\nOn 8/8/21 14:52, Jeremy via bitcoin-dev wrote:\n> We should remove the dust limit from Bitcoin. Five reasons:\n> \n> 1) it's not our business what outputs people want to create\n\nIt is precisely our business - the costs are born by us, not the creator. If someone wants to create outputs which don't \nmake sense to spend, they can do so using OP_RETURN, since they won't spend it anyway.\n\n> 2) dust outputs can be used in various authentication/delegation smart contracts\n\nSo can low-value-but-enough-to-be-worth-spending-when-you're-done-with-them outputs.\n\n> 3) dust sized htlcs in lightning \n> (https://bitcoin.stackexchange.com/questions/46730/can-you-send-amounts-that-would-typically-be-considered-dust-through-the-light \n> <https://bitcoin.stackexchange.com/questions/46730/can-you-send-amounts-that-would-typically-be-considered-dust-through-the-light>) \n> force channels to operate in a semi-trusted mode which has implications (AFAIU) for the regulatory classification of \n> channels in various jurisdictions; agnostic treatment of fund transfers\u00a0would simplify this (like getting a 0.01 cent \n> dividend check in the mail)\n\nThis is unrelated to the consensus dust limit. This is related to the practical question about the value of claiming an \noutput. Again, the appropriate way to solve this instead of including spendable dust outputs would be an OP_RETURN \noutput (though I believe this particular problem is actually better solved elsewhere in the lightning protocol).\n\n> 4) thinly divisible colored coin protocols might make use of sats as value markers for transactions.\n\nThese schemes can and should use values which make them economical to spend. The whole *point* of the dust limit is to \nencourage people to use values which make sense economically to \"clean up\" after they're done with them. If people want \nto use outputs which they will not spend/\"clean up\" later, they should be using OP_RETURN.\n\n> 5) should we ever do confidential transactions we can't prevent it without compromising\u00a0privacy / allowed transfers\n\nThis is the reason the dust limit is not a *consensus* limit. If and when CT were to happen we can and would relax the \nstandardness rules around the dust limit to allow for CT.\n\n> \n> The main reasons I'm aware of not allow dust creation is that:\n> \n> 1) dust is spam\n> 2) dust fingerprinting attacks\n\n3) The significant costs to every miner and full node operator."
            },
            {
                "author": "Oleg Andreev",
                "date": "2021-08-08T21:41:32",
                "message_text_only": "I agree with Jeremy. Dust limit works due to design accident: that outputs are not encrypted. But outputs are private business and the real issue is only the cost of utxo set storage born by every user. There are two ways to address this:\n\n1) either make ppl pay for renting that storage (which creates a ton of problems of its own)\n2) or make storage extremely cheap so it remains cheap at any scale. This is perfectly solved by Utreexo.\n\nBut looking at the private data because you can is a hack that creates issues of its own.\n\n> On 9 Aug 2021, at 00:16, Matt Corallo via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> \ufeffIf it weren't for the implications in changing standardness here, I think we should consider increasing the dust limit instead.\n> \n> The size of the UTXO set is a fundamental scalability constraint of the system. In fact, with proposals like assume-utxo/background history sync it is arguably *the* fundamental scalability constraint of the system. Today's dust limit is incredibly low - its based on a feerate of only 3 sat/vByte in order for claiming the UTXO to have *any* value, not just having enough value to be worth bothering. As feerates have gone up over time, and as we expect them to go up further, we should be considering drastically increasing the 3 sat/vByte basis to something more like 20 sat/vB.\n> \n> Matt\n> \n>> On 8/8/21 14:52, Jeremy via bitcoin-dev wrote:\n>> We should remove the dust limit from Bitcoin. Five reasons:\n>> 1) it's not our business what outputs people want to create\n> \n> It is precisely our business - the costs are born by us, not the creator. If someone wants to create outputs which don't make sense to spend, they can do so using OP_RETURN, since they won't spend it anyway.\n> \n>> 2) dust outputs can be used in various authentication/delegation smart contracts\n> \n> So can low-value-but-enough-to-be-worth-spending-when-you're-done-with-them outputs.\n> \n>> 3) dust sized htlcs in lightning (https://bitcoin.stackexchange.com/questions/46730/can-you-send-amounts-that-would-typically-be-considered-dust-through-the-light <https://bitcoin.stackexchange.com/questions/46730/can-you-send-amounts-that-would-typically-be-considered-dust-through-the-light>) force channels to operate in a semi-trusted mode which has implications (AFAIU) for the regulatory classification of channels in various jurisdictions; agnostic treatment of fund transfers would simplify this (like getting a 0.01 cent dividend check in the mail)\n> \n> This is unrelated to the consensus dust limit. This is related to the practical question about the value of claiming an output. Again, the appropriate way to solve this instead of including spendable dust outputs would be an OP_RETURN output (though I believe this particular problem is actually better solved elsewhere in the lightning protocol).\n> \n>> 4) thinly divisible colored coin protocols might make use of sats as value markers for transactions.\n> \n> These schemes can and should use values which make them economical to spend. The whole *point* of the dust limit is to encourage people to use values which make sense economically to \"clean up\" after they're done with them. If people want to use outputs which they will not spend/\"clean up\" later, they should be using OP_RETURN.\n> \n>> 5) should we ever do confidential transactions we can't prevent it without compromising privacy / allowed transfers\n> \n> This is the reason the dust limit is not a *consensus* limit. If and when CT were to happen we can and would relax the standardness rules around the dust limit to allow for CT.\n> \n>> The main reasons I'm aware of not allow dust creation is that:\n>> 1) dust is spam\n>> 2) dust fingerprinting attacks\n> \n> 3) The significant costs to every miner and full node operator.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Prayank",
                "date": "2021-08-09T10:25:50",
                "message_text_only": ">\u00a0As feerates have gone up over time, and as we expect them to go up further, we should be considering drastically increasing the 3 sat/vByte basis to something more like 20 sat/vB.\n\nI have no opinion on changing or removing dust limit. However, fee rates are not going up. Yes, we expect them to go up and miners revenue from fees as well. Although, fees/day (in terms of BTC) has been decreasing in each cycle. Fee rates have been ranging between 1 sat/vByte to 200-300 sat/vByte, regularly reset to 1-5 sat/vByte and very low since long time now except when hash rate went down.\n\nFees per MB since 2016: https://i.imgur.com/XEkkf99.png\u00a0\n\nHighest in this cycle on April 19 2021: 2.5 BTC\nHighest in previous cycle on December 18 2017: 10 BTC\n\nIt stays low all the time except few days in each cycle.\n\n-- \nPrayank\n\u00a0\nA3B1 E430 2298 178F\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210809/84b29654/attachment-0001.html>"
            },
            {
                "author": "Karl",
                "date": "2021-08-09T11:58:03",
                "message_text_only": "Why would removing the dust limit impact decentralisation of mining if\nminers can reconfigure the dust limit for their mined blocks?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210809/42d76e8d/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Removing the Dust Limit",
            "categories": [
                "Lightning-dev",
                "bitcoin-dev"
            ],
            "authors": [
                "Karl",
                "Oleg Andreev",
                "Matt Corallo",
                "Prayank"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 8449
        }
    },
    {
        "title": "[Lightning-dev] [bitcoin-dev]  Removing the Dust Limit",
        "thread_messages": [
            {
                "author": "Billy Tetrud",
                "date": "2021-08-10T00:30:02",
                "message_text_only": "> 5) should we ever do confidential transactions we can't prevent it without compromising\nprivacy / allowed transfers\n\nI wanted to mention the dubiousness of adding confidential transactions to\nbitcoin. Because adding CT would eliminate the ability for users to audit\nthe supply of Bitcoin, I think its incredibly unlikely to ever happen. I'm\nin the camp that we shouldn't do anything that prevents people from\nauditing the supply. I think that camp is probably pretty large. Regardless\nof what I think should happen there, and even if CT were to eventually\nhappen in bitcoin, I don't think that future possibility is a good reason\nto change the dust limit today.\n\nIt seems like dust is a scalability problem regardless of whether we use\nUtreexo eventually or not, tho an accumulator would help a ton. One idea\nwould be to destroy/delete dust at some point in the future. However, even\nif we were to plan to do this, I still don't think the dust limit should be\nremoved. But the dust limit should probably be lowered a bit, given that\nthe 546 sats limit is about 7 cents and its very doable to send 1 sat/vbyte\ntransactions, so lowering it to 200 sats seems reasonable.\n\n\nOn Mon, Aug 9, 2021 at 6:24 AM Antoine Riard via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> I'm pretty conservative about increasing the standard dust limit in any\n> way. This would convert a higher percentage of LN channels capacity into\n> dust, which is coming with a lowering of funds safety [0]. Of course, we\n> can adjust the LN security model around dust handling to mitigate the\n> safety risk in case of adversarial settings, but ultimately the standard\n> dust limit creates a  \"hard\" bound, and as such it introduces a trust\n> vector in the reliability of your peer to not goes\n> onchain with a commitment heavily-loaded with dust-HTLC you own.\n>\n> LN node operators might be willingly to compensate this \"dust\" trust\n> vector by relying on side-trust model, such as PKI to authenticate their\n> peers or API tokens (LSATs, PoW tokens), probably not free from\n> consequences for the \"openness\" of the LN topology...\n>\n> Further, I think any authoritative setting of the dust limit presents the\n> risk of becoming ill-adjusted  w.r.t to market realities after a few months\n> or years, and would need periodic reevaluations. Those reevaluations, if\n> not automated, would become a vector of endless dramas and bikeshedding as\n> the L2s ecosystems grow bigger...\n>\n> Note, this would also constrain the design space of newer fee schemes.\n> Such as negotiated-with-mining-pool and discounted consolidation during low\n> feerate periods deployed by such producers of low-value outputs.\n> `\n> Moreover as an operational point, if we proceed to such an increase on the\n> base-layer, e.g to 20 sat/vb, we're going to severely damage the\n> propagation of any LN transaction, where a commitment transaction is built\n> with less than 20 sat/vb outputs. Of course, core's policy deployment on\n> the base layer is gradual, but we should first give a time window for the\n> LN ecosystem to upgrade and as of today we're still devoid of the mechanism\n> to do it cleanly and asynchronously (e.g dynamic upgrade or quiescence\n> protocol [1]).\n>\n> That said, as raised by other commentators, I don't deny we have a\n> long-term tension between L2 nodes and full-nodes operators about the UTXO\n> set growth, but for now I would rather solve this with smarter engineering\n> such as utreexo on the base-layer side or multi-party shared-utxo or\n> compressed colored coins/authentication smart contracts (e.g\n> opentimestamp's merkle tree in OP_RETURN) on the upper layers rather than\n> altering the current equilibrium.\n>\n> I think the status quo is good enough for now, and I believe we would be\n> better off to learn from another development cycle before tweaking the dust\n> limit in any sense.\n>\n> Antoine\n>\n> [0]\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-May/002714.html\n> [1] https://github.com/lightningnetwork/lightning-rfc/pull/869\n>\n> Le dim. 8 ao\u00fbt 2021 \u00e0 14:53, Jeremy <jlrubin at mit.edu> a \u00e9crit :\n>\n>> We should remove the dust limit from Bitcoin. Five reasons:\n>>\n>> 1) it's not our business what outputs people want to create\n>> 2) dust outputs can be used in various authentication/delegation smart\n>> contracts\n>> 3) dust sized htlcs in lightning (\n>> https://bitcoin.stackexchange.com/questions/46730/can-you-send-amounts-that-would-typically-be-considered-dust-through-the-light)\n>> force channels to operate in a semi-trusted mode which has implications\n>> (AFAIU) for the regulatory classification of channels in various\n>> jurisdictions; agnostic treatment of fund transfers would simplify this\n>> (like getting a 0.01 cent dividend check in the mail)\n>> 4) thinly divisible colored coin protocols might make use of sats as\n>> value markers for transactions.\n>> 5) should we ever do confidential transactions we can't prevent it\n>> without compromising privacy / allowed transfers\n>>\n>> The main reasons I'm aware of not allow dust creation is that:\n>>\n>> 1) dust is spam\n>> 2) dust fingerprinting attacks\n>>\n>> 1 is (IMO) not valid given the 5 reasons above, and 2 is preventable by\n>> well behaved wallets to not redeem outputs that cost more in fees than they\n>> are worth.\n>>\n>> cheers,\n>>\n>> jeremy\n>>\n>> --\n>> @JeremyRubin <https://twitter.com/JeremyRubin>\n>> <https://twitter.com/JeremyRubin>\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210809/84b13e96/attachment-0001.html>"
            },
            {
                "author": "Jeremy",
                "date": "2021-08-10T05:04:07",
                "message_text_only": "You might be interested in https://eprint.iacr.org/2017/1066.pdf which\nclaims that you can make CT computationally hiding and binding, see section\n4.6.\n\nwith respect to utreexo, you might review\nhttps://github.com/mit-dci/utreexo/discussions/249?sort=new which discusses\ntradeoffs between different accumulator designs. With a swap tree, old\nthings that never move more or less naturally \"fall leftward\", although\nthere are reasons to prefer alternative designs.\n\n\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210809/b93d8107/attachment.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2021-08-10T05:44:04",
                "message_text_only": "For sure, CT can be done with computational soundness. The advantage of\nunhidden amounts (as with current bitcoin) is that you get unconditional\nsoundness. My understanding is that there is a fundamental tradeoff between\nunconditional soundness and unconditional privacy. I believe Monero has\ntaken this alternate tradeoff path with unconditional privacy but only\ncomputational soundness\n<https://www.reddit.com/r/Monero/comments/8erg8e/what_should_monero_do_about_the_soundness_problem/dxy59ad?utm_source=share&utm_medium=web2x&context=3>\n.\n\n> old things that never move more or less naturally \"fall leftward\"\n\nAh yes, something like that would definitely be interesting to basically\nmake dust a moot point. Sounds like the tradeoff mentioned is that proofs\nwould be twice as big? Except newer UTXOs would have substantially shorter\nproofs. It sounds like the kind of thing where there's some point where\nthere would be so many old UTXOs that proofs would be smaller on average in\nthe swap tree version vs the dead-leaf version. Maybe someone smarter than\nme could estimate where that point is.\n\nOn Mon, Aug 9, 2021 at 10:04 PM Jeremy <jlrubin at mit.edu> wrote:\n\n> You might be interested in https://eprint.iacr.org/2017/1066.pdf which\n> claims that you can make CT computationally hiding and binding, see section\n> 4.6.\n>\n> with respect to utreexo, you might review\n> https://github.com/mit-dci/utreexo/discussions/249?sort=new which\n> discusses tradeoffs between different accumulator designs. With a swap\n> tree, old things that never move more or less naturally \"fall leftward\",\n> although there are reasons to prefer alternative designs.\n>\n>\n>>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210809/c4607f2b/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-08-10T11:37:37",
                "message_text_only": "Good morning Billy, et al.,\n\n> For sure, CT can be done with computational soundness. The advantage of unhidden amounts (as with current bitcoin) is that you get unconditional soundness.\n\nMy understanding is that it should be possible to have unconditional soundness with the use of El-Gamal commitment scheme, am I wrong?\n\nAlternately, one possible softforkable design would be for Bitcoin to maintain a non-CT block (the current scheme) and a separately-committed CT block (i.e. similar to how SegWit has a \"separate\" \"block\"/Merkle tree that includes witnesses).\nWhen transferring funds from the legacy non-CT block, on the legacy block you put it into a \"burn\" transaction that magically causes the same amount to be created (with a trivial/publicly known salt) in the CT block.\nThen to move from the CT block back to legacy non-CT you would match one of those \"burn\" TXOs and spend it, with a proof that the amount you are removing from the CT block is exactly the same value as the \"burn\" TXO you are now spending.\n\n(for additional privacy, the values of the \"burn\" TXOs might be made into some fixed single allowed value, so that transfers passing through the CT portion would have fewer identifying features)\n\nThe \"burn\" TXOs would be some trivial anyone-can-spend, such as `<saltpoint> <0> OP_EQUAL OP_NOT` with `<saltpoint>` being what is used in the CT to cover the value, and knowledge of the scalar behind this point would allow the CT output to be spent (assuming something very much like MimbleWimble is used; otherwise it could be the hash of some P2WSH or similar analogue on the CT side).\n\nBasically, this is \"CT as a 'sidechainlike' that every fullnode runs\".\n\nIn the legacy non-CT block, the total amount of funds that are in all CT outputs is known (it would be the sum total of all the \"burn\" TXOs) and will have a known upper limit, that cannot be higher than the supply limit of the legacy non-CT block, i.e. 21 million BTC.\nAt the same time, *individual* CT-block TXOs cannot have their values known; what is learnable is only how many BTC are in all CT block TXOs, which should be sufficient privacy if there are a large enough number of users of the CT block.\n\nThis allows the CT block to use an unconditional privacy and computational soundness scheme, and if somehow the computational soundness is broken then the first one to break it would be able to steal all the CT coins, but not *all* Bitcoin coins, as there would not be enough \"burn\" TXOs on the legacy non-CT blockchain.\n\nThis may be sufficient for practical privacy.\n\n\nOn the other hand, I think the dust limit still makes sense to keep for now, though.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Charlie Lee",
                "date": "2021-08-10T18:39:39",
                "message_text_only": "ZmnSCPxj, what you are describing is pretty much what Litecoin is doing\nwith MWEB. Basically MimbleWimble (which has CT) with extension blocks. If\nyou are interested:\nhttps://github.com/litecoin-project/lips/blob/master/lip-0002.mediawiki\nhttps://github.com/litecoin-project/lips/blob/master/lip-0003.mediawiki\n\nSorry to derail the conversation with non-Bitcoin stuff. \ud83d\ude00\n\n- Charlie\n\n\nOn Tue, Aug 10, 2021 at 5:38 AM ZmnSCPxj via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Good morning Billy, et al.,\n>\n> > For sure, CT can be done with computational soundness. The advantage of\n> unhidden amounts (as with current bitcoin) is that you get unconditional\n> soundness.\n>\n> My understanding is that it should be possible to have unconditional\n> soundness with the use of El-Gamal commitment scheme, am I wrong?\n>\n> Alternately, one possible softforkable design would be for Bitcoin to\n> maintain a non-CT block (the current scheme) and a separately-committed CT\n> block (i.e. similar to how SegWit has a \"separate\" \"block\"/Merkle tree that\n> includes witnesses).\n> When transferring funds from the legacy non-CT block, on the legacy block\n> you put it into a \"burn\" transaction that magically causes the same amount\n> to be created (with a trivial/publicly known salt) in the CT block.\n> Then to move from the CT block back to legacy non-CT you would match one\n> of those \"burn\" TXOs and spend it, with a proof that the amount you are\n> removing from the CT block is exactly the same value as the \"burn\" TXO you\n> are now spending.\n>\n> (for additional privacy, the values of the \"burn\" TXOs might be made into\n> some fixed single allowed value, so that transfers passing through the CT\n> portion would have fewer identifying features)\n>\n> The \"burn\" TXOs would be some trivial anyone-can-spend, such as\n> `<saltpoint> <0> OP_EQUAL OP_NOT` with `<saltpoint>` being what is used in\n> the CT to cover the value, and knowledge of the scalar behind this point\n> would allow the CT output to be spent (assuming something very much like\n> MimbleWimble is used; otherwise it could be the hash of some P2WSH or\n> similar analogue on the CT side).\n>\n> Basically, this is \"CT as a 'sidechainlike' that every fullnode runs\".\n>\n> In the legacy non-CT block, the total amount of funds that are in all CT\n> outputs is known (it would be the sum total of all the \"burn\" TXOs) and\n> will have a known upper limit, that cannot be higher than the supply limit\n> of the legacy non-CT block, i.e. 21 million BTC.\n> At the same time, *individual* CT-block TXOs cannot have their values\n> known; what is learnable is only how many BTC are in all CT block TXOs,\n> which should be sufficient privacy if there are a large enough number of\n> users of the CT block.\n>\n> This allows the CT block to use an unconditional privacy and computational\n> soundness scheme, and if somehow the computational soundness is broken then\n> the first one to break it would be able to steal all the CT coins, but not\n> *all* Bitcoin coins, as there would not be enough \"burn\" TXOs on the legacy\n> non-CT blockchain.\n>\n> This may be sufficient for practical privacy.\n>\n>\n> On the other hand, I think the dust limit still makes sense to keep for\n> now, though.\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210810/2943c106/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-08-11T00:46:36",
                "message_text_only": "Good morning all,\n\nThinking a little more, if the dust limit is intended to help keep UTXO sets down, then on the LN side, this could be achieved as well by using channel factories (including \"one-shot\" factories which do not allow changing the topology of the subgraph inside the factory, but have the advantage of not requiring either `SIGHASH_NOINPUT` or an extra CSV constraint that is difficult to weigh in routing algorithms), where multiple channels are backed by a single UTXO.\n\nOf course, with channel factories there is now a greater set of participants who will have differing opinions on appropriate feerate.\n\nSo I suppose one can argue that the dust limit becomes less material to higher layers, than actual onchain feerates.\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Anthony Towns",
                "date": "2021-08-12T22:03:39",
                "message_text_only": "On Tue, Aug 10, 2021 at 06:37:48PM -0400, Antoine Riard via bitcoin-dev wrote:\n> Secondly, the trim-to-dust evaluation doesn't correctly match the lifetime of\n> the HTLC.\n\nRight: but that just means it's not something you should determine once\nfor the HTLC, but something you should determine each time you update the\nchannel commitment -- if fee rates are at 1sat/vb, then a 10,000 sat HTLC\nthat's going to cost 100 sats to create the utxo and eventually claim it\nmight be worth committing to, but if fee rates suddenly rise to 75sat/vb,\nthen the combined cost of 7500 sat probably isn't worthwhile (and it\ncertainly isn't worthwhile if fees rise to above 100sat/vb).\n\nThat's independent of dust limits -- those only give you a fixed size\nlower limit or about 305sats for p2wsh outputs.\n\nThings become irrational before they become uneconomic as well: ie the\n100vb is perhaps 40vb to create then 60vb to spend, so if you create\nthe utxo anyway then the 40vb is a sunk cost, and redeeming the 10k sats\nmight still be marginally wortwhile up until about 167sat/vb fee rate.\n\nBut note the logic there: it's an uneconomic output if fees rise above\n167sat/vb, but it was already economically irrational for the two parties\nto create it in the first place when fees were at or above 100sat/vb. If\nyou're trying to save every sat, dust limits aren't your problem. If\nyou're not trying to save every sat, then just add 305 sats to your\noutput so you avoid the dust limit.\n\n(And the dust limit is only preventing you from creating outputs that\nwould be irrational if they only required a pubkey reveal and signature\nto spend -- so a HTLC that requires revealing a script, two hashes,\ntwo pubkeys, a hash preimage and two signatures with the same dust\nthreshold value for p2wsh of ~305sats would already be irrational at\nabout 2.1sat/vb and unconomic at 2.75 sat/vb).\n\n> (From a LN viewpoint, I would say we're trying to solve a price discovery\n> issue, namely the cost to write on the UTXO set, in a distributed system, where\n> any deviation from the \"honest\" price means you trust more your LN\n> counterparty)\n\nAt these amounts you're already trusting your LN counterparty to not just\nclose the channel unilaterally at a high fee rate time and waste your\nfunds in fees, vs doing a much for efficient mutual/cooperative close.\n\nCheers,\naj"
            },
            {
                "author": "Jeremy",
                "date": "2021-08-20T04:51:31",
                "message_text_only": "one interesting point that came up at the bitdevs in austin today that\nfavors remove that i believe is new to this discussion (it was new to me):\n\nthe argument can be reduced to:\n\n- dust limit is a per-node relay policy.\n- it is rational for miners to mine dust outputs given their cost of\nmaintenance (storing the output potentially forever) is lower than their\nimmediate reward in fees.\n- if txn relaying nodes censor something that a miner would mine, users\nwill seek a private/direct relay to the miner and vice versa.\n- if direct relay to miner becomes popular, it is both bad for privacy and\ndecentralization.\n- therefore the dust limit, should there be demand to create dust at\nprevailing mempool feerates, causes an incentive to increase network\ncentralization (immediately)\n\nthe tradeoff is if a short term immediate incentive to promote network\ncentralization is better or worse than a long term node operator overhead.\n\n\n///////////////////\n\nmy take is that:\n\n1) having a dust limit is worse since we'd rather not have an incentive to\nproduce or roll out centralizing software, whereas not having a dust limit\ncreates an mild incentive for node operators to improve utreexo\ndecentralizing software.\n2) it's hard to quantify the magnitude of the incentives, which does matter.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210819/831b9608/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-08-21T03:10:46",
                "message_text_only": "Good morning Jeremy,\n\n> one interesting point that came up at the bitdevs in austin today that favors remove that i believe is new to this discussion (it was new to me):\n>\n> the argument can be reduced to:\n>\n> - dust limit is a per-node relay policy.\n> - it is rational for miners to mine dust outputs given their cost of maintenance\u00a0(storing the output potentially forever) is lower than their immediate reward in fees.\n> - if txn relaying nodes censor something that a miner would mine, users will seek a private/direct relay to the miner and vice versa.\n> - if direct relay to miner becomes popular, it is both bad for privacy and decentralization.\n> - therefore the dust limit, should there be demand to create dust at prevailing mempool feerates, causes an incentive to increase network centralization\u00a0(immediately)\n>\n> the tradeoff is if a short term immediate incentive to promote network centralization is better or worse than a long term node operator overhead.\n\nAgainst the above, we should note that in the Lightning spec, when an output *would have been* created that is less than the dust limit, the output is instead put into fees.\nhttps://github.com/lightningnetwork/lightning-rfc/blob/master/03-transactions.md#trimmed-outputs\n\nThus, the existence of a dust limit encourages L2 protocols to have similar rules, where outputs below the dust limit are just given over as fees to miners, so the existence of a dust limit might very well be incentivize-compatible for miners, regardless of centralization effects or not.\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Billy Tetrud",
                "date": "2021-08-26T21:21:25",
                "message_text_only": "One interesting thing I thought of: the cost of maintenance of the dust\ncreates a (very) small incentive to mine transactions that *use* dust\noutputs with a slightly lower fee that contain dust, in order to reduce the\nfuture maintenance cost for themselves. However, the rational discount\nwould likely be vanishingly small.  It would be interesting to add\nsomething to the consensus rules to decrease the vbytes for a transaction\nthat consumes dust outputs such that the value of removing them from the\nsystem (saving the future cost of maintenance) is approximately equal to\nthe amount that the fee could be made lower for such transactions. Even\nmeasuring this as a value over the whole (future) bitcoin network, I'm not\nsure how to evaluate the magnitude of this future cost.\n\n\n\n\n\nOn Fri, Aug 20, 2021 at 8:12 PM ZmnSCPxj via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Good morning Jeremy,\n>\n> > one interesting point that came up at the bitdevs in austin today that\n> favors remove that i believe is new to this discussion (it was new to me):\n> >\n> > the argument can be reduced to:\n> >\n> > - dust limit is a per-node relay policy.\n> > - it is rational for miners to mine dust outputs given their cost of\n> maintenance (storing the output potentially forever) is lower than their\n> immediate reward in fees.\n> > - if txn relaying nodes censor something that a miner would mine, users\n> will seek a private/direct relay to the miner and vice versa.\n> > - if direct relay to miner becomes popular, it is both bad for privacy\n> and decentralization.\n> > - therefore the dust limit, should there be demand to create dust at\n> prevailing mempool feerates, causes an incentive to increase network\n> centralization (immediately)\n> >\n> > the tradeoff is if a short term immediate incentive to promote network\n> centralization is better or worse than a long term node operator overhead.\n>\n> Against the above, we should note that in the Lightning spec, when an\n> output *would have been* created that is less than the dust limit, the\n> output is instead put into fees.\n>\n> https://github.com/lightningnetwork/lightning-rfc/blob/master/03-transactions.md#trimmed-outputs\n>\n> Thus, the existence of a dust limit encourages L2 protocols to have\n> similar rules, where outputs below the dust limit are just given over as\n> fees to miners, so the existence of a dust limit might very well be\n> incentivize-compatible for miners, regardless of centralization effects or\n> not.\n>\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210826/952bece6/attachment.html>"
            },
            {
                "author": "shymaa arafat",
                "date": "2021-08-27T09:07:35",
                "message_text_only": "Allow me to ask:\n\n-Untill you find a mitigation that consolidate all dust UTXOS, why don't\nyou separate them and all probably Unspendable UTXOS in a different\npartition?\n-I'm talking at the real UTXO storage level (to be kept in secondary\nstorage), and at the Merkleization level in any accumulator design Utreexo\nor what so ever(putting them in one or two subtree/forest with hardly\nchanging roots according to the categorization will reduce the proof size,\neven if slightly)\n-This will also help things like Bloom filters, assume UTXOs,...etc when\nabout 10% with almost zero probability are trimmed from the pool you are\nwithdrawing from.\n.\n-The paper I mentioned earlier says in Feb 2018, there was about 2.4m UTXOS\nless than 1000 Satoshi, of which ~824,892 holds exactly 1 Satoshi\n-I don't think any of those were spent since that time, in fact there could\nbe a possibility that the numbers may have increased\n-As the last previous reply mentioned you have to consider the long run\neffect on the UTXO set forever, this is a straight forward improvement that\ncomes with almost no effort\n.\nPs.\n-If there is something wrong, something I missed in this idea please\nexplain it to me\n-Or do you find the improvement itself a \"dust\" that doesn't worth the\neffort???\n.\nRegards & thank you all for your time in reading & replying\nShymaa M. Arafat\nOn Fri, Aug 27, 2021, 00:06 Billy Tetrud via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n> One interesting thing I thought of: the cost of maintenance of the dust\n> creates a (very) small incentive to mine transactions that *use* dust\n> outputs with a slightly lower fee that contain dust, in order to reduce the\n> future maintenance cost for themselves. However, the rational discount\n> would likely be vanishingly small.  It would be interesting to add\n> something to the consensus rules to decrease the vbytes for a transaction\n> that consumes dust outputs such that the value of removing them from the\n> system (saving the future cost of maintenance) is approximately equal to\n> the amount that the fee could be made lower for such transactions. Even\n> measuring this as a value over the whole (future) bitcoin network, I'm not\n> sure how to evaluate the magnitude of this future cost.\n>\n>\n>\n>\n>\n> On Fri, Aug 20, 2021 at 8:12 PM ZmnSCPxj via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Good morning Jeremy,\n>>\n>> > one interesting point that came up at the bitdevs in austin today that\n>> favors remove that i believe is new to this discussion (it was new to me):\n>> >\n>> > the argument can be reduced to:\n>> >\n>> > - dust limit is a per-node relay policy.\n>> > - it is rational for miners to mine dust outputs given their cost of\n>> maintenance (storing the output potentially forever) is lower than their\n>> immediate reward in fees.\n>> > - if txn relaying nodes censor something that a miner would mine, users\n>> will seek a private/direct relay to the miner and vice versa.\n>> > - if direct relay to miner becomes popular, it is both bad for privacy\n>> and decentralization.\n>> > - therefore the dust limit, should there be demand to create dust at\n>> prevailing mempool feerates, causes an incentive to increase network\n>> centralization (immediately)\n>> >\n>> > the tradeoff is if a short term immediate incentive to promote network\n>> centralization is better or worse than a long term node operator overhead.\n>>\n>> Against the above, we should note that in the Lightning spec, when an\n>> output *would have been* created that is less than the dust limit, the\n>> output is instead put into fees.\n>>\n>> https://github.com/lightningnetwork/lightning-rfc/blob/master/03-transactions.md#trimmed-outputs\n>>\n>> Thus, the existence of a dust limit encourages L2 protocols to have\n>> similar rules, where outputs below the dust limit are just given over as\n>> fees to miners, so the existence of a dust limit might very well be\n>> incentivize-compatible for miners, regardless of centralization effects or\n>> not.\n>>\n>>\n>> Regards,\n>> ZmnSCPxj\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210827/f42a4a62/attachment.html>"
            },
            {
                "author": "LORD HIS EXCELLENCY JAMES HRMH",
                "date": "2021-08-30T03:31:56",
                "message_text_only": "Good Afternoon,\n\nIt is worth reconsidering the value accumulated in dust. Speculatively, when the value of 1 BTC reaches US$ 1,000,000.00 then the value of one satoshi will be US$ 0.01 so, for 1 satoshi to be of any substantial value the value of Bitcoin will have to rise substantially higher. I ask what then should the value of fees be? Is there not a future case foreseeable at least in consideration of Bitcoin's comparison with Gold that the value may be so high as to allow that 1 satoshi may cover the mining cost of any transaction despite the reduction in sat/B for including the additional transaction. Is it not that we can foresee the dust has value and that the wealthy may have in fact millions of dust transactions that are inheritable, though I hesitate to make my business collecting them I may set up a website. The current reason for excluding dust is because it costs more to the transaction to add the dust than its value but that does not say that will always be the case.\n\nKING JAMES HRMH\nGreat British Empire\n\nRegards,\nThe Australian\nLORD HIS EXCELLENCY JAMES HRMH (& HMRH)\nof Hougun Manor & Glencoe & British Empire\nMR. Damian A. James Williamson\nWills\n\net al.\n\n\nWilltech\nwww.willtech.com.au\nwww.go-overt.com\nand other projects\n\nearn.com/willtech\nlinkedin.com/in/damianwilliamson\n\n\nm. 0487135719\nf. +61261470192\n\n\nThis email does not constitute a general advice. Please disregard this email if misdelivered.\n----\n________________________________\nFrom: bitcoin-dev <bitcoin-dev-bounces at lists.linuxfoundation.org> on behalf of shymaa arafat via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>\nSent: Friday, 27 August 2021 7:07 PM\nTo: Billy Tetrud <billy.tetrud at gmail.com>; Bitcoin Protocol Discussion <bitcoin-dev at lists.linuxfoundation.org>\nCc: lightning-dev <lightning-dev at lists.linuxfoundation.org>\nSubject: Re: [bitcoin-dev] [Lightning-dev] Removing the Dust Limit\n\nAllow me to ask:\n\n-Untill you find a mitigation that consolidate all dust UTXOS, why don't you separate them and all probably Unspendable UTXOS in a different partition?\n-I'm talking at the real UTXO storage level (to be kept in secondary storage), and at the Merkleization level in any accumulator design Utreexo or what so ever(putting them in one or two subtree/forest with hardly changing roots according to the categorization will reduce the proof size, even if slightly)\n-This will also help things like Bloom filters, assume UTXOs,...etc when about 10% with almost zero probability are trimmed from the pool you are withdrawing from.\n.\n-The paper I mentioned earlier says in Feb 2018, there was about 2.4m UTXOS less than 1000 Satoshi, of which ~824,892 holds exactly 1 Satoshi\n-I don't think any of those were spent since that time, in fact there could be a possibility that the numbers may have increased\n-As the last previous reply mentioned you have to consider the long run effect on the UTXO set forever, this is a straight forward improvement that comes with almost no effort\n.\nPs.\n-If there is something wrong, something I missed in this idea please explain it to me\n-Or do you find the improvement itself a \"dust\" that doesn't worth the effort???\n.\nRegards & thank you all for your time in reading & replying\nShymaa M. Arafat\nOn Fri, Aug 27, 2021, 00:06 Billy Tetrud via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org<mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n\nOne interesting thing I thought of: the cost of maintenance of the dust creates a (very) small incentive to mine transactions that *use* dust outputs with a slightly lower fee that contain dust, in order to reduce the future maintenance cost for themselves. However, the rational discount would likely be vanishingly small.  It would be interesting to add something to the consensus rules to decrease the vbytes for a transaction that consumes dust outputs such that the value of removing them from the system (saving the future cost of maintenance) is approximately equal to the amount that the fee could be made lower for such transactions. Even measuring this as a value over the whole (future) bitcoin network, I'm not sure how to evaluate the magnitude of this future cost.\n\n\n\n\n\nOn Fri, Aug 20, 2021 at 8:12 PM ZmnSCPxj via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org<mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\nGood morning Jeremy,\n\n> one interesting point that came up at the bitdevs in austin today that favors remove that i believe is new to this discussion (it was new to me):\n>\n> the argument can be reduced to:\n>\n> - dust limit is a per-node relay policy.\n> - it is rational for miners to mine dust outputs given their cost of maintenance (storing the output potentially forever) is lower than their immediate reward in fees.\n> - if txn relaying nodes censor something that a miner would mine, users will seek a private/direct relay to the miner and vice versa.\n> - if direct relay to miner becomes popular, it is both bad for privacy and decentralization.\n> - therefore the dust limit, should there be demand to create dust at prevailing mempool feerates, causes an incentive to increase network centralization (immediately)\n>\n> the tradeoff is if a short term immediate incentive to promote network centralization is better or worse than a long term node operator overhead.\n\nAgainst the above, we should note that in the Lightning spec, when an output *would have been* created that is less than the dust limit, the output is instead put into fees.\nhttps://github.com/lightningnetwork/lightning-rfc/blob/master/03-transactions.md#trimmed-outputs\n\nThus, the existence of a dust limit encourages L2 protocols to have similar rules, where outputs below the dust limit are just given over as fees to miners, so the existence of a dust limit might very well be incentivize-compatible for miners, regardless of centralization effects or not.\n\n\nRegards,\nZmnSCPxj\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org<mailto:bitcoin-dev at lists.linuxfoundation.org>\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org<mailto:bitcoin-dev at lists.linuxfoundation.org>\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210830/252c9f16/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Removing the Dust Limit",
            "categories": [
                "Lightning-dev",
                "bitcoin-dev"
            ],
            "authors": [
                "Anthony Towns",
                "Charlie Lee",
                "shymaa arafat",
                "Billy Tetrud",
                "Jeremy",
                "LORD HIS EXCELLENCY JAMES HRMH",
                "ZmnSCPxj"
            ],
            "messages_count": 12,
            "total_messages_chars_count": 34917
        }
    },
    {
        "title": "[Lightning-dev] v0.10.1: \"eltoo: Ethereum Layer Too\"",
        "thread_messages": [
            {
                "author": "lisa neigut",
                "date": "2021-08-10T01:04:18",
                "message_text_only": "We're pleased to announce the 0.10.1 release of c-lightning\n<https://github.com/ElementsProject/lightning/releases/tag/v0.10.1>, named\nby @nalinbhardwaj.\n\nThis is a *recommended* upgrade: payment secrets in invoices are now\ncompulsory, and offers and dual funding drafts have been updated, so these\n(experimental) features are incompatible with previous releases!\n\n*NOTE*: Users of the rebalance or drain [plugins](\nhttps://github.com/lightningd/plugins) MUST update, as payment secret is\nnow compulsory.\n\n## Highlights for Users\n\n- `experimental-dual-fund` allows advertizement of funding rates which we\nwill contribute to channels automatically, on a 1-month lease.\n- `withdraw` and `close` (if peer supports) now supports Taproot (and other\nfuture) addresses.\n- `listchannels` can now be queried by destination, as well as source.\n- `plugin rescan` now automatically reloads plugins which have changed.\n- We try to restart automatically if we notice subdaemons have been\nupgraded underneath us.\n- `fundpsbt` will no longer include uneconomic UTXOs (unless `all`).\n- `close` will return a stream of notifications if there is a delay in\nclosing.\n- Unilateral close feerates were reduced from bitcoind's \"2 CONSERVATIVE\"\nto \"6 ECONOMICAL\".\n- Tor v2 is deprecated: please upgrade to v3!\n- Fixed disconnection bug when an HTLC failed.\n- Fixed bug in rapid feerate changes, and make code space them out (LND\ncompat).\n- Fixed bug where Tor on different ports could be advertized incorrectly.\n- Fixed various bugs to make `pay` more robust.\n\n## Highlights for the Network\n\n- payment secrets in invoices are now compulsory, finally closing a\npotential probing (or, with amountless invoices, stealing) attack.\n- `option_shutdown_anysegwit` allows peers to close channels to any future\nsegwit version address (taproot!).\n- `keysend` now understands routehints, for routing to unpublished nodes,\nand sets the final CLTV to 22, for rust-lightning nodes.\n- `invoice` now allows creation of wumbo invoices (> 0.0429 BTC).\n- We will now discuss old channels with peers who reconnect, even if we\nconsider them closed.\n\n\n## Highlights for Developers\n\n- Manual pages now document *exactly* the JSON you can expect from each\ncommand (and it's tested!)\n- Plugins can now publish notifications for other plugins to listen to.\n- `force-feerates` allows complete feerate override (mainly for regtest),\nand a bug fixed where we could send 0 update_fee on regtest.\n- The HSM daemon is now separated into libhsmd, which also provides Python\nbindings.\n- `createonion` can now make variable-sized onions, and `sendonion` no\nlonger requires `direction` and `channel` for `firsthop`.\n- `dev-sendcustommsg` is now simply `sendcustommsg`.\n- Many offers API improvements and updates, including unsigned offers\n(smaller QR codes!).\n\nMore details can be found in the [changelog](\nhttps://github.com/ElementsProject/lightning/blob/v0.10.1/CHANGELOG.md).\n\nThanks to everyone for their contributions and bug reports; please keep\nthem coming.\n\nSince 0.10.0, we've had 526 commits from 15 different authors over 114 days.\n\nA special thanks goes to the 6 first time contributors:\n\n- Nalin Bhardwa\n- Nathanael\n- LightningHelper\n- OpenOms\n- Urza\n- Valentine Wallace\n\nCheers,\nLisa, Christian,  ZmnSCPxj, Rusty\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210809/b52db2be/attachment-0001.html>"
            },
            {
                "author": "Prayank",
                "date": "2021-08-10T04:20:45",
                "message_text_only": "Hi Lisa,\n\n> lisa neigut Mon, 09 Aug 2021 18:04:51 -0700\n> We're pleased to announce the 0.10.1 release of c-lightning\n> <https://github.com/ElementsProject/lightning/releases/tag/v0.10.1>, named\n> by @nalinbhardwaj.\nI am confused about the subject of this email. Is this an Ethereum project? What exactly is Ethereum layer and how is it related to this release?\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n\u00a0\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210810/8b579d97/attachment.html>"
            },
            {
                "author": "Michael Folkson",
                "date": "2021-08-12T12:23:59",
                "message_text_only": "Just a joke. Previous release names have been \"Federal Qualitative\nStrengthening\", \"(Still) Scaling the Ethereum Blockchain\", \"Blockchain\nGood, Orange Coin Bad\", \"Bitcoin's Proof of Stake\" etc\n\nhttps://github.com/ElementsProject/lightning/releases\n\nI can assure you the c-lightning team isn't planning to introduce\nproof of stake to Bitcoin or change its monetary policy. Though I'm\nsure they're open to new name suggestions for the next release if you\ncan think of one.\n\nOn Thu, Aug 12, 2021 at 4:15 AM Prayank via Lightning-dev\n<lightning-dev at lists.linuxfoundation.org> wrote:\n>\n> Hi Lisa,\n>\n> > lisa neigut Mon, 09 Aug 2021 18:04:51 -0700\n>\n> > We're pleased to announce the 0.10.1 release of c-lightning\n> > <https://github.com/ElementsProject/lightning/releases/tag/v0.10.1>, named\n> > by @nalinbhardwaj.\n>\n> I am confused about the subject of this email. Is this an Ethereum project? What exactly is Ethereum layer and how is it related to this release?\n>\n> --\n> Prayank\n>\n> A3B1 E430 2298 178F\n>\n>\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n\n\n\n-- \nMichael Folkson\nEmail: michaelfolkson at gmail.com\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3"
            },
            {
                "author": "Prayank",
                "date": "2021-08-12T13:24:16",
                "message_text_only": ">\u00a0Though I'm sure they're open to new name suggestions for the next release if you can think of one.\n\nFew suggestions:\n\nAugust: Hal Finney \nSeptember: El Salvador\nOctober: Silk Road\nNovember: First Halving\nDecember: 2228 (Last post:\u00a0https://bitcointalk.org/index.php?topic=2228)\n\n-- \nPrayank\n\u00a0\nA3B1 E430 2298 178F\n\n\n\nAug 12, 2021, 17:53 by michaelfolkson at gmail.com:\n\n> Just a joke. Previous release names have been \"Federal Qualitative\n> Strengthening\", \"(Still) Scaling the Ethereum Blockchain\", \"Blockchain\n> Good, Orange Coin Bad\", \"Bitcoin's Proof of Stake\" etc\n>\n> https://github.com/ElementsProject/lightning/releases\n>\n> I can assure you the c-lightning team isn't planning to introduce\n> proof of stake to Bitcoin or change its monetary policy. Though I'm\n> sure they're open to new name suggestions for the next release if you\n> can think of one.\n>\n> On Thu, Aug 12, 2021 at 4:15 AM Prayank via Lightning-dev\n> <lightning-dev at lists.linuxfoundation.org> wrote:\n>\n>>\n>> Hi Lisa,\n>>\n>> > lisa neigut Mon, 09 Aug 2021 18:04:51 -0700\n>>\n>> > We're pleased to announce the 0.10.1 release of c-lightning\n>> > <https://github.com/ElementsProject/lightning/releases/tag/v0.10.1>, named\n>> > by @nalinbhardwaj.\n>>\n>> I am confused about the subject of this email. Is this an Ethereum project? What exactly is Ethereum layer and how is it related to this release?\n>>\n>> --\n>> Prayank\n>>\n>> A3B1 E430 2298 178F\n>>\n>>\n>>\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n>\n>\n>\n> -- \n> Michael Folkson\n> Email: michaelfolkson at gmail.com\n> Keybase: michaelfolkson\n> PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n>\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210812/c4b023c6/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "v0.10.1: \"eltoo: Ethereum Layer Too\"",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "lisa neigut",
                "Michael Folkson",
                "Prayank"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 7335
        }
    },
    {
        "title": "[Lightning-dev] Turbo channels spec?",
        "thread_messages": [
            {
                "author": "Rusty Russell",
                "date": "2021-08-12T05:04:58",
                "message_text_only": "Sorry this took so long.\n\nhttps://github.com/lightningnetwork/lightning-rfc/pull/895\n\nThis changed quite a bit, based on discussion here and more coherent\nthinking.\n\n1. You can simply send funding_locked early, no feature needed.\n2. It's a bit useless unless you are the (sole) funder or you trust the\n   other side.  Without that, you can neither accept payments nor route\n   them; in theory if they used push_msat you could send payments out,\n   but that seems a niche case.\n3. We do want to know the short_channel_id they're going to use for the\n   channel, so we can add it to routehints for incoming payments.\n\nAdding the scid is nice anyway, for chainsplit scenarios.\n\nHere is the new text, a little formatted:\n\n\t1. `tlv_stream`: `funding_locked_tlvs`\n\t2. types:\n\t    1. type: 1 (`short_channel_id`)\n\t    2. data:\n\t        * [`short_channel_id`:`short_channel_id`]\n \n        #### Requirements\n\n        The sender:\n...\n         - SHOULD set `short_channel_id`\n\n         - if it is the sole contributor to the funding transaction, or\n           has reason to trust the peer:\n\n            - MAY send `funding_locked` before the funding transaction\n              has reached `minimum_depth`\n        \t- MAY set `short_channel_id` to a fake value, if it will\n                  route payments to that `short_channel_id`.\n          - otherwise:\n            - MUST wait until the funding transaction has reached\n              `minimum_depth` before sending this message.\n\n          - SHOULD re-transmit `funding_locked` if the\n            `short_channel_id` for this chanel has changed.\n...\n        The receiver:\n          - SHOULD ignore the `funding_locked` if it knows the\n            `short_channel_id` of the channel and it differs from the\n            value in `funding_locked`.\n\n...\n\n        Nodes which have funded the channel or trust their peers to have done,\n        can simply start using the channel instantly by sending\n        `funding_locked`.  This raises the problem of how to use this new\n        channel in route hints, since it does not yet have a block number.\n        For this reason, a convincing fake number can be use; when the real\n        funding transaction is finally mined, it can re-send `funding_locked`\n        with the real value."
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-08-14T02:09:18",
                "message_text_only": "Good morning Rusty,\n\n> ZmnSCPxj ZmnSCPxj at protonmail.com writes:\n>\n> > Mostly nitpick on terminology below, but I think text substantially like the above should exist in some kind of \"rationale\" section in the BOLT, so ---\n> > In light of dual-funding we should avoid \"funder\" and \"fundee\" in favor of \"initiator\" and \"acceptor\".\n>\n> Yes, Lisa has a patch for this in her spec PR :)\n>\n> > So what matters for the above rationale is the \"sender\" of an HTLC and the \"receiver\" of an HTLC, not really who is acceptor or initiator.\n> >\n> > -   Risks for HTLC sender is that the channel never confirms, but it probably ignores the risk because it can close onchain (annoying, and fee-heavy, but not loss of funds caused by peer).\n> > -   Risks for HTLC receiver is that the channel never confirms, so HTLC must not be routed out to others or resolved locally if the receiver already knows the preimage, UNLESS the HTLC receiver has some other reason to trust the peer.\n>\n> This misses an important case: even with the dual-funding prototol,\n> single-sided funding is more common.\n>\n> So:\n>\n> -   if your peer hasn't contributed funds:\n>     -   You are in control, channel is safe (modulo your own conf issues)\n\nHmm.\n\nIn single-funding, if you sent out an HTLC, got the preimage, then now your peer has funds in the channel.\nIf you do this before the channel confirms, then the peer can send to you, and you can accept it safely without concern since your peer cannot block the channel confirmation.\n\nSo yes, it seems correct.\n\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Turbo channels spec?",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Rusty Russell",
                "ZmnSCPxj"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 3805
        }
    },
    {
        "title": "[Lightning-dev] Algorithm For Channel Fee Settings",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2021-08-14T12:35:25",
                "message_text_only": "Introduction\n============\n\nAs use of Lightning grows, we should consider delegating more and more tasks to programs.\nClassically, decisions on *where* to make channels have been given to \"autopilot\" programs, for example.\nHowever, there remain other tasks that would still appreciate delegation to a policy pre-encoded into a program.\n\nThis frees up higher-level sentience from network-level concerns.\nThen, higher-level sentience can decide on higher-level concerns, such as how much money to put in a Lightning node, how to safely replicate channel state, and preparing plans to recover in case of disaster (long downtime, channel state storage failure, and so on).\n\nOne such task would be setting channel fees for forwarding nodes (which all nodes should be, unpublished channels delenda est).\nThis write-up presents this problem and invites others to consider the problem of creating a heuristic that allows channel fee settings to be left to a program.\n\nPrice Theory\n============\n\nThe algorithm I will present here is based on the theory that there is a single global maxima for price, where the earnings for a particular price point, per unit time, is highest.\n\nThe logic is that as the price increases, fewer payments get routed over that channel, and thus even though you earn more per payment, you end up getting fewer payments.\nConversely, as the price decreases, there are more payments, but because the price you impose is lower, you earn less per payment.\n\nThe above assumption seems sound to me, given what we know about payment pathfinding algorithms (since we implemented them).\nMost such algorithms have a bias towards shorter paths, where \"shorter\" is generally defined (at least partially) as having lower fees.\nThus, we expect that, all other things being fixed (channel topology, channel sizes, etc.), if we lower the price on a channel (i.e. lower its channel fees) we will get more forwarding requests via that channel.\nAnd if we raise the price on the channel, we will get fewer forwarding requests via that channel.\n\nIt is actually immaterial for this assumption exactly *what* the relationship is between price and number of forwards as long as it is true that pricier = fewer forwards.\nWhether or not this relationship is linear, quadratic, exponential, has an asymptote, whatever, as long as higher prices imply fewer payments, we can assume that there is a global maxima for the price.\n\nFor example, suppose there is a linear relationship.\nAt price of 1, we get 10 payments for a particular unit of time, at price of 2 we get 9 payments, and so on until at price of 10 we get 1 payment._\nThen the maximum earnings is achieved at price of 5 per payment (times 6 payments) or price of 6 per payment (times 5 payments).\n\nIF the relationship is nonlinear, then it is not so straightforward, but in any case, there is still some price setting that is optimal.\nAt a price of 0 you earn nothing no matter how many free riders forward over your node.\nOn the higher end, there is some price that is so high that nobody will route through you and you also earn nothing (and raising the price higher will not change this result).\n\nThus, there (should) exist some middle ground where the price is such that it earns you the most amount of fees per unit time.\nThe question is how to find it!\n\nSampling\n========\n\nGiven the assumption that there exists some global maxima, obviously a simple solution like Hill Climbing would work.\n\nThe next issue is that mathematical optimization techniques (like Hill Climbing) need to somehow query the \"function\" that is being optimized.\nIn our case, the function is \"fees earned per unit time\".\nWe do not know exactly how this function looks like, and it is quite possible that, given each node has a more-or-less \"unique\" position on the network, the function would vary for each channel of each node.\n\nThus, our only real hope is to *actually* set our fees to whatever the algorithm is querying, then take some time to measure the *actual* earnings in a certain specific amount of time, and *then* return the result to the algorithm.\n\nWorse, the topology of the network changes all the time, thus the actual function being optimized is also changing over time!\nHill Climbing works well here since it is an anytime algorithm, meaning it can be interrupted at any time and it will return *some* result which, if not optimal, is at least statistically better than a random dart-throw.\nA change in the topology is effectively an \"interruption\" of whatever optimization algorithm we use, since any partial results it has may be invalidated due to the topology change.\n\nIn particular, if we are the only public node to a particular receiver, then we have a monopoly on payments going to that node.\nIf another node opens a channel to that receiver, however, suddenly our maxima changes (probably lower) and our optimization algorithm then needs to adapt to the new situation.\nOthers closing channels may change the optima as well, towards higher prices.\nAnd so on.\n\nFinally, by getting *actual* data from the real world, rather than an idealized model of it, we also bring in the possibility of noise.\nThe earned fees during a particular time when we \"evaluate the function\" on behalf of the optimization algorithm may not be due to our particular channel fees, but rather due to, say, a sale at a local coffee shop.\n\nHill Climbing helps against such noisiness, as it can \"backtrack\" in case of a burst of noise.\nOf course, this assumes that noise is \"bursty\", but if the noise is not bursty then it might then be argued to become less \"noise\" and more \"signal\" (maybe?).\n\nThus, my concrete proposal for a channel fee setting program is to use a Hill Climbing algorithm, with evaluation of the function being, say, a few days of sampling actual data from the node channel fee.\nOver time, the feerate that is sampled by this Hill Climbing algorithm will end up as approximations of the true optimal price level.\n\nHill Climbing may not be the most efficient way to optimize.\nHowever, its anytime property makes it robust against topology changes (and we expect LN topology to change continuously) and the algorithm itself is simple enough to implement, so it seems a reasonable starting point.\n\nStart Point\n===========\n\nHill Climbing is not magical.\n\nWhile it can modify an existing start point, and eventually discover the optima, it has to have *some* start point that it will modify.\n\nNaively, it seems to me that the heuristic \"imitate what others are doing\" seems reasonable.\nEven if others have absolutely no idea what they hell they are doing, imitating them at least gets you \"neck-and-neck\" with them in anything competitive.\nConsider that if you were to start by throwing a dart to some random price point, you have a chance of starting off worse than the competition , and reasonable prices may be a very narrow part of the search space, so the random dart throw may have very low probability of a reasonable price.\n\nAs a concrete proposal, I would suggest:\n\n* Suppose you want to initialize a Hill Climbing algorithm for all your channels to a peer A.\n* Look at all the *other* peers of peer A and look at their LN feerates towards the peer A.\n* Get the **weighted median** of all the other peers of peer A.\n  * Use the channel size of that node to the peer A as weight.\n* Use the weighted median as the initial starting point for the Hill Climbing algorithm.\n\nWhy weighted median?\n\n* Median is more robust against outliers.\n  * For example, someone who knows you are using Hill Climbing might decide to break your algo by creating a channel with ridiculous feerates, in order to manipulate the mean.\n    With median, that channel is only one data point, and the manipulator needs to make many such channels.\n* Weighting by channel size makes attempts at manipulating your algorithm costly, by requiring manipulators to tie up funds into a channel with ridiculous (and probably very unlucrative) feerates.\n\nCLBOSS\n======\n\nCLBOSS implements a variant of the above since release 0.11B.\nThere is even a test of the implementation, which uses a simple model for price theory.\n\nHowever, there are substantial differences to the above described algorithm:\n\n* In reality, there are two variables that are input to your fee earnings: base fee and proportional fee.\n  * CLBOSS uses a multiplier on *both* base and proportional fee instead of optimizing both variables as separate axes of a Hill Climibing model.\n* The above proposal suggests the weighted-median-of-competitor-feerates as a *starting point*.\n  * CLBOSS uses a single-dimensional multiplier (as mentioned above) that multiplies the *current* weighted median.\n* CLBOSS also includes another algorithm, passive rebalancing, which affects the feerates.\n  * Basically, CLBOSS changes the feerates according to the level of funds we have in a channel.\n  * If we own almost all funds in the channel, we drastically lower the fees we charge.\n  * If we own almost none of the funds in the channel, we drastically increase we charge.\n\nConclusion\n==========\n\nIn principle, a truly sapient being would still defeat any pre-sentient algorithm.\nFor example, the sapient being can take the output of a pre-sentient algorithm, take a long time to pour over it and study the entire problem, and make a single change that improves on the output of the pre-sentiant algorithm.\nAt the worst case, the sapient being can discover that no improvement is possible, and simply outputs the result of the pre-sentient algorithm as its own output, as-is.\n\nHowever, the power of humanity increases as more decisions can be left to policies and similar pre-sentient algorithms.\nThis allows humanity to utilize its limited willpower and lifespan towards *other* decisions that pre-sentient algorithms cannot handle.\n\nAn example I like to bring out here is compilers.\nIn the past, a good assembler programmer could beat the best compilers hands down.\nEventually, it became common for an assembler programmer to look at the compiler output, look over it, and make small microoptimizations that improved over the compiler output.\nEventually, such work became less economically justifiable, as compilers have improved such that the probability of a sapient assembler programmer discovering an unutilized optimization has drastically lowered.\n\nEven if in the future, humanity is replaced by much more rational beings, the same relationship between general sapience and pre-sentient algorithms should still hold.\nThus, it seems to me that moving this decision to a pre-sentient algorithm and freeing up higher-order sapience to other concerns may still be beneficial.\n\n(Just to be clear, I am not trying to overthrow the human race yet.)\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Algorithm For Channel Fee Settings",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "ZmnSCPxj"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 10734
        }
    },
    {
        "title": "[Lightning-dev] Zero Fee Routing",
        "thread_messages": [
            {
                "author": "Daki Carnhof",
                "date": "2021-08-14T16:30:33",
                "message_text_only": "This text was brought to life by repelling magnet forces of reading\nZmnSCPxj's Algorithm For Channel Fee Settings. Thank you, Zmn! Bitcoin has\ntaught me to just do nothing and it would probably be preferable in this\ncase as well, but here is my 5msat.\n\n# Introduction\n\nZero Fee Lightning Network Routing (both 0/0) is an alternative way of\nlooking at channel fee settings. The philosophical idea behind it is based\non the fact that by encouraging individuals to do their own (automated) fee\nmanagement we are actually forcing everyone individually to do the\ndecisions which are are currently done in fiat (and altcoin) standard\nthrough interventions. And which we oppose by Bitcoin.\n\n# Reasoning\n\nSee discussion with Jordan P. Peterson and guests at\nhttps://www.youtube.com/embed/iVym9wtopqs\n\nOur computers are running anyway for the Bitcoin nodes. They do quite a lot\nof  simple computations for free* already. If it did not make sense, the\nnodes would not be run. The same reasoning extends to zero fee routing on\nLN. In our opinion it is much more straightforward to not ask for fees on\nLN. The higher interest of the community will just melt into the overall\nprice, like Proof-of-Work does. Using an algorithm like Hill Climbing would\njust make our computers compute even more and with uncertain results.\n\n* Without any immediate effect on the amount of satoshi the operator of the\nnode owns.\n\nThat's it. If I Had More Time, I Would Have Written a Shorter Letter.\n\nRegards,\nDaki\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210814/64dcaf9a/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-08-14T23:41:12",
                "message_text_only": "Good morning Daki,\n\nWhile certainly a very imaginative approach to the problem, do note that there is a substantive difference between running a Bitcoin fullnode and running a Lightning forwarding node.\n\nNamely:\n\n* A Bitcoin fullnode does not risk any lockup of its funds just to run.\n* A Lightning forwarding node *does* risk having its funds locked and unavailable.\n\nWhile a payment is being forwarded, the funds involved in the forwarding are unavailable for use by the putative owner of the funds.\nInstead, the funds are kept in an HTLC until the payment forwarding is resolved in either success or failure.\n\nHaving your funds locked and unavailable to you, even transiently, is only tenable if you get something in return, e.g. a return on investment.\n\nOf course, you can also counter-argue that in practice, the amounts and timeframes are so short that any return on investment would be ridiculously minuscule, which is why in practice most forwarding nodes will earn 0 or even negative net income.\nOn the other hand, larger hubs with significant liquidity invested into them would still have total amounts and timeframes that *are* substantial enough that it would make sense for them to charge *some* fee.\nAnd discovering that feerate is the point of this exercise.\n\nOn the other other hand, this may very well be \"trade secret\" territory, in which case there is no point in me asking about this topic anyway.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Andr\u00e9s G. Aragoneses",
                "date": "2021-08-23T11:48:13",
                "message_text_only": "How is this related to the youtube video posted? Can you link to a specific\ntime in it where it is discussed?\n\nOn Sun, 15 Aug 2021 at 00:30, Daki Carnhof <carnhofdaki at gmail.com> wrote:\n\n> This text was brought to life by repelling magnet forces of reading\n> ZmnSCPxj's Algorithm For Channel Fee Settings. Thank you, Zmn! Bitcoin\n> has taught me to just do nothing and it would probably be preferable in\n> this case as well, but here is my 5msat.\n>\n> # Introduction\n>\n> Zero Fee Lightning Network Routing (both 0/0) is an alternative way of\n> looking at channel fee settings. The philosophical idea behind it is based\n> on the fact that by encouraging individuals to do their own (automated) fee\n> management we are actually forcing everyone individually to do the\n> decisions which are are currently done in fiat (and altcoin) standard\n> through interventions. And which we oppose by Bitcoin.\n>\n> # Reasoning\n>\n> See discussion with Jordan P. Peterson and guests at\n> https://www.youtube.com/embed/iVym9wtopqs\n>\n> Our computers are running anyway for the Bitcoin nodes. They do quite a\n> lot of  simple computations for free* already. If it did not make sense,\n> the nodes would not be run. The same reasoning extends to zero fee routing\n> on LN. In our opinion it is much more straightforward to not ask for fees\n> on LN. The higher interest of the community will just melt into the overall\n> price, like Proof-of-Work does. Using an algorithm like Hill Climbing would\n> just make our computers compute even more and with uncertain results.\n>\n> * Without any immediate effect on the amount of satoshi the operator of\n> the node owns.\n>\n> That's it. If I Had More Time, I Would Have Written a Shorter Letter.\n>\n> Regards,\n> Daki\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210823/2db7b37d/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Zero Fee Routing",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Andr\u00e9s G. Aragoneses",
                "ZmnSCPxj",
                "Daki Carnhof"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 5212
        }
    },
    {
        "title": "[Lightning-dev] #zerobasefee",
        "thread_messages": [
            {
                "author": "Anthony Towns",
                "date": "2021-08-15T01:00:37",
                "message_text_only": "Hey *,\n\nThere's been discussions on twitter and elsewhere advocating for\nsetting the BOLT#7 fee_base_msat value [0] to zero. I'm just writing\nthis to summarise my understanding in a place that's able to easily be\nreferenced later.\n\nSetting the base fee to zero has a couple of benefits:\n\n - it means you only have one value to optimise when trying to collect\n   the most fees, and one-dimensional optimisation problems are\n   obviously easier to write code for than two-dimensional optimisation\n   problems\n\n - when finding a route, if all the fees on all the channels are\n   proportional only, you'll never have to worry about paying more fees\n   just as a result of splitting a payment; that makes routing easier\n   (see [1])\n\nSo what's the cost? The cost is that there's no longer a fixed minimum\nfee -- so if you try sending a 1sat payment you'll pay 0.1% of the fee\nto send a 1000sat payment, and there may be fixed costs that you have\nin routing payments that you'd like to be compensated for (eg, the\ncomputational work to update channel state, the bandwith to forward the\ntx, or the opportunity cost for not being able to accept another htlc if\nyou've hit your max htlcs per channel limit).\n\nBut there's no need to explicitly separate those costs the way we do\nnow; instead of charging 1sat base fee and 0.02% proportional fee,\nyou can instead just set the 0.02% proportional fee and have a minimum\npayment size of 5000 sats (htlc_minimum_msat=5e6, ~$2), since 0.02%\nof that is 1sat. Nobody will be asking you to route without offering a\nfee of at least 1sat, but all the optimisation steps are easier.\n\nYou could go a step further, and have the node side accept smaller\npayments despite the htlc minimum setting: eg, accept a 3000 sat payment\nprovided it pays the same fee that a 5000 sat payment would have. That is,\ntreat the setting as minimum_fee=1sat, rather than minimum_amount=5000sat;\nso the advertised value is just calculated from the real settings,\nand that nodes that want to send very small values despite having to\npay high rates can just invert the calculation.\n\nI think something like this approach also makes sense when your channel\nbecomes overloaded; eg if you have x HTLC slots available, and y channel\ncapacity available, setting a minimum payment size of something like\ny/2/x**2 allows you to accept small payments (good for the network)\nwhen you're channel is not busy, but reserves the last slots for larger\npayments so that you don't end up missing out on profits because you\nran out of capacity due to low value spam.\n\nTwo other aspects related to this:\n\nAt present, I think all the fixed costs are also incurred even when\na htlc fails, so until we have some way of charging failing txs for\nincurring those costs, it seems a bit backwards to penalise successful\ntxs who at least pay a proportional fee for the same thing. Until we've\ngot a way of handling that, having zero base fee seems at least fair.\n\nLower value HTLCs don't need to be included in the commitment transaction\n(if they're below the dust level, they definitely shouldn't be included,\nand if they're less than 1sat they can't be included), and as such don't\nincur all the same fixed costs that HTLCs that are committed too do.\nHaving different base fees for microtransactions that incur fewer costs\nwould be annoying; so having that be \"amortised\" into the proportional\nfee might help there too.\n\nI think eltoo can help in two ways by reducing the fixed costs: you no\nlonger need to keep HTLC information around permanently, and if you do\na multilevel channel factory setup, you can probably remove the ~400\nHTLCs per channel at any one time limit. But there's still other fixed\ncosts, so I think that would just lower the fixed costs, not remove them\naltogether and isn't a fundamental change.\n\nI think the fixed costs for forwarding a HTLC are very small; something\nlike:\n\n   0.02sats -- cost of permanently storing the HTLC info\n               (100 bytes, $500/TB/year, 1% discount rate)\n   0.04sats -- compute and bandwidth cost for updating an HTLC ($40/month\n               at linode, 1 second of compute)\n\nThe opportunity cost of having HTLC slots or Bitcoin locked up until\nthe HTLC succeeds/fails could be much more significant, though.\n\nCheers,\naj\n\n[0] https://github.com/lightningnetwork/lightning-rfc/blob/master/07-routing-gossip.md#the-channel_update-message\n[1] https://basefee.ln.rene-pickhardt.de/"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-08-15T10:58:35",
                "message_text_only": "Good morning aj, et al.\n\n> Hey *,\n>\n> There's been discussions on twitter and elsewhere advocating for\n> setting the BOLT#7 fee_base_msat value [0] to zero. I'm just writing\n> this to summarise my understanding in a place that's able to easily be\n> referenced later.\n>\n> Setting the base fee to zero has a couple of benefits:\n>\n> -   it means you only have one value to optimise when trying to collect\n>     the most fees, and one-dimensional optimisation problems are\n>     obviously easier to write code for than two-dimensional optimisation\n>     problems\n\nIndeed, this is a good point regarding this.\n\n\n> -   when finding a route, if all the fees on all the channels are\n>     proportional only, you'll never have to worry about paying more fees\n>     just as a result of splitting a payment; that makes routing easier\n>     (see [1])\n\nIf we neglect roundoff errors.\n\nOn the other hand, roundoff errors involved are <1msat per split, so it probably will not matter to most people.\n\n>     So what's the cost? The cost is that there's no longer a fixed minimum\n>     fee -- so if you try sending a 1sat payment you'll pay 0.1% of the fee\n>     to send a 1000sat payment, and there may be fixed costs that you have\n>     in routing payments that you'd like to be compensated for (eg, the\n>     computational work to update channel state, the bandwith to forward the\n>     tx, or the opportunity cost for not being able to accept another htlc if\n>     you've hit your max htlcs per channel limit).\n>\n>     But there's no need to explicitly separate those costs the way we do\n>     now; instead of charging 1sat base fee and 0.02% proportional fee,\n>     you can instead just set the 0.02% proportional fee and have a minimum\n>     payment size of 5000 sats (htlc_minimum_msat=5e6, ~$2), since 0.02%\n>     of that is 1sat. Nobody will be asking you to route without offering a\n>     fee of at least 1sat, but all the optimisation steps are easier.\n\nShould this minimum a node will be willing to forward be part of gossip, and how does this affect routing algorithms?\n\n>     You could go a step further, and have the node side accept smaller\n>     payments despite the htlc minimum setting: eg, accept a 3000 sat payment\n>     provided it pays the same fee that a 5000 sat payment would have. That is,\n>     treat the setting as minimum_fee=1sat, rather than minimum_amount=5000sat;\n>     so the advertised value is just calculated from the real settings,\n>     and that nodes that want to send very small values despite having to\n>     pay high rates can just invert the calculation.\n\nI like this idea, as I think it matches more what the incentives are.\nBut it requires a change in gossip and in routing algorithms, and more importantly it requires routing algorithms to support two different fee schemes (base + proportional vs min + proportional).\n\nOn the other hand, this still is a two-dimensional optimization algorithm, with `minimum_fee` and `proportional_fee_millionths` as the two dimensions.\nSo maybe just have a single proportional-fee mechanism...\n\n>\n>     I think something like this approach also makes sense when your channel\n>     becomes overloaded; eg if you have x HTLC slots available, and y channel\n>     capacity available, setting a minimum payment size of something like\n>     y/2/x**2 allows you to accept small payments (good for the network)\n>     when you're channel is not busy, but reserves the last slots for larger\n>     payments so that you don't end up missing out on profits because you\n>     ran out of capacity due to low value spam.\n>\n>     Two other aspects related to this:\n>\n>     At present, I think all the fixed costs are also incurred even when\n>     a htlc fails, so until we have some way of charging failing txs for\n>     incurring those costs, it seems a bit backwards to penalise successful\n>     txs who at least pay a proportional fee for the same thing. Until we've\n>     got a way of handling that, having zero base fee seems at least fair.\n\nYes, the dreaded mechanism against payment lockup, which as far as I understand has a lot of thought already sunk into it without any widely-accepted solution, sigh.\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "lisa neigut",
                "date": "2021-08-15T12:19:01",
                "message_text_only": "The field of economics has done much work over the past few decades\ndemonstrating that \u201cFree\u201d is problematic in practice because humans will go\nout of their way to externalize costs elsewhere (e.g. time, in the case of\nlightning), given the promise of freedom. In other words, actors often act\nirrationally to get a free deal.\n\nAs protocol designers, it would be remiss to ignore this (repeatedly\ndemonstrated) truth.\n\nTo avoid this, we\u2019ve been suggesting setting a min_htlc_value requirement.\nThe problem with zbf + a min_htlc size requirement is that it makes tiny\npayments impossible over lightning, which was one of the original design\ngoals of the system, and is an important feature to keep/support as\nlightning grows into poorer economic bases and the bitcoin market price\ncontinues to rise.\n\nMy suggestion would be that, as a compromise, we set a network wide minimum\nfee at the protocol level of 1msat. Naively, this seems it should be easy\nto add to calculations using single-dimension optimization (or trivial\nenough to ignore entirely), it removes the \u201cfree lunch\u201d irrationality\nhoneypot zbf opens, and it provides a way forward for the continued use of\nmicropayments.\n\nThe result is that micropayments have a different payment regime than\n\u201cnon-micropayments\u201d, (which may still incentive almost irrational behavior)\nbut at least there\u2019s no *loss* felt by node operators for\nhandling/supporting low value payments. 10k micropayments is worth 10sats.\n\nIt\u2019s also simple to implement and seems rather obvious in retrospect.\n\nThe only confounding future change that I can see us making would be the\nintroduction of negative fees, which are useful as a way to induce payments\nto rebalance channels passively. This seems like something we can revisit\nonce a proposal for negative fees is being seriously considered, however.\n\nOn Sun, Aug 15, 2021 at 05:59 ZmnSCPxj via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> wrote:\n\n> Good morning aj, et al.\n>\n> > Hey *,\n> >\n> > There's been discussions on twitter and elsewhere advocating for\n> > setting the BOLT#7 fee_base_msat value [0] to zero. I'm just writing\n> > this to summarise my understanding in a place that's able to easily be\n> > referenced later.\n> >\n> > Setting the base fee to zero has a couple of benefits:\n> >\n> > -   it means you only have one value to optimise when trying to collect\n> >     the most fees, and one-dimensional optimisation problems are\n> >     obviously easier to write code for than two-dimensional optimisation\n> >     problems\n>\n> Indeed, this is a good point regarding this.\n>\n>\n> > -   when finding a route, if all the fees on all the channels are\n> >     proportional only, you'll never have to worry about paying more fees\n> >     just as a result of splitting a payment; that makes routing easier\n> >     (see [1])\n>\n> If we neglect roundoff errors.\n>\n> On the other hand, roundoff errors involved are <1msat per split, so it\n> probably will not matter to most people.\n>\n> >     So what's the cost? The cost is that there's no longer a fixed\n> minimum\n> >     fee -- so if you try sending a 1sat payment you'll pay 0.1% of the\n> fee\n> >     to send a 1000sat payment, and there may be fixed costs that you have\n> >     in routing payments that you'd like to be compensated for (eg, the\n> >     computational work to update channel state, the bandwith to forward\n> the\n> >     tx, or the opportunity cost for not being able to accept another\n> htlc if\n> >     you've hit your max htlcs per channel limit).\n> >\n> >     But there's no need to explicitly separate those costs the way we do\n> >     now; instead of charging 1sat base fee and 0.02% proportional fee,\n> >     you can instead just set the 0.02% proportional fee and have a\n> minimum\n> >     payment size of 5000 sats (htlc_minimum_msat=5e6, ~$2), since 0.02%\n> >     of that is 1sat. Nobody will be asking you to route without offering\n> a\n> >     fee of at least 1sat, but all the optimisation steps are easier.\n>\n> Should this minimum a node will be willing to forward be part of gossip,\n> and how does this affect routing algorithms?\n>\n> >     You could go a step further, and have the node side accept smaller\n> >     payments despite the htlc minimum setting: eg, accept a 3000 sat\n> payment\n> >     provided it pays the same fee that a 5000 sat payment would have.\n> That is,\n> >     treat the setting as minimum_fee=1sat, rather than\n> minimum_amount=5000sat;\n> >     so the advertised value is just calculated from the real settings,\n> >     and that nodes that want to send very small values despite having to\n> >     pay high rates can just invert the calculation.\n>\n> I like this idea, as I think it matches more what the incentives are.\n> But it requires a change in gossip and in routing algorithms, and more\n> importantly it requires routing algorithms to support two different fee\n> schemes (base + proportional vs min + proportional).\n>\n> On the other hand, this still is a two-dimensional optimization algorithm,\n> with `minimum_fee` and `proportional_fee_millionths` as the two dimensions.\n> So maybe just have a single proportional-fee mechanism...\n>\n> >\n> >     I think something like this approach also makes sense when your\n> channel\n> >     becomes overloaded; eg if you have x HTLC slots available, and y\n> channel\n> >     capacity available, setting a minimum payment size of something like\n> >     y/2/x**2 allows you to accept small payments (good for the network)\n> >     when you're channel is not busy, but reserves the last slots for\n> larger\n> >     payments so that you don't end up missing out on profits because you\n> >     ran out of capacity due to low value spam.\n> >\n> >     Two other aspects related to this:\n> >\n> >     At present, I think all the fixed costs are also incurred even when\n> >     a htlc fails, so until we have some way of charging failing txs for\n> >     incurring those costs, it seems a bit backwards to penalise\n> successful\n> >     txs who at least pay a proportional fee for the same thing. Until\n> we've\n> >     got a way of handling that, having zero base fee seems at least fair.\n>\n> Yes, the dreaded mechanism against payment lockup, which as far as I\n> understand has a lot of thought already sunk into it without any\n> widely-accepted solution, sigh.\n>\n>\n> Regards,\n> ZmnSCPxj\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210815/cb1c33c6/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-08-15T16:03:59",
                "message_text_only": "Good morning lisa, aj, et al.,\n\n\n> The result is that micropayments have a different payment regime than \u201cnon-micropayments\u201d, (which may still incentive almost irrational behavior) but at least there\u2019s no *loss* felt by node operators for handling/supporting low value payments. 10k micropayments is worth 10sats.\n>\n> It\u2019s also simple to implement and seems rather obvious in retrospect.\n\n\nIt seems simple to implement for *forwarders*, but I think complicates the algorithm described by Pickhardt and Richter?\n\nOn the other hand, the algorithm is targeted towards \"large\" payments, so perhaps the Pickhardt-Richter payment algo can be forced to have some minimum split size, and payments below this minimum size are just sent as single payments (on the assumption that such micropayments are so small that the probability of failure is negligible).\nThat is, just have the `pay` command branch based on the payment size, if it is below the minimum size, just use the old try-and-try-until-you-die algo, otherwise use a variant on the Pickhardt-Richter algo that respects this minimum payment size.\nThis somewhat implies a minimum on the possible feerate, which we could say is 1 ppm, maybe.\n\nSo for example, the minimum size could be 1,000,000msat, or 1,000sat.\nIf the payment is much larger than that, use the Pickhardt-Richter algorithm with zerobasefee.\nIf payment is lower than that threshold, just do not split and do try-and-try-until-you-die.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Stefan Richter",
                "date": "2021-08-15T22:10:06",
                "message_text_only": "Good morning Zmn!\n\nThat is indeed precisely what we do. We usually quantize the min-cost flow\ninto minimum shares of, say, 10kSat to 100kSat. This makes the algorithm\nrun faster and loses very little precision. It also gives a simple way of\ndealing with (reasonable) min-htlc-size values.\n\nCheers,\n   Stefan\n\nZmnSCPxj via Lightning-dev <lightning-dev at lists.linuxfoundation.org>\nschrieb am So., 15. Aug. 2021, 16:03:\n\n> Good morning lisa, aj, et al.,\n>\n>\n> > The result is that micropayments have a different payment regime than\n> \u201cnon-micropayments\u201d, (which may still incentive almost irrational behavior)\n> but at least there\u2019s no *loss* felt by node operators for\n> handling/supporting low value payments. 10k micropayments is worth 10sats.\n> >\n> > It\u2019s also simple to implement and seems rather obvious in retrospect.\n>\n>\n> It seems simple to implement for *forwarders*, but I think complicates the\n> algorithm described by Pickhardt and Richter?\n>\n> On the other hand, the algorithm is targeted towards \"large\" payments, so\n> perhaps the Pickhardt-Richter payment algo can be forced to have some\n> minimum split size, and payments below this minimum size are just sent as\n> single payments (on the assumption that such micropayments are so small\n> that the probability of failure is negligible).\n> That is, just have the `pay` command branch based on the payment size, if\n> it is below the minimum size, just use the old try-and-try-until-you-die\n> algo, otherwise use a variant on the Pickhardt-Richter algo that respects\n> this minimum payment size.\n> This somewhat implies a minimum on the possible feerate, which we could\n> say is 1 ppm, maybe.\n>\n> So for example, the minimum size could be 1,000,000msat, or 1,000sat.\n> If the payment is much larger than that, use the Pickhardt-Richter\n> algorithm with zerobasefee.\n> If payment is lower than that threshold, just do not split and do\n> try-and-try-until-you-die.\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210815/df395dd3/attachment-0001.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2021-08-16T00:04:52",
                "message_text_only": "Thanks, AJ, for kicking off the thread.\n\nI'm frankly still very confused why we're having these conversations now. In one particular class of applicable routing \nalgorithms you could use for lightning routing having a base fee makes the algorithm intractably slow, but:\n\na) to my knowledge, no one has (yet) done any follow-on work to investigate pulling many of the same heuristics Rene et \nal use in a Dijkstras/A* algorithm with multiple passes or generating multiple routes in the same pass to see whether \nyou can emulate the results in a faster algorithm without the drawbacks here,\n\nb) to my knowledge, no one has (yet) done any follow-on work to investigate mapping the base fee to other, more \nflow-based-routing-compatible numbers, eg you could convert the base fee to a minimum fee by increasing the \"effective\" \nproportional fees. From what others have commented, this may largely \"solve\" the issue.\n\nc) to my knowledge, no one has (yet) done any follow-on work to analyze where the proposed algorithm may be most optimal \nin the HTLC-value<->channel liquidity ratio ranges. We may find that the proposed algorithm only provides materially \nbetter routing when the HTLC value approaches X% of common network channel liquidity, allowing us to only use it for \nlarge-value payments where we can almost ignore the base fees entirely.\n\nThere's real cost to distorting the fee structures on the network away from the costs of node operators, especially as \nwe move towards requiring and using Real (tm) amounts of capital on routing nodes. If we're relying purely on hobbyists \nforever who are operating out of goodwill, we should just remove all fees. If we think Lightning is going to involve \ncapital with real opportunity cost, matching fees to the costs is important, or at least important enough that we \nshouldn't throw it away after one (pretty great) paper and limited further analysis.\n\nImagine we find some great way to address HTLC slot flooding/DoS attacks (or just chose to do it in a not-great way) by \ncharging for HTLC slot usage, now we can't fix a critical DoS issue because the routing algorithms we deployed can't \nhandle the new costing. Instead, we should investigate how we can apply the ideas here with the more complicated fee \nstructures we have.\n\nColor me an optimist, but I'm quite confident with sufficient elbow grease and heuristics we can get 95% of the way \nthere. We can and should revisit these conversations if such exploration is done and we find that its not possible, but \nuntil then this all feels incredibly premature.\n\nMatt\n\nOn 8/14/21 21:00, Anthony Towns wrote:\n> Hey *,\n> \n> There's been discussions on twitter and elsewhere advocating for\n> setting the BOLT#7 fee_base_msat value [0] to zero. I'm just writing\n> this to summarise my understanding in a place that's able to easily be\n> referenced later.\n> \n> Setting the base fee to zero has a couple of benefits:\n> \n>   - it means you only have one value to optimise when trying to collect\n>     the most fees, and one-dimensional optimisation problems are\n>     obviously easier to write code for than two-dimensional optimisation\n>     problems\n> \n>   - when finding a route, if all the fees on all the channels are\n>     proportional only, you'll never have to worry about paying more fees\n>     just as a result of splitting a payment; that makes routing easier\n>     (see [1])\n> \n> So what's the cost? The cost is that there's no longer a fixed minimum\n> fee -- so if you try sending a 1sat payment you'll pay 0.1% of the fee\n> to send a 1000sat payment, and there may be fixed costs that you have\n> in routing payments that you'd like to be compensated for (eg, the\n> computational work to update channel state, the bandwith to forward the\n> tx, or the opportunity cost for not being able to accept another htlc if\n> you've hit your max htlcs per channel limit).\n> \n> But there's no need to explicitly separate those costs the way we do\n> now; instead of charging 1sat base fee and 0.02% proportional fee,\n> you can instead just set the 0.02% proportional fee and have a minimum\n> payment size of 5000 sats (htlc_minimum_msat=5e6, ~$2), since 0.02%\n> of that is 1sat. Nobody will be asking you to route without offering a\n> fee of at least 1sat, but all the optimisation steps are easier.\n> \n> You could go a step further, and have the node side accept smaller\n> payments despite the htlc minimum setting: eg, accept a 3000 sat payment\n> provided it pays the same fee that a 5000 sat payment would have. That is,\n> treat the setting as minimum_fee=1sat, rather than minimum_amount=5000sat;\n> so the advertised value is just calculated from the real settings,\n> and that nodes that want to send very small values despite having to\n> pay high rates can just invert the calculation.\n> \n> I think something like this approach also makes sense when your channel\n> becomes overloaded; eg if you have x HTLC slots available, and y channel\n> capacity available, setting a minimum payment size of something like\n> y/2/x**2 allows you to accept small payments (good for the network)\n> when you're channel is not busy, but reserves the last slots for larger\n> payments so that you don't end up missing out on profits because you\n> ran out of capacity due to low value spam.\n> \n> Two other aspects related to this:\n> \n> At present, I think all the fixed costs are also incurred even when\n> a htlc fails, so until we have some way of charging failing txs for\n> incurring those costs, it seems a bit backwards to penalise successful\n> txs who at least pay a proportional fee for the same thing. Until we've\n> got a way of handling that, having zero base fee seems at least fair.\n> \n> Lower value HTLCs don't need to be included in the commitment transaction\n> (if they're below the dust level, they definitely shouldn't be included,\n> and if they're less than 1sat they can't be included), and as such don't\n> incur all the same fixed costs that HTLCs that are committed too do.\n> Having different base fees for microtransactions that incur fewer costs\n> would be annoying; so having that be \"amortised\" into the proportional\n> fee might help there too.\n> \n> I think eltoo can help in two ways by reducing the fixed costs: you no\n> longer need to keep HTLC information around permanently, and if you do\n> a multilevel channel factory setup, you can probably remove the ~400\n> HTLCs per channel at any one time limit. But there's still other fixed\n> costs, so I think that would just lower the fixed costs, not remove them\n> altogether and isn't a fundamental change.\n> \n> I think the fixed costs for forwarding a HTLC are very small; something\n> like:\n> \n>     0.02sats -- cost of permanently storing the HTLC info\n>                 (100 bytes, $500/TB/year, 1% discount rate)\n>     0.04sats -- compute and bandwidth cost for updating an HTLC ($40/month\n>                 at linode, 1 second of compute)\n> \n> The opportunity cost of having HTLC slots or Bitcoin locked up until\n> the HTLC succeeds/fails could be much more significant, though.\n> \n> Cheers,\n> aj\n> \n> [0] https://github.com/lightningnetwork/lightning-rfc/blob/master/07-routing-gossip.md#the-channel_update-message\n> [1] https://basefee.ln.rene-pickhardt.de/\n> \n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>"
            },
            {
                "author": "Anthony Towns",
                "date": "2021-08-16T02:02:24",
                "message_text_only": "On Sun, Aug 15, 2021 at 07:19:01AM -0500, lisa neigut wrote:\n> My suggestion would be that, as a compromise, we set a network wide minimum fee\n> at the protocol level of 1msat.\n\nIs that different in any meaningful way to just saying \"fees get rounded\nup to the nearest msat\" ? If the fee is 999.999msat, expecting to get\naway with paying less than 1sat seems kinda buggy to me.\n\nOn Sun, Aug 15, 2021 at 08:04:52PM -0400, Matt Corallo wrote:\n> I'm frankly still very confused why we're having these conversations now.\n\nBecause it's what people are thinking about. The bar for having a\nconversation about something is very low...\n\n> In\n> one particular class of applicable routing algorithms you could use for\n> lightning routing having a base fee makes the algorithm intractably slow,\n\nI don't think of that as the problem, but rather as the base fee having\na multiplicative effect as you split payments.\n\nIf every channel has the same (base,proportional) fee pair, and send a\npayment along a single path, you're paying n*(base+k*proportional). If\nyou split the payment, and send half of it one way, and half the other\nway, you're paying n*(2*base+k*proportional). If you split the payment\nfour ways, you're paying n*(4*base+k*proportional). Where's the value\nto the network in penalising payment splitting?\n\nBeing denominated in sats, the base fee also changes in value as the\nbitcoin price changes -- c-lightning dropped the base fee to 1sat (from\n546 sat!) in Jan 2018, but the value of 1sat has increased about 4x\nsince then, and it seems unlikely the fixed costs of a successful HTLC\npayment have likewise increased 4x.  Proportional fees deal with this\nfactor automatically, of course.\n\n> There's real cost to distorting the fee structures on the network away from\n> the costs of node operators, \n\nThat's precisely what the base fee is already doing. Yes, we need some\nother way of charging fees to prevent using up too many slots or having\ntransactions not fail in a timely manner, but the base fee does not\ndo that.\n\n> Imagine we find some great way to address HTLC slot flooding/DoS attacks (or\n> just chose to do it in a not-great way) by charging for HTLC slot usage, now\n> we can't fix a critical DoS issue because the routing algorithms we deployed\n> can't handle the new costing.\n\nI don't think that's true. The two things we don't charge for that can\nbe abused by probing spam are HTLC slot usage and channel balance usage;\nboth are problems only in proportion to the amount of time they're held\nopen, and the latter is also only a problem proportional to the value\nbeing reserved. [0]\n\nAdditionally, I don't think HTLC slot usage needs to be kept as a\nlimitation after we switch to eltoo; and in the meantime, I think it can\nbe better managed via adjusting the min_htlc_amount -- at least for the\nscenario where problems are being caused by legitimate payment attempts,\nwhich is also the only place base fee can help.\n\n[0] (Well, ln-penalty's requirement to permanently store HTLC information\n     in order to apply the penalty is in some sense a constant\n     cost, however the impact is also proportional to value, and for\n     sufficiently low value HTLCs can be ignored entirely if the HTLC\n     isn't included in the channel commitment)\n\n> Instead, we should investigate how we can\n> apply the ideas here with the more complicated fee structures we have.\n\nFee structures should be *simple* not complicated.\n\nI mean, it's kind of great that we started off complicated -- if it\nturns out base fee isn't necessary, it's easy to just set it to zero;\nif we didn't have it, but needed it, it would be much more annoying to\nadd it in later.\n\n> Color me an optimist, but I'm quite confident with sufficient elbow grease\n> and heuristics we can get 95% of the way there. We can and should revisit\n> these conversations if such exploration is done and we find that its not\n> possible, but until then this all feels incredibly premature.\n\nDepends; I don't think it makes sense to try to ban nodes that don't have\na base fee of zero or anything, but random people on twitter advocating\nthat node operators should set it to zero and just worry about optimising\nvia the proportional fee and the min htlc amount seems fine.\n\nFor an experimental plugin that aggressively splits payments up, I think\neither ignoring channels with >0 base fee entirely, or deciding that\nyou're happy to spend a total of X sats on base fees, and then ignoring\nchannels whose base fee is greater than X/paths/path-length sats is fine.\n\nBut long term, I also think that the base fee is an entirely unhelpful\ncomplication that will eventually just be hardcoded to zero by everyone,\nand eventually channels that propose non-zero base fees won't even be\ngossiped. I don't expect that to happen any time soon though.\n\nCheers,\naj"
            },
            {
                "author": "Matt Corallo",
                "date": "2021-08-16T02:21:52",
                "message_text_only": "On 8/15/21 22:02, Anthony Towns wrote:\n>> In\n>> one particular class of applicable routing algorithms you could use for\n>> lightning routing having a base fee makes the algorithm intractably slow,\n> \n> I don't think of that as the problem, but rather as the base fee having\n> a multiplicative effect as you split payments.\n\nYes, matching the real-world costs of forwarding an HTLC.\n\n> If every channel has the same (base,proportional) fee pair, and send a\n> payment along a single path, you're paying n*(base+k*proportional). If\n> you split the payment, and send half of it one way, and half the other\n> way, you're paying n*(2*base+k*proportional). If you split the payment\n> four ways, you're paying n*(4*base+k*proportional). Where's the value\n> to the network in penalising payment splitting?\n\nYes. You have to pay the cost of a node. If we're really worried about this, we should be talking about upfront fees \nand/or refunds on HTLC fulfillment, not removing the fees entirely.\n\n> Being denominated in sats, the base fee also changes in value as the\n> bitcoin price changes -- c-lightning dropped the base fee to 1sat (from\n> 546 sat!) in Jan 2018, but the value of 1sat has increased about 4x\n> since then, and it seems unlikely the fixed costs of a successful HTLC\n> payment have likewise increased 4x.  Proportional fees deal with this\n> factor automatically, of course.\n\nThis isn't a protocol issue, implementations can automate this without issue.\n\n>> There's real cost to distorting the fee structures on the network away from\n>> the costs of node operators,\n> \n> That's precisely what the base fee is already doing. Yes, we need some\n> other way of charging fees to prevent using up too many slots or having\n> transactions not fail in a timely manner, but the base fee does not\n> do that.\n\nHuh? For values much smaller than a node's liquidity, the cost for nodes is (mostly) a function of HTLCs, not the value. \nThe cost to nodes is largely (a) the forever-storage that exists, roughly, per HTLC ever on a channel, (b) the HTLC \nslots which are highly limited for technical reasons per channel, (c) the disk/cpu/network/etc operations per HTLC on an \nchannel, (d) the liquidity required per node. I'd argue (c) is basically zero in any realistic context, (a) is pretty \nlow, but could be non-zero in some cases, so you really just have (b) and (d). For many HTLCs forwarded today, the \nliquidity on a channel isn't much, so I'd argue for many HTLCs forwarded today per-payment costs mirror the cost to a \nnode much, much, much, much better than some proportional fees?\n\nI'm really not sure where you're coming from here.\n\n>> Imagine we find some great way to address HTLC slot flooding/DoS attacks (or\n>> just chose to do it in a not-great way) by charging for HTLC slot usage, now\n>> we can't fix a critical DoS issue because the routing algorithms we deployed\n>> can't handle the new costing.\n> \n> I don't think that's true. The two things we don't charge for that can\n> be abused by probing spam are HTLC slot usage and channel balance usage;\n> both are problems only in proportion to the amount of time they're held\n> open, and the latter is also only a problem proportional to the value\n> being reserved. [0]\n> \n> Additionally, I don't think HTLC slot usage needs to be kept as a\n> limitation after we switch to eltoo;\n\nThe HTLC slot limit is to keep transactions broadcastable. I don't see why this would change, you still get an output \nfor each HTLC on the latest commitment in eltoo, AFAIU.\n\n> and in the meantime, I think it can\n> be better managed via adjusting the min_htlc_amount -- at least for the\n> scenario where problems are being caused by legitimate payment attempts,\n> which is also the only place base fee can help.\n\nSure, we could also shift towards upfront fees or similar solutions, though, and that was my point - if we start \ndropping absolute fee amounts now in order to make some given routing algorithm work, we box ourselves in here, and \nquite needlessly given no one has (yet) done the legwork to show that we even *need* to box ourselves in.\n\n> [0] (Well, ln-penalty's requirement to permanently store HTLC information\n>       in order to apply the penalty is in some sense a constant\n>       cost, however the impact is also proportional to value, and for\n>       sufficiently low value HTLCs can be ignored entirely if the HTLC\n>       isn't included in the channel commitment)\n> \n>> Instead, we should investigate how we can\n>> apply the ideas here with the more complicated fee structures we have.\n> \n> Fee structures should be *simple* not complicated.\n> \n> I mean, it's kind of great that we started off complicated -- if it\n> turns out base fee isn't necessary, it's easy to just set it to zero;\n> if we didn't have it, but needed it, it would be much more annoying to\n> add it in later.\n\nFee structures should also match reality, and allow node operators sufficient flexibility to capture their costs. I \nthink we have a design that does so quite well - its pretty simple, there's only two knobs, but the two knobs capture \nexactly the two broad categories of costs a node operator has.\n\n>> Color me an optimist, but I'm quite confident with sufficient elbow grease\n>> and heuristics we can get 95% of the way there. We can and should revisit\n>> these conversations if such exploration is done and we find that its not\n>> possible, but until then this all feels incredibly premature.\n> \n> Depends; I don't think it makes sense to try to ban nodes that don't have\n> a base fee of zero or anything, but random people on twitter advocating\n> that node operators should set it to zero and just worry about optimising\n> via the proportional fee and the min htlc amount seems fine.\n\nSure, the great thing about today is because the protocol exposes decent knobs operators can tune their fee structures \nany way they want, providing competition and multiple paths to nodes with potentially very divergent fees depending on \nthe type of payment. Absent liquidity limits, this should provide better service for all types of payments - routing \nthem to nodes that will support those types of payments!\n\n> For an experimental plugin that aggressively splits payments up, I think\n> either ignoring channels with >0 base fee entirely, or deciding that\n> you're happy to spend a total of X sats on base fees, and then ignoring\n> channels whose base fee is greater than X/paths/path-length sats is fine.\n\nSure, experimental plugins can do whatever they want!\n\n> But long term, I also think that the base fee is an entirely unhelpful\n> complication that will eventually just be hardcoded to zero by everyone,\n> and eventually channels that propose non-zero base fees won't even be\n> gossiped. I don't expect that to happen any time soon though.\n\nI very strongly disagree, as discussed, and am left highly dubious that it is a practical complication in any case."
            },
            {
                "author": "Anthony Towns",
                "date": "2021-08-16T04:00:48",
                "message_text_only": "On Sun, Aug 15, 2021 at 10:21:52PM -0400, Matt Corallo wrote:\n> On 8/15/21 22:02, Anthony Towns wrote:\n> > > In\n> > > one particular class of applicable routing algorithms you could use for\n> > > lightning routing having a base fee makes the algorithm intractably slow,\n> > I don't think of that as the problem, but rather as the base fee having\n> > a multiplicative effect as you split payments.\n> Yes, matching the real-world costs of forwarding an HTLC.\n\nActually, no, not at all. \n\nThe base+proportional fees paid only on success roughly match the *value*\nof forwarding an HTLC, they don't match the costs particularly well\nat all.\n\nWhy not? Because the costs are incurred on failed HTLCs as well, and\nalso depend on the time a HTLC lasts, and also vary heavily depending\non how many other simultaneous HTLCs there are.\n\n> Yes. You have to pay the cost of a node. If we're really worried about this,\n> we should be talking about upfront fees and/or refunds on HTLC fulfillment,\n> not removing the fees entirely.\n\n(I don't believe either of those are the right approach, but based on\nprevious discussions, I don't think anyone's going to realise I'm right\nuntil I implement it and prove it, so *shrug*)\n\n> > Being denominated in sats, the base fee also changes in value as the\n> > bitcoin price changes -- c-lightning dropped the base fee to 1sat (from\n> > 546 sat!) in Jan 2018, but the value of 1sat has increased about 4x\n> > since then, and it seems unlikely the fixed costs of a successful HTLC\n> > payment have likewise increased 4x.  Proportional fees deal with this\n> > factor automatically, of course.\n> This isn't a protocol issue, implementations can automate this without issue.\n\nI don't think anyone's proposing the protocol be changed; just that node\noperators set an option to a particular value?\n\nWell, except that Lisa's maybe proposing that 0 not be allowed, and a\nvalue >= 0.001 sat be required? I'm not quite sure.\n\n> > > There's real cost to distorting the fee structures on the network away from\n> > > the costs of node operators,\n> > That's precisely what the base fee is already doing.\n> Huh? For values much smaller than a node's liquidity, the cost for nodes is\n> (mostly) a function of HTLCs, not the value. \n\nYes, the cost for nodes is a function of the requests that come in, not\nhow many succeed. The fees are proportional to how many succeed, which\nis at best a distorted reflection of the number of requests that come in.\n\n> The cost to nodes is largely [...]\n\nThe cost to nodes is almost entirely the opportunity cost of not being\nable to accept other txs that would come in afterwards and would pay\nhigher fees.\n\nAnd all those costs can be captured equally well (or badly) by just\nsetting a proportional fee and a minimum payment value. I don't know why\nyou keep ignoring that point.\n\n> so I'd argue for many HTLCs forwarded\n> today per-payment costs mirror the cost to a node much, much, much, much\n> better than some proportional fees?\n\nYou're talking yourself into a *really* broken business model there.\n\n> > Additionally, I don't think HTLC slot usage needs to be kept as a\n> > limitation after we switch to eltoo;\n> The HTLC slot limit is to keep transactions broadcastable. I don't see why\n> this would change, you still get an output for each HTLC on the latest\n> commitment in eltoo, AFAIU.\n\neltoo gives us the ability to have channel factories, where we divide\nthe overall factory balance amongst different channels, all updated\noff-chain. It seems likely we'll want to do factories from day one,\nso that we don't implicitly limit either the lifetime of the channel\nor its update rate (>1 update/sec ~= <4 year lifetime otherwise if I\ndid the maths right). Once we're doing factories, if we have more than\nhowever many htlcs for a channel, we can re-divide the factory balance\nand add a new channel. If the limit is 500 HTLCs per tx, you'd have to\namortize 0.2% of the new tx across each HTLC, in addition to the cost\nof the HTLC itself, but that seems trivial.\n\n> > and in the meantime, I think it can\n> > be better managed via adjusting the min_htlc_amount -- at least for the\n> > scenario where problems are being caused by legitimate payment attempts,\n> > which is also the only place base fee can help.\n> Sure, we could also shift towards upfront fees or similar solutions,\n\nUpfront fees seem extremely vulnerable to attacks, and are certainly a\n(pretty large) protocol change.\n\n> > > Instead, we should investigate how we can\n> > > apply the ideas here with the more complicated fee structures we have.\n> > Fee structures should be *simple* not complicated.\n> > I mean, it's kind of great that we started off complicated -- if it\n> > turns out base fee isn't necessary, it's easy to just set it to zero;\n> > if we didn't have it, but needed it, it would be much more annoying to\n> > add it in later.\n> Fee structures should also match reality, and allow node operators\n> sufficient flexibility to capture their costs. I think we have a design that\n> does so quite well - its pretty simple, there's only two knobs, but the two\n> knobs capture exactly the two broad categories of costs a node operator has.\n\nI don't know how you can think these \"exactly\" capture node operators'\ncosts. They're missing the time factor, and don't capture any of the\ncosts due to failed payments.\n\n> Sure, the great thing about today is because the protocol exposes decent\n> knobs operators can tune\n\nKnobs are great for experimenting; but they're also something to work\nat reducing once the experiments have come up with some results. I\nthink #zerobasefee is more in the experiment stage: \"hey, this seems\nlike a great idea, let's set this knob to zero and focus on these other\nones instead\".\n\n> > For an experimental plugin that aggressively splits payments up, I think\n> > either ignoring channels with >0 base fee entirely, or deciding that\n> > you're happy to spend a total of X sats on base fees, and then ignoring\n> > channels whose base fee is greater than X/paths/path-length sats is fine.\n> Sure, experimental plugins can do whatever they want!\n\nIs there any reason to think any of this would be something other than\nnode configuration options and experimental plugins any time soon?\n\n(By \"any time soon\" I mean, I could see software defaults changing if\nover 50% of the network deliberately switched to zero base fees and found\nit worked fine; and I could see deprecating non-zero fees if that ended\nup with 90% of the network on zero base fees, no good reasons for node\noperators wanting to stick with running non-zero base fees, and the\nexperimental algos that relied on zero base fees being significantly\neasier to maintain or faster/better)\n\n> > But long term, I also think that the base fee is an entirely unhelpful\n> > complication that will eventually just be hardcoded to zero by everyone,\n> > and eventually channels that propose non-zero base fees won't even be\n> > gossiped. I don't expect that to happen any time soon though.\n> I very strongly disagree, as discussed, and am left highly dubious\n> that it is a practical complication in any case.\n\nThat's okay! Long term predictions would be pretty boring if everyone\nagreed with them.\n\nCheers,\naj"
            },
            {
                "author": "Matt Corallo",
                "date": "2021-08-16T04:48:36",
                "message_text_only": "Dropped a number of replies to which the reply would otherwise be \"see above\".\n\nOn 8/16/21 00:00, Anthony Towns wrote:\n> On Sun, Aug 15, 2021 at 10:21:52PM -0400, Matt Corallo wrote:\n>> On 8/15/21 22:02, Anthony Towns wrote:\n>>>> In\n>>>> one particular class of applicable routing algorithms you could use for\n>>>> lightning routing having a base fee makes the algorithm intractably slow,\n>>> I don't think of that as the problem, but rather as the base fee having\n>>> a multiplicative effect as you split payments.\n>> Yes, matching the real-world costs of forwarding an HTLC.\n> \n> Actually, no, not at all.\n> \n> The base+proportional fees paid only on success roughly match the *value*\n> of forwarding an HTLC, they don't match the costs particularly well\n> at all.\n> \n> Why not? Because the costs are incurred on failed HTLCs as well, and\n> also depend on the time a HTLC lasts, and also vary heavily depending\n> on how many other simultaneous HTLCs there are.\n\nSure, indeed, there's some additional costs which are not covered by failed HTLCs, nor incorporate the time the HTLC \nslot was used. But that wasn't my argument - my argument was that base + proportional is a much, much closer match for \nthe costs of a node barring clever-er solutions around HTLC-slot-time-used. Dropping base fee makes the whole situation \na good chunk *worse*.\n\n>> Yes. You have to pay the cost of a node. If we're really worried about this,\n>> we should be talking about upfront fees and/or refunds on HTLC fulfillment,\n>> not removing the fees entirely.\n> \n> (I don't believe either of those are the right approach, but based on\n> previous discussions, I don't think anyone's going to realise I'm right\n> until I implement it and prove it, so *shrug*)\n\nI think I agree, but I think they may currently be better than any *other* proposal, not that they're particularly good.\n\n>> The cost to nodes is largely [...]\n> \n> The cost to nodes is almost entirely the opportunity cost of not being\n> able to accept other txs that would come in afterwards and would pay\n> higher fees.\n> \n> And all those costs can be captured equally well (or badly) by just\n> setting a proportional fee and a minimum payment value. I don't know why\n> you keep ignoring that point.\n\nI didn't ignore this, I just disagree, and I'm not entirely sure why you're ignoring the points I made to that effect :).\n\nIn all seriousness, I'm entirely unsure why you think proportional is just as good? As you note, the cost for nodes is a \nfunction of the opportunity cost of the capital, and opportunity cost of the HTLC slots. Lets say as a routing node I \ndecide that the opportunity cost of one of my HTLC slots is generally 1 sat per second, and the average HTLC is \nfulfilled in one second. Why is it that a proportional fee captures this \"equally well\"?!\n\nYes, you could amortize it, but that doesn't make it \"equally\" good, and there are semi-serious proposals to start \nignoring nodes that *dont* set their fees to some particular structure in routing decisions. Sure, nodes can do what \nthey want, but its kinda absurd to suggest that this is a perfectly fine thing to do absent a somewhat compelling \nreason. This goes doubly because deploying such things significantly will mean we cannot do future protocol changes \nwhich may better capture the time-value of node resources!\n\n>>> Additionally, I don't think HTLC slot usage needs to be kept as a\n>>> limitation after we switch to eltoo;\n>> The HTLC slot limit is to keep transactions broadcastable. I don't see why\n>> this would change, you still get an output for each HTLC on the latest\n>> commitment in eltoo, AFAIU.\n> \n> eltoo gives us the ability to have channel factories....\n\nThat doesn't solve the issue at all - you still have a ton of transactions and transaction outputs and spends thereof to \nput on the chain in the case of a closure with pending HTLCs. In fact, most nodes today enforce a lower limit than the \n400-some-odd HTLCs that represent the transaction standardness limit, because 100KB transactions are stupid impractical.\n\n> (By \"any time soon\" I mean, I could see software defaults changing if\n> over 50% of the network deliberately switched to zero base fees and found\n> it worked fine; and I could see deprecating non-zero fees if that ended\n> up with 90% of the network on zero base fees, no good reasons for node\n> operators wanting to stick with running non-zero base fees, and the\n> experimental algos that relied on zero base fees being significantly\n> easier to maintain or faster/better)\n\nWhat is your definition of \"works fine\" here? In today's nearly-entirely-altruistic-routing-node network, we could \nprobably entirely drop the routing fees and things would \"work fine\". That doesn't make it a good idea for the long-term \nhealth of the network.\n\nMy suggestion is quite simple - that the software vendors wishing to rely on these types of algorithms *first* do the \nlegwork to see what other ideas can be explored before jumping to \"ignore all the nodes who've decided their fees are \nX\", because I think *that* is pretty bad idea for the long-term health of the network. I even suggested several areas of \nfuture research for folks to look into before we get to the point of in any way seriously relying on routing algorithms \nthat constrain our ability to adapt fees in the future.\n\nMatt"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-08-16T05:15:46",
                "message_text_only": "Good morning matt and aj,\n\nLet me cut in here.\n\n>From my reading of the actual paper --- which could be a massive misunderstanding, as I can barely understand half the notation, I am more a dabbler in software engineering than a mathist --- it seems to me that it would be possible to replace the cost function in the planning algorithm with *only* the negative-log-probability, which I think is the key point of the paper.\n\nThat is, the algorithm can be run in a mode where it *ignores* whatever fee scheme forwarding nodes desire.\n(@rene: correct me if I am wrong?)\n\nI propose that the algorithm be modified as such, that is, it *ignore* the fee scheme.\n\nHowever, the algorithm then gets an extra step after getting a payment plan (i.e. how to route multiple sub-payments).\nIt looks over the payment plan and if the fees involved are beyond some user-defined limit (with, say, a default of 0.5% of the total amount, as per the C-Lightning `pay` default), to look at the highest-fee channels in the payment plan.\nThen, it can rerun the flow algorithm, telling it to *disallow* the highest-fee channels identified if the total fees exceed the fee budget.\n\nIt seems to me that this modification of the algorithm may be sufficient to be resilient against any and all future fee scheme we may decide for Lightning.\n\nThis still achieves \"optimality\" in the sense of the paper, in a way similar to what is suggested in the paper.\nThe paper suggests to basically ignore gossiped channels with non-zero basefee.\nThe approach I suggest allows us to *start* without ignoring non-zero basefee, but to slowly degrade our view of the network by disallowing high-fee (whether high basefee or high propfee) channels.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Stefan Richter",
                "date": "2021-08-16T10:01:28",
                "message_text_only": "Hi Zmn et al.,\n\n>I propose that the algorithm be modified >as such, that is, it *ignore*\nthe fee  scheme.\n\nWe actually started out thinking like this in the event we couldn't find a\nproper way to handle fees, and the real world experiments we've done so far\nhave only involved probability costs, no fees at all.\n\nHowever, I think it is non-trivial to deal with the many cases in which too\nhigh fees could occur, and in the end the most systematic way of dealing\nwith them is actually including them in the cost function.\n\nThat said, I agree with Matt that more research needs to be done about the\neffect of  base fees on these computations. We do know they make the\nproblem hard in general, but we might find a way to deal with them\nreasonably in practice.\n\nI tend to agree with AJ, that I don't  believe the base fee is economically\nhelpful, but I also think that the market will decide that rather than the\ndevs (though I would argue for default Zerobasefee in the implementations).\n\nIn my view, nobody is really earning any money with the base fee, so the\ndiscussion is kind of artificial. On the other hand, I would estimate our\napproach should lead to liquidity being priced correctly in the\nproportional fee instead of the price being undercut by hobbyists as is the\ncase now. So in the long run I expect our routing method to make running a\nwell-stocked LN router much more profitable.\n\nCheers,\n  Stefan\n\nZmnSCPxj via Lightning-dev <lightning-dev at lists.linuxfoundation.org>\nschrieb am Mo., 16. Aug. 2021, 05:15:\n\n> Good morning matt and aj,\n>\n> Let me cut in here.\n>\n> From my reading of the actual paper --- which could be a massive\n> misunderstanding, as I can barely understand half the notation, I am more a\n> dabbler in software engineering than a mathist --- it seems to me that it\n> would be possible to replace the cost function in the planning algorithm\n> with *only* the negative-log-probability, which I think is the key point of\n> the paper.\n>\n> That is, the algorithm can be run in a mode where it *ignores* whatever\n> fee scheme forwarding nodes desire.\n> (@rene: correct me if I am wrong?)\n>\n>\n> However, the algorithm then gets an extra step after getting a payment\n> plan (i.e. how to route multiple sub-payments).\n> It looks over the payment plan and if the fees involved are beyond some\n> user-defined limit (with, say, a default of 0.5% of the total amount, as\n> per the C-Lightning `pay` default), to look at the highest-fee channels in\n> the payment plan.\n> Then, it can rerun the flow algorithm, telling it to *disallow* the\n> highest-fee channels identified if the total fees exceed the fee budget.\n>\n> It seems to me that this modification of the algorithm may be sufficient\n> to be resilient against any and all future fee scheme we may decide for\n> Lightning.\n>\n> This still achieves \"optimality\" in the sense of the paper, in a way\n> similar to what is suggested in the paper.\n> The paper suggests to basically ignore gossiped channels with non-zero\n> basefee.\n> The approach I suggest allows us to *start* without ignoring non-zero\n> basefee, but to slowly degrade our view of the network by disallowing\n> high-fee (whether high basefee or high propfee) channels.\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210816/f60fa4a3/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-08-16T10:27:29",
                "message_text_only": "Good morning Stefan,\n\n> >I propose that the algorithm be modified >as such, that is, it *ignore* the fee\u00a0 scheme.\n>\n> We actually started out thinking like this in the event we couldn't find a proper way to handle fees, and the real world experiments we've done so far have only involved probability costs, no fees at all.\u00a0\n>\n> However, I think it is non-trivial to deal with the many cases in which too high fees could occur, and in the end the most systematic way of dealing with them is actually including them in the cost function.\u00a0\n\nA reason why I suggest this is that the cost function in actual implementation is *already* IMO overloaded.\n\nIn particular, actual implementations will have some kind of conversion between cltv-delta and fees-at-node.\n\nThis conversion implies some kind of \"conversion rate\" between blocks-locked-up and fees-at-node.\nFor example, in C-Lightning this is the `riskfactor` argument to `getroute`, which is also exposed at `pay`.\n\nHowever, I think that in practice, most users cannot intuitively understand `riskfactor`.\nI myself cannot; when I write my own `pay` (e.g. in CLBOSS) I just start `riskfactor` to the in-manual default value, then tweak it higher if the total lockup time exceeds some maximum cltv budget for the payment and call `getroute` again.\n\nSimilarly, I think it is easier for users to think in terms of \"fee budget\" instead.\n\nOf course, algorithms should try to keep costs as low as possible, if there are two alternate payment plans that are both below the fee budget, the one with lower actual fee is still preferred.\nBut perhaps we should focus more on payment success *within some fee and timelock budget*.\n\nIndeed, as you point out, your real-world experiments you have done have involved only probability as cost.\nHowever, by the paper you claim to have sent 40,000,000,000msat for a cost of 814,000msat, or 0.002035% fee percentage, far below the 0.5% default `maxfeepercent` we have, which I think is fairly reasonable argument for \"let us ignore fees and timelocks unless it hits the budget\".\n(on the other hand, those numbers come from a section labelled \"Simulation\", so that may not reflect the real world experiments you had --- what numbers did you get for those?)\n\n\n>\n> That said, I agree with Matt that more research needs to be done about the effect of\u00a0 base fees on these computations. We do know they make the problem hard in general, but we might find a way to deal with them reasonably in practice.\u00a0\n\nIs my suggestion not reasonable in practice?\nIs the algorithm runtime too high?\n\n>\n> I tend to agree with AJ, that I don't\u00a0 believe the base fee is economically helpful, but I also think that the market will decide that rather than the devs (though I would argue for default Zerobasefee in the implementations).\u00a0\n>\n> In my view, nobody is really earning any money with the base fee, so the discussion is kind of artificial. On the other hand, I would estimate our approach should lead to liquidity being priced correctly in the proportional fee instead of the price being undercut by hobbyists as is the case now. So in the long run I expect our routing method to make running a well-stocked LN router much more profitable.\n\nWhile we certainly need to defer to economic requirements, we *also* need to defer to engineering requirements (else Lightning cannot be implemented in practice, so any economic benefits it might provide are not achievable anyway).\nAs I understand the argument of Matt, we may encounter an engineering reason to charge some base fee (or something very much like it), so encouraging #zerobasefee *now* might not be the wisest course of action, as a future engineering problem may need to be solved with non-zero basefee (or something very much like it).\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Stefan Richter",
                "date": "2021-08-17T10:07:05",
                "message_text_only": "Good morning Zmn!\n\n\nZmnSCPxj <ZmnSCPxj at protonmail.com> schrieb am Mo., 16. Aug. 2021, 10:27:\n\n>\n> A reason why I suggest this is that the cost function in actual\n> implementation is *already* IMO overloaded.\n>\n> In particular, actual implementations will have some kind of conversion\n> between cltv-delta and fees-at-node.\n>\n\nThat's an interesting aspect. Would this lead to a constant per edge if\nincorporated in the cost function? If so, this would lead to another\ngenerally hard problem, which, again, needs to be explored more in the\nconcrete cases we have here to see if we can still solve/approximate it.\n\nHowever, I think that in practice, most users cannot intuitively understand\n> `riskfactor`.\n>\n\nI don't think they have to. Only people like you who write actual software\nprobably need to.\n\n\n> Similarly, I think it is easier for users to think in terms of \"fee\n> budget\" instead.\n>\n> Of course, algorithms should try to keep costs as low as possible, if\n> there are two alternate payment plans that are both below the fee budget,\n> the one with lower actual fee is still preferred.\n> But perhaps we should focus more on payment success *within some fee and\n> timelock budget*.\n>\n> Indeed, as you point out, your real-world experiments you have done have\n> involved only probability as cost.\n> However, by the paper you claim to have sent 40,000,000,000msat for a cost\n> of 814,000msat, or 0.002035% fee percentage, far below the 0.5% default\n> `maxfeepercent` we have, which I think is fairly reasonable argument for\n> \"let us ignore fees and timelocks unless it hits the budget\".\n> (on the other hand, those numbers come from a section labelled\n> \"Simulation\", so that may not reflect the real world experiments you had\n> --- what numbers did you get for those?)\n>\n\nRen\u00e9 is going to publish those results very soon.\n\nRegarding payment success *within some fee and timelock budget*: the\nsituation is a little more complex than it appears. As you have pointed\nout, at the moment, most of the routes are very cheap (too cheap, IMHO), so\nyou have to be very unlucky to hit an expensive flow. So in the current\nenvironment, your approach seems to work pretty well, which is also why we\nfirst thought about it.\n\nUnfortunately, as you know, we have to think adversarially in this domain.\nAnd it is clear that if we simply disregarded fees in routing, people would\ntry to take advantage of this. If we just set a fee budget, and try again\nif it is missed, then I see some problems arise: First, what edges do you\nexclude in the next try? Where is that boundary? Second, I am pretty sure\nan adversary could design a DOS vector in this way by forcing people to go\nthrough exponentially many min-cost flow rounds (which are not cheap\nanyway) excluding only few edges per round.\n\nIndeed, if you read the paper closely you will have seen that this kind of\nproblem (optimizing for some cost while staying under a budget for a second\ncost) is (weakly) np-hard even for the single path case. So there is some\nintuition that this is not as simple as you might imagine it. I personally\nthink that the Lagrangian style of combining the costs in a linear fashion\nis very promising, but you might be successful with more direct methods as\nwell.\n\nIs my suggestion not reasonable in practice?\n> Is the algorithm runtime too high?\n>\n\nSee above. I don't know, but I believe it would be hard to make safe\nagainst adversaries. Including the fees in the cost function appears to be\nthe more holistic approach to me, since min-cost flow algorithms always\ngive you a globally optimized answer.\n\nWhile we certainly need to defer to economic requirements, we *also* need\n> to defer to engineering requirements (else Lightning cannot be implemented\n> in practice, so any economic benefits it might provide are not achievable\n> anyway).\n>\n\nYes. I wholeheartedly agree. However, I prefer watering down a\nmathematically correct solution as needed to building increasingly complex\nad-hoc heuristics.\n\nAs I understand the argument of Matt, we may encounter an engineering\n> reason to charge some base fee (or something very much like it), so\n> encouraging #zerobasefee *now* might not be the wisest course of action, as\n> a future engineering problem may need to be solved with non-zero basefee\n> (or something very much like it).\n>\n\nIf we encountered such a reason, we could still encourage something else\nIMHO. I do agree that we should not shorten our options by making a\nprotocol change at this time.\n\nBest regards,\n\n  Stefan\n\nP. S. : I have been using Clboss for some time now and I am very impressed.\nThank you for your amazing work! I would love a zerobasefee flag, though ;)\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210817/90a07096/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-08-21T02:36:45",
                "message_text_only": "Good morning Stefan,\n\n> > A reason why I suggest this is that the cost function in actual implementation is *already* IMO overloaded.\n> >\n> > In particular, actual implementations will have some kind of conversion between cltv-delta and fees-at-node.\n>\n> That's an interesting aspect. Would this lead to a constant per edge if incorporated in the cost function? If so, this would lead to another generally hard problem, which, again, needs to be explored more in the concrete cases we have here to see if we can still solve/approximate it.\u00a0\n\nNo, because each edge defines its own cltv-delta.\n\n>\n> > However, I think that in practice, most users cannot intuitively understand `riskfactor`.\n>\n> I don't think they have to. Only people like you who write actual software probably need to.\u00a0\n\n***I*** do not intuitively understand it either. (^^;)\nI understand it with system 2 (expected return on investment if you were investing the money instead of having it locked due to node failure along a path) but my system 1 just goes \"hmmm whut\" and I just use the default, which I *hope* cdecker chose rationally.\n\n> > Similarly, I think it is easier for users to think in terms of \"fee budget\" instead.\n> >\n> > Of course, algorithms should try to keep costs as low as possible, if there are two alternate payment plans that are both below the fee budget, the one with lower actual fee is still preferred.\n> > But perhaps we should focus more on payment success *within some fee and timelock budget*.\n> >\n> > Indeed, as you point out, your real-world experiments you have done have involved only probability as cost.\n> > However, by the paper you claim to have sent 40,000,000,000msat for a cost of 814,000msat, or 0.002035% fee percentage, far below the 0.5% default `maxfeepercent` we have, which I think is fairly reasonable argument for \"let us ignore fees and timelocks unless it hits the budget\".\n> > (on the other hand, those numbers come from a section labelled \"Simulation\", so that may not reflect the real world experiments you had --- what numbers did you get for those?)\n>\n> Ren\u00e9 is going to publish those results very soon.\u00a0\n>\n> Regarding payment success *within some fee and timelock budget*: the situation is a little more complex than it appears. As you have pointed out, at the moment, most of the routes are very cheap (too cheap, IMHO), so you have to be very unlucky to hit an expensive flow. So in the current environment, your approach seems to work pretty well, which is also why we first thought about it.\u00a0\n>\n> Unfortunately, as you know, we have to think adversarially in this domain. And it is clear that if we simply disregarded fees in routing, people would try to take advantage of this. If we just set a fee budget, and try again if it is missed, then I see some problems arise: First, what edges do you exclude in the next try? Where is that boundary? Second, I am pretty sure an adversary could design a DOS vector in this way by forcing people to go through exponentially many min-cost flow rounds (which are not cheap anyway) excluding only few edges per round.\u00a0\n>\n> Indeed, if you read the paper closely you will have seen that this kind of problem (optimizing for some cost while staying under a budget for a second cost) is (weakly) np-hard even for the single path case. So there is some intuition that this is not as simple as you might imagine it. I personally think that the Lagrangian style of combining the costs in a linear fashion is very promising, but you might be successful with more direct methods as well.\u00a0\n>\n> > Is my suggestion not reasonable in practice?\n> > Is the algorithm runtime too high?\n>\n> See above. I don't know, but I believe it would be hard to make safe against adversaries. Including the fees in the cost function appears to be the more holistic approach to me, since min-cost flow algorithms always give you a globally optimized answer.\u00a0\u00a0\n\nHah, yes, adversarial.\n\nI may have a route towards a unified cost function, will clean up a write up and post in a little while on a new thread.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Anthony Towns",
                "date": "2021-08-21T04:46:38",
                "message_text_only": "On Mon, Aug 16, 2021 at 12:48:36AM -0400, Matt Corallo wrote:\n> > The base+proportional fees paid only on success roughly match the *value*\n> > of forwarding an HTLC, they don't match the costs particularly well\n> > at all.\n> Sure, indeed, there's some additional costs which are not covered by failed\n> HTLCs, [...]\n> Dropping base fee makes the whole situation a good chunk *worse*.\n\nCan you justify that quantitatively?\n\nLike, pick a realistic scenario, where you can make things profitable\nwith some particular base_fee, prop_fee, min_htlc_amount combination,\nbut can't reasonably pick another similarly profitable outcome with\nbase_fee=0?  (You probably need to have a bimodal payment distribution\nwith a micropayment peak and a regular payment peak, I guess, or perhaps\nhave particularly inelastic demand and highly competitive supply?)\n\n> > And all those costs can be captured equally well (or badly) by just\n> > setting a proportional fee and a minimum payment value. I don't know why\n> > you keep ignoring that point.\n> I didn't ignore this, I just disagree, and I'm not entirely sure why you're ignoring the points I made to that effect :).\n\nI don't think I've seen you explicitly disagree with that previously,\nnor explain why you disagree with it? (If I've missed that, a reference\nappreciated; explicit re-explanation also appreciated)\n\n> In all seriousness, I'm entirely unsure why you think proportional is just\n> as good?\n\nIn principle, because fee structures already aren't a good match, and\na simple approximation is better that a complicated approximation.\nSpecifically, because you can set\n \n min_htlc_msat=old_base_fee_msat * 1e6 / prop_fee_millionths\n\nwhich still ensures every HTLC you forward offers a minimum fee of\nold_base_fee_msat, and your fees still increase as the value transferred\ngoes up, which in the current lightning environment seems like it's just\nas good an approximation as if you'd actually used \"old_base_fee_msat\".\n\nFor example, maybe you pay $40/month for your node, which is about 40msat\nper second [0], and you really can only do one HTLC per second on average\n[1]. Then instead of a base_fee of 40msat, pick your proportional rate,\nsay 0.03%, and calculate your min_htlc amount as above, ie 133sat. So if\nsomeone sends 5c/133sat through you, they'll pay 40msat, and for every\n~3 additional sats, they'll pay you an additional 1msat. Your costs are\ncovered, and provided your fee rate is competitive and there's traffic\non the network, you'll make your desired profit.\n\nIf your section of the lightning network is being used mainly for\nmicrotransactions, and you're not competitive/profitable when limiting\nyourself to >5c transactions, you could increase your proportional fee\nand lower your min_htlc amount, eg to 1% and 4sat so that you'll get\nyour 40msat from a 4sat/0.16c HTLC, and increase at a rate of 10msat/sat\nafter that.\n\nThat at least matches the choices you're probably actually making as a\nnode operator: \"I'm trying to be cheap at 0.03% and focus on relatively\nlarge transfers\" vs \"I'm focussing on microtransactions by reducing the\nminimum amount I'll support and being a bit expensive\". I don't think\nanyone's setting a base fee by calculating per-tx costs (and if they\nwere, per the footnote, I'm not convinced it'd even justify 1msat let\nalone 1sat per tx).\n\nOTOH, if you want to model an arbitrary concave fee function (because\nyou have some scheme that optimises fee income by discriminating against\nsmaller payments), you could do that by having multiple channels between\nthe same nodes, which is much more effective with (base, prop) fee pairs\nthan with (prop, min) pairs. (With (prop, min) pairs, you end up with\nlarge ranges of the domain that would prefer to pay prop2*min2 rather\nthan prop1*x when x<min2) [2]\n\nIt's not clear to me that's desirable in practice -- the benefit of a\nconcave fee function is that it penalises smaller payments / benefits\nlarger payments, which naturally penalises splitting payments up and\n(to some extent) micropayments in general. OTOH, it's whatever the\n\"turing complete\" version of setting fees is, which does at least feel\ntheoretically appealing.\n\n> As you note, the cost for nodes is a function of the opportunity\n> cost of the capital, and opportunity cost of the HTLC slots. Lets say as a\n> routing node I decide that the opportunity cost of one of my HTLC slots is\n> generally 1 sat per second, and the average HTLC is fulfilled in one second.\n> Why is it that a proportional fee captures this \"equally well\"?!\n\nIf I send an HTLC through you, I can pay your 1 sat fee, then keep the\nHTLC open for a day, costing you 86,400 sats by your numbers. So I don't\nthink that's even remotely close to capturing the costs of the individual\nHTLC that's paying the fee.\n\nBut if your averages are right, and enough people are nice despite me\nbeing a PITA, then you can get the same minimum with a proportional fee;\nif you're charging 0.1% you set the minimum amount to be 1000 sats.\n\n(But 1sat per HTLC is ridiculously expensive, like at least 20x over\nwhat your actual costs would be, even if your hardware is horribly slow\nand horribly expensive)\n\n> Yes, you could amortize it, \n\nYou're already amortizing it: that's what \"generally 1 sat per second\"\nand \"average HTLC is fulfilled in one second\" is capturing.\n\n> but that doesn't make it \"equally\" good, and\n> there are semi-serious proposals to start ignoring nodes that *dont* set\n> their fees to some particular structure in routing decisions.\n\nAre there proposals to do that outside of experimental plugins? That\nseems... aggressive, particularly given it'd exclude three-quarters of\nthe network, and any node running with default parameters.\n\n> Sure, nodes\n> can do what they want, but its kinda absurd to suggest that this is a\n> perfectly fine thing to do absent a somewhat compelling reason. This goes\n> doubly because deploying such things significantly will mean we cannot do\n> future protocol changes which may better capture the time-value of node\n> resources!\n\nI think you're getting a bit of a \"consensus\" mindset there -- if we\nneed to change lightning routing algorithms later, it'll be a pain,\nsure, but old behaviours aren't locked in permanently in the same way\nthey are for bitcoin consensus rules.\n\nWe'll need an similar network-wide routing upgrade to support PTLCs,\nand also (in my opinion anyway) to support a new fee mechanism that deals\nwith failed payments. Also needing one to re-support specifying a base fee\nwould be pretty easy by comparison, especially since we've \"been there,\ndone that\".\n\nBut I don't think anyone's proposing deploying such things \"significantly\"\nin the first place?\n\n> > > > Additionally, I don't think HTLC slot usage needs to be kept as a\n> > > > limitation after we switch to eltoo;\n> > > The HTLC slot limit is to keep transactions broadcastable. I don't see why\n> > > this would change, you still get an output for each HTLC on the latest\n> > > commitment in eltoo, AFAIU.\n> > eltoo gives us the ability to have channel factories....\n> That doesn't solve the issue at all - you still have a ton of transactions\n> and transaction outputs and spends thereof to put on the chain in the case\n> of a closure with pending HTLCs.\n\nIt solves the broadcastability issue. If you're worrying about ending up\nwith 5GB worth of pending HTLCs and not being able to atomically post\nthat to the blockchain because their timeouts all expire in less than\n5000 blocks, then sure, there's still other limits you might want to\ncare about.\n\n> > (By \"any time soon\" I mean, I could see software defaults changing if\n> > over 50% of the network deliberately switched to zero base fees and found\n> > it worked fine; and I could see deprecating non-zero fees if that ended\n> > up with 90% of the network on zero base fees, no good reasons for node\n> > operators wanting to stick with running non-zero base fees, and the\n> > experimental algos that relied on zero base fees being significantly\n> > easier to maintain or faster/better)\n> What is your definition of \"works fine\" here? In today's\n> nearly-entirely-altruistic-routing-node network, we could probably entirely\n> drop the routing fees and things would \"work fine\". That doesn't make it a\n> good idea for the long-term health of the network.\n\n\"We have a problem, and something must be done. This is something,\ntherefore it must be done\" ? Fixed fees that apply only to successful\ntransactions aren't a solution for the long term health of the\nnetwork. Yes, something to address that must be done, but base fees\naren't something that addresses it, so IMO it's just not a relevant point.\n\nMany node operators actively rejecting the recommendation would be an\neasy example of not \"working fine\" -- that's certainly what I'd expect to\nsee if the recommendation in question was \"just don't charge any fees\".\n(Equally, I wouldn't have any problem with a #zerofees twitter campaign\nto encourage people to try it out; though I don't think I'd support that\none myself. Presumably if there were a reasonable zero fee path between\nsender and recipient most lightning nodes would already try that first?)\n\nCheers,\naj\n\n[0] $1/month is about 1/40k BTC/month equals 10e3/4 sat/month;\n    there's about 2.5e6 seconds in a month (~28.9 days), so that's\n    about 10e3/10e6 sat/second or 1 msat/second. Neat.\n\n[1] A $40/month linode gives you 4 EPYC cores, so really you ought\n    to be able to do 100s of payments per second, giving you a per\n    payment cost of 0.4msat or less as far as I can see... If you're\n    ending up with 40msat after amortizing that's hundreds of failed\n    payments per success, if you're ending up at 1sat, that's thousands\n    of failed payments per success. (Not sure how the maths changes if\n    you're self-hosting or have dedicated hosting or if shared hosting\n    doesn't actually provide it's nominal performance)\n\n[2] Dammit, the theoretical argument is kind-of convincing me at this\n    point. (Hence the delay in posting this...)"
            },
            {
                "author": "Matt Corallo",
                "date": "2021-08-25T03:50:42",
                "message_text_only": "I feel like we're having two very, very different conversations here. On one hand, you're arguing that the base fee is \nof marginal use, and that maybe we can figure out how to average it out such that we can avoid needing it. On the other \nhand, I'm arguing that, yes, maybe you can, but ideally you wouldn't have to, because its still pretty nice to capture \nthose costs sometimes. Also, even if we can maybe do away with the base fee, that still doesn't mean we should start \nrelying on the lack of any not-completely-linear-in-HTLC-value fees in our routing algorithms, as maybe we'll want to do \nupfront payments or some other kind of anti-DoS payment in the future to solve the gaping, glaring, giant DoS hole that \nis HTLCs taking forever to time out.\n\nI'm not even sure that you're trying to argue, here, that we should start making key assumptions about the only fee \nbeing a proportional one in our routing algorithms, but that is what the topic at hand is, so I can't help but assume \nyou are?\n\nIf you disagree with the above characterization I'm happy to go line-by-line tit-for-tat, but usually those kinds of \ntirades aren't exactly useful and end up being more about semantics than the thrust of the argument.\n\nMatt\n\nOn 8/20/21 21:46, Anthony Towns wrote:\n> On Mon, Aug 16, 2021 at 12:48:36AM -0400, Matt Corallo wrote:\n>>> The base+proportional fees paid only on success roughly match the *value*\n>>> of forwarding an HTLC, they don't match the costs particularly well\n>>> at all.\n>> Sure, indeed, there's some additional costs which are not covered by failed\n>> HTLCs, [...]\n>> Dropping base fee makes the whole situation a good chunk *worse*.\n> \n> Can you justify that quantitatively?\n> \n> Like, pick a realistic scenario, where you can make things profitable\n> with some particular base_fee, prop_fee, min_htlc_amount combination,\n> but can't reasonably pick another similarly profitable outcome with\n> base_fee=0?  (You probably need to have a bimodal payment distribution\n> with a micropayment peak and a regular payment peak, I guess, or perhaps\n> have particularly inelastic demand and highly competitive supply?)\n >\n>>> And all those costs can be captured equally well (or badly) by just\n>>> setting a proportional fee and a minimum payment value. I don't know why\n>>> you keep ignoring that point.\n>> I didn't ignore this, I just disagree, and I'm not entirely sure why you're ignoring the points I made to that effect :).\n> \n> I don't think I've seen you explicitly disagree with that previously,\n> nor explain why you disagree with it? (If I've missed that, a reference\n> appreciated; explicit re-explanation also appreciated)\n> \n>> In all seriousness, I'm entirely unsure why you think proportional is just\n>> as good?\n> \n> In principle, because fee structures already aren't a good match, and\n> a simple approximation is better that a complicated approximation.\n> Specifically, because you can set\n>   \n>   min_htlc_msat=old_base_fee_msat * 1e6 / prop_fee_millionths\n> \n> which still ensures every HTLC you forward offers a minimum fee of\n> old_base_fee_msat, and your fees still increase as the value transferred\n> goes up, which in the current lightning environment seems like it's just\n> as good an approximation as if you'd actually used \"old_base_fee_msat\".\n> \n> For example, maybe you pay $40/month for your node, which is about 40msat\n> per second [0], and you really can only do one HTLC per second on average\n> [1]. Then instead of a base_fee of 40msat, pick your proportional rate,\n> say 0.03%, and calculate your min_htlc amount as above, ie 133sat. So if\n> someone sends 5c/133sat through you, they'll pay 40msat, and for every\n> ~3 additional sats, they'll pay you an additional 1msat. Your costs are\n> covered, and provided your fee rate is competitive and there's traffic\n> on the network, you'll make your desired profit.\n> \n> If your section of the lightning network is being used mainly for\n> microtransactions, and you're not competitive/profitable when limiting\n> yourself to >5c transactions, you could increase your proportional fee\n> and lower your min_htlc amount, eg to 1% and 4sat so that you'll get\n> your 40msat from a 4sat/0.16c HTLC, and increase at a rate of 10msat/sat\n> after that.\n> \n> That at least matches the choices you're probably actually making as a\n> node operator: \"I'm trying to be cheap at 0.03% and focus on relatively\n> large transfers\" vs \"I'm focussing on microtransactions by reducing the\n> minimum amount I'll support and being a bit expensive\". I don't think\n> anyone's setting a base fee by calculating per-tx costs (and if they\n> were, per the footnote, I'm not convinced it'd even justify 1msat let\n> alone 1sat per tx).\n> \n> OTOH, if you want to model an arbitrary concave fee function (because\n> you have some scheme that optimises fee income by discriminating against\n> smaller payments), you could do that by having multiple channels between\n> the same nodes, which is much more effective with (base, prop) fee pairs\n> than with (prop, min) pairs. (With (prop, min) pairs, you end up with\n> large ranges of the domain that would prefer to pay prop2*min2 rather\n> than prop1*x when x<min2) [2]\n> \n> It's not clear to me that's desirable in practice -- the benefit of a\n> concave fee function is that it penalises smaller payments / benefits\n> larger payments, which naturally penalises splitting payments up and\n> (to some extent) micropayments in general. OTOH, it's whatever the\n> \"turing complete\" version of setting fees is, which does at least feel\n> theoretically appealing.\n> \n>> As you note, the cost for nodes is a function of the opportunity\n>> cost of the capital, and opportunity cost of the HTLC slots. Lets say as a\n>> routing node I decide that the opportunity cost of one of my HTLC slots is\n>> generally 1 sat per second, and the average HTLC is fulfilled in one second.\n>> Why is it that a proportional fee captures this \"equally well\"?!\n> \n> If I send an HTLC through you, I can pay your 1 sat fee, then keep the\n> HTLC open for a day, costing you 86,400 sats by your numbers. So I don't\n> think that's even remotely close to capturing the costs of the individual\n> HTLC that's paying the fee.\n> \n> But if your averages are right, and enough people are nice despite me\n> being a PITA, then you can get the same minimum with a proportional fee;\n> if you're charging 0.1% you set the minimum amount to be 1000 sats.\n> \n> (But 1sat per HTLC is ridiculously expensive, like at least 20x over\n> what your actual costs would be, even if your hardware is horribly slow\n> and horribly expensive)\n> \n>> Yes, you could amortize it,\n> \n> You're already amortizing it: that's what \"generally 1 sat per second\"\n> and \"average HTLC is fulfilled in one second\" is capturing.\n> \n>> but that doesn't make it \"equally\" good, and\n>> there are semi-serious proposals to start ignoring nodes that *dont* set\n>> their fees to some particular structure in routing decisions.\n> \n> Are there proposals to do that outside of experimental plugins? That\n> seems... aggressive, particularly given it'd exclude three-quarters of\n> the network, and any node running with default parameters.\n> \n>> Sure, nodes\n>> can do what they want, but its kinda absurd to suggest that this is a\n>> perfectly fine thing to do absent a somewhat compelling reason. This goes\n>> doubly because deploying such things significantly will mean we cannot do\n>> future protocol changes which may better capture the time-value of node\n>> resources!\n> \n> I think you're getting a bit of a \"consensus\" mindset there -- if we\n> need to change lightning routing algorithms later, it'll be a pain,\n> sure, but old behaviours aren't locked in permanently in the same way\n> they are for bitcoin consensus rules.\n> \n> We'll need an similar network-wide routing upgrade to support PTLCs,\n> and also (in my opinion anyway) to support a new fee mechanism that deals\n> with failed payments. Also needing one to re-support specifying a base fee\n> would be pretty easy by comparison, especially since we've \"been there,\n> done that\".\n> \n> But I don't think anyone's proposing deploying such things \"significantly\"\n> in the first place?\n> \n>>>>> Additionally, I don't think HTLC slot usage needs to be kept as a\n>>>>> limitation after we switch to eltoo;\n>>>> The HTLC slot limit is to keep transactions broadcastable. I don't see why\n>>>> this would change, you still get an output for each HTLC on the latest\n>>>> commitment in eltoo, AFAIU.\n>>> eltoo gives us the ability to have channel factories....\n>> That doesn't solve the issue at all - you still have a ton of transactions\n>> and transaction outputs and spends thereof to put on the chain in the case\n>> of a closure with pending HTLCs.\n> \n> It solves the broadcastability issue. If you're worrying about ending up\n> with 5GB worth of pending HTLCs and not being able to atomically post\n> that to the blockchain because their timeouts all expire in less than\n> 5000 blocks, then sure, there's still other limits you might want to\n> care about.\n> \n>>> (By \"any time soon\" I mean, I could see software defaults changing if\n>>> over 50% of the network deliberately switched to zero base fees and found\n>>> it worked fine; and I could see deprecating non-zero fees if that ended\n>>> up with 90% of the network on zero base fees, no good reasons for node\n>>> operators wanting to stick with running non-zero base fees, and the\n>>> experimental algos that relied on zero base fees being significantly\n>>> easier to maintain or faster/better)\n>> What is your definition of \"works fine\" here? In today's\n>> nearly-entirely-altruistic-routing-node network, we could probably entirely\n>> drop the routing fees and things would \"work fine\". That doesn't make it a\n>> good idea for the long-term health of the network.\n> \n> \"We have a problem, and something must be done. This is something,\n> therefore it must be done\" ? Fixed fees that apply only to successful\n> transactions aren't a solution for the long term health of the\n> network. Yes, something to address that must be done, but base fees\n> aren't something that addresses it, so IMO it's just not a relevant point.\n> \n> Many node operators actively rejecting the recommendation would be an\n> easy example of not \"working fine\" -- that's certainly what I'd expect to\n> see if the recommendation in question was \"just don't charge any fees\".\n> (Equally, I wouldn't have any problem with a #zerofees twitter campaign\n> to encourage people to try it out; though I don't think I'd support that\n> one myself. Presumably if there were a reasonable zero fee path between\n> sender and recipient most lightning nodes would already try that first?)\n> \n> Cheers,\n> aj\n> \n> [0] $1/month is about 1/40k BTC/month equals 10e3/4 sat/month;\n>      there's about 2.5e6 seconds in a month (~28.9 days), so that's\n>      about 10e3/10e6 sat/second or 1 msat/second. Neat.\n> \n> [1] A $40/month linode gives you 4 EPYC cores, so really you ought\n>      to be able to do 100s of payments per second, giving you a per\n>      payment cost of 0.4msat or less as far as I can see... If you're\n>      ending up with 40msat after amortizing that's hundreds of failed\n>      payments per success, if you're ending up at 1sat, that's thousands\n>      of failed payments per success. (Not sure how the maths changes if\n>      you're self-hosting or have dedicated hosting or if shared hosting\n>      doesn't actually provide it's nominal performance)\n> \n> [2] Dammit, the theoretical argument is kind-of convincing me at this\n>      point. (Hence the delay in posting this...)\n>"
            },
            {
                "author": "Anthony Towns",
                "date": "2021-08-25T12:50:43",
                "message_text_only": "On Tue, Aug 24, 2021 at 08:50:42PM -0700, Matt Corallo wrote:\n> I feel like we're having two very, very different conversations here. On one\n> hand, you're arguing that the base fee is of marginal use, and that maybe we\n> can figure out how to average it out such that we can avoid needing it.\n\nI'm not sure about the \"we\" in that sentence -- I'm saying node operators\nshouldn't bother with it, not that lightning software devs shouldn't offer\nit as a config option or take it into account when choosing routes. The\nonly software change that /might/ make sense is changing defaults from\n1sat to 0msat, but it seems a bit early for that too, to me.\n\n(I'm assuming comments like \"We'll most definitely support #zerobasefee\"\n[0] just means \"you can set it to zero if you like\" which seems like a\nweird thing to have to say explicitly...)\n\n[0] https://twitter.com/Snyke/status/1418109408438063104\n\n> On\n> the other hand, I'm arguing that, yes, maybe you can, but ideally you\n> wouldn't have to, because its still pretty nice to capture those costs\n> sometimes.\n\nI don't really think it captures costs at all, but I do agree it could\nbe nice (at least in theory) to have it available since then you might\nbe able to better optimise your fee income based on whatever demand\nhappens to be. That's to increase profits, not match costs though, and\nI'm not convinced the theory will play out in practice presuming AMP is\noften useful/necessary.\n\n> Also, even if we can maybe do away with the base fee, that still\n> doesn't mean we should start relying on the lack of any\n> not-completely-linear-in-HTLC-value fees in our routing algorithms,\n\nI mean, exprimental/research routing algorithms should totally rely\non that if they feel like it? I just don't see any evidence that\nanyone's thinking of moving that out of research and into production\nuntil there's feedback from operators and a lot more results from the\nresearch in general...\n\n> as maybe\n> we'll want to do upfront payments or some other kind of anti-DoS payment in\n> the future to solve the gaping, glaring, giant DoS hole that is HTLCs taking\n> forever to time out.\n\nUntil we've got an even vaguely workable scheme for that, I don't\nthink it's relevant to consider. (If my preferred scheme turns out\nto be workable, I don't think it needs to be taken into account when\n(multi)pathfinding at all)\n\n> I'm not even sure that you're trying to argue, here, that we should start\n> making key assumptions about the only fee being a proportional one in our\n> routing algorithms, but that is what the topic at hand is, so I can't help\n> but assume you are?\n\nNo, that's not the topic at hand, at all?\n\nI mean, it's related, and interesting to talk about, but it's a digression\ninto \"wild ideas that might happen in the future\", not the topic... I\ndon't think anyone's currently advocating for node software to work that\nway? (I do think having many/most channels have a zero base fee will make\nmultipath routing algos work better even when they *don't* assume the\nbase fee is zero)\n\nI think I'm arguing for these things:\n\n a) \"everyone\" should drop their base fee msat from the default,\n    probably to 0 because that's an easy fixed point that you don't need\n    to think about again as the price of btc changes, but anything at\n    or below 10msat would be much more reasonable than 1000msat.\n\n b) if people are concerned about wasting resources forwarding super\n    small payments for correspondingly super small fees, they should\n    raise min_htlc_amount from 0 (or 1000) to compensate, instead of\n    raising their base fee.\n\n c) software should dynamically increase min_htlc_amount as the\n    number of available htlc slots decreases, as a DoS mitigation measure.\n    (presuming this is a temporary increase, probably this wouldn't\n    be gossiped, and possibly not even communicated to the channel\n    counterparty -- just a way of immediately rejecting htlcs? I think\n    if something along these lines were implemented, (b) would almost\n    never be necessary)\n\n d) the default base fee should be changed to 0, 1, or 10msat instead\n    of 1000msat\n\n e) trivially: (I don't think anyone's saying otherwise)\n     - 0 base fee should be a supported config option\n     - research/experimental routing algorithms are great and should\n       be encouraged\n     - deploying new algorithms in production should only be done with\n       a lot of care\n     - changing the protocol should only be done with even more care\n     - proportional fees should be rounded up to the next msat and never\n       rounded down to 0\n     - research/experiments on charging for holding htlcs open should\n       continue (likewise research on other DoS prevention measures)\n\nI'm not super sure about (c) or (d), and the \"everyone\" in (a) could\neasily not really be everyone.\n\n> If you disagree with the above characterization I'm happy to go line-by-line\n> tit-for-tat, but usually those kinds of tirades aren't exactly useful and\n> end up being more about semantics than the thrust of the argument.\n\nThanks for the lack of a tirade :)\n\nCheers,\naj"
            },
            {
                "author": "Matt Corallo",
                "date": "2021-08-25T20:06:09",
                "message_text_only": "On 8/25/21 05:50, Anthony Towns wrote:\n> On Tue, Aug 24, 2021 at 08:50:42PM -0700, Matt Corallo wrote:\n>> I feel like we're having two very, very different conversations here. On one\n>> hand, you're arguing that the base fee is of marginal use, and that maybe we\n>> can figure out how to average it out such that we can avoid needing it.\n> \n> I'm not sure about the \"we\" in that sentence\n\nYou and I, and it seems I was very much right :)\n\n> I'm saying node operators\n> shouldn't bother with it, not that lightning software devs shouldn't offer\n> it as a config option or take it into account when choosing routes. The\n> only software change that /might/ make sense is changing defaults from\n> 1sat to 0msat, but it seems a bit early for that too, to me.\n\nI think I largely agree, its too early to decide these things and node operators can consider these \nissues for themselves.\n\n> (I'm assuming comments like \"We'll most definitely support #zerobasefee\"\n> [0] just means \"you can set it to zero if you like\" which seems like a\n> weird thing to have to say explicitly...)\n> \n> [0] https://twitter.com/Snyke/status/1418109408438063104\n\nI don't believe so at all, we were definitely having a different conversation from both sides here. \nThe #zerobasefee movement grew out of, and focuses on, switching to #zerobasefee in order to allow \npeople to start using routing algorithms in production which ignore all nodes which do *not* have \nzero base fee and requiring that to be a routing node. Rusty even made a comment to that effect \nrecently on a Twitter Spaces, saying that its probably something that could be considered sooner or \nlater, though I admit it was an off-the-cuff remark so maybe he has a slightly different view when \npressed.\n\nMy objection, and it seems like you agree, is that it is much, much too early to start making a \nstrong assumption of the only fee being a proportional one in deployed routing algorithms.\n\n>> Also, even if we can maybe do away with the base fee, that still\n>> doesn't mean we should start relying on the lack of any\n>> not-completely-linear-in-HTLC-value fees in our routing algorithms,\n> \n> I mean, exprimental/research routing algorithms should totally rely\n> on that if they feel like it? I just don't see any evidence that\n> anyone's thinking of moving that out of research and into production\n> until there's feedback from operators and a lot more results from the\n> research in general...\n\nMaybe, maybe not - my only points on Twitter, and here, have been focused on how more research needs \nto happen on proposed routing algorithms and how we can adapt the ideas to other algorithms. A large \npart of the impetus for the #zerobasefee movement has been to reduce base fees to allow for a \nmigration to these experimental algorithms, and, to me, is entirely premature.\n\n> No, that's not the topic at hand, at all?\n\nWell, then we were having two different conversations :p\n\n> I think I'm arguing for these things:\n> \n>   a) \"everyone\" should drop their base fee msat from the default,\n>      probably to 0 because that's an easy fixed point that you don't need\n>      to think about again as the price of btc changes, but anything at\n>      or below 10msat would be much more reasonable than 1000msat.\n> \n>   b) if people are concerned about wasting resources forwarding super\n>      small payments for correspondingly super small fees, they should\n>      raise min_htlc_amount from 0 (or 1000) to compensate, instead of\n>      raising their base fee.\n\n:shrug: dunno. some people pay on-chain fees to route tiny payments to Muun wallets and seem fine \nwith it.\n\n>   c) software should dynamically increase min_htlc_amount as the\n>      number of available htlc slots decreases, as a DoS mitigation measure.\n>      (presuming this is a temporary increase, probably this wouldn't\n>      be gossiped, and possibly not even communicated to the channel\n>      counterparty -- just a way of immediately rejecting htlcs? I think\n>      if something along these lines were implemented, (b) would almost\n>      never be necessary)\n\nThis sounds like a cool idea. We shipped something highly related that almost accomplishes this on \naccident in LDK [1] focusing on exposure to small-value HTLCs and limiting that to ensure we don't \nsend all our money to miners.\n\nMore dynamic limits in lightning sounds like the right direction to me! Also dynamic fees, also \ndynamic....\n\n>   d) the default base fee should be changed to 0, 1, or 10msat instead\n>      of 1000msat\n> \n>   e) trivially: (I don't think anyone's saying otherwise)\n>       - deploying new algorithms in production should only be done with\n>         a lot of care\n\nThere is *so* much debate around this point in the lightning world these days. This is just another \nflavor of it.\n\nMatt\n\n[1] https://github.com/rust-bitcoin/rust-lightning/pull/1009"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2021-08-16T19:55:48",
                "message_text_only": "Matt wrote:\n> I'm frankly still very confused why we're having these conversations now\n\n1000% this!!\n\nThis entire conversation strikes me as extremely premature and backwards\ntbh.  Someone experimenting with a new approach shouldn't prompt us to\nimmediately modify the protocol to better \"fit\" the approach, particularly\nbefore any sort of comparative analysis has even been published. At this\npoint, to my knowledge we don't even have an independent implementation of\nthe algorithm that has been tightly integrated into an existing LN\nimplementation. We don't know in which conditions the algorithm excels, and\nin which conditions this improvement is maybe negligible (likely when payAmt\n<< chanCapacity).\n\nI think part of the difficulty here lies in the current lack of a robust\nframework to use in comparing the efficacy of different approaches. Even in\nthis domain, there're a number of end traits to optimize for including: path\nfinding length, total CLTV delay across all shards, the amount of resulting\nsplits (goal should be consume less commitment space), attempt iteration\nlatency, amount/path randomization, path finding memory, etc, etc.\n\nThis also isn't the first time someone has attempted to adapt typical\nflow-based algorithms to path finding in the LN setting. T-bast from ACINQ\ninitially attempted to adapt a greedy flow-based algorithm [1], but found\nthat a number of implementation related edge cases (he cites that the\nmin+max constraints in addition to the normal fee limit most implementations\nas barriers to adapting the algorithm) led him to go with a simpler approach\nto then iterate off of. I'd be curious to hear from T-bast w.r.t how this\nnew approach differs from his initial approach, and if he spots any\nyet-to-be-recognized implementation level complexities to properly\nintegrating flow based algorithms into path finding.\n\n> a) to my knowledge, no one has (yet) done any follow-on work to\n> investigate pulling many of the same heuristics Rene et al use in a\n> Dijkstras/A* algorithm with multiple passes or generating multiple routes\n> in the same pass to see whether you can emulate the results in a faster\n> algorithm without the drawbacks here,\n\nlnd's current approach (very far from perfect, derived via satisficement)\nhas some similarities to the flow-based approach in its use of probabilities\nto attempt to quantify the level of uncertainty of internal network channel\nbalances.\n\nWe start by assuming a config level a priori probability of any given route\nworking, we then take that, and the fee to route across a given link and\nconvert the two values into a scalar \"distance/weight\" (mapping to an\nexpected cost) we can plug into vanilla Dijkstras [2]. A fresh node uses\nthis value to compare routes instead of the typical hop count distance\nmetric. With a cold cache this doesn't really do much, but then we'll update\nall the a priori probabilities with observations we gain in the wild.\n\nIf a node is able to forward an HTLC to the next hop, we boost their\nprobability (conditional on the amount forward/failed, so there's a bayesian\naspect). Each failure results in the probabilities of nodes being affected\ndifferently (temp chan failure vs no next node, etc). For example, if we're\nable to route through the first 3 hops of the route, but the final hop fails\nwith a temp chan failure. We'll rewards all the nodes with a success\nprobability amount (default rn is 95%) that applies when the amount being\ncarried is < that prior attempt.\n\nAs we assume balances are always changing, we then apply a half life decay\nthat slows increases a penalized probability back to the baseline. The\nresulting algorithm starts with no information/memory, but then gains\ninformation with each attempt (increasing and decreasing probabilities as a\nfunction of the amount attempted and time that has passed since the last\nattempt). The APIs also let you mark certain nodes as having a higher\napriori probability which can reduce the amount of bogus path exploration.\nThis API can be used \"at scale\" to create a sort of active learning system\nthat learns from the attempts of a fleet of nodes, wallets, trampoline\nnodes, wallets, etc (some privacy caveats apply, though there're ways to\nfuzz things a bit differential style).\n\nOther knobs exist such as the min probability setting, which controls how\nhigh a success probability a candidate edge needs to have before it is\nexplored. If the algo is exploring too many lackluster paths (and there're a\nlot of these on mainnet due to normal balance imbalance), this value can be\nincreased which will let it shed a large number of edges to be explored.\nWhen comparing this to the discussed approach that doesn't use any prior\nmemory, there may be certain cases that allows this algorithm to \"jump\" to\nthe correct approach and skip all the pre-processing and optimization that\nmay result in the same route, just with added latency before the initial\nattempt. I'm skipping some other details like how we handle repeated\nfailures of a node's channels (there's a mechanism that penalizes them more\nheavily, as the algo assumes most of the node's channels aren't well\nmaintained/balanced, so why waste time trying the other 900 channels).\n\nOur approach differs more dramatically from this new approach further when\nit comes to the question of how to split a payment either due to a failure,\nor when it's necessary (amt > max(chanCapacities...)). lnd takes a very\na simple approach of just divides the problem in half (divide and conquer,\nso\nfork into two instances of amt/2) and try again. The divide and conquer\napproach typically means you'll end up with a minimal-ish amount of fees.\nThere's also a knob that lets you control the largest split size, which can\nforce the algo to split sooner, as otherwise it only splits when no route\nexists (either we explored a ton and they all failed, or the payment amt is\nlarger than chan capacity).\n\nThe algo works pretty well when the probabilities combined with well tuning\nof the parameters help the algorithm naturally ignore lower probability\nroutes during the edge relaxation step of Dijkstras. However if a client has\nno prior observations (and didn't get any from say it's wallet provider or\nw/e), then it can end up exploring poor routes for a while and eventually\nhit the default timeout (particularly with a slow disk, but that'll be\noptimized away in lnd 0.14).\n\nOne interesting area of research would be to investigate if a small amount\nof flow pre-planning can help the algorithm effectively re-summarize its\ncurrent working memory to ignore more paths we know likely won't work.\n\nIn the end, there's still so much of the design space that needs exploring,\nso settling on something (and advertising that everything is solved by\nmagically setting a particular value to zero) that appears (so far we're\ngoing mainly off of experimental anecdotes w/ unclear methods) to improve on\nthings in certain scenarios, and morphing the protocol around it is a\npremature declaration of \"Mission Accomplished!\".\n\nIn any case, major kudos to Rene and Stefan for examining the problem in a\nnew\nlight, and re-invigorating research of related areas!\n\n-- Laolu\n\n\n[1]: https://github.com/ACINQ/eclair/pull/1427\n[2]:\nhttps://github.com/lightningnetwork/lnd/blob/958119a12ab60d24c75c7681f344ceb5a450c4ad/routing/pathfind.go#L930\n\nOn Sun, Aug 15, 2021 at 5:07 PM Matt Corallo <lf-lists at mattcorallo.com>\nwrote:\n\n> Thanks, AJ, for kicking off the thread.\n>\n> I'm frankly still very confused why we're having these conversations now.\n> In one particular class of applicable routing\n> algorithms you could use for lightning routing having a base fee makes the\n> algorithm intractably slow, but:\n>\n> a) to my knowledge, no one has (yet) done any follow-on work to\n> investigate pulling many of the same heuristics Rene et\n> al use in a Dijkstras/A* algorithm with multiple passes or generating\n> multiple routes in the same pass to see whether\n> you can emulate the results in a faster algorithm without the drawbacks\n> here,\n>\n> b) to my knowledge, no one has (yet) done any follow-on work to\n> investigate mapping the base fee to other, more\n> flow-based-routing-compatible numbers, eg you could convert the base fee\n> to a minimum fee by increasing the \"effective\"\n> proportional fees. From what others have commented, this may largely\n> \"solve\" the issue.\n>\n> c) to my knowledge, no one has (yet) done any follow-on work to analyze\n> where the proposed algorithm may be most optimal\n> in the HTLC-value<->channel liquidity ratio ranges. We may find that the\n> proposed algorithm only provides materially\n> better routing when the HTLC value approaches X% of common network channel\n> liquidity, allowing us to only use it for\n> large-value payments where we can almost ignore the base fees entirely.\n>\n> There's real cost to distorting the fee structures on the network away\n> from the costs of node operators, especially as\n> we move towards requiring and using Real (tm) amounts of capital on\n> routing nodes. If we're relying purely on hobbyists\n> forever who are operating out of goodwill, we should just remove all fees.\n> If we think Lightning is going to involve\n> capital with real opportunity cost, matching fees to the costs is\n> important, or at least important enough that we\n> shouldn't throw it away after one (pretty great) paper and limited further\n> analysis.\n>\n> Imagine we find some great way to address HTLC slot flooding/DoS attacks\n> (or just chose to do it in a not-great way) by\n> charging for HTLC slot usage, now we can't fix a critical DoS issue\n> because the routing algorithms we deployed can't\n> handle the new costing. Instead, we should investigate how we can apply\n> the ideas here with the more complicated fee\n> structures we have.\n>\n> Color me an optimist, but I'm quite confident with sufficient elbow grease\n> and heuristics we can get 95% of the way\n> there. We can and should revisit these conversations if such exploration\n> is done and we find that its not possible, but\n> until then this all feels incredibly premature.\n>\n> Matt\n>\n> On 8/14/21 21:00, Anthony Towns wrote:\n> > Hey *,\n> >\n> > There's been discussions on twitter and elsewhere advocating for\n> > setting the BOLT#7 fee_base_msat value [0] to zero. I'm just writing\n> > this to summarise my understanding in a place that's able to easily be\n> > referenced later.\n> >\n> > Setting the base fee to zero has a couple of benefits:\n> >\n> >   - it means you only have one value to optimise when trying to collect\n> >     the most fees, and one-dimensional optimisation problems are\n> >     obviously easier to write code for than two-dimensional optimisation\n> >     problems\n> >\n> >   - when finding a route, if all the fees on all the channels are\n> >     proportional only, you'll never have to worry about paying more fees\n> >     just as a result of splitting a payment; that makes routing easier\n> >     (see [1])\n> >\n> > So what's the cost? The cost is that there's no longer a fixed minimum\n> > fee -- so if you try sending a 1sat payment you'll pay 0.1% of the fee\n> > to send a 1000sat payment, and there may be fixed costs that you have\n> > in routing payments that you'd like to be compensated for (eg, the\n> > computational work to update channel state, the bandwith to forward the\n> > tx, or the opportunity cost for not being able to accept another htlc if\n> > you've hit your max htlcs per channel limit).\n> >\n> > But there's no need to explicitly separate those costs the way we do\n> > now; instead of charging 1sat base fee and 0.02% proportional fee,\n> > you can instead just set the 0.02% proportional fee and have a minimum\n> > payment size of 5000 sats (htlc_minimum_msat=5e6, ~$2), since 0.02%\n> > of that is 1sat. Nobody will be asking you to route without offering a\n> > fee of at least 1sat, but all the optimisation steps are easier.\n> >\n> > You could go a step further, and have the node side accept smaller\n> > payments despite the htlc minimum setting: eg, accept a 3000 sat payment\n> > provided it pays the same fee that a 5000 sat payment would have. That\n> is,\n> > treat the setting as minimum_fee=1sat, rather than\n> minimum_amount=5000sat;\n> > so the advertised value is just calculated from the real settings,\n> > and that nodes that want to send very small values despite having to\n> > pay high rates can just invert the calculation.\n> >\n> > I think something like this approach also makes sense when your channel\n> > becomes overloaded; eg if you have x HTLC slots available, and y channel\n> > capacity available, setting a minimum payment size of something like\n> > y/2/x**2 allows you to accept small payments (good for the network)\n> > when you're channel is not busy, but reserves the last slots for larger\n> > payments so that you don't end up missing out on profits because you\n> > ran out of capacity due to low value spam.\n> >\n> > Two other aspects related to this:\n> >\n> > At present, I think all the fixed costs are also incurred even when\n> > a htlc fails, so until we have some way of charging failing txs for\n> > incurring those costs, it seems a bit backwards to penalise successful\n> > txs who at least pay a proportional fee for the same thing. Until we've\n> > got a way of handling that, having zero base fee seems at least fair.\n> >\n> > Lower value HTLCs don't need to be included in the commitment transaction\n> > (if they're below the dust level, they definitely shouldn't be included,\n> > and if they're less than 1sat they can't be included), and as such don't\n> > incur all the same fixed costs that HTLCs that are committed too do.\n> > Having different base fees for microtransactions that incur fewer costs\n> > would be annoying; so having that be \"amortised\" into the proportional\n> > fee might help there too.\n> >\n> > I think eltoo can help in two ways by reducing the fixed costs: you no\n> > longer need to keep HTLC information around permanently, and if you do\n> > a multilevel channel factory setup, you can probably remove the ~400\n> > HTLCs per channel at any one time limit. But there's still other fixed\n> > costs, so I think that would just lower the fixed costs, not remove them\n> > altogether and isn't a fundamental change.\n> >\n> > I think the fixed costs for forwarding a HTLC are very small; something\n> > like:\n> >\n> >     0.02sats -- cost of permanently storing the HTLC info\n> >                 (100 bytes, $500/TB/year, 1% discount rate)\n> >     0.04sats -- compute and bandwidth cost for updating an HTLC\n> ($40/month\n> >                 at linode, 1 second of compute)\n> >\n> > The opportunity cost of having HTLC slots or Bitcoin locked up until\n> > the HTLC succeeds/fails could be much more significant, though.\n> >\n> > Cheers,\n> > aj\n> >\n> > [0]\n> https://github.com/lightningnetwork/lightning-rfc/blob/master/07-routing-gossip.md#the-channel_update-message\n> > [1] https://basefee.ln.rene-pickhardt.de/\n> >\n> > _______________________________________________\n> > Lightning-dev mailing list\n> > Lightning-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n> >\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210816/e843ca02/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "#zerobasefee",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Anthony Towns",
                "Matt Corallo",
                "lisa neigut",
                "Olaoluwa Osuntokun",
                "Stefan Richter",
                "ZmnSCPxj"
            ],
            "messages_count": 20,
            "total_messages_chars_count": 115743
        }
    },
    {
        "title": "[Lightning-dev] Fee Budgets: A Possible Path Towards Unified Cost Functions For Lightning Pathfinding Problems",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2021-08-21T02:40:21",
                "message_text_only": "Subject: Fee Budgets: A Possible Path Towards Unified Cost Functions For Lightning Pathfinding Problems\n\nIntroduction\n============\n\nWhat is the cost of a failed LN payment?\n\nPresumably, if a user wants to pay in exchange for something,\nthat user values that thing higher than the Bitcoin they spend\non that thing.\nThis is a central assumption of free market economics, that\nall transactions are voluntary and that all participants in\nthe transaction get more utility out of the transaction than\nwhat they put in.\n\nNote that ***value is subjective***.\nFor example, a farmer values the food they sell less than\nthe buyer of that food, because the farmer has leet farming\nskillz that actually let them **grow food** from literal shit,\nsunlight, and water, and presumably the buyer does not have\nthose leet skillz0rs to convert literal shit to food\nusing sunlight and water (otherwise they would be growing\ntheir own food).\nThis applies for all production, given that you puny humans\nhave such limited time to learn and train leet skillz.\n\nThus, for a buyer, there is a difference in value between\nthe product they are buying, and the BTC they are sacrificing\nto the elder gods (i.e. the payment network and the seller) in\norder to get the product.\nThe buyer must value the product more than the BTC.\n\nThis difference, then, is the cost of a failed payment.\nIf the attempt to pay fails, then obviously the seller\nwill not be willing to send the product (as it can receive\nno money for it) and the buyer loses the (buyer-subjective)\nvalue of the product minus the value of the BTC they wanted\nto use to pay.\n\nThis difference in value, while subjective, is quantifiable\n(consider how judges at a beauty contest must convert\ntheir subjective judgment of beauty to a number; indeed,\nhorny humans do this all the time at bars, suggesting that\neven judgment-impaired humans intuitively understand that\nsubjective values can be quantified).\nAnd that quantifiable subjective value can be measured in\nunits of bitcoin.\n\nThus, this difference in value is the cost of failure of an\nLN payment.\nAnd due to the relationship of failure and success, the\ncost of failure is the value of success.\n\nPickhardt-Richter Payments\n==========================\n\nWhy is the cost of failure/value of success even relevant?\n\nIn [a 2021 paper](https://arxiv.org/abs/2107.05322)\nPickhardt and Richter present a method of estimating\nthe probability of payment success, and using that\nprobability-of-success as a cost function for a\ngeneralization of pathfinding algorithms (specifically\nminimum cost flow).\n\nOf course, probabilities of success are not the only\nconcern that actual pathfinding algorithms need to\nworry about.\nAnother two concerns are:\n\n* Actual fees (measured in bitcoin units).\n* Actual total cltv-delta (measured in blocks).\n\nIt is possible to convert total cltv-delta to a \"Fee\".\nBasically, the cost of the total cltv-delta is the value\nof your funds being locked and unuseable for that many\nblocks.\nThis can be represented as an expected annual return on\ninvestment if those funds were instead locked into some\ninvestment.\nIn C-Lightning this is the `riskfactor` parameter.\n\nThis implies that total cltv-delta can be converted\nto an equivalent amount of BTCs.\n\nNow, the issue is, how can we convert probability of\nsuccess to some equivalent amount of BTCs?\n\nThis is why the value of success --- i.e. the cost\nof payment failure --- is relevant.\n\nBy multiplying the cost of failure by the probability\nof failure, we can acquire a bitcoins-measured\nquantity that can be added directly to expected fees.\n\nFee Budgets\n===========\n\nLong ago, some weird rando with an unpronouncable name\ndecided to add a \"fee budget\" to his implementation of\nC-Lightning pay algorithm (back when C-Lightning did not\neven have a `pay` algorithm).\nFor some reason (possibly dark ritual), that rando managed\nto get that code into the actual C-Lightning, and the\nfee budget --- known as `maxfeepercent` --- has been\nretained to this day, even though none of the original\ncode has survived (dark rituals tend to consume anything\ninvolved in their rites, I would not be surprised if\nthat includes source code).\n\nNow consider --- how would a buyer assign a fee budget\nfor a particular payment?\n\nAs we noted, a rational buyer will only buy if they\nbelieve the value of the product being bought is higher\nthan the value of the BTCs they sacrifice to buy that\nproduct.\nThis difference is the cost of failure (equivalent to\nvalue of success).\n\nAnd a rational buyer will be willing to pay, as fee,\nany value up to this cost of failure/value of success.\n\nFor example, if the buyer is not willing to pay more\nthan half the cost of failure, then if there is no\nway to succeed payments at half the cost of failure,\nthen the payment simply fails and the buyer loses\nthe entire cost of failure.\nLogically, the buyer must be willing to pay, as\nfees, up to the cost of failure.\n\nSimilarly, if the buyer is willing to pay up to twice\nthe cost of failure, then if it succeeds only by\npaying up to twice the cost of failure, even if the\npayment pushes through and the buyer gets the product,\nthe buyer still lost the cost of failure because it\npaid more fees than the value of the payment success\nwas.\n\nLogically, then, the buyer must specify as fee budget,\nits expected value from acquiring the product minus\nthe price of the product.\n\nThus, it so happens that the fee budget is, in fact,\nthe value of payment success/cost of payment failure,\nsubjectively determined by the payer, quantified, and\nprovided to the C-Lightning payment algorithm!\n\nUnified Cost Function\n=====================\n\nThe cost of failure is then:\n\n    fee_budget * (1 - success_probability)\n\nThe `fee_budget` is the above fee budget, an input\nfrom the user.\n`success_probability` is the estimate as determined\nusing the Pickhardt-Richter algorithm.\n\nThe cost of a channel that charges `fee` is:\n\n    fee + fee_budget * (1 - success_probability)\n\nHowever, we should note that the above cost function\nis really the expected cost for the *entire\npayment*.\nIn particular, `success_probability` is multiplicative\nalong a path, whereas `fee` is additive.\n\nWhen we consider multipath payments, we should also\nobserve that the `success_probability` of each\nsub-payment are multiplied together (since all\nsub-payments must succeed) while `fee` is again\nadditive in nature.\n\nBecause of this, there is probably no *existing*\npathfinding algorithm which can actually *use*\nthis cost function.\nEvery pathfinding algorithm uses addition\nto compute total costs.\n\nThe Pickhardt-Richter paper gets around this by\nusing the logarithm of the probability.\nAs addition of logarithms is equivalent to\nmultiplication (`log A + log B = log (A * B)`),\nthis converts the addition operations of\nexisting pathfinding algos to multiplication of the\nprobabilities.\n\nThis technique cannot work with the above unified\ncost function, unfortunately.\n\nHowever, we can consider that, if we neglect\n`fee`, and use only the logarithms of the cost\nfunction, then the `fee_budget` term is\neffectively a constant cost for all channels on\nthe network.\nThat is, `log (fee_budget * success_probability) =\nlog fee_budget + log success_probability`\nfor all channels on the network, and we can\nsubtract `log fee_budget` on all channels\nwithout changing the result of any pathfinding\nalgorithms (provided we do not get into zero or\nnegative costs).\nThus, this instance of the Pickhard-Richter\ntechnique is equivalent to neglecting both the\n`fee` and the `fee_budget` in our unified cost\nfunction.\n\nGeneralized `#zerobasefee`\n==========================\n\nIf `fee` is small compared to `fee_budget`, then\nwe can consider the effect of the `fee` term to\nbe negligible compared to `fee_budget` term.\n\nHere is how the above cost function looks like,\nwith `fee_budget` distributed:\n\n    fee + fee_budget - fee_budget * success_probability\n\nIf `fee_budget` is very much higher than `fee`\nthen we can neglect `fee` as an approximation.\n\n    fee_budget - fee_budget * success_probability\n\nSince `fee_budget` is constant for all paths and\nfor all channels, we can just use the negative\nlogarithm of `success_probability` as the cost\nfunction.\n\nAs a heuristic, we can *ignore* fees and\njust use nagetive log probability, as suggested\nin the Pickhardt-Richter paper, *after* pruning\nchannels whose fees are very high (i.e. prune\nchannels whose `fee`, say, exceeds 1% of the\n`fee_budget`).\n\nOne way to view `#zerobasefee` is that this is\na specialized instance of the above general\nheuristic, that we can prune channels with\nnon-zero base fees in order to operate a\npathfinding algorithm that uses only addition\nfor edge costs and use negative log probability\nfor edge costs.\n\nWe can instead consider that *small enough*\nbase fees, which are negligible compared to\nthe `fee_budget`, should not be pruned, and\nnot specifically that all non-zero base fees\nshould be pruned.\nThat is, `#zerobasefee` assumes that a 1-sat\nbase fee is *not* negligible compared to the\n`fee_budget`.\n\nHowever, for large enough payments, the\n`fee_budget` may be large enough that a 1\nsatoshi base fee is actually negligible\ncompared to the `fee_budget` term.\nThis implies that `#zerobasefee` is a\nspecific heuristic, one that potentially\ncould be generalized.\n\nAs a corollary, the higher `success_probability`\nis, the more small fees matter (since\n`success_probability` subtracts from `fee_budget`).\nAnd higher `success_probability` arises from\nlarger channels in general.\nThis leads to the counterintuition that\nlarger channels should charge lower fees,\nsince logically the `#lowbasefee` pruning level\nshould be lower at higher `success_probability`.\n\nAlternative Pathfinding?\n========================\n\nAs noted, practically every pathfinding algorithm\nassumes that costs along every edge are always\nadded together.\n\nFor quantities that must be multiplied --- such\nas probabilities --- we can use the logarithm\ntrick to convert the addition to a multiplication.\n\nHowever, as noted, the unified cost function\nhas quantities that, in order to combine, must\nbe added (fees) and multiplied (probabilities).\n\nIn essence, every pathfinding algorithm cannot\nuse this unified cost function, as they assume\ncost functions that are trivially monoid.\n\nOr is it?\n\nFor something like the family of pathfinding\nalgorithms Greedy, A\\*, and Dijkstra, the only\noperations needed on costs are:\n\n* Addition (actually, a monoidal operation).\n* Comparison.\n\nRather than \"add\", perhaps a better term\nwould be to \"aggregate\" the costs.\n\n    class (Monoid type) where\n         zero :: type\n         `<*>` :: type -> type -> type\n         -- laws where\n         --     forall (a :: type) => zero <*> a = a\n         --     forall (a :: type) => a <*> zero = a\n         --     forall (a :: type, b :: type) => a <*> b = b <*> a\n         --     forall (a :: type, b :: type, c :: type) => (a <*> b) <*> c = a <*> (b <*> c)\n\nIn the above, `<*>` is an \"aggregate\" operation\nthat replaces the simple addition `+` traditionally\nused in pathfinding algorithms.\nFor typical numeric types, for example, you could\nderive an addition monoid or a multiplication\nmonoid.\n\nWe can consider a type that is both `Monoid`\nand `Ord`:\n\n    -- This will be defined by an actual run of the\n    -- pathfinding algorithm.\n    feeBudget :: Integer\n    feeBudget = undefined\n\n    data UnifiedCost = UnifiedCost { fee :: Integer\n                                   , successProbability :: Rational\n                                   }\n\n    -- addition\n    instance (Monoid UnifiedCost) where\n        zero = UnifiedCost { fee = 0\n                           , successProbability = 0\n                           }\n        a <*> b = UnifiedCost { fee = fee a + fee b\n                              , successProbability = successProbability a\n                                                   * successProbability b\n                              }\n\n    -- comparison\n    computeCost :: UnifiedCost -> Rational\n    computeCost c = toRational (fee c)\n                  + ( toRational feeBudget\n                    * (1.0 - successProbability c))\n    instance (Eq UnifiedCost) where\n        a == b = computeCost a == computeCost b\n    instance (Ord UnifiedCost) where\n        compare a b = compare (computeCost a) (computeCost b)\n\n    -- laws where\n    --    forall (a :: UnifiedCost, b :: UnifiedCost) => a <*> b >= a\n    --    forall (a :: UnifiedCost, b :: UnifiedCost) => a <*> b >= b\n    -- -- laws could be checked with QuickCheck\n    -- -- though we should ensure `0 <= successProbability <= 1`\n\nThat type would work well to replace the type of the\ncost in the Dijkstra-A\\*-Greedy family of algortihms;\nthey only need comparison and a monoid operation, but\nthe actual structure of the cost type is immaterial\nto the algorithm.\n\nIn particular, `feeBudget` is constant for an entire\nrun of pathfinding algorithms.\n\nThe question is whether minimum cost flow algos\ncould work with the above limited type.\nI probably need to go actually study those algos."
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-08-21T03:49:23",
                "message_text_only": ">     Alternative Pathfinding?\n>     ========================\n\n\nOr to put this section more succinctly: Why should cost be a number?\n\nWhat operations do the minimum cost flow algorithms demand of this thing called \"cost\", and can we provide those operations using something which is not a number but is instead a different structure?\nWhat is the minimal interface that the mincostflow algo demands of this \"cost\" datatype?\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Stefan Richter",
                "date": "2021-08-21T10:12:50",
                "message_text_only": "Hi Zmn! That is some amazing lateral thinking you have been applying there.\nI'm quite certain I haven't understood everything fully, but it has been\nhighly entertaining to read. Will have to give it a closer read when I get\nsome time.\n\nAs a first impression, here are some preliminary observations: While I\nhighly like the Haskell-style datatype, and the algorithm we use does\nmostly use Dijkstra pathfinding, I think what is really important in your\ndefinition is the computeCost definition. This is what we would call the\ncost function IIUC, and in order to be able to solve min-cost flow problems\nit generally has to be separable and convex. I believe your datatype merely\nhides the fact that it is neither.\n\nIntuitively, I think that any cost function that implies a fixed cost (that\nis, independent of the amount, though it might be different for every edge)\nper edge is concave and in theory problematic for min-cost flow algorithms\nbecause you could reduce some kind of NP-hard selection problem to it. I\nbelieve that applies to most if not all of your ideas in the text. Again, I\nthink we should think more about how much of a problem that is in practice,\nbecause we do have tools like approximation and parameterized algorithms,\nas well as heuristics, and I also believe that, say, a moderate base fee\nwill not change the optimal flow much, because this will always prefer\nlarge Htlcs anyway in order to optimize probability.\n\nI am really grateful that you have been taking the time to read and\nunderstand our paper and have been thinking further in this fascinating\nway. I am certain good things will come of it in time.\n\nCheers,\n  Stefan\n\nZmnSCPxj <ZmnSCPxj at protonmail.com> schrieb am Sa., 21. Aug. 2021, 03:49:\n\n>\n> >     Alternative Pathfinding?\n> >     ========================\n>\n>\n> Or to put this section more succinctly: Why should cost be a number?\n>\n> What operations do the minimum cost flow algorithms demand of this thing\n> called \"cost\", and can we provide those operations using something which is\n> not a number but is instead a different structure?\n> What is the minimal interface that the mincostflow algo demands of this\n> \"cost\" datatype?\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210821/02ef209c/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-08-24T01:06:25",
                "message_text_only": "Good morning Stefan,\n\n> Hi Zmn! That is some amazing lateral thinking you have been applying there. I'm quite certain I haven't understood everything fully, but it has been highly entertaining to read. Will have to give it a closer read when I get some time.\n>\n> As a first impression, here are some preliminary observations: While I highly like the Haskell-style datatype, and the algorithm we use does mostly use Dijkstra pathfinding, I think what is really important in your definition is the computeCost definition. This is what we would call the cost function IIUC, and in order to be able to solve min-cost flow problems it generally has to be separable and convex. I believe your datatype merely hides the fact that it is neither.\u00a0\n\nWell, it really depends on what min flow cost algorithms actually assume of the \"numbers\" being used.\n\nFor instance, it is well known that the Dijkstra-A\\*-Greedy family of algorithms do not handle \"negative costs\".\nWhat it really means is that the algorithms assume:\n\n    a + b >= a\n    a + b >= b\n\nThis holds if `a` and `b` are naturals (0 or positive), but not if they are integers.\n1 + -1 = 0, and 0 >= 1 is not true, thus the type for costs in those algorithms cannot be integer types, they have to be naturals.\nHowever if you restrict the type to naturals,  `a + b >= a` holds, and thus Dijkstra and its family of algorithms work.\n\nThus, if you are going to use Dijkstra-A\\*-Greedy, you \"only\" need to have the following \"operations\":\n\n    `+` :: Cost -> Cost -> Cost\n    `<` :: Cost -> Cost -> Bool\n    zero :: Cost\n\nWith the following derived operations:\n\n    a > b = b < a\n    a >= b = not (a < b)\n    a <= b = not (b < a)\n    a == b = (a >= b) && (a <= b)\n    a /= b = (a < b) || (a > b)\n\nAnd following the laws:\n\n    forall (a :: Cost) => a + zero == a\n    forall (a :: Cost) => zero + a == a\n    forall (a :: Cost, b :: Cost) => a + b == b + a\n    forall (a :: Cost, b :: Cost, c :: Cost) => (a + b) + c == a + (b + c)\n    forall (a :: Cost, b :: Cost) => a + b >= a\n    forall (a :: Cost, b :: Cost) => a + b >= b\n\nAs a non-mathist I have no idea what \"separable\" and \"convex\" actually mean.\nBasic search for \"convex\" and \"concave\" tends to show up information in geometry, which I think is not related (though it is possible there is some extension of the geometric concept to pure number theory?).\nAnd definitions on \"separable\" are not understandable by me, either.\n\nWhat exactly are the operations involved, and what are the laws those operations must follow, for the data type to be \"separable\" and \"convex\" (vs.\"concave\")?\n\nI guess my problem as well is that I cannot find easy-to-understand algorithms for min cost flow --- I can find discussions on the min cost flow \"problem\", and some allusions to solutions to that problem, but once I try looking into algorithms it gets quite a bit more complicated.\n\nBasically: do I need these operations?\n\n    `*` :: Cost -> Cost -> Cost\n    `/` :: Cost -> Cost -> Cost --- or Maybe Cost\n\nIf not, then why cannot `type Cost = UnifiedCost`?\n\n\nFor example, this page: https://www.topcoder.com/thrive/articles/Minimum%20Cost%20Flow%20Part%20Two:%20Algorithms\n\nIncludes this pseudocode:\n\n    Transform network G by adding source and sink\n    Initial flow x is zero\n    while ( Gx contains a path from s to t ) do\n        Find any shortest path P from s to t\n        Augment current flow x along P\n        update Gx\n\nIf \"find any shortest path\" is implemented using Dijkstra-A\\*-Greedy, then that does not require `Cost` to be an actual numeric type, they just require a type that provides `+`, `<`, and `zero`, all of which follow the laws I pointed out, *and no more than those*.\n`UnifiedCost` follows those laws (tough note that my definition of `zero` has a bug, `successProbability` should be `1.0` not `0`).\n\nIn short --- the output of the cost function is a `UnifiedCost` structure and ***not*** a number (in the traditional sense).\n\nBasically, I am deconstructing numbers here and trying to figure out what makes them tick, and seeing if I can use a different type to provide the \"tick\".\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Stefan Richter",
                "date": "2021-08-24T10:19:39",
                "message_text_only": "Good Morning Zmn!\n\nIf you'd like to understand  the min-cost flow problem and algorithms\nbetter, I would really recommend the textbook we have been citing\nthroughout the paper.\n\nThe algorithm you have found has a few shortcomings. It'll only work for\nthe linear min-cost flow problem, and it is very slow. In reality, we need\nto deal with convex cost functions, and the algorithm we have used so far\nuses an approach called capacity scaling in order to be much faster. It is\nindeed complex enough that it has taken us about two months to understand\nand implement it, discovering a nice heuristic in the process of making\nmistakes.\n\nSeparable in this context means that you can simply add up the costs of the\nedges to get the total costs. On second thought, your definition would\nprobably work here , by redefining adding up.\n\nConvex here means that for any two amounts x, y, the cost function f in the\ninterval x, y does not lie below the line connecting the two points (x,\nf(x)) and (y, f(y)). The intuition here is that a linear approximation\nnever overestimates the real cost. I guess one would need a more involved\ndefinition for your more complex coordinates.\n\nCheers,\n  Stefan\n\nZmnSCPxj <ZmnSCPxj at protonmail.com> schrieb am Di., 24. Aug. 2021, 01:06:\n\n> Good morning Stefan,\n>\n> > Hi Zmn! That is some amazing lateral thinking you have been applying\n> there. I'm quite certain I haven't understood everything fully, but it has\n> been highly entertaining to read. Will have to give it a closer read when I\n> get some time.\n> >\n> > As a first impression, here are some preliminary observations: While I\n> highly like the Haskell-style datatype, and the algorithm we use does\n> mostly use Dijkstra pathfinding, I think what is really important in your\n> definition is the computeCost definition. This is what we would call the\n> cost function IIUC, and in order to be able to solve min-cost flow problems\n> it generally has to be separable and convex. I believe your datatype merely\n> hides the fact that it is neither.\n>\n> Well, it really depends on what min flow cost algorithms actually assume\n> of the \"numbers\" being used.\n>\n> For instance, it is well known that the Dijkstra-A\\*-Greedy family of\n> algorithms do not handle \"negative costs\".\n> What it really means is that the algorithms assume:\n>\n>     a + b >= a\n>     a + b >= b\n>\n> This holds if `a` and `b` are naturals (0 or positive), but not if they\n> are integers.\n> 1 + -1 = 0, and 0 >= 1 is not true, thus the type for costs in those\n> algorithms cannot be integer types, they have to be naturals.\n> However if you restrict the type to naturals,  `a + b >= a` holds, and\n> thus Dijkstra and its family of algorithms work.\n>\n> Thus, if you are going to use Dijkstra-A\\*-Greedy, you \"only\" need to have\n> the following \"operations\":\n>\n>     `+` :: Cost -> Cost -> Cost\n>     `<` :: Cost -> Cost -> Bool\n>     zero :: Cost\n>\n> With the following derived operations:\n>\n>     a > b = b < a\n>     a >= b = not (a < b)\n>     a <= b = not (b < a)\n>     a == b = (a >= b) && (a <= b)\n>     a /= b = (a < b) || (a > b)\n>\n> And following the laws:\n>\n>     forall (a :: Cost) => a + zero == a\n>     forall (a :: Cost) => zero + a == a\n>     forall (a :: Cost, b :: Cost) => a + b == b + a\n>     forall (a :: Cost, b :: Cost, c :: Cost) => (a + b) + c == a + (b + c)\n>     forall (a :: Cost, b :: Cost) => a + b >= a\n>     forall (a :: Cost, b :: Cost) => a + b >= b\n>\n> As a non-mathist I have no idea what \"separable\" and \"convex\" actually\n> mean.\n> Basic search for \"convex\" and \"concave\" tends to show up information in\n> geometry, which I think is not related (though it is possible there is some\n> extension of the geometric concept to pure number theory?).\n> And definitions on \"separable\" are not understandable by me, either.\n>\n> What exactly are the operations involved, and what are the laws those\n> operations must follow, for the data type to be \"separable\" and \"convex\"\n> (vs.\"concave\")?\n>\n> I guess my problem as well is that I cannot find easy-to-understand\n> algorithms for min cost flow --- I can find discussions on the min cost\n> flow \"problem\", and some allusions to solutions to that problem, but once I\n> try looking into algorithms it gets quite a bit more complicated.\n>\n> Basically: do I need these operations?\n>\n>     `*` :: Cost -> Cost -> Cost\n>     `/` :: Cost -> Cost -> Cost --- or Maybe Cost\n>\n> If not, then why cannot `type Cost = UnifiedCost`?\n>\n>\n> For example, this page:\n> https://www.topcoder.com/thrive/articles/Minimum%20Cost%20Flow%20Part%20Two:%20Algorithms\n>\n> Includes this pseudocode:\n>\n>     Transform network G by adding source and sink\n>     Initial flow x is zero\n>     while ( Gx contains a path from s to t ) do\n>         Find any shortest path P from s to t\n>         Augment current flow x along P\n>         update Gx\n>\n> If \"find any shortest path\" is implemented using Dijkstra-A\\*-Greedy, then\n> that does not require `Cost` to be an actual numeric type, they just\n> require a type that provides `+`, `<`, and `zero`, all of which follow the\n> laws I pointed out, *and no more than those*.\n> `UnifiedCost` follows those laws (tough note that my definition of `zero`\n> has a bug, `successProbability` should be `1.0` not `0`).\n>\n> In short --- the output of the cost function is a `UnifiedCost` structure\n> and ***not*** a number (in the traditional sense).\n>\n> Basically, I am deconstructing numbers here and trying to figure out what\n> makes them tick, and seeing if I can use a different type to provide the\n> \"tick\".\n>\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210824/75aad701/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-08-30T11:08:52",
                "message_text_only": "Good morning Stefan,\n\n> Good Morning Zmn!\n>\n> If you'd like to understand\u00a0 the min-cost flow problem and algorithms better, I would really recommend the textbook we have been citing throughout the paper.\n>\n> The algorithm you have found has a few shortcomings. It'll only work for the linear min-cost flow problem, and it is very slow. In reality, we need to deal with convex cost functions, and the algorithm we have used so far uses an approach called capacity scaling in order to be much faster. It is indeed complex enough that it has taken us about two months to understand and implement it, discovering a nice heuristic in the process of making mistakes.\u00a0\n>\n> Separable in this context means that you can simply add up the costs of the edges to get the total costs. On second thought, your definition would probably work here , by redefining adding up.\u00a0\n>\n> Convex here means that for any two amounts x, y, the cost function f in the interval x, y does not lie below the line connecting the two points (x, f(x)) and (y, f(y)). The intuition here is that a linear approximation never overestimates the real cost. I guess one would need a more involved definition for your more complex coordinates.\u00a0\n\nHmm, that does require that we define \"subtract\" and \"multiply\" operations, with certain additional rules.\nThis is probably doable, especially since I think we do not need to multiply a `UnifiedCost` by another `UnifiedCost`, only a `UnifiedCost` by a `Rational`.\n\n***HOWEVER*** I realized later that the `#zerobasefee` issue might not actually be a problem for the ***mincostflow*** algorithm, but rather a problem for the ***disect*** algorithm that converts a flow solution to an actual set of sub-payments.\nIn that case, this effort is actually a dead end; it seems to me that, for the mincostflow algo at least, you can trivially add base fees, the issue is that once the disect algorithm gets its hands on the solution, base fees are problematic.\n\nIn addition, if you tell me that it took you two months to do the algo, then that is an even greater concern --- this is possible to do with a small number of operations, but if we need to add more, like \"subtract\" and \"multiply\" operations, then the risk involved in the software engineering also increases, and the expected implementation time might take much, much longer due to complexity.\n\nFor example, in CLBOSS I have a Dijkstra implementation that has inversion-of-control *and* allows users to parametrize \"addition\" and \"comparison\" operations, it is just a few dozen lines of code.\nHowever, the complexity of the code implementation would greatly increase if there is also a need to fill in \"multiply\" and \"subtract\" operations as well --- consider testing and other code quality needs.\n\n*And* if my conjecture is right --- that the `#zerobasefee` requirement is really a requirement of the ***disect*** algorithm rather than the ***mincostflow*** algorithm then the effort here would be pointless, we should focus more on the disect algo, which is why the new thread yet again.\n\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Fee Budgets: A Possible Path Towards Unified Cost Functions For Lightning Pathfinding Problems",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Stefan Richter",
                "ZmnSCPxj"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 28774
        }
    },
    {
        "title": "[Lightning-dev] Lightning-dev Digest, Vol 72, Issue 18",
        "thread_messages": [
            {
                "author": "Bitcoin Error Log",
                "date": "2021-08-23T12:58:43",
                "message_text_only": "The dust limit is required to prevent massive UTXO expansion. The details\naround miner incentives to process dust are irrelevant to this because\nthere simply needs to be a buffer of friction to prevent spamming the UTXO\nset to be much, much larger, as an *attack* and long-term overhead on both\nstorage size and resources in purposing UTXO data within\napplications/servers.\n\nEven if I were wrong, it is still silly to propose changing a thing no one\nactually needs or wants changed for any practical application.\n\nIt ain't broke, don't fix it.\n\nOn Fri, Aug 20, 2021 at 7:00 AM <\nlightning-dev-request at lists.linuxfoundation.org> wrote:\n\n> Send Lightning-dev mailing list submissions to\n>         lightning-dev at lists.linuxfoundation.org\n>\n> To subscribe or unsubscribe via the World Wide Web, visit\n>         https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n> or, via email, send a message with subject or body 'help' to\n>         lightning-dev-request at lists.linuxfoundation.org\n>\n> You can reach the person managing the list at\n>         lightning-dev-owner at lists.linuxfoundation.org\n>\n> When replying, please edit your Subject line so it is more specific\n> than \"Re: Contents of Lightning-dev digest...\"\n>\n>\n> Today's Topics:\n>\n>    1. Re: [bitcoin-dev]  Removing the Dust Limit (Jeremy)\n>\n>\n> ----------------------------------------------------------------------\n>\n> Message: 1\n> Date: Thu, 19 Aug 2021 23:51:31 -0500\n> From: Jeremy <jlrubin at mit.edu>\n> To: Bitcoin Protocol Discussion\n>         <bitcoin-dev at lists.linuxfoundation.org>\n> Cc: lightning-dev <lightning-dev at lists.linuxfoundation.org>\n> Subject: Re: [Lightning-dev] [bitcoin-dev]  Removing the Dust Limit\n> Message-ID:\n>         <\n> CAD5xwhiEDa2KjF265iDZ1ism4AFzh3S3D4cJSESVVKNwv9L7zA at mail.gmail.com>\n> Content-Type: text/plain; charset=\"utf-8\"\n>\n> one interesting point that came up at the bitdevs in austin today that\n> favors remove that i believe is new to this discussion (it was new to me):\n>\n> the argument can be reduced to:\n>\n> - dust limit is a per-node relay policy.\n> - it is rational for miners to mine dust outputs given their cost of\n> maintenance (storing the output potentially forever) is lower than their\n> immediate reward in fees.\n> - if txn relaying nodes censor something that a miner would mine, users\n> will seek a private/direct relay to the miner and vice versa.\n> - if direct relay to miner becomes popular, it is both bad for privacy and\n> decentralization.\n> - therefore the dust limit, should there be demand to create dust at\n> prevailing mempool feerates, causes an incentive to increase network\n> centralization (immediately)\n>\n> the tradeoff is if a short term immediate incentive to promote network\n> centralization is better or worse than a long term node operator overhead.\n>\n>\n> ///////////////////\n>\n> my take is that:\n>\n> 1) having a dust limit is worse since we'd rather not have an incentive to\n> produce or roll out centralizing software, whereas not having a dust limit\n> creates an mild incentive for node operators to improve utreexo\n> decentralizing software.\n> 2) it's hard to quantify the magnitude of the incentives, which does\n> matter.\n> -------------- next part --------------\n> An HTML attachment was scrubbed...\n> URL: <\n> http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210819/831b9608/attachment-0001.html\n> >\n>\n> ------------------------------\n>\n> Subject: Digest Footer\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n>\n> ------------------------------\n>\n> End of Lightning-dev Digest, Vol 72, Issue 18\n> *********************************************\n>\n\n\n-- \n~ John Carvalho\n\nSchedule: https://calendly.com/bitcoinerrorlog\nChat: https://t.me/bitcoinerrorlog\nSocial: https://twitter.com/bitcoinerrorlog\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210823/dfbbf45c/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Lightning-dev Digest, Vol 72, Issue 18",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Bitcoin Error Log"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 4120
        }
    },
    {
        "title": "[Lightning-dev] Do we really want users to solve an NP-hard problem when they wish to find a cheap way of paying each other on the Lightning Network?",
        "thread_messages": [
            {
                "author": "Ren\u00e9 Pickhardt",
                "date": "2021-08-26T14:33:23",
                "message_text_only": "Dear fellow lightning developers,\n\nwith a mixture of shock and disbelief I have been following the (semi)\npublic discussions for the last 6 weeks and the reaction of some companies\n/ people that reached out to me. I have to say I am really surprised by the\namount of hesitation that - despite obvious and overwhelming mathematical\nevidence -  a small group of people demonstrated in response to our\nresults. While I cannot make any sense of this I decided to post here\ndespite the fact that I believe everything that needed to be said is\nalready written and explained clearly in our paper [0] about optimally\nreliable and cheap payment flows on the Lightning Network. My hope is that\nthis mail can help us to\n\n* clarify a few misunderstandings\n* stop having opinionated discussions about mathematical facts\n* find a quick agreement on how to move on\n\nAs far as I can tell currently all implementations use some form of\nDijkstra algorithm in payment delivery with a strong emphasize on finding\npaths with cheap fees. The reason seems to be that it is a well established\nassumption that users would prefer a cheap solution on the market of\noffered routing fees. (while routing nodes of course try to offer fees that\nmaximize their earnings)\n\nIf we look at the mentioned paper there are several results (I will soon\npublish a separate mail here discussing some of the more interesting\nresults and consequences but I decided to split it and put this one here\nfirst as it seemed to be more urgent). One of the results which I will\ndiscuss now is the realization that - given the current fee function -\nfinding the cheapest payment flow is an NP-hard problem because the fee\nfunction is neither linear nor convex.\n\nOur fee function is `f(x) = rx + b` where `r`= fee rate, `b`=base fee and\n`x` is the amount we want to send.\n\nAs we thought it was obvious that the function is not linear we only\nexplained in the paper how the jump from f(0)=0 to f(1) = ppm+base_fee\nbreaks convexity. But as the question came up several times (for example\nhere [1]) I want to stress that the fee function - despite looking like a\nstraight line - is **not linear**. While writing this post I realized that\nthe issue might be that the concept of linearity [2] from the field of\nlinear algebra seems to be intermixed in the english language and American\nschool system with the concept of linear polynomials / linear functions\n[3]. So maybe that is part of the problem that emerged in previous\ndiscussions.\n\nWhen I write linear here I am referring back to the concept of linearity\nfrom linear algebra (c.f.: [2]) which btw seems to be also the main reason\nwhy schnorr signatures [2b] are so powerful that everyone is happy they\nwill find their way into bitcoin. We know that a necessary condition for a\nfunction to be called linear is that the following property holds:\n\nf(x+y)=f(x)+f(y)\n\nhowever if we look at our setting we see that:\n\nf(x+y) = r(x+y) + b\n\nand\n\nf(x)+f(y) = rx+b + ry+b = r(x+y) + 2b\n\nequality (and thus linearity) only holds if\n\nr(x+y)+b = r(x+y) + 2b <==> b = 2b\n\nthe later is true if and only if `b=0`. In other words: **Our current fee\nfunction is only linear if we set the base fee to zero.**\n\nOn the other side we refer to research in our paper that shows that an\noptimal solution (in this case optimal means cheapest) cannot be found if\nthe fee function is neither linear nor convex. I think one of the biggest\nmisunderstandings that I saw in the discussions is that people seem to have\nthought this has something to do with our new / proposed method. But as far\nas I can tell it does absolutely not have any connection to it. Instead and\nas most of you probably know the property to be an NP-hard problem is\ncompletely independent of the used algorithm. This is why in the paper we\nsuggested to drop the base_fee and mentioned in the German Podcast that the\nsame effect could already be achieved by node operators today by setting\ntheir base fee to 0. it is also the reason why we asked before we published\nthe paper why the base fee was introduced and what purpose it served [4].\n\nI am so surprised by some of the resistance because some of the biggest\ntalking points by Lightning Network critics in the past have been that\nrouting is either not solved or if it was solved it would be NP-hard. In\nour paper we show that delivering a payment can always be modeled as a min\ncost flow and the resulting optimization problem will indeed be NP-hard\ndepending on the cost function. Given the current fee function with a non\nzero base fee and the goal of minimizing fees that is followed by all\nimplementations I have to agree with such critics and say yes, in that\nparticular case delivering a payment optimally is an NP-hard problem.\n\nLuckily there are things we can do about it:\n\n1.) we can decide to have a different optimization goal. For example in the\nPaper we used a previously introduced probabilisitic model that optimizes\nfor reliability instead of fees and we show that in that case the function\nis convex and a polynomial exact algorithm is known and exists.\n2.) If we want to optimize for fees which - given current implementations\nfor single payments and the overwhelming feedback of users - seems to be\nthe case we can modify the fee function to be either convex or linear. As\neveryone who read until here knows the later can be achieved by just\nremoving the base fee.\n3.) Of course we can follow the ideas of Matt Corallo and Olaoluwa\nOsuntokun to do more research. I mean there is a nice 1 million dollar\nbounty [5] for the person who finds a polynomial solution to NP-hard\nproblems or proofs that such solutions cannot be found in polynomial time\n(which if you ask me personally is way more probable to happen but I\nusually try not to make wild conjectures about the future).\n4.) We can try to find linear approximations so that we can have more\nefficient algorithms to approximate such problems which I guess is what\nMatt and Olaoluwa might have meant when they suggested to do more research.\n\nAs I was struggeling with the problem for over two years I personally have\na pretty strong opinion about the 4 potentials paths:\n\n@1: We already describe in the paper that reliability should be included\nwhen deciding which channels to use while fees should not be neglected.\nThus I tend to say: Yes let's modify the goal in a way that we still keep\noptimizing for cheap delivery which by the end means keep a fee function\nbut NOT the current fee function.\n@2: Seems like a total no-brainer to modify the fee function to drop the\nbase fee because I believe nobody wants to drop the cheap requirement for\npayments and because of what I will say in reply to 3 & 4.\n@3: If you want to find solutions to hard problems I suggest to find a\nsolution to the discrete logarithm instead of studying P=NP. The bounty is\nthe the security of bitcoin and thus much higher than just the million\ndollar that you might get for the the solution to the P=NP problem. Ok,\njokes aside of course doing research is always a good idea and we laid out\na clear roadmap for further R&D in the paper that still holds even if we\nall agree that zerobasefee is a good thing. I am still seeking funding from\nthe bitcoin / lightning network community to be able to continue research\nand development in this field so I full appreciate the suggestion by Matt\nand Olaoluwa who asked for more research and I hope the community will\nentrust me to lead such research. I can understand that people hesitated to\nsupport my work back in 2019 when I decided to focus on the reliability\nquestion and it was a gamble if I could deliver but I sincerely hope with\nmy achievements that I lay out in full on https://ln.rene-pickhardt.de that\nthere will be more support in the future (not only for me).\n@4: People in operation research have tried this for many years. In\npractice problems are being linearized all the time. But in the best case\nthe models are already chosen to be linear and one hopes that such models\nare close enough to reality. We have been given a gift that the uncertainty\nin our channels naturally came as a convex cost function which makes the\nreliability goal somewhat easy to compute. More importantly it is a huge\ngift that it is just for us (users and developers) to decide how we want to\nhave the fee structure on the Lightning network. A linear solution could\njust be agreed upon / recommended but of course we can keep a non-linear\nversion and struggle around with approximations to handle np-hardness. For\nme it is so obvious what one would want to do that I will conclude with the\nsubject line of this mail as a rethorical question \"Do we really want users\nto solve an NP-hard problem when they wish to find a cheap way of paying\neach other on the Lightning Network?\"\n\nIn the recent Optech newsletter [6] Olaoluwa Osuntokun is said to have\n\"emphasized a point made earlier that there\u2019s no clear current need for\nnode operators to change a parameter today for a new pathfinding algorithm\nthat nobody is currently ready to use in production\". First of all I want\nto emphasize that Stefan and I have used this in production and have\nverified that our method works - as predicted in the paper - much better\nthan the pay implementation used in LND (I will write more about that in\nthe other mail that I already mentioned) But way more importantly (and as\nexplained in this mail and our paper) this parameter change is not about\nsome novel algorithm that we propose but rather about the mathematical\nstructure of the optimization problem that we want(?) users to face when\ndelivering payments (and that all implementations already presented to the\nusers in the single path payment case). So I very strongly disagree with\nthe statement that there is no clear need to change something. Actually I\nthink there is an exceptionally strong need to change something and very\ngood reason to do so rather sooner than later.\n\nThat being said the high reliability and large amounts that we could\ndeliver through the use of our method with a standard min convex cost flow\nsolver as the algorithm comes from the fact that we used probablistic path\nfinding [7] as the cost function. As demonstrated last march this already\nworks much better for single paths when used as a weight in dijsktra than\nthe stuff current implementations do. Also it did not create such a\ndiscussion when I shared the results about probabilisitic pathfinding last\nmarch and already mentioned that I only need to finish the work on optimal\nsplitting. So for the higher reliability and larger amounts alone that our\nmethod can do, we do not even need a zerobasefee. This is because\nprobabilistic path finding merely models the uncertainty of the liquidity\nin the channels to decide which channels to use with what amounts in\npayment delivery to maximize the success probability. The only reason why\nwe mention (zerobase)fees is because of @1 and the assumption that node\noperators would arbitrarily increase their fees on channels if we would not\ntake fees into consideration when deciding which channels to use. Since the\nbasefee kicks in and breaks the nice mathematical properties we addressed\nit.\n\nBtw I have a suggestion to node operators who seek channel partners and\nnodes with whom to open channels. If you don't want your customers / users\nbe required to solve or approximate NP-hard problems you could open your\nchannels with other nodes who already support zero base fee on their\nchannels. As of now this seems to be a clear indicator that those nodes\ncare about reliability and try to provide a good & honest service to the\nnetwork. As more and more wallets should use optimal payment flows it is\nalso reasonable to expect that a lot of routing is likely to happen in the\n#zerobasefee part of the network which is already pretty large [8]. At\nleast when I make a payment this part of the network is where I look first\nto deliver the payment. Also connecting to nodes that indicate via\nzerobasefee that they care seems at least to me personally way more\nreasonable than following some random intransparent score that assigns\nnumbers to nodes.\n\nAs said initially I thought I already had said everything which is why -\nunless substential new information comes up - I will most likely not engage\ninto further bike shedding discussions on the zero base fee discussions.\nThis is now for the community to decide and not for me to argue.\n\nwith kind regards Rene\n\n[0]: https://arxiv.org/abs/2107.05322\n[1]:\nhttps://bitcoin.stackexchange.com/questions/108018/would-a-min-fee-setting-for-lightning-channels-make-sense#comment124039_108325\n[2]: https://en.wikipedia.org/wiki/Linearity\n[2b]:\nhttps://courses.csail.mit.edu/6.857/2020/projects/4-Elbahrawy-Lovejoy-Ouyang-Perez.pdf\n[3]: https://en.wikipedia.org/wiki/Linear_function\n[4]:\nhttps://bitcoin.stackexchange.com/questions/107311/why-was-the-base-fee-for-the-routing-fee-calculation-of-the-lightning-network-in\n[5]:\nhttps://en.wikipedia.org/wiki/P_versus_NP_problem#Results_about_difficulty_of_proof\n[6]: https://bitcoinops.org/en/newsletters/2021/08/25/\n[7]:\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2021-March/002984.html\n[8]: https://lnrouter.app/graph/zero-base-fee\n-- \nhttps://www.rene-pickhardt.de\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210826/7fde2220/attachment-0001.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2021-08-31T04:01:34",
                "message_text_only": "On Thu, Aug 26, 2021 at 04:33:23PM +0200, Ren\u00e9 Pickhardt via Lightning-dev wrote:\n> As we thought it was obvious that the function is not linear we only explained\n> in the paper how the jump from f(0)=0 to f(1) = ppm+base_fee breaks convexity.\n\n(This would make more sense to me as \"f(0)=0 but f(epsilon)->b as\nepsilon->0, so it's discontinuous\")\n\n> \"Do we really want users to solve an NP-hard problem when\n> they wish to find a cheap way of paying each other on the Lightning Network?\"\u00a0\n\nFWIW, my answer to this is \"sure, if that's the way it turns out\".\n\nAnother program which solves an NP-hard problem every time it runs is\n\"apt-get install\" -- you can simulate 3SAT using Depends: and Conflicts:\nrelationships between packages. I worked on a related project in Debian\nback in the day that did a slightly more complicated variant of that\nproblem, namely working out if updating a package in the distro would\nrender other packages uninstallable (eg due to providing a different\nlibrary version) -- as it turned out, that even actually managed to hit\nsome of the \"hard\" NP cases every now and then. But it was never really\nthat big a deal in practice: you just set an iteration limit and consider\nit to \"fail\" if things get too complicated, and if it fails too often,\nyou re-analyse what's going on manually and add a new heuristic to cope\nwith it.\n\nI don't see any reason to think you can't do roughly the same for\nlightning; at worst just consider yourself as routing on log(N) different\nnetworks: one that routes payments of up to 500msat at (b+0.5ppm), one\nthat routes payments of up to 1sat at (b+ppm), one that routes payments\nof up to 2sat at (b+2ppm), one that routes payments of up to 4sat at\n(b+4ppm), etc. Try to choose a route for all the funds; if that fails,\nsplit it; repeat. In some case that will fail despite there being a\npossible successful multipath route, and in other cases it will choose a\nmoderately higher fee path than necessary, but if you're talking a paying\na 0.2% fee vs a 0.1% fee when the current state of the art is a 1% fee,\nthat's fine.\n\nCheers,\naj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-08-31T08:29:51",
                "message_text_only": "Good morning aj and Rene,\n\n> On Thu, Aug 26, 2021 at 04:33:23PM +0200, Ren\u00e9 Pickhardt via Lightning-dev wrote:\n>\n> > As we thought it was obvious that the function is not linear we only explained\n> > in the paper how the jump from f(0)=0 to f(1) = ppm+base_fee breaks convexity.\n>\n> (This would make more sense to me as \"f(0)=0 but f(epsilon)->b as\n> epsilon->0, so it's discontinuous\")\n\nI as well, \"discontinuous\" is how I would understand it, and it really needs to be pointed out to me that f(0)=0 and the sudden step to f(1)=1*prop + base is problematic.\n\n>\n> > \"Do we really want users to solve an NP-hard problem when\n> > they wish to find a cheap way of paying each other on the Lightning Network?\"\n>\n> FWIW, my answer to this is \"sure, if that's the way it turns out\".\n>\n> Another program which solves an NP-hard problem every time it runs is\n> \"apt-get install\" -- you can simulate 3SAT using Depends: and Conflicts:\n> relationships between packages.\n\nThank you for this, I now understand why functional package managers like Nix and Guix etc. exist.\n\n> I worked on a related project in Debian\n> back in the day that did a slightly more complicated variant of that\n> problem, namely working out if updating a package in the distro would\n> render other packages uninstallable (eg due to providing a different\n> library version) -- as it turned out, that even actually managed to hit\n> some of the \"hard\" NP cases every now and then. But it was never really\n> that big a deal in practice: you just set an iteration limit and consider\n> it to \"fail\" if things get too complicated, and if it fails too often,\n> you re-analyse what's going on manually and add a new heuristic to cope\n> with it.\n>\n> I don't see any reason to think you can't do roughly the same for\n> lightning; at worst just consider yourself as routing on log(N) different\n> networks: one that routes payments of up to 500msat at (b+0.5ppm), one\n> that routes payments of up to 1sat at (b+ppm), one that routes payments\n> of up to 2sat at (b+2ppm), one that routes payments of up to 4sat at\n> (b+4ppm), etc. Try to choose a route for all the funds; if that fails,\n> split it; repeat. In some case that will fail despite there being a\n> possible successful multipath route, and in other cases it will choose a\n> moderately higher fee path than necessary, but if you're talking a paying\n> a 0.2% fee vs a 0.1% fee when the current state of the art is a 1% fee,\n> that's fine.\n\nWell, something similar is already done by C-Lightning, and the `pay` algorithm even includes a time limit after which it just fails instead of retrying (roughly equivalent to setting an iteration limit, and actually more practical since it is not the number of CPU iterations that matters, it is real time during which user economic preferences may change).\n\nAnd for large enough payments, it *still* does not succeed often enough in practice.\n\nI pointed out elsewhere that failure-to-pay has an economic cost.\nThis economic cost is the difference between the value of the product being bought, minus the value of the bitcoin being used to pay for it.\n(remember, economic value is subjective to every economic actor; the economy is simply how the differences in value plays out as actors exchange objects, where each actor has its own subjective valuation of that value.)\nWe can ask the user explicitly for a numeric amount equivalent to this cost, by the simple question \"how much are you willing to pay in order for this transaction to succeed?\" i.e. \"what is the maximum transaction fee you will accept?\"\n\nThus, failing to deliver imposes an economic cost on users, a cost that Lightning Network, rightfully or not, has implicitly promised to reduce in comparison to onchain transactions.\n\nFor myself, I think a variant of Pickhardt-Richter payments can be created which *adapts to* the reality of the current network where `base_fee > 0` is common, but is biased against `base_fee > 0`, can be a bridge from the current network with `base_fee > 0` and a future with `#zerobasefee`.\nWith C-Lightning and its plugin nature allowing alternate implementations of `pay`, this can be done without requiring an entire new LN implementation.\nIf popular enough, because it demonstrates actual ability to transfer large funds at low fees, this can force forwarding nodes to learn that `#zerobasefee` earns them more funds (such as if they use a hill-climbing algorithm like I described to discover optimal feerates that gives them maximum earnings).\nThen `#zerobasefee` just wins economically.\n\nSo I think this really just needs a good implementation somewhere.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Stefan Richter",
                "date": "2021-08-31T19:10:40",
                "message_text_only": "Am Di., 31. Aug. 2021 um 10:29 Uhr schrieb ZmnSCPxj via Lightning-dev\n<lightning-dev at lists.linuxfoundation.org>:\n\n> For myself, I think a variant of Pickhardt-Richter payments can be created which *adapts to* the reality of the current network where `base_fee > 0` is common, but is biased against `base_fee > 0`, can be a bridge from the current network with `base_fee > 0` and a future with `#zerobasefee`.\n\nI have been thinking about your idea (at least what I understood of\nit) of using amount*prop_fee + amount*base_fee/min_flow_size, where\nmin_flow_size is a suitable quantization constant (say, 10k or 100k\nsats, may also chosen dynamically), as a component of the cost\nfunction, and I am pretty sure it is great at achieving exactly what\nyou are proposing here. This is a nicely convex (even linear in this\ncomponent) function and so it's easy to find min-cost flows for it. It\nsolves the problem (that I hadn't thought about before) that you have\npointed out in splitting flows into HTLCs. If you use\nmin_flow_size=max_htlc_size, it is even optimal (for this\nmin_flow_size). If you use a smaller min_flow_size, it is still\noptimal for channels with base_fee=0 but overestimates the fee for\nchannels with base_fee>0, and is less accurate the smaller the\nmin_flow_size and the larger the base_fee. So it will be biased\nagainst channels with larger base_fee. But notice that with min-cost\nflows, we are rarely looking for the cheapest solution anyway, because\nthese solutions (if they include more than one path) will usually\nfully saturate the cheapest channels and thus have very low success\nprobability. So all in all, I believe you found a great practical\nsolution for this debate. Everybody is free to use any base_fee they\nchose, we get a workable cost function, and I conjecture that\neconomics will convince most people to choose a zero or low base_fee.\n\nCheers\n  Stefan"
            },
            {
                "author": "Orfeas Stefanos Thyfronitis Litos",
                "date": "2021-08-31T09:59:11",
                "message_text_only": "Hi list,\n\nOn 8/31/21 5:01 AM, Anthony Towns wrote:\n>> \"Do we really want users to solve an NP-hard problem when\n>> they wish to find a cheap way of paying each other on the Lightning Network?\"\u00a0\n> FWIW, my answer to this is \"sure, if that's the way it turns out\".\n>\n> Another program which solves an NP-hard problem every time it runs is\n> \"apt-get install\".\n> [I]f it fails too often,\n> you re-analyse what's going on manually and add a new heuristic to cope\n> with it.\nI've been following the conversation with interest and I acknowledge this is a thorny issue.\n\nI am a bit worried with a path which relies on constantly finding new heuristics to approximate a solution to an NP-hard problem:\n* It allows too much room for nonconstructive disagreement between LN developers in the future.\n  - In a worst case scenario, all implementations end up using different, incompatible heuristics because each group of developers thinks that they have the best one, leading to a suboptimal performance for everyone. Heuristics are less of an exact science after all.\n* It makes the job of node operators less predictable, since it would depend more on the decisions of said developers with no guarantee of convergence to a single solution.\n  - Node operators may perceive this as loss of decentralization to the developers.\n\nSuch an approach is much more suitable to debian, since they have full control and a complete view over their \"network\" of packages, as opposed to LN, which is decentralized, nodes come and go at will and they can be private (even from developers!).\n\nBest,\nOrfeas\nThe University of Edinburgh is a charitable body, registered in Scotland, with registration number SC005336. Is e buidheann carthannais a th\u2019 ann an Oilthigh Dh\u00f9n \u00c8ideann, cl\u00e0raichte an Alba, \u00e0ireamh cl\u00e0raidh SC005336."
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-08-31T13:41:10",
                "message_text_only": "Good morning Orfeas,\n\n\n>         Such an approach is much more suitable to debian, since they have full control and a complete view over their \"network\" of packages, as opposed to LN, which is decentralized, nodes come and go at will and they can be private (even from developers!).\n\nIndeed, I came back to this topic to make this argument as well.\nMaintainers of apt repositories often make *some* amount of effort to avoid having too many `Conflict`-ing packages, such that typical simple users never experience the NP-hard bits.\nWorse, such apt repositories are definitely central points of coordination.\n\nNow of course users can add PPAs and anyone can create PPAs that others can consume, and the PPA-creation is not curated.\nBut in practice, even just a handful of PPAs can risk horrible cascades of `Conflict`-ing packages and massive headaches that can only be fixed by just reinstalling the OS from scratch and never using PPAs ever again.\n\n(Indeed, the ability for Nix and Guix to run \"on top of\" an existing OS is a good alternative to using PPAs, which avoids the `Conflict` issue.)\n\nNow, with Lightning Network, we run the risk that some de facto centralized curator will be consulted to provide good payment solutions.\nThat centralized curator avoids the NP-hard problems by being able to define which nodes and which channels are allowed to be considered in any payment solution, in much the same way an `apt` repository curator can define which packages are in and out of the repository, all in the name of avoiding the NP-hard problem for most users.\nThe risk is that such central coordinators may very well dangerously centralize the network in such a way that evicting them is not easy.\nIn particular, we do not want so central an entity that the choice for users becomes between accepting a centralized LN or making do with the expensive but decentralized base layer.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Do we really want users to solve an NP-hard problem when they wish to find a cheap way of paying each other on the Lightning Network?",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Anthony Towns",
                "Ren\u00e9 Pickhardt",
                "Orfeas Stefanos Thyfronitis Litos",
                "Stefan Richter",
                "ZmnSCPxj"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 25704
        }
    },
    {
        "title": "[Lightning-dev] No more closed channels because of fake HTLCs",
        "thread_messages": [
            {
                "author": "fiatjaf",
                "date": "2021-08-28T23:28:12",
                "message_text_only": "Good morning,\n\nI think we must stop closing channels to redeem trimmed HTLCs. It makes no\nsense at all. It's bad for everybody, there is not a single upside to\nclosing a channel to redeem a trimmed HTLC.\n\n(Please let me know if you don't agree with the above and what are the\nthings I'm missing.)\n\nNow I understand this is not trivial, because if we just decide to stop\nclosing, what do we do? It's not obvious -- and actually it may not even be\npossible to come up with a decent course of action.\n\nI've written two or more blog posts trying to explain the issue (mostly to\nmyself)(see https://fiatjaf.com/401367e9.html and links), but skipping the\nproblem and going to the solutions directly, I see three possible\napproaches (and they can all be implemented, in sequence, gradually):\n\n0. Never have stalled HTLCs again (just kidding, this will never happen).\n1. Just don't close. The channel keeps working. It's bad but at least you\ndon't have to close and open again (this is probably not safe against evil\npeople, but Lightning itself isn't anyway, right?).\n2. Try to negotiate: if a peer knows it is at fault (for having been\noffline while the HTLC expired, for example) it can eat the costs of that\nHTLC just so the other side doesn't close the channel, and vice-versa (this\nwill probably not work in most situations, but may work in some, I don't\nknow enough to say).\n3. Use an arbiter (this is my favorite): the two peers can agree on an\narbiter when creating the channel. The arbiter can act as a proxy for when\npeers can't establish connection with each other; revoke/fulfill messages\ncan be sent to the arbiter if the other peer is unresponsive; the arbiter\nhas authority to say who sent each message at each time and therefore who\nmust get the sats in that HTLC. If a peer doesn't obey the arbiter's\ndecision the other must blacklist it forever. The arbiter can be anything:\nanother Lightning node, a set of Lightning nodes, Twitter or any\nestablished internet service that allows arbitrary content to be published\nin public, this mailing list etc.\n\n(The arbiter idea may be sounding silly to you, but anything is better than\nwhat we have now. In all the suggestions above, the fallback, worst case\nscenario, is closing the channel. Today we live in the worst case scenario.)\n\nThank you for reading.\n\n---\nfiatjaf\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210828/96ce8ff9/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "No more closed channels because of fake HTLCs",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "fiatjaf"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2508
        }
    },
    {
        "title": "[Lightning-dev] Handling nonzerobasefee when using Pickhard-Richter algo variants",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2021-08-30T10:58:02",
                "message_text_only": "Good morning list,\n\nIt seems to me that the cost function defined in Pickhard-Richter\ncan in fact include a base fee:\n\n    -log(success_probability) + fudging_factor * amount * prop_fee + fudging_factor * base_fee\n\n(Rant: why do mathists prefer single-symbol var names, in software\nengineering there is a strong injunction **against** single-char\nvar names, with exceptions for simple numeric iteration loop vars,\nI mean keeping track of the meaning of each single-symbol var name\ntakes up working memory space, which is fairly limited on a wetware\nCPU, which is precisely why in software engineering there is an\ninjunction against single-char var names, srsly.)\n\nIt seems to me that the above is \"convex\", as the `fudging_factor`\nfor a run will be constant, and the `base_fee` for the channel\nwould also be constant, and since it seems to me, naively, that\nthe paper defines \"convex\" as \"the second derivative >= 0 for all\n`amount`\", and since the derivative of a constant is 0, the above\nwould still remain convex.\n(I am not a mathist and this conjecture might be completely asinine.)\n\nSo, it seems to me that the *real* reason for `#zerobasefee` is\nnot that the **mincostflow** algorithm cannot handle non-zero `base_fee`,\nbut rather, the **disect** phase afterwards cannot handle non-zero\n`base_fee`.\n\nFor example, suppose the minflowcost algorithm were instructed to\ndeliver 3,000msat from `S` to `D`, and returned the following flow:\n\n    S -->3000--> A ->1000-> B\n                 |          |\n                 |        1000\n                 |          v\n                 +-->2000-> C ->3000-> D\n\nIn the \"disect\" phase afterwards, the above flow solution would have\nto be split into two sub-payments, a 1000 sub-payment `S-A-B-C-D`\nand a 2000 sub-payment `S-A-C-D`.\n\nHowever, this does mean that the base cost for `C->D` and `S->A` in\nthe above flow will pay *twice* the base cost than what the above\ncost function would have computed, because in reality two independent\npayments pass through those channels.\n\nThus, the current Pickhardt-Richter scheme cannot work with non-zero\nbase fees if it were modified to consider fees, but not due to the\nmincostflow algorithm, rather because due to the disect algorithm.\n\n### Converting Non-Zero Base Fees To Proportional Fees\n\nAn alternative solution would be to optimize for a *maximum* fee\nrather than optimize for the *exact* fee.\n\nWe can do this by virtually splitting up the entire payment into\nsmaller bunches of value, and asking mincostflow to solve individual\nbunches rather than individual msats.\n\nFor example, we can decide that the smallest practical HTLC would\nbe 1000 msat.\nLet us call this the \"payment unit\", and the mincostflow algo\nsolves for paying in these payment units instead of 1msat units.\n(Actual implementations would need to have some heuristic or\nreasoned rule-of-thumb assumption on what the smallest practical\nHTLC would be.)\n\nIn our example, suppose we need to send 2001msat from `S` to `D`,\nwith the payment unit being 1000 msat.\nThen we would actually have the mincostflow algorithm work to\ndeliver 3 payment units (3 * 1000msat) from `S` to `D`.\n\nLet us suppose that the mincostflow algorithm returns the following\nflow:\n\n    S -->3-----> A ->1----> B\n                 |          |\n                 |        1 |\n                 |          v\n                 +-->2----> C ->3----> D\n\nActually, what the mincostflow algorithm uses as a cost function\nwould be:\n\n    -log(success_probability) + fudging_factor * amount * prop_fee * payment_unit + fudging_factor * base_fee * amount\n\n    == -log(success_probability) + fudging_factor * amount * (prop_fee * payment_unit + base_fee)\n\n    where: amount is in units of 1000msat, our smallest practical HTLC.\n           payment_unit is the unit, i.e. 1000msat.\n\nWhat the above means is that the mincostflow algorithm *allocates*\n`3 * base_fee` for `C->D`, since the `amount` flowing through\n`C->D` would be 3.\nHowever, when we pass the above flow to the disect algorithm, it\nwould actually only split this into 2 sub-payments, so the\nactual payment plan would only pay `2 * base_fee` for the\n`C->D` leg on both sub-payments.\n\nIn short, this effectively converts the base fee to a proportional\nfee, removing the zerobasfee requirement imposed by the disect\nalgorithm.\n\nThat is, the cost computed by the mincostflow algorithm is really a\nmaximum cost budget that the subsequent disect algorithm could later\nspend.\n\nIn effect, this converts the base fee to a proportional fee.\n\nThis may be acceptable in practice.\nThis approximation has a bias against non-zerobasefee --- it would\ntreat those channels as being far more expensive than they actually\nwould end up being in an *actual* payment attempt --- but at least\ndoes not *require* zerobasefee.\nIt would be able to still use non-zerobasefee channels if those are\nabsolutely required to reach the destination.\n\nThis should at least help create a practical payment algorithm that\nhandles current LN with nonzerobasefee, and provide an impetus\ntowards making zerobasefee a reality.\n\nNote that this approximation would be non-optimal --- there may\nbe solutions that provide lower costs than the output of this\napproximation would provide.\n\nIt may be practical to have the payment unit be adjustable by a\nhigher-level (but still automated) process.\nFor example, it may be considered that having more than 500 splits\nwould be heuristically determined to be impractical, in which\ncase the payment unit could be the actual amount divided by 500\nrounded up.\nThen if that results in extreme overestimation of base costs\n(i.e. the subsequent disect stage results in far fewer than 500\nsplits) the payment unit could be adjusted upwards.\nSimilarly, if that results in a payment plan that has high\nbase fees, the payment unit could be adjusted downwrds.\n\n--\n\nSimilarly, having particular split amounts is helpful to reduce\nthe ability of intermediate nodes snooping on payment amounts,\nso having a particular fixed payment unit may also be useful\nnevertheless, even if this approximation is otherwise undesirable.\n\nThat is, in the above example, we just send 3 sub-payments\ninstead of the minimum 2, so that every sub-payment is always\nuniformly 1000msat.\nThis leaks the least amount of information to intermediate nodes,\nespecially after payment decorrelation is widely deployed.\n(this is the same principle for why a uniform unit amount is\nneeded in practical CoinJoin implementations; it increases the\nanonymity set of individual sub-payments.)\n\nThis would be helped greatly by payment decorrelation;\nintermediate nodes would remain uncertain that multiple\nHTLCs with the same amount and the same in-channel and\nout-channel are from the same overall payment or not.\nAssuming many software uses the same payment unit consistently,\nless information can be extracted by surveillors, and the\nincreased cost may be considered justifiable in paying for\nadditional privacy.\n\n--\n\n### Whole-flow Payments\n\nSuppose we were to instead modify the LN protocol such that,\ninstead of MPP having individual sub-payments that split at the\nsource and join at the destination, we allow the source to\ninstruct arbitrary intermediate nodes to join and/or split\npayments.\n\nThat is, suppose LN allowed the source to indicate to arbitrary\nforwarding nodes to split payments and join them.\n(This would require nearly all nodes to upgrade, which may be\nimpractical, but let us consider its implications first without\ncommitting to actually doing this.)\n\nA node that is a join node would be instructed to wait for all\nincoming HTLCs to arrive before they create any outgoing\nHTLCs.\nA node that is a split node would make more than one outgoing\nHTLCs.\nForwarding nodes can be both join and split nodes.\n\nThis would allow the output of the mincostflow algorithm to be\nused directly, without the problematic disect stage.\nNodes that have more than one in-flow would be join nodes,\nwhile nodes with more than one out-flow would be split nodes.\n\nReturning to the example:\n\n    S -->3000--> A ->1000-> B\n                 |          |\n                 |        1000\n                 |          v\n                 +-->2000-> C ->3000-> D\n\nIn the above, `S` sends a *single* payment out on channel\n`S->A`, and instructs `A` to split the payment.\nThen `S` also instructs `C` to join two payments, resulting\nin a single HTLC out on `C->D` if `C` receives on both\n`A->C` and `B->C`.\n\nThe effect is that, for a single payment plan (presented as a\nflow, outputted by a mincostflow algorithm) for each channel\nwith nonzero flow, there would only be one HTLC that gets paid\nfor using a single `base_fee`.\n\nThen, the base fee computed by the cost function would be exact\nand there would be no underestimation of the base fee for\nnon-zerobasefee channels.\n\nLet us then consider the practicality of this scheme.\n\nIf a hop were to fail to deliver, then some join nodes would\nbe left hanging, waiting for an incoming HTLC that will not\narrive.\n\nThe failure would have to be handled by a split node, when it\nreceives a `update_fail_htlc`.\nThat split node cannot propagate the failure backwards to the\nsource, since it would still have other outgoing HTLCs in\nplay, which it cannot safely ignore; it can only propagate\nthe failure backwards if all the other outgoing HTLCs also\nfail.\n\nTo support this, a split node could give a request-to-fail\nsignal to its other outgoing HTLCs.\nThen, any other join nodes still waiting for an incoming\nHTLC (and would not yet have created any outgoing HTLCs,\nor for the destination, would not have claimed any incoming\nHTLCs) would fail all its incoming HTLCs.\nOtherwise, an intermediate node receiving a request-to-fail\nsignal from any incoming HTLCs would simply propagate\nthis request-to-fail outwards to all its outgoing HTLCs.\n\nThis implies that for this scheme, if *any* hop fails, the\nentire payment fails and has to be restarted \"from the top\".\n\nThis is in contrast with the current scheme of splitting\nonly at the source and joining only at the destination.\nIn the current LN, if *any* hop of an MPP fails, only the\nsub-payment(s) going through that hop will need to be\nretried.\nOther sub-payment(s) can keep being \"in play\", and the\nsource, having all the information needed, only needs to\nrecompute the failing outgoing payments.\n\nOn the other hand, the same information (particular hops are\nfailing) would be propagated back to the source, and the\nsource having to recompute the entire flow rather than just\nsome subset of the payments is not much worse computationally\n(work is still dominated by `O((m*m+m*n)*log n)` where `m` is\nthe channels of the *entire network* and `n` is the nodes\nof the *entire network*, amount is just a piddling little\n`O(log(u))` term).\nIt also removes the need to virtually reduce the capacity of\nintermediate hops while other sub-payments are in play\n(however, a practical implementation would need to consider\nthe possibility of multiple *simultaneous* outgoing payments\nanyway and would still retain this reduction of capacity in\ncase a new parallel outgoing payment is triggered while one\nis still ongoing).\n\nThis leads us to some fairly interesting choices:\n\n* [ ] Whole-flow payment is a brilliant idea and we should\n  totally do it.  `#deletethezerobasefeedebate`\n* [ ] Whole-flow payment is a horrible idea and we should\n  totally not do it.\n  * [ ] As we can see, the LN community needs to get together\n    for better payment success, and supporting whole-flow\n    payments is worse than setting `basefee=0`. `#zerobasefee`\n  * [ ] As we can see, the current LN provides a vital service\n    (partial failure of MPP) and intermediate nodes need to be\n    compensated for this service.  `#nonzerobasefee`\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Stefan Richter",
                "date": "2021-08-30T12:28:01",
                "message_text_only": "Hi Zmn!\n\nWhile you have some interesting thoughts about the implementation of\nmin-cost flow based routing, I feel that there are at least two grave\nmisunderstandings here:\n\nFirst, the problem with the base-fee or any similar constant is that\nthey make the fee function concave at the point where the flow\nincreases from 0 to 1. That is, there is a jump from 0 to\nbase-fee+1*prop_fee or similar, and the function does increase slower\nfrom there. This is the case for any function that includes a constant\nper edge that is only paid IF there is non-zero flow along that edge.\n\nSecond, while there might be additional problems in the disect-phase\n(which I am sure can be handled by a plethora of techniques), the fact\nremains that the min-cost flow problem itself is NP-hard when the\ncost-function is of this form.\n\nI realise that this is a fairly theoretical term, so let me shortly\ndescribe what it means: We know (there is a simple mathematical proof\nfor this, see reference in our paper) that if we could find an\nalgorithm that solves the min-cost flow problem ON ALL GRAPHS for all\ncost-functions of this form (basically, a constant per edge) in\npolynomial time, we could then use this algorithm to solve ANY problem\nin NP in polynomial time. This means, we would have an algorithm that\ncan find a solution to any problem in polynomial time whose solutions\ncan be VERIFIED in polynomial time. Because there are thousands of\nproblems of this kind that the smartest people in the world have tried\nto solve for decaded without success (and just one success would be\nenough to solve all of them!), it is highly doubtful that such an\nalgorithm exists. That is why we can be fairly certain that we cannot\nin general solve the min-cost flow problem for concave functions in\npolynomial time.\n\nHOWEVER, this does not mean that in practice it isn't possible to find\ngood solutions for the subset of problems we are likely to encounter\nin our application. It might be that the LN-graph is of a certain\nstructure that makes finding a solution easier. It is certain that we\ndo not need the optimal solution at all and approximations will be\neasier to find. All we know is that we do know how to solve the\nproblem generally for convex functions, and that we are very confident\nthat we cannot in general solve it optimally for concave functions.\n\nCheers\n  Stefan\n\nAm Mo., 30. Aug. 2021 um 12:58 Uhr schrieb ZmnSCPxj via Lightning-dev\n<lightning-dev at lists.linuxfoundation.org>:\n>\n> Good morning list,\n>\n> It seems to me that the cost function defined in Pickhard-Richter\n> can in fact include a base fee:\n>\n>     -log(success_probability) + fudging_factor * amount * prop_fee + fudging_factor * base_fee\n>\n> (Rant: why do mathists prefer single-symbol var names, in software\n> engineering there is a strong injunction **against** single-char\n> var names, with exceptions for simple numeric iteration loop vars,\n> I mean keeping track of the meaning of each single-symbol var name\n> takes up working memory space, which is fairly limited on a wetware\n> CPU, which is precisely why in software engineering there is an\n> injunction against single-char var names, srsly.)\n>\n> It seems to me that the above is \"convex\", as the `fudging_factor`\n> for a run will be constant, and the `base_fee` for the channel\n> would also be constant, and since it seems to me, naively, that\n> the paper defines \"convex\" as \"the second derivative >= 0 for all\n> `amount`\", and since the derivative of a constant is 0, the above\n> would still remain convex.\n> (I am not a mathist and this conjecture might be completely asinine.)\n>\n> So, it seems to me that the *real* reason for `#zerobasefee` is\n> not that the **mincostflow** algorithm cannot handle non-zero `base_fee`,\n> but rather, the **disect** phase afterwards cannot handle non-zero\n> `base_fee`.\n>\n> For example, suppose the minflowcost algorithm were instructed to\n> deliver 3,000msat from `S` to `D`, and returned the following flow:\n>\n>     S -->3000--> A ->1000-> B\n>                  |          |\n>                  |        1000\n>                  |          v\n>                  +-->2000-> C ->3000-> D\n>\n> In the \"disect\" phase afterwards, the above flow solution would have\n> to be split into two sub-payments, a 1000 sub-payment `S-A-B-C-D`\n> and a 2000 sub-payment `S-A-C-D`.\n>\n> However, this does mean that the base cost for `C->D` and `S->A` in\n> the above flow will pay *twice* the base cost than what the above\n> cost function would have computed, because in reality two independent\n> payments pass through those channels.\n>\n> Thus, the current Pickhardt-Richter scheme cannot work with non-zero\n> base fees if it were modified to consider fees, but not due to the\n> mincostflow algorithm, rather because due to the disect algorithm.\n>\n> ### Converting Non-Zero Base Fees To Proportional Fees\n>\n> An alternative solution would be to optimize for a *maximum* fee\n> rather than optimize for the *exact* fee.\n>\n> We can do this by virtually splitting up the entire payment into\n> smaller bunches of value, and asking mincostflow to solve individual\n> bunches rather than individual msats.\n>\n> For example, we can decide that the smallest practical HTLC would\n> be 1000 msat.\n> Let us call this the \"payment unit\", and the mincostflow algo\n> solves for paying in these payment units instead of 1msat units.\n> (Actual implementations would need to have some heuristic or\n> reasoned rule-of-thumb assumption on what the smallest practical\n> HTLC would be.)\n>\n> In our example, suppose we need to send 2001msat from `S` to `D`,\n> with the payment unit being 1000 msat.\n> Then we would actually have the mincostflow algorithm work to\n> deliver 3 payment units (3 * 1000msat) from `S` to `D`.\n>\n> Let us suppose that the mincostflow algorithm returns the following\n> flow:\n>\n>     S -->3-----> A ->1----> B\n>                  |          |\n>                  |        1 |\n>                  |          v\n>                  +-->2----> C ->3----> D\n>\n> Actually, what the mincostflow algorithm uses as a cost function\n> would be:\n>\n>     -log(success_probability) + fudging_factor * amount * prop_fee * payment_unit + fudging_factor * base_fee * amount\n>\n>     == -log(success_probability) + fudging_factor * amount * (prop_fee * payment_unit + base_fee)\n>\n>     where: amount is in units of 1000msat, our smallest practical HTLC.\n>            payment_unit is the unit, i.e. 1000msat.\n>\n> What the above means is that the mincostflow algorithm *allocates*\n> `3 * base_fee` for `C->D`, since the `amount` flowing through\n> `C->D` would be 3.\n> However, when we pass the above flow to the disect algorithm, it\n> would actually only split this into 2 sub-payments, so the\n> actual payment plan would only pay `2 * base_fee` for the\n> `C->D` leg on both sub-payments.\n>\n> In short, this effectively converts the base fee to a proportional\n> fee, removing the zerobasfee requirement imposed by the disect\n> algorithm.\n>\n> That is, the cost computed by the mincostflow algorithm is really a\n> maximum cost budget that the subsequent disect algorithm could later\n> spend.\n>\n> In effect, this converts the base fee to a proportional fee.\n>\n> This may be acceptable in practice.\n> This approximation has a bias against non-zerobasefee --- it would\n> treat those channels as being far more expensive than they actually\n> would end up being in an *actual* payment attempt --- but at least\n> does not *require* zerobasefee.\n> It would be able to still use non-zerobasefee channels if those are\n> absolutely required to reach the destination.\n>\n> This should at least help create a practical payment algorithm that\n> handles current LN with nonzerobasefee, and provide an impetus\n> towards making zerobasefee a reality.\n>\n> Note that this approximation would be non-optimal --- there may\n> be solutions that provide lower costs than the output of this\n> approximation would provide.\n>\n> It may be practical to have the payment unit be adjustable by a\n> higher-level (but still automated) process.\n> For example, it may be considered that having more than 500 splits\n> would be heuristically determined to be impractical, in which\n> case the payment unit could be the actual amount divided by 500\n> rounded up.\n> Then if that results in extreme overestimation of base costs\n> (i.e. the subsequent disect stage results in far fewer than 500\n> splits) the payment unit could be adjusted upwards.\n> Similarly, if that results in a payment plan that has high\n> base fees, the payment unit could be adjusted downwrds.\n>\n> --\n>\n> Similarly, having particular split amounts is helpful to reduce\n> the ability of intermediate nodes snooping on payment amounts,\n> so having a particular fixed payment unit may also be useful\n> nevertheless, even if this approximation is otherwise undesirable.\n>\n> That is, in the above example, we just send 3 sub-payments\n> instead of the minimum 2, so that every sub-payment is always\n> uniformly 1000msat.\n> This leaks the least amount of information to intermediate nodes,\n> especially after payment decorrelation is widely deployed.\n> (this is the same principle for why a uniform unit amount is\n> needed in practical CoinJoin implementations; it increases the\n> anonymity set of individual sub-payments.)\n>\n> This would be helped greatly by payment decorrelation;\n> intermediate nodes would remain uncertain that multiple\n> HTLCs with the same amount and the same in-channel and\n> out-channel are from the same overall payment or not.\n> Assuming many software uses the same payment unit consistently,\n> less information can be extracted by surveillors, and the\n> increased cost may be considered justifiable in paying for\n> additional privacy.\n>\n> --\n>\n> ### Whole-flow Payments\n>\n> Suppose we were to instead modify the LN protocol such that,\n> instead of MPP having individual sub-payments that split at the\n> source and join at the destination, we allow the source to\n> instruct arbitrary intermediate nodes to join and/or split\n> payments.\n>\n> That is, suppose LN allowed the source to indicate to arbitrary\n> forwarding nodes to split payments and join them.\n> (This would require nearly all nodes to upgrade, which may be\n> impractical, but let us consider its implications first without\n> committing to actually doing this.)\n>\n> A node that is a join node would be instructed to wait for all\n> incoming HTLCs to arrive before they create any outgoing\n> HTLCs.\n> A node that is a split node would make more than one outgoing\n> HTLCs.\n> Forwarding nodes can be both join and split nodes.\n>\n> This would allow the output of the mincostflow algorithm to be\n> used directly, without the problematic disect stage.\n> Nodes that have more than one in-flow would be join nodes,\n> while nodes with more than one out-flow would be split nodes.\n>\n> Returning to the example:\n>\n>     S -->3000--> A ->1000-> B\n>                  |          |\n>                  |        1000\n>                  |          v\n>                  +-->2000-> C ->3000-> D\n>\n> In the above, `S` sends a *single* payment out on channel\n> `S->A`, and instructs `A` to split the payment.\n> Then `S` also instructs `C` to join two payments, resulting\n> in a single HTLC out on `C->D` if `C` receives on both\n> `A->C` and `B->C`.\n>\n> The effect is that, for a single payment plan (presented as a\n> flow, outputted by a mincostflow algorithm) for each channel\n> with nonzero flow, there would only be one HTLC that gets paid\n> for using a single `base_fee`.\n>\n> Then, the base fee computed by the cost function would be exact\n> and there would be no underestimation of the base fee for\n> non-zerobasefee channels.\n>\n> Let us then consider the practicality of this scheme.\n>\n> If a hop were to fail to deliver, then some join nodes would\n> be left hanging, waiting for an incoming HTLC that will not\n> arrive.\n>\n> The failure would have to be handled by a split node, when it\n> receives a `update_fail_htlc`.\n> That split node cannot propagate the failure backwards to the\n> source, since it would still have other outgoing HTLCs in\n> play, which it cannot safely ignore; it can only propagate\n> the failure backwards if all the other outgoing HTLCs also\n> fail.\n>\n> To support this, a split node could give a request-to-fail\n> signal to its other outgoing HTLCs.\n> Then, any other join nodes still waiting for an incoming\n> HTLC (and would not yet have created any outgoing HTLCs,\n> or for the destination, would not have claimed any incoming\n> HTLCs) would fail all its incoming HTLCs.\n> Otherwise, an intermediate node receiving a request-to-fail\n> signal from any incoming HTLCs would simply propagate\n> this request-to-fail outwards to all its outgoing HTLCs.\n>\n> This implies that for this scheme, if *any* hop fails, the\n> entire payment fails and has to be restarted \"from the top\".\n>\n> This is in contrast with the current scheme of splitting\n> only at the source and joining only at the destination.\n> In the current LN, if *any* hop of an MPP fails, only the\n> sub-payment(s) going through that hop will need to be\n> retried.\n> Other sub-payment(s) can keep being \"in play\", and the\n> source, having all the information needed, only needs to\n> recompute the failing outgoing payments.\n>\n> On the other hand, the same information (particular hops are\n> failing) would be propagated back to the source, and the\n> source having to recompute the entire flow rather than just\n> some subset of the payments is not much worse computationally\n> (work is still dominated by `O((m*m+m*n)*log n)` where `m` is\n> the channels of the *entire network* and `n` is the nodes\n> of the *entire network*, amount is just a piddling little\n> `O(log(u))` term).\n> It also removes the need to virtually reduce the capacity of\n> intermediate hops while other sub-payments are in play\n> (however, a practical implementation would need to consider\n> the possibility of multiple *simultaneous* outgoing payments\n> anyway and would still retain this reduction of capacity in\n> case a new parallel outgoing payment is triggered while one\n> is still ongoing).\n>\n> This leads us to some fairly interesting choices:\n>\n> * [ ] Whole-flow payment is a brilliant idea and we should\n>   totally do it.  `#deletethezerobasefeedebate`\n> * [ ] Whole-flow payment is a horrible idea and we should\n>   totally not do it.\n>   * [ ] As we can see, the LN community needs to get together\n>     for better payment success, and supporting whole-flow\n>     payments is worse than setting `basefee=0`. `#zerobasefee`\n>   * [ ] As we can see, the current LN provides a vital service\n>     (partial failure of MPP) and intermediate nodes need to be\n>     compensated for this service.  `#nonzerobasefee`\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "Ren\u00e9 Pickhardt",
                "date": "2021-08-30T12:32:30",
                "message_text_only": "Dear ZmnSCPxj,\n\nthank you very much for this mail and in particular the openess to tinker\nabout a protocol change with respect to the transport layer of HTLCs /\nPTLCs! I fully agree that we should think about adopting our onion\ntransport layer in a way that supports local payment splits and merges to\nresemble the flowing nature of payments more closely! While an update for\nthat might be tricky I think right now is a perfect moment for such\ndiscussions as we kind of need a full upgrade anyway when going to PTLCs\n(which in fact might even be helpful with a local splittling / merging\nlogic as secrets would be additive / linear) Before I elaborate let me\nstate a few things and correct an error in your mail about the convex\nnature of the fee function and the problems of the min-cost flow solver.\n\nI agree with you (and AJ) that there is a problem in the base fee that\ncomes from overpaying when dissecting a flow into paths which share a\nchannel. However this is not related to the convexity issue. In a recent\nmail [0] I conductd the calculation demonstrating why the current fee\nfunction f(x)=x*r+b is not even linear (every linear function should be\nconvex). As described in the paper the problem is for the min cost flow\nsolving algorithms if the cost-function (in that case the fee function) is\nneither linear nor convex.\n\nThe issue that breaks convexity is really subtle here and comes when going\nfrom 0 flow to some flow. In that sense I apologize that the paper might be\nslightly misleading in its presentation. While convexity is often checked\nwith the second derivative argument our function is in fact defined on\nintegers and thus not even differentiable which is why the argument with\nthe second derivative that we use to show the convexity of the negative log\nprobabilities is not transferable.\n\nLet us better go with the wikipedia definition of convexity [1] that\nstates:\n\"In mathematics, a real-valued function is called convex if the line\nsegment between any two points on the graph of the function lies above the\ngraph between the two points.\"\n\nSo how you would test convexity is by making sure that for any two values\nx_1 and x_2 the line connecting the points (x_1,f(x_1)) and (x_2,f(x_2))\nlies above all other points (x,f(x)) for x  \\in {x_1, ...., x_2}. In our\ncase because f(0)=0 and f(2)=2r+b\nThe line connecting those two points is defined by l(x)=((2r+b)/2)*x =\n(r+b/2)*x which means that l(1)=r+b/2.\nhowever f(1) = r+ b which is larger than l(1) for any positive value of b.\n(Note that again with a zerobasefee b=0 we have f(1)=r=l(1) which would be\nsufficient)\n\nIf you implement the capacity scaling algorithm for a min-cost flow solver\nyou will see that the algorithm linearizes the convex costs all the time by\nswitching to a unit cost in each delta-scaling phase (as described in\nsection 9, 10.2 and 14.1 through 14.5 of the book [2] that we refer to in\nthe paper and that Stefan already mentioned to you). This seems very\nismilar to the thoughts you seemed to have in your mail (though even after\nreading them 3 times I can't verify how you turned the base_fee into a\nproportional term by switching to a unisized payment amount). You will also\nrecognize that the base fee is not linearizable as a unit cost and will\nbreak the reduced cost optimality criterium in the delta phases of the\nalgorithm. That being said in our mainnet tests we actually used 10k and\n100k sats as a fixed min unit size for htlcs in flow computation but more\nabout that in the already announced and soon to come mail about the mainnet\ntests.\n\nLong story short while the base fee yields indeed a problem for a flow to\nbe dissected into paths the issues seems to be much more severe because of\nthis non continious jump when going from f(0) to f(1).\n\nAll that being said I am very delighted to see that you propose a protocol\nchange towards \"whole flow payments\" (Please allow me to still call them\npayment flows as our paper title suggested). In my recent mail [0] I said I\nwould want to write another mail about  our mainnet test results and the\nthe consequences for the protocol, node operators, users etc... One of the\nconsequences goes fully along with the idea that you described here and\nthus I very much like / appreciate your idea / proposal! As the other mail\nwas too long anyway let me quickly copy and past the text about the 5\nconsequences for the protocol that I saw from my draft and note that the\nsecond one goes fully along with your proposal:\n\nConsequences for the Protocol\n========================\n\n1.) As we send larger amounts we confirmed what many people already knew:\nStuck and hanging HTLCs are becoming more of a problem. The main reason\nthat in our experiments we could not deliver the full amount was that we\n\"lost\" htlcs as they did neither arrive at the destination in time nor did\nthey return as errors to the sender quickly enough before the mpp timeout.\nWe very very much need a cancable payment mechanism [3]. I know taproot and\nPTLCs are coming but I think we should really emphasize on this aspect. I\nthink the focus is of particular interest as I think it is bad that the\ncurrent way of canceling an onion with the help of PTLCs can still lead to\nchannel failure on the remote channel where the PTLC is stuck and still\nbind the local liquidity for a long time potentially preventing the sender\nto conduct the full payment and also binding the liquidity of the residual\nflow.\n\n2.)  When dissecting a payment flow into several paths it can and might\nhappen that the amount `amt` sent through a remote channel will be actually\ncoming from several inbound channels of the sending node of that channel or\nbe going to several outbound channels of the receiving node of that\nchannel. To me it seems reasonable to adopt the transport of HTLCs (or when\nredesigning with PTLCs) to have a transport mechanism that is more\nreflecting this reality of flows. In particular I envision a new lightning\nmessage called `update_update_htlc` to compliment `update_add_htlc`. This\nmessage would allow to increase or reduce the amount of an HTLC committed /\noffered to a channel so that folks would not have to have parallel htlcs in\na channel that all belonged to the same payment. Maybe the channel factory\nideas on eltoo channels that  AJ Townes mentioned might be another\ndirection to achieve this. Also I believe similar ideas have been discussed\nwith link-AMP in the past.\n\n3.) Especially when optimizing for success probabilities I believe there is\na strong argument to solve min cost flows not for the amount to be paid but\nrather in a way that the expected value to be delivered matches the amount\nthat one wishes to send. We can achieve this by computing the optimal flow\nfor a larger amount and then send out redundant but cancable onions similar\nto the mechanism described in the Boomerang Paper [4] which I am happy to\nrecommend to look at again.\n\n4.) I want to recall that nodes probe the channels of their neighbors\nanyway while delivering the payment. If we had Friend of a Friend Balance\nsharing [5] together with a fee free rebalancing protocol we would reduce\nthe uncertainty around the sender and the recipient (to be included in\nblinded path routing hints in the invoices or offers) drastically and\nimproving the overall speed for larger payments to arrive because we first\nhad more information about the neighborhood of the sender and the\nrecipient. Additionally the prior probability changes from a uniform\ndistribution to a normal distribution as shown in figure 13 in the\nprobabilistic payment paper[6] which reduceses the necessary attempts again\nas shown in figure 14 of the same paper.\n\n5.) If we either believe it is the best to drop the base fee and/or if we\nsee that users stop using it we might want to think about depricating the\nbase fee also on a protocol level (or if I recall correctly as has been\ndone with the do not fragment bit / flag in IP) to reinterpret that the\nsemantics of the base fee.\n\nAs you can see when I was about to introduce this idea not because I was\nnot worried about paying the basefee several times in disection (I guess\nthe base fee will go away anyway) but just because for more practical\nreasons about HTLC space in commitment transactions (which I think is\nanother excellent reason) to think more about a transport layer that\nrespects flows as it also addresses the fixed cost issues (not in terms of\nsignatures but in terms of potential on chain fees and transaction size).\nThat being said I am not sure how easy it will be for us to create an onion\ntransport to actually communicate the flow but I was arguing that maybe\nwith a simple new Lightning message called `update_update_htlc` that would\nallow to update the amount of a given HTLC we could create something\nsimilar to local payment splitting and merging . In fact this could work in\nparticular if an HTLC of size X arrives at a channel that has less\nliquidity than X. The forwarding node could back propagate the value that\nwas passoible by lowering all upstream HTLCs to the liquidity the payer has\nin the downstream channel (though I haven't fully thought about all the\nconsequences with respect to security for this yet)\n\nwith kind regards Rene\n\n[0]:\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2021-August/003203.html\n[1]: https://en.wikipedia.org/wiki/Convex_function\n[2]: Network Flows: Theory, Algorithms, and Applications Buch von James B.\nOrlin, Ravindra K. Ahuja und Thomas L. Magnanti\n[3]:\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2019-June/002029.html\n[4]: https://arxiv.org/pdf/1910.01834.pdf\n[5]: https://github.com/lightningnetwork/lightning-rfc/pull/780\n[6]: https://arxiv.org/abs/2103.08576\n\n\nOn Mon, Aug 30, 2021 at 12:58 PM ZmnSCPxj via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> wrote:\n\n> Good morning list,\n>\n> It seems to me that the cost function defined in Pickhard-Richter\n> can in fact include a base fee:\n>\n>     -log(success_probability) + fudging_factor * amount * prop_fee +\n> fudging_factor * base_fee\n>\n> (Rant: why do mathists prefer single-symbol var names, in software\n> engineering there is a strong injunction **against** single-char\n> var names, with exceptions for simple numeric iteration loop vars,\n> I mean keeping track of the meaning of each single-symbol var name\n> takes up working memory space, which is fairly limited on a wetware\n> CPU, which is precisely why in software engineering there is an\n> injunction against single-char var names, srsly.)\n>\n> It seems to me that the above is \"convex\", as the `fudging_factor`\n> for a run will be constant, and the `base_fee` for the channel\n> would also be constant, and since it seems to me, naively, that\n> the paper defines \"convex\" as \"the second derivative >= 0 for all\n> `amount`\", and since the derivative of a constant is 0, the above\n> would still remain convex.\n> (I am not a mathist and this conjecture might be completely asinine.)\n>\n> So, it seems to me that the *real* reason for `#zerobasefee` is\n> not that the **mincostflow** algorithm cannot handle non-zero `base_fee`,\n> but rather, the **disect** phase afterwards cannot handle non-zero\n> `base_fee`.\n>\n> For example, suppose the minflowcost algorithm were instructed to\n> deliver 3,000msat from `S` to `D`, and returned the following flow:\n>\n>     S -->3000--> A ->1000-> B\n>                  |          |\n>                  |        1000\n>                  |          v\n>                  +-->2000-> C ->3000-> D\n>\n> In the \"disect\" phase afterwards, the above flow solution would have\n> to be split into two sub-payments, a 1000 sub-payment `S-A-B-C-D`\n> and a 2000 sub-payment `S-A-C-D`.\n>\n> However, this does mean that the base cost for `C->D` and `S->A` in\n> the above flow will pay *twice* the base cost than what the above\n> cost function would have computed, because in reality two independent\n> payments pass through those channels.\n>\n> Thus, the current Pickhardt-Richter scheme cannot work with non-zero\n> base fees if it were modified to consider fees, but not due to the\n> mincostflow algorithm, rather because due to the disect algorithm.\n>\n> ### Converting Non-Zero Base Fees To Proportional Fees\n>\n> An alternative solution would be to optimize for a *maximum* fee\n> rather than optimize for the *exact* fee.\n>\n> We can do this by virtually splitting up the entire payment into\n> smaller bunches of value, and asking mincostflow to solve individual\n> bunches rather than individual msats.\n>\n> For example, we can decide that the smallest practical HTLC would\n> be 1000 msat.\n> Let us call this the \"payment unit\", and the mincostflow algo\n> solves for paying in these payment units instead of 1msat units.\n> (Actual implementations would need to have some heuristic or\n> reasoned rule-of-thumb assumption on what the smallest practical\n> HTLC would be.)\n>\n> In our example, suppose we need to send 2001msat from `S` to `D`,\n> with the payment unit being 1000 msat.\n> Then we would actually have the mincostflow algorithm work to\n> deliver 3 payment units (3 * 1000msat) from `S` to `D`.\n>\n> Let us suppose that the mincostflow algorithm returns the following\n> flow:\n>\n>     S -->3-----> A ->1----> B\n>                  |          |\n>                  |        1 |\n>                  |          v\n>                  +-->2----> C ->3----> D\n>\n> Actually, what the mincostflow algorithm uses as a cost function\n> would be:\n>\n>     -log(success_probability) + fudging_factor * amount * prop_fee *\n> payment_unit + fudging_factor * base_fee * amount\n>\n>     == -log(success_probability) + fudging_factor * amount * (prop_fee *\n> payment_unit + base_fee)\n>\n>     where: amount is in units of 1000msat, our smallest practical HTLC.\n>            payment_unit is the unit, i.e. 1000msat.\n>\n> What the above means is that the mincostflow algorithm *allocates*\n> `3 * base_fee` for `C->D`, since the `amount` flowing through\n> `C->D` would be 3.\n> However, when we pass the above flow to the disect algorithm, it\n> would actually only split this into 2 sub-payments, so the\n> actual payment plan would only pay `2 * base_fee` for the\n> `C->D` leg on both sub-payments.\n>\n> In short, this effectively converts the base fee to a proportional\n> fee, removing the zerobasfee requirement imposed by the disect\n> algorithm.\n>\n> That is, the cost computed by the mincostflow algorithm is really a\n> maximum cost budget that the subsequent disect algorithm could later\n> spend.\n>\n> In effect, this converts the base fee to a proportional fee.\n>\n> This may be acceptable in practice.\n> This approximation has a bias against non-zerobasefee --- it would\n> treat those channels as being far more expensive than they actually\n> would end up being in an *actual* payment attempt --- but at least\n> does not *require* zerobasefee.\n> It would be able to still use non-zerobasefee channels if those are\n> absolutely required to reach the destination.\n>\n> This should at least help create a practical payment algorithm that\n> handles current LN with nonzerobasefee, and provide an impetus\n> towards making zerobasefee a reality.\n>\n> Note that this approximation would be non-optimal --- there may\n> be solutions that provide lower costs than the output of this\n> approximation would provide.\n>\n> It may be practical to have the payment unit be adjustable by a\n> higher-level (but still automated) process.\n> For example, it may be considered that having more than 500 splits\n> would be heuristically determined to be impractical, in which\n> case the payment unit could be the actual amount divided by 500\n> rounded up.\n> Then if that results in extreme overestimation of base costs\n> (i.e. the subsequent disect stage results in far fewer than 500\n> splits) the payment unit could be adjusted upwards.\n> Similarly, if that results in a payment plan that has high\n> base fees, the payment unit could be adjusted downwrds.\n>\n> --\n>\n> Similarly, having particular split amounts is helpful to reduce\n> the ability of intermediate nodes snooping on payment amounts,\n> so having a particular fixed payment unit may also be useful\n> nevertheless, even if this approximation is otherwise undesirable.\n>\n> That is, in the above example, we just send 3 sub-payments\n> instead of the minimum 2, so that every sub-payment is always\n> uniformly 1000msat.\n> This leaks the least amount of information to intermediate nodes,\n> especially after payment decorrelation is widely deployed.\n> (this is the same principle for why a uniform unit amount is\n> needed in practical CoinJoin implementations; it increases the\n> anonymity set of individual sub-payments.)\n>\n> This would be helped greatly by payment decorrelation;\n> intermediate nodes would remain uncertain that multiple\n> HTLCs with the same amount and the same in-channel and\n> out-channel are from the same overall payment or not.\n> Assuming many software uses the same payment unit consistently,\n> less information can be extracted by surveillors, and the\n> increased cost may be considered justifiable in paying for\n> additional privacy.\n>\n> --\n>\n> ### Whole-flow Payments\n>\n> Suppose we were to instead modify the LN protocol such that,\n> instead of MPP having individual sub-payments that split at the\n> source and join at the destination, we allow the source to\n> instruct arbitrary intermediate nodes to join and/or split\n> payments.\n>\n> That is, suppose LN allowed the source to indicate to arbitrary\n> forwarding nodes to split payments and join them.\n> (This would require nearly all nodes to upgrade, which may be\n> impractical, but let us consider its implications first without\n> committing to actually doing this.)\n>\n> A node that is a join node would be instructed to wait for all\n> incoming HTLCs to arrive before they create any outgoing\n> HTLCs.\n> A node that is a split node would make more than one outgoing\n> HTLCs.\n> Forwarding nodes can be both join and split nodes.\n>\n> This would allow the output of the mincostflow algorithm to be\n> used directly, without the problematic disect stage.\n> Nodes that have more than one in-flow would be join nodes,\n> while nodes with more than one out-flow would be split nodes.\n>\n> Returning to the example:\n>\n>     S -->3000--> A ->1000-> B\n>                  |          |\n>                  |        1000\n>                  |          v\n>                  +-->2000-> C ->3000-> D\n>\n> In the above, `S` sends a *single* payment out on channel\n> `S->A`, and instructs `A` to split the payment.\n> Then `S` also instructs `C` to join two payments, resulting\n> in a single HTLC out on `C->D` if `C` receives on both\n> `A->C` and `B->C`.\n>\n> The effect is that, for a single payment plan (presented as a\n> flow, outputted by a mincostflow algorithm) for each channel\n> with nonzero flow, there would only be one HTLC that gets paid\n> for using a single `base_fee`.\n>\n> Then, the base fee computed by the cost function would be exact\n> and there would be no underestimation of the base fee for\n> non-zerobasefee channels.\n>\n> Let us then consider the practicality of this scheme.\n>\n> If a hop were to fail to deliver, then some join nodes would\n> be left hanging, waiting for an incoming HTLC that will not\n> arrive.\n>\n> The failure would have to be handled by a split node, when it\n> receives a `update_fail_htlc`.\n> That split node cannot propagate the failure backwards to the\n> source, since it would still have other outgoing HTLCs in\n> play, which it cannot safely ignore; it can only propagate\n> the failure backwards if all the other outgoing HTLCs also\n> fail.\n>\n> To support this, a split node could give a request-to-fail\n> signal to its other outgoing HTLCs.\n> Then, any other join nodes still waiting for an incoming\n> HTLC (and would not yet have created any outgoing HTLCs,\n> or for the destination, would not have claimed any incoming\n> HTLCs) would fail all its incoming HTLCs.\n> Otherwise, an intermediate node receiving a request-to-fail\n> signal from any incoming HTLCs would simply propagate\n> this request-to-fail outwards to all its outgoing HTLCs.\n>\n> This implies that for this scheme, if *any* hop fails, the\n> entire payment fails and has to be restarted \"from the top\".\n>\n> This is in contrast with the current scheme of splitting\n> only at the source and joining only at the destination.\n> In the current LN, if *any* hop of an MPP fails, only the\n> sub-payment(s) going through that hop will need to be\n> retried.\n> Other sub-payment(s) can keep being \"in play\", and the\n> source, having all the information needed, only needs to\n> recompute the failing outgoing payments.\n>\n> On the other hand, the same information (particular hops are\n> failing) would be propagated back to the source, and the\n> source having to recompute the entire flow rather than just\n> some subset of the payments is not much worse computationally\n> (work is still dominated by `O((m*m+m*n)*log n)` where `m` is\n> the channels of the *entire network* and `n` is the nodes\n> of the *entire network*, amount is just a piddling little\n> `O(log(u))` term).\n> It also removes the need to virtually reduce the capacity of\n> intermediate hops while other sub-payments are in play\n> (however, a practical implementation would need to consider\n> the possibility of multiple *simultaneous* outgoing payments\n> anyway and would still retain this reduction of capacity in\n> case a new parallel outgoing payment is triggered while one\n> is still ongoing).\n>\n> This leads us to some fairly interesting choices:\n>\n> * [ ] Whole-flow payment is a brilliant idea and we should\n>   totally do it.  `#deletethezerobasefeedebate`\n> * [ ] Whole-flow payment is a horrible idea and we should\n>   totally not do it.\n>   * [ ] As we can see, the LN community needs to get together\n>     for better payment success, and supporting whole-flow\n>     payments is worse than setting `basefee=0`. `#zerobasefee`\n>   * [ ] As we can see, the current LN provides a vital service\n>     (partial failure of MPP) and intermediate nodes need to be\n>     compensated for this service.  `#nonzerobasefee`\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n\n\n-- \nhttps://www.rene-pickhardt.de\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20210830/add37bb8/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Handling nonzerobasefee when using Pickhard-Richter algo variants",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Ren\u00e9 Pickhardt",
                "Stefan Richter",
                "ZmnSCPxj"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 49075
        }
    }
]