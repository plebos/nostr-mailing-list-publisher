[
    {
        "title": "[Lightning-dev] [bitcoin-dev] Continuing the discussion about noinput / anyprevout",
        "thread_messages": [
            {
                "author": "Chris Stewart",
                "date": "2019-10-01T12:23:47",
                "message_text_only": "I do have some concerns about SIGHASH_NOINPUT, mainly that it does\nintroduce another footgun into the bitcoin protocol with address reuse.\nIt's common practice for bitcoin businesses to re-use addresses. Many\nexchanges [1] reuse addresses for cold storage with very large sums of\nmoney that is stored in these addreses.\n\nIt is my understanding with this part of BIP118\n\n>Using NOINPUT the input containing the signature no longer references a\nspecific output. Any participant can take a transaction and rewrite it by\nchanging the hash reference to the previous output, without invalidating\nthe signatures. This allows transactions to be bound to any output that\nmatches the value committed to in the witness and whose witnessProgram,\ncombined with the spending transaction's witness returns true.\n\nif an exchange were to once produce a digital signature from that cold\nstorage address with a SIGHASH_NOINPUT signature, that signature can be\nreplayed again and again on the blockchain until their wallet is drained.\nThis might be able to mitigated since the signatures commit to outputs,\nwhich may be small in value for the transaction that SIGHASH_NOINPUT was\nused. This means that an exchange could move coins from the address with a\nlarger transaction that spends money to a new output (and presumably pays a\nhigher fee than the smaller transactions).\n\n### Why does this matter?\n\nIt seems that SIGHASH_NOINPUT will be an extremely useful tool for offchain\nprotocols like Lightning. This gives us the building blocks for enforcing\nspecific offchain states to end up onchain [2].\n\nSince this tool is useful, we can presume that it will be integrated into\nthe signing path of large economic entities in bitcoin -- namely exchanges.\nMany exchanges have specific signing procedures for transactions that are\nleaving an exchange that is custom software. Now -- presuming wide adoption\nof off chain protocols -- they will need to have a _second unique signing\npath that uses SIGHASH_NOINPUT_.\n\nIt is imperative that this second signing path -- which uses\nSIGHASH_NOINPUT -- does NOT get mixed up with the first signing path that\ncontrols an exchanges onchain funds. If this were to happen, fund lost\ncould occur if the exchange is reusing address, which seems to be common\npractice.\n\nThis is stated here in BIP118:\n\n>This also means that particular care has to be taken in order to avoid\nunintentionally enabling this rebinding mechanism. NOINPUT MUST NOT be\nused, unless it is explicitly needed for the application, e.g., it MUST NOT\nbe a default signing flag in a wallet implementation. Rebinding is only\npossible when the outputs the transaction may bind to all use the same\npublic keys. Any public key that is used in a NOINPUT signature MUST only\nbe used for outputs that the input may bind to, and they MUST NOT be used\nfor transactions that the input may not bind to. For example an application\nSHOULD generate a new key-pair for the application instance using NOINPUT\nsignatures and MUST NOT reuse them afterwards.\n\nThis means we need to encourage onchain hot wallet signing procedures to be\nkept separate from offchain hot wallet signing procedures, which introduces\nmore complexity for key management (two keychains).\n\nOne (of the few) upsides of the current Lightning penalty mechanism is that\nfund loss can be contained to balance of the channel. You cannot do\nsomething in the current protocol that will effect your funds outside of\nthat channel. With SIGHASH_NOINPUT, that property changes.\n\n### A side note\nIn general, i think we should start disallowing uses of the SIGHASH\nprotocols that have unexpected behavior. The classic example of this is\nSIGHASH_SINGLE [3]. I get uneasy about adding more footguns to the\nprotocol, which with current network behavior (address re-use)\nSIGHASH_NOINPUT would be a big one.\n\n\n[1] - https://bitinfocharts.com/top-100-richest-bitcoin-addresses.html\n[2] -\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2019-September/002136.html\n[3] -\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-May/016048.html\n\nOn Mon, Sep 30, 2019 at 9:24 AM Christian Decker via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> With the recently renewed interest in eltoo, a proof-of-concept\n> implementation\n> [1], and the discussions regarding clean abstractions for off-chain\n> protocols\n> [2,3], I thought it might be time to revisit the `sighash_noinput` proposal\n> (BIP-118 [4]), and AJ's `bip-anyprevout` proposal [5].\n>\n> (sorry for the long e-mail. I wanted to give enough context and describe\n> the\n> various tradeoffs so people don't have to stitch them together from\n> memory. If\n> you're impatient there are a couple of open questions at the bottom)\n>\n> Both proposals are ways to allow rebinding of transactions to new outputs,\n> by\n> adding a sighash flag that excludes the output when signing. This allows\n> the\n> transaction to be bound to any output, without needing a new signature, as\n> long as output script and input script are compatible, e.g., the signature\n> matches the public key specified in the output.\n>\n> BIP-118 is limited to explaining the details of signature verification, and\n> omits anything related to deployment and dependency on other proposals.\n> This\n> was done in order not to depend on bip-taproot which is also in draft-phase\n> currently, and to allow deployment alongside the next version of segwit\n> script. `bip-anyprevout` builds on top of BIP-118, adding integration with\n> `bip-taproot`, chaperone signatures, limits the use of the sighash flag to\n> script path spends, as well as a new pubkey serialization which uses the\n> first\n> byte to signal opt-in.\n>\n> I'd like to stress that both proposals are complementary and not competing,\n> which is something that I've heard a couple of times.\n>\n> There remain a couple of unclear points which I hope we can address in the\n> coming days, to get this thing moving again, and hopefully get a new tool\n> in\n> our toolbox soon(ish).\n>\n> In the following I will quote a couple of things that were discussed during\n> the CoreDev meeting earlier this year, but not everybody could join, and\n> it is\n> important that we engage the wider community, to get a better picture, and\n> I\n> think not everybody is up-to-date about the current state.\n>\n>\n> ## Dangers of `sighash_noinput`\n>\n> An argument I have heard against noinput is that it is slightly less\n> complex\n> or compute intensive than `sighash_all` signatures, which may encourage\n> wallet\n> creators to only implement the noinput variant, and use it indiscrimi-\n> nately. This is certainly a good argument, and indeed we have seen at least\n> one developer proposing to use noinput for all transactions to discourage\n> address reuse.\n>\n> This was also mentioned at CoreDev [6]:\n>\n> > When [...] said he wanted to write a wallet that only used\n> SIGHASH\\_NOINPUT,\n> > that was pause for concern. Some people might want to use\n> SIGHASH\\_NOINPUT as a\n> > way to cheapen or reduce the complexity of making a wallet\n> > implementation. SIGHASH\\_NOINPUT is from a purely procedural point of\n> view\n> > easier than doing a SIGHASH\\_ALL, that's all I'm saying. So you're\n> hashing\n> > less. It's way faster. That concern has been brought to my attention and\n> it's\n> > something I can see. Do we want to avoid people being stupid and shooting\n> > themselves and their customers in the foot? Or do we treat this as a\n> special\n> > case where you mark we're aware of how it should be used and we just try\n> to\n> > get that awareness out?\n>\n> Another issue that is sometimes brought up is that an external user may\n> attempt to send funds to a script that was really part of a higher-level\n> protocol. This leads to those funds becoming inaccessible unless you gather\n> all the participants and sign off on those funds. I don't believe this is\n> anything new, and if users really want to shoot themselves in the foot and\n> send funds to random addresses they fish out of a blockexplorer there's\n> little\n> we can do. What we could do is make the scripts used internally in our\n> protocols unaddressable (see output tagging below), removing this issue\n> altogether.\n>\n>\n> ## Chaperone signatures\n>\n> Chaperone signatures are signatures that ensure that there is no\n> third-party\n> malleability of transactions. The idea is to have an additional signature,\n> that doesn't use noinput, or any of its variants, and therefore needs to be\n> authored by one of the pubkeys in the output script, i.e., one or more of\n> the\n> participants of the contract the transaction belongs to. Concretely in\n> eltoo\n> we'd be using a shared key known to all participants in the eltoo\n> instance, so\n> any participant can sign an update to rebind it to the desired output.\n>\n> Chaperone signatures have a number of downsides however:\n>\n> -   Additional size: both the public key and the signature actually need\n> to be\n>     stored along with the real noinput signature, resulting in transfer,\n>     computational and storage overhead. We can't reuse the same pubkey\n> from the\n>     noinput signature since that'd require access to the matching privkey\n> which\n>     is what we want to get rid of using noinput in the first place.\n> -   Protocols can still simply use a globally known privkey, voiding the\n>     benefit of chaperone signatures, since third-parties can sign again. I\n>     argue that third-party malleability is a subset of first-party\n>     malleability, and we should protect against first-party malleability\n> first\n>     and foremost. My counterparty has the incentive to trick me, a\n> third-party\n>     may not.\n>\n> On the plus side chaperone signatures certainly address the lazy-wallet-dev\n> scenario, and as AJ points out in [bip-anyprevout] we get back the same\n> security guarantees as we had without noinput.\n>\n> From what I remember and the transcript (thanks Kanzure for your awesome\n> work\n> by the way), there was no strong support for chaperone signatures during\n> the\n> meeting [6], but feedback from people that were not present is needed:\n>\n> > if everyone who wanted to use NOINPUT was convinced there was a problem,\n> then\n> > they would pick the right thing, but clearly people aren't. It's not a\n> > foot-gun defense mechanism because it's easily bypassed, and it's easier\n> to\n> > bypass it than to use it. Whereas for tagged outputs, it's that if you\n> want\n> > any NOINPUT then you must tag.\n>\n>\n> ## Output tagging\n>\n> One proposal that I found rather fascinating during the discussion in\n> Amsterdam was that we could achieve the same disincentive to use on\n> non-smart-contract cases by simply making the output scripts\n> unaddressable. This can be done by specifying a version of taproot outputs\n> for\n> which the bech32 addressing scheme simply doesn't have a representation\n> [6]:\n>\n> > The tagged outputs idea is that we don't have NOINPUT ANYPREVOUT\n> supported for\n> > taproot v1 outputs, instead we have a segwit version 16 v16 that supports\n> > taproot. The reason for v16 is that we redefine bech32 to not cover\n> > v16. There's no addresses for this type of output. If you're an exchange\n> and\n> > receive a bech32 address, you declare it invalid. You make it less user\n> > friendly here; and there shouldn't be an address anyway. You might want\n> to see\n> > it on a block explorer, but you don't want to pass it around to anyone.\n>\n> We don't need addresses in our contract constructions because we deal\n> directly\n> with the scripts. This would also have the desired effect of no allowing\n> generic wallets to send to these addresses, or users accidentally sending\n> funds to what was supposed to be a one-off script used internally in the\n> off-chain contract.\n>\n> Notice that this idea was already used by Russell O'Connor when performing\n> a\n> transaction on elements using his new scripting language simplicity\n> [7]:\n>\n> > For this experimental development, we created an improper segwit version,\n> > \"version 31\" for Simplicity addresses. The payload of this segwit\n> version 31\n> > address contains a commitment Merkle root of a Simplicity program to\n> control\n> > the UTXO.\n>\n> The concern with output tagging is that it hurts fungibility, marking\n> outputs\n> used in a contract as such and making them identifiable. But maybe it\n> would be\n> a good idea to create two domains anyway: one for user-addressable\n> destinations which users can use with their general purpose wallets, and\n> one\n> domain for contracts, which users cannot send to directly.\n>\n> This also came up during the CoreDev meeting [ams-coredev]:\n>\n> > these sort of NOINPUT signatures are only things that are within some\n> > application or within some protocol that gets negotiated between\n> participants,\n> > but they don't cross-independent domains where you see a wallet or a\n> protocol\n> > as a kind of domain. You can't tell the difference, is this an address I\n> can\n> > give to someone else or not? It's all scripts, no real addresses. There\n> are\n> > types of outputs that are completely insecure unconditionally; there are\n> > things that are protected and I can give to anyone, you don't want to\n> reuse\n> > it, but there's no security issue from doing so. This is an additional\n> class\n> > that is secure perfectly but only when used in the right way.\n>\n>\n> ## Open questions\n>\n> The questions that remain to be addressed are the following:\n>\n> 1.  General agreement on the usefulness of noinput / anyprevoutanyscript /\n>     anyprevout. While at the CoreDev meeting I think everybody agreed that\n>     these proposals a useful, also beyond eltoo, not everybody could be\n>     there. I'd therefore like to elicit some feedback from the wider\n> community.\n> 2.  Is there strong support or opposition to the chaperone signatures\n>     introduced in anyprevout / anyprevoutanyscript? I think it'd be best to\n>     formulate a concrete set of pros and contras, rather than talk about\n>     abstract dangers or advantages.\n> 3.  The same for output tagging / explicit opt-in. What are the advantages\n> and\n>     disadvantages?\n> 4.  Shall we merge BIP-118 and bip-anyprevout. This would likely reduce the\n>     confusion and make for simpler discussions in the end.\n> 5.  Anything I forgot to mention :-)\n>\n> Cheers,\n> Christian\n>\n> [1] <\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-September/002131.html\n> >\n> [2] <\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-September/017285.html\n> >\n> [3] <\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2018-August/001383.html\n> >\n> [4] <https://github.com/bitcoin/bips/blob/master/bip-0118.mediawiki>\n> [5] <\n> https://github.com/ajtowns/bips/blob/bip-anyprevout/bip-anyprevout.mediawiki\n> >\n> [6] <\n> http://diyhpl.us/wiki/transcripts/bitcoin-core-dev-tech/2019-06-06-noinput-etc/\n> >\n> [7] <https://lists.ozlabs.org/pipermail/simplicity/2019/000018.html>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191001/6260f1ba/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-01T13:31:49",
                "message_text_only": "Good morning lists,\n\nLet me summarize concerns brought up:\n\n* Chris concern, is that an ordinary UTXO that is not allocated for `SIGHASH_NOINPUT` use, is inadvertently spent using `SIGHASH_NOINPUT`.\n* My concern, is that unless a UTXO allocated for `SIGHASH_NOINPUT` use, is *indeed* used with SIGHASH_NOINPUT`, it should look exactly the same as any other SegWit v1 output.\n\nI propose the below instead:\n\n* Do ***NOT*** allocate SegWit v16 for `SIGHASH_NOINPUT`.\n* Instead, allocate SegWit v1 Tapscript v16 for `SIGHASH_NOINPUT`.\n\nThen, on usage:\n\n* Exchange hoards can be protected by simple MuSig bip-schnorr SegWit v1 outputs, or a NUMS Taproot internal point with a MAST branch Tapscript v0 `OP_CHECKSIG_ADD` sequence.\n* Decker-Russell-Osuntokun constructions are backed by a n-of-n MuSig Taproot internal point, with a MAST branch containing a Tapscript v16 with `OP_1 OP_CHECKSIG`.\n\nThis solves both concerns:\n\n* Ordinary UTXOs not allocated for `SIGHASH_NOINPUT` use simply do not commit to any Taproot that has a Tapscript v16 branch, and thus `SIGHASH_NOINPUT` is unuseable to claim it.\n* If a UTXO used for an offchain protocol ends up in a cooperative-resolution state, nobody has to know that a Tapscript v16 branch existed that could have used `SIGHASH_NOINPUT`.\n\nAgain, my objection to output tagging is that it is **publicly visible** as soon as the funding transaction is confirmed onchain that this is a special output used for a Decker-Russell-Osuntokun construction, greatly damaging privacy.\nBut if this fact is kept secret *unless* the very specific case of unilateral uncooperative enforcement, then it is quite fine with me.\n\nWould this alternate proposal hold better muster?\n\nRegards,\nZmnSCPxj\n\n\n\n> I do have some concerns about SIGHASH_NOINPUT, mainly that it does introduce another footgun into the bitcoin protocol with address reuse. It's common practice for bitcoin businesses to re-use addresses. Many exchanges [1] reuse addresses for cold storage with very large sums of money that is stored in these addreses.\n>\n> It is my understanding with this part of BIP118\n>\n> >Using NOINPUT the input containing the signature no longer references a specific output. Any participant can take a transaction and rewrite it by changing the hash reference to the previous output, without invalidating the signatures. This allows transactions to be bound to any output that matches the value committed to in the witness and whose witnessProgram, combined with the spending transaction's witness returns true.\n>\n> if an exchange were to once produce a digital signature from that cold storage address with a SIGHASH_NOINPUT signature, that signature can be replayed again and again on the blockchain until their wallet is drained. This might be able to mitigated since the signatures commit to outputs, which may be small in value for the transaction that SIGHASH_NOINPUT was used. This means that an exchange could move coins from the address with a larger transaction that spends money to a new output (and presumably pays a higher fee than the smaller transactions).\n>\n> ### Why does this matter?\n>\n> It seems that SIGHASH_NOINPUT will be an extremely useful tool for offchain protocols like Lightning. This gives us the building blocks for enforcing specific offchain states to end up onchain [2].\n>\n> Since this tool is useful, we can presume that it will be integrated into the signing path of large economic entities in bitcoin -- namely exchanges. Many exchanges have specific signing procedures for transactions that are leaving an exchange that is custom software. Now -- presuming wide adoption of off chain protocols -- they will need to have a _second unique signing path that uses SIGHASH_NOINPUT_.\n>\n> It is imperative that this second signing path -- which uses SIGHASH_NOINPUT -- does NOT get mixed up with the first signing path that controls an exchanges onchain funds. If this were to happen, fund lost could occur if the exchange is reusing address, which seems to be common practice.\n>\n> This is stated here in BIP118:\n>\n> >This also means that particular care has to be taken in order to avoid unintentionally enabling this rebinding mechanism. NOINPUT MUST NOT be used, unless it is explicitly needed for the application, e.g., it MUST NOT be a default signing flag in a wallet implementation. Rebinding is only possible when the outputs the transaction may bind to all use the same public keys. Any public key that is used in a NOINPUT signature MUST only be used for outputs that the input may bind to, and they MUST NOT be used for transactions that the input may not bind to. For example an application SHOULD generate a new key-pair for the application instance using NOINPUT signatures and MUST NOT reuse them afterwards.\n>\n> This means we need to encourage onchain hot wallet signing procedures to be kept separate from offchain hot wallet signing procedures, which introduces more complexity for key management (two keychains).\n>\n> One (of the few) upsides of the current Lightning penalty mechanism is that fund loss can be contained to balance of the channel. You cannot do something in the current protocol that will effect your funds outside of that channel. With SIGHASH_NOINPUT, that property changes.\n>\n> ### A side note\n> In general, i think we should start disallowing uses of the SIGHASH protocols that have unexpected behavior. The classic example of this is SIGHASH_SINGLE [3]. I get uneasy about adding more footguns to the protocol, which with current network behavior (address re-use) SIGHASH_NOINPUT would be a big one.\n>\n> [1] - https://bitinfocharts.com/top-100-richest-bitcoin-addresses.html\n> [2] - https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-September/002136.html\n> [3] - https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-May/016048.html\n>\n> On Mon, Sep 30, 2019 at 9:24 AM Christian Decker via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> > With the recently renewed interest in eltoo, a proof-of-concept implementation\n> > [1], and the discussions regarding clean abstractions for off-chain protocols\n> > [2,3], I thought it might be time to revisit the `sighash_noinput` proposal\n> > (BIP-118 [4]), and AJ's `bip-anyprevout` proposal [5].\n> >\n> > (sorry for the long e-mail. I wanted to give enough context and describe the\n> > various tradeoffs so people don't have to stitch them together from memory. If\n> > you're impatient there are a couple of open questions at the bottom)\n> >\n> > Both proposals are ways to allow rebinding of transactions to new outputs, by\n> > adding a sighash flag that excludes the output when signing. This allows the\n> > transaction to be bound to any output, without needing a new signature, as\n> > long as output script and input script are compatible, e.g., the signature\n> > matches the public key specified in the output.\n> >\n> > BIP-118 is limited to explaining the details of signature verification, and\n> > omits anything related to deployment and dependency on other proposals. This\n> > was done in order not to depend on bip-taproot which is also in draft-phase\n> > currently, and to allow deployment alongside the next version of segwit\n> > script. `bip-anyprevout` builds on top of BIP-118, adding integration with\n> > `bip-taproot`, chaperone signatures, limits the use of the sighash flag to\n> > script path spends, as well as a new pubkey serialization which uses the first\n> > byte to signal opt-in.\n> >\n> > I'd like to stress that both proposals are complementary and not competing,\n> > which is something that I've heard a couple of times.\n> >\n> > There remain a couple of unclear points which I hope we can address in the\n> > coming days, to get this thing moving again, and hopefully get a new tool in\n> > our toolbox soon(ish).\n> >\n> > In the following I will quote a couple of things that were discussed during\n> > the CoreDev meeting earlier this year, but not everybody could join, and it is\n> > important that we engage the wider community, to get a better picture, and I\n> > think not everybody is up-to-date about the current state.\n> >\n> > ## Dangers of `sighash_noinput`\n> >\n> > An argument I have heard against noinput is that it is slightly less complex\n> > or compute intensive than `sighash_all` signatures, which may encourage wallet\n> > creators to only implement the noinput variant, and use it indiscrimi-\n> > nately. This is certainly a good argument, and indeed we have seen at least\n> > one developer proposing to use noinput for all transactions to discourage\n> > address reuse.\n> >\n> > This was also mentioned at CoreDev [6]:\n> >\n> > > When [...] said he wanted to write a wallet that only used SIGHASH\\_NOINPUT,\n> > > that was pause for concern. Some people might want to use SIGHASH\\_NOINPUT as a\n> > > way to cheapen or reduce the complexity of making a wallet\n> > > implementation. SIGHASH\\_NOINPUT is from a purely procedural point of view\n> > > easier than doing a SIGHASH\\_ALL, that's all I'm saying. So you're hashing\n> > > less. It's way faster. That concern has been brought to my attention and it's\n> > > something I can see. Do we want to avoid people being stupid and shooting\n> > > themselves and their customers in the foot? Or do we treat this as a special\n> > > case where you mark we're aware of how it should be used and we just try to\n> > > get that awareness out?\n> >\n> > Another issue that is sometimes brought up is that an external user may\n> > attempt to send funds to a script that was really part of a higher-level\n> > protocol. This leads to those funds becoming inaccessible unless you gather\n> > all the participants and sign off on those funds. I don't believe this is\n> > anything new, and if users really want to shoot themselves in the foot and\n> > send funds to random addresses they fish out of a blockexplorer there's little\n> > we can do. What we could do is make the scripts used internally in our\n> > protocols unaddressable (see output tagging below), removing this issue\n> > altogether.\n> >\n> > ## Chaperone signatures\n> >\n> > Chaperone signatures are signatures that ensure that there is no third-party\n> > malleability of transactions. The idea is to have an additional signature,\n> > that doesn't use noinput, or any of its variants, and therefore needs to be\n> > authored by one of the pubkeys in the output script, i.e., one or more of the\n> > participants of the contract the transaction belongs to. Concretely in eltoo\n> > we'd be using a shared key known to all participants in the eltoo instance, so\n> > any participant can sign an update to rebind it to the desired output.\n> >\n> > Chaperone signatures have a number of downsides however:\n> >\n> > -   Additional size: both the public key and the signature actually need to be\n> >     stored along with the real noinput signature, resulting in transfer,\n> >     computational and storage overhead. We can't reuse the same pubkey from the\n> >     noinput signature since that'd require access to the matching privkey which\n> >     is what we want to get rid of using noinput in the first place.\n> > -   Protocols can still simply use a globally known privkey, voiding the\n> >     benefit of chaperone signatures, since third-parties can sign again. I\n> >     argue that third-party malleability is a subset of first-party\n> >     malleability, and we should protect against first-party malleability first\n> >     and foremost. My counterparty has the incentive to trick me, a third-party\n> >     may not.\n> >\n> > On the plus side chaperone signatures certainly address the lazy-wallet-dev\n> > scenario, and as AJ points out in [bip-anyprevout] we get back the same\n> > security guarantees as we had without noinput.\n> >\n> > From what I remember and the transcript (thanks Kanzure for your awesome work\n> > by the way), there was no strong support for chaperone signatures during the\n> > meeting [6], but feedback from people that were not present is needed:\n> >\n> > > if everyone who wanted to use NOINPUT was convinced there was a problem, then\n> > > they would pick the right thing, but clearly people aren't. It's not a\n> > > foot-gun defense mechanism because it's easily bypassed, and it's easier to\n> > > bypass it than to use it. Whereas for tagged outputs, it's that if you want\n> > > any NOINPUT then you must tag.\n> >\n> > ## Output tagging\n> >\n> > One proposal that I found rather fascinating during the discussion in\n> > Amsterdam was that we could achieve the same disincentive to use on\n> > non-smart-contract cases by simply making the output scripts\n> > unaddressable. This can be done by specifying a version of taproot outputs for\n> > which the bech32 addressing scheme simply doesn't have a representation [6]:\n> >\n> > > The tagged outputs idea is that we don't have NOINPUT ANYPREVOUT supported for\n> > > taproot v1 outputs, instead we have a segwit version 16 v16 that supports\n> > > taproot. The reason for v16 is that we redefine bech32 to not cover\n> > > v16. There's no addresses for this type of output. If you're an exchange and\n> > > receive a bech32 address, you declare it invalid. You make it less user\n> > > friendly here; and there shouldn't be an address anyway. You might want to see\n> > > it on a block explorer, but you don't want to pass it around to anyone.\n> >\n> > We don't need addresses in our contract constructions because we deal directly\n> > with the scripts. This would also have the desired effect of no allowing\n> > generic wallets to send to these addresses, or users accidentally sending\n> > funds to what was supposed to be a one-off script used internally in the\n> > off-chain contract.\n> >\n> > Notice that this idea was already used by Russell O'Connor when performing a\n> > transaction on elements using his new scripting language simplicity\n> > [7]:\n> >\n> > > For this experimental development, we created an improper segwit version,\n> > > \"version 31\" for Simplicity addresses. The payload of this segwit version 31\n> > > address contains a commitment Merkle root of a Simplicity program to control\n> > > the UTXO.\n> >\n> > The concern with output tagging is that it hurts fungibility, marking outputs\n> > used in a contract as such and making them identifiable. But maybe it would be\n> > a good idea to create two domains anyway: one for user-addressable\n> > destinations which users can use with their general purpose wallets, and one\n> > domain for contracts, which users cannot send to directly.\n> >\n> > This also came up during the CoreDev meeting [ams-coredev]:\n> >\n> > > these sort of NOINPUT signatures are only things that are within some\n> > > application or within some protocol that gets negotiated between participants,\n> > > but they don't cross-independent domains where you see a wallet or a protocol\n> > > as a kind of domain. You can't tell the difference, is this an address I can\n> > > give to someone else or not? It's all scripts, no real addresses. There are\n> > > types of outputs that are completely insecure unconditionally; there are\n> > > things that are protected and I can give to anyone, you don't want to reuse\n> > > it, but there's no security issue from doing so. This is an additional class\n> > > that is secure perfectly but only when used in the right way.\n> >\n> > ## Open questions\n> >\n> > The questions that remain to be addressed are the following:\n> >\n> > 1.  General agreement on the usefulness of noinput / anyprevoutanyscript /\n> >     anyprevout. While at the CoreDev meeting I think everybody agreed that\n> >     these proposals a useful, also beyond eltoo, not everybody could be\n> >     there. I'd therefore like to elicit some feedback from the wider community.\n> > 2.  Is there strong support or opposition to the chaperone signatures\n> >     introduced in anyprevout / anyprevoutanyscript? I think it'd be best to\n> >     formulate a concrete set of pros and contras, rather than talk about\n> >     abstract dangers or advantages.\n> > 3.  The same for output tagging / explicit opt-in. What are the advantages and\n> >     disadvantages?\n> > 4.  Shall we merge BIP-118 and bip-anyprevout. This would likely reduce the\n> >     confusion and make for simpler discussions in the end.\n> > 5.  Anything I forgot to mention :-)\n> >\n> > Cheers,\n> > Christian\n> >\n> > [1] <https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-September/002131.html>\n> > [2] <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-September/017285.html>\n> > [3] <https://lists.linuxfoundation.org/pipermail/lightning-dev/2018-August/001383.html>\n> > [4] <https://github.com/bitcoin/bips/blob/master/bip-0118.mediawiki>\n> > [5] <https://github.com/ajtowns/bips/blob/bip-anyprevout/bip-anyprevout.mediawiki>\n> > [6] <http://diyhpl.us/wiki/transcripts/bitcoin-core-dev-tech/2019-06-06-noinput-etc/>\n> > [7] <https://lists.ozlabs.org/pipermail/simplicity/2019/000018.html>\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Christian Decker",
                "date": "2019-10-03T10:01:58",
                "message_text_only": "ZmnSCPxj via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> writes:\n\n> Good morning lists,\n>\n> Let me summarize concerns brought up:\n>\n> * Chris concern, is that an ordinary UTXO that is not allocated for `SIGHASH_NOINPUT` use, is inadvertently spent using `SIGHASH_NOINPUT`.\n> * My concern, is that unless a UTXO allocated for `SIGHASH_NOINPUT` use, is *indeed* used with SIGHASH_NOINPUT`, it should look exactly the same as any other SegWit v1 output.\n>\n> I propose the below instead:\n>\n> * Do ***NOT*** allocate SegWit v16 for `SIGHASH_NOINPUT`.\n> * Instead, allocate SegWit v1 Tapscript v16 for `SIGHASH_NOINPUT`.\n>\n> Then, on usage:\n>\n> * Exchange hoards can be protected by simple MuSig bip-schnorr SegWit v1 outputs, or a NUMS Taproot internal point with a MAST branch Tapscript v0 `OP_CHECKSIG_ADD` sequence.\n> * Decker-Russell-Osuntokun constructions are backed by a n-of-n MuSig Taproot internal point, with a MAST branch containing a Tapscript v16 with `OP_1 OP_CHECKSIG`.\n>\n> This solves both concerns:\n>\n> * Ordinary UTXOs not allocated for `SIGHASH_NOINPUT` use simply do not commit to any Taproot that has a Tapscript v16 branch, and thus `SIGHASH_NOINPUT` is unuseable to claim it.\n> * If a UTXO used for an offchain protocol ends up in a cooperative-resolution state, nobody has to know that a Tapscript v16 branch existed that could have used `SIGHASH_NOINPUT`.\n>\n> Again, my objection to output tagging is that it is **publicly visible** as soon as the funding transaction is confirmed onchain that this is a special output used for a Decker-Russell-Osuntokun construction, greatly damaging privacy.\n> But if this fact is kept secret *unless* the very specific case of unilateral uncooperative enforcement, then it is quite fine with me.\n>\n> Would this alternate proposal hold better muster?\n\nIntriguing idea, this would be an invisible tagging, since the opt-in to\nnoinput and friends is hidden inside the committed script, which only\ngets revealed whenever we actually need it.\n\nFor eltoo this would mean that the funding output would be invisibly\ntagged, and the cooperative close would use the taproot pubkey, while\nthe uncooperative close, which would require noinput opt-in, reveals the\nscript, proving prior opt-in, and provides a matching signature.\n\nIf I'm not mistaken this would require AJ's alternative pubkey encoding\n(0x01 or 0x00 prefixed pubkey) to make the opt-in visible, correct?"
            },
            {
                "author": "Christian Decker",
                "date": "2019-10-03T09:57:05",
                "message_text_only": "Chris Stewart <chris at suredbits.com> writes:\n\n> I do have some concerns about SIGHASH_NOINPUT, mainly that it does\n> introduce another footgun into the bitcoin protocol with address reuse.\n> It's common practice for bitcoin businesses to re-use addresses. Many\n> exchanges [1] reuse addresses for cold storage with very large sums of\n> money that is stored in these addreses.\n>\n> It is my understanding with this part of BIP118\n>\n>>Using NOINPUT the input containing the signature no longer references a\n> specific output. Any participant can take a transaction and rewrite it by\n> changing the hash reference to the previous output, without invalidating\n> the signatures. This allows transactions to be bound to any output that\n> matches the value committed to in the witness and whose witnessProgram,\n> combined with the spending transaction's witness returns true.\n>\n> if an exchange were to once produce a digital signature from that cold\n> storage address with a SIGHASH_NOINPUT signature, that signature can be\n> replayed again and again on the blockchain until their wallet is drained.\n> This might be able to mitigated since the signatures commit to outputs,\n> which may be small in value for the transaction that SIGHASH_NOINPUT was\n> used. This means that an exchange could move coins from the address with a\n> larger transaction that spends money to a new output (and presumably pays a\n> higher fee than the smaller transactions).\n\nThanks for sharing your concerns Chris, I do agree that noinput and\nfriends are a very sharp knife that needs to be treated carefully, but\nultimately it's exactly its sharpness that makes it useful :-)\n\n> ### Why does this matter?\n>\n> It seems that SIGHASH_NOINPUT will be an extremely useful tool for offchain\n> protocols like Lightning. This gives us the building blocks for enforcing\n> specific offchain states to end up onchain [2].\n>\n> Since this tool is useful, we can presume that it will be integrated into\n> the signing path of large economic entities in bitcoin -- namely exchanges.\n> Many exchanges have specific signing procedures for transactions that are\n> leaving an exchange that is custom software. Now -- presuming wide adoption\n> of off chain protocols -- they will need to have a _second unique signing\n> path that uses SIGHASH_NOINPUT_.\n>\n> It is imperative that this second signing path -- which uses\n> SIGHASH_NOINPUT -- does NOT get mixed up with the first signing path that\n> controls an exchanges onchain funds. If this were to happen, fund lost\n> could occur if the exchange is reusing address, which seems to be common\n> practice.\n\nTotally agreed, and as you point out, BIP118 is careful to mandate\nseparate private keys be used for off-chain contracts and that the\noff-chain contract never be mixed with the remainder of your funds. The\nway eltoo uses noinput we selectively open us up to replay attacks\n(because that's what the update mechanism is after all) by controlling\nthe way the transactions can be replayed very carefully, and any other\nuse of noinput would need to make sure to have the same guarantees.\nHowever, once we have separated the two domains, we can simply use a\nseparate (hardened) derivation path from a seed key, and never mix them\nafterwards. We never exchange any private keys, so even leaking info\nacross derived keys is not an issue here.\n\n> This is stated here in BIP118:\n>\n>>This also means that particular care has to be taken in order to avoid\n> unintentionally enabling this rebinding mechanism. NOINPUT MUST NOT be\n> used, unless it is explicitly needed for the application, e.g., it MUST NOT\n> be a default signing flag in a wallet implementation. Rebinding is only\n> possible when the outputs the transaction may bind to all use the same\n> public keys. Any public key that is used in a NOINPUT signature MUST only\n> be used for outputs that the input may bind to, and they MUST NOT be used\n> for transactions that the input may not bind to. For example an application\n> SHOULD generate a new key-pair for the application instance using NOINPUT\n> signatures and MUST NOT reuse them afterwards.\n>\n> This means we need to encourage onchain hot wallet signing procedures to be\n> kept separate from offchain hot wallet signing procedures, which introduces\n> more complexity for key management (two keychains).\n\nThis is already the case: off-chain systems always require access to the\nsigning key in real-time in order to be useful. If any state change is\nperformed in a channel, even just adjusting fees or receiving a payment,\nrequires the signature from the key associated with the channel. With\nhigh security on-chain systems on the other hand you should never have a\nhot key that automatically signs off on transfers without human\nintervention. So I find it unlikely that mandating the on-chain keys to\nbe kept separate from off-chain keys is any harder than what should be\ndone with the current systems.\n\n> One (of the few) upsides of the current Lightning penalty mechanism is that\n> fund loss can be contained to balance of the channel. You cannot do\n> something in the current protocol that will effect your funds outside of\n> that channel. With SIGHASH_NOINPUT, that property changes.\n\nGood point, but if the key hygiene is maintained as detailed in BIP118,\ni.e., off-chain keys must be kept separate from on-chain keys, and that\neach off-chain contract instance uses a separate set of keys, that\nproperty is maintained.\n\nRegards,\nChristian"
            },
            {
                "author": "Christian Decker",
                "date": "2019-10-01T14:20:25",
                "message_text_only": "ZmnSCPxj <ZmnSCPxj at protonmail.com> writes:\n> I rather strongly oppose output tagging.\n>\n> The entire point of for example Taproot was to reduce the variability\n> of how outputs look like, so that unspent Taproot outputs look exactly\n> like other unspent Taproot outputs regardless of the SCRIPT (or lack\n> of SCRIPT) used to protect the outputs.  That is the reason why we\n> would prefer to not support P2SH-wrapped Taproot even though\n> P2SH-wrapping was intended to cover all future uses of SegWit,\n> including SegWit v1 that Taproot will eventually get.\n\nThat is a bit reductive if you ask me. Taproot brings a number of\nimprovements such as the reduction of on-chain footprint in the\ncollaborative spend case, the hiding of complex logic in that case, and\nyes, the uniformity of UTXOs that you mentioned. I do agree that it'd be\nto make everything look identical to the outside observer, but saying\nthat separating outputs into two coarse-grained domains is equivalent to\nthrowing the baby out with the bath-water :-)\n\nThat being said, I should clarify that I would prefer not having to make\nspecial accomodations on top of the raw sighash_noinput proposal, for\nsome perceived, but abstract danger that someone might shoot themselves\nin the foot. I think we're all old enough not to need too much\nhandholding :-)\n\nOutput tagging is my second choice, since it minimizes the need for\npeople to get creative to work around other proposals, and minimizes the\non-chain footprint, and finally chaperone signatures are my least\npreferred option due to its heavy-handed nature and the increased cost.\n\n> Indeed, if it is output tagging that gets into Bitcoin base layer, I\n> would strongly suggest the below for all Decker-Russell-Osuntokun\n> implementations:\n>\n> * A standard MuSig 2-of-2 bip-schnorr SegWit v1 Funding Transaction Output, confirmed onchain\n> * A \"translator transaction\" spending the above and paying out to a SegWit v16 output-tagged output, kept offchain.\n> * Decker-Russell-Osuntokun update transaction, signed with `SIGHASH_NOINPUT` spending the translator transaction output.\n> * Decker-Russell-Osuntokun state transaction, signed with `SIGHASH_NOINPUT` spending the update transaction output.\n\nThat is very much how I was planning to implement it anyway, using a\ntrigger transaction to separate timeout start and the actual\nupdate/settlement pairs (cfr. eltoo paper Section 4.2). So for eltoo\nthere shouldn't be an issue here :-)\n\n> The point regarding use of a commonly-known privkey to work around\n> chaperone signatures is appropriate to the above, incidentally.  In\n> short: this is a workaround, plain and simple, and one wonders the\n> point of adding *either* chaperones *or* output tagging if we will, in\n> practice, just work around them anyway.\n\nExactly, why introduce the extra burden of chaperone signatures or\noutput tagging if we're just going to sidestep it?\n\n> Again, the *more* important point is that special blockchain\n> constructions should only be used in the \"bad\" unilateral close case.\n> In the cooperative case, we want to use simple plain\n> bip-schnorr-signed outputs getting spent to further bip-schnor/Taproot\n> SegWit v1 addresses, to increase the anonymity set of all uses of\n> Decker-Russell-Osuntokun and other applications that might use\n> `SIGHASH_NOINPUT` in some edge case (but which resolve down to simple\n> bip-schnorr-signed n-of-n cases when the protocol is completed\n> successfully by all participants).\n\nWhile I do agree that we should keep outputs as unidentifiable as\npossible, I am starting to question whether that is possible for\noff-chain payment networks since we are gossiping about the existence of\nchannels and binding them to outpoints to prove their existence anyway.\n\nNot the strongest argument I know, but there's little point in talking\nideal cases when we need to weaken that later again. \n\n>> Open questions\n>>\n>> ---------------\n>>\n>> The questions that remain to be addressed are the following:\n>>\n>> 1.  General agreement on the usefulness of noinput / anyprevoutanyscript /\n>>     anyprevout. While at the CoreDev meeting I think everybody agreed that\n>>     these proposals a useful, also beyond eltoo, not everybody could be\n>>     there. I'd therefore like to elicit some feedback from the wider community.\n>\n> I strongly agree that `NOINPUT` is useful, and I was not able to attend CoreDev (at least, not with any human fleshbot already known to you --- I checked).\n\nGreat, good to know that I'm not shouting into the void, and that I'm\nnot just that crazy guy trying to get his hairbrained scheme to work :-)\n\n>> 2.  Is there strong support or opposition to the chaperone signatures\n>>     introduced in anyprevout / anyprevoutanyscript? I think it'd be best to\n>>     formulate a concrete set of pros and contras, rather than talk about\n>>     abstract dangers or advantages.\n>\n> No opposition, we will just work around this by publishing a common\n> known private key to use for all chaperone signatures, since all the\n> important security is in the `NOINPUT` signature anyway.\n>\n>>\n>> 3.  The same for output tagging / explicit opt-in. What are the advantages and\n>>     disadvantages?\n>\n> Strongly oppose, see above about my argument.\n>\n>>\n>> 4.  Shall we merge BIP-118 and bip-anyprevout. This would likely reduce the\n>>     confusion and make for simpler discussions in the end.\n>\n> Ambivalent, mildly support.\n>\n>>\n>> 5.  Anything I forgot to mention :-)\n>\n> Cats are very interesting creatures, and are irrelevant to `SIGHASH_NOINPUT` discussion, but are extremely cute nonetheless.\n\nDefinitely agreed :+1:"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-01T15:35:34",
                "message_text_only": "Good morning Christian,\n\n> > -   A standard MuSig 2-of-2 bip-schnorr SegWit v1 Funding Transaction Output, confirmed onchain\n> > -   A \"translator transaction\" spending the above and paying out to a SegWit v16 output-tagged output, kept offchain.\n> > -   Decker-Russell-Osuntokun update transaction, signed with `SIGHASH_NOINPUT` spending the translator transaction output.\n> > -   Decker-Russell-Osuntokun state transaction, signed with `SIGHASH_NOINPUT` spending the update transaction output.\n>\n> That is very much how I was planning to implement it anyway, using a\n> trigger transaction to separate timeout start and the actual\n> update/settlement pairs (cfr. eltoo paper Section 4.2). So for eltoo\n> there shouldn't be an issue here :-)\n\nMy understanding is that a trigger transaction is not in fact necessary for Decker-Russell-Osuntokun: any update transaction could spend the funding transaction output directly, and thereby start the relative timelock.\nAt least, if we could arrange the funding transaction output to be spendable directly using `SIGHASH_NOINPUT` or variants thereof.\n\n\n> > Again, the more important point is that special blockchain\n> > constructions should only be used in the \"bad\" unilateral close case.\n> > In the cooperative case, we want to use simple plain\n> > bip-schnorr-signed outputs getting spent to further bip-schnor/Taproot\n> > SegWit v1 addresses, to increase the anonymity set of all uses of\n> > Decker-Russell-Osuntokun and other applications that might use\n> > `SIGHASH_NOINPUT` in some edge case (but which resolve down to simple\n> > bip-schnorr-signed n-of-n cases when the protocol is completed\n> > successfully by all participants).\n>\n> While I do agree that we should keep outputs as unidentifiable as\n> possible, I am starting to question whether that is possible for\n> off-chain payment networks since we are gossiping about the existence of\n> channels and binding them to outpoints to prove their existence anyway.\n\n* Lightning supports unpublished channels, so we do not gossip some outpoints even though they are in fact channels underneath.\n  * I confess the existence of unpublished channels in the spec fails to summon any reaction other than incredulity from me, but they exist nonetheless, my incredulity notwithstanding.\n* Historical channels that have been cooperatively closed are no longer normally gossiped, so the fact that they used to be channels is no longer widely broadcast, and may eventually be forgotten by most or all of the network.\n  * This means anyone who wants to record the historical use of Lightning will have to retain the information themselves, rather than delegating it to fullnodes everywhere.\n\n>\n> Not the strongest argument I know, but there's little point in talking\n> ideal cases when we need to weaken that later again.\n\nThe point of ideal cases is to strive to approach them, not necessarily achieve them.\nJust as a completely unbiased rational reasoner is almost impossible to achieve, does not mean we should give up all attempts to reduce bias.\n\nOutpoints that used to be channels, but have now been closed using cooperative closes, will potentially no longer be widely gossiped as having once been channels, thus it may happen that they will eventually be forgotten by most of the network as once having been channels.\nBut if the outpoints of those channels are specially marked, then that cannot be forgotten, as the initial block download thereafter will have that history indelibly etched forevermore.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Christian Decker",
                "date": "2019-10-03T09:42:00",
                "message_text_only": "ZmnSCPxj <ZmnSCPxj at protonmail.com> writes:\n>> That is very much how I was planning to implement it anyway, using a\n>> trigger transaction to separate timeout start and the actual\n>> update/settlement pairs (cfr. eltoo paper Section 4.2). So for eltoo\n>> there shouldn't be an issue here :-)\n>\n> My understanding is that a trigger transaction is not in fact\n> necessary for Decker-Russell-Osuntokun: any update transaction could\n> spend the funding transaction output directly, and thereby start the\n> relative timelock.  At least, if we could arrange the funding\n> transaction output to be spendable directly using `SIGHASH_NOINPUT` or\n> variants thereof.\n\nThis is the case in which we don't have a pre-signed settlement\ntransaction (or in this case refund transaction) that uses a relative\ntimelock. In order to have a refund transaction we would need to have\nthe first update and settlement pair be signed before funding (otherwise\nthe funder isn't sure she is getting her funds back). Since that first\nupdate and settlement pair do not need to be rebound (they can only ever\nbe bound to the funding transaction) they can be signed without\nnoinput/anyprevoutanyscript. If we use output tagging we would mandate\nthat this first update must be published, so that the funding output is\nindistinguishable from a normal output, and the first update switches\nfrom non-noinput/anyprevoutanyscript to enabling it. Collaborative\ncloses are still indistinguishable, unilateral closes require the\nswitch, but then would be identifiable anyway.\n\nThe one downside I can see is that we now mandate that unilateral closes\nalso publish the first update, which is a bit annoying.\n\n>> While I do agree that we should keep outputs as unidentifiable as\n>> possible, I am starting to question whether that is possible for\n>> off-chain payment networks since we are gossiping about the existence of\n>> channels and binding them to outpoints to prove their existence anyway.\n>\n> * Lightning supports unpublished channels, so we do not gossip some outpoints even though they are in fact channels underneath.\n>   * I confess the existence of unpublished channels in the spec fails to summon any reaction other than incredulity from me, but they exist nonetheless, my incredulity notwithstanding.\n\nThat is true, we do however selectively tell others about the channel's\nexistence (in invoices, our peers, ...) so I wouldn't consider that to\nbe the most secret information :-)\n\nAs for why they exist: nodes need to have the option of not announcing\ntheir channels to reduce the noise in the network with channels that are\nunlikely to be useable in order to forward payments. If every node were\nto announce their channels we'd have a much larger routing table, mostly\nconsisting of unusable channels going to leafs in the\nnetwork. Furthermore, the sheer threat that there might be unannounced\nchannels adds uncertainty for attackers trying to profile nodes: \"I see\nonly my channel with my peer, but he might have unannounced channels, so\nI can't really tell whether the payment I forwarded to it is destined\nfor it or one of its unannounced peers\".\n\n> * Historical channels that have been cooperatively closed are no longer normally gossiped, so the fact that they used to be channels is no longer widely broadcast, and may eventually be forgotten by most or all of the network.\n>   * This means anyone who wants to record the historical use of Lightning will have to retain the information themselves, rather than delegating it to fullnodes everywhere.\n\nGood point, it requires storing the ephemeral data from gossip, that's\nnot all that hard, but I agree that it puts up a small barrier for\nnewcomers."
            },
            {
                "author": "Christian Decker",
                "date": "2019-10-01T14:26:39",
                "message_text_only": "ZmnSCPxj <ZmnSCPxj at protonmail.com> writes:\n> To elucidate further ---\n>\n> Suppose rather than `SIGHASH_NOINPUT`, we created a new opcode,\n> `OP_CHECKSIG_WITHOUT_INPUT`.\n>\n> This new opcode ignores any `SIGHASH` flags, if present, on a\n> signature, but instead hashes the current transaction without the\n> input references, then checks that hash to the signature.\n>\n> This is equivalent to `SIGHASH_NOINPUT`.\n>\n> Yet as an opcode, it would be possible to embed in a Taproot script.\n>\n> For example, a Decker-Russell-Osuntokun would have an internal Taproot\n> point be a 2-of-2, then have a script `OP_1\n> OP_CHECKSIG_WITHOUT_INPUT`.  Unilateral closes would expose the hidden\n> script, but cooperative closes would use the 2-of-2 directly.\n>\n> Of note, is that any special SCRIPT would already be supportable by Taproot.\n> This includes SCRIPTs that may potentially lose funds for the user.\n> Yet such SCRIPTs are already targetable by a Taproot address.\n>\n> If we are so concerned about `SIGHASH_NOINPUT` abuse, why are we not\n> so concerned about Taproot abuse?\n\nThat would certainly be another possibility, which I have not explored\nin detail so far. Due to the similarity between the various signature\nchecking op-codes it felt that it should be a sighash flag, and it\nneatly slotted into the already existing flags. If we go for a separate\nopcode we might end up reinventing the wheel, and to be honest I feared\nthat proposing a new opcode would get us into bikeshedding territory\n(which I apparently failed to avoid with the sighash flag anyway...).\n\nThe advantage would be that with the sighash flag the spender is in\ncharge of specifying the flags, whereas with an opcode the output\ndictates the signature verification modalities. The downside is the\nincreased design space.\n\nWhat do others think? Would this be an acceptable opt-in mechanism that\naddresses the main concerns?\n\nCheers,\nChristian"
            },
            {
                "author": "Anthony Towns",
                "date": "2019-10-01T14:45:48",
                "message_text_only": "On Mon, Sep 30, 2019 at 11:28:43PM +0000, ZmnSCPxj via bitcoin-dev wrote:\n> Suppose rather than `SIGHASH_NOINPUT`, we created a new opcode, `OP_CHECKSIG_WITHOUT_INPUT`.\n\nI don't think there's any meaningful difference between making a new\nopcode and making a new tapscript public key type; the difference is\njust one of encoding:\n\n   3301<key>AC   [CHECKSIG of public key type 0x01]\n   32<key>B3     [CHECKSIG_WITHOUT_INPUT (replacing NOP4) of key]\n\n> This new opcode ignores any `SIGHASH` flags, if present, on a signature,\n\n(How sighash flags are treated can be redefined by new public key types;\nif that's not obvious already)\n\nCheers,\naj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-01T15:42:08",
                "message_text_only": "Good morning aj,\n\n\n> On Mon, Sep 30, 2019 at 11:28:43PM +0000, ZmnSCPxj via bitcoin-dev wrote:\n>\n> > Suppose rather than `SIGHASH_NOINPUT`, we created a new opcode, `OP_CHECKSIG_WITHOUT_INPUT`.\n>\n> I don't think there's any meaningful difference between making a new\n> opcode and making a new tapscript public key type; the difference is\n> just one of encoding:\n>\n> 3301<key>AC [CHECKSIG of public key type 0x01]\n> 32<key>B3 [CHECKSIG_WITHOUT_INPUT (replacing NOP4) of key]\n>\n> > This new opcode ignores any `SIGHASH` flags, if present, on a signature,\n>\n> (How sighash flags are treated can be redefined by new public key types;\n> if that's not obvious already)\n\n\nThank you for this thought,\nI believe under tapscript v0 we can give `OP_1` as the public key to `OP_CHECKSIG` to mean to reuse the internal Taproot pubkey, would it be possible to have some similar mechanism here, to copy the internal Taproot pubkey but also to enable new `SIGHASH` flag for this particular script only?\n\nThis seems fine, as then a Decker-Russell-Osuntokun funding tx output between nodes A, B, and C would have:\n\n* Taproot internal key: `P = MuSig(A, B, C)`\n* Script 1: leaf version 0, `<MuSig(A,B,C) + pubkeytype 1> OP_CHECKSIG`\n\nThen, update transactions could use `MuSig(A,B,C)` for signing along the \"update\" path, with unique \"state\" keys.\nAnd cooperative closes would sign using `P + h(P | MAST(<MuSig(A,B,C) + pubkeytype 1> OPCHECKSIG)) * G`, not revealing the fact that this was in fact a Decker-Russell-Osuntokun output.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Anthony Towns",
                "date": "2019-10-01T15:59:29",
                "message_text_only": "On Mon, Sep 30, 2019 at 03:23:56PM +0200, Christian Decker via bitcoin-dev wrote:\n> With the recently renewed interest in eltoo, a proof-of-concept implementation\n> [1], and the discussions regarding clean abstractions for off-chain protocols\n> [2,3], I thought it might be time to revisit the `sighash_noinput` proposal\n> (BIP-118 [4]), and AJ's `bip-anyprevout` proposal [5].\n\nHey Christian, thanks for the write up!\n\n> ## Open questions\n> The questions that remain to be addressed are the following:\n> 1.  General agreement on the usefulness of noinput / anyprevoutanyscript /\n>     anyprevout[?]\n> 2.  Is there strong support or opposition to the chaperone signatures[?]\n> 3.  The same for output tagging / explicit opt-in[?]\n> 4.  Shall we merge BIP-118 and bip-anyprevout. This would likely reduce the\n>     confusion and make for simpler discussions in the end.\n\nI think there's an important open question you missed from this list:\n(1.5) do we really understand what the dangers of noinput/anyprevout-style\nconstructions actually are?\n\nMy impression on the first 3.5 q's is: (1) yes, (1.5) not really,\n(2) weak opposition for requiring chaperone sigs, (3) mixed (weak)\nsupport/opposition for output tagging.\n\nMy thinking at the moment (subject to change!) is:\n\n * anyprevout signatures make the address you're signing for less safe,\n   which may cause you to lose funds when additional coins are sent to\n   the same address; this can be avoided if handled with care (or if you\n   don't care about losing funds in the event of address reuse)\n\n * being able to guarantee that an address can never be signed for with\n   an anyprevout signature is therefore valuable; so having it be opt-in\n   at the tapscript level, rather than a sighash flag available for\n   key-path spends is valuable (I call this \"opt-in\", but it's hidden\n   until use via taproot rather than \"explicit\" as output tagging\n   would be)\n\n * receiving funds spent via an anyprevout signature does not involve any\n   qualitatively new double-spending/malleability risks.\n   \n   (eltoo is unavoidably malleable if there are multiple update\n   transactions (and chaperone signatures aren't used or are used with\n   well known keys), but while it is better to avoid this where possible,\n   it's something that's already easily dealt with simply by waiting\n   for confirmations, and whether a transaction is malleable is always\n   under the control of the sender not the receiver)\n\n * as such, output tagging is also unnecessary, and there is also no\n   need for users to mark anyprevout spends as \"tainted\" in order to\n   wait for more confirmations than normal before considering those funds\n   \"safe\"\n\nI think it might be good to have a public testnet (based on Richard Myers\net al's signet2 work?) where we have some fake exchanges/merchants/etc\nand scheduled reorgs, and demo every weird noinput/anyprevout case anyone\ncan think of, and just work out if we need any extra code/tagging/whatever\nto keep those fake exchanges/merchants from losing money (and write up\nthe weird cases we've found in a wiki or a paper so people can easily\ntell if we missed something obvious).\n\nCheers,\naj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-02T02:03:43",
                "message_text_only": "Good morning lists,\n\nLet me propose the below radical idea:\n\n* `SIGHASH` flags attached to signatures are a misdesign, sadly retained from the original BitCoin 0.1.0 Alpha for Windows design, on par with:\n  * 1 RETURN\n  * higher-`nSequence` replacement\n  * DER-encoded pubkeys\n  * unrestricted `scriptPubKey`\n  * Payee-security-paid-by-payer (i.e. lack of P2SH)\n  * `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n  * transaction malleability\n  * probably many more\n\nSo let me propose the more radical excision, starting with SegWit v1:\n\n* Remove `SIGHASH` from signatures.\n* Put `SIGHASH` on public keys.\n\nPublic keys are now encoded as either 33-bytes (implicit `SIGHASH_ALL`) or 34-bytes (`SIGHASH` byte, followed by pubkey type, followed by pubkey coordinate).\n`OP_CHECKSIG` and friends then look at the *public key* to determine sighash algorithm rather than the signature.\n\nAs we expect public keys to be indirectly committed to on every output `scriptPubKey`, this is automatically output tagging to allow particular `SIGHASH`.\nHowever, we can then utilize the many many ways to hide public keys away until they are needed, exemplified in MAST-inside-Taproot.\n\nI propose also the addition of the opcode:\n\n    <sighash> <pubkey> OP_SETPUBKEYSIGHASH\n\n* `sighash` must be one byte.\n* `pubkey` may be the special byte `0x1`, meaning \"just use the Taproot internal pubkey\".\n* `pubkey` may be 33-byte public key, in which case the `sighash` byte is just prepended to it.\n* `pubkey` may be 34-byte public key with sighash, in which case the first byte is replaced with `sighash` byte.\n* If `sighash` is `0x00` then the result is a 33-byte public key (the sighash byte is removed) i.e. `SIGHASH_ALL` implicit.\n\nThis retains the old feature where the sighash is selected at time-of-spending rather than time-of-payment.\nThis is done by using the script:\n\n    <pubkey> OP_SETPUBKEYSIGHASH OP_CHECKSIG\n\nThen the sighash can be put in the witness stack after the signature, letting the `SIGHASH` flag be selected at time-of-signing, but only if the SCRIPT specifically is formed to do so.\nThis is malleability-safe as the signature still commits to the `SIGHASH` it was created for.\n\nHowever, by default, public keys will not have an attached `SIGHASH` byte, implying `SIGHASH_ALL` (and disallowing-by-default non-`SIGHASH_ALL`).\n\nThis removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they are allowed only if the output specifically says they are allowed.\n\nWould this not be a superior solution?\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Anthony Towns",
                "date": "2019-10-03T01:47:58",
                "message_text_only": "On Wed, Oct 02, 2019 at 02:03:43AM +0000, ZmnSCPxj via Lightning-dev wrote:\n> So let me propose the more radical excision, starting with SegWit v1:\n> * Remove `SIGHASH` from signatures.\n> * Put `SIGHASH` on public keys.\n>     <sighash> <pubkey> OP_SETPUBKEYSIGHASH\n\nI don't think you could reasonably do this for key path spends -- if\nyou included the sighash as part of the scriptpubkey explicitly, that\nwould lose some of the indistinguishability of taproot addresses, and be\nmore expensive than having the sighash be in witness data. So I think\nthat means sighashes would still be included in key path signatures,\nwhich would make the behaviour a little confusingly different between\nsigning for key path and script path spends.\n\n> This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they are allowed only if the output specifically says they are allowed.\n\nI don't think the problems with NONE and SINGLE are any worse than using\nSIGHASH_ALL to pay to \"1*G\" -- someone may steal the money you send,\nbut that's as far as it goes. NOINPUT/ANYPREVOUT is worse in that if\nyou use it, someone may steal funds from other UTXOs too -- similar\nto nonce-reuse. So I think having to commit to enabling NOINPUT for an\naddress may make sense; but I don't really see the need for doing the\nsame for other sighashes generally.\n\nFWIW, one way of looking at a transaction spending UTXO \"U\" to address\n\"A\" is something like:\n\n * \"script\" lets you enforce conditions on the transaction when you\n   create \"A\" [0]\n * \"sighash\" lets you enforce conditions on the transaction when\n   you sign the transaction\n * nlocktime, nsequence, taproot annex are ways you express conditions\n   on the transaction\n\nIn that view, \"sighash\" is actually an *extremely* simple scripting\nlanguage itself (with a total of six possible scripts).\n\nThat doesn't seem like a bad design to me, fwiw.\n\nCheers,\naj\n\n[0] \"graftroot\" lets you update those conditions for address \"A\" after\n    the fact"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-03T03:07:55",
                "message_text_only": "> > let me propose the more radical excision, starting with SegWit v1:\n> >\n> > -   Remove `SIGHASH` from signatures.\n> > -   Put `SIGHASH` on public keys.\n> >     <sighash> <pubkey> OP_SETPUBKEYSIGHASH\n> >\n>\n> I don't think you could reasonably do this for key path spends -- if\n> you included the sighash as part of the scriptpubkey explicitly, that\n> would lose some of the indistinguishability of taproot addresses, and be\n> more expensive than having the sighash be in witness data.\n\nNonexistence of sighash byte implies `SIGHASH_ALL`, and for offchain anyway the desired path is to end up with an n-of-n MuSig `SIGHASH_ALL` signed mutual close transaction.\nIndeed we can even restrict keypath spends to not having a sighash byte and just implicitly requiring `SIGHASH_ALL` with no loss of privacy for offchain while attaining safety against `SIGHASH_NOINPUT` for MuSig and VSSS multisignature adresses.\n\n\n> So I think\n> that means sighashes would still be included in key path signatures,\n> which would make the behaviour a little confusingly different between\n> signing for key path and script path spends.\n>\n> > This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they are allowed only if the output specifically says they are allowed.\n>\n> I don't think the problems with NONE and SINGLE are any worse than using\n> SIGHASH_ALL to pay to \"1*G\" -- someone may steal the money you send,\n> but that's as far as it goes. NOINPUT/ANYPREVOUT is worse in that if\n> you use it, someone may steal funds from other UTXOs too -- similar\n> to nonce-reuse. So I think having to commit to enabling NOINPUT for an\n> address may make sense; but I don't really see the need for doing the\n> same for other sighashes generally.\n\nAs the existing sighashes are not particularly used anyway, additional restrictions on them are relatively immaterial.\n\n>\n> FWIW, one way of looking at a transaction spending UTXO \"U\" to address\n> \"A\" is something like:\n>\n> -   \"script\" lets you enforce conditions on the transaction when you\n>     create \"A\" [0]\n>\n> -   \"sighash\" lets you enforce conditions on the transaction when\n>     you sign the transaction\n>\n> -   nlocktime, nsequence, taproot annex are ways you express conditions\n>     on the transaction\n>\n>     In that view, \"sighash\" is actually an extremely simple scripting\n>     language itself (with a total of six possible scripts).\n>\n>     That doesn't seem like a bad design to me, fwiw.\n\n\nOnly one of the scripts is widely used, another has an edge use it sucks at (assurance contracts).\n\nDoes not seem to be good design, rather legacy cruft.\n\nRegards,\nZmnSCPxj\n\n>\n>     Cheers,\n>     aj\n>\n>     [0] \"graftroot\" lets you update those conditions for address \"A\" after\n>     the fact\n>"
            },
            {
                "author": "s7r",
                "date": "2019-10-02T15:11:25",
                "message_text_only": "Anthony Towns via bitcoin-dev wrote:\n[SNIP]\n> \n> My thinking at the moment (subject to change!) is:\n> \n>  * anyprevout signatures make the address you're signing for less safe,\n>    which may cause you to lose funds when additional coins are sent to\n>    the same address; this can be avoided if handled with care (or if you\n>    don't care about losing funds in the event of address reuse)\n> \n\nIt's not necessarily like this. Address re-use is many times OUTSIDE the\ncontrol of the address owner. Say I give my address to a counterparty.\nThey send me a transaction which I successfully spend. So far so good.\n\nAfter that, I have no control over that counterparty. If they decide to\nre-use that address, it does not mean I wanted to re-use it and it also\ndoes not mean that I don't care about those funds being lost.\n\nThis could create a lot of problems in the industry and I think it\nshould be avoided. Address re-use has been strongly discouraged ever\nsince I can remember, and all (proper) wallet implementations try as\nhard as possible to enforce it, but it's not always possible. A\ncounterparty that decides to re-use an address, either accidentally or\nnot, is not under the control of the user who handed out the address in\nthe first place.\n\nThere are also a lot of use cases with P2SH addresses that are some\nsmart contracts particularly designed to be re-used multiple times over\ntime.\n\nMy 2 cents are that this is not a good way to go. If you try to index\nthe entire blockchain until now you'll see that address re-use is more\ncommon than we'd want it to be and there's no clear way to prevent this\nfrom further happening without hurting the economic interests of the users.\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 488 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191002/282e8910/attachment-0001.sig>"
            },
            {
                "author": "Christian Decker",
                "date": "2019-10-03T11:08:29",
                "message_text_only": "Anthony Towns <aj at erisian.com.au> writes:\n\n> On Mon, Sep 30, 2019 at 03:23:56PM +0200, Christian Decker via bitcoin-dev wrote:\n>> With the recently renewed interest in eltoo, a proof-of-concept implementation\n>> [1], and the discussions regarding clean abstractions for off-chain protocols\n>> [2,3], I thought it might be time to revisit the `sighash_noinput` proposal\n>> (BIP-118 [4]), and AJ's `bip-anyprevout` proposal [5].\n>\n> Hey Christian, thanks for the write up!\n>\n>> ## Open questions\n>> The questions that remain to be addressed are the following:\n>> 1.  General agreement on the usefulness of noinput / anyprevoutanyscript /\n>>     anyprevout[?]\n>> 2.  Is there strong support or opposition to the chaperone signatures[?]\n>> 3.  The same for output tagging / explicit opt-in[?]\n>> 4.  Shall we merge BIP-118 and bip-anyprevout. This would likely reduce the\n>>     confusion and make for simpler discussions in the end.\n>\n> I think there's an important open question you missed from this list:\n> (1.5) do we really understand what the dangers of noinput/anyprevout-style\n> constructions actually are?\n>\n> My impression on the first 3.5 q's is: (1) yes, (1.5) not really,\n> (2) weak opposition for requiring chaperone sigs, (3) mixed (weak)\n> support/opposition for output tagging.\n>\n> My thinking at the moment (subject to change!) is:\n>\n>  * anyprevout signatures make the address you're signing for less safe,\n>    which may cause you to lose funds when additional coins are sent to\n>    the same address; this can be avoided if handled with care (or if you\n>    don't care about losing funds in the event of address reuse)\n>\n>  * being able to guarantee that an address can never be signed for with\n>    an anyprevout signature is therefore valuable; so having it be opt-in\n>    at the tapscript level, rather than a sighash flag available for\n>    key-path spends is valuable (I call this \"opt-in\", but it's hidden\n>    until use via taproot rather than \"explicit\" as output tagging\n>    would be)\n>\n>  * receiving funds spent via an anyprevout signature does not involve any\n>    qualitatively new double-spending/malleability risks.\n>    \n>    (eltoo is unavoidably malleable if there are multiple update\n>    transactions (and chaperone signatures aren't used or are used with\n>    well known keys), but while it is better to avoid this where possible,\n>    it's something that's already easily dealt with simply by waiting\n>    for confirmations, and whether a transaction is malleable is always\n>    under the control of the sender not the receiver)\n>\n>  * as such, output tagging is also unnecessary, and there is also no\n>    need for users to mark anyprevout spends as \"tainted\" in order to\n>    wait for more confirmations than normal before considering those funds\n>    \"safe\"\n\nExcellent points, I had missed the hidden nature of the opt-in via\npubkey prefix while reading your proposal. I'm starting to like that\noption more and more. In that case we'd only ever be revealing that we\nopted into anyprevout when we're revealing the entire script anyway, at\nwhich point all fungibility concerns go out the window anyway.\n\nWould this scheme be extendable to opt into all sighash flags the\noutpoint would like to allow (e.g., adding opt-in for sighash_none and\nsighash_anyonecanpay as well)? That way the pubkey prefix could act as a\nmask for the sighash flags and fail verification if they don't match.\n\n> I think it might be good to have a public testnet (based on Richard Myers\n> et al's signet2 work?) where we have some fake exchanges/merchants/etc\n> and scheduled reorgs, and demo every weird noinput/anyprevout case anyone\n> can think of, and just work out if we need any extra code/tagging/whatever\n> to keep those fake exchanges/merchants from losing money (and write up\n> the weird cases we've found in a wiki or a paper so people can easily\n> tell if we missed something obvious).\n\nThat'd be great, however even that will not ensure that every possible\ncorner case is handled and from experience it seems that people are\nunwilling to invest a lot of time testing on a network unless their\nmoney is on the line. That's not to say that we shouldn't try, we\nabsolutely should, I'm just not sure it alone is enough to dispell all\nremaining doubts :-)\n\nCheers,\nChristian"
            },
            {
                "author": "Anthony Towns",
                "date": "2019-10-05T10:06:15",
                "message_text_only": "On Thu, Oct 03, 2019 at 01:08:29PM +0200, Christian Decker wrote:\n> >  * anyprevout signatures make the address you're signing for less safe,\n> >    which may cause you to lose funds when additional coins are sent to\n> >    the same address; this can be avoided if handled with care (or if you\n> >    don't care about losing funds in the event of address reuse)\n> Excellent points, I had missed the hidden nature of the opt-in via\n> pubkey prefix while reading your proposal. I'm starting to like that\n> option more and more. In that case we'd only ever be revealing that we\n> opted into anyprevout when we're revealing the entire script anyway, at\n> which point all fungibility concerns go out the window anyway.\n>\n> Would this scheme be extendable to opt into all sighash flags the\n> outpoint would like to allow (e.g., adding opt-in for sighash_none and\n> sighash_anyonecanpay as well)? That way the pubkey prefix could act as a\n> mask for the sighash flags and fail verification if they don't match.\n\nFor me, the thing that distinguishes ANYPREVOUT/NOINPUT as warranting\nan opt-in step is that it affects the security of potentially many\nUTXOs at once; whereas all the other combinations (ALL,SINGLE,NONE\ncross ALL,ANYONECANPAY) still commit to the specific UTXO being spent,\nso at most you only risk somehow losing the funds from the specific UTXO\nyou're working with (apart from the SINGLE bug, which taproot doesn't\nsupport anyway).\n\nHaving a meaningful prefix on the taproot scriptpubkey (ie paying to\n\"[SIGHASH_SINGLE][32B pubkey]\") seems like it would make it a bit easier\nto distinguish wallets, which taproot otherwise avoids -- \"oh this address\nis going to be a SIGHASH_SINGLE? probably some hacker, let's ban it\".\n\n> > I think it might be good to have a public testnet (based on Richard Myers\n> > et al's signet2 work?) where we have some fake exchanges/merchants/etc\n> > and scheduled reorgs, and demo every weird noinput/anyprevout case anyone\n> > can think of, and just work out if we need any extra code/tagging/whatever\n> > to keep those fake exchanges/merchants from losing money (and write up\n> > the weird cases we've found in a wiki or a paper so people can easily\n> > tell if we missed something obvious).\n> That'd be great, however even that will not ensure that every possible\n> corner case is handled [...]\n\nWell, sure. I'm thinking of it more as a *necessary* step than a\n*sufficient* one, though. If we can't demonstrate that we can deal with\nthe theoretical attacks people have dreamt up in a \"laboratory\" setting,\nthen it doesn't make much sense to deploy things in a real world setting,\ndoes it?\n\nI think if it turns out that we can handle every case we can think of\neasily, that will be good evidence that output tagging and the like isn't\nnecessary; and conversely if it turns out we can't handle them easily,\nit at least gives us a chance to see how output tagging (or chaperone\nsigs, or whatever else) would actually work, and if they'd provide any\nmeaningful protection at all. At the moment the best we've got is ideas\nand handwaving...\n\nCheers,\naj"
            }
        ],
        "thread_summary": {
            "title": "Continuing the discussion about noinput / anyprevout",
            "categories": [
                "Lightning-dev",
                "bitcoin-dev"
            ],
            "authors": [
                "Chris Stewart",
                "Anthony Towns",
                "s7r",
                "ZmnSCPxj",
                "Christian Decker"
            ],
            "messages_count": 17,
            "total_messages_chars_count": 76807
        }
    },
    {
        "title": "[Lightning-dev] Continuing the discussion about noinput / anyprevout",
        "thread_messages": [
            {
                "author": "Richard Myers",
                "date": "2019-10-01T13:35:10",
                "message_text_only": "Thanks Christian for pulling together this concise summary.\n\n1.  General agreement on the usefulness of noinput / anyprevoutanyscript /\n>     anyprevout.\n>\n\nI certainly support the unification and adoption of the sighash_noinput and\nanyprevoutput* proposals to enable eltoo, but also to make possible better\noff-chain protocol designs generally.\n\nAmong the various advantages previously discussed, the particular use case\nbenefits from eltoo I want to take advantage of is less interactive payment\nchannel negotiation.\n\nIn talking with people about eltoo this summer, I found most people\ngenerally support adding this as an option to Lightning. The only general\nconcern I heard, if any,  was the vague idea that rebindable transactions\ncould be somehow misused or abused.\n\nI believe when these concerns are made more concrete they can be classified\nand addressed.\n\nI don't find too compelling the potential problem of a 'bad wallet\ndesigner', whether lazy or dogmatic, misusing noinput. I think there are\nsimpler ways to cut corners and there will always be plenty of good wallet\noptions people can choose.\n\nBecause scripts signed with no_input signatures are only really exchanged\nand used for off-chain negotiations, very few should ever appear on chain.\nThose that do should represent non-cooperative situations that involve\nsigning parties who know not to reuse or share scripts with these public\nkeys again. No third party has any reason to spend value to a\nmultisignature script they don't control, whether or not a no_input\nsignature exists for it.\n\n2.  Is there strong support or opposition to the chaperone signatures\n>     introduced in anyprevout / anyprevoutanyscript? I think it'd be best to\n>     formulate a concrete set of pros and contras, rather than talk about\n>     abstract dangers or advantages.\n>\n\nAs I mentioned before, I don't think the lazy wallet designer advantage is\nenough to justify the downsides of chaperone signatures. One additional\ndownside is the additional code complexity required to flag whether or not\na chaperone output is included. By comparison, the code changes for\ncreating a no_input digest that skips the prevout and prevscript parts of a\ntx is much less intrusive and easier to maintain.\n\n3.  The same for output tagging / explicit opt-in. What are the advantages\n> and\n>     disadvantages?\n>\n\nI see the point ZmnSCPxj makes about tagged outputs negatively impacting\nthe anonymity set of taproot transactions. The suggested work around would\nimpose a cost to unilateral closes of an additional translation transaction\nand not using the work around would cause a hit to anonymity for off-chain\nscript users. I feel both costs are too high relative to the benefit gained\nof preventing sloppy reuse of public keys.\n\n4.  Shall we merge BIP-118 and bip-anyprevout. This would likely reduce the\n>     confusion and make for simpler discussions in the end.\n\n\nI believe they should be merged. I also think both chaperone signatures and\noutput tagging should become part of the discussion of security\nalternatives, but not part of the initial specification.\n\nI understand the desire to be conservative with protocol changes that could\nbe misused. However, with just taproot and taproot public key types the\nanyprevout functionality is already very opt-in and not something that\nmight accidentally get used. Belt-and-suspender protections like chaperone\nsignatures and tagged outputs have their own impacts on code complexity,\non-chain transaction sizes and transaction anonymity that also must be\nconsidered.\n\nI believe efforts like descriptors will help people follow best practices\nwhen working with complex scripts without pushing extra complexity for\nsafety into the consensus layer of bitcoin. Anywhere we can make core code\nsimpler, and handle foot-guns in higher level non-consensus code, the\nbetter.\n\n_______________________________________________\n\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191001/f57d3131/attachment-0001.html>"
            },
            {
                "author": "Ethan Heilman",
                "date": "2019-10-01T14:27:21",
                "message_text_only": ">I don't find too compelling the potential problem of a 'bad wallet designer', whether lazy or dogmatic, misusing noinput. I think there are simpler ways to cut corners and there will always be plenty of good wallet options people can choose.\n\nI want to second this. The most expensive part of wallet design is\nengineering time. Writing code that uses a new sighash or a custom\nscript with a OP_CODE is a very large barrier to use. How many wallets\nsupport multisig or RBF? How much BTC has been stolen over the entire\nhistory of Bitcoin because of sighash SIGHASH_NONE or SIGHASH_SINGLE\nvs ECDSA nonce reuse?\n\nOn Tue, Oct 1, 2019 at 9:35 AM Richard Myers <rich at gotenna.com> wrote:\n>\n> Thanks Christian for pulling together this concise summary.\n>\n>> 1.  General agreement on the usefulness of noinput / anyprevoutanyscript /\n>>     anyprevout.\n>\n>\n> I certainly support the unification and adoption of the sighash_noinput and anyprevoutput* proposals to enable eltoo, but also to make possible better off-chain protocol designs generally.\n>\n> Among the various advantages previously discussed, the particular use case benefits from eltoo I want to take advantage of is less interactive payment channel negotiation.\n>\n> In talking with people about eltoo this summer, I found most people generally support adding this as an option to Lightning. The only general concern I heard, if any,  was the vague idea that rebindable transactions could be somehow misused or abused.\n>\n> I believe when these concerns are made more concrete they can be classified and addressed.\n>\n> I don't find too compelling the potential problem of a 'bad wallet designer', whether lazy or dogmatic, misusing noinput. I think there are simpler ways to cut corners and there will always be plenty of good wallet options people can choose.\n>\n> Because scripts signed with no_input signatures are only really exchanged and used for off-chain negotiations, very few should ever appear on chain. Those that do should represent non-cooperative situations that involve signing parties who know not to reuse or share scripts with these public keys again. No third party has any reason to spend value to a multisignature script they don't control, whether or not a no_input signature exists for it.\n>\n>> 2.  Is there strong support or opposition to the chaperone signatures\n>>     introduced in anyprevout / anyprevoutanyscript? I think it'd be best to\n>>     formulate a concrete set of pros and contras, rather than talk about\n>>     abstract dangers or advantages.\n>\n>\n> As I mentioned before, I don't think the lazy wallet designer advantage is enough to justify the downsides of chaperone signatures. One additional downside is the additional code complexity required to flag whether or not a chaperone output is included. By comparison, the code changes for creating a no_input digest that skips the prevout and prevscript parts of a tx is much less intrusive and easier to maintain.\n>\n>> 3.  The same for output tagging / explicit opt-in. What are the advantages and\n>>     disadvantages?\n>\n>\n> I see the point ZmnSCPxj makes about tagged outputs negatively impacting the anonymity set of taproot transactions. The suggested work around would impose a cost to unilateral closes of an additional translation transaction and not using the work around would cause a hit to anonymity for off-chain script users. I feel both costs are too high relative to the benefit gained of preventing sloppy reuse of public keys.\n>\n>> 4.  Shall we merge BIP-118 and bip-anyprevout. This would likely reduce the\n>>     confusion and make for simpler discussions in the end.\n>\n>\n> I believe they should be merged. I also think both chaperone signatures and output tagging should become part of the discussion of security alternatives, but not part of the initial specification.\n>\n> I understand the desire to be conservative with protocol changes that could be misused. However, with just taproot and taproot public key types the anyprevout functionality is already very opt-in and not something that might accidentally get used. Belt-and-suspender protections like chaperone signatures and tagged outputs have their own impacts on code complexity, on-chain transaction sizes and transaction anonymity that also must be considered.\n>\n> I believe efforts like descriptors will help people follow best practices when working with complex scripts without pushing extra complexity for safety into the consensus layer of bitcoin. Anywhere we can make core code simpler, and handle foot-guns in higher level non-consensus code, the better.\n>\n> _______________________________________________\n>>\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "Chris Stewart",
                "date": "2019-10-01T15:14:56",
                "message_text_only": "> I don't find too compelling the potential problem of a 'bad wallet\ndesigner', whether lazy or dogmatic, misusing noinput. I think there are\nsimpler ways to cut corners and there will always be plenty of good wallet\noptions people can choose.\n\nIn my original post, the business that I am talking about don't use \"off\nthe shelf\" wallet options. It isn't a \"let's switch from wallet A to wallet\nB\" kind of situation. Usually this involves design from ground up with\nsecurity considerations that businesses of scale need to consider (signing\nprocedures and key handling being the most important!).\n\n>Because scripts signed with no_input signatures are only really exchanged\nand used for off-chain negotiations, very few should ever appear on chain.\nThose that do should represent non-cooperative situations that involve\nsigning parties who know not to reuse or share scripts with these public\nkeys again. No third party has any reason to spend value to a\nmultisignature script they don't control, whether or not a no_input\nsignature exists for it.\n\nJust because some one is your friend today, doesn't mean they aren't\nnecessarily your adversary tomorrow. I don't think a signature being\nonchain really matters, as you have to give it to your counterparty\nregardless. How do you know your counterparty won't replay that\nSIGHASH_NOINPUT signature later? Offchain protocols shouldn't rely on\n\"good-will\" for their counter parties for security.\n\n>As I mentioned before, I don't think the lazy wallet designer advantage is\nenough to justify the downsides of chaperone signatures. One additional\ndownside is the additional code complexity required to flag whether or not\na chaperone output is included. By comparison, the code changes for\ncreating a no_input digest that skips the prevout and prevscript parts of a\ntx is much less intrusive and easier to maintain.\n\n>I want to second this. The most expensive part of wallet design is\nengineering time. Writing code that uses a new sighash or a custom\nscript with a OP_CODE is a very large barrier to use. How many wallets\nsupport multisig or RBF? How much BTC has been stolen over the entire\nhistory of Bitcoin because of sighash SIGHASH_NONE or SIGHASH_SINGLE\nvs ECDSA nonce reuse\n\nI actually think lazy wallet designer is a really compelling reason to fix\nfootguns in the bitcoin protocol. Mt Gox is allegedly a product of lazy\nwallet design. Now we have non-malleable transactions in the form of segwit\n(yay!) that prevent this exploit. We can wish that the Mt Gox wallet\ndesigners were more aware of bitcoin protocol vulnerabilities, but at the\nend of the day the best thing to do was offering an alternative that\ncircumvents the vulnerability all together.\n\nEthan made a great point about SIGHASH_NONE or SIGHASH_SINGLE -- which have\nvirtually no use AFAIK -- vs the ECDSA nonce reuse which is used in nearly\nevery transaction. The feature -- ECDSA in this case -- was managed to be\ndone wrong by wallet developers causing fund loss. Unfortunately we can't\nprotect against this type of bug in the protocol.\n\nIf things aren't used -- such as SIGHASH_NONE or SIGHASH_SINGLE -- it\ndoesn't matter if they are secure or insecure. I'm hopefully that offchain\nprotocols will achieve wide adoption, and I would hate to see money lost\nbecause of this. Even though they aren't used, in my OP I do advocate for\nfixing these.\n\n> understand the desire to be conservative with protocol changes that could\nbe misused. However, with just taproot and taproot public key types the\nanyprevout functionality is already very opt-in and not something that\nmight accidentally get used. Belt-and-suspender protections like chaperone\nsignatures and tagged outputs have their own impacts on code complexity,\non-chain transaction sizes and transaction anonymity that also must be\nconsidered.\n\nI'm making the assumption that the business has decided to use this\nfeature, and in my OP I talk about the engineering decisions that will have\nto be made support this. I'm hoping the \"lazy wallet designers\" -- or\nperhaps people that don't follow bitcoin protocol development as rabidly as\nus :-) -- can see that nuance.\n\n-Chris\n\n\n\nOn Tue, Oct 1, 2019 at 8:36 AM Richard Myers <rich at gotenna.com> wrote:\n\n> Thanks Christian for pulling together this concise summary.\n>\n> 1.  General agreement on the usefulness of noinput / anyprevoutanyscript /\n>>     anyprevout.\n>>\n>\n> I certainly support the unification and adoption of the sighash_noinput\n> and anyprevoutput* proposals to enable eltoo, but also to make possible\n> better off-chain protocol designs generally.\n>\n> Among the various advantages previously discussed, the particular use case\n> benefits from eltoo I want to take advantage of is less interactive payment\n> channel negotiation.\n>\n> In talking with people about eltoo this summer, I found most people\n> generally support adding this as an option to Lightning. The only general\n> concern I heard, if any,  was the vague idea that rebindable transactions\n> could be somehow misused or abused.\n>\n> I believe when these concerns are made more concrete they can be\n> classified and addressed.\n>\n> I don't find too compelling the potential problem of a 'bad wallet\n> designer', whether lazy or dogmatic, misusing noinput. I think there are\n> simpler ways to cut corners and there will always be plenty of good wallet\n> options people can choose.\n>\n> Because scripts signed with no_input signatures are only really exchanged\n> and used for off-chain negotiations, very few should ever appear on chain.\n> Those that do should represent non-cooperative situations that involve\n> signing parties who know not to reuse or share scripts with these public\n> keys again. No third party has any reason to spend value to a\n> multisignature script they don't control, whether or not a no_input\n> signature exists for it.\n>\n> 2.  Is there strong support or opposition to the chaperone signatures\n>>     introduced in anyprevout / anyprevoutanyscript? I think it'd be best\n>> to\n>>     formulate a concrete set of pros and contras, rather than talk about\n>>     abstract dangers or advantages.\n>>\n>\n> As I mentioned before, I don't think the lazy wallet designer advantage is\n> enough to justify the downsides of chaperone signatures. One additional\n> downside is the additional code complexity required to flag whether or not\n> a chaperone output is included. By comparison, the code changes for\n> creating a no_input digest that skips the prevout and prevscript parts of a\n> tx is much less intrusive and easier to maintain.\n>\n> 3.  The same for output tagging / explicit opt-in. What are the advantages\n>> and\n>>     disadvantages?\n>>\n>\n> I see the point ZmnSCPxj makes about tagged outputs negatively impacting\n> the anonymity set of taproot transactions. The suggested work around would\n> impose a cost to unilateral closes of an additional translation transaction\n> and not using the work around would cause a hit to anonymity for off-chain\n> script users. I feel both costs are too high relative to the benefit gained\n> of preventing sloppy reuse of public keys.\n>\n> 4.  Shall we merge BIP-118 and bip-anyprevout. This would likely reduce the\n>>     confusion and make for simpler discussions in the end.\n>\n>\n> I believe they should be merged. I also think both chaperone signatures\n> and output tagging should become part of the discussion of security\n> alternatives, but not part of the initial specification.\n>\n> I understand the desire to be conservative with protocol changes that\n> could be misused. However, with just taproot and taproot public key types\n> the anyprevout functionality is already very opt-in and not something that\n> might accidentally get used. Belt-and-suspender protections like chaperone\n> signatures and tagged outputs have their own impacts on code complexity,\n> on-chain transaction sizes and transaction anonymity that also must be\n> considered.\n>\n> I believe efforts like descriptors will help people follow best practices\n> when working with complex scripts without pushing extra complexity for\n> safety into the consensus layer of bitcoin. Anywhere we can make core code\n> simpler, and handle foot-guns in higher level non-consensus code, the\n> better.\n>\n> _______________________________________________\n>\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191001/4f71dd93/attachment-0001.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2019-10-03T10:30:03",
                "message_text_only": "Chris Stewart <chris at suredbits.com> writes:\n\n>> I don't find too compelling the potential problem of a 'bad wallet\n> designer', whether lazy or dogmatic, misusing noinput. I think there are\n> simpler ways to cut corners and there will always be plenty of good wallet\n> options people can choose.\n>\n> In my original post, the business that I am talking about don't use \"off\n> the shelf\" wallet options. It isn't a \"let's switch from wallet A to wallet\n> B\" kind of situation. Usually this involves design from ground up with\n> security considerations that businesses of scale need to consider (signing\n> procedures and key handling being the most important!).\n\nIn this case I'd hope that the custom wallet designers/developers are\nwell-versed in the issues they might encounter when implementing their\nwallet. This is especially true if they decide to opt into using some\nlesser known sighash flags, such as noinput, that come with huge warning\nsigns (I forgot to mention that renaming noinput to noinput_dangerous is\nalso still on the table).\n\n>>Because scripts signed with no_input signatures are only really exchanged\n> and used for off-chain negotiations, very few should ever appear on chain.\n> Those that do should represent non-cooperative situations that involve\n> signing parties who know not to reuse or share scripts with these public\n> keys again. No third party has any reason to spend value to a\n> multisignature script they don't control, whether or not a no_input\n> signature exists for it.\n>\n> Just because some one is your friend today, doesn't mean they aren't\n> necessarily your adversary tomorrow. I don't think a signature being\n> onchain really matters, as you have to give it to your counterparty\n> regardless. How do you know your counterparty won't replay that\n> SIGHASH_NOINPUT signature later? Offchain protocols shouldn't rely on\n> \"good-will\" for their counter parties for security.\n>\n>>As I mentioned before, I don't think the lazy wallet designer advantage is\n> enough to justify the downsides of chaperone signatures. One additional\n> downside is the additional code complexity required to flag whether or not\n> a chaperone output is included. By comparison, the code changes for\n> creating a no_input digest that skips the prevout and prevscript parts of a\n> tx is much less intrusive and easier to maintain.\n>\n>>I want to second this. The most expensive part of wallet design is\n> engineering time. Writing code that uses a new sighash or a custom\n> script with a OP_CODE is a very large barrier to use. How many wallets\n> support multisig or RBF? How much BTC has been stolen over the entire\n> history of Bitcoin because of sighash SIGHASH_NONE or SIGHASH_SINGLE\n> vs ECDSA nonce reuse\n>\n> I actually think lazy wallet designer is a really compelling reason to fix\n> footguns in the bitcoin protocol. Mt Gox is allegedly a product of lazy\n> wallet design. Now we have non-malleable transactions in the form of segwit\n> (yay!) that prevent this exploit. We can wish that the Mt Gox wallet\n> designers were more aware of bitcoin protocol vulnerabilities, but at the\n> end of the day the best thing to do was offering an alternative that\n> circumvents the vulnerability all together.\n\nIt's worth pointing out that the transaction malleability issue and the\nintroduction of a new sighash flag are fundamentally different: a wallet\ndeveloper has to take active measures to guard against transaction\nmalleability since it was present even for the most minimal\nimplementation, whereas with sighash flags the developers have to\nactively add support for it. Where transaction malleability you just had\nto know that it might be an issue, with noinput you actively have to do\nwork yo expose yourself to it.\n\nI'd argue that you have to have a very compelling reason to opt into\nsupporting noinput, and that's usually because you want to support a\nmore complex protocol such as an off-chain contract anyway, at which\npoint I'd hope you know about the tradeoffs of various sighash flags :-)\n\n> Ethan made a great point about SIGHASH_NONE or SIGHASH_SINGLE -- which have\n> virtually no use AFAIK -- vs the ECDSA nonce reuse which is used in nearly\n> every transaction. The feature -- ECDSA in this case -- was managed to be\n> done wrong by wallet developers causing fund loss. Unfortunately we can't\n> protect against this type of bug in the protocol.\n>\n> If things aren't used -- such as SIGHASH_NONE or SIGHASH_SINGLE -- it\n> doesn't matter if they are secure or insecure. I'm hopefully that offchain\n> protocols will achieve wide adoption, and I would hate to see money lost\n> because of this. Even though they aren't used, in my OP I do advocate for\n> fixing these.\n\nI do share the feeling that we better make a commonly used sighash flag\nas useable and safe as possible, but it's rather unrealistic to have a\ndeveloper that is able to implement a complex off-chain system, but\nfails to understand the importance of using the correct sighash flags in\ntheir wallet. That being said, I think this concern would be addressed\nby any form of explicit opt-in on the output side (whether hidden or\nnot), right?\n\n\nCheers,\nChristian"
            }
        ],
        "thread_summary": {
            "title": "Continuing the discussion about noinput / anyprevout",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Chris Stewart",
                "Ethan Heilman",
                "Richard Myers",
                "Christian Decker"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 23152
        }
    },
    {
        "title": "[Lightning-dev] OP_CAT was Re: [bitcoin-dev] Continuing the discussion about noinput / anyprevout",
        "thread_messages": [
            {
                "author": "Ethan Heilman",
                "date": "2019-10-03T15:05:52",
                "message_text_only": "To avoid derailing the NO_INPUT conversation, I have changed the\nsubject to OP_CAT.\n\nResponding to:\n\"\"\"\n* `SIGHASH` flags attached to signatures are a misdesign, sadly\nretained from the original BitCoin 0.1.0 Alpha for Windows design, on\npar with:\n[..]\n* `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n[..]\n\"\"\"\n\nOP_CAT is an extremely valuable op code. I understand why it was\nremoved as the situation at the time with scripts was dire. However\nmost of the protocols I've wanted to build on Bitcoin run into the\nlimitation that stack values can not be concatenated. For instance\nTumbleBit would have far smaller transaction sizes if OP_CAT was\nsupported in Bitcoin. If it happens to me as a researcher it is\nprobably holding other people back as well. If I could wave a magic\nwand and turn on one of the disabled op codes it would be OP_CAT.  Of\ncourse with the change that size of each concatenated value must be 64\nBytes or less.\n\n\nOn Tue, Oct 1, 2019 at 10:04 PM ZmnSCPxj via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> Good morning lists,\n>\n> Let me propose the below radical idea:\n>\n> * `SIGHASH` flags attached to signatures are a misdesign, sadly retained from the original BitCoin 0.1.0 Alpha for Windows design, on par with:\n>   * 1 RETURN\n>   * higher-`nSequence` replacement\n>   * DER-encoded pubkeys\n>   * unrestricted `scriptPubKey`\n>   * Payee-security-paid-by-payer (i.e. lack of P2SH)\n>   * `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n>   * transaction malleability\n>   * probably many more\n>\n> So let me propose the more radical excision, starting with SegWit v1:\n>\n> * Remove `SIGHASH` from signatures.\n> * Put `SIGHASH` on public keys.\n>\n> Public keys are now encoded as either 33-bytes (implicit `SIGHASH_ALL`) or 34-bytes (`SIGHASH` byte, followed by pubkey type, followed by pubkey coordinate).\n> `OP_CHECKSIG` and friends then look at the *public key* to determine sighash algorithm rather than the signature.\n>\n> As we expect public keys to be indirectly committed to on every output `scriptPubKey`, this is automatically output tagging to allow particular `SIGHASH`.\n> However, we can then utilize the many many ways to hide public keys away until they are needed, exemplified in MAST-inside-Taproot.\n>\n> I propose also the addition of the opcode:\n>\n>     <sighash> <pubkey> OP_SETPUBKEYSIGHASH\n>\n> * `sighash` must be one byte.\n> * `pubkey` may be the special byte `0x1`, meaning \"just use the Taproot internal pubkey\".\n> * `pubkey` may be 33-byte public key, in which case the `sighash` byte is just prepended to it.\n> * `pubkey` may be 34-byte public key with sighash, in which case the first byte is replaced with `sighash` byte.\n> * If `sighash` is `0x00` then the result is a 33-byte public key (the sighash byte is removed) i.e. `SIGHASH_ALL` implicit.\n>\n> This retains the old feature where the sighash is selected at time-of-spending rather than time-of-payment.\n> This is done by using the script:\n>\n>     <pubkey> OP_SETPUBKEYSIGHASH OP_CHECKSIG\n>\n> Then the sighash can be put in the witness stack after the signature, letting the `SIGHASH` flag be selected at time-of-signing, but only if the SCRIPT specifically is formed to do so.\n> This is malleability-safe as the signature still commits to the `SIGHASH` it was created for.\n>\n> However, by default, public keys will not have an attached `SIGHASH` byte, implying `SIGHASH_ALL` (and disallowing-by-default non-`SIGHASH_ALL`).\n>\n> This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they are allowed only if the output specifically says they are allowed.\n>\n> Would this not be a superior solution?\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-03T23:42:25",
                "message_text_only": "Good morning Ethan,\n\n\n> To avoid derailing the NO_INPUT conversation, I have changed the\n> subject to OP_CAT.\n>\n> Responding to:\n> \"\"\"\n>\n> -   `SIGHASH` flags attached to signatures are a misdesign, sadly\n>     retained from the original BitCoin 0.1.0 Alpha for Windows design, on\n>     par with:\n>     [..]\n>\n> -   `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n>     [..]\n>     \"\"\"\n>\n>     OP_CAT is an extremely valuable op code. I understand why it was\n>     removed as the situation at the time with scripts was dire. However\n>     most of the protocols I've wanted to build on Bitcoin run into the\n>     limitation that stack values can not be concatenated. For instance\n>     TumbleBit would have far smaller transaction sizes if OP_CAT was\n>     supported in Bitcoin. If it happens to me as a researcher it is\n>     probably holding other people back as well. If I could wave a magic\n>     wand and turn on one of the disabled op codes it would be OP_CAT. Of\n>     course with the change that size of each concatenated value must be 64\n>     Bytes or less.\n\nWhy 64 bytes in particular?\n\nIt seems obvious to me that this 64 bytes is most suited for building Merkle trees, being the size of two SHA256 hashes.\n\nHowever we have had issues with the use of Merkle trees in Bitcoin blocks.\nSpecifically, it is difficult to determine if a hash on a Merkle node is the hash of a Merkle subnode, or a leaf transaction.\nMy understanding is that this is the reason for now requiring transactions to be at least 80 bytes.\n\nThe obvious fix would be to prepend the type of the hashed object, i.e. add at least one byte to determine this type.\nTaproot for example uses tagged hash functions, with a different tag for leaves, and tagged hashes are just prepend-this-32-byte-constant-twice-before-you-SHA256.\n\nThis seems to indicate that to check merkle tree proofs, an `OP_CAT` with only 64 bytes max output size would not be sufficient.\n\nOr we could implement tagged SHA256 as a new opcode...\n\nRegards,\nZmnSCPxj\n\n\n>\n>     On Tue, Oct 1, 2019 at 10:04 PM ZmnSCPxj via bitcoin-dev\n>     bitcoin-dev at lists.linuxfoundation.org wrote:\n>\n>\n> > Good morning lists,\n> > Let me propose the below radical idea:\n> >\n> > -   `SIGHASH` flags attached to signatures are a misdesign, sadly retained from the original BitCoin 0.1.0 Alpha for Windows design, on par with:\n> >     -   1 RETURN\n> >     -   higher-`nSequence` replacement\n> >     -   DER-encoded pubkeys\n> >     -   unrestricted `scriptPubKey`\n> >     -   Payee-security-paid-by-payer (i.e. lack of P2SH)\n> >     -   `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n> >     -   transaction malleability\n> >     -   probably many more\n> >\n> > So let me propose the more radical excision, starting with SegWit v1:\n> >\n> > -   Remove `SIGHASH` from signatures.\n> > -   Put `SIGHASH` on public keys.\n> >\n> > Public keys are now encoded as either 33-bytes (implicit `SIGHASH_ALL`) or 34-bytes (`SIGHASH` byte, followed by pubkey type, followed by pubkey coordinate).\n> > `OP_CHECKSIG` and friends then look at the public key to determine sighash algorithm rather than the signature.\n> > As we expect public keys to be indirectly committed to on every output `scriptPubKey`, this is automatically output tagging to allow particular `SIGHASH`.\n> > However, we can then utilize the many many ways to hide public keys away until they are needed, exemplified in MAST-inside-Taproot.\n> > I propose also the addition of the opcode:\n> >\n> >     <sighash> <pubkey> OP_SETPUBKEYSIGHASH\n> >\n> >\n> > -   `sighash` must be one byte.\n> > -   `pubkey` may be the special byte `0x1`, meaning \"just use the Taproot internal pubkey\".\n> > -   `pubkey` may be 33-byte public key, in which case the `sighash` byte is just prepended to it.\n> > -   `pubkey` may be 34-byte public key with sighash, in which case the first byte is replaced with `sighash` byte.\n> > -   If `sighash` is `0x00` then the result is a 33-byte public key (the sighash byte is removed) i.e. `SIGHASH_ALL` implicit.\n> >\n> > This retains the old feature where the sighash is selected at time-of-spending rather than time-of-payment.\n> > This is done by using the script:\n> >\n> >     <pubkey> OP_SETPUBKEYSIGHASH OP_CHECKSIG\n> >\n> >\n> > Then the sighash can be put in the witness stack after the signature, letting the `SIGHASH` flag be selected at time-of-signing, but only if the SCRIPT specifically is formed to do so.\n> > This is malleability-safe as the signature still commits to the `SIGHASH` it was created for.\n> > However, by default, public keys will not have an attached `SIGHASH` byte, implying `SIGHASH_ALL` (and disallowing-by-default non-`SIGHASH_ALL`).\n> > This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they are allowed only if the output specifically says they are allowed.\n> > Would this not be a superior solution?\n> > Regards,\n> > ZmnSCPxj\n> >\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "Lloyd Fournier",
                "date": "2019-10-06T07:02:52",
                "message_text_only": "Hi Thread,\n\nI made a reply to the OP but didn't \"reply all\" so it just went directly to\nEthan. Since the comments were interesting I'll attempt to salvage them by\nposting them in full:\n\n== Lloyd's post ==\nHi Ethan,\n\nI'd be interested to know what protocols you need OP_CAT for. I'm trying to\nfigure out if there really exists any script based protocol that doesn't\nhave a more efficient scriptless counterpart.  For example,\nA\u00b2L[1] achieves the same thing as Tumblebit but requires no script. I can\nimagine paying based on a merkle path could be useful, but a protocol was\nrecently suggested on lightning-dev [2] that does this but without OP_CAT\n(and without any script!).\n\n\n[1] https://eprint.iacr.org/2019/589.pdf\n[2]\nhttps://www.mail-archive.com/lightning-dev@lists.linuxfoundation.org/msg01427.html\n(*I linked to the wrong thread in the original email*).\n\nLL\n\n== Ethan's response ==\nHi Lloyd,\n\nThanks for your response. I am not sure if you intended to take this off\nlist or not.\n\nI plan to at some point to enumerate in detail protocols that OP_CAT would\nbenefit. A more important point is that OP_CAT is a basic building block\nand that we don't know what future protocols it would allow. In my own\nresearch I have avoiding going down certain paths because it isn't worth\nthe time to investigate knowing that OP_CAT wouldn't make the protocol\npractical.\n\nIn regards to scriptless scripts they almost always require an interactive\nprotocol and sometimes ZKPs. A2L is very impressive but like TumbleBit it\nplaces a large burden on the developer. Additionally I am aware of no way\nto reveal a subset of preimages with scriptless scripts, do a conditioned\nreveal i.e. these preimages can only spend under these two pubkeys and\ntimelockA where after timelockZ this other pubkey can spend without a\npreimages. Scriptless scripts are a fantastic tool but they shouldn't be\nthe only tool that we have.\n\nI'm not sure I follow what you are saying with [2]\n\nThis brings me back a philosophical point:\nBitcoin should give people basic tools to build protocols without first\nknowing what all those protocols are especially when those tools have very\nlittle downside.\n\nI really appreciate your comments.\n\nThanks,\nEthan\n==\n\n*Back to normal thread*\n\nHi Ethan,\n\nThanks for the insightful reply and sorry for my mailing list errors.\n\n> I plan to at some point to enumerate in detail protocols that OP_CAT\nwould benefit.\n\nSweet. Thanks.\n\n> Additionally I am aware of no way to reveal a subset of preimages with\nscriptless scripts, do a conditioned reveal i.e. these preimages can only\nspend under these two pubkeys and timelockA where after timelockZ this\nother pubkey can spend without a preimages. Scriptless scripts are a\nfantastic tool but they shouldn't be the only tool that we have.\n\nYes. With adaptor signatures there is no way to reveal more than one\npre-image; you are limited to revealing a single scalar. But you can have\nmultiple transactions spending from the same output, each with a different\nset of scriptless conditions (absolute time locks, relative time locks and\npre-image reveal). This is enough to achieve what I think you are\ndescribing. FWIW there's a growing consensus that you can do lightning\nwithout script [1]. Perhaps we can't do everything with this technique. My\ncurrent focus is figuring out what useful things we can't do like this\n(even if we were to go wild and add whatever opcodes we wanted). So far it\nlooks like covenants are the main exception.\n\n> I'm not sure I follow what you are saying with [2]\n\nThat is perfectly understandable as I linked the wrong thread (sorry!).\nHere's the right one:\nhttps://www.mail-archive.com/lightning-dev@lists.linuxfoundation.org/msg01427.html\n\nI was pointing to the surprising result that you can actually pay for a\nmerkle path with a particular merkle root leading to a particular leaf that\nyou're interested in without validating the merkle path on chain (e.g.\nOP_CAT and OP_SHA256). The catch is that the leaves have to be pedersen\ncommitments and you prove the existence of your data in the merkle root by\nshowing an opening to the leaf pedersen commitment. This may not be general\nenough to cover every merkle tree use case (but I'm not sure what those\nare!).\n\n> This brings me back a philosophical point:\n> Bitcoin should give people basic tools to build protocols without first\nknowing what all those protocols are especially when those tools have very\nlittle downside.\n\nThis is a really powerful idea. But I've started feeling like you have to\njust design the layer 2 protocols first and then design layer 1! It seems\nlike almost every protocol that people want to make requires very\nparticular fundamental changes: SegWit for LN-penalty and NOINPUT for eltoo\nfor example. On top of that it seems like just having the right signature\nscheme (schnorr) at layer 1 is enough to enable most useful stuff in an\nelegant way.\n\n[1]\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-September/017309.html\n\nCheers,\n\nLL\n\nOn Fri, Oct 4, 2019 at 1:08 AM Ethan Heilman <eth3rs at gmail.com> wrote:\n\n> To avoid derailing the NO_INPUT conversation, I have changed the\n> subject to OP_CAT.\n>\n> Responding to:\n> \"\"\"\n> * `SIGHASH` flags attached to signatures are a misdesign, sadly\n> retained from the original BitCoin 0.1.0 Alpha for Windows design, on\n> par with:\n> [..]\n> * `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n> [..]\n> \"\"\"\n>\n> OP_CAT is an extremely valuable op code. I understand why it was\n> removed as the situation at the time with scripts was dire. However\n> most of the protocols I've wanted to build on Bitcoin run into the\n> limitation that stack values can not be concatenated. For instance\n> TumbleBit would have far smaller transaction sizes if OP_CAT was\n> supported in Bitcoin. If it happens to me as a researcher it is\n> probably holding other people back as well. If I could wave a magic\n> wand and turn on one of the disabled op codes it would be OP_CAT.  Of\n> course with the change that size of each concatenated value must be 64\n> Bytes or less.\n>\n>\n> On Tue, Oct 1, 2019 at 10:04 PM ZmnSCPxj via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >\n> > Good morning lists,\n> >\n> > Let me propose the below radical idea:\n> >\n> > * `SIGHASH` flags attached to signatures are a misdesign, sadly retained\n> from the original BitCoin 0.1.0 Alpha for Windows design, on par with:\n> >   * 1 RETURN\n> >   * higher-`nSequence` replacement\n> >   * DER-encoded pubkeys\n> >   * unrestricted `scriptPubKey`\n> >   * Payee-security-paid-by-payer (i.e. lack of P2SH)\n> >   * `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n> >   * transaction malleability\n> >   * probably many more\n> >\n> > So let me propose the more radical excision, starting with SegWit v1:\n> >\n> > * Remove `SIGHASH` from signatures.\n> > * Put `SIGHASH` on public keys.\n> >\n> > Public keys are now encoded as either 33-bytes (implicit `SIGHASH_ALL`)\n> or 34-bytes (`SIGHASH` byte, followed by pubkey type, followed by pubkey\n> coordinate).\n> > `OP_CHECKSIG` and friends then look at the *public key* to determine\n> sighash algorithm rather than the signature.\n> >\n> > As we expect public keys to be indirectly committed to on every output\n> `scriptPubKey`, this is automatically output tagging to allow particular\n> `SIGHASH`.\n> > However, we can then utilize the many many ways to hide public keys away\n> until they are needed, exemplified in MAST-inside-Taproot.\n> >\n> > I propose also the addition of the opcode:\n> >\n> >     <sighash> <pubkey> OP_SETPUBKEYSIGHASH\n> >\n> > * `sighash` must be one byte.\n> > * `pubkey` may be the special byte `0x1`, meaning \"just use the Taproot\n> internal pubkey\".\n> > * `pubkey` may be 33-byte public key, in which case the `sighash` byte\n> is just prepended to it.\n> > * `pubkey` may be 34-byte public key with sighash, in which case the\n> first byte is replaced with `sighash` byte.\n> > * If `sighash` is `0x00` then the result is a 33-byte public key (the\n> sighash byte is removed) i.e. `SIGHASH_ALL` implicit.\n> >\n> > This retains the old feature where the sighash is selected at\n> time-of-spending rather than time-of-payment.\n> > This is done by using the script:\n> >\n> >     <pubkey> OP_SETPUBKEYSIGHASH OP_CHECKSIG\n> >\n> > Then the sighash can be put in the witness stack after the signature,\n> letting the `SIGHASH` flag be selected at time-of-signing, but only if the\n> SCRIPT specifically is formed to do so.\n> > This is malleability-safe as the signature still commits to the\n> `SIGHASH` it was created for.\n> >\n> > However, by default, public keys will not have an attached `SIGHASH`\n> byte, implying `SIGHASH_ALL` (and disallowing-by-default non-`SIGHASH_ALL`).\n> >\n> > This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they\n> are allowed only if the output specifically says they are allowed.\n> >\n> > Would this not be a superior solution?\n> >\n> > Regards,\n> > ZmnSCPxj\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191006/f0e27c02/attachment-0001.html>"
            },
            {
                "author": "Andrew Poelstra",
                "date": "2019-10-09T16:56:51",
                "message_text_only": "On Thu, Oct 03, 2019 at 11:05:52AM -0400, Ethan Heilman wrote:\n> To avoid derailing the NO_INPUT conversation, I have changed the\n> subject to OP_CAT.\n> \n> Responding to:\n> \"\"\"\n> * `SIGHASH` flags attached to signatures are a misdesign, sadly\n> retained from the original BitCoin 0.1.0 Alpha for Windows design, on\n> par with:\n> [..]\n> * `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n> [..]\n> \"\"\"\n> \n> OP_CAT is an extremely valuable op code. I understand why it was\n> removed as the situation at the time with scripts was dire. However\n> most of the protocols I've wanted to build on Bitcoin run into the\n> limitation that stack values can not be concatenated. For instance\n> TumbleBit would have far smaller transaction sizes if OP_CAT was\n> supported in Bitcoin. If it happens to me as a researcher it is\n> probably holding other people back as well. If I could wave a magic\n> wand and turn on one of the disabled op codes it would be OP_CAT.  Of\n> course with the change that size of each concatenated value must be 64\n> Bytes or less.\n>\n\nJust throwing my two cents in here - as others have noted, OP_CAT\nlets you create Merkle trees (allowing e.g. log-sized accountable\nthreshold sigs, at least in a post-Schnorr future).\n\nIt also allows manipulating signatures - e.g. forcing the revelation\nof discrete logs by requiring the user use the (1/2) point as a nonce\n(this starts with 11 zero bytes, which no other computationally\naccessible point does), or by requiring two sigs with the same nonce.\n\nIt also lets you do proof-of-work-like computations on hashes or\ncurvepoints; or enforce that EC points come from a hash and have\nno known discrete log. You can also switch on hashes, something\ncurrently impossible because of the 4-byte limitation on numeric\nopcodes. I don't have specific application of these in mind but\ndefinitely have cut off many lines of inquiry because they were\nimpossible.\n\nYou could build a crappy Lamport signature, though the key would\nbe so big that you'd never do this pre-MAST :P.\n\n\n-- \nAndrew Poelstra\nDirector of Research, Blockstream\nEmail: apoelstra at wpsoftware.net\nWeb:   https://www.wpsoftware.net/andrew\n\nThe sun is always shining in space\n    -Justin Lewis-Webster\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 488 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191009/8edaa1f9/attachment.sig>"
            },
            {
                "author": "Ethan Heilman",
                "date": "2019-10-04T00:48:17",
                "message_text_only": "I hope you are having an great afternoon ZmnSCPxj,\n\nYou make an excellent point!\n\nI had thought about doing the following to tag nodes\n\n|| means OP_CAT\n\n`node = SHA256(type||SHA256(data))`\nso a subnode would be\n`subnode1 = SHA256(1||SHA256(subnode2||subnode3))`\nand a leaf node would be\n`leafnode = SHA256(0||SHA256(leafdata))`\n\nYet, I like your idea better. Increasing the size of the two inputs to\nOP_CAT to be 260 Bytes each where 520 Bytes is the maximum allowable\nsize of object on the stack seems sensible and also doesn't special\ncase the logic of OP_CAT.\n\nIt would also increase performance. SHA256(tag||subnode2||subnode3)\nrequires 2 compression function calls whereas\nSHA256(1||SHA256(subnode2||subnode3)) requires 2+1=3 compression\nfunction calls (due to padding).\n\n>Or we could implement tagged SHA256 as a new opcode...\n\nI agree that tagged SHA256 as an op code that would certainty be\nuseful, but OP_CAT provides far more utility and is a simpler change.\n\nThanks,\nEthan\n\nOn Thu, Oct 3, 2019 at 7:42 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n>\n> Good morning Ethan,\n>\n>\n> > To avoid derailing the NO_INPUT conversation, I have changed the\n> > subject to OP_CAT.\n> >\n> > Responding to:\n> > \"\"\"\n> >\n> > -   `SIGHASH` flags attached to signatures are a misdesign, sadly\n> >     retained from the original BitCoin 0.1.0 Alpha for Windows design, on\n> >     par with:\n> >     [..]\n> >\n> > -   `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n> >     [..]\n> >     \"\"\"\n> >\n> >     OP_CAT is an extremely valuable op code. I understand why it was\n> >     removed as the situation at the time with scripts was dire. However\n> >     most of the protocols I've wanted to build on Bitcoin run into the\n> >     limitation that stack values can not be concatenated. For instance\n> >     TumbleBit would have far smaller transaction sizes if OP_CAT was\n> >     supported in Bitcoin. If it happens to me as a researcher it is\n> >     probably holding other people back as well. If I could wave a magic\n> >     wand and turn on one of the disabled op codes it would be OP_CAT. Of\n> >     course with the change that size of each concatenated value must be 64\n> >     Bytes or less.\n>\n> Why 64 bytes in particular?\n>\n> It seems obvious to me that this 64 bytes is most suited for building Merkle trees, being the size of two SHA256 hashes.\n>\n> However we have had issues with the use of Merkle trees in Bitcoin blocks.\n> Specifically, it is difficult to determine if a hash on a Merkle node is the hash of a Merkle subnode, or a leaf transaction.\n> My understanding is that this is the reason for now requiring transactions to be at least 80 bytes.\n>\n> The obvious fix would be to prepend the type of the hashed object, i.e. add at least one byte to determine this type.\n> Taproot for example uses tagged hash functions, with a different tag for leaves, and tagged hashes are just prepend-this-32-byte-constant-twice-before-you-SHA256.\n>\n> This seems to indicate that to check merkle tree proofs, an `OP_CAT` with only 64 bytes max output size would not be sufficient.\n>\n> Or we could implement tagged SHA256 as a new opcode...\n>\n> Regards,\n> ZmnSCPxj\n>\n>\n> >\n> >     On Tue, Oct 1, 2019 at 10:04 PM ZmnSCPxj via bitcoin-dev\n> >     bitcoin-dev at lists.linuxfoundation.org wrote:\n> >\n> >\n> > > Good morning lists,\n> > > Let me propose the below radical idea:\n> > >\n> > > -   `SIGHASH` flags attached to signatures are a misdesign, sadly retained from the original BitCoin 0.1.0 Alpha for Windows design, on par with:\n> > >     -   1 RETURN\n> > >     -   higher-`nSequence` replacement\n> > >     -   DER-encoded pubkeys\n> > >     -   unrestricted `scriptPubKey`\n> > >     -   Payee-security-paid-by-payer (i.e. lack of P2SH)\n> > >     -   `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n> > >     -   transaction malleability\n> > >     -   probably many more\n> > >\n> > > So let me propose the more radical excision, starting with SegWit v1:\n> > >\n> > > -   Remove `SIGHASH` from signatures.\n> > > -   Put `SIGHASH` on public keys.\n> > >\n> > > Public keys are now encoded as either 33-bytes (implicit `SIGHASH_ALL`) or 34-bytes (`SIGHASH` byte, followed by pubkey type, followed by pubkey coordinate).\n> > > `OP_CHECKSIG` and friends then look at the public key to determine sighash algorithm rather than the signature.\n> > > As we expect public keys to be indirectly committed to on every output `scriptPubKey`, this is automatically output tagging to allow particular `SIGHASH`.\n> > > However, we can then utilize the many many ways to hide public keys away until they are needed, exemplified in MAST-inside-Taproot.\n> > > I propose also the addition of the opcode:\n> > >\n> > >     <sighash> <pubkey> OP_SETPUBKEYSIGHASH\n> > >\n> > >\n> > > -   `sighash` must be one byte.\n> > > -   `pubkey` may be the special byte `0x1`, meaning \"just use the Taproot internal pubkey\".\n> > > -   `pubkey` may be 33-byte public key, in which case the `sighash` byte is just prepended to it.\n> > > -   `pubkey` may be 34-byte public key with sighash, in which case the first byte is replaced with `sighash` byte.\n> > > -   If `sighash` is `0x00` then the result is a 33-byte public key (the sighash byte is removed) i.e. `SIGHASH_ALL` implicit.\n> > >\n> > > This retains the old feature where the sighash is selected at time-of-spending rather than time-of-payment.\n> > > This is done by using the script:\n> > >\n> > >     <pubkey> OP_SETPUBKEYSIGHASH OP_CHECKSIG\n> > >\n> > >\n> > > Then the sighash can be put in the witness stack after the signature, letting the `SIGHASH` flag be selected at time-of-signing, but only if the SCRIPT specifically is formed to do so.\n> > > This is malleability-safe as the signature still commits to the `SIGHASH` it was created for.\n> > > However, by default, public keys will not have an attached `SIGHASH` byte, implying `SIGHASH_ALL` (and disallowing-by-default non-`SIGHASH_ALL`).\n> > > This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they are allowed only if the output specifically says they are allowed.\n> > > Would this not be a superior solution?\n> > > Regards,\n> > > ZmnSCPxj\n> > >\n> > > bitcoin-dev mailing list\n> > > bitcoin-dev at lists.linuxfoundation.org\n> > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> >\n> > Lightning-dev mailing list\n> > Lightning-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n>"
            },
            {
                "author": "Jeremy",
                "date": "2019-10-04T05:02:14",
                "message_text_only": "Awhile back, Ethan and I discussed having, rather than OP_CAT, an\nOP_SHA256STREAM that uses the streaming properties of a SHA256 hash\nfunction to allow concatenation of an unlimited amount of data, provided\nthe only use is to hash it.\n\nYou can then use it perhaps as follows:\n\n// start a new hash with item\nOP_SHA256STREAM  (-1) -> [state]\n// Add item to the hash in state\nOP_SHA256STREAM n [item] [state] -> [state]\n// Finalize\nOP_SHA256STREAM (-2) [state] -> [Hash]\n\n<-1> OP_SHA256STREAM <tag> <subnode 2> <subnode 3> <3> OP_SHA256STREAM <-2>\nOP_SHA256STREAM\n\n\nOr it coul\n\n\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n\n\nOn Thu, Oct 3, 2019 at 8:04 PM Ethan Heilman <eth3rs at gmail.com> wrote:\n\n> I hope you are having an great afternoon ZmnSCPxj,\n>\n> You make an excellent point!\n>\n> I had thought about doing the following to tag nodes\n>\n> || means OP_CAT\n>\n> `node = SHA256(type||SHA256(data))`\n> so a subnode would be\n> `subnode1 = SHA256(1||SHA256(subnode2||subnode3))`\n> and a leaf node would be\n> `leafnode = SHA256(0||SHA256(leafdata))`\n>\n> Yet, I like your idea better. Increasing the size of the two inputs to\n> OP_CAT to be 260 Bytes each where 520 Bytes is the maximum allowable\n> size of object on the stack seems sensible and also doesn't special\n> case the logic of OP_CAT.\n>\n> It would also increase performance. SHA256(tag||subnode2||subnode3)\n> requires 2 compression function calls whereas\n> SHA256(1||SHA256(subnode2||subnode3)) requires 2+1=3 compression\n> function calls (due to padding).\n>\n> >Or we could implement tagged SHA256 as a new opcode...\n>\n> I agree that tagged SHA256 as an op code that would certainty be\n> useful, but OP_CAT provides far more utility and is a simpler change.\n>\n> Thanks,\n> Ethan\n>\n> On Thu, Oct 3, 2019 at 7:42 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n> >\n> > Good morning Ethan,\n> >\n> >\n> > > To avoid derailing the NO_INPUT conversation, I have changed the\n> > > subject to OP_CAT.\n> > >\n> > > Responding to:\n> > > \"\"\"\n> > >\n> > > -   `SIGHASH` flags attached to signatures are a misdesign, sadly\n> > >     retained from the original BitCoin 0.1.0 Alpha for Windows design,\n> on\n> > >     par with:\n> > >     [..]\n> > >\n> > > -   `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n> > >     [..]\n> > >     \"\"\"\n> > >\n> > >     OP_CAT is an extremely valuable op code. I understand why it was\n> > >     removed as the situation at the time with scripts was dire. However\n> > >     most of the protocols I've wanted to build on Bitcoin run into the\n> > >     limitation that stack values can not be concatenated. For instance\n> > >     TumbleBit would have far smaller transaction sizes if OP_CAT was\n> > >     supported in Bitcoin. If it happens to me as a researcher it is\n> > >     probably holding other people back as well. If I could wave a magic\n> > >     wand and turn on one of the disabled op codes it would be OP_CAT.\n> Of\n> > >     course with the change that size of each concatenated value must\n> be 64\n> > >     Bytes or less.\n> >\n> > Why 64 bytes in particular?\n> >\n> > It seems obvious to me that this 64 bytes is most suited for building\n> Merkle trees, being the size of two SHA256 hashes.\n> >\n> > However we have had issues with the use of Merkle trees in Bitcoin\n> blocks.\n> > Specifically, it is difficult to determine if a hash on a Merkle node is\n> the hash of a Merkle subnode, or a leaf transaction.\n> > My understanding is that this is the reason for now requiring\n> transactions to be at least 80 bytes.\n> >\n> > The obvious fix would be to prepend the type of the hashed object, i.e.\n> add at least one byte to determine this type.\n> > Taproot for example uses tagged hash functions, with a different tag for\n> leaves, and tagged hashes are just\n> prepend-this-32-byte-constant-twice-before-you-SHA256.\n> >\n> > This seems to indicate that to check merkle tree proofs, an `OP_CAT`\n> with only 64 bytes max output size would not be sufficient.\n> >\n> > Or we could implement tagged SHA256 as a new opcode...\n> >\n> > Regards,\n> > ZmnSCPxj\n> >\n> >\n> > >\n> > >     On Tue, Oct 1, 2019 at 10:04 PM ZmnSCPxj via bitcoin-dev\n> > >     bitcoin-dev at lists.linuxfoundation.org wrote:\n> > >\n> > >\n> > > > Good morning lists,\n> > > > Let me propose the below radical idea:\n> > > >\n> > > > -   `SIGHASH` flags attached to signatures are a misdesign, sadly\n> retained from the original BitCoin 0.1.0 Alpha for Windows design, on par\n> with:\n> > > >     -   1 RETURN\n> > > >     -   higher-`nSequence` replacement\n> > > >     -   DER-encoded pubkeys\n> > > >     -   unrestricted `scriptPubKey`\n> > > >     -   Payee-security-paid-by-payer (i.e. lack of P2SH)\n> > > >     -   `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n> > > >     -   transaction malleability\n> > > >     -   probably many more\n> > > >\n> > > > So let me propose the more radical excision, starting with SegWit v1:\n> > > >\n> > > > -   Remove `SIGHASH` from signatures.\n> > > > -   Put `SIGHASH` on public keys.\n> > > >\n> > > > Public keys are now encoded as either 33-bytes (implicit\n> `SIGHASH_ALL`) or 34-bytes (`SIGHASH` byte, followed by pubkey type,\n> followed by pubkey coordinate).\n> > > > `OP_CHECKSIG` and friends then look at the public key to determine\n> sighash algorithm rather than the signature.\n> > > > As we expect public keys to be indirectly committed to on every\n> output `scriptPubKey`, this is automatically output tagging to allow\n> particular `SIGHASH`.\n> > > > However, we can then utilize the many many ways to hide public keys\n> away until they are needed, exemplified in MAST-inside-Taproot.\n> > > > I propose also the addition of the opcode:\n> > > >\n> > > >     <sighash> <pubkey> OP_SETPUBKEYSIGHASH\n> > > >\n> > > >\n> > > > -   `sighash` must be one byte.\n> > > > -   `pubkey` may be the special byte `0x1`, meaning \"just use the\n> Taproot internal pubkey\".\n> > > > -   `pubkey` may be 33-byte public key, in which case the `sighash`\n> byte is just prepended to it.\n> > > > -   `pubkey` may be 34-byte public key with sighash, in which case\n> the first byte is replaced with `sighash` byte.\n> > > > -   If `sighash` is `0x00` then the result is a 33-byte public key\n> (the sighash byte is removed) i.e. `SIGHASH_ALL` implicit.\n> > > >\n> > > > This retains the old feature where the sighash is selected at\n> time-of-spending rather than time-of-payment.\n> > > > This is done by using the script:\n> > > >\n> > > >     <pubkey> OP_SETPUBKEYSIGHASH OP_CHECKSIG\n> > > >\n> > > >\n> > > > Then the sighash can be put in the witness stack after the\n> signature, letting the `SIGHASH` flag be selected at time-of-signing, but\n> only if the SCRIPT specifically is formed to do so.\n> > > > This is malleability-safe as the signature still commits to the\n> `SIGHASH` it was created for.\n> > > > However, by default, public keys will not have an attached `SIGHASH`\n> byte, implying `SIGHASH_ALL` (and disallowing-by-default non-`SIGHASH_ALL`).\n> > > > This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as\n> they are allowed only if the output specifically says they are allowed.\n> > > > Would this not be a superior solution?\n> > > > Regards,\n> > > > ZmnSCPxj\n> > > >\n> > > > bitcoin-dev mailing list\n> > > > bitcoin-dev at lists.linuxfoundation.org\n> > > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> > >\n> > > Lightning-dev mailing list\n> > > Lightning-dev at lists.linuxfoundation.org\n> > > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n> >\n> >\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191003/e3a709c9/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-04T07:00:13",
                "message_text_only": "Good morning Jeremy,\n\n> Awhile back, Ethan and I discussed having, rather than OP_CAT, an OP_SHA256STREAM that uses the streaming properties of a SHA256 hash function to allow concatenation of an unlimited amount of data, provided the only use is to hash it.\n>\n> You can then use it perhaps as follows:\n>\n> // start a new hash with item\n> OP_SHA256STREAM\u00a0 (-1) -> [state]\n> // Add item to the hash in state\n> OP_SHA256STREAM n [item] [state] -> [state]\n> // Finalize\n> OP_SHA256STREAM (-2) [state] -> [Hash]\n>\n> <-1> OP_SHA256STREAM <tag> <subnode 2> <subnode 3> <3> OP_SHA256STREAM <-2> OP_SHA256STREAM\n>\n> Or it coul\n>\n\nThis seems a good idea.\n\nThough it brings up the age-old tension between:\n\n* Generically-useable components, but due to generalization are less efficient.\n* Specific-use components, which are efficient, but which may end up not being useable in the future.\n\nIn particular, `OP_SHA256STREAM` would no longer be useable if SHA256 eventually is broken, while the `OP_CAT` will still be useable in the indefinite future.\nIn the future a new hash function can simply be defined and the same technique with `OP_CAT` would still be useable.\n\n\nRegards,\nZmnSCPxj\n\n> --\n> @JeremyRubin\n>\n> On Thu, Oct 3, 2019 at 8:04 PM Ethan Heilman <eth3rs at gmail.com> wrote:\n>\n> > I hope you are having an great afternoon ZmnSCPxj,\n> >\n> > You make an excellent point!\n> >\n> > I had thought about doing the following to tag nodes\n> >\n> > || means OP_CAT\n> >\n> > `node = SHA256(type||SHA256(data))`\n> > so a subnode would be\n> > `subnode1 = SHA256(1||SHA256(subnode2||subnode3))`\n> > and a leaf node would be\n> > `leafnode = SHA256(0||SHA256(leafdata))`\n> >\n> > Yet, I like your idea better. Increasing the size of the two inputs to\n> > OP_CAT to be 260 Bytes each where 520 Bytes is the maximum allowable\n> > size of object on the stack seems sensible and also doesn't special\n> > case the logic of OP_CAT.\n> >\n> > It would also increase performance. SHA256(tag||subnode2||subnode3)\n> > requires 2 compression function calls whereas\n> > SHA256(1||SHA256(subnode2||subnode3)) requires 2+1=3 compression\n> > function calls (due to padding).\n> >\n> > >Or we could implement tagged SHA256 as a new opcode...\n> >\n> > I agree that tagged SHA256 as an op code that would certainty be\n> > useful, but OP_CAT provides far more utility and is a simpler change.\n> >\n> > Thanks,\n> > Ethan\n> >\n> > On Thu, Oct 3, 2019 at 7:42 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n> > >\n> > > Good morning Ethan,\n> > >\n> > >\n> > > > To avoid derailing the NO_INPUT conversation, I have changed the\n> > > > subject to OP_CAT.\n> > > >\n> > > > Responding to:\n> > > > \"\"\"\n> > > >\n> > > > -\u00a0 \u00a0`SIGHASH` flags attached to signatures are a misdesign, sadly\n> > > >\u00a0 \u00a0 \u00a0retained from the original BitCoin 0.1.0 Alpha for Windows design, on\n> > > >\u00a0 \u00a0 \u00a0par with:\n> > > >\u00a0 \u00a0 \u00a0[..]\n> > > >\n> > > > -\u00a0 \u00a0`OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n> > > >\u00a0 \u00a0 \u00a0[..]\n> > > >\u00a0 \u00a0 \u00a0\"\"\"\n> > > >\n> > > >\u00a0 \u00a0 \u00a0OP_CAT is an extremely valuable op code. I understand why it was\n> > > >\u00a0 \u00a0 \u00a0removed as the situation at the time with scripts was dire. However\n> > > >\u00a0 \u00a0 \u00a0most of the protocols I've wanted to build on Bitcoin run into the\n> > > >\u00a0 \u00a0 \u00a0limitation that stack values can not be concatenated. For instance\n> > > >\u00a0 \u00a0 \u00a0TumbleBit would have far smaller transaction sizes if OP_CAT was\n> > > >\u00a0 \u00a0 \u00a0supported in Bitcoin. If it happens to me as a researcher it is\n> > > >\u00a0 \u00a0 \u00a0probably holding other people back as well. If I could wave a magic\n> > > >\u00a0 \u00a0 \u00a0wand and turn on one of the disabled op codes it would be OP_CAT. Of\n> > > >\u00a0 \u00a0 \u00a0course with the change that size of each concatenated value must be 64\n> > > >\u00a0 \u00a0 \u00a0Bytes or less.\n> > >\n> > > Why 64 bytes in particular?\n> > >\n> > > It seems obvious to me that this 64 bytes is most suited for building Merkle trees, being the size of two SHA256 hashes.\n> > >\n> > > However we have had issues with the use of Merkle trees in Bitcoin blocks.\n> > > Specifically, it is difficult to determine if a hash on a Merkle node is the hash of a Merkle subnode, or a leaf transaction.\n> > > My understanding is that this is the reason for now requiring transactions to be at least 80 bytes.\n> > >\n> > > The obvious fix would be to prepend the type of the hashed object, i.e. add at least one byte to determine this type.\n> > > Taproot for example uses tagged hash functions, with a different tag for leaves, and tagged hashes are just prepend-this-32-byte-constant-twice-before-you-SHA256.\n> > >\n> > > This seems to indicate that to check merkle tree proofs, an `OP_CAT` with only 64 bytes max output size would not be sufficient.\n> > >\n> > > Or we could implement tagged SHA256 as a new opcode...\n> > >\n> > > Regards,\n> > > ZmnSCPxj\n> > >\n> > >\n> > > >\n> > > >\u00a0 \u00a0 \u00a0On Tue, Oct 1, 2019 at 10:04 PM ZmnSCPxj via bitcoin-dev\n> > > >\u00a0 \u00a0 \u00a0bitcoin-dev at lists.linuxfoundation.org wrote:\n> > > >\n> > > >\n> > > > > Good morning lists,\n> > > > > Let me propose the below radical idea:\n> > > > >\n> > > > > -\u00a0 \u00a0`SIGHASH` flags attached to signatures are a misdesign, sadly retained from the original BitCoin 0.1.0 Alpha for Windows design, on par with:\n> > > > >\u00a0 \u00a0 \u00a0-\u00a0 \u00a01 RETURN\n> > > > >\u00a0 \u00a0 \u00a0-\u00a0 \u00a0higher-`nSequence` replacement\n> > > > >\u00a0 \u00a0 \u00a0-\u00a0 \u00a0DER-encoded pubkeys\n> > > > >\u00a0 \u00a0 \u00a0-\u00a0 \u00a0unrestricted `scriptPubKey`\n> > > > >\u00a0 \u00a0 \u00a0-\u00a0 \u00a0Payee-security-paid-by-payer (i.e. lack of P2SH)\n> > > > >\u00a0 \u00a0 \u00a0-\u00a0 \u00a0`OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n> > > > >\u00a0 \u00a0 \u00a0-\u00a0 \u00a0transaction malleability\n> > > > >\u00a0 \u00a0 \u00a0-\u00a0 \u00a0probably many more\n> > > > >\n> > > > > So let me propose the more radical excision, starting with SegWit v1:\n> > > > >\n> > > > > -\u00a0 \u00a0Remove `SIGHASH` from signatures.\n> > > > > -\u00a0 \u00a0Put `SIGHASH` on public keys.\n> > > > >\n> > > > > Public keys are now encoded as either 33-bytes (implicit `SIGHASH_ALL`) or 34-bytes (`SIGHASH` byte, followed by pubkey type, followed by pubkey coordinate).\n> > > > > `OP_CHECKSIG` and friends then look at the public key to determine sighash algorithm rather than the signature.\n> > > > > As we expect public keys to be indirectly committed to on every output `scriptPubKey`, this is automatically output tagging to allow particular `SIGHASH`.\n> > > > > However, we can then utilize the many many ways to hide public keys away until they are needed, exemplified in MAST-inside-Taproot.\n> > > > > I propose also the addition of the opcode:\n> > > > >\n> > > > >\u00a0 \u00a0 \u00a0<sighash> <pubkey> OP_SETPUBKEYSIGHASH\n> > > > >\n> > > > >\n> > > > > -\u00a0 \u00a0`sighash` must be one byte.\n> > > > > -\u00a0 \u00a0`pubkey` may be the special byte `0x1`, meaning \"just use the Taproot internal pubkey\".\n> > > > > -\u00a0 \u00a0`pubkey` may be 33-byte public key, in which case the `sighash` byte is just prepended to it.\n> > > > > -\u00a0 \u00a0`pubkey` may be 34-byte public key with sighash, in which case the first byte is replaced with `sighash` byte.\n> > > > > -\u00a0 \u00a0If `sighash` is `0x00` then the result is a 33-byte public key (the sighash byte is removed) i.e. `SIGHASH_ALL` implicit.\n> > > > >\n> > > > > This retains the old feature where the sighash is selected at time-of-spending rather than time-of-payment.\n> > > > > This is done by using the script:\n> > > > >\n> > > > >\u00a0 \u00a0 \u00a0<pubkey> OP_SETPUBKEYSIGHASH OP_CHECKSIG\n> > > > >\n> > > > >\n> > > > > Then the sighash can be put in the witness stack after the signature, letting the `SIGHASH` flag be selected at time-of-signing, but only if the SCRIPT specifically is formed to do so.\n> > > > > This is malleability-safe as the signature still commits to the `SIGHASH` it was created for.\n> > > > > However, by default, public keys will not have an attached `SIGHASH` byte, implying `SIGHASH_ALL` (and disallowing-by-default non-`SIGHASH_ALL`).\n> > > > > This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they are allowed only if the output specifically says they are allowed.\n> > > > > Would this not be a superior solution?\n> > > > > Regards,\n> > > > > ZmnSCPxj\n> > > > >\n> > > > > bitcoin-dev mailing list\n> > > > > bitcoin-dev at lists.linuxfoundation.org\n> > > > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> > > >\n> > > > Lightning-dev mailing list\n> > > > Lightning-dev at lists.linuxfoundation.org\n> > > > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n> > >\n> > >\n> > _______________________________________________\n> > Lightning-dev mailing list\n> > Lightning-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "Jeremy",
                "date": "2019-10-04T18:33:09",
                "message_text_only": "Good point -- in our discussion, we called it OP_FFS -- Fold Functional\nStream, and it could be initialized with a different integer to select for\ndifferent functions. Therefore the stream processing opcodes would be\ngeneric, but extensible.\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n\n\nOn Fri, Oct 4, 2019 at 12:00 AM ZmnSCPxj via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> wrote:\n\n> Good morning Jeremy,\n>\n> > Awhile back, Ethan and I discussed having, rather than OP_CAT, an\n> OP_SHA256STREAM that uses the streaming properties of a SHA256 hash\n> function to allow concatenation of an unlimited amount of data, provided\n> the only use is to hash it.\n> >\n> > You can then use it perhaps as follows:\n> >\n> > // start a new hash with item\n> > OP_SHA256STREAM  (-1) -> [state]\n> > // Add item to the hash in state\n> > OP_SHA256STREAM n [item] [state] -> [state]\n> > // Finalize\n> > OP_SHA256STREAM (-2) [state] -> [Hash]\n> >\n> > <-1> OP_SHA256STREAM <tag> <subnode 2> <subnode 3> <3> OP_SHA256STREAM\n> <-2> OP_SHA256STREAM\n> >\n> > Or it coul\n> >\n>\n> This seems a good idea.\n>\n> Though it brings up the age-old tension between:\n>\n> * Generically-useable components, but due to generalization are less\n> efficient.\n> * Specific-use components, which are efficient, but which may end up not\n> being useable in the future.\n>\n> In particular, `OP_SHA256STREAM` would no longer be useable if SHA256\n> eventually is broken, while the `OP_CAT` will still be useable in the\n> indefinite future.\n> In the future a new hash function can simply be defined and the same\n> technique with `OP_CAT` would still be useable.\n>\n>\n> Regards,\n> ZmnSCPxj\n>\n> > --\n> > @JeremyRubin\n> >\n> > On Thu, Oct 3, 2019 at 8:04 PM Ethan Heilman <eth3rs at gmail.com> wrote:\n> >\n> > > I hope you are having an great afternoon ZmnSCPxj,\n> > >\n> > > You make an excellent point!\n> > >\n> > > I had thought about doing the following to tag nodes\n> > >\n> > > || means OP_CAT\n> > >\n> > > `node = SHA256(type||SHA256(data))`\n> > > so a subnode would be\n> > > `subnode1 = SHA256(1||SHA256(subnode2||subnode3))`\n> > > and a leaf node would be\n> > > `leafnode = SHA256(0||SHA256(leafdata))`\n> > >\n> > > Yet, I like your idea better. Increasing the size of the two inputs to\n> > > OP_CAT to be 260 Bytes each where 520 Bytes is the maximum allowable\n> > > size of object on the stack seems sensible and also doesn't special\n> > > case the logic of OP_CAT.\n> > >\n> > > It would also increase performance. SHA256(tag||subnode2||subnode3)\n> > > requires 2 compression function calls whereas\n> > > SHA256(1||SHA256(subnode2||subnode3)) requires 2+1=3 compression\n> > > function calls (due to padding).\n> > >\n> > > >Or we could implement tagged SHA256 as a new opcode...\n> > >\n> > > I agree that tagged SHA256 as an op code that would certainty be\n> > > useful, but OP_CAT provides far more utility and is a simpler change.\n> > >\n> > > Thanks,\n> > > Ethan\n> > >\n> > > On Thu, Oct 3, 2019 at 7:42 PM ZmnSCPxj <ZmnSCPxj at protonmail.com>\n> wrote:\n> > > >\n> > > > Good morning Ethan,\n> > > >\n> > > >\n> > > > > To avoid derailing the NO_INPUT conversation, I have changed the\n> > > > > subject to OP_CAT.\n> > > > >\n> > > > > Responding to:\n> > > > > \"\"\"\n> > > > >\n> > > > > -   `SIGHASH` flags attached to signatures are a misdesign, sadly\n> > > > >     retained from the original BitCoin 0.1.0 Alpha for Windows\n> design, on\n> > > > >     par with:\n> > > > >     [..]\n> > > > >\n> > > > > -   `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n> > > > >     [..]\n> > > > >     \"\"\"\n> > > > >\n> > > > >     OP_CAT is an extremely valuable op code. I understand why it\n> was\n> > > > >     removed as the situation at the time with scripts was dire.\n> However\n> > > > >     most of the protocols I've wanted to build on Bitcoin run into\n> the\n> > > > >     limitation that stack values can not be concatenated. For\n> instance\n> > > > >     TumbleBit would have far smaller transaction sizes if OP_CAT\n> was\n> > > > >     supported in Bitcoin. If it happens to me as a researcher it is\n> > > > >     probably holding other people back as well. If I could wave a\n> magic\n> > > > >     wand and turn on one of the disabled op codes it would be\n> OP_CAT. Of\n> > > > >     course with the change that size of each concatenated value\n> must be 64\n> > > > >     Bytes or less.\n> > > >\n> > > > Why 64 bytes in particular?\n> > > >\n> > > > It seems obvious to me that this 64 bytes is most suited for\n> building Merkle trees, being the size of two SHA256 hashes.\n> > > >\n> > > > However we have had issues with the use of Merkle trees in Bitcoin\n> blocks.\n> > > > Specifically, it is difficult to determine if a hash on a Merkle\n> node is the hash of a Merkle subnode, or a leaf transaction.\n> > > > My understanding is that this is the reason for now requiring\n> transactions to be at least 80 bytes.\n> > > >\n> > > > The obvious fix would be to prepend the type of the hashed object,\n> i.e. add at least one byte to determine this type.\n> > > > Taproot for example uses tagged hash functions, with a different tag\n> for leaves, and tagged hashes are just\n> prepend-this-32-byte-constant-twice-before-you-SHA256.\n> > > >\n> > > > This seems to indicate that to check merkle tree proofs, an `OP_CAT`\n> with only 64 bytes max output size would not be sufficient.\n> > > >\n> > > > Or we could implement tagged SHA256 as a new opcode...\n> > > >\n> > > > Regards,\n> > > > ZmnSCPxj\n> > > >\n> > > >\n> > > > >\n> > > > >     On Tue, Oct 1, 2019 at 10:04 PM ZmnSCPxj via bitcoin-dev\n> > > > >     bitcoin-dev at lists.linuxfoundation.org wrote:\n> > > > >\n> > > > >\n> > > > > > Good morning lists,\n> > > > > > Let me propose the below radical idea:\n> > > > > >\n> > > > > > -   `SIGHASH` flags attached to signatures are a misdesign,\n> sadly retained from the original BitCoin 0.1.0 Alpha for Windows design, on\n> par with:\n> > > > > >     -   1 RETURN\n> > > > > >     -   higher-`nSequence` replacement\n> > > > > >     -   DER-encoded pubkeys\n> > > > > >     -   unrestricted `scriptPubKey`\n> > > > > >     -   Payee-security-paid-by-payer (i.e. lack of P2SH)\n> > > > > >     -   `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n> > > > > >     -   transaction malleability\n> > > > > >     -   probably many more\n> > > > > >\n> > > > > > So let me propose the more radical excision, starting with\n> SegWit v1:\n> > > > > >\n> > > > > > -   Remove `SIGHASH` from signatures.\n> > > > > > -   Put `SIGHASH` on public keys.\n> > > > > >\n> > > > > > Public keys are now encoded as either 33-bytes (implicit\n> `SIGHASH_ALL`) or 34-bytes (`SIGHASH` byte, followed by pubkey type,\n> followed by pubkey coordinate).\n> > > > > > `OP_CHECKSIG` and friends then look at the public key to\n> determine sighash algorithm rather than the signature.\n> > > > > > As we expect public keys to be indirectly committed to on every\n> output `scriptPubKey`, this is automatically output tagging to allow\n> particular `SIGHASH`.\n> > > > > > However, we can then utilize the many many ways to hide public\n> keys away until they are needed, exemplified in MAST-inside-Taproot.\n> > > > > > I propose also the addition of the opcode:\n> > > > > >\n> > > > > >     <sighash> <pubkey> OP_SETPUBKEYSIGHASH\n> > > > > >\n> > > > > >\n> > > > > > -   `sighash` must be one byte.\n> > > > > > -   `pubkey` may be the special byte `0x1`, meaning \"just use\n> the Taproot internal pubkey\".\n> > > > > > -   `pubkey` may be 33-byte public key, in which case the\n> `sighash` byte is just prepended to it.\n> > > > > > -   `pubkey` may be 34-byte public key with sighash, in which\n> case the first byte is replaced with `sighash` byte.\n> > > > > > -   If `sighash` is `0x00` then the result is a 33-byte public\n> key (the sighash byte is removed) i.e. `SIGHASH_ALL` implicit.\n> > > > > >\n> > > > > > This retains the old feature where the sighash is selected at\n> time-of-spending rather than time-of-payment.\n> > > > > > This is done by using the script:\n> > > > > >\n> > > > > >     <pubkey> OP_SETPUBKEYSIGHASH OP_CHECKSIG\n> > > > > >\n> > > > > >\n> > > > > > Then the sighash can be put in the witness stack after the\n> signature, letting the `SIGHASH` flag be selected at time-of-signing, but\n> only if the SCRIPT specifically is formed to do so.\n> > > > > > This is malleability-safe as the signature still commits to the\n> `SIGHASH` it was created for.\n> > > > > > However, by default, public keys will not have an attached\n> `SIGHASH` byte, implying `SIGHASH_ALL` (and disallowing-by-default\n> non-`SIGHASH_ALL`).\n> > > > > > This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`,\n> as they are allowed only if the output specifically says they are allowed.\n> > > > > > Would this not be a superior solution?\n> > > > > > Regards,\n> > > > > > ZmnSCPxj\n> > > > > >\n> > > > > > bitcoin-dev mailing list\n> > > > > > bitcoin-dev at lists.linuxfoundation.org\n> > > > > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> > > > >\n> > > > > Lightning-dev mailing list\n> > > > > Lightning-dev at lists.linuxfoundation.org\n> > > > > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n> > > >\n> > > >\n> > > _______________________________________________\n> > > Lightning-dev mailing list\n> > > Lightning-dev at lists.linuxfoundation.org\n> > > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191004/709217b1/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "OP_CAT was Re: Continuing the discussion about noinput / anyprevout",
            "categories": [
                "Lightning-dev",
                "bitcoin-dev"
            ],
            "authors": [
                "Jeremy",
                "Andrew Poelstra",
                "Ethan Heilman",
                "Lloyd Fournier",
                "ZmnSCPxj"
            ],
            "messages_count": 8,
            "total_messages_chars_count": 53481
        }
    },
    {
        "title": "[Lightning-dev] Proposal: AMPs With Proof of Payment",
        "thread_messages": [
            {
                "author": "Nadav Kohen",
                "date": "2019-10-02T21:28:56",
                "message_text_only": "Hi list,\n\nLike most people I am super excited for AMPs to hit the lightning network!\n\n(For the remainder of this post when I say AMP I mean OG AMP (\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2018-February/000993.html)\nsince that is the one I know)\n\nIt is my understanding however that it is not possible to do AMPs with\nProof of Payment (PoP). This is because the payment pre-images must be\nfully known to the payer since they must compute a hash of each payment\npre-image. And if the payer must know the pre-image in advance then there\nis nothing for them to learn atomically with payment completion.\n\n This makes me sad because PoP is really important for many applications\nand any application that uses PoP will not be AMP compatible.\n\nQueue Payment Points to the rescue! Once the lightning network moves to\nsupport Payment Points and PTLCs instead of Payment Hashes with their\nHTLCs, OG AMP can be modified in the following simple way in order to allow\nfor PoP-enabled AMPs:\n\n    Let PP = pop*G be the payment point (e.g. in an invoice) where pop is\nknown to the receiver.\n    Like in the original proposal, let V be the total amount with pieces\nv_1 through v_n.\n    Like in the original proposal, let BS = s_1 ^ ... ^ s_n (where I use BS\n= Base Secret)\n    Like in the original proposal, let r_i = H(BS || i), but this will not\nbe our pre-image.\n    Instead, let the payment point for partial payment i be: P_i = r_i*G +\nPP\n    This makes each payment scalar (as opposed to pre-image) r_i + pop\n    The rest is the same as OG AMP: Use the triple (ID, v_i, s_i) in each\npayment's EOB\n\n    This allows the receiver to add pop to each r_i which is required to\ncomplete each payment.\n\n*TLDR*: Have a Payment Point, PP = pop*G, and add it to each partial\npayment point!\n\nOnce we have Payment Points I propose that this be how AMPs work (and\nsimply set PP = 0 in the case of spontaneous AMPs). This will allow AMPs to\nenjoy all of the awesome things that come with PoP!\n\nIf it is true, as I believe it to be, that it is not possible to have PoP\nin AMPs without Payment Points, then I find this to be (yet another) really\ncompelling reason to move to Payment Points as soon as we can (likely when\nbip-schnorr enters base layer I believe?).\n\nAll feedback is welcome.\n\nBest,\nNadav\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191002/accb329a/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-03T00:48:46",
                "message_text_only": "Good morning Nadav,\n\nYes, this is possible.\n\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2018-March/001100.html\n\nThis is called as \"High\" AMP to contrast with OG and Base, and was discussed in Adelaide 2018.\nAt Adelaide 2018 only path decorrelation and High AMP were the known advantages of payment point/scalar, thus the decision to wait for Schnorr-like signatures to hit the base layer rather than implement 2p-ECDSA.\n\nThe possibility of Stuckless (which potentially allows Escrow-over-Lightning as well) gives additional boost to the use of payment points.\nSince bip-schnorr probably will have 1 year of arguing, 1 year of testing+ developing, and 2 years of miners delaying activation before another UASF, I am currently tempted to consider implementing 2p-ECDSA already.\n\nRegards,\nZmnSCPxj\n\n\n>\n> Like most people I am super excited for AMPs to hit the lightning network!\n>\n> (For the remainder of this post when I say AMP I mean OG AMP (https://lists.linuxfoundation.org/pipermail/lightning-dev/2018-February/000993.html) since that is the one I know)\n>\n> It is my understanding however that it is not possible to do AMPs with Proof of Payment (PoP). This is because the payment pre-images must be fully known to the payer since they must compute a hash of each payment pre-image. And if the payer must know the pre-image in advance then there is nothing for them to learn atomically with payment completion.\n>\n> \u00a0This makes me sad because PoP is really important for many applications and any application that uses PoP will not be AMP compatible.\n>\n> Queue Payment Points to the rescue! Once the lightning network moves to support Payment Points and PTLCs instead of Payment Hashes with their HTLCs, OG AMP can be modified in the following simple way in order to allow for PoP-enabled AMPs:\n>\n> \u00a0\u00a0\u00a0 Let PP = pop*G be the payment point (e.g. in an invoice) where pop is known to the receiver.\n> \u00a0\u00a0\u00a0 Like in the original proposal, let V be the total amount with pieces v_1 through v_n.\n> \u00a0\u00a0\u00a0 Like in the original proposal, let BS = s_1 ^ ... ^ s_n (where I use BS = Base Secret)\n> \u00a0\u00a0\u00a0 Like in the original proposal, let r_i = H(BS || i), but this will not be our pre-image.\n> \u00a0\u00a0\u00a0 Instead, let the payment point for partial payment i be: P_i = r_i*G + PP\n> \u00a0\u00a0\u00a0 This makes each payment scalar (as opposed to pre-image) r_i + pop\n> \u00a0\u00a0\u00a0 The rest is the same as OG AMP: Use the triple (ID, v_i, s_i) in each payment's EOB\n> \u00a0\u00a0\u00a0\n> \u00a0\u00a0\u00a0 This allows the receiver to add pop to each r_i which is required to complete each payment.\n>\n> TLDR: Have a Payment Point, PP = pop*G, and add it to each partial payment point!\n>\n> Once we have Payment Points I propose that this be how AMPs work (and simply set PP = 0 in the case of spontaneous AMPs). This will allow AMPs to enjoy all of the awesome things that come with PoP!\n>\n> If it is true, as I believe it to be, that it is not possible to have PoP in AMPs without Payment Points, then I find this to be (yet another) really compelling reason to move to Payment Points as soon as we can (likely when bip-schnorr enters base layer I believe?).\n>\n> All feedback is welcome.\n>\n> Best,\n> Nadav"
            },
            {
                "author": "Nadav Kohen",
                "date": "2019-10-03T03:11:48",
                "message_text_only": "Oh darn!\n\nMy bad for posting so fast, I should have looked around more :P\n\nThanks for the info!\n\nAnd agreed, it would be awesome to have even if just as a proof of concept.\n\nBest,\nNadav\n\nOn Wed, Oct 2, 2019 at 6:48 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Nadav,\n>\n> Yes, this is possible.\n>\n>\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2018-March/001100.html\n>\n> This is called as \"High\" AMP to contrast with OG and Base, and was\n> discussed in Adelaide 2018.\n> At Adelaide 2018 only path decorrelation and High AMP were the known\n> advantages of payment point/scalar, thus the decision to wait for\n> Schnorr-like signatures to hit the base layer rather than implement\n> 2p-ECDSA.\n>\n> The possibility of Stuckless (which potentially allows\n> Escrow-over-Lightning as well) gives additional boost to the use of payment\n> points.\n> Since bip-schnorr probably will have 1 year of arguing, 1 year of testing+\n> developing, and 2 years of miners delaying activation before another UASF,\n> I am currently tempted to consider implementing 2p-ECDSA already.\n>\n> Regards,\n> ZmnSCPxj\n>\n>\n> >\n> > Like most people I am super excited for AMPs to hit the lightning\n> network!\n> >\n> > (For the remainder of this post when I say AMP I mean OG AMP (\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2018-February/000993.html)\n> since that is the one I know)\n> >\n> > It is my understanding however that it is not possible to do AMPs with\n> Proof of Payment (PoP). This is because the payment pre-images must be\n> fully known to the payer since they must compute a hash of each payment\n> pre-image. And if the payer must know the pre-image in advance then there\n> is nothing for them to learn atomically with payment completion.\n> >\n> >  This makes me sad because PoP is really important for many applications\n> and any application that uses PoP will not be AMP compatible.\n> >\n> > Queue Payment Points to the rescue! Once the lightning network moves to\n> support Payment Points and PTLCs instead of Payment Hashes with their\n> HTLCs, OG AMP can be modified in the following simple way in order to allow\n> for PoP-enabled AMPs:\n> >\n> >     Let PP = pop*G be the payment point (e.g. in an invoice) where pop\n> is known to the receiver.\n> >     Like in the original proposal, let V be the total amount with pieces\n> v_1 through v_n.\n> >     Like in the original proposal, let BS = s_1 ^ ... ^ s_n (where I use\n> BS = Base Secret)\n> >     Like in the original proposal, let r_i = H(BS || i), but this will\n> not be our pre-image.\n> >     Instead, let the payment point for partial payment i be: P_i = r_i*G\n> + PP\n> >     This makes each payment scalar (as opposed to pre-image) r_i + pop\n> >     The rest is the same as OG AMP: Use the triple (ID, v_i, s_i) in\n> each payment's EOB\n> >\n> >     This allows the receiver to add pop to each r_i which is required to\n> complete each payment.\n> >\n> > TLDR: Have a Payment Point, PP = pop*G, and add it to each partial\n> payment point!\n> >\n> > Once we have Payment Points I propose that this be how AMPs work (and\n> simply set PP = 0 in the case of spontaneous AMPs). This will allow AMPs to\n> enjoy all of the awesome things that come with PoP!\n> >\n> > If it is true, as I believe it to be, that it is not possible to have\n> PoP in AMPs without Payment Points, then I find this to be (yet another)\n> really compelling reason to move to Payment Points as soon as we can\n> (likely when bip-schnorr enters base layer I believe?).\n> >\n> > All feedback is welcome.\n> >\n> > Best,\n> > Nadav\n>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191002/5ab9c900/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Proposal: AMPs With Proof of Payment",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Nadav Kohen",
                "ZmnSCPxj"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 9379
        }
    },
    {
        "title": "[Lightning-dev] [bitcoin-dev] OP_CAT was Re: Continuing the discussion about noinput / anyprevout",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2019-10-04T11:15:36",
                "message_text_only": "On Thu, Oct 03, 2019 at 10:02:14PM -0700, Jeremy via bitcoin-dev wrote:\n> Awhile back, Ethan and I discussed having, rather than OP_CAT, an\n> OP_SHA256STREAM that uses the streaming properties of a SHA256 hash\n> function to allow concatenation of an unlimited amount of data, provided\n> the only use is to hash it.\n> \n> You can then use it perhaps as follows:\n> \n> // start a new hash with item\n> OP_SHA256STREAM  (-1) -> [state]\n> // Add item to the hash in state\n> OP_SHA256STREAM n [item] [state] -> [state]\n> // Finalize\n> OP_SHA256STREAM (-2) [state] -> [Hash]\n> \n> <-1> OP_SHA256STREAM <tag> <subnode 2> <subnode 3> <3> OP_SHA256STREAM <-2>\n> OP_SHA256STREAM\n\nOne issue with this is the simplest implementation where the state is just raw\nbytes would expose raw SHA256 midstates, allowing people to use them directly;\npreventing that would require adding types to the stack. Specifically I could\nwrite a script that rather than initializing the state correctly from the\nofficial IV, instead takes an untrusted state as input.\n\nSHA256 isn't designed to be used in situations where adversaries control the\ninitialization vector. I personally don't know one way or the other if anyone\nhas analyzed this in detail, but I'd be surprised if that's secure. I\nconsidered adding midstate support to OpenTimestamps but decided against it for\nexactly that reason.\n\nI don't have the link handy but there's even an example of an experienced\ncryptographer on this very list (bitcoin-dev) proposing a design that falls\nvictim to this attack. It's a subtle issue and we probably don't want to\nencourage it.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191004/9c383745/attachment.sig>"
            },
            {
                "author": "Jeremy",
                "date": "2019-10-04T18:40:53",
                "message_text_only": "Interesting point.\n\nThe script is under your control, so you should be able to ensure that you\nare always using a correctly constructed midstate, e.g., something like:\n\nscriptPubKey: <-1> OP_SHA256STREAM DEPTH OP_SHA256STREAM <-2>\nOP_SHA256STREAM\n<hash> OP_EQUALVERIFY\n\nwould hash all the elements on the stack and compare to a known hash.\nHow is that sort of thing weak to midstateattacks?\n\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n\n\nOn Fri, Oct 4, 2019 at 4:16 AM Peter Todd <pete at petertodd.org> wrote:\n\n> On Thu, Oct 03, 2019 at 10:02:14PM -0700, Jeremy via bitcoin-dev wrote:\n> > Awhile back, Ethan and I discussed having, rather than OP_CAT, an\n> > OP_SHA256STREAM that uses the streaming properties of a SHA256 hash\n> > function to allow concatenation of an unlimited amount of data, provided\n> > the only use is to hash it.\n> >\n> > You can then use it perhaps as follows:\n> >\n> > // start a new hash with item\n> > OP_SHA256STREAM  (-1) -> [state]\n> > // Add item to the hash in state\n> > OP_SHA256STREAM n [item] [state] -> [state]\n> > // Finalize\n> > OP_SHA256STREAM (-2) [state] -> [Hash]\n> >\n> > <-1> OP_SHA256STREAM <tag> <subnode 2> <subnode 3> <3> OP_SHA256STREAM\n> <-2>\n> > OP_SHA256STREAM\n>\n> One issue with this is the simplest implementation where the state is just\n> raw\n> bytes would expose raw SHA256 midstates, allowing people to use them\n> directly;\n> preventing that would require adding types to the stack. Specifically I\n> could\n> write a script that rather than initializing the state correctly from the\n> official IV, instead takes an untrusted state as input.\n>\n> SHA256 isn't designed to be used in situations where adversaries control\n> the\n> initialization vector. I personally don't know one way or the other if\n> anyone\n> has analyzed this in detail, but I'd be surprised if that's secure. I\n> considered adding midstate support to OpenTimestamps but decided against\n> it for\n> exactly that reason.\n>\n> I don't have the link handy but there's even an example of an experienced\n> cryptographer on this very list (bitcoin-dev) proposing a design that falls\n> victim to this attack. It's a subtle issue and we probably don't want to\n> encourage it.\n>\n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191004/bda6e24c/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2019-10-05T15:49:02",
                "message_text_only": "On Fri, Oct 04, 2019 at 11:40:53AM -0700, Jeremy wrote:\n> Interesting point.\n> \n> The script is under your control, so you should be able to ensure that you\n> are always using a correctly constructed midstate, e.g., something like:\n> \n> scriptPubKey: <-1> OP_SHA256STREAM DEPTH OP_SHA256STREAM <-2>\n> OP_SHA256STREAM\n> <hash> OP_EQUALVERIFY\n> \n> would hash all the elements on the stack and compare to a known hash.\n> How is that sort of thing weak to midstateattacks?\n\nObviously with care you can get the computation right. But at that point what's\nthe actual advantage over OP_CAT?\n\nWe're limited by the size of the script anyway; if the OP_CAT output size limit\nis comparable to that for almost anything you could use SHA256STREAM on you\ncould just as easily use OP_CAT, followed by a single OP_SHA256.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191005/213e4e81/attachment.sig>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-06T08:46:59",
                "message_text_only": "Good morning Peter, Jeremy, and lists,\n\n> On Fri, Oct 04, 2019 at 11:40:53AM -0700, Jeremy wrote:\n>\n> > Interesting point.\n> > The script is under your control, so you should be able to ensure that you\n> > are always using a correctly constructed midstate, e.g., something like:\n> > scriptPubKey: <-1> OP_SHA256STREAM DEPTH OP_SHA256STREAM <-2>\n> > OP_SHA256STREAM\n> > <hash> OP_EQUALVERIFY\n> > would hash all the elements on the stack and compare to a known hash.\n> > How is that sort of thing weak to midstateattacks?\n>\n> Obviously with care you can get the computation right. But at that point what's\n> the actual advantage over OP_CAT?\n>\n> We're limited by the size of the script anyway; if the OP_CAT output size limit\n> is comparable to that for almost anything you could use SHA256STREAM on you\n> could just as easily use OP_CAT, followed by a single OP_SHA256.\n\nTheoretically, `OP_CAT` is less efficient.\n\nIn cases where the memory area used to back the data cannot be resized, new backing memory must be allocated elsewhere and the existing data copied.\nThis leads to possible O( n^2 ) behavior for `OP_CAT` (degenerate case where we add 1 byte per `OP_CAT` and each time find that the memory area currently in use is exactly fitting the data and cannot be resized in-place).\n\n`OP_SHASTREAM` would not require new allocations once the stream state is in place and would not require any copying.\n\n\nThis may be relevant in considering the cost of executing `OP_CAT`.\n\nAdmittedly a sufficiently-limited  maximum `OP_CAT` output would be helpful in reducing the worst-case `OP_CAT` behavior.\nThe question is what limit would be reasonable.\n64 bytes feels too small if one considers Merkle tree proofs, due to mentioned issues of lack of typechecking.\n\n\nRegards,\nZmnSCPxj\n\n\n>\n> --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n>\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n>\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "Peter Todd",
                "date": "2019-10-06T09:12:21",
                "message_text_only": "On Sun, Oct 06, 2019 at 08:46:59AM +0000, ZmnSCPxj wrote:\n> > Obviously with care you can get the computation right. But at that point what's\n> > the actual advantage over OP_CAT?\n> >\n> > We're limited by the size of the script anyway; if the OP_CAT output size limit\n> > is comparable to that for almost anything you could use SHA256STREAM on you\n> > could just as easily use OP_CAT, followed by a single OP_SHA256.\n> \n> Theoretically, `OP_CAT` is less efficient.\n> \n> In cases where the memory area used to back the data cannot be resized, new backing memory must be allocated elsewhere and the existing data copied.\n> This leads to possible O( n^2 ) behavior for `OP_CAT` (degenerate case where we add 1 byte per `OP_CAT` and each time find that the memory area currently in use is exactly fitting the data and cannot be resized in-place).\n\nIn even that degenerate case allocators also free memory.\n\nAnyway, every execution step in script evaluation has a maximum output size,\nand the number of steps is limited. At worst you can allocate the entire\npossible stack up-front for relatively little cost (eg fitting in the MB or two\nthat is a common size for L2 cache).\n\n> Admittedly a sufficiently-limited  maximum `OP_CAT` output would be helpful in reducing the worst-case `OP_CAT` behavior.\n> The question is what limit would be reasonable.\n> 64 bytes feels too small if one considers Merkle tree proofs, due to mentioned issues of lack of typechecking.\n\n256 bytes is more than enough for even the most complex summed merkle tree with\n512-byte hashes and full-sized sum commitments. Yet that's still less than the\n~500byte limit proposed elsewhere.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191006/13e0402e/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "OP_CAT was Re: Continuing the discussion about noinput / anyprevout",
            "categories": [
                "Lightning-dev",
                "bitcoin-dev"
            ],
            "authors": [
                "Jeremy",
                "Peter Todd",
                "ZmnSCPxj"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 10017
        }
    },
    {
        "title": "[Lightning-dev] Payment point+scalar (was: Re: Proposal: AMPs With Proof of Payment)",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-07T09:00:08",
                "message_text_only": "Good morning list,\n\nI was contacted off-list about the below statement:\n\n> Since bip-schnorr probably will have 1 year of arguing, 1 year of testing+ developing, and 2 years of miners delaying activation before another UASF, I am currently tempted to consider implementing 2p-ECDSA already.\n\nThe person who contacted was concerned that encouraging 2p-ECDSA would divert attention from development of bip-schnorr, and in particular pointed out that developers should in fact try to implement Schnorr-like signatures and taproot in their software so as to further validate the design that will eventually be put into Bitcoin, hopefully faster.\n\nLet me apologize for not considering this and I will now consider this possibility further in the future.\n\n-----\n\nIn any case, I will now also pointlessly spam the list with a long post that needlessly analyzes the above single statement in detail.\n\n> 1 year of arguing, 1 year of testing+ developing, and 2 years of miners delaying activation before another UASF\n\nThe above is my understanding of a high-level view of what occurred with SegWit v0 deployment.\nThis is a technique known as Reference Class Forecasting.\n\n* https://en.wikipedia.org/wiki/Reference_class_forecasting\n* https://conceptually.org/concepts/reference-class-forecasting\n* https://simplicable.com/new/reference-class-forecasting\n\nPut simply, \"what happened before is likely to happen again\", or \"history repeats\".\n\nI observe the below facts:\n\n* Humans have a well-known flaw known as \"Planning Fallacy\", wherein they *consistently* underestimate how long it takes to do anything.\n  * https://en.wikipedia.org/wiki/Planning_fallacy\n  * https://medium.com/the-mission/the-planning-fallacy-why-you-miss-your-deadlines-and-what-to-do-about-it-db5e162307b7\n* Nearly all Bitcoin developers are either human, suspected of being human, or claim to be human.\n* Aggregation of quantifiable information (\"wisdom of crowds\" technique) remains subject to pervasive systemic flaws, including the above \"Planning Fallacy\".\n\nAgainst planning fallacy the most reliable weapon is the aforementioned Reference Class Forecasting.\nThis is simply taking a previous effort with known time-to-completion, then reusing its actual time to create estimates.\n\nNow, it can be argued that:\n\n* We already learned things from previous efforts, thus our current effort should be shorter.\n* We cannot find any reason for bad things to happen during performance of the new task.\n\nAnd therefore we should take less time for the new task than for the tasks in the existing reference class.\n\nHowever, the above is a manifestation of the well-known \"Optimism Bias\" humans pervasively have, and which is implicated in the persistence of the \"Planning Fallacy\" among humans.\n\n* https://en.wikipedia.org/wiki/Optimism_bias\n* https://www.verywellmind.com/what-is-the-optimism-bias-2795031\n\nCounterarguments include:\n\n* We had to learn new things in the previous effort, what makes us think we will not be forced to learn new things in our new effort?\n* We already found reasons for the bad things happening that delayed the previous task, what makes us think we will not encounter bad things that delay our new task?\n\nThus, it is far better to ignore **all** arguments for reducing the estimate (and thereby avoid Optimism Bias, and also reduce cognitive resource utilization as a bonus, which can be important when you are renting cognitive hardware, do not let sub-agents run wild and consume CPU needlessly people) and go with the original Reference Class Forecasting result.\n\nFor what it is worth I consider the \"1 year of arguing\" to already be drawing to a close by now, so potentially we might have Schnorr-like signatures on Bitcoin mainnet in 3 years or so.\n\nArguments for an earlier or later estimate of Schnorr-like signatures activated on mainnet must consider far more data and statistics, and the basic analysis above should still hold for much of it.\n\n\n> I am currently tempted to consider implementing 2p-ECDSA already\n\nNote the precision of this statement.\nIt does not say \"I am planning to implement 2p-ECDSA\".\nIt does not say \"I am considering to implement 2p-ECDSA\".\n\nIt simply says \"I am tempted to consider to implement 2p-ECDSA\".\nLet us also be very precise: \"to consider\" means \"to think about whether to do so or not\", and is in no way a precommitment to actually implementing 2p-ECDSA, merely a precommitment to *consider* whether or not to do so.\n\nSince I was already so tempted to ***consider*** this, let me now ***actually consider*** this at this point, and lay out the reasoning in detail.\n\nThe decision to not take 2p-ECDSA and instead focus on Schnorr-like signatures was considered in the Adelaide 2018 meetup.\nThe inputs to the decision were:\n\n* The known complexity of implementing 2p-ECDSA.\n* The known relative security drawbacks of 2p-ECDSA relative to Schnorr-like multisignatures.\n* The known technique of payment decorrelation, which requires payment points, which requires either 2p-ECDSA or Schnorr-like signatures.\n* The known technique of High AMP, which requires payment points, which requires either 2p-ECDSA or Schnorr-like signatures.\n\nHowever, after the Adelaide 2018 meetup, the following new techniques were derived, all of which require payment points and scalars.\n\n* Stuckless payments, which uses the blinding key to allow multiple parallel payments to be made for the same invoice while ensuring only one payment succeeds.\n  * Escrow-over-Lightning, which shares the blinding key between the payer and the escrow service.\n* Pay-for-signature, which uses the proof-of-payment to encode a component of an ECDSA or Schnorr-like signature.\n\nThe above new features serve as trigger to be tempted to consider whether to take the 2p-ECDSA \"sidestep\".\n\nWe should now also lay out the reasoning why 2p-ECDSA was rejected.\n\n* 2p-ECDSA requires two cryptosystems, Paillier and SECP256K1.\n  * The Paillier cryptosystem has fewer bits of security (80 bit, I believe) than SECP256K1.\n    * Thus we expect that we *will* use Schnorr-like signatures on SECP256K1 even if we ever implement 2p-ECDSA.\n  * We already have a widely-vetted library for SECP256K1, *including a branch that already implements the necessary primitives for using Schnorr-like signatures to implement payment point+scalar*, but not for Paillier.\n    * Creating our own library for Paillier is dangerous as it effectively \"rolls our own crypto\", violating the sacred Zeroth Commandment of Cryptographers:\n\n> Zeroth Commandment: Thou shalt not roll thy own crypto, until thou can beat Daniel Bernstein, Bruce Schneier, *and* Pieter Wuille in an epic rap battle.\n\n(That is obviously a joke! / The intent is not to choke / the development of crypto just to keep them rappers woke! / The intent is instead / make you smile and shake your head! / So don't you go a-rapping / when you should be go a-crypting / YO~! Zeeman out!)\n\nWith regards to Schnorr-like signatures on Bitcoin, on the other hand, we can consider the below new fact that was also not available during Adelaide 2018:\n\n* There now exists a proposal bip-schnorr to add Schnorr signatures to Bitcoin.\n\nThere is no similar new fact that improves on the use of 2p-ECDSA rather than Schnorr-like signatures.\n\nThus, the core of this consideration lies in the tension of the below facts:\n\n* Given the many now-known benefits of payment points and scalars, we should consider getting them enabled on Lightning earlier if possible.\n* 2p-ECDSA is theoretically implementable on Bitcoin as it exists today without waiting for SegWit v1.\n* However, we will still switch to Schnorr-like signatures later anyway, due to improved security relative to 2p-ECDSA.\n\nAgain, when considering whether to take 2p-ECDSA or not, we should also remember the \"planning fallacy\", and consider that implementation and debugging of a decent library for Paillier would take at least as much time as implementation and debugging for a decent library for SECP256K1.\n\nThe first commit of bitcoin-core/secp256k1 is in March 2013.\nThe commit in bitcoin/bitcoin that makes libsecp256k1 from an optional requirement to a permanent one (I think) was merged in November 2015.\nSo I will take this time (1 year and 8 months, 1 + 2/3 years) as the time it would take to polish an existing Paillier library to sufficient level to be acceptable for Bitcoin mainnet (i.e. Reference Class Forecast).\n\nNow let us consider the libsecp256k1 fork that includes Schnorr-like signatures, libsecp256k1-zkp.\nThe common commit between it and libsecp256k1 is from May 2019, but the first unique commit in libsecp256k1-zkp is in August 2019.\nThis makes it difficult to judge when the fork started, so let us take the center of May 2019 to August 2019 and call it as June 2019.\nThis means that the Schnorr-like part of libsecp256k1 has been in development for the past 5 months.\nWe can thus expect a \"good enough for Bitcoin mainnet\" in about 1 year and 3 months (subtracting 5 months from the previous forecast of 1 year and 8 months).\n\nThe question now is how long it takes to implement pointlocked timelocked contracts to replace the hashlocked timelocked contracts.\nI now take as reference, AdamISZ/CoinSwapCS.\nCoinSwap is \"just\" HTLC implementation, thus seems a good reference for implementing new pointlocked timelocked contracts that implement essentially the same swapping behavior.\nCoinSwapCS started in April 2017 and last commit was in October 2017.\nHowever, looking over the commits, it seems most of the code was stable as of June 2017, so I will take the region April 2017 to June 2017, 3 months, as the reference to implement a new atomic swap mechanism.\n\nThus, going the 2p-ECDSA route, looks like it will take 1 year and 11 months for a proof-of-concept reckless mainnet implementation.\n\nGoing the Schnorr-like route, looks like it will take 1 year and 6 months for a proof-of-concept testnet implementation with a stable libsecp256k1 interface for Schnorr-like signatures.\nAs we expect mainnet to activate SegWit v1 3 years from now, such an implementation cannot be deployed on mainnet until then.\n\n------\n\nIn any case, let us consider further how points and scalars would deploy.\n\nA common issue is that \"channel closure == bad\", because fees.\nThis means that we would have to implement pointlocked timelocked contracts on top of existing multiple-signature ECDA Poon-Dryja channels, with only a software update, and without closing and re-opening channels.\n\nFortunately, anything that the blockchain can enforce, any offchain updateable cryptocurrency system, including Poon-Dryja, can also enforce (by the threat of dropping onchain).\nThus, existing channels can enforce pointlocked timelocked contracts once they are supported onchain.\n\nObviously, we would want to eventually switch to Schnorr-like for channel funding transaction outputs.\nThis is because multiparticipant signatures can be implemented with a single MuSig signature, slightly reducing the costs of channel closure.\n\nThe other question is whether to take the new Decker-Russell-Osuntokun (which would make sense to bring directly to Schnorr-like) and leave Poon-Dryja using the existing ECDSA P2WSH, or to also upgrade Poon-Dryja mechanism to using Schnorr-like signatures.\n\nOf note is that Poon-Dryja does not in fact need Taproot:\n\n* Pointlocked timelocked contracts can be implemented using pre-signed transactions: one with a future `nLockTime` representing the timelock branch, the other with a current `nLockTime` with an adaptor signature that leaks the pre-negotiated secret.\n  * This would require `NOINPUT` signatures for those transactions, though, as the PTLCs need to be re-instantiated on each update.\n    * Even if the timelock branch is expressed in SCRIPT, the pointlock branch is not expressible in SCRIPT (*cough* \"Scriptless\" Script *cough*), so you need `NOINPUT` for that branch.\n    * But note that we exchange signatures for all surviving HTLCs on each update anyway, so we could use non-`NOINPUT` keypath spend by simply re-exchanging signature for each PTLC.\n      * Note the counter however, that MuSig signatures (and 2p-ECDSA signatures for that matter) need two communication rounds rather than one, requiring a more complex `commitment_signed` protocol.\n  * Given that `NOINPUT` might require an opt-in hidden via the Taproot mechanism, we might still end up requiring Taproot anyway.\n\nRegards,\nZmnSCPxj\n\n(Out yo~!)"
            }
        ],
        "thread_summary": {
            "title": "Payment point+scalar (was: Re: Proposal: AMPs With Proof of Payment)",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "ZmnSCPxj"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 12425
        }
    },
    {
        "title": "[Lightning-dev] A Payment Point Feature Family (MultiSig, DLC, Escrow, ...)",
        "thread_messages": [
            {
                "author": "Nadav Kohen",
                "date": "2019-10-09T23:42:54",
                "message_text_only": "Hi list,\n\nI'm back again with another idea about Payment Points and fun things to do\nwith them! Hopefully this time I'm not entirely just hashing out old ideas\nin public like an out-of-the-loop person :)\n\n*TLDR: Adding and ECDH-ing points gives us AND and OR functionality which\nwe can compose to make cool lightning contracts like Multisig, Escrow, and\nDLCs*\n\nSo when looking at the following (likely incomplete) list of things you can\ndo with payment points:\n\n1) Payment De-correlation\n2) \"Stuckless\" Payments\n3) High AMP\n4) Selling Signatures\n5) Selling Pedersen De-commitment\n6) Escrow Contracts\n\nI started of trying to classify what kind of thing these new features are\nin hopes of coming across new ones. The first three I clumped into a group\nI called \"Payment point addition allows us to do cool things while\nmaintaining the Proof of Payment (PoP) property\". The next two (4 and 5) I\ncalled \"Commitment applications where point is public\". But ZmnSCPxj's\nproposal for lightning escrow contracts (\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2019-June/002028.html)\nstruck me as something different that might somehow be made more general.\n\nEssentially the idea is to use the point S + ECDH(B, E) where S is the\nseller's point, B is the buyer's point and E is the escrow's point. This\nmeans that the scalar can be discovered by the seller in collaboration with\nthe buyer or the escrow, that is, S AND (B OR E). I propose that under\ncertain circumstances (such as the parties involved being able to\ninteract), this can be generalized to have payments conditioned on\narbitrary AND/OR circuits.\n\n I believe that AND is very straightforward as you simply take two\nconditions A and B and add them together to get a point that requires both\nof their scalars are discoverable (except maybe under certain bad\ncircumstances that can be avoided where like B = C - A, this must be\nguarded against).\n\nOR is harder but I think that it can be achieved in the two party case by\nECDH and in the n-party case by multi-party key exchanges (which I know\npretty much nothing about other than that they exist). Given some key\nexchange protocol (preferably non-interactive), KE, KE(A_1, ..., A_n)\nshould result in a number known only to those who know any scalar a_1, ...,\na_n and no one else. Assuming this exists and we can manage to trustlessly\n(in some possibly stretched sense of the word) compute shared keys\n(including such things as KE(A+B, C)), then KE(A, B) acts as A OR B in our\npayment condition contract.\n\nTo restate the escrow contract idea in this setting, the payment is\nconditioned on S + KE(B, E). Important to note is that not all parties must\nknow about the details of the payment itself: the Escrow in this example\nknows nothing about this payment (other than that some payment exists)\nunless there is a dispute.\n\nLastly, note that contracts following this scheme look indistinguishable to\nnormal payments on the network, and are fully compatible with High AMPs\nsince we can simply take the payment point specified by our contract and\nadd that point to each partial payment point.\n\nWell this is all nice in theory, but is there anything that will actually\ncome out of this thinking? I'll detail the two things I've thought of so\nfar for which I'd love critique! I'd also love to hear if anyone else\nthings of any cool application using this line of thought (or really\nanything cool with payment points :P).\n\nIdea 1: \"MultiSignature\" Lightning Contracts\nI mean \"MultiSignature\" here only in the sense that m-of-n parties must\nagree to the payment in order for it to happen, no actual signatures are\nused. A \"MultiSignature\" contract is simply a bunch of ANDs and ORs! For\nexample a 2-of-3 multisig between parties A, B, and C can be represented as\n(A AND B) OR (B AND C) OR (C AND A). As such, if some seller has a point S\nand three parties have points A, B, and C where a certain payment must go\nthrough if any two of them think it should, then the payment point used for\nthe payment should be S + KE(A + B, B + C, C + A). We add S to this point\nso that the scalar, s, can act as proof of payment. And that's it! I\nhaven't thought long enough to come up with any situation where this might\nbe useful but hoping someone who reads this will!\n\nIdea 2: DLCs Routed over Lightning\nSay that some DLC oracle will either broadcast s_A or s_B whose public\npoints are S_A and S_B (which can be computed from public information as\nper the DLC scheme). Then say that Alice and Bob enter into a contract\nunder which Alice wins some amount if s_A is broadcasted and Bob if s_B is\nbroadcasted. Say Alice has a point A and Bob has a point B. They each send\nthe other a payment with the amount that party must receive if they win\nwith the payment point A + S_A for Bob's payment to Alice and B + S_B for\nAlice's payment to Bob. And this is it! If s_A is broadcasted then Alice\ngets paid (and Bob gets proof of payment a, which is the scalar to A),\notherwise s_B is broadcasted and Bob gets paid (with Alice receiving b as\nPoP). An interesting note is that under this scheme neither party is forced\nto pay extra on-chain fees in the case of a counter-party who doesn't\ncooperate whilst in the wrong.\nOne wrinkle with this scheme is that setup isn't trustless. Although it is\ngenerally true that one party must sign the funding transaction for a DLC\nbefore the other party for on-chain DLCs, at least there is the mitigation\nthat when your counter-party goes silent, you can move your input funds\ninvalidating the funding transaction you signed (at a cost because fees).\nSo what can we do here to ensure that both payments are setup at the same\ntime in the case that Alice and Bob don't trust each other?\nSay that although they don't trust each other, they're both willing to\ntrust some escrow entity who generates some point E for their payment.\nAlice's payment point to Bob becomes B + S_B + E and Bob's to Alice becomes\nA + S_A + E. The escrow now waits to hear from Alice and Bob that they have\nincoming payments setup and only once both of them attest to this (using\nsignatures, for example) does the escrow release the scalar to E to them\nboth. The escrow can also have a timeout at which it will never reveal the\nscalar to E: forcing both parties to commit to the contract well before the\nDLC event. In this way, trust has been moved from counter-party to\ntrustworthy (hopefully) escrow in such a way that the Escrow learns nothing\nabout the contract itself (other than that there is one of some kind).\nI believe that this scheme can be extended to more events through the use\nof multiple payments being setup (usually in both directions) but this\nseems complicated and I've rambled for long enough. One last note is that\nDLC oracles can be composed in the usual way under this scheme (by\naddition) and potentially even threshold multi-oracles can be supported in\nthis way, although this would require the oracles to attest to some shared\nkey's points with other oracles which isn't necessarily optimal.\n\nBest,\nNadav\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191009/630cee2b/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-10T04:31:06",
                "message_text_only": "Good morning Nadav,\n\nThank you very much for this framework!\nIt seems very good idea, kudos to making this framework.\n\nI think, it is possible to make, a miniscript-like language for such things.\nIndeed, the only material difference, between SCRIPT-based `OP_CHECKSIG`s and Lightning, would be the requirement to reveal scalars rather than prove your knowledge of them.\n\n\n> Idea 2: DLCs Routed over Lightning\n> Say that some DLC oracle will either broadcast s_A or s_B whose public points are S_A and S_B (which can be computed from public information as per the DLC scheme). Then say that Alice and Bob enter into a contract under which Alice wins some amount if s_A is broadcasted and Bob if s_B is broadcasted. Say Alice has a point A and Bob has a point B. They each send the other a payment with the amount that party must receive if they win with the payment point A + S_A for Bob's payment to Alice and B + S_B for Alice's payment to Bob. And this is it! If s_A is broadcasted then Alice gets paid (and Bob gets proof of payment a, which is the scalar to A), otherwise s_B is broadcasted and Bob gets paid (with Alice receiving b as PoP). An interesting note is that under this scheme neither party is forced to pay extra on-chain fees in the case of a counter-party who doesn't cooperate whilst in the wrong.\n> One wrinkle with this scheme is that setup isn't trustless. Although it is generally true that one party must sign the funding transaction for a DLC before the other party for on-chain DLCs, at least there is the mitigation that when your counter-party goes silent, you can move your input funds invalidating the funding transaction you signed (at a cost because fees). So what can we do here to ensure that both payments are setup at the same time in the case that Alice and Bob don't trust each other?\n> Say that although they don't trust each other, they're both willing to trust some escrow entity who generates some point E for their payment. Alice's payment point to Bob becomes B + S_B + E and Bob's to Alice becomes A + S_A + E. The escrow now waits to hear from Alice and Bob that they have incoming payments setup and only once both of them attest to this (using signatures, for example) does the escrow release the scalar to E to them both. The escrow can also have a timeout at which it will never reveal the scalar to E: forcing both parties to commit to the contract well before the DLC event. In this way, trust has been moved from counter-party to trustworthy (hopefully) escrow in such a way that the Escrow learns nothing about the contract itself (other than that there is one of some kind).\n\nI think we can call this a \"barrier escrow\".\n\n* It is similar to the concept of synchronization barriers for multithread coordination: https://en.wikipedia.org/wiki/Barrier_(computer_science)\n* In effect, each sub-transaction of the entire arrangement is a \"thread\" of operation, and the \"barrier escrow\" ensures that all the threads have reached it before letting them continue to claim the payments.\n\n\nI seem, this is applicable to *not only* DLC, but others as well.\nInstead, it seems to me to also provide an alternate solution to the Premium-free American Call Option Problem.\n\nLet us introduce our characters:\n\n* B, a supposed buyer, holding some Bitcoin.\n* S, a supposed seller, who wishes to be paid in another asset.\n* X, a cross-currency exchange Lightning node.\n* E, a tr\\*sted escrow.\n\nX would like to facilitate exchanges across different assets, but wants to be paid a premium in order to prevent the Premium-free American Call Option Problem.\nB would like to swap its Bitcoin for another asset to pay S, and would be willing to pay the above premium, but only conditional on S actually getting the asset it wants to be paid in.\nX and B are non-trusting to each other, but are willing to tr\\*st escrow E.\n\nThis again is another \"two transactions\" setup, where we move tr\\*st from X and B and transfer it to E.\n\n* B forwards the payment to be exchanged through X and onward to S, conditional on receiving the scalar behind `S + E`.\n* B gives a payment to X conditional on receiving the scalar behind `X + E`.\n* X continues to forward the payment, in the target asset, to S.\n* X contacts E and requests for the secret behind `E`.\n* S, on receiving the incoming payment, contacts E and requests for the secret behind `E`.\n* As E has now received both requests, it releases the secret simultaneously to both branches.\n* S and X can now claim their payments.\n\nNow, I would also like to observe the below fact:\n\n* Proof-of-payment can be reconsidered, under payment point + scalar scheme, to be \"pay-for-scalar\" scheme.\n\nI would also like to point out the below assumption, which underlies Bass Amplifier (\"multipath payments\", \"base AMP\") and its claim to atomicity:\n\n* If we agree that my secret `s` is worth `m` millisatoshi, then I (as a perfectly rational economic agent) will not claim any payments conditional on my revelation of `s` that sum up to less than `m` millisatoshi: I will only claim *all* of them when I can get multiple payments conditional on my revelation of `s` that sum up to `m` millisatoshi or more.\n  * I now call this \"economically-rational atomicity\", and describe also them mildly here: https://lists.linuxfoundation.org/pipermail/lightning-dev/2018-March/001109.html\n\nThe above is *exactly* the behavior we want E to perform: only if it receives both requests to reveal its secret `e` behind `E`, will it actually reveal the secret.\nIn our case, we turn the requests to E into partial payments of a single invoice (though of course, the payment from S to E would be in the asset that S wants rather than in Bitcoin).\nE would not like to \"sell short\" its secret `e` for less than the price of its service, thus if E is economically rational, it would only claim payments (and reveal `e`) once *all* incoming payments sum up to the price of its service.\n\nIn particular, the existing High AMP scheme would be reused to implement the escrow E, and all that would be needed would be some way to request invoices from E!\nNo additional code on top of High AMP is needed (very important, being a lazy developer is hard work).\nThis should help make setting up new Es to be very simple, improving competition in this space and helping further ensure economically-rational behavior.\n\nIf we squint, we can even say that:\n\n* This is actually a payment from B to E.\n* The payment is split into two paths, B->X->E and B->X->S->E.\n* X and S are just being paid surprisingly high fees, and are not in fact being paid premiums or payment-for-product/service.\n\nThus, any argument that we could make, about the safety of Bass Amplifier, would also serve as an argument we could make about E behaving as we would like it to behave!\nThis should greatly reduce the tr\\*st requirement from B and X to E: they only need to trust that E operates econmically-rationally, and not trust that E will, from the goodness of its own heart, operate correctly.\n\n(A remaining issue for the above scheme is that B learns `s + e` and `x + e`, not either `s` or `x`, and thus does not actually get full proof-of-payment.)\n\n\nFurther, this lets us extend this scheme to more than just two simultaneous transactions.\nIf for example we need to set up 5 simultaneous transactions, and E charges 1000 millisatoshi for its service, then we just have each receiver forward 200 millisatoshi each to E once their incoming payment has been set up.\n\nIn fact, the above problem, where B learns `s + e` and `x + e` but not either `s` or `x`, could be solved by splitting the payment to E three ways, with a third split from B to E, so that B learns `e` and can solve `(s + e) - e` == `s` and `(x + e) - e` == `x` to get proof-of-payment for both the actual purchase, and the premium charged by the exchange.\n\nIn all of these, E remains unaware of the exact details of the arrangement, it only cares that it is paid, with the use of High AMP being the \"coordination\n\n\nThus this scheme is surprisingly flexible, and may solve more problems than you might think.\nThank you for this insight, Nadav!\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-10T15:15:30",
                "message_text_only": "Good morning list, and Nadav,\n\n\n>     I would also like to point out the below assumption, which underlies Bass Amplifier (\"multipath payments\", \"base AMP\") and its claim to atomicity:\n>\n> -   If we agree that my secret `s` is worth `m` millisatoshi, then I (as a perfectly rational economic agent) will not claim any payments conditional on my revelation of `s` that sum up to less than `m` millisatoshi: I will only claim all of them when I can get multiple payments conditional on my revelation of `s` that sum up to `m` millisatoshi or more.\n>     -   I now call this \"economically-rational atomicity\", and describe also them mildly here: https://lists.linuxfoundation.org/pipermail/lightning-dev/2018-March/001109.html\n>\n>         The above is exactly the behavior we want E to perform: only if it receives both requests to reveal its secret `e` behind `E`, will it actually reveal the secret.\n>         In our case, we turn the requests to E into partial payments of a single invoice (though of course, the payment from S to E would be in the asset that S wants rather than in Bitcoin).\n>         E would not like to \"sell short\" its secret `e` for less than the price of its service, thus if E is economically rational, it would only claim payments (and reveal `e`) once all incoming payments sum up to the price of its service.\n\nRight right, except an economically-rational actor will also accept a *single* payment that exceeds the price `m` miilisatoshi.\nMeaning anyone can then subvert the behavior of the barrier escrow E.\nFor example, as soon as the exchange X is able to get the incoming premium payment conditional on release of the secret behind `X + E`, the exchange can simply directly send the entire payment asked by the escrow E for the secret behind `e` and claim the premium without actually performing the swap and forwarding to `S`.\n\nFortunately, https://lists.linuxfoundation.org/pipermail/lightning-dev/2018-March/001109.html also notes that it is possible to upgrade the High AMP with economically-rational security to a High AMP with computationally-intractable security.\nThe sketch given in that link is different, but we can use a conversion that utilizes points and scalars in much the same way as Nadav suggests as part of the framework.\n\nThat is, we can create a High AMP with the following rules for the receiver:\n\n* If the receiver knows a secret `e` behind a point `E`, payment is demanded in exchange for the secret `e + z` behind a point `E + Z` instead.\n  * The receiver is informed what `E` is being talked about, in some TLV to the receiver, as well as the point `Z`.\n* Every branch of the multipath also includes, in some TLV, a secret `z[i]`.\n  * `sum[i=1..n](z[i]) == z`, i.e. the sum of all the branch secrets is the secret `z` behind the point `Z`.\n* Thus, only once the receiver acquires all branches as incoming payments, will it be able to reconstruct the secret `z` and thereby claim its payment (and incidentally reveal `e + z`; if the ultimate payer then knows `z` it is able to acquire `e` as proof-of-payment).\n\nSo let us reconsider the protocol:\n\n* B, S, and X generate additional keypairs `b2, B2`, `s2, S2`, and `x2, X2` respectively and give the public keys to buyer B.\n* B generates `Z` from `B2 + S2 + X2`.\n* B sends the premium payment to X, in exchange for revelation of the secret behind `X + E + Z`.\n* X sends the request to the barrier escrow E to reveal the secret behind `E + Z`, also providing its share of the secret behind `Z`, `x2`.\n* B sends the payment to be swapped to the other asset to X, in exchange for revelation of the secret behind `S + E + Z`.\n* X swaps the asset and forwards to S, in exchange for the revelation of the secret behind `S + E + Z`.\n* S sends the request to the barrier escrow E to reveal the secret behind `E + Z`, also providing its share of the secret behind `Z`, `s2`.\n* B sends the request to the barrier escrow E to reveal the secret behind `E + Z`, also providing its share of the secret behind `Z`, `b2`.\n* E reconstructs `z` from `b2 + s2 + x2`, then reveals `e + z` to claim all the escrow service sub-payments.\n* X learns `e + z` and is able to reveal `x + e + z` to claim the premium.\n* S learns `e + z` and is able to reveal `s + e + z` to claim the premium.\n* B learns `e + z`, `x + e + z`, and `s + e + z`, and is able to recover proof-of-payment for the premium as `(x + e + z) - (e + z)`, and proof-of-payment for the actual service or product as `(s + e + z) - (e + z)`.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Nadav Kohen",
                "date": "2019-10-10T16:22:45",
                "message_text_only": "Hi list and ZmnSCPxj,\n\nGlad this has been helpful and I'm not just stating obvious things :)\nalways hard to tell once the idea has been had.\n\n>  I think, it is possible to make, a miniscript-like language for such\nthings.\n>  Indeed, the only material difference, between SCRIPT-based\n`OP_CHECKSIG`s and Lightning, would be the requirement to reveal scalars\nrather than prove your knowledge of them.\n\nTotally agree, and furthermore, another idea I had is that if we want to\nadjust any of this to a scheme where you reveal proof of knowledge rather\nthan the scalars themselves, then rather than just a point, the parties can\nagree on a point, a nonce point, and a message and then essentially use\npay-for-signature to have Schnorr signatures revealed rather than the\nscalars themselves. This scheme even lets participants use partial\nsignatures that they can add (though there is potentially some nonce\nnastiness involved). It certainly would add complexity but is an option\nshould it ever be useful. I think. Another thought is that maybe users can\nhave one key be static (like the node id?) and have the nonce be the\n\"Payment Point\" in my scheme so that the signature is proof that this set\nof node ids has collective access to the nonce point?\n\nI really like the Premium-free American Call Option mitigation you've\ndescribed here! I totally agree that it is probably best to move to\ncomputational security rather than rational security. I hadn't thought\nabout how well High AMP inter-operates with the AND/OR scheme to make\nthings like barrier escrows :) I wonder if there is some nice\ngeneralization of this kind of setup so that being a Barrier Escrow as a\nService can be another way to make money by participating in the Lightning\necosystem. Maybe this modified High AMP is all there is to it and all we\nneed is some standard communication interface for these Barrier Escrows?\n\nBest,\nNadav\n\nOn Thu, Oct 10, 2019 at 10:15 AM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning list, and Nadav,\n>\n>\n> >     I would also like to point out the below assumption, which underlies\n> Bass Amplifier (\"multipath payments\", \"base AMP\") and its claim to\n> atomicity:\n> >\n> > -   If we agree that my secret `s` is worth `m` millisatoshi, then I (as\n> a perfectly rational economic agent) will not claim any payments\n> conditional on my revelation of `s` that sum up to less than `m`\n> millisatoshi: I will only claim all of them when I can get multiple\n> payments conditional on my revelation of `s` that sum up to `m`\n> millisatoshi or more.\n> >     -   I now call this \"economically-rational atomicity\", and describe\n> also them mildly here:\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2018-March/001109.html\n> >\n> >         The above is exactly the behavior we want E to perform: only if\n> it receives both requests to reveal its secret `e` behind `E`, will it\n> actually reveal the secret.\n> >         In our case, we turn the requests to E into partial payments of\n> a single invoice (though of course, the payment from S to E would be in the\n> asset that S wants rather than in Bitcoin).\n> >         E would not like to \"sell short\" its secret `e` for less than\n> the price of its service, thus if E is economically rational, it would only\n> claim payments (and reveal `e`) once all incoming payments sum up to the\n> price of its service.\n>\n> Right right, except an economically-rational actor will also accept a\n> *single* payment that exceeds the price `m` miilisatoshi.\n> Meaning anyone can then subvert the behavior of the barrier escrow E.\n> For example, as soon as the exchange X is able to get the incoming premium\n> payment conditional on release of the secret behind `X + E`, the exchange\n> can simply directly send the entire payment asked by the escrow E for the\n> secret behind `e` and claim the premium without actually performing the\n> swap and forwarding to `S`.\n>\n> Fortunately,\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2018-March/001109.html\n> also notes that it is possible to upgrade the High AMP with\n> economically-rational security to a High AMP with\n> computationally-intractable security.\n> The sketch given in that link is different, but we can use a conversion\n> that utilizes points and scalars in much the same way as Nadav suggests as\n> part of the framework.\n>\n> That is, we can create a High AMP with the following rules for the\n> receiver:\n>\n> * If the receiver knows a secret `e` behind a point `E`, payment is\n> demanded in exchange for the secret `e + z` behind a point `E + Z` instead.\n>   * The receiver is informed what `E` is being talked about, in some TLV\n> to the receiver, as well as the point `Z`.\n> * Every branch of the multipath also includes, in some TLV, a secret\n> `z[i]`.\n>   * `sum[i=1..n](z[i]) == z`, i.e. the sum of all the branch secrets is\n> the secret `z` behind the point `Z`.\n> * Thus, only once the receiver acquires all branches as incoming payments,\n> will it be able to reconstruct the secret `z` and thereby claim its payment\n> (and incidentally reveal `e + z`; if the ultimate payer then knows `z` it\n> is able to acquire `e` as proof-of-payment).\n>\n> So let us reconsider the protocol:\n>\n> * B, S, and X generate additional keypairs `b2, B2`, `s2, S2`, and `x2,\n> X2` respectively and give the public keys to buyer B.\n> * B generates `Z` from `B2 + S2 + X2`.\n> * B sends the premium payment to X, in exchange for revelation of the\n> secret behind `X + E + Z`.\n> * X sends the request to the barrier escrow E to reveal the secret behind\n> `E + Z`, also providing its share of the secret behind `Z`, `x2`.\n> * B sends the payment to be swapped to the other asset to X, in exchange\n> for revelation of the secret behind `S + E + Z`.\n> * X swaps the asset and forwards to S, in exchange for the revelation of\n> the secret behind `S + E + Z`.\n> * S sends the request to the barrier escrow E to reveal the secret behind\n> `E + Z`, also providing its share of the secret behind `Z`, `s2`.\n> * B sends the request to the barrier escrow E to reveal the secret behind\n> `E + Z`, also providing its share of the secret behind `Z`, `b2`.\n> * E reconstructs `z` from `b2 + s2 + x2`, then reveals `e + z` to claim\n> all the escrow service sub-payments.\n> * X learns `e + z` and is able to reveal `x + e + z` to claim the premium.\n> * S learns `e + z` and is able to reveal `s + e + z` to claim the premium.\n> * B learns `e + z`, `x + e + z`, and `s + e + z`, and is able to recover\n> proof-of-payment for the premium as `(x + e + z) - (e + z)`, and\n> proof-of-payment for the actual service or product as `(s + e + z) - (e +\n> z)`.\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191010/7b38f17c/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-11T07:46:44",
                "message_text_only": "Good morning Nadav and Lloyd,\n\n\n> I really like the Premium-free American Call Option mitigation you've described here! I totally agree that it is probably best to move to computational security rather than rational security. I hadn't thought about how well High AMP inter-operates with the AND/OR scheme to make things like barrier escrows :) I wonder if there is some nice generalization of this kind of setup so that being a Barrier Escrow as a Service can be another way to make money by participating in the Lightning ecosystem. Maybe this modified High AMP is all there is to it and all we need is some standard communication interface for these Barrier Escrows?\n\n\nIndeed, from my current thinking, only the computationally-intractable-security High AMP is needed.\n\nNote that the secret `z` behind `Z` can itself also serve as:\n\n* The blinding scalar in route decorrelation.\n* The ACK response scalar in Stuckless.\n\nThis discussion is valuable in fact because ***I would have been perfectly and totally fine with economically-rational High AMP***.\nBut being able to ab^H^Hreuse the High AMP mechanism to implement barrier escrow means we might very well do better to design a unified package of features that implement decorrelation, High AMP, Stuckless, barrier escrow, etc. using the same mechanism, while providing a proof-of-payment (\"pay-for-scalar\") on top of which we can hook things like pay-for-signature and pay-for-Pedersen-decommitment.\n\nSo let us define some terms.\n\n* Attempt - a single sequence of pointlocked timelocked contracts that terminate at some payee.\n  It either fails or succeeds atomically.\n* Sub-payment - one or more attempts, each of which semantically pay for \"the same thing\" for \"the same value\", set up in parallel.\n  Pre-Stuckless, each sub-payment only has a single attempt at a time.\n  After Stuckless, a sub-payment may have multiple attempts running in parallel, but only one attempt will be claimable.\n* Payment - one or more sub-payments, each of which semantically pay for \"different parts of an entire thing\", set up in parallel.\n  Pre-AMP, a payment contains only a single sub-payment.\n  After AMP, a payment may have multiple sub-payments, and all sub-payments will be claimed atomically.\n\nSo let me propose the below, when we switch to payment points:\n\n* The payee receives a PTLC offer:\n  * It pays in exchange for the secret behind `S + Z`.\n  * The peyee knows the secret behind `S`, and the packet includes a TLV identifying *which* `S` exactly is being used.\n    * I believe the `S + Z` and `S` should be enough for the payee to determine `Z`.\n  * The offer also includes an onion-encrypted packet that tells the payee where to get the secret `z` behind `Z`.\n    * On contacting the payer, the payee gets:\n      * `z_route` a blinding factor for this route (cross-route decorrelation / Stuckless ACK)\n        * This uniquely identifies a single attempt of a sub-payment.\n      * `z_all` a blinding factor (High AMP computational-intractability).\n        * This uniquely identifies a single sub-payment.\n      * To claim *this* attempt the payee computes the sum of all `z_all` for all routes, plus the `z_route` for this particular incoming payment.\n      * If a different attempt of the same sub-payment is already requested by the payee, then the payer will send only `z_all` (which the payee then matches with the `z_all` of a different attempt) but not `z_route`, indicating to the payee that the two attempts are parallel attempts of the same sub-payment.\n      * Having separate blinding factors lets payers do Stuckless and multipath simultaneously, i.e. split the payment in paths, then make more than one parallel attempt for each split-out path.\n\n* A forwarding node receives a PTLC offer that it should forward:\n  * It pays in exchange for the secret behind `F`.\n  * It includes a secret `y`.\n  * The forwarding node should unwrap the onion packet by one layer and send to the next node a PTLC offer in exchange for the secret behind `F + Y`, where `Y = y * G`.\n\nForwarding nodes thus learn almost nothing, and the use of higher constructions like barrier escrows etc. are not known to forwarding nodes (and thus cannot be censored by them).\n\nThen:\n\n* Suppose a payer wants to make a single payment.\n  * It selects random `z_all`.\n  * It generates a route.\n  * It selects random `z_blind` for each forwarding node and the payee in the route and computes `z_route` as the sum of all `z_blind`.\n  * It computes `Z` for this route as `(z_route + z_all) * G`.\n  * It offers a PTLC to the first forwarding node for `S + z_blind[0] * G + z_all * G`.\n    * In the packet, the first hop node gets `y = z_blind[1]`, next gets `y = z_blind[2]`, the final payee has no `y`.\n  * The payee then receives a payment for `S + Z`, where `Z` is the sum of all `z_blind` times `G`, plus `z_all * G`.\n    * The payee then contacts the payer via an anonymous contact point and learns `z_all` and `z_route`.\n    * The payee can then learn `z` as `z_all + z_route` and acquire the money.\n  * The payer learns `s + z_route + z_all`.\n    * the payer knows `z_all` and `z_route`, and can subtract them to acquire `s`, the proof-of-payment (on top of which pay-for-data, pay-for-signature, and pay-for-Pedersen-decommitment can be acquired).\n\n* Suppose a payer wants to make a payment, splitting it up via AMP to two sub-payments.\n  * It selects two random `z_all`: `z_all[0]`, `z_all[1]`.\n  * It generates two routes.\n  * It generates random `z_blind` for each forwarding node and `z_route` for the two routes (`z_route[0]`, `z_route[1]`).\n  * It computes Z for each sub-payment as `(z_route[i] + z_all[0] + z_all[1]) * G`.\n  * For each route:\n    * It offers a PTLC to the first forwarding node for `S + z_blind[0] * G + (z_all[0] + z_all[1]) * G`.\n  * The payee receives one sub-payment, where `Z` is `(z_route[0] + z_all[0] + z_all[1]) * G`.\n    * The payee then contacts the payer and learns `z_all[0]` and `z_route[0]`.\n    * The payee is still unable to claim this sub-payment, as it only knows `z_all[0]` but needs to know `z_all`, i.e. `z_all[0] + z_all[1]`.\n  * The payee receives the other sub-payment, where `Z` is `(z_route[1] + z_all[0] + z_all[1]) * G`.\n    * The payee then contacts the payer and learns `z_all[1]` and `z_route[1]`.\n  * The payee now knows `z_all[0]` and `z_all[1]`, which is enough to compute `z_all`, and can now claim `each attempt / sub-payment.\n\n* Suppose a payer wants to make a payment, splitting it up via AMP to two sub-payments.\n  Further, the payer wants to make two simultaneous attempts per sub-payment, via Stuckless.\n  * Decide how to split the sub-payments.\n    * Each sub-payment has its own `z_all[i]`, and a summed up `z_all` can be created.\n      * The AMP splitting can be decided dynamically: given an existing sub-payment with a given `z_all` for it, we can destroy that sub-payment (if it has no attempts on-going) and create two new ones simply by selecting a new random `z_all[0]`, and computing `z_all[1] = z_all - z_all[0]`, with a similar rule to split up the amounts to be sent: the sum of `z_all[i]` for these two new sub-payments would then equal the original `z_all` for the destroyed sub-payment.\n    * Decide to make multiple attempts for each sub-payment.\n      * Generate random blinding factors for the route the attempt will take.\n      * Each attempt then has its own `z_route` that blinds the underlying factor.\n      * (basically I got lazy at this point and decided to just say: reader can probably figure this out themselves, just remember the hierarhcy payment -> sub-payment -> attempt.)\n\nIf a barrier escrow is desired, then all that is needed is to locate some node that supports High AMP and request (somehow) an invoice from it.\nThen each of the participants in the setup of the complicated multi-transaction setup designates its own `z_all[p]` share and provide `z_all[p] * G`, possibly with a signature to ensure that the participant knows exactly what the `z_all[p]` is and is not trying to cancel keys, or using the MuSig technique of multiplying `h(a * G | b * G ... p * G)` to all the public keys before summing them up.\n\n\n> I've been pitching the language name as \"Improv\" (since it's scriptless!).\n\nExcellent choice, I see you are a sentience possessing much acculturation.\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Orfeas Stefanos Thyfronitis Litos",
                "date": "2019-10-11T10:36:29",
                "message_text_only": "Good morning,\n\n> * Sub-payment - one or more attempts, each of which semantically pay\nfor \"the same thing\" for \"the same value\", set up in parallel.\n> a sub-payment may have multiple attempts running in parallel, but only\none attempt will be claimable.\n> * Payment - one or more sub-payments, each of which semantically pay\nfor \"different parts of an entire thing\", set up in parallel.\n> a payment may have multiple sub-payments, and all sub-payments will be\nclaimed atomically.\n\nThis can be also thought of as:\n\nPayment = ONE-OF(attempt_11, ..., attempt_m1) AND ... AND\nONE-OF(attempt_n1, ..., attempt_m'n)\n\nIts dual also deserves some thought:\n\nPayment = ONE-OF(attempt_11 AND ... AND attempt_m1), ..., (attempt_n1\nAND ... AND attempt_m'n))\n\nor in words, \"A payment is an atomic value transfer through many paths,\neach of which carry a part of the entire value -- many alternative\ngroups of paths are available to be used, but only one of the groups\neventually goes through.\"\n\nIs there a reason to design in preference of one of the two?\n\nSpeaking of generalization, it would be nice to have arbitrary AND-OR\ncombinations, but this needs further exploration:\n\n> If we want to create more complex access structures then we use\nverifiable secret sharing where the discrete log of B is split up into\nshares and distributed according the the desired structure.\n\nOne possible milestone of this generalisation would be to enable atomic\npayments where the paying wallet says \"there are all these known paths,\neach with such and such capacity; I want some to go through such that\nthe desired value is transferred in aggregate, no more, no less\n(possibly within a margin of error)\".\n\nKindly ignore me if I'm regurgitating already discussed stuff.\n\nBest,\nOrfeas\n\n-- \nThe University of Edinburgh is a charitable body, registered in\nScotland, with registration number SC005336."
            },
            {
                "author": "Lloyd Fournier",
                "date": "2019-10-11T00:57:21",
                "message_text_only": "Hi ZmnSCPxj,\n\n> I think, it is possible to make, a miniscript-like language for such\nthings.\n> Indeed, the only material difference, between SCRIPT-based `OP_CHECKSIG`s\nand Lightning, would be the requirement to reveal scalars rather than prove\nyour knowledge of them.\n\nI've thought about this too. I've been pitching the language name as\n\"Improv\" (since it's scriptless!). I think it would allow you to specify\nthe rules by which each output on each transaction can be spent and would\ncompile those rules into a protocol that does the secure key and signature\nexchange for all the transactions in the scaffold. In other words, a\nprotocol compiler. I think this could be really useful for formal\nspecification of layer-2 protocols.\n\nLL\n\nOn Thu, Oct 10, 2019 at 3:31 PM ZmnSCPxj via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> wrote:\n\n> Good morning Nadav,\n>\n> Thank you very much for this framework!\n> It seems very good idea, kudos to making this framework.\n>\n> I think, it is possible to make, a miniscript-like language for such\n> things.\n> Indeed, the only material difference, between SCRIPT-based `OP_CHECKSIG`s\n> and Lightning, would be the requirement to reveal scalars rather than prove\n> your knowledge of them.\n>\n>\n> > Idea 2: DLCs Routed over Lightning\n> > Say that some DLC oracle will either broadcast s_A or s_B whose public\n> points are S_A and S_B (which can be computed from public information as\n> per the DLC scheme). Then say that Alice and Bob enter into a contract\n> under which Alice wins some amount if s_A is broadcasted and Bob if s_B is\n> broadcasted. Say Alice has a point A and Bob has a point B. They each send\n> the other a payment with the amount that party must receive if they win\n> with the payment point A + S_A for Bob's payment to Alice and B + S_B for\n> Alice's payment to Bob. And this is it! If s_A is broadcasted then Alice\n> gets paid (and Bob gets proof of payment a, which is the scalar to A),\n> otherwise s_B is broadcasted and Bob gets paid (with Alice receiving b as\n> PoP). An interesting note is that under this scheme neither party is forced\n> to pay extra on-chain fees in the case of a counter-party who doesn't\n> cooperate whilst in the wrong.\n> > One wrinkle with this scheme is that setup isn't trustless. Although it\n> is generally true that one party must sign the funding transaction for a\n> DLC before the other party for on-chain DLCs, at least there is the\n> mitigation that when your counter-party goes silent, you can move your\n> input funds invalidating the funding transaction you signed (at a cost\n> because fees). So what can we do here to ensure that both payments are\n> setup at the same time in the case that Alice and Bob don't trust each\n> other?\n> > Say that although they don't trust each other, they're both willing to\n> trust some escrow entity who generates some point E for their payment.\n> Alice's payment point to Bob becomes B + S_B + E and Bob's to Alice becomes\n> A + S_A + E. The escrow now waits to hear from Alice and Bob that they have\n> incoming payments setup and only once both of them attest to this (using\n> signatures, for example) does the escrow release the scalar to E to them\n> both. The escrow can also have a timeout at which it will never reveal the\n> scalar to E: forcing both parties to commit to the contract well before the\n> DLC event. In this way, trust has been moved from counter-party to\n> trustworthy (hopefully) escrow in such a way that the Escrow learns nothing\n> about the contract itself (other than that there is one of some kind).\n>\n> I think we can call this a \"barrier escrow\".\n>\n> * It is similar to the concept of synchronization barriers for multithread\n> coordination: https://en.wikipedia.org/wiki/Barrier_(computer_science)\n> * In effect, each sub-transaction of the entire arrangement is a \"thread\"\n> of operation, and the \"barrier escrow\" ensures that all the threads have\n> reached it before letting them continue to claim the payments.\n>\n>\n> I seem, this is applicable to *not only* DLC, but others as well.\n> Instead, it seems to me to also provide an alternate solution to the\n> Premium-free American Call Option Problem.\n>\n> Let us introduce our characters:\n>\n> * B, a supposed buyer, holding some Bitcoin.\n> * S, a supposed seller, who wishes to be paid in another asset.\n> * X, a cross-currency exchange Lightning node.\n> * E, a tr\\*sted escrow.\n>\n> X would like to facilitate exchanges across different assets, but wants to\n> be paid a premium in order to prevent the Premium-free American Call Option\n> Problem.\n> B would like to swap its Bitcoin for another asset to pay S, and would be\n> willing to pay the above premium, but only conditional on S actually\n> getting the asset it wants to be paid in.\n> X and B are non-trusting to each other, but are willing to tr\\*st escrow E.\n>\n> This again is another \"two transactions\" setup, where we move tr\\*st from\n> X and B and transfer it to E.\n>\n> * B forwards the payment to be exchanged through X and onward to S,\n> conditional on receiving the scalar behind `S + E`.\n> * B gives a payment to X conditional on receiving the scalar behind `X +\n> E`.\n> * X continues to forward the payment, in the target asset, to S.\n> * X contacts E and requests for the secret behind `E`.\n> * S, on receiving the incoming payment, contacts E and requests for the\n> secret behind `E`.\n> * As E has now received both requests, it releases the secret\n> simultaneously to both branches.\n> * S and X can now claim their payments.\n>\n> Now, I would also like to observe the below fact:\n>\n> * Proof-of-payment can be reconsidered, under payment point + scalar\n> scheme, to be \"pay-for-scalar\" scheme.\n>\n> I would also like to point out the below assumption, which underlies Bass\n> Amplifier (\"multipath payments\", \"base AMP\") and its claim to atomicity:\n>\n> * If we agree that my secret `s` is worth `m` millisatoshi, then I (as a\n> perfectly rational economic agent) will not claim any payments conditional\n> on my revelation of `s` that sum up to less than `m` millisatoshi: I will\n> only claim *all* of them when I can get multiple payments conditional on my\n> revelation of `s` that sum up to `m` millisatoshi or more.\n>   * I now call this \"economically-rational atomicity\", and describe also\n> them mildly here:\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2018-March/001109.html\n>\n> The above is *exactly* the behavior we want E to perform: only if it\n> receives both requests to reveal its secret `e` behind `E`, will it\n> actually reveal the secret.\n> In our case, we turn the requests to E into partial payments of a single\n> invoice (though of course, the payment from S to E would be in the asset\n> that S wants rather than in Bitcoin).\n> E would not like to \"sell short\" its secret `e` for less than the price of\n> its service, thus if E is economically rational, it would only claim\n> payments (and reveal `e`) once *all* incoming payments sum up to the price\n> of its service.\n>\n> In particular, the existing High AMP scheme would be reused to implement\n> the escrow E, and all that would be needed would be some way to request\n> invoices from E!\n> No additional code on top of High AMP is needed (very important, being a\n> lazy developer is hard work).\n> This should help make setting up new Es to be very simple, improving\n> competition in this space and helping further ensure economically-rational\n> behavior.\n>\n> If we squint, we can even say that:\n>\n> * This is actually a payment from B to E.\n> * The payment is split into two paths, B->X->E and B->X->S->E.\n> * X and S are just being paid surprisingly high fees, and are not in fact\n> being paid premiums or payment-for-product/service.\n>\n> Thus, any argument that we could make, about the safety of Bass Amplifier,\n> would also serve as an argument we could make about E behaving as we would\n> like it to behave!\n> This should greatly reduce the tr\\*st requirement from B and X to E: they\n> only need to trust that E operates econmically-rationally, and not trust\n> that E will, from the goodness of its own heart, operate correctly.\n>\n> (A remaining issue for the above scheme is that B learns `s + e` and `x +\n> e`, not either `s` or `x`, and thus does not actually get full\n> proof-of-payment.)\n>\n>\n> Further, this lets us extend this scheme to more than just two\n> simultaneous transactions.\n> If for example we need to set up 5 simultaneous transactions, and E\n> charges 1000 millisatoshi for its service, then we just have each receiver\n> forward 200 millisatoshi each to E once their incoming payment has been set\n> up.\n>\n> In fact, the above problem, where B learns `s + e` and `x + e` but not\n> either `s` or `x`, could be solved by splitting the payment to E three\n> ways, with a third split from B to E, so that B learns `e` and can solve\n> `(s + e) - e` == `s` and `(x + e) - e` == `x` to get proof-of-payment for\n> both the actual purchase, and the premium charged by the exchange.\n>\n> In all of these, E remains unaware of the exact details of the\n> arrangement, it only cares that it is paid, with the use of High AMP being\n> the \"coordination\n>\n>\n> Thus this scheme is surprisingly flexible, and may solve more problems\n> than you might think.\n> Thank you for this insight, Nadav!\n>\n> Regards,\n> ZmnSCPxj\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191011/0c0cc4f2/attachment.html>"
            },
            {
                "author": "Lloyd Fournier",
                "date": "2019-10-11T00:41:34",
                "message_text_only": "Hi Nadav,\n\nI've thought about similar problems before. Essentially you are trying to\ncreate an \"access structure\" on discrete logarithm (the completion of the\nadaptor signature in \"pay-to-point\"). I think the term for arbitrary\ncombinations of AND and ORs and even N-of-M is called a *monotone\naccess structure*.\n\n> Essentially the idea is to use the point S + ECDH(B, E) where S is the\nseller's point, B is the buyer's point and E is the escrow's point\n\nI can't see how you can create an access structure like this with ECDH.\nNeither B nor E know the discrete log of ECDH(B,E). I can see that you can\nhash it and use it as a scalar but then you have to make a heavy zkp to\nprove the validity (or interact with the escrow which violates the premise).\n\nFortunately, I think it is possible to create any monotone access structure\non a discrete logarithm using *verifiable encryption*.  Applying this to\nthe Escrow payment problem, the buyer decides on the payment point B and\nverifiably encrypts the discrete logarithm of B under the Escrow's public\nkey E and sends the encryption to the seller. If there's a dispute, the\nseller sends the encryption to the Escrow. If the Escrow resolve the\ndispute in favour of the seller they just decrypt the ciphertext and send\nthe discrete log of B to the seller. The seller can now redeem the payment.\n\nIf we want to create more complex access structures then we use verifiable\nsecret sharing where the discrete log of B is split up into shares and\ndistributed according the the desired structure.\n\nSo how do we do this verifiable encryption/secret sharing? Well it's not\nreally straight forward. In the case of Escrow, Camenisch-Shoup [1]\nverifiable encryption might be attractive since the Escrow can be trusted\nto produce the Paillier modulus properly. Otherwise there's\nCamenisch-Damgaard [2] which is much less efficient but only relies on CDH\nassumption. As far as I know there are no usable implementations of either\nof these schemes but Camenisch-Damgaard is relatively straightforward and I\nthink it's practical.\n\n[1] https://www.shoup.net/papers/verenc.pdf\n[2] https://eprint.iacr.org/1999/008\n\nLL\n\nOn Thu, Oct 10, 2019 at 10:43 AM Nadav Kohen <nadav at suredbits.com> wrote:\n\n>  Hi list,\n>\n> I'm back again with another idea about Payment Points and fun things to do\n> with them! Hopefully this time I'm not entirely just hashing out old ideas\n> in public like an out-of-the-loop person :)\n>\n> *TLDR: Adding and ECDH-ing points gives us AND and OR functionality which\n> we can compose to make cool lightning contracts like Multisig, Escrow, and\n> DLCs*\n>\n> So when looking at the following (likely incomplete) list of things you\n> can do with payment points:\n>\n> 1) Payment De-correlation\n> 2) \"Stuckless\" Payments\n> 3) High AMP\n> 4) Selling Signatures\n> 5) Selling Pedersen De-commitment\n> 6) Escrow Contracts\n>\n> I started of trying to classify what kind of thing these new features are\n> in hopes of coming across new ones. The first three I clumped into a group\n> I called \"Payment point addition allows us to do cool things while\n> maintaining the Proof of Payment (PoP) property\". The next two (4 and 5) I\n> called \"Commitment applications where point is public\". But ZmnSCPxj's\n> proposal for lightning escrow contracts (\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-June/002028.html)\n> struck me as something different that might somehow be made more general.\n>\n> Essentially the idea is to use the point S + ECDH(B, E) where S is the\n> seller's point, B is the buyer's point and E is the escrow's point. This\n> means that the scalar can be discovered by the seller in collaboration with\n> the buyer or the escrow, that is, S AND (B OR E). I propose that under\n> certain circumstances (such as the parties involved being able to\n> interact), this can be generalized to have payments conditioned on\n> arbitrary AND/OR circuits.\n>\n>  I believe that AND is very straightforward as you simply take two\n> conditions A and B and add them together to get a point that requires both\n> of their scalars are discoverable (except maybe under certain bad\n> circumstances that can be avoided where like B = C - A, this must be\n> guarded against).\n>\n> OR is harder but I think that it can be achieved in the two party case by\n> ECDH and in the n-party case by multi-party key exchanges (which I know\n> pretty much nothing about other than that they exist). Given some key\n> exchange protocol (preferably non-interactive), KE, KE(A_1, ..., A_n)\n> should result in a number known only to those who know any scalar a_1, ...,\n> a_n and no one else. Assuming this exists and we can manage to trustlessly\n> (in some possibly stretched sense of the word) compute shared keys\n> (including such things as KE(A+B, C)), then KE(A, B) acts as A OR B in our\n> payment condition contract.\n>\n> To restate the escrow contract idea in this setting, the payment is\n> conditioned on S + KE(B, E). Important to note is that not all parties must\n> know about the details of the payment itself: the Escrow in this example\n> knows nothing about this payment (other than that some payment exists)\n> unless there is a dispute.\n>\n> Lastly, note that contracts following this scheme look indistinguishable\n> to normal payments on the network, and are fully compatible with High AMPs\n> since we can simply take the payment point specified by our contract and\n> add that point to each partial payment point.\n>\n> Well this is all nice in theory, but is there anything that will actually\n> come out of this thinking? I'll detail the two things I've thought of so\n> far for which I'd love critique! I'd also love to hear if anyone else\n> things of any cool application using this line of thought (or really\n> anything cool with payment points :P).\n>\n> Idea 1: \"MultiSignature\" Lightning Contracts\n> I mean \"MultiSignature\" here only in the sense that m-of-n parties must\n> agree to the payment in order for it to happen, no actual signatures are\n> used. A \"MultiSignature\" contract is simply a bunch of ANDs and ORs! For\n> example a 2-of-3 multisig between parties A, B, and C can be represented as\n> (A AND B) OR (B AND C) OR (C AND A). As such, if some seller has a point S\n> and three parties have points A, B, and C where a certain payment must go\n> through if any two of them think it should, then the payment point used for\n> the payment should be S + KE(A + B, B + C, C + A). We add S to this point\n> so that the scalar, s, can act as proof of payment. And that's it! I\n> haven't thought long enough to come up with any situation where this might\n> be useful but hoping someone who reads this will!\n>\n> Idea 2: DLCs Routed over Lightning\n> Say that some DLC oracle will either broadcast s_A or s_B whose public\n> points are S_A and S_B (which can be computed from public information as\n> per the DLC scheme). Then say that Alice and Bob enter into a contract\n> under which Alice wins some amount if s_A is broadcasted and Bob if s_B is\n> broadcasted. Say Alice has a point A and Bob has a point B. They each send\n> the other a payment with the amount that party must receive if they win\n> with the payment point A + S_A for Bob's payment to Alice and B + S_B for\n> Alice's payment to Bob. And this is it! If s_A is broadcasted then Alice\n> gets paid (and Bob gets proof of payment a, which is the scalar to A),\n> otherwise s_B is broadcasted and Bob gets paid (with Alice receiving b as\n> PoP). An interesting note is that under this scheme neither party is forced\n> to pay extra on-chain fees in the case of a counter-party who doesn't\n> cooperate whilst in the wrong.\n> One wrinkle with this scheme is that setup isn't trustless. Although it is\n> generally true that one party must sign the funding transaction for a DLC\n> before the other party for on-chain DLCs, at least there is the mitigation\n> that when your counter-party goes silent, you can move your input funds\n> invalidating the funding transaction you signed (at a cost because fees).\n> So what can we do here to ensure that both payments are setup at the same\n> time in the case that Alice and Bob don't trust each other?\n> Say that although they don't trust each other, they're both willing to\n> trust some escrow entity who generates some point E for their payment.\n> Alice's payment point to Bob becomes B + S_B + E and Bob's to Alice becomes\n> A + S_A + E. The escrow now waits to hear from Alice and Bob that they have\n> incoming payments setup and only once both of them attest to this (using\n> signatures, for example) does the escrow release the scalar to E to them\n> both. The escrow can also have a timeout at which it will never reveal the\n> scalar to E: forcing both parties to commit to the contract well before the\n> DLC event. In this way, trust has been moved from counter-party to\n> trustworthy (hopefully) escrow in such a way that the Escrow learns nothing\n> about the contract itself (other than that there is one of some kind).\n> I believe that this scheme can be extended to more events through the use\n> of multiple payments being setup (usually in both directions) but this\n> seems complicated and I've rambled for long enough. One last note is that\n> DLC oracles can be composed in the usual way under this scheme (by\n> addition) and potentially even threshold multi-oracles can be supported in\n> this way, although this would require the oracles to attest to some shared\n> key's points with other oracles which isn't necessarily optimal.\n>\n> Best,\n> Nadav\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191011/44faca88/attachment-0001.html>"
            },
            {
                "author": "Jonas Nick",
                "date": "2019-10-11T11:49:44",
                "message_text_only": "> hashing out old ideas in public like an out-of-the-loop person\n\nBeing fully in-the-loop literally seems undesirable anyway. As the scriptless\nscripts space gets bigger, I invite everyone to consider contributing to the\nscriptless scripts repo at\nhttps://github.com/ElementsProject/scriptless-scripts. This allows to have the\nideas at one place, improve them over time and standardize on terminology and\nnotation. In particular, the escrow idea seems quite established and would be a\nnice to have written down comprehensively along with it's shortcomings.\n\n> OR is harder\n\nUnless you have additional requirements OR is simpler because it just means\ncreating and revealing the key to the other party. In your exmple \"(A AND B) OR\n(B AND C) OR (C AND A)\", A and B can generate each generate their key and add\nit. Then they each split their key into two shares, one is created by adding a\nrandom scalar, the other by subtracting the same random scalar. They each\nencrypt their first share to B and the second share to C. They do the same for\nC and A. The receivers must check that they've received valid shares, i.e.\ntheir shares sum up to the secret to the A + B key. That's not very efficient\nbecause it requires a lot of communication, but I don't know of a better scheme\nfor general disjunctive normal forms like that and this helps as a mental\nmodel. Any policy consisting of ANDs and ORs policy can be written in\ndisjunctive normal form and can therefore be constructed similar to above\nexample.\n\n\nOn 10/9/19 11:42 PM, Nadav Kohen wrote:\n>  Hi list,\n> \n> I'm back again with another idea about Payment Points and fun things to do\n> with them! Hopefully this time I'm not entirely just hashing out old ideas\n> in public like an out-of-the-loop person :)\n> \n> *TLDR: Adding and ECDH-ing points gives us AND and OR functionality which\n> we can compose to make cool lightning contracts like Multisig, Escrow, and\n> DLCs*\n> \n> So when looking at the following (likely incomplete) list of things you can\n> do with payment points:\n> \n> 1) Payment De-correlation\n> 2) \"Stuckless\" Payments\n> 3) High AMP\n> 4) Selling Signatures\n> 5) Selling Pedersen De-commitment\n> 6) Escrow Contracts\n> \n> I started of trying to classify what kind of thing these new features are\n> in hopes of coming across new ones. The first three I clumped into a group\n> I called \"Payment point addition allows us to do cool things while\n> maintaining the Proof of Payment (PoP) property\". The next two (4 and 5) I\n> called \"Commitment applications where point is public\". But ZmnSCPxj's\n> proposal for lightning escrow contracts (\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-June/002028.html)\n> struck me as something different that might somehow be made more general.\n> \n> Essentially the idea is to use the point S + ECDH(B, E) where S is the\n> seller's point, B is the buyer's point and E is the escrow's point. This\n> means that the scalar can be discovered by the seller in collaboration with\n> the buyer or the escrow, that is, S AND (B OR E). I propose that under\n> certain circumstances (such as the parties involved being able to\n> interact), this can be generalized to have payments conditioned on\n> arbitrary AND/OR circuits.\n> \n>  I believe that AND is very straightforward as you simply take two\n> conditions A and B and add them together to get a point that requires both\n> of their scalars are discoverable (except maybe under certain bad\n> circumstances that can be avoided where like B = C - A, this must be\n> guarded against).\n> \n> OR is harder but I think that it can be achieved in the two party case by\n> ECDH and in the n-party case by multi-party key exchanges (which I know\n> pretty much nothing about other than that they exist). Given some key\n> exchange protocol (preferably non-interactive), KE, KE(A_1, ..., A_n)\n> should result in a number known only to those who know any scalar a_1, ...,\n> a_n and no one else. Assuming this exists and we can manage to trustlessly\n> (in some possibly stretched sense of the word) compute shared keys\n> (including such things as KE(A+B, C)), then KE(A, B) acts as A OR B in our\n> payment condition contract.\n> \n> To restate the escrow contract idea in this setting, the payment is\n> conditioned on S + KE(B, E). Important to note is that not all parties must\n> know about the details of the payment itself: the Escrow in this example\n> knows nothing about this payment (other than that some payment exists)\n> unless there is a dispute.\n> \n> Lastly, note that contracts following this scheme look indistinguishable to\n> normal payments on the network, and are fully compatible with High AMPs\n> since we can simply take the payment point specified by our contract and\n> add that point to each partial payment point.\n> \n> Well this is all nice in theory, but is there anything that will actually\n> come out of this thinking? I'll detail the two things I've thought of so\n> far for which I'd love critique! I'd also love to hear if anyone else\n> things of any cool application using this line of thought (or really\n> anything cool with payment points :P).\n> \n> Idea 1: \"MultiSignature\" Lightning Contracts\n> I mean \"MultiSignature\" here only in the sense that m-of-n parties must\n> agree to the payment in order for it to happen, no actual signatures are\n> used. A \"MultiSignature\" contract is simply a bunch of ANDs and ORs! For\n> example a 2-of-3 multisig between parties A, B, and C can be represented as\n> (A AND B) OR (B AND C) OR (C AND A). As such, if some seller has a point S\n> and three parties have points A, B, and C where a certain payment must go\n> through if any two of them think it should, then the payment point used for\n> the payment should be S + KE(A + B, B + C, C + A). We add S to this point\n> so that the scalar, s, can act as proof of payment. And that's it! I\n> haven't thought long enough to come up with any situation where this might\n> be useful but hoping someone who reads this will!\n> \n> Idea 2: DLCs Routed over Lightning\n> Say that some DLC oracle will either broadcast s_A or s_B whose public\n> points are S_A and S_B (which can be computed from public information as\n> per the DLC scheme). Then say that Alice and Bob enter into a contract\n> under which Alice wins some amount if s_A is broadcasted and Bob if s_B is\n> broadcasted. Say Alice has a point A and Bob has a point B. They each send\n> the other a payment with the amount that party must receive if they win\n> with the payment point A + S_A for Bob's payment to Alice and B + S_B for\n> Alice's payment to Bob. And this is it! If s_A is broadcasted then Alice\n> gets paid (and Bob gets proof of payment a, which is the scalar to A),\n> otherwise s_B is broadcasted and Bob gets paid (with Alice receiving b as\n> PoP). An interesting note is that under this scheme neither party is forced\n> to pay extra on-chain fees in the case of a counter-party who doesn't\n> cooperate whilst in the wrong.\n> One wrinkle with this scheme is that setup isn't trustless. Although it is\n> generally true that one party must sign the funding transaction for a DLC\n> before the other party for on-chain DLCs, at least there is the mitigation\n> that when your counter-party goes silent, you can move your input funds\n> invalidating the funding transaction you signed (at a cost because fees).\n> So what can we do here to ensure that both payments are setup at the same\n> time in the case that Alice and Bob don't trust each other?\n> Say that although they don't trust each other, they're both willing to\n> trust some escrow entity who generates some point E for their payment.\n> Alice's payment point to Bob becomes B + S_B + E and Bob's to Alice becomes\n> A + S_A + E. The escrow now waits to hear from Alice and Bob that they have\n> incoming payments setup and only once both of them attest to this (using\n> signatures, for example) does the escrow release the scalar to E to them\n> both. The escrow can also have a timeout at which it will never reveal the\n> scalar to E: forcing both parties to commit to the contract well before the\n> DLC event. In this way, trust has been moved from counter-party to\n> trustworthy (hopefully) escrow in such a way that the Escrow learns nothing\n> about the contract itself (other than that there is one of some kind).\n> I believe that this scheme can be extended to more events through the use\n> of multiple payments being setup (usually in both directions) but this\n> seems complicated and I've rambled for long enough. One last note is that\n> DLC oracles can be composed in the usual way under this scheme (by\n> addition) and potentially even threshold multi-oracles can be supported in\n> this way, although this would require the oracles to attest to some shared\n> key's points with other oracles which isn't necessarily optimal.\n> \n> Best,\n> Nadav\n> \n> \n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-15T05:01:09",
                "message_text_only": "Good morning Jonas, Orfeas, and all,\n\n> > hashing out old ideas in public like an out-of-the-loop person\n>\n> Being fully in-the-loop literally seems undesirable anyway. As the scriptless\n> scripts space gets bigger, I invite everyone to consider contributing to the\n> scriptless scripts repo at\n> https://github.com/ElementsProject/scriptless-scripts. This allows to have the\n> ideas at one place, improve them over time and standardize on terminology and\n> notation. In particular, the escrow idea seems quite established and would be a\n> nice to have written down comprehensively along with it's shortcomings.\n\nThank you, this seems of interest.\n\n>\n> > OR is harder\n>\n> Unless you have additional requirements OR is simpler because it just means\n> creating and revealing the key to the other party. In your exmple \"(A AND B) OR\n> (B AND C) OR (C AND A)\", A and B can generate each generate their key and add\n> it. Then they each split their key into two shares, one is created by adding a\n> random scalar, the other by subtracting the same random scalar. They each\n> encrypt their first share to B and the second share to C. They do the same for\n> C and A. The receivers must check that they've received valid shares, i.e.\n> their shares sum up to the secret to the A + B key. That's not very efficient\n> because it requires a lot of communication, but I don't know of a better scheme\n> for general disjunctive normal forms like that and this helps as a mental\n> model. Any policy consisting of ANDs and ORs policy can be written in\n> disjunctive normal form and can therefore be constructed similar to above\n> example.\n\nI believe we do have additional requirements in some cases.\n\nFor example, consider the case of a DLC oracle which, at some future point, will be constrained to reveal the scalar behind one of the points `A`, `B`, `C`, or `D`.\n\nThen I make a bet with Nadav that the oracle will reveal the scalar behind either point A or point B, and that he should pay me if the oracle publishes either point.\n\nWhat can the oracle safely reveal beforehand, that would let Nadav and I arrange so that I am paid if I learn the secret behind point `A` or point `B`?\nIn particular, it should not be possible to, for example, learn `c` after learning only `a`, or learn `b` after learning only `a`, etc.\n\nI suppose for this part we could just have a separate \"aggregating oracle' which itself would be tr\\*sted to report the result of the first oracle accurately, adding more \"he said she said\" layers and reducing reliability and increasing the tr\\*sted set.\n\nI suppose a \"compiler\" for a language that implements arbitrary ANDs / ORs policy would then output some protocol specification about how the participants set things up.\n\n> > -   Sub-payment - one or more attempts, each of which semantically pay\n>\n> for \"the same thing\" for \"the same value\", set up in parallel.\n>\n> > a sub-payment may have multiple attempts running in parallel, but only\n>\n> one attempt will be claimable.\n>\n> > -   Payment - one or more sub-payments, each of which semantically pay\n>\n> for \"different parts of an entire thing\", set up in parallel.\n>\n> > a payment may have multiple sub-payments, and all sub-payments will be\n>\n> claimed atomically.\n>\n> This can be also thought of as:\n>\n> Payment = ONE-OF(attempt_11, ..., attempt_m1) AND ... AND\n> ONE-OF(attempt_n1, ..., attempt_m'n)\n>\n> Its dual also deserves some thought:\n>\n> Payment = ONE-OF(attempt_11 AND ... AND attempt_m1), ..., (attempt_n1\n> AND ... AND attempt_m'n))\n>\n> or in words, \"A payment is an atomic value transfer through many paths,\n> each of which carry a part of the entire value -- many alternative\n> groups of paths are available to be used, but only one of the groups\n> eventually goes through.\"\n>\n> Is there a reason to design in preference of one of the two?\n\nEngineering-wise, it seems better to group individual parallel attempts into a group that transfers a share of the total value.\nI imagine that grouping shares of the total value into a group that can be attempted in parallel with others, would require more funds to potentially be locked up.\nThat is, the payer needs to lock up units of the payment amount in order to run in parallel.\n\nIn the scheme as proposed, for example:\n\n* I decide to pay a 10 mBTC invoice by splitting it up to 5 mBTC and 5 mBTC sub-payments.\n* The first sub-payment reaches the destination immediately.\n* The second sub-payment takes a long time to reach the destination and I worry it is stuck.\n* Fortunately, I have some spare 5mBTC in another channel and I make a new stuckless attempt with that instead.\n\nIn sum total, I have put three different 5mBTC attempts to make a single 10mBTC payment, and I can control (because of the stuckless ACK) exactly how much the receiver can claim of those attempts.\n\nIf we were to group them differently, then it seems to me that I would have to speculatively allocate entire units of 10mBTC in order to create a new stuckless attempt.\n\n>\n> Speaking of generalization, it would be nice to have arbitrary AND-OR\n> combinations, but this needs further exploration:\n>\n> > If we want to create more complex access structures then we use\n>\n> verifiable secret sharing where the discrete log of B is split up into\n> shares and distributed according the the desired structure.\n>\n> One possible milestone of this generalisation would be to enable atomic\n> payments where the paying wallet says \"there are all these known paths,\n> each with such and such capacity; I want some to go through such that\n> the desired value is transferred in aggregate, no more, no less\n> (possibly within a margin of error)\".\n\nStuckless should be able to do this, if we compose it with AMP as I described above.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Nadav Kohen",
                "date": "2019-10-19T06:00:26",
                "message_text_only": "Hi all,\n\nAfter having some more thinking I think I have another cool thing that can\nbe done with barrier escrows.\n\nWe already have established that pay-for-atomic-multi-payment-setup can be\nprovided by using a High AMP to a node that is trusted not to collude with\nthe parties involved (along with payment point addition). Before moving on\nto my idea I'd also like to note that a sub-case of this is\npay-for-payment-setup where two parties pay an AMP to a barrier through\neach other where one way is a payment being setup (which likely has some\ncomplicated payment point) while the other direction is a simple payment\nthat requires only a secret known to the receiving node as well as the\nbarrier escrow secret. In this way, when the barrier subsides, one party\ngets paid and the other party has a payment setup to it. This can be used\nfor such things as option-like DLCs.\n\nAnyway, aside from very general multi-payment setup, I've realized that we\ncan likely use barrier escrows to re-negotiate payments/contracts. I'll\nstart with what I think is the most simple example:\n\nBob has a payment setup to Sally with payment point (S + X) where Sally\nknows S and X enforces some contract. The goal here is for Carol to set up\na payment to Sally for the point S' + X' (X' could be X), atomic with\nhaving Bob leave his contract (note that Bob and Carol could be the same\nentity in the case where payment amount is being re-negotiated).\n\nIn order to make this possible, instead of using (S + X) Bob and Sally\ninstead use (S + KE(X, E)) where KE is the key exchange function and E is a\npoint known to a barrier escrow. Carol can now set up her payment to sally\nby adding E to that payment. Sally sets up a payment to Bob for the amount\nof his original payment (perhaps minus some fee) with the point B + E\n(where Bob knows B). And lastly, Sally and Bob create an AMP to the barrier\nescrow in return for the scalar to E. Upon receiving this secret, Bob and\nSally can exit their contract by claiming (near) equal amounts, and Sally\nknow has Carol as her new contract participant.\n\nThis is the description of the simplest pay-for-payment-re-negotiation I\ncould think of but this scheme is fully extensible to re-negotiating\nmultiple set up payments. For example, say that Alice and Bob have set up a\nDLC over lightning as described earlier in this thread. Bob can sell his\nposition to Carol by atomically closing his position while Carol opens hers\nby using a barrier escrow. This is done by essentially reusing the escrow\nthat is baked in to Alice and Bob's contract for payment re-negotiation as\nthe barrier escrow used for atomic multi-payment setup between Alice and\nCarol.\n\nTo recap the many nuggets I can remember from this thread (so far) Payment\nPoints allow for:\n\n0) Everything listed in the first post in this thread\n1) Payments conditional on some discrete logarithm-based access structures\n(think a bunch of ANDs and ORs)\n    1a) Multisig-like access structures\n    1b) Oracle-dependent \"option\"\n2) Atomic multi-payment setup using barrier escrows\n    2a) Atomic payment for return payment setup\n    2b) Premium to cross-currency lightning nodes for use to avoid Free\nOption Problem\n    2c) Lightning DLCs\n    2d) Lightning DLC \"option\"s or one-way DLCs with a premium\n3) Multi-payment re-negotiation using barrier escrows\n    3a) Selling DLC positions\n4) Also, High AMP is interoperable with \"Stuckless\" payments\n5) It would be nice to have a language like miniscript (perhaps call it\n\"Improv\" because scriptless --due to LLoyd) that compiled to \"Layer 3\"\nprotocol specifications\n6) We should start writing these things up and adding them to:\nhttps://github.com/ElementsProject/scriptless-scripts\n\nThis stuff is super fun, I can't wait for there to be payment points on LN\n:)\n\nBest,\nNadav\n\nOn Tue, Oct 15, 2019 at 12:01 AM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Jonas, Orfeas, and all,\n>\n> > > hashing out old ideas in public like an out-of-the-loop person\n> >\n> > Being fully in-the-loop literally seems undesirable anyway. As the\n> scriptless\n> > scripts space gets bigger, I invite everyone to consider contributing to\n> the\n> > scriptless scripts repo at\n> > https://github.com/ElementsProject/scriptless-scripts. This allows to\n> have the\n> > ideas at one place, improve them over time and standardize on\n> terminology and\n> > notation. In particular, the escrow idea seems quite established and\n> would be a\n> > nice to have written down comprehensively along with it's shortcomings.\n>\n> Thank you, this seems of interest.\n>\n> >\n> > > OR is harder\n> >\n> > Unless you have additional requirements OR is simpler because it just\n> means\n> > creating and revealing the key to the other party. In your exmple \"(A\n> AND B) OR\n> > (B AND C) OR (C AND A)\", A and B can generate each generate their key\n> and add\n> > it. Then they each split their key into two shares, one is created by\n> adding a\n> > random scalar, the other by subtracting the same random scalar. They each\n> > encrypt their first share to B and the second share to C. They do the\n> same for\n> > C and A. The receivers must check that they've received valid shares,\n> i.e.\n> > their shares sum up to the secret to the A + B key. That's not very\n> efficient\n> > because it requires a lot of communication, but I don't know of a better\n> scheme\n> > for general disjunctive normal forms like that and this helps as a mental\n> > model. Any policy consisting of ANDs and ORs policy can be written in\n> > disjunctive normal form and can therefore be constructed similar to above\n> > example.\n>\n> I believe we do have additional requirements in some cases.\n>\n> For example, consider the case of a DLC oracle which, at some future\n> point, will be constrained to reveal the scalar behind one of the points\n> `A`, `B`, `C`, or `D`.\n>\n> Then I make a bet with Nadav that the oracle will reveal the scalar behind\n> either point A or point B, and that he should pay me if the oracle\n> publishes either point.\n>\n> What can the oracle safely reveal beforehand, that would let Nadav and I\n> arrange so that I am paid if I learn the secret behind point `A` or point\n> `B`?\n> In particular, it should not be possible to, for example, learn `c` after\n> learning only `a`, or learn `b` after learning only `a`, etc.\n>\n> I suppose for this part we could just have a separate \"aggregating oracle'\n> which itself would be tr\\*sted to report the result of the first oracle\n> accurately, adding more \"he said she said\" layers and reducing reliability\n> and increasing the tr\\*sted set.\n>\n> I suppose a \"compiler\" for a language that implements arbitrary ANDs / ORs\n> policy would then output some protocol specification about how the\n> participants set things up.\n>\n> > > -   Sub-payment - one or more attempts, each of which semantically pay\n> >\n> > for \"the same thing\" for \"the same value\", set up in parallel.\n> >\n> > > a sub-payment may have multiple attempts running in parallel, but only\n> >\n> > one attempt will be claimable.\n> >\n> > > -   Payment - one or more sub-payments, each of which semantically pay\n> >\n> > for \"different parts of an entire thing\", set up in parallel.\n> >\n> > > a payment may have multiple sub-payments, and all sub-payments will be\n> >\n> > claimed atomically.\n> >\n> > This can be also thought of as:\n> >\n> > Payment = ONE-OF(attempt_11, ..., attempt_m1) AND ... AND\n> > ONE-OF(attempt_n1, ..., attempt_m'n)\n> >\n> > Its dual also deserves some thought:\n> >\n> > Payment = ONE-OF(attempt_11 AND ... AND attempt_m1), ..., (attempt_n1\n> > AND ... AND attempt_m'n))\n> >\n> > or in words, \"A payment is an atomic value transfer through many paths,\n> > each of which carry a part of the entire value -- many alternative\n> > groups of paths are available to be used, but only one of the groups\n> > eventually goes through.\"\n> >\n> > Is there a reason to design in preference of one of the two?\n>\n> Engineering-wise, it seems better to group individual parallel attempts\n> into a group that transfers a share of the total value.\n> I imagine that grouping shares of the total value into a group that can be\n> attempted in parallel with others, would require more funds to potentially\n> be locked up.\n> That is, the payer needs to lock up units of the payment amount in order\n> to run in parallel.\n>\n> In the scheme as proposed, for example:\n>\n> * I decide to pay a 10 mBTC invoice by splitting it up to 5 mBTC and 5\n> mBTC sub-payments.\n> * The first sub-payment reaches the destination immediately.\n> * The second sub-payment takes a long time to reach the destination and I\n> worry it is stuck.\n> * Fortunately, I have some spare 5mBTC in another channel and I make a new\n> stuckless attempt with that instead.\n>\n> In sum total, I have put three different 5mBTC attempts to make a single\n> 10mBTC payment, and I can control (because of the stuckless ACK) exactly\n> how much the receiver can claim of those attempts.\n>\n> If we were to group them differently, then it seems to me that I would\n> have to speculatively allocate entire units of 10mBTC in order to create a\n> new stuckless attempt.\n>\n> >\n> > Speaking of generalization, it would be nice to have arbitrary AND-OR\n> > combinations, but this needs further exploration:\n> >\n> > > If we want to create more complex access structures then we use\n> >\n> > verifiable secret sharing where the discrete log of B is split up into\n> > shares and distributed according the the desired structure.\n> >\n> > One possible milestone of this generalisation would be to enable atomic\n> > payments where the paying wallet says \"there are all these known paths,\n> > each with such and such capacity; I want some to go through such that\n> > the desired value is transferred in aggregate, no more, no less\n> > (possibly within a margin of error)\".\n>\n> Stuckless should be able to do this, if we compose it with AMP as I\n> described above.\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191019/4bb2a626/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-24T06:39:27",
                "message_text_only": "Good morning list, and all,\n\nRene Pickhardt mentioned strongly about the new Boomerang paper: https://arxiv.org/pdf/1910.01834.pdf\n\nI would like to compare it with my own proposal which combines High AMP (\"Discrete Log AMP\" or \"DAMP\" or \"DLAMP\") with Stuckless to provide a similar facility:\n\n* https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-October/002225.html\n* https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-October/002232.html\n\nSo let me describe Boomerang mildly:\n\n* Boomerang requires payment points and scalars.\n  It uses the term \"HTLC\" still, but means \"pointlocked\" for the \"H\" here.\n* Boomerang uses a pay-for-contract construction involving two PTLC layers.\n  The first layer has a pointlocked branch involving the signatures of *both* channel endpoints, rather than the receiver.\n  The pointlocked branch is spent by the second-layer PTLC; thus both endpoints need to sign off on this in order to enforce that the second-layer PTLC is used.\n  * I describe \"pay for contract\" here: https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-January/001795.html\n    Of note is that the \"direction\" of the second layer is different in the Boomerang paper, whereas my above description has the same direction for the second-layer PTLC.\n    But I consider \"pay for contract\" a way to set up multiple transactions with specific outputs, and the \"second layer\" can be anything; the example I gave in my original treatment, to me, is simply one possibility among many possibilities.\n    Indeed, I also gave another example of pay-for-contract (though did not use the term): https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-July/002055.html\n  * The first layer sets up the second layer atomically.\n    The second layer lets the payer reclaim its funds in case the payee attempts to claim more than the allowed number of incoming payments.\n    If the payee is honest, however, inevitably the second layer times out and returns the funds to the payee, letting the payee use its newly-accepted funds.\n  * In a normal Boomerang payment where everyone acts honestly:\n    1.  The first-layer PTLC is claimed by the payee, instantiating the second-layer PTLC.\n    2.  The payer fails the second-layer PTLC, putting the money to the payee.\n\nI would like to raise the following points against Boomerang:\n\n* It requires more layers of PTLCs than my High AMP + Stuckless proposal.\n  While this \"should\" be negligible given that we are running offchain, it does increase onchain footprint if the channel is unilaterally dropped onchain while a Boomerang payment is ongoing.\n  * Either we have a separate Boomerang style of forwarding (which all forwarding nodes have to support in addition to \"normal\" PTLC forwards), or we enforce that all payment-point forwards use Boomerang at all times.\n    The former has the drawback that forwarding nodes might specifically monitor Boomerang (i.e. it is distinguishable from other formats), the latter has the drawback that small payments that would succeed on a single path would have the same overhead in case they get dropped onchain.\n* As I pointed out before: https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-June/002030.html\n  If the \"return path\" for \"ACK\" in Stuckless is the same as the normal payment path, then it becomes possible for forwarding nodes to guess the distance-to-payer, information it cannot determine currently.\n  * The \"ACK\" in Stuckless is equivalent to the first-layer PTLCs in Boomerang being claimed (and instantiating the second-layer PTLCs).\n    This lets forwarding nodes guess the distance-to-payer (by measuring the time between the first-layer PTLC is claimed and the second-layer PTLC is failed).\n    Stuckless has the possibility of sending the \"ACK\" via another means (hopefully not leaking timing information to forwarding nodes), but Boomerang, due to using pay-for-contract, cannot do so at all.\n* There is no incentive for the payer to actually fail the second-layer PTLC and thus release the payment immediately.\n  * In particular, the first-layer PTLC being claimed is sufficient to provide proof-of-payment to the payer.\n    The payer having acquired what it wanted (proof-of-payment), it can then refuse to cancel the second-layer PTLC until its timelock expires, on the off-chance the payee is hacked and leaks the secret that would let the payer claim the second-layer PTLC.\n  * In the High AMP + Stuckless proposal, the payee will not release the proof-of-payment until the payer provides the required secret to claim the money after the ACK.\n    Release of proof-of-payment is incentivized by being paid using an incoming PTLC.\n* Boomerang does not enable those cool hacks like Barrier Escrows.\n  One can argue that pay-for-contract implements the same behavior as barrier escrows.\n  But barrier escrows allow *any* contract arrangement to be made permissionlessly, without forwarding nodes becoming involved.\n  Using pay-for-contract requires that forwarding nodes be willing to support pay-for-contract, and being able to determine that the second-layer contract is something that is transportable over the Lightning Network, whereas barrier escrows can only be used with contracts that *are* transportable already.\n\nThus I believe the High AMP + Stuckless proposal I made would be better than Boomerang.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "A Payment Point Feature Family (MultiSig, DLC, Escrow, ...)",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Nadav Kohen",
                "Orfeas Stefanos Thyfronitis Litos",
                "Jonas Nick",
                "Lloyd Fournier",
                "ZmnSCPxj"
            ],
            "messages_count": 12,
            "total_messages_chars_count": 86611
        }
    },
    {
        "title": "[Lightning-dev] Increasing fee defaults to 5000+500 for a healthier network?",
        "thread_messages": [
            {
                "author": "Rusty Russell",
                "date": "2019-10-11T02:33:41",
                "message_text_only": "Hi all,\n\n        I've been looking at the current lightning network fees, and it\nshows that 2/3 are sitting on the default (1000 msat + 1 ppm).\n\nThis has two problems:\n1. Low fees are now a negative signal: defaults actually indicate\n   lower reliability, and routing gets tarpitted trying them all.\n2. There's no meaningful market signal in fees, since you can't\n   drop much below 1ppm.\n\nCompare lightningpowerusers.com which charges (10000 msat + 5000 ppm),\nand seems to have significant usage, so there clearly is market\ntolerance for higher fees.\n\nI am proposing that as of next release of c-lighting, we change defaults\non new channels to 5000 msat + 500ppm, and I'd like the other\nimplementations to join me.\n\nOver time, that should move the noise floor up.  I picked 500ppm because\nthat's still 1% at 20 hops, so minimally centralizing.  I picked 5000\nmsat base for less quantifiable reasons.\n\nHere's default fee a rate table in USD (@10k per BTC):\n\nAmount   Before      After\n0.1c     0.0100001c  0.05005c\n1c       0.010001c   0.0505c\n10c      0.01001c    0.055c\n$1       0.0101c     0.1c\n$10      0.011c      0.55c\n$100     0.02c       5.05c\n$1000    0.11c       50.05c\n\nThoughts?\nRusty."
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-11T03:15:50",
                "message_text_only": "Good morning,\n\nLooks fine to me.\n\nRegards,\nZmnSCPxj\n\n> Hi all,\n>\n> I've been looking at the current lightning network fees, and it\n> shows that 2/3 are sitting on the default (1000 msat + 1 ppm).\n>\n> This has two problems:\n>\n> 1.  Low fees are now a negative signal: defaults actually indicate\n>     lower reliability, and routing gets tarpitted trying them all.\n>\n> 2.  There's no meaningful market signal in fees, since you can't\n>     drop much below 1ppm.\n>\n>     Compare lightningpowerusers.com which charges (10000 msat + 5000 ppm),\n>     and seems to have significant usage, so there clearly is market\n>     tolerance for higher fees.\n>\n>     I am proposing that as of next release of c-lighting, we change defaults\n>     on new channels to 5000 msat + 500ppm, and I'd like the other\n>     implementations to join me.\n>\n>     Over time, that should move the noise floor up. I picked 500ppm because\n>     that's still 1% at 20 hops, so minimally centralizing. I picked 5000\n>     msat base for less quantifiable reasons.\n>\n>     Here's default fee a rate table in USD (@10k per BTC):\n>\n>     Amount Before After\n>     0.1c 0.0100001c 0.05005c\n>     1c 0.010001c 0.0505c\n>     10c 0.01001c 0.055c\n>     $1 0.0101c 0.1c\n>     $10 0.011c 0.55c\n>     $100 0.02c 5.05c\n>     $1000 0.11c 50.05c\n>\n>     Thoughts?\n>     Rusty.\n>\n>\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "Pierre",
                "date": "2019-10-11T08:12:53",
                "message_text_only": "Hi Rusty,\n\nThat seems reasonable.\n\nCheers,\n\nPierre"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2019-10-12T01:23:04",
                "message_text_only": "Hi Rusty,\n\nI think this change may be a bit misguided, and we should be careful about\nmaking sweeping changes to default values like this such as fees. I'm\nworried that this post (and the subsequent LGTMs by some developers)\npromotes the notion that somehow in Lightning, developers decide on fees\n(fees are too low, let's raise them!).\n\nIMO, there're a number of flaws in the reasoning behind this proposal:\n\n> defaults actually indicate lower reliability, and routing gets tarpitted\n> trying them all\n\nDefaults don't necessarily indicate higher/lower reliability. Issuing a\nsingle CLI command to raise/lower the fees on one's node doesn't magically\nmake the owner of said node a _better_ routing node operator. If a node has\nmany channels, with all of them poorly managed, then path finding algorithms\ncan move extrapolate the overall reliability of a node based on failures of\na sample of channels connected to that node. We've start to experiment with\nsuch an approach here, so far the results are promising[1].\n\n> There's no meaningful market signal in fees, since you can't drop much\n> below 1ppm.\n\nThe market signal one should be extracting from the current state is: a true\nmarket hasn't yet emerged as routing node operators are mostly hands off (as\nthey're used to being with their exiting bitcoin node) and have yet to begin\nto factor in the various costs of operating a node into their fees schedule.\nOnly a handful of routing node operators have started to experiment with\ndistinct fee settings in an attempt to feel out the level of elasticity in\nthe forwarding market today (if I double by fees, by how much do my daily\nforwards and fee revenue drop off?).\n\nKen Sedgwick had a pretty good talk on this topic as the most recent SF\nLightning Devs meet up[2]. The talk itself unfortunately wasn't recorded,\nbut there're a ton of cool graphs really digging into the various parameters\nin the current market. He draws a similar conclusion stating that: \"Many\ncurrent lightning channels are not charging enough fees to cover on-chain\nreplacement\".\n\nDevelopers raising the default fees (on their various implementations) won't\naddress this as it shows that the majority of participants today (routing\nnode operators) aren't yet thinking about their break even costs. IMO\ngenerally this is due to a lack of education, which we're working to address\nwith our blog post series (eventually to be a fully fledged standalone\nguide) on routing node operation[3]. Tooling also needs to improve to give\nrouting node operators better insight into their current level of\nperformance and efficacy of their capital allocation decisions.\n\n> Compare lightningpowerusers.com which charges (10000 msat + 5000 ppm),\n> and seems to have significant usage, so there clearly is market tolerance\n> for higher fees.\n\nIIRC, the fees on that node are only that high due to user error by the\noperator when setting their fees. `lnd` exposes fees on the command line\nusing the fixed point numerator which some find confusing. We'll likely add\nanother argument that allows users to specify their fees using their basis\npoints (bps) or a plain old percentage.\n\nIndependent of that, I don't think you can draw the conclusion that they\nhave \"significant\" usage, based on solely the number of channels they have.\nThat node has many channels due to the operator campaigning for users to\nopen channels with them on Twitter, as they provided an easy way to package\nlnd for desktop users. A node having numerous channels doesn't necessarily\nmean that they have significant usage, as it's easy to \"paint the tape\" with\non-chain transactions. What really matters is how effectively the node is\nmanaged.\n\nIn terms of market signals, IMO the gradual rise of fees _above_ the current\nwidely used default is a strong signal as it will indicate a level of\nmaturation in the market. Preemptively raising defaults only adds noise as\nthen the advertised fees are less indicative of the actual market\nconditions. Instead, we should (to promote a healthier network) educate\nprospective routing node operators on best practices, provide analysis\ntools t\nhey can use to make channel management and liquidity allocation decisions,\nand leave it up to the market participants to converge on steady state\neconomically rational fees!\n\n[1]: https://github.com/lightningnetwork/lnd/pull/3462\n[2]:\nhttps://github.com/ksedgwic/lndtool/blob/graphstats/lightning-fee-market.pdf\n[3]:\nhttps://blog.lightning.engineering/posts/2019/08/15/routing-quide-1.html\n\n\nOn Thu, Oct 10, 2019 at 7:50 PM Rusty Russell <rusty at rustcorp.com.au> wrote:\n\n> Hi all,\n>\n>         I've been looking at the current lightning network fees, and it\n> shows that 2/3 are sitting on the default (1000 msat + 1 ppm).\n>\n> This has two problems:\n> 1. Low fees are now a negative signal: defaults actually indicate\n>    lower reliability, and routing gets tarpitted trying them all.\n> 2. There's no meaningful market signal in fees, since you can't\n>    drop much below 1ppm.\n>\n> Compare lightningpowerusers.com which charges (10000 msat + 5000 ppm),\n> and seems to have significant usage, so there clearly is market\n> tolerance for higher fees.\n>\n> I am proposing that as of next release of c-lighting, we change defaults\n> on new channels to 5000 msat + 500ppm, and I'd like the other\n> implementations to join me.\n>\n> Over time, that should move the noise floor up.  I picked 500ppm because\n> that's still 1% at 20 hops, so minimally centralizing.  I picked 5000\n> msat base for less quantifiable reasons.\n>\n> Here's default fee a rate table in USD (@10k per BTC):\n>\n> Amount   Before      After\n> 0.1c     0.0100001c  0.05005c\n> 1c       0.010001c   0.0505c\n> 10c      0.01001c    0.055c\n> $1       0.0101c     0.1c\n> $10      0.011c      0.55c\n> $100     0.02c       5.05c\n> $1000    0.11c       50.05c\n>\n> Thoughts?\n> Rusty.\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191011/a52bb47f/attachment.html>"
            },
            {
                "author": "ecurrencyhodler",
                "date": "2019-10-12T17:16:09",
                "message_text_only": "Good morning Rusty.\n\nTo add to roasbeef's point, I don't think lightningpowerusers.com is a good\nindicator for market tolerance for higher fees either. It's highly\nconnected and does a lot of routing because Pierre has on boarded many\nusers through the node launcher. That means most of these users aren't\npower users and therefore aren't technically skilled enough or aware of how\nto find a cheaper route.\n\nI agree with the sentiment that the network is still too young to derive\nany meaningful observations around a routing fee market. This will become\nclearer though as more routing tools and analytics are created for routing\nnode operators as well wallet users.\n\nOn Fri, Oct 11, 2019 at 6:34 PM Olaoluwa Osuntokun <laolu32 at gmail.com>\nwrote:\n\n> Hi Rusty,\n>\n> I think this change may be a bit misguided, and we should be careful about\n> making sweeping changes to default values like this such as fees. I'm\n> worried that this post (and the subsequent LGTMs by some developers)\n> promotes the notion that somehow in Lightning, developers decide on fees\n> (fees are too low, let's raise them!).\n>\n> IMO, there're a number of flaws in the reasoning behind this proposal:\n>\n> > defaults actually indicate lower reliability, and routing gets tarpitted\n> > trying them all\n>\n> Defaults don't necessarily indicate higher/lower reliability. Issuing a\n> single CLI command to raise/lower the fees on one's node doesn't magically\n> make the owner of said node a _better_ routing node operator. If a node has\n> many channels, with all of them poorly managed, then path finding\n> algorithms\n> can move extrapolate the overall reliability of a node based on failures of\n> a sample of channels connected to that node. We've start to experiment with\n> such an approach here, so far the results are promising[1].\n>\n> > There's no meaningful market signal in fees, since you can't drop much\n> > below 1ppm.\n>\n> The market signal one should be extracting from the current state is: a\n> true\n> market hasn't yet emerged as routing node operators are mostly hands off\n> (as\n> they're used to being with their exiting bitcoin node) and have yet to\n> begin\n> to factor in the various costs of operating a node into their fees\n> schedule.\n> Only a handful of routing node operators have started to experiment with\n> distinct fee settings in an attempt to feel out the level of elasticity in\n> the forwarding market today (if I double by fees, by how much do my daily\n> forwards and fee revenue drop off?).\n>\n> Ken Sedgwick had a pretty good talk on this topic as the most recent SF\n> Lightning Devs meet up[2]. The talk itself unfortunately wasn't recorded,\n> but there're a ton of cool graphs really digging into the various\n> parameters\n> in the current market. He draws a similar conclusion stating that: \"Many\n> current lightning channels are not charging enough fees to cover on-chain\n> replacement\".\n>\n> Developers raising the default fees (on their various implementations)\n> won't\n> address this as it shows that the majority of participants today (routing\n> node operators) aren't yet thinking about their break even costs. IMO\n> generally this is due to a lack of education, which we're working to\n> address\n> with our blog post series (eventually to be a fully fledged standalone\n> guide) on routing node operation[3]. Tooling also needs to improve to give\n> routing node operators better insight into their current level of\n> performance and efficacy of their capital allocation decisions.\n>\n> > Compare lightningpowerusers.com which charges (10000 msat + 5000 ppm),\n> > and seems to have significant usage, so there clearly is market tolerance\n> > for higher fees.\n>\n> IIRC, the fees on that node are only that high due to user error by the\n> operator when setting their fees. `lnd` exposes fees on the command line\n> using the fixed point numerator which some find confusing. We'll likely add\n> another argument that allows users to specify their fees using their basis\n> points (bps) or a plain old percentage.\n>\n> Independent of that, I don't think you can draw the conclusion that they\n> have \"significant\" usage, based on solely the number of channels they have.\n> That node has many channels due to the operator campaigning for users to\n> open channels with them on Twitter, as they provided an easy way to package\n> lnd for desktop users. A node having numerous channels doesn't necessarily\n> mean that they have significant usage, as it's easy to \"paint the tape\"\n> with\n> on-chain transactions. What really matters is how effectively the node is\n> managed.\n>\n> In terms of market signals, IMO the gradual rise of fees _above_ the\n> current\n> widely used default is a strong signal as it will indicate a level of\n> maturation in the market. Preemptively raising defaults only adds noise as\n> then the advertised fees are less indicative of the actual market\n> conditions. Instead, we should (to promote a healthier network) educate\n> prospective routing node operators on best practices, provide analysis\n> tools t\n> hey can use to make channel management and liquidity allocation decisions,\n> and leave it up to the market participants to converge on steady state\n> economically rational fees!\n>\n> [1]: https://github.com/lightningnetwork/lnd/pull/3462\n> [2]:\n> https://github.com/ksedgwic/lndtool/blob/graphstats/lightning-fee-market.pdf\n> [3]:\n> https://blog.lightning.engineering/posts/2019/08/15/routing-quide-1.html\n>\n>\n> On Thu, Oct 10, 2019 at 7:50 PM Rusty Russell <rusty at rustcorp.com.au>\n> wrote:\n>\n>> Hi all,\n>>\n>>         I've been looking at the current lightning network fees, and it\n>> shows that 2/3 are sitting on the default (1000 msat + 1 ppm).\n>>\n>> This has two problems:\n>> 1. Low fees are now a negative signal: defaults actually indicate\n>>    lower reliability, and routing gets tarpitted trying them all.\n>> 2. There's no meaningful market signal in fees, since you can't\n>>    drop much below 1ppm.\n>>\n>> Compare lightningpowerusers.com which charges (10000 msat + 5000 ppm),\n>> and seems to have significant usage, so there clearly is market\n>> tolerance for higher fees.\n>>\n>> I am proposing that as of next release of c-lighting, we change defaults\n>> on new channels to 5000 msat + 500ppm, and I'd like the other\n>> implementations to join me.\n>>\n>> Over time, that should move the noise floor up.  I picked 500ppm because\n>> that's still 1% at 20 hops, so minimally centralizing.  I picked 5000\n>> msat base for less quantifiable reasons.\n>>\n>> Here's default fee a rate table in USD (@10k per BTC):\n>>\n>> Amount   Before      After\n>> 0.1c     0.0100001c  0.05005c\n>> 1c       0.010001c   0.0505c\n>> 10c      0.01001c    0.055c\n>> $1       0.0101c     0.1c\n>> $10      0.011c      0.55c\n>> $100     0.02c       5.05c\n>> $1000    0.11c       50.05c\n>>\n>> Thoughts?\n>> Rusty.\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191012/a54966e1/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2019-10-14T01:32:23",
                "message_text_only": "Olaoluwa Osuntokun <laolu32 at gmail.com> writes:\n> Hi Rusty,\n>\n> I think this change may be a bit misguided, and we should be careful about\n> making sweeping changes to default values like this such as fees. I'm\n> worried that this post (and the subsequent LGTMs by some developers)\n> promotes the notion that somehow in Lightning, developers decide on fees\n> (fees are too low, let's raise them!).\n\nPhew, I'm glad someone else is uncomfortable!  Yes, I held off this\nkind of proposal for a long time for exactly this reason.  However,\nthe truth seems to be that defaults have that power, whether we want\nit or not :(\n\nIf we make defaults more awkward, it will encourage people to change\nthem.  In the end, I settled on a simple number because I want to to\nbe easy to filter out these defaults from further analysis.\n\n> IMO, there're a number of flaws in the reasoning behind this proposal:\n>\n>> defaults actually indicate lower reliability, and routing gets tarpitted\n>> trying them all\n>\n> Defaults don't necessarily indicate higher/lower reliability. Issuing a\n> single CLI command to raise/lower the fees on one's node doesn't magically\n> make the owner of said node a _better_ routing node operator.\n\nNo, but those who put effort into their node presumably have more\nreliable nodes, and this is a signal of that.\n\nAnyone have data on channel reliability that they can correlate with\nchannel fees?\n\n> If a node has\n> many channels, with all of them poorly managed, then path finding algorithms\n> can move extrapolate the overall reliability of a node based on failures of\n> a sample of channels connected to that node. We've start to experiment with\n> such an approach here, so far the results are promising[1].\n\nThat's great if you're making many payments, but then you have many\nheuristics at your disposal.   Most people won't be making many\npayments, so such techniques are not useful.\n\n>> There's no meaningful market signal in fees, since you can't drop much\n>> below 1ppm.\n>\n> The market signal one should be extracting from the current state is: a true\n> market hasn't yet emerged as routing node operators are mostly hands off (as\n> they're used to being with their exiting bitcoin node) and have yet to begin\n> to factor in the various costs of operating a node into their fees schedule.\n> Only a handful of routing node operators have started to experiment with\n> distinct fee settings in an attempt to feel out the level of elasticity in\n> the forwarding market today (if I double by fees, by how much do my daily\n> forwards and fee revenue drop off?).\n>\n> Ken Sedgwick had a pretty good talk on this topic as the most recent SF\n> Lightning Devs meet up[2]. The talk itself unfortunately wasn't recorded,\n> but there're a ton of cool graphs really digging into the various parameters\n> in the current market. He draws a similar conclusion stating that: \"Many\n> current lightning channels are not charging enough fees to cover on-chain\n> replacement\".\n\nThis is all true, too.\n\n> Developers raising the default fees (on their various implementations) won't\n> address this as it shows that the majority of participants today (routing\n> node operators) aren't yet thinking about their break even costs. IMO\n> generally this is due to a lack of education, which we're working to address\n> with our blog post series (eventually to be a fully fledged standalone\n> guide) on routing node operation[3]. Tooling also needs to improve to give\n> routing node operators better insight into their current level of\n> performance and efficacy of their capital allocation decisions.\n\nAssuming a network in which many people are running nodes for their\nown use and only forwarding as a side-effect, the biggest factor will\n*always* be the default settings.\n\nBTW, a quick look at the percentiles (ignoring \"default setting\" channels):\n\nPercentile      Min Capacity    Max Capacity    Median Base     Median PPM\n                (sats)          (sats)          (msat)          \n0-10            1100            100000  \t0               10\n10-20           100000          200000  \t1               10\n20-30           200000          358517  \t1               8\n30-40           358517          546639  \t1               10\n40-50           546639          1000000 \t1               42\n50-60           1000000         2000000 \t106             10\n60-70           2000000         3143170 \t2               35\n70-80           3145265         5550000 \t1               800\n80-90           5561878         16000000\t0               800\n90-100          1600000         200000000\t0               1\nOverall:                                        1               10\n\n>> Compare lightningpowerusers.com which charges (10000 msat + 5000 ppm),\n>> and seems to have significant usage, so there clearly is market tolerance\n>> for higher fees.\n>\n> IIRC, the fees on that node are only that high due to user error by the\n> operator when setting their fees.\n\nNo; fiatjaf measured incorrectly and thought he was charging 5% and\nthere was much confusion.  It was always 0.5%, as it was supposed to be.\n\n> Independent of that, I don't think you can draw the conclusion that they\n> have \"significant\" usage, based on solely the number of channels they have.\n> That node has many channels due to the operator campaigning for users to\n> open channels with them on Twitter, as they provided an easy way to package\n> lnd for desktop users. A node having numerous channels doesn't necessarily\n> mean that they have significant usage, as it's easy to \"paint the tape\" with\n> on-chain transactions. What really matters is how effectively the node is\n> managed.\n\nCan Pierre weigh in with some usage information?\n\n> In terms of market signals, IMO the gradual rise of fees _above_ the current\n> widely used default is a strong signal as it will indicate a level of\n> maturation in the market.\n\nThose conditions are met, it seems?\n\n> Preemptively raising defaults only adds noise as\n> then the advertised fees are less indicative of the actual market\n> conditions.\n\nPreemptively?  How do you decide when to that is?  I agree low defaults\nwere great for testing, but they're probably distorting the market now.\n\n> Instead, we should (to promote a healthier network) educate\n> prospective routing node operators on best practices, provide analysis\n> tools t\n> hey can use to make channel management and liquidity allocation decisions,\n> and leave it up to the market participants to converge on steady state\n> economically rational fees!\n\nIf there's no anchoring effect of the defaults, then there's no harm in\nchanging them; at least increasing them would serve as a reliability signal\n(assuming that's true).\n\nIf they are distorting the market, we should change them so the market\ncan be more efficient.\n\nCheers,\nRusty.\n\n\n\n\n\n\n\n\n\n\n\n\n\"We just need to educate everyone\" is not really going to get us there,\nis it?  We want almost all users running nodes, and the benefit of them\nto tweak things is so small they're just going to stick with the\ndefaults.\n\n\nIn the long run, \"defaults\" will get smarter (eg. based on a comparison\nof other similar channels), but I suspect we'll see anchoring around\n*todays* defaults."
            }
        ],
        "thread_summary": {
            "title": "Increasing fee defaults to 5000+500 for a healthier network?",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "ecurrencyhodler",
                "Pierre",
                "Rusty Russell",
                "Olaoluwa Osuntokun",
                "ZmnSCPxj"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 23497
        }
    },
    {
        "title": "[Lightning-dev] Atomic Secrets Exchange",
        "thread_messages": [
            {
                "author": "CJP",
                "date": "2019-10-19T04:07:04",
                "message_text_only": "(Note to IAEA & others: this is not about nuclear technology. Feel free\nto read it though.)\n(Note 2: beware of awkward misspellings like atomic secret sexchange or\nthe like. I have no idea how to fix this.)\n\nHi,\n\nI admit I spend way too little time reading this mailing list, but in\nmy recent futile attempt to get up-to-date, I got inspired by the\nrecent elliptic curve based smart contracting discussions. I'd like to\npresent my still somewhat half-baked solution to a problem I found in\nthe discussion. I believe my solution, Atomic Secrets Exchange, is\nlikely to have applications beyond this particular problem. In the\ndescription below, I am assuming some future Lightning where elliptic\ncurve payment points are already used which allow EC arithmetic.\n\n# The use case\nThe problem I'm trying to solve is in the proposed protocol for placing\nbets, as described by Nadav[1]. The idea is:\n\n* There is an oracle that promises to either publish p, or publish q,\nat a certain point in time, for instance, based on some real-world\ninformation.\n* Alice and Bob place bets on what the oracle will publish: Alice pays\nBob if the oracle publishes p; Bob pays Alice if the oracle publishes\nq.\n* This can be realized by locking funds in two Lightning transactions:\none from Alice to Bob, which can be redeemed by Bob with knowledge of\np, and one from Bob to Alice, which can be redeemed by Alice with\nknowledge of q. The time-out of these transactions must be after the\npoint of publication of the oracle.\n\nThe problem is that one of the two transactions will always be created\nfirst; if, for instance, the tx from Alice to Bob is the first, then\nBob no longer has an incentive to create his tx to Alice. Not creating\nthe second tx results in a one-sided bet; this makes it risky to take\npart in the protocol (in this case, Alice is the victim).\n\nNadav proposed a solution with a partially trusted escrow party. I will\ntry to find a solution without an escrow party.\n\n#Solution outline\nIn my approach, the payment from Alice to Bob requires Bob to know p\n*and* know a secret sa, which is initially only known by Alice.\nSimilarly, the payment from Bob to Alice requires Alice to know q *and*\na secret sb, which is initially only known to Bob. As long as they\ndon't reveal these secrets to anyone, these are bound to time out (or\nvoluntarily canceled), even after the oracle has spoken. This makes\nthem safe to be locked in in any order. After locking in the\ntransactions, Alice and Bob must reveal their secrets to each other, to\nmake the locked-in transactions equivalent to an honest, two-sided bet.\n\nThis changes the problem of atomically locking two transactions into\natomically exchanging two secrets.\n\nMaybe the problem of atomically exchanging two secrets has already been\nsolved in a more elegant way (ECDH, anyone?), but I came up with this\nmethod:\n\n* Alice locks in a Lightning tx to Bob that requires Bob to know sa and\nsb, and reveal at least sb to Alice.\n* Bob then locks in a Lightning tx, with a similar amount of funds,\nback to Alice that requires Alice to know sa and reveal sa to Bob. This\nmust time out sooner than the first tx.\n* Alice redeems the second tx, revealing sa to Bob.\n* Bob redeems the first tx, revealing sb to Alice.\n\nNote that Bob can actually 'split the atom' by not redeeming the first\ntx, but he receives a penalty for this that roughly equals the tx\namount. This amount can be made sufficiently large (in comparison to\ne.g. the bet) as required to move Bob's incentives towards honest\nbehavior.\n\n#Some details\nIn the application of paid bets, one detail is time-outs of the three\ntransactions. I believe this is the correct order:\n\n* Locking in the bet txes\n* Locking in the secrets exchange txes\n* Time-out of the secrets exchange txes \n* Publication by the oracle\n* Time-out of the bet txes \n\nAnother \"detail\" is how to do the elliptic curve magic. This is my\nbeginners' attempt:\nP = p*G\nQ = q*G\nSA = sa*G\nSB = sb*G\nBet txes: PP_b0 = P+SA, PP_b1 = Q+SB\nSecrets Exchange txes: PP_x0 = SA+SB, PP_x1 = SA\nBob has to know SA to verify the value of PP_x0, and to generate PP_x1.\nI don't know if a subtract operation exists for elliptic curve points;\nin that case Bob could calculate SA = PP_b0 - P. Otherwise, Alice could\njust tell Bob SA, e.g. as meta-data included in the first bet tx.\nSimilarly, Alice has to know SB to generate PP_x0; Alice could\ncalculate SB = PP_b1 - Q, or Bob could tell Alice SB, e.g. as meta-data \nincluded in the second bet tx.\n\nCJP\n\n[1] idea 2 in https://lists.linuxfoundation.org/pipermail/lightning-dev\n/2019-October/002213.html"
            },
            {
                "author": "Nadav Kohen",
                "date": "2019-10-19T06:40:38",
                "message_text_only": "Hey CJP and list,\n\nThanks for the post!\n\n> Maybe the problem of atomically exchanging two secrets has already been\n> solved in a more elegant way (ECDH, anyone?)\n\nI don't know of any nice way of doing this since it seems like it should be\nimpossible for both parties to learn something in a single action (since\none person would have to perform that action and learn nothing) though I'd\nbe happy to be wrong on that if anyone else on the list knows something.\nBut I do believe that the best that can be done is motivate one party to\nact once the other party has, much like you suggest.\n\n> * Alice locks in a Lightning tx to Bob that requires Bob to know sa and\n> sb, and reveal at least sb to Alice.\n> * Bob then locks in a Lightning tx, with a similar amount of funds,\n> back to Alice that requires Alice to know sa and reveal sa to Bob. This\n> must time out sooner than the first tx.\n>  * Alice redeems the second tx, revealing sa to Bob.\n> * Bob redeems the first tx, revealing sb to Alice.\n\n> Note that Bob can actually 'split the atom' by not redeeming the first\n> tx, but he receives a penalty for this that roughly equals the tx\n> amount. This amount can be made sufficiently large (in comparison to\n> e.g. the bet) as required to move Bob's incentives towards honest\n> behavior.\n\nVery nice :) I think this approach is correct and should be successful in\nthe two-party binary outcome case. I'd be curious if it is possible to\nextend this to multiple payments in each direction (which is how you do\nmore than two outcomes) and if it is possible to involve more parties as\nwell, though I suspect these two things are essentially the same in terms\nof the process required/complexity involved.\n\nI would say the up-side to this scheme is no third party is required, which\nof course is great!\n\nThe down-side seems to be that you must temporarily over-collateralize\nwhich means more than double the routing (and fees associated) as well as\nmore than double the capital requirements. I don't see this being an issue\nin a micro-betting setting but in such a setting it is also a little\nsub-optimal that you would have to temporarily over-collateralize and open\nyourself up to grieving attacks of 2-3x a single bet's size while trying to\nkeep things micro in micro-betting.\n\n> P = p*G\n> Q = q*G\n> SA = sa*G\n> SB = sb*G\n> Bet txes: PP_b0 = P+SA, PP_b1 = Q+SB\n> Secrets Exchange txes: PP_x0 = SA+SB, PP_x1 = SA\n> Bob has to know SA to verify the value of PP_x0, and to generate PP_x1.\n> I don't know if a subtract operation exists for elliptic curve points;\n> in that case Bob could calculate SA = PP_b0 - P. Otherwise, Alice could\n> just tell Bob SA, e.g. as meta-data included in the first bet tx.\n\nSubtraction is a thing and that is indeed hou you would use it.\n\nI will note that as a detail both of the Bet txs should also include one\nmore point that gets added in by the person getting paid (which can be in\nan invoice or something like that) so that the scalar to that point can act\nas Proof of Payment (since for example p and sa are both already known to\nAlice, it is nice to have Bob add a scalar b to this, by invoicing B = b*G\nbefore setup, so that Alice can subtract off the things she knows and get\nan atomic receipt of payment).\n\nOne last detail is that I think the oracle scheme is nicest if rather than\nhaving points P and Q with their pre-images, which is quite\nlightning-specific, we instead use normal DLC oracles which pre-commit to a\npublic key P and a public nonce for each event R from which for each\npossible signature that will be broadcasted, sig_Event, the point\nsig_Event*G = R + h(event)*P can be computed and thus we can use these\nsignatures as our scalar pre-images :)\n\nBest,\nNadav\n\nOn Fri, Oct 18, 2019 at 11:14 PM CJP <cjp at ultimatestunts.nl> wrote:\n\n> (Note to IAEA & others: this is not about nuclear technology. Feel free\n> to read it though.)\n> (Note 2: beware of awkward misspellings like atomic secret sexchange or\n> the like. I have no idea how to fix this.)\n>\n> Hi,\n>\n> I admit I spend way too little time reading this mailing list, but in\n> my recent futile attempt to get up-to-date, I got inspired by the\n> recent elliptic curve based smart contracting discussions. I'd like to\n> present my still somewhat half-baked solution to a problem I found in\n> the discussion. I believe my solution, Atomic Secrets Exchange, is\n> likely to have applications beyond this particular problem. In the\n> description below, I am assuming some future Lightning where elliptic\n> curve payment points are already used which allow EC arithmetic.\n>\n> # The use case\n> The problem I'm trying to solve is in the proposed protocol for placing\n> bets, as described by Nadav[1]. The idea is:\n>\n> * There is an oracle that promises to either publish p, or publish q,\n> at a certain point in time, for instance, based on some real-world\n> information.\n> * Alice and Bob place bets on what the oracle will publish: Alice pays\n> Bob if the oracle publishes p; Bob pays Alice if the oracle publishes\n> q.\n> * This can be realized by locking funds in two Lightning transactions:\n> one from Alice to Bob, which can be redeemed by Bob with knowledge of\n> p, and one from Bob to Alice, which can be redeemed by Alice with\n> knowledge of q. The time-out of these transactions must be after the\n> point of publication of the oracle.\n>\n> The problem is that one of the two transactions will always be created\n> first; if, for instance, the tx from Alice to Bob is the first, then\n> Bob no longer has an incentive to create his tx to Alice. Not creating\n> the second tx results in a one-sided bet; this makes it risky to take\n> part in the protocol (in this case, Alice is the victim).\n>\n> Nadav proposed a solution with a partially trusted escrow party. I will\n> try to find a solution without an escrow party.\n>\n> #Solution outline\n> In my approach, the payment from Alice to Bob requires Bob to know p\n> *and* know a secret sa, which is initially only known by Alice.\n> Similarly, the payment from Bob to Alice requires Alice to know q *and*\n> a secret sb, which is initially only known to Bob. As long as they\n> don't reveal these secrets to anyone, these are bound to time out (or\n> voluntarily canceled), even after the oracle has spoken. This makes\n> them safe to be locked in in any order. After locking in the\n> transactions, Alice and Bob must reveal their secrets to each other, to\n> make the locked-in transactions equivalent to an honest, two-sided bet.\n>\n> This changes the problem of atomically locking two transactions into\n> atomically exchanging two secrets.\n>\n> Maybe the problem of atomically exchanging two secrets has already been\n> solved in a more elegant way (ECDH, anyone?), but I came up with this\n> method:\n>\n> * Alice locks in a Lightning tx to Bob that requires Bob to know sa and\n> sb, and reveal at least sb to Alice.\n> * Bob then locks in a Lightning tx, with a similar amount of funds,\n> back to Alice that requires Alice to know sa and reveal sa to Bob. This\n> must time out sooner than the first tx.\n> * Alice redeems the second tx, revealing sa to Bob.\n> * Bob redeems the first tx, revealing sb to Alice.\n>\n> Note that Bob can actually 'split the atom' by not redeeming the first\n> tx, but he receives a penalty for this that roughly equals the tx\n> amount. This amount can be made sufficiently large (in comparison to\n> e.g. the bet) as required to move Bob's incentives towards honest\n> behavior.\n>\n> #Some details\n> In the application of paid bets, one detail is time-outs of the three\n> transactions. I believe this is the correct order:\n>\n> * Locking in the bet txes\n> * Locking in the secrets exchange txes\n> * Time-out of the secrets exchange txes\n> * Publication by the oracle\n> * Time-out of the bet txes\n>\n> Another \"detail\" is how to do the elliptic curve magic. This is my\n> beginners' attempt:\n> P = p*G\n> Q = q*G\n> SA = sa*G\n> SB = sb*G\n> Bet txes: PP_b0 = P+SA, PP_b1 = Q+SB\n> Secrets Exchange txes: PP_x0 = SA+SB, PP_x1 = SA\n> Bob has to know SA to verify the value of PP_x0, and to generate PP_x1.\n> I don't know if a subtract operation exists for elliptic curve points;\n> in that case Bob could calculate SA = PP_b0 - P. Otherwise, Alice could\n> just tell Bob SA, e.g. as meta-data included in the first bet tx.\n> Similarly, Alice has to know SB to generate PP_x0; Alice could\n> calculate SB = PP_b1 - Q, or Bob could tell Alice SB, e.g. as meta-data\n> included in the second bet tx.\n>\n> CJP\n>\n> [1] idea 2 in https://lists.linuxfoundation.org/pipermail/lightning-dev\n> /2019-October/002213.html\n> <https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-October/002213.html>\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191019/eab1ea4a/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Atomic Secrets Exchange",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Nadav Kohen",
                "CJP"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 13607
        }
    },
    {
        "title": "[Lightning-dev] Nodelets, a layer 3 proposal",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-20T14:00:58",
                "message_text_only": "Introduction\n============\n\nAs the Lightning Network grows, we should remember that pathfinding algorithms are, at best, O(n log n) (mildly superlinear) on the number of nodes in the map.\nTo be more specific, the number of nodes scanned by e.g. Dijkstra will be b^d , where d is the route length and b is the \"branching factor\", i.e. the number of candidate paths going out from each node, i.e. proportional to the number of channels that a node has.\n\nOne method of handling large-scale public networks is to encourage the use of unpublished channels, to reduce the number of publicly-admitted nodes and channels that need to be scanned.\nHowever, unpublished channels have no incentive on-network, and can incentivize instead surveillance of users of unpublished channels.\n\nOne observation from Channel Factories, is the consideration that part of the scaling benefit we can build on top of Bitcoin is the ability of multiple parties to share ownership of a single UTXO.\nChannel factories allow multiple channels to be rooted from a single UTXO, providing tremendous onchain scaling.\nHowever, if the channels within a factory are published, this increases the routemap size and further increases the effort needed for pathfinding algorithms.\n\nThus in this writeup I propose Nodelets.\nNodelets are share-owners of a single Lightning Node, combining the reduced routemap of unpublished channels with the raw onchain scaling of channel factories.\nBy use of Schnorr MuSig, Nodelets can provide shared ownership of a Lightning node, allowing multiple users to be represented by a single Lightning node, while retaining non-custodiality, at the drawback of requiring full onlineness of all participants and leaking their financial activity to each other.\n\nPrerequisites\n=============\n\nThe blockchain on which Nodelets can be implemented, must have these features:\n\n* Schnorr signatures.\n  This is needed in order to use MuSig to generate a single signature for the node that the nodelets are share-owners of.\n* `SIGHASH_NOINPUT` or similar feature.\n  This is needed in order to allow transactions to be created that spend a particular address of a particular value, without knowing what transaction will actually contain an output of that address and value.\n\nThe Lightning Network also needs to switch to using Schnorr signatures for node announcements, channel announcements, and channel updates.\n\nUnpublished Channels\n====================\n\nUnpublished channels seek to reduce the routemap size (and thus prevent explosion of pathfinding effort) by reducing the number of channels, and reducing the number of public nodes (a node only exists on the public routemap if it has at least one published channel).\n\nHowever, unpublished channels have the following drawbacks:\n\n* Unpublished channels will earn less in routing fees, as nobody else will use them other than the unpublished node.\n* Activity on unpublished channels is **obviously** arising or terminating to the unpublished node, as no other node can possibly know of the channel.\n\nFurther, let us consider what would happen if a public node allows an unpublished channel to be created between itself and an unpublished node.\nThe public node can surveill every activity that occurs on the unpublished channel; the public node can generally assume with very high probability that the channel is used only by the counterparty and no one else, given that no on else knows of the channel.\n\n(while it is true that the channel is known by others who have received an invoice using that channel for routeboost, it should be noted, that it is only the receiving direction of that channel that is known; a node with only unpublished channels still cannot hide its activity among forwarding requests of others)\n\nThus, the public node is in a position to log and potentially resell later the financial activity occurring on the channel.\nA node which refuses to do this loses opportunity to resell the information, earning less money.\n\nFurther, if the public node has many public channels, it has an informational advantage over the unpublished node.\nThe unpublished node reveals all its financial information over the channel to the public node, but the public node does not reveal any financial information to the unpublished node.\n\n* If the other node is unpublished, then the public node will never route a payment via the node, thus will never use the unpublished channel.\n  * The only time the public node will use the unpublished channel is if it directly paying the unpublished node.\n    But the payee cannot get information on which node is the source of a payment, a deliberate design decision to preserve privacy.\n* The public node can use its published channels to send and receive payments.\n  Due to the fact that those channels are published, its counterparties in the public network cannot be sure that the payment arises from / terminates in the public node.\n* Thus, the unpublished node surrenders all of its financial information occurring on the unpublished channel, while the counterparty public node does not give any financial information to anyone.\n\nThus a basic assumption of unpublished channels is:\n\n* Anyone you form an unpublished channel with, is being tr\\*sted to not monitor and resell your offchain financial activity occurring on that channel.\n\nChannel Factories\n=================\n\nChannel factories utilize an n-of-n UTXO that is then split into multiple 2-of-2 channels.\nThe splitting transaction is placed inside a multiparty update mechanism, such as Decker-Wattenhofer or Decker-Russell-Osuntokun.\n\nThere is a minor drawback to the existing multiparty update mechanisms:\n\n* HTLCs transported across any channel directly or indirectly rooted from a Decker-Wattenhofer or Decker-Russell-Osuntokun need to have lock times larger than the CSV security parameter of the Decker-Wattenhofer / Decker-Russell-Osuntokun construction.\n  * In particular, if a payment is put in a hodl invoice, any HTLCs pending on channels inside or using Decker-Wattenhofer / Decker-Russell-Osuntokun need to be closed, by the CSV number of blocks, before the HTLC times out.\n    This is in contrast with Poon-Dryja mechanism, where channel closure of a hedl invoice can be deferred to just 1 block before the HTLC times out.\n\nHowever, channel factories provide tremendous scaling benefits, thus we are generally willing to accept the above drawback.\nBy use of channel factories, we can (eventually) scale even further using Lightning, as many people can effectively share ownership of a single UTXO.\n\nFurther, channel factories allow for graceful degradation.\nSince channel factories require many participants, the probability of one participant being offline is much higher than with 2-participant channels.\nIf one participant is offline, this prevents channel factories from reorganizing the channels.\nHowever, any payments moving through the channels inside the factory can still push through, on channels where the offline participant is not involved in.\n\nThus, we can say that channel factories have the property:\n\n* Channel factories degrade gracefully when a participant is offline.\n\nNodelets\n========\n\nA Nodelet is a share-owner of a Lightning Network node.\n\nThis means that the onchain and offchain funds of the LN node are the total funds of all nodelets composing the node.\nAs a basic consideration, the node public key (the \"node id\") is the n-of-n MuSig of all nodelets composing the node.\n\nAll funds owned by the node are publicly seen as a single owner, but is really a MuSig of the nodelet public keys.\n\nIn the below, we have a MuSig(A,B,C) node composed of A, B, and C, nodelets.\n\nTransport ECDH\n--------------\n\nPart of the BOLT 8 transport protocol is an ECDH between a node id and the remote node ephemeral public key.\n\nIf a node composed of nodelets `A`, `B`, and `C` uses the `MuSig(A,B,C)` of the node, the public key of the node is:\n\n    MuSig(A, B, C) = h(A | B | C | A) * A + h(A | B | C | B) * B + h(A | B | C | C) * C\n\nThe corresponding private key (given `a * G = A` et al.) is:\n\n    h(A | B | C | A) * a + h(A | B | C | B) * b + h(A | B | C | C) * c\n\nThus, the ECDH with the remote ephemeral public key `E` is the private key of our node, times the public key `E`:\n\n    (h(A | B | C | A) * a + h(A | B | C | B) * b + h(A | B | C | C) * c) * E\n    == h(A | B | C | A) * a * E + h(A | B | C | B) * b * E + h(A | B | C | C) * c * E\n\nEach of the nodelets can then individually compute their `h(A | B | C | A) * a * E`, then provide the product to the other nodelets, without revealing their own private keys.\nThe sum of these products is then the above ECDH between the aggregated node, and the remote ephemeral key.\n\nThe same operation can be done with the ECDH between the remote node id and the node ephemeral key.\nThe nodelets simply perform MuSig on their own ephemeral keys.\n\nA concern arises in that it is possible one or more of the nodelets does not perform the multiplication properly.\nIn such a case, establishment of the BOLT 8 transport fails and the remote node disconnects.\nIn case of multiple such events, the other nodelets may very well destroy the node and unilaterally redeem their funds onchain.\n\nRevocation\n----------\n\nUnder Poon-Dryja, each update requires the generation of a revocation secret.\nThe secret needs to be revealed to the channel counterparty in order to progress the state.\n\nHowever, revocation means that nodes composed of nodelets cannot safely use Poon-Dryja.\n\n* Suppose there exists a node composed of multiple nodelets.\n* Secretly, one of the nodelets (the *rogue nodelet*) starts up a full Lightning node and initiates a channel with the MuSig-composed node.\n* The nodelets sign the revocable commitment transactions of both itself and the counterparty.\n* The new Lightning node sends out its value elsewhere on the network.\n  This revokes older commitments include the original one.\n* The rogue nodelet knows the initial revocable commitment transaction and publishes it.\n* The new Lightning node (really owned by the rogue nodelet) revokes the published transaction.\n\nThus, nodelets cannot use Poon-Dryja channels at all.\n\nInstead, we can simply use the newer Decker-Russell-Osuntokun construction for all channels.\nThis has the advantage that the completed signature of the next state transaction serves as a sufficient revocation of every older state transaction, and revocation shenanigans are reduced.\n\nFunding Channels\n----------------\n\nSuppose we have a node composed of three nodelets A, B, and C.\nNow suppose nodelets A and B own some onchain funds and want to create a channel between the MuSig(A,B,C) node and another node, Z.\nFor purpose of example, let us suppose that both A and B want to put 5 mBTC each to form a 10mBTC channel.\n\nPart of the funding process involves exchanging signatures between the node MuSig(A,B,C) and Z for the initial commitment transactions (the initial update and settlement transactions under Decker-Russell-Osuntokun).\n\nBetween the nodelets A, B, and C, before creating the signature for the initial commitment transactions, the nodelets must first create a transaction which spends the MuSig(A,B,C) output.\nThese nodelet-settlement transactions spend from the initial commitment transactions, the output that will return the value to the funder.\nIt then splits up this value to two outputs, 5 mBTC to A and 5 mBTC to B.\nThen all nodelets first sign the nodelet-settlement transaction, before signing the initial commitment transaction that is sent in `funding_created`.\n\nAfter the channel is established, MuSig(A,B,C) not only tracks how the fund is split between MuSig(A,B,C) and Z, but also how the MuSig(A,B,C) is split between A, B, and C.\n\nAccepting Channels\n------------------\n\nSuppose another node, Y, wants to make a 10mBTC channel to MuSig(A,B,C).\nFor this case, MuSig(A,B,C) simply contacts its component nodelets A, B, and C to generate the signature needed for the `funding_signed` message.\n\nNoteworthy is that Y itself may itself in reality be a MuSig(U, V, W,X), but MuSig(A,B,C) need not be aware of this fact.\n\nForwarding HTLCs\n----------------\n\nNow that MuSig(A,B,C) has an incoming channel from Y and an outgoing channel to Z, and both channels are (of course) published, it is now possible to forward payments via Y-to-MuSig(A,B,C)-to-Z.\n\nSuppose MuSig(A,B,C) receives an incoming 2mBTC HTLC for forwarding from Y, going to Z.\nThe question now is how will the incoming HTLC be distributed among A, B, and C?\n\nFortunately, the aggregate node is able to decode the onion packet as soon as it is sent.\nIt can determine the next channel even before the incoming HTLC is irrevocably committed.\n\nThe next channel from MuSig(A,B,C)-to-Z only contains funds of A and B (5mBTC) each.\nThus, the incoming HTLC should only be claimable by A and B.\nA and B can assure this by building a claim transaction spending the 2mBTC HTLC output on both versions of the commitment transaction (remote and local) and splitting it up to A-only and B-only outputs of 1mBTC each.\nThen, before A and B sign the new version of the commitment transaction, they will demand that C provide the partial signature for this claim transaction before providing the partial signatures for the commitment transaction.\n`SIGHASH_NOINPUT` will let the nodelets reduce the communication overhead (they can reuse the signatures for the claim transactions even as new commitment transactions are made), but is not strictly necessary.\n\nIn effect, the MuSig(A,B,C) node needs to \"pre-reserve\" some capacity in the MuSig(A,B,C)-to-Z channel.\nThen later, when the incoming HTLC becomes irrevocably committed, it then uses the funds of A and B (5mBTC each) to form an outgoing 2mBTC HTLC.\n\nCreating Invoices\n-----------------\n\nSuppose C wants to receive 3mBTC.\nC simply needs to generate an invoice by itself, with a preimage only C knows, but using MuSig(A,B,C) as the node receiving the payment.\nC would then ask the other nodelets to sign the invoice, in order to create an invoice signed by MuSig(A,B,C)\n\nC wants to leak as little information about the invoice contents as possible.\nFortunately, we can use `h` key of invoice to store the hash of the invoice description, to prevent A and B from learning the invoice description.\nHowever, this has the drawback that there is no standard way to provide the invoice description behind an `h` invoice key.\n\nAn issue here is that the node, as an aggregate, is provably liable to the payment, once the proof-of-payment is released.\nHowever, since the node is in fact an aggregate, there must be some way to track back that C was the actual payee, and that A and B were simply cooperating nodelets.\nThis can be done by having C also sign the invoice using only its own private key.\nThis signature is insufficient by itself for the invoice (as the invoice points to MuSig(A,B,C) as being the one paid, thus requiring the signature of MuSig(A,B,C)).\nHowever, A and B can reveal this later to show that the invoice is actually a liability accepted only by C in exchange for payment of the invoice.\n\nThus, the procedure is:\n\n1.  C generates the invoice with `h` for description.\n2.  C provides the invoice and a signature using only C for this invoice to A and B.\n3.  A, B, and C perform a MuSig signing of the entire invoice and provides the completed signature to C.\n4.  C provides the MuSig(A,B,C)-signed invoice to the payer, as well as the description of the liability C is taking on (e.g. \"pay 500,000 BTC onchain to Daniel Kleiman\").\n\nReceiving Funds\n---------------\n\nNow that C has an invoice, it will want to receive funds using that invoice.\nNow until the invoice is paid, no other nodelet other than C knows the preimage.\nHowever, both A and B know that C knows the preimage to the payment hash.\n\nThus, on receive of an onion route that terminates at the MuSig(A, B, C) node, both A and B generate a transaction which claims the incoming HTLC and gives it solely to C.\nA and B then initiate signing that transaction.\nOnce C acquires the signature and the transaction, it is possible for C to claim the funds even if it has to enforce it onchain.\nAt that point, C can safely provide the preimage to the other nodelets, as well as the `update_fulfill_htlc` to the counterparty of the channel.\n\n\nComparing Nodelets to Unpublished Channels and Channel Factories\n================================================================\n\nAs claimed earlier, nodelets combine the map size reduction of unpublished channels with the raw scaling of channel factories.\n\nChannel factories containing published channels, in particular, will increase the branching factor experienced by pathfinding algorithms.\nThus, we expect that the deployment of channel factories will increase the size of the public routemap.\n\nNodelets vs Unpublished Channels\n--------------------------------\n\nNodelets having share-ownership of a node leak part of their financial information to each other.\nSpecifically, the timing and amount of transactions is learned by the other nodelets.\n\nThis is similar to the situation where multiple nodes only have unpublished channels, and happen to make channels to the same public node.\nIn addition, the number of nodes seen on the public routemap is also the same.\n\nDifferences are:\n\n* With nodelets, there is no additional party that is not one of the unpublished nodes / nodelets.\n  * Nodelets have \"symmetrical\" information: their payment timings and amounts are known, but they also learn that of others.\n    * This symmetry might not be useful: \"the only winning move is not to play\" (i.e. do not make a transaction, just listen in on others).\n* With unpublished nodes, one of the unpublished nodes / nodelets going offline does not disable the public node.\n* Nodelets require less funds locked in channels, with fewer channels (and corresponding reduced blockchain footprint).\n  * Public node channels have to also exist separately from the funds of the unpublished nodes, in order to forward.\n  * With nodelets, the nodelet funds back the channels of the MuSig node directly.\n\nNodelets vs Channel Factories\n-----------------------------\n\nChannel factories and Nodelets have similar scaling improvement: they let 3 or more owners put funds in a single UTXO and still transact using the funds in that UTXO over the Lightning Network.\n\nDifferences are:\n\n* Nodelet-owned channels have a single `nSequence` constraint imposed by the Decker-Russell-Osuntokun mechanism, while channel factories have two `nSequence` constraints:\n  * At the channel factory layer.\n  * At the channel layer.\n    * This `nSequence` constraint could be removed by using Poon-Dryja update mechanism at the channel layer instead of another Decker-Russell-Osuntokun.\n* Channel factories have mildly more onchain footprint on uncooperative closes, due to the increased number of update mechanisms.\n* Channel factories increase the number of channels on the network and increase pathfinding effort, while nodelets reduce the number of nodes."
            }
        ],
        "thread_summary": {
            "title": "Nodelets, a layer 3 proposal",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "ZmnSCPxj"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 19004
        }
    },
    {
        "title": "[Lightning-dev] Rendez-vous on a Trampoline",
        "thread_messages": [
            {
                "author": "Bastien TEINTURIER",
                "date": "2019-10-22T12:01:56",
                "message_text_only": "Good morning everyone,\n\nSince I'm a one-trick pony, I'd like to talk to you about...guess what?\nTrampoline!\nIf you watched my talk at LNConf2019, I mentioned at the end that\nTrampoline enables high AMP very easily.\nEvery Trampoline node in the route may aggregate an incoming multi-part\npayment and then decide on how\nto split the outgoing aggregated payment. It looks like this:\n\n     .-------- 1mBTC --------.    .------- 2mBTC -------.\n    /                                    \\ /\n  \\\nAlice ----- 3mBTC ------> Ted ------ 4mBTC ----> Terry ----- 6mBTC ----> Bob\n   \\                                     /\n    `------- 2mBTC ----------'\n\nIn this example, Alice only has small-ish channels to Ted so she has to\nsplit in 3 parts. Ted has good outgoing\ncapacity to Terry so he's able to split in only two parts. And Terry has a\nbig channel to Bob so he doesn't need\nto split at all.\nThis is interesting because each intermediate Trampoline node has knowledge\nof his local channels balances,\nthus can make more informed decisions than Alice on how to efficiently\nsplit to reach the next node.\n\nBut it doesn't stop there. Trampoline also enables a better rendez-vous\nrouting than normal payments.\nChristian has done most of the hard work to figure out how we could do\nrendez-vous on top of Sphinx [1]\n(thanks Christian!), so I won't detail that here (but I do plan on\nsubmitting a detailed spec proposal with all\nthe crypto equations and nice diagrams someday, unless Christian does it\nfirst).\n\nOne of the issues with rendez-vous routing is that once Alice (the\nrecipient) has created her part of the onion,\nshe needs to communicate that to Bob (the sender). If we use a Bolt 11\ninvoice for that, it means we need to\nput 1366 additional bytes to the invoice (plus some additional information\nfor the ephemeral key switch).\nIf the amount Alice wants to receive is big and may require multi-part,\nAlice has to decide upfront on how to split\nand provide multiple pre-encrypted onions (so we need 1366 bytes *per\npartial payment*, which kinda sucks).\n\nBut guess what? Bitcoin Trampoline fixes that*\u2122*. Instead of doing the\npre-encryption on a normal onion, Alice\nwould do the pre-encryption on a Trampoline onion (which is much smaller,\nin my prototype it's 466 bytes).\nAnd that allows rendez-vous routing to benefit from Trampoline's ability to\ndo multi-part at each hop.\nObviously since the onion is smaller, that limits the number of trampoline\nhops that can be used, but don't\nforget that there are additional \"normal\" hops between each Trampoline node\n(and the final Trampoline spec\ncan choose the size of the Trampoline onion to enable a good enough\nrendez-vous).\n\nHere is what it would look like. Alice chooses to rendez-vous at Terry.\nAlice wants the payment to go through Terry\nand Teddy so she pre-encrypts a Trampoline onion with that route:\n\nAlice <--- Teddy <--- Terry\n\nShe creates a Bolt 11 invoice containing that pre-encrypted onion. Bob\npicks up that invoice and can either reach\nTerry directly (via a normal payment route) or via another Trampoline node\n(Toad?). Bob finalizes the encryption of\nthe Trampoline onion and sends it onward. Bob can use multi-part and split\nthe payment however he wishes,\nbecause every Trampoline node in the route will be free to aggregate and\nre-split differently.\nTerry is the only intermediate node to know that rendez-vous routing was\nused. Terry doesn't learn anything about\nAlice because the payment still needs to go through Teddy. Teddy only\nlearns that this is a Trampoline payment, so\nhe doesn't know his position in the Trampoline route (especially since he\ndoesn't know that rendez-vous was used).\n\nI believe this makes rendez-vous routing reasonable to implement: the\ntrade-offs aren't as strong as in the normal\npayment case. If I missed something (maybe other issues related to the\ncurrent rendez-vous proposal) please let me know.\n\nOf course Trampoline itself also has trade-offs that in some cases may\nimpact privacy (e.g. when paying to legacy nodes\nthat don't understand the Trampoline onion). This is why Eclair is\ncurrently implementing it to identify all the places where\nit falls short, so that we can then leverage the community's amazing brain\npower to converge on a spec that everyone is\nhappy with and that minimizes the trade-offs we need to make. Stay tuned\nfor more information and updates to the spec PR\nonce we make progress on our Trampoline experiments.\n\nThank you for reading this, don't hesitate to throw ideas and/or criticize\nthis proposal.\nNote that all the cryptographic details are left as an exercise to the\nreader.\n\nBastien\n\n[1]\nhttps://github.com/lightningnetwork/lightning-rfc/wiki/Rendez-vous-mechanism-on-top-of-Sphinx\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191022/9d0f01f4/attachment.html>"
            },
            {
                "author": "Corn\u00e9 Plooy",
                "date": "2019-10-25T07:14:22",
                "message_text_only": "Cool: non-source rendez-vous routing. Getting closer to 2013 Amiko Pay,\nwith the added experience of 2019 Lightning with Sphinx routing and AMP.\n\nhttps://cornwarecjp.github.io/amiko-pay/doc/amiko_draft_2.pdf\n\n(esp. section 2.1.3)\n\nPlease forgive the use of the term \"Ripple\". 2013 was a different time.\n\n\nCJP\n\n\nOn 22-10-19 14:01, Bastien TEINTURIER wrote:\n> Good morning everyone,\n>\n> Since I'm a one-trick pony, I'd like to talk to you about...guess\n> what? Trampoline!\n> If you watched my talk at LNConf2019, I mentioned at the end that\n> Trampoline enables high AMP very easily.\n> Every Trampoline node in the route may aggregate an incoming\n> multi-part payment and then decide on how\n> to split the outgoing aggregated payment. It looks like this:\n>\n> \u00a0 \u00a0 \u00a0.-------- 1mBTC --------.\u00a0 \u00a0 .------- 2mBTC -------.\n> \u00a0 \u00a0 /\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \\ /\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\n> \u00a0 \u00a0 \u00a0 \u00a0 \\\n> Alice ----- 3mBTC ------> Ted ------ 4mBTC ----> Terry ----- 6mBTC\n> ----> Bob\n> \u00a0 \u00a0\\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0/\n> \u00a0 \u00a0 `------- 2mBTC ----------'\n>\n> In this example, Alice only has small-ish channels to Ted so she has\n> to split in 3 parts. Ted has good outgoing\u00a0\n> capacity to Terry so he's able to split in only two parts. And Terry\n> has a big channel to Bob so he doesn't need\u00a0\n> to split at all.\n> This is interesting because each intermediate Trampoline node has\n> knowledge of his local channels balances,\n> thus can make more informed decisions than Alice on how to efficiently\n> split to reach the next node.\n>\n> But it doesn't stop there. Trampoline also enables a better\n> rendez-vous routing than normal payments.\n> Christian has done most of the hard work to figure out how we could do\n> rendez-vous on top of Sphinx [1]\n> (thanks Christian!), so I won't detail that here (but I do plan on\n> submitting a detailed spec proposal with all\u00a0\n> the crypto equations and nice diagrams someday, unless Christian does\n> it first).\n>\n> One of the issues with rendez-vous routing is that once Alice (the\n> recipient) has created her part of the onion,\n> she needs to communicate that to Bob (the sender). If we use a Bolt 11\n> invoice for that, it means we need to\n> put 1366 additional bytes to the invoice (plus some additional\n> information for the ephemeral key switch).\n> If the amount Alice wants to receive is big and may require\n> multi-part, Alice has to decide upfront on how to split\u00a0\n> and provide multiple pre-encrypted onions (so we need 1366 bytes /per\n> partial payment/, which kinda sucks).\n>\n> But guess what? Bitcoin\u00a0Trampoline fixes that*\u2122*. Instead of doing the\n> pre-encryption on a normal onion, Alice\n> would do the pre-encryption on a Trampoline onion (which is much\n> smaller, in my prototype it's 466 bytes).\n> And that allows rendez-vous routing to benefit from\n> Trampoline's\u00a0ability to do multi-part at each hop.\n> Obviously since the onion is smaller, that limits the number of\n> trampoline hops that can be used, but don't\n> forget that there are additional \"normal\" hops between each Trampoline\n> node (and the final Trampoline spec\u00a0\n> can choose the size of the Trampoline onion to enable a good enough\n> rendez-vous).\n>\n> Here is what it would look like. Alice chooses to rendez-vous at\n> Terry. Alice wants the payment to go through Terry\n> and Teddy so she pre-encrypts a Trampoline onion with that route:\n>\n> Alice\u00a0<--- Teddy <--- Terry\n>\n> She creates a Bolt 11 invoice containing that pre-encrypted onion. Bob\n> picks up that invoice and can either reach\u00a0\n> Terry directly (via a normal payment route) or via another Trampoline\n> node (Toad?). Bob finalizes the encryption of\u00a0\n> the Trampoline onion and sends it onward. Bob can use multi-part and\n> split the payment however he wishes,\u00a0\n> because every Trampoline node in the route will be free to aggregate\n> and re-split differently.\n> Terry is the only intermediate node to know that rendez-vous routing\n> was used. Terry doesn't learn anything about\u00a0\n> Alice because the payment still needs to go through Teddy. Teddy only\n> learns that this is a Trampoline payment, so\u00a0\n> he doesn't know his position in the Trampoline route (especially since\n> he doesn't know that rendez-vous was used).\n>\n> I believe this makes rendez-vous routing reasonable to implement: the\n> trade-offs aren't as strong as in the normal\n> payment case. If I missed something (maybe other issues related to the\n> current rendez-vous proposal) please let me know.\n>\n> Of course Trampoline itself also has trade-offs that in some cases may\n> impact privacy (e.g. when paying to legacy nodes\u00a0\n> that don't understand the Trampoline onion). This is why Eclair is\n> currently implementing it to identify all the places where\n> it falls short, so that we can then leverage the community's amazing\n> brain power to converge on a spec that everyone is\u00a0\n> happy with and that minimizes the trade-offs we need to make. Stay\n> tuned for more information and updates to the spec PR\u00a0\n> once we make progress on our Trampoline experiments.\n>\n> Thank you for reading this, don't hesitate to throw ideas and/or\n> criticize this proposal.\u00a0\n> Note that all the cryptographic details are left as an exercise to the\n> reader.\n>\n> Bastien\n>\n> [1]\u00a0https://github.com/lightningnetwork/lightning-rfc/wiki/Rendez-vous-mechanism-on-top-of-Sphinx\n>\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "Antoine Riard",
                "date": "2019-10-28T02:01:31",
                "message_text_only": "Hi,\n\nDesign reason of trampoline routing was to avoid lite nodes having\nto store the whole network graph and compute long-hop route. Trick\nlays in getting away from source-base routing, which has the nice\nproperty to hide hop position along the payment route (if we forget\npayment hash correleation), by enabling a mechanism for route\ncomputation delegation.\n\nThis delegation trades hardware requirements against privacy leaks\nand higher fees. And we also have now to re-design privacy mechanism\nto constitue an anonymous network on top of the network one. Rendez-vous\nis one of them, multipe-trampoline hops another one. We may want also to\nbe inspired by I2P and its concept of outbound/inbound tunnels, like payer\nconcatenating a second trampoline onion to the rendez-vous onion acquired\nfrom\nthe payee. Trick are known but hard and complex to get right in practice.\n\nThat's said, current trampoline proposal which enables legacy payee doxing\nwithout\nany opt-in from its side is a bit gross. Yes rendez-vous routing by\nreceiver solves\nit (beyond being cool in itself)! but stucks on the same requirement to\nupdate payee nodes.\nIf so, implementing trampoline routing on receiver could be easier and let\nit hide behind the\nfeature flag.\n\nIf Eclair go forward with trampoline, are you going to enforce that\ntrampoline\nrouting is only done with payee flagging support ?\n\nThat's a slowdown but if not people are going to be upset learning that a\nchunk of their\nincoming payment is potentially logged by some intermediate node.\n\nAlso, I'm a bit worried too on how AMP is going to interact with trampoline\nrouting.\nDepend on topology, but a naive implementation only using public channels\nand one-hop\ntrampoline node would let the trampoline learn who is the payer by doing\nintersection\nof the multiple payment paths.\n\nLong-term we may be pleased to have this flexible tools to enable wide-scale\nnetworking without assessing huge routing tables for everyone but I think we\nshould be really careful on how we design and deploy this stuff to avoid\nanother\nfalse promise of privacy like we have known on the base layer, e.g\nbloom-filters.\n\nAntoine\n\nLe ven. 25 oct. 2019 \u00e0 03:20, Corn\u00e9 Plooy via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Cool: non-source rendez-vous routing. Getting closer to 2013 Amiko Pay,\n> with the added experience of 2019 Lightning with Sphinx routing and AMP.\n>\n> https://cornwarecjp.github.io/amiko-pay/doc/amiko_draft_2.pdf\n>\n> (esp. section 2.1.3)\n>\n> Please forgive the use of the term \"Ripple\". 2013 was a different time.\n>\n>\n> CJP\n>\n>\n> On 22-10-19 14:01, Bastien TEINTURIER wrote:\n> > Good morning everyone,\n> >\n> > Since I'm a one-trick pony, I'd like to talk to you about...guess\n> > what? Trampoline!\n> > If you watched my talk at LNConf2019, I mentioned at the end that\n> > Trampoline enables high AMP very easily.\n> > Every Trampoline node in the route may aggregate an incoming\n> > multi-part payment and then decide on how\n> > to split the outgoing aggregated payment. It looks like this:\n> >\n> >      .-------- 1mBTC --------.    .------- 2mBTC -------.\n> >     /                                    \\ /\n> >         \\\n> > Alice ----- 3mBTC ------> Ted ------ 4mBTC ----> Terry ----- 6mBTC\n> > ----> Bob\n> >    \\                                     /\n> >     `------- 2mBTC ----------'\n> >\n> > In this example, Alice only has small-ish channels to Ted so she has\n> > to split in 3 parts. Ted has good outgoing\n> > capacity to Terry so he's able to split in only two parts. And Terry\n> > has a big channel to Bob so he doesn't need\n> > to split at all.\n> > This is interesting because each intermediate Trampoline node has\n> > knowledge of his local channels balances,\n> > thus can make more informed decisions than Alice on how to efficiently\n> > split to reach the next node.\n> >\n> > But it doesn't stop there. Trampoline also enables a better\n> > rendez-vous routing than normal payments.\n> > Christian has done most of the hard work to figure out how we could do\n> > rendez-vous on top of Sphinx [1]\n> > (thanks Christian!), so I won't detail that here (but I do plan on\n> > submitting a detailed spec proposal with all\n> > the crypto equations and nice diagrams someday, unless Christian does\n> > it first).\n> >\n> > One of the issues with rendez-vous routing is that once Alice (the\n> > recipient) has created her part of the onion,\n> > she needs to communicate that to Bob (the sender). If we use a Bolt 11\n> > invoice for that, it means we need to\n> > put 1366 additional bytes to the invoice (plus some additional\n> > information for the ephemeral key switch).\n> > If the amount Alice wants to receive is big and may require\n> > multi-part, Alice has to decide upfront on how to split\n> > and provide multiple pre-encrypted onions (so we need 1366 bytes /per\n> > partial payment/, which kinda sucks).\n> >\n> > But guess what? Bitcoin Trampoline fixes that*\u2122*. Instead of doing the\n> > pre-encryption on a normal onion, Alice\n> > would do the pre-encryption on a Trampoline onion (which is much\n> > smaller, in my prototype it's 466 bytes).\n> > And that allows rendez-vous routing to benefit from\n> > Trampoline's ability to do multi-part at each hop.\n> > Obviously since the onion is smaller, that limits the number of\n> > trampoline hops that can be used, but don't\n> > forget that there are additional \"normal\" hops between each Trampoline\n> > node (and the final Trampoline spec\n> > can choose the size of the Trampoline onion to enable a good enough\n> > rendez-vous).\n> >\n> > Here is what it would look like. Alice chooses to rendez-vous at\n> > Terry. Alice wants the payment to go through Terry\n> > and Teddy so she pre-encrypts a Trampoline onion with that route:\n> >\n> > Alice <--- Teddy <--- Terry\n> >\n> > She creates a Bolt 11 invoice containing that pre-encrypted onion. Bob\n> > picks up that invoice and can either reach\n> > Terry directly (via a normal payment route) or via another Trampoline\n> > node (Toad?). Bob finalizes the encryption of\n> > the Trampoline onion and sends it onward. Bob can use multi-part and\n> > split the payment however he wishes,\n> > because every Trampoline node in the route will be free to aggregate\n> > and re-split differently.\n> > Terry is the only intermediate node to know that rendez-vous routing\n> > was used. Terry doesn't learn anything about\n> > Alice because the payment still needs to go through Teddy. Teddy only\n> > learns that this is a Trampoline payment, so\n> > he doesn't know his position in the Trampoline route (especially since\n> > he doesn't know that rendez-vous was used).\n> >\n> > I believe this makes rendez-vous routing reasonable to implement: the\n> > trade-offs aren't as strong as in the normal\n> > payment case. If I missed something (maybe other issues related to the\n> > current rendez-vous proposal) please let me know.\n> >\n> > Of course Trampoline itself also has trade-offs that in some cases may\n> > impact privacy (e.g. when paying to legacy nodes\n> > that don't understand the Trampoline onion). This is why Eclair is\n> > currently implementing it to identify all the places where\n> > it falls short, so that we can then leverage the community's amazing\n> > brain power to converge on a spec that everyone is\n> > happy with and that minimizes the trade-offs we need to make. Stay\n> > tuned for more information and updates to the spec PR\n> > once we make progress on our Trampoline experiments.\n> >\n> > Thank you for reading this, don't hesitate to throw ideas and/or\n> > criticize this proposal.\n> > Note that all the cryptographic details are left as an exercise to the\n> > reader.\n> >\n> > Bastien\n> >\n> > [1]\n> https://github.com/lightningnetwork/lightning-rfc/wiki/Rendez-vous-mechanism-on-top-of-Sphinx\n> >\n> >\n> > _______________________________________________\n> > Lightning-dev mailing list\n> > Lightning-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191027/9f7446f3/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Rendez-vous on a Trampoline",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Bastien TEINTURIER",
                "Corn\u00e9 Plooy",
                "Antoine Riard"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 18737
        }
    },
    {
        "title": "[Lightning-dev] Sign up for Taproot BIP review by October 30",
        "thread_messages": [
            {
                "author": "Steve Lee",
                "date": "2019-10-23T17:44:49",
                "message_text_only": "Hello everyone,\n\nThe schnorr/taproot/tapscript BIPs are ready for review at this point, and\nwe want to get as much in-depth review from as broad a range of people as\nwe can before we go further on implementation/deployment. Reviewing the\nBIPs is hard in two ways: not many people are familiar with reviewing BIPs\nin the first place, and there are a lot of concepts involved in the three\nBIPs for people to get their heads around.\n\nThis is a proposal <https://github.com/ajtowns/taproot-review> for a\nstructured review period. The idea is that participants will be given some\nguidance/structure for going through the BIPs, and at the end should be\nable to either describe issues with the BIP drafts that warrant changes, or\nbe confident that they\u2019ve examined the proposals thoroughly enough to give\nan \u201cACK\u201d that the drafts should be formalised and move forwards into\nimplementation/deployment phases.\n\nBenefits of participating:\n* Deeply understand schnorr and taproot\n* Be a stakeholder in Bitcoin consensus development\n* Support/safeguard decentralisation of Bitcoin protocol development\n* Have fun!\n\nIf you are interested in participating, please sign up here\n<https://forms.gle/stnJvHQHREdkaxkS8 >.\n\nSpecial thanks to AJ Towns for doing most of the heavy lifting to get this\ngoing!\n\nSteve\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191023/e864c58e/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Sign up for Taproot BIP review by October 30",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Steve Lee"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1477
        }
    },
    {
        "title": "[Lightning-dev] CPFP Carve-Out for Fee-Prediction Issues in Contracting Applications (eg Lightning)",
        "thread_messages": [
            {
                "author": "Johan Tor\u00e5s Halseth",
                "date": "2019-10-24T13:49:09",
                "message_text_only": "Reviving this old thread now that the recently released RC for bitcoind\n0.19 includes the above mentioned carve-out rule.\n\nIn an attempt to pave the way for more robust CPFP of on-chain contracts\n(Lightning commitment transactions), the carve-out rule was added in\nhttps://github.com/bitcoin/bitcoin/pull/15681. However, having worked on an\nimplementation of a new commitment format for utilizing the Bring Your Own\nFees strategy using CPFP, I\u2019m wondering if the special case rule should\nhave been relaxed a bit, to avoid the need for adding a 1 CSV to all\noutputs (in case of Lightning this means HTLC scripts would need to be\nchanged to add the CSV delay).\n\nInstead, what about letting the rule be\n\nThe last transaction which is added to a package of dependent\ntransactions in the mempool must:\n  * Have no more than one unconfirmed parent.\n\nThis would of course allow adding a large transaction to each output of the\nunconfirmed parent, which in effect would allow an attacker to exceed the\nMAX_PACKAGE_VIRTUAL_SIZE limit in some cases. However, is this a problem\nwith the current mempool acceptance code in bitcoind? I would imagine\nevicting transactions based on feerate when the max mempool size is met\nhandles this, but I\u2019m asking since it seems like there has been several\nchanges to the acceptance code and eviction policy since the limit was\nfirst introduced.\n\n- Johan\n\n\nOn Wed, Feb 13, 2019 at 6:57 AM Rusty Russell <rusty at rustcorp.com.au> wrote:\n\n> Matt Corallo <lf-lists at mattcorallo.com> writes:\n> >>> Thus, even if you imagine a steady-state mempool growth, unless the\n> >>> \"near the top of the mempool\" criteria is \"near the top of the next\n> >>> block\" (which is obviously *not* incentive-compatible)\n> >>\n> >> I was defining \"top of mempool\" as \"in the first 4 MSipa\", ie. next\n> >> block, and assumed you'd only allow RBF if the old package wasn't in the\n> >> top and the replacement would be.  That seems incentive compatible; more\n> >> than the current scheme?\n> >\n> > My point was, because of block time variance, even that criteria doesn't\n> hold up. If you assume a steady flow of new transactions and one or two\n> blocks come in \"late\", suddenly \"top 4MWeight\" isn't likely to get\n> confirmed until a few blocks come in \"early\". Given block variance within a\n> 12 block window, this is a relatively likely scenario.\n>\n> [ Digging through old mail. ]\n>\n> Doesn't really matter.  Lightning close algorithm would be:\n>\n> 1.  Give bitcoind unileratal close.\n> 2.  Ask bitcoind what current expidited fee is (or survey your mempool).\n> 3.  Give bitcoind child \"push\" tx at that total feerate.\n> 4.  If next block doesn't contain unilateral close tx, goto 2.\n>\n> In this case, if you allow a simpified RBF where 'you can replace if\n> 1. feerate is higher, 2. new tx is in first 4Msipa of mempool, 3. old tx\n> isnt',\n> it works.\n>\n> It allows someone 100k of free tx spam, sure.  But it's simple.\n>\n> We could further restrict it by marking the unilateral close somehow to\n> say \"gonna be pushed\" and further limiting the child tx weight (say,\n> 5kSipa?) in that case.\n>\n> Cheers,\n> Rusty.\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191024/065e650f/attachment.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2019-10-24T21:25:14",
                "message_text_only": "I may be missing something, but I'm not sure how this changes anything?\n\nIf you have a commitment transaction, you always need at least, and\nexactly, one non-CSV output per party. The fact that there is a size\nlimitation on the transaction that spends for carve-out purposes only\neffects how many other inputs/outputs you can add, but somehow I doubt\nits ever going to be a large enough number to matter.\n\nMatt\n\nOn 10/24/19 1:49 PM, Johan Tor\u00e5s Halseth wrote:\n> Reviving this old thread now that the recently released RC for bitcoind\n> 0.19 includes the above mentioned carve-out rule.\n> \n> In an attempt to pave the way for more robust CPFP of on-chain contracts\n> (Lightning commitment transactions), the carve-out rule was added in\n> https://github.com/bitcoin/bitcoin/pull/15681. However, having worked on\n> an implementation of a new commitment format for utilizing the Bring\n> Your Own Fees strategy using CPFP, I\u2019m wondering if the special case\n> rule should have been relaxed a bit, to avoid the need for adding a 1\n> CSV to all outputs (in case of Lightning this means HTLC scripts would\n> need to be changed to add the CSV delay).\n> \n> Instead, what about letting the rule be\n> \n> The last transaction which is added to a package of dependent\n> transactions in the mempool must:\n> \u00a0 * Have no more than one unconfirmed parent.\n> \n> This would of course allow adding a large transaction to each output of\n> the unconfirmed parent, which in effect would allow an attacker to\n> exceed the MAX_PACKAGE_VIRTUAL_SIZE limit in some cases. However, is\n> this a problem with the current mempool acceptance code in bitcoind? I\n> would imagine evicting transactions based on feerate when the max\n> mempool size is met handles this, but I\u2019m asking since it seems like\n> there has been several changes to the acceptance code and eviction\n> policy since the limit was first introduced.\n> \n> - Johan\n> \n> \n> On Wed, Feb 13, 2019 at 6:57 AM Rusty Russell <rusty at rustcorp.com.au\n> <mailto:rusty at rustcorp.com.au>> wrote:\n> \n>     Matt Corallo <lf-lists at mattcorallo.com\n>     <mailto:lf-lists at mattcorallo.com>> writes:\n>     >>> Thus, even if you imagine a steady-state mempool growth, unless the\n>     >>> \"near the top of the mempool\" criteria is \"near the top of the next\n>     >>> block\" (which is obviously *not* incentive-compatible)\n>     >>\n>     >> I was defining \"top of mempool\" as \"in the first 4 MSipa\", ie. next\n>     >> block, and assumed you'd only allow RBF if the old package wasn't\n>     in the\n>     >> top and the replacement would be.\u00a0 That seems incentive\n>     compatible; more\n>     >> than the current scheme?\n>     >\n>     > My point was, because of block time variance, even that criteria\n>     doesn't hold up. If you assume a steady flow of new transactions and\n>     one or two blocks come in \"late\", suddenly \"top 4MWeight\" isn't\n>     likely to get confirmed until a few blocks come in \"early\". Given\n>     block variance within a 12 block window, this is a relatively likely\n>     scenario.\n> \n>     [ Digging through old mail. ]\n> \n>     Doesn't really matter.\u00a0 Lightning close algorithm would be:\n> \n>     1.\u00a0 Give bitcoind unileratal close.\n>     2.\u00a0 Ask bitcoind what current expidited fee is (or survey your mempool).\n>     3.\u00a0 Give bitcoind child \"push\" tx at that total feerate.\n>     4.\u00a0 If next block doesn't contain unilateral close tx, goto 2.\n> \n>     In this case, if you allow a simpified RBF where 'you can replace if\n>     1. feerate is higher, 2. new tx is in first 4Msipa of mempool, 3.\n>     old tx isnt',\n>     it works.\n> \n>     It allows someone 100k of free tx spam, sure.\u00a0 But it's simple.\n> \n>     We could further restrict it by marking the unilateral close somehow to\n>     say \"gonna be pushed\" and further limiting the child tx weight (say,\n>     5kSipa?) in that case.\n> \n>     Cheers,\n>     Rusty.\n>     _______________________________________________\n>     Lightning-dev mailing list\n>     Lightning-dev at lists.linuxfoundation.org\n>     <mailto:Lightning-dev at lists.linuxfoundation.org>\n>     https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>"
            },
            {
                "author": "Johan Tor\u00e5s Halseth",
                "date": "2019-10-25T07:05:15",
                "message_text_only": "It essentially changes the rule to always allow CPFP-ing the commitment as\nlong as there is an output available without any descendants. It changes\nthe commitment from \"you always need at least, and exactly, one non-CSV\noutput per party. \" to \"you always need at least one non-CSV output per\nparty. \"\n\nI realize these limits are there for a reason though, but I'm wondering if\ncould relax them. Also now that jeremyrubin has expressed problems with the\ncurrent mempool limits.\n\nOn Thu, Oct 24, 2019 at 11:25 PM Matt Corallo <lf-lists at mattcorallo.com>\nwrote:\n\n> I may be missing something, but I'm not sure how this changes anything?\n>\n> If you have a commitment transaction, you always need at least, and\n> exactly, one non-CSV output per party. The fact that there is a size\n> limitation on the transaction that spends for carve-out purposes only\n> effects how many other inputs/outputs you can add, but somehow I doubt\n> its ever going to be a large enough number to matter.\n>\n> Matt\n>\n> On 10/24/19 1:49 PM, Johan Tor\u00e5s Halseth wrote:\n> > Reviving this old thread now that the recently released RC for bitcoind\n> > 0.19 includes the above mentioned carve-out rule.\n> >\n> > In an attempt to pave the way for more robust CPFP of on-chain contracts\n> > (Lightning commitment transactions), the carve-out rule was added in\n> > https://github.com/bitcoin/bitcoin/pull/15681. However, having worked on\n> > an implementation of a new commitment format for utilizing the Bring\n> > Your Own Fees strategy using CPFP, I\u2019m wondering if the special case\n> > rule should have been relaxed a bit, to avoid the need for adding a 1\n> > CSV to all outputs (in case of Lightning this means HTLC scripts would\n> > need to be changed to add the CSV delay).\n> >\n> > Instead, what about letting the rule be\n> >\n> > The last transaction which is added to a package of dependent\n> > transactions in the mempool must:\n> >   * Have no more than one unconfirmed parent.\n> >\n> > This would of course allow adding a large transaction to each output of\n> > the unconfirmed parent, which in effect would allow an attacker to\n> > exceed the MAX_PACKAGE_VIRTUAL_SIZE limit in some cases. However, is\n> > this a problem with the current mempool acceptance code in bitcoind? I\n> > would imagine evicting transactions based on feerate when the max\n> > mempool size is met handles this, but I\u2019m asking since it seems like\n> > there has been several changes to the acceptance code and eviction\n> > policy since the limit was first introduced.\n> >\n> > - Johan\n> >\n> >\n> > On Wed, Feb 13, 2019 at 6:57 AM Rusty Russell <rusty at rustcorp.com.au\n> > <mailto:rusty at rustcorp.com.au>> wrote:\n> >\n> >     Matt Corallo <lf-lists at mattcorallo.com\n> >     <mailto:lf-lists at mattcorallo.com>> writes:\n> >     >>> Thus, even if you imagine a steady-state mempool growth, unless\n> the\n> >     >>> \"near the top of the mempool\" criteria is \"near the top of the\n> next\n> >     >>> block\" (which is obviously *not* incentive-compatible)\n> >     >>\n> >     >> I was defining \"top of mempool\" as \"in the first 4 MSipa\", ie.\n> next\n> >     >> block, and assumed you'd only allow RBF if the old package wasn't\n> >     in the\n> >     >> top and the replacement would be.  That seems incentive\n> >     compatible; more\n> >     >> than the current scheme?\n> >     >\n> >     > My point was, because of block time variance, even that criteria\n> >     doesn't hold up. If you assume a steady flow of new transactions and\n> >     one or two blocks come in \"late\", suddenly \"top 4MWeight\" isn't\n> >     likely to get confirmed until a few blocks come in \"early\". Given\n> >     block variance within a 12 block window, this is a relatively likely\n> >     scenario.\n> >\n> >     [ Digging through old mail. ]\n> >\n> >     Doesn't really matter.  Lightning close algorithm would be:\n> >\n> >     1.  Give bitcoind unileratal close.\n> >     2.  Ask bitcoind what current expidited fee is (or survey your\n> mempool).\n> >     3.  Give bitcoind child \"push\" tx at that total feerate.\n> >     4.  If next block doesn't contain unilateral close tx, goto 2.\n> >\n> >     In this case, if you allow a simpified RBF where 'you can replace if\n> >     1. feerate is higher, 2. new tx is in first 4Msipa of mempool, 3.\n> >     old tx isnt',\n> >     it works.\n> >\n> >     It allows someone 100k of free tx spam, sure.  But it's simple.\n> >\n> >     We could further restrict it by marking the unilateral close somehow\n> to\n> >     say \"gonna be pushed\" and further limiting the child tx weight (say,\n> >     5kSipa?) in that case.\n> >\n> >     Cheers,\n> >     Rusty.\n> >     _______________________________________________\n> >     Lightning-dev mailing list\n> >     Lightning-dev at lists.linuxfoundation.org\n> >     <mailto:Lightning-dev at lists.linuxfoundation.org>\n> >     https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n> >\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191025/58a3d7b8/attachment.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2019-10-25T17:30:41",
                "message_text_only": "I don\u2019te see how? Let\u2019s imagine Party A has two spendable outputs, now they stuff the package size on one of their spendable outlets until it is right at the limit, add one more on their other output (to meet the Carve-Out), and now Party B can\u2019t do anything.\n\n> On Oct 24, 2019, at 21:05, Johan Tor\u00e5s Halseth <johanth at gmail.com> wrote:\n> \n> \ufeff\n> It essentially changes the rule to always allow CPFP-ing the commitment as long as there is an output available without any descendants. It changes the commitment from \"you always need at least, and exactly, one non-CSV output per party. \" to \"you always need at least one non-CSV output per party. \"\n> \n> I realize these limits are there for a reason though, but I'm wondering if could relax them. Also now that jeremyrubin has expressed problems with the current mempool limits.\n> \n>> On Thu, Oct 24, 2019 at 11:25 PM Matt Corallo <lf-lists at mattcorallo.com> wrote:\n>> I may be missing something, but I'm not sure how this changes anything?\n>> \n>> If you have a commitment transaction, you always need at least, and\n>> exactly, one non-CSV output per party. The fact that there is a size\n>> limitation on the transaction that spends for carve-out purposes only\n>> effects how many other inputs/outputs you can add, but somehow I doubt\n>> its ever going to be a large enough number to matter.\n>> \n>> Matt\n>> \n>> On 10/24/19 1:49 PM, Johan Tor\u00e5s Halseth wrote:\n>> > Reviving this old thread now that the recently released RC for bitcoind\n>> > 0.19 includes the above mentioned carve-out rule.\n>> > \n>> > In an attempt to pave the way for more robust CPFP of on-chain contracts\n>> > (Lightning commitment transactions), the carve-out rule was added in\n>> > https://github.com/bitcoin/bitcoin/pull/15681. However, having worked on\n>> > an implementation of a new commitment format for utilizing the Bring\n>> > Your Own Fees strategy using CPFP, I\u2019m wondering if the special case\n>> > rule should have been relaxed a bit, to avoid the need for adding a 1\n>> > CSV to all outputs (in case of Lightning this means HTLC scripts would\n>> > need to be changed to add the CSV delay).\n>> > \n>> > Instead, what about letting the rule be\n>> > \n>> > The last transaction which is added to a package of dependent\n>> > transactions in the mempool must:\n>> >   * Have no more than one unconfirmed parent.\n>> > \n>> > This would of course allow adding a large transaction to each output of\n>> > the unconfirmed parent, which in effect would allow an attacker to\n>> > exceed the MAX_PACKAGE_VIRTUAL_SIZE limit in some cases. However, is\n>> > this a problem with the current mempool acceptance code in bitcoind? I\n>> > would imagine evicting transactions based on feerate when the max\n>> > mempool size is met handles this, but I\u2019m asking since it seems like\n>> > there has been several changes to the acceptance code and eviction\n>> > policy since the limit was first introduced.\n>> > \n>> > - Johan\n>> > \n>> > \n>> > On Wed, Feb 13, 2019 at 6:57 AM Rusty Russell <rusty at rustcorp.com.au\n>> > <mailto:rusty at rustcorp.com.au>> wrote:\n>> > \n>> >     Matt Corallo <lf-lists at mattcorallo.com\n>> >     <mailto:lf-lists at mattcorallo.com>> writes:\n>> >     >>> Thus, even if you imagine a steady-state mempool growth, unless the\n>> >     >>> \"near the top of the mempool\" criteria is \"near the top of the next\n>> >     >>> block\" (which is obviously *not* incentive-compatible)\n>> >     >>\n>> >     >> I was defining \"top of mempool\" as \"in the first 4 MSipa\", ie. next\n>> >     >> block, and assumed you'd only allow RBF if the old package wasn't\n>> >     in the\n>> >     >> top and the replacement would be.  That seems incentive\n>> >     compatible; more\n>> >     >> than the current scheme?\n>> >     >\n>> >     > My point was, because of block time variance, even that criteria\n>> >     doesn't hold up. If you assume a steady flow of new transactions and\n>> >     one or two blocks come in \"late\", suddenly \"top 4MWeight\" isn't\n>> >     likely to get confirmed until a few blocks come in \"early\". Given\n>> >     block variance within a 12 block window, this is a relatively likely\n>> >     scenario.\n>> > \n>> >     [ Digging through old mail. ]\n>> > \n>> >     Doesn't really matter.  Lightning close algorithm would be:\n>> > \n>> >     1.  Give bitcoind unileratal close.\n>> >     2.  Ask bitcoind what current expidited fee is (or survey your mempool).\n>> >     3.  Give bitcoind child \"push\" tx at that total feerate.\n>> >     4.  If next block doesn't contain unilateral close tx, goto 2.\n>> > \n>> >     In this case, if you allow a simpified RBF where 'you can replace if\n>> >     1. feerate is higher, 2. new tx is in first 4Msipa of mempool, 3.\n>> >     old tx isnt',\n>> >     it works.\n>> > \n>> >     It allows someone 100k of free tx spam, sure.  But it's simple.\n>> > \n>> >     We could further restrict it by marking the unilateral close somehow to\n>> >     say \"gonna be pushed\" and further limiting the child tx weight (say,\n>> >     5kSipa?) in that case.\n>> > \n>> >     Cheers,\n>> >     Rusty.\n>> >     _______________________________________________\n>> >     Lightning-dev mailing list\n>> >     Lightning-dev at lists.linuxfoundation.org\n>> >     <mailto:Lightning-dev at lists.linuxfoundation.org>\n>> >     https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>> > \n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191025/2221408c/attachment.html>"
            },
            {
                "author": "Jeremy",
                "date": "2019-10-27T19:13:09",
                "message_text_only": "Johan,\n\nThe issues with mempool limits for OP_SECURETHEBAG are related, but have\ndistinct solutions.\n\nThere are two main categories of mempool issues at stake. One is relay\ncost, the other is mempool walking.\n\nIn terms of relay cost, if an ancestor can be replaced, it will invalidate\nall it's children, meaning that no one paid for that broadcasting. This can\nbe fixed by appropriately assessing Replace By Fee update fees to\nencapsulate all descendants, but there are some tricky edge cases that make\nthis non-obvious to do.\n\nThe other issue is walking the mempool -- many of the algorithms we use in\nthe mempool can be N log N or N^2 in the number of descendants. (simple\nexample: an input chain of length N to a fan out of N outputs that are all\nspent, is O(N^2) to look up ancestors per-child, unless we're caching).\n\nThe other sort of walking issue is where the indegree or outdegree for a\ntransaction is high. Then when we are computing descendants or ancestors we\nwill need to visit it multiple times. To avoid re-expanding a node, we\ncurrently cache it with a set. This uses O(N) extra memory and makes O(N\nLog N) (we use std::set not unordered_set) comparisons.\n\nI just opened a PR which should help with some of the walking issues by\nallowing us to cheaply cache which nodes we've visited on a run. It makes a\nlot of previously O(N log N) stuff O(N) and doesn't allocate as much new\nmemory. See: https://github.com/bitcoin/bitcoin/pull/17268.\n\n\nNow, for OP_SECURETHEBAG we want a particular property that is very\ndifferent from with lightning htlcs (as is). We want that an unlimited\nnumber of child OP_SECURETHEBAG txns may extend from a confirmed\nOP_SECURETHEBAG, and then at the leaf nodes, we want the same rule as\nlightning (one dangling unconfirmed to permit channels).\n\nOP_SECURETHEBAG can help with the LN issue by putting all HTLCS into a tree\nwhere they are individualized leaf nodes with a preceding CSV. Then, the\nabove fix would ensure each HTLC always has time to close properly as they\nwould have individualized lockpoints. This is desirable for some additional\nreasons and not for others, but it should \"work\".\n\n\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n\n\nOn Fri, Oct 25, 2019 at 10:31 AM Matt Corallo <lf-lists at mattcorallo.com>\nwrote:\n\n> I don\u2019te see how? Let\u2019s imagine Party A has two spendable outputs, now\n> they stuff the package size on one of their spendable outlets until it is\n> right at the limit, add one more on their other output (to meet the\n> Carve-Out), and now Party B can\u2019t do anything.\n>\n> On Oct 24, 2019, at 21:05, Johan Tor\u00e5s Halseth <johanth at gmail.com> wrote:\n>\n> \ufeff\n> It essentially changes the rule to always allow CPFP-ing the commitment as\n> long as there is an output available without any descendants. It changes\n> the commitment from \"you always need at least, and exactly, one non-CSV\n> output per party. \" to \"you always need at least one non-CSV output per\n> party. \"\n>\n> I realize these limits are there for a reason though, but I'm wondering if\n> could relax them. Also now that jeremyrubin has expressed problems with the\n> current mempool limits.\n>\n> On Thu, Oct 24, 2019 at 11:25 PM Matt Corallo <lf-lists at mattcorallo.com>\n> wrote:\n>\n>> I may be missing something, but I'm not sure how this changes anything?\n>>\n>> If you have a commitment transaction, you always need at least, and\n>> exactly, one non-CSV output per party. The fact that there is a size\n>> limitation on the transaction that spends for carve-out purposes only\n>> effects how many other inputs/outputs you can add, but somehow I doubt\n>> its ever going to be a large enough number to matter.\n>>\n>> Matt\n>>\n>> On 10/24/19 1:49 PM, Johan Tor\u00e5s Halseth wrote:\n>> > Reviving this old thread now that the recently released RC for bitcoind\n>> > 0.19 includes the above mentioned carve-out rule.\n>> >\n>> > In an attempt to pave the way for more robust CPFP of on-chain contracts\n>> > (Lightning commitment transactions), the carve-out rule was added in\n>> > https://github.com/bitcoin/bitcoin/pull/15681. However, having worked\n>> on\n>> > an implementation of a new commitment format for utilizing the Bring\n>> > Your Own Fees strategy using CPFP, I\u2019m wondering if the special case\n>> > rule should have been relaxed a bit, to avoid the need for adding a 1\n>> > CSV to all outputs (in case of Lightning this means HTLC scripts would\n>> > need to be changed to add the CSV delay).\n>> >\n>> > Instead, what about letting the rule be\n>> >\n>> > The last transaction which is added to a package of dependent\n>> > transactions in the mempool must:\n>> >   * Have no more than one unconfirmed parent.\n>> >\n>> > This would of course allow adding a large transaction to each output of\n>> > the unconfirmed parent, which in effect would allow an attacker to\n>> > exceed the MAX_PACKAGE_VIRTUAL_SIZE limit in some cases. However, is\n>> > this a problem with the current mempool acceptance code in bitcoind? I\n>> > would imagine evicting transactions based on feerate when the max\n>> > mempool size is met handles this, but I\u2019m asking since it seems like\n>> > there has been several changes to the acceptance code and eviction\n>> > policy since the limit was first introduced.\n>> >\n>> > - Johan\n>> >\n>> >\n>> > On Wed, Feb 13, 2019 at 6:57 AM Rusty Russell <rusty at rustcorp.com.au\n>> > <mailto:rusty at rustcorp.com.au>> wrote:\n>> >\n>> >     Matt Corallo <lf-lists at mattcorallo.com\n>> >     <mailto:lf-lists at mattcorallo.com>> writes:\n>> >     >>> Thus, even if you imagine a steady-state mempool growth, unless\n>> the\n>> >     >>> \"near the top of the mempool\" criteria is \"near the top of the\n>> next\n>> >     >>> block\" (which is obviously *not* incentive-compatible)\n>> >     >>\n>> >     >> I was defining \"top of mempool\" as \"in the first 4 MSipa\", ie.\n>> next\n>> >     >> block, and assumed you'd only allow RBF if the old package wasn't\n>> >     in the\n>> >     >> top and the replacement would be.  That seems incentive\n>> >     compatible; more\n>> >     >> than the current scheme?\n>> >     >\n>> >     > My point was, because of block time variance, even that criteria\n>> >     doesn't hold up. If you assume a steady flow of new transactions and\n>> >     one or two blocks come in \"late\", suddenly \"top 4MWeight\" isn't\n>> >     likely to get confirmed until a few blocks come in \"early\". Given\n>> >     block variance within a 12 block window, this is a relatively likely\n>> >     scenario.\n>> >\n>> >     [ Digging through old mail. ]\n>> >\n>> >     Doesn't really matter.  Lightning close algorithm would be:\n>> >\n>> >     1.  Give bitcoind unileratal close.\n>> >     2.  Ask bitcoind what current expidited fee is (or survey your\n>> mempool).\n>> >     3.  Give bitcoind child \"push\" tx at that total feerate.\n>> >     4.  If next block doesn't contain unilateral close tx, goto 2.\n>> >\n>> >     In this case, if you allow a simpified RBF where 'you can replace if\n>> >     1. feerate is higher, 2. new tx is in first 4Msipa of mempool, 3.\n>> >     old tx isnt',\n>> >     it works.\n>> >\n>> >     It allows someone 100k of free tx spam, sure.  But it's simple.\n>> >\n>> >     We could further restrict it by marking the unilateral close\n>> somehow to\n>> >     say \"gonna be pushed\" and further limiting the child tx weight (say,\n>> >     5kSipa?) in that case.\n>> >\n>> >     Cheers,\n>> >     Rusty.\n>> >     _______________________________________________\n>> >     Lightning-dev mailing list\n>> >     Lightning-dev at lists.linuxfoundation.org\n>> >     <mailto:Lightning-dev at lists.linuxfoundation.org>\n>> >     https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>> >\n>>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191027/184af597/attachment.html>"
            },
            {
                "author": "Johan Tor\u00e5s Halseth",
                "date": "2019-10-28T09:45:39",
                "message_text_only": ">\n>\n> I don\u2019te see how? Let\u2019s imagine Party A has two spendable outputs, now\n> they stuff the package size on one of their spendable outlets until it is\n> right at the limit, add one more on their other output (to meet the\n> Carve-Out), and now Party B can\u2019t do anything.\n\nMatt: With the proposed change, party B would always be able to add a child\nto its output, regardless of what games party A is playing.\n\n\nThanks for the explanation, Jeremy!\n\n\n> In terms of relay cost, if an ancestor can be replaced, it will invalidate\n> all it's children, meaning that no one paid for that broadcasting. This can\n> be fixed by appropriately assessing Replace By Fee update fees to\n> encapsulate all descendants, but there are some tricky edge cases that make\n> this non-obvious to do.\n\n\nRelay cost is the obvious problem with just naively removing all limits.\nRelaxing the current rules by allowing to add a child to each output as\nlong as it has a single unconfirmed parent would still only allow free\nrelay of O(size of parent) extra data (which might not be that bad? Similar\nto the carve-out rule we could put limits on the child size). This would be\nenough for the current LN use case (increasing fee of commitment tx), but\nnot for OP_SECURETHEBAG I guess, as you need the tree of children, as you\nmention.\n\nI imagine walking the mempool wouldn't change much, as you would only have\none extra child per output. But here I'm just speculating, as I don't know\nthe code well enough know what the diff would look like.\n\n\n> OP_SECURETHEBAG can help with the LN issue by putting all HTLCS into a\n> tree where they are individualized leaf nodes with a preceding CSV. Then,\n> the above fix would ensure each HTLC always has time to close properly as\n> they would have individualized lockpoints. This is desirable for some\n> additional reasons and not for others, but it should \"work\".\n\n\nThis is interesting for an LN commitment! You could really hide every\noutput of the commitment within OP_STB, which could either allow bypassing\nthe fee-pinning attack entirely (if the output cannot be spent unconfirmed)\nor adding fees to the commitment using SIGHASH_SINGLE|ANYONECANPAY.\n\n- Johan\n\nOn Sun, Oct 27, 2019 at 8:13 PM Jeremy <jlrubin at mit.edu> wrote:\n\n> Johan,\n>\n> The issues with mempool limits for OP_SECURETHEBAG are related, but have\n> distinct solutions.\n>\n> There are two main categories of mempool issues at stake. One is relay\n> cost, the other is mempool walking.\n>\n> In terms of relay cost, if an ancestor can be replaced, it will invalidate\n> all it's children, meaning that no one paid for that broadcasting. This can\n> be fixed by appropriately assessing Replace By Fee update fees to\n> encapsulate all descendants, but there are some tricky edge cases that make\n> this non-obvious to do.\n>\n> The other issue is walking the mempool -- many of the algorithms we use in\n> the mempool can be N log N or N^2 in the number of descendants. (simple\n> example: an input chain of length N to a fan out of N outputs that are all\n> spent, is O(N^2) to look up ancestors per-child, unless we're caching).\n>\n> The other sort of walking issue is where the indegree or outdegree for a\n> transaction is high. Then when we are computing descendants or ancestors we\n> will need to visit it multiple times. To avoid re-expanding a node, we\n> currently cache it with a set. This uses O(N) extra memory and makes O(N\n> Log N) (we use std::set not unordered_set) comparisons.\n>\n> I just opened a PR which should help with some of the walking issues by\n> allowing us to cheaply cache which nodes we've visited on a run. It makes a\n> lot of previously O(N log N) stuff O(N) and doesn't allocate as much new\n> memory. See: https://github.com/bitcoin/bitcoin/pull/17268.\n>\n>\n> Now, for OP_SECURETHEBAG we want a particular property that is very\n> different from with lightning htlcs (as is). We want that an unlimited\n> number of child OP_SECURETHEBAG txns may extend from a confirmed\n> OP_SECURETHEBAG, and then at the leaf nodes, we want the same rule as\n> lightning (one dangling unconfirmed to permit channels).\n>\n> OP_SECURETHEBAG can help with the LN issue by putting all HTLCS into a\n> tree where they are individualized leaf nodes with a preceding CSV. Then,\n> the above fix would ensure each HTLC always has time to close properly as\n> they would have individualized lockpoints. This is desirable for some\n> additional reasons and not for others, but it should \"work\".\n>\n>\n>\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n> <https://twitter.com/JeremyRubin>\n>\n>\n> On Fri, Oct 25, 2019 at 10:31 AM Matt Corallo <lf-lists at mattcorallo.com>\n> wrote:\n>\n>> I don\u2019te see how? Let\u2019s imagine Party A has two spendable outputs, now\n>> they stuff the package size on one of their spendable outlets until it is\n>> right at the limit, add one more on their other output (to meet the\n>> Carve-Out), and now Party B can\u2019t do anything.\n>>\n>> On Oct 24, 2019, at 21:05, Johan Tor\u00e5s Halseth <johanth at gmail.com> wrote:\n>>\n>> \ufeff\n>> It essentially changes the rule to always allow CPFP-ing the commitment\n>> as long as there is an output available without any descendants. It changes\n>> the commitment from \"you always need at least, and exactly, one non-CSV\n>> output per party. \" to \"you always need at least one non-CSV output per\n>> party. \"\n>>\n>> I realize these limits are there for a reason though, but I'm wondering\n>> if could relax them. Also now that jeremyrubin has expressed problems with\n>> the current mempool limits.\n>>\n>> On Thu, Oct 24, 2019 at 11:25 PM Matt Corallo <lf-lists at mattcorallo.com>\n>> wrote:\n>>\n>>> I may be missing something, but I'm not sure how this changes anything?\n>>>\n>>> If you have a commitment transaction, you always need at least, and\n>>> exactly, one non-CSV output per party. The fact that there is a size\n>>> limitation on the transaction that spends for carve-out purposes only\n>>> effects how many other inputs/outputs you can add, but somehow I doubt\n>>> its ever going to be a large enough number to matter.\n>>>\n>>> Matt\n>>>\n>>> On 10/24/19 1:49 PM, Johan Tor\u00e5s Halseth wrote:\n>>> > Reviving this old thread now that the recently released RC for bitcoind\n>>> > 0.19 includes the above mentioned carve-out rule.\n>>> >\n>>> > In an attempt to pave the way for more robust CPFP of on-chain\n>>> contracts\n>>> > (Lightning commitment transactions), the carve-out rule was added in\n>>> > https://github.com/bitcoin/bitcoin/pull/15681. However, having worked\n>>> on\n>>> > an implementation of a new commitment format for utilizing the Bring\n>>> > Your Own Fees strategy using CPFP, I\u2019m wondering if the special case\n>>> > rule should have been relaxed a bit, to avoid the need for adding a 1\n>>> > CSV to all outputs (in case of Lightning this means HTLC scripts would\n>>> > need to be changed to add the CSV delay).\n>>> >\n>>> > Instead, what about letting the rule be\n>>> >\n>>> > The last transaction which is added to a package of dependent\n>>> > transactions in the mempool must:\n>>> >   * Have no more than one unconfirmed parent.\n>>> >\n>>> > This would of course allow adding a large transaction to each output of\n>>> > the unconfirmed parent, which in effect would allow an attacker to\n>>> > exceed the MAX_PACKAGE_VIRTUAL_SIZE limit in some cases. However, is\n>>> > this a problem with the current mempool acceptance code in bitcoind? I\n>>> > would imagine evicting transactions based on feerate when the max\n>>> > mempool size is met handles this, but I\u2019m asking since it seems like\n>>> > there has been several changes to the acceptance code and eviction\n>>> > policy since the limit was first introduced.\n>>> >\n>>> > - Johan\n>>> >\n>>> >\n>>> > On Wed, Feb 13, 2019 at 6:57 AM Rusty Russell <rusty at rustcorp.com.au\n>>> > <mailto:rusty at rustcorp.com.au>> wrote:\n>>> >\n>>> >     Matt Corallo <lf-lists at mattcorallo.com\n>>> >     <mailto:lf-lists at mattcorallo.com>> writes:\n>>> >     >>> Thus, even if you imagine a steady-state mempool growth,\n>>> unless the\n>>> >     >>> \"near the top of the mempool\" criteria is \"near the top of the\n>>> next\n>>> >     >>> block\" (which is obviously *not* incentive-compatible)\n>>> >     >>\n>>> >     >> I was defining \"top of mempool\" as \"in the first 4 MSipa\", ie.\n>>> next\n>>> >     >> block, and assumed you'd only allow RBF if the old package\n>>> wasn't\n>>> >     in the\n>>> >     >> top and the replacement would be.  That seems incentive\n>>> >     compatible; more\n>>> >     >> than the current scheme?\n>>> >     >\n>>> >     > My point was, because of block time variance, even that criteria\n>>> >     doesn't hold up. If you assume a steady flow of new transactions\n>>> and\n>>> >     one or two blocks come in \"late\", suddenly \"top 4MWeight\" isn't\n>>> >     likely to get confirmed until a few blocks come in \"early\". Given\n>>> >     block variance within a 12 block window, this is a relatively\n>>> likely\n>>> >     scenario.\n>>> >\n>>> >     [ Digging through old mail. ]\n>>> >\n>>> >     Doesn't really matter.  Lightning close algorithm would be:\n>>> >\n>>> >     1.  Give bitcoind unileratal close.\n>>> >     2.  Ask bitcoind what current expidited fee is (or survey your\n>>> mempool).\n>>> >     3.  Give bitcoind child \"push\" tx at that total feerate.\n>>> >     4.  If next block doesn't contain unilateral close tx, goto 2.\n>>> >\n>>> >     In this case, if you allow a simpified RBF where 'you can replace\n>>> if\n>>> >     1. feerate is higher, 2. new tx is in first 4Msipa of mempool, 3.\n>>> >     old tx isnt',\n>>> >     it works.\n>>> >\n>>> >     It allows someone 100k of free tx spam, sure.  But it's simple.\n>>> >\n>>> >     We could further restrict it by marking the unilateral close\n>>> somehow to\n>>> >     say \"gonna be pushed\" and further limiting the child tx weight\n>>> (say,\n>>> >     5kSipa?) in that case.\n>>> >\n>>> >     Cheers,\n>>> >     Rusty.\n>>> >     _______________________________________________\n>>> >     Lightning-dev mailing list\n>>> >     Lightning-dev at lists.linuxfoundation.org\n>>> >     <mailto:Lightning-dev at lists.linuxfoundation.org>\n>>> >     https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>> >\n>>>\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191028/92777d7b/attachment-0001.html>"
            },
            {
                "author": "David A. Harding",
                "date": "2019-10-28T17:14:38",
                "message_text_only": "On Mon, Oct 28, 2019 at 10:45:39AM +0100, Johan Tor\u00e5s Halseth wrote:\n> Relay cost is the obvious problem with just naively removing all limits.\n> Relaxing the current rules by allowing to add a child to each output as\n> long as it has a single unconfirmed parent would still only allow free\n> relay of O(size of parent) extra data (which might not be that bad? Similar\n> to the carve-out rule we could put limits on the child size). \n\nA parent transaction near the limit of 100,000 vbytes could have almost\n10,000 outputs paying OP_TRUE (10 vbytes per output).  If the children\nwere limited to 10,000 vbytes each (the current max carve-out size),\nthat allows relaying 100 mega-vbytes or nearly 400 MB data size (larger\nthan the default maximum mempool size in Bitcoin Core).\n\nAs Matt noted in discussion on #lightning-dev about this issue, it's\npossible to increase second-child carve-out to nth-child carve-out but\nwe'd need to be careful about choosing an appropriately low value for n.\n\nFor example, BOLT2 limits the number of HTLCs to 483 on each side of the\nchannel (so 966 + 2 outputs total), which means the worst case free\nrelay to support the current LN protocol would be approximately:\n\n    (100000 + 968 * 10000) * 4 = ~39 MB\n\nEven if the mempool was empty (as it sometimes is these days), it would\nonly cost an attacker about 1.5 BTC to fill it at the default minimum\nrelay feerate[1] so that they could execute this attack at the minimal\ncost per iteration of paying for a few hundred or a few thousand vbytes\nat slightly higher than the current mempool minimum fee.\n\nInstead, with the existing rules (including second-child carve-out),\nthey'd have to iterate (39 MB / 400 kB = ~100) times more often to\nachieve an equivalent waste of bandwidth, costing them proportionally\nmore in fees.\n\nSo, I think these rough numbers clearly back what Matt said about us\nbeing able to raise the limits a bit if we need to, but that we have to\nbe careful not to raise them so far that attackers can make it\nsignificantly more bandwidth expensive for people to run relaying full\nnodes.\n\n-Dave\n\n[1] Several developers are working on lowering the default minimum in\nBitcoin Core, which would of course make this attack proportionally\ncheaper."
            },
            {
                "author": "Johan Tor\u00e5s Halseth",
                "date": "2019-10-30T07:22:53",
                "message_text_only": "On Mon, Oct 28, 2019 at 6:16 PM David A. Harding <dave at dtrt.org> wrote:\n\n> A parent transaction near the limit of 100,000 vbytes could have almost\n> 10,000 outputs paying OP_TRUE (10 vbytes per output).  If the children\n> were limited to 10,000 vbytes each (the current max carve-out size),\n> that allows relaying 100 mega-vbytes or nearly 400 MB data size (larger\n> than the default maximum mempool size in Bitcoin Core).\n>\n\nThanks, Dave, I wasn't aware the limits would allow this many outputs. And\nas your calculation shows, this opens up the potential for free relay of\nlarge amounts of data.\n\nWe could start special casing to only allow this for \"LN commitment-like\"\ntransactions, but this would be application specific changes, and your\ncalculation shows that even with the BOLT2 numbers there still exists cases\nwith a large number of children.\n\nWe are moving forward with adding a 1 block delay to all outputs to utilize\nthe current carve-out rule, and the changes aren't that bad. See Joost's\npost in \"[PATCH] First draft of option_simplfied_commitment\"\n\n- Johan\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191030/bc2a90f3/attachment.html>"
            },
            {
                "author": "David A. Harding",
                "date": "2019-10-27T22:54:02",
                "message_text_only": "On Thu, Oct 24, 2019 at 03:49:09PM +0200, Johan Tor\u00e5s Halseth wrote:\n> [...] what about letting the rule be\n> \n> The last transaction which is added to a package of dependent\n> transactions in the mempool must:\n>   * Have no more than one unconfirmed parent.\n> [... subsequent email ...]\n> I realize these limits are there for a reason though, but I'm wondering if\n> we could relax them.\n\nJohan,\n\nI'm not sure any of the other replies to this thread addressed your\nrequest for a reason behind the limits related to your proposal, so I\nthought I'd point out that---subsequent to your posting here---a\ndocument[1] was added to the Bitcoin Core developer wiki that I think\ndescribes the risk of the approach you proposed:\n\n> Free relay attack:\n>\n> - Create a low feerate transaction T.\n>\n> - Send zillions of child transactions that are slightly higher feerate\n>   than T until mempool is full.\n>\n> - Create one small transaction with feerate just higher than T\u2019s, and\n>    watch T and all its children get evicted. Total fees in mempool drops\n>    dramatically!\n>\n> - Attacker just relayed (say) 300MB of data across the whole network\n>   but only pays small feerate on one small transaction.\n\nThe document goes on to describe at a high level how Bitcoin Core\nattempts to mitigate this problem as well as other ways it tries to\noptimize the mempool in order to maximize miner profit (and so ensure\nthat miners continue to use public transaction relay).\n\nI hope that's helpful to you and to others in both understanding the\ncurrent state and in thinking about ways in which it might be improved.\n\n-Dave\n\n[1] https://github.com/bitcoin-core/bitcoin-devwiki/wiki/Mempool-and-mining\n    Content adapted from slides by Suhas Daftuar, uploaded and formatted\n    by Gregory Sanders and Marco Falke."
            }
        ],
        "thread_summary": {
            "title": "CPFP Carve-Out for Fee-Prediction Issues in Contracting Applications (eg Lightning)",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Jeremy",
                "David A. Harding",
                "Matt Corallo",
                "Johan Tor\u00e5s Halseth"
            ],
            "messages_count": 9,
            "total_messages_chars_count": 42048
        }
    },
    {
        "title": "[Lightning-dev] Unpublished Channels, and a Proposal for Local Channel Publication",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-24T15:20:59",
                "message_text_only": "Good morning list,\n\nAs is my wont, I again send unnecessary unsolicited spam on this list.\n\nThe Axiom of Terminus\n=====================\n\nLet us consider the below trivial Lightning Network:\n\n    A <-> B <-> C <-> D\n\nThere are three channels:\n\n* AB\n* BC\n* CD\n\nNow, let us consider, what happens if one of these channels is unpublished but the remaining two are published, and whether A and D can send and receive payments back and forth.\n\nLet us consider in terms of the current available specs.\n\nAB Unpublished\n--------------\n\nSuppose AB is unpublished, but BC and CD are published.\n\nIn that case:\n\n* Payments from A to D work, by simply making the first channel the unpublished channel AB which is directly known by A.\n* Payments from D to A work, by A revealing the unpublished channel AB via the `r` field of the invoice sent to D.\n\nCD Unpublished\n--------------\n\nSuppose CD is unpublished, but AB and BC are published.\n\nIn that case:\n\n* Payments from A to D work, by D revealing the unpublished channel CD via the `r` field of the invoice sent to A.\n* Payments from D to A work, by simply making the first channel the unpublished channel CD which is directly known by D.\n\nBC Unpublished\n--------------\n\nSuppose BC is unpublished, but AB and CD are published.\n\nIn that case:\n\n* Payments from A to D are impossible.\n* Payments from D to A are impossible.\n\nThe reason for the impossibility (given the current state of the BOLT spec) is that neither A nor D have knowledge of the unpublished channel BC.\nThus as far as A and D are concerned, they live on separate islands of the Lightning Network, and the channel BC cannot be used to route, due to the fact it is unpublished.\n\nStatement of the Axiom of Terminus\n----------------------------------\n\n> A payment route may have an unpublished channel at one or both termini (start and/or end of route), but not in the middle of the route.\n\nThis so-called \"axium of terminus\" is an assumption that underlies payment analysis on the Lightning Network.\nThe basis for accepting this assumption is the logic shown above: BC cannot be used in forwarding a payment from A to D, and thus it cannot be the middle of the route.\nThus, unpublished channels can only be the start and/or end of a route.\n\nMore concretely, analysis of payments over the Lightning Network can be simplified by accepting the axiom of terminus as true:\n\n* If a node receives from any channel, a request to forward to an unpublished channel:\n  * The node knows that the peer with the unpublished channel is the ultimate receiver of the payment.\n* If a node receives from an unpublished channel, a request to forward to any channel:\n  * The node knows that the peer with the unpublished channel is the ultimate sender of the payment.\n    * This is a strong privacy loss: ultimate senders otherwise have strong privacy if they did not make the first hop via an unpublished channel, whereas ultimate receivers can already be guessed at by looking at the remaining timelock.\n\nThe worst case is the below:\n\n* If a node receives from an unpublished channel, a request to forward to an unpublished channel:\n  * The node knows who the ultimate sender and receiver are!\n\nLess powerfully, an entity that controls *multiple* nodes on the network, and which encourages other nodes to make unpublished channels with their nodes, can monitor the activities of those nodes on the Lightning Network:\n\n* If one of its nodes receives from an unpublished channel, a request to forward to any channel, and a short while later, another of its nodes receives from any channel, a similar smaller amount and cltv-delta, a request to forward to an unpublished channel:\n  * The entity has a good chance of correlating who the ultimate sender and receiver are!\n  * In the current network, without payment decorrelation, the same hash being used on both is definite proof that they are indeed the ultimate sender and receiver.\n    * Worse than with the lack of payment decorrelation, is that the entity now knows exactly who the ultimate payer and receiver are, and not have to guess that they are nearby their nodes (which would be the case if it sees the same hash, but the channels involved are all published channels).\n\nThus, it is not privacy-safe, under the current specs, to use unpublished channels, if there is the possibility of one or more entities controlling multiple nodes on the network that have unpublished channels.\n\nBreaking the Axiom of Terminus\n==============================\n\nThe axiom of terminus leads to a trivial way for entities controlling large amounts of Bitcoin (and thus capable of viably running multiple nodes) to closely monitor participants in the network, especially if they have significant numbers of unpublished channels.\n\nThus, we must have methods of breaking the axiom of terminus, in order to prevent analysis of payments on the Lightning Network.\n\nInvoice `r` Field\n-----------------\n\nThis is the sole existing way to break the axiom of terminus under the current spec.\n\nSuppose as in our example, that BC is unpublished but AB and CD are published.\n\nIf A ever needs to pay C, then C will reveal the existence of BC to A via the invoice `r` field.\nSubsequently, if A then needs to pay D, A now has enough information to route a payment to D via the BC channel.\n\nHowever, this is sadly insufficient.\nInvoice `r` fields only provide feerates in *one* direction.\nUnfortunately channels have different feerates in both directions.\nThus A cannot itself reveal the BC channel to D via its own invoice `r` field, since what A knows is the direction B-to-C of the BC channel, but what D needs to route to A, is the direction C-to-B of the BC channel.\n\nOf greater note is that nodes that have *only* unpublished channels, *cannot break the axiom of terminus at all*.\nThis is because such nodes can only provide their incoming feerates in invoice `r` fields.\nBut forwarding requires that an incoming followed by an outgoing channel direction, thus nodes with only unpublished channels will be unable to break the axiom of terminus even by invoice `r` fields.\n\nLocal Publication\n-----------------\n\nCompletely unpublished channels, as I have demonstrated, are simply bad for privacy, due to the axiom of terminus.\n\nHowever, it should be possible to locally publish such channels.\nI believe Rene Pickhardt has been planning to propose such a local publication of channels for the purpose of assisting JIT Routing.\n\nI propose then the below new messages:\n\n### `request_local_unpublished_channels`\n\n1. type: TBD odd (`request_local_unpublished_channels`)\n2. data:\n  * ['u16`: `max_local_unpublished_channels`]\n\nCauses the receiver of this message to reply with `respond_local_unpublished_channels`.\n\nThe sender:\n\n* MUST NOT send this message if it does not have a channel with the receiver.\n\nThe receiver:\n\n* MUST fail the connection if it receives this message when it does not have a channel with the sender.\n\n### `respond_local_unpublished_channels`\n\n1. type: TBD even (`respond_local_unpublished_channels`)\n2. data:\n  * [`u16` : `num_local_unpublished_channels`]\n  * [`num_local_unpublished_channels * u64` : `short_channel_ids`]\n  * [`num_local_unpublished_channels * u32` : `fee_base_satoshis_0`]\n  * [`num_local_unpublished_channels * u32` : `fee_proportional_millionths_0`]\n  * [`num_local_unpublished_channels * u32` : `cltv_delta_0`]\n  * [`num_local_unpublished_channels * u32` : `fee_base_satoshis_1`]\n  * [`num_local_unpublished_channels * u32` : `fee_proportional_millionths_1`]\n  * [`num_local_unpublished_channels * u32` : `cltv_delta_1`]\n  * [`num_local_unpublished_channels * 33` : `nodeids`]\n\nProvides data about local unpublished channels.\nThese are channels that are directly connected to the sender node, and where the counterparty is the corresponding index in `nodeids`.\n\nThe sender:\n\n* MUST NOT send this message unless the receiver already sent a `request_local_unpublished_channels`.\n* MUST NOT duplicate any peer node IDs.\n  * If the sender has more than one unpublished channel with a particular peer, it SHOULD select one channel as a representative of all the channels.\n* MUST NOT send more than `max_local_unpublished_channels` channels.\n* MUST NOT send a published channel.\n\nThe receiver:\n\n* if it receives this message unsolicited, MAY fail the connection.\n* if a node ID is duplicated, SHOULD fail the connection.\n* if more than `max_local_unpublished_channels` is contained, SHOULD fail the connection.\n* if it has seen a `channel_announcement` of a channel listed in this message, SHOULD fail the connection.\n\n### Operation and Usage\n\nA node might request its peers to provide information about local unpublished channels in order to augment its routemap for normal payment usages.\nThis allows the channels to break the axiom of terminus and let an unpublished channel be used in the middle of a routemap.\n\nHowever, because it can only ask locally, the axiom of terminus is simply deferred by one hop.\nNow, the unpublished channel may be at most one hop away from the ultimate source or ultimate receiver of the payment.\n\nFurther, JIT Routing might trigger this dynamically when it is unable to find a short rebalancing path.\nThis allows for more opportunities to rebalance.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-28T05:09:44",
                "message_text_only": "Good morning list,\n\nOf note, is that long ago Rusty proposed already a \"local\" channel announcement: https://lists.linuxfoundation.org/pipermail/lightning-dev/2018-November/001486.html\n\nBriefly, the above proposal goes this way:\n\n> BigBrother: Hi LittleNode!\n>\n> LittleNode: Hi BigBrother!\n>\n> BigBrother: I want to tell **all** my friends about our little unpublished channel, will you allow me?\n>\n> LittleNode: Sure!\n>\n> **LittleNode thinks he has now beaten the Axiom of Terminus.\n>\n> **BigBrother actually decides not to tell anyone about the unpublished channel.\n\nNote that LittleNode has no ability to actually tell anybody about its unpublished channel with BigBrother, even if it has a channel with a different node.\n\nWhat I am proposing is subtly different:\n\n> BigBrother: Hi LittleNode!\n>\n> LittleNode: Hi BigBrother!\n>\n> ThisIsTheCIA: Hi LittleNode!\n>\n> LittleNode: Hi ThisIsTheCIA!\n>\n> (whisper from BigBrother): LittleNode, can you tell me about any unpublished channels you have?\n>\n> (whisper to BigBrother): Sure, I have an unpublished channel with ThisIsTheCIA.\n>\n> (whisper from ThisIsTheCIA): LittleNode, can you tell me about any unpublished channels you have?\n>\n> (whisper to ThisIsTheCIA): Sure, I have an unpublished channel with BigBrother.\n>\n> ** LittleNode thinks he has now beaten the Axiom of Terminus.\n\nIn my proposal, BigBrother learns about the channel to ThisIsTheCIA, and vice versa, solely from the LittleNode.\nThus, even if LittleNode is completely unpublished (all channels it has are unpublished) it can leak information about its unpublished channels to other peers, and at least break a little the Axiom of Terminus.\n\nNow of course this brings up the concern that the history of LittleNode might be learned by looking up the short-channel-id on the blockchain.\nBut this can be mitigated:\n\n* The short-channel-id might be a completely invented one --- for example, a node might assign the blockheight 0 to unpublished channels and use the transaction index and output index as a single field to uniquely identify its unpublished channels.\n* LittleNode could CoinJoin its money before creating any channels, using JoinMarket or Wasabi.\n\n-----\n\nMore interesting if LittleNode has peers that area also completely unpublished.\nIn that case, it is also possible for LittleNode to leak its unpublished channels to those peers with unpublished channels.\nThen neither ThisIsTheCIA nor BigBrother can determine if a payment coming from LittleNode (or going into LittleNode) is really from (to) LittleNode or not.\n\n-----\n\nNow let us consider if LittleNode and its fellow unpublished nodes create their subnet of the Lightning Network via Channel Factory.\n\nIn that case, they can leak to each other as well, their unpublished channels to the public network.\n\nI would like to point out that such a subnetwork would behave very much like a node composed of nodelets: https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-October/002236.html\n\nThe tradeoffs are:\n\n* With channel factories you get:\n  * Graceful degradation.\n  * Mildly better privacy (not all your fellow nodes on the channel factory have to know about every payment you make, if the payment does not go through them on the subnetwork).\n* With nodelets you get:\n  * Reduced number of channels (channel capacity is not dedicated to some peer which itself might not have channel capacity to the public network).\n  * Reduced number of transactions in unilateral close case.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Unpublished Channels, and a Proposal for Local Channel Publication",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "ZmnSCPxj"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 12729
        }
    },
    {
        "title": "[Lightning-dev] Thoughts on CoinSwap privacy relative to Lightning",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-25T07:27:49",
                "message_text_only": "Good morning list,\n\nBefore, I already expressed the thought, that CoinSwap and related protocols, seem to suspiciously look like payment channels.\n\n* https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-April/016889.html\n* https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-April/016888.html\n\n> Any special contracts are hosted inside a temporary offchain cryptocurrency system (slightly like a Lightning channel), and are not exposed if the protocol runs to completion.\n\nLet me flesh out these thoughts further.\n\nOne way of implementing CoinSwap involves pre-creating a backout transaction that is simply a future-`nLockTime` transaction.\nThis is a \"backout\" that returns the funds to their provider in case the swap protocol aborts.\n\n* https://github.com/AdamISZ/CoinSwapCS/issues/53 - `nLockTime`-protected Backouts\n* https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-April/016888.html - \"pre-swap backout transactions\"\n\nAs it happens, it is useful to review Spillman channels.\n\n* At setup time, a future-`nLockTime` \"backout\" transaction is created and signed, which spends the funding transaction output.\n* The funding transaction is signed and broadcast and confirmed once the signature for the above backout has been exchange.\n* At each update, a transaction without a future `nLockTime` (0, or the current blockheight + 1) is created which moves more money to the receiver direction.\n\nIn fact, the backout transaction in my CoinSwap proposals is similar to the Spillman initial backout transaction.\nIf we squint, we can consider CoinSwap and related protocols to be implementable on top of temporary Spillman channels to execute the coin swap protocol.\n\nBut suppose we would like to consider the below surprising fact, which might not otherwise be obvious to everyone on this mailing list:\n\n* Lightning Network: *exists*.\n\nCoinSwapper: *uses Lightning Network*.\n\nWhy settle for limited-time unidirectional channels when you can have unlimited-lifetime bidirectional channels?\n\nSo let me then propose further, the implementation of a CoinSwap protocol over Lightning.\nLet us suppose we have a number of UTXOs, `U[0..n]`, that we wish to \"clean\".\n\nFirst, the ingredients needed:\n\n* A Lightning Network.\n* An offchain-to-onchain swap service.\n* (Optional) a preexisting Lightning Network node you control.\n\nThen, the ritual to concoct our CoinSwap:\n\n* Set up two nodes.\n  One can be temporary and the other permanent (i.e. the preexisting Lightning Network node you control), or both can be temporary.\n  * One will be the \"sender\", it must be temporary.\n    Set it up over Tor!\n  * Other will be \"receiver\", it may be permanent or temporary.\n* Get inbound liquidity on the \"receiver\" node.\n  * i.e. get at least one channel, let us call this in further discussion the \"buffer channel\".\n    Then use the offchain-to-onchain swap to acquire incoming liquidity.\n    You will need some clean coins for this.\n* On the \"sender\" node, create channels to arbitrary points on the network graph, using the `U[0..n]` UTXOs.\n* Send random amounts from the sender to the receiver node.\n* If the receiver incoming liquidity starts to run out, send out some amount via the offchain-to-onchain swap service.\n* Once the sender channels are exhausted, close them.\n  * Due to the 1% reserve requirement, this will leave a small amount owned by the sender node, and still dirty, which we can:\n    * If it is safe to merge them all, we can just try to put them into a new channel and repeat this again.\n    * Or send it to your favorite controversial charity, if it is safe to associate their history with that charity.\n    * Or just send to 1BitcoinEaterAddressDontSendf59kuE individually.\n    * Or just spend all of it on fees and make an `OP_RETURN` output taunting blockchain analysis -- this is at least \"nice\" in that it reduces the UTXO set.\n\nNow, let me introduce some issues that have been problematic for CoinSwap.\n\n* Same-amount correlation.\n  CoinSwap swaps the histories of two equal-valued coins.\n  However, the values are roughly equal, and thus two UTXOs of equal value that are created in the same block, and subsequently later spent in the same later block, are strong hints of CoinSwap usage.\n  * One of the things that have been proposed is to have multiple UTXOs be swapped for multiple UTXOs, forcing the solution of the subset-sum problem.\n    * But see this: https://github.com/AdamISZ/CoinSwapCS/issues/47#issuecomment-400854870\n      * TLDR: subset-sum is non-polynomial if you are looking for a subset of *any* size, but in practice this will be for just subsets with two or three members, which is doable in almost-polynomial time.\n* Server logs problem.\n  The general idea is that some passive \"server\" or \"maker\" waits for CoinSwap requests, then an active \"client\" or \"taker\" pays for CoinSwap operations.\n  * However, arbitrary servers may be run by chain analysis companies, and thus keep a log of such requests for later analysis.\n    * Thus, clients would have to make multiple swaps with distinct servers to reduce the chance that any single entity controls all of them and is able to reconstruct the history.\n\nThe proposed CoinSwap-over-Lightning helps with the above issues:\n\n* Same-amount correlation.\n  * If the user is patient enough, the time frames of when the channels are created, and later when the offchain-to-onchain swap provides clean coins, can be extended over multiple blocks, greatly increasing the scope of necessary onchain analysis.\n  * The buffer channel helps obscure how much value has been transferred from the dirty coins to clean coins.\n    * In particular, it easily allows the clean coins to be split differently, i.e. you can put dirty 4BTC, 2BTC, 3BTC coins and get back clean 2BTC, 2BTC, 2BTC, 2BTC, 1BTC coins.\n    * The cleaned coins need not appear on the same block --- value can be kept temporarily in the buffer channel (and is the advantage of the buffer channel).\n      * You could even spend some money over the Lightning Network using the buffer channel.\n    * Value can be left in the buffer channel if the receiver node is your permanent Lightning node, such that the onchain clean coins sum *less than* the input onchain dirty coins, meaning subset-sum is not even the problem to be solved anymore.\n* Server logs problem.\n  * The forwarding nodes between the sender and receiver node, and between the receiver node and the offchain-to-onchain swap, could be logging the activity of the buffer channel.\n    However, unless the sender node happens to connect directly to another node that is secretly controlled by your counterparty in the buffer channel, and you are not using unpublished channels, then you should be fine.\n    Having the sender node spread out its outgoing channels helps reduce the chance that all the nodes it connects are controlled by the same entity.\n    Having the sender node use published channels helps confuse payment analysis, since it is possible your sender node is doing a forward rather than being the source of funds.\n    (the fact that the published channel UTXO, which is traceable from your dirty coins, is publicly associated with the sender node, is immaterial --- the sender node is a temporary node that will be destroyed after the swap is done, after all)\n  * The offchain-to-onchain swap service could be logging the activity of moving from offchain to onchain funds.\n    In particular it knows a mapping between proof-of-payments and actual onchain coins released.\n    If the swap service is not the same entity as the buffer channel counterparty, then it is not possible to determine that the payment is arising from the receiving node buffer channel.\n    However, if the offchain-to-onchain swap is capable of attaching a persistent identity to a set of proofs-of-payment, then the offchain-to-onchain swap can still correlate the owner of the suppsedly-clean coins.\n    Obviously, it would be best to use a swap service that does not require a persistent identity.\n\nOf note, is that it is vital to use published channels here, in order to avoid the Axiom of Terminus: https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-October/002241.html\nThe buffer channel is vital to be published, and it is strongly recommended to have multiple buffer channels as well to various points of the network.\nIt is often best if the receiver node is a permanent node, that also performs at least some amount of forwarding in which to hide its traffic.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Thoughts on CoinSwap privacy relative to Lightning",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "ZmnSCPxj"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 8503
        }
    },
    {
        "title": "[Lightning-dev] Make Me An Offer: Next Level Invoicing",
        "thread_messages": [
            {
                "author": "Corn\u00e9 Plooy",
                "date": "2019-10-25T09:10:24",
                "message_text_only": "Rusty,\n\n\nAt the recently held Lightning Conference in Berlin you asked for people\nto send you use cases / feature requests for the next-generation\nLightning invoicing. I already did some work on this back in 2018; that\nwork was interrupted when I started working on other ideas. I believe\nCDecker is member of some W3C(?) commission on payments, so he may also\nhave ideas on this.\n\n\nMy earlier work focused on an interactive protocol for direct\ncommunication between payer and payee. This is an old sketch / WIP:\n\nhttps://github.com/bitonic-cjp/lightning-rfc/blob/payment-protocol/12-payment-protocol.md\n\nI know some people don't like such interaction, but it does make many\nthings easier to accomplish, e.g. rendez-vous routing and stuckless\npayments.\n\n\nFor now, without doing an actual technology proposal, I'd like to share\na way of thinking about payer/payee interaction.\n\n\nIn my view, a transaction is not a line, it's a circle. Bitcoins go from\npayer to payee over the Lightning network, and whatever\nassets/goods/services are paid for go from payee to payer - this closes\nthe circle. Transactions on all hops of the circle should be atomic.\nBetween Lightning hops this is realized with HTLCs. For the payer-payee\nhop this is generally not possible, as non-cryptographic things\ngenerally don't allow you to construct HTLCs on them. Invoices and\nproofs of payment act as cryptographic representatives of\nnon-cryptographic entities.\n\n\nIn a generalized view, the interaction between payer and payee can be\nseen as a virtual payment channel. Full refunds, partial refunds, update\nof what goods are to be delivered (which may depend on an additional\npayment or a partial refund), sending back delivered goods (with\nrefund), are all updates to this virtual channel. Desirable properties\nof such a channel:\n\n* The channel has a state, and that state can be updated.\n\n* A state update invalidates the previous state.\n\n* By default, a state update requires consent of both parties. We may\ninclude a feature where, in the previous state, one party gives consent\nto allow the other party to unilaterally perform certain types of state\nupdates.\n\n* A state update may require a payment of a certain amount in one\ndirection or in the opposite direction. The state update only becomes\nvalid in combination with the corresponding proof of payment.\n\n* The state specifies obligations of parties to each other (e.g. \"A\nshall return <delivered goods> back to B\").\n\n* A state can be \"final\" in the sense that there should be no more\nupdates in the future. This may not need to be enforced by software or\nby protocol: it can simply be by convention. If the \"final\" state\nrequires consent of both parties for an update, any participant can make\nit final by never signing an update, or even by throwing away the\nprivate key used for the channel.\n\n\nThe state updating mechanism and its properties require special\nattention. I think the purpose of the channel is to aid in conflict\nresolution - it should provide evidence of what is agreed between both\nparties. There is nothing in an old state that makes it inherently\ninvalid: in fact, at some point in the past it was valid. The only thing\nthat makes it invalid is the presence of a later version. To reveal the\ntruth in conflict resolution, it is necessary that\n\n(1) the latest state is revealed\n\n(2) it is clear that this latest state is more recent than all other\nstates that are revealed\n\nI hope that (1) is likely incentive-wise: typically, incentives of the\nparties are each other's opposite, so either one party has an interest\nin revealing an update, or the other party has. I haven't worked out\nthis thought very rigorously though.\n\nI think a bit of cryptography can help with (2). One idea is to include\nsequence numbers in states, another is to include the hash of the\nprevious state. Either way, the possibility remains (cryptographically,\nmaybe not incentive-wise) to mess up the evidence by \"forking\" the\nchannel: making multiple state updates that have the same parent state.\nThis may be resolved semantically, with rules like \"if A unilaterally\nmakes a fork, and both sides are revealed, B may choose which side of\nthe fork is to be considered valid\".\n\n\nSome advanced ideas\n\n* Contracts between three or more parties.\n\n* Merging and joining of contracts in state updates\n\n\nCJP"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-25T10:05:48",
                "message_text_only": "Good morning CJP,\n\n\n> In my view, a transaction is not a line, it's a circle.\n\nI strongly agree to this.\nIndeed, in a full-blown economy running completely on Lightning Network, we may very well \"never\" settle onchain, instead simply draining then refilling our channels as we swap so-caled \"real-world\" assets around --- the closing of the Lightning circle.\n\n\n> Bitcoins go from\n> payer to payee over the Lightning network, and whatever\n> assets/goods/services are paid for go from payee to payer - this closes\n> the circle. Transactions on all hops of the circle should be atomic.\n> Between Lightning hops this is realized with HTLCs. For the payer-payee\n> hop this is generally not possible, as non-cryptographic things\n> generally don't allow you to construct HTLCs on them.\n\nA sealed box that can only be opened by inputting the preimage of a hash could work: https://lists.linuxfoundation.org/pipermail/lightning-dev/2018-November/001688.html\n\nOf course, as a cross-asset swap, this is susceptible to the well-known premium-free American Call Option problem --- eh who cares, it is not as if the price of real-world goods changes that fast.\n\n> --->8---\n>\n>     The state updating mechanism and its properties require special\n>     attention. I think the purpose of the channel is to aid in conflict\n>     resolution - it should provide evidence of what is agreed between both\n>     parties. There is nothing in an old state that makes it inherently\n>     invalid: in fact, at some point in the past it was valid. The only thing\n>     that makes it invalid is the presence of a later version. To reveal the\n>     truth in conflict resolution, it is necessary that\n>\n>     (1) the latest state is revealed\n>\n>     (2) it is clear that this latest state is more recent than all other\n>     states that are revealed\n>\n>     I hope that (1) is likely incentive-wise: typically, incentives of the\n>     parties are each other's opposite, so either one party has an interest\n>     in revealing an update, or the other party has. I haven't worked out\n>     this thought very rigorously though.\n\nIf you allow a timeout to show a later state before finalizing the most recent published state, much like Decker-Russell-Osuntokun or Poon-Dryja unilateral closes, then I see no issue.\n\n>\n>     I think a bit of cryptography can help with (2). One idea is to include\n>     sequence numbers in states, another is to include the hash of the\n>     previous state. Either way, the possibility remains (cryptographically,\n>     maybe not incentive-wise) to mess up the evidence by \"forking\" the\n>     channel: making multiple state updates that have the same parent state.\n>     This may be resolved semantically, with rules like \"if A unilaterally\n>     makes a fork, and both sides are revealed, B may choose which side of\n>     the fork is to be considered valid\".\n\nIn this particular case, as long as the overall state is signed by both counterparties, then it is immaterial whether there is a cryptographic protection against forking or not.\nAs long as the other side refuses to fork when one side attempts it, then forking is not possible as it requires both counterparties to sign off on it.\n\nOf course, they can rewrite history if they should like to do so, but it would have to be done bilaterally, and if we use sequence numbers (which are probably what we should prefer rather than hash-of-previous, so that if the non-latest state is published, we can show the latest state without having to reveal all intervening states), then they would have to make artificially large sequence number.\n\n\n>\n>     Some advanced ideas\n>\n> -   Contracts between three or more parties.\n> -   Merging and joining of contracts in state updates\n>\n\nMerging and joining is difficult.\nConsider the difficulties we have been having in designing a good splice-in facility, while a splice-out facility can be done much more easily.\nSince the real world is subservient to the Bitcoin world, we should probably expect to find similar difficulties with merging and joining of contracts.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Make Me An Offer: Next Level Invoicing",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Corn\u00e9 Plooy",
                "ZmnSCPxj"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 8400
        }
    },
    {
        "title": "[Lightning-dev] [PATCH] First draft of option_simplfied_commitment",
        "thread_messages": [
            {
                "author": "Joost Jager",
                "date": "2019-10-26T07:19:43",
                "message_text_only": "We started to look at the `push_me` outputs again. Will refer to them as\n`anchor` outputs from now on, to prevent confusion with `push_msat` on the\n`open_channel` message.\n\nThe cpfp carve-out https://github.com/bitcoin/bitcoin/pull/15681 has been\nmerged and for reasons described earlier in this thread, we now need to add\na csv time lock to every non-anchor output on the commitment transaction.\n\nTo realize this, we are currently considering the following changes:\n\n* Add `to_remote_delay OP_CHECKSEQUENCEVERIFY OP_DROP` to the `to_remote`\noutput. `to_remote_delay` is the csv delay that the remote party accepted\nin the funding flow for their outputs. This not only ensures that the\ncarve-out works as intended, but also removes the incentive to game the\nother party into force-closing. If desired, both parties can still agree to\nhave different `to_self_delay` values.\n\n* Add `1 OP_CHECKSEQUENCEVERIFY OP_DROP` to the non-revocation clause of\nthe HTLC outputs.\n\nFor the anchor outputs we consider:\n\n* Output type: normal P2WKH. At one point, an additional spending path was\nproposed that was unconditional except for a 10 block csv lock. The\nintention of this was to prevent utxo set pollution by allowing anyone to\nclean up. This however also opens up the possibility for an attacker to\n'use up' the cpfp carve-out after those 10 blocks. If the user A is offline\nfor that period of time, a malicious peer B may already have broadcasted\nthe commitment tx and pinned down user A's anchor output with a low fee\nchild. That way, the commitment tx could still remain unconfirmed while an\nimportant htlc expires.\n\n* For the keys to use for `to_remote_anchor` and `to_local_anchor`, we\u2019d\nlike to introduce new addresses that both parties communicate in the\n`open_channel` and `accept_channel` messages. We don\u2019t want to reuse the\nmain commitment output addresses, because those may (at some point) be cold\nstorage addresses and the cpfp is likely to happen from a hot wallet.\n\n* Within each version of the commitment transaction, both anchors always\nhave equal values and are paid for by the initiator. The value of the\nanchors is the dust limit that was negotiated in the `open_channel` or\n`accept_channel` message of the party that publishes the transaction. It\nmeans that the definitive balance of an endpoint is dependent on which\nversion of the commitment transaction confirms. This however is nothing\nnew. In the current commitment format, there are always two or three valid\nversions of the commitment transaction (local, remote and sometimes the not\nyet revoked previous remote tx) which can have slightly different balances.\nFor the initiator, it is important to validate the other party's dust\nlimit. The initiator pays for it and doesn't want to give away more free\nmoney than necessary.\n\nFurthermore, there doesn\u2019t seem to be a compelling reason anymore for\ntweaking the keys (new insights into watchtower designs, encrypt by txid).\nTherefore we think we can remove them entirely in this new commitment\nformat and require less channel state data to sweep the outputs.\n\nJoost\n\n\nOn Wed, Nov 21, 2018 at 3:17 AM Rusty Russell <rusty at rustcorp.com.au> wrote:\n\n> I'm also starting to implement this, to see what I missed!\n>\n> Original at https://github.com/lightningnetwork/lightning-rfc/pull/513\n>\n> Pasted here for your reading convenience:\n>\n> - Option is sticky; it set at open time, it stays with channel\n>   - I didn't want to have to handle penalty txs on channels which switch\n>   - We could, however, upgrade on splice.\n> - Feerate is fixed at 253\n>   - `feerate_per_kw` is still in open /accept (just ignored): multifund\n> may want it.\n> - closing tx negotiates *upwards* not *downwards*\n>   - Starting from base fee of commitment tx = 282 satoshi.\n> - to_remote output is always CSV delayed.\n> - pushme outputs are paid for by funder, but only exist if the matching\n>   to_local/remote output exists.\n> - After 10 blocks, they become anyone-can-spend (they need to see the\n>   to-local/remote witness script though).\n> - remotepubkey is not rotated.\n> - You must spend your pushme output; you may sweep for others.\n>\n> Signed-off-by: Rusty Russell <rusty at rustcorp.com.au>\n>\n> diff --git a/02-peer-protocol.md b/02-peer-protocol.md\n> index 7cf9ebf..6ec1155 100644\n> --- a/02-peer-protocol.md\n> +++ b/02-peer-protocol.md\n> @@ -133,7 +133,9 @@ node can offer.\n>  (i.e. 1/4 the more normally-used 'satoshi per 1000 vbytes') that this\n>  side will pay for commitment and HTLC transactions, as described in\n>  [BOLT #3](03-transactions.md#fee-calculation) (this can be adjusted\n> -later with an `update_fee` message).\n> +later with an `update_fee` message).  Note that if\n> +`option_simplified_commitment` is negotiated, this `feerate_per_kw`\n> +is treated as 253 for all transactions.\n>\n>  `to_self_delay` is the number of blocks that the other node's to-self\n>  outputs must be delayed, using `OP_CHECKSEQUENCEVERIFY` delays; this\n> @@ -208,7 +210,8 @@ The receiving node MUST fail the channel if:\n>    - `push_msat` is greater than `funding_satoshis` * 1000.\n>    - `to_self_delay` is unreasonably large.\n>    - `max_accepted_htlcs` is greater than 483.\n> -  - it considers `feerate_per_kw` too small for timely processing or\n> unreasonably large.\n> +  - if `option_simplified_commitment` is not negotiated:\n> +    - it considers `feerate_per_kw` too small for timely processing or\n> unreasonably large.\n>    - `funding_pubkey`, `revocation_basepoint`, `htlc_basepoint`,\n> `payment_basepoint`, or `delayed_payment_basepoint`\n>  are not valid DER-encoded compressed secp256k1 pubkeys.\n>    - `dust_limit_satoshis` is greater than `channel_reserve_satoshis`.\n> @@ -228,7 +231,7 @@ The *channel reserve* is specified by the peer's\n> `channel_reserve_satoshis`: 1%\n>\n>  The sender can unconditionally give initial funds to the receiver using a\n> non-zero `push_msat`, but even in this case we ensure that the funder has\n> sufficient remaining funds to pay fees and that one side has some amount it\n> can spend (which also implies there is at least one non-dust output). Note\n> that, like any other on-chain transaction, this payment is not certain\n> until the funding transaction has been confirmed sufficiently (with a\n> danger of double-spend until this occurs) and may require a separate method\n> to prove payment via on-chain confirmation.\n>\n> -The `feerate_per_kw` is generally only of concern to the sender (who pays\n> the fees), but there is also the fee rate paid by HTLC transactions; thus,\n> unreasonably large fee rates can also penalize the recipient.\n> +The `feerate_per_kw` is generally only of concern to the sender (who pays\n> the fees), but there is also the fee rate paid by HTLC transactions; thus,\n> unreasonably large fee rates can also penalize the recipient.  It is\n> ignored for `option_simplified_commitment`.\n>\n>  Separating the `htlc_basepoint` from the `payment_basepoint` improves\n> security: a node needs the secret associated with the `htlc_basepoint` to\n> produce HTLC signatures for the protocol, but the secret for the\n> `payment_basepoint` can be in cold storage.\n>\n> @@ -340,6 +343,12 @@ This message introduces the `channel_id` to identify\n> the channel. It's derived f\n>\n>  #### Requirements\n>\n> +Both peers:\n> +  - if `option_simplified_commitment` was negotiated:\n> +    - `option_simplified_commitment` applies to all commitment and HTLC\n> transactions\n> +  - otherwise:\n> +    - `option_simplified_commitment` does not apply to any commitment or\n> HTLC transactions\n> +\n>  The sender MUST set:\n>    - `channel_id` by exclusive-OR of the `funding_txid` and the\n> `funding_output_index` from the `funding_created` message.\n>    - `signature` to the valid signature, using its `funding_pubkey` for\n> the initial commitment transaction, as defined in [BOLT\n> #3](03-transactions.md#commitment-transaction).\n> @@ -351,6 +360,12 @@ The recipient:\n>    - on receipt of a valid `funding_signed`:\n>      - SHOULD broadcast the funding transaction.\n>\n> +#### Rationale\n> +\n> +We decide on `option_simplified_commitment` at this point when we first\n> have to generate the commitment\n> +transaction.  Even if a later reconnection does not negotiate this\n> parameter, this channel will honor it.\n> +This simplifies channel state, particularly penalty transaction handling.\n> +\n>  ### The `funding_locked` Message\n>\n>  This message indicates that the funding transaction has reached the\n> `minimum_depth` asked for in `accept_channel`. Once both nodes have sent\n> this, the channel enters normal operating mode.\n> @@ -508,8 +523,11 @@ The funding node:\n>      - SHOULD send a `closing_signed` message.\n>\n>  The sending node:\n> -  - MUST set `fee_satoshis` less than or equal to the\n> - base fee of the final commitment transaction, as calculated in [BOLT\n> #3](03-transactions.md#fee-calculation).\n> +  - if `option_upfront_shutdown_script` applies to the final commitment\n> transaction:\n> +    - MUST set `fee_satoshis` greater than or equal to 282.\n> +  - otherwise:\n> +    - MUST set `fee_satoshis` less than or equal to the\n> +      base fee of the final commitment transaction, as calculated in\n> [BOLT #3](03-transactions.md#fee-calculation).\n>    - SHOULD set the initial `fee_satoshis` according to its\n>   estimate of cost of inclusion in a block.\n>    - MUST set `signature` to the Bitcoin signature of the close\n> @@ -543,9 +561,18 @@ progress is made, even if only by a single satoshi at\n> a time. To avoid\n>  keeping state and to handle the corner case, where fees have shifted\n>  between disconnection and reconnection, negotiation restarts on\n> reconnection.\n>\n> -Note there is limited risk if the closing transaction is\n> -delayed, but it will be broadcast very soon; so there is usually no\n> -reason to pay a premium for rapid processing.\n> +In the `option_simplified_commitment` case, the fees on the commitment\n> +transaction itself are minimal (it is assumed that a child transaction\n> will\n> +supply additional fee incentive), so that forms a floor for negotiation.\n> +[BOLT #3](03-transactions.md#fee-calculation), gives 282 satoshis (1116\n> +weight, 254 `feerate_per_kw`).\n> +\n> +Otherwise, the commitment transaction usually pays a premium fee, so that\n> +forms a ceiling.\n> +\n> +Note there is limited risk if the closing transaction is delayed, but it\n> will\n> +be broadcast very soon; so there is usually no reason to pay a premium for\n> +rapid processing.\n>\n>  ## Normal Operation\n>\n> @@ -763,7 +790,10 @@ is destined, is described in [BOLT\n> #4](04-onion-routing.md).\n>  A sending node:\n>    - MUST NOT offer `amount_msat` it cannot pay for in the\n>  remote commitment transaction at the current `feerate_per_kw` (see\n> \"Updating\n> -Fees\") while maintaining its channel reserve.\n> +Fees\") while maintaining its channel reserve\n> +  - if `option_simplified_commitment` applies to this commitment\n> transaction and the sending\n> +    node is the funder:\n> +    - MUST be able to additionally pay for `to_local_pushme` and\n> `to_remote_pushme` above its reserve.\n>    - MUST offer `amount_msat` greater than 0.\n>    - MUST NOT offer `amount_msat` below the receiving node's\n> `htlc_minimum_msat`\n>    - MUST set `cltv_expiry` less than 500000000.\n> @@ -782,7 +812,7 @@ Fees\") while maintaining its channel reserve.\n>  A receiving node:\n>    - receiving an `amount_msat` equal to 0, OR less than its own\n> `htlc_minimum_msat`:\n>      - SHOULD fail the channel.\n> -  - receiving an `amount_msat` that the sending node cannot afford at the\n> current `feerate_per_kw` (while maintaining its channel reserve):\n> +  - receiving an `amount_msat` that the sending node cannot afford at the\n> current `feerate_per_kw` (while maintaining its channel reserve and any\n> `to_local_pushme` and `to_remote_pushme` fees):\n>      - SHOULD fail the channel.\n>    - if a sending node adds more than its `max_accepted_htlcs` HTLCs to\n>      its local commitment transaction, OR adds more than its\n> `max_htlc_value_in_flight_msat` worth of offered HTLCs to its local\n> commitment transaction:\n> @@ -997,6 +1027,11 @@ A node:\n>\n>  ### Updating Fees: `update_fee`\n>\n> +If `option_simplified_commitment` applies to the commitment transaction,\n> +`update_fee` is never used: the `feerate_per_kw` is always considered\n> 253, but\n> +the funder also pays 2000 satoshi for the `to_local_pushme` and\n> +`to_remote_pushme` outputs.\n> +\n>  An `update_fee` message is sent by the node which is paying the\n>  Bitcoin fee. Like any update, it's first committed to the receiver's\n>  commitment transaction and then (once acknowledged) committed to the\n> @@ -1020,13 +1055,19 @@ given in [BOLT\n> #3](03-transactions.md#fee-calculation).\n>  #### Requirements\n>\n>  The node _responsible_ for paying the Bitcoin fee:\n> -  - SHOULD send `update_fee` to ensure the current fee rate is sufficient\n> (by a\n> +  - if `option_simplified_commitment` applies to the commitment\n> transaction:\n> +    - MUST NOT send `update_fee`.\n> +  - otherwise:\n> +    - SHOULD send `update_fee` to ensure the current fee rate is\n> sufficient (by a\n>        significant margin) for timely processing of the commitment\n> transaction.\n>\n>  The node _not responsible_ for paying the Bitcoin fee:\n>    - MUST NOT send `update_fee`.\n>\n>  A receiving node:\n> +  - if `option_simplified_commitment` applies to the commitment\n> transaction:\n> +    - SHOULD fail the channel.\n> +       - MUST NOT update the `feerate_per_kw`.\n>    - if the `update_fee` is too low for timely processing, OR is\n> unreasonably large:\n>      - SHOULD fail the channel.\n>    - if the sender is not responsible for paying the Bitcoin fee:\n> @@ -1038,7 +1079,12 @@ A receiving node:\n>\n>  #### Rationale\n>\n> -Bitcoin fees are required for unilateral closes to be effective \u2014\n> +Fee adjustments are unnecessary for `option_simplified_commitment` which\n> +relies on \"pushme\" outputs and a child transaction which will provide\n> +additional fee incentive which can be calculated at the time it is spent,\n> and\n> +replaced by higher-fee children if required.\n> +\n> +Without this option, bitcoin fees are required for unilateral closes to\n> be effective \u2014\n>  particularly since there is no general method for the broadcasting node\n> to use\n>  child-pays-for-parent to increase its effective fee.\n>\n> diff --git a/03-transactions.md b/03-transactions.md\n> index e769961..440bd0d 100644\n> --- a/03-transactions.md\n> +++ b/03-transactions.md\n> @@ -82,6 +82,8 @@ To allow an opportunity for penalty transactions, in\n> case of a revoked commitmen\n>  The reason for the separate transaction stage for HTLC outputs is so that\n> HTLCs can timeout or be fulfilled even though they are within the\n> `to_self_delay` delay.\n>  Otherwise, the required minimum timeout on HTLCs is lengthened by this\n> delay, causing longer timeouts for HTLCs traversing the network.\n>\n> +If `option_simplified_commitment` applies to the commitment transaction,\n> then the `to_self_delay` used for all transactions is the greater of the\n> `to_self_delay` sent by each peer.  Otherwise, each peer sends the\n> `to_self_delay` to be used for the other peer's commitment amd HTLC\n> transactions.\n> +\n>  The amounts for each output MUST be rounded down to whole satoshis. If\n> this amount, minus the fees for the HTLC transaction, is less than the\n> `dust_limit_satoshis` set by the owner of the commitment transaction, the\n> output MUST NOT be produced (thus the funds add to fees).\n>\n>  #### `to_local` Output\n> @@ -109,7 +111,40 @@ If a revoked commitment transaction is published, the\n> other party can spend this\n>\n>  #### `to_remote` Output\n>\n> -This output sends funds to the other peer and thus is a simple P2WPKH to\n> `remotepubkey`.\n> +This output sends funds to the other peer, thus is not encumbered by a\n> +revocation private key.\n> +\n> +If `option_simplified_commitment` applies to the commitment transaction,\n> the `to_remote` output is delayed similarly to the `to_local` output, and\n> is to a fixed key:\n> +\n> +        `to_self_delay`\n> +        OP_CSV\n> +        OP_DROP\n> +        <remote_pubkey>\n> +\n> +The output is spent by a transaction with `nSequence` field set to\n> `to_self_delay` (which can only be valid after that duration has passed)\n> and witness:\n> +\n> +    <remote_sig>\n> +\n> +Otherwise, this output is a simple P2WPKH to `remotepubkey`.\n> +\n> +\n> +#### `to_local_pushme` and `to_remote_pushme` Output\n> (option_simplified_commitment)\n> +\n> +This output can be spent by the local and remote nodes respectivey to\n> provide incentive to mine the transaction, using child-pays-for-parent.\n> They are only added if the `to_local` and `to_remote` outputs exist,\n> respectively.\n> +\n> +    OP_DEPTH\n> +    OP_IF\n> +        <pubkey> OP_CHECKSIG\n> +    OP_ELSE\n> +        10 OP_CSV\n> +    OP_ENDIF\n> +\n> +The `<pubkey>` is `<local_delayedpubkey>` to `to_local_pushme` and\n> +`<remote_delayedpubkey>` for `to_remote_pushme`.  The output amount is\n> +1000 satoshi, to encourage spending of the output.  Once the\n> +`remote_pubkey` is revealed (by spending the `to_local` output) and\n> +the commitment transaction is 10 blocks deep, anyone can spend it.\n> +\n>\n>  #### Offered HTLC Outputs\n>\n> @@ -294,6 +329,9 @@ The fee calculation for both commitment transactions\n> and HTLC\n>  transactions is based on the current `feerate_per_kw` and the\n>  *expected weight* of the transaction.\n>\n> +Note that if `option_simplified_commitment` applies to the commitment\n> +transaction then `feerate_per_kw` is 253.\n> +\n>  The actual and expected weights vary for several reasons:\n>\n>  * Bitcoin uses DER-encoded signatures, which vary in size.\n> @@ -306,10 +344,12 @@ Thus, a simplified formula for *expected weight* is\n> used, which assumes:\n>  * Signatures are 73 bytes long (the maximum length).\n>  * There are a small number of outputs (thus 1 byte to count them).\n>  * There are always both a `to_local` output and a `to_remote` output.\n> +* (if `option_simplified_commitment`) there are always both a\n> `to_local_pushme` and `to_remote_pushme` output.\n>\n>  This yields the following *expected weights* (details of the computation\n> in [Appendix A](#appendix-a-expected-weights)):\n>\n> -    Commitment weight:   724 + 172 * num-untrimmed-htlc-outputs\n> +    Commitment weight (no option_simplified_commitment):   724 + 172 *\n> num-untrimmed-htlc-outputs\n> +    Commitment weight (option_simplified_commitment:  1116 + 172 *\n> num-untrimmed-htlc-outputs\n>      HTLC-timeout weight: 663\n>      HTLC-success weight: 703\n>\n> @@ -366,7 +406,7 @@ outputs) is 7140 satoshi. The final fee may be even\n> higher if the\n>\n>  ### Fee Payment\n>\n> -Base commitment transaction fees are extracted from the funder's amount;\n> if that amount is insufficient, the entire amount of the funder's output is\n> used.\n> +Base commitment transaction fees and amounts for `to_local_pushme` and\n> `to_remote_pushme` outputs are extracted from the funder's amount; if that\n> amount is insufficient, the entire amount of the funder's output is used.\n>\n>  Note that after the fee amount is subtracted from the to-funder output,\n>  that output may be below `dust_limit_satoshis`, and thus will also\n> @@ -390,23 +430,29 @@ committed HTLCs:\n>  2. Calculate the base [commitment transaction fee](#fee-calculation).\n>  3. Subtract this base fee from the funder (either `to_local` or\n> `to_remote`),\n>     with a floor of 0 (see [Fee Payment](#fee-payment)).\n> +4. If `option_simplified_commitment` applies to the commitment\n> transaction,\n> +   subtract 2000 satoshis from the funder (either `to_local` or\n> `to_remote`).\n>  3. For every offered HTLC, if it is not trimmed, add an\n>     [offered HTLC output](#offered-htlc-outputs).\n>  4. For every received HTLC, if it is not trimmed, add an\n>     [received HTLC output](#received-htlc-outputs).\n>  5. If the `to_local` amount is greater or equal to `dust_limit_satoshis`,\n>     add a [`to_local` output](#to_local-output).\n> +6. If `option_simplified_commitment` applies to the commitment\n> transaction,\n> +   and `to_local` was added, add `to_local_pushme`.\n>  6. If the `to_remote` amount is greater or equal to `dust_limit_satoshis`,\n>     add a [`to_remote` output](#to_remote-output).\n> +6. If `option_simplified_commitment` applies to the commitment\n> transaction,\n> +   and `to_remote` was added, add `to_remote_pushme`.\n>  7. Sort the outputs into [BIP 69\n> order](#transaction-input-and-output-ordering).\n>\n>  # Keys\n>\n>  ## Key Derivation\n>\n> -Each commitment transaction uses a unique set of keys: `localpubkey` and\n> `remotepubkey`.\n> +Each commitment transaction uses a unique `localpubkey`, and a\n> `remotepubkey`.\n>  The HTLC-success and HTLC-timeout transactions use `local_delayedpubkey`\n> and `revocationpubkey`.\n> -These are changed for every transaction based on the\n> `per_commitment_point`.\n> +These are changed for every transaction based on the\n> `per_commitment_point`, with the exception of `remotepubkey` if\n> `option_simplified_commitment` is negotiated.\n>\n>  The reason for key change is so that trustless watching for revoked\n>  transactions can be outsourced. Such a _watcher_ should not be able to\n> @@ -419,8 +465,9 @@ avoid storage of every commitment transaction, a\n> _watcher_ can be given the\n>  the scripts required for the penalty transaction; thus, a _watcher_ need\n> only be\n>  given (and store) the signatures for each penalty input.\n>\n> -Changing the `localpubkey` and `remotepubkey` every time ensures that\n> commitment\n> -transaction ID cannot be guessed; every commitment transaction uses an ID\n> +Changing the `localpubkey` every time ensures that commitment\n> +transaction ID cannot be guessed except in the trivial case where there\n> is no\n> +`to_local` output, as every commitment transaction uses an ID\n>  in its output script. Splitting the `local_delayedpubkey`, which is\n> required for\n>  the penalty transaction, allows it to be shared with the _watcher_ without\n>  revealing `localpubkey`; even if both peers use the same _watcher_,\n> nothing is revealed.\n> @@ -434,14 +481,13 @@ For efficiency, keys are generated from a series of\n> per-commitment secrets\n>  that are generated from a single seed, which allows the receiver to\n> compactly\n>  store them (see [below](#efficient-per-commitment-secret-storage)).\n>\n> -### `localpubkey`, `remotepubkey`, `local_htlcpubkey`,\n> `remote_htlcpubkey`, `local_delayedpubkey`, and `remote_delayedpubkey`\n> Derivation\n> +### `localpubkey``local_htlcpubkey`, `remote_htlcpubkey`,\n> `local_delayedpubkey`, and `remote_delayedpubkey` Derivation\n>\n>  These pubkeys are simply generated by addition from their base points:\n>\n>         pubkey = basepoint + SHA256(per_commitment_point || basepoint) * G\n>\n> -The `localpubkey` uses the local node's `payment_basepoint`; the\n> `remotepubkey`\n> -uses the remote node's `payment_basepoint`; the `local_delayedpubkey`\n> +The `localpubkey` uses the local node's `payment_basepoint`; the\n> `local_delayedpubkey`\n>  uses the local node's `delayed_payment_basepoint`; the `local_htlcpubkey`\n> uses the\n>  local node's `htlc_basepoint`; and the `remote_delayedpubkey` uses the\n> remote\n>  node's `delayed_payment_basepoint`.\n> @@ -451,6 +497,17 @@ secrets are known (i.e. the private keys\n> corresponding to `localpubkey`, `local_\n>\n>      privkey = basepoint_secret + SHA256(per_commitment_point || basepoint)\n>\n> +### `remotepubkey` Derivation\n> +\n> +If `option_simplified_commitment` is negotiated the `remotepubkey` is\n> simply the remote node's `payment_basepoint`, otherwise it is calculated as\n> above using the remote node's `payment_basepoint`.\n> +\n> +The simplified derivation means that a node can spend a commitment\n> +transaction even if it has lost data and doesn't know the\n> +corresponding `payment_basepoint`.  A watchtower could correlate\n> +transactions given to it which only have a `to_remote` output if it\n> +sees one of them onchain, but such transactions do not need any\n> +enforcement and should not be handed to a watchtower.\n> +\n>  ### `revocationpubkey` Derivation\n>\n>  The `revocationpubkey` is a blinded key: when the local node wishes to\n> create a new\n> @@ -636,12 +693,22 @@ The *expected weight* of a commitment transaction is\n> calculated as follows:\n>                 - var_int: 1 byte (pk_script length)\n>                 - pk_script (p2wsh): 34 bytes\n>\n> -       output_paying_to_remote: 31 bytes\n> +       output_paying_to_remote (no option_simplified_commitment): 31 bytes\n>                 - value: 8 bytes\n>                 - var_int: 1 byte (pk_script length)\n>                 - pk_script (p2wpkh): 22 bytes\n>\n> -        htlc_output: 43 bytes\n> +       output_paying_to_remote (option_simplified_commitment): 43 bytes\n> +               - value: 8 bytes\n> +               - var_int: 1 byte (pk_script length)\n> +               - pk_script (p2wsh): 34 bytes\n> +\n> +       output_pushme (option_simplified_commitment): 43 bytes\n> +               - value: 8 bytes\n> +               - var_int: 1 byte (pk_script length)\n> +               - pk_script (p2wsh): 34 bytes\n> +\n> +    htlc_output: 43 bytes\n>                 - value: 8 bytes\n>                 - var_int: 1 byte (pk_script length)\n>                 - pk_script (p2wsh): 34 bytes\n> @@ -650,7 +717,7 @@ The *expected weight* of a commitment transaction is\n> calculated as follows:\n>                 - flag: 1 byte\n>                 - marker: 1 byte\n>\n> -        commitment_transaction: 125 + 43 * num-htlc-outputs bytes\n> +        commitment_transaction (no option_simplified_commitment): 125 +\n> 43 * num-htlc-outputs bytes\n>                 - version: 4 bytes\n>                 - witness_header <---- part of the witness data\n>                 - count_tx_in: 1 byte\n> @@ -663,15 +730,32 @@ The *expected weight* of a commitment transaction is\n> calculated as follows:\n>                         ....htlc_output's...\n>                 - lock_time: 4 bytes\n>\n> +        commitment_transaction (option_simplified_commitment): 223 + 43 *\n> num-htlc-outputs bytes\n> +               - version: 4 bytes\n> +               - witness_header <---- part of the witness data\n> +               - count_tx_in: 1 byte\n> +               - tx_in: 41 bytes\n> +                       funding_input\n> +               - count_tx_out: 1 byte\n> +               - tx_out: 172 + 43 * num-htlc-outputs bytes\n> +                       output_paying_to_remote,\n> +                       output_paying_to_local,\n> +                       output_pushme,\n> +                       output_pushme,\n> +                       ....htlc_output's...\n> +               - lock_time: 4 bytes\n> +\n>  Multiplying non-witness data by 4 results in a weight of:\n>\n> -       // 500 + 172 * num-htlc-outputs weight\n> +       // 500 + 172 * num-htlc-outputs weight (no\n> option_simplified_commitment)\n> +       // 892 + 172 * num-htlc-outputs weight\n> (option_simplified_commitment)\n>         commitment_transaction_weight = 4 * commitment_transaction\n>\n>         // 224 weight\n>         witness_weight = witness_header + witness\n>\n> -       overall_weight = 500 + 172 * num-htlc-outputs + 224 weight\n> +       overall_weight (no option_simplified_commitment) = 500 + 172 *\n> num-htlc-outputs + 224 weight\n> +       overall_weight (option_simplified_commitment) = 892 + 172 *\n> num-htlc-outputs + 224 weight\n>\n>  ## Expected Weight of HTLC-timeout and HTLC-success Transactions\n>\n> diff --git a/05-onchain.md b/05-onchain.md\n> index 231c209..c5fb5e1 100644\n> --- a/05-onchain.md\n> +++ b/05-onchain.md\n> @@ -89,21 +89,29 @@ trigger any action.\n>  # Commitment Transaction\n>\n>  The local and remote nodes each hold a *commitment transaction*. Each of\n> these\n> -commitment transactions has four types of outputs:\n> +commitment transactions has six types of outputs:\n>\n>  1. _local node's main output_: Zero or one output, to pay to the *local\n> node's*\n> -commitment pubkey.\n> +delayed pubkey.\n>  2. _remote node's main output_: Zero or one output, to pay to the *remote\n> node's*\n> -commitment pubkey.\n> +pubkey.\n> +1. _local node's push output_: Zero or one output, to pay to the *local\n> node's*\n> +delayed pubkey.\n> +2. _remote node's push output_: Zero or one output, to pay to the *remote\n> node's*\n> +pubkey.\n>  3. _local node's offered HTLCs_: Zero or more pending payments (*HTLCs*),\n> to pay\n>  the *remote node* in return for a payment preimage.\n>  4. _remote node's offered HTLCs_: Zero or more pending payments\n> (*HTLCs*), to\n>  pay the *local node* in return for a payment preimage.\n>\n>  To incentivize the local and remote nodes to cooperate, an\n> `OP_CHECKSEQUENCEVERIFY`\n> -relative timeout encumbers the *local node's outputs* (in the *local\n> node's\n> +relative timeout encumbers some outputs: the *local node's outputs* (in\n> the *local node's\n>  commitment transaction*) and the *remote node's outputs* (in the *remote\n> node's\n> -commitment transaction*). So for example, if the local node publishes its\n> +commitment transaction*). If `option_simplified_commitment` applies\n> +to the commitment transaction, then the *to_remote* output of each\n> commitment is\n> +identically encumbered, for fairness.\n> +\n> +Without `option_simplified_commitment`, if the local node publishes its\n>  commitment transaction, it will have to wait to claim its own funds,\n>  whereas the remote node will have immediate access to its own funds. As a\n>  consequence, the two commitment transactions are not identical, but they\n> are\n> @@ -140,6 +148,11 @@ A node:\n>        - otherwise:\n>          - MUST use the *last commitment transaction*, for which it has a\n>          signature, to perform a *unilateral close*.\n> +      - MUST spend any `to_local_pushme` output, providing sufficient\n> fees as incentive to include the commitment transaction in a block\n> +           - SHOULD use [replace-by-fee](\n> https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki) or other\n> mechanism on the spending transaction if it proves insufficient for timely\n> inclusion in a block.\n> +\n> +A node:\n> +  - MAY monitor the blockchain for unspent `to_local_pushme` and\n> `to_remote_pushme` outputs and try to spend them after 10 confirmations.\n>\n>  ## Rationale\n>\n> @@ -154,7 +167,8 @@ need not consume resources monitoring the channel\n> state.\n>  There exists a bias towards preferring mutual closes over unilateral\n> closes,\n>  because outputs of the former are unencumbered by a delay and are directly\n>  spendable by wallets. In addition, mutual close fees tend to be less\n> exaggerated\n> -than those of commitment transactions. So, the only reason not to use the\n> +than those of commitment transactions (or in the case of\n> `option_simplified_commitment`,\n> +the commitment transaction may require a child transaction to cause it to\n> be mined). So, the only reason not to use the\n>  signature from `closing_signed` would be if the fee offered was too small\n> for\n>  it to be processed.\n>\n> diff --git a/09-features.md b/09-features.md\n> index d06fcff..caea38b 100644\n> --- a/09-features.md\n> +++ b/09-features.md\n> @@ -26,6 +26,7 @@ These flags may only be used in the `init` message:\n>  | 3  | `initial_routing_sync` | Indicates that the sending node needs a\n> complete routing information dump | [BOLT\n> #7](07-routing-gossip.md#initial-sync) |\n>  | 4/5  | `option_upfront_shutdown_script` | Commits to a shutdown\n> scriptpubkey when opening channel | [BOLT\n> #2](02-peer-protocol.md#the-open_channel-message) |\n>  | 6/7  | `gossip_queries`           | More sophisticated gossip control |\n> [BOLT #7](07-routing-gossip.md#query-messages) |\n> +| 8/9  | `option_simplified_commitment`           | Simplified commitment\n> transactions | [BOLT #3](03-transactions.md) |\n>\n>  ## Assigned `globalfeatures` flags\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191026/fe4f49f3/attachment-0001.html>"
            },
            {
                "author": "Joost Jager",
                "date": "2019-10-26T20:28:38",
                "message_text_only": ">\n> * Output type: normal P2WKH. At one point, an additional spending path was\n> proposed that was unconditional except for a 10 block csv lock. The\n> intention of this was to prevent utxo set pollution by allowing anyone to\n> clean up. This however also opens up the possibility for an attacker to\n> 'use up' the cpfp carve-out after those 10 blocks. If the user A is offline\n> for that period of time, a malicious peer B may already have broadcasted\n> the commitment tx and pinned down user A's anchor output with a low fee\n> child. That way, the commitment tx could still remain unconfirmed while an\n> important htlc expires.\n>\n\nOk, this 'attack' scenario doesn't make sense. Of course with a csv lock,\nthis spend path is closed when the commitment tx is unconfirmed. But it is\nstill a question whether user A would appreciate their anchor output being\ntaken by someone else when they are offline for more than 10 blocks.\n\nIf we do like this utxo set clean up path, one could also argue that this\nshould then be applied to every near-dust output on the commitment tx (eg\nsmall htlcs).\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191026/8c9a5819/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2019-10-30T10:04:04",
                "message_text_only": "Joost Jager <joost.jager at gmail.com> writes:\n> We started to look at the `push_me` outputs again. Will refer to them as\n> `anchor` outputs from now on, to prevent confusion with `push_msat` on the\n> `open_channel` message.\n>\n> The cpfp carve-out https://github.com/bitcoin/bitcoin/pull/15681 has been\n> merged and for reasons described earlier in this thread, we now need to a=\ndd\n> a csv time lock to every non-anchor output on the commitment transaction.\n>\n> To realize this, we are currently considering the following changes:\n>\n> * Add `to_remote_delay OP_CHECKSEQUENCEVERIFY OP_DROP` to the `to_remote`\n> output. `to_remote_delay` is the csv delay that the remote party accepted\n> in the funding flow for their outputs. This not only ensures that the\n> carve-out works as intended, but also removes the incentive to game the\n> other party into force-closing. If desired, both parties can still agree =\nto\n> have different `to_self_delay` values.\n\nI think we should unify to_self_delay if we're doing this.  Otherwise\nthe game returns.\n\n> * Add `1 OP_CHECKSEQUENCEVERIFY OP_DROP` to the non-revocation clause of\n> the HTLC outputs.\n\n> For the anchor outputs we consider:\n>\n> * Output type: normal P2WKH. At one point, an additional spending path was\n> proposed that was unconditional except for a 10 block csv lock. The\n> intention of this was to prevent utxo set pollution by allowing anyone to\n> clean up. This however also opens up the possibility for an attacker to\n> 'use up' the cpfp carve-out after those 10 blocks. If the user A is offli=\nne\n> for that period of time, a malicious peer B may already have broadcasted\n> the commitment tx and pinned down user A's anchor output with a low fee\n> child. That way, the commitment tx could still remain unconfirmed while an\n> important htlc expires.\n\nAgreed, this doesn't really work.  We actually needed a bitcoin rule\nthat allowed a single anyone-can-spend output.  Seems like we didn't get\nthat.\n\n> * For the keys to use for `to_remote_anchor` and `to_local_anchor`, we=E2=\n=80=99d\n> like to introduce new addresses that both parties communicate in the\n> `open_channel` and `accept_channel` messages. We don=E2=80=99t want to re=\nuse the\n> main commitment output addresses, because those may (at some point) be co=\nld\n> storage addresses and the cpfp is likely to happen from a hot wallet.\n\nThis is horribly spammy.  At the moment we see ~ one unilateral close\nevery 3 blocks.  Hopefully that will reduce, but there'll always be\nsome.\n\n> * Within each version of the commitment transaction, both anchors always\n> have equal values and are paid for by the initiator.\n\nWho pays if they can't afford it?  What if they push_msat everything to\nthe other side?\n\n> The value of the\n> anchors is the dust limit that was negotiated in the `open_channel` or\n> `accept_channel` message of the party that publishes the transaction.\n\nNow initiator has to care about the other side's dust limit, which is\nbad.  And as accepter I now want this much higher, since I get those\nfunds instantly.  I don't think we gain anything by making this\nconfigurable at all; just pick a number and be done.\n\nSomewhere between 1000 and 10,000 sat is a good idea.\n\n> Furthermore, there doesn=E2=80=99t seem to be a compelling reason anymore=\n for\n> tweaking the keys (new insights into watchtower designs, encrypt by txid).\n\nThat's not correct.  This seems more like \"forgotten insights\" than \"new\ninsights\", which isn't surprising how long ago Tadge and I did the\nwatchtower design (BTW: I was the one who came up with encrypt by\ntxid for that!).\n\nThere are several design constraints in the original watchtowers:\n\n1. A watchtower shouldn't be able to guess the channel history.\n2. ... even if it sees a unilateral tx.\n3. ... even if it sees a revoked unilateral tx it has a penalty for.\n4. A watchtower shouldn't be able to tell if it's being used for both\n   parties in the same channel.\n\nIf you don't rotate keys, a watchtower can brute-force the HTLCs for all\nprevious transactions it was told about, and previous channel balances.\n\nWe removed key rotation on the to-remote output because you can simply\nnot tell the watchtower about txs which don't have anything but an\noutput to you.\n\nHere are the options I see:\n\n1. Abandon privacy from watchtowers and don't rotate keys.  Watchtowers\n   will be able to brute-force your history if they see a unilateral\n   close.\n\n2. Remove HTLC output key rotation, and assume watchtowers don't handle\n   HTLCs (so you don't tell them about early txs where the peer has no\n   output but there are HTLCs pending).  This seems less useful, since\n   HTLC txs require metadata anyway.\n\n3. Change to-local key rotation to use BIP32 (unhardened).  We could\n   also take some of the 48 bits (maybe 24?) we currently use to encode\n   the commitment number, to encode a BIP32 sub-path for this channel.\n   This would make it easier for hardware wallets to reconstruct.\n\nCheers,\nRusty."
            },
            {
                "author": "Joost Jager",
                "date": "2019-10-30T13:03:55",
                "message_text_only": ">\n> > * Add `to_remote_delay OP_CHECKSEQUENCEVERIFY OP_DROP` to the `to_remote`\n> > output. `to_remote_delay` is the csv delay that the remote party accepted\n> > in the funding flow for their outputs. This not only ensures that the\n> > carve-out works as intended, but also removes the incentive to game the\n> > other party into force-closing. If desired, both parties can still agree\n> =\n> to\n> > have different `to_self_delay` values.\n>\n> I think we should unify to_self_delay if we're doing this.  Otherwise\n> the game returns.\n>\n\nThe game returns, but both parties will be aware of the game they are\nplaying. They agreed to their peer's to_self_delay up front. (This is\ndifferent from the current situation where both peers are forced to accept\na remote_to_self_delay of 0.) With validation on the open/accept_channel\nmessage, a node can still enforce both to_self_delays to be equal. We could\nsimplify this to a single to_self_delay that is proposed by the initiator,\nbut what was the original reason to allow distinct values?\n\n\n> > For the anchor outputs we consider:\n> >\n> > * Output type: normal P2WKH. At one point, an additional spending path\n> was\n> > proposed that was unconditional except for a 10 block csv lock. The\n> > intention of this was to prevent utxo set pollution by allowing anyone to\n> > clean up. This however also opens up the possibility for an attacker to\n> > 'use up' the cpfp carve-out after those 10 blocks. If the user A is\n> offli=\n> ne\n> > for that period of time, a malicious peer B may already have broadcasted\n> > the commitment tx and pinned down user A's anchor output with a low fee\n> > child. That way, the commitment tx could still remain unconfirmed while\n> an\n> > important htlc expires.\n>\n> Agreed, this doesn't really work.  We actually needed a bitcoin rule\n> that allowed a single anyone-can-spend output.  Seems like we didn't get\n> that.\n>\n\nWith the mempool acceptance carve-out in bitcoind 0.19, we indeed won't be\nable to safely produce a single OP_TRUE output for anyone to spend. An\nattacker could attach low fee child transactions, reach the limits and\nblock further fee bumping.\n\n> * For the keys to use for `to_remote_anchor` and `to_local_anchor`, we=E2=\n> =80=99d\n> > like to introduce new addresses that both parties communicate in the\n> > `open_channel` and `accept_channel` messages. We don=E2=80=99t want to\n> re=\n> use the\n> > main commitment output addresses, because those may (at some point) be\n> co=\n> ld\n> > storage addresses and the cpfp is likely to happen from a hot wallet.\n>\n> This is horribly spammy.  At the moment we see ~ one unilateral close\n> every 3 blocks.  Hopefully that will reduce, but there'll always be\n> some.\n>\n\nIt seems there isn't another way to do the anchor outputs given the mempool\nlimitations that exist? Each party needs to have their own anchor,\nprotected by a key. Otherwise it would open up these attack scenarios where\nan attacker blocks the commitment tx confirmation until htlcs time out.\nEven with the script OP_DEPTH OP_IF <pubkey> OP_CHECKSIG OP_ELSE 10 OP_CSV\nOP_ENDIF, the \"anyones\" don't know the pubkey and still can't sweep after\n10 blocks.\n\n> * Within each version of the commitment transaction, both anchors always\n> > have equal values and are paid for by the initiator.\n>\n> Who pays if they can't afford it?  What if they push_msat everything to\n> the other side?\n>\n\nSimilar to how it currently works. There should never be a commitment\ntransaction in which the initiator cannot pay the fee. With anchor outputs\nthere should never be a commitment tx in which the initiator cannot pay the\nfee and the anchors. Also currently you cannot push everything to the other\nside with push_msat. The initiator still needs to have enough balance to\npay for the on-chain costs (miner fee and anchors).\n\n> The value of the\n> > anchors is the dust limit that was negotiated in the `open_channel` or\n> > `accept_channel` message of the party that publishes the transaction.\n>\n> Now initiator has to care about the other side's dust limit, which is\n> bad.  And as accepter I now want this much higher, since I get those\n> funds instantly.  I don't think we gain anything by making this\n> configurable at all; just pick a number and be done.\n>\n> Somewhere between 1000 and 10,000 sat is a good idea.\n>\n\nYes, it is free money. Therefore we need to validate the dust limit in the\nfunding flow. Check whether it is reasonable. That should also be done in\nthe current implementation. Otherwise your peer can set a really high dust\nlimit that lets your htlc disappear on-chain (although that is only free\nmoney for the miner).\n\nIf we hard-code a constant, we won't be able to adapt to changes of\n`dustRelayFee` in the bitcoin network. And we'd also need to deal with a\npeer picking a value higher than that constant for its regular funding flow\ndust limit parameter.\n\n\n> There are several design constraints in the original watchtowers:\n>\n> 1. A watchtower shouldn't be able to guess the channel history.\n> 2. ... even if it sees a unilateral tx.\n> 3. ... even if it sees a revoked unilateral tx it has a penalty for.\n> 4. A watchtower shouldn't be able to tell if it's being used for both\n>    parties in the same channel.\n>\n> If you don't rotate keys, a watchtower can brute-force the HTLCs for all\n> previous transactions it was told about, and previous channel balances.\n>\n> We removed key rotation on the to-remote output because you can simply\n> not tell the watchtower about txs which don't have anything but an\n> output to you.\n>\n> Here are the options I see:\n>\n> 1. Abandon privacy from watchtowers and don't rotate keys.  Watchtowers\n>    will be able to brute-force your history if they see a unilateral\n>    close.\n>\n> 2. Remove HTLC output key rotation, and assume watchtowers don't handle\n>    HTLCs (so you don't tell them about early txs where the peer has no\n>    output but there are HTLCs pending).  This seems less useful, since\n>    HTLC txs require metadata anyway.\n>\n> 3. Change to-local key rotation to use BIP32 (unhardened).  We could\n>    also take some of the 48 bits (maybe 24?) we currently use to encode\n>    the commitment number, to encode a BIP32 sub-path for this channel.\n>    This would make it easier for hardware wallets to reconstruct.\n>\n\nInteresting. I wasn't aware of the brute-force method that watchtowers\ncould potentially use. I wanted to bring up the removal of key rotation\njust in case everyone would agree we don't need it anymore. It isn't\nrequired for the anchor outputs, but it would have been one (future)\ncommitment format less. But it seems we do need it.\n\nIn the light of this forgotten insight, is there a reason why the anchor\noutput would need key rotation? Having no rotation makes it easier to let\nthose anchors go straight into the wallet, which may mitigate the dust utxo\nproblem a bit. At least then they can be easily coin-selected for any\non-chain spent, if the market fees are low enough.\n\nJoost\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191030/170f6756/attachment.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2019-10-30T16:08:41",
                "message_text_only": "> On Oct 30, 2019, at 03:04, Rusty Russell <rusty at rustcorp.com.au> wrote:\n> \n> \ufeffJoost Jager <joost.jager at gmail.com> writes:\n>> * Add `1 OP_CHECKSEQUENCEVERIFY OP_DROP` to the non-revocation clause of\n>> the HTLC outputs.\n> \n>> For the anchor outputs we consider:\n>> \n>> * Output type: normal P2WKH. At one point, an additional spending path was\n>> proposed that was unconditional except for a 10 block csv lock. The\n>> intention of this was to prevent utxo set pollution by allowing anyone to\n>> clean up. This however also opens up the possibility for an attacker to\n>> 'use up' the cpfp carve-out after those 10 blocks. If the user A is offli=\n> ne\n>> for that period of time, a malicious peer B may already have broadcasted\n>> the commitment tx and pinned down user A's anchor output with a low fee\n>> child. That way, the commitment tx could still remain unconfirmed while an\n>> important htlc expires.\n> \n> Agreed, this doesn't really work.  We actually needed a bitcoin rule\n> that allowed a single anyone-can-spend output.  Seems like we didn't get\n> that.\n\nHmm? If you have a CSV lock, it can\u2019t be used for carve-out/CPFP period. I don\u2019t see why this doesn\u2019t work. We definitely should stick to this.\n\n>> * For the keys to use for `to_remote_anchor` and `to_local_anchor`, we=E2=\n> =80=99d\n>> like to introduce new addresses that both parties communicate in the\n>> `open_channel` and `accept_channel` messages. We don=E2=80=99t want to re=\n> use the\n>> main commitment output addresses, because those may (at some point) be co=\n> ld\n>> storage addresses and the cpfp is likely to happen from a hot wallet.\n> \n> This is horribly spammy.  At the moment we see ~ one unilateral close\n> every 3 blocks.  Hopefully that will reduce, but there'll always be\n> some.\n> \n>> * Within each version of the commitment transaction, both anchors always\n>> have equal values and are paid for by the initiator.\n> \n> Who pays if they can't afford it?  What if they push_msat everything to\n> the other side?\n> \n>> The value of the\n>> anchors is the dust limit that was negotiated in the `open_channel` or\n>> `accept_channel` message of the party that publishes the transaction.\n> \n> Now initiator has to care about the other side's dust limit, which is\n> bad.  And as accepter I now want this much higher, since I get those\n> funds instantly.  I don't think we gain anything by making this\n> configurable at all; just pick a number and be done.\n> \n> Somewhere between 1000 and 10,000 sat is a good idea.\n> \n>> Furthermore, there doesn=E2=80=99t seem to be a compelling reason anymore=\n> for\n>> tweaking the keys (new insights into watchtower designs, encrypt by txid).\n> \n> That's not correct.  This seems more like \"forgotten insights\" than \"new\n> insights\", which isn't surprising how long ago Tadge and I did the\n> watchtower design (BTW: I was the one who came up with encrypt by\n> txid for that!).\n> \n> There are several design constraints in the original watchtowers:\n> \n> 1. A watchtower shouldn't be able to guess the channel history.\n> 2. ... even if it sees a unilateral tx.\n> 3. ... even if it sees a revoked unilateral tx it has a penalty for.\n> 4. A watchtower shouldn't be able to tell if it's being used for both\n>   parties in the same channel.\n> \n> If you don't rotate keys, a watchtower can brute-force the HTLCs for all\n> previous transactions it was told about, and previous channel balances.\n> \n> We removed key rotation on the to-remote output because you can simply\n> not tell the watchtower about txs which don't have anything but an\n> output to you.\n> \n> Here are the options I see:\n> \n> 1. Abandon privacy from watchtowers and don't rotate keys.  Watchtowers\n>   will be able to brute-force your history if they see a unilateral\n>   close.\n> \n> 2. Remove HTLC output key rotation, and assume watchtowers don't handle\n>   HTLCs (so you don't tell them about early txs where the peer has no\n>   output but there are HTLCs pending).  This seems less useful, since\n>   HTLC txs require metadata anyway.\n> \n> 3. Change to-local key rotation to use BIP32 (unhardened).  We could\n>   also take some of the 48 bits (maybe 24?) we currently use to encode\n>   the commitment number, to encode a BIP32 sub-path for this channel.\n>   This would make it easier for hardware wallets to reconstruct.\n> \n> Cheers,\n> Rusty.\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "Matt Corallo",
                "date": "2019-10-30T16:21:43",
                "message_text_only": "\ufeff(resend from the right src)\n\n>> On Oct 30, 2019, at 06:04, Joost Jager <joost.jager at gmail.com> wrote:\n>> > For the anchor outputs we consider:\n>> >\n>> > * Output type: normal P2WKH. At one point, an additional spending path was\n>> > proposed that was unconditional except for a 10 block csv lock. The\n>> > intention of this was to prevent utxo set pollution by allowing anyone to\n>> > clean up. This however also opens up the possibility for an attacker to\n>> > 'use up' the cpfp carve-out after those 10 blocks. If the user A is offli=\n>> ne\n>> > for that period of time, a malicious peer B may already have broadcasted\n>> > the commitment tx and pinned down user A's anchor output with a low fee\n>> > child. That way, the commitment tx could still remain unconfirmed while an\n>> > important htlc expires.\n>> \n>> Agreed, this doesn't really work.  We actually needed a bitcoin rule\n>> that allowed a single anyone-can-spend output.  Seems like we didn't get\n>> that.\n> \n> With the mempool acceptance carve-out in bitcoind 0.19, we indeed won't be able to safely produce a single OP_TRUE output for anyone to spend. An attacker could attach low fee child transactions, reach the limits and block further fee bumping.\n\nQuick correction. This is only partially true. You can still RBF the sub-package, the only issue I see immediately is you have to  pay for the otherwise-free relay of everything the attacker relayed.\n\nWhy not stick with the original design from Adelaide with a spending path with a 1CSV that is anyone can spend (or that is revealed by spending another output).\n\n>> > * For the keys to use for `to_remote_anchor` and `to_local_anchor`, we=E2=\n>> =80=99d\n>> > like to introduce new addresses that both parties communicate in the\n>> > `open_channel` and `accept_channel` messages. We don=E2=80=99t want to re=\n>> use the\n>> > main commitment output addresses, because those may (at some point) be co=\n>> ld\n>> > storage addresses and the cpfp is likely to happen from a hot wallet.\n>> \n>> This is horribly spammy.  At the moment we see ~ one unilateral close\n>> every 3 blocks.  Hopefully that will reduce, but there'll always be\n>> some.\n> \n> It seems there isn't another way to do the anchor outputs given the mempool limitations that exist? Each party needs to have their own anchor, protected by a key. Otherwise it would open up these attack scenarios where an attacker blocks the commitment tx confirmation until htlcs time out. Even with the script OP_DEPTH OP_IF <pubkey> OP_CHECKSIG OP_ELSE 10 OP_CSV OP_ENDIF, the \"anyones\" don't know the pubkey and still can't sweep after 10 blocks.\n> \n>> > * Within each version of the commitment transaction, both anchors always\n>> > have equal values and are paid for by the initiator.\n>> \n>> Who pays if they can't afford it?  What if they push_msat everything to\n>> the other side?\n> \n> Similar to how it currently works. There should never be a commitment transaction in which the initiator cannot pay the fee. With anchor outputs there should never be a commitment tx in which the initiator cannot pay the fee and the anchors. Also currently you cannot push everything to the other side with push_msat. The initiator still needs to have enough balance to pay for the on-chain costs (miner fee and anchors).\n> \n>> > The value of the\n>> > anchors is the dust limit that was negotiated in the `open_channel` or\n>> > `accept_channel` message of the party that publishes the transaction.\n>> \n>> Now initiator has to care about the other side's dust limit, which is\n>> bad.  And as accepter I now want this much higher, since I get those\n>> funds instantly.  I don't think we gain anything by making this\n>> configurable at all; just pick a number and be done.\n>> \n>> Somewhere between 1000 and 10,000 sat is a good idea.\n> \n> Yes, it is free money. Therefore we need to validate the dust limit in the funding flow. Check whether it is reasonable. That should also be done in the current implementation. Otherwise your peer can set a really high dust limit that lets your htlc disappear on-chain (although that is only free money for the miner).\n> \n> If we hard-code a constant, we won't be able to adapt to changes of `dustRelayFee` in the bitcoin network. And we'd also need to deal with a peer picking a value higher than that constant for its regular funding flow dust limit parameter.\n>  \n>> There are several design constraints in the original watchtowers:\n>> \n>> 1. A watchtower shouldn't be able to guess the channel history.\n>> 2. ... even if it sees a unilateral tx.\n>> 3. ... even if it sees a revoked unilateral tx it has a penalty for.\n>> 4. A watchtower shouldn't be able to tell if it's being used for both\n>>    parties in the same channel.\n>> \n>> If you don't rotate keys, a watchtower can brute-force the HTLCs for all\n>> previous transactions it was told about, and previous channel balances.\n>> \n>> We removed key rotation on the to-remote output because you can simply\n>> not tell the watchtower about txs which don't have anything but an\n>> output to you.\n>> \n>> Here are the options I see:\n>> \n>> 1. Abandon privacy from watchtowers and don't rotate keys.  Watchtowers\n>>    will be able to brute-force your history if they see a unilateral\n>>    close.\n>> \n>> 2. Remove HTLC output key rotation, and assume watchtowers don't handle\n>>    HTLCs (so you don't tell them about early txs where the peer has no\n>>    output but there are HTLCs pending).  This seems less useful, since\n>>    HTLC txs require metadata anyway.\n>> \n>> 3. Change to-local key rotation to use BIP32 (unhardened).  We could\n>>    also take some of the 48 bits (maybe 24?) we currently use to encode\n>>    the commitment number, to encode a BIP32 sub-path for this channel.\n>>    This would make it easier for hardware wallets to reconstruct.\n> \n> Interesting. I wasn't aware of the brute-force method that watchtowers could potentially use. I wanted to bring up the removal of key rotation just in case everyone would agree we don't need it anymore. It isn't required for the anchor outputs, but it would have been one (future) commitment format less. But it seems we do need it.\n> \n> In the light of this forgotten insight, is there a reason why the anchor output would need key rotation? Having no rotation makes it easier to let those anchors go straight into the wallet, which may mitigate the dust utxo problem a bit. At least then they can be easily coin-selected for any on-chain spent, if the market fees are low enough.\n>  \n> Joost\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191030/60cb8778/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2019-10-31T00:17:56",
                "message_text_only": "Matt Corallo <lf-lists at mattcorallo.com> writes:\n> Why not stick with the original design from Adelaide with a spending path with a 1CSV that is anyone can spend (or that is revealed by spending another output).\n\nThe original design IIRC was a single anyone-can-spend anchor output.\n\nIf we need two anchor outputs, and want the other to turn into an\nanyone-can-spend after it's mined, it's possible by gratuitously\nmentioning the other key in the script, I think:\n\n# If they provide a signature, they can push this:\nOP_DEPTH OP_IF\n  <key1> OP_CHECKSIG\nOP_ELSE\n  # Reveal the other key so you can spend the other anchor, too.\n  <key2> OP_DROP\n  # Now, anyone can spend after 1 block.\n  1 OP_CHECKSEQUENCEVERIFY\n  OP_TRUE\nOP_ENDIF\n\nThe other anchor output reverses <key1> and <key2>.\n\nCheers,\nRusty."
            },
            {
                "author": "Joost Jager",
                "date": "2019-10-31T11:34:48",
                "message_text_only": ">\n> On Oct 30, 2019, at 06:04, Joost Jager <joost.jager at gmail.com> wrote:\n>\n> > For the anchor outputs we consider:\n>> >\n>> > * Output type: normal P2WKH. At one point, an additional spending path\n>> was\n>> > proposed that was unconditional except for a 10 block csv lock. The\n>> > intention of this was to prevent utxo set pollution by allowing anyone\n>> to\n>> > clean up. This however also opens up the possibility for an attacker to\n>> > 'use up' the cpfp carve-out after those 10 blocks. If the user A is\n>> offli=\n>> ne\n>> > for that period of time, a malicious peer B may already have broadcasted\n>> > the commitment tx and pinned down user A's anchor output with a low fee\n>> > child. That way, the commitment tx could still remain unconfirmed while\n>> an\n>> > important htlc expires.\n>>\n>> Agreed, this doesn't really work.  We actually needed a bitcoin rule\n>> that allowed a single anyone-can-spend output.  Seems like we didn't get\n>> that.\n>>\n>\n> With the mempool acceptance carve-out in bitcoind 0.19, we indeed won't be\n> able to safely produce a single OP_TRUE output for anyone to spend. An\n> attacker could attach low fee child transactions, reach the limits and\n> block further fee bumping.\n>\n>\n> Quick correction. This is only partially true. You can still RBF the\n> sub-package, the only issue I see immediately is you have to  pay for the\n> otherwise-free relay of everything the attacker relayed.\n>\n\nOk, so this is always possible because the commitment transaction is\nsignaling opt-in rbf and therefore any child txes are too? From bip125:\n\"Transactions that don't explicitly signal replaceability are replaceable\nunder this policy for as long as any one of their ancestors signals\nreplaceability and remains unconfirmed.\" But yes, it can get unnecessarily\nexpensive to replace everything that the attacker added.\n\n\n> Why not stick with the original design from Adelaide with a spending path\n> with a 1CSV that is anyone can spend (or that is revealed by spending\n> another output).\n>\n\n What script would this be exactly? While still unconfirmed, the anchor\nneeds to be protected from being spent by anyone for the carve-out to work.\nThis also means that anyone spending after the csv needs to know that\nscript too. But how can they know if there is something like a pubkey (that\nprotected it during the unconfirmed phase) in it?\n\nJoost.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191031/2ce49811/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "First draft of option_simplfied_commitment",
            "categories": [
                "Lightning-dev",
                "PATCH"
            ],
            "authors": [
                "Rusty Russell",
                "Joost Jager",
                "Matt Corallo"
            ],
            "messages_count": 8,
            "total_messages_chars_count": 60138
        }
    },
    {
        "title": "[Lightning-dev] c-lightning v0.7.3 \"Bitcoin's Proof of Stake\" Released",
        "thread_messages": [
            {
                "author": "lisa neigut",
                "date": "2019-10-29T00:40:24",
                "message_text_only": "We're pleased to announce 0.7.3, named by @trueptolemy\n\nNote: new dependency gettext is now required when building from source.\n\n\n*Highlights for Users*- lightningd now supports different SQL backends.\nThis release includes a PostgresSQL driver, in addition to the default\nsqlite3 driver.\n- Ability to supply a Bitcoin address to close a channel to. Note that if\nyou've set an upfront-shutdown script, it'll need to be the same script.\n- New plugin notifications: sendpay_success and sendpay_failure\n- Encryption of the BIP32 master seed is now available.\n- UTXO selection is provided for withdraw and txprepare, which allows you\nto finely control which lightningd wallet UTXOs are spent or used.\n- Our Bolt11 tools now parse feature bits\n- Adds the ability to exclude nodes from route consideration in getroute\n- Please note that the deprecated RPC call listpayments has been removed;\nyou want listpays.\n\n\n*Highlights for the Network*- Elements support. lightningd can now be made\nto support L-BTC.\n- c-lightning nodes now announce features in node_announcement broadcasts.\n- Further on the feature bits front, we've consolidated them -- all bits\nare now advertised in both global + local feature fields\n- Support for gossip_queries_ex, for finer grained gossip control\n- Tighter gossip bandwidth usage. We now take advantage of gossip queries\nand peer rotation to narrow the number of peers that we gossip with and the\namount of gossip we're requesting.\n- In further gossip news, we now no longer ask for initial_routing_sync.\n\nMore details can be found in\nhttps://github.com/ElementsProject/lightning/blob/v0.7.3/CHANGELOG.md\n\n*Contributions*\nThanks to everyone for their contributions and bug reports; please keep\nthem coming!\n\nSince 0.7.2.1 we've had 522 commits from 18 different authors, with 5\nfirst-time contributors!\n\nLuca Ambrosini\nYash Bhutwala\n@willcl-ark\nJacob Rapoport\n@fiatjaf\n\nRelease details and binaries can be found at\nhttps://github.com/ElementsProject/lightning/releases/tag/v0.7.3\n\nCheers,\nLisa, Rusty, Christian, and ZmnSCPxj.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191028/d66f1a4f/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "c-lightning v0.7.3 \"Bitcoin's Proof of Stake\" Released",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "lisa neigut"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2235
        }
    },
    {
        "title": "[Lightning-dev] asynchronous Lightning network payments",
        "thread_messages": [
            {
                "author": "Konstantin Ketterer",
                "date": "2019-10-29T20:46:38",
                "message_text_only": "## Problem\nCurrently, both parties have to be online at the same time to send/ receive\nmoney. This can be avoided by using something like the Lightning Rod\nProtocol by Breez (https://github.com/breez/LightningRod). However, funds\nhave to be locked up longer than usual. We can do better than that.\n\n## Solution\nThe payer A pre-signs a transaction handing over money to their local\nchannel partner S and sends the transaction to the payee B in an end to end\nencrypted communication channel. B can then sell the signature for the\ntransaction to S using pay-for-signature made possible by payment points\nand scalars. (\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2019-July/002077.html)\n\n\nWe will be using eltoo because we don't have to worry about toxic channel\nstates.\n\nA and S are online, A and S have a channel\n1. A contacts S: they commit and exchange the R (= k*G) part of the Schnorr\nSignature for the update and settlement transaction\n2. A and S sign the settlement transaction increasing the balance between\nthem by amt + fee in favor of S and exchange the signatures\n3. A signs the corresponding update transaction but does not give it so S\n4. A sends the update transaction to B using an end to end encrypted\nasynchronous communication channel\n\nA can go offline\nB comes online\n\n4. Decrypts the update transaction and sells the signature s to S for amt\n\nWhen A comes back online S gives A the invoice (with the payment point s*G)\nsigned by B, the corresponding scalar s (the signature from A for the\nupdate transaction) and signature from S for the update transaction. They\ncan now proceed as normal.\n\nIn addition to enabling asynchronous payments, this outsources the routing\nto S.\n\n### Potential issues\n\n#### Privacy\nS currently knows both the sender and the receiver of the payment. If we\nsplit the payment from S to B into two payments between S and a public\nrouting node P and P and B by still using the same scalar + payment point,\nS now only knows the sender and P only knows the receiver. To further\nincrease privacy we can split the payment multiple times but all nodes\ninvolved must support this feature.\n\n#### Locked up capital\nWhile B hasn't yet claimed its money, the funds in the channel between A\nand S are essentially locked up. However, A and S could simply overwrite\nthe payment (new update + settlement transaction), then A could send\nmultiple payments with the remaining balance and before going offline A\nwould do the procedure again. If A has sufficient inbound capacity in other\nchannels it can also re-balance its channel A-S so that the outbound\ncapacity - (amt + fees) in this channel is zero and then do the procedure.\n\n#### Communication channel\nObviously, the communication channel must be end to end encrypted otherwise\nthis is highly insecure. Hopefully, we will have a sort of decentralized\npaid mail server system which is compatible across all LN wallets and part\nof BOLT.\n\n#### Proof of payment\nThe invoice by B with the payment point s*G and s are not sufficient as a\nPoP because S can simply give A the invoice and A already knows s.\n\n## the other way around\nWe can also do it in a way that A can instantly send B (who is offline)\nmoney but now A is in charge of enforcing the channel state if S cheats.\nBecause it has more issues like who pays the transaction fees if S cheats\nand because I believe Lightning is generally not designed for people who\nare offline for a long time I prefer the first one. But here is the other\none:\n\nB and S are online, B and S have a channel\n\n1. B and S sign a new settlement transaction increasing balance in favor of\nB by amt\n2. B signs the corresponding update transaction, encrypts it and sends it\ntogether with the settlement transaction to A\n\nB can go offline\nA comes online\n\n3. A decrypts the transactions, A pays S to sign the update transaction\nwhich makes this new channel state valid/ enforceable\n4. now A essentially acts like a watchtower while B is offline\n\n## Conclusion\n\nThis enables truly asynchronous Lightning network payments and is yet\nanother reason to move to payment points and scalars.\n\n\nRegards\nKonstantin Ketterer\n\nGithub: https://github.com/ko-redtruck/async-payments-ln\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191029/accb60ee/attachment.html>"
            },
            {
                "author": "Nadav Kohen",
                "date": "2019-10-29T21:20:28",
                "message_text_only": "Hi Konstantin,\n\nThis looks cool and I haven't had a chance yet to spend time with the idea,\nbut I would like to add that you can likely solve the PoP problem mentioned\nunder potential issues by having S pay B with an AMP-like setup. We use the\npoint B + x*G on one payment where x is a nonce generated by S, and the\npoint B + s*G + x*G on the other payment. The nonce, x, is revealed to the\nsignature seller only when they receive both onions. This way when the\npayment completes, S learns b + x and b + x + s so that they can compute b\n= (b + x) - x and s = (b + x + s) - (b + x). s is the signature that they\nwanted and b is the PoP.\n\nBest,\nNadav\n\nOn Tue, Oct 29, 2019 at 3:47 PM Konstantin Ketterer <\nketterer.konstantin at gmail.com> wrote:\n\n> ## Problem\n> Currently, both parties have to be online at the same time to send/\n> receive money. This can be avoided by using something like the Lightning\n> Rod Protocol by Breez (https://github.com/breez/LightningRod). However,\n> funds have to be locked up longer than usual. We can do better than that.\n>\n> ## Solution\n> The payer A pre-signs a transaction handing over money to their local\n> channel partner S and sends the transaction to the payee B in an end to end\n> encrypted communication channel. B can then sell the signature for the\n> transaction to S using pay-for-signature made possible by payment points\n> and scalars. (\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-July/002077.html)\n>\n>\n> We will be using eltoo because we don't have to worry about toxic channel\n> states.\n>\n> A and S are online, A and S have a channel\n> 1. A contacts S: they commit and exchange the R (= k*G) part of the\n> Schnorr Signature for the update and settlement transaction\n> 2. A and S sign the settlement transaction increasing the balance between\n> them by amt + fee in favor of S and exchange the signatures\n> 3. A signs the corresponding update transaction but does not give it so S\n> 4. A sends the update transaction to B using an end to end encrypted\n> asynchronous communication channel\n>\n> A can go offline\n> B comes online\n>\n> 4. Decrypts the update transaction and sells the signature s to S for amt\n>\n> When A comes back online S gives A the invoice (with the payment point\n> s*G) signed by B, the corresponding scalar s (the signature from A for the\n> update transaction) and signature from S for the update transaction. They\n> can now proceed as normal.\n>\n> In addition to enabling asynchronous payments, this outsources the routing\n> to S.\n>\n> ### Potential issues\n>\n> #### Privacy\n> S currently knows both the sender and the receiver of the payment. If we\n> split the payment from S to B into two payments between S and a public\n> routing node P and P and B by still using the same scalar + payment point,\n> S now only knows the sender and P only knows the receiver. To further\n> increase privacy we can split the payment multiple times but all nodes\n> involved must support this feature.\n>\n> #### Locked up capital\n> While B hasn't yet claimed its money, the funds in the channel between A\n> and S are essentially locked up. However, A and S could simply overwrite\n> the payment (new update + settlement transaction), then A could send\n> multiple payments with the remaining balance and before going offline A\n> would do the procedure again. If A has sufficient inbound capacity in other\n> channels it can also re-balance its channel A-S so that the outbound\n> capacity - (amt + fees) in this channel is zero and then do the procedure.\n>\n> #### Communication channel\n> Obviously, the communication channel must be end to end encrypted\n> otherwise this is highly insecure. Hopefully, we will have a sort of\n> decentralized paid mail server system which is compatible across all LN\n> wallets and part of BOLT.\n>\n> #### Proof of payment\n> The invoice by B with the payment point s*G and s are not sufficient as a\n> PoP because S can simply give A the invoice and A already knows s.\n>\n> ## the other way around\n> We can also do it in a way that A can instantly send B (who is offline)\n> money but now A is in charge of enforcing the channel state if S cheats.\n> Because it has more issues like who pays the transaction fees if S cheats\n> and because I believe Lightning is generally not designed for people who\n> are offline for a long time I prefer the first one. But here is the other\n> one:\n>\n> B and S are online, B and S have a channel\n>\n> 1. B and S sign a new settlement transaction increasing balance in favor\n> of B by amt\n> 2. B signs the corresponding update transaction, encrypts it and sends it\n> together with the settlement transaction to A\n>\n> B can go offline\n> A comes online\n>\n> 3. A decrypts the transactions, A pays S to sign the update transaction\n> which makes this new channel state valid/ enforceable\n> 4. now A essentially acts like a watchtower while B is offline\n>\n> ## Conclusion\n>\n> This enables truly asynchronous Lightning network payments and is yet\n> another reason to move to payment points and scalars.\n>\n>\n> Regards\n> Konstantin Ketterer\n>\n> Github: https://github.com/ko-redtruck/async-payments-ln\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191029/f44fc4d8/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-30T04:28:28",
                "message_text_only": "Good morning Konstantin,\n\n> ## Solution\n> The payer A pre-signs a transaction handing over money to their local channel partner S and sends the transaction to the payee B in an end to end encrypted communication channel. B can then sell the signature for the transaction to S using pay-for-signature made possible by payment points and scalars. (https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-July/002077.html)\n>\n> We will be using eltoo because we don't have to worry about toxic channel states.\n>\n> A and S are online, A and S have a channel\n> 1. A contacts S: they commit and exchange the R (= k*G) part of the Schnorr Signature for the update and settlement transaction\n> 2. A and S sign the settlement transaction increasing the balance between them by amt + fee in favor of S and exchange the signatures\n> 3. A signs the corresponding update transaction but does not give it so S\n> 4. A sends the update transaction to B using an end to end encrypted asynchronous communication channel\n>\n> A can go offline\n> B comes online\n\nHow does S learn that B has come online?\n\nPresumably there is some kind of polling involved.\nWhat do you imagine the polling rate to be?\nOne block?\n100 blocks?\nOne difficulty adjustment?\nOne halving period?\n\n>\n> 4. Decrypts the update transaction and sells the signature s to S for amt\n>\n> When A comes back online S gives A the invoice (with the payment point s*G) signed by B, the corresponding scalar s (the signature from A for the update transaction) and signature from S for the update transaction. They can now proceed as normal.\n>\n> In addition to enabling asynchronous payments, this outsources the routing to S.\n>\n> ### Potential issues\n>\n> #### Privacy\n> S currently knows both the sender and the receiver of the payment. If we split the payment from S to B into two payments between S and a public routing node P and P and B by still using the same scalar + payment point, S now only knows the sender and P only knows the receiver. To further increase privacy we can split the payment multiple times but all nodes involved must support this feature.\n>\n> #### Locked up capital\n> While B hasn't yet claimed its money, the funds in the channel between A and S are essentially locked up. However, A and S could simply overwrite the payment (new update + settlement transaction), then A could send multiple payments with the remaining balance and before going offline A would do the procedure again. If A has sufficient inbound capacity in other channels it can also re-balance its channel A-S so that the outbound capacity - (amt + fees) in this channel is zero and then do the procedure.\n\nAn issue is when the channel is forced onchain by either A or S.\nFor example, what if S is attacked by an army of shiny robots which destroys the keys of S?\n(Disclaimer: I do not control, nor have I ever controlled, an army of shiny robots to take over the world.\nShininess does not increase combat capability, thus not useful property of robots to have.)\nObviously S can no longer participate in the channel update, thus A *must* force the channel onchain and execute the contract there.\n\nThus, the UTXO for this should be claimable in both a secret-revelation path and a timeout path, both enforceable onchain, else S could hold the funds hostage by threatening unilateral close of the channel.\ni.e. you still need a PTLC for this.\n\nIndeed, it seems to me that much of the advantage of this lies solely in S being directly a peer of the payer.\nIt seems doable with just HTLCs and a normal BOLT11 invoice from B (possibly sent in plaintext to A via a deferred messaging service such as email --- invoices contain no private data anyway).\nThen the arrangement boils down to A and S agreeing that S will do all the routing (by polling the network) until just before the timeout indicated by the HTLC occurs.\nTrampoline routing can then be used to mask B from S.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Konstantin Ketterer",
                "date": "2019-10-30T12:39:31",
                "message_text_only": "Hi ZmnSCPxj,\n\nHow does S learn that B has come online?\n>\n> Presumably there is some kind of polling involved.\n> What do you imagine the polling rate to be?\n> One block?\n> 100 blocks?\n> One difficulty adjustment?\n> One halving period?\n>\n\nI imagined that A somehow send the IP address of S or any information to\ncontact S alongside the transaction and then B would connect to S and\nrequest the payment. If it had to be a polling rate I would choose\nsomething like 6 blocks.\n\n\n> > #### Locked up capital\n> > While B hasn't yet claimed its money, the funds in the channel between A\n> and S are essentially locked up. However, A and S could simply overwrite\n> the payment (new update + settlement transaction), then A could send\n> multiple payments with the remaining balance and before going offline A\n> would do the procedure again. If A has sufficient inbound capacity in other\n> channels it can also re-balance its channel A-S so that the outbound\n> capacity - (amt + fees) in this channel is zero and then do the procedure.\n>\n> An issue is when the channel is forced onchain by either A or S.\n> For example, what if S is attacked by an army of shiny robots which\n> destroys the keys of S?\n> (Disclaimer: I do not control, nor have I ever controlled, an army of\n> shiny robots to take over the world.\n> Shininess does not increase combat capability, thus not useful property of\n> robots to have.)\n> Obviously S can no longer participate in the channel update, thus A *must*\n> force the channel onchain and execute the contract there.\n>\n> Thus, the UTXO for this should be claimable in both a secret-revelation\n> path and a timeout path, both enforceable onchain, else S could hold the\n> funds hostage by threatening unilateral close of the channel.\n> i.e. you still need a PTLC for this.\n>\n\nI don't think I understand what you are trying to say. Because we are using\neltoo A can at any point in time publish the trigger transaction, the\nprevious update transaction and the  corresponding settlement transaction\nwithout any repercussions and recover its money after some amount of time.\nThis essentially allows A to cancel the payment to B until B has claimed\nthe payment from S. Because if S has already paid B and has not lost its\nkeys ( or already signed the new update transaction beforehand) it can\ndouble spend the older settlement transaction from A with its newer update\nand settlement transaction. All in all, A can still force the channel to\nsettle on chain even if S does not cooperate anymore and doesn't tell A\nabout the new update + settlement transaction.\n\nRegards\nKonstantin\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191030/3c07d8bb/attachment.html>"
            },
            {
                "author": "Yaacov Akiba Slama",
                "date": "2019-10-30T15:20:57",
                "message_text_only": "An HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20191030/9fb9e1bf/attachment-0001.html>"
            },
            {
                "author": "Yaacov Akiba Slama",
                "date": "2019-10-30T17:16:55",
                "message_text_only": "[Resending in Plain Text. Sorry for the spam]\nHello Konstantin,\n\nIn asynchronous payments we want to be sure that there are only two states:\n1) Before B received the payment.\n2) After B received the payment.\nIn state 1), we want A to still have the control of the amount, and at\nthe same time we want to let B decide (until a certain timeout) to\nreceive the payment. In this state, S can only help B to be paid, or\ncancel the whole transfer, but in this case, A cannot lose the amount.\nIn state 2), we want to be sure that no money is lost by S (and\nconsequently that the amount in A wallet is decremented by amt).\nAs ZmnSCPxj wrote in\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2019-October/002260.html\n(as I understand the last sentences), there is no way to do that without\nlocking the amount.\n\nIn order to increase the privacy, we are improving the specification to\nbe be able to use several Rod nodes: a list of nodes chosen by the payer\nand another list by the payee.\nThe Rod nodes are supposed to be almost always online, so standard https\ncan be used to communicate with them and between them.\n--yas\n\nOn 29/10/2019 22:46, Konstantin Ketterer wrote:\n> ## Problem\n> Currently, both parties have to be online at the same time to send/\n> receive money. This can be avoided by using something like the\n> Lightning Rod Protocol by Breez\n> (https://github.com/breez/LightningRod). However, funds have to be\n> locked up longer than usual. We can do better than that.\n>\n> ## Solution\n> The payer A pre-signs a transaction handing over money to their local\n> channel partner S and sends the transaction to the payee B in an end\n> to end encrypted communication channel. B can then sell the signature\n> for the transaction to S using pay-for-signature made possible by\n> payment points and scalars.\n> (https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-July/002077.html)\n>\n>\n> We will be using eltoo because we don't have to worry about toxic\n> channel states.\n>\n> A and S are online, A and S have a channel\n> 1. A contacts S: they commit and exchange the R (= k*G) part of the\n> Schnorr Signature for the update and settlement transaction\n> 2. A and S sign the settlement transaction increasing the balance\n> between them by amt + fee in favor of S and exchange the signatures\n> 3. A signs the corresponding update transaction but does not give it so S\n> 4. A sends the update transaction to B using an end to end encrypted\n> asynchronous communication channel\n>\n> A can go offline\n> B comes online\n>\n> 4. Decrypts the update transaction and sells the signature s to S for amt\n>\n> When A comes back online S gives A the invoice (with the payment point\n> s*G) signed by B, the corresponding scalar s (the signature from A for\n> the update transaction) and signature from S for the update\n> transaction. They can now proceed as normal.\n>\n> In addition to enabling asynchronous payments, this outsources the\n> routing to S.\n>\n> ### Potential issues\n>\n> #### Privacy\n> S currently knows both the sender and the receiver of the payment. If\n> we split the payment from S to B into two payments between S and a\n> public routing node P and P and B by still using the same scalar +\n> payment point, S now only knows the sender and P only knows the\n> receiver. To further increase privacy we can split the payment\n> multiple times but all nodes involved must support this feature.\n>\n> #### Locked up capital\n> While B hasn't yet claimed its money, the funds in the channel between\n> A and S are essentially locked up. However, A and S could simply\n> overwrite the payment (new update + settlement transaction), then A\n> could send multiple payments with the remaining balance and before\n> going offline A would do the procedure again. If A has sufficient\n> inbound capacity in other channels it can also re-balance its channel\n> A-S so that the outbound capacity - (amt + fees) in this channel is\n> zero and then do the procedure.\n>\n> #### Communication channel\n> Obviously, the communication channel must be end to end encrypted\n> otherwise this is highly insecure. Hopefully, we will have a sort of\n> decentralized paid mail server system which is compatible across all\n> LN wallets and part of BOLT.\n>\n> #### Proof of payment\n> The invoice by B with the payment point s*G and s are not sufficient\n> as a PoP because S can simply give A the invoice and A already knows s.\n>\n> ## the other way around\n> We can also do it in a way that A can instantly send B (who is\n> offline) money but now A is in charge of enforcing the channel state\n> if S cheats. Because it has more issues like who pays the transaction\n> fees if S cheats and because I believe Lightning is generally not\n> designed for people who are offline for a long time I prefer the first\n> one. But here is the other one:\n>\n> B and S are online, B and S have a channel\n>\n> 1. B and S sign a new settlement transaction increasing balance in\n> favor of B by amt\n> 2. B signs the corresponding update transaction, encrypts it and sends\n> it together with the settlement transaction to A\n>\n> B can go offline\n> A comes online\n>\n> 3. A decrypts the transactions, A pays S to sign the update\n> transaction which makes this new channel state valid/ enforceable\n> 4. now A essentially acts like a watchtower while B is offline\n>\n> ## Conclusion\n>\n> This enables truly asynchronous Lightning network payments and is yet\n> another reason to move to payment points and scalars.\n>\n>\n> Regards\n> Konstantin Ketterer\n>\n> Github: https://github.com/ko-redtruck/async-payments-ln\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            }
        ],
        "thread_summary": {
            "title": "asynchronous Lightning network payments",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Konstantin Ketterer",
                "Nadav Kohen",
                "Yaacov Akiba Slama",
                "ZmnSCPxj"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 22443
        }
    }
]