[{"title": "[Lightning-dev] Taro: A Taproot Asset Representation Overlay", "thread_messages": [{"author": "Anthony Towns", "date": "2023-02-06T10:48:09", "message_text_only": "On Mon, Apr 11, 2022 at 02:59:16PM -0400, Olaoluwa Osuntokun wrote:\n\nThread necromancy, but hey.\n\n> > anything about Taro or the way you plan to implement support for\n> > transferring fungible assets via asset-aware LN endpoints[1] will address\n> > the \"free call option\" problem, which I think was first discussed on this\n> > list by Corn\u00e9 Plooy[2] and was later extended by ZmnSCPxj[3], with Tamas\n> > Blummer[4] providing the following summary\n> \n> I agree w/ Tamas' quote there in that the problem doesn't exist for\n> transfers using the same asset. Consider a case of Alice sending to Bob,\n> with both of them using a hypothetical asset, USD-beef: if the final/last\n> hop withholds the HTLC, then they risk Bob not accepting the HTLC either due\n> to the payment timing out, or exchange rate fluctuations resulting in an\n> insufficient amount delivered to the destination (Bob wanted 10 USD-beef,\n> but the bound BTC in the onion route is only now 9 USD-beef), in either case\n> the payment would be cancelled.\n\nI don't think this defense actually works. If you have:\n\n Alice -> Bob -> Carol -> Dave -> Elizabeth\n\nwith Alice/Bob and Dave/Elizabeth having USD channels, but\nBob/Carol and Carol/Dave being BTC channels, then Dave has\na reasonable opportunity to cheat:\n\n - he can be pretty confident that Elizabeth is the final recipient\n   (since USD is meant to be at the edges, and this is a BTC to USD\n   conversion)\n\n - he knows the expected USD value of the payment to Elizabeth\n\n - he knows what the on-chain timeout of the USD payment to Elizabeth\n   will be, because he shares the channel, so can likely be confident\n   Elizabeth won't cancel the tx as long as he forwards it to her by then\n\n - he can hold up the outbound USD payment while holding onto the\n   inbound BTC payment, only forwarding the payment on to Elizabeth if\n   the price of BTC stays the same or increases.\n\nI'm not an expert, but I tried a Black Scholes calculator with an\nestimate for Bitcoin's volatility, and it suggests that the fair price\nof an option like that that lasts an hour is about 0.3% of the par value\n(ie, for a $1000 payment, the ability to hold up the BTC/USD conversion\nfor an hour and only do it when it's profitable, is worth about $3). That\nseems substantial compared to normal lightning fee rates, which I think\nare often in the 0.01% to 0.1% range?\n\n(Note that this way of analysing the free option problem means it's\nonly an issue when the two assets have high volatility -- if they're\nsufficiently correlated, like USDT and USDC, or perhaps even USD and EUR,\nthen the value of the free option is minimised, perhaps to the point\nwhere it's not worth trying to exploit)\n\nBob may have a similar ability to interfere with the payment, but is\nmuch more constrained: he probably doesn't know Elizabeth's timeout;\nand if he's making a profit because the price of BTC has gone down,\nthen Dave is likely to cancel the transaction rather than forwarding it\nto Elizabeth, since he'd be making a lock when converting the BTC amount\nto its pre-drop USD value. However, if there wasn't a followup conversion\nback from BTC to USD, and Bill was willing to guess at the final timeout\nof the payment, he could still make a profit from delaying payments.\n(Though it's also less harmful here: only the Alice/Bob funds are being\nheld up indefinitely, not the funds from random channels)\n\nI think maybe a better approach might be:\n\n Alice -> Bob -BTC-> Carol -BTC-> Elizabeth -BTC-> Dave -USD-> Elizabeth\n\nThat is, Alice sends $100 to Bob who forwards 0.004 BTC (or whatever) to\nCarol and then Elizabeth; then, before accepting the payment, Elizabeth\nextends the path with a BTC/USD exchange with Dave via a short loop. If\nDave doesn't immediately forward the USD to Elizabeth, she can cancel\nthe transaction, refunding Carol all the way back to Alice, even while\nwaiting for Dave. She doesn't need to be concerned that Dave could\nclaim funds from her, as all the transfers are conditional on a secret\nonly Elizabeth knows, and that she has not yet revealed. If Dave tries\nexploiting the free option, Elizabeth can see he doesn't reliably finish\nthe loop quickly, and try finding another, better, exchange.\n\nThat approach also means Alice doesn't need to know what Elizabeth's\ncurrency preference is; she's just sending BTC, so she only needs to\nknow about the exchange rate between BTC and her own currency, which\nseems like it means there's one less thing that could go wrong.\n\nCheers,\naj", "summary": "A potential issue with Lightning Network's asset-aware endpoints is the \"free call option\" problem, where a node can hold up a payment to profit from price fluctuations. This is more of an issue with assets of high volatility."}], "thread_summary": {"title": "Taro: A Taproot Asset Representation Overlay", "categories": ["Lightning-dev"], "authors": ["Anthony Towns"], "messages_count": 1, "total_messages_chars_count": 4476}}, {"title": "[Lightning-dev] Jamming Mitigation Call Summary - 02/06", "thread_messages": [{"author": "Carla Kirk-Cohen", "date": "2023-02-08T20:17:28", "message_text_only": "Hi list,\n\nUnfortunately we had some technical issues with the recording for\nMonday's call so we're going to have to rely on my memory (a severely\ncorrupted data store). Thankfully, Clara jotted down some notes as well,\nbut please chime in if you attended and we've missed something out!\n\nDetails for next call:\n* Monday 20 February\n* 18:00 UTC (possibly 19:00, be confirmed in a follow up email)\n* https://meet.jit.si/UnjammingLN\n* Agenda: https://github.com/ClaraShk/LNJamming/issues/3\n\n# Meeting Summary\n\n1. Proof of Forwarding\nWe started the call with a discussion of the various \"proof of\nforwarding\" schemes that have been kicked around this mailing list\nin the past.\n\nWorking of the assumption that upfront fees must always be sourced\nfrom the sender, we ran into similar issues around accumulated fees\nand differential rates even in the case where we have some proof of\nforward, because nodes can still choose to collaborate to \"forward\"\na payment.\n\nGiven the following topology and conditions:\nAlice ------ Bob ------ Carol ------ Dave ----- Evelyn\n\n* Alice is sending a payment to Evenlyn, a popular sink on the network.\n* Evenlyn has zero final-hop upfront fees (for simplicity's sake).\n* Bob and Carol are malicious actors, each charging 10 msat upfront.\n* Dave is an honest node with 4000 msat upfront fees (which is a rational\n  upfront fee for a channel to a sink node that would have high success\n  case fees).\n\nUsing a proof of forward scheme where Alice places a secret in the\n_next_ node's onion which is required to claim upfront fees:\n* Bob may claim 4020 msat in upfront fees with a secret from Carol\n* Carol may claim 4010 msat in upfront fees with a secret from Dave\n* Dave may claim 4000 msat in upfront fees with a secret from Evelyn\n\nIn the honest case, this nets out to 10 msat for Bob, 10 msat for\nCarol, and 4000 msat for Dave. In the dishonest case, Carol can\ndisclose the forwarding secret to Bob, allowing them to claim the\naccumulated 4020 sat, then fail the payment anyway.\n\nWe also spoke about the case where the downstream peer refuses to give\nup the secret, but this breakdown in cooperation is likely remedied by\nclosing your channel. We didn't consider the case where upfront fees\ndo not accumulate along the route, because this exposes us to a whole\nlot of (even worse) draining attacks.\n\nWe once again discussed the complexity of this nature of jamming\nmitigation, how practical it would be to implement it and whether it\nis worthwhile pursuing if it will be an imperfect solution to upfront\nfee theft.\n\n2. Upfront Fees + Attributable Errors\nThe next point of discussion was the combination of upfront fees with\nattributable errors [1] to allow senders to more severely punish\nnodes that choose to steal upfront fees rather than forward the HTLC\n(due to the incentive issues noted in [2] when there are large fee\ndifferentials across the route).\n\nThis led to a discussion about how effectively senders can enforce\ngood upfront fee behavior - a question that remains open. We discussed:\n- Strict punishment of nodes that fail payments with upfront fees.\n- Sender side protections against selection of routes with incentive\n  incompatible upfront fees.\n- The suboptimal, yet doable, solution of starting with an upper bound\n  on upfront fees. This has the drawback of not properly compensating\n  nodes that have a legitimate claim to high upfront fees because they\n  face high opportunity cost.\n\n3. Experimentation with Circuit Breaker\nWe discussed the possibility of using circuit breaker[3] in the context\nof local reputation (or HTLC endorsement) in two different ways:\n(a) To begin surfacing the type of information that end users would\nrequire to decide to endorse a payment, and possibly automating\nthat decision making.\n(b) In conjunction with an experimental TLV to add binary HTLC\nendorsement to update_add_htlc, though it was noted that this\nwould require wider spread adoption of circuit breaker, because\nnodes that do not have it installed would not include the TLV on\nforwarding. This would also require LND changes to surface TLVs\nincluded with update_add_htlc on the interceptor API.\n\nAs always, thank you to all that attended and hope to see ya'll in the\nnext one!\n\nBest,\nCarla + Clara\n\n[1] https://github.com/lightning/bolts/pull/1044\n[2]\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2023-January/003834.html\n[3] https://github.com/lightningequipment/circuitbreaker\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20230208/c383d310/attachment.html>", "summary": "Technical issues prevented recording of a call, so a summary was provided. Topics discussed included proof of forwarding and upfront fees."}, {"author": "Vincenzo Palazzo", "date": "2023-02-11T11:41:32", "message_text_only": "On Wed Feb 8, 2023 at 9:17 PM CET, Carla Kirk-Cohen wrote:\n> Hi list,\n>\n> Unfortunately we had some technical issues with the recording for\n> Monday's call so we're going to have to rely on my memory (a severely\n> corrupted data store). Thankfully, Clara jotted down some notes as well,\n> but please chime in if you attended and we've missed something out!\n>\n> Details for next call:\n> * Monday 20 February\n> * 18:00 UTC (possibly 19:00, be confirmed in a follow up email)\n> * https://meet.jit.si/UnjammingLN\n> * Agenda: https://github.com/ClaraShk/LNJamming/issues/3\n>\nThanks to write this down, do you think that it is a good idea \nto make a public google calendar (or other kind of shared calendar)?\n\n>4. Reputation discussions in LSP Specification\n>?Some attendees have been working on a reputation system for the LSP\n>specification group [8]. This system is intended to provide standard\n>metrics about what makes a node good/bad to connect to in the context\n>of a decentralized marketplace. While this work looks at the problem\n>from a different perspective, there's a possibility to consolidate\n>the efforts. It seems particularly useful in the anti-jamming case\n>where a node has newly connected to you, and needs a \"start state\"\n>reputation score. The idea of defining what qualifies as good\n>behavior in the Lightning Network is useful across the board. The\n>LSP specification group also has bi-weekly calls, and welcomes\n>additional participants!\n\nThere is also an ongoing attempt to define a kind of standard \nfor the lightning network metrics started more thatn 1 year \nago, and I also discussed in Oakland (but maybe some people \nwas scared by the word `server`)\n\nThe source code is available here https://github.com/LNOpenMetrics \nand for now the basic idea is to collect some data from different \nkind of node and analyze them before taking any decision to define \nany kind of protocol support via BOLT or BLIPS (I also I'm more inclined\nto a BLIPS because I think that different kind of node mobile vs LSP)\ncan deserve different kind of metrics/reputation.\n\nThere are some data already accessible via public API (for now) https://api.lnmetrics.info\nwhere it is possible to see some forwarding payment scoring by channels and node\nuptime.\n\nCheers!\n\nVincent.", "summary": "Technical issues caused the recording of a recent call to be lost, but notes were taken for the next call on February 20th. The group is also discussing a reputation system for the Lightning Network."}, {"author": "Clara Shikhelman", "date": "2023-02-13T20:15:21", "message_text_only": "Hi Vincent,\n\nThanks to write this down, do you think that it is a good idea\n> to make a public google calendar (or other kind of shared calendar)?\n>\n\nLet's try this out:\nhttps://calendar.google.com/calendar/u/2?cid=Mjc1ODRlNDNhMjE4NDQyYWMyN2JiZDY4NTlkNWQyZTU5ZWE3NGJiMWM2Mzk4Y2VmMzNhYTcwYWYzYjZjOTc2OEBncm91cC5jYWxlbmRhci5nb29nbGUuY29t\n<https://calendar.google.com/calendar/u/2?cid=Mjc1ODRlNDNhMjE4NDQyYWMyN2JiZDY4NTlkNWQyZTU5ZWE3NGJiMWM2Mzk4Y2VmMzNhYTcwYWYzYjZjOTc2OEBncm91cC5jYWxlbmRhci5nb29nbGUuY29t>\n\nThere is also an ongoing attempt to define a kind of standard\n> for the lightning network metrics started more thatn 1 year\n> ago, and I also discussed in Oakland (but maybe some people\n> was scared by the word `server`)\n>\n> The source code is available here https://github.com/LNOpenMetrics\n> and for now the basic idea is to collect some data from different\n> kind of node and analyze them before taking any decision to define\n> any kind of protocol support via BOLT or BLIPS (I also I'm more inclined\n> to a BLIPS because I think that different kind of node mobile vs LSP)\n> can deserve different kind of metrics/reputation.\n>\n> There are some data already accessible via public API (for now)\n> https://api.lnmetrics.info\n> where it is possible to see some forwarding payment scoring by channels\n> and node\n> uptime.\n>\n>\nThis could be very useful, thanks for pointing it out!\n\nBest,\nClara\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20230213/a11062ac/attachment.html>", "summary": "The email discusses the possibility of creating a public Google calendar or shared calendar, as well as an ongoing attempt to define a standard for lightning network metrics. The source code is available on GitHub and there are already some data accessible via public API."}], "thread_summary": {"title": "Jamming Mitigation Call Summary - 02/06", "categories": ["Lightning-dev"], "authors": ["Carla Kirk-Cohen", "Clara Shikhelman", "Vincenzo Palazzo"], "messages_count": 3, "total_messages_chars_count": 8472, "convo_summary": "The group discussed proof of forwarding and upfront fees, but technical issues prevented the call from being recorded. Notes were taken for the next call on February 20th. They also talked about creating a reputation system for the Lightning Network and defining a standard for metrics. The source code is available on GitHub and there are plans to create a public or shared calendar."}}, {"title": "[Lightning-dev] Highly Available Lightning Channels", "thread_messages": [{"author": "Joost Jager", "date": "2023-02-13T11:45:54", "message_text_only": "Hi,\n\nFor a long time I've held the expectation that eventually payers on the\nlightning network will become very strict about node performance. That they\nwill require a routing node to operate flawlessly or else apply a hefty\npenalty such as completely avoiding the node for an extended period of time\n- multiple weeks. The consequence of this is that routing nodes would need\nto manage their liquidity meticulously because every failure potentially\nhas a large impact on future routing revenue.\n\nI think movement in this direction is important to guarantee\ncompetitiveness with centralised payment systems and their (at least\ntheoretical) ability to process a payment in the blink of an eye. A\nlightning wallet trying multiple paths to find one that works doesn't help\nwith this.\n\nA common argument against strict penalisation is that it would lead to less\nefficient use of capital. Routing nodes would need to maintain pools of\nliquidity to guarantee successes all the time. My opinion on this is that\nlightning is already enormously capital efficient at scale and that it is\nworth sacrificing a slight part of that efficiency to also achieve the\nlowest possible latency.\n\nThis brings me to the actual subject of this post. Assuming strict\npenalisation is good, it may still not be ideal to flip the switch from one\nday to the other. Routing nodes may not offer the required level of service\nyet, causing senders to end up with no nodes to choose from.\n\nOne option is to gradually increase the strength of the penalties, so that\nrouting nodes are given time to adapt to the new standards. This does\nrequire everyone to move along and leaves no space for cheap routing nodes\nwith less leeway in terms of liquidity.\n\nTherefore I am proposing another way to go about it: extend the\n`channel_update` field `channel_flags` with a new bit that the sender can\nuse to signal `highly_available`.\n\nIt's then up to payers to decide how to interpret this flag. One way could\nbe to prefer `highly_available` channels during pathfinding. But if the\nrouting node then returns a failure, a much stronger than normal penalty\nwill be applied. For routing nodes this creates an opportunity to attract\nmore traffic by marking some channels as `highly_available`, but it also\ncomes with the responsibility to deliver.\n\nWithout shadow channels, it is impossible to guarantee liquidity up to the\nchannel capacity. It might make sense for senders to only assume high\navailability for amounts up to `htlc_maximum_msat`.\n\nA variation on this scheme that requires no extension of `channel_update`\nis to signal availability implicitly through routing fees. So the more\nexpensive a channel is, the stronger the penalty that is applied on failure\nwill be. It seems less ideal though, because it could disincentivize cheap\nbut reliable channels on high traffic links.\n\nThe effort required to implement some form of a `highly_available` flag\nseem limited and it may help to get payment success rates up. Interested to\nhear your thoughts.\n\nJoost\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20230213/adf0c913/attachment.html>", "summary": "The Lightning Network may require routing nodes to operate flawlessly or face penalties, to ensure competitiveness with centralized payment systems. A proposed solution is to extend the `channel_update` field with a new bit to signal `highly_available` channels, allowing payers to prefer them during pathfinding, but with a stronger penalty for failure. This creates an opportunity for routing nodes to attract more traffic but also comes with the responsibility to deliver."}, {"author": "Matt Corallo", "date": "2023-02-13T14:40:24", "message_text_only": "Hi Joost,\n\nI\u2019m not sure I agree that lightning is \u201ccapital efficient\u201d (or even close to it), but more generally I don\u2019t see why this needs a signal.\n\nIf nodes start aggressively preferring routes through nodes that reliably route payments (which I believe lnd already does, in effect, to some large extent), they should do so by measurement, not signaling.\n\nIn practice, many channels on the network are \u201chigh availability\u201d today, but only in one direction (I.e. they aren\u2019t regularly spliced/rebalanced and are regularly unbalanced). A node strongly preferring a high payment success rate *should* prefer such a channel, but in your scheme would not.\n\nThis ignores the myriad of \u201cat what threshold do you signal HA\u201d issues, which likely make such a signal DOA, anyway.\n\nFinally, I\u2019m very dismayed at this direction in thinking on how ln should work - nodes should be measuring the network and routing over paths that it thinks are reliable for what it wants, *robustly over an unreliable network*. We should absolutely not be expecting the lightning network to be built out of high reliability nodes, that creates strong centralization pressure. To truly meet a \u201chigh availability\u201d threshold, realistically, you\u2019d need to be able to JIT 0conf splice-in, which would drive lightning to actually being a credit network.\n\nWith reasonable volume, lightning today is very reliable and relatively fast, with few retries required. I don\u2019t think we need to change anything to fix it. :)\n\nMatt\n\n> On Feb 13, 2023, at 06:46, Joost Jager <joost.jager at gmail.com> wrote:\n> \n> \ufeff\n> Hi,\n> \n> For a long time I've held the expectation that eventually payers on the lightning network will become very strict about node performance. That they will require a routing node to operate flawlessly or else apply a hefty penalty such as completely avoiding the node for an extended period of time - multiple weeks. The consequence of this is that routing nodes would need to manage their liquidity meticulously because every failure potentially has a large impact on future routing revenue.\n> \n> I think movement in this direction is important to guarantee competitiveness with centralised payment systems and their (at least theoretical) ability to process a payment in the blink of an eye. A lightning wallet trying multiple paths to find one that works doesn't help with this.\n> \n> A common argument against strict penalisation is that it would lead to less efficient use of capital. Routing nodes would need to maintain pools of liquidity to guarantee successes all the time. My opinion on this is that lightning is already enormously capital efficient at scale and that it is worth sacrificing a slight part of that efficiency to also achieve the lowest possible latency.\n> \n> This brings me to the actual subject of this post. Assuming strict penalisation is good, it may still not be ideal to flip the switch from one day to the other. Routing nodes may not offer the required level of service yet, causing senders to end up with no nodes to choose from.\n> \n> One option is to gradually increase the strength of the penalties, so that routing nodes are given time to adapt to the new standards. This does require everyone to move along and leaves no space for cheap routing nodes with less leeway in terms of liquidity.\n> \n> Therefore I am proposing another way to go about it: extend the `channel_update` field `channel_flags` with a new bit that the sender can use to signal `highly_available`. \n> \n> It's then up to payers to decide how to interpret this flag. One way could be to prefer `highly_available` channels during pathfinding. But if the routing node then returns a failure, a much stronger than normal penalty will be applied. For routing nodes this creates an opportunity to attract more traffic by marking some channels as `highly_available`, but it also comes with the responsibility to deliver.\n> \n> Without shadow channels, it is impossible to guarantee liquidity up to the channel capacity. It might make sense for senders to only assume high availability for amounts up to `htlc_maximum_msat`.\n> \n> A variation on this scheme that requires no extension of `channel_update` is to signal availability implicitly through routing fees. So the more expensive a channel is, the stronger the penalty that is applied on failure will be. It seems less ideal though, because it could disincentivize cheap but reliable channels on high traffic links.\n> \n> The effort required to implement some form of a `highly_available` flag seem limited and it may help to get payment success rates up. Interested to hear your thoughts.\n> \n> Joost\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev", "summary": "Matt argues against the idea of signaling for high availability (HA) nodes on the Lightning Network, stating that nodes should measure the network and route payments reliably over an unreliable network. He believes that relying on HA nodes creates centralization pressure and that lightning is already efficient at scale."}, {"author": "Christian Decker", "date": "2023-02-13T15:32:44", "message_text_only": "Hi Matt,\nHi Joost,\n\nlet me chime in here, since we seem to be slowly reinventing all the\nresearch on reputation systems that is already out there. First of all\nlet me say that I am personally not a fan of reputation systems in\ngeneral, just to get my own biases out of the way, now on to the why :-)\n\nReputation systems are great when they work, but they are horrible to\nget right, and certainly the patchworky approach we see being proposed\ntoday will end up with a system that is easy to exploit and hard to\nunderstand. The last time I encountered this kind of scenario was during\nmy work on Bittorrent, where the often theorized tit-for-tat approach\nfailed spectacularly, and leeching (i.e., not contributing to other\npeople's download) is rampant even today (BT only works because a few\ndon't care about their upload bandwidth).\n\nFirst of all let's see what types of reputation system exist (and yes,\nthis is my very informal categorization):\n\n - First hand experience\n - Inferred experience\n - Hearsay\n\nThe first two are likely the setup we all are comfortable with: we ourselves\nexperienced something, and make some decisions based on that\nexperience. This is probably what we're all doing at the moment: we\nattempt a payment, it fails, we back off for a bit from that channel\nbeing used again. This requires either being able to witness the issue\ndirectly (local peer) or infer from unforgeable error messages (the\nfailing node returns an error, and it can't point the finger at someone\nelse). Notice that this also includes some transitive constructions,\nsuch as the backpressure mechanism we were discussing for ariard's\ncredentials proposal.\n\nIdeally we'd only rely on the first two to make decisions, but here's\nexactly the issue we ran into with Bittorrent: repeat interactions are\ntoo rare. In addition, our local knowledge gets out of date the longer\nwe wait, and a previously failing channel may now be good again, and\nvice-versa. For us to have sufficient knowledge to make good decisions\nwe need to repeatedly interact with the same nodes in the network, and\nsince end-users will be very unlikely to do that, we might end up in a\nsituation were we instinctively fall back to the hearsay method, either\nby sharing our local reputation with peers and then somehow combine that\nwith our own view. To the best of my knowledge such a system has never\nbeen built successfully, and all attempts have ended in a system that\nwas either way too simple or is gameable by rational players.\n\nI also object to the wording of penalizing nodes that haven't been as\nreliable in the past. It's not penalizing them if, based on our local\ninformation, we decide to route over other nodes for a bit. Our goal is\noptimize the payment process, chosing the best possible routes, not\nmaking a judgement on the honesty or reliability of a node. When talking\nabout penalizing we see node operators starting to play stupid games to\navoid that perceived penalty, when in reality they should do their best\nto route as many payments successfully as possible (the negative fees\nfor direct peers \"exhausting\" a balanced flow is one such example of\npremature optimization in that direction imho).\n\nSo I guess what I'm saying is that we need to get away from this\npatchwork mode of building the protocol, and have a much clearer model\nfor a) what we want to achieve, b) how much untrustworthy information we\nwant to rely on, and c) how we protect (and possibly prove security)\nagainst manipulation by rational players. For the last question we at\nleast have one nice feature (for now), namely that the identities are\nsemi-permanent, and so white-washing attacks at least are not free.\n\nAnd after all this rambling, let's get back to the topic at hand: I\ndon't think enshrining the differences of availability in the protocol,\nthus creating two classes of nodes, is a desirable\nfeature. Communicating up-front that I intend to be reliable does\nnothing, and penalizing after the fact isn't worth much due to the\nrepeat interactions issue. It'd be even worse if now we had to rely on a\nthird party to aggregate and track the reliability, in order to get\nenough repeat interactions to build a good model of their liquidity,\nsince we're now back in the hearsay world, and the third party can feed\nus wrong information to maximize their profits.\n\nRegards,\nChristian\n\n\nMatt Corallo <lf-lists at mattcorallo.com> writes:\n> Hi Joost,\n>\n> I\u2019m not sure I agree that lightning is \u201ccapital efficient\u201d (or even close to it), but more generally I don\u2019t see why this needs a signal.\n>\n> If nodes start aggressively preferring routes through nodes that reliably route payments (which I believe lnd already does, in effect, to some large extent), they should do so by measurement, not signaling.\n>\n> In practice, many channels on the network are \u201chigh availability\u201d today, but only in one direction (I.e. they aren\u2019t regularly spliced/rebalanced and are regularly unbalanced). A node strongly preferring a high payment success rate *should* prefer such a channel, but in your scheme would not.\n>\n> This ignores the myriad of \u201cat what threshold do you signal HA\u201d issues, which likely make such a signal DOA, anyway.\n>\n> Finally, I\u2019m very dismayed at this direction in thinking on how ln should work - nodes should be measuring the network and routing over paths that it thinks are reliable for what it wants, *robustly over an unreliable network*. We should absolutely not be expecting the lightning network to be built out of high reliability nodes, that creates strong centralization pressure. To truly meet a \u201chigh availability\u201d threshold, realistically, you\u2019d need to be able to JIT 0conf splice-in, which would drive lightning to actually being a credit network.\n>\n> With reasonable volume, lightning today is very reliable and relatively fast, with few retries required. I don\u2019t think we need to change anything to fix it. :)\n>\n> Matt\n>\n>> On Feb 13, 2023, at 06:46, Joost Jager <joost.jager at gmail.com> wrote:\n>> \n>> \ufeff\n>> Hi,\n>> \n>> For a long time I've held the expectation that eventually payers on the lightning network will become very strict about node performance. That they will require a routing node to operate flawlessly or else apply a hefty penalty such as completely avoiding the node for an extended period of time - multiple weeks. The consequence of this is that routing nodes would need to manage their liquidity meticulously because every failure potentially has a large impact on future routing revenue.\n>> \n>> I think movement in this direction is important to guarantee competitiveness with centralised payment systems and their (at least theoretical) ability to process a payment in the blink of an eye. A lightning wallet trying multiple paths to find one that works doesn't help with this.\n>> \n>> A common argument against strict penalisation is that it would lead to less efficient use of capital. Routing nodes would need to maintain pools of liquidity to guarantee successes all the time. My opinion on this is that lightning is already enormously capital efficient at scale and that it is worth sacrificing a slight part of that efficiency to also achieve the lowest possible latency.\n>> \n>> This brings me to the actual subject of this post. Assuming strict penalisation is good, it may still not be ideal to flip the switch from one day to the other. Routing nodes may not offer the required level of service yet, causing senders to end up with no nodes to choose from.\n>> \n>> One option is to gradually increase the strength of the penalties, so that routing nodes are given time to adapt to the new standards. This does require everyone to move along and leaves no space for cheap routing nodes with less leeway in terms of liquidity.\n>> \n>> Therefore I am proposing another way to go about it: extend the `channel_update` field `channel_flags` with a new bit that the sender can use to signal `highly_available`. \n>> \n>> It's then up to payers to decide how to interpret this flag. One way could be to prefer `highly_available` channels during pathfinding. But if the routing node then returns a failure, a much stronger than normal penalty will be applied. For routing nodes this creates an opportunity to attract more traffic by marking some channels as `highly_available`, but it also comes with the responsibility to deliver.\n>> \n>> Without shadow channels, it is impossible to guarantee liquidity up to the channel capacity. It might make sense for senders to only assume high availability for amounts up to `htlc_maximum_msat`.\n>> \n>> A variation on this scheme that requires no extension of `channel_update` is to signal availability implicitly through routing fees. So the more expensive a channel is, the stronger the penalty that is applied on failure will be. It seems less ideal though, because it could disincentivize cheap but reliable channels on high traffic links.\n>> \n>> The effort required to implement some form of a `highly_available` flag seem limited and it may help to get payment success rates up. Interested to hear your thoughts.\n>> \n>> Joost\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev", "summary": "Reputation systems are difficult to get right and easy to exploit. There are three types: first-hand experience, inferred experience, and hearsay. Repeat interactions are rare, and local knowledge gets out of date. Penalizing nodes is not the goal; optimizing the payment process is."}, {"author": "Matt Corallo", "date": "2023-02-13T16:12:02", "message_text_only": "Thanks Christian,\n\nOn 2/13/23 7:32\u202fAM, Christian Decker wrote:\n> Hi Matt,\n> Hi Joost,\n> \n> let me chime in here, since we seem to be slowly reinventing all the\n> research on reputation systems that is already out there. First of all\n> let me say that I am personally not a fan of reputation systems in\n> general, just to get my own biases out of the way, now on to the why :-)\n> \n> Reputation systems are great when they work, but they are horrible to\n> get right, and certainly the patchworky approach we see being proposed\n> today will end up with a system that is easy to exploit and hard to\n> understand. The last time I encountered this kind of scenario was during\n> my work on Bittorrent, where the often theorized tit-for-tat approach\n> failed spectacularly, and leeching (i.e., not contributing to other\n> people's download) is rampant even today (BT only works because a few\n> don't care about their upload bandwidth).\n> \n> First of all let's see what types of reputation system exist (and yes,\n> this is my very informal categorization):\n> \n>   - First hand experience\n>   - Inferred experience\n>   - Hearsay\n> \n> The first two are likely the setup we all are comfortable with: we ourselves\n> experienced something, and make some decisions based on that\n> experience. This is probably what we're all doing at the moment: we\n> attempt a payment, it fails, we back off for a bit from that channel\n> being used again. This requires either being able to witness the issue\n> directly (local peer) or infer from unforgeable error messages (the\n> failing node returns an error, and it can't point the finger at someone\n> else). Notice that this also includes some transitive constructions,\n> such as the backpressure mechanism we were discussing for ariard's\n> credentials proposal.\n> \n> Ideally we'd only rely on the first two to make decisions, but here's\n> exactly the issue we ran into with Bittorrent: repeat interactions are\n> too rare. In addition, our local knowledge gets out of date the longer\n> we wait, and a previously failing channel may now be good again, and\n> vice-versa. For us to have sufficient knowledge to make good decisions\n> we need to repeatedly interact with the same nodes in the network, and\n> since end-users will be very unlikely to do that, we might end up in a\n> situation were we instinctively fall back to the hearsay method, either\n> by sharing our local reputation with peers and then somehow combine that\n> with our own view. To the best of my knowledge such a system has never\n> been built successfully, and all attempts have ended in a system that\n> was either way too simple or is gameable by rational players.\n\nIn lightning we have a trivial solution to this - your wallet vendor/LSP is already extracting a fee \nfrom you for every HTLC routed through it, it has you captive and can set the fee (largely) \narbitrarily (up to you paying on-chain fees to switch LSPs). They can happily tell you their view of \nthe network ~live and you should generally accept it. Its by no means perfect, and there's plenty of \ngames they could play on, eg, your privacy, but its pretty damned good.\n\nIf we care a ton about the risks here, we could have a few altruistic nodes that release similar \ninfo and users can median-filter the data in one way or another to reduce risk.\n\nI just do not buy that this is a difficult problem for the \"end user\" part of the network. For \nlarger nodes its (obviously, and trivially) not a problem either, which leaves the \"middle nodes\" \nstranded without good data but without an LSP they want to use for data. I believe that isn't a \nlarge enough cohort to change the whole network around for, and them asking a few altruistic (let's \nsay, developer?) nodes for scoring data seems more than sufficient.\n\n> I also object to the wording of penalizing nodes that haven't been as\n> reliable in the past. It's not penalizing them if, based on our local\n> information, we decide to route over other nodes for a bit. Our goal is\n> optimize the payment process, chosing the best possible routes, not\n> making a judgement on the honesty or reliability of a node. When talking\n> about penalizing we see node operators starting to play stupid games to\n> avoid that perceived penalty, when in reality they should do their best\n> to route as many payments successfully as possible (the negative fees\n> for direct peers \"exhausting\" a balanced flow is one such example of\n> premature optimization in that direction imho).\n\nYes! Very much yes! I hate this line of thinking.\n\n> So I guess what I'm saying is that we need to get away from this\n> patchwork mode of building the protocol, and have a much clearer model\n> for a) what we want to achieve, b) how much untrustworthy information we\n> want to rely on, and c) how we protect (and possibly prove security)\n> against manipulation by rational players. For the last question we at\n> least have one nice feature (for now), namely that the identities are\n> semi-permanent, and so white-washing attacks at least are not free.\n> \n> And after all this rambling, let's get back to the topic at hand: I\n> don't think enshrining the differences of availability in the protocol,\n> thus creating two classes of nodes, is a desirable\n> feature. Communicating up-front that I intend to be reliable does\n> nothing, and penalizing after the fact isn't worth much due to the\n> repeat interactions issue. It'd be even worse if now we had to rely on a\n> third party to aggregate and track the reliability, in order to get\n> enough repeat interactions to build a good model of their liquidity,\n> since we're now back in the hearsay world, and the third party can feed\n> us wrong information to maximize their profits.\n> \n> Regards,\n> Christian\n> \n> \n> Matt Corallo <lf-lists at mattcorallo.com> writes:\n>> Hi Joost,\n>>\n>> I\u2019m not sure I agree that lightning is \u201ccapital efficient\u201d (or even close to it), but more generally I don\u2019t see why this needs a signal.\n>>\n>> If nodes start aggressively preferring routes through nodes that reliably route payments (which I believe lnd already does, in effect, to some large extent), they should do so by measurement, not signaling.\n>>\n>> In practice, many channels on the network are \u201chigh availability\u201d today, but only in one direction (I.e. they aren\u2019t regularly spliced/rebalanced and are regularly unbalanced). A node strongly preferring a high payment success rate *should* prefer such a channel, but in your scheme would not.\n>>\n>> This ignores the myriad of \u201cat what threshold do you signal HA\u201d issues, which likely make such a signal DOA, anyway.\n>>\n>> Finally, I\u2019m very dismayed at this direction in thinking on how ln should work - nodes should be measuring the network and routing over paths that it thinks are reliable for what it wants, *robustly over an unreliable network*. We should absolutely not be expecting the lightning network to be built out of high reliability nodes, that creates strong centralization pressure. To truly meet a \u201chigh availability\u201d threshold, realistically, you\u2019d need to be able to JIT 0conf splice-in, which would drive lightning to actually being a credit network.\n>>\n>> With reasonable volume, lightning today is very reliable and relatively fast, with few retries required. I don\u2019t think we need to change anything to fix it. :)\n>>\n>> Matt\n>>\n>>> On Feb 13, 2023, at 06:46, Joost Jager <joost.jager at gmail.com> wrote:\n>>>\n>>> \ufeff\n>>> Hi,\n>>>\n>>> For a long time I've held the expectation that eventually payers on the lightning network will become very strict about node performance. That they will require a routing node to operate flawlessly or else apply a hefty penalty such as completely avoiding the node for an extended period of time - multiple weeks. The consequence of this is that routing nodes would need to manage their liquidity meticulously because every failure potentially has a large impact on future routing revenue.\n>>>\n>>> I think movement in this direction is important to guarantee competitiveness with centralised payment systems and their (at least theoretical) ability to process a payment in the blink of an eye. A lightning wallet trying multiple paths to find one that works doesn't help with this.\n>>>\n>>> A common argument against strict penalisation is that it would lead to less efficient use of capital. Routing nodes would need to maintain pools of liquidity to guarantee successes all the time. My opinion on this is that lightning is already enormously capital efficient at scale and that it is worth sacrificing a slight part of that efficiency to also achieve the lowest possible latency.\n>>>\n>>> This brings me to the actual subject of this post. Assuming strict penalisation is good, it may still not be ideal to flip the switch from one day to the other. Routing nodes may not offer the required level of service yet, causing senders to end up with no nodes to choose from.\n>>>\n>>> One option is to gradually increase the strength of the penalties, so that routing nodes are given time to adapt to the new standards. This does require everyone to move along and leaves no space for cheap routing nodes with less leeway in terms of liquidity.\n>>>\n>>> Therefore I am proposing another way to go about it: extend the `channel_update` field `channel_flags` with a new bit that the sender can use to signal `highly_available`.\n>>>\n>>> It's then up to payers to decide how to interpret this flag. One way could be to prefer `highly_available` channels during pathfinding. But if the routing node then returns a failure, a much stronger than normal penalty will be applied. For routing nodes this creates an opportunity to attract more traffic by marking some channels as `highly_available`, but it also comes with the responsibility to deliver.\n>>>\n>>> Without shadow channels, it is impossible to guarantee liquidity up to the channel capacity. It might make sense for senders to only assume high availability for amounts up to `htlc_maximum_msat`.\n>>>\n>>> A variation on this scheme that requires no extension of `channel_update` is to signal availability implicitly through routing fees. So the more expensive a channel is, the stronger the penalty that is applied on failure will be. It seems less ideal though, because it could disincentivize cheap but reliable channels on high traffic links.\n>>>\n>>> The effort required to implement some form of a `highly_available` flag seem limited and it may help to get payment success rates up. Interested to hear your thoughts.\n>>>\n>>> Joost\n>>> _______________________________________________\n>>> Lightning-dev mailing list\n>>> Lightning-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev", "summary": "Reputation systems are difficult to get right and can be easily exploited. Lightning Network's solution is to rely on wallet vendors/LSPs to provide live network views."}, {"author": "ZmnSCPxj", "date": "2023-02-14T03:05:52", "message_text_only": "Good morning all,\n\n> > First of all let's see what types of reputation system exist (and yes,\n> > this is my very informal categorization):\n> > \n> > - First hand experience\n> > - Inferred experience\n> > - Hearsay\n> > \n> > The first two are likely the setup we all are comfortable with: we ourselves\n> > experienced something, and make some decisions based on that\n> > experience. This is probably what we're all doing at the moment: we\n> > attempt a payment, it fails, we back off for a bit from that channel\n> > being used again. This requires either being able to witness the issue\n> > directly (local peer) or infer from unforgeable error messages (the\n> > failing node returns an error, and it can't point the finger at someone\n> > else). Notice that this also includes some transitive constructions,\n> > such as the backpressure mechanism we were discussing for ariard's\n> > credentials proposal.\n> > \n> > Ideally we'd only rely on the first two to make decisions, but here's\n> > exactly the issue we ran into with Bittorrent: repeat interactions are\n> > too rare. In addition, our local knowledge gets out of date the longer\n> > we wait, and a previously failing channel may now be good again, and\n> > vice-versa. For us to have sufficient knowledge to make good decisions\n> > we need to repeatedly interact with the same nodes in the network, and\n> > since end-users will be very unlikely to do that, we might end up in a\n> > situation were we instinctively fall back to the hearsay method, either\n> > by sharing our local reputation with peers and then somehow combine that\n> > with our own view. To the best of my knowledge such a system has never\n> > been built successfully, and all attempts have ended in a system that\n> > was either way too simple or is gameable by rational players.\n> \n> \n> In lightning we have a trivial solution to this - your wallet vendor/LSP is already extracting a fee\n> from you for every HTLC routed through it, it has you captive and can set the fee (largely)\n> arbitrarily (up to you paying on-chain fees to switch LSPs). They can happily tell you their view of\n> the network ~live and you should generally accept it. Its by no means perfect, and there's plenty of\n> games they could play on, eg, your privacy, but its pretty damned good.\n> \n> If we care a ton about the risks here, we could have a few altruistic nodes that release similar\n> info and users can median-filter the data in one way or another to reduce risk.\n> \n> I just do not buy that this is a difficult problem for the \"end user\" part of the network. For\n> larger nodes its (obviously, and trivially) not a problem either, which leaves the \"middle nodes\"\n> stranded without good data but without an LSP they want to use for data. I believe that isn't a\n> large enough cohort to change the whole network around for, and them asking a few altruistic (let's\n> say, developer?) nodes for scoring data seems more than sufficient.\n\nBut this is all ultimately hearsay.\n\nLSPs can be bought out, and developers can go rogue.\nNeither should be trusted if at all possible.\n\nWhich is why I think forwardable peerswaps fixes this: it *creates* paths that allow payment routing, without requiring pervasive monitoring (which is horrible because eventually the network will be large enough that you will never encounter the same node twice if you're a plebeian, and if you're an aristocrat, you have every incentive to lie to the plebeians to solidify your power) of the network.\n\nUltimately the network gets healthier if flows are bidirectional, swaps are essential to bootstrapping from the starting state where there are distinct \"customers\" and \"merchants\", but current one-hop-only peerswaps are too local for the blockchain cost, and multi-hop source-routed swaps have the same issue as standard payments.\nThe advantage of forwardable peerswaps is that it is specifically not source routed --- intermediate nodes make decisions of where to forward, and they are thus incentivized to benefit the network because they benefit themselves.\n\n\nI think it should be a principle of protocol design to embrace a capitalistic mindset, by which I mean: ensuring the rules make \"beneficial for me\" the same as \"beneficial to everyone\".\nCertainly I can take a common knife from my kitchen and stick the pointy end into my neighbor, then take all their belongings, which would be very beneficial to me, but would not be beneficial to everyone, which is why laws against manslaughter and theft exist.\nUltimately, protocol design is the laying down of laws, and the proper function of this lawmaking position is to ensure that \"beneficial for me\" will be something that is \"beneficial to everyone\".\nIndeed, the entire point of having a punitive Poon-Dryja is to ensure that \"beneficial for me\" does not include theft of the channel funds by using old state, and is exemplary of this principle.\n\"Greed is Good\" might not be true, but perhaps: \"We should strive to MAKE Greed Good\".\n\nForwardable peerswaps are beneficial to every participant, and thus beneficial to the network, and thus should be part of the law of Lightning Network protocol.\nIt *creates* high availability of channels and routing, without self-assertions or hearsay; it only requires local reputation (a forwarding node will forward a peerswap only if it knows it can benefit from whichever hop it is forwarding to: the locality avoids the issue where a node may never interact with another node again, since the node has a channel with them and is expected to repeatedly interact with them in the close future, and thus has very good local information).\n\nRegards,\nZmnSCPxj", "summary": "The Lightning Network's reputation system relies on first-hand and inferred experience, but repeat interactions are rare. Wallet vendors can provide live network views, but this is ultimately hearsay. Altruistic nodes could provide scoring data, but LSPs can be bought out."}, {"author": "Matt Corallo", "date": "2023-02-14T21:27:07", "message_text_only": "On 2/13/23 7:05\u202fPM, ZmnSCPxj wrote:\n> Good morning all,\n> \n>>> First of all let's see what types of reputation system exist (and yes,\n>>> this is my very informal categorization):\n>>>\n>>> - First hand experience\n>>> - Inferred experience\n>>> - Hearsay\n>>>\n>>> The first two are likely the setup we all are comfortable with: we ourselves\n>>> experienced something, and make some decisions based on that\n>>> experience. This is probably what we're all doing at the moment: we\n>>> attempt a payment, it fails, we back off for a bit from that channel\n>>> being used again. This requires either being able to witness the issue\n>>> directly (local peer) or infer from unforgeable error messages (the\n>>> failing node returns an error, and it can't point the finger at someone\n>>> else). Notice that this also includes some transitive constructions,\n>>> such as the backpressure mechanism we were discussing for ariard's\n>>> credentials proposal.\n>>>\n>>> Ideally we'd only rely on the first two to make decisions, but here's\n>>> exactly the issue we ran into with Bittorrent: repeat interactions are\n>>> too rare. In addition, our local knowledge gets out of date the longer\n>>> we wait, and a previously failing channel may now be good again, and\n>>> vice-versa. For us to have sufficient knowledge to make good decisions\n>>> we need to repeatedly interact with the same nodes in the network, and\n>>> since end-users will be very unlikely to do that, we might end up in a\n>>> situation were we instinctively fall back to the hearsay method, either\n>>> by sharing our local reputation with peers and then somehow combine that\n>>> with our own view. To the best of my knowledge such a system has never\n>>> been built successfully, and all attempts have ended in a system that\n>>> was either way too simple or is gameable by rational players.\n>>\n>>\n>> In lightning we have a trivial solution to this - your wallet vendor/LSP is already extracting a fee\n>> from you for every HTLC routed through it, it has you captive and can set the fee (largely)\n>> arbitrarily (up to you paying on-chain fees to switch LSPs). They can happily tell you their view of\n>> the network ~live and you should generally accept it. Its by no means perfect, and there's plenty of\n>> games they could play on, eg, your privacy, but its pretty damned good.\n>>\n>> If we care a ton about the risks here, we could have a few altruistic nodes that release similar\n>> info and users can median-filter the data in one way or another to reduce risk.\n>>\n>> I just do not buy that this is a difficult problem for the \"end user\" part of the network. For\n>> larger nodes its (obviously, and trivially) not a problem either, which leaves the \"middle nodes\"\n>> stranded without good data but without an LSP they want to use for data. I believe that isn't a\n>> large enough cohort to change the whole network around for, and them asking a few altruistic (let's\n>> say, developer?) nodes for scoring data seems more than sufficient.\n> \n> But this is all ultimately hearsay.\n> \n> LSPs can be bought out, and developers can go rogue.\n> Neither should be trusted if at all possible.\n\nYou're missing the point - if your LSP wants to \"go rogue\" here, at worst they can charge you more \nfees. They could also do this by...charging you more fees. I'm not really sure what your concern is.\n\n> Which is why I think forwardable peerswaps fixes this: it *creates* paths that allow payment routing, without requiring pervasive monitoring (which is horrible because eventually the network will be large enough that you will never encounter the same node twice if you're a plebeian, and if you're an aristocrat, you have every incentive to lie to the plebeians to solidify your power) of the network.\n\nNo, this is much, much worse for the network. In order to do this \"live\" (i.e. without failing a \npayment) you have to establish trust relationships across the network (i.e. make giving your peers \ncredit a requirement to be considered a \"robust node\" and, thus, receive fee revenue).\n\nDoing splicing/peerswap as a better way to rebalance is, of course, awesome, but it doesn't solve \nthe issue of \"what do I do if I'm just too low on capacity *right now* to clear this HTLC\".\n\nMatt", "summary": "The Lightning Network faces a challenge in creating a reputation system due to the rarity of repeat interactions, but a solution could be for wallet vendors to provide live network views."}, {"author": "Joost Jager", "date": "2023-02-14T11:00:26", "message_text_only": "Hi Christian,\n\n\n> And after all this rambling, let's get back to the topic at hand: I\n> don't think enshrining the differences of availability in the protocol,\n> thus creating two classes of nodes, is a desirable\n> feature.\n\n\nYes so to be clear, the HA signaling is not on the node level but on the\nchannel level. So each node can decide per channel whether they want to\npotentially attract additional traffic at the cost of severe penalties (or\navoidance if you want to use a different wording) if the channel can't be\nused. They can still maintain a set of less reliable channels along side.\n\n\n> Communicating up-front that I intend to be reliable does\n> nothing, and penalizing after the fact isn't worth much due to the\n> repeat interactions issue.\n\n\nI think it is currently quite common for pathfinders to try another channel\nof the same node for the payment at hand. Or re-attempt the same channel\nfor a future payment to the same destination. I understand the repeat\ninteractions issue, but not sure about the extent to which it applies to\nlightning in practice. A think a common pattern for payments in general is\nto pay to the same destinations repeatedly, for example for a daily coffee.\n\n\n> It'd be even worse if now we had to rely on a\n> third party to aggregate and track the reliability, in order to get\n> enough repeat interactions to build a good model of their liquidity,\n> since we're now back in the hearsay world, and the third party can feed\n> us wrong information to maximize their profits.\n>\n\nYes, using 3rd party info seems difficult. As mentioned in my reply to\nMatt, the idea of HA signaling is to make local reliability tracking more\nefficient so that it becomes less likely that senders need to rely on\nexternal aggregators for their view on the network.\n\nJoost\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20230214/a7f355e6/attachment-0001.html>", "summary": "The Lightning Network should not create two classes of nodes based on availability, according to a post on the Lightning-dev mailing list. The post argued that communicating reliability up front would not work, and penalising nodes after the fact would be ineffective. The author also said relying on third-party information to track reliability would be problematic. Instead, the post suggested using high-availability signalling to make local reliability tracking more efficient."}, {"author": "Joost Jager", "date": "2023-02-14T10:34:05", "message_text_only": "Hi Matt,\n\nIf nodes start aggressively preferring routes through nodes that reliably\n> route payments (which I believe lnd already does, in effect, to some large\n> extent), they should do so by measurement, not signaling.\n>\n\nThe signaling is intended as a way to make measurement more efficient. If a\nnode signals that a particular channel is HA and it fails, no other\nmeasurements on that same node need to be taken by the sender. They can\nskip the node altogether for a longer period of time.\n\n\n> In practice, many channels on the network are \u201chigh availability\u201d today,\n> but only in one direction (I.e. they aren\u2019t regularly spliced/rebalanced\n> and are regularly unbalanced). A node strongly preferring a high payment\n> success rate *should* prefer such a channel, but in your scheme would not.\n>\n\nThis shouldn't be a problem, because the HA signaling is also directional.\nEach end can decide independently on whether to add the flag for a\nparticular channel.\n\n\n> This ignores the myriad of \u201cat what threshold do you signal HA\u201d issues,\n> which likely make such a signal DOA, anyway.\n>\n\nI think this is a product of sender preference for HA channels and the\nseverity of the penalty if an HA channel fails. Given this, routing nodes\nwill need to decide whether they can offer a service level that increases\ntheir routing revenue overall if they would signal HA. It is indeed\ndynamic, but I think the market is able to work it out.\n\n\n> Finally, I\u2019m very dismayed at this direction in thinking on how ln should\n> work - nodes should be measuring the network and routing over paths that it\n> thinks are reliable for what it wants, *robustly over an unreliable\n> network*. We should absolutely not be expecting the lightning network to be\n> built out of high reliability nodes, that creates strong centralization\n> pressure. To truly meet a \u201chigh availability\u201d threshold, realistically,\n> you\u2019d need to be able to JIT 0conf splice-in, which would drive lightning\n> to actually being a credit network.\n>\n\nDifferent people can have different opinions about how ln should work, that\nis fine. I see a trade-off between the reliability of the network and the\nbarrier of entry, and I don't think the optimum is on one of the ends of\nthe scale.\n\n\n> With reasonable volume, lightning today is very reliable and relatively\n> fast, with few retries required. I don\u2019t think we need to change anything\n> to fix it. :)\n>\n\nHow can you be sure about this? This isn't publicly visible data.\n\nJoost\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20230214/da656e7a/attachment.html>", "summary": "The Lightning Network should not be built out of high reliability nodes to avoid centralization pressure, according to a developer. Nodes should measure the network and route over paths that are reliable for what they want, robustly over an unreliable network."}, {"author": "Matt Corallo", "date": "2023-02-14T21:38:34", "message_text_only": "On 2/14/23 2:34\u202fAM, Joost Jager wrote:\n> Hi Matt,\n> \n>     If nodes start aggressively preferring routes through nodes that reliably route payments (which\n>     I believe lnd already does, in effect, to some large extent), they should do so by measurement,\n>     not signaling.\n> \n> \n> The signaling is intended as a way to make measurement more efficient. If a node signals that a \n> particular channel is HA and it fails, no other measurements on that same node need to be taken by \n> the sender. They can skip the node altogether for a longer period of time.\n\nBut as a lightning node I don't actually care if a node is binary good/bad. I care about what \nsuccess rate a node has. If you make the decision binary, suddenly in order for a node to be \"good\" \nI *have* to establish a credit relationship with my peers (i.e. support 0conf splicing). I think \nthat is a very, very bad thing to do to the lightning network.\n\nIf someone wants to establish such a relationship with their peers, so be it, but as developers we \nshould strongly avoid adding features which push node operators in that direction, and part of that \nis writing good routing scoring so that we aren't boxing ourselves into some binary good/bad idea of \na node but rather estimating liquidity.\n\nHonestly this just strikes me as developers being too lazy to do things right. If we do things \ncarefully and we are seeing issues then we can consider breaking lightning, but until we give it a \ngood shot, let's not!\n\n>     In practice, many channels on the network are \u201chigh availability\u201d today, but only in one\n>     direction (I.e. they aren\u2019t regularly spliced/rebalanced and are regularly unbalanced). A node\n>     strongly preferring a high payment success rate *should* prefer such a channel, but in your\n>     scheme would not.\n> \n> \n> This shouldn't be a problem, because the HA signaling is also directional. Each end can decide \n> independently on whether to add the flag for a particular channel.\n\nBut how do you decide to set it without a credit relationship? Do I measure my channel and set the \nbit because the channel is \"usually\" (at what threshold?) saturating in the inbound direction? What \nhappens if this changes for an hour and I get unlucky? Did I just screw myself?\n\n>     This ignores the myriad of \u201cat what threshold do you signal HA\u201d issues, which likely make such a\n>     signal DOA, anyway.\n> \n> \n> I think this is a product of sender preference for HA channels and the severity of the penalty if an \n> HA channel fails. Given this, routing nodes will need to decide whether they can offer a service \n> level that increases their routing revenue overall if they would signal HA. It is indeed dynamic, \n> but I think the market is able to work it out.\n\nI'm afraid this is going to immediately fall into a cargo cult of \"set the bit\" vs \"don't set the \nbit\" and we'll never get useful data out of it. But you may be right.\n\n>     Finally, I\u2019m very dismayed at this direction in thinking on how ln should work - nodes should be\n>     measuring the network and routing over paths that it thinks are reliable for what it wants,\n>     *robustly over an unreliable network*. We should absolutely not be expecting the lightning\n>     network to be built out of high reliability nodes, that creates strong centralization pressure.\n>     To truly meet a \u201chigh availability\u201d threshold, realistically, you\u2019d need to be able to JIT 0conf\n>     splice-in, which would drive lightning to actually being a credit network.\n> \n> \n> Different people can have different opinions about how ln should work, that is fine. I see a \n> trade-off between the reliability of the network and the barrier of entry, and I don't think the \n> optimum is on one of the ends of the scale.\n\nMy point wasn't that lightning should be unreliable, but rather a reliable network build on \nunreliable hops. I'm very confident we can accomplish that without falling back to forcing nodes to \nestablish credit to meet \"reliability requirements\".\n\n>     With reasonable volume, lightning today is very reliable and relatively fast, with few retries\n>     required. I don\u2019t think we need to change anything to fix it. :)\n> \n> \n> How can you be sure about this? This isn't publicly visible data.\n\nSure it is! https://river.com/learn/files/river-lightning-report.pdf\n\nI'm also quite confident we can do substantially better than this.\n\nMatt", "summary": "Developers are discussing the use of signaling to make lightning network routing more efficient, but some argue that it could lead to a binary good/bad classification of nodes and push node operators towards establishing credit relationships with peers. They suggest that routing scoring should be used instead to estimate liquidity. There are also concerns about the threshold for signaling high availability and the potential for a cargo cult mentality."}, {"author": "Joost Jager", "date": "2023-02-15T07:36:26", "message_text_only": ">\n> But how do you decide to set it without a credit relationship? Do I\n> measure my channel and set the\n>\nbit because the channel is \"usually\" (at what threshold?) saturating in the\n> inbound direction? What\n> happens if this changes for an hour and I get unlucky? Did I just screw\n> myself?\n>\n\nAs a node setting the flag, you'll have to make sure you open new channels,\nrebalance or swap-in in time to maintain outbound liquidity. That's part of\nthe game of running an HA channel.\n\n\n> > How can you be sure about this? This isn't publicly visible data.\n>\n> Sure it is! https://river.com/learn/files/river-lightning-report.pdf\n\n\nSome operators publish data, but are the experiences of one of the most\nwell connected (custodial) nodes representative for the network as a whole\nwhen evaluating payment success rates? In the end you can't know what's\nhappening on the lightning network.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20230215/1fb6f297/attachment.html>", "summary": "Running a high availability channel on the Lightning Network requires maintaining outbound liquidity by opening new channels, rebalancing, or swapping in. Publicly visible data may not be representative of the network as a whole."}, {"author": "Matt Corallo", "date": "2023-02-16T00:42:54", "message_text_only": "On 2/14/23 11:36\u202fPM, Joost Jager wrote:\n>     But how do you decide to set it without a credit relationship? Do I measure my channel and set the\n> \n>     bit because the channel is \"usually\" (at what threshold?) saturating in the inbound direction? What\n>     happens if this changes for an hour and I get unlucky? Did I just screw myself?\n> \n> \n> As a node setting the flag, you'll have to make sure you open new channels, rebalance or swap-in in \n> time to maintain outbound liquidity. That's part of the game of running an HA channel.\n\nDefine \"in time\" in a way that results in senders not punishing you for not meeting your \"HA \nguarantees\" due to a large flow. I don't buy that this results in anything other than pressure to \nadd credit.\n\n>      > How can you be sure about this? This isn't publicly visible data.\n> \n>     Sure it is! https://river.com/learn/files/river-lightning-report.pdf\n>     <https://river.com/learn/files/river-lightning-report.pdf>\n> \n> \n> Some operators publish data, but are the experiences of one of the most well connected (custodial) \n> nodes representative for the network as a whole when evaluating payment success rates? In the end \n> you can't know what's happening on the lightning network.\n\nRight, that was my above point about fetching scoring data - there's three relevant \"buckets\" of \nnodes, I think - (a) large nodes sending lots of payments, like the above, (b) \"client nodes\" that \njust connect to an LSP or two, (c) nodes that route some but don't send a lot of payments (but do \nsend *some* payments), and may have lots or not very many channels.\n\n(a) I think we're getting there, and we don't need to add anything extra for this use-case beyond \nthe network maturing and improving our scoring algorithms.\n(b) I think is trivially solved by downloading the data from a node in category (a), presumably the \nLSP(s) in question (see other branch of this thread)\n(c) is trickier, but I think the same solution of just fetching semi-trusted data here more than \nsufficies. For most routing nodes that don't send a lot of payments we're talking about a very small \namount of payments, so trusting a third-party for scoring data seems reasonable.\n\nOnce we do that, everyone gets a similar experience as the River report :).\n\nMatt", "summary": "Running a highly available (HA) channel on the Lightning Network requires maintaining outbound liquidity by opening new channels, rebalancing, or swapping-in in time. Fetching semi-trusted data from large nodes, LSPs, and third-party scoring data can help evaluate payment success rates."}, {"author": "Antoine Riard", "date": "2023-02-17T01:26:32", "message_text_only": "Yeah definitely looking forward to talk more about highly available\nlightning channels. During next LN channel jamming meetup! .\n\nLe jeu. 16 f\u00e9vr. 2023 \u00e0 00:43, Matt Corallo <lf-lists at mattcorallo.com> a\n\u00e9crit :\n\n>\n>\n> On 2/14/23 11:36\u202fPM, Joost Jager wrote:\n> >     But how do you decide to set it without a credit relationship? Do I\n> measure my channel and set the\n> >\n> >     bit because the channel is \"usually\" (at what threshold?) saturating\n> in the inbound direction? What\n> >     happens if this changes for an hour and I get unlucky? Did I just\n> screw myself?\n> >\n> >\n> > As a node setting the flag, you'll have to make sure you open new\n> channels, rebalance or swap-in in\n> > time to maintain outbound liquidity. That's part of the game of running\n> an HA channel.\n>\n> Define \"in time\" in a way that results in senders not punishing you for\n> not meeting your \"HA\n> guarantees\" due to a large flow. I don't buy that this results in anything\n> other than pressure to\n> add credit.\n>\n> >      > How can you be sure about this? This isn't publicly visible data.\n> >\n> >     Sure it is! https://river.com/learn/files/river-lightning-report.pdf\n> >     <https://river.com/learn/files/river-lightning-report.pdf>\n> >\n> >\n> > Some operators publish data, but are the experiences of one of the most\n> well connected (custodial)\n> > nodes representative for the network as a whole when evaluating payment\n> success rates? In the end\n> > you can't know what's happening on the lightning network.\n>\n> Right, that was my above point about fetching scoring data - there's three\n> relevant \"buckets\" of\n> nodes, I think - (a) large nodes sending lots of payments, like the above,\n> (b) \"client nodes\" that\n> just connect to an LSP or two, (c) nodes that route some but don't send a\n> lot of payments (but do\n> send *some* payments), and may have lots or not very many channels.\n>\n> (a) I think we're getting there, and we don't need to add anything extra\n> for this use-case beyond\n> the network maturing and improving our scoring algorithms.\n> (b) I think is trivially solved by downloading the data from a node in\n> category (a), presumably the\n> LSP(s) in question (see other branch of this thread)\n> (c) is trickier, but I think the same solution of just fetching\n> semi-trusted data here more than\n> sufficies. For most routing nodes that don't send a lot of payments we're\n> talking about a very small\n> amount of payments, so trusting a third-party for scoring data seems\n> reasonable.\n>\n> Once we do that, everyone gets a similar experience as the River report :).\n>\n> Matt\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20230217/a4c356bf/attachment.html>", "summary": "Matt Corallo and Joost Jager discuss the challenges of maintaining outbound liquidity in highly available lightning channels, and the need for reliable scoring data."}, {"author": "Joost Jager", "date": "2023-02-17T10:47:51", "message_text_only": ">\n> Right, that was my above point about fetching scoring data - there's three\n> relevant \"buckets\" of\n> nodes, I think - (a) large nodes sending lots of payments, like the above,\n> (b) \"client nodes\" that\n> just connect to an LSP or two, (c) nodes that route some but don't send a\n> lot of payments (but do\n> send *some* payments), and may have lots or not very many channels.\n>\n> (a) I think we're getting there, and we don't need to add anything extra\n> for this use-case beyond\n> the network maturing and improving our scoring algorithms.\n> (b) I think is trivially solved by downloading the data from a node in\n> category (a), presumably the\n> LSP(s) in question (see other branch of this thread)\n> (c) is trickier, but I think the same solution of just fetching\n> semi-trusted data here more than\n> sufficies. For most routing nodes that don't send a lot of payments we're\n> talking about a very small\n> amount of payments, so trusting a third-party for scoring data seems\n> reasonable.\n>\n\nI see that in your view all nodes will either be large nodes themselves, or\nbe downloading scoring data from large nodes. I'd argue that that is more\nof a move towards centralisation than the `ha` flag is. The flag at least\nallows small nodes to build up their view of the network in an efficient\nand independently manner.\n\nJoost\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20230217/d0b4ac41/attachment.html>", "summary": "The author suggests that relying on large nodes for scoring data could lead to centralization, and argues that the `ha` flag allows small nodes to build their own view of the network."}, {"author": "Antoine Riard", "date": "2023-02-17T11:19:01", "message_text_only": "As long as protocol development and design is done neutrally, I'm all fine!\n\n\nLe ven. 17 f\u00e9vr. 2023 \u00e0 10:48, Joost Jager <joost.jager at gmail.com> a \u00e9crit :\n\n> Right, that was my above point about fetching scoring data - there's three\n>> relevant \"buckets\" of\n>> nodes, I think - (a) large nodes sending lots of payments, like the\n>> above, (b) \"client nodes\" that\n>> just connect to an LSP or two, (c) nodes that route some but don't send a\n>> lot of payments (but do\n>> send *some* payments), and may have lots or not very many channels.\n>>\n>> (a) I think we're getting there, and we don't need to add anything extra\n>> for this use-case beyond\n>> the network maturing and improving our scoring algorithms.\n>> (b) I think is trivially solved by downloading the data from a node in\n>> category (a), presumably the\n>> LSP(s) in question (see other branch of this thread)\n>> (c) is trickier, but I think the same solution of just fetching\n>> semi-trusted data here more than\n>> sufficies. For most routing nodes that don't send a lot of payments we're\n>> talking about a very small\n>> amount of payments, so trusting a third-party for scoring data seems\n>> reasonable.\n>>\n>\n> I see that in your view all nodes will either be large nodes themselves,\n> or be downloading scoring data from large nodes. I'd argue that that is\n> more of a move towards centralisation than the `ha` flag is. The flag at\n> least allows small nodes to build up their view of the network in an\n> efficient and independently manner.\n>\n> Joost\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20230217/36b81f39/attachment-0001.html>", "summary": "The discussion is about the development and design of protocols, and the need for neutral approaches to ensure decentralization."}, {"author": "Antoine Riard", "date": "2023-02-14T21:42:24", "message_text_only": "Hi Joost,\n\n> For a long time I've held the expectation that eventually payers on the\nlightning network will become very strict about node performance. That they\nwill > require a routing node to operate flawlessly or else apply a hefty\npenalty such as completely avoiding the node for an extended period of time\n- multiple > weeks. The consequence of this is that routing nodes would\nneed to manage their liquidity meticulously because every failure\npotentially has a large\n> impact on future routing revenue.\n\nI think the performance question depends on the type of payment flows\nconsidered. If you're an\nend-user sending a payment to your local Starbucks for coffee, here fast\npayment sounds the end-goal.\nIf you're doing remittance payment, cheap fees might be favored, and in\nfunction of those flows you're\nprobably not going to select the same \"performant\" routing nodes. I think\nadding latency as a criteria for\npathfinding construction has already been mentioned in the past for LDK [0].\n\n> I think movement in this direction is important to guarantee\ncompetitiveness with centralised payment systems and their (at least\ntheoretical) ability to\n> process a payment in the blink of an eye. A lightning wallet trying\nmultiple paths to find one that works doesn't help with this.\n\nOr there is the direction to build forward-error-correction code on top of\nMPP, like in traditional\nnetworking [1]. The rough idea, you send more payment shards than the\nrequested sum, and then\nyou reveal the payment secrets to the receiver after an onion interactivity\nround to finalize payment.\n\n> A common argument against strict penalisation is that it would lead to\nless efficient use of capital. Routing nodes would need to maintain pools of\n> liquidity to guarantee successes all the time. My opinion on this is that\nlightning is already enormously capital efficient at scale and that it is\nworth\n> sacrificing a slight part of that efficiency to also achieve the lowest\npossible latency.\n\nAt the end of the day, we add more signal channels between HTLC senders and\nthe routing\nnodes offering capital liquidity, if the signal mechanisms are efficient, I\nthink they should lead\nto better allocation of the capital. So yes, I think more liquidity might\nbe used by routing nodes\nto serve finely tailored HTLC requests by senders, however this liquidity\nshould be rewarded\nby higher routing fees.\n\n> This brings me to the actual subject of this post. Assuming strict\npenalisation is good, it may still not be ideal to flip the switch from one\nday to the other. > Routing nodes may not offer the required level of\nservice yet, causing senders to end up with no nodes to choose from.\n\n> One option is to gradually increase the strength of the penalties, so\nthat routing nodes are given time to adapt to the new standards. This does\nrequire > everyone to move along and leaves no space for cheap routing\nnodes with less leeway in terms of liquidity.\n\nI think if we have lessons to learn on policy rules design and deployment\non the base-layer\n(the full-rbf saga), it's to be careful in the initial set of rules, and\nhow we ensure smooth\nupgradeability, from one version to another. Otherwise the re-deployment\ncost towards\nthe new version might incentive the old routing node to stay on the\nnon-optimal versions,\nand as we have historical buckets in routing algorithms, or preference for\nolder channels,\nthis might lead the end-user to pay higher fees, than they could access to.\n\n> Therefore I am proposing another way to go about it: extend the\n`channel_update` field `channel_flags` with a new bit that the sender can\nuse to signal > `highly_available`.\n\n> It's then up to payers to decide how to interpret this flag. One way\ncould be to prefer `highly_available` channels during pathfinding. But if\nthe routing\n> node then returns a failure, a much stronger than normal penalty will be\napplied. For routing nodes this creates an opportunity to attract more\ntraffic by > marking some channels as `highly_available`, but it also comes\nwith the responsibility to deliver.\n\nThis is where the open question lies to me - \"highly available\" can be\ndefined with multiple\nsenses, like fault-tolerance, latency processing, equilibrated liquidity.\nAnd a routing node might\nnot be able to optimize its architecture for the same end-goal (e.g more\nwatchtower on remote\nhost probably increases the latency processing).\n\n> Without shadow channels, it is impossible to guarantee liquidity up to\nthe channel capacity. It might make sense for senders to only assume high\n> availability for amounts up to `htlc_maximum_msat`.\n\nAs a note, I think \"senders assumption\" should be well-documented,\notherwise there will be\nperformance discrepancies between node implementations or even versions.\nE.g, an upgraded\nsender penalizing a node for the lack of shadow/parallel channels\nfulfilling HTLC amounts up to\n`htlc_maximum_msat`.\n\n> A variation on this scheme that requires no extension of `channel_update`\nis to signal availability implicitly through routing fees. So the more\nexpensive > a channel is, the stronger the penalty that is applied on\nfailure will be. It seems less ideal though, because it\ncould disincentivize cheap but reliable\n> channels on high traffic links.\n\n> The effort required to implement some form of a `highly_available` flag\nseem limited and it may help to get payment success rates up. Interested to\n> hear your thoughts.\n\nI think signal availability should be explicit rather than implicit. Even\nif it's coming with more\ngossip bandwidth data consumed. I would say for bandwidth performance\nmanagement, relying\non new gossip messages, where they can be filtered in function of the level\nof services required\nis interesting.\n\nBest,\nAntoine\n\n[0] https://github.com/lightningdevkit/rust-lightning/issues/1647\n[1] https://www.rfc-editor.org/rfc/rfc6363.html\n\n\n\n\nLe lun. 13 f\u00e9vr. 2023 \u00e0 11:46, Joost Jager <joost.jager at gmail.com> a \u00e9crit :\n\n> Hi,\n>\n> For a long time I've held the expectation that eventually payers on the\n> lightning network will become very strict about node performance. That they\n> will require a routing node to operate flawlessly or else apply a hefty\n> penalty such as completely avoiding the node for an extended period of time\n> - multiple weeks. The consequence of this is that routing nodes would need\n> to manage their liquidity meticulously because every failure potentially\n> has a large impact on future routing revenue.\n>\n> I think movement in this direction is important to guarantee\n> competitiveness with centralised payment systems and their (at least\n> theoretical) ability to process a payment in the blink of an eye. A\n> lightning wallet trying multiple paths to find one that works doesn't help\n> with this.\n>\n> A common argument against strict penalisation is that it would lead to\n> less efficient use of capital. Routing nodes would need to maintain pools\n> of liquidity to guarantee successes all the time. My opinion on this is\n> that lightning is already enormously capital efficient at scale and that it\n> is worth sacrificing a slight part of that efficiency to also achieve the\n> lowest possible latency.\n>\n> This brings me to the actual subject of this post. Assuming strict\n> penalisation is good, it may still not be ideal to flip the switch from one\n> day to the other. Routing nodes may not offer the required level of service\n> yet, causing senders to end up with no nodes to choose from.\n>\n> One option is to gradually increase the strength of the penalties, so that\n> routing nodes are given time to adapt to the new standards. This does\n> require everyone to move along and leaves no space for cheap routing nodes\n> with less leeway in terms of liquidity.\n>\n> Therefore I am proposing another way to go about it: extend the\n> `channel_update` field `channel_flags` with a new bit that the sender can\n> use to signal `highly_available`.\n>\n> It's then up to payers to decide how to interpret this flag. One way could\n> be to prefer `highly_available` channels during pathfinding. But if the\n> routing node then returns a failure, a much stronger than normal penalty\n> will be applied. For routing nodes this creates an opportunity to attract\n> more traffic by marking some channels as `highly_available`, but it also\n> comes with the responsibility to deliver.\n>\n> Without shadow channels, it is impossible to guarantee liquidity up to the\n> channel capacity. It might make sense for senders to only assume high\n> availability for amounts up to `htlc_maximum_msat`.\n>\n> A variation on this scheme that requires no extension of `channel_update`\n> is to signal availability implicitly through routing fees. So the more\n> expensive a channel is, the stronger the penalty that is applied on failure\n> will be. It seems less ideal though, because it could disincentivize cheap\n> but reliable channels on high traffic links.\n>\n> The effort required to implement some form of a `highly_available` flag\n> seem limited and it may help to get payment success rates up. Interested to\n> hear your thoughts.\n>\n> Joost\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20230214/893867dd/attachment-0001.html>", "summary": "The Lightning Network may require routing nodes to operate flawlessly or face penalties, potentially impacting future routing revenue. Gradual implementation of strict penalties may be necessary for routing nodes to adapt."}, {"author": "Matt Corallo", "date": "2023-02-15T04:44:32", "message_text_only": "On 2/14/23 1:42\u202fPM, Antoine Riard wrote:\n> Hi Joost,\n> \n>  > I think movement in this direction is important to guarantee competitiveness with centralised \n> payment systems and their (at least theoretical) ability to\n>  > process a payment in the blink of an eye. A lightning wallet trying multiple paths to find one \n> that works doesn't help with this.\n> \n> Or there is the direction to build forward-error-correction code on top of MPP, like in traditional\n> networking [1]. The rough idea, you send more payment shards than the requested sum, and then\n> you reveal the payment secrets to the receiver after an onion interactivity round to finalize payment.\n\nAh, thank you for bringing this up! I'd thought about it and then forgot to mention it in this thread.\n\nI think this is very important to highlight as we talk about \"building a reliable lightning network \nout of unreliable nodes\" - this is an *incredibly* powerful feature for this.\n\nWhile its much less capital-effecient, the ability to over-commit upfront and then only allow the \nrecipient to claim a portion of the total committed funds would substantially reduce the impact of \nfailed HTLCs on payment latency. Of course the extra round-trip to request the \"unlock keys\" for the \ncorrect set of HTLCs adds a chunk to total latency, so senders will have to be careful about \ndeciding when to do this or not.\n\nStill, now that we have onion messages, we should do (well, try) this! Its not super complicated to \nimplement (like everything it seems, the obvious implementation forgoes proof-of-payment, and like \neverything the obvious solution is PTLCs, I think). Its not clear to me how we get good data from \ntrials, though, we'd need a sufficient set of the network to support this that we could actually \ntest it, which is hard to get for a test.\n\nMaybe someone (anyone?) wants to do some experiments doing simulations using real probing success \nrates to figure out how successful this would be and propose a concrete sender strategy that would \nimprove success rates.\n\nMatt", "summary": "The Lightning Network could use forward-error-correction code on top of MPP to send more payment shards than the requested sum, reducing the impact of failed HTLCs on payment latency."}, {"author": "Joost Jager", "date": "2023-02-15T10:56:23", "message_text_only": ">\n> I think the performance question depends on the type of payment flows\n> considered. If you're an\n> end-user sending a payment to your local Starbucks for coffee, here fast\n> payment sounds the end-goal.\n> If you're doing remittance payment, cheap fees might be favored, and in\n> function of those flows you're\n> probably not going to select the same \"performant\" routing nodes. I think\n> adding latency as a criteria for\n> pathfinding construction has already been mentioned in the past for LDK\n> [0].\n>\n\nMy hopes are that eventually lightning nodes can run so efficient that in\npractice there is no real trade-off anymore between cost and speed. But of\ncourse hard to say how that's going to play out. I am all for adding\nlatency as an input to pathfinding. Attributable errors should help with\nthat too.\n\n\n> Or there is the direction to build forward-error-correction code on top of\n> MPP, like in traditional\n> networking [1]. The rough idea, you send more payment shards than the\n> requested sum, and then\n> you reveal the payment secrets to the receiver after an onion\n> interactivity round to finalize payment.\n>\n\nThis is not very different from payment pre-probing is it? So try a larger\nset of possible routes simultaneously and when one proves to be open, send\nthe real payment across that route. Of course a balance may have shifted in\nthe mean time, but seems unlikely enough to prevent the approach from being\nusable. The obvious downside is that the user needs more total liquidity to\nhave multiple htlcs outstanding at the same time. Nevertheless an\ninteresting way to reduce payment latency.\n\n\n> At the end of the day, we add more signal channels between HTLC senders\n> and the routing\n> nodes offering capital liquidity, if the signal mechanisms are efficient,\n> I think they should lead\n> to better allocation of the capital. So yes, I think more liquidity might\n> be used by routing nodes\n> to serve finely tailored HTLC requests by senders, however this liquidity\n> should be rewarded\n> by higher routing fees.\n>\n\nThis is indeed part of the idea. By signalling HA, you may not only attract\nmore traffic, but also be able to command a higher fee.\n\n\n> I think if we have lessons to learn on policy rules design and deployment\n> on the base-layer\n> (the full-rbf saga), it's to be careful in the initial set of rules, and\n> how we ensure smooth\n> upgradeability, from one version to another. Otherwise the re-deployment\n> cost towards\n> the new version might incentive the old routing node to stay on the\n> non-optimal versions,\n> and as we have historical buckets in routing algorithms, or preference for\n> older channels,\n> this might lead the end-user to pay higher fees, than they could access to.\n>\n\nI see the parallel, but also it seems that we have this situation already\ntoday on lightning. Senders apply penalties and routing nodes need to make\nassumptions about how they are penalised. Perhaps more explicit signalling\ncan actually help to reduce the degree of uncertainty as to how a routing\nnodes is supposed to perform to keep senders happy?\n\n\n> This is where the open question lies to me - \"highly available\" can be\n> defined with multiple\n> senses, like fault-tolerance, latency processing, equilibrated liquidity.\n> And a routing node might\n> not be able to optimize its architecture for the same end-goal (e.g more\n> watchtower on remote\n> host probably increases the latency processing).\n>\n\nYes, good point. So maybe a few more bits to signal what a sender may\nexpect from a channel exactly?\n\n\n> > Without shadow channels, it is impossible to guarantee liquidity up to\n> the channel capacity. It might make sense for senders to only assume high\n> > availability for amounts up to `htlc_maximum_msat`.\n>\n> As a note, I think \"senders assumption\" should be well-documented,\n> otherwise there will be\n> performance discrepancies between node implementations or even versions.\n> E.g, an upgraded\n> sender penalizing a node for the lack of shadow/parallel channels\n> fulfilling HTLC amounts up to\n> `htlc_maximum_msat`.\n>\n\nWell documented, or maybe even explicit in the name of the feature bit. For\nexample `htlc_max_guaranteed`.\n\n\n> I think signal availability should be explicit rather than implicit. Even\n> if it's coming with more\n> gossip bandwidth data consumed. I would say for bandwidth performance\n> management, relying\n> on new gossip messages, where they can be filtered in function of the\n> level of services required\n> is interesting.\n>\n\nIn terms of implementation, I think this kind of signalling is easier as an\nextension of `channel_update`, but it can probably work as a separate\nmessage too.\n\nJoost\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20230215/c17c1304/attachment-0001.html>", "summary": "Lightning network performance depends on the type of payment flows, with fast payment being favored for end-users and cheap fees for remittance payments. Adding latency as a criteria for pathfinding construction and using forward-error-correction code on top of MPP are potential solutions. More liquidity may be used by routing nodes to serve tailored HTLC requests, but this should be rewarded by higher routing fees. Careful policy rules design and upgradeability are necessary to prevent higher fees for end-users."}], "thread_summary": {"title": "Highly Available Lightning Channels", "categories": ["Lightning-dev"], "authors": ["Joost Jager", "Antoine Riard", "ZmnSCPxj", "Matt Corallo", "Christian Decker"], "messages_count": 17, "total_messages_chars_count": 73274, "convo_summary": "Lightning Network's reputation system and the challenges of maintaining outbound liquidity in highly available channels. Some developers suggest using high-availability signaling to make local reliability tracking more efficient, while others argue that relying on wallet vendors to provide live network views is a better solution. There are concerns about the potential for centralization and the need for reliable scoring data."}}, {"title": "[Lightning-dev] Local Reputation to Mitigate Slow Jamming", "thread_messages": [{"author": "Clara Shikhelman", "date": "2023-02-16T21:28:44", "message_text_only": "Hi List,\n\nWe\u2019re writing to seek early feedback on a draft for a neighbour reputation\nsetting recommendation as a jamming mitigation. The main idea is that\nallowing full access to liquidity and slots in a channel can result in\njamming. To prevent this, we allow full access only to neighbours that\nforward HTLC that resolve quickly and generate more profit than the damage\nthey can potentially create.\n\nThe full suggested jamming mitigation solution includes upfront fees\ntogether with reputation, see [1] for details.\n\nIn the previous episodes:\n\nAs presented here [1], we suggest a two part jamming mitigation strategy.\nReputation-based forwarding is aimed to solve \u201cslow jamming\u201d, where the\njamming transaction takes a long time to resolve.\n\nThe main idea is that each node gives a binary reputation to its neighbour.\nEach channel has a quota of liquidity and slots (say 50% of the channel\nsize and 50% of the slots in the channel) dedicated to transactions coming\nfrom neighbours with reputation 0, or for transactions coming from\nneighbours with reputation 1 that were not endorsed by the neighbour.\n\nFor example, when Alice asks Bob to forward to Charlie then:\n\nIf (Alice has reputation 1 with Bob) and (Alice endorses transaction):\n\nForward and endorse\n\nElse:\n\nIf (amount < available liquidity quota) and (available slots in quota>0):\n\nForward HTLC without endorsing\n\nReduce available liquidity and slots\n\nElse:\n\nReject\n\nReputation:\n\nThe question we discuss here is how does Alice gain \u201cgood\u201d reputation\n(i.e., a score of 1). Alice starts at 0, and she gains and keeps her good\nreputation of 1 by continuously paying more fees to Bob than the damage she\ncan inflict.\n\nThe 3 main parameters for reputation that each node operator picks are S,L and\nM. Our recommendations are as follows:\n\n   -\n\n   S should be chosen as the maximum time an HTLC can be unresolved in any\n   of Bob\u2019s channels.\n   -\n\n   M is the revenue generated by Bob\u2019s node in the time S, representing the\n   damage Alice could inflict.\n   -\n\n   L is the time in which Alice should generate M revenue for Bob for her\n   to have a good reputation of 1. We suggest L=10S.\n\n\nAlice has reputation 1 if, in the last L seconds, she has forwarded\npayments that generated M satoshi in fees.\n\nAs an example:\n\n   -\n\n   Bob has a maximum CLTV delta of 2 weeks [2]\n   -\n\n   Over the last 2 weeks, he has earned 0.5 BTC in routing fees\n   -\n\n   Alice will be considered to have good reputation if she has forwarded\n   0.5 BTC of routing revenue to Bob over the last 20 weeks\n\n\nFormally:\n\nLet t be the current time, and let S and L be constants.\n\nM is calculated to be the revenue of Bob in time [t-S,t]. The revenue of\nBob is the sum of fees from transactions forwarded by any neighbour besides\nAlice + any payments received by Bob. Note that Bob can choose to also take\ninto account utility gained from sending payments or anything of value to\nthe node operator.\n\nAlice has reputation 1 if in the time [t-L,t] she has forwarded HTLCs that\npaid M in normalized fees.\n\nWe normalize fees by resolution time to reward payments that resolve\nquickly and discount slow resolving payments. Here we assume 10 seconds is\nthe \u201cnormal\u201d resolution time, this number can be bikesheded, and we round\nup to avoid penalizing transactions resolved quicker than the \u201cnormal\u201d.\n\nThe fee from a single transaction is normalized by the time it took for the\nHTLC to resolve, counted in slots of 10 seconds. That is:\n\nNormalized_fee = (fee)/[ceiling(time_to_resolve/10s)]\n\n\n\nSome notes\n\n   1.\n\n   The reputation management happens locally, that is, the only protocol\n   change needed is the ability to signal endorsement as a TLV in\n   UpdateAddHTLC. The various parameters can be selected for various risk\n   preferences.\n   2.\n\n   We currently suggest a binary reputation for simplicity. Having several\n   buckets could be interesting to study, yet we don\u2019t think that the\n   complexity and the possible privacy issues are worth the potential benefits.\n   3.\n\n   For most use cases, having reputation 0 is more than enough. If we send\n   and receive transactions at a low rate, we usually don\u2019t need the full\n   liquidity and slots available in a channel. Reputation mostly comes into\n   play only when a channel is under attack, and then not all transaction are\n   allowed to go through.\n   4.\n\n   Following this thread [3]: it is important to note that we are only\n   giving reputation to our direct neighbours. An advantage of this is that we\n   have repeated interactions with them. In practice, this is also the only\n   clean data we have to use when deciding whether to forward an HTLC or not.\n\n\nBest,\n\nCarla and Clara\n\n\n[1]\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2022-November/003740.html\n[2]\nhttps://github.com/lightningnetwork/lnd/blob/de94a4ea5e81799330a72dfde111817b38565d99/htlcswitch/link.go#L51\n[3]\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2023-February/003842.html\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20230216/e3574a61/attachment-0001.html>", "summary": "A draft for a neighbor reputation setting recommendation as a jamming mitigation has been proposed to prevent jamming by allowing full access only to neighbors that forward HTLC that resolve quickly and generate more profit than the damage they can potentially create. Reputation is gained by continuously paying more fees to the neighbor than the damage they can inflict. The recommendation includes upfront fees together with reputation."}], "thread_summary": {"title": "Local Reputation to Mitigate Slow Jamming", "categories": ["Lightning-dev"], "authors": ["Clara Shikhelman"], "messages_count": 1, "total_messages_chars_count": 5141}}, {"title": "[Lightning-dev] (no subject)", "thread_messages": [{"author": "Victor Umobi", "date": "2023-02-17T11:21:15", "message_text_only": "-- \nVictor Umobi\nSystems/AI Engineer\nMSc Candidate, Cornell University\nMIT Driverless Control Systems\nT: (039)-34951-59614\nE: victorumobi at gmail.com\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20230217/b91a24df/attachment.html>", "summary": "Victor Umobi is a Systems/AI Engineer and MSc Candidate at Cornell University with experience in MIT Driverless Control Systems."}], "thread_summary": {"title": "(no subject)", "categories": ["Lightning-dev"], "authors": ["Victor Umobi"], "messages_count": 1, "total_messages_chars_count": 335}}, {"title": "[Lightning-dev] 3 way channels and 0 conf channels", "thread_messages": [{"author": "l0k1", "date": "2023-02-19T16:37:05", "message_text_only": "Currently,\u00a0LN primarily uses 2 of 2 multisig channel, though I have heard people talk about opening channels in more complex transactions than 2 of 2 multisigs.\n\nThinking through all the topology and number theory aspects of it, I think that if channels were mostly between 3 nodes instead of 2, there could be some big advantages:\n\n- Channels can advertise which node has the lowest balance, helping with balance of channels, and overall liquidity. There would need to be vague enough thresholds to define when to even bother mentioning this, I imagine something like under 10% remaining on one versus the other two would be sufficiently anonymous.\n\n- A node with 3 channels attached to it can be considered as completely connected, and can basically route to 10 different next hops for only 3 opening transaction fees.\n\n- The time cost is basically doubled - two nodes for a channel means request and two messages between the peers to propagate their PSBT channel state, 3 nodes to a channel means 1 request and 6 messages to settle a new payment, which each node in the trio can more or less dispatch two messages at the same time, so, 3 message cycles, or average around 600ms from anywhere to anywhere on the internet.\n\n- The channel's lowest balance could be one-bit boolean value publicly broadcast, meaning that peers selecting hops for a payment route can easily avoid adding to a channel stuck on one side. Pathfinding is a real hassle in the current design of LN. It is hard to navigate in the dark, but if you can sense the distance to the nearest object you can orient easily.\n\nThe magic of tesselation gave us lightning fast 3d raster based 3d modeling, and is based on the infinite tesselation of triangles. Oh, there is 4 points possible, but it really just complicates things. I'm pretty sure that the new Unreal 5 \"nanite\" engine only works with uniform 3 point surfaces, at least, it definitely looks like that based on the 4 color map versions that show the polygons. And anyhow, a \"rectangle\" is just two adjacent triangles, why bother with this extra, extraneous nonsense of calling two polygons with a common axis \"squares\".\n\nThe only other issue that is on my mind lately relating to LN is 0 conf channels. I hadn't thought of \"channels\" as being 2 dimensional, since they represent a midpoint between two other points. But a midpoint is an abstract term, not just a word used for 1D lines but also 2D and nD shapes.\n\nIt seems to me like there could be a negotiation protocol to pre-arrange a not yet mined opening tx for a 3 way lightning channel, that could effectively lock in every party such that it can't wriggle out of the commitment. This just wouldn't be possible with a 2 way. It then wouldn't matter a bit how big the fee was since the parties are already in consensus and have the ability to back out at any moment.\n\n~ l0k1\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: publickey - stalker.loki at protonmail.ch - 0x0AC723EB.asc\nType: application/pgp-keys\nSize: 665 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20230219/0ae8526b/attachment.bin>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 249 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20230219/0ae8526b/attachment.sig>", "summary": "Opening Lightning Network channels between three nodes instead of two could have advantages such as better balance and liquidity, more efficient routing, and easier pathfinding. A negotiation protocol for pre-arranging a not-yet-mined opening transaction for a three-way channel could also be possible."}, {"author": "ZmnSCPxj", "date": "2023-02-23T10:55:14", "message_text_only": "Good morning 10k1,\n\n\n> Currently, LN primarily uses 2 of 2 multisig channel, though I have heard people talk about opening channels in more complex transactions than 2 of 2 multisigs.\n> \n> Thinking through all the topology and number theory aspects of it, I think that if channels were mostly between 3 nodes instead of 2, there could be some big advantages:\n> \n> - Channels can advertise which node has the lowest balance, helping with balance of channels, and overall liquidity. There would need to be vague enough thresholds to define when to even bother mentioning this, I imagine something like under 10% remaining on one versus the other two would be sufficiently anonymous.\n> \n> - A node with 3 channels attached to it can be considered as completely connected, and can basically route to 10 different next hops for only 3 opening transaction fees.\n> \n> - The time cost is basically doubled - two nodes for a channel means request and two messages between the peers to propagate their PSBT channel state, 3 nodes to a channel means 1 request and 6 messages to settle a new payment, which each node in the trio can more or less dispatch two messages at the same time, so, 3 message cycles, or average around 600ms from anywhere to anywhere on the internet.\n> \n> - The channel's lowest balance could be one-bit boolean value publicly broadcast, meaning that peers selecting hops for a payment route can easily avoid adding to a channel stuck on one side. Pathfinding is a real hassle in the current design of LN. It is hard to navigate in the dark, but if you can sense the distance to the nearest object you can orient easily.\n> \n> The magic of tesselation gave us lightning fast 3d raster based 3d modeling, and is based on the infinite tesselation of triangles. Oh, there is 4 points possible, but it really just complicates things. I'm pretty sure that the new Unreal 5 \"nanite\" engine only works with uniform 3 point surfaces, at least, it definitely looks like that based on the 4 color map versions that show the polygons. And anyhow, a \"rectangle\" is just two adjacent triangles, why bother with this extra, extraneous nonsense of calling two polygons with a common axis \"squares\".\n\nThe problem with N > 2 participants in an offchain updateable cryptocurrency system is that if any participant is offline, then the entire system cannot update and the online participants can only back out using an expensive onchain procedure, or wait until the missing participant is online again.\n\n(Use of k-of-n here would allow any quorum of `k` to steal the funds of the `n - k` remaining participants, so security-wise you should really go for n-of-n multisignatures)\n\nIf any arbitrary participant has a probability `P` of being offline at any particular moment, then the probability of the entire system being not updateable is `1 - (1 - P)^N`.\nCrucially, because of real-world considerations, the probability `P` cannot be 0 (though this can be made arbitrarily near 0, but then the cost of doing so would have to be paid by *somebody*).\n\nFurther, if the offchain updateable cryptocurrency system has N participants, if one participant is offline, then N - 1 participants have their funds forcibly locked.\nThus we should multiply the probability of an individual system (N-participant \"channel\") being non-operational with the number of participants who suffer having their funds locked.\n\nWith 2-participant channels, the probability of any single channel being non-operational is at its lowest and the number of affected counterparties is also at its lowest.\n\nThis is why Channel Factories are even a thing.\n\n--\n\nBasically:\n\n* A channel is just a sub-class of offchain-updateable cryptocurrency systems.\n* An offchain-updateable cryptocurrency system must be hosted by another cryptocurrency system, such as a blockchain.\n* An offchain-updateable cryptocurrency system does *not* require activity on its hosting system in order to update its state.\n* An offchain-updateable cryptocurrency system can be hosted inside another offchain-updateable cryptocurrency system.\n  * The outer offchain-updateable cryptocurrency system is hosted directly by a blockchain (a blockchain only requires a physics system with entropy in order to be instantiated).\n  * The inner offchain-updateable cryptocurrency system is hosted on the outer one.\n    The outer hosting system can host multiple inner systems.\n  * The outer offchain-updateable cryptocurrency system is N-of-N where N > 2.\n  * Each inner system is 2-of-2.\n* The outer offchain-updateable cryptocurrency system is the \"channel factory\".\n* Each inner offchain-updateable cryptocurrency system is a \"channel\".\n\n\nThis provides a good balance of availability and efficiency.\n\n* When a participant is offline, the outer system is non-operational, but each inner system deos not care.\n  Each inner system can update without activity on its hosting system, that is the point of offchain protocols.\n  So each 2-of-2 channel can update even if the hosting N-of-N channel factory cannot be updated.\n* The blockchain only hosts a single N-of-N UTXO for multiple channels.\n\n> \n> The only other issue that is on my mind lately relating to LN is 0 conf channels. I hadn't thought of \"channels\" as being 2 dimensional, since they represent a midpoint between two other points. But a midpoint is an abstract term, not just a word used for 1D lines but also 2D and nD shapes.\n> \n> It seems to me like there could be a negotiation protocol to pre-arrange a not yet mined opening tx for a 3 way lightning channel, that could effectively lock in every party such that it can't wriggle out of the commitment. This just wouldn't be possible with a 2 way. It then wouldn't matter a bit how big the fee was since the parties are already in consensus and have the ability to back out at any moment.\n\nYou first need to get the 3 committed, and that is the rub here: it requires onchain activity to commit funds to any N parties, whether N=2 or N=3.\nAnd that onchain activity will always be subject to reversal / double-spending unless confirmed deeply enough that we believe reorganizing the chain would be unlikely.\n\nBasically: what are the inputs to the transaction that backs your 3-participant channel?\nIf any input can be signed by just one participant, then that participant can sign a different transaction spending that input in another way, then bribe a miner to include it in a mined block.\n\nThus, this fixes nothing: in order to get funds committed to the 3 participants, you need *some* confirmed transaction to commit those funds in the first place, and everyone still has to wait for that transaction to confirm, thus you cannot sidestep the confirmation-or-trust requirement.\n\nRegards,\nZmnSCPxj", "summary": "A proposal suggests that Lightning Network channels should be between three nodes instead of two, offering advantages such as better balance and liquidity, and easier routing. However, having more than two participants in an offchain updateable cryptocurrency system can pose security risks."}, {"author": "l0k1", "date": "2023-02-23T11:11:49", "message_text_only": "The use case is related to my work on Indranet, where channel sizes are going to be a lot lower than normal because they only have to transport very small payments (in the hundreds of sats at a time) and maintaining balance of the channels to keep them as much available as possible, the LN instance itself will be part and parcel of the indra service (as well as a node, either neutrino or probably bitcoind). \n\n\nThe semi-automated creation of channels between relays on the network requires 3 an optimal 3 channels so as to optimise for ensuring that messages are at least 3 layers, 4 layers including our special nodes we are calling \"seeds\" that clients open zero conf, one way channels to for, again, very small balances, in the tens of thousands of sats kinda size. Ideally, they are always 4 layers deep and may need to be given hints based on network topology that can be provided by Indra itself.\n\nIt is not a concern if a peer out of the 3 is not available temporarily, the path will just be retried with a different path, the chances of cheating are pretty low and this element of our network's LN mesh is going to be very large - hopefully, and with all nodes having at least 3 peers with channels to them. The payments need to be relatively low latency, and the proportion of nodes that might be congested or fallen offline compared to the whole network is usually going to be very small because relays that are not running are not making the relay operator money - the payments between peers are not going to have fees to further simplify route selection, and because the real income of the relay is not from routing the tiny payments but from users requesting relay services, and premium services that have a higher cost than network-internal.\n\nMy aim in opening up this discussion relates not to the general use case of routing payments but the use of LN as an anti-spam anti-sybil rate limiting scheme, as an adjunct and integral part of a separate peer to peer network system, in this case an anonymising relay system that creates source routed onions, similar to LN payments, but with more advanced structures that enable bidirectional traffic to act as a network tunnel that enables client side anonymity directly, and I'm currently working on adding a scheme for rendezvous-free bidirectional anonymity.\n\nThis kind of application couldbe relevant for many different kinds of monetised p2p networks, Nostr, for example, could use something like this to create a general mechanism for users to anonymously pay other relays to host and deliver content for designated user identities, and would enable this to become a distributed service rather than having users need to depend on the trustworthiness of relays.\n\n~ l0k1\n\n\n------- Original Message -------\nOn Thursday, February 23rd, 2023 at 10:55, ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n\n> Good morning 10k1,\n> \n\n> > Currently, LN primarily uses 2 of 2 multisig channel, though I have heard people talk about opening channels in more complex transactions than 2 of 2 multisigs.\n> > \n\n> > Thinking through all the topology and number theory aspects of it, I think that if channels were mostly between 3 nodes instead of 2, there could be some big advantages:\n> > \n\n> > - Channels can advertise which node has the lowest balance, helping with balance of channels, and overall liquidity. There would need to be vague enough thresholds to define when to even bother mentioning this, I imagine something like under 10% remaining on one versus the other two would be sufficiently anonymous.\n> > \n\n> > - A node with 3 channels attached to it can be considered as completely connected, and can basically route to 10 different next hops for only 3 opening transaction fees.\n> > \n\n> > - The time cost is basically doubled - two nodes for a channel means request and two messages between the peers to propagate their PSBT channel state, 3 nodes to a channel means 1 request and 6 messages to settle a new payment, which each node in the trio can more or less dispatch two messages at the same time, so, 3 message cycles, or average around 600ms from anywhere to anywhere on the internet.\n> > \n\n> > - The channel's lowest balance could be one-bit boolean value publicly broadcast, meaning that peers selecting hops for a payment route can easily avoid adding to a channel stuck on one side. Pathfinding is a real hassle in the current design of LN. It is hard to navigate in the dark, but if you can sense the distance to the nearest object you can orient easily.\n> > \n\n> > The magic of tesselation gave us lightning fast 3d raster based 3d modeling, and is based on the infinite tesselation of triangles. Oh, there is 4 points possible, but it really just complicates things. I'm pretty sure that the new Unreal 5 \"nanite\" engine only works with uniform 3 point surfaces, at least, it definitely looks like that based on the 4 color map versions that show the polygons. And anyhow, a \"rectangle\" is just two adjacent triangles, why bother with this extra, extraneous nonsense of calling two polygons with a common axis \"squares\".\n> \n\n> \n\n> The problem with N > 2 participants in an offchain updateable cryptocurrency system is that if any participant is offline, then the entire system cannot update and the online participants can only back out using an expensive onchain procedure, or wait until the missing participant is online again.\n> \n\n> \n\n> (Use of k-of-n here would allow any quorum of `k` to steal the funds of the `n - k` remaining participants, so security-wise you should really go for n-of-n multisignatures)\n> \n\n> If any arbitrary participant has a probability `P` of being offline at any particular moment, then the probability of the entire system being not updateable is `1 - (1 - P)^N`.\n> Crucially, because of real-world considerations, the probability `P` cannot be 0 (though this can be made arbitrarily near 0, but then the cost of doing so would have to be paid by somebody).\n> \n\n> Further, if the offchain updateable cryptocurrency system has N participants, if one participant is offline, then N - 1 participants have their funds forcibly locked.\n> Thus we should multiply the probability of an individual system (N-participant \"channel\") being non-operational with the number of participants who suffer having their funds locked.\n> \n\n> With 2-participant channels, the probability of any single channel being non-operational is at its lowest and the number of affected counterparties is also at its lowest.\n> \n\n> This is why Channel Factories are even a thing.\n> \n\n> --\n> \n\n> Basically:\n> \n\n> * A channel is just a sub-class of offchain-updateable cryptocurrency systems.\n> * An offchain-updateable cryptocurrency system must be hosted by another cryptocurrency system, such as a blockchain.\n> * An offchain-updateable cryptocurrency system does not require activity on its hosting system in order to update its state.\n> * An offchain-updateable cryptocurrency system can be hosted inside another offchain-updateable cryptocurrency system.\n> * The outer offchain-updateable cryptocurrency system is hosted directly by a blockchain (a blockchain only requires a physics system with entropy in order to be instantiated).\n> * The inner offchain-updateable cryptocurrency system is hosted on the outer one.\n> The outer hosting system can host multiple inner systems.\n> * The outer offchain-updateable cryptocurrency system is N-of-N where N > 2.\n> \n\n> * Each inner system is 2-of-2.\n> * The outer offchain-updateable cryptocurrency system is the \"channel factory\".\n> * Each inner offchain-updateable cryptocurrency system is a \"channel\".\n> \n\n> \n\n> This provides a good balance of availability and efficiency.\n> \n\n> * When a participant is offline, the outer system is non-operational, but each inner system deos not care.\n> Each inner system can update without activity on its hosting system, that is the point of offchain protocols.\n> So each 2-of-2 channel can update even if the hosting N-of-N channel factory cannot be updated.\n> * The blockchain only hosts a single N-of-N UTXO for multiple channels.\n> \n\n> > The only other issue that is on my mind lately relating to LN is 0 conf channels. I hadn't thought of \"channels\" as being 2 dimensional, since they represent a midpoint between two other points. But a midpoint is an abstract term, not just a word used for 1D lines but also 2D and nD shapes.\n> > \n\n> > It seems to me like there could be a negotiation protocol to pre-arrange a not yet mined opening tx for a 3 way lightning channel, that could effectively lock in every party such that it can't wriggle out of the commitment. This just wouldn't be possible with a 2 way. It then wouldn't matter a bit how big the fee was since the parties are already in consensus and have the ability to back out at any moment.\n> \n\n> \n\n> You first need to get the 3 committed, and that is the rub here: it requires onchain activity to commit funds to any N parties, whether N=2 or N=3.\n> And that onchain activity will always be subject to reversal / double-spending unless confirmed deeply enough that we believe reorganizing the chain would be unlikely.\n> \n\n> Basically: what are the inputs to the transaction that backs your 3-participant channel?\n> If any input can be signed by just one participant, then that participant can sign a different transaction spending that input in another way, then bribe a miner to include it in a mined block.\n> \n\n> Thus, this fixes nothing: in order to get funds committed to the 3 participants, you need some confirmed transaction to commit those funds in the first place, and everyone still has to wait for that transaction to confirm, thus you cannot sidestep the confirmation-or-trust requirement.\n> \n\n> Regards,\n> ZmnSCPxj\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: publickey - stalker.loki at protonmail.ch - 0x0AC723EB.asc\nType: application/pgp-keys\nSize: 665 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20230223/5541d843/attachment.bin>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 249 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20230223/5541d843/attachment.sig>", "summary": "Indranet will use low-sized channels to transport small payments and maintain channel balance. The LN instance will be part of the Indra service."}, {"author": "ZmnSCPxj", "date": "2023-02-23T12:37:07", "message_text_only": "Good morning 10k1,\n\n\n> The use case is related to my work on Indranet, where channel sizes are going to be a lot lower than normal because they only have to transport very small payments (in the hundreds of sats at a time) and maintaining balance of the channels to keep them as much available as possible, the LN instance itself will be part and parcel of the indra service (as well as a node, either neutrino or probably bitcoind).\n> \n> The semi-automated creation of channels between relays on the network requires 3 an optimal 3 channels so as to optimise for ensuring that messages are at least 3 layers, 4 layers including our special nodes we are calling \"seeds\" that clients open zero conf, one way channels to for, again, very small balances, in the tens of thousands of sats kinda size. Ideally, they are always 4 layers deep and may need to be given hints based on network topology that can be provided by Indra itself.\n\nWell then, it looks like the seeds trust that clients do not double-spend zero-conf channels, and thus is largely not interesting to me personally, as I prefer trust-minimized systems myself.\nLikely you do, or will be forced to eventually do, some kind of gathering and validation of client information that would allow physical threat (e.g. possibility of lawsuit and imprisonment) to be imposed in case trust fails, as is typical in such systems.\n\n> It is not a concern if a peer out of the 3 is not available temporarily, the path will just be retried with a different path, the chances of cheating are pretty low and this element of our network's LN mesh is going to be very large - hopefully, and with all nodes having at least 3 peers with channels to them. The payments need to be relatively low latency, and the proportion of nodes that might be congested or fallen offline compared to the whole network is usually going to be very small because relays that are not running are not making the relay operator money - the payments between peers are not going to have fees to further simplify route selection, and because the real income of the relay is not from routing the tiny payments but from users requesting relay services, and premium services that have a higher cost than network-internal.\n\nSo let me see if I get this straight: the intended setup is that the 3 participants are 1 client, and 2 relays.\nThe 3-participant construction uses a 3-of-3 address between all 3 participants (2-of-3 would allow two participants to collude to steal money from the third participant).\n\nRelays may be motivated to increase their uptime to as close as 100% as possible, but there is simply no way to have 100% uptime: accidents and unknown unknowns exist.\nIndeed, I would point out that forwarding nodes on the LN, where all channels are 2-participant, have the same incentive to have uptime be as close to 100% as possible, because they get routing fees.\n\nHowever, this may be acceptable.\nGenerally, \"high reliability\" would be 6 nines, i.e 0.999999 uptime.\nIf you have 2 relays, and you need both to be up in order to operate a single 3-participant system, you would degrade to slightly better than 5 nines, which may be acceptable.\n\nBut basically there is that, random \"relays\" on the LN network will still sometimes disappear or become disoperational, if you allow random plebeian relays on your network rather than those that have been specifically been allowed by you or your organization, you cannot really control their uptime very much, and you would suffer worse uptime issues than what LN already has.\n\nThat is why I brought up channel factories, it is just an explicit tesselation of your 3-participant channel into 3x 2-participant channels, I suggest considering that instead.\nThat way, even if one relay disappears, a channel with the other relay is still operational.\n\n> My aim in opening up this discussion relates not to the general use case of routing payments but the use of LN as an anti-spam anti-sybil rate limiting scheme, as an adjunct and integral part of a separate peer to peer network system, in this case an anonymising relay system that creates source routed onions, similar to LN payments, but with more advanced structures that enable bidirectional traffic to act as a network tunnel that enables client side anonymity directly, and I'm currently working on adding a scheme for rendezvous-free bidirectional anonymity.\n\nI am unsure about the anti-sybilness here.\n\nIt is easy to generate an arbitrary LN node ID, you just pick 32 bytes from `/dev/urandom`.\n\nIf you are referring to the fact that you have to back channels with actual onchain funds, and using that as your anti-sibyl protection, take note that a pseudonymous identity is now attached to onchain funds.\nOnchain coin tracking can remove the privacy you tried to get on your network because you are now using channels (and the UTXOs that back them) to track psuedonyms.\n\n\n> This kind of application couldbe relevant for many different kinds of monetised p2p networks, Nostr, for example, could use something like this to create a general mechanism for users to anonymously pay other relays to host and deliver content for designated user identities, and would enable this to become a distributed service rather than having users need to depend on the trustworthiness of relays.\n\nOh, you could look up some of the work Tamas Blummer and I speculated on, in using onchain funds as an anti-sibyl mechanism in an information-sharing network, search stuff like \"defiads\".\n\nRegards,\nZmnSCPxj", "summary": "Indranet plans to use low channel sizes for small payments and maintain channel balance for availability. Three channels are required for optimal routing, with \"seeds\" acting as special nodes. Trust is required for zero-conf channels, but cheating is unlikely. Relays aim for high uptime, but accidents can happen."}], "thread_summary": {"title": "3 way channels and 0 conf channels", "categories": ["Lightning-dev"], "authors": ["l0k1", "ZmnSCPxj"], "messages_count": 4, "total_messages_chars_count": 26063, "convo_summary": "A proposal suggests opening Lightning Network channels between three nodes instead of two, offering advantages such as better balance and liquidity, more efficient routing, and easier pathfinding. However, this could pose security risks. Indranet plans to use low channel sizes for small payments and maintain channel balance for availability, with three channels required for optimal routing. Trust is required for zero-conf channels, but cheating is unlikely."}}, {"title": "[Lightning-dev] Jamming Mitigation Call 02/20", "thread_messages": [{"author": "Carla Kirk-Cohen", "date": "2023-02-19T21:07:16", "message_text_only": "Hi List,\n\nA reminder that we've got another jamming call coming up tomorrow.\n\nMonday 20 Feb\n19:00 UTC\nhttps://meet.jit.si/UnjammingLN\n\nWe'll be talking about local reputation scoring.\nFeel free to add additional agenda items here:\nhttps://github.com/ClaraShk/LNJamming/issues/4\n\nCheers,\nCarla and Clara\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20230219/5b1741e1/attachment.html>", "summary": "Join the local reputation scoring discussion on the Lightning Network during the jamming call on Monday, February 20th at 19:00 UTC."}], "thread_summary": {"title": "Jamming Mitigation Call 02/20", "categories": ["Lightning-dev"], "authors": ["Carla Kirk-Cohen"], "messages_count": 1, "total_messages_chars_count": 487}}, {"title": "[Lightning-dev] [Proposal] Payment Route Reservation", "thread_messages": [{"author": "g0b1el", "date": "2023-02-26T13:40:28", "message_text_only": "Hi, lightning devs\n\nIt's my first mail, so first, I'd like to say a big thanks to everyone involved in the development of the lightning network.\n\nI've just finished going through this mailing list, so I'm not sure if Payment Route Reservation is already proposed in some other place. If there is a similar proposal, please point me to the right place. \n\nI'll first list some potential improvements with this proposal:\n\n1)  Higher reliability of payments\n2)  Lower payment latency (on average)\n3)  Lower fees\n4)  Increase in privacy\n5)  Trampoline payment improvements\n6)  Significant reduction of gossip messages\n7)  Healthier, more decentralized network\n8)  Simplified routing algorithm\n9)  LN Wallet UX improvements\n10) Fast spam prevention\n11) Channel liquidity probing prevention\n12) Eliminates the need for splice-in (and partially splice-out)\n13) Eliminates the need for rebalancing and fee update scripts \n14) Eliminates the need for global nodes' reputation\n\nSo what is a payment route reservation?\n  \n\nPayment Route Reservation \n========================= \n\nThe idea behind Payment Route Reservation is to split the payment into two steps. In the first step, we reserve the route, and in the second, we send a payment. In the reservation step, routing nodes will not sign a new commitment state. Instead, they will just reserve a specified amount of satoshis. Those satoshis might be used with the next payment step or not if the reservation is canceled. Each reservation has a timeout (let's say 1 min). After a timeout, the reservation is removed from the channel reservation set and considered canceled. Reservation can be canceled manually by any upstream node.\n\nFor example:\n\n\n          add_reservation(payment_hash, amount) -->\n             \u250c\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2510\n     \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\n   S         \u2502 A \u2502         \u2502 B \u2502         \u2502 C \u2502          R\n     \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n             \u2514\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2518\n          <- reservation_success(fees_sum, cltv_delta_sum)\n\n\nS is a sender, R is a receiver, and A, B, and C are routing nodes.\n\nS wants to send `amount` satoshis with `payment hash`[1] to R. S first creates a reservation onion for A->B->C->R route and then sends the onion to the first node in the route, using `add_reservation` call. A unwraps the onion, and if he has enough unreserved balance in A->B channel, he will add a new reservation for the `amount` request to the channel reservation set and then forward the onion to B. Reservation in reservation set can be addressed by `payment hash`.\nIf A has an insufficient channel reservation balance, A will return 'reservation_failed' to the upstream node(sender S). B and C will do the same. Assuming there is enough reservation balance in all channels to reserve a payment, reservation flow eventually reaches R. R unwraps the onion and checks if he knows the preimage. If he knows, R returns `reservation_success` to upstream node C.\n\nAt this point, we have successfully reserved the payment route for `amount` satoshis from S to R. But, our route nodes haven't included the fees(or cltv_delta) of downstream nodes. `reservation_success` returns to upstream route node tuple (fees_sum, cltv_delta_sum). In this case, R will return (0, 0) to C because he is the last node in the route[2]. C will try to extend his reservation by fees from the R. If reservation extension is successful, C sends `reservation_success` to B, with a tuple increased by his fees[3] and his cltv delta. If C can't extend the reservation, he will send to B `reservation_failure` and to R `cancel_reservation`[4]. A will do the same. Eventually, if everything is ok with nodes A and B, S will receive a tuple (fees_sum, cltv_delta_sum). Now S knows precisely how much fees and cltv_delta route expects, and he can now send the payment onion. If S doesn't like the fees or cltv_delta, he can try some other route.\n\nLet's compare current payment forwarding to payment with reservation forwarding:\n\n\n  +-------+                               +-------+             +-------+                               +-------+\n  |       |--(1)---- update_add_htlc ---->|       |             |       |--(1)---- add_reservation ---->|       |\n  |       |                               |       |             |       |<-(2)---- reservation_success--|       | reservation onion\n  |       |                               |       |           \u2500\u2500|\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  |       |--(2)--- commitment_signed --->|       |             |       |--(3)--- commitment_signed --->|       |\n  |   A   |<-(3)---- revoke_and_ack ------|   B   |             |   A   |<-(4)---- revoke_and_ack ------|   B   |\n  |       |                               |       |             |       |                               |       | HTLC onion\n  |       |<-(4)--- commitment_signed ----|       |             |       |<-(5)--- commitment_signed ----|       |\n  |       |--(5)---- revoke_and_ack ----->|       |             |       |--(6)---- revoke_and_ack ----->|       |\n  |       |                               |       |             |       |                               |       |\n  +-------+                               +-------+             +-------+                               +-------+\n\nOne `update_add_htlc` is now replaced with two calls - `add_reservation` and `reservation_success`. Also, we have two onions now.\n\nWhen node A receives reservation_success with a tuple (fees_sum, cltv_delta_sum), nodes A and B have all the information to sign a new commitment transaction. The new commitment transaction amount would be `amount` + fees_sum, and cltv_delta would be cltv_delta_sum. \n\nFor now, we've just increased the latency. I have yet to check any implementation, so it is hard to guess how significant this increase is. The thing to note is that the reservation set is stored only in memory. No information needs to be stored in DB or replicated. If the node crashes with a live reservation, nothing bad can happen. So increase in latency would probably be proportional to 1 additional call between hops and how fast hops can unwrap the onion.\n\nBut before we tackle latency, let's first address the reliability of payments.\n\n\n[1] For PTLC case, we can use points\n[2] R can return some low random values to prevent C from discovering that R is the receiver.\n[3] Fees can be ppm, base, or whatever fee function the node operator prefers.\n[4] To ensure the node can always extend a reservation, we can set aside part of the channel reserve balance just for this case (let's say 0.5 percent).\n\n\nRoute Split Reservation \n=======================\n\nIf the routing node can't add a new reservation because the channel reservation balance is insufficient, the node can try to find some other route to reserve the missing amount for the next downstream node.\n\nFor instance, A needs to forward a reservation of 10 mBTC to B, but A->B channel has only 8 mBTC left of its channel reservation capacity[5].\n\n           \u250c\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2510\n       10  \u2502   \u2502     8     \u2502   \u2502   10\n      \u2500\u2500\u2500\u2500\u25ba\u2502 A \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502 B \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\n           \u2502   \u2502           \u2502   \u2502\n           \u2514\u2500\u252c\u2500\u2518           \u2514\u2500\u25b2\u2500\u2518\n             \u2502               \u2502\n             \u25022    \u250c\u2500\u2500\u2500\u2510     \u25022\n             \u2502     \u2502   \u2502     \u2502\n             \u2514\u2500\u2500\u2500\u2500\u25ba\u2502 C \u251c\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502   \u2502\n                   \u2514\u2500\u2500\u2500\u2518\n\n\nRather than returning `reservation_failure` and losing the possibility to earn fees on the remaining 8 mBTC, A will try to find a route for the remaining 2 mBTC. Let's say A picks A->C->B route. It is important to note that the new route should share the same payment hash as the main route(->A->B->)[6]. Now B will accept to forward the reservation of 10 mBTC further because he knows if a payment route is ever constructed, and if the preimage is revealed, he will receive his 10 mBTC.\n\nThere can be optimization in terms of fees. For instance, A can renounce his A->C channel fees. So the only fee A needs to pay for the new route is for 2 mBTC on C->B route. Assuming network convergences to roughly the same fees on all nodes, cumulative route split fees effect would be the same as if the whole amount was routed through A->B. Although this example looks a bit too perfect, it's quite common on the plebnet part of the lightning network, where most nodes are created using Triangle Liquidity Swaps.\nIt is important to note that A now needs to take into account the fees and cltv_delta of the new route. Fees are the sum of the fees in A->B channel and A->C->B route. Cumulative cltv_delta would be maximum between A->B and A->C->B route.\n\nNote that A can try multiple reservation routes. Also, if C doesn't have enough reserve balance, C can split a route through some node D to B, etc. To prevent infinite reservation recursion calls (in case of huge payment), the route split node will pass a tuple (max_fees, max_cltv_delta) that he will accept from the new route. Each node in the route will first decrement tuple by his fees and cltv_delta. If the new tuple contains any element below 0, the node will return `reservation_error`. If tuple elements are still positive, the reservation will continue, and decremented tuple will be passed to the next downstream node.\n\nTo hide the difference between the main route and route split, the payment sender will also send the tuple(max_fees, max_cltv_delta) that he accepts for the main payment route. Now, if someone is observing lightning network traffic, he wouldn't know if a reservation is for payment or route split. Thus we have an increase in privacy of payment.\n\nNote that now we can also send payments larger than channel capacity A->B.\n\n\n# Trampoline route split reservation\n\nWhat if A and B don't have a direct channel, or the sender doesn't know the whole network topology or all the channels between A and B are suddenly disabled?\n\n\n                          \u250c\u2500\u2500\u2500\u2510         .         \u250c\u2500\u2500\u2500\u2510\n                       10 \u2502   \u2502---->   . .   ---->\u2502   \u2502 10\n                     \u2500\u2500\u2500\u2500\u25ba\u2502 A \u2502---->   . .   ---->\u2502 B \u251c\u2500\u2500\u2500\u2500\u2500\u25ba\n                          \u2502   \u2502---->    .    ---->\u2502   \u2502\n                          \u2514\u2500\u2500\u2500\u2518                   \u2514\u2500\u2500\u2500\u2518\n\nA still can try to reserve payments through different routes to B. Idea is similar to what is currently known as Trampoline Routing. \nThe biggest benefit of using Trampoline route split reservation payment over Trampoline Routing payment is that a sender, before initiating the payment, knows exactly how much in fees he needs to pay. As well as how big cltv_delta will be.\n\n\n\nHow expensive is a route split reservation?\nA needs to create a new reservation onion, and until that route split reservation propagates to B, B can't forward the reservation further downstream.\nSo we increased latency again. \n\nMy guess is that reservations with route split payment will increase the reliability of single payments to the range of 90-95%. But can we reach 99% and finally lower the average latency?\n\n[5] The outbound liquidity can be greater than 10 mBTC, but the reservation set has only 8 mBTC left.\n[6] For PTLC A->B and C->B, payment hops need to commit to the same point.\n\n\nRedundant Multi Route Payment Reservation\n========================================\n\nThe idea is similar to what is now called Multi-Path Payments and Stuckless Payments[7].\n\n\n                          \u250c\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2510\n                          \u2502   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502   \u2502\n                     \u2500\u2500\u2500\u2500\u25ba\u2502 R \u2502       \u2502 T \u2502       \u2502 U \u251c\u2500\u2500\u2500\u2500\u2500\u25ba\n                          \u2502   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502   \u2502   \u250c\u2500\u2500\u25ba\u2502   \u2502\n                          \u2514\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2518   \u2502   \u2514\u2500\u2500\u2500\u2518\n                                              \u2502\n                             .......................\n\n                          \u250c\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2510  \u2502    \u250c\u2500\u2500\u2500\u2510\n              S           \u2502   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502   \u2502  \u2514\u2500\u2500\u2500\u25ba\u2502   \u2502\n                     \u2500\u2500\u2500\u2500\u25ba\u2502 D \u2502       \u2502 E \u2502       \u2502 F \u251c\u2500\u2500\u2500\u2500\u2500\u25ba    R\n                          \u2502   \u251c\u2500\u2500\u2500\u2500\u2510  \u2502   \u251c\u2500\u2500\u2510    \u2502   \u2502\n                          \u2514\u2500\u2500\u2500\u2518    \u2502  \u2514\u2500\u2500\u2500\u2518  \u2502    \u2514\u2500\u2500\u2500\u2518\n                                   \u2502         \u2502\n                          \u250c\u2500\u2500\u2500\u2510    \u2502  \u250c\u2500\u2500\u2500\u2510  \u2502    \u250c\u2500\u2500\u2500\u2510        \n                          \u2502   \u2502    \u2514\u2500\u25ba\u2502   \u2502\u25c4\u2500\u2518    \u2502   \u2502\n                     \u2500\u2500\u2500\u2500\u25ba\u2502 A \u2502       \u2502 B \u2502       \u2502 C \u251c\u2500\u2500\u2500\u2500\u2500\u25ba\n                          \u2502   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502   \u251c\u2500\u2500\u2500\u2500\u2500\u25ba \u2502   \u2502\n                          \u2514\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2518\n\nThe problem with a single route reservation is that reservation fees(or cltv) might be too high, and the sender will have to create a new reservation route hoping to get a better fee. Also, reservations might take longer than we are willing to wait. \n\nTo overcome this issue, we first split the amount into N parts[8], and then we create 2*N[9] reservation routes each for the amount of `payment amount`/N. Now a sender needs to wait for just N routes out of 2N to return `reservation_success`, and then initiate payment. Note that multiple reservation routes can go through the same node.\n\nThis now creates an interesting market dynamic where the fastest route wins. And the fastest route is with the fastest nodes. Now nodes are incentivized to compete with better internet and better hardware. There would also be competition between lightning node developers to create the fastest lightning node. \n\nIf a sender is not that interested in the speed of payment, he can wait for all 2N routes to return reservation results and then pick the cheapest N routes.\nNow nodes also need to compete in fees as well. Cheaper the fees, the higher probability of the sender picking the route.\n\nAll this eventually will benefit lightning network users.\n\nIt doesn't matter if the channels are balanced or not. If the node is well connected and has enough outbound liquidity, the reservation route split should(in most cases) find a route to the next node. So there is no need for rebalancing or fee update scripts. \n\nJust because nodes reserved the route doesn't mean they have to route the payment. There is still a probability of payment failure. Payment can fail because of reservation time out, node going offline, griefing node, etc. To further increase reliability, we can also add stuckles payments.\n\nWith Redundant Multi Route Payment Reservations, I believe the reliability of payments would be around the 99% percent range.\nLatency, on average, should converge around a value much lower than the average value we have now. \n\n[7] https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-June/002029.html\n[8] Unless it is a small amount, for instance, below 0.5 mBTC. \n[9] Or some other multiplier\n\n\nLN Wallet UX improvements\n=========================\n\nNot every payment is the same. For instance, if I'm paying a groceries bill in a supermarket, I value the speed of payments more than fees. On the other hand, if I'm paying my electricity bill at home, I value fees more than the speed of transactions. Also, if I'm paying a VPN subscription, I care more about privacy and cost over transaction speed.\n\nWith route reservation, we can present the exact user fees he is going to pay, so wallet developers can add the following element to UX:\n\nRadio box: Fast Payment | Cheap Payment | MaxPrivacy\n\n- If a user selects `Fast Payment` wallet will wait for the first N fastest routes and initiate payment automatically(unless fees exceed in percents some config value). \n- If a user selects the `Cheap Payments` node will wait for all 2N routes to return, pick N cheapest routes, and then present the user with fees and a send button. If fees are too high, the user can press the `Try again` button.\n- For `MaxPrivacy` wallet will behave as in `Cheap Payment` mode, except the routing algorithm will prefer longer routes over smaller nodes, with preferably at least one Tor node.\n\n*N routes can be N trampoline routes if a wallet doesn't have a whole network topology.\n\n\nGossip messages spam\n====================\n\nCurrently, 97 percent or more LN gossip messages go to `channel_update` messages. The reason is that most nodes on the network are running some form of automatic fee adjustment script. If the number of nodes and channels continues to grow, soon, this can become unsustainable.\n\nReservation is not used just to reserve the route but also to inform a sender of fees and cltvs. Thus there is no need for nodes to publish their fees to the whole network. And thus, information like base_fee, ppm fee, and cltv can be removed from the `channel_update` message.\n\nWith route reservation, there is no need for channel rebalancing or fee update scripts. Node operators can still run fee update scripts without restrictions, but those updates would not propagate to the network.\n\n\nRouting algorithm and network decentralization\n=============================================\n\nIn terms of network topology, a current lightning network can be seen as a decentralized network. But in terms of a payment network, it is more a hub base network, where the majority of payments go through ~30 largest nodes. Medium and smaller nodes rarely forward any payment, and they are mostly used by big nodes for channel rebalancing. This is a big issue in terms of fairness(the rich are getting richer faster), privacy, reliability of the network, cost of a transaction, etc.\n\nI would argue that this problem is not because the rich are rich but because of the routing algorithm. Currently, used routing algorithms usually calculate the probability of routing a payment over some channel as a channel capacity function. If the channel has a large capacity probability is higher, small channels will have a lower probability of payment routing. Because all wallets are effectively running in `Fast Payment` mode, the routing algorithm will almost always choose bigger channel routes than smaller ones and thus create the hub-based model we have today.\n\nIf we remove `fees` and `cltvs` from the gossip, what is left for the routing algorithm is `channel_capacity`, `htlc_minimum_msat` and `htlc_maximum_msat`. \n`htlc_maximum_msat` - can be a local node config. If the reservation attempts to reserve a greater amount than the maximum allowed for a single payment, a node can try to split the reservation over multiple channels.\n`htlc_maximum_msat` - can remain a global config to avoid unnecessary reservation failures.\n\nWhat about channel capacity? \n\nChannel capacity can be the biggest misleading factor for LN routing algorithms. For instance, I can create a node and then buy 100 channels with 1BTC each of income liquidity. Then I set all channel fees to 0. Every routing algorithm will try to route payments through my node, but my node can't route a single satoshi. I would suspect that today's most popular nodes on the LN network probably have way more inbound than outbound liquidity.\nSo I think that capacity should not be used at all as a heuristic in the routing algorithms.\nI think a better approach would be if nodes would just publish their cumulative outbound liquidity over all channels. If there are routing node A and routing node B, and they are well connected, we know that we can send(in theory) from node A to node B, with route split, a maximum payment of `min(A.balance, B.balance)` amount.\nThis would also signal to the rest of the network where liquidity is needed and where there is too much liquidity. There would be no liquidity sinks, and network liquidity could be better deployed. Automatic channel opening becomes trivial.\n\nBut would the nodes lie? \nIf the node lies, the reservation will fail on his node, and if there is a lot of reservation failure on misleading nodes, neighbor nodes might close the channel with him. Also, while processing huge payment reservations, which will eventually fail, a node could have routed a smaller payment and earned fees. So nodes will be incentives to report the real outbound liquidity or lower if privacy is the issue.\n\nNow routing algorithm has become very simple. We make routing decisions just on hop count from our node, on node reported balance, and a bit of randomness. Randomness is the most important part. If some route fails, we can't know which node to blame. It can be any node in the route. The routing algorithm should not track any blame information about individual nodes and would not try to blacklist any particular node. If the node is well connected and the routing algorithm is random enough, we should be able eventually to find enough routes to the sender. If there is a faulty, griefing, or just slow node in the network, we let nodes handle deputies themselves. Each node will track the local reputation of each of its neighbors. If there are a lot of failed payments from one neighbor, the node operator might contact that neighbor for an explanation or just close the channel to that neighbor node.\n \nFor privacy currencies, not publishing channel capacity is a necessity. Privacy currency users (like Monero) don't want to relieve any of their UTXO amounts. Privacy currency LN can avoid publishing their channel capacity and outbound node liquidity, and only make routing decisions on hop counts and randomness. There would probably be more reservation failures, especially for larger amounts, but it should be worth privacy preserved.\n\n\nFast spam prevention\n====================\n\nFast spam is a situation on the network when malicious nodes start creating a lot of payments to the random nodes, which will never be resolved because the receiver does not know the payment preimage. The receiver can only fail the payment. \nThis is not possible with a reservation anymore because receivers will reject a reservation, and there will be no payment. Now, this is a lot better because the reservation does not commit to a new channel state, and no DB operations are involved.\n\nBut how to prevent reservation spam?\nIf the node sees a lot of reservation errors per minute coming from one channel, the node can throttle reservations coming from that channel. Eventually, if spam continues node operator can inform the neighbor node operator to investigate the reason. The neighbor node will do the same. And if the problem persists, the node operator can just close the channel.\n\nBut what if an attacker controls both nodes(receiver and sender) or if the attacker creates a circular route?\n\nThe malicious receiver node will accept the reservation, and then when HTLC comes, it just fails the payment. This would be basically fast spam again, with a bit more work to be done by the attacker.\nAs proposed before[10], we can demand a prepayment. If a node sees a large amount of failed payments per minute on some of its channels, a node can start demanding some prepayment to route HTLC. The problem is how much to charge. If we charge too little, spam will continue. If we charge too much, this will affect regular micro-payments. \nWe can extend reservation_success tuple with a prepayment fee. And we can adjust the fee on the fly. There is no need to propagate it to the whole network. A node can initially charge 0 prepayment fees. This is in their best interest because the routing algorithm will prefer routes without prepayment. The reason is that there is still a chance of payment failure. In a payment failure case, the sender loses the pre-payed amount unconditionally. Now if a node observes a large number of payment failures per minute, he can start aggressively increasing his prepayment fees, thus making the attack very costly.\n\nThere was also a discussion on how to send a prepayment. Wouldn't the first node in the route just steal all prepayment and then fail the payment?\nThere are two possible scenarios here:\n - prepayment is less than or equal to the fees the node is going to earn routing the payment. In this case, it is in the node's best interest to route the payment.\n - prepayment amount is greater than node payment fees. I can see two possible solutions:\n    - let a first node steal the prepayment. This should be fine because if prepayment gets so big, most likely, the attacker is the one being robbed. Honest users will prefer routes without a prepayment, especially large prepayments.\n    - we can also try to send multiple keysend prepayments, but there are some privacy issues here because the length of the route is revealed to the first node, and the last route node finds out who the receiver is. A slightly less bad option is to send prepayments for the parts of the route. For instance, one keysend is for the first half of the route. The second is for the second half of the route.\n\n[10] https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-February/002547.html\n\nChannel liquidity probing\n=========================\n\nWith a route reservation split, inspecting channel liquidity from other nodes becomes significantly more demanding. The reason is the fact that reservations should rarely fail. Now observer needs to deduct the channel liquidity from the reservation response (fee,cltv_delta). If there is a reservation split, fees and cltv_delta will most likely be higher than if there is no reservation split. But every node can change fee and cltv_delta on the fly, and by just randomizing those values on every return, potential channel observer jobs get much harder.\nAlso, if the attacker starts sending a lot of reservation requests, those requests might be seen as a DOS attempt, and the node might start throttling the reservation requests.\n\n\nNo need for splice-in(and partially splice-out)\n===============================================\n\nThe idea behind the splicing is to increase the capacity of the channel, so that channel can route bigger payments. Also, the routing algorithm prefers a bigger channel so the node would route more payments.\nWith route split reservations, this is not necessary anymore. For example, if nodes A and B have N channels between them, node A  would have the option to reserve a route through all N channels during reservation. Thus N smaller channels can be seen as one big channel, not by routing algorithm but by node A. Routing algorithm will just care if there is a connection between A and B.\n\nIf nodes A and B have N channels between them, closing one channel basically becomes a splice-out operation.\n\n\n\nLooking forward to your feedback.\n\nBest Regards,\ng0b1el", "summary": "Proposal for Payment Route Reservation in Lightning Network to improve reliability, lower latency and fees, increase privacy, and simplify routing algorithm."}, {"author": "g0b1el", "date": "2023-02-26T13:57:02", "message_text_only": "If ASCII graphics are not rendering correctly, you can read the proposal on the mailing list archive, where for some reason are rendered correctly - https://lists.linuxfoundation.org/pipermail/lightning-dev/2023-February/003867.html \n\nCheers,\ng0b1el", "summary": "Proposal on ASCII graphics rendering issue can be read on the mailing list archive where they are rendered correctly. Link provided."}], "thread_summary": {"title": "Payment Route Reservation", "categories": ["Lightning-dev", "Proposal"], "authors": ["g0b1el"], "messages_count": 2, "total_messages_chars_count": 26433, "convo_summary": "g0b1el proposed a Payment Route Reservation in Lightning Network to enhance reliability, lower latency and fees, increase privacy, and simplify routing algorithm. They also shared a link to a mailing list archive where their proposal on ASCII graphics rendering issue can be read, with correct rendering."}}, {"title": "[Lightning-dev] Onion messages for probing scheme", "thread_messages": [{"author": "vwallace", "date": "2023-02-27T21:31:15", "message_text_only": "Hi list!\n\nI wanted to bring up the idea of using onion messages for payment probing, which was briefly touched\non at the 2022 LN summit. Tadge Dryja has also brought up a similar idea.\n\nI recommend reading the gist instead since it has the relevant diagrams in-line:\nhttps://gist.github.com/valentinewallace/ebe1741f0438c2360eda0f80f0e075c9 but the scheme is also\nposted below for convenience.\n\n## Introduction\n\nFor context, in today\u2019s lightning, payment reliability tends to heavily depend on having sufficient\npayment volume to determine current liquidity balances of channels, which is how most big nodes can\ntell whether a given channel has enough liquidity to forward a given amount. If a node is using HTLC\nprobing to achieve this payment volume, they use a regular `update_add_htlc` message with a bogus\npayment hash, where the error returned informs the sender of whether the payment reached the final\nrecipient. Note that there is a tradeoff between always routing through nodes that are known to\nrebalance their channels vs leaning on probing smaller nodes and \u201crisking\u201d payments through them\nbased on what\u2019s learned.\n\nToday\u2019s HTLC payment probing can work well, but risks channel liquidity being locked up if a peer\nalong the route goes offline. On the upside, for just-in-time probes, it works to loosely \u201creserve\u201d\nthe channel liquidity along the route for the actual payment.\n\nOnion messages (OMs) present a convenient way to probe without risking locked liquidity along the\nroute.\n\n## Design Rationale\n\nA naive approach could be to send onion message probes directly to individual nodes along the\ndesired route, including those to which you don\u2019t have a direct channel. However, this scheme is\nproblematic because it would enable monitoring the payment flows of arbitrary nodes across the\nnetwork without having to have a channel path to them. This would be a significant degradation of\nprivacy because, for comparison, in HTLC probing it is quite difficult to probe the balances of\nfar-off nodes. And if you can\u2019t probe a node using HTLCs, you can\u2019t send over it anyway, so there\u2019s\nnot a lot of benefit (and significant privacy downside).\n\nTherefore, it is probably best to design a scheme that probes along channel paths, like HTLC\nprobing. This adds more complexity to the case where an intermediate node cannot forward the desired\namount due to the stateless nature of OMs, discussed further down.\n\n## Scheme\n\nLet\u2019s go through the happy path, where Alice is probing Alice > Bob > Carol > Dave for a 100k msat\npayment.\n\nShe\u2019ll construct an onion message for Bob, the first hop, as such: https://imgur.com/BZg8yt8\n\nBob receives this OM, sees that he\u2019s able to forward 110k msats to his next-hop Carol, and forwards\nCarol\u2019s onion packet to her. Carol sees she can forward 105k msats to Dave, and forwards his onion\npacket. Finally, Dave receives his onion packet, sees he can receive 100k msats from Carol, and uses\nthe provided reply path to send a simple probe success onion message back to Alice:\n\n```\nonion_message_probe_result {\ndata_tlv {\ntype: 4242,\nprobe_id: [u8; 32],\ncan_forward_desired_amt: true,\n}\n.. regular OM TLVs\n}\n```\n\nNote that Dave will use this same onion message if he can\u2019t receive; he\u2019ll just set\n`can_forward_desired_amt` to false.\n\nAs an example of the sad path for an intermediate node, if Carol can\u2019t forward 105k msats to Dave,\nshe\u2019ll fail the probe back to Bob by sending this onion message: https://imgur.com/a/hqlzw4I\n\nThis step justifies why we need to encode a failure onion for each intermediate hop of a probe. If\nwe hadn\u2019t done that, and Carol responded to Bob with an empty \u201cfailed\u201d message, Bob would have no\nidea which peer to fail the probe back to, because OMs are stateless. Instead, Bob unwraps his error\nonion and sees that he needs to fail back to Alice with her provided error onion. Alice receives the\nfailure onion and can see that Carol was not able to forward the desired amount corresponding to the\nprobe id, thus completing the probe.\n\n## Outro\n\nWhile there is nothing stopping nodes from lying about their ability to forward, they may be\nnegatively scored if they do so. Further, adopting a protocol like this could help routing nodes\nattract more forwarding traffic, which is a nice incentive.\n\nI view this feature as a low priority enhancement, given there are a lot more pressing issues in LN,\nbut open to feedback. Mostly, I thought it could be useful to spark ideas and highlight the\nflexibility of OMs.\n\nCheers,\nVal\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20230227/fe72ef42/attachment.html>", "summary": "A proposal to use onion messages for payment probing in Lightning Network was discussed at the 2022 LN summit. Onion messages can probe without risking locked liquidity along the route."}], "thread_summary": {"title": "Onion messages for probing scheme", "categories": ["Lightning-dev"], "authors": ["vwallace"], "messages_count": 1, "total_messages_chars_count": 4674}}]