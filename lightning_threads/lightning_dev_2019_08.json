[
    {
        "title": "[Lightning-dev] Improving Lightning Network Pathfinding Latency by Path Splicing and Other Real-Time Strategy Game Techniques",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2019-08-01T01:35:21",
                "message_text_only": "Introduction\n============\n\nI found out recently (mid-2019) that mainnet Lightning nodes take an inordinate amount of time to find a route between themselves and an arbitrary payee node.\nTypical quotes suggested that commodity hardware would take 2 seconds to find a route, then take a few hundred milliseconds for the actual payment attempt.\nWith the help of Rene Pickhardt I was able to confirm that indeed, much of payment latency actually arises from the pathfinding algorithm and not the actual payment-over-routes.\n\nThis is concerning, of course, since we would like the public Lightning Network to grow a few more orders of magnitude.\nGiven that the best pathfinding search algorithms will be O(n log n) on the size of the network, we need to consider how to speed up the finding of routes.\n\n`permuteroute` and Faster Next Pathfinding Attempts\n===================================================\n\nAs I was collaborating with Rene, JIT-Routing was activated in my core processing hardware.\n\nAs I was contemplating this problem, I considered, that JIT-Routing would (ignoring fees) effectively \"reroute\" the original route around the failing channel.\n\nIn particular, JIT-Routing is advantageous for these reasons:\n\n1.  There is no need to report back the failure to the original payer.\n2.  The node performing JIT-Routing has accurate information about its channel balances and which of its outgoing channels would be most effective to route through instead of that indicated by the original payer.\n    It also knows of non-published channels it has.\n3.  Searching for a circular rebalancing route could be done much quicker since the JIT-Routing node could restrict itself to looking only in its friend-of-friend network, and simply fail if it could not find a circular rebalancing route quickly in the reduced search space.\n\nThe first two advantages cannot be emulated by the original payer.\n\nHowever, I realized that the third advantage *could* be emulated by the original payer.\nThis is advantageous as the payer node can implement emulating this, without having to wait for the rest of the network to actually implement JIT-Routing as well.\n\nBasically:\n\n1.  On payment failure, the original payer is informed of which node reported the failure and which channel failed.\n2.  The original payer looks for a *short* route from the reporting node to the next node (or any node after the reporting node in the original path).\n    The original payer restricts its search to within 3 hops, roughly equivalent to searching the friend-of-friend network, thus able to quickly get a route or a failure indication.\n    (we also need to get a set of known-bad channels other than the most-recently failing one, so that we do not bother to scan those channels)\n3.  The original payer then replaces the failing channel with the new route found.\n4.  As the new route might \"backtrack\" the original payer checks for duplicated nodes and removes the unneeded hops.\n\nThe payer is now in possession of a variation of the original route, one which avoids the known-failing channel.\nIt has a prefix that we know was recently reliable (the error got reported back to us, after all).\nFinally, the payer did not take much time to find this alternate route, improving payment latency.\n\nI dubbed this new algorithm `permuteroute`.\n\nIn a `pay` implementation, we would do:\n\n1.  For the *first* routing attempt, use `getroute`.\n2.  Use the current route for `sendpay`.\n3.  If the pay attempt fails, use `permuteroute`.\n    If returned route is too expensive or it fails, fall back to `getroute`.\n4.  GOTO 2.\n\nThe `permuteroute` is fast due to not having to scan a large number of channels.\nIdeally it would use Dijkstra with a restricted number of hops, but this is not strictly necessary and a simple flood-fill can work about as well.\n\nJIT-Routing is still strictly superior to `permuteroute`, as the node reporting the failure has strictly more information about local conditions.\n(Yes, it could return with a \"suggested other channel\", but that requires a spec change and it would be better to focus a spec change on feeless rebalancing to support JIT-Routing instead, especially since JIT-Routing can potentially effectively create a \"local multipath payment\" by rebalancing from multiple other channels, especially if rebalancing can be made feeless.)\nHowever, JIT-Routing benefits start being noticeable only once a majority of nodes have supported it, whereas `permuteroute` gets immediate benefits to the node that upgrades to utilize it.\n\nIn particular, `permuteroute` would be helpful to support JIT-Routing nodes while there are still few JIT-Routing nodes.\nA particular part of `permuteroute` is that it reuses the known-successful prefix of an existing route, replacing only channels from the failed portion and later.\nIf only one JIT-Routing node existed, it would suffer somewhat since while it would increase payment success rates, it would still lose (due to rebalancing fees) if a later channel in the hop fails and the node with that channel does not itself do JIT-Routing.\nWith `permuteroute` thie JIT-Routing node and its outgoing channel would end up being in the known-successful prefix of a `permuteroute` from the same payer, thus not wasting its \"investment\" in rebalancing in favor of its outgoing channel.\n\nAn unfinished pull request for `permuteroute` on C-Lightning exists: https://github.com/ElementsProject/lightning/pull/2890\n\nLightning Network Pathfinding Problem Statement\n===============================================\n\nLet us mildly digress.\n\nPathfinding on Lightning has these requirements:\n\n1.  We are looking for a \"good enough\" path, not the most optimal path.\n    Going for the optimal path is good, but users are more interested in payment success than purely fee optimization.\n2.  The map we are doing the pathfinding in is dynamically changing and we might not have access to the latest state.\n3.  We need to look for paths in a large map in a very short time.\n\nReal-Time Strategy Game Pathfinding Problem Statement\n=====================================================\n\nRene also mentioned that what LN generally calls \"routefinding\" is also called \"pathfinding\" elsewhere.\nThis triggered me to consider real-time strategy games and pathfinding algorithms used in them.\n\nIn particular, the above requirements we have, have analogies to the problem of real-time strategy games.\n\n1.  Players want their units to start moving as soon as commanded; they have a myriad other things to manage and do not want to have to micromanage certain movements.\n    For example, moving through known-safe locations or scouting into unknown places often does not need to be optimal: they just need to be *done* in a manner that is not thoroughly stupid.\n    If players need a specific path to be followed, they will micromanage their units, but non-micromanaged units are expected to perform their actions in the background, and non-optimal paths for non-micromanaged units are often acceptable as long as it is not ***too*** sub-optimal.\n2.  Good real-time strategy games have a dynamically-changing world: trees are cut down for wood, walls are erected by opponents, structures are placed for various optimal results, mobile units deploy into immobile modes, and so on.\n    Good real-time strategy games will also not let the units of a player know the ***current*** real state of the game world, as that leaks strategic information about where walls and defense structures have been built, where resources have been harvested, what chokepoints are blocked by immobilized units with increased combat effectiveness, etc.\n    Instead, units are given fog-of-war-covered maps, with obsolete information, and thus may find their pre-solved paths blocked, or may find that shortcuts now exist which did not exist before.\n    Even so, units must be able to correct their current paths once they discover this fact, without too much latency (as they might now be subject to attack by the immobile units siege-moded against them, or defense structures behind walls, etc.).\n3.  As a real-time game, commands must be reacted to in a time-frame that is below typical human notice.\n    While real-time games need to traverse simpler, smaller, and more regular maps than LN routemaps, they also need to get results for possibly dozens of different units in a few dozen milliseconds, whereas LN pathfinding would probably be well-satisfied by getting one route after a few hundred milliseconds.\n\nThus, I started looking for techniques and shortcuts used in real-time strategy games.\n\nA\\* (A-star) is the preferred algorithm in the \"geographic\" maps usually used in real-time strategy games.\nIn many cases we can replace it \"seamlessly\" with Dijkstra in the LN case: a constant `h(n)` function makes A\\* devolve to Dijkstra, and if we admit that we have no decent heuristic \"distance\" function, we can consider ourselves as \"really\" using A\\* with a constant `h(n)` (which happens to make the algorithm behave as Dijkstra).\n\nBut strictly speaking, using A\\* (and its \"degenerate\" form Dijkstra) is overrated: http://bitsquid.blogspot.com/2010/10/is-overrated.html\n\nPath Splicing\n=============\n\nOne technique I found was Path Splicing: http://theory.stanford.edu/~amitp/GameProgramming/MovingObstacles.html#path-splicing\n\nI realized that this technique was exactly what my `permuteroute` algorithm did: instead of solving for a completely new path, it would attempt to find a short route to \"heal\" a recently-discovered blockage of the original path.\nThe technique even recommends keeping the search space small; if the path needed to heal the original path becomes too long, it would be better to fall back to a \"full\" pathfinding.\n\nAs a corollary to this, JIT-Routing is itself an implementation of a form of Path Splicing, except with the potential to form a \"local multipath payment\" when a payment is routed through the node.\n\nEmboldened, I began reading more and considering other ways to improve pathfinding latency, as pioneered by real-time strategy games.\n\nDigression: Parametrizing `pay`\n===============================\n\nC-Lightning `pay` command (whose first \"major version\" was created by me, though the current implementation is vastly different from my work, but keeps most of its features and interface) is parametrized with two tweakable arguments:\n\n1.  `maxfeepercent` - The maximum fee, in percentage of the payment value, that the user is willing to pay for payment success.\n2.  `maxdelay` - The maximum number of blocks that the user is willing to wait in case of long-term failure of direct peer.\n\nThis is relevant since costs in Lightning are not a single \"cost of travelling over this terrain\" as in real-time strategy games.\nInstead, costs in Lightning involve two things: the fee charged by the node, and the number of blocks in the difference of the incoming versus outgoing HTLCs.\n\nIn particular, internally to C-Lightning, `getroute` has a `riskfactor` parameter, which is intended to balance between these two costs (fees vs blocks).\nThis parameter is expressed as \"annual cost of your funds being stuck\".\n\nI personally cannot understand what this \"annual cost of your funds being stuck\" means exactly in relation to my own money and risk-aversion, and have just used whatever value is the default for `getroute`.\nI can understand the `maxfeepercent` and `maxdelay` better, and I imagine most users would also understand these better (though I have not talked to any users at all).\nThe documentation for the `getroute` command even contains a nice table about how the various settings for `riskfactor` affect your estimation of the cost of risk depending on how large the payment is and how many blocks the hop node charges.\nI still cannot understand it even so, and my own expectation is that most users will also fail to grasp this intuitively.\nOn the other hand, `maxfeepercent` and `maxdelay` require no tables to explain.\n\nIn particular, `pay` always compares the total delay and the total fee to the given `maxfeepercent` and `maxdelay`.\nIn theory, if only one is breached but not the other, `pay` could tweak the `riskfactor` in order to change the balance.\n\nThe difficulty here is that in many of the succeeding sections, we will find that we will need to use some precomputed data, for which this balance of time vs. fee needs to be selected somehow.\nThus we want to use some reasonable default for this balancing parameter.\nIf the user wants to diverge from this default balancing, they would suffer as they would need to fall back on not using the precomputed data, reducing the speed of their payments.\n\nThis should be kept in mind for the succeeding sections: algorithms using precomputed data must by necessity be less flexible (it is restricted to the domain for which the data was precomputed for), and the price of flexibility will be slowness of a more general algorithm.\n\nThe possibility is that we are in fact being more flexible than absolutely necessary, and some shortcuts may be acceptable as a trade-off for faster pathfinding.\n\nMap Preprocessing\n=================\n\nOften, real-time strategy game maps can be preprocessed.\n\nFor example, wide-open areas often take A\\* a lot of time exploring this space.\nHowever, in principle such wide-open areas should not be a pathfinding problem: any arbitrary path through those areas is generally fine and we should really have A\\* focus on the fiddly bits between mountain passes and going through rough terrain and avoiding cul-de-sacs.\nThis is an example where finding the \"optimum\" path can take a back seat to finding \"any acceptably-good\" path: A\\* searches along every good paths through plains in case there is some special path that is really optimal, even though usually any straight-line path through the plains is going to work well enough that the player will not feel he or she has to micromanage the unit path.\n\nMap preprocessing can help by reweighing the costs in wide-open areas, making A\\* go through a small number of acceptably-good paths rather than exploring all possible ways to walk a wide-open field (and eating up processing time in exploring all of them).\n\nNow, we might not have the advantage of having a \"geographical\" map that can provide some reasonable heuristic distance function `h(n)` for A\\*.\nBut we do have one advantage over real-time strategy games:\n\n* Every payment that requires us to find a path, always starts from one node: our own node.\n\nThis is in stark contrast to real-time strategy games, where arbitrary units arise at various locations, and the pathfinding system must be capable of handling multiple possible start points.\n\nIn particular, Dijkstra has a variant where it does *not* look for the shortest path between two nodes, but instead finds the shortest path tree from one node to all other nodes on the map.\n(This is arguably the \"true\" form of Dijkstra.)\n\nThis \"true\" variant of Dijkstra is a good thing to use, as we know that every payment sourced by our node requires a path starting from our node.\n\nIn C-Lightning in particular, every node object has a `struct`, named `dijkstra`, containing information used by the Dijkstra algorithm, size of 16 bytes, which is sufficient to derive a path from the destination to the source.\nWe can instantiate another copy of this `struct` to every node, let us call this `dijkstra_cached`, and run the Dijkstra algorithm for all nodes to the source using this alternate copy.\n\nThen we can provide a new `getroutequick` command which simply looks up the destination node, then builds the route back to the specific source (our own node).\nIf route-building fails because the channels have been closed, it can simply fail (and the client has to fall back to the slow O(n log n) `getroute`, which uses the existing `dijkstra` field instead of `dijkstra_cached`).\n\nA `pay` algorithm would then do:\n\n1.  For the *first* payment attempt, use `getroutequick`.\n    If the returned route is too expensive or `getroutequick` fails, fall back to `getroute`.\n2.  Use the current route for `sendpay`.\n3.  If the pay attempt fails, use `permuteroute`.\n    If returned route is too expensive or it fails, fall back to `getroute`.\n4.  GOTO 2.\n\nThus, the slow `getroute` only exists in the \"fallback\" case, and we have effectively optimized payment to arbitrary nodes.\nPayment attempts can start to be sent out almost immediately.\n\n***Now of course, the `dijkstra_cached` map will become obsolete as channels are created and destroyed***.\n\nPeriodically, we can start a background process to perform a new Dijkstra run to update the cached data.\nWe can implement this by a \"fast\" timer that performs some fraction of the Dijkstra run, then lets normal operation for some time before grabbing the CPU again to update the Dijkstra.\nThis lets us process normal requests (updates of channel, `getroute` and `permuteroute` requests, etc.) while refreshing the cached shortest path tree.\n(other implementations might consider using separate threads, or some concept of low-priority thread: the C-lightning `gossipd` is single-threaded so we can avoid many of the issues with thread synchronization)\n\nOf course, that means that `getroutequick` cannot work while we are performing this refresh.\nTo fix this, we turn to another game implementation technique: double-buffering.\n\nWe use a `dijkstra_cached[2]` array, then fill in *one* entry during the refresh while `getroutequick` calls follows the *other* entry.\nThen when the refresh is completed, we \"flip\" the buffer that `getroutequick` uses (a single variable somewhere in our map object).\nThen on the next refresh, we use the `dijkstra_cached` entry that is not being used for `getroutequick`, then flip again.\n\nThus the additional storage is 32 bytes per node, but potentially speeding up the first route request in a `pay` algorithm.\n\nOf note, is that this is not intended to replace `getroute` --- instead this is a faster but more limited alternative.\nWe should still retain the generic `getroute` as a fallback in case the easy-to-find path exceeds `maxfeepercent` or `maxdelay`.\n\nA\\* on Lightning\n----------------\n\nAs mentioned above, A\\* is the typical algorithm in use by real-time strategy and other games.\nIts primary difference from Dijkstra, is the existence of the `h(n)` heuristic function, which accepts an arbitrary node `n` and returns the expected cost of going from that node to the destination.\n\nA\\* has the same complexity O(n log n) as Dijkstra, but in practice finds paths faster than Dijkstra can since A\\* focuses on exploring nodes that seem likely to reach the destination faster.\nRuns of A\\* vs Dijkstra on the same map require fewer nodes to be visited by A\\* before reaching the destination, resulting in faster runs of A\\* than Dijkstra.\n\nHowever A\\* use is not possible under Lightning as we cannot provide any estimate of the cost from a node to a particular other node, which is needed for the heuristic function `h(n)`.\n\nThat is, unless we already have the costs pre-computed and cached somewhere... such as the `getroutequick` Dijkstra run.\n\nThe `dijkstra` structure is basically just the total cost of arriving at that node from the source node in the Dijkstra run.\nFor the `dijkstra_cached` double-buffered structures, these are the total cost of arriving at that node from the singular source node we have, our own self node.\n\nAn A\\* is now possible by starting at the payee node with the goal node being our own (payer) node.\nThe `dijkstra_cached` structures contain the cost of reaching the node from our own node, and serves as our `h(n)` for the A\\*.\nAssuming the channel topology then does not change, the `h(n)` is now an admissible heuristic, and in fact is an exact heuristic and A\\* will run very quickly.\nhttp://theory.stanford.edu/~amitp/GameProgramming/Heuristics.html#exact-heuristics\n\nNow, as noted previously, this cached data may become stale and no longer exact.\nThere are two cases:\n\n1.  A new channel reduces the actual cost of going to a node from our own node.\n    This makes the cached data an inadmissible heuristic, as the cost of reaching that node from our own node is now lower than the cached cost.\n2.  A deleted channel removes the shortest path to a node from our own node.\n    The cached data remains an admissible heuristic and A\\* can find an optimal route quickly with help of the existing cached `h(n)`.\n\nThe drawback of the first case may be acceptable in most cases: our tradeoff is faster payment attempts, not most-optimal-ever paths.\nThe second case is precisely the case that would cause the above `getroutequick` proposal to fail, so A\\* still succeeding here is a bonus.\n\nThus, we can instead change the `getroutequick` algorithm as so:\n\n1.  Locate the payee node.\n2.  Attempt to generate a route from the payee by looking up the shortest-path tree to our own node.\n3.  If the above fails because a channel has been closed, instead use A\\* starting on the node we were unable to continue the quick path on, with the precomputed total costs used for `h(b)`.\n    Splice from the generated partial route to the A\\* route result, then fix up the path by removing loops (as in the `permuteroute` algorithm).\n\nFirst using the `dijkstra_cached` data to form a route without A\\* is faster since A\\* will need to set up an OPEN set.\nIn addition, in case multiple candidates with same `h(n)` occur, A\\* will insist on putting both in the OPEN set, when in principle we can just take either path (as both paths are equal in cost).\n\nWe expect `getroutequick` to still be fast in the best case where channels have not been removed and the shortest-path tree is still valid for the payment node.\nIf channels have been removed, then it falls back to almost-as-fast A\\*, using the same cost data to approximate the distance to the source.\nThus, the same `pay` algorithm will work, and has an improved chance of being able to quickly find its first route.\n\nDirect Channels\n---------------\n\nA user might notice that our node is taking long routes to a payee.\nThis user might then decide to manually open a direct channel to that payee in hope of reducing their fee and risk costs.\n\nHowever, because we only periodically refresh the `dijkstra_cached` data, our initial attempts at paying will ignore the direct route.\nThis will disappoint the user and cause him or her to file an issue in our bug database, which is generally considered bad.\n\nA simple way to fix this is to provide a `getroutedirect` that only scans for direct channels from our own node to the destination.\nThen we simply prioritize using such a direct route first regardless of the state of our pathfinding acceleration structure.\n\nAs the user can only control its own node (typically), we generally can assume that the user will consider it acceptable if channels between nodes the user does not control are ignored in our pathfinding acceleration.\nHowever since the user is in direct control of our own node, he or she expects that the node can find its own direct channels to the payee easily.\nWe can consider this equivalent to the user micromanaging our payments, and since the user can only control our own node, it can only do so by making direct channels.\n\nOur `pay` algorithm then becomes:\n\n1.  Use `getroutedirect`.\n2.  If that fails, use `getroutequick`.\n3.  If that fails, use `getroute`.\n4.  Use the current route for `sendpay`.\n5.  If `sendpay` fails, use `permuteroute`.\n6.  If that fails, use `getroute`.\n7.  GOTO 4.\n\nAlternately, we can simply add this direct-channel behavior to `getroutequick`, having it try three sub-algorithms: direct channel finding, shortest-path tree traversal, and finally A\\*.\n\nSkip Links\n==========\n\nNote that `getroute` remains a bottleneck.\nIt is the fallback when `getroutequick` or `permuteroute` cannot find a path or returns a too-expensive path.\n\nAnother technique from the real-time strategy game cookbook is: https://theory.stanford.edu/~amitp/GameProgramming/MapRepresentations.html#skip-links\n\nThis is yet another \"preprocessing\" technique.\nWe have a continuously-running background task that simply randomly explores the map.\n\nIt starts at some randomly-selected node, then follows one least-cost channel to another node (avoiding already-visited nodes).\nAfter a few such hops, it creates a \"skip link\" from the first node to the last node.\nThis is an extra link containing the total cost of the path from the first node to the last node, as well as the exact path itself.\n\nThe stored total cost of the path is discounted mildly; typical guidelines are 1% reduction of the cost.\nThe Dijkstra algorithm immediately puts the other end of the skip link to the unvisited set, pointing through the skip link.\n\nThe discount exists to \"break ties\".\nA \"tie\" occurs when the cost on two possible paths is equal.\nThis makes the discount bias towards the preexisting skip link.\n\nSkip links need to store the individual channels passed through.\nIn particular, if we know that one of the component channels of the skip link is currently failing, we should ignore the entire skip link.\n\nWith skip links, there is now a tiny possibility of non-optimal pathfinding, which should still be fine.\n(we could implement a `getrouteslow` that ignores skip links.)\nIn particular we might reach a destination by first going through a skip link that goes through that node, then a \"back out\" hop from the end of the skip link to the destination node.\nThis can be fixed by a postprocessing step (similar to that in `permuteroute`) to shortcut such loops in the route.\n\nHierarchical Maps and Rough/Smooth Pathfinding\n==============================================\n\nAnother preprocessing idea for maps is to create a high-level \"rough\" map and a low-level \"smooth\" map.\nThen, we can start with a \"rough\" path and then incrementally promote this to a \"smooth\" path with all the details as our game unit / payment approaches the next location / node in the rough path.\n\nSee: https://theory.stanford.edu/~amitp/GameProgramming/MapRepresentations.html#hierarchical\n\nNow this might not make sense in the current Lightning Network.\n\nHowever, do note that one incoming improvement we are planning to introduce is \"trampoline routing\", where we have a \"rough\" path of various (not necessarily adjacent) nodes embedded in a \"smooth\" path of directly-adjacent nodes.\n\nSo we can build a \"rough\" map from the normal detailed map as follows:\n\n1.  Randomly select some small fraction of nodes with high liquidity in channels, and (if we can get this information) high uptime.\n    We might want to randomly eliminate some of these for no reason from the set of nodes to consider.\n    We shall call these the \"high-level\" nodes.\n2.  Use the smooth map to determine the costs of the shortest paths between these nodes.\n    Use a limited-hops Dijkstra to limit the rough links between nodes to some number of hops on the smooth map.\n    Set the cost of the links in the \"rough\" map based on the cost of the smooth path.\n3.  Find the largest \"island\" of interconnected high-levels nodes and use that as the high-level \"rough\" map.\n\nThis is a preprocessing stage very much like what we would do for `getroutequick`, and double-bufferring while refreshing this high-level map is likely to be useful as in the `getroutequick` case.\n\nThen, when attempting to pay to an arbitrary node:\n\n1.  Find a route from the destination node to *any* of the nodes in the high-level map; use the normal low-level \"smooth\" map.\n    Dijkstra can also work with multiple candidate destinations, exiting once any of those candidates is reached.\n    (we can also add the source node as a candidate, and with such a direct route can exit at this point without a trampoline route)\n    Keep track of the \"destination-side high-level node\" and discard the actual route.\n2.  Find a route from the source node to *any* of the nodes in the high-level map; use the normal low-level \"smooth\" map.\n    Keep track of the \"source-side high-level node\" but retain the actual route.\n3.  On the high-level \"rough\" map, find a route from source-side high-level node to the destination-side high-level node.\n    Append the destination node to this to form the trampoline part of the route.\n\nThis provides a normal route to a high-level \"rough\" node, then a short trampoline route from that node to the destination.\n\nStep #2 above can be cached: we can run a Dijkstra starting at our own node running on the \"smooth\" map, that terminates once it has reached some small number of high-level nodes (say the 5 nearest high-level nodes), then just round-robining among those nodes per invocation of route finding.\nThis caching can be done in the same refresh as the one that generated the high-level rough map in the first place.\n\nStep #3 is expected to be very fast since it looks at the rough map, which has a tiny fraction of the nodes in the smooth map, and Dijkstra is O(n log n) on number of nodes.\n\nThis leaves step #1 to be the potentially slow part of this algorithm.\nThis is mitigated by the fact that this stage has multiple candidate destinations, and terminates as soon as it reaches *any* candidate.\nThus we expect Dijkstra to only have to explore a small amount of nodes.\nThe more distributed the high-level nodes are, the more likely that we can reach any high-level node from the destination in a small number of hops.\n\nAs channels open and close, the trampoline route remains potentially valid: as a trampoline node opens one hop of the trampoline route it can derive the \"smooth\" path from itself to the next trampoline node.\nThis thus implements the Rough/Smooth pathfinding idea.\n\nIf trampoline routes can be concatenated without permission from the original source of the trampoline route, then a trampoline node may itself utilize this algorithm when sending to the next trampoline node.\n\nA privacy-preserving implementation would prefer to periodically \"refresh\" the high-level map (possibly changing the nodes that are put in the high-level map to increase the chances that it will not use a high-level node repeatedly).\nAgain, this will be similar to the `getroutequick` refresh, we double-buffer so that we can use the existing high-level rough map while generating the next high-level rough map.\n\nTr\\*sted Rough Maps\n-------------------\n\nCentralized tr\\*sted servers might exist that provide rough maps to nodes with extremely limited space (and cannot store the smooth, detailed channel-level map at all).\nThe server selects a small fraction of the available nodes, preferring high-uptime high-connectivity nodes, and generates a rough map as above.\n\nThen, for each node, it determines the nearest high-level node.\nEach high-level node has some fixed-size probabilistic set representation, such as a Bloom filter or a Goulomb-coded set.\nWhen it determines for a node which high-level node it is nearest to, it adds that node to the Bloom filter / Goulomb-coded set for that high-level node.\n\nA payer node that has no knowledge of the detailed channel map beyond its own channels can generate a rough trampoline route as follows:\n\n1.  It searches the payee in each high-level node in the rough map, using the Bloom filter / Goulomb-coded set.\n    Looking up in a Bloom filter would be O(1), and we would have to scan the entire map, so complexity is O(n) effectively.\n2.  It searches for the nearest high-level node to itself, again using the Bloom filter / Goulomb-coded set.\n3.  It uses the rough map to route from the high-level node near itself to the high-level node near the payee.\n    Then it appends the payee to this route.\n\nKnowing only its own channels, it can simply deliver the above generated trampoline route to any adjacent node that claims to support trampoline routing.\n\nThis provides some measure of privacy: the payer does not need to reveal any data to the server, other than the fact that it wants a rough map.\nIf the map is small enough, it could cache for multiple payment attempts.\nFinally, it could also get multiple such maps from different servers, and score each map based on how successful payment ends up being when using that map.\n\nThese servers are tr\\*sted to indicate high-uptime nodes with good liquidity and connectivity to the network, and to not lie about the actual network topology.\nThey are also tr\\*sted to not generate maps that have themselves in the center of the map and have every other high-level node only have edges to their central node.\nAs such, users of this system must consider how much trust they can put in these servers.\n\nThe payer node will need to periodically get updates of the rough map as the topology changes.\nA tr\\*sted server might use the pay-for-data protocol to require payment for providing this map.\nA newly deployed node will need to be started with a tr\\*sted (and probably obsolete) rough map, however.\nAlternately a protocol delivering a proof-of-payment can be executed onchain, or the server can publish its node pubkey and public contact point so that the node can start its first channel with the server and pay for the rough map over a direct channel.\n\nSelf-Serving\n------------\n\nServers providing rough maps like the above can be written with free and open source software.\nThus, one might run a limited-smooth-map node on the same hardware as a server generating rough maps for the limited-smooth-map node.\n\nOne might wonder though, whether it might be better to just run a node that can keep the entire smooth map and not use a rough map at all.\n\nThe difference here is that the server does not need to perform any pathfinding in a tight schedule.\nAll its smooth-map pathfinding is done to simply determine which high-level node every node on the smooth map is near to, and thiis not need to be fast in order to serve outgoing payments quickly.\nThis means the server can potentially store the smooth routemap in a database in slow persistent storage rather than in high-speed memory.\n\nIndeed, since the rough map is likely to still be mostly valid even as the network changes topology, the server can generate new rough maps while the hardware is idle, then coordinate with the limited-smooth-map node on every update of a fresh rough map.\n\nThis removes tr\\*st issues, in much the same way that running Electrum Personal Server on a fullnode one controls removes the tr\\*st issue in Electrum and leaves it as a high-quality wallet interface.\n\nThis also provides an approach towards scaling up our nodes to handle routemaps with 100 billion channels.\nWe can conceptually split our routemap between a \"total global routemap\" stored strictly on disk, and a \"myopic local routemap\" that is kept in memory.\nThe server stores the entire detailed global routemap on-disk.\nThe node software queries the server for two things:\n\n1.  Nodes and channels within N hops of the node.\n    This is the myopic local routemap.\n2.  A rough routemap of the rest of the network as described above.\n\nThe node keeps both of the above in-memory, but they should be small compared to the total global routemap stored on-disk.\nKeeping the map in-memory lets the node quickly find routes.\n\n1.  If the destination is in the local routemap already, then the node does not need to do trampoline routing.\n2.  If the destination is not in the local routemap, then the node uses the rough routemap to find a reasonable trampoline route, then finds a path in the local routemap to the first node in the trampoline.\n\nOur bandwidth requirements still remain (we still need to gossip the entire routemap containing 100 billion channels).\nSimilarly, our on-disk space requirement still remains.\nHowever it does greatly reduce our memory requirement, making it possible to still fit Lightning nodes on single-board-computer-level devices (admittedly, those that are connected to large permanent storage and fast Internet connections, but let me work on one problem at a time mkay?).\n\nMyopic Trampoline Nodes\n-----------------------\n\nI have argued this before, but I think that nodes that do not have a complete smooth-level routemap should still be allowed to advertise themselves as trampoline nodes.\n\nAs the network grows to 100 billion channels, fewer and fewer nodes will have the ability to find detailed paths from themselves to any arbitrary next-trampoline-node.\nIt is important that the risk of running trampoline nodes be shared widely, and we should strongly prefer the \"trampoline advertising feature bit\" to be a proxy for \"not too lazy to update the software\" rather than \"rich enough to afford really good hardware\".\n\nHowever, this implies that some trampoline nodes will fall back to themselves using a rough map to reach some nodes.\nThis implies that they need to prepend additional nodes to the trampoline route, without permission from whoever created the original trampoline route.\n\nFlocking\n========\n\nA common issue in real-time strategy games is that a player, fighting for an important objective, needs to immediately send a large group of units to a location.\nIf all the units were to individually request for paths from the pathfinding system, then the units would lag and start moving one by one, as the pathfinding system gets overloaded and gives the paths one by one.\nThis is particularly bad behavior if the player is currently in a high-pressure situation where the objective might be lost soon if the commanded units are unable to move immediately to the objective.\n\nWorse, often the pathfinding system will make the units follow almost the exact same path, meaning that slower units can prevent faster units from going around them, and they will all travel in single file, which is usually a strategic problem when their line is intercepted by enemy units.\n\nThe usual solution for this is, when multiple units are selected and commanded to move to a single location, is:\n\n1.  Select one unit at random as leader.\n2.  Have only the leader request a path from the pathfinding system.\n3.  Have the other units \"flock\" around that leader: http://www.red3d.com/cwr/boids/\n\nFlocking simply means that units follow some rules:\n\n1.  Do not get too near other flock members.\n2.  Do not get too far other flock members.\n3.  Face in the same direction as other flock members (if units in the game world have this sense).\n4.  Leaders ignore the above rules, but are considered flock members by the other flock members.\n\nThis reduces the pathfinding load to only one unit, the leader unit, and the rest of the units simply follow the designated leader until the leader reaches the objective location, or is killed or given another command (in which case another leader is chosen).\nThe individual units can request shorter paths from the pathfinding system to any point near the leader, or if it is \"near enough\" to the leader it can simply stop where it is.\nAs they have been following the leader, other units should be \"near\" to the indicated location when the ledaer has reached the destination, so the paths they request should be short and it should not take too long for the pathfinding system to get those paths.\n\nFurther, this often encourages the grouped units to move as a \"blob\" on multiple nearby paths rather than in single file, which is generally better for strategic defense when they are intercepted by enemies.\n\nNow of course in Lightning Network we do not have anything like this problem, right?\n\nRight?\n\nNow consider the Feature Either Called AMP or Multi-Part Payments or Bass Amplifier.\nIn this case, we have multiple payments arising from a source, all of which are commanded to then converge at a singular destination.\nWe would like:\n\n1.  Not to spend two entire seconds for each alternate path.\n2.  Have the various split payments travel in different paths, not in \"single file\" where they mostly travel the same path except they start at different channels but almost immediately converge to travelling over the same channels.\n\nFlocking Over Lightning\n-----------------------\n\nSuppose our node is asked to provide a large payment to some destination.\nOur node then computes a path (possibly using any of the previous shortcuts to speed up this initial pathfinding).\n\nHowever, the direct channel where this route starts from has too little liquidity on our side.\nSo our node also selects another channel from which to make up the difference in liquidity, using AMP/Multi-part/Bass Amplifier.\n\nThis is a \"split\" payment, and we then use a flocking system, with the first payment as the leader.\nOur rules are:\n\n1.  At each hop of the leader route, we should seek any route that gets us to within one node away from the leader.\n2.  Channels going to nodes that the leader uses on its route (or which have been used by the other split payments in the flock) have a much higher cost.\n3.  Channels directly connected to the source cannot be used at all (as that defeats the entire point of doing multipath).\n\nWe simulate the leader following its path one hop at a time.\nThe leader starts at the source, then takes one hop to the next node.\n\nThe split payment starts at the other node of the channel we selected to make up the difference in payments.\nWe then use a multi-destination Dijkstra that terminates as soon as it finds any node that is within one hop of where the leader currently is.\nChannels used by the leader, and those already used by other splits, are penalized in cost so that we avoid reusing them, but still can pass through them if they are the only options available.\nThis initial Dijkstra can potentially take a long time, but hopefully it should be easy to cut short since it is a multiple-destination Dijkstra and achieving any destination terminates the algorithm.\n\nFor subsequent hops of the leader, we use a multi-destination limited-hops Dijkstra.\nAs the split payment starts one node away from the previous node of the leader, in the worst case it can simply just jump to the previous node of the leader if it cannot find an alternate path close by.\nJumping to this is penalized by rule 2 above so other paths are preferred, but this still allows the \"flock\" to pass through chokepoints in the network.\n\nWhen we simulate the leader reaching the destination node, then we use a multi-destination limited-hops Dijkstra to the destination node.\nThis lets every split payment reach the leader.\n\nThus, the payments form a \"blob\" around the leader, instead of almost always following the same channels.\nThis provides better reliability of the entire payment attempt.\n\nConclusion\n==========\n\nLightning Network does not need to pioneer any new research into pathfinding.\nThere already exists an entire field of programming that has similar issues (dynamic map, incomplete information, large maps with low acceptable latency of pathfinding, quickly-found \"okay\" paths are better than slowly-found \"best\" paths).\nThat is real-time strategy game programming, and such games have existed in some form or another for the past 4 decades.\nShortcuts, techniques, and optimizations used in real-time strategy games deserve closer examination, as potential points of inspiration for similar techniques for Lightning.\n\nThis writeup is a result of a quick review of pathfinding techniques used in real-time strategy games.\nThere are potentially many more techniques whose ideas might be possible to adapt in finding Lightning Network routes in large public networks in short amounts of time with acceptable fees and locktimes."
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2019-08-01T02:29:46",
                "message_text_only": "> I found out recently (mid-2019) that mainnet Lightning nodes take an\n> inordinate amount of time to find a route between themselves and an\n> arbitrary payee node.\n> Typical quotes suggested that commodity hardware would take 2 seconds to\n> find a route\n\nCan you provide a reproducible benchmark or further qualify this number (2\nseconds)? Not commenting on the rest of this email as I haven't read the\nrest of it yet, but this sounds like just an issue of engineering\noptimization. AFAIK, most implementations are using unoptimized on-disk\nrepresentations of the graph, do minimal caching, and really haven't made\nany sort of push to optimize these hot spots. There's no reason that finding\na path in a graph of a 10s of thousands of edges should take _2 seconds_.\n\nBeyond that, to my knowledge, all implementations other and lnd implement a\nvery rudimentary time based edge/node pruning in response to failures. I\ncall it rudimentary, as it just waits a small period of time, then forgets\nall its past path finding history. As a result, each attempt will include\nnodes that have been known to be offline, or nonoperational channels,\neffectively doing redundant work each attempt.\n\nThe latest version of our software has moved beyond this [1], and will\nfactor in past path finding attempts into its central \"mission control\",\nallowing it to learn from each attempt, and even import existing state into\nits path finding memory (essentially a confidence factor that takes into\naccount the cost of a failed attempt mapped into a scalar weight we can use\nfor comparison purposes). This is just an initial first step, but we've seen\na significant improvement with just a _little_ bit more intelligence in our\npath finding heuristics. We should take care to not get distracted by more\ndistant \"pie in the sky\" like ideas (since many of them are half-baked),\nlest we ignore these low hanging engineering fruits and incremental\nalgorithmic updates.\n\n> This is concerning, of course, since we would like the public Lightning\n> Network to grow a few more orders of magnitude.\n\nI'd say exactly _how large_ the _public_ graph needs to be is an open\nquestion. Most of the public channels in the network today are more\nextremely underutilized with capital largely being over allocated. Based on\nour active network analysis, only a few hundred nodes are actively\nmanaging their channels effectively, allowing them to be effective routing\nnodes.\n\nmoar channels != better\n\nAs a result, clients today are able to ignore a _majority_ of the known\ngraph, and still have their payments attempts be successful, as they'll\nignore all the routing nodes that aren't actually walking the walk (proper\nchannel management).\n\n-- Laolu\n\n[1]: https://github.com/lightningnetwork/lnd/pull/2802\n\n\nOn Wed, Jul 31, 2019 at 6:52 PM ZmnSCPxj via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> wrote:\n\n> Introduction\n> ============\n>\n> I found out recently (mid-2019) that mainnet Lightning nodes take an\n> inordinate amount of time to find a route between themselves and an\n> arbitrary payee node.\n> Typical quotes suggested that commodity hardware would take 2 seconds to\n> find a route, then take a few hundred milliseconds for the actual payment\n> attempt.\n> With the help of Rene Pickhardt I was able to confirm that indeed, much of\n> payment latency actually arises from the pathfinding algorithm and not the\n> actual payment-over-routes.\n>\n> This is concerning, of course, since we would like the public Lightning\n> Network to grow a few more orders of magnitude.\n> Given that the best pathfinding search algorithms will be O(n log n) on\n> the size of the network, we need to consider how to speed up the finding of\n> routes.\n>\n> `permuteroute` and Faster Next Pathfinding Attempts\n> ===================================================\n>\n> As I was collaborating with Rene, JIT-Routing was activated in my core\n> processing hardware.\n>\n> As I was contemplating this problem, I considered, that JIT-Routing would\n> (ignoring fees) effectively \"reroute\" the original route around the failing\n> channel.\n>\n> In particular, JIT-Routing is advantageous for these reasons:\n>\n> 1.  There is no need to report back the failure to the original payer.\n> 2.  The node performing JIT-Routing has accurate information about its\n> channel balances and which of its outgoing channels would be most effective\n> to route through instead of that indicated by the original payer.\n>     It also knows of non-published channels it has.\n> 3.  Searching for a circular rebalancing route could be done much quicker\n> since the JIT-Routing node could restrict itself to looking only in its\n> friend-of-friend network, and simply fail if it could not find a circular\n> rebalancing route quickly in the reduced search space.\n>\n> The first two advantages cannot be emulated by the original payer.\n>\n> However, I realized that the third advantage *could* be emulated by the\n> original payer.\n> This is advantageous as the payer node can implement emulating this,\n> without having to wait for the rest of the network to actually implement\n> JIT-Routing as well.\n>\n> Basically:\n>\n> 1.  On payment failure, the original payer is informed of which node\n> reported the failure and which channel failed.\n> 2.  The original payer looks for a *short* route from the reporting node\n> to the next node (or any node after the reporting node in the original\n> path).\n>     The original payer restricts its search to within 3 hops, roughly\n> equivalent to searching the friend-of-friend network, thus able to quickly\n> get a route or a failure indication.\n>     (we also need to get a set of known-bad channels other than the\n> most-recently failing one, so that we do not bother to scan those channels)\n> 3.  The original payer then replaces the failing channel with the new\n> route found.\n> 4.  As the new route might \"backtrack\" the original payer checks for\n> duplicated nodes and removes the unneeded hops.\n>\n> The payer is now in possession of a variation of the original route, one\n> which avoids the known-failing channel.\n> It has a prefix that we know was recently reliable (the error got reported\n> back to us, after all).\n> Finally, the payer did not take much time to find this alternate route,\n> improving payment latency.\n>\n> I dubbed this new algorithm `permuteroute`.\n>\n> In a `pay` implementation, we would do:\n>\n> 1.  For the *first* routing attempt, use `getroute`.\n> 2.  Use the current route for `sendpay`.\n> 3.  If the pay attempt fails, use `permuteroute`.\n>     If returned route is too expensive or it fails, fall back to\n> `getroute`.\n> 4.  GOTO 2.\n>\n> The `permuteroute` is fast due to not having to scan a large number of\n> channels.\n> Ideally it would use Dijkstra with a restricted number of hops, but this\n> is not strictly necessary and a simple flood-fill can work about as well.\n>\n> JIT-Routing is still strictly superior to `permuteroute`, as the node\n> reporting the failure has strictly more information about local conditions.\n> (Yes, it could return with a \"suggested other channel\", but that requires\n> a spec change and it would be better to focus a spec change on feeless\n> rebalancing to support JIT-Routing instead, especially since JIT-Routing\n> can potentially effectively create a \"local multipath payment\" by\n> rebalancing from multiple other channels, especially if rebalancing can be\n> made feeless.)\n> However, JIT-Routing benefits start being noticeable only once a majority\n> of nodes have supported it, whereas `permuteroute` gets immediate benefits\n> to the node that upgrades to utilize it.\n>\n> In particular, `permuteroute` would be helpful to support JIT-Routing\n> nodes while there are still few JIT-Routing nodes.\n> A particular part of `permuteroute` is that it reuses the known-successful\n> prefix of an existing route, replacing only channels from the failed\n> portion and later.\n> If only one JIT-Routing node existed, it would suffer somewhat since while\n> it would increase payment success rates, it would still lose (due to\n> rebalancing fees) if a later channel in the hop fails and the node with\n> that channel does not itself do JIT-Routing.\n> With `permuteroute` thie JIT-Routing node and its outgoing channel would\n> end up being in the known-successful prefix of a `permuteroute` from the\n> same payer, thus not wasting its \"investment\" in rebalancing in favor of\n> its outgoing channel.\n>\n> An unfinished pull request for `permuteroute` on C-Lightning exists:\n> https://github.com/ElementsProject/lightning/pull/2890\n>\n> Lightning Network Pathfinding Problem Statement\n> ===============================================\n>\n> Let us mildly digress.\n>\n> Pathfinding on Lightning has these requirements:\n>\n> 1.  We are looking for a \"good enough\" path, not the most optimal path.\n>     Going for the optimal path is good, but users are more interested in\n> payment success than purely fee optimization.\n> 2.  The map we are doing the pathfinding in is dynamically changing and we\n> might not have access to the latest state.\n> 3.  We need to look for paths in a large map in a very short time.\n>\n> Real-Time Strategy Game Pathfinding Problem Statement\n> =====================================================\n>\n> Rene also mentioned that what LN generally calls \"routefinding\" is also\n> called \"pathfinding\" elsewhere.\n> This triggered me to consider real-time strategy games and pathfinding\n> algorithms used in them.\n>\n> In particular, the above requirements we have, have analogies to the\n> problem of real-time strategy games.\n>\n> 1.  Players want their units to start moving as soon as commanded; they\n> have a myriad other things to manage and do not want to have to micromanage\n> certain movements.\n>     For example, moving through known-safe locations or scouting into\n> unknown places often does not need to be optimal: they just need to be\n> *done* in a manner that is not thoroughly stupid.\n>     If players need a specific path to be followed, they will micromanage\n> their units, but non-micromanaged units are expected to perform their\n> actions in the background, and non-optimal paths for non-micromanaged units\n> are often acceptable as long as it is not ***too*** sub-optimal.\n> 2.  Good real-time strategy games have a dynamically-changing world: trees\n> are cut down for wood, walls are erected by opponents, structures are\n> placed for various optimal results, mobile units deploy into immobile\n> modes, and so on.\n>     Good real-time strategy games will also not let the units of a player\n> know the ***current*** real state of the game world, as that leaks\n> strategic information about where walls and defense structures have been\n> built, where resources have been harvested, what chokepoints are blocked by\n> immobilized units with increased combat effectiveness, etc.\n>     Instead, units are given fog-of-war-covered maps, with obsolete\n> information, and thus may find their pre-solved paths blocked, or may find\n> that shortcuts now exist which did not exist before.\n>     Even so, units must be able to correct their current paths once they\n> discover this fact, without too much latency (as they might now be subject\n> to attack by the immobile units siege-moded against them, or defense\n> structures behind walls, etc.).\n> 3.  As a real-time game, commands must be reacted to in a time-frame that\n> is below typical human notice.\n>     While real-time games need to traverse simpler, smaller, and more\n> regular maps than LN routemaps, they also need to get results for possibly\n> dozens of different units in a few dozen milliseconds, whereas LN\n> pathfinding would probably be well-satisfied by getting one route after a\n> few hundred milliseconds.\n>\n> Thus, I started looking for techniques and shortcuts used in real-time\n> strategy games.\n>\n> A\\* (A-star) is the preferred algorithm in the \"geographic\" maps usually\n> used in real-time strategy games.\n> In many cases we can replace it \"seamlessly\" with Dijkstra in the LN case:\n> a constant `h(n)` function makes A\\* devolve to Dijkstra, and if we admit\n> that we have no decent heuristic \"distance\" function, we can consider\n> ourselves as \"really\" using A\\* with a constant `h(n)` (which happens to\n> make the algorithm behave as Dijkstra).\n>\n> But strictly speaking, using A\\* (and its \"degenerate\" form Dijkstra) is\n> overrated: http://bitsquid.blogspot.com/2010/10/is-overrated.html\n>\n> Path Splicing\n> =============\n>\n> One technique I found was Path Splicing:\n> http://theory.stanford.edu/~amitp/GameProgramming/MovingObstacles.html#path-splicing\n>\n> I realized that this technique was exactly what my `permuteroute`\n> algorithm did: instead of solving for a completely new path, it would\n> attempt to find a short route to \"heal\" a recently-discovered blockage of\n> the original path.\n> The technique even recommends keeping the search space small; if the path\n> needed to heal the original path becomes too long, it would be better to\n> fall back to a \"full\" pathfinding.\n>\n> As a corollary to this, JIT-Routing is itself an implementation of a form\n> of Path Splicing, except with the potential to form a \"local multipath\n> payment\" when a payment is routed through the node.\n>\n> Emboldened, I began reading more and considering other ways to improve\n> pathfinding latency, as pioneered by real-time strategy games.\n>\n> Digression: Parametrizing `pay`\n> ===============================\n>\n> C-Lightning `pay` command (whose first \"major version\" was created by me,\n> though the current implementation is vastly different from my work, but\n> keeps most of its features and interface) is parametrized with two\n> tweakable arguments:\n>\n> 1.  `maxfeepercent` - The maximum fee, in percentage of the payment value,\n> that the user is willing to pay for payment success.\n> 2.  `maxdelay` - The maximum number of blocks that the user is willing to\n> wait in case of long-term failure of direct peer.\n>\n> This is relevant since costs in Lightning are not a single \"cost of\n> travelling over this terrain\" as in real-time strategy games.\n> Instead, costs in Lightning involve two things: the fee charged by the\n> node, and the number of blocks in the difference of the incoming versus\n> outgoing HTLCs.\n>\n> In particular, internally to C-Lightning, `getroute` has a `riskfactor`\n> parameter, which is intended to balance between these two costs (fees vs\n> blocks).\n> This parameter is expressed as \"annual cost of your funds being stuck\".\n>\n> I personally cannot understand what this \"annual cost of your funds being\n> stuck\" means exactly in relation to my own money and risk-aversion, and\n> have just used whatever value is the default for `getroute`.\n> I can understand the `maxfeepercent` and `maxdelay` better, and I imagine\n> most users would also understand these better (though I have not talked to\n> any users at all).\n> The documentation for the `getroute` command even contains a nice table\n> about how the various settings for `riskfactor` affect your estimation of\n> the cost of risk depending on how large the payment is and how many blocks\n> the hop node charges.\n> I still cannot understand it even so, and my own expectation is that most\n> users will also fail to grasp this intuitively.\n> On the other hand, `maxfeepercent` and `maxdelay` require no tables to\n> explain.\n>\n> In particular, `pay` always compares the total delay and the total fee to\n> the given `maxfeepercent` and `maxdelay`.\n> In theory, if only one is breached but not the other, `pay` could tweak\n> the `riskfactor` in order to change the balance.\n>\n> The difficulty here is that in many of the succeeding sections, we will\n> find that we will need to use some precomputed data, for which this balance\n> of time vs. fee needs to be selected somehow.\n> Thus we want to use some reasonable default for this balancing parameter.\n> If the user wants to diverge from this default balancing, they would\n> suffer as they would need to fall back on not using the precomputed data,\n> reducing the speed of their payments.\n>\n> This should be kept in mind for the succeeding sections: algorithms using\n> precomputed data must by necessity be less flexible (it is restricted to\n> the domain for which the data was precomputed for), and the price of\n> flexibility will be slowness of a more general algorithm.\n>\n> The possibility is that we are in fact being more flexible than absolutely\n> necessary, and some shortcuts may be acceptable as a trade-off for faster\n> pathfinding.\n>\n> Map Preprocessing\n> =================\n>\n> Often, real-time strategy game maps can be preprocessed.\n>\n> For example, wide-open areas often take A\\* a lot of time exploring this\n> space.\n> However, in principle such wide-open areas should not be a pathfinding\n> problem: any arbitrary path through those areas is generally fine and we\n> should really have A\\* focus on the fiddly bits between mountain passes and\n> going through rough terrain and avoiding cul-de-sacs.\n> This is an example where finding the \"optimum\" path can take a back seat\n> to finding \"any acceptably-good\" path: A\\* searches along every good paths\n> through plains in case there is some special path that is really optimal,\n> even though usually any straight-line path through the plains is going to\n> work well enough that the player will not feel he or she has to micromanage\n> the unit path.\n>\n> Map preprocessing can help by reweighing the costs in wide-open areas,\n> making A\\* go through a small number of acceptably-good paths rather than\n> exploring all possible ways to walk a wide-open field (and eating up\n> processing time in exploring all of them).\n>\n> Now, we might not have the advantage of having a \"geographical\" map that\n> can provide some reasonable heuristic distance function `h(n)` for A\\*.\n> But we do have one advantage over real-time strategy games:\n>\n> * Every payment that requires us to find a path, always starts from one\n> node: our own node.\n>\n> This is in stark contrast to real-time strategy games, where arbitrary\n> units arise at various locations, and the pathfinding system must be\n> capable of handling multiple possible start points.\n>\n> In particular, Dijkstra has a variant where it does *not* look for the\n> shortest path between two nodes, but instead finds the shortest path tree\n> from one node to all other nodes on the map.\n> (This is arguably the \"true\" form of Dijkstra.)\n>\n> This \"true\" variant of Dijkstra is a good thing to use, as we know that\n> every payment sourced by our node requires a path starting from our node.\n>\n> In C-Lightning in particular, every node object has a `struct`, named\n> `dijkstra`, containing information used by the Dijkstra algorithm, size of\n> 16 bytes, which is sufficient to derive a path from the destination to the\n> source.\n> We can instantiate another copy of this `struct` to every node, let us\n> call this `dijkstra_cached`, and run the Dijkstra algorithm for all nodes\n> to the source using this alternate copy.\n>\n> Then we can provide a new `getroutequick` command which simply looks up\n> the destination node, then builds the route back to the specific source\n> (our own node).\n> If route-building fails because the channels have been closed, it can\n> simply fail (and the client has to fall back to the slow O(n log n)\n> `getroute`, which uses the existing `dijkstra` field instead of\n> `dijkstra_cached`).\n>\n> A `pay` algorithm would then do:\n>\n> 1.  For the *first* payment attempt, use `getroutequick`.\n>     If the returned route is too expensive or `getroutequick` fails, fall\n> back to `getroute`.\n> 2.  Use the current route for `sendpay`.\n> 3.  If the pay attempt fails, use `permuteroute`.\n>     If returned route is too expensive or it fails, fall back to\n> `getroute`.\n> 4.  GOTO 2.\n>\n> Thus, the slow `getroute` only exists in the \"fallback\" case, and we have\n> effectively optimized payment to arbitrary nodes.\n> Payment attempts can start to be sent out almost immediately.\n>\n> ***Now of course, the `dijkstra_cached` map will become obsolete as\n> channels are created and destroyed***.\n>\n> Periodically, we can start a background process to perform a new Dijkstra\n> run to update the cached data.\n> We can implement this by a \"fast\" timer that performs some fraction of the\n> Dijkstra run, then lets normal operation for some time before grabbing the\n> CPU again to update the Dijkstra.\n> This lets us process normal requests (updates of channel, `getroute` and\n> `permuteroute` requests, etc.) while refreshing the cached shortest path\n> tree.\n> (other implementations might consider using separate threads, or some\n> concept of low-priority thread: the C-lightning `gossipd` is\n> single-threaded so we can avoid many of the issues with thread\n> synchronization)\n>\n> Of course, that means that `getroutequick` cannot work while we are\n> performing this refresh.\n> To fix this, we turn to another game implementation technique:\n> double-buffering.\n>\n> We use a `dijkstra_cached[2]` array, then fill in *one* entry during the\n> refresh while `getroutequick` calls follows the *other* entry.\n> Then when the refresh is completed, we \"flip\" the buffer that\n> `getroutequick` uses (a single variable somewhere in our map object).\n> Then on the next refresh, we use the `dijkstra_cached` entry that is not\n> being used for `getroutequick`, then flip again.\n>\n> Thus the additional storage is 32 bytes per node, but potentially speeding\n> up the first route request in a `pay` algorithm.\n>\n> Of note, is that this is not intended to replace `getroute` --- instead\n> this is a faster but more limited alternative.\n> We should still retain the generic `getroute` as a fallback in case the\n> easy-to-find path exceeds `maxfeepercent` or `maxdelay`.\n>\n> A\\* on Lightning\n> ----------------\n>\n> As mentioned above, A\\* is the typical algorithm in use by real-time\n> strategy and other games.\n> Its primary difference from Dijkstra, is the existence of the `h(n)`\n> heuristic function, which accepts an arbitrary node `n` and returns the\n> expected cost of going from that node to the destination.\n>\n> A\\* has the same complexity O(n log n) as Dijkstra, but in practice finds\n> paths faster than Dijkstra can since A\\* focuses on exploring nodes that\n> seem likely to reach the destination faster.\n> Runs of A\\* vs Dijkstra on the same map require fewer nodes to be visited\n> by A\\* before reaching the destination, resulting in faster runs of A\\*\n> than Dijkstra.\n>\n> However A\\* use is not possible under Lightning as we cannot provide any\n> estimate of the cost from a node to a particular other node, which is\n> needed for the heuristic function `h(n)`.\n>\n> That is, unless we already have the costs pre-computed and cached\n> somewhere... such as the `getroutequick` Dijkstra run.\n>\n> The `dijkstra` structure is basically just the total cost of arriving at\n> that node from the source node in the Dijkstra run.\n> For the `dijkstra_cached` double-buffered structures, these are the total\n> cost of arriving at that node from the singular source node we have, our\n> own self node.\n>\n> An A\\* is now possible by starting at the payee node with the goal node\n> being our own (payer) node.\n> The `dijkstra_cached` structures contain the cost of reaching the node\n> from our own node, and serves as our `h(n)` for the A\\*.\n> Assuming the channel topology then does not change, the `h(n)` is now an\n> admissible heuristic, and in fact is an exact heuristic and A\\* will run\n> very quickly.\n>\n> http://theory.stanford.edu/~amitp/GameProgramming/Heuristics.html#exact-heuristics\n>\n> Now, as noted previously, this cached data may become stale and no longer\n> exact.\n> There are two cases:\n>\n> 1.  A new channel reduces the actual cost of going to a node from our own\n> node.\n>     This makes the cached data an inadmissible heuristic, as the cost of\n> reaching that node from our own node is now lower than the cached cost.\n> 2.  A deleted channel removes the shortest path to a node from our own\n> node.\n>     The cached data remains an admissible heuristic and A\\* can find an\n> optimal route quickly with help of the existing cached `h(n)`.\n>\n> The drawback of the first case may be acceptable in most cases: our\n> tradeoff is faster payment attempts, not most-optimal-ever paths.\n> The second case is precisely the case that would cause the above\n> `getroutequick` proposal to fail, so A\\* still succeeding here is a bonus.\n>\n> Thus, we can instead change the `getroutequick` algorithm as so:\n>\n> 1.  Locate the payee node.\n> 2.  Attempt to generate a route from the payee by looking up the\n> shortest-path tree to our own node.\n> 3.  If the above fails because a channel has been closed, instead use A\\*\n> starting on the node we were unable to continue the quick path on, with the\n> precomputed total costs used for `h(b)`.\n>     Splice from the generated partial route to the A\\* route result, then\n> fix up the path by removing loops (as in the `permuteroute` algorithm).\n>\n> First using the `dijkstra_cached` data to form a route without A\\* is\n> faster since A\\* will need to set up an OPEN set.\n> In addition, in case multiple candidates with same `h(n)` occur, A\\* will\n> insist on putting both in the OPEN set, when in principle we can just take\n> either path (as both paths are equal in cost).\n>\n> We expect `getroutequick` to still be fast in the best case where channels\n> have not been removed and the shortest-path tree is still valid for the\n> payment node.\n> If channels have been removed, then it falls back to almost-as-fast A\\*,\n> using the same cost data to approximate the distance to the source.\n> Thus, the same `pay` algorithm will work, and has an improved chance of\n> being able to quickly find its first route.\n>\n> Direct Channels\n> ---------------\n>\n> A user might notice that our node is taking long routes to a payee.\n> This user might then decide to manually open a direct channel to that\n> payee in hope of reducing their fee and risk costs.\n>\n> However, because we only periodically refresh the `dijkstra_cached` data,\n> our initial attempts at paying will ignore the direct route.\n> This will disappoint the user and cause him or her to file an issue in our\n> bug database, which is generally considered bad.\n>\n> A simple way to fix this is to provide a `getroutedirect` that only scans\n> for direct channels from our own node to the destination.\n> Then we simply prioritize using such a direct route first regardless of\n> the state of our pathfinding acceleration structure.\n>\n> As the user can only control its own node (typically), we generally can\n> assume that the user will consider it acceptable if channels between nodes\n> the user does not control are ignored in our pathfinding acceleration.\n> However since the user is in direct control of our own node, he or she\n> expects that the node can find its own direct channels to the payee easily.\n> We can consider this equivalent to the user micromanaging our payments,\n> and since the user can only control our own node, it can only do so by\n> making direct channels.\n>\n> Our `pay` algorithm then becomes:\n>\n> 1.  Use `getroutedirect`.\n> 2.  If that fails, use `getroutequick`.\n> 3.  If that fails, use `getroute`.\n> 4.  Use the current route for `sendpay`.\n> 5.  If `sendpay` fails, use `permuteroute`.\n> 6.  If that fails, use `getroute`.\n> 7.  GOTO 4.\n>\n> Alternately, we can simply add this direct-channel behavior to\n> `getroutequick`, having it try three sub-algorithms: direct channel\n> finding, shortest-path tree traversal, and finally A\\*.\n>\n> Skip Links\n> ==========\n>\n> Note that `getroute` remains a bottleneck.\n> It is the fallback when `getroutequick` or `permuteroute` cannot find a\n> path or returns a too-expensive path.\n>\n> Another technique from the real-time strategy game cookbook is:\n> https://theory.stanford.edu/~amitp/GameProgramming/MapRepresentations.html#skip-links\n>\n> This is yet another \"preprocessing\" technique.\n> We have a continuously-running background task that simply randomly\n> explores the map.\n>\n> It starts at some randomly-selected node, then follows one least-cost\n> channel to another node (avoiding already-visited nodes).\n> After a few such hops, it creates a \"skip link\" from the first node to the\n> last node.\n> This is an extra link containing the total cost of the path from the first\n> node to the last node, as well as the exact path itself.\n>\n> The stored total cost of the path is discounted mildly; typical guidelines\n> are 1% reduction of the cost.\n> The Dijkstra algorithm immediately puts the other end of the skip link to\n> the unvisited set, pointing through the skip link.\n>\n> The discount exists to \"break ties\".\n> A \"tie\" occurs when the cost on two possible paths is equal.\n> This makes the discount bias towards the preexisting skip link.\n>\n> Skip links need to store the individual channels passed through.\n> In particular, if we know that one of the component channels of the skip\n> link is currently failing, we should ignore the entire skip link.\n>\n> With skip links, there is now a tiny possibility of non-optimal\n> pathfinding, which should still be fine.\n> (we could implement a `getrouteslow` that ignores skip links.)\n> In particular we might reach a destination by first going through a skip\n> link that goes through that node, then a \"back out\" hop from the end of the\n> skip link to the destination node.\n> This can be fixed by a postprocessing step (similar to that in\n> `permuteroute`) to shortcut such loops in the route.\n>\n> Hierarchical Maps and Rough/Smooth Pathfinding\n> ==============================================\n>\n> Another preprocessing idea for maps is to create a high-level \"rough\" map\n> and a low-level \"smooth\" map.\n> Then, we can start with a \"rough\" path and then incrementally promote this\n> to a \"smooth\" path with all the details as our game unit / payment\n> approaches the next location / node in the rough path.\n>\n> See:\n> https://theory.stanford.edu/~amitp/GameProgramming/MapRepresentations.html#hierarchical\n>\n> Now this might not make sense in the current Lightning Network.\n>\n> However, do note that one incoming improvement we are planning to\n> introduce is \"trampoline routing\", where we have a \"rough\" path of various\n> (not necessarily adjacent) nodes embedded in a \"smooth\" path of\n> directly-adjacent nodes.\n>\n> So we can build a \"rough\" map from the normal detailed map as follows:\n>\n> 1.  Randomly select some small fraction of nodes with high liquidity in\n> channels, and (if we can get this information) high uptime.\n>     We might want to randomly eliminate some of these for no reason from\n> the set of nodes to consider.\n>     We shall call these the \"high-level\" nodes.\n> 2.  Use the smooth map to determine the costs of the shortest paths\n> between these nodes.\n>     Use a limited-hops Dijkstra to limit the rough links between nodes to\n> some number of hops on the smooth map.\n>     Set the cost of the links in the \"rough\" map based on the cost of the\n> smooth path.\n> 3.  Find the largest \"island\" of interconnected high-levels nodes and use\n> that as the high-level \"rough\" map.\n>\n> This is a preprocessing stage very much like what we would do for\n> `getroutequick`, and double-bufferring while refreshing this high-level map\n> is likely to be useful as in the `getroutequick` case.\n>\n> Then, when attempting to pay to an arbitrary node:\n>\n> 1.  Find a route from the destination node to *any* of the nodes in the\n> high-level map; use the normal low-level \"smooth\" map.\n>     Dijkstra can also work with multiple candidate destinations, exiting\n> once any of those candidates is reached.\n>     (we can also add the source node as a candidate, and with such a\n> direct route can exit at this point without a trampoline route)\n>     Keep track of the \"destination-side high-level node\" and discard the\n> actual route.\n> 2.  Find a route from the source node to *any* of the nodes in the\n> high-level map; use the normal low-level \"smooth\" map.\n>     Keep track of the \"source-side high-level node\" but retain the actual\n> route.\n> 3.  On the high-level \"rough\" map, find a route from source-side\n> high-level node to the destination-side high-level node.\n>     Append the destination node to this to form the trampoline part of the\n> route.\n>\n> This provides a normal route to a high-level \"rough\" node, then a short\n> trampoline route from that node to the destination.\n>\n> Step #2 above can be cached: we can run a Dijkstra starting at our own\n> node running on the \"smooth\" map, that terminates once it has reached some\n> small number of high-level nodes (say the 5 nearest high-level nodes), then\n> just round-robining among those nodes per invocation of route finding.\n> This caching can be done in the same refresh as the one that generated the\n> high-level rough map in the first place.\n>\n> Step #3 is expected to be very fast since it looks at the rough map, which\n> has a tiny fraction of the nodes in the smooth map, and Dijkstra is O(n log\n> n) on number of nodes.\n>\n> This leaves step #1 to be the potentially slow part of this algorithm.\n> This is mitigated by the fact that this stage has multiple candidate\n> destinations, and terminates as soon as it reaches *any* candidate.\n> Thus we expect Dijkstra to only have to explore a small amount of nodes.\n> The more distributed the high-level nodes are, the more likely that we can\n> reach any high-level node from the destination in a small number of hops.\n>\n> As channels open and close, the trampoline route remains potentially\n> valid: as a trampoline node opens one hop of the trampoline route it can\n> derive the \"smooth\" path from itself to the next trampoline node.\n> This thus implements the Rough/Smooth pathfinding idea.\n>\n> If trampoline routes can be concatenated without permission from the\n> original source of the trampoline route, then a trampoline node may itself\n> utilize this algorithm when sending to the next trampoline node.\n>\n> A privacy-preserving implementation would prefer to periodically \"refresh\"\n> the high-level map (possibly changing the nodes that are put in the\n> high-level map to increase the chances that it will not use a high-level\n> node repeatedly).\n> Again, this will be similar to the `getroutequick` refresh, we\n> double-buffer so that we can use the existing high-level rough map while\n> generating the next high-level rough map.\n>\n> Tr\\*sted Rough Maps\n> -------------------\n>\n> Centralized tr\\*sted servers might exist that provide rough maps to nodes\n> with extremely limited space (and cannot store the smooth, detailed\n> channel-level map at all).\n> The server selects a small fraction of the available nodes, preferring\n> high-uptime high-connectivity nodes, and generates a rough map as above.\n>\n> Then, for each node, it determines the nearest high-level node.\n> Each high-level node has some fixed-size probabilistic set representation,\n> such as a Bloom filter or a Goulomb-coded set.\n> When it determines for a node which high-level node it is nearest to, it\n> adds that node to the Bloom filter / Goulomb-coded set for that high-level\n> node.\n>\n> A payer node that has no knowledge of the detailed channel map beyond its\n> own channels can generate a rough trampoline route as follows:\n>\n> 1.  It searches the payee in each high-level node in the rough map, using\n> the Bloom filter / Goulomb-coded set.\n>     Looking up in a Bloom filter would be O(1), and we would have to scan\n> the entire map, so complexity is O(n) effectively.\n> 2.  It searches for the nearest high-level node to itself, again using the\n> Bloom filter / Goulomb-coded set.\n> 3.  It uses the rough map to route from the high-level node near itself to\n> the high-level node near the payee.\n>     Then it appends the payee to this route.\n>\n> Knowing only its own channels, it can simply deliver the above generated\n> trampoline route to any adjacent node that claims to support trampoline\n> routing.\n>\n> This provides some measure of privacy: the payer does not need to reveal\n> any data to the server, other than the fact that it wants a rough map.\n> If the map is small enough, it could cache for multiple payment attempts.\n> Finally, it could also get multiple such maps from different servers, and\n> score each map based on how successful payment ends up being when using\n> that map.\n>\n> These servers are tr\\*sted to indicate high-uptime nodes with good\n> liquidity and connectivity to the network, and to not lie about the actual\n> network topology.\n> They are also tr\\*sted to not generate maps that have themselves in the\n> center of the map and have every other high-level node only have edges to\n> their central node.\n> As such, users of this system must consider how much trust they can put in\n> these servers.\n>\n> The payer node will need to periodically get updates of the rough map as\n> the topology changes.\n> A tr\\*sted server might use the pay-for-data protocol to require payment\n> for providing this map.\n> A newly deployed node will need to be started with a tr\\*sted (and\n> probably obsolete) rough map, however.\n> Alternately a protocol delivering a proof-of-payment can be executed\n> onchain, or the server can publish its node pubkey and public contact point\n> so that the node can start its first channel with the server and pay for\n> the rough map over a direct channel.\n>\n> Self-Serving\n> ------------\n>\n> Servers providing rough maps like the above can be written with free and\n> open source software.\n> Thus, one might run a limited-smooth-map node on the same hardware as a\n> server generating rough maps for the limited-smooth-map node.\n>\n> One might wonder though, whether it might be better to just run a node\n> that can keep the entire smooth map and not use a rough map at all.\n>\n> The difference here is that the server does not need to perform any\n> pathfinding in a tight schedule.\n> All its smooth-map pathfinding is done to simply determine which\n> high-level node every node on the smooth map is near to, and thiis not need\n> to be fast in order to serve outgoing payments quickly.\n> This means the server can potentially store the smooth routemap in a\n> database in slow persistent storage rather than in high-speed memory.\n>\n> Indeed, since the rough map is likely to still be mostly valid even as the\n> network changes topology, the server can generate new rough maps while the\n> hardware is idle, then coordinate with the limited-smooth-map node on every\n> update of a fresh rough map.\n>\n> This removes tr\\*st issues, in much the same way that running Electrum\n> Personal Server on a fullnode one controls removes the tr\\*st issue in\n> Electrum and leaves it as a high-quality wallet interface.\n>\n> This also provides an approach towards scaling up our nodes to handle\n> routemaps with 100 billion channels.\n> We can conceptually split our routemap between a \"total global routemap\"\n> stored strictly on disk, and a \"myopic local routemap\" that is kept in\n> memory.\n> The server stores the entire detailed global routemap on-disk.\n> The node software queries the server for two things:\n>\n> 1.  Nodes and channels within N hops of the node.\n>     This is the myopic local routemap.\n> 2.  A rough routemap of the rest of the network as described above.\n>\n> The node keeps both of the above in-memory, but they should be small\n> compared to the total global routemap stored on-disk.\n> Keeping the map in-memory lets the node quickly find routes.\n>\n> 1.  If the destination is in the local routemap already, then the node\n> does not need to do trampoline routing.\n> 2.  If the destination is not in the local routemap, then the node uses\n> the rough routemap to find a reasonable trampoline route, then finds a path\n> in the local routemap to the first node in the trampoline.\n>\n> Our bandwidth requirements still remain (we still need to gossip the\n> entire routemap containing 100 billion channels).\n> Similarly, our on-disk space requirement still remains.\n> However it does greatly reduce our memory requirement, making it possible\n> to still fit Lightning nodes on single-board-computer-level devices\n> (admittedly, those that are connected to large permanent storage and fast\n> Internet connections, but let me work on one problem at a time mkay?).\n>\n> Myopic Trampoline Nodes\n> -----------------------\n>\n> I have argued this before, but I think that nodes that do not have a\n> complete smooth-level routemap should still be allowed to advertise\n> themselves as trampoline nodes.\n>\n> As the network grows to 100 billion channels, fewer and fewer nodes will\n> have the ability to find detailed paths from themselves to any arbitrary\n> next-trampoline-node.\n> It is important that the risk of running trampoline nodes be shared\n> widely, and we should strongly prefer the \"trampoline advertising feature\n> bit\" to be a proxy for \"not too lazy to update the software\" rather than\n> \"rich enough to afford really good hardware\".\n>\n> However, this implies that some trampoline nodes will fall back to\n> themselves using a rough map to reach some nodes.\n> This implies that they need to prepend additional nodes to the trampoline\n> route, without permission from whoever created the original trampoline\n> route.\n>\n> Flocking\n> ========\n>\n> A common issue in real-time strategy games is that a player, fighting for\n> an important objective, needs to immediately send a large group of units to\n> a location.\n> If all the units were to individually request for paths from the\n> pathfinding system, then the units would lag and start moving one by one,\n> as the pathfinding system gets overloaded and gives the paths one by one.\n> This is particularly bad behavior if the player is currently in a\n> high-pressure situation where the objective might be lost soon if the\n> commanded units are unable to move immediately to the objective.\n>\n> Worse, often the pathfinding system will make the units follow almost the\n> exact same path, meaning that slower units can prevent faster units from\n> going around them, and they will all travel in single file, which is\n> usually a strategic problem when their line is intercepted by enemy units.\n>\n> The usual solution for this is, when multiple units are selected and\n> commanded to move to a single location, is:\n>\n> 1.  Select one unit at random as leader.\n> 2.  Have only the leader request a path from the pathfinding system.\n> 3.  Have the other units \"flock\" around that leader:\n> http://www.red3d.com/cwr/boids/\n>\n> Flocking simply means that units follow some rules:\n>\n> 1.  Do not get too near other flock members.\n> 2.  Do not get too far other flock members.\n> 3.  Face in the same direction as other flock members (if units in the\n> game world have this sense).\n> 4.  Leaders ignore the above rules, but are considered flock members by\n> the other flock members.\n>\n> This reduces the pathfinding load to only one unit, the leader unit, and\n> the rest of the units simply follow the designated leader until the leader\n> reaches the objective location, or is killed or given another command (in\n> which case another leader is chosen).\n> The individual units can request shorter paths from the pathfinding system\n> to any point near the leader, or if it is \"near enough\" to the leader it\n> can simply stop where it is.\n> As they have been following the leader, other units should be \"near\" to\n> the indicated location when the ledaer has reached the destination, so the\n> paths they request should be short and it should not take too long for the\n> pathfinding system to get those paths.\n>\n> Further, this often encourages the grouped units to move as a \"blob\" on\n> multiple nearby paths rather than in single file, which is generally better\n> for strategic defense when they are intercepted by enemies.\n>\n> Now of course in Lightning Network we do not have anything like this\n> problem, right?\n>\n> Right?\n>\n> Now consider the Feature Either Called AMP or Multi-Part Payments or Bass\n> Amplifier.\n> In this case, we have multiple payments arising from a source, all of\n> which are commanded to then converge at a singular destination.\n> We would like:\n>\n> 1.  Not to spend two entire seconds for each alternate path.\n> 2.  Have the various split payments travel in different paths, not in\n> \"single file\" where they mostly travel the same path except they start at\n> different channels but almost immediately converge to travelling over the\n> same channels.\n>\n> Flocking Over Lightning\n> -----------------------\n>\n> Suppose our node is asked to provide a large payment to some destination.\n> Our node then computes a path (possibly using any of the previous\n> shortcuts to speed up this initial pathfinding).\n>\n> However, the direct channel where this route starts from has too little\n> liquidity on our side.\n> So our node also selects another channel from which to make up the\n> difference in liquidity, using AMP/Multi-part/Bass Amplifier.\n>\n> This is a \"split\" payment, and we then use a flocking system, with the\n> first payment as the leader.\n> Our rules are:\n>\n> 1.  At each hop of the leader route, we should seek any route that gets us\n> to within one node away from the leader.\n> 2.  Channels going to nodes that the leader uses on its route (or which\n> have been used by the other split payments in the flock) have a much higher\n> cost.\n> 3.  Channels directly connected to the source cannot be used at all (as\n> that defeats the entire point of doing multipath).\n>\n> We simulate the leader following its path one hop at a time.\n> The leader starts at the source, then takes one hop to the next node.\n>\n> The split payment starts at the other node of the channel we selected to\n> make up the difference in payments.\n> We then use a multi-destination Dijkstra that terminates as soon as it\n> finds any node that is within one hop of where the leader currently is.\n> Channels used by the leader, and those already used by other splits, are\n> penalized in cost so that we avoid reusing them, but still can pass through\n> them if they are the only options available.\n> This initial Dijkstra can potentially take a long time, but hopefully it\n> should be easy to cut short since it is a multiple-destination Dijkstra and\n> achieving any destination terminates the algorithm.\n>\n> For subsequent hops of the leader, we use a multi-destination limited-hops\n> Dijkstra.\n> As the split payment starts one node away from the previous node of the\n> leader, in the worst case it can simply just jump to the previous node of\n> the leader if it cannot find an alternate path close by.\n> Jumping to this is penalized by rule 2 above so other paths are preferred,\n> but this still allows the \"flock\" to pass through chokepoints in the\n> network.\n>\n> When we simulate the leader reaching the destination node, then we use a\n> multi-destination limited-hops Dijkstra to the destination node.\n> This lets every split payment reach the leader.\n>\n> Thus, the payments form a \"blob\" around the leader, instead of almost\n> always following the same channels.\n> This provides better reliability of the entire payment attempt.\n>\n> Conclusion\n> ==========\n>\n> Lightning Network does not need to pioneer any new research into\n> pathfinding.\n> There already exists an entire field of programming that has similar\n> issues (dynamic map, incomplete information, large maps with low acceptable\n> latency of pathfinding, quickly-found \"okay\" paths are better than\n> slowly-found \"best\" paths).\n> That is real-time strategy game programming, and such games have existed\n> in some form or another for the past 4 decades.\n> Shortcuts, techniques, and optimizations used in real-time strategy games\n> deserve closer examination, as potential points of inspiration for similar\n> techniques for Lightning.\n>\n> This writeup is a result of a quick review of pathfinding techniques used\n> in real-time strategy games.\n> There are potentially many more techniques whose ideas might be possible\n> to adapt in finding Lightning Network routes in large public networks in\n> short amounts of time with acceptable fees and locktimes.\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20190731/ae5331d5/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-08-03T07:27:42",
                "message_text_only": ">     Map Preprocessing\n>     =================\n>\n\n\nDifferential Heuristics for A\\*\n-------------------------------\n\nWhile researching further, I came upon some hints of a concept called \"differential heuristics\".\nI tried to search further for this:\n\n* https://theory.stanford.edu/~amitp/GameProgramming/Heuristics.html#approximate-heuristics\n* Slide 44: https://www.slideshare.net/StavrosVassos/interactive-objects-pathfindingpart3\n* http://research.microsoft.com/pubs/154937/soda05.pdf <- this seems to be the original paper\n* https://www.aaai.org/ocs/index.php/SOCS/SOCS11/paper/viewFile/4020/4340\n\nIn summary, differential heuristics involve an A\\* `h(n)` function built from precomputed distances from multiple landmarks.\nThe primary advantage of differential heuristics is that they make A\\* search fewer nodes than e.g. Manhattan distance.\n\nIdeally, `h(n)` is the `distance(n, t)` where `t` is the destination.\nWith differential heuristics, `h(n)` is then the maximum of `abs(distance(n, l) - distance(t, l))` among all landmarks `l`, where `t` is the destination.\n`distance(_, l)` for all landmarks is precomputed.\n\nNow of course in LN we cannot use heuristics like Manhattan distance anyway.\nBut we can definitely use precomputed distances.\n\nI proposed a `getroutequick` which uses precomputed distances from the source in order to guide an algorithm towards the source.\nThis is based on the observation that our source tends to be always a single location, our own node.\n\nHowever, we may still want to acquire a route that does not arise from our own node.\nFor example, consider a variant of `permuteroute` that, if it cannot heal from the pivot to some postfix node within 3 hops, falls back to performing A\\* from the pivot to the final destination.\nPresumably, the pivot is nearer to the destination and we are more likely to quickly reach the destination from the pivot rather than the actual payment source.\n\nAnother example of creating a route not from our own node is generalized rebalancing.\nRebalancing transfers funds from one of our local channels to another of our local channels.\nWe can derive this path by finding a route from the counterparty of the source channel to the counterparty of the destination channel, exclude our own node from consideration in pathfinding.\nThen we just append the destination channel and prepend the source channel.\n(Admittedly a friend-of-friend limited subgraph like proposed by Rene will work almost as well and would still be much faster, but this can be a fallback in case we cannot find a rebalancing route when scanning the friend-of-friend.)\n\nSuppose we treat our own node as the single landmark in a differential heuristic system.\nWhen finding a route, the payee is actually the source and the payer (our own node) is the destination.\nThe distance from the destination (the payer, our own node) to the landmark (again, our own node) --- i.e. `distance(t, l)` --- is 0, so the `abs(distance(n, l) - distance(t, l))` becomes `abs(distance(n, l) - 0)` becomes `distance(n, l)`.\nSo differential heuristics is actually a generalization of the preprocessing I proposed earlier that enables us to use A\\*, but only for routes arising from our own node.\n\nThus, we can use this same preprocessed stored cost data of total-distance-from-our-own-node, not only to implement a `getroutequick` that works only for routes where our own node is one end of the route, but also for a `getroutequick` that can arise from any node.\nThe speed benefit is much much greater for routes where we are one end of the route, but we still get the benefit of using A\\* if neither end of the requested route is not our own node.\n\nAs we would still want to store the preprocessed total-distance-from-our-own-node anyway to implement a `getroutequick` that uses A\\* on an almost-exact heuristic, it means that we can implement a `getroutequick` from any node without increasing our memory usage, and with a relatively small tweak to the code.\nSpecifically, we need to look up `distance(t, l)` where `t` is the start of the route, and to compute the heuristic, subtract this from `distance(n, l)` and get the absolute value.\nIf the start of the route is our own node (which is the landmark `l`) then that distance is 0 and we automatically fall back to the A\\* variant that uses an almost-exact heuristic to reach our own node.\n\nFurther landmarks beyond our own node can be selected (preferably distant from our own node) to improve the speed of searching in the case where we need to find from an arbitrary node, at the cost of increasing the memory space needed."
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-08-01T05:02:59",
                "message_text_only": "Good morning laolu,\n\n\nSent with ProtonMail Secure Email.\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\nOn Thursday, August 1, 2019 10:29 AM, Olaoluwa Osuntokun <laolu32 at gmail.com> wrote:\n\n> > I found out recently (mid-2019) that mainnet Lightning nodes take an\n> > inordinate amount of time to find a route between themselves and an\n> > arbitrary payee node.\n> > Typical quotes suggested that commodity hardware would take 2 seconds to\n> > find a route\n> \u00a0\n> Can you provide a reproducible benchmark or further qualify this number (2\n> seconds)?\n\nNo reproducible benchmark.\nHowever, this is my reference: https://medium.com/@cryptotony/why-does-ln-payments-arent-instantaneous-d24f7e5f88cb which claims this 2 seconds for LND implementations.\n(It is entirely possible this information is obsolete, as it was published a month ago and things move fast in LN.)\n\nAs per Rene, from his C-Lightning mainnet node, `getroute` typically takes 1.1 to 1.3s to a particular unspecified destination.\nI do not know details of his hardware; it would be better to ask him.\n\n> Not commenting on the rest of this email as I haven't read the\n> rest of it yet, but this sounds like just an issue of engineering\n> optimization.\n\nThe rest of the email *is* engineering optimization.\n\n> AFAIK, most implementations are using unoptimized on-disk\n> representations of the graph, do minimal caching, and really haven't made\n> any sort of push to optimize these hot spots.\n\nC-Lightning has always used in-memory representation of the graph (helped massively by the low-level nature of C so we can fit a larger graph in the same space), and has the \"million channel project\" to attempt to generate graphs at 1 million channels that seem to represent the distribution of actual graphs today.\nC-Lightning is barely able to fit in a RPi-level computer today with the actual mainnet graph.\n\n> There's no reason that finding\n> a path in a graph of a 10s of thousands of edges should take _2 seconds_.\n>\n> Beyond that, to my knowledge, all implementations other and lnd implement a\n> very rudimentary time based edge/node pruning in response to failures. I\n> call it rudimentary, as it just waits a small period of time, then forgets\n> all its past path finding history. As a result, each attempt will include\n> nodes that have been known to be offline, or nonoperational channels,\n> effectively doing redundant work each attempt.\n\nIndeed, C-Lightning does this.\n\n>\n> The latest version of our software has moved beyond this [1], and will\n> factor in past path finding attempts into its central \"mission control\",\n> allowing it to learn from each attempt, and even import existing state into\n> its path finding memory (essentially a confidence factor that takes into\n> account the cost of a failed attempt mapped into a scalar weight we can use\n> for comparison purposes). This is just an initial first step, but we've seen\n> a significant improvement with just a _little_ bit more intelligence in our\n> path finding heuristics.\n\nIndeed.\nOur main algorithm for pathfinding is Dijkstra, which is O(n log n) formally with proper data structure implementation, though at large sizes approaches O(n ^ 2) as caching and so on get involved.\nI believe this is approximately what you will find in the \"best\" pathfinding algorithms.\n\nLimiting what you scan to a smaller slice of the graph is a valid engineering change to optimize pathfinding, as you will get near-optimal results while greatly cutting down on runtime.\n\n> We should take care to not get distracted by more\n> distant \"pie in the sky\" like ideas (since many of them are half-baked),\n> lest we ignore these low hanging engineering fruits and incremental\n> algorithmic updates.\n\nAs you have not read the rest of the email, I believe you should.\nThey are almost entirely low-hanging engineering fruits, and many are practically deployable today.\n\nThe primary point of this thread is to show that there exists a similar field (real-time strategy games) whose pathfinding problems are suspiciously similar to ours:\n\n1.  Dynamically-changing world.\n2.  Limited knowledge of actual world conditions (fog of war).\n3.  Low-latency for UX.\n4.  Large maps.\n\nTaking a short visit to this field may be helpful, regardless.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2019-08-01T08:33:33",
                "message_text_only": "Good morning ZmnSCPxj,\n\nThanks for sharing this analysis, you're touching on a lot of interesting\npoints and giving a lot of good resource pointers.\nIt echoes many ideas we also had to improve eclair's routing algorithm\n(which currently uses Yen's k-shortest paths with\nDijkstra, a few configurable heuristics and a compact in-memory\nrepresentation of the graph).\n\nI think that the main points that make routing in the Lightning Network\ndifferent from game path-finding algorithms are:\n\n   - Paths are consumed by payments, effectively moving available balance\n   between sides of a channel\n   - The routing algorithm doesn't know remote channels balance allocation\n   (and that changes constantly)\n   - The cost of a path depends on the value you're sending (proportional\n   fees)\n   -\n   - This encourages algorithms not to search for an optimal solution\n   (because an optimal solution on outdated/incomplete data doesn't even make\n   sense) but rather fast and good enough solutions with retries\n\nThere are a few technicalities that might be a problem for some of your\nsuggestions, I'm interested in your opinion on how to address them.\n\nFor `permuteroute`, you mention the following pre-requisite:\n\nthe original payer is informed of which node reported the failure and which\n> channel failed.\n>\n\nWe currently don't have a solution for reliable error reporting, as pointed\nout in [1].\nI think making progress on this thread would be interesting and useful for\nrouting heuristics.\n\nI thought about path pre-computation and path caching, but what bothered me\nis that the cost depends on the amount you want to send.\nWhen pre-computing / caching, you have to either ignore that completely\n(which can be fine, I don't think trying to always find the most\ncost-efficient route is a reasonable goal) or take into account some kind\nof \"universal\" factor that works for most amounts. How did you take\nthat into account in your pre-computation experiments?\n\nI do agree that multi-part payments and trampoline (hierarchical routing)\ncan offer a lot of room for algorithmic improvements and your\nideas on how to leverage them resonate with mine.\n\nAn interesting thing to note is that trampoline (in the current proposal at\nleast) allows trampoline nodes to leverage multi-part payments\n\"at each hop\", meaning that a trampoline node can join/split arbitrarily an\nincoming payment to reach the next trampoline node.\n\nWhile implementing a first version of multi-part payments, I realized that\nthey need to be tightly integrated to the routing algorithm.\nSince each payment \"consumes\" a path, potentially \"stealing\" it from other\npayments, a naive implementation of multi-part payments\n would try to use different paths for each sub-payment, but that's an\ninefficient way of solving it. Working on multi-part payments made\nme think that maybe our routing problem is more similar to a circulation or\nnetwork flow problem [2] rather than path-finding. Have you\nthought about this? If so what is your opinion?\n\nThanks again for sharing all this and starting those interesting\ndiscussions.\n\nCheers,\nBastien\n\n[1]\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2019-June/002015.html\n[2] https://en.wikipedia.org/wiki/Circulation_problem\n\nLe jeu. 1 ao\u00fbt 2019 \u00e0 07:14, ZmnSCPxj via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Good morning laolu,\n>\n>\n> Sent with ProtonMail Secure Email.\n>\n> \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n> On Thursday, August 1, 2019 10:29 AM, Olaoluwa Osuntokun <\n> laolu32 at gmail.com> wrote:\n>\n> > > I found out recently (mid-2019) that mainnet Lightning nodes take an\n> > > inordinate amount of time to find a route between themselves and an\n> > > arbitrary payee node.\n> > > Typical quotes suggested that commodity hardware would take 2 seconds\n> to\n> > > find a route\n> >\n> > Can you provide a reproducible benchmark or further qualify this number\n> (2\n> > seconds)?\n>\n> No reproducible benchmark.\n> However, this is my reference:\n> https://medium.com/@cryptotony/why-does-ln-payments-arent-instantaneous-d24f7e5f88cb\n> which claims this 2 seconds for LND implementations.\n> (It is entirely possible this information is obsolete, as it was published\n> a month ago and things move fast in LN.)\n>\n> As per Rene, from his C-Lightning mainnet node, `getroute` typically takes\n> 1.1 to 1.3s to a particular unspecified destination.\n> I do not know details of his hardware; it would be better to ask him.\n>\n> > Not commenting on the rest of this email as I haven't read the\n> > rest of it yet, but this sounds like just an issue of engineering\n> > optimization.\n>\n> The rest of the email *is* engineering optimization.\n>\n> > AFAIK, most implementations are using unoptimized on-disk\n> > representations of the graph, do minimal caching, and really haven't made\n> > any sort of push to optimize these hot spots.\n>\n> C-Lightning has always used in-memory representation of the graph (helped\n> massively by the low-level nature of C so we can fit a larger graph in the\n> same space), and has the \"million channel project\" to attempt to generate\n> graphs at 1 million channels that seem to represent the distribution of\n> actual graphs today.\n> C-Lightning is barely able to fit in a RPi-level computer today with the\n> actual mainnet graph.\n>\n> > There's no reason that finding\n> > a path in a graph of a 10s of thousands of edges should take _2 seconds_.\n> >\n> > Beyond that, to my knowledge, all implementations other and lnd\n> implement a\n> > very rudimentary time based edge/node pruning in response to failures. I\n> > call it rudimentary, as it just waits a small period of time, then\n> forgets\n> > all its past path finding history. As a result, each attempt will include\n> > nodes that have been known to be offline, or nonoperational channels,\n> > effectively doing redundant work each attempt.\n>\n> Indeed, C-Lightning does this.\n>\n> >\n> > The latest version of our software has moved beyond this [1], and will\n> > factor in past path finding attempts into its central \"mission control\",\n> > allowing it to learn from each attempt, and even import existing state\n> into\n> > its path finding memory (essentially a confidence factor that takes into\n> > account the cost of a failed attempt mapped into a scalar weight we can\n> use\n> > for comparison purposes). This is just an initial first step, but we've\n> seen\n> > a significant improvement with just a _little_ bit more intelligence in\n> our\n> > path finding heuristics.\n>\n> Indeed.\n> Our main algorithm for pathfinding is Dijkstra, which is O(n log n)\n> formally with proper data structure implementation, though at large sizes\n> approaches O(n ^ 2) as caching and so on get involved.\n> I believe this is approximately what you will find in the \"best\"\n> pathfinding algorithms.\n>\n> Limiting what you scan to a smaller slice of the graph is a valid\n> engineering change to optimize pathfinding, as you will get near-optimal\n> results while greatly cutting down on runtime.\n>\n> > We should take care to not get distracted by more\n> > distant \"pie in the sky\" like ideas (since many of them are half-baked),\n> > lest we ignore these low hanging engineering fruits and incremental\n> > algorithmic updates.\n>\n> As you have not read the rest of the email, I believe you should.\n> They are almost entirely low-hanging engineering fruits, and many are\n> practically deployable today.\n>\n> The primary point of this thread is to show that there exists a similar\n> field (real-time strategy games) whose pathfinding problems are\n> suspiciously similar to ours:\n>\n> 1.  Dynamically-changing world.\n> 2.  Limited knowledge of actual world conditions (fog of war).\n> 3.  Low-latency for UX.\n> 4.  Large maps.\n>\n> Taking a short visit to this field may be helpful, regardless.\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20190801/66ff2de3/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-08-01T23:02:39",
                "message_text_only": "Good morning Bastien,\n\n>\n> I think that the main points that make routing in the Lightning Network different from game path-finding algorithms are:\n>\n> -   Paths are consumed by payments, effectively moving available balance between sides of a channel\n> -   The routing algorithm doesn't know remote channels balance allocation (and that changes constantly)\n> -   The cost of a path depends on the value you're sending (proportional fees)\n> -\n> -   This encourages algorithms not to search for an optimal solution (because an optimal solution on outdated/incomplete data doesn't even make sense) but rather fast and good enough solutions with retries\n\nI believe the differences are smaller than you might initially think.\n\nUnits move around on the map and a pathfinding algorithm cannot predict how the *other* units owned by allied players will be, once the current units asking for a path have moved along the path.\ni.e. the algorithm does not know how remote tiles are occupied (and that changes constantly)\n\nFaster units really should be able to walk around slower units, because there is often a tradeoff between speed and combat effectiveness and a player asking a faster unit to move probably is depending on their speed.\ni.e. paths can be blocked by slower units, effectively becoming slow-moving obstacles that need to be worked around.\n\nAnd so on.\n\n>\n> There are a few technicalities that might be a problem for some of your suggestions, I'm interested in your opinion on how to address them.\n>\n> For `permuteroute`, you mention the following pre-requisite:\n>\n> > the original payer is informed of which node reported the failure and which channel failed.\n>\n> We currently don't have a solution for reliable error reporting, as pointed out in [1].\n> I think making progress on this thread would be interesting and useful for routing heuristics.\u00a0\n\nWe already have sufficiently-good error reporting on route failures: https://github.com/lightningnetwork/lightning-rfc/blob/master/04-onion-routing.md#returning-errors\n\n> When an origin node receives an error message matching a transfer it initiated\n> (i.e. it cannot return-forward the error any further) it generates the `ammag`\n> and `um` keys for each hop in the route.\n> It then iteratively decrypts the error message, using each hop's `ammag`\n> key, and computes the HMAC, using each hop's `um` key.\n\nThe number of iterations of decryption is how distant the error-reporting node is from the payer.\nKnowing the entire route, we can know which node reported the error.\nThe channel that is failing is then the channel *after* the error-reporting node (assuming bit `NODE` (`0x2000`) is not set in the `failure_code`: if it is a node-level error we should back off by one node and mark the erring node as unreliable).\nThe node reporting the error is the node that we start the limited-range search to \"heal\" the path.\n`permuteroute` does not *need* better error reporting than we already have.\n\nOf course, if a node is malingering and does not report anything, there is nothing we can do, but this is unavoidable anyway and does not prevent use of `permuteroute` in other cases either.\n\nIndeed, the other insight here is that, if we were able to receive an error report from forwarding node N, this implies that every node and channel between us and node N is reliable.\n`permuteroute` reuses this prefix, since it is known-reliable.\n\n>\n> I thought about path pre-computation and path caching, but what bothered me is that the cost depends on the amount you want to send.\n> When pre-computing / caching, you have to either ignore that completely (which can be fine, I don't think trying to always find the most\n> cost-efficient route is a reasonable goal) or take into account some kind of \"universal\" factor that works for most amounts. How did you take\n> that into account in your pre-computation experiments?\n\nJust to be clear: I have not run any experiments.\nI work on Lightning in a hobbyist capacity, am a small-time HODLer, and cannot even afford to run a mainnet LN node (which is why I had to ask Rene to time `getroute`).\nI mostly get by on sheer code review, tons of armchair reasoning, and sheer force of will.\n\nWhile costs depend on the amount you want to send, we observe that there are three main costs:\n\n* Risk of locking up funds for `cltv_delta` blocks\n* `fee_proportional_millionths`\n* `fee_base_msatoshi`\n\nOf these, the first two are proportional to the value being sent.\nSo if you double the value, you double the cost, but you also double this same cost on *every* alternate path.\nAnd pathfinding algorithms do not judge the absolute cost, but the relative cost of every path.\nIn short the cases below are equivalent and given the same map and the same source and destination, will result in the same path (assuming your variables do not overflow, of course):\n\n* Plains cost 1 movement point, Forests cost 2 movement point\n* Plains cost 2 movement point, Forests cost 4 movement point\n* Plains cost 3241 movement point, Forests cost 6482 movement point\n\nThe issue is not so much that costs are proportional to the value being sent.\nThe *real* issue is that costs are *both* fixed and proportional.\nSo we need to select a balancing factor between the fixed and proportional costs.\n\nWe can assume \"past performance is an indicator of future performance\" and record the average payment size of the user in order to determine how to balance the fixed and proportional costs.\nPicking an example value of say 1mBTC at the start, when the user has not used the node yet, seems reasonable.\n\nUsing the average value here, assuming the distribution of values the user sends is the same in the future, minimizes the error between the cached result vs the actual resulting fees.\n\nAgain, the point is that we need some sort of way to set limits on the fees and risk the user has for payments, hence the need for `maxfeepercent` and `maxdelay`.\nWe cannot reliably get perfect paths on potentially-stale data anyway.\nSo I think we can just use whatever path we find using precomputation and caching (using some \"example value\"), and then do a double-check that the generated path does not get past `maxfeepercent` and `maxdelay`.\nIf the generated path gets past the limits, we fall back to a non-precomputed search: given a good-enough \"example value\" this should be rare in practice.\n\n>\n> I do agree that multi-part payments and trampoline (hierarchical routing) can offer a lot of room for algorithmic improvements and your\u00a0\n> ideas on how to leverage them resonate with mine.\n>\n> An interesting thing to note is that trampoline (in the current proposal at least) allows trampoline nodes to leverage multi-part payments\n> \"at each hop\", meaning that a trampoline node can join/split arbitrarily an incoming payment to reach the next trampoline node.\n\nI agree, this is an interesting thing.\n\n> While implementing a first version of multi-part payments, I realized that they need to be tightly integrated to the routing algorithm.\n> Since each payment \"consumes\" a path, potentially \"stealing\" it from other payments, a naive implementation of multi-part payments\n> \u00a0would try to use different paths for each sub-payment, but that's an inefficient way of solving it. Working on multi-part payments made\n> me think that maybe our routing problem is more similar to a circulation or network flow problem [2] rather than path-finding. Have you\n> thought about this? If so what is your opinion?\n\nMost solutions to the network flow problem seem to require an accurate view of flows at each node, which we do not have.\nFor multi-part, this is actually similar to the issue of sending a blob of units from one location to another, while keeping the units in a cohesive blob without them forming a line of units where everyone walks nearly the exact same path.\n(older RTSs tend to form these lines when sending blobs of units on long-distance trips, with the downside that on reaching the combat location units come to battle one at a time rather than all at once, reducing the impact of the blob; players learned to micromanage these lines near the combat location so that at least the first entry into the attack is a small group of units rather than a solitary one.)\nWalking nearly the exact same path is, incidentally, the thing we want to avoid in multi-part payments, incidentally, so we have a similar problem as RTS games with lines of units going into battle one-by-one.\n\nHence, why I proposed the use of flocking, which is a technique used to retain cohesion of a blob of units.\nFor example, some RTS games have a concept of putting their units \"in formation\", which is actually just a way to excuse the flocking behavior to the player.\n\nAnother solution is to use `permuteoute`.\nRun normal single-pathfinding algo.\nFind the smallest-capacity channels in the returned route and `permuteroute` around those channels, resulting in an alternate route that avoids the smallest-capacity channels (which are more likely to fail when multiple payments run through them) but shares the rest of the path with the original route.\nKeep doing this until `permuteroute` fails, then we have a bunch of alternate routes we can try to use for multi-part routing.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2019-08-02T08:30:30",
                "message_text_only": "Good morning ZmnSCPxj,\n\nThe channel that is failing is then the channel *after* the error-reporting\n> node (assuming bit `NODE` (`0x2000`) is not set in the `failure_code`: if\n> it is a node-level error we should back off by one node and mark the erring\n> node as unreliable).\n>\n> Indeed, the other insight here is that, if we were able to receive an\n> error report from forwarding node N, this implies that every node and\n> channel between us and node N is reliable.\n> `permuteroute` reuses this prefix, since it is known-reliable.\n>\n\nI think this is more subtle than that. The thread I linked provided more\ndetails, but in many cases you can't decide whether you should blacklist\nonly the channel *after* the failing node or also the channel *before* the\nfailing node. And it's even worse than that, if a node before the failing\none is malicious, it can force some next node to fail (by simply holding\nthe HTLC until close to the expiry) and in that case you should also\nblacklist\nsome of the nodes *before* the failing node. And note that malicious nodes\nwith that behavior would happily forward the error onion because it\ndirectly incriminates someone else.\n\nI agree that we should optimize for the most common use-case (which\nprobably means ignoring these malicious node scenario for now), but I think\nit's important to keep them in mind. At some point people will attack the\nnetwork so we need to give some thoughts about potential attacks and make\nsure our algorithms can heal properly.\nBut that's not the most important discussion for this thread so let's\nshelve that for now :).\n\nThe *real* issue is that costs are *both* fixed and proportional.\n> So we need to select a balancing factor between the fixed and proportional\n> costs.\n>\n\nI fully agree with that.\n\nWe can assume \"past performance is an indicator of future performance\" and\n> record the average payment size of the user in order to determine how to\n> balance the fixed and proportional costs.\n> Picking an example value of say 1mBTC at the start, when the user has not\n> used the node yet, seems reasonable.\n>\n\nI also agree this sounds reasonable, this is what we had in mind as a\nstarting point for eclair.\n\nMost solutions to the network flow problem seem to require an accurate view\n> of flows at each node, which we do not have.\n>\n\nInteresting, but for the first hop (local channels) we have the exact\nbalance available for sending, and for next hops we can consider the\nchannels\nbalanced (with a random perturbation of X%). The combination of that\nand retries could provide interesting results (I plan on testing that on\nrealistic simulations of the network, I can't know for sure if this will\nwork until then).\n\nMy first implementation of MPP for eclair uses an algorithm similar to\nflocking.\nI think your last suggestion of using something similar to `permuteroute`\ncan be interesting to try too.\nI'll give that a shot if we're not satisfied with the results of the\nflocking implementation. Is it what you plan on doing for MPP in\nc-lightning?\n\nCheers,\nBastien\n\n\nLe ven. 2 ao\u00fbt 2019 \u00e0 01:02, ZmnSCPxj <ZmnSCPxj at protonmail.com> a \u00e9crit :\n\n> Good morning Bastien,\n>\n> >\n> > I think that the main points that make routing in the Lightning Network\n> different from game path-finding algorithms are:\n> >\n> > -   Paths are consumed by payments, effectively moving available balance\n> between sides of a channel\n> > -   The routing algorithm doesn't know remote channels balance\n> allocation (and that changes constantly)\n> > -   The cost of a path depends on the value you're sending (proportional\n> fees)\n> > -\n> > -   This encourages algorithms not to search for an optimal solution\n> (because an optimal solution on outdated/incomplete data doesn't even make\n> sense) but rather fast and good enough solutions with retries\n>\n> I believe the differences are smaller than you might initially think.\n>\n> Units move around on the map and a pathfinding algorithm cannot predict\n> how the *other* units owned by allied players will be, once the current\n> units asking for a path have moved along the path.\n> i.e. the algorithm does not know how remote tiles are occupied (and that\n> changes constantly)\n>\n> Faster units really should be able to walk around slower units, because\n> there is often a tradeoff between speed and combat effectiveness and a\n> player asking a faster unit to move probably is depending on their speed.\n> i.e. paths can be blocked by slower units, effectively becoming\n> slow-moving obstacles that need to be worked around.\n>\n> And so on.\n>\n> >\n> > There are a few technicalities that might be a problem for some of your\n> suggestions, I'm interested in your opinion on how to address them.\n> >\n> > For `permuteroute`, you mention the following pre-requisite:\n> >\n> > > the original payer is informed of which node reported the failure and\n> which channel failed.\n> >\n> > We currently don't have a solution for reliable error reporting, as\n> pointed out in [1].\n> > I think making progress on this thread would be interesting and useful\n> for routing heuristics.\n>\n> We already have sufficiently-good error reporting on route failures:\n> https://github.com/lightningnetwork/lightning-rfc/blob/master/04-onion-routing.md#returning-errors\n>\n> > When an origin node receives an error message matching a transfer it\n> initiated\n> > (i.e. it cannot return-forward the error any further) it generates the\n> `ammag`\n> > and `um` keys for each hop in the route.\n> > It then iteratively decrypts the error message, using each hop's `ammag`\n> > key, and computes the HMAC, using each hop's `um` key.\n>\n> The number of iterations of decryption is how distant the error-reporting\n> node is from the payer.\n> Knowing the entire route, we can know which node reported the error.\n> The channel that is failing is then the channel *after* the\n> error-reporting node (assuming bit `NODE` (`0x2000`) is not set in the\n> `failure_code`: if it is a node-level error we should back off by one node\n> and mark the erring node as unreliable).\n> The node reporting the error is the node that we start the limited-range\n> search to \"heal\" the path.\n> `permuteroute` does not *need* better error reporting than we already have.\n>\n> Of course, if a node is malingering and does not report anything, there is\n> nothing we can do, but this is unavoidable anyway and does not prevent use\n> of `permuteroute` in other cases either.\n>\n> Indeed, the other insight here is that, if we were able to receive an\n> error report from forwarding node N, this implies that every node and\n> channel between us and node N is reliable.\n> `permuteroute` reuses this prefix, since it is known-reliable.\n>\n> >\n> > I thought about path pre-computation and path caching, but what bothered\n> me is that the cost depends on the amount you want to send.\n> > When pre-computing / caching, you have to either ignore that completely\n> (which can be fine, I don't think trying to always find the most\n> > cost-efficient route is a reasonable goal) or take into account some\n> kind of \"universal\" factor that works for most amounts. How did you take\n> > that into account in your pre-computation experiments?\n>\n> Just to be clear: I have not run any experiments.\n> I work on Lightning in a hobbyist capacity, am a small-time HODLer, and\n> cannot even afford to run a mainnet LN node (which is why I had to ask Rene\n> to time `getroute`).\n> I mostly get by on sheer code review, tons of armchair reasoning, and\n> sheer force of will.\n>\n> While costs depend on the amount you want to send, we observe that there\n> are three main costs:\n>\n> * Risk of locking up funds for `cltv_delta` blocks\n> * `fee_proportional_millionths`\n> * `fee_base_msatoshi`\n>\n> Of these, the first two are proportional to the value being sent.\n> So if you double the value, you double the cost, but you also double this\n> same cost on *every* alternate path.\n> And pathfinding algorithms do not judge the absolute cost, but the\n> relative cost of every path.\n> In short the cases below are equivalent and given the same map and the\n> same source and destination, will result in the same path (assuming your\n> variables do not overflow, of course):\n>\n> * Plains cost 1 movement point, Forests cost 2 movement point\n> * Plains cost 2 movement point, Forests cost 4 movement point\n> * Plains cost 3241 movement point, Forests cost 6482 movement point\n>\n> The issue is not so much that costs are proportional to the value being\n> sent.\n> The *real* issue is that costs are *both* fixed and proportional.\n> So we need to select a balancing factor between the fixed and proportional\n> costs.\n>\n> We can assume \"past performance is an indicator of future performance\" and\n> record the average payment size of the user in order to determine how to\n> balance the fixed and proportional costs.\n> Picking an example value of say 1mBTC at the start, when the user has not\n> used the node yet, seems reasonable.\n>\n> Using the average value here, assuming the distribution of values the user\n> sends is the same in the future, minimizes the error between the cached\n> result vs the actual resulting fees.\n>\n> Again, the point is that we need some sort of way to set limits on the\n> fees and risk the user has for payments, hence the need for `maxfeepercent`\n> and `maxdelay`.\n> We cannot reliably get perfect paths on potentially-stale data anyway.\n> So I think we can just use whatever path we find using precomputation and\n> caching (using some \"example value\"), and then do a double-check that the\n> generated path does not get past `maxfeepercent` and `maxdelay`.\n> If the generated path gets past the limits, we fall back to a\n> non-precomputed search: given a good-enough \"example value\" this should be\n> rare in practice.\n>\n> >\n> > I do agree that multi-part payments and trampoline (hierarchical\n> routing) can offer a lot of room for algorithmic improvements and your\n> > ideas on how to leverage them resonate with mine.\n> >\n> > An interesting thing to note is that trampoline (in the current proposal\n> at least) allows trampoline nodes to leverage multi-part payments\n> > \"at each hop\", meaning that a trampoline node can join/split arbitrarily\n> an incoming payment to reach the next trampoline node.\n>\n> I agree, this is an interesting thing.\n>\n> > While implementing a first version of multi-part payments, I realized\n> that they need to be tightly integrated to the routing algorithm.\n> > Since each payment \"consumes\" a path, potentially \"stealing\" it from\n> other payments, a naive implementation of multi-part payments\n> >  would try to use different paths for each sub-payment, but that's an\n> inefficient way of solving it. Working on multi-part payments made\n> > me think that maybe our routing problem is more similar to a circulation\n> or network flow problem [2] rather than path-finding. Have you\n> > thought about this? If so what is your opinion?\n>\n> Most solutions to the network flow problem seem to require an accurate\n> view of flows at each node, which we do not have.\n> For multi-part, this is actually similar to the issue of sending a blob of\n> units from one location to another, while keeping the units in a cohesive\n> blob without them forming a line of units where everyone walks nearly the\n> exact same path.\n> (older RTSs tend to form these lines when sending blobs of units on\n> long-distance trips, with the downside that on reaching the combat location\n> units come to battle one at a time rather than all at once, reducing the\n> impact of the blob; players learned to micromanage these lines near the\n> combat location so that at least the first entry into the attack is a small\n> group of units rather than a solitary one.)\n> Walking nearly the exact same path is, incidentally, the thing we want to\n> avoid in multi-part payments, incidentally, so we have a similar problem as\n> RTS games with lines of units going into battle one-by-one.\n>\n> Hence, why I proposed the use of flocking, which is a technique used to\n> retain cohesion of a blob of units.\n> For example, some RTS games have a concept of putting their units \"in\n> formation\", which is actually just a way to excuse the flocking behavior to\n> the player.\n>\n> Another solution is to use `permuteoute`.\n> Run normal single-pathfinding algo.\n> Find the smallest-capacity channels in the returned route and\n> `permuteroute` around those channels, resulting in an alternate route that\n> avoids the smallest-capacity channels (which are more likely to fail when\n> multiple payments run through them) but shares the rest of the path with\n> the original route.\n> Keep doing this until `permuteroute` fails, then we have a bunch of\n> alternate routes we can try to use for multi-part routing.\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20190802/1cd3530b/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-08-02T13:26:53",
                "message_text_only": "Good morning Bastien,\n\n> > Most solutions to the network flow problem seem to require an accurate view of flows at each node, which we do not have.\n>\n> Interesting, but for the first hop (local channels) we have the exact balance available for sending, and for next hops we can consider the channels\n> balanced (with a random perturbation of X%). The combination of that and\u00a0retries could provide interesting results (I plan on testing that on\u00a0\n> realistic simulations of the network, I can't know for sure if this will work until then).\n>\n> My first implementation of MPP for eclair uses an algorithm similar to flocking.\n> I think your last suggestion of using something similar to `permuteroute` can be interesting to try too.\n> I'll give that a shot if we're not satisfied with the results of the flocking implementation. Is it what you plan on doing for MPP in c-lightning?\n\nOriginal plan was to derive some kind of `getmultiroute` that does a single graph scan and somehow generates multiple paths, but I kept finding problems in my naive implementations.\nI never got around to searching for existing algorithms of the needed specifications of getting multiple \"good\" paths.\n\nSo maybe `permuteroute` is one idea for implementing multipath we can try to use.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Rusty Russell",
                "date": "2019-08-08T05:56:13",
                "message_text_only": "ZmnSCPxj via Lightning-dev <lightning-dev at lists.linuxfoundation.org> writes:\n>> > Typical quotes suggested that commodity hardware would take 2 seconds to\n>> > find a route\n>> \u00a0\n>> Can you provide a reproducible benchmark or further qualify this number (2\n>> seconds)?\n>\n> No reproducible benchmark.\n\nOK, on my digital ocean 2-cpu 4GB ram ntel(R) Xeon(R) CPU E5-2630L 0 @\n2.00GHz (which is pretty old hardware), an unoptimized c-lighting\nimplementation returns random routes on mainnet in:\n\n        Between 3 to 347 msec, mean 220 msec.\n\nThat's forking lightning-cli, querying, printing result and exiting\n(mainnet, 941 successes, 1353 failures, I ignored the times on\nfailures since they were usually v fast).\n\nOn my Raspberry Pi 2B (compiled with -O3):\n\n        Between 21 to 3330 msec, mean 388 msec.\n\nCheers,\nRusty."
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-08-09T10:37:38",
                "message_text_only": "Good morning Rusty,\n\n> > > > Typical quotes suggested that commodity hardware would take 2 seconds to\n> > > > find a route\n> > > > \u00a0\n> > > > Can you provide a reproducible benchmark or further qualify this number (2\n> > > > seconds)?\n> >\n> > No reproducible benchmark.\n>\n> OK, on my digital ocean 2-cpu 4GB ram ntel(R) Xeon(R) CPU E5-2630L 0 @\n> 2.00GHz (which is pretty old hardware), an unoptimized c-lighting\n> implementation returns random routes on mainnet in:\n>\n> Between 3 to 347 msec, mean 220 msec.\n>\n> That's forking lightning-cli, querying, printing result and exiting\n> (mainnet, 941 successes, 1353 failures, I ignored the times on\n> failures since they were usually v fast).\n>\n> On my Raspberry Pi 2B (compiled with -O3):\n>\n> Between 21 to 3330 msec, mean 388 msec.\n\nThank you very much for this testing!\n\nA rule of thumb in UX is \"the user remembers your worst-case performance, not your average-case performance\".\nPerhaps it is a few tries on a RPi to a *really* remote node that started this random unqualified report of \"2 seconds\".\n\nMy understanding, Dijkstra, and other non-heuristic-guided algorithms, explore a \"ball\" around the starting node.\nThus, if the other end of the path is far, the ball to explore is larger and the practical algorithm runtime quickly goes up.\n\nA\\*, on the other hand, by use of the heuristic, tends to only form balls around large obstacles, but otherwise has a much smaller frontier it explores.\nThis may help reduce the worst-case times.\n\nCurrently I am working on an implementation of `getroutequick` for C-Lightning.\nBasically, we periodically measure the distance of each node from our node, and store this in a cache in each node.\nThen during pathfinding, we use A\\* and use the stored distance-to-our-node as part of a differential heuristic.\nHopefully, the simple fact that we *have* a heuristic we can feed to A\\* would be helpful in cutting down the worst-case runtime of pathfinding.\n\nDijkstra, A\\*, and another algorithm called \"Greedy Best First Search\" have particular relationships to each other.\nBasically, all three algorithms require a priority queue, where nodes are sorted in order from least-cost to most-cost.\nThe source node is put into the priority queue.\nThe processing loop takes a node from the priority queue (taking the least-cost node) and then expands each of the nodes it is connected to, pushing them into the priority queue according to their cost.\nTheir difference lies in how the priority used in the priority queue is computed:\n\n* Dijkstra: f(n) = g(n)\n* A\\*: f(n) = g(n) + h(n)\n* Greedy Best First: f(n) = h(n)\n\nwhere:\n\n* g(n) is the total cost from the source to this node\n* h(n) is the estimated cost from this node to the destination.\n\nI have come across very few references to Greedy Best First.\nHere is one: http://theory.stanford.edu/~amitp/GameProgramming/AStarComparison.html#dijkstras-algorithm-and-best-first-search\n\nGreedy Best First scans fewer nodes, but may yield non-optimal paths.\nDijkstra assuredly finds optimal paths, but scans many nodes due to its scanning a \"ball\" around the source.\n\nOf note is that g(n) and h(n) should be \"appropriately scaled\" to each other when used in A\\*.\nThat is, ideally h(n) should use the same units and should use the same costing estimates as costs of movement between nodes.\nIf h(n) is scaled down, then A\\* behaves closer to Dijkstra (assured optimal path, but slow).\nIf h(n) is scaled up, then A\\* behaves closer to Greedy Best First (faster, but may yield sub-optimal path).\n\nIndeed, heuristic admissibiilty is simply the recognition that if we scale down h(n) so that it will never give more than the actual distance to target, A\\* will fall back to Dijkstra.\n\n-----\n\nPriority Queues For Dijkstra and A\\*\n====================================\n\nDijkstra (and the related A\\* and Greedy Best First) uses a priority queue.\nOf note, is that Dijkstra tends to require an operation called \"decrease priority\".\nThis operation is used if a node is visited another time from a different path, which turns out to reduce its f(n).\n\nHowever, according to this paper: http://www3.cs.stonybrook.edu/~rezaul/papers/TR-07-54.pdf\n\n> all Dijkstra-NoDec implementations (i.e., AH-Dij, FBin-Dij, SBin-Dij, Al4-Dij and Seq-Dij) ran at least 1.4 times faster than any Dijkstra-Dec implementation (i.e.,BH-Dij, Bin-Dij and Pair-Dij).\n\nWhere:\n\n* Dijkstra-NoDec is for implementations whose priority queue did *not* have this \"decrease priority\" operation (i.e. \"NoDec\")\n* Dijkstra-Dec is for implementations whose priority queue *did* have this \"decrease priority\" operation.\n\nThat is: having a \"decrease priority\" operation was a drawback, not a benefit!\n\nOf course, the algorithm has to ensure it does not expand the same node twice.\nOften, the \"NoDec\" variants have to store the visited-ness of a node.\n\nNow the visited-ness of a node can be stored in the structure that also stores the f(n) of that node.\nf(n) is the priority of the node, and is basically the cost: under Dijkstra, the cost of reaching that node; under A\\*, the estimated cost of reaching the goal node via the path that goes through this node.\nAnd costs in Lightning are measurable in millisatoshis.\n\nThe maximum number of satoshis is known to fit in 53 bits.\nMillisatoshis requires 1000x larger numbers, which requires 10 additional bits.\nThus, 63 bits can fit the largest possible cost (and if it costs more than what can fit 63 bits, then the cost of that path is immaterial: nobody can ever afford it, so saturating to this maximum value is perfectly valid).\nNow, we can add one more bit as a flag meaning \"we have pulled this node out of the priority queue and expanded its neighbors already\", thus fitting into 64 bits."
            }
        ],
        "thread_summary": {
            "title": "Improving Lightning Network Pathfinding Latency by Path Splicing and Other Real-Time Strategy Game Techniques",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Bastien TEINTURIER",
                "Rusty Russell",
                "Olaoluwa Osuntokun",
                "ZmnSCPxj"
            ],
            "messages_count": 10,
            "total_messages_chars_count": 138630
        }
    },
    {
        "title": "[Lightning-dev] Trampoline Routing",
        "thread_messages": [
            {
                "author": "Bastien TEINTURIER",
                "date": "2019-08-02T09:29:25",
                "message_text_only": "Good morning list,\n\nI realized that trampoline routing has only been briefly described to this\nlist (credits to cdecker and pm47 for laying\nout the foundations). I just published an updated PR [1] and want to take\nthis opportunity to present the high level\nview here and the parts that need a concept ACK and more feedback.\n\nTrampoline routing is conceptually quite simple. Alice wants to send a\npayment to Bob, but she doesn't know a\nroute to get there because Alice only keeps a small area of the routing\ntable locally (Alice has a crappy phone,\ndamn it Alice sell some satoshis and buy a real phone). However, Alice has\na few trampoline nodes in her\nfriends-of-friends and knows some trampoline nodes outside of her local\narea (but she doesn't know how to reach\nthem). Alice would like to send a payment to a trampoline node she can\nreach and defer calculation of the rest of\nthe route to that node.\n\nThe onion routing part is very simple now that we have variable-length\nonion payloads (thanks again cdecker!).\nJust like russian dolls, we simply put a small onion inside a big onion.\nAnd the HTLC management forwards very\nnaturally.\n\nIt's always simpler with an example. Let's imagine that Alice can reach\nthree trampoline nodes: T1, T2 and T3.\nShe also knows the details of many remote trampoline nodes that she cannot\nreach: RT1, RT2, RT3 and RT4.\nAlice selects T1 and RT2 to use as trampoline hops. She builds a small\nonion that describes the following route:\n\n*Alice -> T1 -> RT2 -> Bob*\n\nShe finds a route to T1 and builds a normal onion to send a payment to T1:\n\n*Alice -> N1 -> N2 -> T1*\n\nIn the payload for T1, Alice puts the small trampoline onion.\nWhen T1 receives the payment, he is able to peel one layer of the\ntrampoline onion and discover that he must\nforward the payment to RT2. T1 finds a route to RT2 and builds a normal\nonion to send a payment to RT2:\n\n*T1 -> N3 -> RT2*\n\nIn the payload for RT2, T1 puts the peeled small trampoline onion.\nWhen RT2 receives the payment, he is able to peel one layer of the\ntrampoline onion and discover that he must\nforward the payment to Bob. RT2 finds a route to Bob and builds a normal\nonion to send a payment:\n\n*RT2 -> N4 -> N5 -> Bob*\n\nIn the payload for Bob, RT2 puts the peeled small trampoline onion.\nWhen Bob receives the payment, he is able to peel the last layer of the\ntrampoline onion and discover that he is\nthe final recipient, and fulfills the payment.\n\nAlice has successfully sent a payment to Bob deferring route calculation to\nsome chosen trampoline nodes.\nThat part was simple and (hopefully) not controversial, but it left out\nsome important details:\n\n   1. How do trampoline nodes specify their fees and cltv requirements?\n   2. How does Alice sync the fees and cltv requirements for her remote\n   trampoline nodes?\n\nTo answer 1., trampoline nodes needs to estimate a fee and cltv that allows\nthem to route to (almost) any other\ntrampoline node. This is likely going to increase the fees paid by\nend-users, but they can't eat their cake and\nhave it too: by not syncing the whole network, users are trading fees for\nease of use and payment reliability.\n\nTo answer 2., we can re-use the existing gossip infrastructure to exchange\na new *node_update *message that\ncontains the trampoline fees and cltv. However Alice doesn't want to\nreceive every network update because she\ndoesn't have the bandwidth to support it (damn it again Alice, upgrade your\nmobile plan). My suggestion is to\ncreate a filter system (similiar to BIP37) where Alice sends gossip filters\nto her peers, and peers only forward to\nAlice updates that match these filters. This doesn't have the issues BIP37\nhas for Bitcoin because it has a cost\nfor Alice: she has to open a channel (and thus lock funds) to get a\nconnection to a peer. Peers can refuse to serve\nfilters if they are too expensive to compute, but the filters I propose in\nthe PR are very cheap (a simple xor or a\nnode distance comparison).\n\nIf you're interested in the technical details, head over to [1].\nI would really like to get feedback from this list on the concept itself,\nand especially on the gossip and fee estimation\nparts. If you made it that far, I'm sure you have many questions and\nsuggestions ;).\n\nCheers,\nBastien\n\n[1] https://github.com/lightningnetwork/lightning-rfc/pull/654\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20190802/c9e8fa12/attachment.html>"
            },
            {
                "author": "fiatjaf",
                "date": "2019-08-02T16:29:22",
                "message_text_only": "Ok, since you seem to imply each question is valuable, here's mine: how\ndoes Alice know RT2 has a route to Bob? If she knows that, can she also\nknow T1 has a route to Bob? In any case, why can't she just build her small\nonion with Alice -> T1 -> Bob? I would expect that to be the most common\ncase, am I right?\n\nOn Friday, August 2, 2019, Bastien TEINTURIER <bastien at acinq.fr> wrote:\n\n> Good morning list,\n>\n> I realized that trampoline routing has only been briefly described to this\n> list (credits to cdecker and pm47 for laying\n> out the foundations). I just published an updated PR [1] and want to take\n> this opportunity to present the high level\n> view here and the parts that need a concept ACK and more feedback.\n>\n> Trampoline routing is conceptually quite simple. Alice wants to send a\n> payment to Bob, but she doesn't know a\n> route to get there because Alice only keeps a small area of the routing\n> table locally (Alice has a crappy phone,\n> damn it Alice sell some satoshis and buy a real phone). However, Alice has\n> a few trampoline nodes in her\n> friends-of-friends and knows some trampoline nodes outside of her local\n> area (but she doesn't know how to reach\n> them). Alice would like to send a payment to a trampoline node she can\n> reach and defer calculation of the rest of\n> the route to that node.\n>\n> The onion routing part is very simple now that we have variable-length\n> onion payloads (thanks again cdecker!).\n> Just like russian dolls, we simply put a small onion inside a big onion.\n> And the HTLC management forwards very\n> naturally.\n>\n> It's always simpler with an example. Let's imagine that Alice can reach\n> three trampoline nodes: T1, T2 and T3.\n> She also knows the details of many remote trampoline nodes that she cannot\n> reach: RT1, RT2, RT3 and RT4.\n> Alice selects T1 and RT2 to use as trampoline hops. She builds a small\n> onion that describes the following route:\n>\n> *Alice -> T1 -> RT2 -> Bob*\n>\n> She finds a route to T1 and builds a normal onion to send a payment to T1:\n>\n> *Alice -> N1 -> N2 -> T1*\n>\n> In the payload for T1, Alice puts the small trampoline onion.\n> When T1 receives the payment, he is able to peel one layer of the\n> trampoline onion and discover that he must\n> forward the payment to RT2. T1 finds a route to RT2 and builds a normal\n> onion to send a payment to RT2:\n>\n> *T1 -> N3 -> RT2*\n>\n> In the payload for RT2, T1 puts the peeled small trampoline onion.\n> When RT2 receives the payment, he is able to peel one layer of the\n> trampoline onion and discover that he must\n> forward the payment to Bob. RT2 finds a route to Bob and builds a normal\n> onion to send a payment:\n>\n> *RT2 -> N4 -> N5 -> Bob*\n>\n> In the payload for Bob, RT2 puts the peeled small trampoline onion.\n> When Bob receives the payment, he is able to peel the last layer of the\n> trampoline onion and discover that he is\n> the final recipient, and fulfills the payment.\n>\n> Alice has successfully sent a payment to Bob deferring route calculation\n> to some chosen trampoline nodes.\n> That part was simple and (hopefully) not controversial, but it left out\n> some important details:\n>\n>    1. How do trampoline nodes specify their fees and cltv requirements?\n>    2. How does Alice sync the fees and cltv requirements for her remote\n>    trampoline nodes?\n>\n> To answer 1., trampoline nodes needs to estimate a fee and cltv that\n> allows them to route to (almost) any other\n> trampoline node. This is likely going to increase the fees paid by\n> end-users, but they can't eat their cake and\n> have it too: by not syncing the whole network, users are trading fees for\n> ease of use and payment reliability.\n>\n> To answer 2., we can re-use the existing gossip infrastructure to exchange\n> a new *node_update *message that\n> contains the trampoline fees and cltv. However Alice doesn't want to\n> receive every network update because she\n> doesn't have the bandwidth to support it (damn it again Alice, upgrade\n> your mobile plan). My suggestion is to\n> create a filter system (similiar to BIP37) where Alice sends gossip\n> filters to her peers, and peers only forward to\n> Alice updates that match these filters. This doesn't have the issues BIP37\n> has for Bitcoin because it has a cost\n> for Alice: she has to open a channel (and thus lock funds) to get a\n> connection to a peer. Peers can refuse to serve\n> filters if they are too expensive to compute, but the filters I propose in\n> the PR are very cheap (a simple xor or a\n> node distance comparison).\n>\n> If you're interested in the technical details, head over to [1].\n> I would really like to get feedback from this list on the concept itself,\n> and especially on the gossip and fee estimation\n> parts. If you made it that far, I'm sure you have many questions and\n> suggestions ;).\n>\n> Cheers,\n> Bastien\n>\n> [1] https://github.com/lightningnetwork/lightning-rfc/pull/654\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20190802/305c6328/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-08-03T03:48:48",
                "message_text_only": "Good morning fiatjaf,\n\nI proposed before that we could institute a rule where nodes are mapped to some virtual space, and nodes should preferably retain the part of the network graph that connects itself to those nodes near to it in this virtual space (and possibly prefer to channel to those nodes).\n\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2019-April/001959.html\n\nThus Alice might **not** know that some route exists between T1 and T2.\n\nT1 itself might not know of a route from itself to T2.\nBut if T1 knows a route to T1.5, and it knows that T1.5 is nearer to T2 than to itself in the virtual space, it can **try** to route through T1.5 in the hope T1.5 knows a route from itself to T2.\nThis can be done if T1 can remove itself from the trampoline route and replace itself with T1.5, offerring in exchange some of the fee to T1.5.\n\nOther ways of knowing some distillation of the public network without remembering the channel level details are also possible.\nMy recent pointlessly long spam email for example has a section on Hierarchical Maps.\n\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2019-August/002095.html\n\nRegards,\nZmnSCPxj\n\n\nSent with ProtonMail Secure Email.\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\nOn Saturday, August 3, 2019 12:29 AM, fiatjaf <fiatjaf at alhur.es> wrote:\n\n> Ok, since you seem to imply each question is valuable, here's mine: how does Alice know RT2 has a route to Bob? If she knows that, can she also know T1 has a route to Bob? In any case, why can't she just build her small onion with Alice -> T1 -> Bob? I would expect that to be the most common case, am I right?\n>\n> On Friday, August 2, 2019, Bastien TEINTURIER <bastien at acinq.fr> wrote:\n>\n> > Good morning list,\n> >\n> > I realized that trampoline routing has only been briefly described to this list (credits to cdecker and pm47 for laying\u00a0\n> > out the foundations). I just published an updated PR [1] and want to take this opportunity to present the high level\n> > view here and the parts that need a concept ACK and more feedback.\n> >\n> > Trampoline routing is conceptually quite simple. Alice wants to send a payment to Bob, but she doesn't know a\n> > route to get there because Alice only keeps a small area of the routing table locally (Alice has a crappy phone,\n> > damn it Alice sell some satoshis and buy a real phone). However, Alice has a few trampoline nodes in her\u00a0\n> > friends-of-friends and knows some trampoline nodes outside of her local area (but she doesn't know how to reach\n> > them). Alice would like to send a payment to a trampoline node she can reach and defer calculation of the rest of\n> > the route to that node.\n> >\n> > The onion routing part is very simple now that we have variable-length onion payloads (thanks again cdecker!).\n> > Just like russian dolls, we simply put a small onion inside a big onion. And the HTLC management forwards very\n> > naturally.\n> >\n> > It's always simpler with an example. Let's imagine that Alice can reach three trampoline nodes: T1, T2 and T3.\n> > She also knows the details of many remote trampoline nodes that she cannot reach: RT1, RT2, RT3 and RT4.\n> > Alice selects T1 and RT2 to use as trampoline hops. She builds a small onion that describes the following route:\n> >\n> > Alice -> T1 -> RT2 -> Bob\n> >\n> > She finds a route to T1 and builds a normal onion to send a payment to T1:\n> >\n> > Alice -> N1 -> N2 -> T1\n> >\n> > In the payload for T1, Alice puts the small trampoline onion.\n> > When T1 receives the payment, he is able to peel one layer of the trampoline onion and discover that he must\n> > forward the payment to RT2. T1 finds a route to RT2 and builds a normal onion to send a payment to RT2:\n> >\n> > T1 -> N3 -> RT2\n> >\n> > In the payload for RT2, T1 puts the peeled small trampoline onion.\n> > When RT2 receives the payment, he is able to peel one layer of the trampoline onion and discover that he must\n> > forward the payment to Bob. RT2 finds a route to Bob and builds a normal onion to send a payment:\n> >\n> > RT2 -> N4 -> N5 -> Bob\n> >\n> > In the payload for Bob, RT2 puts the peeled small trampoline onion.\n> > When Bob receives the payment, he is able to peel the last layer of the trampoline onion and discover that he is\n> > the final recipient, and fulfills the payment.\n> >\n> > Alice has successfully sent a payment to Bob deferring route calculation to some chosen trampoline nodes.\n> > That part was simple and (hopefully) not controversial, but it left out some important details:\n> >\n> > 1.  How do trampoline nodes specify their fees and cltv requirements?\n> > 2.  How does Alice sync the fees and cltv requirements for her remote trampoline nodes?\n> >\n> > To answer 1., trampoline nodes needs to estimate a fee and cltv that allows them to route to (almost) any other\n> > trampoline node. This is likely going to increase the fees paid by end-users, but they can't eat their cake and\n> > have it too: by not syncing the whole network, users are trading fees for ease of use and payment reliability.\n> >\n> > To answer 2., we can re-use the existing gossip infrastructure to exchange a new node_update message that\n> > contains the trampoline fees and cltv. However Alice doesn't want to receive every network update because she\n> > doesn't have the bandwidth to support it (damn it again Alice, upgrade your mobile plan). My suggestion is to\n> > create a filter system (similiar to BIP37) where Alice sends gossip filters to her peers, and peers only forward to\n> > Alice updates that match these filters. This doesn't have the issues BIP37 has for Bitcoin because it has a cost\n> > for Alice: she has to open a channel (and thus lock funds) to get a connection to a peer. Peers can refuse to serve\n> > filters if they are too expensive to compute, but the filters I propose in the PR are very cheap (a simple xor or a\n> > node distance comparison).\n> >\n> > If you're interested in the technical details, head over to [1].\n> > I would really like to get feedback from this list on the concept itself, and especially on the gossip and fee estimation\n> > parts. If you made it that far, I'm sure you have many questions and suggestions ;).\n> >\n> > Cheers,\n> > Bastien\n> >\n> > [1]\u00a0https://github.com/lightningnetwork/lightning-rfc/pull/654"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2019-08-05T07:37:52",
                "message_text_only": "Good morning fiatjaf,\n\nThis is a good question, I'm glad you asked.\n\nAs ZmnSCPxj points out, Alice doesn't know. By not syncing the full network\ngraph, Alice has to accept\n\"being in the dark\" for some decisions. She is merely hoping that RT2 *can\nfind a route* to Bob. Note that\nit's quite easy to help Alice make informed decision by proving routing\nhints in the invoice and in gossip\nmessages (which we already do for \"normal\" routing).\n\nThe graph today is strongly connected, so it's quite a reasonable\nassumption (and Alice can easily retry\nwith another choice of trampoline node if the first one fails - just like\nwe do today with normal payments).\n\nI fully agree with ZmnSCPxj though that in the future this might not be\ntrue anymore. When/if the network\nbecomes too large we will likely lose its strongly connected nature. When\nthat happens, the Lightning\nNetwork will need some kind of hierarchical / packet switched routing\narchitecture and we won't require\ntrampoline nodes to know the whole network graph and be able to route to\nmostly anyone.\nI argue that trampoline routing is a first step towards enabling that. It's\na good engineering trade-off between\nease of implementation and deployment, fixing a problem we have today and\nenabling future scaling for\nproblems we'll have tomorrow. It's somewhat easy once we have trampoline\npayments to evolve that to a\nsystem closer to the internet's packet switching infrastructure, so we'll\ndeal with that once the need for it\nbecomes obvious.\n\nDoes that answer your question?\n\nCheers,\nBastien\n\nLe sam. 3 ao\u00fbt 2019 \u00e0 05:48, ZmnSCPxj <ZmnSCPxj at protonmail.com> a \u00e9crit :\n\n> Good morning fiatjaf,\n>\n> I proposed before that we could institute a rule where nodes are mapped to\n> some virtual space, and nodes should preferably retain the part of the\n> network graph that connects itself to those nodes near to it in this\n> virtual space (and possibly prefer to channel to those nodes).\n>\n>\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-April/001959.html\n>\n> Thus Alice might **not** know that some route exists between T1 and T2.\n>\n> T1 itself might not know of a route from itself to T2.\n> But if T1 knows a route to T1.5, and it knows that T1.5 is nearer to T2\n> than to itself in the virtual space, it can **try** to route through T1.5\n> in the hope T1.5 knows a route from itself to T2.\n> This can be done if T1 can remove itself from the trampoline route and\n> replace itself with T1.5, offerring in exchange some of the fee to T1.5.\n>\n> Other ways of knowing some distillation of the public network without\n> remembering the channel level details are also possible.\n> My recent pointlessly long spam email for example has a section on\n> Hierarchical Maps.\n>\n>\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-August/002095.html\n>\n> Regards,\n> ZmnSCPxj\n>\n>\n> Sent with ProtonMail Secure Email.\n>\n> \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n> On Saturday, August 3, 2019 12:29 AM, fiatjaf <fiatjaf at alhur.es> wrote:\n>\n> > Ok, since you seem to imply each question is valuable, here's mine: how\n> does Alice know RT2 has a route to Bob? If she knows that, can she also\n> know T1 has a route to Bob? In any case, why can't she just build her small\n> onion with Alice -> T1 -> Bob? I would expect that to be the most common\n> case, am I right?\n> >\n> > On Friday, August 2, 2019, Bastien TEINTURIER <bastien at acinq.fr> wrote:\n> >\n> > > Good morning list,\n> > >\n> > > I realized that trampoline routing has only been briefly described to\n> this list (credits to cdecker and pm47 for laying\n> > > out the foundations). I just published an updated PR [1] and want to\n> take this opportunity to present the high level\n> > > view here and the parts that need a concept ACK and more feedback.\n> > >\n> > > Trampoline routing is conceptually quite simple. Alice wants to send a\n> payment to Bob, but she doesn't know a\n> > > route to get there because Alice only keeps a small area of the\n> routing table locally (Alice has a crappy phone,\n> > > damn it Alice sell some satoshis and buy a real phone). However, Alice\n> has a few trampoline nodes in her\n> > > friends-of-friends and knows some trampoline nodes outside of her\n> local area (but she doesn't know how to reach\n> > > them). Alice would like to send a payment to a trampoline node she can\n> reach and defer calculation of the rest of\n> > > the route to that node.\n> > >\n> > > The onion routing part is very simple now that we have variable-length\n> onion payloads (thanks again cdecker!).\n> > > Just like russian dolls, we simply put a small onion inside a big\n> onion. And the HTLC management forwards very\n> > > naturally.\n> > >\n> > > It's always simpler with an example. Let's imagine that Alice can\n> reach three trampoline nodes: T1, T2 and T3.\n> > > She also knows the details of many remote trampoline nodes that she\n> cannot reach: RT1, RT2, RT3 and RT4.\n> > > Alice selects T1 and RT2 to use as trampoline hops. She builds a small\n> onion that describes the following route:\n> > >\n> > > Alice -> T1 -> RT2 -> Bob\n> > >\n> > > She finds a route to T1 and builds a normal onion to send a payment to\n> T1:\n> > >\n> > > Alice -> N1 -> N2 -> T1\n> > >\n> > > In the payload for T1, Alice puts the small trampoline onion.\n> > > When T1 receives the payment, he is able to peel one layer of the\n> trampoline onion and discover that he must\n> > > forward the payment to RT2. T1 finds a route to RT2 and builds a\n> normal onion to send a payment to RT2:\n> > >\n> > > T1 -> N3 -> RT2\n> > >\n> > > In the payload for RT2, T1 puts the peeled small trampoline onion.\n> > > When RT2 receives the payment, he is able to peel one layer of the\n> trampoline onion and discover that he must\n> > > forward the payment to Bob. RT2 finds a route to Bob and builds a\n> normal onion to send a payment:\n> > >\n> > > RT2 -> N4 -> N5 -> Bob\n> > >\n> > > In the payload for Bob, RT2 puts the peeled small trampoline onion.\n> > > When Bob receives the payment, he is able to peel the last layer of\n> the trampoline onion and discover that he is\n> > > the final recipient, and fulfills the payment.\n> > >\n> > > Alice has successfully sent a payment to Bob deferring route\n> calculation to some chosen trampoline nodes.\n> > > That part was simple and (hopefully) not controversial, but it left\n> out some important details:\n> > >\n> > > 1.  How do trampoline nodes specify their fees and cltv requirements?\n> > > 2.  How does Alice sync the fees and cltv requirements for her remote\n> trampoline nodes?\n> > >\n> > > To answer 1., trampoline nodes needs to estimate a fee and cltv that\n> allows them to route to (almost) any other\n> > > trampoline node. This is likely going to increase the fees paid by\n> end-users, but they can't eat their cake and\n> > > have it too: by not syncing the whole network, users are trading fees\n> for ease of use and payment reliability.\n> > >\n> > > To answer 2., we can re-use the existing gossip infrastructure to\n> exchange a new node_update message that\n> > > contains the trampoline fees and cltv. However Alice doesn't want to\n> receive every network update because she\n> > > doesn't have the bandwidth to support it (damn it again Alice, upgrade\n> your mobile plan). My suggestion is to\n> > > create a filter system (similiar to BIP37) where Alice sends gossip\n> filters to her peers, and peers only forward to\n> > > Alice updates that match these filters. This doesn't have the issues\n> BIP37 has for Bitcoin because it has a cost\n> > > for Alice: she has to open a channel (and thus lock funds) to get a\n> connection to a peer. Peers can refuse to serve\n> > > filters if they are too expensive to compute, but the filters I\n> propose in the PR are very cheap (a simple xor or a\n> > > node distance comparison).\n> > >\n> > > If you're interested in the technical details, head over to [1].\n> > > I would really like to get feedback from this list on the concept\n> itself, and especially on the gossip and fee estimation\n> > > parts. If you made it that far, I'm sure you have many questions and\n> suggestions ;).\n> > >\n> > > Cheers,\n> > > Bastien\n> > >\n> > > [1] https://github.com/lightningnetwork/lightning-rfc/pull/654\n>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20190805/b9f66da2/attachment.html>"
            },
            {
                "author": "fiatjaf",
                "date": "2019-08-09T02:35:41",
                "message_text_only": "Ok, here's another question/probably-bad-idea: how feasible is it for these\ntrampoline nodes to return the route they've calculated somehow to the\noriginal caller so it can cache the route and use it without trampolines\nthe next time? I don't know if caching routes is a good way improve routing\noverall in larger networks, but it seems to be working well for BLW[1]\ncurrently and overall it does make sense: in day-to-day payments we tend to\npay the same people and stores over and over, rarely paying someone else\n(but of course these rare cases are still very common to be ignored).\n\nAnyway, I don't enough even to ask the question, but I guess it's\ntheoretically possible for some information to be returned along with the\npreimage in the payment route, right?\n\nThere remains a question of if and why the trampoline nodes would be or not\ninterested in cheating Alice, sending back a bad route that favors them, or\nnot returning any route at all as that would undermine their profits as\ntrampolines.\n\n[1]: https://lightning-wallet.com/what-does-olympus-server-do\n\nOn Friday, August 2, 2019, Bastien TEINTURIER <bastien at acinq.fr> wrote:\n\n> Good morning list,\n>\n> I realized that trampoline routing has only been briefly described to this\n> list (credits to cdecker and pm47 for laying\n> out the foundations). I just published an updated PR [1] and want to take\n> this opportunity to present the high level\n> view here and the parts that need a concept ACK and more feedback.\n>\n> Trampoline routing is conceptually quite simple. Alice wants to send a\n> payment to Bob, but she doesn't know a\n> route to get there because Alice only keeps a small area of the routing\n> table locally (Alice has a crappy phone,\n> damn it Alice sell some satoshis and buy a real phone). However, Alice has\n> a few trampoline nodes in her\n> friends-of-friends and knows some trampoline nodes outside of her local\n> area (but she doesn't know how to reach\n> them). Alice would like to send a payment to a trampoline node she can\n> reach and defer calculation of the rest of\n> the route to that node.\n>\n> The onion routing part is very simple now that we have variable-length\n> onion payloads (thanks again cdecker!).\n> Just like russian dolls, we simply put a small onion inside a big onion.\n> And the HTLC management forwards very\n> naturally.\n>\n> It's always simpler with an example. Let's imagine that Alice can reach\n> three trampoline nodes: T1, T2 and T3.\n> She also knows the details of many remote trampoline nodes that she cannot\n> reach: RT1, RT2, RT3 and RT4.\n> Alice selects T1 and RT2 to use as trampoline hops. She builds a small\n> onion that describes the following route:\n>\n> *Alice -> T1 -> RT2 -> Bob*\n>\n> She finds a route to T1 and builds a normal onion to send a payment to T1:\n>\n> *Alice -> N1 -> N2 -> T1*\n>\n> In the payload for T1, Alice puts the small trampoline onion.\n> When T1 receives the payment, he is able to peel one layer of the\n> trampoline onion and discover that he must\n> forward the payment to RT2. T1 finds a route to RT2 and builds a normal\n> onion to send a payment to RT2:\n>\n> *T1 -> N3 -> RT2*\n>\n> In the payload for RT2, T1 puts the peeled small trampoline onion.\n> When RT2 receives the payment, he is able to peel one layer of the\n> trampoline onion and discover that he must\n> forward the payment to Bob. RT2 finds a route to Bob and builds a normal\n> onion to send a payment:\n>\n> *RT2 -> N4 -> N5 -> Bob*\n>\n> In the payload for Bob, RT2 puts the peeled small trampoline onion.\n> When Bob receives the payment, he is able to peel the last layer of the\n> trampoline onion and discover that he is\n> the final recipient, and fulfills the payment.\n>\n> Alice has successfully sent a payment to Bob deferring route calculation\n> to some chosen trampoline nodes.\n> That part was simple and (hopefully) not controversial, but it left out\n> some important details:\n>\n>    1. How do trampoline nodes specify their fees and cltv requirements?\n>    2. How does Alice sync the fees and cltv requirements for her remote\n>    trampoline nodes?\n>\n> To answer 1., trampoline nodes needs to estimate a fee and cltv that\n> allows them to route to (almost) any other\n> trampoline node. This is likely going to increase the fees paid by\n> end-users, but they can't eat their cake and\n> have it too: by not syncing the whole network, users are trading fees for\n> ease of use and payment reliability.\n>\n> To answer 2., we can re-use the existing gossip infrastructure to exchange\n> a new *node_update *message that\n> contains the trampoline fees and cltv. However Alice doesn't want to\n> receive every network update because she\n> doesn't have the bandwidth to support it (damn it again Alice, upgrade\n> your mobile plan). My suggestion is to\n> create a filter system (similiar to BIP37) where Alice sends gossip\n> filters to her peers, and peers only forward to\n> Alice updates that match these filters. This doesn't have the issues BIP37\n> has for Bitcoin because it has a cost\n> for Alice: she has to open a channel (and thus lock funds) to get a\n> connection to a peer. Peers can refuse to serve\n> filters if they are too expensive to compute, but the filters I propose in\n> the PR are very cheap (a simple xor or a\n> node distance comparison).\n>\n> If you're interested in the technical details, head over to [1].\n> I would really like to get feedback from this list on the concept itself,\n> and especially on the gossip and fee estimation\n> parts. If you made it that far, I'm sure you have many questions and\n> suggestions ;).\n>\n> Cheers,\n> Bastien\n>\n> [1] https://github.com/lightningnetwork/lightning-rfc/pull/654\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20190808/7986db4c/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-08-09T04:41:17",
                "message_text_only": "Good morning fiatjaf,\n\n\nSent with ProtonMail Secure Email.\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\nOn Friday, August 9, 2019 10:35 AM, fiatjaf <fiatjaf at alhur.es> wrote:\n\n> Ok, here's another question/probably-bad-idea: how feasible is it for these trampoline nodes to return the route they've calculated somehow to the original caller so it can cache the route and use it without trampolines the next time? I don't know if caching routes is a good way improve routing overall in larger networks, but it seems to be working well for BLW[1] currently and overall it does make sense: in day-to-day payments we tend to pay the same people and stores over and over, rarely paying someone else (but of course these rare cases are still very common to be ignored).\n>\n> Anyway, I don't enough even to ask the question, but I guess it's theoretically possible for some information to be returned along with the preimage in the payment route, right?\n>\n> There remains a question of if and why the trampoline nodes would be or not interested in cheating Alice, sending back a bad route that favors them, or not returning any route at all as that would undermine their profits as trampolines.\n>\n> [1]: https://lightning-wallet.com/what-does-olympus-server-do\n\nIt is doable, and potentially a good idea.\n\nAs to economic incentive: the route returned would pass through the node that provides the route, so there is still economic incentive to do so.\nIn addition, the trampoline node would not have to cache it itself; instead the original payer does the caching, so this is a mild reduction in resources shouldered by the trampoline node (it avoids having to choose between recalculating the route vs caching it in its own storage).\n\nIt is even doable to support multipart payment, by simply allowing multiple routes to be returned.\n\nThough we need to redesign the route serialization.\nCurrent route serialization contains exact amount to be sent, and cannot be reused if amount is changed.\n\nRegards,\nZmnSCPxj\n\n\n\n>\n> On Friday, August 2, 2019, Bastien TEINTURIER <bastien at acinq.fr> wrote:\n>\n> > Good morning list,\n> >\n> > I realized that trampoline routing has only been briefly described to this list (credits to cdecker and pm47 for laying\u00a0\n> > out the foundations). I just published an updated PR [1] and want to take this opportunity to present the high level\n> > view here and the parts that need a concept ACK and more feedback.\n> >\n> > Trampoline routing is conceptually quite simple. Alice wants to send a payment to Bob, but she doesn't know a\n> > route to get there because Alice only keeps a small area of the routing table locally (Alice has a crappy phone,\n> > damn it Alice sell some satoshis and buy a real phone). However, Alice has a few trampoline nodes in her\u00a0\n> > friends-of-friends and knows some trampoline nodes outside of her local area (but she doesn't know how to reach\n> > them). Alice would like to send a payment to a trampoline node she can reach and defer calculation of the rest of\n> > the route to that node.\n> >\n> > The onion routing part is very simple now that we have variable-length onion payloads (thanks again cdecker!).\n> > Just like russian dolls, we simply put a small onion inside a big onion. And the HTLC management forwards very\n> > naturally.\n> >\n> > It's always simpler with an example. Let's imagine that Alice can reach three trampoline nodes: T1, T2 and T3.\n> > She also knows the details of many remote trampoline nodes that she cannot reach: RT1, RT2, RT3 and RT4.\n> > Alice selects T1 and RT2 to use as trampoline hops. She builds a small onion that describes the following route:\n> >\n> > Alice -> T1 -> RT2 -> Bob\n> >\n> > She finds a route to T1 and builds a normal onion to send a payment to T1:\n> >\n> > Alice -> N1 -> N2 -> T1\n> >\n> > In the payload for T1, Alice puts the small trampoline onion.\n> > When T1 receives the payment, he is able to peel one layer of the trampoline onion and discover that he must\n> > forward the payment to RT2. T1 finds a route to RT2 and builds a normal onion to send a payment to RT2:\n> >\n> > T1 -> N3 -> RT2\n> >\n> > In the payload for RT2, T1 puts the peeled small trampoline onion.\n> > When RT2 receives the payment, he is able to peel one layer of the trampoline onion and discover that he must\n> > forward the payment to Bob. RT2 finds a route to Bob and builds a normal onion to send a payment:\n> >\n> > RT2 -> N4 -> N5 -> Bob\n> >\n> > In the payload for Bob, RT2 puts the peeled small trampoline onion.\n> > When Bob receives the payment, he is able to peel the last layer of the trampoline onion and discover that he is\n> > the final recipient, and fulfills the payment.\n> >\n> > Alice has successfully sent a payment to Bob deferring route calculation to some chosen trampoline nodes.\n> > That part was simple and (hopefully) not controversial, but it left out some important details:\n> >\n> > 1.  How do trampoline nodes specify their fees and cltv requirements?\n> > 2.  How does Alice sync the fees and cltv requirements for her remote trampoline nodes?\n> >\n> > To answer 1., trampoline nodes needs to estimate a fee and cltv that allows them to route to (almost) any other\n> > trampoline node. This is likely going to increase the fees paid by end-users, but they can't eat their cake and\n> > have it too: by not syncing the whole network, users are trading fees for ease of use and payment reliability.\n> >\n> > To answer 2., we can re-use the existing gossip infrastructure to exchange a new node_update message that\n> > contains the trampoline fees and cltv. However Alice doesn't want to receive every network update because she\n> > doesn't have the bandwidth to support it (damn it again Alice, upgrade your mobile plan). My suggestion is to\n> > create a filter system (similiar to BIP37) where Alice sends gossip filters to her peers, and peers only forward to\n> > Alice updates that match these filters. This doesn't have the issues BIP37 has for Bitcoin because it has a cost\n> > for Alice: she has to open a channel (and thus lock funds) to get a connection to a peer. Peers can refuse to serve\n> > filters if they are too expensive to compute, but the filters I propose in the PR are very cheap (a simple xor or a\n> > node distance comparison).\n> >\n> > If you're interested in the technical details, head over to [1].\n> > I would really like to get feedback from this list on the concept itself, and especially on the gossip and fee estimation\n> > parts. If you made it that far, I'm sure you have many questions and suggestions ;).\n> >\n> > Cheers,\n> > Bastien\n> >\n> > [1]\u00a0https://github.com/lightningnetwork/lightning-rfc/pull/654"
            },
            {
                "author": "fiatjaf",
                "date": "2019-08-05T12:15:45",
                "message_text_only": "Thank you very much. These were very clarifying answers and ramblings.\n\nOn Monday, August 5, 2019, ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n> Good morning fiatjaf,\n>\n>> No. My question was more like why does Alice decide to build a route\nthat for through T1 and RT2 and not only through one trampoline router she\nknows.\n>\n> If Alice only always used one trampoline node, then the trampoline node\ncan assume the next hop is always the payee, and thus record who the payee\nis (eroding privacy).\n> If Alice uses two, then a trampoline node would have a 50/50 chance of\nknowing who the final payee is, reducing the privacy erosion.\n>\n> Similarly, onion routing over Tor typically passes through 3 \"trampoline\"\nnodes before going to the actual site being accessed.\n>\n>>\n>> That makes sense you me in the context of ZmnSCPxj's virtual space idea,\nbut not necessarily in the current network conditions. You also said we're\ngoing to need some hierarchy, but what it's that? Is it required?\n>\n> I believe in the future we will see a public network that is too large to\nfit on most devices available to most people.\n> We may or may not want to have such an enormous network, but the cost of\nadvertising a public channel is the same as the cost of creating a\nnon-public channel, thus there is no incentive for random end-user nodes to\n*not* publish their channels, and incentive to publish (there is a tiny but\nnon-zero chance of being routed through, especially as local-area\nspecializations like JIT-Routing get implemented).\n>\n> Thus, I believe it is eventually required that we hierarchicalize how we\nstore information, with a \"myopic\" detailed channel map and a \"rough\"\nglobal map with just trampoline-payee association mappings.\n> I think it is best for each payer to define its own hierarchy or split,\npreferentially with some random component.\n>\n> One might consider, however, that my ramblings are too indefinite and it\nwould be better to see the network as it evolves.\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20190805/85da457a/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Trampoline Routing",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Bastien TEINTURIER",
                "fiatjaf",
                "ZmnSCPxj"
            ],
            "messages_count": 7,
            "total_messages_chars_count": 38776
        }
    },
    {
        "title": "[Lightning-dev] Fwd:  Trampoline Routing",
        "thread_messages": [
            {
                "author": "fiatjaf",
                "date": "2019-08-05T09:30:34",
                "message_text_only": "No. My question was more like why does Alice decide to build a route that\nfor through T1 and RT2 and not only through one trampoline router she knows.\n\nThat makes sense you me in the context of ZmnSCPxj's virtual space idea,\nbut not necessarily in the current network conditions. You also said we're\ngoing to need some hierarchy, but what it's that? Is it required?\n\nAnyway, I'm probably missing something, but another way of putting my\nquestion would be: why does your example use 2 trampolines instead of 1?\n\nOn Monday, August 5, 2019, Bastien TEINTURIER <bastien at acinq.fr> wrote:\n> Good morning fiatjaf,\n> This is a good question, I'm glad you asked.\n> As:m ZmnSCPxj points out, Alice doesn't know. By not syncing the full\nnetwork graph, Alice has to accept\n> \"being in the dark\" for some decisions. She is merely hoping that RT2 can\nfind a route to Bob. Note that\n> it's quite easy to help Alice make informed decision by proving routing\nhints in the invoice and in gossip\n> messages (which we already do for \"normal\" routing).\n> The graph today is strongly connected, so it's quite a reasonable\nassumption (and Alice can easily retry\n> with another choice of trampoline node if the first one fails - just like\nwe do today with normal payments).\n> I fully agree with ZmnSCPxj though that in the future this might not be\ntrue anymore. When/if the network\n> becomes too large we will likely lose its strongly connected nature. When\nthat happens, the Lightning\n> Network will need some kind of hierarchical / packet switched routing\narchitecture and we won't require\n> trampoline nodes to know the whole network graph and be able to route to\nmostly anyone.\n> I argue that trampoline routing is a first step towards enabling that.\nIt's a good engineering trade-off between\n> ease of implementation and deployment, fixing a problem we have today and\nenabling future scaling for\n> problems we'll have tomorrow. It's somewhat easy once we have trampoline\npayments to evolve that to a\n> system closer to the internet's packet switching infrastructure, so we'll\ndeal with that once the need for it\n> becomes obvious.\n> Does that answer your question?\n> Cheers,\n> Bastien\n> Le sam. 3 ao\u00fbt 2019 \u00e0 05:48, ZmnSCPxj <ZmnSCPxj at protonmail.com> a \u00e9crit :\n>>\n>> Good morning fiatjaf,\n>>\n>> I proposed before that we could institute a rule where nodes are mapped\nto some virtual space, and nodes should preferably retain the part of the\nnetwork graph that connects itself to those nodes near to it in this\nvirtual space (and possibly prefer to channel to those nodes).\n>>\n>>\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2019-April/001959.html\n>>\n>> Thus Alice might **not** know that some route exists between T1 and T2.\n>>\n>> T1 itself might not know of a route from itself to T2.\n>> But if T1 knows a route to T1.5, and it knows that T1.5 is nearer to T2\nthan to itself in the virtual space, it can **try** to route through T1.5\nin the hope T1.5 knows a route from itself to T2.\n>> This can be done if T1 can remove itself from the trampoline route and\nreplace itself with T1.5, offerring in exchange some of the fee to T1.5.\n>>\n>> Other ways of knowing some distillation of the public network without\nremembering the channel level details are also possible.\n>> My recent pointlessly long spam email for example has a section on\nHierarchical Maps.\n>>\n>>\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2019-August/002095.html\n>>\n>> Regards,\n>> ZmnSCPxj\n>>\n>>\n>> Sent with ProtonMail Secure Email.\n>>\n>> \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n>> On Saturday, August 3, 2019 12:29 AM, fiatjaf <fiatjaf at alhur.es> wrote:\n>>\n>> > Ok, since you seem to imply each question is valuable, here's mine:\nhow does Alice know RT2 has a route to Bob? If she knows that, can she also\nknow T1 has a route to Bob? In any case, why can't she just build her small\nonion with Alice -> T1 -> Bob? I would expect that to be the most common\ncase, am I right?\n>> >\n>> > On Friday, August 2, 2019, Bastien TEINTURIER <bastien at acinq.fr> wrote:\n>> >\n>> > > Good morning list,\n>> > >\n>> > > I realized that trampoline routing has only been briefly described\nto this list (credits to cdecker and pm47 for laying\n>> > > out the foundations). I just published an updated PR [1] and want to\ntake this opportunity to present the high level\n>> > > view here and the parts that need a concept ACK and more feedback.\n>> > >\n>> > > Trampoline routing is conceptually quite simple. Alice wants to send\na payment to Bob, but she doesn't know a\n>> > > route to get there because Alice only keeps a small area of the\nrouting table locally (Alice has a crappy phone,\n>> > > damn it Alice sell some satoshis and buy a real phone). However,\nAlice has a few trampoline nodes in her\n>> > > friends-of-friends and knows some trampoline nodes outside of her\nlocal area (but she doesn't know how to reach\n>> > > them). Alice would like to send a payment to a trampoline node she\ncan reach and defer calculation of the rest of\n>> > > the route to that node.\n>> > >\n>> > > The onion routing part is very simple now that we have\nvariable-length onion payloads (thanks again cdecker!).\n>> > > Just like russian dolls, we simply put a small onion inside a big\nonion. And the HTLC management forwards very\n>> > > naturally.\n>> > >\n>> > > It's always simpler with an example. Let's imagine that Alice can\nreach three trampoline nodes: T1, T2 and T3.\n>> > > She also knows the details of many remote trampoline nodes that she\ncannot reach: RT1, RT2, RT3 and RT4.\n>> > > Alice selects T1 and RT2 to use as trampoline hops. She builds a\nsmall onion that describes the following route:\n>> > >\n>> > > Alice -> T1 -> RT2 -> Bob\n>> > >\n>> > > She finds a route to T1 and builds a normal onion to send a payment\nto T1:\n>> > >\n>> > > Alice -> N1 -> N2 -> T1\n>> > >\n>> > > In the payload for T1, Alice puts the small trampoline onion.\n>> > > When T1 receives the payment, he is able to peel one layer of the\ntrampoline onion and discover that he must\n>> > > forward the payment to RT2. T1 finds a route to RT2 and builds a\nnormal onion to send a payment to RT2:\n>> > >\n>> > > T1 -> N3 -> RT2\n>> > >\n>> > > In the payload for RT2, T1 puts the peeled small trampoline onion.\n>> > > When RT2 receives the payment, he is able to peel one layer of the\ntrampoline onion and discover that he must\n>> > > forward the payment to Bob. RT2 finds a route to Bob and builds a\nnormal onion to send a payment:\n>> > >\n>> > > RT2 -> N4 -> N5 -> Bob\n>> > >\n>> > > In the payload for Bob, RT2 puts the peeled small trampoline onion.\n>> > > When Bob receives the payment, he is able to peel the last layer of\nthe trampoline onion and discover that he is\n>> > > the final recipient, and fulfills the payment.\n>> > >\n>> > > Alice has successfully sent a payment to Bob deferring route\ncalculation to some chosen trampoline nodes.\n>> > > That part was simple and (hopefully) not controversial, but it left\nout some important details:\n>> > >\n>> > > 1.  How do trampoline nodes specify their fees and cltv requirements?\n>> > > 2.  How does Alice sync the fees and cltv requirements for her\nremote trampoline nodes?\n>> > >\n>> > > To answer 1., trampoline nodes needs to estimate a fee and cltv that\nallows them to route to (almost) any other\n>> > > trampoline node. This is likely going to increase the fees paid by\nend-users, but they can't eat their cake and\n>> > > have it too: by not syncing the whole network, users are trading\nfees for ease of use and payment reliability.\n>> > >\n>> > > To answer 2., we can re-use the existing gossip infrastructure to\nexchange a new node_update message that\n>> > > contains the trampoline fees and cltv. However Alice doesn't want to\nreceive every network update because she\n>> > > doesn't have the bandwidth to support it (damn it again Alice,\nupgrade your mobile plan). My suggestion is to\n>> > > create a filter system (similiar to BIP37) where Alice sends gossip\nfilters to her peers, and peers only forward to\n>> > > Alice updates that match these filters. This doesn't have the issues\nBIP37 has for Bitcoin because it has a cost\n>> > > for Alice: she has to open a channel (and thus lock funds) to get a\nconnection to a peer. Peers can refuse to serve\n>> > > filters if they are too expensive to compute, but the filters I\npropose in the PR are very cheap (a simple xor or a\n>> > > node distance comparison).\n>> > >\n>> > > If you're interested in the technical details, head over to [1].\n>> > > I would really like to get feedback from this list on the concept\nitself, and especially on the gossip and fee estimation\n>> > > parts. If you made it that far, I'm sure you have many questions and\nsuggestions ;).\n>> > >\n>> > > Cheers,\n>> > > Bastien\n>> > >\n>> > > [1] https://github.com/lightningnetwork/lightning-rfc/pull/654\n>>\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20190805/7c8c7c1b/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-08-05T10:00:04",
                "message_text_only": "Good morning fiatjaf,\n\n> No. My question was more like why does Alice decide to build a route that for through T1 and RT2 and not only through one trampoline router she knows.\n\nIf Alice only always used one trampoline node, then the trampoline node can assume the next hop is always the payee, and thus record who the payee is (eroding privacy).\nIf Alice uses two, then a trampoline node would have a 50/50 chance of knowing who the final payee is, reducing the privacy erosion.\n\nSimilarly, onion routing over Tor typically passes through 3 \"trampoline\" nodes before going to the actual site being accessed.\n\n>\n> That makes sense you me in the context of ZmnSCPxj's virtual space idea, but not necessarily in the current network conditions. You also said we're going to need some hierarchy, but what it's that? Is it required?\n\nI believe in the future we will see a public network that is too large to fit on most devices available to most people.\nWe may or may not want to have such an enormous network, but the cost of advertising a public channel is the same as the cost of creating a non-public channel, thus there is no incentive for random end-user nodes to *not* publish their channels, and incentive to publish (there is a tiny but non-zero chance of being routed through, especially as local-area specializations like JIT-Routing get implemented).\n\nThus, I believe it is eventually required that we hierarchicalize how we store information, with a \"myopic\" detailed channel map and a \"rough\" global map with just trampoline-payee association mappings.\nI think it is best for each payer to define its own hierarchy or split, preferentially with some random component.\n\nOne might consider, however, that my ramblings are too indefinite and it would be better to see the network as it evolves.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Fwd:  Trampoline Routing",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "fiatjaf",
                "ZmnSCPxj"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 10809
        }
    },
    {
        "title": "[Lightning-dev] Fwd: Trampoline Routing",
        "thread_messages": [
            {
                "author": "Bastien TEINTURIER",
                "date": "2019-08-05T09:42:30",
                "message_text_only": ">\n> Anyway, I'm probably missing something, but another way of putting my\n> question would be: why does your example use 2 trampolines instead of 1?\n\n\nBecause I wanted to show the generality of the scheme: the number of\ntrampoline hops is entirely Alice's choice.\nIf Alice only cares about cost-efficiency, she will choose a single\ntrampoline hop (in the current network's conditions).\nIf Alice cares about privacy, she will likely chose more than one\ntrampoline hop.\nThe fact that she *may* use multiple trampoline hops is important because\nit increases her anonymity set (even if she\nuses only one in the end).\n\nYou also said we're going to need some hierarchy, but what it's that? Is it\n> required?\n\n\nThis is not needed in the current network because the routing table is\nstill small.\nIf the network eventually reaches a billion channels, we can't expect even\ntrampoline nodes to sync everything and\nbe able to find a route to any other node in the network: when/if that\nhappens, we will need to introduce some kind\nof hierarchy / packet-switching as ZmnCSPxj previously mentioned.\nBut we don't know yet if that will happen, or when it will happen. It's\nimportant to think about it and make sure we can\nhave a working solution if that happens, but this isn't a short-term need.\n\n\nLe lun. 5 ao\u00fbt 2019 \u00e0 11:30, fiatjaf <fiatjaf at alhur.es> a \u00e9crit :\n\n> No. My question was more like why does Alice decide to build a route that\n> for through T1 and RT2 and not only through one trampoline router she knows.\n>\n> That makes sense you me in the context of ZmnSCPxj's virtual space idea,\n> but not necessarily in the current network conditions. You also said we're\n> going to need some hierarchy, but what it's that? Is it required?\n>\n> Anyway, I'm probably missing something, but another way of putting my\n> question would be: why does your example use 2 trampolines instead of 1?\n>\n> On Monday, August 5, 2019, Bastien TEINTURIER <bastien at acinq.fr> wrote:\n> > Good morning fiatjaf,\n> > This is a good question, I'm glad you asked.\n> > As:m ZmnSCPxj points out, Alice doesn't know. By not syncing the full\n> network graph, Alice has to accept\n> > \"being in the dark\" for some decisions. She is merely hoping that RT2\n> can find a route to Bob. Note that\n> > it's quite easy to help Alice make informed decision by proving routing\n> hints in the invoice and in gossip\n> > messages (which we already do for \"normal\" routing).\n> > The graph today is strongly connected, so it's quite a reasonable\n> assumption (and Alice can easily retry\n> > with another choice of trampoline node if the first one fails - just\n> like we do today with normal payments).\n> > I fully agree with ZmnSCPxj though that in the future this might not be\n> true anymore. When/if the network\n> > becomes too large we will likely lose its strongly connected nature.\n> When that happens, the Lightning\n> > Network will need some kind of hierarchical / packet switched routing\n> architecture and we won't require\n> > trampoline nodes to know the whole network graph and be able to route to\n> mostly anyone.\n> > I argue that trampoline routing is a first step towards enabling that.\n> It's a good engineering trade-off between\n> > ease of implementation and deployment, fixing a problem we have today\n> and enabling future scaling for\n> > problems we'll have tomorrow. It's somewhat easy once we have trampoline\n> payments to evolve that to a\n> > system closer to the internet's packet switching infrastructure, so\n> we'll deal with that once the need for it\n> > becomes obvious.\n> > Does that answer your question?\n> > Cheers,\n> > Bastien\n> > Le sam. 3 ao\u00fbt 2019 \u00e0 05:48, ZmnSCPxj <ZmnSCPxj at protonmail.com> a\n> \u00e9crit :\n> >>\n> >> Good morning fiatjaf,\n> >>\n> >> I proposed before that we could institute a rule where nodes are mapped\n> to some virtual space, and nodes should preferably retain the part of the\n> network graph that connects itself to those nodes near to it in this\n> virtual space (and possibly prefer to channel to those nodes).\n> >>\n> >>\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-April/001959.html\n> >>\n> >> Thus Alice might **not** know that some route exists between T1 and T2.\n> >>\n> >> T1 itself might not know of a route from itself to T2.\n> >> But if T1 knows a route to T1.5, and it knows that T1.5 is nearer to T2\n> than to itself in the virtual space, it can **try** to route through T1.5\n> in the hope T1.5 knows a route from itself to T2.\n> >> This can be done if T1 can remove itself from the trampoline route and\n> replace itself with T1.5, offerring in exchange some of the fee to T1.5.\n> >>\n> >> Other ways of knowing some distillation of the public network without\n> remembering the channel level details are also possible.\n> >> My recent pointlessly long spam email for example has a section on\n> Hierarchical Maps.\n> >>\n> >>\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-August/002095.html\n> >>\n> >> Regards,\n> >> ZmnSCPxj\n> >>\n> >>\n> >> Sent with ProtonMail Secure Email.\n> >>\n> >> \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n> >> On Saturday, August 3, 2019 12:29 AM, fiatjaf <fiatjaf at alhur.es> wrote:\n> >>\n> >> > Ok, since you seem to imply each question is valuable, here's mine:\n> how does Alice know RT2 has a route to Bob? If she knows that, can she also\n> know T1 has a route to Bob? In any case, why can't she just build her small\n> onion with Alice -> T1 -> Bob? I would expect that to be the most common\n> case, am I right?\n> >> >\n> >> > On Friday, August 2, 2019, Bastien TEINTURIER <bastien at acinq.fr>\n> wrote:\n> >> >\n> >> > > Good morning list,\n> >> > >\n> >> > > I realized that trampoline routing has only been briefly described\n> to this list (credits to cdecker and pm47 for laying\n> >> > > out the foundations). I just published an updated PR [1] and want\n> to take this opportunity to present the high level\n> >> > > view here and the parts that need a concept ACK and more feedback.\n> >> > >\n> >> > > Trampoline routing is conceptually quite simple. Alice wants to\n> send a payment to Bob, but she doesn't know a\n> >> > > route to get there because Alice only keeps a small area of the\n> routing table locally (Alice has a crappy phone,\n> >> > > damn it Alice sell some satoshis and buy a real phone). However,\n> Alice has a few trampoline nodes in her\n> >> > > friends-of-friends and knows some trampoline nodes outside of her\n> local area (but she doesn't know how to reach\n> >> > > them). Alice would like to send a payment to a trampoline node she\n> can reach and defer calculation of the rest of\n> >> > > the route to that node.\n> >> > >\n> >> > > The onion routing part is very simple now that we have\n> variable-length onion payloads (thanks again cdecker!).\n> >> > > Just like russian dolls, we simply put a small onion inside a big\n> onion. And the HTLC management forwards very\n> >> > > naturally.\n> >> > >\n> >> > > It's always simpler with an example. Let's imagine that Alice can\n> reach three trampoline nodes: T1, T2 and T3.\n> >> > > She also knows the details of many remote trampoline nodes that she\n> cannot reach: RT1, RT2, RT3 and RT4.\n> >> > > Alice selects T1 and RT2 to use as trampoline hops. She builds a\n> small onion that describes the following route:\n> >> > >\n> >> > > Alice -> T1 -> RT2 -> Bob\n> >> > >\n> >> > > She finds a route to T1 and builds a normal onion to send a payment\n> to T1:\n> >> > >\n> >> > > Alice -> N1 -> N2 -> T1\n> >> > >\n> >> > > In the payload for T1, Alice puts the small trampoline onion.\n> >> > > When T1 receives the payment, he is able to peel one layer of the\n> trampoline onion and discover that he must\n> >> > > forward the payment to RT2. T1 finds a route to RT2 and builds a\n> normal onion to send a payment to RT2:\n> >> > >\n> >> > > T1 -> N3 -> RT2\n> >> > >\n> >> > > In the payload for RT2, T1 puts the peeled small trampoline onion.\n> >> > > When RT2 receives the payment, he is able to peel one layer of the\n> trampoline onion and discover that he must\n> >> > > forward the payment to Bob. RT2 finds a route to Bob and builds a\n> normal onion to send a payment:\n> >> > >\n> >> > > RT2 -> N4 -> N5 -> Bob\n> >> > >\n> >> > > In the payload for Bob, RT2 puts the peeled small trampoline onion.\n> >> > > When Bob receives the payment, he is able to peel the last layer of\n> the trampoline onion and discover that he is\n> >> > > the final recipient, and fulfills the payment.\n> >> > >\n> >> > > Alice has successfully sent a payment to Bob deferring route\n> calculation to some chosen trampoline nodes.\n> >> > > That part was simple and (hopefully) not controversial, but it left\n> out some important details:\n> >> > >\n> >> > > 1.  How do trampoline nodes specify their fees and cltv\n> requirements?\n> >> > > 2.  How does Alice sync the fees and cltv requirements for her\n> remote trampoline nodes?\n> >> > >\n> >> > > To answer 1., trampoline nodes needs to estimate a fee and cltv\n> that allows them to route to (almost) any other\n> >> > > trampoline node. This is likely going to increase the fees paid by\n> end-users, but they can't eat their cake and\n> >> > > have it too: by not syncing the whole network, users are trading\n> fees for ease of use and payment reliability.\n> >> > >\n> >> > > To answer 2., we can re-use the existing gossip infrastructure to\n> exchange a new node_update message that\n> >> > > contains the trampoline fees and cltv. However Alice doesn't want\n> to receive every network update because she\n> >> > > doesn't have the bandwidth to support it (damn it again Alice,\n> upgrade your mobile plan). My suggestion is to\n> >> > > create a filter system (similiar to BIP37) where Alice sends gossip\n> filters to her peers, and peers only forward to\n> >> > > Alice updates that match these filters. This doesn't have the\n> issues BIP37 has for Bitcoin because it has a cost\n> >> > > for Alice: she has to open a channel (and thus lock funds) to get a\n> connection to a peer. Peers can refuse to serve\n> >> > > filters if they are too expensive to compute, but the filters I\n> propose in the PR are very cheap (a simple xor or a\n> >> > > node distance comparison).\n> >> > >\n> >> > > If you're interested in the technical details, head over to [1].\n> >> > > I would really like to get feedback from this list on the concept\n> itself, and especially on the gossip and fee estimation\n> >> > > parts. If you made it that far, I'm sure you have many questions\n> and suggestions ;).\n> >> > >\n> >> > > Cheers,\n> >> > > Bastien\n> >> > >\n> >> > > [1] https://github.com/lightningnetwork/lightning-rfc/pull/654\n> >>\n> >>\n> >\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20190805/b8c3e417/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Fwd: Trampoline Routing",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Bastien TEINTURIER"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 10927
        }
    },
    {
        "title": "[Lightning-dev] Paper - Modeling a Steady-State Lightning Network Economy",
        "thread_messages": [
            {
                "author": "Gregorio Guidi",
                "date": "2019-08-09T21:36:31",
                "message_text_only": "Hello,\n\nI am a mostly a passive observer in this mailing list, trying to follow \nthe very interesting discussions and experimenting a bit with running a \nnode on mainnet (even if I cannot always keep pace with the new \ndevelopments due to limited time).\n\nRunning a node during the last months inspired some reflections and some \nmildly interesting mathematical explorations, which I collected in a \nsort of paper which might be of interest to the list:\n\nhttps://github.com/gr-g/ln-steady-state-model/releases/download/v20190808/Modeling.a.Steady-State.Lightning.Network.Economy.pdf\n\nI say \"sort of paper\" because there is no bibliography, and no awareness \nof the greater context in which these ideas can fit. That said, if \nanyone would be interested in making something out of these ideas (e.g. \na real co-authored paper), they are very welcome to do so.\n\nBelow is the abstract. Any comment is appreciated!\n\nBest,\n\nGregorio\n\n\nAbstract:\n\nIn this paper, we consider an idealized scenario in which the Lightning \nNetwork (or any similar payment network) has scaled to the size and \nvolume of a self-sustained economy, meaning that the number of on-chain \ntransactions - including channel opening and closing - has become \nnegligible when compared to the number of off-chain transactions, and \npayments continuously flow across a network with relatively stable \ntopology. We take this scenario to the extreme and model a network where \nthe channels are fixed, so that payments form a completely closed \nsystem, and where nodes have (on a long enough timescale) stable and \nperfectly balanced incoming and outgoing payments (i.e. they spend \nexactly what they earn). We call this scenario the \"steady-state \neconomy\" of the payment network.\n\nWe argue that in such scenario, in a network of /n/ connected nodes, \nthere is a tendency towards a state where exactly /n/-1 channels have \nperfectly balanced flows in the two directions (\"self-balancing\" \nchannels), while all other channels are either unused, or have a \npermanent tendency towards imbalance: the channel balance accumulates at \none end and the channel is only intermittently available in one \ndirection (\"stuttering\" channels). We note that the \"self-balancing\" \nchannels form a spanning tree of the network graph, which we call the \n\"core spanning tree\" of the payment network.\n\nWe also try to derive some practical lessons from this idealized \nscenario, hopefully providing some useful insight to node operators of \nthe current (embryonic) Lightning Network.\n\nAt the end of the paper, we provide some remarks on the more general \ncase in which nodes do not balance their income and expenses.\n\nhttps://github.com/gr-g/ln-steady-state-model\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20190809/6b25beb6/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-08-13T04:23:31",
                "message_text_only": "Good morning Gregorio,\n\n\n> We argue that in such scenario, in a network of n connected nodes, there is a tendency towards a state where exactly n-1 channels have perfectly balanced flows in the two directions (\"self-balancing\" channels), while all other channels are either unused, or have a permanent tendency towards imbalance: the channel balance accumulates at one end and the channel is only intermittently available in one direction (\"stuttering\" channels). We note that the \"self-balancing\" channels form a spanning tree of the network graph, which we call the \"core spanning tree\" of the payment network.\n\nI have observed this as well in my armchair.\nIndeed, the worst-case scenario for this would be a single central hub with n-1 channels to n-1 client nodes, with the hub itself as the nth node.\n\nFortunately, it seems to me that such a steady state will mildly shift and additional channels will still be used intermittently.\n(I have not read your paper in completeness yet, only the abstract above, so if my ramblings have already been considered in your paper, do feel free to ignore me).\n\nFor instance, consider a world where Lamborghini production experiences a paradigm shift that massively reduces the cost of production while increase quality.\nInevitably, Lambo prices in terms of Bitcoin will decline, as a consequence of this improvement.\nThus, capacity in channels going Lamborghini producers becomes underutilized as demand for the product saturates while supply increases.\n\nHowever, \"complementary goods\" of such products are highly unlikely to experience a commensurate increase in their own production.\nFor example, simply because Lamborghini production increases its efficiency, does not mean that car insurance will experience the same increase in its efficiency.\nThus, the supply of car insurance will remain largely the same, but demand increases due to increased usage of Lamborghinis (a well-known phenomenon in economics).\nThis implies that capacity in channels going to car insurance providers will be overutilized and eventually saturate, leading to opportunistic reallocation of funds from Lamborghini producers to car insurance providers (and concomitant onchain activity to do this reallocation!).\n\nThus, I think that the minimum of n-1 channels would not remain stable for long, given an actual real-world where change *is* the steady-state.\nAssuming a continuously thriving and innovating global economy, we will expect that there will be transient situations where demand and supply for various products changes wildly.\nIn such situations, any \"extra, unneeded\" channels would end up catching the additional capacity need as various innovations and improvements in the economy occur and create change in demand and supply.\nI believe the overall steady state will have c*n channels rather than merely n-1, where c is some constant greater than 1, due to various local transient demand/supply shocks.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Gregorio Guidi",
                "date": "2019-08-14T14:12:13",
                "message_text_only": "On August 13, 2019 6:23:31 AM GMT+02:00, ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n>Good morning Gregorio,\n>\n>\n>> We argue that in such scenario, in a network of n connected nodes,\n>there is a tendency towards a state where exactly n-1 channels have\n>perfectly balanced flows in the two directions (\"self-balancing\"\n>channels), while all other channels are either unused, or have a\n>permanent tendency towards imbalance: the channel balance accumulates\n>at one end and the channel is only intermittently available in one\n>direction (\"stuttering\" channels). We note that the \"self-balancing\"\n>channels form a spanning tree of the network graph, which we call the\n>\"core spanning tree\" of the payment network.\n>\n>I have observed this as well in my armchair.\n>Indeed, the worst-case scenario for this would be a single central hub\n>with n-1 channels to n-1 client nodes, with the hub itself as the nth\n>node.\n>\n>Fortunately, it seems to me that such a steady state will mildly shift\n>and additional channels will still be used intermittently.\n>(I have not read your paper in completeness yet, only the abstract\n>above, so if my ramblings have already been considered in your paper,\n>do feel free to ignore me).\n\nIndeed even in the abstract steady-state scenario described in the paper, more than n-1 channels are expected to be used (although intermittently).\n\nBut the more general point, anyway, is that no steady state will ever exist in practice. Thinking in terms of black and white in this case helped to clarify my thoughts around the problem (and makes for more elegant math :) ), and as sometimes happens it lets some useful knowledge emerge: this is in the end the spirit of the paper. I would say that your intuition about the continuously shifting equilibrium is in line with mine.\n\n>For instance, consider a world where Lamborghini production experiences\n>a paradigm shift that massively reduces the cost of production while\n>increase quality.\n>Inevitably, Lambo prices in terms of Bitcoin will decline, as a\n>consequence of this improvement.\n>Thus, capacity in channels going Lamborghini producers becomes\n>underutilized as demand for the product saturates while supply\n>increases.\n>\n> [...]\n>\n>Thus, I think that the minimum of n-1 channels would not remain stable\n>for long, given an actual real-world where change *is* the\n>steady-state.\n>Assuming a continuously thriving and innovating global economy, we will\n>expect that there will be transient situations where demand and supply\n>for various products changes wildly.\n>In such situations, any \"extra, unneeded\" channels would end up\n>catching the additional capacity need as various innovations and\n>improvements in the economy occur and create change in demand and\n>supply.\n>I believe the overall steady state will have c*n channels rather than\n>merely n-1, where c is some constant greater than 1, due to various\n>local transient demand/supply shocks.\n\nI would put it simply in this way: the always-shifting patterns of demand and supply in a dense web of channels will naturally cause agents to add traffic to channels that were previously not used, and remove traffic from channels previously used, resulting in a continuous shift in equilibrium.\n\nIn this case I would not say it is a matter of \"additional capacity needs\", though. I think it could be misleading... a hypothetical Lambo dealer could have just one channel used for inflows/outflows of funds, and immediately  send back through the channel all the received income (to buy some store of value). In such case the capacity would not matter much even with a big shift in demand.\n\n(... and not included in the paper are a whole bunch of considerations about how having many channels helps the general resilience and flexibility of the network, but those should also be kept in mind.)\n\nThanks for the feedback!\n\nRegards,\nGregorio\n\n>Regards,\n>ZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Paper - Modeling a Steady-State Lightning Network Economy",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Gregorio Guidi",
                "ZmnSCPxj"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 9723
        }
    },
    {
        "title": "[Lightning-dev] Proposal: Automated Inbound Liquidity With Invoices",
        "thread_messages": [
            {
                "author": "Ecurrencyhodler Blockchains",
                "date": "2019-08-12T03:42:32",
                "message_text_only": "Hi. I'd like to propose a way for instant inbound liquidity to be automated\nvia invoices and would appreciate your feedback.  It's similar to Thor's\ninstant channel but this proposal would only be valuable if it becomes a\nstandard across all lightning implementations and wallets.  It won't work\nif it's limited to just one Lightning wallet.\n\n*Proposal:* Automated Inbound Liquidity With Invoices\n\n*For Who:* Full Lightning Network nodes\n\n*Problem:* Waiting for inbound liquidity as channel establishes when you\nfirst come online and want to receive a LN payment.\n\n*Solution: *Embedding the node uri of the invoice creator, along with\namount to be routed.\n\n*Scenario: *\n\n   1. Bob wants to send me 100,000 sats.\n   2. My node just came online and has 0 inbound liquidity.\n   3. I create an invoice for 100,000 sats.  My LN node recognizes I have 0\n   inbound liquidity so my wallet also embeds my URI in the invoice.\n   4. Bob\u2019s wallet sees an invoice + uri.  Maybe even tries to route.  When\n   it doesn\u2019t see anything, it auto opens a channel + pushes 100,000 sat\n   payment.\n   5. I now own and can spend 100,000 sats instantly.\n\n*Considerations:*\n\n   - This auto establishing of channels and pushing payments isn\u2019t for all\n   LN nodes.  Just routing nodes.\n   - Bob doesn\u2019t need to be the one to establish the channel.  He can push\n   the information down the line until a node dedicated to routing is found.\n   The routing node can then be the one to establish the channel with me.\n   - Certain specifics need to be flushed out such as the size of Bob\u2019s\n   channel.  Right now I think Bob can manually set the size of the channels\n   to be established on his end.  Should be smaller channels at first.  If the\n   person gets paid again, just establish another channel towards the same\n   node if there isn't enough capacity.\n   - Routing nodes who provide this service can charge a premium.\n   - Bob, as a liquidity provider, won't cheat against himself so I can\n   make LN payments instantly.\n   - The beauty behind this proposal is that I can receive a payment\n   instantly, I can send payments instantly, and that it hides everything from\n   me as an end user.\n   - Can possibly be extended to neutrino LN wallets if they are public.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20190811/08167b21/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2019-08-13T01:34:57",
                "message_text_only": "Ecurrencyhodler Blockchains <ecurrencyhodler at gmail.com> writes:\n>    1. Bob wants to send me 100,000 sats.\n>    2. My node just came online and has 0 inbound liquidity.\n>    3. I create an invoice for 100,000 sats.  My LN node recognizes I have 0\n>    inbound liquidity so my wallet also embeds my URI in the invoice.\n>    4. Bob\u2019s wallet sees an invoice + uri.  Maybe even tries to route.  When\n>    it doesn\u2019t see anything, it auto opens a channel + pushes 100,000 sat\n>    payment.\n>    5. I now own and can spend 100,000 sats instantly.\n\nIf you publish your node address, Bob can already get this from the\ngossip network, or the DNS seed as a last resort (and I expect\nimplementations to start doing this: I did it manually to buy a\nthelightningconference.com ticket recently, for example).\n\nSo this proposal is mainly useful where you have no channels at all\n(thus cannot advertize your node), or don't want to publish it\ngenerally.  And in both those cases, Bob probably doesn't want a channel\nwith you because it wouldn't be useful for paying anyone else.\n\nCheers,\nRusty."
            },
            {
                "author": "ecurrencyhodler",
                "date": "2019-08-13T22:42:18",
                "message_text_only": "Hey Rusty.  Thanks for your feedback.\n\n>If you publish your node address, Bob can already get this from the\ngossip network, or the DNS seed as a last resort (and I expect\nimplementations to start doing this: I did it manually to buy a\nthelightningconference.com ticket recently, for example).\n\nThis is a good point and very cool you were able to do that for the\nLightning Conference (hopefully I'll see you there!).  Perhaps another\ncondition should be that if I am a public node and already connected to a\nnode, the URI is not embedded in the invoice but instead should be gathered\nvia the gossip network.\n\nBut keep in mind that the intended goal of this proposal is for the end\nuser to have a completely automated experience from invoice payment to\npayment reception without having to manage any channels.\n\n>So this proposal is mainly useful where you have no channels at all\n(thus cannot advertize your node), or don't want to publish it\ngenerally.  And in both those cases, Bob probably doesn't want a channel\nwith you because it wouldn't be useful for paying anyone else.\n\nI'm not sure I agree.\n\nAll nodes who come online for the first time are not connected to any\nchannels.  And even if I hand Bob my URI, I still have to wait for the\nchannels to be established before I'm able to receive payment.  Bob could\nopen and push a payment to me but this doesn't have to be a requirement.\nIt could be pushed to a routing node Bob is connected to.\n\nThis would also be helpful for non-technical node managers.  Rather than\ngoing through the process of finding out a URI and trying to manage their\nchannels constantly by checking for inbound liquidity, they could simply\ncreate an invoice.  If inbound liquidity is lacking, the problem would be\nautomatically solved for them.  This wouldn't be the most efficient way to\nobtain inbound liquidity, I think that would only really be more important\nfor more advanced LN users.  Especially because asking someone to open and\ncommit BTC to you is already a bit of a difficult relationship to negotiate\nas expected amount of usage will vary.\n\nLastly, routing nodes are financially incentivized to do open a channel\nwith me because they could charge a premium.  Thor's instant channel is an\nexample of this.\n\nOn Tue, Aug 13, 2019 at 3:59 AM Rusty Russell <rusty at rustcorp.com.au> wrote:\n\n> Ecurrencyhodler Blockchains <ecurrencyhodler at gmail.com> writes:\n> >    1. Bob wants to send me 100,000 sats.\n> >    2. My node just came online and has 0 inbound liquidity.\n> >    3. I create an invoice for 100,000 sats.  My LN node recognizes I\n> have 0\n> >    inbound liquidity so my wallet also embeds my URI in the invoice.\n> >    4. Bob\u2019s wallet sees an invoice + uri.  Maybe even tries to route.\n> When\n> >    it doesn\u2019t see anything, it auto opens a channel + pushes 100,000 sat\n> >    payment.\n> >    5. I now own and can spend 100,000 sats instantly.\n>\n> If you publish your node address, Bob can already get this from the\n> gossip network, or the DNS seed as a last resort (and I expect\n> implementations to start doing this: I did it manually to buy a\n> thelightningconference.com ticket recently, for example).\n>\n> So this proposal is mainly useful where you have no channels at all\n> (thus cannot advertize your node), or don't want to publish it\n> generally.  And in both those cases, Bob probably doesn't want a channel\n> with you because it wouldn't be useful for paying anyone else.\n>\n> Cheers,\n> Rusty.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20190813/339ee04d/attachment.html>"
            },
            {
                "author": "Hampus Sj\u00f6berg",
                "date": "2019-08-13T13:30:55",
                "message_text_only": "While I do agree that this is a problem that we needs to be addressed\nsomehow, I don't agree on your proposal because I don't think we should\nconnect two end-users this way. It won't work out in the long run because\nif you connect say mobile wallets this way, one mobile could be offline,\nwhich locks the funds for the other part.\n\nAnother approach could be that wallets start using the already existing\nfallback tag (`f`) on BOLT11 invoices, where you can embed a Bitcoin\naddress.\nThis way, the sender could send the funds on-chain should it fail to send\nover Lightning.\nThis however requires the sender to have off-chain funds available which is\nprobably not the case. What could be done here is a splice out or a\nsubmarine swap, but they are not well established yet unfortunately.\n\nAnother way is to set up a \"temporary\" custodian channel if the receiver\ndoesn't have enough inbound capacity.\nHow it would work is that you have a third party custodian (i.e the wallet\nprovider) receives the money on your behalf. The next time you want to send\nsomething, this channel takes top priority.\nThis way the on-boarding process is pretty much solved, if you are OK with\nsome trust.\n\nWhat do you think?\n\nCheers\nHampus\n\nDen m\u00e5n 12 aug. 2019 kl 05:43 skrev Ecurrencyhodler Blockchains <\necurrencyhodler at gmail.com>:\n\n> Hi. I'd like to propose a way for instant inbound liquidity to be\n> automated via invoices and would appreciate your feedback.  It's similar to\n> Thor's instant channel but this proposal would only be valuable if it\n> becomes a standard across all lightning implementations and wallets.  It\n> won't work if it's limited to just one Lightning wallet.\n>\n> *Proposal:* Automated Inbound Liquidity With Invoices\n>\n> *For Who:* Full Lightning Network nodes\n>\n> *Problem:* Waiting for inbound liquidity as channel establishes when you\n> first come online and want to receive a LN payment.\n>\n> *Solution: *Embedding the node uri of the invoice creator, along with\n> amount to be routed.\n>\n> *Scenario: *\n>\n>    1. Bob wants to send me 100,000 sats.\n>    2. My node just came online and has 0 inbound liquidity.\n>    3. I create an invoice for 100,000 sats.  My LN node recognizes I have\n>    0 inbound liquidity so my wallet also embeds my URI in the invoice.\n>    4. Bob\u2019s wallet sees an invoice + uri.  Maybe even tries to route.\n>    When it doesn\u2019t see anything, it auto opens a channel + pushes 100,000 sat\n>    payment.\n>    5. I now own and can spend 100,000 sats instantly.\n>\n> *Considerations:*\n>\n>    - This auto establishing of channels and pushing payments isn\u2019t for\n>    all LN nodes.  Just routing nodes.\n>    - Bob doesn\u2019t need to be the one to establish the channel.  He can\n>    push the information down the line until a node dedicated to routing is\n>    found.  The routing node can then be the one to establish the channel with\n>    me.\n>    - Certain specifics need to be flushed out such as the size of Bob\u2019s\n>    channel.  Right now I think Bob can manually set the size of the channels\n>    to be established on his end.  Should be smaller channels at first.  If the\n>    person gets paid again, just establish another channel towards the same\n>    node if there isn't enough capacity.\n>    - Routing nodes who provide this service can charge a premium.\n>    - Bob, as a liquidity provider, won't cheat against himself so I can\n>    make LN payments instantly.\n>    - The beauty behind this proposal is that I can receive a payment\n>    instantly, I can send payments instantly, and that it hides everything from\n>    me as an end user.\n>    - Can possibly be extended to neutrino LN wallets if they are public.\n>\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20190813/7f534589/attachment.html>"
            },
            {
                "author": "ecurrencyhodler",
                "date": "2019-08-13T22:58:34",
                "message_text_only": "Hi Hampus!\n\n>It won't work out in the long run because if you connect say mobile\nwallets this way, one mobile could be offline, which locks the funds for\nthe other part.\n\nHmm I didn't consider mobile wallets being offline for a long period of\ntime. That's a good point.  But if smaller channels are preferred and they\nare charging a premium, I wonder if the opportunity cost here would be\nworth it.  It's also possible to set shorter HTLC's for unilateral closures\nfor these specific channels.\n\n>Another approach could be that wallets start using the already existing\nfallback tag (`f`) on BOLT11 invoices, where you can embed a Bitcoin\naddress.\n\nYou know I actually really like this feature of LN invoices.  It's very\npractical and a great stop gap.  My only gripe is that it keeps the user\noff the LN and they still would have to wait confirmations in order for\ntheir BTC to be \"confirmed\".  Automating inbound liquidity with push\npayments would make it instant as well as keep users on the LN.\n\n>Another way is to set up a \"temporary\" custodian channel if the receiver\ndoesn't have enough inbound capacity.\nHow it would work is that you have a third party custodian (i.e the wallet\nprovider) receives the money on your behalf. The next time you want to send\nsomething, this channel takes top priority.\n\nYea.  This is a great suggestion.  And probably where things will end up\nfor mobile neutrino Ln wallets in the near future.  But the benefits to\nautomating inbound liquidity with invoices is that it would be\nnon-custodial while offering almost the exact same experience.\n\nOn Tue, Aug 13, 2019 at 6:31 AM Hampus Sj\u00f6berg <hampus.sjoberg at gmail.com>\nwrote:\n\n> While I do agree that this is a problem that we needs to be addressed\n> somehow, I don't agree on your proposal because I don't think we should\n> connect two end-users this way. It won't work out in the long run because\n> if you connect say mobile wallets this way, one mobile could be offline,\n> which locks the funds for the other part.\n>\n> Another approach could be that wallets start using the already existing\n> fallback tag (`f`) on BOLT11 invoices, where you can embed a Bitcoin\n> address.\n> This way, the sender could send the funds on-chain should it fail to send\n> over Lightning.\n> This however requires the sender to have off-chain funds available which\n> is probably not the case. What could be done here is a splice out or a\n> submarine swap, but they are not well established yet unfortunately.\n>\n> Another way is to set up a \"temporary\" custodian channel if the receiver\n> doesn't have enough inbound capacity.\n> How it would work is that you have a third party custodian (i.e the wallet\n> provider) receives the money on your behalf. The next time you want to send\n> something, this channel takes top priority.\n> This way the on-boarding process is pretty much solved, if you are OK with\n> some trust.\n>\n> What do you think?\n>\n> Cheers\n> Hampus\n>\n> Den m\u00e5n 12 aug. 2019 kl 05:43 skrev Ecurrencyhodler Blockchains <\n> ecurrencyhodler at gmail.com>:\n>\n>> Hi. I'd like to propose a way for instant inbound liquidity to be\n>> automated via invoices and would appreciate your feedback.  It's similar to\n>> Thor's instant channel but this proposal would only be valuable if it\n>> becomes a standard across all lightning implementations and wallets.  It\n>> won't work if it's limited to just one Lightning wallet.\n>>\n>> *Proposal:* Automated Inbound Liquidity With Invoices\n>>\n>> *For Who:* Full Lightning Network nodes\n>>\n>> *Problem:* Waiting for inbound liquidity as channel establishes when you\n>> first come online and want to receive a LN payment.\n>>\n>> *Solution: *Embedding the node uri of the invoice creator, along with\n>> amount to be routed.\n>>\n>> *Scenario: *\n>>\n>>    1. Bob wants to send me 100,000 sats.\n>>    2. My node just came online and has 0 inbound liquidity.\n>>    3. I create an invoice for 100,000 sats.  My LN node recognizes I\n>>    have 0 inbound liquidity so my wallet also embeds my URI in the invoice.\n>>    4. Bob\u2019s wallet sees an invoice + uri.  Maybe even tries to route.\n>>    When it doesn\u2019t see anything, it auto opens a channel + pushes 100,000 sat\n>>    payment.\n>>    5. I now own and can spend 100,000 sats instantly.\n>>\n>> *Considerations:*\n>>\n>>    - This auto establishing of channels and pushing payments isn\u2019t for\n>>    all LN nodes.  Just routing nodes.\n>>    - Bob doesn\u2019t need to be the one to establish the channel.  He can\n>>    push the information down the line until a node dedicated to routing is\n>>    found.  The routing node can then be the one to establish the channel with\n>>    me.\n>>    - Certain specifics need to be flushed out such as the size of Bob\u2019s\n>>    channel.  Right now I think Bob can manually set the size of the channels\n>>    to be established on his end.  Should be smaller channels at first.  If the\n>>    person gets paid again, just establish another channel towards the same\n>>    node if there isn't enough capacity.\n>>    - Routing nodes who provide this service can charge a premium.\n>>    - Bob, as a liquidity provider, won't cheat against himself so I can\n>>    make LN payments instantly.\n>>    - The beauty behind this proposal is that I can receive a payment\n>>    instantly, I can send payments instantly, and that it hides everything from\n>>    me as an end user.\n>>    - Can possibly be extended to neutrino LN wallets if they are public.\n>>\n>>\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20190813/5a9a3fbd/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-08-14T01:27:35",
                "message_text_only": "Good morning Ecurrencyhodler,\n\nA current and practical way to set up incoming liquidity would be to take some onchain funds, create a channel to a high-uptime node on the network (just run an autopilot), then use a submarine swap (i.e. pay offchain funds to buy onchain funds).\nThen you can reuse the same onchain funds over and over to make more liquidity until the submarine swap provider runs out of onchain funds or you have sufficient liquidity or your money has been drained by the fees involved.\n\nWhile this requires onchain funds, presumably as a new business or merchant you will have capital in some form before starting your business.\nThe most sensible way to store and transport financial capital is, of course, Bitcoin, thus you already have what is needed to start this, you simply have to do it before you perform other operations.\nFurther, while it involves fees, it does give you control over what nodes you make channels with, and would be a good investment in your future accessibility over the Lightning Network.\n\nRegards,\nZmnSCPxj\n\n\nSent with ProtonMail Secure Email.\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\nOn Monday, August 12, 2019 11:42 AM, Ecurrencyhodler Blockchains <ecurrencyhodler at gmail.com> wrote:\n\n> Hi.\u00a0I'd like to propose a way for instant inbound liquidity to be automated via invoices and would appreciate your feedback.\u00a0 It's similar to Thor's instant channel but this proposal would only be valuable if it becomes a standard across all lightning implementations and wallets.\u00a0 It won't work if it's limited to just one Lightning wallet.\n>\n> Proposal: Automated Inbound Liquidity With Invoices\n>\n> For Who: Full Lightning Network nodes\n>\n> Problem: Waiting for inbound liquidity as channel establishes when you first come online and want to receive a LN payment.\n>\n> Solution:\u00a0Embedding the node uri of the invoice creator, along with amount to be routed.\n>\n> Scenario:\u00a0\n>\n> 1.  Bob wants to send me 100,000 sats.\n> 2.  My node just came online and has 0 inbound liquidity.\n> 3.  I create an invoice for 100,000 sats.\u00a0 My LN node recognizes I have 0 inbound liquidity so my wallet also embeds my URI in the invoice.\n> 4.  Bob\u2019s wallet sees an invoice + uri.\u00a0 Maybe even tries to route.\u00a0 When it doesn\u2019t see anything, it auto opens a channel + pushes 100,000 sat payment.\n> 5.  I now own and can spend 100,000 sats instantly.\n>\n> Considerations:\n>\n> -   This auto establishing of channels and pushing payments isn\u2019t for all LN nodes.\u00a0 Just routing nodes.\n> -   Bob doesn\u2019t need to be the one to establish the channel.\u00a0 He can push the information down the line until a node dedicated to routing is found.\u00a0 The routing node can then be the one to establish the channel with me.\n> -   Certain specifics need to be flushed out such as the size of Bob\u2019s channel.\u00a0 Right now I think Bob can manually set the size of the channels to be established on his end.\u00a0 Should be smaller channels at first.\u00a0 If the person gets paid again, just establish another channel towards the same node if there isn't enough capacity.\n> -   Routing nodes who provide this service can charge a premium.\n> -   Bob, as a liquidity provider, won't cheat against himself so I can make LN payments instantly.\n> -   The beauty behind this proposal is that I can receive a payment instantly, I can send payments instantly, and that it hides everything from me as an end user.\n> -   Can possibly be extended to neutrino LN wallets if they are public."
            },
            {
                "author": "ecurrencyhodler",
                "date": "2019-08-14T05:14:25",
                "message_text_only": "Hi ZmnSCPxj!\n\nSubmarine swaps are a great current solution, but we still have to wait for\nconfirmations.\n\n>Further, while it involves fees, it does give you control over what nodes\nyou make channels with, and would be a good investment in your future\naccessibility over the Lightning Network.\n\nWhat disadvantages do you see over this proposal and say something like\nautopilot?  Or do you just prefer manual channel management overall?\n\nOn Tue, Aug 13, 2019 at 6:27 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Ecurrencyhodler,\n>\n> A current and practical way to set up incoming liquidity would be to take\n> some onchain funds, create a channel to a high-uptime node on the network\n> (just run an autopilot), then use a submarine swap (i.e. pay offchain funds\n> to buy onchain funds).\n> Then you can reuse the same onchain funds over and over to make more\n> liquidity until the submarine swap provider runs out of onchain funds or\n> you have sufficient liquidity or your money has been drained by the fees\n> involved.\n>\n> While this requires onchain funds, presumably as a new business or\n> merchant you will have capital in some form before starting your business.\n> The most sensible way to store and transport financial capital is, of\n> course, Bitcoin, thus you already have what is needed to start this, you\n> simply have to do it before you perform other operations.\n> Further, while it involves fees, it does give you control over what nodes\n> you make channels with, and would be a good investment in your future\n> accessibility over the Lightning Network.\n>\n> Regards,\n> ZmnSCPxj\n>\n>\n> Sent with ProtonMail Secure Email.\n>\n> \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n> On Monday, August 12, 2019 11:42 AM, Ecurrencyhodler Blockchains <\n> ecurrencyhodler at gmail.com> wrote:\n>\n> > Hi. I'd like to propose a way for instant inbound liquidity to be\n> automated via invoices and would appreciate your feedback.  It's similar to\n> Thor's instant channel but this proposal would only be valuable if it\n> becomes a standard across all lightning implementations and wallets.  It\n> won't work if it's limited to just one Lightning wallet.\n> >\n> > Proposal: Automated Inbound Liquidity With Invoices\n> >\n> > For Who: Full Lightning Network nodes\n> >\n> > Problem: Waiting for inbound liquidity as channel establishes when you\n> first come online and want to receive a LN payment.\n> >\n> > Solution: Embedding the node uri of the invoice creator, along with\n> amount to be routed.\n> >\n> > Scenario:\n> >\n> > 1.  Bob wants to send me 100,000 sats.\n> > 2.  My node just came online and has 0 inbound liquidity.\n> > 3.  I create an invoice for 100,000 sats.  My LN node recognizes I have\n> 0 inbound liquidity so my wallet also embeds my URI in the invoice.\n> > 4.  Bob\u2019s wallet sees an invoice + uri.  Maybe even tries to route.\n> When it doesn\u2019t see anything, it auto opens a channel + pushes 100,000 sat\n> payment.\n> > 5.  I now own and can spend 100,000 sats instantly.\n> >\n> > Considerations:\n> >\n> > -   This auto establishing of channels and pushing payments isn\u2019t for\n> all LN nodes.  Just routing nodes.\n> > -   Bob doesn\u2019t need to be the one to establish the channel.  He can\n> push the information down the line until a node dedicated to routing is\n> found.  The routing node can then be the one to establish the channel with\n> me.\n> > -   Certain specifics need to be flushed out such as the size of Bob\u2019s\n> channel.  Right now I think Bob can manually set the size of the channels\n> to be established on his end.  Should be smaller channels at first.  If the\n> person gets paid again, just establish another channel towards the same\n> node if there isn't enough capacity.\n> > -   Routing nodes who provide this service can charge a premium.\n> > -   Bob, as a liquidity provider, won't cheat against himself so I can\n> make LN payments instantly.\n> > -   The beauty behind this proposal is that I can receive a payment\n> instantly, I can send payments instantly, and that it hides everything from\n> me as an end user.\n> > -   Can possibly be extended to neutrino LN wallets if they are public.\n>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20190813/390ffc4c/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-08-14T07:03:50",
                "message_text_only": "Good morning Ecurrencyhodler,\n\n> Hi ZmnSCPxj!\u00a0\n>\n> Submarine swaps are a great current solution, but we still have to wait for confirmations.\n\nSo would `push_msat`; until confirmed deeply the channel opening can still be cancelled by double-spending and it would be foolhardy to deliver the product until the channel is deeply confirmed to be opened.\nAt least this way, you can perform the preparation in parallel to your other startup operations for starting your business before actual launch of your merchant website.\n\n>\n> >Further, while it involves fees, it does give you control over what nodes you make channels with, and would be a good investment in your future accessibility over the Lightning Network.\n>\n> What disadvantages do you see over this proposal and say something like autopilot?\u00a0 Or do you just prefer manual channel management overall?\n\nThis should eventually be implementable by some kind of auto-system.\nIt is still early days and a lot of infrastructure is yet to be written.\n\nRegards,\nZmnSCPxj\n\n>\n> On Tue, Aug 13, 2019 at 6:27 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n>\n> > Good morning Ecurrencyhodler,\n> >\n> > A current and practical way to set up incoming liquidity would be to take some onchain funds, create a channel to a high-uptime node on the network (just run an autopilot), then use a submarine swap (i.e. pay offchain funds to buy onchain funds).\n> > Then you can reuse the same onchain funds over and over to make more liquidity until the submarine swap provider runs out of onchain funds or you have sufficient liquidity or your money has been drained by the fees involved.\n> >\n> > While this requires onchain funds, presumably as a new business or merchant you will have capital in some form before starting your business.\n> > The most sensible way to store and transport financial capital is, of course, Bitcoin, thus you already have what is needed to start this, you simply have to do it before you perform other operations.\n> > Further, while it involves fees, it does give you control over what nodes you make channels with, and would be a good investment in your future accessibility over the Lightning Network.\n> >\n> > Regards,\n> > ZmnSCPxj\n> >\n> > Sent with ProtonMail Secure Email.\n> >\n> > \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n> > On Monday, August 12, 2019 11:42 AM, Ecurrencyhodler Blockchains <ecurrencyhodler at gmail.com> wrote:\n> >\n> > > Hi.\u00a0I'd like to propose a way for instant inbound liquidity to be automated via invoices and would appreciate your feedback.\u00a0 It's similar to Thor's instant channel but this proposal would only be valuable if it becomes a standard across all lightning implementations and wallets.\u00a0 It won't work if it's limited to just one Lightning wallet.\n> > >\n> > > Proposal: Automated Inbound Liquidity With Invoices\n> > >\n> > > For Who: Full Lightning Network nodes\n> > >\n> > > Problem: Waiting for inbound liquidity as channel establishes when you first come online and want to receive a LN payment.\n> > >\n> > > Solution:\u00a0Embedding the node uri of the invoice creator, along with amount to be routed.\n> > >\n> > > Scenario:\u00a0\n> > >\n> > > 1.\u00a0 Bob wants to send me 100,000 sats.\n> > > 2.\u00a0 My node just came online and has 0 inbound liquidity.\n> > > 3.\u00a0 I create an invoice for 100,000 sats.\u00a0 My LN node recognizes I have 0 inbound liquidity so my wallet also embeds my URI in the invoice.\n> > > 4.\u00a0 Bob\u2019s wallet sees an invoice + uri.\u00a0 Maybe even tries to route.\u00a0 When it doesn\u2019t see anything, it auto opens a channel + pushes 100,000 sat payment.\n> > > 5.\u00a0 I now own and can spend 100,000 sats instantly.\n> > >\n> > > Considerations:\n> > >\n> > > -\u00a0 \u00a0This auto establishing of channels and pushing payments isn\u2019t for all LN nodes.\u00a0 Just routing nodes.\n> > > -\u00a0 \u00a0Bob doesn\u2019t need to be the one to establish the channel.\u00a0 He can push the information down the line until a node dedicated to routing is found.\u00a0 The routing node can then be the one to establish the channel with me.\n> > > -\u00a0 \u00a0Certain specifics need to be flushed out such as the size of Bob\u2019s channel.\u00a0 Right now I think Bob can manually set the size of the channels to be established on his end.\u00a0 Should be smaller channels at first.\u00a0 If the person gets paid again, just establish another channel towards the same node if there isn't enough capacity.\n> > > -\u00a0 \u00a0Routing nodes who provide this service can charge a premium.\n> > > -\u00a0 \u00a0Bob, as a liquidity provider, won't cheat against himself so I can make LN payments instantly.\n> > > -\u00a0 \u00a0The beauty behind this proposal is that I can receive a payment instantly, I can send payments instantly, and that it hides everything from me as an end user.\n> > > -\u00a0 \u00a0Can possibly be extended to neutrino LN wallets if they are public."
            },
            {
                "author": "ecurrencyhodler",
                "date": "2019-08-14T18:05:26",
                "message_text_only": ">So would `push_msat`; until confirmed deeply the channel opening can still\nbe cancelled by double-spending and it would be foolhardy to deliver the\nproduct until the channel is deeply confirmed to be opened.\n\nOkay so there's 2 situations here I'd like to explore:\n\n1. Bob -> routing node -> Me\n\n2. Bob -> Me\n\n*Scenario 1*\nIf Bob pays the invoice and the routing node opens a payment channel and\npushes sats to me, you could stipulate that the routing node isn't able to\nfully take ownership of the sats until 6 confirmations potentially via Hodl\nInvoices (by the time the routing nodes channel with pushed payments\nconfirms with mine).  But I could still make LN payments instantly through\nthe routing node because the routing node just needs to wait until the 6\nconfirmations and settle all accounts after the fact.\n\n*Scenario 2*\nBob and I know each other so if channel disappears, it's basically the same\nsituation with Thor's instant channel.  But we could completely remove\nscenario 2 and only allow routing nodes to open channels to me as long as\nBob makes the payment.\n\n\nOn Wed, Aug 14, 2019 at 12:03 AM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Ecurrencyhodler,\n>\n> > Hi ZmnSCPxj!\n> >\n> > Submarine swaps are a great current solution, but we still have to wait\n> for confirmations.\n>\n> So would `push_msat`; until confirmed deeply the channel opening can still\n> be cancelled by double-spending and it would be foolhardy to deliver the\n> product until the channel is deeply confirmed to be opened.\n> At least this way, you can perform the preparation in parallel to your\n> other startup operations for starting your business before actual launch of\n> your merchant website.\n>\n> >\n> > >Further, while it involves fees, it does give you control over what\n> nodes you make channels with, and would be a good investment in your future\n> accessibility over the Lightning Network.\n> >\n> > What disadvantages do you see over this proposal and say something like\n> autopilot?  Or do you just prefer manual channel management overall?\n>\n> This should eventually be implementable by some kind of auto-system.\n> It is still early days and a lot of infrastructure is yet to be written.\n>\n> Regards,\n> ZmnSCPxj\n>\n> >\n> > On Tue, Aug 13, 2019 at 6:27 PM ZmnSCPxj <ZmnSCPxj at protonmail.com>\n> wrote:\n> >\n> > > Good morning Ecurrencyhodler,\n> > >\n> > > A current and practical way to set up incoming liquidity would be to\n> take some onchain funds, create a channel to a high-uptime node on the\n> network (just run an autopilot), then use a submarine swap (i.e. pay\n> offchain funds to buy onchain funds).\n> > > Then you can reuse the same onchain funds over and over to make more\n> liquidity until the submarine swap provider runs out of onchain funds or\n> you have sufficient liquidity or your money has been drained by the fees\n> involved.\n> > >\n> > > While this requires onchain funds, presumably as a new business or\n> merchant you will have capital in some form before starting your business.\n> > > The most sensible way to store and transport financial capital is, of\n> course, Bitcoin, thus you already have what is needed to start this, you\n> simply have to do it before you perform other operations.\n> > > Further, while it involves fees, it does give you control over what\n> nodes you make channels with, and would be a good investment in your future\n> accessibility over the Lightning Network.\n> > >\n> > > Regards,\n> > > ZmnSCPxj\n> > >\n> > > Sent with ProtonMail Secure Email.\n> > >\n> > > \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n> > > On Monday, August 12, 2019 11:42 AM, Ecurrencyhodler Blockchains <\n> ecurrencyhodler at gmail.com> wrote:\n> > >\n> > > > Hi. I'd like to propose a way for instant inbound liquidity to be\n> automated via invoices and would appreciate your feedback.  It's similar to\n> Thor's instant channel but this proposal would only be valuable if it\n> becomes a standard across all lightning implementations and wallets.  It\n> won't work if it's limited to just one Lightning wallet.\n> > > >\n> > > > Proposal: Automated Inbound Liquidity With Invoices\n> > > >\n> > > > For Who: Full Lightning Network nodes\n> > > >\n> > > > Problem: Waiting for inbound liquidity as channel establishes when\n> you first come online and want to receive a LN payment.\n> > > >\n> > > > Solution: Embedding the node uri of the invoice creator, along with\n> amount to be routed.\n> > > >\n> > > > Scenario:\n> > > >\n> > > > 1.  Bob wants to send me 100,000 sats.\n> > > > 2.  My node just came online and has 0 inbound liquidity.\n> > > > 3.  I create an invoice for 100,000 sats.  My LN node recognizes I\n> have 0 inbound liquidity so my wallet also embeds my URI in the invoice.\n> > > > 4.  Bob\u2019s wallet sees an invoice + uri.  Maybe even tries to route.\n> When it doesn\u2019t see anything, it auto opens a channel + pushes 100,000 sat\n> payment.\n> > > > 5.  I now own and can spend 100,000 sats instantly.\n> > > >\n> > > > Considerations:\n> > > >\n> > > > -   This auto establishing of channels and pushing payments isn\u2019t\n> for all LN nodes.  Just routing nodes.\n> > > > -   Bob doesn\u2019t need to be the one to establish the channel.  He can\n> push the information down the line until a node dedicated to routing is\n> found.  The routing node can then be the one to establish the channel with\n> me.\n> > > > -   Certain specifics need to be flushed out such as the size of\n> Bob\u2019s channel.  Right now I think Bob can manually set the size of the\n> channels to be established on his end.  Should be smaller channels at\n> first.  If the person gets paid again, just establish another channel\n> towards the same node if there isn't enough capacity.\n> > > > -   Routing nodes who provide this service can charge a premium.\n> > > > -   Bob, as a liquidity provider, won't cheat against himself so I\n> can make LN payments instantly.\n> > > > -   The beauty behind this proposal is that I can receive a payment\n> instantly, I can send payments instantly, and that it hides everything from\n> me as an end user.\n> > > > -   Can possibly be extended to neutrino LN wallets if they are\n> public.\n>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20190814/fff0674c/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-08-14T23:41:59",
                "message_text_only": "Good morning Ecurrencyhodler,\n\nIt seems to me a trusted model then.\nRegardless of who makes the channel (the payee cannot determine who the payer is anyway) the payee cannot trustlessly release the product until the channel is deeply confirmed, else your security is only 0-conf, not off-chain.\n\nFurther, `push_msat` has the drawback of not providing proof-of-payment, thus an intermediate hop node may be unable to claim funds.\n(I believe `push_msat` was a mistake: you should simply make a normal HTLC payment (that provides proof-of-payment) after the channel is deeply confirmed, and `push_msat`, if you read lightning-rfc, does have this warning that you cannot trust money you receive that way until the channel is deeply confirmed.)\n\nRegards,\nZmnSCPxj\n\nSent with ProtonMail Secure Email.\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\nOn Thursday, August 15, 2019 2:05 AM, ecurrencyhodler <ecurrencyhodler at gmail.com> wrote:\n\n> >So would `push_msat`; until confirmed deeply the channel opening can still be cancelled by double-spending and it would be foolhardy to deliver the product until the channel is deeply confirmed to be opened.\n>\n> Okay so there's 2 situations here I'd like to explore:\n>\n> 1. Bob -> routing node -> Me\n>\n> 2. Bob -> Me\n>\n> Scenario 1\n> If Bob pays the invoice and the routing node opens a payment channel and pushes sats to me, you could stipulate that the routing node isn't able to fully take ownership of the sats until 6 confirmations potentially via Hodl Invoices (by the time the routing nodes channel with pushed payments confirms with mine).\u00a0 But I could still make LN payments instantly through the routing node because the routing node just needs to wait until the 6 confirmations and settle all accounts after the fact.\u00a0\u00a0\n>\n> Scenario 2\n> Bob and I know each other so if channel disappears, it's basically the same situation with Thor's instant channel.\u00a0 But we could completely remove scenario 2 and only allow routing nodes to open channels to me as long as Bob makes the payment.\n>\n> On Wed, Aug 14, 2019 at 12:03 AM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n>\n> > Good morning Ecurrencyhodler,\n> >\n> > > Hi ZmnSCPxj!\u00a0\n> > >\n> > > Submarine swaps are a great current solution, but we still have to wait for confirmations.\n> >\n> > So would `push_msat`; until confirmed deeply the channel opening can still be cancelled by double-spending and it would be foolhardy to deliver the product until the channel is deeply confirmed to be opened.\n> > At least this way, you can perform the preparation in parallel to your other startup operations for starting your business before actual launch of your merchant website.\n> >\n> > >\n> > > >Further, while it involves fees, it does give you control over what nodes you make channels with, and would be a good investment in your future accessibility over the Lightning Network.\n> > >\n> > > What disadvantages do you see over this proposal and say something like autopilot?\u00a0 Or do you just prefer manual channel management overall?\n> >\n> > This should eventually be implementable by some kind of auto-system.\n> > It is still early days and a lot of infrastructure is yet to be written.\n> >\n> > Regards,\n> > ZmnSCPxj\n> >\n> > >\n> > > On Tue, Aug 13, 2019 at 6:27 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n> > >\n> > > > Good morning Ecurrencyhodler,\n> > > >\n> > > > A current and practical way to set up incoming liquidity would be to take some onchain funds, create a channel to a high-uptime node on the network (just run an autopilot), then use a submarine swap (i.e. pay offchain funds to buy onchain funds).\n> > > > Then you can reuse the same onchain funds over and over to make more liquidity until the submarine swap provider runs out of onchain funds or you have sufficient liquidity or your money has been drained by the fees involved.\n> > > >\n> > > > While this requires onchain funds, presumably as a new business or merchant you will have capital in some form before starting your business.\n> > > > The most sensible way to store and transport financial capital is, of course, Bitcoin, thus you already have what is needed to start this, you simply have to do it before you perform other operations.\n> > > > Further, while it involves fees, it does give you control over what nodes you make channels with, and would be a good investment in your future accessibility over the Lightning Network.\n> > > >\n> > > > Regards,\n> > > > ZmnSCPxj\n> > > >\n> > > > Sent with ProtonMail Secure Email.\n> > > >\n> > > > \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n> > > > On Monday, August 12, 2019 11:42 AM, Ecurrencyhodler Blockchains <ecurrencyhodler at gmail.com> wrote:\n> > > >\n> > > > > Hi.\u00a0I'd like to propose a way for instant inbound liquidity to be automated via invoices and would appreciate your feedback.\u00a0 It's similar to Thor's instant channel but this proposal would only be valuable if it becomes a standard across all lightning implementations and wallets.\u00a0 It won't work if it's limited to just one Lightning wallet.\n> > > > >\n> > > > > Proposal: Automated Inbound Liquidity With Invoices\n> > > > >\n> > > > > For Who: Full Lightning Network nodes\n> > > > >\n> > > > > Problem: Waiting for inbound liquidity as channel establishes when you first come online and want to receive a LN payment.\n> > > > >\n> > > > > Solution:\u00a0Embedding the node uri of the invoice creator, along with amount to be routed.\n> > > > >\n> > > > > Scenario:\u00a0\n> > > > >\n> > > > > 1.\u00a0 Bob wants to send me 100,000 sats.\n> > > > > 2.\u00a0 My node just came online and has 0 inbound liquidity.\n> > > > > 3.\u00a0 I create an invoice for 100,000 sats.\u00a0 My LN node recognizes I have 0 inbound liquidity so my wallet also embeds my URI in the invoice.\n> > > > > 4.\u00a0 Bob\u2019s wallet sees an invoice + uri.\u00a0 Maybe even tries to route.\u00a0 When it doesn\u2019t see anything, it auto opens a channel + pushes 100,000 sat payment.\n> > > > > 5.\u00a0 I now own and can spend 100,000 sats instantly.\n> > > > >\n> > > > > Considerations:\n> > > > >\n> > > > > -\u00a0 \u00a0This auto establishing of channels and pushing payments isn\u2019t for all LN nodes.\u00a0 Just routing nodes.\n> > > > > -\u00a0 \u00a0Bob doesn\u2019t need to be the one to establish the channel.\u00a0 He can push the information down the line until a node dedicated to routing is found.\u00a0 The routing node can then be the one to establish the channel with me.\n> > > > > -\u00a0 \u00a0Certain specifics need to be flushed out such as the size of Bob\u2019s channel.\u00a0 Right now I think Bob can manually set the size of the channels to be established on his end.\u00a0 Should be smaller channels at first.\u00a0 If the person gets paid again, just establish another channel towards the same node if there isn't enough capacity.\n> > > > > -\u00a0 \u00a0Routing nodes who provide this service can charge a premium.\n> > > > > -\u00a0 \u00a0Bob, as a liquidity provider, won't cheat against himself so I can make LN payments instantly.\n> > > > > -\u00a0 \u00a0The beauty behind this proposal is that I can receive a payment instantly, I can send payments instantly, and that it hides everything from me as an end user.\n> > > > > -\u00a0 \u00a0Can possibly be extended to neutrino LN wallets if they are public."
            },
            {
                "author": "Dario Sneidermanis",
                "date": "2019-08-15T16:13:33",
                "message_text_only": "Hello Ecurrencyhodler,\n\nWe've been considering this flow for muun wallet, which has native\nsubmarine swap functionality, so it wouldn't be too difficult to implement.\nHowever, there's some problems with the idea:\n\n* As ZmnSCPxj notes, the push_msat functionality doesn't work for\nnon-custodial setups, like a submarine swap. It does work for Bitrefill's\nThor (ie. turbo channels) because you are trusting them to hold your money\nuntil the channel fully confirms. Per the RFC, \"push_msat is an amount of\ninitial funds that the sender is *unconditionally* giving to the receiver\".\n\n* You *can* use an HTLC payment once the channel is deep enough, but you'll\nhave to wait until the channel is locked to receive the money and be able\nto spend it. While this might be good enough for some use cases, like\ncharging your own node, it doesn't provide the seamless UX for lightning\npayments you're looking for.\n\nHaving said that, if the usability of the scheme \"open channel, wait until\nit's locked, then send HTLC payment\" were deemed good enough, then routing\nnodes could implement this idea to route payments \"just in time\", even if\nthere aren't any pre-existing routes to the destination.\n\nOn Wed, Aug 14, 2019 at 8:42 PM ZmnSCPxj via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> wrote:\n\n> Good morning Ecurrencyhodler,\n>\n> It seems to me a trusted model then.\n> Regardless of who makes the channel (the payee cannot determine who the\n> payer is anyway) the payee cannot trustlessly release the product until the\n> channel is deeply confirmed, else your security is only 0-conf, not\n> off-chain.\n>\n> Further, `push_msat` has the drawback of not providing proof-of-payment,\n> thus an intermediate hop node may be unable to claim funds.\n> (I believe `push_msat` was a mistake: you should simply make a normal HTLC\n> payment (that provides proof-of-payment) after the channel is deeply\n> confirmed, and `push_msat`, if you read lightning-rfc, does have this\n> warning that you cannot trust money you receive that way until the channel\n> is deeply confirmed.)\n>\n> Regards,\n> ZmnSCPxj\n>\n> Sent with ProtonMail Secure Email.\n>\n> \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n> On Thursday, August 15, 2019 2:05 AM, ecurrencyhodler <\n> ecurrencyhodler at gmail.com> wrote:\n>\n> > >So would `push_msat`; until confirmed deeply the channel opening can\n> still be cancelled by double-spending and it would be foolhardy to deliver\n> the product until the channel is deeply confirmed to be opened.\n> >\n> > Okay so there's 2 situations here I'd like to explore:\n> >\n> > 1. Bob -> routing node -> Me\n> >\n> > 2. Bob -> Me\n> >\n> > Scenario 1\n> > If Bob pays the invoice and the routing node opens a payment channel and\n> pushes sats to me, you could stipulate that the routing node isn't able to\n> fully take ownership of the sats until 6 confirmations potentially via Hodl\n> Invoices (by the time the routing nodes channel with pushed payments\n> confirms with mine).  But I could still make LN payments instantly through\n> the routing node because the routing node just needs to wait until the 6\n> confirmations and settle all accounts after the fact.\n> >\n> > Scenario 2\n> > Bob and I know each other so if channel disappears, it's basically the\n> same situation with Thor's instant channel.  But we could completely remove\n> scenario 2 and only allow routing nodes to open channels to me as long as\n> Bob makes the payment.\n> >\n> > On Wed, Aug 14, 2019 at 12:03 AM ZmnSCPxj <ZmnSCPxj at protonmail.com>\n> wrote:\n> >\n> > > Good morning Ecurrencyhodler,\n> > >\n> > > > Hi ZmnSCPxj!\n> > > >\n> > > > Submarine swaps are a great current solution, but we still have to\n> wait for confirmations.\n> > >\n> > > So would `push_msat`; until confirmed deeply the channel opening can\n> still be cancelled by double-spending and it would be foolhardy to deliver\n> the product until the channel is deeply confirmed to be opened.\n> > > At least this way, you can perform the preparation in parallel to your\n> other startup operations for starting your business before actual launch of\n> your merchant website.\n> > >\n> > > >\n> > > > >Further, while it involves fees, it does give you control over what\n> nodes you make channels with, and would be a good investment in your future\n> accessibility over the Lightning Network.\n> > > >\n> > > > What disadvantages do you see over this proposal and say something\n> like autopilot?  Or do you just prefer manual channel management overall?\n> > >\n> > > This should eventually be implementable by some kind of auto-system.\n> > > It is still early days and a lot of infrastructure is yet to be\n> written.\n> > >\n> > > Regards,\n> > > ZmnSCPxj\n> > >\n> > > >\n> > > > On Tue, Aug 13, 2019 at 6:27 PM ZmnSCPxj <ZmnSCPxj at protonmail.com>\n> wrote:\n> > > >\n> > > > > Good morning Ecurrencyhodler,\n> > > > >\n> > > > > A current and practical way to set up incoming liquidity would be\n> to take some onchain funds, create a channel to a high-uptime node on the\n> network (just run an autopilot), then use a submarine swap (i.e. pay\n> offchain funds to buy onchain funds).\n> > > > > Then you can reuse the same onchain funds over and over to make\n> more liquidity until the submarine swap provider runs out of onchain funds\n> or you have sufficient liquidity or your money has been drained by the fees\n> involved.\n> > > > >\n> > > > > While this requires onchain funds, presumably as a new business or\n> merchant you will have capital in some form before starting your business.\n> > > > > The most sensible way to store and transport financial capital is,\n> of course, Bitcoin, thus you already have what is needed to start this, you\n> simply have to do it before you perform other operations.\n> > > > > Further, while it involves fees, it does give you control over\n> what nodes you make channels with, and would be a good investment in your\n> future accessibility over the Lightning Network.\n> > > > >\n> > > > > Regards,\n> > > > > ZmnSCPxj\n> > > > >\n> > > > > Sent with ProtonMail Secure Email.\n> > > > >\n> > > > > \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n> > > > > On Monday, August 12, 2019 11:42 AM, Ecurrencyhodler Blockchains <\n> ecurrencyhodler at gmail.com> wrote:\n> > > > >\n> > > > > > Hi. I'd like to propose a way for instant inbound liquidity to\n> be automated via invoices and would appreciate your feedback.  It's similar\n> to Thor's instant channel but this proposal would only be valuable if it\n> becomes a standard across all lightning implementations and wallets.  It\n> won't work if it's limited to just one Lightning wallet.\n> > > > > >\n> > > > > > Proposal: Automated Inbound Liquidity With Invoices\n> > > > > >\n> > > > > > For Who: Full Lightning Network nodes\n> > > > > >\n> > > > > > Problem: Waiting for inbound liquidity as channel establishes\n> when you first come online and want to receive a LN payment.\n> > > > > >\n> > > > > > Solution: Embedding the node uri of the invoice creator, along\n> with amount to be routed.\n> > > > > >\n> > > > > > Scenario:\n> > > > > >\n> > > > > > 1.  Bob wants to send me 100,000 sats.\n> > > > > > 2.  My node just came online and has 0 inbound liquidity.\n> > > > > > 3.  I create an invoice for 100,000 sats.  My LN node recognizes\n> I have 0 inbound liquidity so my wallet also embeds my URI in the invoice.\n> > > > > > 4.  Bob\u2019s wallet sees an invoice + uri.  Maybe even tries to\n> route.  When it doesn\u2019t see anything, it auto opens a channel + pushes\n> 100,000 sat payment.\n> > > > > > 5.  I now own and can spend 100,000 sats instantly.\n> > > > > >\n> > > > > > Considerations:\n> > > > > >\n> > > > > > -   This auto establishing of channels and pushing payments\n> isn\u2019t for all LN nodes.  Just routing nodes.\n> > > > > > -   Bob doesn\u2019t need to be the one to establish the channel.  He\n> can push the information down the line until a node dedicated to routing is\n> found.  The routing node can then be the one to establish the channel with\n> me.\n> > > > > > -   Certain specifics need to be flushed out such as the size of\n> Bob\u2019s channel.  Right now I think Bob can manually set the size of the\n> channels to be established on his end.  Should be smaller channels at\n> first.  If the person gets paid again, just establish another channel\n> towards the same node if there isn't enough capacity.\n> > > > > > -   Routing nodes who provide this service can charge a premium.\n> > > > > > -   Bob, as a liquidity provider, won't cheat against himself so\n> I can make LN payments instantly.\n> > > > > > -   The beauty behind this proposal is that I can receive a\n> payment instantly, I can send payments instantly, and that it hides\n> everything from me as an end user.\n> > > > > > -   Can possibly be extended to neutrino LN wallets if they are\n> public.\n>\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20190815/df1e0a9d/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-08-16T03:58:25",
                "message_text_only": "Good morning Dario,\n\n> Having\u00a0said that, if the usability of the scheme \"open channel, wait until it's locked, then send HTLC payment\" were deemed good enough, then routing nodes could implement this idea to route payments \"just in time\", even if there aren't any pre-existing routes to the destination.\n\nThis is a good idea, but one with some difficulties in implementation.\n\n* The current onion route format contains the next short-channel-id (and in particular not the node-id of the next hop in the route).\n  Indeed, short-channel-ids were invented to reduce the size of the onion route format.\n  If a channel used to exist between nodes, then the payer might have gotten this short-channel-id in the past via gossip.\n  Then later, if the channel is closed, most implementations will forget the short-channel-id (and thus would probably also forget *which* node the short-channel-id used to be connected to, so determining the next node for the just-in-time channel opening would be difficult).\n  * C-Lightning retains this information for some blocks but will forget it at some point.\n  * Implementations that do this \"just-in-time\" channel-opening will need to remember this short-channel-id for longer.\n* The final HTLC going to the payee has the tightest time schedule.\n  If this HTLC has a timeout that is too near, the payee will reject the payment.\n  Since channel opening requires blocks to pass in order to confirm the funding transaction, by the time the HTLC reaches the payee, the timeout might now be judged too near and the payee will reject the payment anyway.\n  * The spec itself recommends the use of \"shadow routing\".\n    Briefly, the payer obscures who the payee is by adding a greater timeout to the payee than the minimum required by the payee.\n    (since timeouts decrease at each hop, an intermediate node can guess who the payee is by determining how small the remaining timeout looks.)\n    This can mitigate the above effect.\n    C-Lightning implements shadow routing.\n\nHowever, this same idea would be greatly helped by trampoline routing:\n\n* The planned trampoline routing indicates the node id of the next trampoline hop, thus not requiring implementations to remember who a closed short-channel-id used to be connected to.\n* Trampoline nodes will generally require a much larger fee and timelock budget, because they also have to build routes.\n  If the fee and timelock budgets are big enough, then the trampoline node might decide to open a direct channel to the next trampoline node \"just-in-time\" for the next trampoline hop.\n\nRegards,\nZmnSCPxj\n\n>\n> On Wed, Aug 14, 2019 at 8:42 PM ZmnSCPxj via Lightning-dev <lightning-dev at lists.linuxfoundation.org> wrote:\n>\n> > Good morning Ecurrencyhodler,\n> >\n> > It seems to me a trusted model then.\n> > Regardless of who makes the channel (the payee cannot determine who the payer is anyway) the payee cannot trustlessly release the product until the channel is deeply confirmed, else your security is only 0-conf, not off-chain.\n> >\n> > Further, `push_msat` has the drawback of not providing proof-of-payment, thus an intermediate hop node may be unable to claim funds.\n> > (I believe `push_msat` was a mistake: you should simply make a normal HTLC payment (that provides proof-of-payment) after the channel is deeply confirmed, and `push_msat`, if you read lightning-rfc, does have this warning that you cannot trust money you receive that way until the channel is deeply confirmed.)\n> >\n> > Regards,\n> > ZmnSCPxj\n> >\n> > Sent with ProtonMail Secure Email.\n> >\n> > \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n> > On Thursday, August 15, 2019 2:05 AM, ecurrencyhodler <ecurrencyhodler at gmail.com> wrote:\n> >\n> > > >So would `push_msat`; until confirmed deeply the channel opening can still be cancelled by double-spending and it would be foolhardy to deliver the product until the channel is deeply confirmed to be opened.\n> > >\n> > > Okay so there's 2 situations here I'd like to explore:\n> > >\n> > > 1. Bob -> routing node -> Me\n> > >\n> > > 2. Bob -> Me\n> > >\n> > > Scenario 1\n> > > If Bob pays the invoice and the routing node opens a payment channel and pushes sats to me, you could stipulate that the routing node isn't able to fully take ownership of the sats until 6 confirmations potentially via Hodl Invoices (by the time the routing nodes channel with pushed payments confirms with mine).\u00a0 But I could still make LN payments instantly through the routing node because the routing node just needs to wait until the 6 confirmations and settle all accounts after the fact.\u00a0\u00a0\n> > >\n> > > Scenario 2\n> > > Bob and I know each other so if channel disappears, it's basically the same situation with Thor's instant channel.\u00a0 But we could completely remove scenario 2 and only allow routing nodes to open channels to me as long as Bob makes the payment.\n> > >\n> > > On Wed, Aug 14, 2019 at 12:03 AM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n> > >\n> > > > Good morning Ecurrencyhodler,\n> > > >\n> > > > > Hi ZmnSCPxj!\u00a0\n> > > > >\n> > > > > Submarine swaps are a great current solution, but we still have to wait for confirmations.\n> > > >\n> > > > So would `push_msat`; until confirmed deeply the channel opening can still be cancelled by double-spending and it would be foolhardy to deliver the product until the channel is deeply confirmed to be opened.\n> > > > At least this way, you can perform the preparation in parallel to your other startup operations for starting your business before actual launch of your merchant website.\n> > > >\n> > > > >\n> > > > > >Further, while it involves fees, it does give you control over what nodes you make channels with, and would be a good investment in your future accessibility over the Lightning Network.\n> > > > >\n> > > > > What disadvantages do you see over this proposal and say something like autopilot?\u00a0 Or do you just prefer manual channel management overall?\n> > > >\n> > > > This should eventually be implementable by some kind of auto-system.\n> > > > It is still early days and a lot of infrastructure is yet to be written.\n> > > >\n> > > > Regards,\n> > > > ZmnSCPxj\n> > > >\n> > > > >\n> > > > > On Tue, Aug 13, 2019 at 6:27 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n> > > > >\n> > > > > > Good morning Ecurrencyhodler,\n> > > > > >\n> > > > > > A current and practical way to set up incoming liquidity would be to take some onchain funds, create a channel to a high-uptime node on the network (just run an autopilot), then use a submarine swap (i.e. pay offchain funds to buy onchain funds).\n> > > > > > Then you can reuse the same onchain funds over and over to make more liquidity until the submarine swap provider runs out of onchain funds or you have sufficient liquidity or your money has been drained by the fees involved.\n> > > > > >\n> > > > > > While this requires onchain funds, presumably as a new business or merchant you will have capital in some form before starting your business.\n> > > > > > The most sensible way to store and transport financial capital is, of course, Bitcoin, thus you already have what is needed to start this, you simply have to do it before you perform other operations.\n> > > > > > Further, while it involves fees, it does give you control over what nodes you make channels with, and would be a good investment in your future accessibility over the Lightning Network.\n> > > > > >\n> > > > > > Regards,\n> > > > > > ZmnSCPxj\n> > > > > >\n> > > > > > Sent with ProtonMail Secure Email.\n> > > > > >\n> > > > > > \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n> > > > > > On Monday, August 12, 2019 11:42 AM, Ecurrencyhodler Blockchains <ecurrencyhodler at gmail.com> wrote:\n> > > > > >\n> > > > > > > Hi.\u00a0I'd like to propose a way for instant inbound liquidity to be automated via invoices and would appreciate your feedback.\u00a0 It's similar to Thor's instant channel but this proposal would only be valuable if it becomes a standard across all lightning implementations and wallets.\u00a0 It won't work if it's limited to just one Lightning wallet.\n> > > > > > >\n> > > > > > > Proposal: Automated Inbound Liquidity With Invoices\n> > > > > > >\n> > > > > > > For Who: Full Lightning Network nodes\n> > > > > > >\n> > > > > > > Problem: Waiting for inbound liquidity as channel establishes when you first come online and want to receive a LN payment.\n> > > > > > >\n> > > > > > > Solution:\u00a0Embedding the node uri of the invoice creator, along with amount to be routed.\n> > > > > > >\n> > > > > > > Scenario:\u00a0\n> > > > > > >\n> > > > > > > 1.\u00a0 Bob wants to send me 100,000 sats.\n> > > > > > > 2.\u00a0 My node just came online and has 0 inbound liquidity.\n> > > > > > > 3.\u00a0 I create an invoice for 100,000 sats.\u00a0 My LN node recognizes I have 0 inbound liquidity so my wallet also embeds my URI in the invoice.\n> > > > > > > 4.\u00a0 Bob\u2019s wallet sees an invoice + uri.\u00a0 Maybe even tries to route.\u00a0 When it doesn\u2019t see anything, it auto opens a channel + pushes 100,000 sat payment.\n> > > > > > > 5.\u00a0 I now own and can spend 100,000 sats instantly.\n> > > > > > >\n> > > > > > > Considerations:\n> > > > > > >\n> > > > > > > -\u00a0 \u00a0This auto establishing of channels and pushing payments isn\u2019t for all LN nodes.\u00a0 Just routing nodes.\n> > > > > > > -\u00a0 \u00a0Bob doesn\u2019t need to be the one to establish the channel.\u00a0 He can push the information down the line until a node dedicated to routing is found.\u00a0 The routing node can then be the one to establish the channel with me.\n> > > > > > > -\u00a0 \u00a0Certain specifics need to be flushed out such as the size of Bob\u2019s channel.\u00a0 Right now I think Bob can manually set the size of the channels to be established on his end.\u00a0 Should be smaller channels at first.\u00a0 If the person gets paid again, just establish another channel towards the same node if there isn't enough capacity.\n> > > > > > > -\u00a0 \u00a0Routing nodes who provide this service can charge a premium.\n> > > > > > > -\u00a0 \u00a0Bob, as a liquidity provider, won't cheat against himself so I can make LN payments instantly.\n> > > > > > > -\u00a0 \u00a0The beauty behind this proposal is that I can receive a payment instantly, I can send payments instantly, and that it hides everything from me as an end user.\n> > > > > > > -\u00a0 \u00a0Can possibly be extended to neutrino LN wallets if they are public.\n> >\n> > _______________________________________________\n> > Lightning-dev mailing list\n> > Lightning-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            }
        ],
        "thread_summary": {
            "title": "Proposal: Automated Inbound Liquidity With Invoices",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "ecurrencyhodler",
                "Dario Sneidermanis",
                "Rusty Russell",
                "Ecurrencyhodler Blockchains",
                "Hampus Sj\u00f6berg",
                "ZmnSCPxj"
            ],
            "messages_count": 12,
            "total_messages_chars_count": 62260
        }
    },
    {
        "title": "[Lightning-dev] CVEs assigned for lightning projects: please upgrade!",
        "thread_messages": [
            {
                "author": "Rusty Russell",
                "date": "2019-08-30T09:32:48",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\n\nSecurity issues have been found in various lightning projects which\ncould cause loss of funds.\n\nFull details will be released in 4 weeks (2019-09-27), please uprade\nwell before then.\n\nEffected releases:\n\n    CVE-2019-12998 c-lightning < 0.7.1\n    CVE-2019-12999 lnd < 0.7\n    CVE-2019-13000 eclair <= 0.3\n\nCheers,\nRusty.\n-----BEGIN PGP SIGNATURE-----\n\niQIzBAEBCAAdFiEEFe6NbKsOfwz5mb/L2SAObNGtuPEFAl1o7UAACgkQ2SAObNGt\nuPFR7xAAqlcY/gCzfx5Sl49BwLIvr5EZlKYxasIoU4FoiAxLN0sRMksBLY+gUA3L\n7XuPi7oJSsnJc0Gvq6DnWo8W/jqAETgK0XeCyESdtX1tLeXMEiCoAXccRBT/hNbr\naHRiyeRO6YnrfzJN2CKStzXUvoVEvyB4lpMZ+dTJYdulOUs20ELU/zzSQe/syGnD\n7kujvBVyk4LJIYQ9piGl1pc4Y8mORK2ttYCVk4HCy+eu1RGHRVze135ve2MhQVOd\nMzs57lqXM8k+ZUumD5eB6pgvENlFzgFVaywYvf7+RSZIx185qosHTbQU84icyunp\nW68FhCk9DMUYlhU8lBVyX1qS1+YhBYvm79zK4lCSJ9CQBZ2Oox2tz9RuO/3DPSol\nRCZ3+h8SCKai8ZASXhz4dL4nXSpdKNjJrQdRvp7I1e2netkZpaF2Dyd7FDvFnhad\nSWP/juo/n9rmkyfbuxQYj5sdixV9G9cpV85BnQDX558r+AMRPVin/xs5NBZMknkN\nS7Wc9aq8nlVUeoTV5+TnGbz8NPXyYLNSotJdwBnA+RWTD9emCBah3UOxVlJR7N5e\nnZuumPauLJyZESzxvRDgQ0Hca7hMCMBh+xJ/OFDy+n4oHxFLihCtY3EktSE43v2N\n+PXbLFXw9w7jSPxn5FgqzB9D/E/eqkLe/+UKsnQ0ji8trEd36DU=\n=Z6RL\n-----END PGP SIGNATURE-----"
            }
        ],
        "thread_summary": {
            "title": "CVEs assigned for lightning projects: please upgrade!",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Rusty Russell"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1202
        }
    }
]