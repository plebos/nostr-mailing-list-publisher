[
    {
        "title": "[Lightning-dev] Inbound channel routing fees",
        "thread_messages": [
            {
                "author": "Joost Jager",
                "date": "2022-07-01T10:34:54",
                "message_text_only": "Currently routing nodes on the lightning network charge fees based on a\npolicy that pertains to the outgoing channel only.\n\nSeveral mentions have been made by routing node operators that this limits\nthe control that they can exert over the flow of traffic. The movement of\nfunds on all of the incoming channels is free of charge, which does not\nmatch the reality of not all inbound liquidity being equal.\n\nOne option to fix this is to add two additional fields to the\n`channel_update` message:\n* `inbound_fee_base_msat`\n* `inbound_fee_proportional_millionths`\n\nWith the previously introduced tlv message extensions, it should be\npossible to let these fields propagate throughout the network without any\nupgrades required.\n\nSenders must pay each routing node the sum of its advertised inbound and\noutbound fee for the channels used:\n\noutbound_fee(amt_to_fwd) + inbound_fee(amt_to_fwd +\noutbound_fee(amt_to_fwd))\n\nSo the inbound_fee is calculated based on the actual balance change in the\nincoming channel. This includes the amount to forward as well as the\noutbound fee.\n\nAn important characteristic of any solution that is to be deployed in an\nexisting network, is that it is backwards compatible. If routing nodes\nstart to require inbound fees, every sender that hasn\u2019t upgraded their node\nsoftware will no longer be able to use that routing node. The routing node\nwill miss out on routing fees.\n\nOne mitigation is to charge zero inbound fees until a sufficiently large\nportion of the senders has upgraded. It may be unclear though when this is\nthe case, and will likely take a significant amount of time. A test could\nbe to temporarily charge a minimal inbound fee, and watch for a reduction\nin traffic and increase in `fee_insufficient` failures returned. If there\nis little or no effect, then most senders have probably upgraded.\n\nAnother way to go about this is to set negative inbound fees during the\ntransitory phase. It is effectively a discount for using specific inbound\nchannels. So a routing node that charges 10 sats for forwarding today, may\nin the future increase that to 13 sats and set the inbound fee to -3 sats.\n\nSenders ignoring inbound fees will overpay (13 sats whereas 10 sats would\nhave been sufficient), but are still able to use the routing node. The\nrouting node may see a reduction in traffic though because it effectively\nincreased its fee for older senders only. But inbound fees could be\nincreased (decreased really because they are negative) gradually while\nmonitoring for fee over-payments. Over-payments are indicative of senders\nignoring the inbound fee discount.\n\nPath-finding algorithms that are currently in use generally don\u2019t support\nnegative fees. But in this case, the sum of inbound and outbound fees is\nstill positive and therefore not a problem. If routing nodes set their\npolicies accidentally or intentionally so that the sum of fees turns out\nnegative, senders can just round up to zero and find a path as normal.\n\nOverall I think this can be a relatively compact change that may ultimately\nlead to better capital placement on the network and lower routing fees.\n\nLooking for feedback on the idea from both lightning devs and routing node\noperators.\n\nJoost\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220701/c0768856/attachment-0001.html>"
            },
            {
                "author": "Joost Jager",
                "date": "2022-07-01T11:10:09",
                "message_text_only": ">\n> Path-finding algorithms that are currently in use generally don\u2019t support\n> negative fees. But in this case, the sum of inbound and outbound fees is\n> still positive and therefore not a problem. If routing nodes set their\n> policies accidentally or intentionally so that the sum of fees turns out\n> negative, senders can just round up to zero and find a path as normal.\n>\n\nCorrection to this:\n\nThe sum of inbound and outbound are not the fees set by one single routing\nnode. When path-finding considers a candidate hop, this adds the outbound\nfee of the \"from\" node and the inbound fee of the \"to\" node. Because those\nnodes don't necessarily coordinate fees, it may happen more often that the\nfee goes negative. Rounding up to zero is still a quick fix and better than\nignoring inbound fees completely.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220701/632a50ce/attachment.html>"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2022-07-01T11:19:45",
                "message_text_only": "Hi Joost,\n\nAs I've already stated every time this has been previously discussed, I\nbelieve\nthis doesn't make any sense. The funds that are on the other side of the\nchannel belong to your peer, not you, so they're free to use it however they\nwant. If you're not happy with the way your peer is managing their fees,\nthen\ndon't open channels to them and let the network decide whether you're right\nor\nnot.\n\nMoreover, you shouldn't care at all. If all the funds are on your peer's\nside,\nthis isn't your problem, you used up all the money that was yours. As long\nas\nthe channel is open, this is free inbound liquidity for you, so you're even\nbenefiting from this.\n\nIf Alice could set fees for Bob's side of the channel, Alice could\narbitrarily\nDoS Bob's payments by setting a high fee. This is just one example of the\nmany\nways this idea completely breaks the routing incentives.\n\nCheers,\nBastien\n\nLe ven. 1 juil. 2022 \u00e0 13:10, Joost Jager <joost.jager at gmail.com> a \u00e9crit :\n\n> Path-finding algorithms that are currently in use generally don\u2019t support\n>> negative fees. But in this case, the sum of inbound and outbound fees is\n>> still positive and therefore not a problem. If routing nodes set their\n>> policies accidentally or intentionally so that the sum of fees turns out\n>> negative, senders can just round up to zero and find a path as normal.\n>>\n>\n> Correction to this:\n>\n> The sum of inbound and outbound are not the fees set by one single routing\n> node. When path-finding considers a candidate hop, this adds the outbound\n> fee of the \"from\" node and the inbound fee of the \"to\" node. Because those\n> nodes don't necessarily coordinate fees, it may happen more often that the\n> fee goes negative. Rounding up to zero is still a quick fix and better than\n> ignoring inbound fees completely.\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220701/6b0313dc/attachment.html>"
            },
            {
                "author": "Joost Jager",
                "date": "2022-07-01T12:02:06",
                "message_text_only": "Hi Bastien,\n\nI vaguely remembered that the idea of inbound fees had been discussed\nbefore. Before writing my post, I scanned through old ML posts and bolts\nissues but couldn't find the discussion. Maybe it was part of a different\nbut related email or a bolts pr?\n\nWith regards to your objections, isn't it the case that it is always\npossible to DoS your peer by just rejecting any forward that comes in from\nthem? Or indirectly affecting them negatively by setting high fees on all\noutbound channels? To me it seems that there is nothing to lose by adding\ninbound fees.\n\nMy thinking is that if I accept an incoming htlc, my local balance\nincreases on that incoming channel. My money gets locked up in a channel\nthat may or may not be interesting to me. Wouldn't it be fair to be\ncompensated for that?\n\nAny thoughts from routing node operators would be welcome too (or links to\nprevious threads).\n\nJoost\n\nOn Fri, Jul 1, 2022 at 1:19 PM Bastien TEINTURIER <bastien at acinq.fr> wrote:\n\n> Hi Joost,\n>\n> As I've already stated every time this has been previously discussed, I\n> believe\n> this doesn't make any sense. The funds that are on the other side of the\n> channel belong to your peer, not you, so they're free to use it however\n> they\n> want. If you're not happy with the way your peer is managing their fees,\n> then\n> don't open channels to them and let the network decide whether you're\n> right or\n> not.\n>\n> Moreover, you shouldn't care at all. If all the funds are on your peer's\n> side,\n> this isn't your problem, you used up all the money that was yours. As long\n> as\n> the channel is open, this is free inbound liquidity for you, so you're even\n> benefiting from this.\n>\n> If Alice could set fees for Bob's side of the channel, Alice could\n> arbitrarily\n> DoS Bob's payments by setting a high fee. This is just one example of the\n> many\n> ways this idea completely breaks the routing incentives.\n>\n> Cheers,\n> Bastien\n>\n> Le ven. 1 juil. 2022 \u00e0 13:10, Joost Jager <joost.jager at gmail.com> a\n> \u00e9crit :\n>\n>> Path-finding algorithms that are currently in use generally don\u2019t support\n>>> negative fees. But in this case, the sum of inbound and outbound fees is\n>>> still positive and therefore not a problem. If routing nodes set their\n>>> policies accidentally or intentionally so that the sum of fees turns out\n>>> negative, senders can just round up to zero and find a path as normal.\n>>>\n>>\n>> Correction to this:\n>>\n>> The sum of inbound and outbound are not the fees set by one single\n>> routing node. When path-finding considers a candidate hop, this adds the\n>> outbound fee of the \"from\" node and the inbound fee of the \"to\" node.\n>> Because those nodes don't necessarily coordinate fees, it may happen more\n>> often that the fee goes negative. Rounding up to zero is still a quick fix\n>> and better than ignoring inbound fees completely.\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220701/5b33a7c3/attachment.html>"
            },
            {
                "author": "Thomas HUET",
                "date": "2022-07-01T12:17:07",
                "message_text_only": "Hi Joost,\n\nIt was discussed in this issue:\nhttps://github.com/lightning/bolts/issues/835\n\nOn the network, the traffic is not balanced. Some nodes tend to receive\nmore than they send, merchants for instance. For the lightning network to\nbe reliable, we need to incentivise people to open channels to such nodes,\nor else there won't be enough liquidity available and payments will fail.\nThe current fee structure provides this incentive: You pay some onchain\nfees and lock some funds and in exchange you will earn routing fees. My\nconcern is that your proposed change would break that incentive and make\nthe network less reliable.\n\nThomas\n\nLe ven. 1 juil. 2022 \u00e0 14:02, Joost Jager <joost.jager at gmail.com> a \u00e9crit :\n\n> Hi Bastien,\n>\n> I vaguely remembered that the idea of inbound fees had been discussed\n> before. Before writing my post, I scanned through old ML posts and bolts\n> issues but couldn't find the discussion. Maybe it was part of a different\n> but related email or a bolts pr?\n>\n> With regards to your objections, isn't it the case that it is always\n> possible to DoS your peer by just rejecting any forward that comes in from\n> them? Or indirectly affecting them negatively by setting high fees on all\n> outbound channels? To me it seems that there is nothing to lose by adding\n> inbound fees.\n>\n> My thinking is that if I accept an incoming htlc, my local balance\n> increases on that incoming channel. My money gets locked up in a channel\n> that may or may not be interesting to me. Wouldn't it be fair to be\n> compensated for that?\n>\n> Any thoughts from routing node operators would be welcome too (or links to\n> previous threads).\n>\n> Joost\n>\n> On Fri, Jul 1, 2022 at 1:19 PM Bastien TEINTURIER <bastien at acinq.fr>\n> wrote:\n>\n>> Hi Joost,\n>>\n>> As I've already stated every time this has been previously discussed, I\n>> believe\n>> this doesn't make any sense. The funds that are on the other side of the\n>> channel belong to your peer, not you, so they're free to use it however\n>> they\n>> want. If you're not happy with the way your peer is managing their fees,\n>> then\n>> don't open channels to them and let the network decide whether you're\n>> right or\n>> not.\n>>\n>> Moreover, you shouldn't care at all. If all the funds are on your peer's\n>> side,\n>> this isn't your problem, you used up all the money that was yours. As\n>> long as\n>> the channel is open, this is free inbound liquidity for you, so you're\n>> even\n>> benefiting from this.\n>>\n>> If Alice could set fees for Bob's side of the channel, Alice could\n>> arbitrarily\n>> DoS Bob's payments by setting a high fee. This is just one example of the\n>> many\n>> ways this idea completely breaks the routing incentives.\n>>\n>> Cheers,\n>> Bastien\n>>\n>> Le ven. 1 juil. 2022 \u00e0 13:10, Joost Jager <joost.jager at gmail.com> a\n>> \u00e9crit :\n>>\n>>> Path-finding algorithms that are currently in use generally don\u2019t\n>>>> support negative fees. But in this case, the sum of inbound and outbound\n>>>> fees is still positive and therefore not a problem. If routing nodes set\n>>>> their policies accidentally or intentionally so that the sum of fees turns\n>>>> out negative, senders can just round up to zero and find a path as normal.\n>>>>\n>>>\n>>> Correction to this:\n>>>\n>>> The sum of inbound and outbound are not the fees set by one single\n>>> routing node. When path-finding considers a candidate hop, this adds the\n>>> outbound fee of the \"from\" node and the inbound fee of the \"to\" node.\n>>> Because those nodes don't necessarily coordinate fees, it may happen more\n>>> often that the fee goes negative. Rounding up to zero is still a quick fix\n>>> and better than ignoring inbound fees completely.\n>>> _______________________________________________\n>>> Lightning-dev mailing list\n>>> Lightning-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>>\n>> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220701/3c0f507d/attachment.html>"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2022-07-01T13:02:40",
                "message_text_only": "Hi Joost,\n\n> isn't it the case that it is always possible to DoS your peer by just\nrejecting any forward that comes in from them?\n\nYes, this is a good point. But there is a difference though. If you do that\nwith inbound fees, the \"malicious\" peer is able to prevent _everyone_ from\neven trying to route through you (because it's advertised).\n\nWhereas if they selectively fail HTLCs you forward to them, only the payer\nfor\nthat HTLC knows about it, and they can attribute the failure to the\nmalicious\nnode, not to you.\n\nOf course, that malicious node could also withhold the HTLC or return a\nmalformed error, but unfortunately we cannot easily protect against this.\nMy point is that this is bad behavior, and we shouldn't be giving more\ntools for nodes to misbehave, and inbound fees are a very powerful tool\nto help misbehaving nodes.\n\n> Or indirectly affecting them negatively by setting high fees on all\noutbound channels?\n\nThis case is completely different, because the \"malicious\" node can't\nselectively\nadvertise that, it will affect traffic coming from all of their peers so\nthey\nwould really be shooting themselves in the foot if they did that.\n\n> My thinking is that if I accept an incoming htlc, my local balance\nincreases\n> on that incoming channel. My money gets locked up in a channel that may or\n> may not be interesting to me. Wouldn't it be fair to be compensated for\nthat?\n\nIf that channel isn't interesting to you, then by all means you should fail\nthat HTLC or close the channel? Or you shouldn't have accepted it in the\nfirst place?\n\nI understand the will to optimize revenue here, but I fear this concrete\nproposal leads to many kinds of unhealthy incentives. I agree that there is\na\nrisk in accepting channels from unknown nodes, but I think it should be\naddressed differently: you could for example make the opener pay a fee when\nthey open a channel to you to compensate that risk (some kind of reversed\nliquidity ads).\n\nCheers,\nBastien\n\nLe ven. 1 juil. 2022 \u00e0 14:17, Thomas HUET <thomas.huet at acinq.fr> a \u00e9crit :\n\n> Hi Joost,\n>\n> It was discussed in this issue:\n> https://github.com/lightning/bolts/issues/835\n>\n> On the network, the traffic is not balanced. Some nodes tend to receive\n> more than they send, merchants for instance. For the lightning network to\n> be reliable, we need to incentivise people to open channels to such nodes,\n> or else there won't be enough liquidity available and payments will fail.\n> The current fee structure provides this incentive: You pay some onchain\n> fees and lock some funds and in exchange you will earn routing fees. My\n> concern is that your proposed change would break that incentive and make\n> the network less reliable.\n>\n> Thomas\n>\n> Le ven. 1 juil. 2022 \u00e0 14:02, Joost Jager <joost.jager at gmail.com> a\n> \u00e9crit :\n>\n>> Hi Bastien,\n>>\n>> I vaguely remembered that the idea of inbound fees had been discussed\n>> before. Before writing my post, I scanned through old ML posts and bolts\n>> issues but couldn't find the discussion. Maybe it was part of a different\n>> but related email or a bolts pr?\n>>\n>> With regards to your objections, isn't it the case that it is always\n>> possible to DoS your peer by just rejecting any forward that comes in from\n>> them? Or indirectly affecting them negatively by setting high fees on all\n>> outbound channels? To me it seems that there is nothing to lose by adding\n>> inbound fees.\n>>\n>> My thinking is that if I accept an incoming htlc, my local balance\n>> increases on that incoming channel. My money gets locked up in a channel\n>> that may or may not be interesting to me. Wouldn't it be fair to be\n>> compensated for that?\n>>\n>> Any thoughts from routing node operators would be welcome too (or links\n>> to previous threads).\n>>\n>> Joost\n>>\n>> On Fri, Jul 1, 2022 at 1:19 PM Bastien TEINTURIER <bastien at acinq.fr>\n>> wrote:\n>>\n>>> Hi Joost,\n>>>\n>>> As I've already stated every time this has been previously discussed, I\n>>> believe\n>>> this doesn't make any sense. The funds that are on the other side of the\n>>> channel belong to your peer, not you, so they're free to use it however\n>>> they\n>>> want. If you're not happy with the way your peer is managing their fees,\n>>> then\n>>> don't open channels to them and let the network decide whether you're\n>>> right or\n>>> not.\n>>>\n>>> Moreover, you shouldn't care at all. If all the funds are on your peer's\n>>> side,\n>>> this isn't your problem, you used up all the money that was yours. As\n>>> long as\n>>> the channel is open, this is free inbound liquidity for you, so you're\n>>> even\n>>> benefiting from this.\n>>>\n>>> If Alice could set fees for Bob's side of the channel, Alice could\n>>> arbitrarily\n>>> DoS Bob's payments by setting a high fee. This is just one example of\n>>> the many\n>>> ways this idea completely breaks the routing incentives.\n>>>\n>>> Cheers,\n>>> Bastien\n>>>\n>>> Le ven. 1 juil. 2022 \u00e0 13:10, Joost Jager <joost.jager at gmail.com> a\n>>> \u00e9crit :\n>>>\n>>>> Path-finding algorithms that are currently in use generally don\u2019t\n>>>>> support negative fees. But in this case, the sum of inbound and outbound\n>>>>> fees is still positive and therefore not a problem. If routing nodes set\n>>>>> their policies accidentally or intentionally so that the sum of fees turns\n>>>>> out negative, senders can just round up to zero and find a path as normal.\n>>>>>\n>>>>\n>>>> Correction to this:\n>>>>\n>>>> The sum of inbound and outbound are not the fees set by one single\n>>>> routing node. When path-finding considers a candidate hop, this adds the\n>>>> outbound fee of the \"from\" node and the inbound fee of the \"to\" node.\n>>>> Because those nodes don't necessarily coordinate fees, it may happen more\n>>>> often that the fee goes negative. Rounding up to zero is still a quick fix\n>>>> and better than ignoring inbound fees completely.\n>>>> _______________________________________________\n>>>> Lightning-dev mailing list\n>>>> Lightning-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>>>\n>>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220701/c3c96202/attachment-0001.html>"
            },
            {
                "author": "Joost Jager",
                "date": "2022-07-04T08:14:29",
                "message_text_only": ">\n> > isn't it the case that it is always possible to DoS your peer by just\n> rejecting any forward that comes in from them?\n>\n> Yes, this is a good point. But there is a difference though. If you do that\n> with inbound fees, the \"malicious\" peer is able to prevent _everyone_ from\n> even trying to route through you (because it's advertised).\n>\n\nIf I understand it correctly, we're talking about nodes A and B, where B is\nmalicious and sets a high inbound fee on the A-B channel?\n\nI'd think that for the network, it's actually better if B advertises their\nhigh inbound fee and nobody even tries that route, instead of everyone\ntrying and having to wait for a failure because B drops packets?\n\n\n> Whereas if they selectively fail HTLCs you forward to them, only the payer\n> for\n> that HTLC knows about it, and they can attribute the failure to the\n> malicious\n> node, not to you.\n>\n\nIsn't the same true for a high inbound fee set by B? This would make it\nclear to everyone that B is the node that makes the A-B channel too\nexpensive to be useful?\n\n\n> Of course, that malicious node could also withhold the HTLC or return a\n> malformed error, but unfortunately we cannot easily protect against this.\n> My point is that this is bad behavior, and we shouldn't be giving more\n> tools for nodes to misbehave, and inbound fees are a very powerful tool\n> to help misbehaving nodes.\n>\n\nI fundamentally disagree with not giving nodes tools to misbehave. To me\nthis indicates that the system is fragile. I'd actually rather go the\nopposite way: give them tools and show that the system is unaffected.\n\nBut on the point of DoS'ing a particular node: I think there are so many\nways to do this already, that inbound fees probably won't be the tool of\nchoice even if it was available.\n\n\n> > Or indirectly affecting them negatively by setting high fees on all\n> outbound channels?\n>\n> This case is completely different, because the \"malicious\" node can't\n> selectively\n> advertise that, it will affect traffic coming from all of their peers so\n> they\n> would really be shooting themselves in the foot if they did that.\n>\n\nIt's different, but in my view not completely different. If a routing node\nall of a sudden decides to charge 10% outbound across all channels for\nwhatever reason, its peers will be affected because their capital will at\nthat point be misplaced for earning routing fees.\n\nIf you say 'shoot themselves in the foot', you seem to have a rational\nrouting node in mind looking to maximize fees? How does DoS'ing a\nparticular peer fit in that picture, why would they do this?\n\n\n> > My thinking is that if I accept an incoming htlc, my local balance\n> increases\n> > on that incoming channel. My money gets locked up in a channel that may\n> or\n> > may not be interesting to me. Wouldn't it be fair to be compensated for\n> that?\n>\n> If that channel isn't interesting to you, then by all means you should fail\n> that HTLC or close the channel? Or you shouldn't have accepted it in the\n> first place?\n>\n\nAgreed, if it isn't interesting at all, you should close. I should have put\nthat more nuanced. Some channels will likely be more interesting than\nothers and inbound fees could help with keeping the less interesting ones\nafloat. It's another option besides plainly closing the channel.\n\nSuppose I have three peers A, B and C. I am routing traffic back and forth\nbetween A and B at a low fee of 0.1%.\n\nThen C comes along and opens a 1 BTC channel with me. They push out the\nfull balance towards B and pay 0.1% for that. After that, there is very\nminimal activity and after a month I decide to close the channel. A big\nopportunity cost for me because I could have placed that 1 BTC local\nbalance in a much better way. With an inbound fee, I could have earned more.\n\n\n> I understand the will to optimize revenue here, but I fear this concrete\n> proposal leads to many kinds of unhealthy incentives. I agree that there\n> is a\n> risk in accepting channels from unknown nodes\n>\n\nI'd say that the lack of inbound fees requires more trust from the acceptor\nof the channel and leads to more centralization.\n\n\n> , but I think it should be\n> addressed differently: you could for example make the opener pay a fee when\n> they open a channel to you to compensate that risk (some kind of reversed\n> liquidity ads).\n>\n\nYes, can see that work too. The advantage of an inbound fee though is that\nthe fee that you pay is proportional to the balance of the counter party.\nSo you only start paying when you actually move the balance and you don't\nneed to pay everything upfront (which requires some trust from the\ninitiator).\n\nJoost\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220704/7f219542/attachment-0001.html>"
            },
            {
                "author": "Joost Jager",
                "date": "2022-07-04T08:17:31",
                "message_text_only": "On Fri, Jul 1, 2022 at 2:17 PM Thomas HUET <thomas.huet at acinq.fr> wrote:\n\n> It was discussed in this issue:\n> https://github.com/lightning/bolts/issues/835\n>\n\nAh yes that was it. Thanks for the pointer!\n\n\n> On the network, the traffic is not balanced. Some nodes tend to receive\n> more than they send, merchants for instance. For the lightning network to\n> be reliable, we need to incentivise people to open channels to such nodes,\n> or else there won't be enough liquidity available and payments will fail.\n> The current fee structure provides this incentive: You pay some onchain\n> fees and lock some funds and in exchange you will earn routing fees. My\n> concern is that your proposed change would break that incentive and make\n> the network less reliable.\n>\n\nI'd think that if a merchant charges inbound fees, others won't open\nchannels and the merchant won't have inbound liquidity. So why would they\ndo this? Also if they wanted to charge inbound fees, they could already do\nso today by setting up a fee-charging gateway with a private channel to the\nmain node that accepts the payments.\n\nJoost\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220704/d673ed26/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Inbound channel routing fees",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Bastien TEINTURIER",
                "Joost Jager",
                "Thomas HUET"
            ],
            "messages_count": 8,
            "total_messages_chars_count": 26552
        }
    },
    {
        "title": "[Lightning-dev] Using BOLT 8 to Send Wumbo Messages",
        "thread_messages": [
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2022-07-02T00:16:08",
                "message_text_only": "Hi y'all,\n\nQuick post...\n\nA few weeks ago, some of the dlcspecs developers reached out to ask for\nfeedback on this PR [1] that attempts to specify a way to send messages\nlarger\nthan 65 KB using BOLT 8 (Noise based encrypted transport). After taking a\nglance at the PR, I realized that it isn't totally obvious from reading BOLT\n8 that it's actually possible to do this w/o adding any new application\nlayer messages (as the PR proposes).\n\nAs I explained in my comment [2], all the sender needs to do is chunk their\nmessages, and the receiver reads out messages into a read buffer exposed\nover\na stream-like interface. This is no different than using TCP/IP to send a 65\nKB message over the wire: a series of messages below the Maximum\nTransmission Unit at each hop are sent, w/ the receiver\ncollecting/re-ordering them all before delivering up the API stack.\n\nThis was actually in the OG spec, but then was removed to make things a bit\nsimpler. Here's my commit from way back when implementing this behavior [3].\nIf we wanted to re-introduce this behavior (so we can do things like\nincrease the max HTLC limit w/o having to worry about messages being to\nlarge due to all the extra sigs), afaict, we could just add a new wumbo\nmessage feature bit. This bit indicates that a peer knows how to properly\nchunk and aggregate larger messages.\n\n\n[1]: https://github.com/discreetlogcontracts/dlcspecs/pull/192\n[2]:\nhttps://github.com/discreetlogcontracts/dlcspecs/pull/192#issuecomment-1171569378\n[3]:\nhttps://github.com/lightningnetwork/lnd/commit/767c550d65ef97a765eabe09c97941d91e05f054\n\n-- Laolu\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220701/3f9bc643/attachment.html>"
            },
            {
                "author": "Thibaut Le Guilly",
                "date": "2022-07-04T00:29:14",
                "message_text_only": "Hi Laolu,\n\nThanks a lot for bringing this topic to the list as well as your comments\nand pointers to your code. I'll just share what I wrote in the PR below.\n\nOne of the motivations I had for having the segment explicit was to be able\nto handle disconnects/reconnects without requiring access to the low level\nnetwork layer. Granted it might not be the best reason, but another reason\nI can think of was mentioned by Christian Decker [1], the ability to send\nan urgent message in the middle of sending a large one (I cannot think how\nthis could be achieved without headers, but I might just lack imagination).\n\nThat being said, I don't have a strong opinion on what is the best, and if\nthere is a need for something similar in LN it'd be really nice that we use\nthe same thing (less thing for our small community to maintain, and we\nalready rely on quite some of the LN messages anyway so I think it'd make\nsense).\n\nAlso thanks to all the other developers who provided comments/feedback (and\nto Chris for asking them), I really appreciate it.\n\nCheers,\n\nThibaut\n\n[1]:\nhttps://github.com/discreetlogcontracts/dlcspecs/pull/192#discussion_r894383344\n\nOn Sat, Jul 2, 2022 at 9:16 AM Olaoluwa Osuntokun <laolu32 at gmail.com> wrote:\n\n> Hi y'all,\n>\n> Quick post...\n>\n> A few weeks ago, some of the dlcspecs developers reached out to ask for\n> feedback on this PR [1] that attempts to specify a way to send messages\n> larger\n> than 65 KB using BOLT 8 (Noise based encrypted transport). After taking a\n> glance at the PR, I realized that it isn't totally obvious from reading\n> BOLT\n> 8 that it's actually possible to do this w/o adding any new application\n> layer messages (as the PR proposes).\n>\n> As I explained in my comment [2], all the sender needs to do is chunk their\n> messages, and the receiver reads out messages into a read buffer exposed\n> over\n> a stream-like interface. This is no different than using TCP/IP to send a\n> 65\n> KB message over the wire: a series of messages below the Maximum\n> Transmission Unit at each hop are sent, w/ the receiver\n> collecting/re-ordering them all before delivering up the API stack.\n>\n> This was actually in the OG spec, but then was removed to make things a bit\n> simpler. Here's my commit from way back when implementing this behavior\n> [3].\n> If we wanted to re-introduce this behavior (so we can do things like\n> increase the max HTLC limit w/o having to worry about messages being to\n> large due to all the extra sigs), afaict, we could just add a new wumbo\n> message feature bit. This bit indicates that a peer knows how to properly\n> chunk and aggregate larger messages.\n>\n>\n> [1]: https://github.com/discreetlogcontracts/dlcspecs/pull/192\n> [2]:\n> https://github.com/discreetlogcontracts/dlcspecs/pull/192#issuecomment-1171569378\n> [3]:\n> https://github.com/lightningnetwork/lnd/commit/767c550d65ef97a765eabe09c97941d91e05f054\n>\n> -- Laolu\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220704/3e18714f/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Using BOLT 8 to Send Wumbo Messages",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Olaoluwa Osuntokun",
                "Thibaut Le Guilly"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 5043
        }
    },
    {
        "title": "[Lightning-dev] Achieving Zero Downtime Splicing in Practice via Chain Signals",
        "thread_messages": [
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2022-07-02T00:26:54",
                "message_text_only": "Hi Lisa,\n\n> Adding a noticeable on-chain signal runs counter to the goal of the move\n> to taproot / gossip v2, which is to make lightning's onchain footprint\n> indistinguishable from any other onchain usage\n\nMy model of gossip v2 is something like:\n\n  * there's no longer a 1:1 mapping of channels and UTXOs\n  * verifiers don't actually care if the advertised UTXO is actually a\n    channel or not\n  * verifiers aren't watching the chain for spends, as channel\n    advertisements expire after 2 weeks or w/e\n  * there might be a degree of \"leverage\" allowing someone to advertise a 1\n    BTC UTXO as having 10 BTC capacity (or w/e)\n\nSo in this model, splicing on the gossip network wouldn't really be an\nexplicit event. Since I'm free to advertise a series of channels that might\nnot actually exist, I can just say: ok, this set of 5 channels is now\nactually 2 channels, and you can route a bit more over them. In this world,\nre-organizing by a little corner of the channel graph isn't necessarily\ntied to\nmaking a series of on-chain transactions.\n\nIn the realm of the gossip network as it's defined today, the act of\nsplicing is already itself a noticeable chain signal: I see a channel close,\nthen another one advertised that uses that old channel as inputs, and the\nclosing and opening transactions are the same. As a result, for _public_\nchannels any of the chain signals I listed above don't actually give away\nany additional information: splices are already identifiable (in theory).\n\nI don't disagree that waiting N blocks is probably \"good enough\" for most\ncases (ignoring block storms, rare long intervals between blocks, etc, etc).\nInstead this is suggested in the spirit of a belt-and-suspenders approach:\nif I can do something to make the signal 100% reliable, that doesn't add\nextra bytes to the chain, and doesn't like additional information for public\nchannels (the only case where the message even matters), then why not?\n\n-- Laolu\n\n\nOn Wed, Jun 29, 2022 at 5:43 PM lisa neigut <niftynei at gmail.com> wrote:\n\n> Adding a noticeable on-chain signal runs counter to the goal of the move\n> to taproot / gossip v2, which is to make lightning's onchain footprint\n> indistinguishable from\n> any other onchain usage.\n>\n> I'm admittedly a bit confused as to why onchain signals are even being\n> seriously\n>  proposed. Aside from \"infallibility\", is there another reason for\n> suggesting\n> we add an onchain detectable signal for this? Seems heavy handed imo,\n> given\n> that the severity of a comms failure is pretty minimal (*potential* for\n> lost routing fees).\n>\n> > So it appears you don't agree that the \"wait N blocks before you close\n> your\n> channels\" isn't a fool proof solution? Why 12 blocks, why not 15? Or 144?\n>\n> fwiw I seem to remember seeing that it takes  ~an hour for gossip to\n> propagate\n> (no link sorry). Given that, 2x an hour or 12 blocks is a reasonable first\n> estimate.\n> I trust we'll have time to tune this after we've had some real-world\n> experience with them.\n>\n> Further, we can always add more robust signaling later, if lost routing\n> fees turns\n> out to be a huge issue.\n>\n> Finally, worth noting that Alex Myer's minisketch project may well\n> help/improve gossip\n> reconciliation efficiency to the point where gossip reliability is less\n> of an issue.\n>\n> ~nifty\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220701/5ce89e1b/attachment.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2022-07-02T00:29:53",
                "message_text_only": "> That's not 100% reliable at all.   How long to you want for the new\ngossip?\n\nSo you know it's a new channel, with a new capacity (look at the on-chain\noutput), between the same parties (assuming ppl use that multi-sig signal).\nIf\nyou attempt to route over it and have a stale policy, you'll get the latest\npolicy. Therefore, it doesn't really matter how long you wait, as you aren't\nremoving the channel from your graph, as you know it didn't really close.\n\nIf you don't see a message after 2 weeks or w/e, then you mark it as a\nzombie just like any other channel.\n\n-- Laolu\n\n\nOn Wed, Jun 29, 2022 at 5:35 PM Rusty Russell <rusty at rustcorp.com.au> wrote:\n\n> Olaoluwa Osuntokun <laolu32 at gmail.com> writes:\n> > Hi Rusty,\n> >\n> > Thanks for the feedback!\n> >\n> >> This is over-design: if you fail to get reliable gossip, your routing\n> will\n> >> suffer anyway.  Nothing new here.\n> >\n> > Idk, it's pretty simple: you're already watching for closes, so if a\n> close\n> > looks a certain way, it's a splice. When you see that, you can even take\n> > note of the _new_ channel size (funds added/removed) and update your\n> > pathfinding/blindedpaths/hophints accordingly.\n>\n> Why spam the chain?\n>\n> > If this is an over-designed solution, that I'd categorize _only_ waiting\n> N\n> > blocks as wishful thinking, given we have effectively no guarantees w.r.t\n> > how long it'll take a message to propagate.\n>\n> Sure, it's a simplification on \"wait 6 blocks plus 30 minutes\".\n>\n> > If by routing you mean a sender, then imo still no: you don't necessarily\n> > need _all_ gossip, just the latest policies of the nodes you route most\n> > frequently to. On top of that, since you can get the latest policy each\n> time\n> > you incur a routing failure, as you make payments, you'll get the latest\n> > policies of the nodes you care about over time. Also consider that you\n> might\n> > fail to get \"reliable\" gossip, simply just due to your peer neighborhood\n> > aggressively rate limiting gossip (they only allow 1 update a day for a\n> > node, you updated your fee, oops, no splice msg for you).\n>\n> There's no ratelimiting on new channel announcements?\n>\n> > So it appears you don't agree that the \"wait N blocks before you close\n> your\n> > channels\" isn't a fool proof solution? Why 12 blocks, why not 15? Or 144?\n>\n> Because it's simple.\n>\n> >>From my PoV, the whole point of even signalling that a splice is on\n> going,\n> > is for the sender's/receivers: they can continue to send/recv payments\n> over\n> > the channel while the splice is in process. It isn't that a node isn't\n> > getting any gossip, it's that if the node fails to obtain the gossip\n> message\n> > within the N block period of time, then the channel has effectively\n> closed\n> > from their PoV, and it may be an hour+ until it's seen as a usable (new)\n> > channel again.\n>\n> Sure.  If you want to not forget channels at all on close, that works too.\n>\n> > If there isn't a 100% reliable way to signal that a splice is in\n> progress,\n> > then this disincentives its usage, as routers can lose out on potential\n> fee\n> > revenue, and sends/receivers may grow to favor only very long lived\n> > channels. IMO _only_ having a gossip message simply isn't enough:\n> there're\n> > no real guarantees w.r.t _when_ all relevant parties will get your gossip\n> > message. So why not give them a 100% reliable on chain signal that:\n> > something is in progress here, stay tuned for the gossip message,\n> whenever\n> > you receive that.\n>\n> That's not 100% reliable at all.   How long to you want for the new\n> gossip?\n>\n> Just treat every close as signalling \"stay tuned for the gossip\n> message\".  That's reliable.  And simple.\n>\n> Cheers,\n> Rusty.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220701/fa6db025/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Achieving Zero Downtime Splicing in Practice via Chain Signals",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Olaoluwa Osuntokun"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 7381
        }
    },
    {
        "title": "[Lightning-dev] Onion messages rate-limiting",
        "thread_messages": [
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2022-07-02T00:48:32",
                "message_text_only": "Hi Val,\n\n> Another huge win of backpressure is that it only needs to happen in DoS\n> situations, meaning it doesn\u2019t have to impact users in the normal case.\n\nI agree, I think the same would apply to prepayments as well (0 or 1 msat in\ncalm times). My main concern with relying _only_ on backpressure rate\nlimiting is that we'd end up w/ your first scenario more often than not,\nwhich means routine (and more important to the network) things like fetching\ninvoices becomes unreliable.\n\nI'm not saying we should 100% compare onion messages to Tor, but that we\nmight\nbe able to learn from what works and what isn't working for them. The\nsystems\naren't identical, but have some similarities.\n\nOn the topic of parameters across the network: could we end up in a scenario\nwhere someone is doing like streaming payments for a live stream (or w/e),\nends up fetching a ton of invoices (actual traffic leading to payments), but\nthen ends up being erroneously rate limited by their peers? Assuming they\nhave 1 or 2 channels that have now all been clamped down, is waiting N\nminutes (or w/e) their only option? If so then this might lead to their\nlivestream (data being transmitted elsewhere) being shut off. Oops, they\njust\nmissed the greatest World Cup goal in history!  You had to be there, you\nhad to\nbe there, you had to *be* there...\n\nAnother question on my mind is: if this works really well for rate limiting\nof\nonion messages, then why can't we use it for HTLCs as well?\n\n-- Laolu\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220701/bb864893/attachment.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2022-07-02T01:09:05",
                "message_text_only": "Hi Matt,\n\n> Ultimately, paying suffers from the standard PoW-for-spam issue - you\n> cannot assign a reasonable cost that an attacker cares about without\n> impacting the system's usability due to said cost.\n\nApplying this statement to related a area, would you also agree that\nproposals\nto introduce pre-payments for HTLCs to mitigate jamming attacks is similarly\na dead end?  Personally, this has been my opinion for some time now. Which\nis why I advocate for the forwarding pass approach (gracefully degrade to\nstratified topology), which in theory would allow the major flows of the\nnetwork to continue in the face of disruption.\n\n-- Laolu\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220701/06c96445/attachment.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2022-07-04T02:00:26",
                "message_text_only": "On 7/1/22 9:09 PM, Olaoluwa Osuntokun wrote:\n> Hi Matt,\n> \n>  > Ultimately, paying suffers from the standard PoW-for-spam issue - you\n>  > cannot assign a reasonable cost that an attacker cares about without\n>  > impacting the system's usability due to said cost.\n> \n> Applying this statement to related a area\n\nI mean, I think its only mostly-related, cause HTLCs are pretty different in cost, but.\n\n> would you also agree that proposals\n> to introduce pre-payments for HTLCs to mitigate jamming attacks is similarly\n> a dead end?\n\nI dunno if its a \"dead end\", but, indeed, the naive proposals I'm definitely no fan of whatsoever. I \ncertainly remain open to being shown I'm wrong.\n\n> Personally, this has been my opinion for some time now. Which\n> is why I advocate for the forwarding pass approach (gracefully degrade to\n> stratified topology), which in theory would allow the major flows of the\n> network to continue in the face of disruption.\n\nI'm starting to come around to allowing a \"pay per HTLC-locked-time\" fee, with Rusty's proposal \naround allowing someone to force-close a channel to \"blame\"\n  a hop for not failing back after fees stop coming in. Its really nifty in theory and doesn't have \nall the classic issues that up-front-fees have, but it puts a very, very, very high premium on high \nuptime, which may be catastrophic, dunno.\n\nMatt"
            },
            {
                "author": "Matt Corallo",
                "date": "2022-07-04T02:07:16",
                "message_text_only": "On 7/1/22 8:48 PM, Olaoluwa Osuntokun wrote:\n> Hi Val,\n> \n>  > Another huge win of backpressure is that it only needs to happen in DoS\n>  > situations, meaning it doesn\u2019t have to impact users in the normal case.\n> \n> I agree, I think the same would apply to prepayments as well (0 or 1 msat in\n> calm times). My main concern with relying _only_ on backpressure rate\n> limiting is that we'd end up w/ your first scenario more often than not,\n> which means routine (and more important to the network) things like fetching\n> invoices becomes unreliable.\n\nYou're still thinking about this in a costing world, but this really is a networking problem, not a \ncosting one.\n\n> I'm not saying we should 100% compare onion messages to Tor, but that we might\n> be able to learn from what works and what isn't working for them. The systems\n> aren't identical, but have some similarities.\n\nTo DoS here you have to have *very* asymmetric attack power - regular ol' invoice requests are \ntrivial amounts of bandwidth, like, really, really trivial. Like, 1000x less bandwidth than an \naverage ol' home node on a DOCSIS high-latency line with 20Mbps up has available. Closer to \n1,000,000x less if we're talking about \"real metal\".\n\nMore importantly, Tor's current attack actually *isn't* a simple DoS attack. The attack there isn't \nrelevant to onion messages at all, you're just throwing up roadblocks with nonsense here.\n\n\n> On the topic of parameters across the network: could we end up in a scenario\n> where someone is doing like streaming payments for a live stream (or w/e),\n> ends up fetching a ton of invoices (actual traffic leading to payments), but\n> then ends up being erroneously rate limited by their peers? Assuming they\n> have 1 or 2 channels that have now all been clamped down, is waiting N\n> minutes (or w/e) their only option? If so then this might lead to their\n> livestream (data being transmitted elsewhere) being shut off. Oops, they just\n> missed the greatest World Cup goal in history!\u00a0 You had to be there, you had to\n> be there, you had to *be* there...\n\nYou're basically making a \"you had to have more inbound capacity\" argument, which, sure, yes, you \ndo. Even better, though, onion messages are *cheap*, like absurdly cheap, so if you have enough \ninbound capacity you're almost certain to have enough inbound *network* capacity to handle some \ninvoice requests, hell, they're a millionth the cost of the HTLCs you're about to receive \nanyway...this argument is just nonsense.\n\n\n> Another question on my mind is: if this works really well for rate limiting of\n> onion messages, then why can't we use it for HTLCs as well?\n\nWe do? 400-some-odd HTLCs in flight at once is a *really* tight rate limit, even! Order of \nmagnitudes tighter than onion message rate limits need to be :)\n\nMatt"
            },
            {
                "author": "Antoine Riard",
                "date": "2022-07-05T19:58:59",
                "message_text_only": "Hi Bastien,\n\nThanks for the proposal,\n\nWhile I think establishing the rate limiting based on channel topology\nshould be effective to mitigate against DoS attackers, there is still a\nconcern that the damage inflicted might be beyond the channel cost. I.e, as\nthe onion messages routing is source-based, an attacker could exhaust or\nreduce targeted onion communication channels to prevent invoices exchanges\nbetween LN peers, and thus disrupt their HTLC traffic. Moreover, if the\nHTLC traffic is substitutable (\"Good X sold by Merchant Alice can be\nsubstituted by good Y sold by Merchant Mallory\"), the attacker could\nextract income from the DoS attack, compensating for the channel cost.\n\nIf plausible, such targeted onion bandwidth attacks would be fairly\nsophisticated so it might not be a concern for the short-term. Though we\nmight have to introduce some proportion between onion bandwidth units\nacross the network and the cost of opening channels in the future...\n\nOne further concern, we might have \"spontaneous\" bandwidth DoS in the\nfuture, if the onion traffic is leveraged beyond offers such as for\ndiscovery of LSP liquidity services (e.g PeerSwap, instant inbound\nchannels, etc). For confidentiality reasons, a LN node might not use the\nNoise connections to learn about such services. The LN node might be also\ninterested to do real market-discovery by fetching the services rates from\nall the LSP, while engaging with only one, therefore provoking a spike in\nonion bandwidth consumed across the network without symmetric\nHTLC traffic. This concern is hypothetical as that class of traffic might\nend up announced in gossip.\n\nSo I think backpressure based rate limiting is good to boostrap as a\n\"naive\" DoS protection for onion messages though I'm not sure it will be\nrobust enough in the long-term.\n\nAntoine\n\nLe mer. 29 juin 2022 \u00e0 04:28, Bastien TEINTURIER <bastien at acinq.fr> a\n\u00e9crit :\n\n> During the recent Oakland Dev Summit, some lightning engineers got together to discuss DoS\n> protection for onion messages. Rusty proposed a very simple rate-limiting scheme that\n> statistically propagates back to the correct sender, which we describe in details below.\n>\n> You can also read this in gist format if that works better for you [1].\n>\n> Nodes apply per-peer rate limits on _incoming_ onion messages that should be relayed (e.g.\n> N/seconds with some burst tolerance). It is recommended to allow more onion messages from\n> peers with whom you have channels, for example 10/seconds when you have a channel and 1/second\n> when you don't.\n>\n> When relaying an onion message, nodes keep track of where it came from (by using the `node_id` of\n> the peer who sent that message). Nodes only need the last such `node_id` per outgoing connection,\n> which ensures the memory footprint is very small. Also, this data doesn't need to be persisted.\n>\n> Let's walk through an example to illustrate this mechanism:\n>\n> * Bob receives an onion message from Alice that should be relayed to Carol\n> * After relaying that message, Bob stores Alice's `node_id` in its per-connection state with Carol\n> * Bob receives an onion message from Eve that should be relayed to Carol\n> * After relaying that message, Bob replaces Alice's `node_id` with Eve's `node_id` in its\n> per-connection state with Carol\n> * Bob receives an onion message from Alice that should be relayed to Dave\n> * After relaying that message, Bob stores Alice's `node_id` in its per-connection state with Dave\n> * ...\n>\n> We introduce a new message that will be sent when dropping an incoming onion message because it\n> reached rate limits:\n>\n> 1. type: 515 (`onion_message_drop`)\n> 2. data:\n>    * [`rate_limited`:`u8`]\n>    * [`shared_secret_hash`:`32*byte`]\n>\n> Whenever an incoming onion message reaches the rate limit, the receiver sends `onion_message_drop`\n> to the sender. The sender looks at its per-connection state to find where the message was coming\n> from and relays `onion_message_drop` to the last sender, halving their rate limits with that peer.\n>\n> If the sender doesn't overflow the rate limit again, the receiver should double the rate limit\n> after 30 seconds, until it reaches the default rate limit again.\n>\n> The flow will look like:\n>\n> Alice                      Bob                      Carol\n>   |                         |                         |\n>   |      onion_message      |                         |\n>   |------------------------>|                         |\n>   |                         |      onion_message      |\n>   |                         |------------------------>|\n>   |                         |    onion_message_drop   |\n>   |                         |<------------------------|\n>   |    onion_message_drop   |                         |\n>   |<------------------------|                         |\n>\n> The `shared_secret_hash` field contains a BIP 340 tagged hash of the Sphinx shared secret of the\n> rate limiting peer (in the example above, Carol):\n>\n> * `shared_secret_hash = SHA256(SHA256(\"onion_message_drop\") || SHA256(\"onion_message_drop\") || sphinx_shared_secret)`\n>\n> This value is known by the node that created the onion message: if `onion_message_drop` propagates\n> all the way back to them, it lets them know which part of the route is congested, allowing them\n> to retry through a different path.\n>\n> Whenever there is some latency between nodes and many onion messages, `onion_message_drop` may\n> be relayed to the incorrect incoming peer (since we only store the `node_id` of the _last_ incoming\n> peer in our outgoing connection state). The following example highlights this:\n>\n>  Eve                       Bob                      Carol\n>   |      onion_message      |                         |\n>   |------------------------>|      onion_message      |\n>   |      onion_message      |------------------------>|\n>   |------------------------>|      onion_message      |\n>   |      onion_message      |------------------------>|\n>   |------------------------>|      onion_message      |\n>                             |------------------------>|\n> Alice                       |    onion_message_drop   |\n>   |      onion_message      |                    +----|\n>   |------------------------>|      onion_message |    |\n>   |                         |--------------------|--->|\n>   |                         |                    |    |\n>   |                         |                    |    |\n>   |                         |                    |    |\n>   |    onion_message_drop   |<-------------------+    |\n>   |<------------------------|                         |\n>\n> In this example, Eve is spamming but `onion_message_drop` is propagated back to Alice instead.\n> However, this scheme will _statistically_ penalize the right incoming peer (with a probability\n> depending on the volume of onion messages that the spamming peer is generating compared to the\n> volume of legitimate onion messages).\n>\n> It is an interesting research problem to find formulas for those probabilities to evaluate how\n> efficient this will be against various types of spam. We hope researchers on this list will be\n> interested in looking into it and will come up with a good model to evaluate that scheme.\n>\n> To increase the accuracy of attributing `onion_message_drop`, more data could be stored in the\n> future if it becomes necessary. We need more research to quantify how much accuracy would be\n> gained by storing more data and making the protocol more complex.\n>\n> Cheers,\n> Bastien\n>\n> [1] https://gist.github.com/t-bast/e37ee9249d9825e51d260335c94f0fcf\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220705/ce8d1fbe/attachment.html>"
            },
            {
                "author": "Joost Jager",
                "date": "2022-07-10T08:43:27",
                "message_text_only": "On Thu, Jun 30, 2022 at 4:19 AM Matt Corallo <lf-lists at mattcorallo.com>\nwrote:\n\n> Better yet, as Val points out, requiring a channel to relay onion messages\n> puts a very real,\n> nontrivial (in a world of msats) cost to getting an onion messaging\n> channel. Better yet, with\n> backpressure ability to DoS onion message links isn't denominated in\n> number of messages, but instead\n> in number of channels you are able to create, making the backpressure\n> system equivalent to today's\n> HTLC DoS considerations, whereas explicit payment allows an attacker to\n> pay much less to break the\n> system.\n>\n\nIt can also be considered a bad thing that DoS ability is not based on a\nnumber of messages. It means that for the one time cost of channel\nopen/close, the attacker can generate spam forever if they stay right below\nthe rate limit.\n\n\n> Ultimately, paying suffers from the standard PoW-for-spam issue - you\n> cannot assign a reasonable\n> cost that an attacker cares about without impacting the system's usability\n> due to said cost. Indeed,\n> making it expensive enough to mount a months-long DDoS without impacting\n> legitimate users be pretty\n> easy - at 1msat per relay of a 1366 byte onion message you can only\n> saturate an average home users'\n> 30Mbps connection for 30 minutes before you rack up a dollar in costs, but\n> if your concern is\n> whether someone can reasonably trivially take out the network for minutes\n> at a time to make it have\n> perceptibly high failure rates, no reasonable cost scheme will work. Quite\n> the opposite - the only\n> reasonable way to respond is to respond to a spike in traffic while\n> maintaining QoS is to rate-limit\n> by inbound edge!\n>\n\nSuppose the attacker has enough channels to hit the rate limit on an\nimportant connection some hops away from themselves. They can then sustain\nthat attack indefinitely, assuming that they stay below the rate limit on\nthe routes towards the target connection. What will the response be in that\ncase? Will node operators work together to try to trace back to the source\nand take down the attacker? That requires operators to know each other.\n\nMaybe this is a difference between lightning network and the internet that\nis relevant for this discussion. That routers on the internet know each\nother and have physical links between them, where as in lightning ties can\nbe much looser.\n\nJoost\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220710/d7fe084e/attachment.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2022-07-10T19:14:17",
                "message_text_only": "On 7/10/22 4:43 AM, Joost Jager wrote:\n> It can also be considered a bad thing that DoS ability is not based on a number of messages. It \n> means that for the one time cost of channel open/close, the attacker can generate spam forever if \n> they stay right below the rate limit.\n\nI don't see why this is a problem? This seems to assume some kind of per-message cost that nodes \nhave to bear, but there is simply no such thing. Indeed, if message spam causes denial of service to \nother network participants, this would be an issue, but an attacker generating spam from one \nspecific location within the network should not cause that, given some form of backpressure within \nthe network.\n\n> Suppose the attacker has enough channels to hit the rate limit on an important connection some hops \n> away from themselves. They can then sustain that attack indefinitely, assuming that they stay below \n> the rate limit on the routes towards the target connection. What will the response be in that case? \n> Will node operators work together to try to trace back to the source and take down the attacker? \n> That requires operators to know each other.\n\nNo it doesn't, backpressure works totally fine and automatically applies pressure backwards until \nnodes, in an automated fashion, are appropriately ratelimiting the source of the traffic.\n\n> Maybe this is a difference between lightning network and the internet that is relevant for this \n> discussion. That routers on the internet know each other and have physical links between them, where \n> as in lightning ties can be much looser.\n\nNo? The internet does not work by ISPs calling each other up on the phone to apply backpressure \nmanually whenever someone sends a lot of traffic? If anything lightning ties between nodes are much, \nmuch stronger than ISPs on the internet - you generally are at least loosely trusting your peer with \nyour money, not just your customer's customer's bits.\n\nMatt"
            },
            {
                "author": "Joost Jager",
                "date": "2022-07-11T09:08:53",
                "message_text_only": "On Sun, Jul 10, 2022 at 9:14 PM Matt Corallo <lf-lists at mattcorallo.com>\nwrote:\n\n> > It can also be considered a bad thing that DoS ability is not based on a\n> number of messages. It\n> > means that for the one time cost of channel open/close, the attacker can\n> generate spam forever if\n> > they stay right below the rate limit.\n>\n> I don't see why this is a problem? This seems to assume some kind of\n> per-message cost that nodes\n> have to bear, but there is simply no such thing. Indeed, if message spam\n> causes denial of service to\n> other network participants, this would be an issue, but an attacker\n> generating spam from one\n> specific location within the network should not cause that, given some\n> form of backpressure within\n> the network.\n>\n\nIt's more a general observation that an attacker can open a set of channels\nin multiple locations once and can use them forever to support potential\nattacks. That is assuming attacks aren't entirely thwarted with\nbackpressure.\n\n\n> > Suppose the attacker has enough channels to hit the rate limit on an\n> important connection some hops\n> > away from themselves. They can then sustain that attack indefinitely,\n> assuming that they stay below\n> > the rate limit on the routes towards the target connection. What will\n> the response be in that case?\n> > Will node operators work together to try to trace back to the source and\n> take down the attacker?\n> > That requires operators to know each other.\n>\n> No it doesn't, backpressure works totally fine and automatically applies\n> pressure backwards until\n> nodes, in an automated fashion, are appropriately ratelimiting the source\n> of the traffic.\n>\n\nTurns out I did not actually fully understand the proposal. This version of\nbackpressure is nice indeed.\n\nTo get a better feel for how it works, I've coded up a simple single node\nsimulation (\nhttps://gist.github.com/joostjager/bca727bdd4fc806e4c0050e12838ffa3), which\nproduces output like this:\nhttps://gist.github.com/joostjager/682c4232c69f3c19ec41d7dd4643bb27. There\nare a few spammers and one real user. You can see that after some time, the\nspammers are all throttled down and the user packets keep being handled.\n\nIf you add enough spammers, they are obviously still able to hit the next\nhop rate limit and affect the user. But because their incoming limits have\nbeen throttled down, you need a lot of them - depending on the minimum rate\nthat the node goes down to.\n\nI am wondering about that spiraling-down effect for legitimate users. Once\nyou hit the limit, it is decreased and it becomes easier to hit it again.\nIf you don't adapt, you'll end up with a very low rate. You need to take a\nbreak to recover from that. I guess the assumption is that legitimate users\nnever end up there, because the rate limits are much much higher than what\nthey need. Even if they'd occasionally hit a limit on a busy connection,\nthey can go through a lot of halvings before they'll get close to the rate\nthat they require and it becomes a problem.\n\nBut how would that work if the user only has a single channel and wants to\nretry? I suppose they need to be careful to use a long enough delay to not\nget into that down-spiral. But how do they determine what is long enough?\nProbably not a real problem in practice with network latency etc, even\nthough a concrete value does need to be picked.\n\nSpammers are probably also not going to spam at max speed. They'd want to\navoid their rate limit being slashed. In the simulation, I've added a\n`perfectSpammers` mode that creates spammers that have complete information\non the state of the rate limiter. Not possible in reality of course. If you\nenable this mode, it does get hard for the user. Spammers keep pushing the\nlimiter to right below the tripping point and an unknowing user trips it\nand spirals down. (\nhttps://gist.github.com/joostjager/6eef1de0cf53b5314f5336acf2b2a48a)\n\nI don't know to what extent spammers without perfect information can still\nbe smart and optimize their spam rate. They can probably do better than\nkeep sending at max speed.\n\n> Maybe this is a difference between lightning network and the internet\n> that is relevant for this\n> > discussion. That routers on the internet know each other and have\n> physical links between them, where\n> > as in lightning ties can be much looser.\n>\n> No? The internet does not work by ISPs calling each other up on the phone\n> to apply backpressure\n> manually whenever someone sends a lot of traffic? If anything lightning\n> ties between nodes are much,\n> much stronger than ISPs on the internet - you generally are at least\n> loosely trusting your peer with\n> your money, not just your customer's customer's bits.\n>\n\nHaha, okay, yes, I actually don't know what ISPs do in case of DoS attacks.\nJust trying to find differences between lightning and the internet that\ncould be relevant for this discussion.\n\nSeems to me that lightning's onion routing makes it hard to trace back to\nthe source without node operators calling each other up. Harder than it is\non the internet. Of course if backpressure works, you don't need to trace\nnothing so it all doesn't matter.\n\n> Another question on my mind is: if this works really well for rate\n> limiting of\n> > onion messages, then why can't we use it for HTLCs as well?\n\n\n\nWe do? 400-some-odd HTLCs in flight at once is a *really* tight rate limit,\n> even! Order of\n> magnitudes tighter than onion message rate limits need to be :)\n\n\nWhat we don't yet do is create backpressure on the incoming channels by\nlowering the `max_pending_htlc` limit dynamically.\n\nThe idea could also be extended to htlc forwarding rate limiters, to combat\nshort-lived htlc spam.\n\nJoost\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220711/ed9a9e97/attachment.html>"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2022-07-26T08:16:24",
                "message_text_only": "Hey all,\n\nThanks for the comments!\nHere are a few answers inline to some points that aren't fully addressed\nyet.\n\n@laolu\n\n> Another question on my mind is: if this works really well for rate\nlimiting of\n> onion messages, then why can't we use it for HTLCs as well?\n\nBecause HTLC DoS is fundamentally different: the culprit isn't always\nupstream, most of the time it's downstream (holding an HTLC), so back\npressure cannot work.\n\nOnion messages don't have this issue at all because there's no\nequivalent to holding an onion message downstream, it doesn't have\nany impact on previous intermediate nodes.\n\n@ariard\n\n> as the onion messages routing is source-based, an attacker could\n> exhaust or reduce targeted onion communication channels to prevent\n> invoices exchanges between LN peers\n\nCan you detail how? That's exactly what this scheme is trying to prevent.\nThis looks similar to Joost's early comment, but I think it's based on a\nmisunderstanding of the proposal (as Joost then acknowledged). Spammers\nwill be statistically penalized, which will allow honest messages to go\nthrough. As Joost details below, attackers with perfect information about\nthe state of rate-limits can in theory saturate links, but in practice I\nbelieve this cannot work for an extended period of time.\n\n@joost\n\nCool work with the simulation, thanks!\nLet us know if that yields other interesting results.\n\nCheers,\nBastien\n\nLe lun. 11 juil. 2022 \u00e0 11:09, Joost Jager <joost.jager at gmail.com> a \u00e9crit :\n\n> On Sun, Jul 10, 2022 at 9:14 PM Matt Corallo <lf-lists at mattcorallo.com>\n> wrote:\n>\n>> > It can also be considered a bad thing that DoS ability is not based on\n>> a number of messages. It\n>> > means that for the one time cost of channel open/close, the attacker\n>> can generate spam forever if\n>> > they stay right below the rate limit.\n>>\n>> I don't see why this is a problem? This seems to assume some kind of\n>> per-message cost that nodes\n>> have to bear, but there is simply no such thing. Indeed, if message spam\n>> causes denial of service to\n>> other network participants, this would be an issue, but an attacker\n>> generating spam from one\n>> specific location within the network should not cause that, given some\n>> form of backpressure within\n>> the network.\n>>\n>\n> It's more a general observation that an attacker can open a set of\n> channels in multiple locations once and can use them forever to support\n> potential attacks. That is assuming attacks aren't entirely thwarted with\n> backpressure.\n>\n>\n>> > Suppose the attacker has enough channels to hit the rate limit on an\n>> important connection some hops\n>> > away from themselves. They can then sustain that attack indefinitely,\n>> assuming that they stay below\n>> > the rate limit on the routes towards the target connection. What will\n>> the response be in that case?\n>> > Will node operators work together to try to trace back to the source\n>> and take down the attacker?\n>> > That requires operators to know each other.\n>>\n>> No it doesn't, backpressure works totally fine and automatically applies\n>> pressure backwards until\n>> nodes, in an automated fashion, are appropriately ratelimiting the source\n>> of the traffic.\n>>\n>\n> Turns out I did not actually fully understand the proposal. This version\n> of backpressure is nice indeed.\n>\n> To get a better feel for how it works, I've coded up a simple single node\n> simulation (\n> https://gist.github.com/joostjager/bca727bdd4fc806e4c0050e12838ffa3),\n> which produces output like this:\n> https://gist.github.com/joostjager/682c4232c69f3c19ec41d7dd4643bb27.\n> There are a few spammers and one real user. You can see that after some\n> time, the spammers are all throttled down and the user packets keep being\n> handled.\n>\n> If you add enough spammers, they are obviously still able to hit the next\n> hop rate limit and affect the user. But because their incoming limits have\n> been throttled down, you need a lot of them - depending on the minimum rate\n> that the node goes down to.\n>\n> I am wondering about that spiraling-down effect for legitimate users. Once\n> you hit the limit, it is decreased and it becomes easier to hit it again.\n> If you don't adapt, you'll end up with a very low rate. You need to take a\n> break to recover from that. I guess the assumption is that legitimate users\n> never end up there, because the rate limits are much much higher than what\n> they need. Even if they'd occasionally hit a limit on a busy connection,\n> they can go through a lot of halvings before they'll get close to the rate\n> that they require and it becomes a problem.\n>\n> But how would that work if the user only has a single channel and wants to\n> retry? I suppose they need to be careful to use a long enough delay to not\n> get into that down-spiral. But how do they determine what is long enough?\n> Probably not a real problem in practice with network latency etc, even\n> though a concrete value does need to be picked.\n>\n> Spammers are probably also not going to spam at max speed. They'd want to\n> avoid their rate limit being slashed. In the simulation, I've added a\n> `perfectSpammers` mode that creates spammers that have complete information\n> on the state of the rate limiter. Not possible in reality of course. If you\n> enable this mode, it does get hard for the user. Spammers keep pushing the\n> limiter to right below the tripping point and an unknowing user trips it\n> and spirals down. (\n> https://gist.github.com/joostjager/6eef1de0cf53b5314f5336acf2b2a48a)\n>\n> I don't know to what extent spammers without perfect information can still\n> be smart and optimize their spam rate. They can probably do better than\n> keep sending at max speed.\n>\n> > Maybe this is a difference between lightning network and the internet\n>> that is relevant for this\n>> > discussion. That routers on the internet know each other and have\n>> physical links between them, where\n>> > as in lightning ties can be much looser.\n>>\n>> No? The internet does not work by ISPs calling each other up on the phone\n>> to apply backpressure\n>> manually whenever someone sends a lot of traffic? If anything lightning\n>> ties between nodes are much,\n>> much stronger than ISPs on the internet - you generally are at least\n>> loosely trusting your peer with\n>> your money, not just your customer's customer's bits.\n>>\n>\n> Haha, okay, yes, I actually don't know what ISPs do in case of DoS\n> attacks. Just trying to find differences between lightning and the internet\n> that could be relevant for this discussion.\n>\n> Seems to me that lightning's onion routing makes it hard to trace back to\n> the source without node operators calling each other up. Harder than it is\n> on the internet. Of course if backpressure works, you don't need to trace\n> nothing so it all doesn't matter.\n>\n> > Another question on my mind is: if this works really well for rate\n>> limiting of\n>> > onion messages, then why can't we use it for HTLCs as well?\n>\n>\n>\n> We do? 400-some-odd HTLCs in flight at once is a *really* tight rate\n>> limit, even! Order of\n>> magnitudes tighter than onion message rate limits need to be :)\n>\n>\n> What we don't yet do is create backpressure on the incoming channels by\n> lowering the `max_pending_htlc` limit dynamically.\n>\n> The idea could also be extended to htlc forwarding rate limiters, to\n> combat short-lived htlc spam.\n>\n> Joost\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220726/c52eaf33/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Onion messages rate-limiting",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Matt Corallo",
                "Antoine Riard",
                "Bastien TEINTURIER",
                "Joost Jager",
                "Olaoluwa Osuntokun"
            ],
            "messages_count": 9,
            "total_messages_chars_count": 32447
        }
    },
    {
        "title": "[Lightning-dev] Three Strategies for Lightning Forwarding Nodes",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2022-07-02T02:58:10",
                "message_text_only": "Good morning Michael,\n\n> Hey ZmnSCPxj\n>\n> It is an interesting topic. Alex Bosworth did a presentation at the Lightning Hack Day last year with a similar attempt at categorizing the different strategies for a routing/forwarding node (Ping Pong, Liquidity Battery, Inbound Sourcing, Liquidity Trader, Last Mile, Swap etc)\n>\n> https://btctranscripts.com/lightning-hack-day/2021-03-27-alex-bosworth-lightning-routing/\n>\n> It seems like your attempt is a little more granular and unstructured (based on individual responses) but perhaps it fits into the broad categories Alex suggested maybe with some additional ones?\n\nI think all the broad categories Alex suggested have a common theme: all of them are public forwarding nodes.\n\nWhat I point out in this writeup is the strategy that ANY public forwarding node may have.\n\nFor example, a node may select passive rebalance, wall, or low fee forwarding strategies, independently of whether or not it is JUST a forwarding node, or is a merchant / personal node in addition to being a forwarding node, or is selling inbound liquidity elsewhere, or is selling onchain-offchain swap services, or multiple of those.\n\nIt seems to me that the general forwarding strategy is orthogonal to the strategies that Alex presented.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Three Strategies for Lightning Forwarding Nodes",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "ZmnSCPxj"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1279
        }
    }
]