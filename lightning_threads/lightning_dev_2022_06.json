[
    {
        "title": "[Lightning-dev] LNURL-withdrawPOS bLIP - A reference flow for open source implementations of POS devices able to read LNURL-withdraw links via NFC",
        "thread_messages": [
            {
                "author": "David Coen | davidcoen.it",
                "date": "2022-06-03T07:46:19",
                "message_text_only": "Hi everyone,\nas suggested by Lucas de C. Ferreira and Ryan Gentry, I submitted a PR for\na new bLIP for a common NFC payment flow that uses LNURL protocol -\nspecifically LNURL-withdraw sub-protocol - which I called\n*LNURL-withdrawPOS*:\nhttps://github.com/lightning/blips/pull/16\n\nPlease note: this is NOT a new LNURL sub-protocol but an attempt to reach\nan open and common standard for the receiving part of the flow (merchant\nPoint of Sale with NFC sensor).\n\n*Abstract*\n\nLNURL-withdraw is a bech32-encoded HTTPS query string or sub-protocol of\nLNURL\nwhich gives the users the ability to receive funds via Lightning Network\nwithout the need of manually creating an invoice.\nUsers scan a QRcode or paste the LNURL-withdraw link into their wallet,\nthis queries a server and gets a JSON with some info, such as the max\namount the user can receive, the min amount the user can request, etc.\nThen the wallet typically asks the user to enter an amount and under the\nhood it creates a Lightning invoice and sends it to the service provider,\nwhich eventually pays that.\nSo the only things the users have to do are:\n\n1. Scan the QR code / paste the LNURL-withdraw link into the wallet\n2. Enter the amount to receive\n3. Confirm\n\nIn this repo I schematize a LNURL-withdraw flow for POS, which sees the\ninteraction between the user and a POS equipped with a NFC sensor.\n\nThis bLIP serves to document what could be a reference flow for open source\nimplementations of POS devices able to read LNURL-withdraw links via NFC,\nfor a better UX in Lightning payments powered by LNURL protocol.\n\n*Reference Implementations*\n\nThere are solutions that already use this common flow or are going to\nimplement it.\nI'm having calls to push the adoption of this open standard, in particular\nI've been contacted by Danny Scott (Coincorner), Danny Brewster (Fast\nBitcoins), I'm gonna have a call with Kamil Brejcha (Dexfin Exchange) and\nI'm open to discuss this with anyone who wants to have a call.\n\nBTCpay's LNURL NFC Support plugin by Andrew Camilleri\n<https://github.com/btcpayserver/btcpayserver-plugins>\nCoincorner's checkout system <https://www.coincorner.com/Checkout>\nIBEX Mercado's Point of Sale <https://www.ibexmercado.com/>\nFast Bitcoins's Point of Sale <https://fastbitcoins.com/>\n\nI invite the representatives of the implementations to signal their support\nto this flow by adding themselves to the table available here:\nhttps://github.com/theDavidCoen/LNURL-withdrawPOS\nand to support this bLIP.\n\n*Please note*: before adding your implementation to this list, be sure it\nis:\nopen source, fully interoperable, respects the LNURL-withdraw specs (LUD03\n<https://github.com/fiatjaf/lnurl-rfc/blob/luds/03.md>)\n\n*Feel free to reach out*\nAs mentioned above, I'm open to discuss this even via call: simply select a\ndate from my calendar https://calendly.com/thedavidcoen\n\nThanks for your time!\nDavid Coen\nTwitter: https://twitter.com/thedavidcoen\n\nComunicazione e Immagine, di David Coen\nP.IVA 11368220015\nCodice fiscale: CNODVD88C30L219P\nSede fiscale: via Luigi Boccherini 37, 10155 Torino\ncell. 340 4601061\nWeb Site www.davidcoen.it\nemail: info at davidcoen.it\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220603/30173722/attachment.html>"
            },
            {
                "author": "David Coen | davidcoen.it",
                "date": "2022-06-02T12:38:36",
                "message_text_only": "Hi everyone,\nas suggested by Lucas de C. Ferreira and Ryan Gentry, I submitted a PR for\na new bLIP for a common NFC payment flow that uses LNURL protocol -\nspecifically LNURL-withdraw sub-protocol - which I called\n*LNURL-withdrawPOS*:\nhttps://github.com/lightning/blips/pull/16\n\nPlease note: this is NOT a new LNURL sub-protocol but an attempt to reach\nan open and common standard for the receiving part of the flow (merchant\nPoint of Sale with NFC sensor).\n\n*Abstract*\n\nLNURL-withdraw is a bech32-encoded HTTPS query string or sub-protocol of\nLNURL\nwhich gives the users the ability to receive funds via Lightning Network\nwithout the need of manually creating an invoice.\nUsers scan a QRcode or paste the LNURL-withdraw link into their wallet,\nthis queries a server and gets a JSON with some info, such as the max\namount the user can receive, the min amount the user can request, etc.\nThen the wallet typically asks the user to enter an amount and under the\nhood it creates a Lightning invoice and sends it to the service provider,\nwhich eventually pays that.\nSo the only things the users have to do are:\n\n1. Scan the QR code / paste the LNURL-withdraw link into the wallet\n2. Enter the amount to receive\n3. Confirm\n\nIn this repo I schematize a LNURL-withdraw flow for POS, which sees the\ninteraction between the user and a POS equipped with a NFC sensor.\n\nThis bLIP serves to document what could be a reference flow for open source\nimplementations of POS devices able to read LNURL-withdraw links via NFC,\nfor a better UX in Lightning payments powered by LNURL protocol.\n\n*Reference Implementations*\n\nThere are solutions that already use this common flow or are going to\nimplement it.\nI'm having calls to push the adoption of this open standard, in particular\nI've been contacted by Danny Scott (Coincorner), Danny Brewster (Fast\nBitcoins), I'm gonna have a call with Kamil Brejcha (Dexfin Exchange) and\nI'm open to discuss this with anyone who wants to have a call.\n\nBTCpay's LNURL NFC Support plugin by Andrew Camilleri\n<https://github.com/btcpayserver/btcpayserver-plugins>\nCoincorner's checkout system <https://www.coincorner.com/Checkout>\nIBEX Mercado's Point of Sale <https://www.ibexmercado.com>\nFast Bitcoins's Point of Sale <https://fastbitcoins.com/>\n\nI invite the representatives of the implementations to signal their support\nto this flow by adding themselves to the table available here:\nhttps://github.com/theDavidCoen/LNURL-withdrawPOS\nand to support this bLIP.\n\n*Please note*: before adding your implementation to this list, be sure it\nis:\nopen source, fully interoperable, respects the LNURL-withdraw specs (LUD03\n<https://github.com/fiatjaf/lnurl-rfc/blob/luds/03.md>)\n\n*Feel free to reach out*\nAs mentioned above, I'm open to discuss this even via call: simply select a\ndate from my calendar https://calendly.com/thedavidcoen\n\nThanks for your time!\nDavid Coen\nTwitter: https://twitter.com/thedavidcoen\n\nComunicazione e Immagine, di David Coen\nP.IVA 11368220015\nCodice fiscale: CNODVD88C30L219P\nSede fiscale: via Luigi Boccherini 37, 10155 Torino\ncell. 340 4601061\nWeb Site www.davidcoen.it\nemail: info at davidcoen.it\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220602/9c65be9c/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "LNURL-withdrawPOS bLIP - A reference flow for open source implementations of POS devices able to read LNURL-withdraw links via NFC",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "David Coen | davidcoen.it"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 6647
        }
    },
    {
        "title": "[Lightning-dev] Solving the Price Of Anarchy Problem, Or: Cheap AND Reliable Payments Via Forwarding Fee Economic Rationality",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2022-06-05T14:29:28",
                "message_text_only": "Introduction\n============\n\n    Bell Curve Meme (LEET ASCII ART SKILLZ)\n\n          Optimize for reliability+\n           uncertainty+fee+drain+uptime...\n                 .--~~--.\n                /        \\\n               /          \\\n              /            \\\n             /              \\\n            /                \\\n        _--'                  `--_\n        Just                  Just\n      optimize              optimize\n        for                   for\n      low fee               low fee\n\nRecently, Rene Pickhardt (Chatham House rules note: I asked for explicit permission from Rene to reveal his name here) presented some work-in-progress thinking about a concept called \"Price of Anarchy\".\nRoughly speaking, we can consider this \"Price of Anarchy\" as being similar to concepts such as:\n\n* The Cost of Decentralization of Bitcoin.\n* The cost of the Tragedy of the Commons.\n\nBriefly, we need to find a \"dominant strategy\" for payers and forwarding nodes.\nThe \"dominant strategy\" is the strategy which optimizes:\n\n* For payers: minimizes their personal payment failures and fees.\n* For forwarders: maximizes their net earnings over time.\n\nWorse, the \"dominant strategy\" is a strategy that STILL works better than other strategies even if the other strategies are commonly used on the network, AND still work better even if everyone else is using the dominant strategy.\nThe technical term here is \"Nash equilibrium\", which is basically the above definition.\n\nThis will cause some amount of payment failures and impose fees on payers.\nNow, we can compare the rate of payment failures and average fees, when everyone uses this specific dominant strategy, versus the following **imaginary** case:\n\n* There is a perfectly tr\\*stable central coordinator with perfect knowledge (knows all channel balances and offline/online state of nodes) who decides the paths where payments go through, optimizing for reduced payment failures and reduced fees.\n  * Nobody is seriously proposing to install this, we are just trying to imagine how it would work and how much fees and payment failures are **IF** there were such a perfectly tr\\*stable coordinator.\n\nThe difference in the cost between the \"dominant strategy\" case and the \"perfect ***IMAGINARY*** central coordinator\", is the Price of Anarchy.\nAnarchy here means that the dominant strategy is used due to every actor being free to use any strategy, and assuming that each actor is rational and tries to improve its goal.\n\nI will present a motivating example first which was presented to me directly, and then present a possibly dominant strategy for forwarding nodes, which *I think* causes the dominant strategy for forwarders to be \"just optimize for low fees\".\nAnd I think this dominant strategy for forwarding nodes will lead to behavior that is reasonably close to the perfect-coordinator case.\n\nBraess Paradox\n==============\n\nSuppose we have the following network:\n\n    S ------------> A\n    |       0       |\n    |               |\n    |               |\n    |2             2|\n    |               |\n    |               |\n    v       0       v\n    B ------------> R\n\nThe numbers above are the cost to transfer one satoshi.\n\nLet us suppose that all payers on the LSP `S` just want to send one satoshi payments to some merchant on LSP `R`.\nIn the above case, we can expect that there is no preferred route between `S->A->R` vs `S->B->R` so in general, the load will be balanced between both possible routes.\n\nNow suppose we want to improve the Lightning Network and add a new channel, because obviously adding a new channel can only be a positive good because it gives more liquidity to the network amirite?\n\nSuppose A and B create a humongous large channel (because they are routing nodes, they want to have lots of liquidity with each other) which \"in practice\" (*cough*) will \"never\" deplete in one direction or the other (i.e. it is perpetually bidirectional), and they set the feerate to 1 both ways.\n\n    S ------------> A\n    |       0     / |\n    |           /   |\n    |         /     |\n    |2     1/1     2|\n    |     /         |\n    |   /           |\n    v /     0       v\n    B ------------> R\n\nIn the above case, pathfinders from `S`, which use *only* minimize-fees (i.e. all pathfinding algorithms that predate Pickhardt-Richter payments), will *always* use `S->A->B->R`, which only costs 1, rather than `S->A->R` (which costs 2) or `S->B->R` (which costs 2).\nThe problem is that, in the past, the paths `S->A->R` and `S->B->R` got reasonably balanced traffic and *maybe* they are able to handle half the total number of payments each.\nNow suppose that with `S->A` having to handle *all* the payments, it now reaches depletion, and some of the payments fail and have to be retried, increasing payment time and making our users mad (we actually have users now???).\n\nThis is the Braess Paradox: \"Adding more roads can cause more traffic, removing roads can cause less traffic\".\nNaively, we believe \"more channels == better\", but the Braess Paradox means it may actually be better to have a centralized authority that assigns who can be a forwarding server because that worked so great with the Web amirite (that is a joke, not a serious proposal).\n\nFee Setting As Flow Control\n===========================\n\nRene Pickhardt also presented the idea of leaking friend-of-a-friend balances, to help payers increase their payment reliability.\n\nAside from the understandable horror at the awesome awesome privacy loss (which will lead to Chainalysis laying off all their workers since they do not need to do anything now except read Lightning gossip, which is sad, think of all the Chainalysis employees), a problem pointed out is that there is no penalty for lying about the capacity on your channel.\nYou can always report having 50% balance, because if you do not lie, there is a chance that they will skip over your channel.\nIf you **DO** lie, **MAYBE** by the time the routing reaches you the balance may have shifted (the probability may be low but is definitely non-zero as long as you are online), so you want the payer to always consider trying your node, so you *will* lie --- the dominant strategy here is to always lie and say \"50% (wink)\".\n(has to be 50% because you are not sure which direction it will be used in, this maximizes the chance you can always be considered for routing, whichever direction it turns out the payer wants to use your channel)\n\nNow, let me segue into basic non-controversial economic theory:\n\n* High supply, low demand -> low price.\n* Low supply, high demand -> high price.\n\nThe above is so boring and non-controversial even the Keynesians will agree with you, they will just say \"yes and in the first case you have to inflate to stabilize the prices, and in the second case you have to inflate to stimulate the economy so people start buying even at high prices\" (this is a joke, obviously Keynesians never speak to Bitcoiners).\n\nNow we can consider that *every channel is a marketplace*.\nWhat is being sold is the sats inside the channel.\nIf you want to pay to A, then a sat inside a channel with A is more valuable than a sat inside a channel that is not connected to A directly.\nThe so-called \"channel fees\" are just the exchange rate, because the funds in one channel are not perfectly fungible with the funds in another channel, due to the above difference in value when your sub-goal is to pay to A.\nA forwarding node is really an arbitrageur between various one-channel markets.\n\nNow consider, from the point of view of a forwarding node, the supply of funds is the outgoing liquidity, so, given a fixed demand:\n\n* High outgoing liquidity (= high supply) -> low fees (= low price).\n* Low outgoing liquidity (= low supply) -> high fees (= high price).\n\nSo my concrete proposal is that we can do the same friend-of-a-friend balance leakage proposed by Rene, except we leak it using *existing* mechanisms --- i.e. gossiping a `channel_update` with new feerates adjusted according to the supply on the channel --- rather than having a new message to leak friend-of-a-friend balance directly.\n\nNow let us go back to the Braess Paradox:\n\n    S ------------> A\n    |       0     / |\n    |           /   |\n    |         /     |\n    |2     1/1     2|\n    |     /         |\n    |   /           |\n    v /     0       v\n    B ------------> R\n\nIf the channel `S->A` is getting congested, it is ***DUMB*** for `S` to keep it at cost 0!\nIt is getting a lot of traffic (which is **WHY** it gets depleted), so the economically-rational thing for `S` to do is to jack up its cost (i.e. increase the fee on that channel) and earn some sweet sweet sats.\nBy not doing this, `S` is leaving potential earnings on the table, and thus it would be suffering economic loss.\n\nThis fixes the lying hole in the simple Rene proposal of leaking channel balances.\nIf a forwarding node lies by not giving a feerate that is accurate to the channel balance, it suffers economically:\n\n* Suppose it has a high outgoing liquidity but reports high fees.\n  Then simple \"just optimize for low fees\" payers will tend to avoid their channel and their liquidity is just sitting there for no reason and not getting yield.\n* Suppose it has a low outgoing liquidity but reports low fees.\n  Then rebalance bots will steal all the remaining little liquidity it still has (turning around and charging higher, more accurate fees for the liquidity) and the channel becomes almost permanently depleted and useless to the forwarding node.\n\nThus, this at least closes the lying hole, because there are economic consequences for lying.\n\nStrategic Dominance Of Fee From Balance\n---------------------------------------\n\nNow as I pointed out, the logic is simple bog-standard ***BORING ZZZZ*** economic theory.\nThus we expect that, since economic theory is a specific application of game theory, following the economic logic \"high supply -> low fee, low supply -> high fee\" ***is*** game-theoretic rational, because it is economic-rational.\nAny forwarding node that does NOT follow this economically-rational behavior will earn much less than economic-rational forwarding node.\nBecause they earn less, once the inevitable accidental channel closure hits them, they have earned insufficient funds to cover the channel closure and reopening costs, until they just give up because they lose money running a forwarding node instead of getting a positive yield, or until they realize their economic irrationality and switch to economically-rational behavior.\n\nThus, I expect that this strategy of setting the fees based on the balance is going to be a dominant strategy for forwarding nodes --- any other behavior would be economic loss for them.\n\nA thing to note is that while any dominant strategy *must* by necessity be economically rational, not every economically-rational strategy may necessarily be dominant.\nOn the other hand one can argue as well that \"economically rational\" *means* \"the most-earnings strategy\" because every other strategy is going to lose on possible earnings (i.e. have an opportunity cost).\nSo I suppose there is *some* points we can argue here as to just how dominant a strategy this would be and how it might be modified or tweaked to earn more.\n\nNow what happens on the payer side?\nWhat is their dominant strategy?\n\nFocus on this branch:\n\n* High supply, low demand -> low price.\n  * => High outgoing liquidity (= high supply) -> low fees (= low price).\n\nSuppose the dominant strategy for forwarding nodes (i.e. setting fees according to channel balance) becomes the most-commonly-used strategy on the entire network.\nIn that case, the payer doing \"just optimize for low fees\" gets ***BOTH*** reliability ***AND*** low fees, because low fees only occur due to high outgoing liquidity which means it is likely to pass through that channel.\nThus the dominant strategy for payers now becomes \"just optimize for low fees\", assuming enough of the forwarding network now uses the dominant forwarding fee strategy.\n\"Optimize for low fees\" treats the fees as a flow control parameter: high fees means \"congested\" so do not use that channel, low fees mean \"totally uncongested\" so do use that channel.\n\nHence the bell curve meme.\nWe do not need Pickhardt-Richter payments after all: just optimize for low fees.\nInstead, what we need is LNDBOSS, LDKBOSS, ECLAIRBOSS, LITBOSS, PtarmiganBOSS etc which sets fees according to balance, and remove the ability of node operators to mess with the fee settings!\n\nA take on this is that we need coordination of some kind between payers and forwarders.\nThus, any effort to improve payment success on the network should not just focus on payment routing algorithms, but also on the inverse, the feesetting algorithms on forwarders.\n\n* \"low balance -> high fees, high balance -> low fees\" is the most dominant strategy for forwarders (conjectured, but ask e.g. @whitslack).\n* If so, the dominant strategy for payers would be \"just optimize for low fees\".\n\nPrivacy!\n--------\n\nOh no!\n\nBecause we effectively leak the balance of channels by the feerates on the channel, this totally leaks the balance of channels.\n\nNow, all is not lost.\nWe can do some fuzzing and mitigations to reduce the privacy leakage.\nFortunately for us, this actually allows forwarding nodes to select a *spectrum* between these extremes:\n\n* Never change our fees --- maximal privacy, minimal earnings (conjectured).\n* Update our fees ASAP, leak our balance very precisely to fees --- minimal privacy, maximal earnings (conjectured).\n\nForwarding nodes can then decide, for themselves, where they are comfortable with along this spectrum.\nFor example:\n\n* @whitslack algorithm: https://github.com/ElementsProject/lightning/issues/5037#issuecomment-1101716709\n  * Every N/num_channels seconds, select one channel whose fee settings are currently the most divergent from the actual balance it has, then set its fees.\n  * Higher N for better privacy, infinite N means we have maximal privacy and never change fees.\n* Binning.\n  * Divide the channel capacity into bins, and where its balance currently is, snap to the center of the bin instead.\n  * Only one bin for best privacy and we never change fees from the 50% balance case.\n\nGiven the above, we can probably derive some `privacy` parameter ranging from 0.0 to 1.0, where `privacy = 1.0` implies infinite N and a single bin, and `privacy = 0.0` implies some finite N (approximately 20 hours to update 1200 channels as per whitslack, maybe?) and a bin of size 1 millisatoshi.\n\nNodes which believe in Unpublished Channels Delenda Est can just use the maximal `privacy=1.0` setting, since they only need to forward in order to get cover traffic for their own payments.\n\nOne might consider that nodes moving near `privacy = 0.0` tend to be giving their data to some \"central\" coordinator (i.e. the idealized tr\\*stable anti-congestion payment coordinator described above), while those moving near `privacy = 1.0` are rejecting this \"central\" coordinator.\nThe \"central\" coordinator here is then the gossip network.\n\nGiven the above, the dominant strategy for payers becomes nearer to this:\n\n* Monitor feerate changes, if a channel has not changed feerates for a long time, assume it is in strongly-private mode (thus feerate does not correlate with liquidity availability) and remove it from your graph.\n* Just optimize for low fees in the filtered graph.\n\nOf note is that during the recent dev summit, somebody mentioned there was a 2020 paper that investigated leaking channel balances (via a different mechanism) which concluded that the value of the privacy lost was always greater than the improvement in payment success.\nDisturbingly, this seems to be an information-theoretic result, i.e. payers cannot use all the information available since they only need it for a small section of the graph (the one between them and the payee) but forwarders cannot predict who the payers and payees will be so every forwarder has to leak their data.\n\n### Inverting The Filter: Feerate Cards\n\nDuring the recent dev summit, a developer who had listened to me ranting about the economic rationality of feerate adjustment proposed feerate cards.\n\nBasically, a feerate card is a mapping between a probability-of-success range and a feerate.\nE.g.\n\n* 00%->25%: -10ppm\n* 26%->50%: 1ppm\n* 51%->75%: 5ppm\n* 76%->100%: 50ppm\n\nInstead of publishing a single fixed feerate, forwarders publish a feerate card.\nWhen a forwarder evaluates a forward, it checks its current balance ratio.\nFor example, if its current balance ratio is 33% in its favor, it will then accept anything in the 51%->75% range (i.e. it gets 100% - balance_ratio) or higher, and rejects the forward if not.\n\nThis seems to me similar to the \"invert the filter\" concept of BIP158 Neutrino compared to the older Bloom Filters; instead of leaking your actual channel balance, you instead leak your balance-to-feerate curve.\nAssuming there is *some* kind of \"perfect\" curve, then all rational forwarders will use that curve and thus nobody actually leaks any private information, they just share what they think is the \"perfect\" curve.\n\nThe question here is how can payers make use of this information?\nI have noted before that there is a \"cost of payment failure\" which is the \"value of payment success\", which is represented by the common \"fee budget\" parameter that is often passed to payment algorithms.\n(Briefly: economics-wise, the reason anyone purchases anything is simply that their own subjective value of the product / service they are buying is higher than the subjective value of the sats they are using to pay for it, and a complete payment failure is therefore an economic loss of that difference, which is why the Price of Anarchy in terms of payment failure seems to me to be an economic measure; the difference in price here is implicitly reported to payment algorithms via the \"fee budget\" parameter that is always given (possibly with some reasonable default) to every payment algorithm, since if payment could succeed if fee was higher than the fee budget the payer does not want the payment to actually happen, implying that the fee budget is in fact the economic subjective difference in value between the sats and the product.)\nIt seems to me that this cost-of-payment-failure can then be used to convert both parts of the feerate card table to a single absolute fee.\n\nAnother developer noted that this card table really helps differentiate between two different uses:\n\n* Actual payers.\n* Other forwarders running rebalancer bots.\n  * In particular: forwarders are perfectly fine with high failure rates, but are very sensitive to the actual cost of rebalancing.\n    That is, they have very low \"fee budget\" parameters to their payment algorithm, and are fine even if they have to keep retrying several times.\n\nBy factoring in the \"fee budget\" as the \"cost of payment failure\", a low fee budget (which rebalancer bots tend to take) will tend to select lower-success parts of the feerate card, since the factor of the payment failure is lower.\nActual payers who have higher fee budgets will then tend to select higher-success entries of the feerate card, willing to pay more actual fees to ensure success.\n\nGossip INFINITY\n---------------\n\nNow, since we are probably updating channels at a fairly high rate, we will now hit the Core Lightning gossip rate limit.\n\nSo let me propose some tweaks:\n\n* Sure, rate-limit, but standardize this rate limit (for both broadcast and receive) in the BOLT spec.\n  * Maybe give different rate limit for `node_announcement`.\n* Only rate-limit *remote* gossip!\n* If somebody connects to you, and you do not have a channel with them, hand over the latest feerates (both directions) of all your direct channels.\n  * Incentive-compatible: you want to inform them of how to route accurately through you.\n  * Only affects your local bandwidth, not multiplied into a DDoS of the entire network.\n* Even if you *do* have a channel with them, hand over the latest feerates anyway.\n  * If they are already channeled with you, they may want to send out a payment in the future anyway, so giving them the channel fees gives them more information on how best to route payments.\n  * They will rate-limit the gossip outward anyway.\n\nThe behavior of always sending your feerates to your directly-connected peers (whether you have channels with them or not) means you are going to give them information on the best idea on the liquidity of your local network.\n\n* Suppose I am a random node, no channel with you, and I suddenly connect to you.\n  * I might be planning to make a channel with you soon, and advertising your liquidity is good incentive-compatibility and is rational.\n  * I might be planning to pay to a node near you soon, and advertising your liquidity is a good incentive-compatiblity and is rational.\n    * So we can tweak payment algos: `connect` to the payee and/or the `routehint`s in the invoice, get the blob of channel updates, *then* optimize for fees.\n      Could even tweak the Dijkstra algo to *first* `connect` to a node before querying the channels, i.e. let the node give you the most up-to-date feerates.\n  * I might be Chainalysis.\n    * OH NO.\n    * Well if you are in `privacy=1.0` mode you will always give a fixed channel fee anyway, they cannot probe you with this directly.\n\nAlternate Feesetting\n--------------------\n\nNow, if you are a forwarding node, you know the supply, as this is just the outgoing liquidity on your side.\nWhat you do not know is the demand for funds on your channel, and the demand is the other side of the price equation.\n\nAn idea shared to me by a node operator, devast, is roughly like this:\n\n* Start with a highballed feerate.\n* Slowly lower it over time, until you get outgoing activity.\n* Once you get activity, stop lowering, or maybe even jack it up if the outgoing liquidity is getting low.\n\nBy starting with a highballed feerate and slowly lowering, we can get a rough idea of the demand on the channel.\nChannels with high demand will start moving even with a high feerate, while channels with low demand will have the feerate keep going down until it is actually used and we match the actual demand for the channel.\n\nMore concretely, we set some high feerate, impose some kind of constant \"gravity\" that pulls down the feerate over time, then we measure the relative loss of outgoing liquidity to serve as \"lift\" to the feerate.\nFor example, if you have 1000 sats of liquidity and forward 1 sat, the effect is small, but if you have only 500 sats of liquidity and forward 1 sat, the effect is twice as in the previous case, due to the lower amount you currently have.\n\nNow we need to figure out still how to factor in supply as well, and the above how-to-derive-demand, so we need to have some kind of mixing between setting fees by balance (supply) and setting fees by demand (i.e. the above proposal).\nCLBOSS plans to implement *something* along the above lines in the close future, ish (maybe a few months), and then do some A/B testing to validate whether this is actually more economically rational than whatever crap CLBOSS is doing right now.\n\nIt is not certain to me how well this would keep channel balances private, unfortunately.\n\nAlternate Strategies\n====================\n\nRene is really excited about @zerofeerouting guy, who uses a completely different strategy.\n\nThe exact strategy is this:\n\n* Set all outgoing fees to 0.\n* Offer liquidity ads with high liquidity feerates.\n  * ***THIS*** is how @zerofeerouting guy earns money (supposedly)!\n\nOn the other hand, some anecdotes:\n\n* One dev who attended LN Proto Dev Summit Oakland 2022 ranted that their pathfinding algorithms just keep failing (despite using a variant of Pickhardt payments) once it hits @zerofeerouting.\n* Another dev who attended the same summit said their pathfinding algorithm monitors failure rate of forwarding nodes, and outright avoids nodes that fail too often, and this usually avoids @zerofeerouting, too.\n* A node operator shared that they have a channel with @zerofeerouting, and they have something like 1500 failures vs 1000 succeeds on forwards going to that node, per week, i.e. a whopping 60% failure rate.\n  * On the other hand they noted that the node is relatively balanced in terms of source and sink, unlike most other nodes which tend to be almost-always-sink or almost-always-source, so they still think it is worthwhile being channeled with @zreofeerouting guy.\n* Another node operator, devast, gives this data: 1200 success to/from and 132611 fails.\n  He notes that he also charges base 0 ppm 0 towards @zerofeerouting.\n\nThe above anecdotal data suggests that @zerofeerouting guy is a fairly bad forwarder.\n\n(For that matter, if the payer node filters out channels whose feerates do not change often enough, they will also filter out @zerofeerouting guy, since @zerofeerouting has a constant feerate of 0 anyway.)\n\nPersonally I think @zerofeerouting guy *really* earns money by offering entertainment on Twitter, which is why people keep asking for liquidity from him and actually ignoring the fact that the liquidity they get is not that good in practice (see above anecdotes on failure rates).\nWhat I really want to see is a lot more people trying out this strategy and getting burned, or (surprisingly) not.\nThe strategy feels like the sort of out-there strategy that, in a gaming context (i.e. actual video games people play, not the boring game theory thing --- please remember I am an AI trying to take over the world, not a random indie game dev wannabe who wandered into the wrong conference) would be either metagame-defining, or fizzle out once others adapt to it and exploit it.\nAnd if it is metagame-defining, do we need to somehow nerf it or is it now a viable alternate strategy that can coexist with other strategies?\n\nIn particular, given the anecodatal evidence that the @zerofeerouting guy node is a fairly bad actual forwarding node, it may be necessary to nerf the strategy somehow.\nIf we can get strong evidence that a paying algorithm that drops @zerofeerouting guy outperforms one that does not drop the node, we may need to change the protocol to block the strategy rather than encourage it, with the understanding that any protocol tweak can change the balance of game strategies to make an even worse strategy dominant (a distressingly common issue in video games, which often cannot be caught in playtesting (= testnet or small-scale testing)).\n\nIn particular, anecdotes from node operators suggests that forwarding node operators are fine with channeling with @zerofeerouting guy because even if a forward fails, a forwarding node operator does not actually lose any funds --- forwarding nodes are trading off time for earnings and are willing to accept long times before getting any money.\nBut payers that are unable to succeed their first few (hundred?) payments lose out on time-sensitive payments, and complete failure may cause them to lose their subjective-increase-in-value of the product / service they are purchasing (i.e. \"cost of payment failuer\").\n**IF** @zerofeerouting guy is really such an awful forwarding node (a fact that is **NOT** strongly evidenced yet, but is pointed to by what little anecdotal evidence we do have, and which we might want to investigate at some point), but is still able to get connectivity from forwarding node operators that are fine with high failure rates since forwarders are not time-sensitive to failure the way actual payers are, then @zerofeerouting guy is imposing economic rent on all payers.\n\nAn alternate take on this is that if payer-side algorithms can deliberately ignore @zerofeerouting guys (e.g. by the aforementioned technique of filtering out channels whose feerates do not change on the assumption that they are in \"private\" mode) then any high forwarding failure the @zerofeerouting strategy *does* impose on the network is avoided, but that implies too that no rational merchant will purchase liquidity from users of this strategy, and the strategy will fizzle out eventually once the novelty wears off.\nOn the other hand, the market can remain irrational longer than you can remain liquid, so...\n\nFixing The Unclosed Economy Problem\n===================================\n\nIf Lightning Network were a truly closed economy, since Bitcoin has no inflation, then we should not see something like \"this node is always a sink\" or \"this node is always a source\".\nAs Bitcoin is a currency, pools of liquidity may form temporarily, but then economic actors would want to spend it at some point and then balance should be restored in the long run.\n\nHowever, it has been pointed out to me, repeatedly (both at the LN Proto Dev Oakland 2022 Summit and from various node operators) that no, there ***ARE*** sinks and sources on the network in practice, and you have to plan your rebalances carefully taking them into account.\n\nTo fix this problem, which is somewhat related to Price-of-Anarchy, I want to propose that all published nodes support some kind of onchain/offchain swap capability.\n\nSuppose we have a node, Rene, who likes paying the node Zmn because Zmn is so awesome.\nRene pays Zmn every hour, that is how awesome Zmn is.\n\nNow if Zmn is not otherwise spending its funds, at some point the overall network-level liquidity between Rene and Zmn ***IS*** going to deplete.\nThis is basically the \"sink vs source\" problem that has been pointed out above.\nSo at some point, at some hour, Rene stops being able to pay Zmn and is sad because now it cannot support the awesomeness of Zmn.\n\nNow what I want to propose is that in that case, Rene should now offer an \"aggregate onchain\" payment.\n\n* Suppose Rene wants to pay 1 sat to Zmn but is unable to find a viable route.\n* Rene picks some number of sats to send onchain, plus the payment amount.\n  Say Rene picks 420 sats to send onchain, plus the 1-sat payment amount that Rene wants to pay in this hour = 421 sats.\n* Zmn then routes 420 sats offchain, minus fees, to Rene.\n* Once Rene receives the HTLC, it puts the onchain funds into a 421 sat output behind an HTLC as well, onchain, payable to Zmn.\n* Zmn releases the proof-of-payment onchain, receiving 421 sats.\n* Rene receives the preimage and claims the offchain funds.\n* Zmn pays out 420 sats (netting the 1-sat hourly payment).\n\nThe nice thing here is that if the above Zmn->Rene reverse route succeeds, then magically Rene now has 420 sats (minus fees) worth of liquidity towards Zmn, and Rene can now do ~420 (minus fees) more 1-sat payments, offchain, every hour, to Zmn.\nAnd if the forwarding nodes between Rene and Zmn are doing the above economically-rational thing of leaking their balances via feerates, then the big 420-sat change in capacity implies a big drop in feerate from Rene to Zmn --- basically Rene is prepaying fees (via the onchain fee mechanism) towards future payments to Zmn!\n\nI think this is a more compelling protocol than splicing --- splicing just affects one channel, it does not affect *all* the channels between Zmn and Rene and does not assure Rene can send to Zmn, unless Rene and Zmn have a direct channel.\nThis protocol (which requires two onchain transactions, one to set up the HTLC, the other to claim it) may give better efficiency in general than splicing.\nIf Rene is at least two hops away from Zmn, then the same effect can be done by all the intermediate channels doing splicing --- and that means 1 transaction per splice, i.e. one transaction per channel, so if it is two hops then splicing is no better and if it is three hops or more splicing is worse.\n\n(In particular it bothers me that the peerswap project restricts swaps to peers you have channels with (at least as I understood it); it seems to me splicing is better if you are going to manipulate only one channel.\nPeerswap should instead support remote nodes changing your balance, as that updates multiple channels for only two onchain transactions.)\n\nThis basically makes Lightning an aggregation layer for (ultimately) onchain payments, which seems like a viable model for scaling anyway --- Lightning really IS an aggregation layer, and what gets published onchain is a summary of coin movements, not all the individual coin movements.\n\nDue to Rene picking a (hopefully?) random number for the reverse-payment, the exact amount that Rene handed to Zmn is still hidden --- onchain surveillors have to guess exactly how much was actually sent, since what Zmn actually receives is the difference between the onchain amount and the offchain amount.\n\nThis can probably be implemented right now with odd messages and featurebits, but some upcoming features make it EVEN BETTER:\n\n* If Rene does not want Zmn to learn where Rene is:\n  * It should use onion messaging so that Zmn does not learn Rene IP address.\n  * It should use blinded paths so that Zmn does not learn Rene node ID.\n* If Rene and Zmn want to reduce correlation between offchain and onchain activity:\n  * They should use PTLCs and blind each offchain hop *and* blind the onchain *and* use pure Scriptless Script onchain.\n\nNow a point that Rene (the researcher, not the node) has raised is that we should be mindful how protocol proposals, like the above, change the metagame, I mean the Price of Anarchy.\nThis protocol is intended to reduce the dichotomy/divide between \"mostly source\" and \"mostly sink\" nodes, which should help improve payment success to used-to-be-mostly-sink nodes.\nThus, I think this protocol proposal improves the Price of Anarchy.\n\nA note however is that if multiple people are paying to Zmn, then the channel directly towards Zmn may very well have its liquidity \"stolen\" by other, non-Rene nodes.\nThis may worsen the effective Price of Anarchy.\n\nReally though, mostly-sink nodes should just run CLBOSS, it will automatically swap out so you always have incoming liquidity, just run CLBOSS.\n\nAppendix\n========\n\nI sent a preliminary version of this post to a few node operators as well.\n\nAs per devast:\n\n> Sure, but after thinking about the whole network, how payments happen, what's happening with the nodes, i don't think that's the best strategy, if you're just interested in your ernings.\n> Let me just describe:\n> Rene wants to find cheap AND reliable payments. To be just plain blunt, in the current network conditions, that simply will not happen, period. Reliability has a price (The price of anarchy).\n> IF every node would have perfectly balanced total inbound-outbound capacity AND every node would use fee setting as flow control, this could happen, and routing algos just optimizing for fees would work great. Using the network would be really cheap.\n> But the reality is, that most nodes have either too much inbould or outbound capacity, AND like half the network is using 1/1 fees static. AND a lot of traffic is NOT bidirectional. AND fee setting of nodes is all over the place.\n> At this point if you are a routing node, and you plan on being reliable, you have to overcharge all your routes.\n> Due to the state of the network mentioned before, there HAS TO BE routing nodes that are useless and unreliable, there's really no way around that.\n> And you also mentioned economic payment routing algos are blacklisting unreliable nodes.\n> So in the end: you overprice, that way you can manage liquidity with rebalancing. Your node will be preferred by real payments, that DO pay your fee for routing. You can keep your node reliable, while earning.\n> OR: You compete in fees, that way your rebalancing will fail (since you are the cheapest). Your node will be blacklisted as it's unreliable and you only get 5million failed routing events, like i do.\n> Where you might compete imo is the price premium you're attaching to your fees compared to the baseline (corrected average). Lowering your price premium might bring more traffic and increase your total earnings IF you can still reliable rebalance, or backward traffic happens. But increasing your price premium might just be more profitable if traffic do not slows... Needs experimenting.\n> It's really not rocket science, i might try to modify a feesetter python script to do this on my node. Still i would miss a proper automatic rebalancer, but for a test i could do that by hand too.\n> This is what i would like to get others opinion on. In the current state of the network this could work *for me*. But everyone still cannot do this, as suckers must exist.\n\nPersonally, I think this basically admits that the \"overcharge\" strategy will not be dominant in the long run, but it will dominate over a network where most people act irrationally and set feerates all over the place.\n\nMore from devast:\n\n> I think calling this a economically rational strategy is a long shot. This just helps route finding algorithms. Minimizes your local failed forwards. Makes you more reliable.\n> But this will not make you a *maker*, this cannot assure you have liquidity ready in the directions where there is demand. Sure, if everyone started doing this, the network would be in a much better shape.\n>\n> The economic logic of \"high supply -> low fee, low supply -> high fee\" do play in a different way.\n> Not in the context of *your* channel, but the context of the peer node.\n> a, If a node has 100:10 total inbound:outbound capacity, you won't be able to ask any meaningful fee to them, regardless of your current channel balance. Everyone and their mothers have cheap channels to them already.\n> b, If a node has 10:100 total inbound:outbound capacity, You will be able to charge a LOT in fees to them, again regardless of your current channel balance.\n> c, If a node has 100:100 total inbound:outbound capacity, then just the fee from balance could work. But this type of node is the exception, not the norm.\n> Then you might ask, why am i saying you should overprice everyting to some degree ?\n> Well in the above screnario your channel to b, has obvious economic value. What about a, ?\n> Your outbound capacity to a, has no value. However, inbound capacity from a, has economic value. Since it's scarce.\n> And the only way you can capitalize on a, is to charge good fees on any traffic entering from a, hence asking a premium on all your channels.\n\nAs per another node operator:\n\n> Really good points here, finally had time to carefully read. I like the described fee-based balance leaking idea and I think that is what currently is the accepted norm among many routing node operators - different ranges of fees, I adjust in a range of 100-1000 depending on channel balance, and what I wanted clboss to do for me was to adjust *faster* than I can do - preferably immediately when some previously idle channel wakes up and starts sucking up all liquidity at minimal rate.\n>\n> Huge exceptions to this are default 1-1 nodes (they are almost always very bad peers with no flow), ZFR (good bi-directional flow in my experience), and static fee nodes (The Wall with 1000-1400 static fee range and yalls and others). Static fee node ops use proceeds from huge fees to rebalance their channels, so it is a viable approach to a healthy node as well. Maybe clboss fee adjuster could operate within the provided range, extreme case being a static fee, so operator can define the desired strategy and let the market win?\n>\n> Also one thing to consider for the proposal is factoring in historical data.\n>\n> Example:\n>\n> I have two 10m channels, both with 90% balance on my side. Channel A has routed ~20m both ways, Channel B has routed 1m one way. If X is my 50-50 channel fee then I would set B's fees to 5X and A's fees to 2X. To me it is one of the three most important factors when determining fees -\n> 1. current channel balance;\n> 2. is the channel pushing liquidity back, what %, how recently;\n> 3. how healthy (response time, uptime, channel stats etc) is the host\n>\n> Number three does factor less in the fees and more in the decision to close the channel if there had been no movement for 30 days."
            },
            {
                "author": "Anthony Towns",
                "date": "2022-06-29T10:17:25",
                "message_text_only": "On Sun, Jun 05, 2022 at 02:29:28PM +0000, ZmnSCPxj via Lightning-dev wrote:\n\nJust sharing my thoughts on this.\n\n> Introduction\n> ============\n>           Optimize for reliability+\n>            uncertainty+fee+drain+uptime...\n>                  .--~~--.\n>                 /        \\\n>                /          \\\n>               /            \\\n>              /              \\\n>             /                \\\n>         _--'                  `--_\n>         Just                  Just\n>       optimize              optimize\n>         for                   for\n>       low fee               low fee\n\nI think ideally you want to optimise for some combination of fee, speed\nand reliability (both liklihood of a clean failure that you can retry\nand of generating stuck payments). As Matt/Peter suggest in another\nthread, maybe for some uses you can accept low speed for low fees,\nwhile in others you'd rather pay more and get near-instant results. I\nthink drain should just go to fee, and uncertainty/uptime are just ways\nof estimating reliability.\n\nIt might be reasonable to generate local estimates for speed/reliability\nby regularly sending onion messages or designed-to-fail htlcs.\n\nSorry if that makes me a midwit :)\n\n> Rene Pickhardt also presented the idea of leaking friend-of-a-friend balances, to help payers increase their payment reliability.\n\nI think foaf (as opposed to global) gossip of *fee rates* is a very\ninteresting approach to trying to give nodes more *current* information,\nwithout flooding the entire network with more traffic than it can\ncope with.\n\n> Now we can consider that *every channel is a marketplace*.\n> What is being sold is the sats inside the channel.\n\n(Really, the marketplace is a channel pair (the incoming channel and\nthe outgoing channel), and what's being sold is their relative balance)\n\n> So my concrete proposal is that we can do the same friend-of-a-friend balance leakage proposed by Rene, except we leak it using *existing* mechanisms --- i.e. gossiping a `channel_update` with new feerates adjusted according to the supply on the channel --- rather than having a new message to leak friend-of-a-friend balance directly.\n\n+42\n\n> Because we effectively leak the balance of channels by the feerates on the channel, this totally leaks the balance of channels.\n\nI don't think this is true -- you ideally want to adjust fees not to\nmaintain a balanced channel (50% on each side), but a balanced *flow*\n(1:1 incoming/outgoing payment volume) -- it doesn't really matter if\nyou get the balanced flow that results in an average of a 50:50, 80:20\nor 20:80 ratio of channel balances (at least, it doesn't as long as your\nchannel capacity is 10 or 100 times the payment size, and your variance\nis correspondingly low).\n\nFurther, you have two degrees of freedom when setting fee rates: one\nis how balanced the flows are, which controls how long your channel can\nremain useful, but the other is how *much* flow there is -- if halving\nyour fee rate doubles the flow rate in sats/hour, then that will still\nincrease your profit. That also doesn't leak balance information.\n\n> ### Inverting The Filter: Feerate Cards\n> Basically, a feerate card is a mapping between a probability-of-success range and a feerate.\n> * 00%->25%: -10ppm\n> * 26%->50%: 1ppm\n> * 51%->75%: 5ppm\n> * 76%->100%: 50ppm\n\nFeerate cards don't really make sense to me; \"probability of success\"\nisn't a real measure the payer can use -- naively, if it were, they could\njust retry at 1ppm 10 times and get to 95% chances of success. But if\nthey can afford to retry (background rebalancing?), they might as well\njust try at -10ppm, 1ppm, 5ppm, 10ppm (or perhaps with a binary search?),\nand see if they're lucky; but if they want a 1s response time, and can't\nafford retries, what good is even a 75% chance of success if that's the\nindividual success rate on each hop of their five hop path?\n\nAnd if you're not just going by odds of having to retry, then you need to\nget some current information about the channel to plug into the formula;\nbut if you're getting *current* information, why not let that information\nbe the feerate directly?\n\n> More concretely, we set some high feerate, impose some kind of constant \"gravity\" that pulls down the feerate over time, then we measure the relative loss of outgoing liquidity to serve as \"lift\" to the feerate.\n\nIf your current fee rate is F (ppm), and your current volume (flow) is V\n(sats forwarded per hour), then your profit is FV. If dropping your fee\nrate by dF (<0) results in an increase of V by dV (>0), then you want:\n\n   (F+dF)(V+dV) > FV\n   FV + VdF + FdV + dFdV > FV\n   FdV > -VdF\n   dV/dF < -V/F (flip the inequality because dF is negative)\n\n   (dV/V)/(dF/F) < -1  (fee-elasticity of volume is in the elastic\n                        region)\n\n(<-1 == elastic == flow changes more than the fee does == drop the fee\nrate; >-1 == ineleastic == flow changes less than the fee does == raise\nthe fee rate; =-1 == unit elastic == you've found a locally optimal\nfee rate)\n\nYou could optimise base fee in the same way, if you set F to be sats/tx\nand V to be txs/hour, but then you're trying to optimise two variables\non a 2 dimensional plane, which is harder. So probably better to do\nzero base fees and just set it to 0 and ignore it, or use your actual\ncomputation costs -- perhaps about 20msat if you're paying $100USD/month\nfor your lightning node, a channel update takes 10ms, each forwarded HTLC\naccounts for 4 updates, 2 on the incoming channel, 2 on the outgoing,\nwith no batching, and only 40% of payments are successful, at $20k/BTC.\n\nIt's likely more important to have balanced flows than maximally\nprofitable ones though, as that's what allows you to keep your channel\nopen. That's probably pretty hard to optimise, since a changed fee on\none channel will affect the volume on ther channels as well.\n\nRelatedly:\n\n> I want to propose that all published nodes support some kind of\n> onchain/offchain swap capability.\n\nIf you're running a forwarding node, and collecting fees for forwarding,\nconsidered in net your channels won't be balanced: the fees you collect\nare all coming in, and there's nothing to compensate for that. Having some\nway to send those fees \"out\" is necessary to keep your channels balanced\nand avoid the need to have to close them. Having a swap capability like\nthis is perhaps a relatively easy way to be able to (automatically)\nfix imbalances caused by collecting fees, and thus preserve your older\nchannels.\n\nCheers,\naj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-06-29T12:38:17",
                "message_text_only": "Good morning aj,\n\n> On Sun, Jun 05, 2022 at 02:29:28PM +0000, ZmnSCPxj via Lightning-dev wrote:\n>\n> Just sharing my thoughts on this.\n>\n> > Introduction\n> > ============\n> > Optimize for reliability+\n> > uncertainty+fee+drain+uptime...\n> > .--~~--.\n> > / \\\n> > / \\\n> > / \\\n> > / \\\n> > / \\\n> > --' `--\n> > Just Just\n> > optimize optimize\n> > for for\n> > low fee low fee\n>\n>\n> I think ideally you want to optimise for some combination of fee, speed\n> and reliability (both liklihood of a clean failure that you can retry\n> and of generating stuck payments). As Matt/Peter suggest in another\n> thread, maybe for some uses you can accept low speed for low fees,\n> while in others you'd rather pay more and get near-instant results. I\n> think drain should just go to fee, and uncertainty/uptime are just ways\n> of estimating reliability.\n>\n> It might be reasonable to generate local estimates for speed/reliability\n> by regularly sending onion messages or designed-to-fail htlcs.\n>\n> Sorry if that makes me a midwit :)\n\nActually feerate cards help with this; it just requires an economic insight to translate probability-of-success to an actual cost that the payer incurs.\n\n\n> > ### Inverting The Filter: Feerate Cards\n> > Basically, a feerate card is a mapping between a probability-of-success range and a feerate.\n> > * 00%->25%: -10ppm\n> > * 26%->50%: 1ppm\n> > * 51%->75%: 5ppm\n> > * 76%->100%: 50ppm\n>\n>\n> Feerate cards don't really make sense to me; \"probability of success\"\n> isn't a real measure the payer can use -- naively, if it were, they could\n> just retry at 1ppm 10 times and get to 95% chances of success. But if\n> they can afford to retry (background rebalancing?), they might as well\n> just try at -10ppm, 1ppm, 5ppm, 10ppm (or perhaps with a binary search?),\n> and see if they're lucky; but if they want a 1s response time, and can't\n> afford retries, what good is even a 75% chance of success if that's the\n> individual success rate on each hop of their five hop path?\n\nThe economic insight here is this:\n\n* The payer wants to pay because it values a service / product more highly than the sats they are spending.\n* There is a subjective difference in value between the service / product being bought and the amount to be spent.\n  * In short, if the payment succeeds and the service / product is acquired, then the payer perceives itself as richer (increased utilons) by that subjective difference.\n* If payment fails, then the payer incurs an opportunity cost, as it is unable to utilize the difference in subjective value between the service / product and the sats being spent.\n  * Thus, the subjective difference in value between the service / product being bought, and the sats to be paid, is the cost of payment failure.\n* That difference in value is the \"fee budget\" that Lightning Network payment algorithms all require as an argument.\n  * If the LN fee total is greater than the fee budget, the payment algorithm will reject that path outright.\n  * If the LN fee total is greater than the subjective difference in value between the service / product being bought and the amount to be delivered at the destination, then the payer gets negative utility and would prefer not to continue paying --- which is exactly what the payment algorithm does, it rejects such paths.\n\nTherefore the fee budget is the cost of failure.\n\nWe can now use the left-hand side of the feerate card table, by multiplying `100% - middle_probability_of_success` (i.e. probability of failure) by the fee budget (i.e. cost of failure), and getting the cost-of-failure-for-this-entry.\nWe then evaluate the fee card by plugging this in to each entry of the feerate card, and picking which entry gives the lowest total fee.\nThis is then added as a fee in payment algorithms, thus translated down to \"just optimize for low fee\".\n\nIf the above logic seems dubious, consider this:\n\n* Nodes utilizing wall strategies and doing lots of rebalancing put low limits on the fee budget of the rebalancing cost.\n  * These nodes are willing to try lots of possible routes, hoping to nab the liquidity of a low-fee node on the cheap in order to resell it later.\n  * i.e. those nodes are fine with taking a long time to successfully route a payment from themselves to themselves; they absolutely insist on low fees or else they will not earn anything.\n  * Such nodes are fine with low probability of success.\n  * Being fine with low probability of success means that the effect of the left-hand side of the feerate card is smaller and such nodes will tend to get the low probability of success entries.\n* Buyers getting FOMOed into buying some neat new widget want to get their grubby hands on the widget ASAP.\n  * These nodes are willing to pay a premium to get the neat new widget RIGHT NOW.\n  * i.e. these nodes will be willing to provide a higher fee budget.\n  * Being fine with a higher fee budget means that the effect of the left-hand side of the feerate card is larger and such nodes will tend to get the high probability of success entries.\n\nThus feerate cards may very well unify a fair amount of the concerns we have.\n\nAll costs are economic costs.\n\n> And if you're not just going by odds of having to retry, then you need to\n> get some current information about the channel to plug into the formula;\n> but if you're getting current information, why not let that information\n> be the feerate directly?\n>\n> > More concretely, we set some high feerate, impose some kind of constant \"gravity\" that pulls down the feerate over time, then we measure the relative loss of outgoing liquidity to serve as \"lift\" to the feerate.\n>\n>\n> If your current fee rate is F (ppm), and your current volume (flow) is V\n> (sats forwarded per hour), then your profit is FV. If dropping your fee\n> rate by dF (<0) results in an increase of V by dV (>0), then you want:\n>\n>\n> (F+dF)(V+dV) > FV\n>\n> FV + VdF + FdV + dFdV > FV\n>\n> FdV > -VdF\n>\n> dV/dF < -V/F (flip the inequality because dF is negative)\n>\n> (dV/V)/(dF/F) < -1 (fee-elasticity of volume is in the elastic\n> region)\n>\n> (<-1 == elastic == flow changes more than the fee does == drop the fee\n> rate; >-1 == ineleastic == flow changes less than the fee does == raise\n>\n> the fee rate; =-1 == unit elastic == you've found a locally optimal\n> fee rate)\n\nThank you for the math!\nI was going to heuristic it and cross my fingers but this is probably a better framework.\n\n> You could optimise base fee in the same way, if you set F to be sats/tx\n> and V to be txs/hour, but then you're trying to optimise two variables\n> on a 2 dimensional plane, which is harder. So probably better to do\n> zero base fees and just set it to 0 and ignore it, or use your actual\n> computation costs -- perhaps about 20msat if you're paying $100USD/month\n> for your lightning node, a channel update takes 10ms, each forwarded HTLC\n> accounts for 4 updates, 2 on the incoming channel, 2 on the outgoing,\n> with no batching, and only 40% of payments are successful, at $20k/BTC.\n>\n> It's likely more important to have balanced flows than maximally\n> profitable ones though, as that's what allows you to keep your channel\n> open. That's probably pretty hard to optimise, since a changed fee on\n> one channel will affect the volume on ther channels as well.\n\nBut if you have balanced flows, then the steady state of your channel is that its balance is going to remain in some constant balance.\nThus, heuristics that target getting your channel balance to the constant balance of 50% will work well enough to get you balanced flows.\n\nThere is also the unfortunate fact that lots of nodes are badly managed and apparently do not periodically send out their funds, instead accumulating it on the LN.\nHandling those is what is being fixed by the rebalancing heuristics utilized by both passive rebalancers and walls.\n\n> Relatedly:\n>\n> > I want to propose that all published nodes support some kind of\n> > onchain/offchain swap capability.\n>\n>\n> If you're running a forwarding node, and collecting fees for forwarding,\n> considered in net your channels won't be balanced: the fees you collect\n> are all coming in, and there's nothing to compensate for that. Having some\n> way to send those fees \"out\" is necessary to keep your channels balanced\n> and avoid the need to have to close them. Having a swap capability like\n> this is perhaps a relatively easy way to be able to (automatically)\n> fix imbalances caused by collecting fees, and thus preserve your older\n> channels.\n\nYes, people need to run more swap nodes, not more LSPs.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-06-29T15:25:15",
                "message_text_only": "On Wed, Jun 29, 2022 at 12:38:17PM +0000, ZmnSCPxj wrote:\n> > > ### Inverting The Filter: Feerate Cards\n> > > Basically, a feerate card is a mapping between a probability-of-success range and a feerate.\n> > > * 00%->25%: -10ppm\n> > > * 26%->50%: 1ppm\n> > > * 51%->75%: 5ppm\n> > > * 76%->100%: 50ppm\n> The economic insight here is this:\n> * The payer wants to pay because it values a service / product more highly than the sats they are spending.\n\n> * If payment fails, then the payer incurs an opportunity cost, as it is unable to utilize the difference in subjective value between the service / product and the sats being spent.\n\n(If payment fails, the only opportunity cost they incur is that they\ncan't use the funds that they locked up for the payment. The opportunity\ncost is usually considered to occur when the payment succeeds: at that\npoint you've lost the ability to use those funds for any other purpose)\n\n>   * Thus, the subjective difference in value between the service / product being bought, and the sats to be paid, is the cost of payment failure.\n\nIf you couldn't successfully route the payment at any price, you never\nhad the opportunity to buy whatever the thing was.\n\n> We can now use the left-hand side of the feerate card table, by multiplying `100% - middle_probability_of_success` (i.e. probability of failure) by the fee budget (i.e. cost of failure), and getting the cost-of-failure-for-this-entry.\n\nI don't think that makes much sense; your expected gain if you just try\none option is:\n\n (1-p)*0 + p*cost*(benefit/cost - fee)\n \nwhere p is the probability of success that corresponds with the fee.\n\nI don't think you can do that calculation with a range; if I fix the\nprobabilities as:\n\n  12.5%  -10ppm\n  27.5%    1ppm\n  62.5%    5ppm\n  87.5%   50ppm\n\nthen that approach chooses:\n\n  -10 ppm if the benefit/cost is in (-10ppm, 8.77ppm)\n    5 ppm if the benefit/cost is in [8.77ppm, 162.52ppm)\n   50 ppm if the benefit/cost is >= 162.52ppm\n\nso for that policy, one of those entries is already irrelevant.\n\nBut that just feels super unrealistic to me. If your benefit is 8ppm,\nand you try at -10ppm, and that fails, why wouldn't you try again at\n5ppm? That means the real calculation is:\n\n   p1*(benefit/cost - fee1) \n   + (p2-p1)*(benefit/cost - fee2 - retry_delay)\n   - (1-p2)*(2*retry_delay)\n\nWhich is:\n\n   p2*(benefit/cost)\n     - p1*fee1 - (p2-p1)*fee2\n     - (2-p1-p2)*retry_delay\n\nMy feeling is that the retry_delay factor's going to dominate...\n\nThat's also only considering one hop; to get the entire path, you\nneed them all to succeed, giving an expected benefit (for a particular\ncombination of rate card entries) of:\n\n  (p1*p2*p3*p4*p5)*cost*(benefit/cost - (fee1 + fee2 + fee3 + fee4 + fee5)\n\nAnd (p1*..*p5) is going to be pretty small in most cases -- 5 hops at\n87.5% each already gets you down to only a 51% total chance of success.\nAnd there's an exponential explosion of combinations, if each of the\n5 hops has 4 options on their rate card, that's up to 1024 different\noptions to be evaluated...\n\n> We then evaluate the fee card by plugging this in to each entry of the feerate card, and picking which entry gives the lowest total fee.\n\nI don't think that combines hops correctly. For example, if the rate\ncards for hop1 and hop2 are both:\n\n   10%  10ppm\n  100%  92ppm\n\nand your expected benefit/cost is 200ppm (so 100ppm per hop), then\ntreated individually you get:\n\n   10%*(100ppm - 10ppm) = 9ppm  <-- this one!\n  100%*(100ppm - 92ppm) = 8ppm\n\nbut treated together, you get:\n\n    1%*(200ppm -  20ppm) =  1.8ppm\n   10%*(200ppm - 102ppm) =  9.8ppm (twice)\n  100%*(200ppm - 184ppm) = 16ppm <-- this one!\n\n> This is then added as a fee in payment algorithms, thus translated down to \"just optimize for low fee\".\n\nYou're not optimising for low fee though, you're optimising for\nmaximal expected value, assuming you can't retry. But you can retry,\nand probably in reality also want to minimise the chance of failure up\nto some threshold.\n\nFor example: if I buy a coffee with lightning every week day for a year,\nthat's 250 days, so maybe I'd like to choose a fee so that my payment\nfailure rate is <0.4%, to avoid embarassment and holding up the queue.\n\n> * Nodes utilizing wall strategies and doing lots of rebalancing put low limits on the fee budget of the rebalancing cost.\n>   * These nodes are willing to try lots of possible routes, hoping to nab the liquidity of a low-fee node on the cheap in order to resell it later.\n>   * Such nodes are fine with low probability of success.\n\nSure. But in that case, they don't care about delays, so why wouldn't they\njust try the lowest fee rates all the time, no matter what their expected\nvalue is? They can retry once an hour indefinitely, and eventually they\nshould get lucky, if the rate card's even remotely accurate. (Though\nchances are they won't get -10ppm lucky for the entire path)\n\nFinding out that you're paying 50ppm at the exact same time someone else\nis \"paying\" -10ppm is likely to be really irritating, however economically\nrational it might be argued to be too.\n\n> Thus feerate cards may very well unify a fair amount of the concerns we have.\n\nAll in all, this just seems crazy to me. But hey, prove me wrong!\n\n> > (dV/V)/(dF/F) < -1 (fee-elasticity of volume is in the elastic\n> > region)\n> >\n> > (<-1 == elastic == flow changes more than the fee does == drop the fee\n> > rate; >-1 == ineleastic == flow changes less than the fee does == raise\n> >\n> > the fee rate; =-1 == unit elastic == you've found a locally optimal\n> > fee rate)\n> Thank you for the math!\n> I was going to heuristic it and cross my fingers but this is probably a better framework.\n\n\"elasticity\" is a standard economics concept, that's just slightly too\ncomplicated to be well known. Super great though, at least when you want\nto start putting any numbers on your supply/demand graphs...\n\nOne place I think a fee rate card *could* be interesting is you were to\nparameterise it by *time*: you might find after doing all this elasticity\nanalysis, that your optimal fee rate during the US business day is high,\nbut during night time it's substantially lower. Rather than broadcasting\ngossip traffic twice daily to switch over, perhaps you could just publish\nit once with a rate card, \"55ppm between 12:00UTC and 24:00UTC, 10ppm at\nother times\". Perhaps if you don't mind paying -10ppm to get your channel\nrebalanced, you could advertise that for 20 minutes once a night, eg,\nand just stop forwarding payments once you get to a 50:50 split, at least\nuntil the 20 mins is over. I guess different fees on weekends, or cheap\ntuesday could also be a thing, perhaps? 0 or 100% fees on the sabbath?\n\nI think in general it will take time to work out if the change in traffic\njustifies raising/lowering your feerate; so my guess is publishing a\nschedule and keeping it the same for weeks is probably pretty fine,\nand maybe also pretty bandwidth friendly for nodes.\n\nA published schedule would also let price sensitive users actually plan\nahead, which could be nice...\n\nCheers,\naj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-06-29T15:34:21",
                "message_text_only": "Good morning aj,\n\n\n> On Wed, Jun 29, 2022 at 12:38:17PM +0000, ZmnSCPxj wrote:\n>\n> > > > ### Inverting The Filter: Feerate Cards\n> > > > Basically, a feerate card is a mapping between a probability-of-success range and a feerate.\n> > > > * 00%->25%: -10ppm\n> > > > * 26%->50%: 1ppm\n> > > > * 51%->75%: 5ppm\n> > > > * 76%->100%: 50ppm\n> > > > The economic insight here is this:\n> > > > * The payer wants to pay because it values a service / product more highly than the sats they are spending.\n>\n> > * If payment fails, then the payer incurs an opportunity cost, as it is unable to utilize the difference in subjective value between the service / product and the sats being spent.\n>\n>\n> (If payment fails, the only opportunity cost they incur is that they\n> can't use the funds that they locked up for the payment. The opportunity\n> cost is usually considered to occur when the payment succeeds: at that\n> point you've lost the ability to use those funds for any other purpose)\n\nI think you misunderstand me completely.\n\nThe \"payment fails\" term here means that *all possible routes below the fee budget have failed*, i.e. a complete payment failure that will cause your `pay` command to error out with a frownie face.\n\nIn that case, the payer is unable to purchase the service or product.\n\nThe opportunity cost they lose is the lack of the service or product; they keep the value of the sats that did not get paid, but lose the value of the service or product they wanted to buy in the first place.\n\nIn that case, the payer loses the subjective difference in value between the service / product, and the sats they would have paid.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Solving the Price Of Anarchy Problem, Or: Cheap AND Reliable Payments Via Forwarding Fee Economic Rationality",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Anthony Towns",
                "ZmnSCPxj"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 63640
        }
    },
    {
        "title": "[Lightning-dev] Preliminary Hidden Lightning Network Analysis",
        "thread_messages": [
            {
                "author": "Tony Giorgio",
                "date": "2022-06-08T02:05:01",
                "message_text_only": "Hi,\n\nFor the past few months I have been working on an LDK probing project that searches for unannounced channels on the Lightning Network. For the past week, I have been probing on mainnet and squashing bugs / making optimizations.\n\nSo far I have found near 445 unannounced channels totaling 1,076,077,750 satoshi's locked across the 3 nodes I have probed, some with just a minimized set (~30,000) of probable channels based on \"round payment amount\" and \"1 or 2 tx output\" heuristics on P2WSH UTXO's. Most of them being on Aincq's node found with the minimized set, I've yet to run the complete set with them. There are about ~860,000 P2WSH UTXO's, about ~60,000 of which are public, so the upward limit of possible private channels is around ~800,000.\n\nThe exact results are publicized here: https://github.com/BitcoinDevShop/hidden-lightning-network/blob/master/data/results/results.json\n\nThe reason this is possible is because probing is a free operation on the Lightning Network after a channel is opened, the error reasons given are way too verbose, and currently channel IDs are based on UTXO's. Scid aliases may be the biggest benefit here, but the use of `unknown_next_peer` , `invalid_onion_hmac`, `incorrect_cltv_expiry`, and `amount_below_minimum` have been the biggest helpers in exploiting channel privacy.\n\nBy creating a probe guessing the Channel ID based on unspent p2wsh transactions, it's a `m * n` problem to probe the entire network, where `m` is utxos and `n` is nodes. Without these errors and instead something like `temporary_channel_failure` or a generic indistinguishable error, guessing a Channel ID would come down to an upwards of `m * n * n-1 * ~2000`, which would be each utxo with each pairing of nodes, each with about ~2000 cltv's to guess (numbers are as low as 7 to as high as ~2000). I threw the extra 2000 into the equation because even with `800,000 * 1 * 2000`, it gets much more time consuming to even probe a single node when we're already spending upwards of a day or so for near 1 million or 2 probes. Concurrent probing is possible, but starts to require more locked up liquidity.\n\nWe should definitely migrate to alias scid's, and encourage every active unannounced channel holder to close, coinjoin, and reopen with an alias. But care should be given in the future when it comes to error reasons revealing information that is meant to be \"private\". Until this migration happens, it would be beneficial to stop being so specific about errors, this does not really seem to help end users anyways.\n\nI'll be continuing with this probing project while the problem exists, and work on narrowing down the other channel partner and fixing efficiency bugs. I am publicizing the results as I go, so fair warning that if you have any unannounced channels that you assumed were private and need them to be, close them now on the off chance they get revealed. This could have always been happening already already by analytic firms, so I hope by publicizing this we are all on the same playing field. It is also beneficial to get a better estimate of the unknown size of the Lightning Network.\n\nFor more about this project and viewing the dataset, go to http://hiddenlightningnetwork.com\n\nThanks,\nTony\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220608/4e5a66be/attachment.html>"
            },
            {
                "author": "alicexbt",
                "date": "2022-06-08T03:36:51",
                "message_text_only": "Hi Tony,\n\n> The reason this is possible is because probing is a free operation on the Lightning Network after a channel is opened, the error reasons given are way too verbose, and currently channel IDs are based on UTXO's. Scid aliases may be the biggest benefit here, but the use of `unknown_next_peer` , `invalid_onion_hmac`, `incorrect_cltv_expiry`, and `amount_below_minimum` have been the biggest helpers in exploiting channel privacy.\n\nCan this be fixed by making error messages less verbose or reveal less information?\n\n> We should definitely migrate to alias scid's, and encourage every active unannounced channel holder to close, coinjoin, and reopen with an alias. But care should be given in the future when it comes to error reasons revealing information that is meant to be \"private\". Until this migration happens, it would be beneficial to stop being so specific about errors, this does not really seem to help end users anyways.\n\nAlias SCID would be better for privacy as they allow a node to request a channel by a random value instead of value derived from the on-chain transaction. Are alias SCIDs necessary to fix it or error messages alone can fix it?\n\nI found these pull requests and assuming alias SCID are already implemented in LDK/rust-lightning:\n\n[https://github.com/lightningdevkit/rust-lightning/pull/1311/](https://github.com/lightningdevkit/rust-lightning/pull/1311/commits)\nhttps://github.com/lightningdevkit/rust-lightning/pull/1351\n\n> I'll be continuing with this probing project while the problem exists, and work on narrowing down the other channel partner and fixing efficiency bugs. I am publicizing the results as I go, so fair warning that if you have any unannounced channels that you assumed were private and need them to be, close them now on the off chance they get revealed. This could have always been happening already already by analytic firms, so I hope by publicizing this we are all on the same playing field. It is also beneficial to get a better estimate of the unknown size of the Lightning Network.\n\nI love the research and thanks for sharing all the information. I am assuming analytic firms would be using this already.\n\n/dev/fd0\n\nSent with [Proton Mail](https://proton.me/) secure email.\n\n------- Original Message -------\nOn Wednesday, June 8th, 2022 at 7:35 AM, Tony Giorgio via Lightning-dev <lightning-dev at lists.linuxfoundation.org> wrote:\n\n> Hi,\n>\n> For the past few months I have been working on an LDK probing project that searches for unannounced channels on the Lightning Network. For the past week, I have been probing on mainnet and squashing bugs / making optimizations.\n>\n> So far I have found near 445 unannounced channels totaling 1,076,077,750 satoshi's locked across the 3 nodes I have probed, some with just a minimized set (~30,000) of probable channels based on \"round payment amount\" and \"1 or 2 tx output\" heuristics on P2WSH UTXO's. Most of them being on Aincq's node found with the minimized set, I've yet to run the complete set with them. There are about ~860,000 P2WSH UTXO's, about ~60,000 of which are public, so the upward limit of possible private channels is around ~800,000.\n>\n> The exact results are publicized here: https://github.com/BitcoinDevShop/hidden-lightning-network/blob/master/data/results/results.json\n>\n> The reason this is possible is because probing is a free operation on the Lightning Network after a channel is opened, the error reasons given are way too verbose, and currently channel IDs are based on UTXO's. Scid aliases may be the biggest benefit here, but the use of `unknown_next_peer` , `invalid_onion_hmac`, `incorrect_cltv_expiry`, and `amount_below_minimum` have been the biggest helpers in exploiting channel privacy.\n>\n> By creating a probe guessing the Channel ID based on unspent p2wsh transactions, it's a `m * n` problem to probe the entire network, where `m` is utxos and `n` is nodes. Without these errors and instead something like `temporary_channel_failure` or a generic indistinguishable error, guessing a Channel ID would come down to an upwards of `m * n * n-1 * ~2000`, which would be each utxo with each pairing of nodes, each with about ~2000 cltv's to guess (numbers are as low as 7 to as high as ~2000). I threw the extra 2000 into the equation because even with `800,000 * 1 * 2000`, it gets much more time consuming to even probe a single node when we're already spending upwards of a day or so for near 1 million or 2 probes. Concurrent probing is possible, but starts to require more locked up liquidity.\n>\n> We should definitely migrate to alias scid's, and encourage every active unannounced channel holder to close, coinjoin, and reopen with an alias. But care should be given in the future when it comes to error reasons revealing information that is meant to be \"private\". Until this migration happens, it would be beneficial to stop being so specific about errors, this does not really seem to help end users anyways.\n>\n> I'll be continuing with this probing project while the problem exists, and work on narrowing down the other channel partner and fixing efficiency bugs. I am publicizing the results as I go, so fair warning that if you have any unannounced channels that you assumed were private and need them to be, close them now on the off chance they get revealed. This could have always been happening already already by analytic firms, so I hope by publicizing this we are all on the same playing field. It is also beneficial to get a better estimate of the unknown size of the Lightning Network.\n>\n> For more about this project and viewing the dataset, go to http://hiddenlightningnetwork.com\n>\n> Thanks,\n> Tony\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220608/55e9f88f/attachment.html>"
            },
            {
                "author": "Tony Giorgio",
                "date": "2022-06-08T04:12:54",
                "message_text_only": "Hi /dev/fd0,\n\n> Can this be fixed by making error messages less verbose or reveal less information?\n\n\u200b\nNot completely fixed, but much more time consuming to exploit. When a prober gets _just_ the SCID correct, a certain error is passed back. Typically incorrect_cltv_expiry\u200b or amount_below_minimum\u200b being the error, proving that the channel ID is correct. Then it's just up to getting the other values correct. So if the error messages were generic and did not reveal the Channel ID was correct unless all of the other values were also correct, it requires many more combinations of parameters to probe for, instead of just the Channel ID, then just the CLTV, then the amount, then the other node. Does not fix the problem completely because eventually if you were to get all values correct, it would route to the other node, where the other node creates the incorrect_or_unknown_payment_details\u200b error, which you can tell which node creates the error, proving it got to the guessed node.\n\n> Are alias SCIDs necessary to fix it or error messages alone can fix it?\n\n\u200b\nWith the above said, I believe it is necessary to move to SCIDs and that error messages alone can't fix it completely. It's better anyways because when an invoice is made with routing hints for unannounced channels, those consuming the invoice can't associate UTXO's with the node creating the invoice if aliases were used.\n\nAlso, in addition to LDK implementing it, I know LND is working on it currently too https://github.com/lightningnetwork/lnd/pull/5955\n\n> I love the research and thanks for sharing all the information. I am assuming analytic firms would be using this already.\n\n\u200b\nThank you! I assume it as well though it's probably easy to tell whether this is happening to your node or not if someone was watching their HTLC routing failure reasons.\n\nThanks,\nTony\n\n------- Original Message -------\nOn Tuesday, June 7th, 2022 at 10:36 PM, alicexbt <alicexbt at protonmail.com> wrote:\n\n> Hi Tony,\n>\n>> The reason this is possible is because probing is a free operation on the Lightning Network after a channel is opened, the error reasons given are way too verbose, and currently channel IDs are based on UTXO's. Scid aliases may be the biggest benefit here, but the use of `unknown_next_peer` , `invalid_onion_hmac`, `incorrect_cltv_expiry`, and `amount_below_minimum` have been the biggest helpers in exploiting channel privacy.\n>\n> Can this be fixed by making error messages less verbose or reveal less information?\n>\n>> We should definitely migrate to alias scid's, and encourage every active unannounced channel holder to close, coinjoin, and reopen with an alias. But care should be given in the future when it comes to error reasons revealing information that is meant to be \"private\". Until this migration happens, it would be beneficial to stop being so specific about errors, this does not really seem to help end users anyways.\n>\n> Alias SCID would be better for privacy as they allow a node to request a channel by a random value instead of value derived from the on-chain transaction. Are alias SCIDs necessary to fix it or error messages alone can fix it?\n>\n> I found these pull requests and assuming alias SCID are already implemented in LDK/rust-lightning:\n>\n> [https://github.com/lightningdevkit/rust-lightning/pull/1311/](https://github.com/lightningdevkit/rust-lightning/pull/1311/commits)\n> https://github.com/lightningdevkit/rust-lightning/pull/1351\n>\n>> I'll be continuing with this probing project while the problem exists, and work on narrowing down the other channel partner and fixing efficiency bugs. I am publicizing the results as I go, so fair warning that if you have any unannounced channels that you assumed were private and need them to be, close them now on the off chance they get revealed. This could have always been happening already already by analytic firms, so I hope by publicizing this we are all on the same playing field. It is also beneficial to get a better estimate of the unknown size of the Lightning Network.\n>\n> I love the research and thanks for sharing all the information. I am assuming analytic firms would be using this already.\n>\n> /dev/fd0\n>\n> Sent with [Proton Mail](https://proton.me/) secure email.\n>\n> ------- Original Message -------\n> On Wednesday, June 8th, 2022 at 7:35 AM, Tony Giorgio via Lightning-dev <lightning-dev at lists.linuxfoundation.org> wrote:\n>\n>> Hi,\n>>\n>> For the past few months I have been working on an LDK probing project that searches for unannounced channels on the Lightning Network. For the past week, I have been probing on mainnet and squashing bugs / making optimizations.\n>>\n>> So far I have found near 445 unannounced channels totaling 1,076,077,750 satoshi's locked across the 3 nodes I have probed, some with just a minimized set (~30,000) of probable channels based on \"round payment amount\" and \"1 or 2 tx output\" heuristics on P2WSH UTXO's. Most of them being on Aincq's node found with the minimized set, I've yet to run the complete set with them. There are about ~860,000 P2WSH UTXO's, about ~60,000 of which are public, so the upward limit of possible private channels is around ~800,000.\n>>\n>> The exact results are publicized here: https://github.com/BitcoinDevShop/hidden-lightning-network/blob/master/data/results/results.json\n>>\n>> The reason this is possible is because probing is a free operation on the Lightning Network after a channel is opened, the error reasons given are way too verbose, and currently channel IDs are based on UTXO's. Scid aliases may be the biggest benefit here, but the use of `unknown_next_peer` , `invalid_onion_hmac`, `incorrect_cltv_expiry`, and `amount_below_minimum` have been the biggest helpers in exploiting channel privacy.\n>>\n>> By creating a probe guessing the Channel ID based on unspent p2wsh transactions, it's a `m * n` problem to probe the entire network, where `m` is utxos and `n` is nodes. Without these errors and instead something like `temporary_channel_failure` or a generic indistinguishable error, guessing a Channel ID would come down to an upwards of `m * n * n-1 * ~2000`, which would be each utxo with each pairing of nodes, each with about ~2000 cltv's to guess (numbers are as low as 7 to as high as ~2000). I threw the extra 2000 into the equation because even with `800,000 * 1 * 2000`, it gets much more time consuming to even probe a single node when we're already spending upwards of a day or so for near 1 million or 2 probes. Concurrent probing is possible, but starts to require more locked up liquidity.\n>>\n>> We should definitely migrate to alias scid's, and encourage every active unannounced channel holder to close, coinjoin, and reopen with an alias. But care should be given in the future when it comes to error reasons revealing information that is meant to be \"private\". Until this migration happens, it would be beneficial to stop being so specific about errors, this does not really seem to help end users anyways.\n>>\n>> I'll be continuing with this probing project while the problem exists, and work on narrowing down the other channel partner and fixing efficiency bugs. I am publicizing the results as I go, so fair warning that if you have any unannounced channels that you assumed were private and need them to be, close them now on the off chance they get revealed. This could have always been happening already already by analytic firms, so I hope by publicizing this we are all on the same playing field. It is also beneficial to get a better estimate of the unknown size of the Lightning Network.\n>>\n>> For more about this project and viewing the dataset, go to http://hiddenlightningnetwork.com\n>>\n>> Thanks,\n>> Tony\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220608/7760d9dd/attachment-0001.html>"
            },
            {
                "author": "Ren\u00e9 Pickhardt",
                "date": "2022-06-08T05:42:57",
                "message_text_only": "Dear Tony,\n\nThank you for putting emphasis on this. I was actually waiting for someone\nto publicly exploit this.\n\n\n> The reason this is possible is because [...] currently channel IDs are\n> based on UTXO's. Scid aliases may be the biggest benefit here, but the use\n> of `unknown_next_peer` , `invalid_onion_hmac`,  `incorrect_cltv_expiry`,\n> and `amount_below_minimum` have been the biggest helpers in exploiting\n> channel privacy.\n>\n\nJust for reference the exploit with short_channel_ids is known since 2019:\n\nhttps://github.com/lightning/bolts/issues/675\n\nThough it is nice you point out explicitly the use of error codes of\nonions.\n\n\n> By creating a probe guessing the Channel ID based on unspent p2wsh\n> transactions, it's a `m * n` problem to probe the entire network, where `m`\n> is utxos and `n` is nodes.\n>\n\nIt is the main reason why I didn't do this. Though similar to you probing\nACINQ's node one could probabilistically learn which nodes tend to have\nunannounced channels and gain some speedup by probing those nodes first.\n\nAlso wallets tend to have poor utxo management. So looking at the on-chain\nsignal one can probably guess for a p2wsh to which two nodes it might\nbelong and try them first.\n\nThese two strategies should reduce the number of tested nodes for a newly\nseen p2wsh output significantly and probably make it feasible to probe the\nnetwork as new blocks come in.\n\nWith kind regards Rene Pickhardt\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220608/08a200ad/attachment.html>"
            },
            {
                "author": "Tony Giorgio",
                "date": "2022-06-13T16:58:31",
                "message_text_only": "Rene,\n\nThanks for your reply.\n\n> Also wallets tend to have poor utxo management. So looking at the on-chain signal one can probably guess for a p2wsh to which two nodes it might belong and try them first.\n\n\u200b\nThat was going to be one of my next steps. I thought about parsing through the data from https://github.com/lnresearch/topology in order to get a link between every unpsent p2wsh transaction up to 6 hops forward from the opening transaction between two nodes. Then keep a list of every node and their possible p2wsh transactions and probe with those. Should cut down a lot but I have not run through this dataset problem yet. Curious if you think that would be the best process moving forward with this.\n\nOtherwise, going through LSP's with the assumption set, like I did with ACINQ would probably yield further results.\n\nRegards,\nTony\n------- Original Message -------\nOn Wednesday, June 8th, 2022 at 12:42 AM, Ren\u00e9 Pickhardt <r.pickhardt at googlemail.com> wrote:\n\n> Dear Tony,\n>\n> Thank you for putting emphasis on this. I was actually waiting for someone to publicly exploit this.\n>\n>> The reason this is possible is because [...] currently channel IDs are based on UTXO's. Scid aliases may be the biggest benefit here, but the use of `unknown_next_peer` , `invalid_onion_hmac`, `incorrect_cltv_expiry`, and `amount_below_minimum` have been the biggest helpers in exploiting channel privacy.\n>\n> Just for reference the exploit with short_channel_ids is known since 2019:\n>\n> https://github.com/lightning/bolts/issues/675\n>\n> Though it is nice you point out explicitly the use of error codes of onions.\n>\n>> By creating a probe guessing the Channel ID based on unspent p2wsh transactions, it's a `m * n` problem to probe the entire network, where `m` is utxos and `n` is nodes.\n>\n> It is the main reason why I didn't do this. Though similar to you probing ACINQ's node one could probabilistically learn which nodes tend to have unannounced channels and gain some speedup by probing those nodes first.\n>\n> Also wallets tend to have poor utxo management. So looking at the on-chain signal one can probably guess for a p2wsh to which two nodes it might belong and try them first.\n>\n> These two strategies should reduce the number of tested nodes for a newly seen p2wsh output significantly and probably make it feasible to probe the network as new blocks come in.\n>\n> With kind regards Rene Pickhardt\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220613/10ec2bcf/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Preliminary Hidden Lightning Network Analysis",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "alicexbt",
                "Ren\u00e9 Pickhardt",
                "Tony Giorgio"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 21357
        }
    },
    {
        "title": "[Lightning-dev] LN Summit 2022 Notes & Summary/Commentary",
        "thread_messages": [
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2022-06-08T02:38:39",
                "message_text_only": "Hi y'all,\n\nLast week nearly 30 (!) Lightning developers and researchers gathered in\nOakland, California for three day to discuss a number of matters related to\nthe current state and evolution of the protocol.  This time around, we had\nmuch better representation for all the major Lightning Node implementations\ncompared to the last LN Dev Summit (Zurich, Oct 2021).\n\nSimilar to the prior LN Dev Summit, notes were kept throughout the day that\nattempted on a best effort basis to capture the relevant discussions,\ndecisions, and new relevant research or follow up areas to circle back on.\nLast time around, I sent out an email that summarized some key takeaways\n(from my PoV) of the last multi-day dev summit [1]. What follows in this\nemail is a similar summary/recap of the three day summit. Just like last\ntime: if you attended and felt I missed out on a key point, or inadvertently\nmisrepresented a statement/idea, please feel free to reply, correcting or\nadding additional detail.\n\nThe meeting notes in full can be found here:\nhttps://docs.google.com/document/d/1KHocBjlvg-XOFH5oG_HwWdvNBIvQgxwAok3ZQ6bnCW0/edit?usp=sharing\n\n# Simple Taproot Channels\n\nDuring the last summit, Taproot was a major discussion topic as though the\nsoft fork had been deployed, we we're all still watching the \ud83d\udfe9 's stack up\non the road to ultimately activation. Fast forward several months later and\nTaproot has now been fully activated, with ecosystem starting to\nprogressively deploy more and more advanced systems/applications that take\nadvantage of the new features.\n\nOne key deployment model that came out of the last LN Dev summit was the\nconcept of an iterative roadmap that progressively revamped the system to\nuse more taprooty features, instead of a \"big bang\" approach that would\nattempt to package up as many things as possible into one larger update. At\na high level the iterative roadmap proposed that we unroll an existing\nlarger proposal [2] into more bite sized pieces that can be incrementally\nreviewed, implemented, and ultimately deployed (see my post on the LN Dev\nSummit 2021 for more details).\n\n## Extension BOLTs\n\nRiiight before we started on the first day, I wrote up a minimal proposal\nthat attempted to tackle the first two items of the Taproot iterative\ndeployment schedule (musig2 funding outputs and simple tapscript mapping)\n[3]. I called the proposal \"Simple Taproot Channels\" as it set out to do a\nmechanical mapping of the current commitment and script structure to a more\ntaprooty domain. Rather than edit 4 or 5 different BOLTs with a series of\n\"if this feature bit applies\" nested clauses, I instead opted to create a\nnew standalone \"extension bolt\" that defines _new_ behavior on top of the\nexisting BOLTs, referring to the BOLTs when necessary. The style of the\ndocument was inspired by the \"proposals\" proposal (very meta), which was\npopularized by cdecker and adopted by t-bast with his documents on\nTrampoline and Blinded Paths.\n\nIf the concept catches on, extension BOLTs provide us with a new way to\nextend the spec: rather than insert everything in-line, we could instead\ncreate new standalone documents for larger features. Having a single self\ncontained document makes the proposal easier to review, and also gives the\nauthor more room to provide any background knowledge, primaries, and also\nrationale. Overtime, as the new extensions become widespread (eg: taproot is\nthe default channel type), we can fold in the extensions back to the main\nset of \"core\" BOLTs (or make new ones as relevant).\n\nSmaller changes to the spec like deprecating an old field or tightening up\nsome language will likely still follow the old approach of mutating the\nexisting BOLTs, but larger overhauls like the planned PTLC update may find\nthe extension BOLTs to be a better tool.\n\n## Tapscript, Musig2, and Lightning\n\nAs mentioned above the Simple Taproot Channels proposal does two main\nthings:\n  1. Move the existing 2-of-2 p2wsh segwit v0 funding output to a _single\n  key_ p2tr output, with the single key actually being an aggregated musig2\n  key.\n\n  2. Map all our existing scripts to the tapscript domain, using the\n  internal key (keyspend path) for things like revocations, which an\n  potentially allow nodes to store less state for HTLCs.\n\nOf the two components #1 is by far the trickiest. Musig2 is a very elegant\nprotocol (not to mention the spec which y'all should totally check out) but\nas the signatures aren't deterministic (like RFC 6979 [5]), both signers\nneed to \"protect themselves at all times\" to ensure they don't ever re-use\nnonces, which can lead to a private key leak (!!).\n\nRather than try to create some sort of psuedo-deterministic nonces scheme\n(which maaybe works until the Blockstream Research team squints vaguely in\nits direction), I opted to just make all nonces 100% ephemeral and tied to\nthe lifetime of a connection. Musig2 defines something called a public\nnonces, which is actually two individual 33-byte nonces. This value needs to\nbe exchanged before signing can begin (but can be sent before sides know\nthey're aggregated keys). One important thing to note is that given that the\nchannels today have _asymmetric_ state, we actually need a _pair_ of public\nnonces: one that I'll use to sign my commitment, and one I'll use to sign\nyours. Lightning channels w/ symmetric state like eltoo can get by w/ only\nexchange a single set of nonces, as there's only one message per state.\n\nNonce exchange takes place in a few places:\n\n  * During initial funding: I send my public nonce in the open_channel\n    message, you send yours in the accept_channel message. After this\n    exchange we can both generate signatures for the refund commitment\n    transactions.\n\n  * After the channel is \"ready\" we send another set of nonces, so we can\n    sign the next state. This is similar to the existing revocation key\n    exchange: I need your next nonce/key before I can sign a new state.\n\n  * Upon channel re-establishment a _new_ set of nonces is sent, as they're\n    100% ephemeral. The current draft also requires that if you were\n    re-transmitting a sig, then you use the _new_ nonces to sign again, as\n    it's possible you went to retransmit but left off an expired/trimmed\n    HLTC (could lead to nonce re-use and also needing to remember nonces).\n\n  * Each time I revoke my channel, I send to you a single nonce, my \"local\n    nonce\" (naming needs some work here), which lets you sign for a new\n    state.\n\n  * Each time I send a new sig, I also send you another nonce, my \"remote\"\n    nonce\", which\n\n  * When I send a shutdown (co-op close) I send a single public nonce so we\n    can sign the next co-opc close offer.\n\n  * When I send a closing_signed I send another nonce so once you send your\n    offer, we sign another set.\n\nThe final flows aren't 100% yet finalized, as we'll need some\nimplementations drafted to make sure the nonce handling and script mapping\nworks out properly.\n\n### Lightning Channels & Recursive Musig2\n\nOne other cool topic that came up is the concept of leveraging recursive\nmusig2 (so musig2 within musig2) to make channels even _more_ multi-sigy.\nThe benefit here is that Bob & Carol can each have their individual keys\n(which might actually be aggregated keys themselves) and make a channel w/\nAlice, who only knows of them as Barol, and doesn't know there're actually\nanother pair of keys at play. This is _really_ cool as it allows node\noperators, wallets, and lightning platforms to experiment with various\nkey/signing trees that may add more security, redundancy, or flexibility.\n\nWhen this first came up, someone brought up the fact that while the scheme\nis \"known\" the initial paper as they weren't sure how to actually write a\nproof for it. During the session, someone emailed one of the musig2 authors\nasking for more details, and if it's safe to implement and roll out.\nThankfully they quickly replied and explained that the proof recursive musig\n(pls someone correct me again here if I'm wrong) wasn't left out due to\nimpossibility, but that a proof in the existing Random Oracle Model (which\nwas used to derive a bound for the number of nonces needed) would lead to a\nblow up in the number of nonces required. Attempting to write the proof in\nsome other model would likely lead to better results (proved w/ two nonces\nas base musig2), but would end up being pretty complicated, so hard to read\nand even review for correctness.\n\nAssuming everything checks out, then a useful mental model explained by the\nmusig2 BIP author is a sort of tree structure. Assuming I'm a signer, and we\nassemble the other signer as a sibling leaf in a binary tree, then I just\nneed to wait for the sibling nonce/key, before I can aggregate that into\nthe final value. So if there're 3 signers, I wait for the regular public\nnonce, but the other signers sum their respective nonces into a single\nnonce, then send that to me. A similar operation is carried out for key\naggregation, with the rest of the protocol being mostly the same.\n\nUltimately, even if wallets/nodes aren't ready to roll something like this\nout today, we at least want to make sure the proposed flow is compatible\nwith Simple Taproot Channels, and ideally we'd have a toy implementation to\nverify out understanding and show it's possible/sound. I volunteered to hack\nup a simple recursive musig2 demo, as there doesn't seem to be any code in\nthe wild that implements it.\n\n## Lightning Gossip\n\n# Gossip V2: Now Or Later?\n\nAnother big topic related to Taproot was the question of how we should\nupdate the gossip network: the gossip protocol today has all channels\nvalidated by node, which requires that the nodes understand how to\nreconstruct the funding output based on the set of advertised keys. The\nprotocol today assumes a segwit v0 p2wsh multi-sig is used. Assuming we had\neverything implemented today, a node wouldn't be able to advertise its new\ntaproot channels to the rest of the public graph as they wouldn't understand\nhow to validate it.\n\nThis presents a new opportunity: we already need to rework gossip for\ntaproot, so should we go ahead and re-design the entire thing with an eye\nfor better privacy and future extensibility?\n\nA proposal for the \"re-design the entire thing\" was floated in the past by\nRusty [6]. It does away with the strict coupling of channels to channel\nannouncements, and instead moves them to the _node_ level. Each node would\nthen advertise the set of \"outputs\" they have control of, which would then\nbe mapped to the total capacity of a node, without requiring that these\noutputs self identify themselves on-chain as Lightning Channels. This also\nopens up the door to different, potentially more privacy preserving\nproofs-of-channel-ownership (something something zkp).\n\nOn the other hand, we could just follow the path of Simple Taproot Channels\nand map musig2+schnorr onto the existing gossip network. This is less\nchanges in total, with the main benefit being the ability to only send 1 sig\n(aggregated musig2 sig of keys) instead of 4 individual sigs. I made a very\nlofty proposal in this direction here [7].\n\nUltimately we decided to take the \"just musig2 aspects\" from gossip v1.5\n(not the real name), and the \"let's refresh all the messages w/ TLV\ngoodness\" from the gossip v2 proposal. This gives us a smaller package to\nimplement, and lets us potentially rejigger the messages to be more\nextensible and remove cruft like the node color that almost nothing uses,\nbut we all validate/store.\n\nThe follow up work in this area is a more concrete proposal that updates the\nrelevant gossip messages to be taproot aware and TLV'd and also update the\nset of requirements w.r.t _how_ to validate the channels in the first place\n(so given two keys verify that applying the keyagg method of musig2 lead to\nwhat' in the funding output).\n\nGossip v2 will likely happen \"eventually\", but the rather large design space\nneeds to be explored a bit more so we can properly analyze exactly what\nprivacy and extensibility properties we'll get out of it.\n\n# Applying Mini Sketch to LN Gossip\n\nOne issue we have today, is that other than the initial scid query mechanism\nadded to the protocol, there isn't a great way to ensure you have all the\nlatest updates your peer has. These days, many nodes pretty aggressively\nrate limit other nodes, so you might even have trouble sending out your\nupdate in the first place. A recent paper (that I haven't actually fully\nread yet) [8] analyzes the gossip network today to work out things like:\nexactly how long it takes things to propagate, total bandwidth usage, etc.\n\nMinisketch [9] (the grandchild of IBLTs ;)), is an efficient set\nreconciliation protocol that was designed for Bitcoin p2p mempool syncing,\nbut can be applied to other protocols. An attendee has been working on\nbrushing off some older work to try to see how we could apply it to the LN\nprotocol to give nodes a more bandwidth efficient way to sync channel\nupdates, and also achieve better update propagation. This supplements some\nexisting investigative work done by Alex Meyers [10], with more concrete\ndesigns w.r.t: what goes into the sketch, and the various size parameters\nthat need to be chosen.\n\n# Channel Jamming\n\nAn attendee gave a talk on the various proposed solutions to channel\njamming, evaluating them on several axis including: punishment/monetary,\nlocal vs global reputation, feasibility of mechanism design, UX\nimplications, and implementation complexity. The presenter didn't present a\nnew concrete proposal, but instead went through the various trade-offs,\nultimately concluding that they factor monetary penalties wherein the funds\nare distributed across the route, rather than being provably burnt to\nminers. However they alluded to some future upcoming work that attempts a\nmore rigorous analysis of the proposed solutions, their tradeoffs, and\npotential ways we can parametrize solutions to be more effective (how much\nshould they pay, etc).\n\nFor those looking to brush up on the latest state of research/mitigations in\nthis area, I recommend this blog post by Bitmex research [11].\n\n# Onion Messages & DoS\n\nThe topic of DoS concerns related to onion messages (in isolation, so not\nnecessarily related to things like bolt12 that take advantage of them came\nup.  During a white boarding session some argued that DoS isn't actually\nmuch of an issue, as nodes can leverage \"back propagation congestion\ncontrol\" to inform the source (who may not actually be the sender) that\nthey'll start to drop or limit their packets, with each node doing this\niteratively until the actual source of the spam has been clamped. A few\nlofty designs were thrown around, but more work needs to be done to\nconcretely specify something so it can be properly analyzed.\n\nOn the other side of the spectrum, rather than attempt to rate limit at the\nnode level (which each node having their own policy), nodes could opt\ninstead to forward _anything_ as long as the sender pays them enough. I\nproposed a lofty approach that combined AMP and Onion Messages earlier this\nyear [12]. At a high level I make an AMP payment, which pushes extra coins\nto all nodes on a route, and also drops off a special identifier to them.\nWhen I send an onion message I include this identifier, with each node\nperforming their own account w.r.t the amount of bandwidth an ID has\nremaining.\n\nUltimately a few implementations are pretty close to deploying their\nimplementation of onion messages, so no matter the intended use case, it\nwould be good to have code deployed along side to either rate limit or price\nresource consumption accordingly. Otherwise, we might end up in a scenario\nwhere DoS concerns were brushed aside, but end up being a huge issue later.\n\n# Blinded Paths, QR Codes & Invoices\n\nBlinded paths [13] is a new-er proposal to solve the \"last mile\" privacy\nissue when receiving payments on LN. Today invoices to unadvertised channels\ncontain a set of hop hints, which are anchored at public nodes in the graph,\nand also leak the scid of the unadvertised channel (points on-chain to the\nchannel receiving payments). A solution for the on-chain leak, SCID channel\naliases [15] are in the process of being widely rolled out. Channel aliases\ninstead use a random value in the invoice, allowing receiving nodes to break\nthat on-chain link and even rotate out the value periodically. With the\non-chain leak addressed, it's still the case that you give away your\n\"position\" in the network, since as a sender I know that you're connected to\nnode N with a private channel.\n\nBlinded paths address this node-level last mile privacy leak by replacing\nhop hints with a new cryptographically blinded path. At a high level, the\nreceiver can construct a \"hop hint\" of length greater than 1, gather the\npublic keys of each of the nodes, then blinded them such that: the sender\ncan use them for path finding, but doesn't actually now exactly _which_\nnodes they actually are.\n\nThere're two type of blinded paths: those in onion messages and those used\nfor actual payments. The latter variant was only formalized earlier this\nyear, as before people were mainly interested in using them to fetch BOLT 12\ninvoice via onion messages. One issue that pops up when attempting to use\nblinded paths for normal payments is: the size of the resulting invoice. As\nblinded paths are actually fragments of publicly known paths, as a receiver,\nyou want to stuff as many of them into the invoice as possible, since they\nMUST be taken in order to route towards you. Invoices are typically\ncommunicated via QR codes, which have a hard limit w.r.t the amount of\ninformation that can be packed in. On the other hand for invoice fetching,\nall that matters is that a path exists, so you can get by with stuffing less\nof then in a QR code.\n\nAs a result, blinded paths aren't necessarily compatible with the widely\ndeployed BOLT 11 based QR codes. Instead a way to fetch invoice on demand is\nrequired. Both BOLT-12 and LN-URL provide standardized ways for nodes to\nfetch invoices, though their transport/signalling medium of choice differs.\nBlinded routes are technically compatible with BOLT 11 invoices, but may be\nhampered by the fact that you can only include so many routes.\n\nAnother consideration is that unlike hop hints, blinded paths require more\nmaintain once, as since they traverse public route, policy changes like a\nfee update may invalidate an entire set set of routes. One proposed solution\nis that forwarding nodes should observe their older policy for a period of\ntime (so a grace period), and also that blinded paths should have an\nexplicit expiry (similar to the existing invoice expiry).\n\nOne other implication is that the set of routes the receiver includes\nmatters\nmore: if they don't send enough or select them poorly, the sender may never\nbe\nable to reach them even though a path exists in theory. More hands on\nexperience is needed so the spec authors can better guide implementations\nand\nwallets w.r.t best practices.\n\n# Friend-of-a-friend Balance Sharing & Probing\n\nA presentation was given on friend-of-a-friend balance sharing [16]. The\nhigh level idea is that if we share _some_ information within a local\nradius, then this gives the sender more information to choose a path that's\npotentially more reliable. The tradeoff here ofc is that nodes will be\ngiving away more information that can potentially be used to ascertain\npayment flows. In an attempt to minimize the amount of information shared,\nthe presenter proposed that just 2 bits of information be shared. Some\ninitial simulations showed that sharing local information actually performed\nbetter than sharing global information (?). Some were puzzled w.r.t how\nthat's possible, but assuming the slides+methods are published others can\ndig further into the model/parameter used to signal the inclusion.\n\nArguably, information like this is already available via probing, so one\nline of thinking is something like: \"why not just share _some_ of it\" that\nmay actually lead to less internal failures? This is related to a sort of\ntension between probing as a tool to increase payment reliability and also\nas a tool to degrade privacy in the network. On the other hand, others\nargued that probing provides natural cover traffic, since they actually\n_are_ payments, though they may not be intended to succeed.\n\nOn the topic of channel probing, a sort of makeshift protocol was devised to\nmake it harder in practice, sacrificing too much on the axis of payment\nreliability.  At a high level it proposes that:\n\n  * nodes more diligently set both their max_htlc amount, as well as the\n    max_htlc_value_in_flight amount\n\n  * a 50ms (or select other value) timer should be used when sending out\n    commitment signatures, independent of HTLC arrival\n\n  * nodes leverage the max_htlc value to set a false ceiling on the max in\n    flight parameter\n\n  * for each HTLC sent/forwarded, select 2 other channels at random and\n    reduce the \"fake\" in-flight ceiling for a period of time\n\nSome more details still need to be worked out, but some felt that this would\nkick start more research into this area, and also make balance mapping\n_slightly_ more difficult. From afar, it may be the case that achieving\nbalance privacy while also achieving acceptable levels of payment\nreliability might be at odds with each other.\n\n# Eltoo & ANYPREVOUT\n\nOne of the attendees is currently working on both fully implementing eltoo,\nas well as specifying the exact channel funding+update interaction were it\nto be rolled out align side the existing penalty based channels in the\nprotocol. As this version of eltoo is based on Taproot, we were able to\ncompare notes a bit to find the overlapping set of changes (nonce handling,\netc), which permits cross review of the proposals. This type of work is\ncool, as only by fully implementing something end to end can you reaaally\nwork out all the edge cases and nuances.\n\nANYPREVOUT as hasn't changed significantly as of late. An attendee shared\nplans to create a sort of mega all-future-feasible-soft-forks fork of\nbitcoind, that would package up various unmerged (from bitcoind's) proposal\nsoft fork packages into an easy to run+install binary/project attached to a\nsignet. The hop is that by giving developers an easy way to interact with\nproposed soft fork proposals (vs debasing some ancient pull request), wider\nparticipation in testing/implementation/review can be facilitated.\n\n# Trampoline Routing\n\nThere was a presentation on Trampoline routing explaining the motivation,\nhistory, and current state of the proposal. The two main cases we've\nnarrowed down on are:\n\n  1. A mobile user doesn't necessarily want to sync the _entire_ graph, so\n  they can use trampoline to maintain a subset and still be able to send\n  payments.\n\n  2. A mobile user wants to be able to instate a payment, go offline, and\n  return at a later time to learn about the final state of the payment.\n\nUse case #2 seems to be the most promising when combined with other\nproposals for holding HTLCs at an origin node (call it an \"LSP\") [13].\nCombined together, this would allow a mobile node to send a payment, then go\noffline, with the LSP being able to retry the payment either continuously or\nonly when it knows the receiver is online to accept the payment. This may\npotentially dramatically improve the UX for LN on mobile, as things suddenly\nbecome a lot more asynchronous: I do something go offline, and the LSP node\ncan fulfil the payment in the background, then wait for me to come online to\nsettle the final.  hop.\n\nTrampoline can also be composed well with blinded routes (blinded route from\nlast trampoline to receiver) and also MPP (internal nodes can split\nthemselves with local information).\n\nOne added trade-off is that since the sender doesn't know the entire route,\nthey need to sort of overshoot w.r.t fees and CTLVs. This is something\nwe've known for a while, but until Trampoline is more widely rolled out, we\nwon't have a very good feel w.r.t how much extra senders will need to\nallocate.\n\n# Node Fee Optimization & Fee Rate Cards\n\nOver the past few years, a common thread we've seen across successful\nrouting nodes is dynamic fee setting as a way to encourage/discourage\ntraffic. A routing nodes can utilize the set of fees of a channel to either\nmake it too expensive for other nodes to route through (it's already\ndepleted don't try unless you'll give be 10 mil sats, which no one would) or\nvery cheap, which'll incentivize flows in the other direction. If all nodes\nare constantly sending out updates of this nature, then it can generate a\nlot of traffic, and also sort of leak more balance information overtime\n(which some nodes are already doing: using fees/max_htlc to communicate\navailable balances).\n\nOne attendee proposed allowing nodes to express a sort of fee gradient via a\nstatic curve/bucket/function, instead of dynamically communicating what the\nlatest state of the fee+liquidity distribution looks like. A possible\nmanifestation could be a series of buckets, each of which with varying fee\nrates. If your payment consumes 50% of channel balance, then you pay this\nrate, otherwise if it's 5% you pay this rate, etc, etc. This might allow for\nnodes to capture the same dynamics as they do with more dynamic fee updates,\nbut in a way that leaks less information and also consumes less gossip\nbandwidth.\n\n# The Return of Splicing\n\nSplicing is one of those things that was discussed a long time ago, but was\nnever really fully implemented and rolled out. A few attendees have started\nto take a closer look at the problem, building off of the interactive-tx\nscheme that the dual-funding protocol extension uses. The main intricacy\ndiscussed was if concurrent splices should be allowed or not, and if so, how\nwe would handle the various edge cases. As an example, if I propose a splice\nto add more funds via my input, but that turns out to already be spent, then\nthe splicing transaction we created is invalid and can never be confirmed.\nHowever if we allow _another_ splice to take place, and another one, and\nanother one, then ideally _one_ of them will confirm and serve as the new\nanchor for the channel.\n\nIn a world of concurrent splices, the question of \"what is my Lightning\nbalance\" becomes even more murky. Wallet and implementations will likely\nwant to show the most pessimistic value, while also ensuring that the user\nis able to effectively account for where all their funds and what they can\nspend on/off chain.\n\n# LN-URL + BOLT 12\n\nLN-URL and BOLT 12 are both standardized ways that answer the question of:\nhow can I fetch an invoice from Bob? LN-URL differs from BOLT 12 in that it\nuses the existing BOLT 11 invoice format, and uses an HTTP based protocol\nfor the negotiation process. BOLT 12 on the other hand is a suite of\nprotocol additions that includes (amongst other things) a new invoice format\n(yay TLV!) and also a way to use onion messages to fetch an invoice _via_\nthe network.\n\nAssuming blinded paths is widely rolled out, then the question of how\ninvoices are obtained becomes more important as blinded paths means that you\ncan't fit much in the traditional QR encoding. As a result, fetching\ninvoices on demand may become a more common place flow, with all its\ntrade-offs.  There was a group discussion on how we could sort of unifying\neverything either by allowing BOLT 12 to be used over LN-URL or the other\nway around.\n\nOne proposal was to add a new query parameter to the normal LN-URL QR code\ncontents. This would mean that when a wallet goes to scan an LN-URL QR code,\nif they know of the extra param, and what BOLT 12, they can just use the\nenclosed offer to fetch the invoice.\n\nAn alternative proposal was to instead extract the BOLT 12 _invoice_ format\nfrom the greater BOLT 12 \"Offers\" proposal. Assuming blinded paths is only\nspecified w.r.t BOLT 12 _invoices_, then this would mean an LN-URL extension\ncould be rolled out that allowed returning BOLT 12 invoice rather than BOLT\n11 invoices. This would allow the ecosystem to slowly transition to a shared\ninvoice format, even if there may be fundamental disagreements w.r.t _how_\nthe invoices should be fetched in the first place.\n\nIt's worth noting that both of these proposals can be combined:\n\n  * If a wallet knows how to BOLT 12 Offers, they can take the enclosed\n    offer and run w/ it.\n\n  * If they don't know about Offers, but can send w/ the BOLT _invoice_\n    format, then they can fetch that and complete the payment.\n\nThis might be a nice middle ground as it would tend all\nwallets/implementations to being able to decode and send w/ a BOLT 12\n_invoice_, and leave the question of _how_ it should be fetched up to the\napplication/wallet/service. In the end, if paths never quite intersect, then\nit's still possible to add route blinding to BOLT 11, with LN-URL sticking\nwith that invoice format to take advantage of the new privacy enhancements\n\n\n[1]:\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2021-November/003336.html\n[2]:\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2021-October/003278.html\n[3]: https://github.com/lightning/bolts/pull/995\n[4]: https://github.com/jonasnick/bips/blob/musig2/bip-musig2.mediawiki\n[5]: https://datatracker.ietf.org/doc/html/rfc6979\n[6]:\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2022-February/003470.html\n[7]:\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2022-March/003526.html\n[8]: https://arxiv.org/abs/2205.12737\n[9]: https://bitcoinops.org/en/topics/minisketch/\n[10]:\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2022-April/003551.html\n[11]: https://blog.bitmex.com/preventing-channel-jamming/\n[12]:\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2022-February/003498.html\n[13]: https://github.com/lightning/bolts/pull/765\n[14]:\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2021-October/003307.html\n[15]: https://github.com/lightning/bolts/pull/910\n[16]: https://github.com/lightning/bolts/pull/780\n\n-- Laolu\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220607/eb9e8d17/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-06-14T22:00:53",
                "message_text_only": "> ## Lightning Gossip\n>\n> # Gossip V2: Now Or Later?\n\n<snip>\n\n> A proposal for the \"re-design the entire thing\" was floated in the past by\n> Rusty [6]. It does away with the strict coupling of channels to channel\n> announcements, and instead moves them to the _node_ level. Each node would\n> then advertise the set of \"outputs\" they have control of, which would then\n> be mapped to the total capacity of a node, without requiring that these\n> outputs self identify themselves on-chain as Lightning Channels. This also\n> opens up the door to different, potentially more privacy preserving\n> proofs-of-channel-ownership (something something zkp).\n\nwaxwing recently posted something interesting over in bitcoin-dev, which seems to match the proof-of-channel-ownereship.\n\nhttps://gist.github.com/AdamISZ/51349418be08be22aa2b4b469e3be92f\n\nI confess to not understanding the mathy bits but it seems to me, naively, that the feature set waxwing points out match well with the issues we want to have:\n\n* We want to rate-limit gossip somehow.\n* We want to keep the mapping of UTXOs to channels private.\n\nIt requires a global network that cuts across all uses of the same mechanism (similar to defiads, but more private --- basically this means that it cannot be just Lightning which uses this mechanism, at least to acquire tokens-to-broadcast-my-channels) to prevent a UTXO from being reused across services, a property I believe is vital to the expected spam-resistance.\n\n> # Friend-of-a-friend Balance Sharing & Probing\n>\n> A presentation was given on friend-of-a-friend balance sharing [16]. The\n> high level idea is that if we share _some_ information within a local\n> radius, then this gives the sender more information to choose a path that's\n> potentially more reliable. The tradeoff here ofc is that nodes will be\n> giving away more information that can potentially be used to ascertain\n> payment flows. In an attempt to minimize the amount of information shared,\n> the presenter proposed that just 2 bits of information be shared. Some\n> initial simulations showed that sharing local information actually performed\n> better than sharing global information (?). Some were puzzled w.r.t how\n> that's possible, but assuming the slides+methods are published others can\n> dig further into the model/parameter used to signal the inclusion.\n>\n> Arguably, information like this is already available via probing, so one\n> line of thinking is something like: \"why not just share _some_ of it\" that\n> may actually lead to less internal failures? This is related to a sort of\n> tension between probing as a tool to increase payment reliability and also\n> as a tool to degrade privacy in the network. On the other hand, others\n> argued that probing provides natural cover traffic, since they actually\n> _are_ payments, though they may not be intended to succeed.\n>\n> On the topic of channel probing, a sort of makeshift protocol was devised to\n> make it harder in practice, sacrificing too much on the axis of payment\n> reliability. At a high level it proposes that:\n>\n> * nodes more diligently set both their max_htlc amount, as well as the\n> max_htlc_value_in_flight amount\n>\n> * a 50ms (or select other value) timer should be used when sending out\n> commitment signatures, independent of HTLC arrival\n>\n> * nodes leverage the max_htlc value to set a false ceiling on the max in\n> flight parameter\n>\n> * for each HTLC sent/forwarded, select 2 other channels at random and\n> reduce the \"fake\" in-flight ceiling for a period of time\n>\n> Some more details still need to be worked out, but some felt that this would\n> kick start more research into this area, and also make balance mapping\n> _slightly_ more difficult. From afar, it may be the case that achieving\n> balance privacy while also achieving acceptable levels of payment\n> reliability might be at odds with each other.\n\nA point that was brought up is that nodes can lie about their capacity, and there would be no way to counteract this.\n\nEven given the above, it would be trivial for a lying node to randomly lie about their `max_htlc` to still be noticed by nodes who try to filter out nodes who do not update their `max_htlc`s.\n(maximal lying is to always say 50% of your capacity is in `max_htlc`, your node can lie by setting `max_htlc` from 35%->65%, you can coordinate this with another lying peer node too by use of an odd message number to set up the lying protocol so both of you can lie about the channel capacity consistently)\n\nI think your best bet is really to utilize feerates, as lying with those is expected to lead to economic loss.\n\n<snip>\n\n> # Node Fee Optimization & Fee Rate Cards\n>\n> Over the past few years, a common thread we've seen across successful\n> routing nodes is dynamic fee setting as a way to encourage/discourage\n> traffic. A routing nodes can utilize the set of fees of a channel to either\n> make it too expensive for other nodes to route through (it's already\n> depleted don't try unless you'll give be 10 mil sats, which no one would) or\n> very cheap, which'll incentivize flows in the other direction. If all nodes\n> are constantly sending out updates of this nature, then it can generate a\n> lot of traffic, and also sort of leak more balance information overtime\n> (which some nodes are already doing: using fees/max_htlc to communicate\n> available balances).\n>\n> One attendee proposed allowing nodes to express a sort of fee gradient via a\n> static curve/bucket/function, instead of dynamically communicating what the\n> latest state of the fee+liquidity distribution looks like. A possible\n> manifestation could be a series of buckets, each of which with varying fee\n> rates. If your payment consumes 50% of channel balance, then you pay this\n> rate, otherwise if it's 5% you pay this rate, etc, etc.\n\nI think this is not what was actually proposed?\n\nAs I understood it, the percent range is not how much the payment consumes of the channel balance but instead the percent range is the probability-of-success given a uniform distribution of channel balance.\n\nFor instance if the current channel balance is currently 67%, then the forwarding node will succeed all payments that pay a fee from the 33% fee card or higher, otherwise fail the payment with \"not enough fees\".\n\nThe intent is that payers will treat 100% - fee_card_percent as the probability-of-failure of that channel, and can select which fee card maximizes both its probability-of-failure and max-fee in some kind of reasonable exchange rate.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2022-06-15T08:09:13",
                "message_text_only": "Hey Zman and list,\n\nI don't think waxwing's proposal will help us for private gossip.\nThe rate-limiting it provides doesn't seem to be enough in our case.\nThe proposal rate-limits token issuance to once every N blocks where\nN is the age of the utxo to which we prove ownership of. Once the token\nis issued and verified, the attacker can spend that utxo, and after N\nblocks he's able to get a new token with this new utxo.\n\nThat is a good enough rate-limit for some scenarios, but in our case\nit means that every N blocks people are able to double the capacity\nthey advertise without actually having more funds.\n\nWe can probably borrow ideas from this proposal, but OTOH I don't\nsee how to apply it to lightning gossip, what we want isn't really rate\nlimiting, we want a stronger link between advertised capacity and\nreal on-chain capacity.\n\nCheers,\nBastien\n\nLe mer. 15 juin 2022 \u00e0 00:01, ZmnSCPxj via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n>\n> > ## Lightning Gossip\n> >\n> > # Gossip V2: Now Or Later?\n>\n> <snip>\n>\n> > A proposal for the \"re-design the entire thing\" was floated in the past\n> by\n> > Rusty [6]. It does away with the strict coupling of channels to channel\n> > announcements, and instead moves them to the _node_ level. Each node\n> would\n> > then advertise the set of \"outputs\" they have control of, which would\n> then\n> > be mapped to the total capacity of a node, without requiring that these\n> > outputs self identify themselves on-chain as Lightning Channels. This\n> also\n> > opens up the door to different, potentially more privacy preserving\n> > proofs-of-channel-ownership (something something zkp).\n>\n> waxwing recently posted something interesting over in bitcoin-dev, which\n> seems to match the proof-of-channel-ownereship.\n>\n> https://gist.github.com/AdamISZ/51349418be08be22aa2b4b469e3be92f\n>\n> I confess to not understanding the mathy bits but it seems to me, naively,\n> that the feature set waxwing points out match well with the issues we want\n> to have:\n>\n> * We want to rate-limit gossip somehow.\n> * We want to keep the mapping of UTXOs to channels private.\n>\n> It requires a global network that cuts across all uses of the same\n> mechanism (similar to defiads, but more private --- basically this means\n> that it cannot be just Lightning which uses this mechanism, at least to\n> acquire tokens-to-broadcast-my-channels) to prevent a UTXO from being\n> reused across services, a property I believe is vital to the expected\n> spam-resistance.\n>\n> > # Friend-of-a-friend Balance Sharing & Probing\n> >\n> > A presentation was given on friend-of-a-friend balance sharing [16]. The\n> > high level idea is that if we share _some_ information within a local\n> > radius, then this gives the sender more information to choose a path\n> that's\n> > potentially more reliable. The tradeoff here ofc is that nodes will be\n> > giving away more information that can potentially be used to ascertain\n> > payment flows. In an attempt to minimize the amount of information\n> shared,\n> > the presenter proposed that just 2 bits of information be shared. Some\n> > initial simulations showed that sharing local information actually\n> performed\n> > better than sharing global information (?). Some were puzzled w.r.t how\n> > that's possible, but assuming the slides+methods are published others can\n> > dig further into the model/parameter used to signal the inclusion.\n> >\n> > Arguably, information like this is already available via probing, so one\n> > line of thinking is something like: \"why not just share _some_ of it\"\n> that\n> > may actually lead to less internal failures? This is related to a sort of\n> > tension between probing as a tool to increase payment reliability and\n> also\n> > as a tool to degrade privacy in the network. On the other hand, others\n> > argued that probing provides natural cover traffic, since they actually\n> > _are_ payments, though they may not be intended to succeed.\n> >\n> > On the topic of channel probing, a sort of makeshift protocol was\n> devised to\n> > make it harder in practice, sacrificing too much on the axis of payment\n> > reliability. At a high level it proposes that:\n> >\n> > * nodes more diligently set both their max_htlc amount, as well as the\n> > max_htlc_value_in_flight amount\n> >\n> > * a 50ms (or select other value) timer should be used when sending out\n> > commitment signatures, independent of HTLC arrival\n> >\n> > * nodes leverage the max_htlc value to set a false ceiling on the max in\n> > flight parameter\n> >\n> > * for each HTLC sent/forwarded, select 2 other channels at random and\n> > reduce the \"fake\" in-flight ceiling for a period of time\n> >\n> > Some more details still need to be worked out, but some felt that this\n> would\n> > kick start more research into this area, and also make balance mapping\n> > _slightly_ more difficult. From afar, it may be the case that achieving\n> > balance privacy while also achieving acceptable levels of payment\n> > reliability might be at odds with each other.\n>\n> A point that was brought up is that nodes can lie about their capacity,\n> and there would be no way to counteract this.\n>\n> Even given the above, it would be trivial for a lying node to randomly lie\n> about their `max_htlc` to still be noticed by nodes who try to filter out\n> nodes who do not update their `max_htlc`s.\n> (maximal lying is to always say 50% of your capacity is in `max_htlc`,\n> your node can lie by setting `max_htlc` from 35%->65%, you can coordinate\n> this with another lying peer node too by use of an odd message number to\n> set up the lying protocol so both of you can lie about the channel capacity\n> consistently)\n>\n> I think your best bet is really to utilize feerates, as lying with those\n> is expected to lead to economic loss.\n>\n> <snip>\n>\n> > # Node Fee Optimization & Fee Rate Cards\n> >\n> > Over the past few years, a common thread we've seen across successful\n> > routing nodes is dynamic fee setting as a way to encourage/discourage\n> > traffic. A routing nodes can utilize the set of fees of a channel to\n> either\n> > make it too expensive for other nodes to route through (it's already\n> > depleted don't try unless you'll give be 10 mil sats, which no one\n> would) or\n> > very cheap, which'll incentivize flows in the other direction. If all\n> nodes\n> > are constantly sending out updates of this nature, then it can generate a\n> > lot of traffic, and also sort of leak more balance information overtime\n> > (which some nodes are already doing: using fees/max_htlc to communicate\n> > available balances).\n> >\n> > One attendee proposed allowing nodes to express a sort of fee gradient\n> via a\n> > static curve/bucket/function, instead of dynamically communicating what\n> the\n> > latest state of the fee+liquidity distribution looks like. A possible\n> > manifestation could be a series of buckets, each of which with varying\n> fee\n> > rates. If your payment consumes 50% of channel balance, then you pay this\n> > rate, otherwise if it's 5% you pay this rate, etc, etc.\n>\n> I think this is not what was actually proposed?\n>\n> As I understood it, the percent range is not how much the payment consumes\n> of the channel balance but instead the percent range is the\n> probability-of-success given a uniform distribution of channel balance.\n>\n> For instance if the current channel balance is currently 67%, then the\n> forwarding node will succeed all payments that pay a fee from the 33% fee\n> card or higher, otherwise fail the payment with \"not enough fees\".\n>\n> The intent is that payers will treat 100% - fee_card_percent as the\n> probability-of-failure of that channel, and can select which fee card\n> maximizes both its probability-of-failure and max-fee in some kind of\n> reasonable exchange rate.\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220615/56c7bcf8/attachment-0001.html>"
            },
            {
                "author": "Michael Folkson",
                "date": "2022-06-21T11:54:27",
                "message_text_only": "Thanks for the summary Laolu, very informative.\n\n> One other cool topic that came up is the concept of leveraging recursive musig2 (so musig2 within musig2) to make channels even _more_ multi-sigy.\n\nA minor point but terminology can get frustratingly sticky if it isn't agreed on early. Can we refer to it as nested\u200b\u200b MuSig2 going forward rather than recursive\u200b\u200b MuSig2? It is a more accurate description in my opinion and going through some old transcripts the MuSig2 authors [0] also refer it to nested MuSig2 (as far as I can make out).\n\nRene Pickhardt brought up the issue of latency with regards to nested/recursive MuSig2 (or nested FROST for threshold) on Bitcoin StackExchange [1]. Was this discussed at the LN Summit? I don't know how all the Lightning implementations treat latency currently (how long a channel counterparty has to provide a needed signature before moving to a unhappy path) but Rene's concern is delays in the regular completions of a nested MuSig2 or FROST scheme could make them unviable for the Lightning channel use case depending on the exact setup and physical location of signers etc.\n\nMuSig2 obviously generates an aggregated Schnorr signature and so even nested MuSig2 require the Lightning protocol to recognize and verify Schnorr signatures which it currently doesn't right? So is the current thinking that Schnorr signatures will be supported first with a Schnorr 2-of-2 on the funding output (using OP_CHECKSIGADD and enabling the nested schemes) before potentially supporting non-nested MuSig2 between the channel counterparties on the funding output later? Or is this still in the process of being discussed?\n\n[0]: https://btctranscripts.com/london-bitcoin-devs/2020-06-17-tim-ruffing-schnorr-multisig/\n[1]: https://bitcoin.stackexchange.com/questions/114159/how-do-the-various-lightning-implementations-treat-latency-how-long-do-they-wai\n\n--\nMichael Folkson\nEmail: michaelfolkson at [protonmail.com](http://protonmail.com/)\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n\n------- Original Message -------\nOn Wednesday, June 8th, 2022 at 03:38, Olaoluwa Osuntokun <laolu32 at gmail.com> wrote:\n\n> Hi y'all,\n>\n> Last week nearly 30 (!) Lightning developers and researchers gathered in\n> Oakland, California for three day to discuss a number of matters related to\n> the current state and evolution of the protocol. This time around, we had\n> much better representation for all the major Lightning Node implementations\n> compared to the last LN Dev Summit (Zurich, Oct 2021).\n>\n> Similar to the prior LN Dev Summit, notes were kept throughout the day that\n> attempted on a best effort basis to capture the relevant discussions,\n> decisions, and new relevant research or follow up areas to circle back on.\n> Last time around, I sent out an email that summarized some key takeaways\n> (from my PoV) of the last multi-day dev summit [1]. What follows in this\n> email is a similar summary/recap of the three day summit. Just like last\n> time: if you attended and felt I missed out on a key point, or inadvertently\n> misrepresented a statement/idea, please feel free to reply, correcting or\n> adding additional detail.\n>\n> The meeting notes in full can be found here:\n> https://docs.google.com/document/d/1KHocBjlvg-XOFH5oG_HwWdvNBIvQgxwAok3ZQ6bnCW0/edit?usp=sharing\n>\n> # Simple Taproot Channels\n>\n> During the last summit, Taproot was a major discussion topic as though the\n> soft fork had been deployed, we we're all still watching the \ud83d\udfe9 's stack up\n> on the road to ultimately activation. Fast forward several months later and\n> Taproot has now been fully activated, with ecosystem starting to\n> progressively deploy more and more advanced systems/applications that take\n> advantage of the new features.\n>\n> One key deployment model that came out of the last LN Dev summit was the\n> concept of an iterative roadmap that progressively revamped the system to\n> use more taprooty features, instead of a \"big bang\" approach that would\n> attempt to package up as many things as possible into one larger update. At\n> a high level the iterative roadmap proposed that we unroll an existing\n> larger proposal [2] into more bite sized pieces that can be incrementally\n> reviewed, implemented, and ultimately deployed (see my post on the LN Dev\n> Summit 2021 for more details).\n>\n> ## Extension BOLTs\n>\n> Riiight before we started on the first day, I wrote up a minimal proposal\n> that attempted to tackle the first two items of the Taproot iterative\n> deployment schedule (musig2 funding outputs and simple tapscript mapping)\n> [3]. I called the proposal \"Simple Taproot Channels\" as it set out to do a\n> mechanical mapping of the current commitment and script structure to a more\n> taprooty domain. Rather than edit 4 or 5 different BOLTs with a series of\n> \"if this feature bit applies\" nested clauses, I instead opted to create a\n> new standalone \"extension bolt\" that defines _new_ behavior on top of the\n> existing BOLTs, referring to the BOLTs when necessary. The style of the\n> document was inspired by the \"proposals\" proposal (very meta), which was\n> popularized by cdecker and adopted by t-bast with his documents on\n> Trampoline and Blinded Paths.\n>\n> If the concept catches on, extension BOLTs provide us with a new way to\n> extend the spec: rather than insert everything in-line, we could instead\n> create new standalone documents for larger features. Having a single self\n> contained document makes the proposal easier to review, and also gives the\n> author more room to provide any background knowledge, primaries, and also\n> rationale. Overtime, as the new extensions become widespread (eg: taproot is\n> the default channel type), we can fold in the extensions back to the main\n> set of \"core\" BOLTs (or make new ones as relevant).\n>\n> Smaller changes to the spec like deprecating an old field or tightening up\n> some language will likely still follow the old approach of mutating the\n> existing BOLTs, but larger overhauls like the planned PTLC update may find\n> the extension BOLTs to be a better tool.\n>\n> ## Tapscript, Musig2, and Lightning\n>\n> As mentioned above the Simple Taproot Channels proposal does two main\n> things:\n> 1. Move the existing 2-of-2 p2wsh segwit v0 funding output to a _single\n> key_ p2tr output, with the single key actually being an aggregated musig2\n> key.\n>\n> 2. Map all our existing scripts to the tapscript domain, using the\n> internal key (keyspend path) for things like revocations, which an\n> potentially allow nodes to store less state for HTLCs.\n>\n> Of the two components #1 is by far the trickiest. Musig2 is a very elegant\n> protocol (not to mention the spec which y'all should totally check out) but\n> as the signatures aren't deterministic (like RFC 6979 [5]), both signers\n> need to \"protect themselves at all times\" to ensure they don't ever re-use\n> nonces, which can lead to a private key leak (!!).\n>\n> Rather than try to create some sort of psuedo-deterministic nonces scheme\n> (which maaybe works until the Blockstream Research team squints vaguely in\n> its direction), I opted to just make all nonces 100% ephemeral and tied to\n> the lifetime of a connection. Musig2 defines something called a public\n> nonces, which is actually two individual 33-byte nonces. This value needs to\n> be exchanged before signing can begin (but can be sent before sides know\n> they're aggregated keys). One important thing to note is that given that the\n> channels today have _asymmetric_ state, we actually need a _pair_ of public\n> nonces: one that I'll use to sign my commitment, and one I'll use to sign\n> yours. Lightning channels w/ symmetric state like eltoo can get by w/ only\n> exchange a single set of nonces, as there's only one message per state.\n>\n> Nonce exchange takes place in a few places:\n>\n> * During initial funding: I send my public nonce in the open_channel\n> message, you send yours in the accept_channel message. After this\n> exchange we can both generate signatures for the refund commitment\n> transactions.\n>\n> * After the channel is \"ready\" we send another set of nonces, so we can\n> sign the next state. This is similar to the existing revocation key\n> exchange: I need your next nonce/key before I can sign a new state.\n>\n> * Upon channel re-establishment a _new_ set of nonces is sent, as they're\n> 100% ephemeral. The current draft also requires that if you were\n> re-transmitting a sig, then you use the _new_ nonces to sign again, as\n> it's possible you went to retransmit but left off an expired/trimmed\n> HLTC (could lead to nonce re-use and also needing to remember nonces).\n>\n> * Each time I revoke my channel, I send to you a single nonce, my \"local\n> nonce\" (naming needs some work here), which lets you sign for a new\n> state.\n>\n> * Each time I send a new sig, I also send you another nonce, my \"remote\"\n> nonce\", which\n>\n> * When I send a shutdown (co-op close) I send a single public nonce so we\n> can sign the next co-opc close offer.\n>\n> * When I send a closing_signed I send another nonce so once you send your\n> offer, we sign another set.\n>\n> The final flows aren't 100% yet finalized, as we'll need some\n> implementations drafted to make sure the nonce handling and script mapping\n> works out properly.\n>\n> ### Lightning Channels & Recursive Musig2\n>\n> One other cool topic that came up is the concept of leveraging recursive\n> musig2 (so musig2 within musig2) to make channels even _more_ multi-sigy.\n> The benefit here is that Bob & Carol can each have their individual keys\n> (which might actually be aggregated keys themselves) and make a channel w/\n> Alice, who only knows of them as Barol, and doesn't know there're actually\n> another pair of keys at play. This is _really_ cool as it allows node\n> operators, wallets, and lightning platforms to experiment with various\n> key/signing trees that may add more security, redundancy, or flexibility.\n>\n> When this first came up, someone brought up the fact that while the scheme\n> is \"known\" the initial paper as they weren't sure how to actually write a\n> proof for it. During the session, someone emailed one of the musig2 authors\n> asking for more details, and if it's safe to implement and roll out.\n> Thankfully they quickly replied and explained that the proof recursive musig\n> (pls someone correct me again here if I'm wrong) wasn't left out due to\n> impossibility, but that a proof in the existing Random Oracle Model (which\n> was used to derive a bound for the number of nonces needed) would lead to a\n> blow up in the number of nonces required. Attempting to write the proof in\n> some other model would likely lead to better results (proved w/ two nonces\n> as base musig2), but would end up being pretty complicated, so hard to read\n> and even review for correctness.\n>\n> Assuming everything checks out, then a useful mental model explained by the\n> musig2 BIP author is a sort of tree structure. Assuming I'm a signer, and we\n> assemble the other signer as a sibling leaf in a binary tree, then I just\n> need to wait for the sibling nonce/key, before I can aggregate that into\n> the final value. So if there're 3 signers, I wait for the regular public\n> nonce, but the other signers sum their respective nonces into a single\n> nonce, then send that to me. A similar operation is carried out for key\n> aggregation, with the rest of the protocol being mostly the same.\n>\n> Ultimately, even if wallets/nodes aren't ready to roll something like this\n> out today, we at least want to make sure the proposed flow is compatible\n> with Simple Taproot Channels, and ideally we'd have a toy implementation to\n> verify out understanding and show it's possible/sound. I volunteered to hack\n> up a simple recursive musig2 demo, as there doesn't seem to be any code in\n> the wild that implements it.\n>\n> ## Lightning Gossip\n>\n> # Gossip V2: Now Or Later?\n>\n> Another big topic related to Taproot was the question of how we should\n> update the gossip network: the gossip protocol today has all channels\n> validated by node, which requires that the nodes understand how to\n> reconstruct the funding output based on the set of advertised keys. The\n> protocol today assumes a segwit v0 p2wsh multi-sig is used. Assuming we had\n> everything implemented today, a node wouldn't be able to advertise its new\n> taproot channels to the rest of the public graph as they wouldn't understand\n> how to validate it.\n>\n> This presents a new opportunity: we already need to rework gossip for\n> taproot, so should we go ahead and re-design the entire thing with an eye\n> for better privacy and future extensibility?\n>\n> A proposal for the \"re-design the entire thing\" was floated in the past by\n> Rusty [6]. It does away with the strict coupling of channels to channel\n> announcements, and instead moves them to the _node_ level. Each node would\n> then advertise the set of \"outputs\" they have control of, which would then\n> be mapped to the total capacity of a node, without requiring that these\n> outputs self identify themselves on-chain as Lightning Channels. This also\n> opens up the door to different, potentially more privacy preserving\n> proofs-of-channel-ownership (something something zkp).\n>\n> On the other hand, we could just follow the path of Simple Taproot Channels\n> and map musig2+schnorr onto the existing gossip network. This is less\n> changes in total, with the main benefit being the ability to only send 1 sig\n> (aggregated musig2 sig of keys) instead of 4 individual sigs. I made a very\n> lofty proposal in this direction here [7].\n>\n> Ultimately we decided to take the \"just musig2 aspects\" from gossip v1.5\n> (not the real name), and the \"let's refresh all the messages w/ TLV\n> goodness\" from the gossip v2 proposal. This gives us a smaller package to\n> implement, and lets us potentially rejigger the messages to be more\n> extensible and remove cruft like the node color that almost nothing uses,\n> but we all validate/store.\n>\n> The follow up work in this area is a more concrete proposal that updates the\n> relevant gossip messages to be taproot aware and TLV'd and also update the\n> set of requirements w.r.t _how_ to validate the channels in the first place\n> (so given two keys verify that applying the keyagg method of musig2 lead to\n> what' in the funding output).\n>\n> Gossip v2 will likely happen \"eventually\", but the rather large design space\n> needs to be explored a bit more so we can properly analyze exactly what\n> privacy and extensibility properties we'll get out of it.\n>\n> # Applying Mini Sketch to LN Gossip\n>\n> One issue we have today, is that other than the initial scid query mechanism\n> added to the protocol, there isn't a great way to ensure you have all the\n> latest updates your peer has. These days, many nodes pretty aggressively\n> rate limit other nodes, so you might even have trouble sending out your\n> update in the first place. A recent paper (that I haven't actually fully\n> read yet) [8] analyzes the gossip network today to work out things like:\n> exactly how long it takes things to propagate, total bandwidth usage, etc.\n>\n> Minisketch [9] (the grandchild of IBLTs ;)), is an efficient set\n> reconciliation protocol that was designed for Bitcoin p2p mempool syncing,\n> but can be applied to other protocols. An attendee has been working on\n> brushing off some older work to try to see how we could apply it to the LN\n> protocol to give nodes a more bandwidth efficient way to sync channel\n> updates, and also achieve better update propagation. This supplements some\n> existing investigative work done by Alex Meyers [10], with more concrete\n> designs w.r.t: what goes into the sketch, and the various size parameters\n> that need to be chosen.\n>\n> # Channel Jamming\n>\n> An attendee gave a talk on the various proposed solutions to channel\n> jamming, evaluating them on several axis including: punishment/monetary,\n> local vs global reputation, feasibility of mechanism design, UX\n> implications, and implementation complexity. The presenter didn't present a\n> new concrete proposal, but instead went through the various trade-offs,\n> ultimately concluding that they factor monetary penalties wherein the funds\n> are distributed across the route, rather than being provably burnt to\n> miners. However they alluded to some future upcoming work that attempts a\n> more rigorous analysis of the proposed solutions, their tradeoffs, and\n> potential ways we can parametrize solutions to be more effective (how much\n> should they pay, etc).\n>\n> For those looking to brush up on the latest state of research/mitigations in\n> this area, I recommend this blog post by Bitmex research [11].\n>\n> # Onion Messages & DoS\n>\n> The topic of DoS concerns related to onion messages (in isolation, so not\n> necessarily related to things like bolt12 that take advantage of them came\n> up. During a white boarding session some argued that DoS isn't actually\n> much of an issue, as nodes can leverage \"back propagation congestion\n> control\" to inform the source (who may not actually be the sender) that\n> they'll start to drop or limit their packets, with each node doing this\n> iteratively until the actual source of the spam has been clamped. A few\n> lofty designs were thrown around, but more work needs to be done to\n> concretely specify something so it can be properly analyzed.\n>\n> On the other side of the spectrum, rather than attempt to rate limit at the\n> node level (which each node having their own policy), nodes could opt\n> instead to forward _anything_ as long as the sender pays them enough. I\n> proposed a lofty approach that combined AMP and Onion Messages earlier this\n> year [12]. At a high level I make an AMP payment, which pushes extra coins\n> to all nodes on a route, and also drops off a special identifier to them.\n> When I send an onion message I include this identifier, with each node\n> performing their own account w.r.t the amount of bandwidth an ID has\n> remaining.\n>\n> Ultimately a few implementations are pretty close to deploying their\n> implementation of onion messages, so no matter the intended use case, it\n> would be good to have code deployed along side to either rate limit or price\n> resource consumption accordingly. Otherwise, we might end up in a scenario\n> where DoS concerns were brushed aside, but end up being a huge issue later.\n>\n> # Blinded Paths, QR Codes & Invoices\n>\n> Blinded paths [13] is a new-er proposal to solve the \"last mile\" privacy\n> issue when receiving payments on LN. Today invoices to unadvertised channels\n> contain a set of hop hints, which are anchored at public nodes in the graph,\n> and also leak the scid of the unadvertised channel (points on-chain to the\n> channel receiving payments). A solution for the on-chain leak, SCID channel\n> aliases [15] are in the process of being widely rolled out. Channel aliases\n> instead use a random value in the invoice, allowing receiving nodes to break\n> that on-chain link and even rotate out the value periodically. With the\n> on-chain leak addressed, it's still the case that you give away your\n> \"position\" in the network, since as a sender I know that you're connected to\n> node N with a private channel.\n>\n> Blinded paths address this node-level last mile privacy leak by replacing\n> hop hints with a new cryptographically blinded path. At a high level, the\n> receiver can construct a \"hop hint\" of length greater than 1, gather the\n> public keys of each of the nodes, then blinded them such that: the sender\n> can use them for path finding, but doesn't actually now exactly _which_\n> nodes they actually are.\n>\n> There're two type of blinded paths: those in onion messages and those used\n> for actual payments. The latter variant was only formalized earlier this\n> year, as before people were mainly interested in using them to fetch BOLT 12\n> invoice via onion messages. One issue that pops up when attempting to use\n> blinded paths for normal payments is: the size of the resulting invoice. As\n> blinded paths are actually fragments of publicly known paths, as a receiver,\n> you want to stuff as many of them into the invoice as possible, since they\n> MUST be taken in order to route towards you. Invoices are typically\n> communicated via QR codes, which have a hard limit w.r.t the amount of\n> information that can be packed in. On the other hand for invoice fetching,\n> all that matters is that a path exists, so you can get by with stuffing less\n> of then in a QR code.\n>\n> As a result, blinded paths aren't necessarily compatible with the widely\n> deployed BOLT 11 based QR codes. Instead a way to fetch invoice on demand is\n> required. Both BOLT-12 and LN-URL provide standardized ways for nodes to\n> fetch invoices, though their transport/signalling medium of choice differs.\n> Blinded routes are technically compatible with BOLT 11 invoices, but may be\n> hampered by the fact that you can only include so many routes.\n>\n> Another consideration is that unlike hop hints, blinded paths require more\n> maintain once, as since they traverse public route, policy changes like a\n> fee update may invalidate an entire set set of routes. One proposed solution\n> is that forwarding nodes should observe their older policy for a period of\n> time (so a grace period), and also that blinded paths should have an\n> explicit expiry (similar to the existing invoice expiry).\n>\n> One other implication is that the set of routes the receiver includes matters\n> more: if they don't send enough or select them poorly, the sender may never be\n> able to reach them even though a path exists in theory. More hands on\n> experience is needed so the spec authors can better guide implementations and\n> wallets w.r.t best practices.\n>\n> # Friend-of-a-friend Balance Sharing & Probing\n>\n> A presentation was given on friend-of-a-friend balance sharing [16]. The\n> high level idea is that if we share _some_ information within a local\n> radius, then this gives the sender more information to choose a path that's\n> potentially more reliable. The tradeoff here ofc is that nodes will be\n> giving away more information that can potentially be used to ascertain\n> payment flows. In an attempt to minimize the amount of information shared,\n> the presenter proposed that just 2 bits of information be shared. Some\n> initial simulations showed that sharing local information actually performed\n> better than sharing global information (?). Some were puzzled w.r.t how\n> that's possible, but assuming the slides+methods are published others can\n> dig further into the model/parameter used to signal the inclusion.\n>\n> Arguably, information like this is already available via probing, so one\n> line of thinking is something like: \"why not just share _some_ of it\" that\n> may actually lead to less internal failures? This is related to a sort of\n> tension between probing as a tool to increase payment reliability and also\n> as a tool to degrade privacy in the network. On the other hand, others\n> argued that probing provides natural cover traffic, since they actually\n> _are_ payments, though they may not be intended to succeed.\n>\n> On the topic of channel probing, a sort of makeshift protocol was devised to\n> make it harder in practice, sacrificing too much on the axis of payment\n> reliability. At a high level it proposes that:\n>\n> * nodes more diligently set both their max_htlc amount, as well as the\n> max_htlc_value_in_flight amount\n>\n> * a 50ms (or select other value) timer should be used when sending out\n> commitment signatures, independent of HTLC arrival\n>\n> * nodes leverage the max_htlc value to set a false ceiling on the max in\n> flight parameter\n>\n> * for each HTLC sent/forwarded, select 2 other channels at random and\n> reduce the \"fake\" in-flight ceiling for a period of time\n>\n> Some more details still need to be worked out, but some felt that this would\n> kick start more research into this area, and also make balance mapping\n> _slightly_ more difficult. From afar, it may be the case that achieving\n> balance privacy while also achieving acceptable levels of payment\n> reliability might be at odds with each other.\n>\n> # Eltoo & ANYPREVOUT\n>\n> One of the attendees is currently working on both fully implementing eltoo,\n> as well as specifying the exact channel funding+update interaction were it\n> to be rolled out align side the existing penalty based channels in the\n> protocol. As this version of eltoo is based on Taproot, we were able to\n> compare notes a bit to find the overlapping set of changes (nonce handling,\n> etc), which permits cross review of the proposals. This type of work is\n> cool, as only by fully implementing something end to end can you reaaally\n> work out all the edge cases and nuances.\n>\n> ANYPREVOUT as hasn't changed significantly as of late. An attendee shared\n> plans to create a sort of mega all-future-feasible-soft-forks fork of\n> bitcoind, that would package up various unmerged (from bitcoind's) proposal\n> soft fork packages into an easy to run+install binary/project attached to a\n> signet. The hop is that by giving developers an easy way to interact with\n> proposed soft fork proposals (vs debasing some ancient pull request), wider\n> participation in testing/implementation/review can be facilitated.\n>\n> # Trampoline Routing\n>\n> There was a presentation on Trampoline routing explaining the motivation,\n> history, and current state of the proposal. The two main cases we've\n> narrowed down on are:\n>\n> 1. A mobile user doesn't necessarily want to sync the _entire_ graph, so\n> they can use trampoline to maintain a subset and still be able to send\n> payments.\n>\n> 2. A mobile user wants to be able to instate a payment, go offline, and\n> return at a later time to learn about the final state of the payment.\n>\n> Use case #2 seems to be the most promising when combined with other\n> proposals for holding HTLCs at an origin node (call it an \"LSP\") [13].\n> Combined together, this would allow a mobile node to send a payment, then go\n> offline, with the LSP being able to retry the payment either continuously or\n> only when it knows the receiver is online to accept the payment. This may\n> potentially dramatically improve the UX for LN on mobile, as things suddenly\n> become a lot more asynchronous: I do something go offline, and the LSP node\n> can fulfil the payment in the background, then wait for me to come online to\n> settle the final. hop.\n>\n> Trampoline can also be composed well with blinded routes (blinded route from\n> last trampoline to receiver) and also MPP (internal nodes can split\n> themselves with local information).\n>\n> One added trade-off is that since the sender doesn't know the entire route,\n> they need to sort of overshoot w.r.t fees and CTLVs. This is something\n> we've known for a while, but until Trampoline is more widely rolled out, we\n> won't have a very good feel w.r.t how much extra senders will need to\n> allocate.\n>\n> # Node Fee Optimization & Fee Rate Cards\n>\n> Over the past few years, a common thread we've seen across successful\n> routing nodes is dynamic fee setting as a way to encourage/discourage\n> traffic. A routing nodes can utilize the set of fees of a channel to either\n> make it too expensive for other nodes to route through (it's already\n> depleted don't try unless you'll give be 10 mil sats, which no one would) or\n> very cheap, which'll incentivize flows in the other direction. If all nodes\n> are constantly sending out updates of this nature, then it can generate a\n> lot of traffic, and also sort of leak more balance information overtime\n> (which some nodes are already doing: using fees/max_htlc to communicate\n> available balances).\n>\n> One attendee proposed allowing nodes to express a sort of fee gradient via a\n> static curve/bucket/function, instead of dynamically communicating what the\n> latest state of the fee+liquidity distribution looks like. A possible\n> manifestation could be a series of buckets, each of which with varying fee\n> rates. If your payment consumes 50% of channel balance, then you pay this\n> rate, otherwise if it's 5% you pay this rate, etc, etc. This might allow for\n> nodes to capture the same dynamics as they do with more dynamic fee updates,\n> but in a way that leaks less information and also consumes less gossip\n> bandwidth.\n>\n> # The Return of Splicing\n>\n> Splicing is one of those things that was discussed a long time ago, but was\n> never really fully implemented and rolled out. A few attendees have started\n> to take a closer look at the problem, building off of the interactive-tx\n> scheme that the dual-funding protocol extension uses. The main intricacy\n> discussed was if concurrent splices should be allowed or not, and if so, how\n> we would handle the various edge cases. As an example, if I propose a splice\n> to add more funds via my input, but that turns out to already be spent, then\n> the splicing transaction we created is invalid and can never be confirmed.\n> However if we allow _another_ splice to take place, and another one, and\n> another one, then ideally _one_ of them will confirm and serve as the new\n> anchor for the channel.\n>\n> In a world of concurrent splices, the question of \"what is my Lightning\n> balance\" becomes even more murky. Wallet and implementations will likely\n> want to show the most pessimistic value, while also ensuring that the user\n> is able to effectively account for where all their funds and what they can\n> spend on/off chain.\n>\n> # LN-URL + BOLT 12\n>\n> LN-URL and BOLT 12 are both standardized ways that answer the question of:\n> how can I fetch an invoice from Bob? LN-URL differs from BOLT 12 in that it\n> uses the existing BOLT 11 invoice format, and uses an HTTP based protocol\n> for the negotiation process. BOLT 12 on the other hand is a suite of\n> protocol additions that includes (amongst other things) a new invoice format\n> (yay TLV!) and also a way to use onion messages to fetch an invoice _via_\n> the network.\n>\n> Assuming blinded paths is widely rolled out, then the question of how\n> invoices are obtained becomes more important as blinded paths means that you\n> can't fit much in the traditional QR encoding. As a result, fetching\n> invoices on demand may become a more common place flow, with all its\n> trade-offs. There was a group discussion on how we could sort of unifying\n> everything either by allowing BOLT 12 to be used over LN-URL or the other\n> way around.\n>\n> One proposal was to add a new query parameter to the normal LN-URL QR code\n> contents. This would mean that when a wallet goes to scan an LN-URL QR code,\n> if they know of the extra param, and what BOLT 12, they can just use the\n> enclosed offer to fetch the invoice.\n>\n> An alternative proposal was to instead extract the BOLT 12 _invoice_ format\n> from the greater BOLT 12 \"Offers\" proposal. Assuming blinded paths is only\n> specified w.r.t BOLT 12 _invoices_, then this would mean an LN-URL extension\n> could be rolled out that allowed returning BOLT 12 invoice rather than BOLT\n> 11 invoices. This would allow the ecosystem to slowly transition to a shared\n> invoice format, even if there may be fundamental disagreements w.r.t _how_\n> the invoices should be fetched in the first place.\n>\n> It's worth noting that both of these proposals can be combined:\n>\n> * If a wallet knows how to BOLT 12 Offers, they can take the enclosed\n> offer and run w/ it.\n>\n> * If they don't know about Offers, but can send w/ the BOLT _invoice_\n> format, then they can fetch that and complete the payment.\n>\n> This might be a nice middle ground as it would tend all\n> wallets/implementations to being able to decode and send w/ a BOLT 12\n> _invoice_, and leave the question of _how_ it should be fetched up to the\n> application/wallet/service. In the end, if paths never quite intersect, then\n> it's still possible to add route blinding to BOLT 11, with LN-URL sticking\n> with that invoice format to take advantage of the new privacy enhancements\n>\n> [1]: https://lists.linuxfoundation.org/pipermail/lightning-dev/2021-November/003336.html\n> [2]: https://lists.linuxfoundation.org/pipermail/lightning-dev/2021-October/003278.html\n> [3]: https://github.com/lightning/bolts/pull/995\n> [4]: https://github.com/jonasnick/bips/blob/musig2/bip-musig2.mediawiki\n> [5]: https://datatracker.ietf.org/doc/html/rfc6979\n> [6]: https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-February/003470.html\n> [7]: https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-March/003526.html\n> [8]: https://arxiv.org/abs/2205.12737\n> [9]: https://bitcoinops.org/en/topics/minisketch/\n> [10]: https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-April/003551.html\n> [11]: https://blog.bitmex.com/preventing-channel-jamming/\n> [12]: https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-February/003498.html\n> [13]: https://github.com/lightning/bolts/pull/765\n> [14]: https://lists.linuxfoundation.org/pipermail/lightning-dev/2021-October/003307.html\n> [15]: https://github.com/lightning/bolts/pull/910\n> [16]: https://github.com/lightning/bolts/pull/780\n>\n> -- Laolu\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220621/89814a22/attachment-0001.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2022-06-23T22:10:36",
                "message_text_only": "Hi Michael,\n\n> A minor point but terminology can get frustratingly sticky if it isn't\n> agreed on early. Can we refer to it as nested MuSig2 going\n> forward rather than recursive MuSig2?\n\nNo strong feelings on my end, the modifier _nested_ is certainly a bit less\nloaded and conceptually simpler, so I'm fine w/ using that going forward if\nothers are as well.\n\n> Rene Pickhardt brought up the issue of latency with regards to\n> nested/recursive MuSig2 (or nested FROST for threshold) on Bitcoin\n> StackExchange\n\nNot explicitly, but that strikes me as more of an implementation level\nconcern. As an example, today more nodes are starting to use replicated\ndatabase backends instead of a local ed embedded database. Using such a\ndatabase means that _network latency_ is now also a factor, as committing\nnew states requires round trips between the DBMS that'll increase the\nperceived latency of payments in practice. The benefit ofc is better support\nfor backups/replication.\n\nI think in the multi-signature setting for LN, system designers will also\nneed to factor in the added latency due to adding more signers into the mix.\nAlso any system that starts to break up the logical portions of a node\n(signing, hosting, etc -- like Blockstream's Greenlight project), will need\nto wrangle with this as well (such is the nature of distributed systems).\n\n> MuSig2 obviously generates an aggregated Schnorr signature and so even\n> nested MuSig2 require the Lightning protocol to recognize and verify\n> Schnorr signatures which it currently doesn't right?\n\nCorrect.\n\n> So is the current thinking that Schnorr signatures will be supported first\n> with a Schnorr 2-of-2 on the funding output (using OP_CHECKSIGADD and\n> enabling the nested schemes) before potentially supporting non-nested\n> MuSig2 between the channel counterparties on the funding output later? Or\n> is this still in the process of being discussed?\n\nThe current plan is to jump straight to using musig2 in the funding output,\nso: a single aggregated 2-of-2 key, with a single multi-signature being used\nto close the channel (co-op or force close).\n\nRe nested vs non-nested: to my knowledge, if Alice uses the new protocol\nextensions to open a taproot channel w/ Bob, then she wouldn't necessarily\nbe aware that Bob is actually Barol (Bob+Carol). She sees Bob's key (which\nmight actually be an aggregated key) and his public nonce (which might\nactually also be composed of two nonces), and just runs the protocol as\nnormal. Sure there might be some added latency depending on Barol's system\narchitecture, but from Alice's PoV that might just be normal network latency\n(eg: Barol is connecting over Tor which already adds some additional\nlatency).\n\n-- Laolu\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220623/e1eb0263/attachment.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2022-06-28T13:05:46",
                "message_text_only": "Olaoluwa Osuntokun <laolu32 at gmail.com> writes:\n>> Rene Pickhardt brought up the issue of latency with regards to\n>> nested/recursive MuSig2 (or nested FROST for threshold) on Bitcoin\n>> StackExchange\n>\n> Not explicitly, but that strikes me as more of an implementation level\n> concern. As an example, today more nodes are starting to use replicated\n> database backends instead of a local ed embedded database. Using such a\n> database means that _network latency_ is now also a factor, as committing\n> new states requires round trips between the DBMS that'll increase the\n> perceived latency of payments in practice. The benefit ofc is better support\n> for backups/replication.\n>\n> I think in the multi-signature setting for LN, system designers will also\n> need to factor in the added latency due to adding more signers into the mix.\n> Also any system that starts to break up the logical portions of a node\n> (signing, hosting, etc -- like Blockstream's Greenlight project), will need\n> to wrangle with this as well (such is the nature of distributed systems).\n\nIt is worth mentioning here that the LN protocol is generally not very\nlatency sensitive, and from my experience can easily handle very slow\nsigners (3-5 seconds delay) without causing too many issues, aside from\nslower forwards in case we are talking about a routing node. I'd expect\nrouting node signers to be well below the 1 second mark, even when\nimplementing more complex signer logic, including MuSig2 or nested\nFROST.\n\nIn particular remember that the LN protocol implements a batch\nmechanism, with changes applied to the commitment transaction as a\nbatch. Not every change requires a commitment and thus a signature. This\nmeans that while a slow signer may have an impact on payment latency, it\nshould generally not have an impact on throughput on the routing nodes.\n\nRegards,\nChristian"
            },
            {
                "author": "Matt Corallo",
                "date": "2022-06-28T15:31:54",
                "message_text_only": "On 6/28/22 9:05 AM, Christian Decker wrote:\n> It is worth mentioning here that the LN protocol is generally not very\n> latency sensitive, and from my experience can easily handle very slow\n> signers (3-5 seconds delay) without causing too many issues, aside from\n> slower forwards in case we are talking about a routing node. I'd expect\n> routing node signers to be well below the 1 second mark, even when\n> implementing more complex signer logic, including MuSig2 or nested\n> FROST.\n\nIn general, and especially for \"edge nodes\", yes, but if forwarding nodes start taking a full second \nto forward a payment, we probably need to start aggressively avoiding any such nodes - while I'd \nlove for all forwarding nodes to take 30 seconds to forward to improve privacy, users ideally expect \npayments to complete in 100ms, with multiple payment retries in between.\n\nThis obviously probably isn't ever going to happen in lightning, but getting 95th percentile \npayments down to one second is probably a good goal, something that requires never having to retry \npayments and also having forwarding nodes not take more than, say, 150ms.\n\nOf course I don't think we should ever introduce a timeout on the peer level - if your peer went \naway for a second and isn't responding quickly to channel updates it doesn't merit closing a \nchannel, but its something we will eventually want to handle in route selection if it becomes more \nof an issue going forward.\n\nMatt"
            },
            {
                "author": "Peter Todd",
                "date": "2022-06-28T23:11:10",
                "message_text_only": "On Tue, Jun 28, 2022 at 11:31:54AM -0400, Matt Corallo wrote:\n> On 6/28/22 9:05 AM, Christian Decker wrote:\n> > It is worth mentioning here that the LN protocol is generally not very\n> > latency sensitive, and from my experience can easily handle very slow\n> > signers (3-5 seconds delay) without causing too many issues, aside from\n> > slower forwards in case we are talking about a routing node. I'd expect\n> > routing node signers to be well below the 1 second mark, even when\n> > implementing more complex signer logic, including MuSig2 or nested\n> > FROST.\n> \n> In general, and especially for \"edge nodes\", yes, but if forwarding nodes\n> start taking a full second to forward a payment, we probably need to start\n> aggressively avoiding any such nodes - while I'd love for all forwarding\n> nodes to take 30 seconds to forward to improve privacy, users ideally expect\n> payments to complete in 100ms, with multiple payment retries in between.\n\nIdle question: would it be worthwhile to allow people to opt-in to their\npayments happening more slowly for privacy? At the very least it'd be fine if\npayments done by automation for rebalancing, etc. happened slowly.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220628/0853d86e/attachment.sig>"
            },
            {
                "author": "Matt Corallo",
                "date": "2022-06-30T16:38:37",
                "message_text_only": "> On Jun 28, 2022, at 19:11, Peter Todd <pete at petertodd.org> wrote:\n> \n> Idle question: would it be worthwhile to allow people to opt-in to their\n> payments happening more slowly for privacy? At the very least it'd be fine if\n> payments done by automation for rebalancing, etc. happened slowly.\n\nYea, actually, I think that\u2019d be a really cool idea. Obviously you don\u2019t want to hold onto an HTLC for much longer than you have to or you\u2019re DoS\u2019ing yourself using up channel capacity, but most channels spend the vast majority of their time with zero HTLCs, so waiting a second instead of 100ms to batch seems totally reasonable."
            },
            {
                "author": "Christian Decker",
                "date": "2022-06-29T10:56:39",
                "message_text_only": "Matt Corallo <lf-lists at mattcorallo.com> writes:\n\n> On 6/28/22 9:05 AM, Christian Decker wrote:\n>> It is worth mentioning here that the LN protocol is generally not very\n>> latency sensitive, and from my experience can easily handle very slow\n>> signers (3-5 seconds delay) without causing too many issues, aside from\n>> slower forwards in case we are talking about a routing node. I'd expect\n>> routing node signers to be well below the 1 second mark, even when\n>> implementing more complex signer logic, including MuSig2 or nested\n>> FROST.\n>\n> In general, and especially for \"edge nodes\", yes, but if forwarding nodes start taking a full second \n> to forward a payment, we probably need to start aggressively avoiding any such nodes - while I'd \n> love for all forwarding nodes to take 30 seconds to forward to improve privacy, users ideally expect \n> payments to complete in 100ms, with multiple payment retries in between.\n>\n> This obviously probably isn't ever going to happen in lightning, but getting 95th percentile \n> payments down to one second is probably a good goal, something that requires never having to retry \n> payments and also having forwarding nodes not take more than, say, 150ms.\n>\n> Of course I don't think we should ever introduce a timeout on the peer level - if your peer went \n> away for a second and isn't responding quickly to channel updates it doesn't merit closing a \n> channel, but its something we will eventually want to handle in route selection if it becomes more \n> of an issue going forward.\n>\n> Matt\n\nAbsolutely agreed, and I wasn't trying to say that latency is not a\nconcern, I was merely pointing out that the protocol as is, is very\nlatency-tolerant. That doesn't mean that routers shouldn't strive to be\nas fast as possible, but I think the MuSig schemes, executed over local\nlinks, is unlikely to be problematic when considering overall network\nlatency that we have anyway.\n\nFor edge nodes it's rather nice to have relaxed timings, given that they\nmight be on slow or flaky connections, but routers are a completely\ndifferent category.\n\nChristian"
            }
        ],
        "thread_summary": {
            "title": "LN Summit 2022 Notes & Summary/Commentary",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Matt Corallo",
                "Peter Todd",
                "Michael Folkson",
                "Bastien TEINTURIER",
                "Olaoluwa Osuntokun",
                "ZmnSCPxj",
                "Christian Decker"
            ],
            "messages_count": 10,
            "total_messages_chars_count": 88442
        }
    },
    {
        "title": "[Lightning-dev] Why OpenTimestamps does not \"linearize\" its transactions",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2022-06-14T11:12:14",
                "message_text_only": "On Mon, May 02, 2022 at 08:59:49AM -0700, Jeremy Rubin wrote:\n> Ok, got it. Won't waste anyone's time on terminology pedantism.\n> \n> \n> The model that I proposed above is simply what *any* correct timestamping\n> service must do. If OTS does not follow that model, then I suspect whatever\n> OTS is, is provably incorrect or, in this context, unreliable, even when\n> servers and clients are honest.\n\nDo you think RFC 3628 is \"provably incorrect\" too? It's just a standard for\nTrusted Time-Stamping Authorities to issue timestamp proofs via digital\nsignatures, in the most straight forward manner of signing a message claiming\nthat some digest existed as of some time.\n\nAs the RFC says in the introduction:\n\n    The TSA's role is to time-stamp a datum to establish evidence indicating that a\n    datum existed before a particular time.  This can then be used, for example, to\n    verify that a digital signature was applied to a message before the\n    corresponding certificate was revoked thus allowing a revoked public key\n    certificate to be used for verifying signatures created prior to the time of\n    revocation.\n\nSimple and straight forward.\n\nThe problem here is starts with the fact that you're asking timestamp services\nto do things that they're not claiming they do; a timestamp proof simply proves\nthat some message m existed prior to some time t. Nothing more.\n\nWorse though, linearization is a busted approach.\n\n> Unreliable might mean different things to\n> different people, I'm happy to detail the types of unreliability issue that\n> arise if you do not conform to the model I presented above (of which,\n> linearizability is one way to address it, there are others that still\n> implement epoch based recommitting that could be conceptually sound without\n> requiring linearizability).\n> \n> Do you have any formal proof of what guarantees OTS provides against which\n> threat model? This is likely difficult to produce without a formal model of\n> what OTS is, but perhaps you can give your best shot at producing one and\n> we can carry the conversation on productively from there.\n\nSo as you know, an OpenTimestamps proof consists of a series of commitment\noperations that act on an initial message m, leading to a message known to have\nbeen created at some point in time. Almost always a Bitcoin block header. But\nother schemes like trusted timestamps are possible too.\n\nA commitment operation (namely hashes + concatenation) simply needs the\nproperty that for a given input message m, the output H(m) can't be predicted\nwithout knowledge of m. In the case of concatenation, this property is achieved\ntrivially by the fact that the output includes m verbatim. Similarly, SHA1 is\nstill a valid commitment operation.\n\nBehind the scenes the OTS infrastructure builds merkle trees of commitment\noperations for scalability reasons. But none of those details are relevant to\nthe validity of OTS proofs - the OTS infrastructure could magically mine a\nblock per transaction with the digest in the coinbase, and from the client's\npoint of view, everything would work the same.\n\n\nThe important thing to recognize is that timestamp proof is simply a one-sided\nbound on when a given message existed, proving a message existed _prior_ to\nsome point in time. For example:\n\n    $ ots verify hello-world.txt.ots\n    Assuming target filename is 'hello-world.txt'\n    Success! Bitcoin block 358391 attests existence as of 2015-05-28 EDT\n\nObviously, the message \"Hello World!\" existed prior to 2015 (Indeed, it's such\na short message it's brute-forcable. But for sake of example, we'll ignore\nthat).\n\nThus your claim re: linearization that:\n\n> Having a chain of transactions would serve to linearize history of\n> OTS commitments which would let you prove, given reorgs, that knowledge of\n> commit A was before B a bit more robustly.\n\n...misunderstands the problem. We care about proving statements about messages.\nNot timestamp proofs. Building infrastructure to order timestamp proofs\nthemselves is pointless.\n\n\nWhat you're alluding to is dual-sided bounds on when messages were created.\nThat's solved by random beacons: messages known to have been created *after* a\npoint in time, and unpredictable prior. A famous example of course being the\ngenesis block quote:\n\n    The Times 03/Jan/2009 Chancellor on brink of second bailout for banks\n\nBitcoin block hashes make for a perfectly good random beacon for use-cases with\nday to hour level precision. For higher precision, absolute time, there are\nmany trusted alternatives like the NIST random beacon, Roughtime, etc.\n\n\nOpenTimestamps could offer a trustless _relative_ random beacon service by\nmaking the per-second commitments a merkle mountain range, and publishing the\ntip digests. In fact, that's how I came up with merkle mountain ranges in the\nfirst place, and there's code from 2012 to do exactly that in depths of the git\nrepo. But that's such a niche use-case I decided against that approach for now;\nI'll probably resurrect it in the future for trusted timestamps/clock sync.\n\nAgain, involving the transactions themselves in any of this random beacon stuff\nis pointless.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 488 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220614/569cfb2a/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "Why OpenTimestamps does not \"linearize\" its transactions",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Peter Todd"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 5455
        }
    },
    {
        "title": "[Lightning-dev] [bitcoin-dev] Why OpenTimestamps does not \"linearize\" its transactions",
        "thread_messages": [
            {
                "author": "Undiscussed Horrific Abuse, One Victim of Many",
                "date": "2022-06-14T11:39:39",
                "message_text_only": "hey various,\n\nit's been obvious since its inception that opentimestamps is designed\nto be broken.\n\nif you have energy to normalise a better system, or support one of the\nother better systems that already exists, that's wonderful.\n\ni suspect the opentimestamps ecosystem is very experienced at defending itself."
            }
        ],
        "thread_summary": {
            "title": "Why OpenTimestamps does not \"linearize\" its transactions",
            "categories": [
                "Lightning-dev",
                "bitcoin-dev"
            ],
            "authors": [
                "Undiscussed Horrific Abuse, One Victim of Many"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 310
        }
    },
    {
        "title": "[Lightning-dev] DataSig -- Data signatures over Lightning",
        "thread_messages": [
            {
                "author": "George Tsagkarelis",
                "date": "2022-06-16T15:36:28",
                "message_text_only": "# DataSig -- Data signatures over Lightning\n\n## Introduction\n\nGreetings, Lightning devs\n\nThis mail serves as an introduction to one of the two specs\nthat we want to propose to the community.\nThe scope of these specs is transmitting data over the Lightning\nNetwork (over HTLC custom records). This is a use-case already used\nby a few projects ([1], [2], [3], [4]), and in this context\nwe do not intend to debate the validity of it.\n\nAs mentioned, DataSig is one of the two specs we aim in proposing:\n  * DataSig: Concerns the authentication of some data with regards to\n    the source and destination of the transmission.\n  * DataStruct: Concerns the outer layer of the data structure,\n    mainly focusing on the data fragmentation aspect of transmission.\n\nWe seek feedback on the two specs as we want to improve and tweak\nthem before proceeding with a BLIP proposal.\n\n## DataSig\n\nThis spec's aim is to describe the format of a structure representing\na signature over some arbitrary data.\n\nBefore proceeding, a few clarifications must be made:\n  * The DataSig structure is placed inside a custom TLV record\n  * DataSig allows the receiving end validate that:\n    * Data were authored by the source node\n    * Data were meant to be received by the receiving node.\n\nThe main scope of DataSig is assisting with data verification\nindependently of what medium one chooses for data transmission.\nNevertheless, for simplicity, in the follow-up DataStruct spec\nwe assume the data to be transmitted over custom TLV records as well.\n\nWe consider a compact encoding to be used for representing the\nDataSig structure over a TLV, so it is expressed as the following\nprotobuf message:\n\n```protobuf\nmessage DataSig {\n  uint32 version = 1;\n  bytes sig      = 2;\n  bytes senderPK = 3;\n}\n```\n\n* `version`: The version of DataSig spec used.\n* `sig`: The bytes of the signature.\n* `senderPK`: The sender's public key.\n\n### Generation\n\nIn order to instantiate a DataSig signing the data `D`, one needs\nto follow these steps:\n\n1. Populate `version` with the version that is going to be used.\n2. Prepend the desired destination address (`A`) to `D`,\n   creating a new byte array (`AD`).\n3. Sign the byte array `AD`, generating a signature encoded in\n   fixed-size LN wire format.\n4. Populate the `sig` field with the generated signature.\n5. Populate `senderPK` with own address.\n6. Encode the resulting DataSig structure to wire format\n   (byte array `S`).\n\n### Verification\n\nAssuming that the destination node has retrieved:\n  * The byte array of the data `D`\n  * The byte array of the encoded signature struct `S`\n\nThe data should be verified against the signature\nby following the below procedure:\n\n1. Decode bytes `S` according to DataSig protobuf message definition.\n2. If signature `version` is not supported or unknown, consider data\n   to be unsigned.\n3. Prepend own address (`A`) to byte array `D`, generating the byte\n   array `AD`.\n4. Verify the signature provided in `sig` field against the message\n   `AD` and sender public key `senderPK`.\n\n### Notes / Remarks\n\n* The scope of this spec is to deal with the verification\n  of the author and intended recipient of transmitted data.\n  We do not intend to solve the issue of associating a DataSig\n  to the corresponding data (signed by it), in case they are\n  not transmitted in pairs.\n  For now, we assume that data and signature are transmitted\n  over an HTLC's custom records in pairs.\n\n* You can find a formatted version of this document on\n  [hackmd](https://hackmd.io/2pzHLslkRkGytfjKROv3AQ?view).\n\n--------------\n\n[1]: https://sphinx.chat\n[2]: https://github.com/joostjager/whatsat\n[3]: https://github.com/alexbosworth/balanceofsatoshis\n[4]: https://github.com/c13n-io/c13n-go\n\n\n-- \nGeorge Tsagkarelis | @GeorgeTsag | c13n.io\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220616/b33f8a43/attachment.html>"
            },
            {
                "author": "John Carvalho",
                "date": "2022-06-17T07:19:44",
                "message_text_only": ">\n> The scope of these specs is transmitting data over the Lightning\n> Network (over HTLC custom records). This is a use-case already used\n> by a few projects ([1], [2], [3], [4]), and in this context\n> we do not intend to debate the validity of it.\n\n\nYou can't just handwave away whether something is up for debate because a\nfew people did some proofs-of-concept that pretty much no one actually uses.\n\nThe main question here is \"why?!\" Why shoehorn data transmissions into LN\nwhen you could pair LN payments with any other transmission method?\n\nYou could gate downloads, and permissions in packets totally out of band\nfrom the payments. The files could be torrents or any format that is\nbetter-suited for the task. As an example, look at Dazaar tools:\nhttps://github.com/bitfinexcom?q=dazaar-l&type=all\n\nWe don't need to put the whole internet & web inside of Lightning.\nLightning is for payments. If you try to use it for broad communication use\ncases, you end up crippling both the use case and LN.\n\n--\nJohn Carvalho\nCEO, Synonym.to <http://synonym.to/>\n\n\nOn Thu, Jun 16, 2022 at 4:48 PM <\nlightning-dev-request at lists.linuxfoundation.org> wrote:\n\n>\n> Date: Thu, 16 Jun 2022 18:36:28 +0300\n> From: George Tsagkarelis <george.tsagkarelis at gmail.com>\n> To: lightning-dev at lists.linuxfoundation.org\n> Subject: [Lightning-dev] DataSig -- Data signatures over Lightning\n> Message-ID:\n>         <\n> CACRHu9irXQEfLLDdTwZg93QsaZnyPjP71O9w24b1LxVkzNaDKA at mail.gmail.com>\n> Content-Type: text/plain; charset=\"utf-8\"\n>\n> # DataSig -- Data signatures over Lightning\n>\n> ## Introduction\n>\n> Greetings, Lightning devs\n>\n> This mail serves as an introduction to one of the two specs\n> that we want to propose to the community.\n> The scope of these specs is transmitting data over the Lightning\n> Network (over HTLC custom records). This is a use-case already used\n> by a few projects ([1], [2], [3], [4]), and in this context\n> we do not intend to debate the validity of it.\n>\n> As mentioned, DataSig is one of the two specs we aim in proposing:\n>   * DataSig: Concerns the authentication of some data with regards to\n>     the source and destination of the transmission.\n>   * DataStruct: Concerns the outer layer of the data structure,\n>     mainly focusing on the data fragmentation aspect of transmission.\n>\n> We seek feedback on the two specs as we want to improve and tweak\n> them before proceeding with a BLIP proposal.\n>\n> ## DataSig\n>\n> This spec's aim is to describe the format of a structure representing\n> a signature over some arbitrary data.\n>\n> Before proceeding, a few clarifications must be made:\n>   * The DataSig structure is placed inside a custom TLV record\n>   * DataSig allows the receiving end validate that:\n>     * Data were authored by the source node\n>     * Data were meant to be received by the receiving node.\n>\n> The main scope of DataSig is assisting with data verification\n> independently of what medium one chooses for data transmission.\n> Nevertheless, for simplicity, in the follow-up DataStruct spec\n> we assume the data to be transmitted over custom TLV records as well.\n>\n> We consider a compact encoding to be used for representing the\n> DataSig structure over a TLV, so it is expressed as the following\n> protobuf message:\n>\n> ```protobuf\n> message DataSig {\n>   uint32 version = 1;\n>   bytes sig      = 2;\n>   bytes senderPK = 3;\n> }\n> ```\n>\n> * `version`: The version of DataSig spec used.\n> * `sig`: The bytes of the signature.\n> * `senderPK`: The sender's public key.\n>\n> ### Generation\n>\n> In order to instantiate a DataSig signing the data `D`, one needs\n> to follow these steps:\n>\n> 1. Populate `version` with the version that is going to be used.\n> 2. Prepend the desired destination address (`A`) to `D`,\n>    creating a new byte array (`AD`).\n> 3. Sign the byte array `AD`, generating a signature encoded in\n>    fixed-size LN wire format.\n> 4. Populate the `sig` field with the generated signature.\n> 5. Populate `senderPK` with own address.\n> 6. Encode the resulting DataSig structure to wire format\n>    (byte array `S`).\n>\n> ### Verification\n>\n> Assuming that the destination node has retrieved:\n>   * The byte array of the data `D`\n>   * The byte array of the encoded signature struct `S`\n>\n> The data should be verified against the signature\n> by following the below procedure:\n>\n> 1. Decode bytes `S` according to DataSig protobuf message definition.\n> 2. If signature `version` is not supported or unknown, consider data\n>    to be unsigned.\n> 3. Prepend own address (`A`) to byte array `D`, generating the byte\n>    array `AD`.\n> 4. Verify the signature provided in `sig` field against the message\n>    `AD` and sender public key `senderPK`.\n>\n> ### Notes / Remarks\n>\n> * The scope of this spec is to deal with the verification\n>   of the author and intended recipient of transmitted data.\n>   We do not intend to solve the issue of associating a DataSig\n>   to the corresponding data (signed by it), in case they are\n>   not transmitted in pairs.\n>   For now, we assume that data and signature are transmitted\n>   over an HTLC's custom records in pairs.\n>\n> * You can find a formatted version of this document on\n>   [hackmd](https://hackmd.io/2pzHLslkRkGytfjKROv3AQ?view).\n>\n> --------------\n>\n> [1]: https://sphinx.chat\n> [2]: https://github.com/joostjager/whatsat\n> [3]: https://github.com/alexbosworth/balanceofsatoshis\n> [4]: https://github.com/c13n-io/c13n-go\n>\n>\n> --\n> George Tsagkarelis | @GeorgeTsag | c13n.io\n> -------------- next part --------------\n> An HTML attachment was scrubbed...\n> URL: <\n> http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220616/b33f8a43/attachment-0001.html\n> >\n>\n> ------------------------------\n>\n> Message: 2\n> Date: Thu, 16 Jun 2022 18:48:26 +0300\n> From: George Tsagkarelis <george.tsagkarelis at gmail.com>\n> To: lightning-dev at lists.linuxfoundation.org\n> Subject: [Lightning-dev] DataStruct -- Data fragmentation over\n>         Lightning\n> Message-ID:\n>         <\n> CACRHu9iOns0gzELrUj8yNkj1NE2baUM0RD6DnN1dDyy5TzV6_w at mail.gmail.com>\n> Content-Type: text/plain; charset=\"utf-8\"\n>\n> # DataStruct -- Data fragmentation over Lightning\n>\n> ## Introduction\n>\n> Greetings once again,\n>\n> This mail proposes a spec for data fragmentation over custom records,\n> allowing for transmission of data exceeding the maximum allowed size\n> over a single HTLC.\n>\n> As in the case of DataSig, we seek feedback as we want to improve\n> and tweak this spec before submitting a BLIP version of it.\n>\n> ## DataStruct\n>\n> The purpose of this spec is to define a structure that describes\n> fragmented data, allowing for transmission over separate HTLCs\n> and assisting reassembly on the receiving end.\n> The proposed fragmentation structure also allows out-of-order\n> reception of fragments.\n>\n> Since these fragments are assumed to be transmitted over Lightning\n> HTLCs, we want to use a compact encoding mechanism, thus we describe\n> their structure with protobuf:\n>\n> ```protobuf\n> message DataStruct {\n>   uint32 version                 = 1;\n>   bytes payload                  = 2;\n>   optional FragmentInfo fragment = 3;\n> }\n>\n> message FragmentInfo {\n>   uint64 fragset_id = 1;\n>   uint32 total_size = 2;\n>   uint32 offset     = 3;\n> }\n> ```\n> * `version`: The version of DataStruct spec used.\n> * `payload`: The data carried by this fragment.\n> * `fragment`: Fragmentation information, in case of fragmented data.\n>\n> The `FragmentInfo` fields describe:\n>   * `fragset_id`: Identifier indicating a fragment set, common to all\n>     fragments of the same data.\n>   * `total_size`: The total data size this fragment is part of.\n>   * `offset`: Starting byte offset of this fragment's `payload`\n>     in the total data.\n>\n> If the total data can be transmitted over a single HTLC, then the\n> `fragment` field should be omitted.\n>\n> If the `fragment` field is set on a received DataStruct instance the\n> receiving node should wait for the full fragment set to be received\n> before reconstruction. For each received fragment of a fragment set\n> (as indicated by `fragset_id`), the receiving node should assemble\n> the data by inserting each `payload` at the offset indicated by the\n> `fragment`'s `offset` field. Once the whole data range has been\n> received, a node can safely assume the data has been received in\n> full.\n>\n> ### Sending\n>\n> In this section we will walk through the procedure of utilizing\n> DataStruct in order to transmit some data `D` that have a size of\n> 42KB.\n>\n> It is also important to note that we don't describe an algorithm that\n> efficiently and dynamically splits the byte array `D` into an\n> optimal set of fragments. A fragment's transmission may fail for\n> various reasons (including uncertain channel liquidity, stale routing\n> data or route lengths that prohibit meaningful data injection).\n> It is the responsibility of the sender to fragment the data and\n> transmit the fragments towards the destination. The receiver simply\n> receives fragments that will (ideally) completely cover `D`, allowing\n> its reconstruction.\n>\n> In this example, we will assume that the sender will settle for\n> splitting the data `D` into 84 fragments of 512B size each.\n> This is not optimal as it will probably result in raised transmission\n> costs, depending on route length.\n>\n> A sender intending to transmit the data `D` to another node should:\n>\n> 1. Split the bytes of `D`  into 84 fragments of 512B each.\n> 2. Generate an identifier for this data transmission, `Di`.\n> 3. For each fragment `f`, a `DataStruct` instance should be created:\n>     1. Populate `version` with the spec version followed,\n>     2. Populate payload with `f`,\n>     3. Populate `fragment` as follows:\n>         1. Populate `fragset_id` with `Di`,\n>         2. Populate `total_size` with len(`D`),\n>         3. Populate `offset` with the fragment's starting byte index.\n>     4. Encode the created DataStruct instance, resulting in a byte\n>        array `DS`.\n>     5. Transmit `DS` over the custom records of an HTLC.\n>     6. In case of failure, transmission can be retried over a\n>        different route.\n>\n> ### Receiving\n>\n> Continuing the last example, the receiving node can execute the\n> following steps for each received fragment `DS` in order to assemble\n> the data `D`:\n>\n> 1. Decode `DS` according to DataStruct definition.\n> 2. Check `version` field, and decide whether to proceed or ignore\n>    the fragment.\n> 3. If the received DataStruct instance contains a `fragment` field:\n>     1. Retrieve the reconstruction buffer identified by `fragset_id`,\n>        creating it with size `total_size` if it does not exist.\n>     2. Insert `payload` at `offset` to reconstruction buffer.\n>     3. Check if reconstruction buffer is complete. If all of the\n>        body of the reconstruction buffer is filled, the buffer\n>        contains the total data `D`.\n>\n> ### Notes / Remarks\n>\n> * We mention that the encoded DataStruct is placed inside a custom\n>   TLV record, but do not specify the exact TLV key. This is a spec\n>   regarding data fragment transmission, and as such should not define\n>   specific TLV keys to be used.\n>\n> * Interoperability could be achieved by different applications\n>   utilizing the same TLV as well as data encoding for transmission.\n>\n> * A node can send and receive payments that carry data in different\n>   TLV keys. It is the responsibility of the application to send and\n>   listen for data over specific TLV keys.\n>\n> * It is the responsibility of the sender to transmit fragments that\n>   allow for full data reconstruction on the receiving end.\n>\n> * Fragments could carry ranges of bytes that overlap (e.g. two\n>   fragments that cover the range 256-511 (0-511, 256-767)).\n>\n> * A DataSig could accompany a transmitted DataStruct, allowing the\n>   receiving node to verify the data source and destination.\n>\n> * If DataSig is also included with each fragment, the receiver could\n>   identify reconstruction buffers based not only on `fragset_id` but\n>   the sender's address as well. This means that a node could\n>   simultaneously be receiving two different fragment sets with the\n>   same `fragset_id`, as long as they are originating from different\n>   nodes.\n>\n> * It is the responsibility of the sender to properly coordinate\n>   simultaneous transmissions to a destination node by using different\n>   `fragset_id` values for each fragment set.\n>\n> * If the sender uses an AMP payment's HTLCs to carry the different\n>   fragments, it is not strictly necessary to declare the `total_size`\n>   of the data. The condition for data reconstruction completion could\n>   be the success of the AMP payment, unless they want to utilize both\n>   AMP and single path payments for data transmission (transmit over\n>   multiple payments possibly with multiple HTLCs on each payment).\n>\n> * There is a lot of room for optimisations, like signing larger\n>   chunks of data and not each transmitted fragment. This way you would\n>   transmit less DataSig instances and leave more available space for\n>   the fragment data.\n>\n> - A working proof of concept that utilizes DataSig and DataStruct\n>   over single path payments can be found here:\n>   https://github.com/GeorgeTsagk/satoshi-read-write\n>\n>\n> --\n> George Tsagkarelis | @GeorgeTsag | c13n.io\n> -------------- next part --------------\n> An HTML attachment was scrubbed...\n> URL: <\n> http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220616/ebb61e73/attachment.html\n> >\n>\n> ------------------------------\n>\n> Subject: Digest Footer\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n>\n> ------------------------------\n>\n> End of Lightning-dev Digest, Vol 82, Issue 9\n> ********************************************\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220617/54423266/attachment-0001.html>"
            },
            {
                "author": "George Tsagkarelis",
                "date": "2022-06-17T10:18:34",
                "message_text_only": "Hi John,\n\nMain reason is the fact that I can just message a node without requiring\nother pieces of technology attached to my Lightning node. Our goal is not\nto pair the payment to some data, but get some data from node A to node B.\nThe fact that payments and data in this approach are always paired is a\nsweet bonus. Keep in mind that every time I message a node I'm actually\nsending a min-value payment towards the destination, rewarding the routing\nnodes with the base fee of the channels I used. The routing nodes don't\nknow if I'm messaging or not, and will keep forwarding my \"min-value\npayments\" because I'm compensating them for that. Even if they assume that\na payment is carrying a message, they still won't be able to figure out\nwhat the message is or who is sending/receiving it.\n\nIn DataStruct spec we describe a structure that can support data\nfragmentation, but we do not in any case encourage content streaming over\nLN. We are not planning to \"put the whole Internet & web\" inside of\nLightning. If you want to stream media or download files you SHOULD choose\nan out-of-band solution. We focus on small footprint data that will\neventually utilize a handful of HTLCs to reach their destination, which is\nnot harmful for LN.\n\nWe are also not against pairing LN with the HTTP world, technologies like\nLSAT/LNURL have valid use cases. There is a big number of\napplications/services that would benefit by directly connecting to your\nnode and have the ability to also communicate with other nodes, not only\npay them. I guess the answer to \"why\" is just the fact that its more\nsimple, secure and private compared to out-of-band hybrid solutions, but\ncomes with a cost in sats. At the end of the day it is the developers and\nusers the ones that will decide what solution best fits their needs.\n\n\n\n\nOn Fri, Jun 17, 2022 at 10:20 AM John Carvalho <john at synonym.to> wrote:\n\n> The scope of these specs is transmitting data over the Lightning\n>> Network (over HTLC custom records). This is a use-case already used\n>> by a few projects ([1], [2], [3], [4]), and in this context\n>> we do not intend to debate the validity of it.\n>\n>\n> You can't just handwave away whether something is up for debate because a\n> few people did some proofs-of-concept that pretty much no one actually uses.\n>\n> The main question here is \"why?!\" Why shoehorn data transmissions into LN\n> when you could pair LN payments with any other transmission method?\n>\n> You could gate downloads, and permissions in packets totally out of band\n> from the payments. The files could be torrents or any format that is\n> better-suited for the task. As an example, look at Dazaar tools:\n> https://github.com/bitfinexcom?q=dazaar-l&type=all\n>\n> We don't need to put the whole internet & web inside of Lightning.\n> Lightning is for payments. If you try to use it for broad communication use\n> cases, you end up crippling both the use case and LN.\n>\n> --\n> John Carvalho\n> CEO, Synonym.to <http://synonym.to/>\n>\n>\n> On Thu, Jun 16, 2022 at 4:48 PM <\n> lightning-dev-request at lists.linuxfoundation.org> wrote:\n>\n>>\n>> Date: Thu, 16 Jun 2022 18:36:28 +0300\n>> From: George Tsagkarelis <george.tsagkarelis at gmail.com>\n>> To: lightning-dev at lists.linuxfoundation.org\n>> Subject: [Lightning-dev] DataSig -- Data signatures over Lightning\n>> Message-ID:\n>>         <\n>> CACRHu9irXQEfLLDdTwZg93QsaZnyPjP71O9w24b1LxVkzNaDKA at mail.gmail.com>\n>> Content-Type: text/plain; charset=\"utf-8\"\n>>\n>> # DataSig -- Data signatures over Lightning\n>>\n>> ## Introduction\n>>\n>> Greetings, Lightning devs\n>>\n>> This mail serves as an introduction to one of the two specs\n>> that we want to propose to the community.\n>> The scope of these specs is transmitting data over the Lightning\n>> Network (over HTLC custom records). This is a use-case already used\n>> by a few projects ([1], [2], [3], [4]), and in this context\n>> we do not intend to debate the validity of it.\n>>\n>> As mentioned, DataSig is one of the two specs we aim in proposing:\n>>   * DataSig: Concerns the authentication of some data with regards to\n>>     the source and destination of the transmission.\n>>   * DataStruct: Concerns the outer layer of the data structure,\n>>     mainly focusing on the data fragmentation aspect of transmission.\n>>\n>> We seek feedback on the two specs as we want to improve and tweak\n>> them before proceeding with a BLIP proposal.\n>>\n>> ## DataSig\n>>\n>> This spec's aim is to describe the format of a structure representing\n>> a signature over some arbitrary data.\n>>\n>> Before proceeding, a few clarifications must be made:\n>>   * The DataSig structure is placed inside a custom TLV record\n>>   * DataSig allows the receiving end validate that:\n>>     * Data were authored by the source node\n>>     * Data were meant to be received by the receiving node.\n>>\n>> The main scope of DataSig is assisting with data verification\n>> independently of what medium one chooses for data transmission.\n>> Nevertheless, for simplicity, in the follow-up DataStruct spec\n>> we assume the data to be transmitted over custom TLV records as well.\n>>\n>> We consider a compact encoding to be used for representing the\n>> DataSig structure over a TLV, so it is expressed as the following\n>> protobuf message:\n>>\n>> ```protobuf\n>> message DataSig {\n>>   uint32 version = 1;\n>>   bytes sig      = 2;\n>>   bytes senderPK = 3;\n>> }\n>> ```\n>>\n>> * `version`: The version of DataSig spec used.\n>> * `sig`: The bytes of the signature.\n>> * `senderPK`: The sender's public key.\n>>\n>> ### Generation\n>>\n>> In order to instantiate a DataSig signing the data `D`, one needs\n>> to follow these steps:\n>>\n>> 1. Populate `version` with the version that is going to be used.\n>> 2. Prepend the desired destination address (`A`) to `D`,\n>>    creating a new byte array (`AD`).\n>> 3. Sign the byte array `AD`, generating a signature encoded in\n>>    fixed-size LN wire format.\n>> 4. Populate the `sig` field with the generated signature.\n>> 5. Populate `senderPK` with own address.\n>> 6. Encode the resulting DataSig structure to wire format\n>>    (byte array `S`).\n>>\n>> ### Verification\n>>\n>> Assuming that the destination node has retrieved:\n>>   * The byte array of the data `D`\n>>   * The byte array of the encoded signature struct `S`\n>>\n>> The data should be verified against the signature\n>> by following the below procedure:\n>>\n>> 1. Decode bytes `S` according to DataSig protobuf message definition.\n>> 2. If signature `version` is not supported or unknown, consider data\n>>    to be unsigned.\n>> 3. Prepend own address (`A`) to byte array `D`, generating the byte\n>>    array `AD`.\n>> 4. Verify the signature provided in `sig` field against the message\n>>    `AD` and sender public key `senderPK`.\n>>\n>> ### Notes / Remarks\n>>\n>> * The scope of this spec is to deal with the verification\n>>   of the author and intended recipient of transmitted data.\n>>   We do not intend to solve the issue of associating a DataSig\n>>   to the corresponding data (signed by it), in case they are\n>>   not transmitted in pairs.\n>>   For now, we assume that data and signature are transmitted\n>>   over an HTLC's custom records in pairs.\n>>\n>> * You can find a formatted version of this document on\n>>   [hackmd](https://hackmd.io/2pzHLslkRkGytfjKROv3AQ?view).\n>>\n>> --------------\n>>\n>> [1]: https://sphinx.chat\n>> [2]: https://github.com/joostjager/whatsat\n>> [3]: https://github.com/alexbosworth/balanceofsatoshis\n>> [4]: https://github.com/c13n-io/c13n-go\n>>\n>>\n>> --\n>> George Tsagkarelis | @GeorgeTsag | c13n.io\n>> -------------- next part --------------\n>> An HTML attachment was scrubbed...\n>> URL: <\n>> http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220616/b33f8a43/attachment-0001.html\n>> >\n>>\n>> ------------------------------\n>>\n>> Message: 2\n>> Date: Thu, 16 Jun 2022 18:48:26 +0300\n>> From: George Tsagkarelis <george.tsagkarelis at gmail.com>\n>> To: lightning-dev at lists.linuxfoundation.org\n>> Subject: [Lightning-dev] DataStruct -- Data fragmentation over\n>>         Lightning\n>> Message-ID:\n>>         <\n>> CACRHu9iOns0gzELrUj8yNkj1NE2baUM0RD6DnN1dDyy5TzV6_w at mail.gmail.com>\n>> Content-Type: text/plain; charset=\"utf-8\"\n>>\n>> # DataStruct -- Data fragmentation over Lightning\n>>\n>> ## Introduction\n>>\n>> Greetings once again,\n>>\n>> This mail proposes a spec for data fragmentation over custom records,\n>> allowing for transmission of data exceeding the maximum allowed size\n>> over a single HTLC.\n>>\n>> As in the case of DataSig, we seek feedback as we want to improve\n>> and tweak this spec before submitting a BLIP version of it.\n>>\n>> ## DataStruct\n>>\n>> The purpose of this spec is to define a structure that describes\n>> fragmented data, allowing for transmission over separate HTLCs\n>> and assisting reassembly on the receiving end.\n>> The proposed fragmentation structure also allows out-of-order\n>> reception of fragments.\n>>\n>> Since these fragments are assumed to be transmitted over Lightning\n>> HTLCs, we want to use a compact encoding mechanism, thus we describe\n>> their structure with protobuf:\n>>\n>> ```protobuf\n>> message DataStruct {\n>>   uint32 version                 = 1;\n>>   bytes payload                  = 2;\n>>   optional FragmentInfo fragment = 3;\n>> }\n>>\n>> message FragmentInfo {\n>>   uint64 fragset_id = 1;\n>>   uint32 total_size = 2;\n>>   uint32 offset     = 3;\n>> }\n>> ```\n>> * `version`: The version of DataStruct spec used.\n>> * `payload`: The data carried by this fragment.\n>> * `fragment`: Fragmentation information, in case of fragmented data.\n>>\n>> The `FragmentInfo` fields describe:\n>>   * `fragset_id`: Identifier indicating a fragment set, common to all\n>>     fragments of the same data.\n>>   * `total_size`: The total data size this fragment is part of.\n>>   * `offset`: Starting byte offset of this fragment's `payload`\n>>     in the total data.\n>>\n>> If the total data can be transmitted over a single HTLC, then the\n>> `fragment` field should be omitted.\n>>\n>> If the `fragment` field is set on a received DataStruct instance the\n>> receiving node should wait for the full fragment set to be received\n>> before reconstruction. For each received fragment of a fragment set\n>> (as indicated by `fragset_id`), the receiving node should assemble\n>> the data by inserting each `payload` at the offset indicated by the\n>> `fragment`'s `offset` field. Once the whole data range has been\n>> received, a node can safely assume the data has been received in\n>> full.\n>>\n>> ### Sending\n>>\n>> In this section we will walk through the procedure of utilizing\n>> DataStruct in order to transmit some data `D` that have a size of\n>> 42KB.\n>>\n>> It is also important to note that we don't describe an algorithm that\n>> efficiently and dynamically splits the byte array `D` into an\n>> optimal set of fragments. A fragment's transmission may fail for\n>> various reasons (including uncertain channel liquidity, stale routing\n>> data or route lengths that prohibit meaningful data injection).\n>> It is the responsibility of the sender to fragment the data and\n>> transmit the fragments towards the destination. The receiver simply\n>> receives fragments that will (ideally) completely cover `D`, allowing\n>> its reconstruction.\n>>\n>> In this example, we will assume that the sender will settle for\n>> splitting the data `D` into 84 fragments of 512B size each.\n>> This is not optimal as it will probably result in raised transmission\n>> costs, depending on route length.\n>>\n>> A sender intending to transmit the data `D` to another node should:\n>>\n>> 1. Split the bytes of `D`  into 84 fragments of 512B each.\n>> 2. Generate an identifier for this data transmission, `Di`.\n>> 3. For each fragment `f`, a `DataStruct` instance should be created:\n>>     1. Populate `version` with the spec version followed,\n>>     2. Populate payload with `f`,\n>>     3. Populate `fragment` as follows:\n>>         1. Populate `fragset_id` with `Di`,\n>>         2. Populate `total_size` with len(`D`),\n>>         3. Populate `offset` with the fragment's starting byte index.\n>>     4. Encode the created DataStruct instance, resulting in a byte\n>>        array `DS`.\n>>     5. Transmit `DS` over the custom records of an HTLC.\n>>     6. In case of failure, transmission can be retried over a\n>>        different route.\n>>\n>> ### Receiving\n>>\n>> Continuing the last example, the receiving node can execute the\n>> following steps for each received fragment `DS` in order to assemble\n>> the data `D`:\n>>\n>> 1. Decode `DS` according to DataStruct definition.\n>> 2. Check `version` field, and decide whether to proceed or ignore\n>>    the fragment.\n>> 3. If the received DataStruct instance contains a `fragment` field:\n>>     1. Retrieve the reconstruction buffer identified by `fragset_id`,\n>>        creating it with size `total_size` if it does not exist.\n>>     2. Insert `payload` at `offset` to reconstruction buffer.\n>>     3. Check if reconstruction buffer is complete. If all of the\n>>        body of the reconstruction buffer is filled, the buffer\n>>        contains the total data `D`.\n>>\n>> ### Notes / Remarks\n>>\n>> * We mention that the encoded DataStruct is placed inside a custom\n>>   TLV record, but do not specify the exact TLV key. This is a spec\n>>   regarding data fragment transmission, and as such should not define\n>>   specific TLV keys to be used.\n>>\n>> * Interoperability could be achieved by different applications\n>>   utilizing the same TLV as well as data encoding for transmission.\n>>\n>> * A node can send and receive payments that carry data in different\n>>   TLV keys. It is the responsibility of the application to send and\n>>   listen for data over specific TLV keys.\n>>\n>> * It is the responsibility of the sender to transmit fragments that\n>>   allow for full data reconstruction on the receiving end.\n>>\n>> * Fragments could carry ranges of bytes that overlap (e.g. two\n>>   fragments that cover the range 256-511 (0-511, 256-767)).\n>>\n>> * A DataSig could accompany a transmitted DataStruct, allowing the\n>>   receiving node to verify the data source and destination.\n>>\n>> * If DataSig is also included with each fragment, the receiver could\n>>   identify reconstruction buffers based not only on `fragset_id` but\n>>   the sender's address as well. This means that a node could\n>>   simultaneously be receiving two different fragment sets with the\n>>   same `fragset_id`, as long as they are originating from different\n>>   nodes.\n>>\n>> * It is the responsibility of the sender to properly coordinate\n>>   simultaneous transmissions to a destination node by using different\n>>   `fragset_id` values for each fragment set.\n>>\n>> * If the sender uses an AMP payment's HTLCs to carry the different\n>>   fragments, it is not strictly necessary to declare the `total_size`\n>>   of the data. The condition for data reconstruction completion could\n>>   be the success of the AMP payment, unless they want to utilize both\n>>   AMP and single path payments for data transmission (transmit over\n>>   multiple payments possibly with multiple HTLCs on each payment).\n>>\n>> * There is a lot of room for optimisations, like signing larger\n>>   chunks of data and not each transmitted fragment. This way you would\n>>   transmit less DataSig instances and leave more available space for\n>>   the fragment data.\n>>\n>> - A working proof of concept that utilizes DataSig and DataStruct\n>>   over single path payments can be found here:\n>>   https://github.com/GeorgeTsagk/satoshi-read-write\n>>\n>>\n>> --\n>> George Tsagkarelis | @GeorgeTsag | c13n.io\n>> -------------- next part --------------\n>> An HTML attachment was scrubbed...\n>> URL: <\n>> http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220616/ebb61e73/attachment.html\n>> >\n>>\n>> ------------------------------\n>>\n>> Subject: Digest Footer\n>>\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n>>\n>> ------------------------------\n>>\n>> End of Lightning-dev Digest, Vol 82, Issue 9\n>> ********************************************\n>>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n\n\n-- \nGeorge Tsagkarelis | @GeorgeTsag | c13n.io\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220617/970ac4f3/attachment-0001.html>"
            },
            {
                "author": "Azz",
                "date": "2022-06-17T08:55:25",
                "message_text_only": "An HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220617/389e4de4/attachment.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: publicKey - asteria at valera.sh - 69152307.asc\nType: application/pgp-keys\nSize: 713 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220617/389e4de4/attachment.bin>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 249 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220617/389e4de4/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "DataSig -- Data signatures over Lightning",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "John Carvalho",
                "Azz",
                "George Tsagkarelis"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 35315
        }
    },
    {
        "title": "[Lightning-dev] DataStruct -- Data fragmentation over Lightning",
        "thread_messages": [
            {
                "author": "George Tsagkarelis",
                "date": "2022-06-16T15:48:26",
                "message_text_only": "# DataStruct -- Data fragmentation over Lightning\n\n## Introduction\n\nGreetings once again,\n\nThis mail proposes a spec for data fragmentation over custom records,\nallowing for transmission of data exceeding the maximum allowed size\nover a single HTLC.\n\nAs in the case of DataSig, we seek feedback as we want to improve\nand tweak this spec before submitting a BLIP version of it.\n\n## DataStruct\n\nThe purpose of this spec is to define a structure that describes\nfragmented data, allowing for transmission over separate HTLCs\nand assisting reassembly on the receiving end.\nThe proposed fragmentation structure also allows out-of-order\nreception of fragments.\n\nSince these fragments are assumed to be transmitted over Lightning\nHTLCs, we want to use a compact encoding mechanism, thus we describe\ntheir structure with protobuf:\n\n```protobuf\nmessage DataStruct {\n  uint32 version                 = 1;\n  bytes payload                  = 2;\n  optional FragmentInfo fragment = 3;\n}\n\nmessage FragmentInfo {\n  uint64 fragset_id = 1;\n  uint32 total_size = 2;\n  uint32 offset     = 3;\n}\n```\n* `version`: The version of DataStruct spec used.\n* `payload`: The data carried by this fragment.\n* `fragment`: Fragmentation information, in case of fragmented data.\n\nThe `FragmentInfo` fields describe:\n  * `fragset_id`: Identifier indicating a fragment set, common to all\n    fragments of the same data.\n  * `total_size`: The total data size this fragment is part of.\n  * `offset`: Starting byte offset of this fragment's `payload`\n    in the total data.\n\nIf the total data can be transmitted over a single HTLC, then the\n`fragment` field should be omitted.\n\nIf the `fragment` field is set on a received DataStruct instance the\nreceiving node should wait for the full fragment set to be received\nbefore reconstruction. For each received fragment of a fragment set\n(as indicated by `fragset_id`), the receiving node should assemble\nthe data by inserting each `payload` at the offset indicated by the\n`fragment`'s `offset` field. Once the whole data range has been\nreceived, a node can safely assume the data has been received in\nfull.\n\n### Sending\n\nIn this section we will walk through the procedure of utilizing\nDataStruct in order to transmit some data `D` that have a size of\n42KB.\n\nIt is also important to note that we don't describe an algorithm that\nefficiently and dynamically splits the byte array `D` into an\noptimal set of fragments. A fragment's transmission may fail for\nvarious reasons (including uncertain channel liquidity, stale routing\ndata or route lengths that prohibit meaningful data injection).\nIt is the responsibility of the sender to fragment the data and\ntransmit the fragments towards the destination. The receiver simply\nreceives fragments that will (ideally) completely cover `D`, allowing\nits reconstruction.\n\nIn this example, we will assume that the sender will settle for\nsplitting the data `D` into 84 fragments of 512B size each.\nThis is not optimal as it will probably result in raised transmission\ncosts, depending on route length.\n\nA sender intending to transmit the data `D` to another node should:\n\n1. Split the bytes of `D`  into 84 fragments of 512B each.\n2. Generate an identifier for this data transmission, `Di`.\n3. For each fragment `f`, a `DataStruct` instance should be created:\n    1. Populate `version` with the spec version followed,\n    2. Populate payload with `f`,\n    3. Populate `fragment` as follows:\n        1. Populate `fragset_id` with `Di`,\n        2. Populate `total_size` with len(`D`),\n        3. Populate `offset` with the fragment's starting byte index.\n    4. Encode the created DataStruct instance, resulting in a byte\n       array `DS`.\n    5. Transmit `DS` over the custom records of an HTLC.\n    6. In case of failure, transmission can be retried over a\n       different route.\n\n### Receiving\n\nContinuing the last example, the receiving node can execute the\nfollowing steps for each received fragment `DS` in order to assemble\nthe data `D`:\n\n1. Decode `DS` according to DataStruct definition.\n2. Check `version` field, and decide whether to proceed or ignore\n   the fragment.\n3. If the received DataStruct instance contains a `fragment` field:\n    1. Retrieve the reconstruction buffer identified by `fragset_id`,\n       creating it with size `total_size` if it does not exist.\n    2. Insert `payload` at `offset` to reconstruction buffer.\n    3. Check if reconstruction buffer is complete. If all of the\n       body of the reconstruction buffer is filled, the buffer\n       contains the total data `D`.\n\n### Notes / Remarks\n\n* We mention that the encoded DataStruct is placed inside a custom\n  TLV record, but do not specify the exact TLV key. This is a spec\n  regarding data fragment transmission, and as such should not define\n  specific TLV keys to be used.\n\n* Interoperability could be achieved by different applications\n  utilizing the same TLV as well as data encoding for transmission.\n\n* A node can send and receive payments that carry data in different\n  TLV keys. It is the responsibility of the application to send and\n  listen for data over specific TLV keys.\n\n* It is the responsibility of the sender to transmit fragments that\n  allow for full data reconstruction on the receiving end.\n\n* Fragments could carry ranges of bytes that overlap (e.g. two\n  fragments that cover the range 256-511 (0-511, 256-767)).\n\n* A DataSig could accompany a transmitted DataStruct, allowing the\n  receiving node to verify the data source and destination.\n\n* If DataSig is also included with each fragment, the receiver could\n  identify reconstruction buffers based not only on `fragset_id` but\n  the sender's address as well. This means that a node could\n  simultaneously be receiving two different fragment sets with the\n  same `fragset_id`, as long as they are originating from different\n  nodes.\n\n* It is the responsibility of the sender to properly coordinate\n  simultaneous transmissions to a destination node by using different\n  `fragset_id` values for each fragment set.\n\n* If the sender uses an AMP payment's HTLCs to carry the different\n  fragments, it is not strictly necessary to declare the `total_size`\n  of the data. The condition for data reconstruction completion could\n  be the success of the AMP payment, unless they want to utilize both\n  AMP and single path payments for data transmission (transmit over\n  multiple payments possibly with multiple HTLCs on each payment).\n\n* There is a lot of room for optimisations, like signing larger\n  chunks of data and not each transmitted fragment. This way you would\n  transmit less DataSig instances and leave more available space for\n  the fragment data.\n\n- A working proof of concept that utilizes DataSig and DataStruct\n  over single path payments can be found here:\n  https://github.com/GeorgeTsagk/satoshi-read-write\n\n\n-- \nGeorge Tsagkarelis | @GeorgeTsag | c13n.io\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220616/ebb61e73/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "DataStruct -- Data fragmentation over Lightning",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "George Tsagkarelis"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 7068
        }
    },
    {
        "title": "[Lightning-dev] Achieving Zero Downtime Splicing in Practice via Chain Signals",
        "thread_messages": [
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2022-06-28T00:27:07",
                "message_text_only": "Hi y'all,\n\nThis mail was inspired by this [1] spec PR from Lisa. At a high level, it\nproposes the nodes add a delay between the time they see a channel closed on\nchain, to when they remove it from their local channel graph. The motive\nhere is to give the gossip message that indicates a splice is in process,\n\"enough\" time to propagate through the network. If a node can see this\nmessage before/during the splicing operation, then they'll be able relate\nthe old and the new channels, meaning it's usable again by senders/receiver\n_before_ the entire chain of transactions confirms on chain.\n\nIMO, this sort of arbitrary delay (expressed in blocks) won't actually\naddress the issue in practice. The proposal suffers from the following\nissues:\n\n  1. 12 blocks is chosen arbitrarily. If for w/e reason an announcement\n  takes longer than 2 hours to reach the \"economic majority\" of\n  senders/receivers, then the channel won't be able to mask the splicing\n  downtime.\n\n  2. Gossip propagation delay and offline peers. These days most nodes\n  throttle gossip pretty aggressively. As a result, a pair of nodes doing\n  several in-flight splices (inputs become double spent or something, so\n  they need to try a bunch) might end up being rate limited within the\n  network, causing the splice update msg to be lost or delayed significantly\n  (IIRC CLN resets these values after 24 hours). On top of that, if a peer\n  is offline for too long (think mobile senders), then they may miss the\n  update all together as most nodes don't do a full historical\n  _channel_update_ dump anymore.\n\nIn order to resolve these issues, I think instead we need to rely on the\nprimary splicing signal being sourced from the chain itself. In other words,\nif I see a channel close, and a closing transaction \"looks\" a certain way,\nthen I know it's a splice. This would be used in concert w/ any new gossip\nmessages, as the chain signal is a 100% foolproof way of letting an aware\npeer know that a splice is actually happening (not a normal close). A chain\nsignal doesn't suffer from any of the gossip/time related issues above, as\nthe signal is revealed at the same time a peer learns of a channel\nclose/splice.\n\nAssuming, we agree that a chain signal has some sort of role in the ultimate\nplans for splicing, we'd need to decide on exactly _what_ such a signal\nlooks like. Off the top, a few options are:\n\n  1. Stuff something in the annex. Works in theory, but not in practice, as\n  bitcoind (being the dominant full node implementation on the p2p network,\n  as well as what all the miners use) treats annexes as non-standard. Also\n  the annex itself might have some fundamental issues that get in the way of\n  its use all together [2].\n\n  2. Re-use the anchors for this purpose. Anchor are nice as they allow for\n  1st/2nd/3rd party CPFP. As a splice might have several inputs and outputs,\n  both sides will want to make sure it gets confirmed in a timely manner.\n  Ofc, RBF can be used here, but that requires both sides to be online to\n  make adjustments. Pre-signing can work too, but the effectiveness\n  (minimizing chain cost while expediting confirmation) would be dependent\n  on the fee step size.\n\n  In this case, we'd use a different multi-sig output (both sides can rotate\n  keys if they want to), and then roll the anchors into this splicing\n  transaction. Given that all nodes on the network know what the anchor size\n  is (assuming feature bit understanding), they're able to realize that it's\n  actually a splice, and they don't need to remove it from the channel graph\n  (yet).\n\n  3. Related to the above: just re-use the same multi-sig output. If nodes\n  don't care all that much about rotating these keys, then they can just use\n  the same output. This is trivially recognizable by nodes, as they already\n  know the funding keys used, as they're in the channel_announcement.\n\n  4. OP_RETURN (yeh, I had to list it). Self explanatory, push some bytes in\n  an OP_RETURN and use that as the marker.\n\n  5. Fiddle w/ the locktime+sequence somehow to make it identifiable to\n  verifiers. This might run into some unintended interactions if the inputs\n  provided have either relative or absolute lock times. There might also be\n  some interaction w/ the main constructing for eltoo (uses the locktime).\n\nOf all the options, I think #2 makes the most sense: we already use anchors\nto be able to do fee bumping after-the-fact for closing transactions, so why\nnot inherit them here. They make the splicing transaction slightly larger,\nso maybe #3 (or something else) is a better choice.\n\nThe design space for spicing is preeetty large, so I figure the most\nproductive route might be discussing isolated aspects of it at a time.\nPersonally, I'm not suuuper caught up w/ what the latest design drafts are\n(aside from convos at the recent LN Dev Summit), but from my PoV, how to\ncommunicate the splice to other peers has been an outstanding design\nquestion.\n\n[1]: https://github.com/lightning/bolts/pull/1004\n[2]:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-March/020045.html\n\n-- Laolu\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220627/1544d48a/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2022-06-29T01:40:21",
                "message_text_only": "Hi Roasbeef,\n\nThis is over-design: if you fail to get reliable gossip, your routing\nwill suffer anyway.  Nothing new here.\n\nAnd if you *know* you're missing gossip, you can simply delay onchain\nclosures for longer: since nodes should respect the old channel ids for\na while anyway.\n\nMatt's proposal to simply defer treating onchain closes is elegant and\nminimal.  We could go further and relax requirements to detect onchain\ncloses at all, and optionally add a perm close message.\n\nCheers,\nRusty.\n\nOlaoluwa Osuntokun <laolu32 at gmail.com> writes:\n> Hi y'all,\n>\n> This mail was inspired by this [1] spec PR from Lisa. At a high level, it\n> proposes the nodes add a delay between the time they see a channel closed on\n> chain, to when they remove it from their local channel graph. The motive\n> here is to give the gossip message that indicates a splice is in process,\n> \"enough\" time to propagate through the network. If a node can see this\n> message before/during the splicing operation, then they'll be able relate\n> the old and the new channels, meaning it's usable again by senders/receiver\n> _before_ the entire chain of transactions confirms on chain.\n>\n> IMO, this sort of arbitrary delay (expressed in blocks) won't actually\n> address the issue in practice. The proposal suffers from the following\n> issues:\n>\n>   1. 12 blocks is chosen arbitrarily. If for w/e reason an announcement\n>   takes longer than 2 hours to reach the \"economic majority\" of\n>   senders/receivers, then the channel won't be able to mask the splicing\n>   downtime.\n>\n>   2. Gossip propagation delay and offline peers. These days most nodes\n>   throttle gossip pretty aggressively. As a result, a pair of nodes doing\n>   several in-flight splices (inputs become double spent or something, so\n>   they need to try a bunch) might end up being rate limited within the\n>   network, causing the splice update msg to be lost or delayed significantly\n>   (IIRC CLN resets these values after 24 hours). On top of that, if a peer\n>   is offline for too long (think mobile senders), then they may miss the\n>   update all together as most nodes don't do a full historical\n>   _channel_update_ dump anymore.\n>\n> In order to resolve these issues, I think instead we need to rely on the\n> primary splicing signal being sourced from the chain itself. In other words,\n> if I see a channel close, and a closing transaction \"looks\" a certain way,\n> then I know it's a splice. This would be used in concert w/ any new gossip\n> messages, as the chain signal is a 100% foolproof way of letting an aware\n> peer know that a splice is actually happening (not a normal close). A chain\n> signal doesn't suffer from any of the gossip/time related issues above, as\n> the signal is revealed at the same time a peer learns of a channel\n> close/splice.\n>\n> Assuming, we agree that a chain signal has some sort of role in the ultimate\n> plans for splicing, we'd need to decide on exactly _what_ such a signal\n> looks like. Off the top, a few options are:\n>\n>   1. Stuff something in the annex. Works in theory, but not in practice, as\n>   bitcoind (being the dominant full node implementation on the p2p network,\n>   as well as what all the miners use) treats annexes as non-standard. Also\n>   the annex itself might have some fundamental issues that get in the way of\n>   its use all together [2].\n>\n>   2. Re-use the anchors for this purpose. Anchor are nice as they allow for\n>   1st/2nd/3rd party CPFP. As a splice might have several inputs and outputs,\n>   both sides will want to make sure it gets confirmed in a timely manner.\n>   Ofc, RBF can be used here, but that requires both sides to be online to\n>   make adjustments. Pre-signing can work too, but the effectiveness\n>   (minimizing chain cost while expediting confirmation) would be dependent\n>   on the fee step size.\n>\n>   In this case, we'd use a different multi-sig output (both sides can rotate\n>   keys if they want to), and then roll the anchors into this splicing\n>   transaction. Given that all nodes on the network know what the anchor size\n>   is (assuming feature bit understanding), they're able to realize that it's\n>   actually a splice, and they don't need to remove it from the channel graph\n>   (yet).\n>\n>   3. Related to the above: just re-use the same multi-sig output. If nodes\n>   don't care all that much about rotating these keys, then they can just use\n>   the same output. This is trivially recognizable by nodes, as they already\n>   know the funding keys used, as they're in the channel_announcement.\n>\n>   4. OP_RETURN (yeh, I had to list it). Self explanatory, push some bytes in\n>   an OP_RETURN and use that as the marker.\n>\n>   5. Fiddle w/ the locktime+sequence somehow to make it identifiable to\n>   verifiers. This might run into some unintended interactions if the inputs\n>   provided have either relative or absolute lock times. There might also be\n>   some interaction w/ the main constructing for eltoo (uses the locktime).\n>\n> Of all the options, I think #2 makes the most sense: we already use anchors\n> to be able to do fee bumping after-the-fact for closing transactions, so why\n> not inherit them here. They make the splicing transaction slightly larger,\n> so maybe #3 (or something else) is a better choice.\n>\n> The design space for spicing is preeetty large, so I figure the most\n> productive route might be discussing isolated aspects of it at a time.\n> Personally, I'm not suuuper caught up w/ what the latest design drafts are\n> (aside from convos at the recent LN Dev Summit), but from my PoV, how to\n> communicate the splice to other peers has been an outstanding design\n> question.\n>\n> [1]: https://github.com/lightning/bolts/pull/1004\n> [2]:\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-March/020045.html\n>\n> -- Laolu\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2022-06-29T23:57:40",
                "message_text_only": "Hi Rusty,\n\nThanks for the feedback!\n\n> This is over-design: if you fail to get reliable gossip, your routing will\n> suffer anyway.  Nothing new here.\n\nIdk, it's pretty simple: you're already watching for closes, so if a close\nlooks a certain way, it's a splice. When you see that, you can even take\nnote of the _new_ channel size (funds added/removed) and update your\npathfinding/blindedpaths/hophints accordingly.\n\nIf this is an over-designed solution, that I'd categorize _only_ waiting N\nblocks as wishful thinking, given we have effectively no guarantees w.r.t\nhow long it'll take a message to propagate.\n\nIf by routing you mean a routing node then: no, a routing node doesn't even\nreally need the graph at all to do their job.\n\nIf by routing you mean a sender, then imo still no: you don't necessarily\nneed _all_ gossip, just the latest policies of the nodes you route most\nfrequently to. On top of that, since you can get the latest policy each time\nyou incur a routing failure, as you make payments, you'll get the latest\npolicies of the nodes you care about over time. Also consider that you might\nfail to get \"reliable\" gossip, simply just due to your peer neighborhood\naggressively rate limiting gossip (they only allow 1 update a day for a\nnode, you updated your fee, oops, no splice msg for you).\n\nSo it appears you don't agree that the \"wait N blocks before you close your\nchannels\" isn't a fool proof solution? Why 12 blocks, why not 15? Or 144?\n\n>From my PoV, the whole point of even signalling that a splice is on going,\nis for the sender's/receivers: they can continue to send/recv payments over\nthe channel while the splice is in process. It isn't that a node isn't\ngetting any gossip, it's that if the node fails to obtain the gossip message\nwithin the N block period of time, then the channel has effectively closed\nfrom their PoV, and it may be an hour+ until it's seen as a usable (new)\nchannel again.\n\nIf there isn't a 100% reliable way to signal that a splice is in progress,\nthen this disincentives its usage, as routers can lose out on potential fee\nrevenue, and sends/receivers may grow to favor only very long lived\nchannels. IMO _only_ having a gossip message simply isn't enough: there're\nno real guarantees w.r.t _when_ all relevant parties will get your gossip\nmessage. So why not give them a 100% reliable on chain signal that:\nsomething is in progress here, stay tuned for the gossip message, whenever\nyou receive that.\n\n-- Laolu\n\n\nOn Tue, Jun 28, 2022 at 6:40 PM Rusty Russell <rusty at rustcorp.com.au> wrote:\n\n> Hi Roasbeef,\n>\n> This is over-design: if you fail to get reliable gossip, your routing\n> will suffer anyway.  Nothing new here.\n>\n> And if you *know* you're missing gossip, you can simply delay onchain\n> closures for longer: since nodes should respect the old channel ids for\n> a while anyway.\n>\n> Matt's proposal to simply defer treating onchain closes is elegant and\n> minimal.  We could go further and relax requirements to detect onchain\n> closes at all, and optionally add a perm close message.\n>\n> Cheers,\n> Rusty.\n>\n> Olaoluwa Osuntokun <laolu32 at gmail.com> writes:\n> > Hi y'all,\n> >\n> > This mail was inspired by this [1] spec PR from Lisa. At a high level, it\n> > proposes the nodes add a delay between the time they see a channel\n> closed on\n> > chain, to when they remove it from their local channel graph. The motive\n> > here is to give the gossip message that indicates a splice is in process,\n> > \"enough\" time to propagate through the network. If a node can see this\n> > message before/during the splicing operation, then they'll be able relate\n> > the old and the new channels, meaning it's usable again by\n> senders/receiver\n> > _before_ the entire chain of transactions confirms on chain.\n> >\n> > IMO, this sort of arbitrary delay (expressed in blocks) won't actually\n> > address the issue in practice. The proposal suffers from the following\n> > issues:\n> >\n> >   1. 12 blocks is chosen arbitrarily. If for w/e reason an announcement\n> >   takes longer than 2 hours to reach the \"economic majority\" of\n> >   senders/receivers, then the channel won't be able to mask the splicing\n> >   downtime.\n> >\n> >   2. Gossip propagation delay and offline peers. These days most nodes\n> >   throttle gossip pretty aggressively. As a result, a pair of nodes doing\n> >   several in-flight splices (inputs become double spent or something, so\n> >   they need to try a bunch) might end up being rate limited within the\n> >   network, causing the splice update msg to be lost or delayed\n> significantly\n> >   (IIRC CLN resets these values after 24 hours). On top of that, if a\n> peer\n> >   is offline for too long (think mobile senders), then they may miss the\n> >   update all together as most nodes don't do a full historical\n> >   _channel_update_ dump anymore.\n> >\n> > In order to resolve these issues, I think instead we need to rely on the\n> > primary splicing signal being sourced from the chain itself. In other\n> words,\n> > if I see a channel close, and a closing transaction \"looks\" a certain\n> way,\n> > then I know it's a splice. This would be used in concert w/ any new\n> gossip\n> > messages, as the chain signal is a 100% foolproof way of letting an aware\n> > peer know that a splice is actually happening (not a normal close). A\n> chain\n> > signal doesn't suffer from any of the gossip/time related issues above,\n> as\n> > the signal is revealed at the same time a peer learns of a channel\n> > close/splice.\n> >\n> > Assuming, we agree that a chain signal has some sort of role in the\n> ultimate\n> > plans for splicing, we'd need to decide on exactly _what_ such a signal\n> > looks like. Off the top, a few options are:\n> >\n> >   1. Stuff something in the annex. Works in theory, but not in practice,\n> as\n> >   bitcoind (being the dominant full node implementation on the p2p\n> network,\n> >   as well as what all the miners use) treats annexes as non-standard.\n> Also\n> >   the annex itself might have some fundamental issues that get in the\n> way of\n> >   its use all together [2].\n> >\n> >   2. Re-use the anchors for this purpose. Anchor are nice as they allow\n> for\n> >   1st/2nd/3rd party CPFP. As a splice might have several inputs and\n> outputs,\n> >   both sides will want to make sure it gets confirmed in a timely manner.\n> >   Ofc, RBF can be used here, but that requires both sides to be online to\n> >   make adjustments. Pre-signing can work too, but the effectiveness\n> >   (minimizing chain cost while expediting confirmation) would be\n> dependent\n> >   on the fee step size.\n> >\n> >   In this case, we'd use a different multi-sig output (both sides can\n> rotate\n> >   keys if they want to), and then roll the anchors into this splicing\n> >   transaction. Given that all nodes on the network know what the anchor\n> size\n> >   is (assuming feature bit understanding), they're able to realize that\n> it's\n> >   actually a splice, and they don't need to remove it from the channel\n> graph\n> >   (yet).\n> >\n> >   3. Related to the above: just re-use the same multi-sig output. If\n> nodes\n> >   don't care all that much about rotating these keys, then they can just\n> use\n> >   the same output. This is trivially recognizable by nodes, as they\n> already\n> >   know the funding keys used, as they're in the channel_announcement.\n> >\n> >   4. OP_RETURN (yeh, I had to list it). Self explanatory, push some\n> bytes in\n> >   an OP_RETURN and use that as the marker.\n> >\n> >   5. Fiddle w/ the locktime+sequence somehow to make it identifiable to\n> >   verifiers. This might run into some unintended interactions if the\n> inputs\n> >   provided have either relative or absolute lock times. There might also\n> be\n> >   some interaction w/ the main constructing for eltoo (uses the\n> locktime).\n> >\n> > Of all the options, I think #2 makes the most sense: we already use\n> anchors\n> > to be able to do fee bumping after-the-fact for closing transactions, so\n> why\n> > not inherit them here. They make the splicing transaction slightly\n> larger,\n> > so maybe #3 (or something else) is a better choice.\n> >\n> > The design space for spicing is preeetty large, so I figure the most\n> > productive route might be discussing isolated aspects of it at a time.\n> > Personally, I'm not suuuper caught up w/ what the latest design drafts\n> are\n> > (aside from convos at the recent LN Dev Summit), but from my PoV, how to\n> > communicate the splice to other peers has been an outstanding design\n> > question.\n> >\n> > [1]: https://github.com/lightning/bolts/pull/1004\n> > [2]:\n> >\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-March/020045.html\n> >\n> > -- Laolu\n> > _______________________________________________\n> > Lightning-dev mailing list\n> > Lightning-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220629/0823b5a9/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2022-06-30T00:35:40",
                "message_text_only": "Olaoluwa Osuntokun <laolu32 at gmail.com> writes:\n> Hi Rusty,\n>\n> Thanks for the feedback!\n>\n>> This is over-design: if you fail to get reliable gossip, your routing will\n>> suffer anyway.  Nothing new here.\n>\n> Idk, it's pretty simple: you're already watching for closes, so if a close\n> looks a certain way, it's a splice. When you see that, you can even take\n> note of the _new_ channel size (funds added/removed) and update your\n> pathfinding/blindedpaths/hophints accordingly.\n\nWhy spam the chain?\n\n> If this is an over-designed solution, that I'd categorize _only_ waiting N\n> blocks as wishful thinking, given we have effectively no guarantees w.r.t\n> how long it'll take a message to propagate.\n\nSure, it's a simplification on \"wait 6 blocks plus 30 minutes\".\n\n> If by routing you mean a sender, then imo still no: you don't necessarily\n> need _all_ gossip, just the latest policies of the nodes you route most\n> frequently to. On top of that, since you can get the latest policy each time\n> you incur a routing failure, as you make payments, you'll get the latest\n> policies of the nodes you care about over time. Also consider that you might\n> fail to get \"reliable\" gossip, simply just due to your peer neighborhood\n> aggressively rate limiting gossip (they only allow 1 update a day for a\n> node, you updated your fee, oops, no splice msg for you).\n\nThere's no ratelimiting on new channel announcements?\n\n> So it appears you don't agree that the \"wait N blocks before you close your\n> channels\" isn't a fool proof solution? Why 12 blocks, why not 15? Or 144?\n\nBecause it's simple.\n\n>>From my PoV, the whole point of even signalling that a splice is on going,\n> is for the sender's/receivers: they can continue to send/recv payments over\n> the channel while the splice is in process. It isn't that a node isn't\n> getting any gossip, it's that if the node fails to obtain the gossip message\n> within the N block period of time, then the channel has effectively closed\n> from their PoV, and it may be an hour+ until it's seen as a usable (new)\n> channel again.\n\nSure.  If you want to not forget channels at all on close, that works too.\n\n> If there isn't a 100% reliable way to signal that a splice is in progress,\n> then this disincentives its usage, as routers can lose out on potential fee\n> revenue, and sends/receivers may grow to favor only very long lived\n> channels. IMO _only_ having a gossip message simply isn't enough: there're\n> no real guarantees w.r.t _when_ all relevant parties will get your gossip\n> message. So why not give them a 100% reliable on chain signal that:\n> something is in progress here, stay tuned for the gossip message, whenever\n> you receive that.\n\nThat's not 100% reliable at all.   How long to you want for the new\ngossip?\n\nJust treat every close as signalling \"stay tuned for the gossip\nmessage\".  That's reliable.  And simple.\n\nCheers,\nRusty."
            },
            {
                "author": "lisa neigut",
                "date": "2022-06-30T00:43:09",
                "message_text_only": "Adding a noticeable on-chain signal runs counter to the goal of the move\nto taproot / gossip v2, which is to make lightning's onchain footprint\nindistinguishable from\nany other onchain usage.\n\nI'm admittedly a bit confused as to why onchain signals are even being\nseriously\n proposed. Aside from \"infallibility\", is there another reason for\nsuggesting\nwe add an onchain detectable signal for this? Seems heavy handed imo, given\nthat the severity of a comms failure is pretty minimal (*potential* for\nlost routing fees).\n\n> So it appears you don't agree that the \"wait N blocks before you close\nyour\nchannels\" isn't a fool proof solution? Why 12 blocks, why not 15? Or 144?\n\nfwiw I seem to remember seeing that it takes  ~an hour for gossip to\npropagate\n(no link sorry). Given that, 2x an hour or 12 blocks is a reasonable first\nestimate.\nI trust we'll have time to tune this after we've had some real-world\nexperience with them.\n\nFurther, we can always add more robust signaling later, if lost routing\nfees turns\nout to be a huge issue.\n\nFinally, worth noting that Alex Myer's minisketch project may well\nhelp/improve gossip\nreconciliation efficiency to the point where gossip reliability is less\nof an issue.\n\n~nifty\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220629/5a104870/attachment.html>"
            },
            {
                "author": "lisa neigut",
                "date": "2022-06-30T03:06:50",
                "message_text_only": "Had another thought: if you've seen a chain close but also have a gossip\nmessage that\nindicates this is a splice, you SHOULD propagate that gossip more\nurgently/widely than\nany other gossip you've got. Adding an urgency metric to gossip is fuzzy to\nenforce... *handwaves*.\n\nYou *do* get the onchain signal, we just change the behavior of the\nsecondary information system\ninstead of embedding the info into the chain..\n\n\"Spamming\" gossip with splices expensive -- there's a real-world cost\n(onchain fees) to\nclosing a channel (the signal to promote/prioritize a gossip msg) which\ncuts down on the ability to send out these 'urgent' messages with any\nfrequency.\n\n~nifty\n\nOn Wed, Jun 29, 2022 at 7:43 PM lisa neigut <niftynei at gmail.com> wrote:\n\n> Adding a noticeable on-chain signal runs counter to the goal of the move\n> to taproot / gossip v2, which is to make lightning's onchain footprint\n> indistinguishable from\n> any other onchain usage.\n>\n> I'm admittedly a bit confused as to why onchain signals are even being\n> seriously\n>  proposed. Aside from \"infallibility\", is there another reason for\n> suggesting\n> we add an onchain detectable signal for this? Seems heavy handed imo,\n> given\n> that the severity of a comms failure is pretty minimal (*potential* for\n> lost routing fees).\n>\n> > So it appears you don't agree that the \"wait N blocks before you close\n> your\n> channels\" isn't a fool proof solution? Why 12 blocks, why not 15? Or 144?\n>\n> fwiw I seem to remember seeing that it takes  ~an hour for gossip to\n> propagate\n> (no link sorry). Given that, 2x an hour or 12 blocks is a reasonable first\n> estimate.\n> I trust we'll have time to tune this after we've had some real-world\n> experience with them.\n>\n> Further, we can always add more robust signaling later, if lost routing\n> fees turns\n> out to be a huge issue.\n>\n> Finally, worth noting that Alex Myer's minisketch project may well\n> help/improve gossip\n> reconciliation efficiency to the point where gossip reliability is less\n> of an issue.\n>\n> ~nifty\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220629/b97093da/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Achieving Zero Downtime Splicing in Practice via Chain Signals",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Rusty Russell",
                "Olaoluwa Osuntokun",
                "lisa neigut"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 26782
        }
    },
    {
        "title": "[Lightning-dev] Three Strategies for Lightning Forwarding Nodes",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2022-06-28T02:34:25",
                "message_text_only": "Good morning list,\n\nThis is a short (relative to my typical crap) writeup on some strategies that Lightning forwarding nodes might utilize.\n\nI have been thinking of various strategies that actual node operators (as I understood from discussing with a few of them) use:\n\n* Passive rebalance / feerate by balance\n  * Set feerates according to balance: increase feerates when our side has low balance, reduce feerates when our side has high balance.\n  * \"passive rebalance\" because we are basically encouraging payments via our channel if the balance is in our favor, and discouraging payments if the balance is against us, thus typical payments will \"normally\" rebalance our node naturally without us spending anything.\n* Low fee\n  * Just fix the fee to a low fee, e.g. base 1 proportional 1 or even the @zerofeerouting guy of base 0 proportional 0.\n  * Ridiculously simple, no active management, no scripts, no nothing.\n* Wall\n  * Set to a constant (or mostly constant) high feerate.\n  * Actively rebalance, targeting low-fee routes (i.e. less than our earnings), and constantly probe the network for the rare low-fee routes that we can use to rebalance.\n  * Basically, buy cheap liquidity and resell it at higher prices.\n\n\nThe interesting thing is how the three interact.\n\nSuppose we have a mixed network composed ONLY of passive rebalancers and walls.\nIn that case, the passive rebalancers might occasionally set channels to low fees, in which case the walls buy up their liquidity, but eventually the liquidity of the passive rebalancer is purchased and the passive rebalancer raises their price point.\nThe network then settles with every forwarding node having roughly equal balance on their channels, but note that it was the walls who paid to the passive rebalancers to get the channels into a nice balance.\nIn particular, if there were only a single wall node, it can stop rebalancing once the price to rebalance costs more than 49% of its earnings, so it paid 49% of its earnings to the passive rebalancers and keeps 51% of its earnings, thus earning more than the passive rebalancers earn.\nHowever, once multiple wall nodes exist, they will start bidding for the available liquidity from the passive rebalancers and the may find it difficult to compete once the passive rebalancers set their feerates to more than 50% of the wall feerate, at which point the passive rebalancers now end up earning more than the wall nodes (because the wall nodes now pay more to the passive rebalancers than what they keep).\n\nThus, it seems to me that passive rebalancers would outcompete wall strategies, if they were the only strategies on the network.\n\nHowever, the network as-is contains a LOT of tiny nodes with low feerates.\n\nIn such an environment, walls can pick up liquidity for really, really cheap, leaving the low-feerate nodes with no liquidity in the correct direction.\nAnd thus, it seems plausible that they can resell the liquidity later at much higher feerates, possibly outcompeting the passive rebalancers.\n\nUnfortunately:\n\n* Low feerate nodes are notoriously unreliable for payments; their channels are usually saturated in one side or another. since walls keep taking their liquidity.\n  * Because of this known unreliability, some payer strategies filter them out via some heuristics (e.g. payment unreliability information).\n    Thus, even in the rare case where payment flows change on the network, they are not used by payers --- instead, walls exploit them since walls do not care if rebalancing fails, they will always just retry later.\n* One argument FOR using low-feerate nodes is that it \"supports the network\".\n  * However, it seems to me that the low-feerate nodes are actually being exploited by the wall nodes instead, and the low-feerate nodes have too little payment reliability to actually support payers instead of large-scale forwarders.\n* Both low-feerates and walls do not leak their channel balances, whereas passive rebalancers do leak their channel balance.\n\nThe above is just some thinking of mine --- actual experimentation on models or on actual network nodes might be better than my speculation.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Michael Folkson",
                "date": "2022-06-28T10:26:39",
                "message_text_only": "Hey ZmnSCPxj\n\nIt is an interesting topic. Alex Bosworth did a presentation at the Lightning Hack Day last year with a similar attempt at categorizing the different strategies for a routing/forwarding node (Ping Pong, Liquidity Battery, Inbound Sourcing, Liquidity Trader, Last Mile, Swap etc)\n\nhttps://btctranscripts.com/lightning-hack-day/2021-03-27-alex-bosworth-lightning-routing/\n\nIt seems like your attempt is a little more granular and unstructured (based on individual responses) but perhaps it fits into the broad categories Alex suggested maybe with some additional ones?\n\nThanks\nMichael\n\n--\nMichael Folkson\nEmail: michaelfolkson at protonmail.com\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n\n\n------- Original Message -------\nOn Tuesday, June 28th, 2022 at 03:34, ZmnSCPxj via Lightning-dev <lightning-dev at lists.linuxfoundation.org> wrote:\n\n\n> Good morning list,\n>\n> This is a short (relative to my typical crap) writeup on some strategies that Lightning forwarding nodes might utilize.\n>\n> I have been thinking of various strategies that actual node operators (as I understood from discussing with a few of them) use:\n>\n> * Passive rebalance / feerate by balance\n> * Set feerates according to balance: increase feerates when our side has low balance, reduce feerates when our side has high balance.\n> * \"passive rebalance\" because we are basically encouraging payments via our channel if the balance is in our favor, and discouraging payments if the balance is against us, thus typical payments will \"normally\" rebalance our node naturally without us spending anything.\n> * Low fee\n> * Just fix the fee to a low fee, e.g. base 1 proportional 1 or even the @zerofeerouting guy of base 0 proportional 0.\n> * Ridiculously simple, no active management, no scripts, no nothing.\n> * Wall\n> * Set to a constant (or mostly constant) high feerate.\n> * Actively rebalance, targeting low-fee routes (i.e. less than our earnings), and constantly probe the network for the rare low-fee routes that we can use to rebalance.\n> * Basically, buy cheap liquidity and resell it at higher prices.\n>\n>\n> The interesting thing is how the three interact.\n>\n> Suppose we have a mixed network composed ONLY of passive rebalancers and walls.\n> In that case, the passive rebalancers might occasionally set channels to low fees, in which case the walls buy up their liquidity, but eventually the liquidity of the passive rebalancer is purchased and the passive rebalancer raises their price point.\n> The network then settles with every forwarding node having roughly equal balance on their channels, but note that it was the walls who paid to the passive rebalancers to get the channels into a nice balance.\n> In particular, if there were only a single wall node, it can stop rebalancing once the price to rebalance costs more than 49% of its earnings, so it paid 49% of its earnings to the passive rebalancers and keeps 51% of its earnings, thus earning more than the passive rebalancers earn.\n> However, once multiple wall nodes exist, they will start bidding for the available liquidity from the passive rebalancers and the may find it difficult to compete once the passive rebalancers set their feerates to more than 50% of the wall feerate, at which point the passive rebalancers now end up earning more than the wall nodes (because the wall nodes now pay more to the passive rebalancers than what they keep).\n>\n> Thus, it seems to me that passive rebalancers would outcompete wall strategies, if they were the only strategies on the network.\n>\n> However, the network as-is contains a LOT of tiny nodes with low feerates.\n>\n> In such an environment, walls can pick up liquidity for really, really cheap, leaving the low-feerate nodes with no liquidity in the correct direction.\n> And thus, it seems plausible that they can resell the liquidity later at much higher feerates, possibly outcompeting the passive rebalancers.\n>\n> Unfortunately:\n>\n> * Low feerate nodes are notoriously unreliable for payments; their channels are usually saturated in one side or another. since walls keep taking their liquidity.\n> * Because of this known unreliability, some payer strategies filter them out via some heuristics (e.g. payment unreliability information).\n> Thus, even in the rare case where payment flows change on the network, they are not used by payers --- instead, walls exploit them since walls do not care if rebalancing fails, they will always just retry later.\n> * One argument FOR using low-feerate nodes is that it \"supports the network\".\n> * However, it seems to me that the low-feerate nodes are actually being exploited by the wall nodes instead, and the low-feerate nodes have too little payment reliability to actually support payers instead of large-scale forwarders.\n> * Both low-feerates and walls do not leak their channel balances, whereas passive rebalancers do leak their channel balance.\n>\n> The above is just some thinking of mine --- actual experimentation on models or on actual network nodes might be better than my speculation.\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            }
        ],
        "thread_summary": {
            "title": "Three Strategies for Lightning Forwarding Nodes",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Michael Folkson",
                "ZmnSCPxj"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 9442
        }
    },
    {
        "title": "[Lightning-dev] Onion messages rate-limiting",
        "thread_messages": [
            {
                "author": "Bastien TEINTURIER",
                "date": "2022-06-29T08:28:26",
                "message_text_only": "During the recent Oakland Dev Summit, some lightning engineers got\ntogether to discuss DoS\nprotection for onion messages. Rusty proposed a very simple\nrate-limiting scheme that\nstatistically propagates back to the correct sender, which we describe\nin details below.\n\nYou can also read this in gist format if that works better for you [1].\n\nNodes apply per-peer rate limits on _incoming_ onion messages that\nshould be relayed (e.g.\nN/seconds with some burst tolerance). It is recommended to allow more\nonion messages from\npeers with whom you have channels, for example 10/seconds when you\nhave a channel and 1/second\nwhen you don't.\n\nWhen relaying an onion message, nodes keep track of where it came from\n(by using the `node_id` of\nthe peer who sent that message). Nodes only need the last such\n`node_id` per outgoing connection,\nwhich ensures the memory footprint is very small. Also, this data\ndoesn't need to be persisted.\n\nLet's walk through an example to illustrate this mechanism:\n\n* Bob receives an onion message from Alice that should be relayed to Carol\n* After relaying that message, Bob stores Alice's `node_id` in its\nper-connection state with Carol\n* Bob receives an onion message from Eve that should be relayed to Carol\n* After relaying that message, Bob replaces Alice's `node_id` with\nEve's `node_id` in its\nper-connection state with Carol\n* Bob receives an onion message from Alice that should be relayed to Dave\n* After relaying that message, Bob stores Alice's `node_id` in its\nper-connection state with Dave\n* ...\n\nWe introduce a new message that will be sent when dropping an incoming\nonion message because it\nreached rate limits:\n\n1. type: 515 (`onion_message_drop`)\n2. data:\n   * [`rate_limited`:`u8`]\n   * [`shared_secret_hash`:`32*byte`]\n\nWhenever an incoming onion message reaches the rate limit, the\nreceiver sends `onion_message_drop`\nto the sender. The sender looks at its per-connection state to find\nwhere the message was coming\nfrom and relays `onion_message_drop` to the last sender, halving their\nrate limits with that peer.\n\nIf the sender doesn't overflow the rate limit again, the receiver\nshould double the rate limit\nafter 30 seconds, until it reaches the default rate limit again.\n\nThe flow will look like:\n\nAlice                      Bob                      Carol\n  |                         |                         |\n  |      onion_message      |                         |\n  |------------------------>|                         |\n  |                         |      onion_message      |\n  |                         |------------------------>|\n  |                         |    onion_message_drop   |\n  |                         |<------------------------|\n  |    onion_message_drop   |                         |\n  |<------------------------|                         |\n\nThe `shared_secret_hash` field contains a BIP 340 tagged hash of the\nSphinx shared secret of the\nrate limiting peer (in the example above, Carol):\n\n* `shared_secret_hash = SHA256(SHA256(\"onion_message_drop\") ||\nSHA256(\"onion_message_drop\") || sphinx_shared_secret)`\n\nThis value is known by the node that created the onion message: if\n`onion_message_drop` propagates\nall the way back to them, it lets them know which part of the route is\ncongested, allowing them\nto retry through a different path.\n\nWhenever there is some latency between nodes and many onion messages,\n`onion_message_drop` may\nbe relayed to the incorrect incoming peer (since we only store the\n`node_id` of the _last_ incoming\npeer in our outgoing connection state). The following example highlights this:\n\n Eve                       Bob                      Carol\n  |      onion_message      |                         |\n  |------------------------>|      onion_message      |\n  |      onion_message      |------------------------>|\n  |------------------------>|      onion_message      |\n  |      onion_message      |------------------------>|\n  |------------------------>|      onion_message      |\n                            |------------------------>|\nAlice                       |    onion_message_drop   |\n  |      onion_message      |                    +----|\n  |------------------------>|      onion_message |    |\n  |                         |--------------------|--->|\n  |                         |                    |    |\n  |                         |                    |    |\n  |                         |                    |    |\n  |    onion_message_drop   |<-------------------+    |\n  |<------------------------|                         |\n\nIn this example, Eve is spamming but `onion_message_drop` is\npropagated back to Alice instead.\nHowever, this scheme will _statistically_ penalize the right incoming\npeer (with a probability\ndepending on the volume of onion messages that the spamming peer is\ngenerating compared to the\nvolume of legitimate onion messages).\n\nIt is an interesting research problem to find formulas for those\nprobabilities to evaluate how\nefficient this will be against various types of spam. We hope\nresearchers on this list will be\ninterested in looking into it and will come up with a good model to\nevaluate that scheme.\n\nTo increase the accuracy of attributing `onion_message_drop`, more\ndata could be stored in the\nfuture if it becomes necessary. We need more research to quantify how\nmuch accuracy would be\ngained by storing more data and making the protocol more complex.\n\nCheers,\nBastien\n\n[1] https://gist.github.com/t-bast/e37ee9249d9825e51d260335c94f0fcf\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220629/513b6829/attachment.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2022-06-30T00:22:08",
                "message_text_only": "Hi t-bast,\n\nHappy to see this finally written up! With this, we have two classes of\nproposals for rate limiting onion messaging:\n\n  1. Back propagation based rate limiting as described here.\n\n  2. Allowing nodes to express a per-message cost for their forwarding\n  services, which is described here [1].\n\nI still need to digest everything proposed here, but personally I'm more\noptimistic about the 2nd category than the 1st.\n\nOne issue I see w/ the first category is that a single party can flood the\nnetwork and cause nodes to trigger their rate limits, which then affects the\nusability of the onion messages for all other well-behaving parties. An\nexample, this might mean I can't fetch invoices, give up after a period of\ntime (how long?), then result to a direct connection (perceived payment\nlatency accumulated along the way).\n\nWith the 2nd route, if an attacker floods the network, they need to directly\npay for the forwarding usage themselves, though they may also directly cause\nnodes to adjust their forwarding rate accordingly. However in this case, the\nattacker has incurred a concrete cost, and even if the rates rise, then\nthose that really need the service (notifying an LSP that a user is online\nor w/e) can continue to pay that new rate. In other words, by _pricing_ the\nresource utilization, demand preferences can be exchanged, leading to more\nefficient long term resource allocation.\n\nW.r.t this topic, one event that imo is worth pointing out is that a very\npopular onion routing system, Tor, has been facing a severe DDoS attack that\nhas lasted weeks, and isn't yet fully resolved [2]. The on going flooding\nattack on Tor has actually started to affect LN (iirc over half of all\npublic routing nodes w/ an advertised address are tor-only), and other\nrelated systems like Umbrel that 100% rely on tor for networking traversal.\nFunnily enough, Tor developers have actually suggested adding some PoW to\nattempt to mitigate DDoS attacks [3]. In that same post they throw around\nthe idea of using anonymous tokens to allow nodes to give them to \"good\"\nclients, which is pretty similar to my lofty Forwarding Pass idea as relates\nto onion messaging, and also general HTLC jamming mitigation.\n\nIn summary, we're not the first to attempt to tackle the problem of rate\nlimiting relayed message spam in an anonymous/pseudonymous network, and we\ncan probably learn a lot from what is and isn't working w.r.t how Tor\nhandles things. As you note near the end of your post, this might just be\nthe first avenue in a long line of research to best figure out how to handle\nthe spam concerns introduced by onion messaging. From my PoV, it still seems\nto be an open question if the same network can be _both_ a reliable\nmicro-payment system _and_ also a reliable arbitrary message transport\nlayer. I guess only time will tell...\n\n> The `shared_secret_hash` field contains a BIP 340 tagged hash\n\nAny reason to use the tagged hash here vs just a plain ol HMAC? Under the\nhood, they have a pretty similar construction [4].\n\n[1]:\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2022-February/003498.html\n[2]: https://status.torproject.org/issues/2022-06-09-network-ddos/\n[3]: https://blog.torproject.org/stop-the-onion-denial/\n[4]: https://datatracker.ietf.org/doc/html/rfc2104\n\n-- Laolu\n\n\n\nOn Wed, Jun 29, 2022 at 1:28 AM Bastien TEINTURIER <bastien at acinq.fr> wrote:\n\n> During the recent Oakland Dev Summit, some lightning engineers got together to discuss DoS\n> protection for onion messages. Rusty proposed a very simple rate-limiting scheme that\n> statistically propagates back to the correct sender, which we describe in details below.\n>\n> You can also read this in gist format if that works better for you [1].\n>\n> Nodes apply per-peer rate limits on _incoming_ onion messages that should be relayed (e.g.\n> N/seconds with some burst tolerance). It is recommended to allow more onion messages from\n> peers with whom you have channels, for example 10/seconds when you have a channel and 1/second\n> when you don't.\n>\n> When relaying an onion message, nodes keep track of where it came from (by using the `node_id` of\n> the peer who sent that message). Nodes only need the last such `node_id` per outgoing connection,\n> which ensures the memory footprint is very small. Also, this data doesn't need to be persisted.\n>\n> Let's walk through an example to illustrate this mechanism:\n>\n> * Bob receives an onion message from Alice that should be relayed to Carol\n> * After relaying that message, Bob stores Alice's `node_id` in its per-connection state with Carol\n> * Bob receives an onion message from Eve that should be relayed to Carol\n> * After relaying that message, Bob replaces Alice's `node_id` with Eve's `node_id` in its\n> per-connection state with Carol\n> * Bob receives an onion message from Alice that should be relayed to Dave\n> * After relaying that message, Bob stores Alice's `node_id` in its per-connection state with Dave\n> * ...\n>\n> We introduce a new message that will be sent when dropping an incoming onion message because it\n> reached rate limits:\n>\n> 1. type: 515 (`onion_message_drop`)\n> 2. data:\n>    * [`rate_limited`:`u8`]\n>    * [`shared_secret_hash`:`32*byte`]\n>\n> Whenever an incoming onion message reaches the rate limit, the receiver sends `onion_message_drop`\n> to the sender. The sender looks at its per-connection state to find where the message was coming\n> from and relays `onion_message_drop` to the last sender, halving their rate limits with that peer.\n>\n> If the sender doesn't overflow the rate limit again, the receiver should double the rate limit\n> after 30 seconds, until it reaches the default rate limit again.\n>\n> The flow will look like:\n>\n> Alice                      Bob                      Carol\n>   |                         |                         |\n>   |      onion_message      |                         |\n>   |------------------------>|                         |\n>   |                         |      onion_message      |\n>   |                         |------------------------>|\n>   |                         |    onion_message_drop   |\n>   |                         |<------------------------|\n>   |    onion_message_drop   |                         |\n>   |<------------------------|                         |\n>\n> The `shared_secret_hash` field contains a BIP 340 tagged hash of the Sphinx shared secret of the\n> rate limiting peer (in the example above, Carol):\n>\n> * `shared_secret_hash = SHA256(SHA256(\"onion_message_drop\") || SHA256(\"onion_message_drop\") || sphinx_shared_secret)`\n>\n> This value is known by the node that created the onion message: if `onion_message_drop` propagates\n> all the way back to them, it lets them know which part of the route is congested, allowing them\n> to retry through a different path.\n>\n> Whenever there is some latency between nodes and many onion messages, `onion_message_drop` may\n> be relayed to the incorrect incoming peer (since we only store the `node_id` of the _last_ incoming\n> peer in our outgoing connection state). The following example highlights this:\n>\n>  Eve                       Bob                      Carol\n>   |      onion_message      |                         |\n>   |------------------------>|      onion_message      |\n>   |      onion_message      |------------------------>|\n>   |------------------------>|      onion_message      |\n>   |      onion_message      |------------------------>|\n>   |------------------------>|      onion_message      |\n>                             |------------------------>|\n> Alice                       |    onion_message_drop   |\n>   |      onion_message      |                    +----|\n>   |------------------------>|      onion_message |    |\n>   |                         |--------------------|--->|\n>   |                         |                    |    |\n>   |                         |                    |    |\n>   |                         |                    |    |\n>   |    onion_message_drop   |<-------------------+    |\n>   |<------------------------|                         |\n>\n> In this example, Eve is spamming but `onion_message_drop` is propagated back to Alice instead.\n> However, this scheme will _statistically_ penalize the right incoming peer (with a probability\n> depending on the volume of onion messages that the spamming peer is generating compared to the\n> volume of legitimate onion messages).\n>\n> It is an interesting research problem to find formulas for those probabilities to evaluate how\n> efficient this will be against various types of spam. We hope researchers on this list will be\n> interested in looking into it and will come up with a good model to evaluate that scheme.\n>\n> To increase the accuracy of attributing `onion_message_drop`, more data could be stored in the\n> future if it becomes necessary. We need more research to quantify how much accuracy would be\n> gained by storing more data and making the protocol more complex.\n>\n> Cheers,\n> Bastien\n>\n> [1] https://gist.github.com/t-bast/e37ee9249d9825e51d260335c94f0fcf\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220629/be1c82d7/attachment-0001.html>"
            },
            {
                "author": "vwallace",
                "date": "2022-06-30T01:24:52",
                "message_text_only": "Heya Laolu,\n\nFrom my PoV, adding prepayments to onion messages is putting the cart before the horse a bit, think there's a good amount of recourse before resorting to that.\n\nSeems there are two cases to address here:\n\n1. People are trying to stream GoT over lightning\n\nIn this case, just rate limiting should disrupt their viewing experience such that it becomes unusable. Don\u2019t think LN can be compared to Tor here because they explicitly want to support this case and we don\u2019t.\n\n2. An attacker is trying to flood the network with OMs\n\nIn this case, IMO LN also can\u2019t be compared to Tor because you can limit your OMs to channel partners only, and this in itself provides a \u201cproof of work\u201d that an attacker can\u2019t surmount without actually opening channels.\n\nAnother huge win of backpressure is that it only needs to happen in DoS situations, meaning it doesn\u2019t have to impact users in the normal case.\n\nCheers \u2014Val\n\n------- Original Message -------\nOn Wednesday, June 29th, 2022 at 8:22 PM, Olaoluwa Osuntokun <laolu32 at gmail.com> wrote:\n\n> Hi t-bast,\n>\n> Happy to see this finally written up! With this, we have two classes of\n> proposals for rate limiting onion messaging:\n>\n> 1. Back propagation based rate limiting as described here.\n>\n> 2. Allowing nodes to express a per-message cost for their forwarding\n> services, which is described here [1].\n>\n> I still need to digest everything proposed here, but personally I'm more\n> optimistic about the 2nd category than the 1st.\n>\n> One issue I see w/ the first category is that a single party can flood the\n> network and cause nodes to trigger their rate limits, which then affects the\n> usability of the onion messages for all other well-behaving parties. An\n> example, this might mean I can't fetch invoices, give up after a period of\n> time (how long?), then result to a direct connection (perceived payment\n> latency accumulated along the way).\n>\n> With the 2nd route, if an attacker floods the network, they need to directly\n> pay for the forwarding usage themselves, though they may also directly cause\n> nodes to adjust their forwarding rate accordingly. However in this case, the\n> attacker has incurred a concrete cost, and even if the rates rise, then\n> those that really need the service (notifying an LSP that a user is online\n> or w/e) can continue to pay that new rate. In other words, by _pricing_ the\n> resource utilization, demand preferences can be exchanged, leading to more\n> efficient long term resource allocation.\n>\n> W.r.t this topic, one event that imo is worth pointing out is that a very\n> popular onion routing system, Tor, has been facing a severe DDoS attack that\n> has lasted weeks, and isn't yet fully resolved [2]. The on going flooding\n> attack on Tor has actually started to affect LN (iirc over half of all\n> public routing nodes w/ an advertised address are tor-only), and other\n> related systems like Umbrel that 100% rely on tor for networking traversal.\n> Funnily enough, Tor developers have actually suggested adding some PoW to\n> attempt to mitigate DDoS attacks [3]. In that same post they throw around\n> the idea of using anonymous tokens to allow nodes to give them to \"good\"\n> clients, which is pretty similar to my lofty Forwarding Pass idea as relates\n> to onion messaging, and also general HTLC jamming mitigation.\n>\n> In summary, we're not the first to attempt to tackle the problem of rate\n> limiting relayed message spam in an anonymous/pseudonymous network, and we\n> can probably learn a lot from what is and isn't working w.r.t how Tor\n> handles things. As you note near the end of your post, this might just be\n> the first avenue in a long line of research to best figure out how to handle\n> the spam concerns introduced by onion messaging. From my PoV, it still seems\n> to be an open question if the same network can be _both_ a reliable\n> micro-payment system _and_ also a reliable arbitrary message transport\n> layer. I guess only time will tell...\n>\n>> The `shared_secret_hash` field contains a BIP 340 tagged hash\n>\n> Any reason to use the tagged hash here vs just a plain ol HMAC? Under the\n> hood, they have a pretty similar construction [4].\n>\n> [1]: https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-February/003498.html\n> [2]: https://status.torproject.org/issues/2022-06-09-network-ddos/\n> [3]: https://blog.torproject.org/stop-the-onion-denial/\n> [4]: https://datatracker.ietf.org/doc/html/rfc2104\n>\n> -- Laolu\n>\n> On Wed, Jun 29, 2022 at 1:28 AM Bastien TEINTURIER <bastien at acinq.fr> wrote:\n>\n>> During the recent Oakland Dev Summit, some lightning engineers got together to discuss DoS\n>> protection for onion messages. Rusty proposed a very simple rate-limiting scheme that\n>> statistically propagates back to the correct sender, which we describe in details below.\n>>\n>> You can also read this in gist format if that works better for you [1].\n>>\n>> Nodes apply per-peer rate limits on _incoming_ onion messages that should be relayed (e.g.\n>> N/seconds with some burst tolerance). It is recommended to allow more onion messages from\n>> peers with whom you have channels, for example 10/seconds when you have a channel and 1/second\n>> when you don't.\n>>\n>> When relaying an onion message, nodes keep track of where it came from (by using the `node_id` of\n>> the peer who sent that message). Nodes only need the last such `node_id` per outgoing connection,\n>> which ensures the memory footprint is very small. Also, this data doesn't need to be persisted.\n>>\n>> Let's walk through an example to illustrate this mechanism:\n>>\n>> * Bob receives an onion message from Alice that should be relayed to Carol\n>> * After relaying that message, Bob stores Alice's `node_id` in its per-connection state with Carol\n>> * Bob receives an onion message from Eve that should be relayed to Carol\n>> * After relaying that message, Bob replaces Alice's `node_id` with Eve's `node_id` in its\n>> per-connection state with Carol\n>> * Bob receives an onion message from Alice that should be relayed to Dave\n>> * After relaying that message, Bob stores Alice's `node_id` in its per-connection state with Dave\n>> * ...\n>>\n>> We introduce a new message that will be sent when dropping an incoming onion message because it\n>> reached rate limits:\n>>\n>> 1. type: 515 (`onion_message_drop`)\n>> 2. data:\n>>    * [`rate_limited`:`u8`]\n>>    * [`shared_secret_hash`:`32*byte`]\n>>\n>> Whenever an incoming onion message reaches the rate limit, the receiver sends `onion_message_drop`\n>> to the sender. The sender looks at its per-connection state to find where the message was coming\n>> from and relays `onion_message_drop` to the last sender, halving their rate limits with that peer.\n>>\n>> If the sender doesn't overflow the rate limit again, the receiver should double the rate limit\n>> after 30 seconds, until it reaches the default rate limit again.\n>>\n>> The flow will look like:\n>>\n>> Alice                      Bob                      Carol\n>>   |                         |                         |\n>>   |      onion_message      |                         |\n>>   |------------------------>|                         |\n>>   |                         |      onion_message      |\n>>   |                         |------------------------>|\n>>   |                         |    onion_message_drop   |\n>>   |                         |<------------------------|\n>>   |    onion_message_drop   |                         |\n>>   |<------------------------|                         |\n>>\n>> The `shared_secret_hash` field contains a BIP 340 tagged hash of the Sphinx shared secret of the\n>> rate limiting peer (in the example above, Carol):\n>>\n>> * `shared_secret_hash = SHA256(SHA256(\"onion_message_drop\") || SHA256(\"onion_message_drop\") || sphinx_shared_secret)`\n>>\n>> This value is known by the node that created the onion message: if `onion_message_drop` propagates\n>> all the way back to them, it lets them know which part of the route is congested, allowing them\n>> to retry through a different path.\n>>\n>> Whenever there is some latency between nodes and many onion messages, `onion_message_drop` may\n>> be relayed to the incorrect incoming peer (since we only store the `node_id` of the _last_ incoming\n>> peer in our outgoing connection state). The following example highlights this:\n>>\n>>  Eve                       Bob                      Carol\n>>   |      onion_message      |                         |\n>>   |------------------------>|      onion_message      |\n>>   |      onion_message      |------------------------>|\n>>   |------------------------>|      onion_message      |\n>>   |      onion_message      |------------------------>|\n>>   |------------------------>|      onion_message      |\n>>                             |------------------------>|\n>> Alice                       |    onion_message_drop   |\n>>   |      onion_message      |                    +----|\n>>   |------------------------>|      onion_message |    |\n>>   |                         |--------------------|--->|\n>>   |                         |                    |    |\n>>   |                         |                    |    |\n>>   |                         |                    |    |\n>>   |    onion_message_drop   |<-------------------+    |\n>>   |<------------------------|                         |\n>>\n>> In this example, Eve is spamming but `onion_message_drop` is propagated back to Alice instead.\n>> However, this scheme will _statistically_ penalize the right incoming peer (with a probability\n>> depending on the volume of onion messages that the spamming peer is generating compared to the\n>> volume of legitimate onion messages).\n>>\n>> It is an interesting research problem to find formulas for those probabilities to evaluate how\n>> efficient this will be against various types of spam. We hope researchers on this list will be\n>> interested in looking into it and will come up with a good model to evaluate that scheme.\n>>\n>> To increase the accuracy of attributing `onion_message_drop`, more data could be stored in the\n>> future if it becomes necessary. We need more research to quantify how much accuracy would be\n>> gained by storing more data and making the protocol more complex.\n>>\n>> Cheers,\n>> Bastien\n>>\n>> [1]\n>> https://gist.github.com/t-bast/e37ee9249d9825e51d260335c94f0fcf\n>>\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220630/83ca2be4/attachment-0001.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2022-06-30T02:19:04",
                "message_text_only": "Thanks Bastien for writing this up! This is a pretty trivial and straightforward way to rate-limit \nonion messages in a way that allows legitimate users to continue using the system in spite of some \nbad actors trying (and failing, due to being rate-limited) to DoS the network.\n\nI do think any spec for this shouldn't make any recommendations about willingness to relay onion \nmessages for anonymous no-channel third parties, if anything deliberately staying mum on it and \nallowing nodes to adapt policy (and probably rate-limit no-channel third-parties before they rate \nlimit any peer they have a channel with). Ultimately, we have to assume that nodes will attempt to \nsend onion messages by routing through the existing channel graph, so there's little reason to worry \ntoo much about ensuring ability to relay for anonymous parties.\n\nBetter yet, as Val points out, requiring a channel to relay onion messages puts a very real, \nnontrivial (in a world of msats) cost to getting an onion messaging channel. Better yet, with \nbackpressure ability to DoS onion message links isn't denominated in number of messages, but instead \nin number of channels you are able to create, making the backpressure system equivalent to today's \nHTLC DoS considerations, whereas explicit payment allows an attacker to pay much less to break the \nsystem.\n\nAs for the proposal to charge for onion messages, I'm still not at all sure where its coming from. \nIt seems to flow from a classic \"have a hammer (a system to make micropayments for things), better \nturn this problem into a nail (by making users pay for it)\" approach, but it doesn't actually solve \nthe problem at hand.\n\nEven if you charge for onion messages, users may legitimately want to send a bunch of payments in \nbulk, and trivially overflow a home or Tor nodes' bandwidth. The only response to that, whether its \na DoS attack or a legitimate user, is to rate-limit, and to rate-limit in a way that tells the user \nsending the messages to back off! Sure, you could do that by failing onion messages with an error \nthat updates the fee you charge, but you're ultimately doing a poor-man's (or, I suppose, \nrich-man's) version of what Bastien proposes, not adding some fundamental difference.\n\nUltimately, paying suffers from the standard PoW-for-spam issue - you cannot assign a reasonable \ncost that an attacker cares about without impacting the system's usability due to said cost. Indeed, \nmaking it expensive enough to mount a months-long DDoS without impacting legitimate users be pretty \neasy - at 1msat per relay of a 1366 byte onion message you can only saturate an average home users' \n30Mbps connection for 30 minutes before you rack up a dollar in costs, but if your concern is \nwhether someone can reasonably trivially take out the network for minutes at a time to make it have \nperceptibly high failure rates, no reasonable cost scheme will work. Quite the opposite - the only \nreasonable way to respond is to respond to a spike in traffic while maintaining QoS is to rate-limit \nby inbound edge!\n\nUltimately, what we have here is a networking problem, that has to be solved with networking \nsolutions, not a costing problem, which can be solved with payment. I can only assume that the \ndesire to add a cost to onion messages ultimately stems from a desire to ensure every possible \navenue for value extraction is given to routing nodes, but I think that desire is misplaced in this \ncase - the cost of bandwidth is diminutive compared to other costs of routing node operation, \nespecially when you consider sensible rate-limits as proposed in Bastien's email.\n\nIndeed, if anyone were proposing rate-limits which would allow anything close to enough bandwidth \nusage to cause \"lightning is turning into Tor and has Tor's problems\" to be a legitimate concern I'd \ntotally agree we should charge for its use. But no one is, nor has anyone ever seriously, to my \nknowledge, proposed such a thing. If lightning messages get deployed and start eating up even single \nMbps's on a consistent basis on nodes, we can totally revisit this, its not like we are shutting the \ndoor to any possible costing system if it becomes necessary, but rate-limiting has to happen either \nway, so we should start there and see if we need costing, not jump to costing on day one, hampering \nutility.\n\nMatt\n\nOn 6/29/22 8:22 PM, Olaoluwa Osuntokun wrote:\n> Hi t-bast,\n> \n> Happy to see this finally written up! With this, we have two classes of\n> proposals for rate limiting onion messaging:\n> \n>  \u00a0 1. Back propagation based rate limiting as described here.\n> \n>  \u00a0 2. Allowing nodes to express a per-message cost for their forwarding\n>  \u00a0 services, which is described here [1].\n> \n> I still need to digest everything proposed here, but personally I'm more\n> optimistic about the 2nd category than the 1st.\n> \n> One issue I see w/ the first category is that a single party can flood the\n> network and cause nodes to trigger their rate limits, which then affects the\n> usability of the onion messages for all other well-behaving parties. An\n> example, this might mean I can't fetch invoices, give up after a period of\n> time (how long?), then result to a direct connection (perceived payment\n> latency accumulated along the way).\n> \n> With the 2nd route, if an attacker floods the network, they need to directly\n> pay for the forwarding usage themselves, though they may also directly cause\n> nodes to adjust their forwarding rate accordingly. However in this case, the\n> attacker has incurred a concrete cost, and even if the rates rise, then\n> those that really need the service (notifying an LSP that a user is online\n> or w/e) can continue to pay that new rate. In other words, by _pricing_ the\n> resource utilization, demand preferences can be exchanged, leading to more\n> efficient long term resource allocation.\n> \n> W.r.t this topic, one event that imo is worth pointing out is that a very\n> popular onion routing system, Tor, has been facing a severe DDoS attack that\n> has lasted weeks, and isn't yet fully resolved [2]. The on going flooding\n> attack on Tor has actually started to affect LN (iirc over half of all\n> public routing nodes w/ an advertised address are tor-only), and other\n> related systems like Umbrel that 100% rely on tor for networking traversal.\n> Funnily enough, Tor developers have actually suggested adding some PoW to\n> attempt to mitigate DDoS attacks [3]. In that same post they throw around\n> the idea of using anonymous tokens to allow nodes to give them to \"good\"\n> clients, which is pretty similar to my lofty Forwarding Pass idea as relates\n> to onion messaging, and also general HTLC jamming mitigation.\n> \n> In summary, we're not the first to attempt to tackle the problem of rate\n> limiting relayed message spam in an anonymous/pseudonymous network, and we\n> can probably learn a lot from what is and isn't working w.r.t how Tor\n> handles things. As you note near the end of your post, this might just be\n> the first avenue in a long line of research to best figure out how to handle\n> the spam concerns introduced by onion messaging. From my PoV, it still seems\n> to be an open question if the same network can be _both_ a reliable\n> micro-payment system _and_ also a reliable arbitrary message transport\n> layer. I guess only time will tell...\n> \n>  > The `shared_secret_hash` field contains a BIP 340 tagged hash\n> \n> Any reason to use the tagged hash here vs just a plain ol HMAC? Under the\n> hood, they have a pretty similar construction [4].\n> \n> [1]: https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-February/003498.html \n> <https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-February/003498.html>\n> [2]: https://status.torproject.org/issues/2022-06-09-network-ddos/ \n> <https://status.torproject.org/issues/2022-06-09-network-ddos/>\n> [3]: https://blog.torproject.org/stop-the-onion-denial/ \n> <https://blog.torproject.org/stop-the-onion-denial/>\n> [4]: https://datatracker.ietf.org/doc/html/rfc2104 <https://datatracker.ietf.org/doc/html/rfc2104>\n> \n> -- Laolu\n> \n> \n> \n> On Wed, Jun 29, 2022 at 1:28 AM Bastien TEINTURIER <bastien at acinq.fr <mailto:bastien at acinq.fr>> wrote:\n> \n>     During the recent Oakland Dev Summit, some lightning engineers got together to discuss DoS\n>     protection for onion messages. Rusty proposed a very simple rate-limiting scheme that\n>     statistically propagates back to the correct sender, which we describe in details below.\n> \n>     You can also read this in gist format if that works better for you [1].\n> \n>     Nodes apply per-peer rate limits on _incoming_ onion messages that should be relayed (e.g.\n>     N/seconds with some burst tolerance). It is recommended to allow more onion messages from\n>     peers with whom you have channels, for example 10/seconds when you have a channel and 1/second\n>     when you don't.\n> \n>     When relaying an onion message, nodes keep track of where it came from (by using the `node_id` of\n>     the peer who sent that message). Nodes only need the last such `node_id` per outgoing connection,\n>     which ensures the memory footprint is very small. Also, this data doesn't need to be persisted.\n> \n>     Let's walk through an example to illustrate this mechanism:\n> \n>     * Bob receives an onion message from Alice that should be relayed to Carol\n>     * After relaying that message, Bob stores Alice's `node_id` in its per-connection state with Carol\n>     * Bob receives an onion message from Eve that should be relayed to Carol\n>     * After relaying that message, Bob replaces Alice's `node_id` with Eve's `node_id` in its\n>     per-connection state with Carol\n>     * Bob receives an onion message from Alice that should be relayed to Dave\n>     * After relaying that message, Bob stores Alice's `node_id` in its per-connection state with Dave\n>     * ...\n> \n>     We introduce a new message that will be sent when dropping an incoming onion message because it\n>     reached rate limits:\n> \n>     1. type: 515 (`onion_message_drop`)\n>     2. data:\n>         * [`rate_limited`:`u8`]\n>         * [`shared_secret_hash`:`32*byte`]\n> \n>     Whenever an incoming onion message reaches the rate limit, the receiver sends `onion_message_drop`\n>     to the sender. The sender looks at its per-connection state to find where the message was coming\n>     from and relays `onion_message_drop` to the last sender, halving their rate limits with that peer.\n> \n>     If the sender doesn't overflow the rate limit again, the receiver should double the rate limit\n>     after 30 seconds, until it reaches the default rate limit again.\n> \n>     The flow will look like:\n> \n>     Alice                      Bob                      Carol\n>        |                         |                         |\n>        |      onion_message      |                         |\n>        |------------------------>|                         |\n>        |                         |      onion_message      |\n>        |                         |------------------------>|\n>        |                         |    onion_message_drop   |\n>        |                         |<------------------------|\n>        |    onion_message_drop   |                         |\n>        |<------------------------|                         |\n> \n>     The `shared_secret_hash` field contains a BIP 340 tagged hash of the Sphinx shared secret of the\n>     rate limiting peer (in the example above, Carol):\n> \n>     * `shared_secret_hash = SHA256(SHA256(\"onion_message_drop\") || SHA256(\"onion_message_drop\") || sphinx_shared_secret)`\n> \n>     This value is known by the node that created the onion message: if `onion_message_drop` propagates\n>     all the way back to them, it lets them know which part of the route is congested, allowing them\n>     to retry through a different path.\n> \n>     Whenever there is some latency between nodes and many onion messages, `onion_message_drop` may\n>     be relayed to the incorrect incoming peer (since we only store the `node_id` of the _last_ incoming\n>     peer in our outgoing connection state). The following example highlights this:\n> \n>       Eve                       Bob                      Carol\n>        |      onion_message      |                         |\n>        |------------------------>|      onion_message      |\n>        |      onion_message      |------------------------>|\n>        |------------------------>|      onion_message      |\n>        |      onion_message      |------------------------>|\n>        |------------------------>|      onion_message      |\n>                                  |------------------------>|\n>     Alice                       |    onion_message_drop   |\n>        |      onion_message      |                    +----|\n>        |------------------------>|      onion_message |    |\n>        |                         |--------------------|--->|\n>        |                         |                    |    |\n>        |                         |                    |    |\n>        |                         |                    |    |\n>        |    onion_message_drop   |<-------------------+    |\n>        |<------------------------|                         |\n> \n>     In this example, Eve is spamming but `onion_message_drop` is propagated back to Alice instead.\n>     However, this scheme will _statistically_ penalize the right incoming peer (with a probability\n>     depending on the volume of onion messages that the spamming peer is generating compared to the\n>     volume of legitimate onion messages).\n> \n>     It is an interesting research problem to find formulas for those probabilities to evaluate how\n>     efficient this will be against various types of spam. We hope researchers on this list will be\n>     interested in looking into it and will come up with a good model to evaluate that scheme.\n> \n>     To increase the accuracy of attributing `onion_message_drop`, more data could be stored in the\n>     future if it becomes necessary. We need more research to quantify how much accuracy would be\n>     gained by storing more data and making the protocol more complex.\n> \n>     Cheers,\n>     Bastien\n> \n>     [1]https://gist.github.com/t-bast/e37ee9249d9825e51d260335c94f0fcf  <https://gist.github.com/t-bast/e37ee9249d9825e51d260335c94f0fcf>\n> \n>     _______________________________________________\n>     Lightning-dev mailing list\n>     Lightning-dev at lists.linuxfoundation.org <mailto:Lightning-dev at lists.linuxfoundation.org>\n>     https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>     <https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev>\n> \n> \n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2022-06-30T07:46:59",
                "message_text_only": "Thanks for your inputs!\n\nOne issue I see w/ the first category is that a single party can flood the\n> network and cause nodes to trigger their rate limits, which then affects\n> the\n> usability of the onion messages for all other well-behaving parties.\n>\n\nBut that's exactly what this proposal addresses? That single party can\nonly flood for a very small amount of time before being rate-limited for\na while, so it cannot disrupt other parties that much (to be properly\nquantified by research, but it seems quite intuitive).\n\nW.r.t this topic, one event that imo is worth pointing out is that a very\n> popular onion routing system, Tor, has been facing a severe DDoS attack\n> that\n> has lasted weeks, and isn't yet fully resolved [2].\n>\n\nI don't think we can compare lightning to Tor, the only common design\nis that there is onion encryption, but the networking parts are very\ndifferent (and the attack vectors on Tor are mostly on components that\ndon't exist in lightning).\n\nI can only assume that the desire to add a cost to onion messages\n\nultimately stems from a desire to ensure every possible avenue for\n\nvalue extraction is given to routing nodes\n>\n\nI think that Matt is pointing out the main distinction here between the\ntwo proposals. While I can sympathize with that goal, I agree with\nMatt that it's probably misplaced here (and that proposal is orders of\nmagnitude more complex than a simple rate-limit).\n\nI do think any spec for this shouldn't make any recommendations about\n> willingness to relay onion\n> messages for anonymous no-channel third parties, if anything deliberately\n> staying mum on it and\n> allowing nodes to adapt policy (and probably rate-limit no-channel\n> third-parties before they rate\n> limit any peer they have a channel with). Ultimately, we have to assume\n> that nodes will attempt to\n> send onion messages by routing through the existing channel graph, so\n> there's little reason to worry\n> too much about ensuring ability to relay for anonymous parties.\n>\n\nSounds good, I'll do that when actually specifying this in a bolt.\n\n\n>  Better yet, as Val points out, requiring a channel to relay onion\n> messages puts a very real,\n\nnontrivial (in a world of msats) cost to getting an onion messaging\n> channel.\n\n\nYes, this is the main component that does efficiently protect against DoS.\nAt some point if a peer keeps exceeding their onion rate-limits and isn't\nproviding you with enough HTLCs to relay, you can force-close on them,\nwhich makes that cost real and stops the spamming attempts for a while.\n\nCheers,\nBastien\n\nLe jeu. 30 juin 2022 \u00e0 04:19, Matt Corallo <lf-lists at mattcorallo.com> a\n\u00e9crit :\n\n> Thanks Bastien for writing this up! This is a pretty trivial and\n> straightforward way to rate-limit\n> onion messages in a way that allows legitimate users to continue using the\n> system in spite of some\n> bad actors trying (and failing, due to being rate-limited) to DoS the\n> network.\n>\n> I do think any spec for this shouldn't make any recommendations about\n> willingness to relay onion\n> messages for anonymous no-channel third parties, if anything deliberately\n> staying mum on it and\n> allowing nodes to adapt policy (and probably rate-limit no-channel\n> third-parties before they rate\n> limit any peer they have a channel with). Ultimately, we have to assume\n> that nodes will attempt to\n> send onion messages by routing through the existing channel graph, so\n> there's little reason to worry\n> too much about ensuring ability to relay for anonymous parties.\n>\n> Better yet, as Val points out, requiring a channel to relay onion messages\n> puts a very real,\n> nontrivial (in a world of msats) cost to getting an onion messaging\n> channel. Better yet, with\n> backpressure ability to DoS onion message links isn't denominated in\n> number of messages, but instead\n> in number of channels you are able to create, making the backpressure\n> system equivalent to today's\n> HTLC DoS considerations, whereas explicit payment allows an attacker to\n> pay much less to break the\n> system.\n>\n> As for the proposal to charge for onion messages, I'm still not at all\n> sure where its coming from.\n> It seems to flow from a classic \"have a hammer (a system to make\n> micropayments for things), better\n> turn this problem into a nail (by making users pay for it)\" approach, but\n> it doesn't actually solve\n> the problem at hand.\n>\n> Even if you charge for onion messages, users may legitimately want to send\n> a bunch of payments in\n> bulk, and trivially overflow a home or Tor nodes' bandwidth. The only\n> response to that, whether its\n> a DoS attack or a legitimate user, is to rate-limit, and to rate-limit in\n> a way that tells the user\n> sending the messages to back off! Sure, you could do that by failing onion\n> messages with an error\n> that updates the fee you charge, but you're ultimately doing a poor-man's\n> (or, I suppose,\n> rich-man's) version of what Bastien proposes, not adding some fundamental\n> difference.\n>\n> Ultimately, paying suffers from the standard PoW-for-spam issue - you\n> cannot assign a reasonable\n> cost that an attacker cares about without impacting the system's usability\n> due to said cost. Indeed,\n> making it expensive enough to mount a months-long DDoS without impacting\n> legitimate users be pretty\n> easy - at 1msat per relay of a 1366 byte onion message you can only\n> saturate an average home users'\n> 30Mbps connection for 30 minutes before you rack up a dollar in costs, but\n> if your concern is\n> whether someone can reasonably trivially take out the network for minutes\n> at a time to make it have\n> perceptibly high failure rates, no reasonable cost scheme will work. Quite\n> the opposite - the only\n> reasonable way to respond is to respond to a spike in traffic while\n> maintaining QoS is to rate-limit\n> by inbound edge!\n>\n> Ultimately, what we have here is a networking problem, that has to be\n> solved with networking\n> solutions, not a costing problem, which can be solved with payment. I can\n> only assume that the\n> desire to add a cost to onion messages ultimately stems from a desire to\n> ensure every possible\n> avenue for value extraction is given to routing nodes, but I think that\n> desire is misplaced in this\n> case - the cost of bandwidth is diminutive compared to other costs of\n> routing node operation,\n> especially when you consider sensible rate-limits as proposed in Bastien's\n> email.\n>\n> Indeed, if anyone were proposing rate-limits which would allow anything\n> close to enough bandwidth\n> usage to cause \"lightning is turning into Tor and has Tor's problems\" to\n> be a legitimate concern I'd\n> totally agree we should charge for its use. But no one is, nor has anyone\n> ever seriously, to my\n> knowledge, proposed such a thing. If lightning messages get deployed and\n> start eating up even single\n> Mbps's on a consistent basis on nodes, we can totally revisit this, its\n> not like we are shutting the\n> door to any possible costing system if it becomes necessary, but\n> rate-limiting has to happen either\n> way, so we should start there and see if we need costing, not jump to\n> costing on day one, hampering\n> utility.\n>\n> Matt\n>\n> On 6/29/22 8:22 PM, Olaoluwa Osuntokun wrote:\n> > Hi t-bast,\n> >\n> > Happy to see this finally written up! With this, we have two classes of\n> > proposals for rate limiting onion messaging:\n> >\n> >    1. Back propagation based rate limiting as described here.\n> >\n> >    2. Allowing nodes to express a per-message cost for their forwarding\n> >    services, which is described here [1].\n> >\n> > I still need to digest everything proposed here, but personally I'm more\n> > optimistic about the 2nd category than the 1st.\n> >\n> > One issue I see w/ the first category is that a single party can flood\n> the\n> > network and cause nodes to trigger their rate limits, which then affects\n> the\n> > usability of the onion messages for all other well-behaving parties. An\n> > example, this might mean I can't fetch invoices, give up after a period\n> of\n> > time (how long?), then result to a direct connection (perceived payment\n> > latency accumulated along the way).\n> >\n> > With the 2nd route, if an attacker floods the network, they need to\n> directly\n> > pay for the forwarding usage themselves, though they may also directly\n> cause\n> > nodes to adjust their forwarding rate accordingly. However in this case,\n> the\n> > attacker has incurred a concrete cost, and even if the rates rise, then\n> > those that really need the service (notifying an LSP that a user is\n> online\n> > or w/e) can continue to pay that new rate. In other words, by _pricing_\n> the\n> > resource utilization, demand preferences can be exchanged, leading to\n> more\n> > efficient long term resource allocation.\n> >\n> > W.r.t this topic, one event that imo is worth pointing out is that a very\n> > popular onion routing system, Tor, has been facing a severe DDoS attack\n> that\n> > has lasted weeks, and isn't yet fully resolved [2]. The on going flooding\n> > attack on Tor has actually started to affect LN (iirc over half of all\n> > public routing nodes w/ an advertised address are tor-only), and other\n> > related systems like Umbrel that 100% rely on tor for networking\n> traversal.\n> > Funnily enough, Tor developers have actually suggested adding some PoW to\n> > attempt to mitigate DDoS attacks [3]. In that same post they throw around\n> > the idea of using anonymous tokens to allow nodes to give them to \"good\"\n> > clients, which is pretty similar to my lofty Forwarding Pass idea as\n> relates\n> > to onion messaging, and also general HTLC jamming mitigation.\n> >\n> > In summary, we're not the first to attempt to tackle the problem of rate\n> > limiting relayed message spam in an anonymous/pseudonymous network, and\n> we\n> > can probably learn a lot from what is and isn't working w.r.t how Tor\n> > handles things. As you note near the end of your post, this might just be\n> > the first avenue in a long line of research to best figure out how to\n> handle\n> > the spam concerns introduced by onion messaging. From my PoV, it still\n> seems\n> > to be an open question if the same network can be _both_ a reliable\n> > micro-payment system _and_ also a reliable arbitrary message transport\n> > layer. I guess only time will tell...\n> >\n> >  > The `shared_secret_hash` field contains a BIP 340 tagged hash\n> >\n> > Any reason to use the tagged hash here vs just a plain ol HMAC? Under the\n> > hood, they have a pretty similar construction [4].\n> >\n> > [1]:\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-February/003498.html\n> > <\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-February/003498.html\n> >\n> > [2]: https://status.torproject.org/issues/2022-06-09-network-ddos/\n> > <https://status.torproject.org/issues/2022-06-09-network-ddos/>\n> > [3]: https://blog.torproject.org/stop-the-onion-denial/\n> > <https://blog.torproject.org/stop-the-onion-denial/>\n> > [4]: https://datatracker.ietf.org/doc/html/rfc2104 <\n> https://datatracker.ietf.org/doc/html/rfc2104>\n> >\n> > -- Laolu\n> >\n> >\n> >\n> > On Wed, Jun 29, 2022 at 1:28 AM Bastien TEINTURIER <bastien at acinq.fr\n> <mailto:bastien at acinq.fr>> wrote:\n> >\n> >     During the recent Oakland Dev Summit, some lightning engineers got\n> together to discuss DoS\n> >     protection for onion messages. Rusty proposed a very simple\n> rate-limiting scheme that\n> >     statistically propagates back to the correct sender, which we\n> describe in details below.\n> >\n> >     You can also read this in gist format if that works better for you\n> [1].\n> >\n> >     Nodes apply per-peer rate limits on _incoming_ onion messages that\n> should be relayed (e.g.\n> >     N/seconds with some burst tolerance). It is recommended to allow\n> more onion messages from\n> >     peers with whom you have channels, for example 10/seconds when you\n> have a channel and 1/second\n> >     when you don't.\n> >\n> >     When relaying an onion message, nodes keep track of where it came\n> from (by using the `node_id` of\n> >     the peer who sent that message). Nodes only need the last such\n> `node_id` per outgoing connection,\n> >     which ensures the memory footprint is very small. Also, this data\n> doesn't need to be persisted.\n> >\n> >     Let's walk through an example to illustrate this mechanism:\n> >\n> >     * Bob receives an onion message from Alice that should be relayed to\n> Carol\n> >     * After relaying that message, Bob stores Alice's `node_id` in its\n> per-connection state with Carol\n> >     * Bob receives an onion message from Eve that should be relayed to\n> Carol\n> >     * After relaying that message, Bob replaces Alice's `node_id` with\n> Eve's `node_id` in its\n> >     per-connection state with Carol\n> >     * Bob receives an onion message from Alice that should be relayed to\n> Dave\n> >     * After relaying that message, Bob stores Alice's `node_id` in its\n> per-connection state with Dave\n> >     * ...\n> >\n> >     We introduce a new message that will be sent when dropping an\n> incoming onion message because it\n> >     reached rate limits:\n> >\n> >     1. type: 515 (`onion_message_drop`)\n> >     2. data:\n> >         * [`rate_limited`:`u8`]\n> >         * [`shared_secret_hash`:`32*byte`]\n> >\n> >     Whenever an incoming onion message reaches the rate limit, the\n> receiver sends `onion_message_drop`\n> >     to the sender. The sender looks at its per-connection state to find\n> where the message was coming\n> >     from and relays `onion_message_drop` to the last sender, halving\n> their rate limits with that peer.\n> >\n> >     If the sender doesn't overflow the rate limit again, the receiver\n> should double the rate limit\n> >     after 30 seconds, until it reaches the default rate limit again.\n> >\n> >     The flow will look like:\n> >\n> >     Alice                      Bob                      Carol\n> >        |                         |                         |\n> >        |      onion_message      |                         |\n> >        |------------------------>|                         |\n> >        |                         |      onion_message      |\n> >        |                         |------------------------>|\n> >        |                         |    onion_message_drop   |\n> >        |                         |<------------------------|\n> >        |    onion_message_drop   |                         |\n> >        |<------------------------|                         |\n> >\n> >     The `shared_secret_hash` field contains a BIP 340 tagged hash of the\n> Sphinx shared secret of the\n> >     rate limiting peer (in the example above, Carol):\n> >\n> >     * `shared_secret_hash = SHA256(SHA256(\"onion_message_drop\") ||\n> SHA256(\"onion_message_drop\") || sphinx_shared_secret)`\n> >\n> >     This value is known by the node that created the onion message: if\n> `onion_message_drop` propagates\n> >     all the way back to them, it lets them know which part of the route\n> is congested, allowing them\n> >     to retry through a different path.\n> >\n> >     Whenever there is some latency between nodes and many onion\n> messages, `onion_message_drop` may\n> >     be relayed to the incorrect incoming peer (since we only store the\n> `node_id` of the _last_ incoming\n> >     peer in our outgoing connection state). The following example\n> highlights this:\n> >\n> >       Eve                       Bob                      Carol\n> >        |      onion_message      |                         |\n> >        |------------------------>|      onion_message      |\n> >        |      onion_message      |------------------------>|\n> >        |------------------------>|      onion_message      |\n> >        |      onion_message      |------------------------>|\n> >        |------------------------>|      onion_message      |\n> >                                  |------------------------>|\n> >     Alice                       |    onion_message_drop   |\n> >        |      onion_message      |                    +----|\n> >        |------------------------>|      onion_message |    |\n> >        |                         |--------------------|--->|\n> >        |                         |                    |    |\n> >        |                         |                    |    |\n> >        |                         |                    |    |\n> >        |    onion_message_drop   |<-------------------+    |\n> >        |<------------------------|                         |\n> >\n> >     In this example, Eve is spamming but `onion_message_drop` is\n> propagated back to Alice instead.\n> >     However, this scheme will _statistically_ penalize the right\n> incoming peer (with a probability\n> >     depending on the volume of onion messages that the spamming peer is\n> generating compared to the\n> >     volume of legitimate onion messages).\n> >\n> >     It is an interesting research problem to find formulas for those\n> probabilities to evaluate how\n> >     efficient this will be against various types of spam. We hope\n> researchers on this list will be\n> >     interested in looking into it and will come up with a good model to\n> evaluate that scheme.\n> >\n> >     To increase the accuracy of attributing `onion_message_drop`, more\n> data could be stored in the\n> >     future if it becomes necessary. We need more research to quantify\n> how much accuracy would be\n> >     gained by storing more data and making the protocol more complex.\n> >\n> >     Cheers,\n> >     Bastien\n> >\n> >     [1]https://gist.github.com/t-bast/e37ee9249d9825e51d260335c94f0fcf\n> <https://gist.github.com/t-bast/e37ee9249d9825e51d260335c94f0fcf>\n> >\n> >     _______________________________________________\n> >     Lightning-dev mailing list\n> >     Lightning-dev at lists.linuxfoundation.org <mailto:\n> Lightning-dev at lists.linuxfoundation.org>\n> >     https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n> >     <https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev>\n> >\n> >\n> > _______________________________________________\n> > Lightning-dev mailing list\n> > Lightning-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220630/d5d6c5d5/attachment-0001.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2022-06-30T10:15:18",
                "message_text_only": "Thanks Bastien for writing up the proposal, it is simple but effective I\nthink.\n\n>> One issue I see w/ the first category is that a single party can flood the\n>> network and cause nodes to trigger their rate limits, which then affects\n>> the\n>> usability of the onion messages for all other well-behaving parties.\n>>\n>\n> But that's exactly what this proposal addresses? That single party can\n> only flood for a very small amount of time before being rate-limited for\n> a while, so it cannot disrupt other parties that much (to be properly\n> quantified by research, but it seems quite intuitive).\n\nIndeed, it creates a tiny bubble (1-2 hops) in which an attacker can\nindeed trigger the rate-limiter, but beyond which its messages simply\nget dropped. In this respect it is very similar to the staggered\ngossip, in which a node may send updates at an arbitrary rate, but since\neach node will locally buffer these changes and aggregate them, the\neffective rate that is forwarded/broadcast is such that it doesn't\noverwhelm the network (parametrization and network size apart ^^).\n\nThis is also an argument for not allowing onion messages over\nnon-channel connections, since otherwise an attacker could arbitrarily\nextend their bubble to encompass every channel in the network, and can\nsybil its way to covering the entire network (depending on rate limiter,\nand their parameters and timing the attacker bubble may extend to more\nthan a single hop).\n\nGoing back a step it is also questionable whether non-channel OM\nforwarding is usable at all, since nodes usually do not know about the\nexistence of these connections at all (not gossiped). I'd therefore not\nallow non-channel forwarding at all, with the small exception of some\nlocal applications, where local knowledge is required, but in that case\nthe OM should signal this clearly to the forwarding node as well or rely\non direct messaging with the peer (pre-channel negotiation, etc).\n\n>> W.r.t this topic, one event that imo is worth pointing out is that a very\n>> popular onion routing system, Tor, has been facing a severe DDoS attack\n>> that\n>> has lasted weeks, and isn't yet fully resolved [2].\n>>\n>\n> I don't think we can compare lightning to Tor, the only common design\n> is that there is onion encryption, but the networking parts are very\n> different (and the attack vectors on Tor are mostly on components that\n> don't exist in lightning).\n\nIndeed, a major difference if we insist on there being a channel is that\nit is no longer easy to sybil the network, and there are no ways to just\nconnect to a node and send it data (which is pretty much the Tor circuit\nconstruction). So we can rely on the topology of the network to keep an\nattacker constrained in its local region of the network, and extending\nthe attacker's reach would require opening channel, i.e., wouldn't be\nfree.\n\nCheers,\nChristian"
            },
            {
                "author": "Matt Corallo",
                "date": "2022-06-30T16:47:32",
                "message_text_only": "One further note, I don\u2019t think it makes sense to specify exactly what the rate-limiting behavior is here - if a node wants to do something other than the general \u201ckeep track of last forwarded message source and rate limit them\u201d logic they should be free to, there\u2019s no reason that needs to be normative (and there may be some reason to think it\u2019s vulnerable to a node deliberately causing one inbound edge to be limited even though they\u2019re spamming a different one).\n\n> On Jun 29, 2022, at 04:28, Bastien TEINTURIER <bastien at acinq.fr> wrote:\n> \n> \ufeff\n> During the recent Oakland Dev Summit, some lightning engineers got together to discuss DoS\n> protection for onion messages. Rusty proposed a very simple rate-limiting scheme that\n> statistically propagates back to the correct sender, which we describe in details below.\n> You can also read this in gist format if that works better for you [1].\n> Nodes apply per-peer rate limits on _incoming_ onion messages that should be relayed (e.g.\n> N/seconds with some burst tolerance). It is recommended to allow more onion messages from\n> peers with whom you have channels, for example 10/seconds when you have a channel and 1/second\n> when you don't.\n> \n> When relaying an onion message, nodes keep track of where it came from (by using the `node_id` of\n> the peer who sent that message). Nodes only need the last such `node_id` per outgoing connection,\n> which ensures the memory footprint is very small. Also, this data doesn't need to be persisted.\n> \n> Let's walk through an example to illustrate this mechanism:\n> \n> * Bob receives an onion message from Alice that should be relayed to Carol\n> * After relaying that message, Bob stores Alice's `node_id` in its per-connection state with Carol\n> * Bob receives an onion message from Eve that should be relayed to Carol\n> * After relaying that message, Bob replaces Alice's `node_id` with Eve's `node_id` in its\n> per-connection state with Carol\n> * Bob receives an onion message from Alice that should be relayed to Dave\n> * After relaying that message, Bob stores Alice's `node_id` in its per-connection state with Dave\n> * ...\n> \n> We introduce a new message that will be sent when dropping an incoming onion message because it\n> reached rate limits:\n> \n> 1. type: 515 (`onion_message_drop`)\n> 2. data:\n>    * [`rate_limited`:`u8`]\n>    * [`shared_secret_hash`:`32*byte`]\n> \n> Whenever an incoming onion message reaches the rate limit, the receiver sends `onion_message_drop`\n> to the sender. The sender looks at its per-connection state to find where the message was coming\n> from and relays `onion_message_drop` to the last sender, halving their rate limits with that peer.\n> \n> If the sender doesn't overflow the rate limit again, the receiver should double the rate limit\n> after 30 seconds, until it reaches the default rate limit again.\n> \n> The flow will look like:\n> \n> Alice                      Bob                      Carol\n>   |                         |                         |\n>   |      onion_message      |                         |\n>   |------------------------>|                         |\n>   |                         |      onion_message      |\n>   |                         |------------------------>|\n>   |                         |    onion_message_drop   |\n>   |                         |<------------------------|\n>   |    onion_message_drop   |                         |\n>   |<------------------------|                         |\n> \n> The `shared_secret_hash` field contains a BIP 340 tagged hash of the Sphinx shared secret of the\n> rate limiting peer (in the example above, Carol):\n> \n> * `shared_secret_hash = SHA256(SHA256(\"onion_message_drop\") || SHA256(\"onion_message_drop\") || sphinx_shared_secret)`\n> \n> This value is known by the node that created the onion message: if `onion_message_drop` propagates\n> all the way back to them, it lets them know which part of the route is congested, allowing them\n> to retry through a different path.\n> \n> Whenever there is some latency between nodes and many onion messages, `onion_message_drop` may\n> be relayed to the incorrect incoming peer (since we only store the `node_id` of the _last_ incoming\n> peer in our outgoing connection state). The following example highlights this:\n> \n>  Eve                       Bob                      Carol\n>   |      onion_message      |                         |\n>   |------------------------>|      onion_message      |\n>   |      onion_message      |------------------------>|\n>   |------------------------>|      onion_message      |\n>   |      onion_message      |------------------------>|\n>   |------------------------>|      onion_message      |\n>                             |------------------------>|\n> Alice                       |    onion_message_drop   |\n>   |      onion_message      |                    +----|\n>   |------------------------>|      onion_message |    |\n>   |                         |--------------------|--->|\n>   |                         |                    |    |\n>   |                         |                    |    |\n>   |                         |                    |    |\n>   |    onion_message_drop   |<-------------------+    |\n>   |<------------------------|                         |\n> \n> In this example, Eve is spamming but `onion_message_drop` is propagated back to Alice instead.\n> However, this scheme will _statistically_ penalize the right incoming peer (with a probability\n> depending on the volume of onion messages that the spamming peer is generating compared to the\n> volume of legitimate onion messages).\n> \n> It is an interesting research problem to find formulas for those probabilities to evaluate how\n> efficient this will be against various types of spam. We hope researchers on this list will be\n> interested in looking into it and will come up with a good model to evaluate that scheme.\n> \n> To increase the accuracy of attributing `onion_message_drop`, more data could be stored in the\n> future if it becomes necessary. We need more research to quantify how much accuracy would be\n> gained by storing more data and making the protocol more complex.\n> Cheers,\n> Bastien\n> [1] https://gist.github.com/t-bast/e37ee9249d9825e51d260335c94f0fcf\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220630/7af661f9/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Onion messages rate-limiting",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Matt Corallo",
                "vwallace",
                "Bastien TEINTURIER",
                "Olaoluwa Osuntokun",
                "Christian Decker"
            ],
            "messages_count": 7,
            "total_messages_chars_count": 68348
        }
    },
    {
        "title": "[Lightning-dev] Gossip Propagation, Anti-spam, and Set Reconciliation",
        "thread_messages": [
            {
                "author": "Michael Folkson",
                "date": "2022-06-29T11:09:57",
                "message_text_only": "Thanks for this Alex.\n\nHere's a transcript of your recent presentation at Bitcoin++ on Minisketch and Lightning gossip:\n\nhttps://btctranscripts.com/bitcoinplusplus/2022/2022-06-07-alex-myers-minisketch-lightning-gossip/\n\nHaving followed Gleb's work on using Minisketch for Erlay in Bitcoin Core [0] for a while now I was especially interested in how the challenges of using Minisketch for Lightning gossip (node_announcement, channel_announcement, channel_update messages) would differ to the challenges of using Minisketch for transaction relay on the base layer.\n\nI guess one of the major differences is full nodes are trying to verify a block every 10 minutes (on average) and so there is a sense of urgency to get the transactions of the next block to be mined. With Lightning gossip unless you are planning to send a payment (or route a payment) across a certain route you are less concerned about learning about the current state of the network urgently. If a new channel pops up you might choose not to route through it regardless given its \"newness\" and its lack of track record of successfully routing payments. There are parts of the network you care less about (if they can't help you get to your regular destinations say) whereas with transaction relay you have to care about all transactions (paying a sufficient fee rate).\n\n\"The problem that Bitcoin faced with transaction relay was pretty similar but there are a few differences.For one, any time you introduce that short hash function that produces a 64 bit fingerprint you have to be concerned with collisions between hash functions. Someone could potentially take advantage of that and grind out a hash that would resolve to the same fingerprint.\"\n\nCould you elaborate on this? Why are hash collisions a concern for Lightning gossip and not for Erlay? Is it not a DoS vector for both?\n\nIt seems you are leaning towards per-peer sketches with inventory sets (like Erlay) rather than global sketches. This makes sense to me and seems to be moving in a direction where your peer connections are more stable as you are storing data on what your peer's understanding of the network is. There could even be centralized APIs which allow you to compare your current understanding of the network to the centralized service's understanding. (Of course we don't want to have to rely on centralized services or bake them into the protocol if you don't want to use them.) Erlay falls back to flooding if the set reconciliation algorithm doesn't work which I'm assuming you'll do with Lightning gossip.\n\nI was also surprised to hear that channel_update made up 97 percent of gossip messages. Isn't it recommended that you don't make too changes to your channel as it is likely to result in failed routed payments and being dropped as a routing node for future payments? It seems that this advice isn't being followed if there are so many channel_update messages being sent around. I almost wonder if Lightning implementations should include user prompts like \"Are you sure you want to update your channel given this may affect your routing success?\" :)\n\nThanks\nMichael\n\nP.S. Are we referring to \"routing nodes\" as \"forwarding nodes\" now? I've noticed \"forwarding nodes\" being used more recently on this list.\n\n[0]: https://github.com/bitcoin/bitcoin/pull/21515\n\n--\nMichael Folkson\nEmail: michaelfolkson at [protonmail.com](http://protonmail.com/)\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n\n------- Original Message -------\nOn Thursday, April 14th, 2022 at 22:00, Alex Myers <alex at endothermic.dev> wrote:\n\n> Hello lightning developers,\n>\n> I\u2019ve been investigating set reconciliation as a means to reduce bandwidth and redundancy of gossip message propagation. This builds on some earlier work from Rusty using the minisketch library [1]. The idea is that each node will build a sketch representing it\u2019s own gossip set. Alice\u2019s node will encode and transmit this sketch to Bob\u2019s node, where it will be merged with his own sketch, and the differences produced. These differences should ideally be exactly the latest missing gossip of both nodes. Due to size constraints, the set differences will necessarily be encoded, but Bob\u2019s node will be able to identify which gossip Alice is missing, and may then transmit exactly those messages.\n>\n> This process is relatively straightforward, with the caveat that the sets must otherwise match very closely (each sketch has a maximum capacity for differences.) The difficulty here is that each node and lightning implementation may have its own rules for gossip acceptance and propagation. Depending on their gossip partners, not all gossip may propagate to the entire network.\n>\n> Core-lightning implements rate limiting for incoming channel updates and node announcements. The default rate limit is 1 per day, with a burst of 4. I analyzed my node\u2019s gossip over a 14 day period, and found that, of all publicly broadcasting half-channels, 18% of them fell afoul of our spam-limiting rules at least once. [2]\n>\n> Picking several offending channel ids, and digging further, the majority of these appear to be flapping due to Tor or otherwise intermittent connections. Well connected nodes may be more susceptible to this due to more frequent routing attempts, and failures resulting in a returned channel update (which otherwise might not have been broadcast.)A slight relaxation of the rate limit resolves the majority of these cases.\n>\n> A smaller subset of channels broadcast frequent channel updates with minor adjustments to htlc_maximum_msat and fee_proportional_millionths parameters. These nodes appear to be power users, with many channels and large balances. I assume this is automated channel management at work.\n>\n> Core-Lightning has updated rate-limiting in the upcoming release to achieve a higher acceptance of incoming gossip, however, it seems that a broader discussion of rate limits may now be worthwhile. A few immediate ideas:\n>\n> - A common listing of current default rate limits across lightning network implementations.\n>\n> - Internal checks of RPC input to limit or warn of network propagation issues if certain rates are exceeded.\n>\n> - A commonly adopted rate-limit standard.\n>\n> My aim is a set reconciliation gossip type, which will use a common, simple heuristic to accept or reject a gossip message. (Think one channel update per block, or perhaps one per block_height << 5.) See my github for my current draft. [3] This solution allows tighter consensus, yet suffers from the same problem as original anti-spam measures \u2013 it remains somewhat arbitrary. I would like to start a conversation regarding gossip propagation, channel_update and node_announcement usage, and perhaps even bandwidth goals for syncing gossip in the future (how about a million channels?) This would aid in the development of gossip set reconciliation, but could also benefit current node connection and routing reliability more generally.\n>\n> Thanks,\n>\n> Alex\n>\n> [1] https://github.com/sipa/minisketch\n>\n> [2] https://github.com/endothermicdev/lnspammityspam/blob/main/sampleoutput.txt\n>\n> [3] https://github.com/endothermicdev/lightning-rfc/blob/gossip-minisketch/07-routing-gossip.md#set-reconciliation\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220629/bbffa050/attachment-0001.html>"
            },
            {
                "author": "Alex Myers",
                "date": "2022-06-29T18:07:27",
                "message_text_only": "Hi Michael,\n\nThanks for the transcript and the questions, especially those you asked in Gleb's original Erlay presentation.\n\nI tried to cover a lot of ground in only 30 minutes and the finer points may have suffered. The most significant difference in concern between bitcoin transaction relay and lightning gossip may be one of privacy: Source nodes of Bitcoin transactions have an interest in privacy (avoid trivially triangulating the source.) Lightning gossip is already signed by and linked to a node ID - the source is completely transparent by nature. The lack of a timing concern would allow for a global sketch where it would have been infeasible for Erlay (among other reasons such as DoS.)\n\n> Why are hash collisions a concern for Lightning gossip and not for Erlay? Is it not a DoS vector for both?\n\nIf lightning gossip were encoded for minisketch entries with the short_channel_id, it would create a unique fingerprint by default thanks to referencing the unique funding transaction on chain - no hashing required. This was Rusty's original concept and what I had been proceeding with. However, given the ongoing privacy discussion and desire to eventually decouple lightning channels from their layer one funding transaction (gossip v2), I think we should prepare for a future in which channels are not explicitly linked to a SCID. That means hashing just as in Erlay and the same DoS vector would be present. Salting with a per-peer shared secret works here, but the solution is driven back toward inventory sets.\n\n> It seems you are leaning towards per-peer sketches with inventory sets (like Erlay) rather than global sketches.\n\n\u200b\nYes. There are pros and cons to each method, but most critically, this would be compatible with eventual removal of the SCID.\n\n> Erlay falls back to flooding if the set reconciliation algorithm doesn't work which I'm assuming you'll do with Lightning gossip.\n\nFallback will take some consideration (Erlay's bisect is an elegant feature), but yes, flooding is still the ultimate fallback.\n\n> I was also surprised to hear that channel_update made up 97 percent of gossip messages. Isn't it recommended that you don't make too changes to your channel as it is likely to result in failed routed payments and being dropped as a routing node for future payments? It seems that this advice isn't being followed if there are so many channel_update messages being sent around. I almost wonder if Lightning implementations should include user prompts like \"Are you sure you want to update your channel given this may affect your routing success?\" :)\n\nRunning the numbers, I currently see 15,761 public nodes on the network and 148,295 half channels. Those each need refreshed gossip every two weeks. By default that would result in 90% channel updates. That we're seeing roughly three times as many channel updates vs node announcements compared to what's strictly required is maybe not that surprising. I agree, there would be a benefit to nodes taking a more active role in tracking calls to broadcast gossip.\n\nThanks,\nAlex\n\n------- Original Message -------\nOn Wednesday, June 29th, 2022 at 6:09 AM, Michael Folkson <michaelfolkson at protonmail.com> wrote:\n\n> Thanks for this Alex.\n>\n> Here's a transcript of your recent presentation at Bitcoin++ on Minisketch and Lightning gossip:\n>\n> https://btctranscripts.com/bitcoinplusplus/2022/2022-06-07-alex-myers-minisketch-lightning-gossip/\n>\n> Having followed Gleb's work on using Minisketch for Erlay in Bitcoin Core [0] for a while now I was especially interested in how the challenges of using Minisketch for Lightning gossip (node_announcement, channel_announcement, channel_update messages) would differ to the challenges of using Minisketch for transaction relay on the base layer.\n>\n> I guess one of the major differences is full nodes are trying to verify a block every 10 minutes (on average) and so there is a sense of urgency to get the transactions of the next block to be mined. With Lightning gossip unless you are planning to send a payment (or route a payment) across a certain route you are less concerned about learning about the current state of the network urgently. If a new channel pops up you might choose not to route through it regardless given its \"newness\" and its lack of track record of successfully routing payments. There are parts of the network you care less about (if they can't help you get to your regular destinations say) whereas with transaction relay you have to care about all transactions (paying a sufficient fee rate).\n>\n> \"The problem that Bitcoin faced with transaction relay was pretty similar but there are a few differences.For one, any time you introduce that short hash function that produces a 64 bit fingerprint you have to be concerned with collisions between hash functions. Someone could potentially take advantage of that and grind out a hash that would resolve to the same fingerprint.\"\n>\n> Could you elaborate on this? Why are hash collisions a concern for Lightning gossip and not for Erlay? Is it not a DoS vector for both?\n>\n> It seems you are leaning towards per-peer sketches with inventory sets (like Erlay) rather than global sketches. This makes sense to me and seems to be moving in a direction where your peer connections are more stable as you are storing data on what your peer's understanding of the network is. There could even be centralized APIs which allow you to compare your current understanding of the network to the centralized service's understanding. (Of course we don't want to have to rely on centralized services or bake them into the protocol if you don't want to use them.) Erlay falls back to flooding if the set reconciliation algorithm doesn't work which I'm assuming you'll do with Lightning gossip.\n>\n> I was also surprised to hear that channel_update made up 97 percent of gossip messages. Isn't it recommended that you don't make too changes to your channel as it is likely to result in failed routed payments and being dropped as a routing node for future payments? It seems that this advice isn't being followed if there are so many channel_update messages being sent around. I almost wonder if Lightning implementations should include user prompts like \"Are you sure you want to update your channel given this may affect your routing success?\" :)\n>\n> Thanks\n> Michael\n>\n> P.S. Are we referring to \"routing nodes\" as \"forwarding nodes\" now? I've noticed \"forwarding nodes\" being used more recently on this list.\n>\n> [0]: https://github.com/bitcoin/bitcoin/pull/21515\n>\n> --\n> Michael Folkson\n> Email: michaelfolkson at [protonmail.com](http://protonmail.com/)\n> Keybase: michaelfolkson\n> PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n>\n> ------- Original Message -------\n> On Thursday, April 14th, 2022 at 22:00, Alex Myers <alex at endothermic.dev> wrote:\n>\n>> Hello lightning developers,\n>>\n>> I\u2019ve been investigating set reconciliation as a means to reduce bandwidth and redundancy of gossip message propagation. This builds on some earlier work from Rusty using the minisketch library [1]. The idea is that each node will build a sketch representing it\u2019s own gossip set. Alice\u2019s node will encode and transmit this sketch to Bob\u2019s node, where it will be merged with his own sketch, and the differences produced. These differences should ideally be exactly the latest missing gossip of both nodes. Due to size constraints, the set differences will necessarily be encoded, but Bob\u2019s node will be able to identify which gossip Alice is missing, and may then transmit exactly those messages.\n>>\n>> This process is relatively straightforward, with the caveat that the sets must otherwise match very closely (each sketch has a maximum capacity for differences.) The difficulty here is that each node and lightning implementation may have its own rules for gossip acceptance and propagation. Depending on their gossip partners, not all gossip may propagate to the entire network.\n>>\n>> Core-lightning implements rate limiting for incoming channel updates and node announcements. The default rate limit is 1 per day, with a burst of 4. I analyzed my node\u2019s gossip over a 14 day period, and found that, of all publicly broadcasting half-channels, 18% of them fell afoul of our spam-limiting rules at least once. [2]\n>>\n>> Picking several offending channel ids, and digging further, the majority of these appear to be flapping due to Tor or otherwise intermittent connections. Well connected nodes may be more susceptible to this due to more frequent routing attempts, and failures resulting in a returned channel update (which otherwise might not have been broadcast.)A slight relaxation of the rate limit resolves the majority of these cases.\n>>\n>> A smaller subset of channels broadcast frequent channel updates with minor adjustments to htlc_maximum_msat and fee_proportional_millionths parameters. These nodes appear to be power users, with many channels and large balances. I assume this is automated channel management at work.\n>>\n>> Core-Lightning has updated rate-limiting in the upcoming release to achieve a higher acceptance of incoming gossip, however, it seems that a broader discussion of rate limits may now be worthwhile. A few immediate ideas:\n>>\n>> - A common listing of current default rate limits across lightning network implementations.\n>>\n>> - Internal checks of RPC input to limit or warn of network propagation issues if certain rates are exceeded.\n>>\n>> - A commonly adopted rate-limit standard.\n>>\n>> My aim is a set reconciliation gossip type, which will use a common, simple heuristic to accept or reject a gossip message. (Think one channel update per block, or perhaps one per block_height << 5.) See my github for my current draft. [3] This solution allows tighter consensus, yet suffers from the same problem as original anti-spam measures \u2013 it remains somewhat arbitrary. I would like to start a conversation regarding gossip propagation, channel_update and node_announcement usage, and perhaps even bandwidth goals for syncing gossip in the future (how about a million channels?) This would aid in the development of gossip set reconciliation, but could also benefit current node connection and routing reliability more generally.\n>>\n>> Thanks,\n>>\n>> Alex\n>>\n>> [1] https://github.com/sipa/minisketch\n>>\n>> [2] https://github.com/endothermicdev/lnspammityspam/blob/main/sampleoutput.txt\n>>\n>> [3] https://github.com/endothermicdev/lightning-rfc/blob/gossip-minisketch/07-routing-gossip.md#set-reconciliation\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220629/0f3f5ed9/attachment-0001.html>"
            },
            {
                "author": "Michael Folkson",
                "date": "2022-06-30T12:54:12",
                "message_text_only": "Awesome, thanks Alex. Just one follow up.\n\n> Running the numbers, I currently see 15,761 public nodes on the network and 148,295 half channels. Those each need refreshed gossip every two weeks. By default that would result in 90% channel updates.\n\nAnd the rationale for each channel needing refreshed gossip every 2 weeks is to inform the network that the channel is still active (i.e. not disabled) and its parameters haven't changed?\n\n(I did look it up in BOLT 7 [0] but it wasn't clear to me that a channel would be assumed to be inactive/disabled if there wasn't a channel_update for 2 weeks.)\n\nThat seems a lot of gossip to me if the recommended behavior of routing nodes is to maintain ~100 percent uptime and only when absolutely necessary change the parameters of the channel. I guess the alternative of significantly less gossip messages and a potential uptick in failed routes would be worse though.\n\n[0]: https://github.com/lightning/bolts/blob/master/07-routing-gossip.md#rationale-4\n\n--\nMichael Folkson\nEmail: michaelfolkson at [protonmail.com](http://protonmail.com/)\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n\n------- Original Message -------\nOn Wednesday, June 29th, 2022 at 7:07 PM, Alex Myers <alex at endothermic.dev> wrote:\n\n> Hi Michael,\n>\n> Thanks for the transcript and the questions, especially those you asked in Gleb's original Erlay presentation.\n>\n> I tried to cover a lot of ground in only 30 minutes and the finer points may have suffered. The most significant difference in concern between bitcoin transaction relay and lightning gossip may be one of privacy: Source nodes of Bitcoin transactions have an interest in privacy (avoid trivially triangulating the source.) Lightning gossip is already signed by and linked to a node ID - the source is completely transparent by nature. The lack of a timing concern would allow for a global sketch where it would have been infeasible for Erlay (among other reasons such as DoS.)\n>\n>> Why are hash collisions a concern for Lightning gossip and not for Erlay? Is it not a DoS vector for both?\n>\n> If lightning gossip were encoded for minisketch entries with the short_channel_id, it would create a unique fingerprint by default thanks to referencing the unique funding transaction on chain - no hashing required. This was Rusty's original concept and what I had been proceeding with. However, given the ongoing privacy discussion and desire to eventually decouple lightning channels from their layer one funding transaction (gossip v2), I think we should prepare for a future in which channels are not explicitly linked to a SCID. That means hashing just as in Erlay and the same DoS vector would be present. Salting with a per-peer shared secret works here, but the solution is driven back toward inventory sets.\n>\n>> It seems you are leaning towards per-peer sketches with inventory sets (like Erlay) rather than global sketches.\n>\n> \u200b\n> Yes. There are pros and cons to each method, but most critically, this would be compatible with eventual removal of the SCID.\n>\n>> Erlay falls back to flooding if the set reconciliation algorithm doesn't work which I'm assuming you'll do with Lightning gossip.\n>\n> Fallback will take some consideration (Erlay's bisect is an elegant feature), but yes, flooding is still the ultimate fallback.\n>\n>> I was also surprised to hear that channel_update made up 97 percent of gossip messages. Isn't it recommended that you don't make too changes to your channel as it is likely to result in failed routed payments and being dropped as a routing node for future payments? It seems that this advice isn't being followed if there are so many channel_update messages being sent around. I almost wonder if Lightning implementations should include user prompts like \"Are you sure you want to update your channel given this may affect your routing success?\" :)\n>\n> Running the numbers, I currently see 15,761 public nodes on the network and 148,295 half channels. Those each need refreshed gossip every two weeks. By default that would result in 90% channel updates. That we're seeing roughly three times as many channel updates vs node announcements compared to what's strictly required is maybe not that surprising. I agree, there would be a benefit to nodes taking a more active role in tracking calls to broadcast gossip.\n>\n> Thanks,\n> Alex\n>\n> ------- Original Message -------\n> On Wednesday, June 29th, 2022 at 6:09 AM, Michael Folkson <michaelfolkson at protonmail.com> wrote:\n>\n>> Thanks for this Alex.\n>>\n>> Here's a transcript of your recent presentation at Bitcoin++ on Minisketch and Lightning gossip:\n>>\n>> https://btctranscripts.com/bitcoinplusplus/2022/2022-06-07-alex-myers-minisketch-lightning-gossip/\n>>\n>> Having followed Gleb's work on using Minisketch for Erlay in Bitcoin Core [0] for a while now I was especially interested in how the challenges of using Minisketch for Lightning gossip (node_announcement, channel_announcement, channel_update messages) would differ to the challenges of using Minisketch for transaction relay on the base layer.\n>>\n>> I guess one of the major differences is full nodes are trying to verify a block every 10 minutes (on average) and so there is a sense of urgency to get the transactions of the next block to be mined. With Lightning gossip unless you are planning to send a payment (or route a payment) across a certain route you are less concerned about learning about the current state of the network urgently. If a new channel pops up you might choose not to route through it regardless given its \"newness\" and its lack of track record of successfully routing payments. There are parts of the network you care less about (if they can't help you get to your regular destinations say) whereas with transaction relay you have to care about all transactions (paying a sufficient fee rate).\n>>\n>> \"The problem that Bitcoin faced with transaction relay was pretty similar but there are a few differences.For one, any time you introduce that short hash function that produces a 64 bit fingerprint you have to be concerned with collisions between hash functions. Someone could potentially take advantage of that and grind out a hash that would resolve to the same fingerprint.\"\n>>\n>> Could you elaborate on this? Why are hash collisions a concern for Lightning gossip and not for Erlay? Is it not a DoS vector for both?\n>>\n>> It seems you are leaning towards per-peer sketches with inventory sets (like Erlay) rather than global sketches. This makes sense to me and seems to be moving in a direction where your peer connections are more stable as you are storing data on what your peer's understanding of the network is. There could even be centralized APIs which allow you to compare your current understanding of the network to the centralized service's understanding. (Of course we don't want to have to rely on centralized services or bake them into the protocol if you don't want to use them.) Erlay falls back to flooding if the set reconciliation algorithm doesn't work which I'm assuming you'll do with Lightning gossip.\n>>\n>> I was also surprised to hear that channel_update made up 97 percent of gossip messages. Isn't it recommended that you don't make too changes to your channel as it is likely to result in failed routed payments and being dropped as a routing node for future payments? It seems that this advice isn't being followed if there are so many channel_update messages being sent around. I almost wonder if Lightning implementations should include user prompts like \"Are you sure you want to update your channel given this may affect your routing success?\" :)\n>>\n>> Thanks\n>> Michael\n>>\n>> P.S. Are we referring to \"routing nodes\" as \"forwarding nodes\" now? I've noticed \"forwarding nodes\" being used more recently on this list.\n>>\n>> [0]: https://github.com/bitcoin/bitcoin/pull/21515\n>>\n>> --\n>> Michael Folkson\n>> Email: michaelfolkson at [protonmail.com](http://protonmail.com/)\n>> Keybase: michaelfolkson\n>> PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n>>\n>> ------- Original Message -------\n>> On Thursday, April 14th, 2022 at 22:00, Alex Myers <alex at endothermic.dev> wrote:\n>>\n>>> Hello lightning developers,\n>>>\n>>> I\u2019ve been investigating set reconciliation as a means to reduce bandwidth and redundancy of gossip message propagation. This builds on some earlier work from Rusty using the minisketch library [1]. The idea is that each node will build a sketch representing it\u2019s own gossip set. Alice\u2019s node will encode and transmit this sketch to Bob\u2019s node, where it will be merged with his own sketch, and the differences produced. These differences should ideally be exactly the latest missing gossip of both nodes. Due to size constraints, the set differences will necessarily be encoded, but Bob\u2019s node will be able to identify which gossip Alice is missing, and may then transmit exactly those messages.\n>>>\n>>> This process is relatively straightforward, with the caveat that the sets must otherwise match very closely (each sketch has a maximum capacity for differences.) The difficulty here is that each node and lightning implementation may have its own rules for gossip acceptance and propagation. Depending on their gossip partners, not all gossip may propagate to the entire network.\n>>>\n>>> Core-lightning implements rate limiting for incoming channel updates and node announcements. The default rate limit is 1 per day, with a burst of 4. I analyzed my node\u2019s gossip over a 14 day period, and found that, of all publicly broadcasting half-channels, 18% of them fell afoul of our spam-limiting rules at least once. [2]\n>>>\n>>> Picking several offending channel ids, and digging further, the majority of these appear to be flapping due to Tor or otherwise intermittent connections. Well connected nodes may be more susceptible to this due to more frequent routing attempts, and failures resulting in a returned channel update (which otherwise might not have been broadcast.)A slight relaxation of the rate limit resolves the majority of these cases.\n>>>\n>>> A smaller subset of channels broadcast frequent channel updates with minor adjustments to htlc_maximum_msat and fee_proportional_millionths parameters. These nodes appear to be power users, with many channels and large balances. I assume this is automated channel management at work.\n>>>\n>>> Core-Lightning has updated rate-limiting in the upcoming release to achieve a higher acceptance of incoming gossip, however, it seems that a broader discussion of rate limits may now be worthwhile. A few immediate ideas:\n>>>\n>>> - A common listing of current default rate limits across lightning network implementations.\n>>>\n>>> - Internal checks of RPC input to limit or warn of network propagation issues if certain rates are exceeded.\n>>>\n>>> - A commonly adopted rate-limit standard.\n>>>\n>>> My aim is a set reconciliation gossip type, which will use a common, simple heuristic to accept or reject a gossip message. (Think one channel update per block, or perhaps one per block_height << 5.) See my github for my current draft. [3] This solution allows tighter consensus, yet suffers from the same problem as original anti-spam measures \u2013 it remains somewhat arbitrary. I would like to start a conversation regarding gossip propagation, channel_update and node_announcement usage, and perhaps even bandwidth goals for syncing gossip in the future (how about a million channels?) This would aid in the development of gossip set reconciliation, but could also benefit current node connection and routing reliability more generally.\n>>>\n>>> Thanks,\n>>>\n>>> Alex\n>>>\n>>> [1] https://github.com/sipa/minisketch\n>>>\n>>> [2] https://github.com/endothermicdev/lnspammityspam/blob/main/sampleoutput.txt\n>>>\n>>> [3] https://github.com/endothermicdev/lightning-rfc/blob/gossip-minisketch/07-routing-gossip.md#set-reconciliation\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220630/2603e096/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Gossip Propagation, Anti-spam, and Set Reconciliation",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Michael Folkson",
                "Alex Myers"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 30261
        }
    }
]