[
    {
        "title": "[Lightning-dev] Goodbye Bitcoin",
        "thread_messages": [
            {
                "author": "Prayank",
                "date": "2022-03-07T07:50:48",
                "message_text_only": "Hello kanzure mailing list and ln mailing list,\n\nThis is my last email and I won't be involved in anything related to Bitcoin. If my username is used on GitHub or other places it can be considered someone else using it.\n\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220307/755aa9bc/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Goodbye Bitcoin",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Prayank"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 439
        }
    },
    {
        "title": "[Lightning-dev] Interesting thing about Offered HTLCs",
        "thread_messages": [
            {
                "author": "Eugene Siegel",
                "date": "2022-03-07T20:26:51",
                "message_text_only": "I'm not sure if this is known, but I'm pretty sure it's benign and so I\nthought I'd share since I found it interesting and maybe someone else will\ntoo. I'm not sure if this is already known either.\n\nhttps://github.com/lightning/bolts/blob/master/03-transactions.md#offered-htlc-outputs\nOffered HTLCs have three claim paths: the revocation case, the offerer\nclaiming through the HTLC-timeout transaction, and the receiver claiming\nvia their sig + preimage. The offering party can claim via the HTLC-timeout\ncase on their commitment transaction with their signature and the remote's\nsignature (SIGHASH_ALL) after the cltv_expiry timeout. Since the remote\nparty gives them a signature, after the timeout, the offering party can\nclaim with the remote's signature + preimage, but can only spend with the\nHTLC-timeout transaction because of SIGHASH_ALL. This assumes that the\nremote party doesn't claim it first. I can't think of any cases where the\noffering party would know the preimage AND want to force close, so that's\nwhy I think it's benign. It does make the witness smaller. The same trick\nisn't possible with the Received HTLC's due to OP_CHECKLOCKTIMEVERIFY.\n\nEugene (Crypt-iQ on github)\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220307/b2eda6a8/attachment.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2022-03-07T22:46:07",
                "message_text_only": "Hi Eugene,\n\n> Since the remote party gives them a signature, after the timeout, the\noffering party can\nclaim with the remote's signature + preimage, but can only spend with the\nHTLC-timeout transaction because of SIGHASH_ALL.\n\nI've not exercised the witness against our test framework though the\ndescription sounds to me correct.\n\nThe offering counterparty spends the offered HTLC output with a\nHTLC-timeout transaction where the witness is <<remote_sig>\n<payment_preimage>>. SIGHASH_ALL is not committing to the spent Script\nbranch intended to be used. As you raised, it doesn't alleviate the\noffering counterparty to respect the CLTV delay and as such the offered\nHTLC timespan cannot be shortened. The implication I can think of, in case\nof competing HTLC race, once the absolute timelock is expired, the offering\ncounterparty is able to compete against the receiving one with a more\nfeerate-efficient witness. However, from a receiving counterparty safety\nviewpoint, if you're already suffering a contest, it means your HTLC-claim\non your own local commitment transaction inbound HTLC output has been\ninefficient, and your fee-bumping strategy is to blame.\n\nIf we think the issue is relevant, I believe splitting the Script branches\nin two tapleaves and having bip342 signature digest committing to the\ntapleaf_hash solves it.\n\nAntoine\n\nLe lun. 7 mars 2022 \u00e0 15:27, Eugene Siegel <elzeigel at gmail.com> a \u00e9crit :\n\n> I'm not sure if this is known, but I'm pretty sure it's benign and so I\n> thought I'd share since I found it interesting and maybe someone else will\n> too. I'm not sure if this is already known either.\n>\n>\n> https://github.com/lightning/bolts/blob/master/03-transactions.md#offered-htlc-outputs\n> Offered HTLCs have three claim paths: the revocation case, the offerer\n> claiming through the HTLC-timeout transaction, and the receiver claiming\n> via their sig + preimage. The offering party can claim via the HTLC-timeout\n> case on their commitment transaction with their signature and the remote's\n> signature (SIGHASH_ALL) after the cltv_expiry timeout. Since the remote\n> party gives them a signature, after the timeout, the offering party can\n> claim with the remote's signature + preimage, but can only spend with the\n> HTLC-timeout transaction because of SIGHASH_ALL. This assumes that the\n> remote party doesn't claim it first. I can't think of any cases where the\n> offering party would know the preimage AND want to force close, so that's\n> why I think it's benign. It does make the witness smaller. The same trick\n> isn't possible with the Received HTLC's due to OP_CHECKLOCKTIMEVERIFY.\n>\n> Eugene (Crypt-iQ on github)\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220307/b55f369a/attachment.html>"
            },
            {
                "author": "Eugene Siegel",
                "date": "2022-03-10T14:55:24",
                "message_text_only": "Yes I think bip342 should solve it. Maybe splitting up all conditionals\ninto leaves is a good idea for taproot lightning\n\nOn Mon, Mar 7, 2022 at 5:46 PM Antoine Riard <antoine.riard at gmail.com>\nwrote:\n\n> Hi Eugene,\n>\n> > Since the remote party gives them a signature, after the timeout, the\n> offering party can\n> claim with the remote's signature + preimage, but can only spend with the\n> HTLC-timeout transaction because of SIGHASH_ALL.\n>\n> I've not exercised the witness against our test framework though the\n> description sounds to me correct.\n>\n> The offering counterparty spends the offered HTLC output with a\n> HTLC-timeout transaction where the witness is <<remote_sig>\n> <payment_preimage>>. SIGHASH_ALL is not committing to the spent Script\n> branch intended to be used. As you raised, it doesn't alleviate the\n> offering counterparty to respect the CLTV delay and as such the offered\n> HTLC timespan cannot be shortened. The implication I can think of, in case\n> of competing HTLC race, once the absolute timelock is expired, the offering\n> counterparty is able to compete against the receiving one with a more\n> feerate-efficient witness. However, from a receiving counterparty safety\n> viewpoint, if you're already suffering a contest, it means your HTLC-claim\n> on your own local commitment transaction inbound HTLC output has been\n> inefficient, and your fee-bumping strategy is to blame.\n>\n> If we think the issue is relevant, I believe splitting the Script branches\n> in two tapleaves and having bip342 signature digest committing to the\n> tapleaf_hash solves it.\n>\n> Antoine\n>\n> Le lun. 7 mars 2022 \u00e0 15:27, Eugene Siegel <elzeigel at gmail.com> a \u00e9crit :\n>\n>> I'm not sure if this is known, but I'm pretty sure it's benign and so I\n>> thought I'd share since I found it interesting and maybe someone else will\n>> too. I'm not sure if this is already known either.\n>>\n>>\n>> https://github.com/lightning/bolts/blob/master/03-transactions.md#offered-htlc-outputs\n>> Offered HTLCs have three claim paths: the revocation case, the offerer\n>> claiming through the HTLC-timeout transaction, and the receiver claiming\n>> via their sig + preimage. The offering party can claim via the HTLC-timeout\n>> case on their commitment transaction with their signature and the remote's\n>> signature (SIGHASH_ALL) after the cltv_expiry timeout. Since the remote\n>> party gives them a signature, after the timeout, the offering party can\n>> claim with the remote's signature + preimage, but can only spend with the\n>> HTLC-timeout transaction because of SIGHASH_ALL. This assumes that the\n>> remote party doesn't claim it first. I can't think of any cases where the\n>> offering party would know the preimage AND want to force close, so that's\n>> why I think it's benign. It does make the witness smaller. The same trick\n>> isn't possible with the Received HTLC's due to OP_CHECKLOCKTIMEVERIFY.\n>>\n>> Eugene (Crypt-iQ on github)\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220310/dbf38c56/attachment.html>"
            },
            {
                "author": "darosior",
                "date": "2022-03-11T13:47:00",
                "message_text_only": "Also, using Miniscript (whether in Segwit v0 or v1) would prevent this kind of surprises. And many potential others. :-)\n\nI'll post something soon about how we could integrate Miniscript in Lightning.\n-------- Original Message --------\nOn Mar 10, 2022, 2:55 PM, Eugene Siegel wrote:\n\n> Yes I think bip342 should solve it. Maybe splitting up all conditionals into leaves is a good idea for taproot lightning\n>\n> On Mon, Mar 7, 2022 at 5:46 PM Antoine Riard <antoine.riard at gmail.com> wrote:\n>\n>> Hi Eugene,\n>>\n>>> Since the remote party gives them a signature, after the timeout, the offering party can\n>> claim with the remote's signature + preimage, but can only spend with the\n>> HTLC-timeout transaction because of SIGHASH_ALL.\n>>\n>> I've not exercised the witness against our test framework though the description sounds to me correct.\n>>\n>> The offering counterparty spends the offered HTLC output with a HTLC-timeout transaction where the witness is <<remote_sig> <payment_preimage>>. SIGHASH_ALL is not committing to the spent Script branch intended to be used. As you raised, it doesn't alleviate the offering counterparty to respect the CLTV delay and as such the offered HTLC timespan cannot be shortened. The implication I can think of, in case of competing HTLC race, once the absolute timelock is expired, the offering counterparty is able to compete against the receiving one with a more feerate-efficient witness. However, from a receiving counterparty safety viewpoint, if you're already suffering a contest, it means your HTLC-claim on your own local commitment transaction inbound HTLC output has been inefficient, and your fee-bumping strategy is to blame.\n>>\n>> If we think the issue is relevant, I believe splitting the Script branches in two tapleaves and having bip342 signature digest committing to the tapleaf_hash solves it.\n>>\n>> Antoine\n>>\n>> Le lun. 7 mars 2022 \u00e0 15:27, Eugene Siegel <elzeigel at gmail.com> a \u00e9crit :\n>>\n>>> I'm not sure if this is known, but I'm pretty sure it's benign and so I thought I'd share since I found it interesting and maybe someone else will too. I'm not sure if this is already known either.\n>>>\n>>> https://github.com/lightning/bolts/blob/master/03-transactions.md#offered-htlc-outputs\n>>> Offered HTLCs have three claim paths: the revocation case, the offerer claiming through the HTLC-timeout transaction, and the receiver claiming via their sig + preimage. The offering party can claim via the HTLC-timeout case on their commitment transaction with their signature and the remote's signature (SIGHASH_ALL) after the cltv_expiry timeout. Since the remote party gives them a signature, after the timeout, the offering party can claim with the remote's signature + preimage, but can only spend with the HTLC-timeout transaction because of SIGHASH_ALL. This assumes that the remote party doesn't claim it first. I can't think of any cases where the offering party would know the preimage AND want to force close, so that's why I think it's benign. It does make the witness smaller. The same trick isn't possible with the Received HTLC's due to OP_CHECKLOCKTIMEVERIFY.\n>>>\n>>> Eugene (Crypt-iQ on github)\n>>> _______________________________________________\n>>> Lightning-dev mailing list\n>>> Lightning-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220311/b3dd20bc/attachment.html>"
            },
            {
                "author": "Eugene Siegel",
                "date": "2022-03-15T15:26:25",
                "message_text_only": "I'm not familiar with miniscript besides that it's a subset of script - how\nwould it help avoiding an unintended path being taken?\n\nOn Fri, Mar 11, 2022 at 8:47 AM darosior <darosior at protonmail.com> wrote:\n\n> Also, using Miniscript (whether in Segwit v0 or v1) would prevent this\n> kind of surprises. And many potential others. :-)\n>\n>\n> I'll post something soon about how we could integrate Miniscript in\n> Lightning.\n> -------- Original Message --------\n> On Mar 10, 2022, 2:55 PM, Eugene Siegel < elzeigel at gmail.com> wrote:\n>\n>\n> Yes I think bip342 should solve it. Maybe splitting up all conditionals\n> into leaves is a good idea for taproot lightning\n>\n> On Mon, Mar 7, 2022 at 5:46 PM Antoine Riard <antoine.riard at gmail.com>\n> wrote:\n>\n>> Hi Eugene,\n>>\n>> > Since the remote party gives them a signature, after the timeout, the\n>> offering party can\n>> claim with the remote's signature + preimage, but can only spend with the\n>> HTLC-timeout transaction because of SIGHASH_ALL.\n>>\n>> I've not exercised the witness against our test framework though the\n>> description sounds to me correct.\n>>\n>> The offering counterparty spends the offered HTLC output with a\n>> HTLC-timeout transaction where the witness is <<remote_sig>\n>> <payment_preimage>>. SIGHASH_ALL is not committing to the spent Script\n>> branch intended to be used. As you raised, it doesn't alleviate the\n>> offering counterparty to respect the CLTV delay and as such the offered\n>> HTLC timespan cannot be shortened. The implication I can think of, in case\n>> of competing HTLC race, once the absolute timelock is expired, the offering\n>> counterparty is able to compete against the receiving one with a more\n>> feerate-efficient witness. However, from a receiving counterparty safety\n>> viewpoint, if you're already suffering a contest, it means your HTLC-claim\n>> on your own local commitment transaction inbound HTLC output has been\n>> inefficient, and your fee-bumping strategy is to blame.\n>>\n>> If we think the issue is relevant, I believe splitting the Script\n>> branches in two tapleaves and having bip342 signature digest committing to\n>> the tapleaf_hash solves it.\n>>\n>> Antoine\n>>\n>> Le lun. 7 mars 2022 \u00e0 15:27, Eugene Siegel <elzeigel at gmail.com> a \u00e9crit :\n>>\n>>> I'm not sure if this is known, but I'm pretty sure it's benign and so I\n>>> thought I'd share since I found it interesting and maybe someone else will\n>>> too. I'm not sure if this is already known either.\n>>>\n>>>\n>>> https://github.com/lightning/bolts/blob/master/03-transactions.md#offered-htlc-outputs\n>>> Offered HTLCs have three claim paths: the revocation case, the offerer\n>>> claiming through the HTLC-timeout transaction, and the receiver claiming\n>>> via their sig + preimage. The offering party can claim via the HTLC-timeout\n>>> case on their commitment transaction with their signature and the remote's\n>>> signature (SIGHASH_ALL) after the cltv_expiry timeout. Since the remote\n>>> party gives them a signature, after the timeout, the offering party can\n>>> claim with the remote's signature + preimage, but can only spend with the\n>>> HTLC-timeout transaction because of SIGHASH_ALL. This assumes that the\n>>> remote party doesn't claim it first. I can't think of any cases where the\n>>> offering party would know the preimage AND want to force close, so that's\n>>> why I think it's benign. It does make the witness smaller. The same trick\n>>> isn't possible with the Received HTLC's due to OP_CHECKLOCKTIMEVERIFY.\n>>>\n>>> Eugene (Crypt-iQ on github)\n>>> _______________________________________________\n>>> Lightning-dev mailing list\n>>> Lightning-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220315/f3f7ac11/attachment.html>"
            },
            {
                "author": "darosior",
                "date": "2022-03-16T10:03:29",
                "message_text_only": "Miniscripts with duplicate keys are considered insane as it makes it too hard to reason about malleability (there is no CODESEPARATOR in Miniscript).\n\nA policy compiler would never produce such a Miniscript.\n\n-------- Original Message --------\nOn Mar 15, 2022, 4:26 PM, Eugene Siegel < elzeigel at gmail.com> wrote:\n\n> I'm not familiar with miniscript besides that it's a subset of script - how would it help avoiding an unintended path being taken?\n>\n> On Fri, Mar 11, 2022 at 8:47 AM darosior <darosior at protonmail.com> wrote:\n>\n>> Also, using Miniscript (whether in Segwit v0 or v1) would prevent this kind of surprises. And many potential others. :-)\n>>\n>> I'll post something soon about how we could integrate Miniscript in Lightning.\n>> -------- Original Message --------\n>> On Mar 10, 2022, 2:55 PM, Eugene Siegel < elzeigel at gmail.com> wrote:\n>>\n>>> Yes I think bip342 should solve it. Maybe splitting up all conditionals into leaves is a good idea for taproot lightning\n>>>\n>>> On Mon, Mar 7, 2022 at 5:46 PM Antoine Riard <antoine.riard at gmail.com> wrote:\n>>>\n>>>> Hi Eugene,\n>>>>\n>>>>> Since the remote party gives them a signature, after the timeout, the offering party can\n>>>> claim with the remote's signature + preimage, but can only spend with the\n>>>> HTLC-timeout transaction because of SIGHASH_ALL.\n>>>>\n>>>> I've not exercised the witness against our test framework though the description sounds to me correct.\n>>>>\n>>>> The offering counterparty spends the offered HTLC output with a HTLC-timeout transaction where the witness is <<remote_sig> <payment_preimage>>. SIGHASH_ALL is not committing to the spent Script branch intended to be used. As you raised, it doesn't alleviate the offering counterparty to respect the CLTV delay and as such the offered HTLC timespan cannot be shortened. The implication I can think of, in case of competing HTLC race, once the absolute timelock is expired, the offering counterparty is able to compete against the receiving one with a more feerate-efficient witness. However, from a receiving counterparty safety viewpoint, if you're already suffering a contest, it means your HTLC-claim on your own local commitment transaction inbound HTLC output has been inefficient, and your fee-bumping strategy is to blame.\n>>>>\n>>>> If we think the issue is relevant, I believe splitting the Script branches in two tapleaves and having bip342 signature digest committing to the tapleaf_hash solves it.\n>>>>\n>>>> Antoine\n>>>>\n>>>> Le lun. 7 mars 2022 \u00e0 15:27, Eugene Siegel <elzeigel at gmail.com> a \u00e9crit :\n>>>>\n>>>>> I'm not sure if this is known, but I'm pretty sure it's benign and so I thought I'd share since I found it interesting and maybe someone else will too. I'm not sure if this is already known either.\n>>>>>\n>>>>> https://github.com/lightning/bolts/blob/master/03-transactions.md#offered-htlc-outputs\n>>>>> Offered HTLCs have three claim paths: the revocation case, the offerer claiming through the HTLC-timeout transaction, and the receiver claiming via their sig + preimage. The offering party can claim via the HTLC-timeout case on their commitment transaction with their signature and the remote's signature (SIGHASH_ALL) after the cltv_expiry timeout. Since the remote party gives them a signature, after the timeout, the offering party can claim with the remote's signature + preimage, but can only spend with the HTLC-timeout transaction because of SIGHASH_ALL. This assumes that the remote party doesn't claim it first. I can't think of any cases where the offering party would know the preimage AND want to force close, so that's why I think it's benign. It does make the witness smaller. The same trick isn't possible with the Received HTLC's due to OP_CHECKLOCKTIMEVERIFY.\n>>>>>\n>>>>> Eugene (Crypt-iQ on github)\n>>>>> _______________________________________________\n>>>>> Lightning-dev mailing list\n>>>>> Lightning-dev at lists.linuxfoundation.org\n>>>>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220316/fd4e1c3d/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Interesting thing about Offered HTLCs",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "darosior",
                "Antoine Riard",
                "Eugene Siegel"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 19369
        }
    },
    {
        "title": "[Lightning-dev] Code for sub second runtime of piecewise linarization to quickly approximate the minimum convex cost flow problem (makes fast multi part payments with large amounts possible)",
        "thread_messages": [
            {
                "author": "Ren\u00e9 Pickhardt",
                "date": "2022-03-11T14:33:38",
                "message_text_only": "Dear fellow Lightning Developers,\n\nI am pleased (and a bit proud) to be able to inform you that I finally\nfound a quick way to approximate the slow minimum convex cost flow\ncomputation. This is necessary for optimally reliable and cheap payment\nflows [0] to deliver large multi part payments over the Lightning Network.\nThe proposed solution happens via piecewise linearization [1] of the min\ncost flow problem on the uncertainty network which we face in order to\ncompute the optimal split and planning of large amount multi part payments.\nThe notion of \"optimal\" is obviously subjective with respect to the chosen\ncost function. As known we suggest to include the negative logarithm of\nsuccess probabilities based on the likelihood that enough liquidity is\navailable on a channel as a dominant feature of the used cost function. We\ngive the background for this in [2] which since then has already been\npicked up by c-lightning and LDK. The c-lightning team even published\nbenchmarks showing significant improvement in payment speed over their\npreviously used cost function [2b].\n\nLet me recall that one of the largest criticisms and concerns of our\napproach to use minimum cost flows for payment delivery back in July /\nAugust last year (especially by the folks from lightning labs) was that the\nmin cost flow approach would be impractical due to run time constrains.\nThus I am delighted that with the now published code [3] (which has exactly\n100 lines including data import and parsing and ignoring comments) we are\nable to compute a reasonable looking approximation to the optimal solution\nin a sub second run time on the complete public channel graph of the\nLightning Network. This is achieved via piecewise linearization of the\nconvex cost function and invoking of a standard linear min cost flow solver\n[4] for the linearized problem. This works quickly despite the fact that\nthe piecewise linearization adds a significant higher amount of arcs to the\nnetwork and blows up the size of the network on which we solve the min cost\nflow problem. This makes me fairly certain that with proper pruning of the\ngraph we might even reach the 100 millisecond runtime frontier, which would\nbe far faster than what I dreamed & hoped to be possible.\n\nThe currently widely deployed Dijkstra search to generate a single\ncandidate path takes roughly 100ms of runtime. It seems that with the\nruntime of the piecewise linearized problem the min cost flow approach is\nnow competitive from a runtime perspective. The flow computation is still a\nbit slower than Dijkstra in both theory and practice. However the piecewise\nlinearized min cost flow has the huge advantage that it generates several\ncandidate paths for a solid approximation of the optimal MPP split.\nRemember the exact min cost flow corresponds to the optimal MPP split. The\nlater was not used so far as the min cost flow was considered to be too\nslow. Yet the question how to split seems to be addressed as issues in\nimplementations [5][6][7] and acknowledged to be complicated (especially\nwith respect to fees) in [8]. This result is btw of particular interest for\nLSPs. If an LSP has to schedule x payments per second it can just do one\nflow computation with several sink nodes and plan all of those payments\nwith a single min cost flow computation. This globally optimizes the\npotentially heavy load that LSPs might have even if all payments were so\nsmall that no splitting was necessary.\n\nThe iPython notebook which I shared contains about 1 page to explain how to\nconduct a piecewise linear approximation. The quality of the approximation\nis not the major goal here as I am just focused to demonstrate the run time\nof the approach and the principle how to achieve this runtime. Thus I do\nthe piecewise linear approximation very roughly in the published code.\nSelecting the optimal piecewise approximation [9] will not change the the\nruntime of flow computation but only blows up the code to prepare the\nsolver. This is why I decided to keep the code as simple and short as\npossible even if that means that the approximation will be not as close to\nthe optimum as it could be in practice. For the same reason I did not\ninclude any code to update the uncertainty network from previously failed\nor successful attempts by using conditional success probabilities P(X>a |\nmin_liquidity < X < max_liquidity ). People who are interested might look\nat my hands on instructions and explanations for coders in this\nrust-lightning issue [10]. The folks from LDK have picked this up and\nimplemented this already for single path payments in [11] which might be\nrelevant for people who prefer code over math papers. An obvious\noptimization of the piece wise linearization would be to chose the first\nsegment of the piecewise linear approximation with a capacity of the\ncertain liquidity and a unit cost of 0 for that piece.\n\nOur original papers describe everything only from a theoretical point of\nview and with simulations. However our mainnet experiments from July last\nyear [12] indicated that we easily have been able to deliver for example\n0.3679 BTC via a couple rounds of min cost flow computations through the\nLightning Network (given we accept the computational waiting time). This\nexperiment was too slow to be used in practice but confirmed our model and\napproach to deliver such large amounts. Given that the min cost flow\ncomputation is now significantly below the median onion round trip times it\nis my understanding that with these newly introduced techniques we should\nbe able to deliver substantial monetary amounts over the Lightning Network\nwithin a couple of seconds despite the uncertainty about the Liquidity in\nremote channels. While I am very excited about the result I think some\nprinciple roadblocks of the protocol are becoming more and more visible:\n\n1. The well known issue of hanging HTLCs. As of now I believe onion\nmessages would be great to acknowledge incoming HTLCs of multipart payments\nbut I certainly believe this will not be sufficient if we don't go for a\ncancelable & stuckless payment protocol as suggested in [13].\n2. I believe the Lightning Network Protocol should be able to handle\nredundant overpayments as suggested in [14] and [15] (The later paper\nderives independently of us the same success probabilities and tries to\nmaximize them!). This would allow to transform the problem of finding a\nflow that maximizes the success probability to a flow that expects to\ndeliver the amount of the invoice (when redundancy is applied) with a\nsingle round of min cost flow computation on average.\n\nLast but not least, please allow me to make a short remark on the (still to\nme very surprisingly controversial) base fee discussion: For simplicity I\ndid not include any fee considerations to the published code (besides a fee\nreport on how expensive the computed flow is). However in practice we wish\nto optimize at least for high reliability (via neg log success\nprobabilities) and cheap fees which in particular with the ppm is very\neasily possible to be included to the piece wise linearized cost function.\nWhile for small base fees it seems possible to encode the base fee into the\nfirst segment of the piecewise linearized approximation I think the base\nfee will still be tricky to be handled in practice (even with this\napproximation). For example if the base fee is too high the \"base fee\nadjusted\" unit cost of the first segment of the piecewise linearized\nproblem might be higher than the unit cost of the second segment which\neffectively would break the convexity. Thus I reiterate my earlier point\nthat from the perspective of the year long pursued goal of optimizing for\nfees (which all Dijkstra based single path implementations do) it seems to\nbe best if the non linearity that is introduced by the base fee would be\nremoved at all. According to discussions with people who crate Lightning\nNetwork explorer (and according to my last check of gossip) about 90% of\nchannels have a base fee of 1 sat or lower and ~38% of all channels already\nset their base fee away from the default value to 0 [16].\n\nI hope this mail was useful for you. Feel free to ask me anything if there\nshould be questions or if you need help to integrate those results into\nyour software!\n\nWith kind regards Rene Pickhardt\n\n[0]: https://arxiv.org/abs/2107.05322\n[1]: https://en.wikipedia.org/wiki/Piecewise_linear_function\n[2]: https://arxiv.org/abs/2103.08576\n[2b]:\nhttps://medium.com/blockstream/c-lightning-v0-10-2-bitcoin-dust-consensus-rule-33e777d58657\n\n[3]:\nhttps://github.com/renepickhardt/mpp-splitter/blob/master/Minimal%20Linearized%20min%20cost%20flow%20example%20for%20MPP.ipynb\n[4]: https://developers.google.com/optimization/flow/mincostflow\n[5]: https://github.com/lightningnetwork/lnd/issues/4203\n[6]: https://github.com/lightningdevkit/rust-lightning/issues/1276\n[7]: https://github.com/ElementsProject/lightning/issues/4753\n[8]: https://github.com/ACINQ/eclair/pull/1427\n[9]: http://www.iaeng.org/publication/WCECS2008/WCECS2008_pp1191-1194.pdf\n[10]:\nhttps://github.com/lightningdevkit/rust-lightning/issues/1170#issuecomment-972396747\n[11]: https://github.com/lightningdevkit/rust-lightning/pull/1227\n[12]: https://twitter.com/renepickhardt/status/1418849788531990530\n[13]:\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2019-June/002029.html\n[14]: https://berkeley-defi.github.io/assets/material/1910.01834.pdf\n[15]: https://dl.acm.org/doi/10.1145/3479722.3480997\n[16]: https://lnrouter.app/graph/zero-base-fee\n\n-- \nhttps://ln.rene-pickhardt.de\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220311/0ba5d2d0/attachment-0001.html>"
            },
            {
                "author": "Carsten Otto",
                "date": "2022-03-13T19:01:58",
                "message_text_only": "Hi Rene,\n\nthanks a lot for your contribution! This is exactly what I needed to\nstart coding again :) I intend to release a somewhat usable version of\nyour approach in lnd-manageJ [1] soon. Preliminary results indicate that\nresults (i.e. MPPs to try) can be computed in less than a second, which\nis great!\n\nImportant remark: my code can only be used for real MPPs once a usable\nMPP gRPC call is made available, see lnd issue #5746 [2].\n\nWhile working on my implementation, a few questions came to mind:\n\n1) What's the reasoning behind combining parallel channels?\n\nI agree that parallel channels make things a lot more complicated, but I\nalso see the benefit from a node operator's point of view. That being\nsaid, wouldn't it suffice to treat parallel channels individually?\n\n1.1) A payment of size 2 needs to be split into 1+1 to fit through\nparallel channels of size 1+1. Combining the 1+1 channels into a virtual\nchannel of size 2 only complicates the code that has to do come up with\na MPP that doesn't over-saturate the actual channels. On the other hand,\nI don't think the probability for the virtual channel of size 2 is more\nrealistic than reasoning about two individual channels and their\nprobabilities - but I didn't even try to see the math behind that.\nPlease prove me wrong? :)\n\n1.2) The Mission Control information provided by lnd can be used to\nplace a minimum available balance on each of the parallel channels. If\nwe know that node A isn't able to forward N sats to node B, we can treat\nall parallel channels between A and B (in that direction) to have a\ncapacity of at most N-1 sats. How would this look like if we combined\nthe parallel channels into a virtual one? Note that it may still be\npossible to route two individual payments/onions of size N-1 sats from A\nto B, given two parallel channels with that many sats on A's side.\n\n2) Optimal Piecewise Linearization\n\nSee Twitter [3].\n\nIs it worth it cutting a channel into pieces of different sizes, instead\nof just having (as per your example) 5 pieces of the same size? If it\nmakes a noticeable difference, adding some complexity to the code might\nbe worth it.\n\n3) Size of Piecewise Linearization\n\nMy gut feeling is that cutting a 1 BTC channel into 5 pieces is\ndifferent from cutting a 0.01 BTC channel into 5 pieces. Would it make\nsense to use different values of N depending on the channel size?\n\n4) Leftovers after Piecewise Linearization\n\nIf I cut some channel into N pieces, I might end up with up to N-1 sats\nthat don't end up in any of the N pieces, effectively making the channel\nlook smaller than it is. For smaller values of N that's obviously not an\nissue (given the uncertainty we're dealing with), but it might be more\nproblematic if quantization is used with larger values. Any thoughts on\nthis?\n\n5) Fees (and other properties?)\n\nHow can we integrate fees into the function? I must admit, I haven't\neven thought about that, yet. A copy-paste answer would be great,\nthough! :) Maybe it's also a good idea to punish channels based on their\nCLTV delta? Ratio of enabled channels? Age? Manual punishment score? ...\n\n6) Non-Zero Base Fee\n\nSee Twitter [4].\n\nAccording to Stefan [5] it should be possible to integrate ZmnSCPxj's ideas\nto make this work with non-zero base fees. How?\nSimpler approach: Twitter [6].\n\n7) Private Channels\n\n[very niche topic, not really that interesting nor urgent]\n\nI'm a fan of adding private channels to provide more outbound liquidity,\nmainly to reduce gossip and hide my intentions. If my total liquidity to\nsome peer is below the amount announced in public channels, I don't see\nany meaningful complication. However, I might have a public channel of\nsize N and several private channels bringing my local liquidity to some\nvalue >N. It's rather obvious that not announcing this fact is a bad\nidea, as any #pickhardtpayments implementation would think I have 0-N on\nmy side of the channel(s). Assuming I'm willing to accept this tradeoff,\ndo you see other complications or issues with hidden liquidity?\n\nMy gut feeling is that this isn't an issue, at all, as channel balances\nchange all the time, which is something the algorithm already has to\ndeal with.\n\n8) Quality of Approximation\n\nThere are some problems in computer science that are hard/impossible to\napproximate, in the sense that any kind of deviation from the optimum\ncould cause the computed results to be extremely bad. Do you have some\nidea (or proof) that your kind of approximation isn't causing a major\nissue? I guess a piece-wise linearization with an infinite number of\npieces corresponds to the optimal result. Given a finite number of\npieces, how large is the difference to the optimum?\n\nBye,\nCarsten\n\n1: https://github.com/C-Otto/lnd-manageJ/issues/6\n2: https://github.com/lightningnetwork/lnd/issues/5746\n3: https://twitter.com/c_otto83/status/1502329970349248521\n4: https://twitter.com/c_otto83/status/1502329271964033027\n5: https://twitter.com/stefanwouldgo/status/1502681455918473217\n6: https://twitter.com/c_otto83/status/1502330558793363464\n-- \nDr. Carsten Otto\ncarsten at c-otto.de\nhttps://c-otto.de\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 195 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220313/1437e67a/attachment.sig>"
            },
            {
                "author": "Ren\u00e9 Pickhardt",
                "date": "2022-03-14T11:56:24",
                "message_text_only": "Dear Carsten, Martin and fellow lightning developers,\n\nfirst of all thank you very much for independently verifying and\nacknowledging my recent findings about the runtime of finding a pieceweise\nlinearized approximation to the min cost flow problem, for working on\nintegrating them into lnd-manageJ and for your excellent questions &\nthoughts.\n\nOn Sun, Mar 13, 2022 at 8:17 PM Carsten Otto via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> wrote:\n\n\n> 1) What's the reasoning behind combining parallel channels?\n>\n\nGenerally speaking this is pure pragmatism on my end to simplify my life as\nhandling parallel channels in some cases blows up complexity of code and\nsimulations. However I think from a probabilistic point of view ( see below\n) the combination is more accurate to reflect the actual likelihood that\nthe liquidity is available.\n\nI agree that parallel channels make things a lot more complicated, but I\n> also see the benefit from a node operator's point of view. That being\n> said, wouldn't it suffice to treat parallel channels individually?\n>\n\nI think that should work and especially when including fees to the cost\nfunction and considering how nodes handle routing requests on parallel\nchannels we might have to do so anyway. The suggested flows will probably\nchange in a way that disfavors parallel channels even if their virtual\ncapacity is larger than an alternative single channel (see below)\n\n1.1) A payment of size 2 needs to be split into 1+1 to fit through\n> parallel channels of size 1+1. Combining the 1+1 channels into a virtual\n> channel of size 2 only complicates the code that has to do come up with\n> a MPP that doesn't over-saturate the actual channels. On the other hand,\n> I don't think the probability for the virtual channel of size 2 is more\n> realistic than reasoning about two individual channels and their\n> probabilities - but I didn't even try to see the math behind that.\n> Please prove me wrong? :)\n>\n\n* The likelihood that a 1 Satoshi capacity channel has 1 Satoshi to route\nis 1/2.\n* The likelihood that 2 channels of capacity 1 have each 1 satoshi\navailable to route is 1/2*1/2 = 1/4\n* Combining both parallel channels to one virtual channel of capacity 2 and\nasking if 2 satoshis are available to route gives a likelihood of 1/3 which\nis larger than 1/4.\n\nHowever I believe in practice one cannot just send a 2 satoshi onion and\nexpect the routing node to split the amount  correctly / accordingly\nbetween the two parallel channels. (I might be wrong here). So in that case\nmodelling and computing probabilities for parallel channels might be\nnecessary anyway though the math indicates that splitting liquidity in\nparallel channels will get you selected less frequently for routing.\n\n1.2) The Mission Control information provided by lnd can be used to\n> place a minimum available balance on each of the parallel channels. If\n> we know that node A isn't able to forward N sats to node B, we can treat\n> all parallel channels between A and B (in that direction) to have a\n> capacity of at most N-1 sats. How would this look like if we combined\n> the parallel channels into a virtual one? Note that it may still be\n> possible to route two individual payments/onions of size N-1 sats from A\n> to B, given two parallel channels with that many sats on A's side.\n>\n\nI think you talk a about a maximum available balance of a channel (and not\nmin available balance)?\nIn the case of parallel channels I am not even sure if such information is\naccurate as it is my understanding that the routing node may decide to use\nthe parallel channel to forward the amount even though the other channel\nwas specified in the onion.\nAssuming that routing nodes indeed do so we would have learnt that neither\nchannel has an effective capacity of N. So the combined virtual channel\ncould be seen as 2N-1. However if routing nodes don't locally split a\nforwarding request across both channels we would know that calaculating\nwith 2N-1 is bad as a request of N could not be fulfilled. I guess it is\nfor the implementations that support parallel channels to figure out the\ndetails here.\n\n2) Optimal Piecewise Linearization\n>\n> See Twitter [3].\n>\n> Is it worth it cutting a channel into pieces of different sizes, instead\n> of just having (as per your example) 5 pieces of the same size? If it\n> makes a noticeable difference, adding some complexity to the code might\n> be worth it.\n>\n\nI will certainly do experiments or be happy if others are faster to do them\nwhich compare the quality of the approximation with optimal piecewise\nlinearization to my choice of fixed intervals and the selection of various\nnumbers of segments. As long as we don't have numbers it is hard to guess\nif it is worthwhile adding the complexity. Looking at the current results\nit seems that my (geometricly motivated but) arbitrary choice might end up\nto be good and easy enough. However we might very well see quite an\nimprovement of the approximation if we find better piecewise linearizations.\n\n\n> 3) Size of Piecewise Linearization\n>\n> My gut feeling is that cutting a 1 BTC channel into 5 pieces is\n> different from cutting a 0.01 BTC channel into 5 pieces. Would it make\n> sense to use different values of N depending on the channel size?\n>\n\nThe main difference here is that a channel of 1 BTC is highly preferable\nfrom a probabilistic payment delivery perspective over a channel of 0.01\nBTC. Even approximating the 1 BTC channel with 1000 intervalls of 0.001 BTC\nshould still have a lower unit cost in all pieces of the first 0.01 BTC of\nthe liquidity than the first piece of the 0.01 BTC channel. So I think\nsplitting all channels in the equal number of pieces is pretty well\nmotivated but let me elaborate on this:\n\nThe motivation of splitting all channels into the same number of pieces\ncomes from the observation that from a probabilistic point of view (and\nvery roughly speaking!) we want to find a flow that puts the same (high)\nsuccess probability on all edges. Using 20% of the capacity gives an 80%\nprobability that the liquidity is available. This in turn has a 51.2%\nchance to be successful on a three hop path which in my experience is a\ngood probability to aim for as on average one is expected to need two\nattempts. Since the linearized pieces - if included in the flow - tend to\nbe fully saturated I decided that it makes sense to put them in 5 buckets\nof 20% each. If you read the code carefully I don't even approximate the\ncost correctly at the 2nd, 3rd, 4th and 5th piece. I just multiplied the\nlinearized unit cost of the first piece with 2,3,4 and 5 respectively Which\napproximates the negative log probability with a quadratic cost function.\nBut as we can see those geometrically motivated choices work already pretty\nwell. Again I plan to invest more time to find a better approximation and\nwe might end up using it. But I don't expect too much gain from doing so.\n\n If you look at the output of the flow in the iPython notebook (Which I\nwill copy to the end of the mail for your convenience) you see that most\nchannels did not fully saturate the first piece and have a likelihood\nbetween 80% and 100% where as some channels saturated the first piece\nproducing a likelihood of 80% and few channels saturated also the second\npiece giving a likelihood of 60%.\n\nThus I expect that the real value from studying the optimal piece wise\nlinear approximation will give us a better understanding of where to prune\nsegments away. If we note that in this flow not a single channel used the\nthird, forth and fifth piece we could have removed 60% of all edges and\ndoubled the runtime and still compute the same resulting flow.\n\n\n> 4) Leftovers after Piecewise Linearization\n>\n> If I cut some channel into N pieces, I might end up with up to N-1 sats\n> that don't end up in any of the N pieces, effectively making the channel\n> look smaller than it is. For smaller values of N that's obviously not an\n> issue (given the uncertainty we're dealing with), but it might be more\n> problematic if quantization is used with larger values. Any thoughts on\n> this?\n>\n\nI am not sure if I understand your question / issue here. The splitting\nworks by selecting N points on the domain of the function and splitting the\ndomain into segments at those points. This should never leave sats over.\nThe quantization which doesn't boost the runtime too much anyway happens\nbefore piecewise linearization. So as long as the domain is larger than N\nthe piecewiese linearization should not bring up a situation where sats are\nleft over. If the quantization however makes a channel so small  that we\ncannot even create 5 (or N) disjoint segments then I guess the likelihood\nfor being included into the final result is too small anyway. But I agree\nthat an actual implementation might have to watch out for such edge cases.\n\nAgain this yield interesting pruning opportunities to reduce the seize of\nthe network before doing the expensive min cost flow computation. For\nexample I could prune channels with high unit costs on the first segment.\nEspecially if they are further away from the source and destination node.\nThis would overall reduce the size of the graph and improve runtime.\nAlready last July I had a pretty well working heurist that was able to\nthrough away about 90% of all channels and would pretty much always find\nthe same flow in the pruned network as on the full network. I should really\nprioritize the pruning work again now that we have fast solvers.\n\n5) Fees (and other properties?)\n>\n> How can we integrate fees into the function? I must admit, I haven't\n> even thought about that, yet. A copy-paste answer would be great,\n> though! :) Maybe it's also a good idea to punish channels based on their\n> CLTV delta? Ratio of enabled channels? Age? Manual punishment score? ...\n>\n\nplugging in fees (assuming only channels that set a base fee of zero) is\nvery easy and straight forward.\n\nLet's recall from the code that the piecewise linearized unit cost is\ncomputed as follows\n\nunit_cost = int(max_cap/cap)for i in range(N):\n     #arc format is src, dest, capacity, unit_cost\n     arcs.append((src,dest,int(cap/(N*QUANTIZATION)),(i+1)*unit_cost))\n\nAs described in our paper for a good (and to be determined) value of \\mu we\ncould just create the linear combination between the two features which\nwould change the line to:\n\n     arcs.append((src,dest,int(cap/(N*QUANTIZATION)),(i+1)*unit_cost +\nmu*fee_rate_ppm))\n\nNote two things:\n1. the only requirement for the solver to work is that \\mu*fee_rate_ppm\nneeds to be an integer. So in case \\mu was smaller than 1 we could also\nscale the term from the linearized log probabilities by putting a larger mu\nto the feature arising from the cost of the uncertainty.\n\n     arcs.append((src,dest,int(cap/(N*QUANTIZATION)),mu*(i+1)*unit_cost\n+ fee_rate_ppm))\n\n\n2. the cost from the routing fees is the same on each segment of the\npiecewise linearization which makes a lot of sense because the current\nfeerate is indeed a unit cost that does not change with how heavily you\nplan to use a channel independently how the cost that comes from the\nprobability grows.\n\nWith respect to other features I guess the entire topic of feature\nengineering for our cost function should be a separate threat / topic and\nline of research but as you asked I will give you some short thoughts on\nthis here:\n\nAs pointed out to the c-lightning team in\nhttps://github.com/ElementsProject/lightning/pull/4771#issuecomment-930173831\nand\nthe following comment I believe that optimizing for CLTV is a poor choice\nand in min cost flow computations it might very well be non linear anyway\nand thus tricky to include in a meaningful way.Sure one could transform it\nto a unit cost by doing something like CLTV*max_cap/cap and add it to the\ncost function like the ppm. But I still do not really see why this is\nuseful. On the other hand I very much believe we should start to\ninvestigate features that predict channel latency to setup / settle an HTLC\nas suggested in the last paragraph of this comment\nhttps://github.com/lightningdevkit/rust-lightning/issues/1170#issuecomment-972396747\nHowever\nI fear that latency measures again are non linear though they could again\nbe translated to a unit cost with the same trick as the CLTV.\n\n\n> 6) Non-Zero Base Fee\n>\n> See Twitter [4].\n>\n> According to Stefan [5] it should be possible to integrate ZmnSCPxj's ideas\n> to make this work with non-zero base fees. How?\n> Simpler approach: Twitter [6].\n>\n\nI don't have much to add to the base fee discussion at this point besides\nthe emphasize that the above described linearization trick for CLTV or\nlatency based features will not properly work for the base fee because one\nactually has to pay the full base fee at the end - independently of how\nmuch you saturate the channel. This might mess up the optimization.\n\n\n> 7) Private Channels\n>\n> [very niche topic, not really that interesting nor urgent]\n>\n> I'm a fan of adding private channels to provide more outbound liquidity,\n> mainly to reduce gossip and hide my intentions. If my total liquidity to\n> some peer is below the amount announced in public channels, I don't see\n> any meaningful complication. However, I might have a public channel of\n> size N and several private channels bringing my local liquidity to some\n> value >N. It's rather obvious that not announcing this fact is a bad\n> idea, as any #pickhardtpayments implementation would think I have 0-N on\n> my side of the channel(s). Assuming I'm willing to accept this tradeoff,\n> do you see other complications or issues with hidden liquidity?\n>\n> My gut feeling is that this isn't an issue, at all, as channel balances\n> change all the time, which is something the algorithm already has to\n> deal with.\n>\n\nAs you noted from a probabilistic payment delivery point of view I might be\ninterested in signaling all the liquidity that is being provided between\ntwo peers and I shoot myself if I hide it. That being said you might have\nreasons to do so and additionally I never rejected the idea to extend the\ncurrent probabilistic model with a probabilistic node or channel provenance\nvalue that would be similar to the ideas in lnd's mission control or the\nroutescore by https://lnrouter.app. Given the fact that with hidden\nliquidity the probabilities are constantly underestimated such a provenance\nscore will probably be higher than the one of other channels fixing the\n\"introduced issue\" of hidden liquidity\n\n8) Quality of Approximation\n>\n> There are some problems in computer science that are hard/impossible to\n> approximate, in the sense that any kind of deviation from the optimum\n> could cause the computed results to be extremely bad. Do you have some\n> idea (or proof) that your kind of approximation isn't causing a major\n> issue? I guess a piece-wise linearization with an infinite number of\n> pieces corresponds to the optimal result. Given a finite number of\n> pieces, how large is the difference to the optimum?\n>\n\nWe don't have to go to infinite to find a solution without error. We can\nstay with the finite number that corresponds to the channels capacity and\nmake segments of 1 satoshi each encoding what this satoshi would actually\ncost in the original function (assuming all values in the original function\nwere integers).\n\nI have not spent the time to properly express the error in dependence of\nthe number of segments N or even empirically study its tradeoffs in\npractice (as I haven't even included the optimal piecewise linearization\nyet) However the actual flow (see below) as well as the results from Dr.\nMartin Berger\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2022-March/003513.html\nindicate\nthat the error should be possible to be managed and stay small enough in\npractice. That being said and as above it certainly makes sense to invest a\nbit of time on how to conduct the piecewise linearization properly.\n\nOutput of the computed flow from the iPython notebok with a piece wise\nlinearization of 5 equal sized segments per channel\n\n\nPlanning to deliver 0.50 BTC from 5051(03efccf...) to 13006\n(021c97a...) via an approximated optimally reliable payment flow...\n\nRuntime of flow computation: 0.85 sec\nMinimum approximated quadratic cost:  815932\n\n Arc \t\t\t      Flow / Capacity \tprobability \tFee (sats)\n5051 -> 8463     \t  6700000 / 16777215 \t0.600649\t8957.900000\n5051 -> 3437     \t  1800000 / 9000000 \t0.800000\t2406.600000\n5051 -> 9162     \t  1240000 / 6200000 \t0.800000\t1657.880000\n5051 -> 14746     \t  6700000 / 16777215 \t0.600649\t8957.900000\n5051 -> 14832     \t  2000000 / 10000000 \t0.800000\t2674.000000\n6257 -> 14832     \t  3350000 / 2411344242 \t0.998611\t335.100000\n14832 -> 12446     \t  5350000 / 6200000000 \t0.999137\t6.350000\n7870 -> 6257     \t  3350000 / 20000000 \t0.832500\t34.500000\n14746 -> 12446     \t  6700000 / 100000000 \t0.933000\t3350.000000\n7914 -> 13006     \t  6700000 / 200000000 \t0.966500\t33501.000000\n5051 -> 550     \t  3300000 / 15000000 \t0.780000\t4412.100000\n11396 -> 13192     \t  6700000 / 1000000000 \t0.993300\t2010.000000\n5051 -> 11396     \t  6700000 / 16777215 \t0.600649\t8957.900000\n7199 -> 9162     \t  3350000 / 445000000 \t0.992472\t848.550000\n9162 -> 12446     \t  16590000 / 3900000000 \t0.995746\t17.590000\n5051 -> 13287     \t  2000000 / 10000000 \t0.800000\t2674.000000\n5051 -> 1843     \t  6700000 / 16777215 \t0.600649\t8957.900000\n5051 -> 11257     \t  3350000 / 16777215 \t0.800324\t4478.950000\n1953 -> 12446     \t  2160000 / 750000000 \t0.997120\t1080.000000\n550 -> 1843     \t  3300000 / 30000000 \t0.890000\t1155.000000\n12756 -> 12446     \t  2000000 / 16775679 \t0.880780\t301.000000\n5051 -> 12756     \t  2000000 / 10000000 \t0.800000\t2674.000000\n3437 -> 12446     \t  1800000 / 400000000 \t0.995500\t225.001000\n6713 -> 7914     \t  6700000 / 200000000 \t0.966500\t837.501000\n11257 -> 7199     \t  3350000 / 16777215 \t0.800324\t663.300000\n10914 -> 9162     \t  2000000 / 500000000 \t0.996000\t1000.001000\n5051 -> 7870     \t  3350000 / 16777215 \t0.800324\t3.781000\n8463 -> 5288     \t  6700000 / 500000000 \t0.986600\t837.501000\n2781 -> 10914     \t  2000000 / 500000000 \t0.996000\t401.000000\n13192 -> 6713     \t  6700000 / 500000000 \t0.986600\t16750.000000\n32 -> 2781     \t  2000000 / 210000000 \t0.990476\t200.000000\n5051 -> 9530     \t  2160000 / 10811137 \t0.800206\t2.591000\n9530 -> 1953     \t  2160000 / 16777215 \t0.871254\t438.336000\n1843 -> 9162     \t  10000000 / 300000000 \t0.966667\t2500.000000\n5051 -> 14642     \t  2000000 / 10000000 \t0.800000\t2.431000\n14642 -> 13006     \t  2000000 / 50000000 \t0.960000\t6001.000000\n13287 -> 32     \t  2000000 / 16777215 \t0.880791\t500.000000\n12446 -> 13006     \t  34600000 / 200000000 \t0.827000\t103766.400000\n5288 -> 13006     \t  6700000 / 100000000 \t0.933000\t15216.700000\n\nProbability of entire flow: 0.0032\nTotal fee: 248793.763 sats\nEffective fee rate: 0.498 %\nArcs included in payment flow: 39\n\nDon't get confused by a low probability. The first attampt always has\nhigh uncertainty. We will learn fast in each consequitive round.\n\n\nWith kind regards Rene Pickhardt\n\n-- \n> Dr. Carsten Otto\n> carsten at c-otto.de\n> https://c-otto.de\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n\n\n-- \nhttps://www.rene-pickhardt.de\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220314/77da14bd/attachment-0001.html>"
            },
            {
                "author": "Carsten Otto",
                "date": "2022-03-14T12:53:18",
                "message_text_only": "Hi Rene,\n\nOn Mon, Mar 14, 2022 at 12:56:24PM +0100, Ren\u00e9 Pickhardt via Lightning-dev wrote:\n> > 1) What's the reasoning behind combining parallel channels?\n> I think that should work and especially when including fees to the\n> cost function and considering how nodes handle routing requests on\n> parallel channels we might have to do so anyway.\n\nI think lnd accepts the minimum fee of all parallel channels, even if\na higher fee rate is configured for the channel that is actually used.\nI'm not too sure about this, though. Having parallel channels with\ndifferent fee settings seems weird to me, anyway.\n\n> 1.1) A payment of size 2 needs to be split into 1+1 to fit through\n> However I believe in practice one cannot just send a 2 satoshi onion and\n> expect the routing node to split the amount  correctly / accordingly\n> between the two parallel channels. (I might be wrong here).\n\nExactly. This kind of split could be possible in theory, but at least\nlnd doesn't do it. I guess there are lots of interesting questions to\nanswer before this becomes reality (channel jamming?).\n\n> So in that case\n> modelling and computing probabilities for parallel channels might be\n> necessary anyway though the math indicates that splitting liquidity in\n> parallel channels will get you selected less frequently for routing.\n\nWhich is OK, as in reality it IS less likely to succeed.\n\n> 1.2) The Mission Control information provided by lnd [...]\n> I think you talk a about a maximum available balance of a channel (and not\n> min available balance)?\n\nYes, although MC also has information about \"known\" amounts (due to\nfailures that only happened further down the road).\n\n> In the case of parallel channels I am not even sure if such information is\n> accurate as it is my understanding that the routing node may decide to use\n> the parallel channel to forward the amount even though the other channel\n> was specified in the onion.\n\nThe routing node is free to pick any of the parallel channels, yes. The\nMC data only reasons about pairs of nodes, though, not individual\nchannels.\n\n> Assuming that routing nodes indeed do so we would have learnt that neither\n> channel has an effective capacity of N. So the combined virtual channel\n> could be seen as 2N-1.\n\nYou mean 2(N-1) = 2N-2?\n\n> However if routing nodes don't locally split a\n> forwarding request across both channels we would know that calaculating\n> with 2N-1 is bad as a request of N could not be fulfilled.\n\nExactly, also for 2N-2. Only N-1 would be a reasonable assumption.\n\nBased on your responses I'll treat parallel channels individually, and\nsee how it works out.\n\n> > 3) Size of Piecewise Linearization\n> The main difference here is that a channel of 1 BTC is highly preferable\n> from a probabilistic payment delivery perspective over a channel of 0.01\n> BTC. Even approximating the 1 BTC channel with 1000 intervalls of 0.001 BTC\n> should still have a lower unit cost in all pieces of the first 0.01 BTC of\n> the liquidity than the first piece of the 0.01 BTC channel. So I think\n> splitting all channels in the equal number of pieces is pretty well\n> motivated but let me elaborate on this:\n\nOK great, got it.\n\n> > 4) Leftovers after Piecewise Linearization\n> I am not sure if I understand your question / issue here. The splitting\n> works by selecting N points on the domain of the function and splitting the\n> domain into segments at those points. This should never leave sats over.\n\nWith quantization of 10,000 a channel of size 123,456 ends up as an arc\nwith a capacity of 12 units. Cutting this into 5 pieces gives us\n5*2 with 2 units not ending up in any of those pieces. Or am I missing\nsomething here, and we should split into 5 pieces of size 2.4 = 12/5?\n\n> If the quantization however makes a channel so small  that we cannot\n> even create 5 (or N) disjoint segments then I guess the likelihood for\n> being included into the final result is too small anyway.\n\nIt may not be very likely, but flat-out ignoring 20k sat (in my\ncontrived example above) or up to 4*quantization sats (which is the case\nyou described) doesn't feel right.\n\n> Again this yield interesting pruning opportunities to reduce the seize of\n> the network before doing the expensive min cost flow computation. For\n> example I could prune channels with high unit costs on the first segment.\n> Especially if they are further away from the source and destination node.\n> This would overall reduce the size of the graph and improve runtime.\n\nLet's talk about optimizations later :)\n\n> 5) Fees (and other properties?)\n>      arcs.append((src,dest,int(cap/(N*QUANTIZATION)),(i+1)*unit_cost +\n> mu*fee_rate_ppm))\n\nGreat, that helps! Thanks alot!\n\n> Note two things:\n> 1. the only requirement for the solver to work is that \\mu*fee_rate_ppm\n> needs to be an integer. So in case \\mu was smaller than 1 we could also\n> scale the term from the linearized log probabilities by putting a larger mu\n> to the feature arising from the cost of the uncertainty.\n\nGood to know!\n\nBye,\nCarsten\n-- \nDr. Carsten Otto\ncarsten at c-otto.de\nhttps://c-otto.de\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 195 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220314/2418fb43/attachment.sig>"
            },
            {
                "author": "Ren\u00e9 Pickhardt",
                "date": "2022-03-14T17:46:57",
                "message_text_only": "Dear Carsten and fellow lightning developers,\n\nthanks for going into such detail and discovering some of the minor\ninaccuracies of my very rough piecewise linearization!\n\nOn Mon, Mar 14, 2022 at 1:53 PM Carsten Otto <bitcoin at c-otto.de> wrote:\n\n> 1.2) The Mission Control information provided by lnd [...]\n> > I think you talk a about a maximum available balance of a channel (and\n> not\n> > min available balance)?\n>\n> Yes, although MC also has information about \"known\" amounts (due to\n> failures that only happened further down the road).\n>\n\nI am unsure how mission control stores and handles that data. In my\nunderstanding they are mainly interested in a statistic of the ratio of\nsuccessfull payments over the past X attempts on a channel given a certain\ntime interval. But I assume they should have all the relevant data to\nproduce a proper conditional proability to utilize our learnt knowledge.\n\nIn any case from the probabilistic model we can do it mathematically\nprecise by just looking at the conditional probabilities. As said I have\nwritten hands on instructions in the rust repo\nhttps://github.com/lightningdevkit/rust-lightning/issues/1170#issuecomment-972396747\nand\nthey have been fully implemented in\nhttps://github.com/lightningdevkit/rust-lightning/pull/1227. Also in our\nmainnet test and simulations we have updated the priors according to those\nrules and this revealed the full power of the approach.\n\nTo summarize: Basically we need to know the effective uncertainty by only\nlooking at the effective amount that goes above the minimum certain\nliquidity (that we might know from a prior attempt) and the effective\ncapacity (somebody recently suggested that conditional capacity might be a\nbetter wording)\n\n> Assuming that routing nodes indeed do so we would have learnt that neither\n> > channel has an effective capacity of N. So the combined virtual channel\n> > could be seen as 2N-1.\n>\n> You mean 2(N-1) = 2N-2?\n>\n\nProbably though the difference would be neglectable and if I understood you\ncorrectly you will just keep parallel channels separate anyway.\n\n> > 4) Leftovers after Piecewise Linearization\n> > I am not sure if I understand your question / issue here. The splitting\n> > works by selecting N points on the domain of the function and splitting\n> the\n> > domain into segments at those points. This should never leave sats over.\n>\n> With quantization of 10,000 a channel of size 123,456 ends up as an arc\n> with a capacity of 12 units. Cutting this into 5 pieces gives us\n> 5*2 with 2 units not ending up in any of those pieces. Or am I missing\n> something here, and we should split into 5 pieces of size 2.4 = 12/5?\n>\n\nYour observation is correct! Indeed I think my code rounds down the\ncapacity instead of going to the correct points and using all of the\ncapacity in the segmentation by making some channels 1 unit larger than\nothers which would happen if actually finding points on the domain to build\nthe segments. This could easily be fixed. However as always: Fully\nsaturated channels mean very low probabilities so even in my situation\nwhere I may cut off a significant part of the channel I'd say in the\nextreme case where we would need to saturate even those sats the flow will\nand should most likely fail as the min cust is probably just lower than the\namount we would attempt to send. Probably opening a new channel or doing an\non chain transaction will be more useful. Though of course we should build\nthe piecewise linearization correctly by the end of the day without\nthroughing away some capacity.\n\n> If the quantization however makes a channel so small  that we cannot\n> > even create 5 (or N) disjoint segments then I guess the likelihood for\n> > being included into the final result is too small anyway.\n>\n> It may not be very likely, but flat-out ignoring 20k sat (in my\n> contrived example above) or up to 4*quantization sats (which is the case\n> you described) doesn't feel right.\n>\n\nSee above. I agree it is not 100% accurate. but in practice I doubt it\nwould ever become a problem as this will only be an issue when the payment\namount is very close to the min cut which would make flows so unlikely to\nbegin with that we would use other ways to conduct the payment anyway.\n\nwitch kind regards Rene\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220314/0f59c122/attachment.html>"
            },
            {
                "author": "Martin",
                "date": "2022-03-19T21:09:59",
                "message_text_only": "Dear Carsten, Rene and fellow lightning developers,\n\nRegarding the approximation quality of the minimum convex cost flow formulation for multi-part payments on the lightning network [1] and Carsten's discussion points on Twitter [2] and on the mailing list:\n\n> 8) Quality of Approximation\n>\n> There are some problems in computer science that are hard/impossible to\n> approximate, in the sense that any kind of deviation from the optimum\n> could cause the computed results to be extremely bad. Do you have some\n> idea (or proof) that your kind of approximation isn't causing a major\n> issue? I guess a piece-wise linearization with an infinite number of\n> pieces corresponds to the optimal result. Given a finite number of\n> pieces, how large is the difference to the optimum?\n\nI did some literature research and came across an insightful paper [3] by Dorit Hochbaum from 1993, that proves proximity results for integer and continuous optimal solutions of the minimum convex cost flow as well as proximity results of the optimal solutions for a piecewise linear approximation and the original problem.\n\nAdmittedly theoretical results, however, it further underpins that a piecewise linear approximation is a reasonable approach to find optimal flows and even shows that searching for optimal solutions on the continuous domain (e.g. with descent methods from convex optimization) also gives near-optimal solutions on the integer domain.\n\nCheers,\nMartin\n\n[1] https://arxiv.org/abs/2107.05322\n[2] https://twitter.com/renepickhardt/status/1502293438498234371\n[3] https://www.worldscientific.com/doi/abs/10.1142/9789812798190_0005\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220319/f278c153/attachment.html>"
            },
            {
                "author": "Stefan Richter",
                "date": "2022-03-20T09:07:01",
                "message_text_only": "Good morning everyone,\n\nwith regards to zerobasefee, I think that the argument that HTLCs are\ncostly doesn't quite hold up because they are always free to an\nattacker as it stands. However, I fully agree with Zmn's opinion that\nit's not necessary to bang our head against any opposition to this\nbecause we can simply follow his excellent method for overweighing\nbase fee. I believe this is a very natural approach to let the market\ndecide on the relative importance of optimized routing vs base fees.\n\nAs to Martin's approximation research, I have asked myself similar\nquestions. Unfortunately, the paper you cite is paywalled and not\navailable at sci-hub, so I haven't read it. FWIW, I believe I have a\nsimple proof that minimum cost flow preserves approximation FACTORS:\n\nLet O be the original problem and A the approximated problem such that\nevery flow in O can be mapped 1:1 to a flow in A and vice versa. Let\nevery edge e in O be represented by a set of edges in A whose total\ncost is within a factor (1+epsilon) for every possible flow (could be\nover- or underestimating). Note that this means that every flow in O\nhas cost within a factor (1+epsilon) for the corresponding flow in A\nand vice versa.\n\nNow let f_a be the min cost flow in A and f_o the min cost flow in O.\nAssume that c(f_o)(1+epsilon)<c(f_a). Then f_o corresponds to a flow\nin A that is cheaper than f_a, but that's impossible because f_a is\nthe min cost flow.\nQED.\n\nProblems might arise anyway because we represent probabilities only\nlogarithmically in the cost, so that a factor of (1+epsilon)\ncorresponds to an exponent (1+epsilon) for the probabilities. But Ren\u00e9\nseems optimistic that the resulting flows look good enough in\npractice.\n\nI am still optimistic that exact solvers with something like the cost\nscaling approach might also be feasible (as long as they produce\ninteger flows), but I am happy that this simple approximation approach\nseems good enough. This should save us a lot of work because there are\nmany linear min cost solvers available that represent years of\ncumulative work in optimization research.\n\nCheers\n  Stefan\n\nAm Sa., 19. M\u00e4rz 2022 um 22:09 Uhr schrieb Martin via Lightning-dev\n<lightning-dev at lists.linuxfoundation.org>:\n>\n> Dear Carsten, Rene and fellow lightning developers,\n>\n> Regarding the approximation quality of the minimum convex cost flow formulation for multi-part payments on the lightning network [1] and Carsten's discussion points on Twitter [2] and on the mailing list:\n>\n> > 8) Quality of Approximation\n> >\n> > There are some problems in computer science that are hard/impossible to\n> > approximate, in the sense that any kind of deviation from the optimum\n> > could cause the computed results to be extremely bad. Do you have some\n> > idea (or proof) that your kind of approximation isn't causing a major\n> > issue? I guess a piece-wise linearization with an infinite number of\n> > pieces corresponds to the optimal result. Given a finite number of\n> > pieces, how large is the difference to the optimum?\n>\n> I did some literature research and came across an insightful paper [3] by Dorit Hochbaum from 1993, that proves proximity results for integer and continuous optimal solutions of the minimum convex cost flow as well as proximity results of the optimal solutions for a piecewise linear approximation and the original problem.\n>\n> Admittedly theoretical results, however, it further underpins that a piecewise linear approximation is a reasonable approach to find optimal flows and even shows that searching for optimal solutions on the continuous domain (e.g. with descent methods from convex optimization) also gives near-optimal solutions on the integer domain.\n>\n> Cheers,\n> Martin\n>\n> [1] https://arxiv.org/abs/2107.05322\n> [2] https://twitter.com/renepickhardt/status/1502293438498234371\n> [3] https://www.worldscientific.com/doi/abs/10.1142/9789812798190_0005\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "Martin",
                "date": "2022-03-21T13:59:32",
                "message_text_only": "Hello Stefan, hello Lightning Devs,\n\n> As to Martin's approximation research, I have asked myself similar\n>\n> questions. Unfortunately, the paper you cite is paywalled and not\n>\n> available at sci-hub, so I haven't read it.\n\nSorry for the paywalled link, without paywall you can read the article on Google Books preview:\n\nhttps://books.google.de/books?id=hbXsCgAAQBAJ&pg=PA63&source=gbs_toc_r#v=onepage\n\nI like your neat and simple proof idea for approximation preservation!\n\nCheers, Martin"
            },
            {
                "author": "Martin",
                "date": "2022-03-14T08:15:38",
                "message_text_only": "Dear Lightning Developer, Rene & Carsten,\n\nThe min-cost flow formulation for MPP of Rene and Stefan [1] intrigued me and brought me to think about how this optimization problem can be solved efficiently in order to contribute to practically relevant reliable payments on the Lightning network. Applying linear approximation to the convex non-linear cost function C(f) brings runtime improvements [2], however an approximation can deteriorate the solution quality.\n\nThis brought me to think about the approximation quality/error of a piecewise linear approximation for the cost function C(f) and how that translate to the original success probability of a flow P(f). After some back and forth with Rene over the last couple of days, I summarized some preliminary insights in the following:\n\n[3] https://raw.githubusercontent.com/drmartinberger/mpp-approx-pf/main/approxPf.pdf\n\nThe main (admittedly, mostly theoretical) outcome is, that a piecewise linear approximation of C(f) with given approximation error, also gives an approximation of the function P(f) with lower/upper bound.\nHowever, [3] further underpins the \"goodness\" of the approach [1] and might address some of Carsten's concerns [4] and of the previous post of Carsten (especially 8) ).\n\nFeedback of any kind is warmly welcome and feel free to reach out in case of questions.\n\nThank you!\n\nCheers,\nMartin\n\n[1] https://arxiv.org/abs/2107.05322\n[2] https://github.com/renepickhardt/mpp-splitter/blob/master/Minimal%20Linearized%20min%20cost%20flow%20example%20for%20MPP.ipynb\n[3] https://raw.githubusercontent.com/drmartinberger/mpp-approx-pf/main/approxPf.pdf\n[4] https://twitter.com/c_otto83/status/1502329970349248521"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-16T15:27:00",
                "message_text_only": "Good morning Rene, sorry for the lateness,\n\n> Last but not least, please allow me to make a short remark on the (still to me very surprisingly controversial) base fee discussion: For simplicity I did not include any fee considerations to the published code (besides a fee report on how expensive the computed flow is). However in practice we wish to optimize at least for high reliability (via neg log success probabilities) and cheap fees which in particular with the ppm is very easily possible to be included to the piece wise linearized cost function. While for small base fees it seems possible to encode the base fee into the first segment of the piecewise linearized approximation I think the base fee will still be tricky to be handled in practice (even with this approximation). For example if the base fee is too high the \"base fee adjusted\" unit cost of the first segment of the piecewise linearized problem might be higher than the unit cost of the second segment which effectively would break the convexity. Thus I reiterate my earlier point that from the perspective of the year long pursued goal of optimizing for fees (which all Dijkstra based single path implementations do) it seems to be best if the non linearity that is introduced by the base fee would be removed at all. According to discussions with people who crate Lightning Network explorer (and according to my last check of gossip) about 90% of channels have a base fee of 1 sat or lower and ~38% of all channels already set their base fee away from the default value to 0 [16].\n\nI think the issue against 0-base-fee is that, to a forwarding node operator, every HTLC in-flight is a potential cost center (there is always some probability that the channel has to be forced onchain with the HTLC in-flight, and every HTLC has to be published on the commitment tx), and that cost is *not* proportional to the value of the HTLC (because onchain fees do not work that way).\nThus, it seems reasonable for a forwarding node to decide to pass on that cost to their customers, the payers, in the form of base fees.\n\nThe response of customers would be to boycott non-0-base fees, by e.g. using a heuristic that overweighs non-0-base-fee and reducing usage of such channels (but if every forwarding node *has* a base fee, going through them anyway, which is why you just overweigh them, not eliminate them from the graph outright).\nThen forwarding nodes will economically move towards 0-base fee.\n\nSo I think you would find it impossible to remove the base fee field, but you can strongly encourage 0-base-fee usage by integrating the base fee but overweighted.\n(I think my previous formulation --- treat the base fee as a proportional fee --- would do some overweighing of the base fee.)\n\nWhich reminds me, I have not gotten around to make a 0-base-fee flag for `clboss`, haha.\nAnd I might need to figure out a learning algorithm that splits base and proportional fees as well, *sigh*.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Code for sub second runtime of piecewise linarization to quickly approximate the minimum convex cost flow problem (makes fast multi part payments with large amounts possible)",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Carsten Otto",
                "Martin",
                "Ren\u00e9 Pickhardt",
                "Stefan Richter",
                "ZmnSCPxj"
            ],
            "messages_count": 10,
            "total_messages_chars_count": 55364
        }
    },
    {
        "title": "[Lightning-dev] The Eye of Satoshi (rust-teos) alpha release",
        "thread_messages": [
            {
                "author": "Sergi Delgado Segura",
                "date": "2022-03-11T16:54:33",
                "message_text_only": "Hello list,\n\nAfter some months working on this, today I'm releasing the alpha for\nrust-teos, the rust version of The Eye of Satoshi (a lightning watchtower).\n\nThe project is a port of the original Python's codebase I was working on\nlast year, with an improved design, way less boilerplate code, and improved\nperformance. This is partly thanks to Rust itself, but also to LDK. The\nolder codebase was built on top of Core, while the new one leverages some\nof the heavy lifting to the dev kit.\n\nThe current functionality is at a similar stage (if not the same) where the\nPython's codebase was left. An HTTP API is offered for the interaction with\nthe user and the tower, and the internal interface is built using gRPC. The\ncode also includes a CLI for the tower admin to interact with the backend.\nCurrently there's no client for this, the old c-lightning's watchtower\nplugin needs to be updated to work with the new tower, which I'll be\nworking on next, so if you were using the old tower do not switch yet!\n\nThe codebase will benefit from some additional reviews, as well as some\ncontributors. Hi you, dev looking for a FOSS project to contribute to :D.\n\nIn any case, if anyone is interested in giving it a look, the code can be\nfound at: https://github.com/talaia-labs/rust-teos (there's also an\narchitecture diagram at https://github.com/talaia-labs/rust-teos/issues/30).\n\nBest,\n-- \nSergi Delgado.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220311/4f2612f8/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "The Eye of Satoshi (rust-teos) alpha release",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Sergi Delgado Segura"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1583
        }
    },
    {
        "title": "[Lightning-dev] [bitcoin-dev] Removing the Dust Limit",
        "thread_messages": [
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2022-03-12T13:02:24",
                "message_text_only": "> We should remove the dust limit from Bitcoin.\n\nAny node operator can do that. Just put \"dustrelayfee=0.00000000\" in your bitcoin.conf.\n\nAnd there is more: you can also conditionally allow free transactions:\n\nmintxfee=0.00000001\nminrelaytxfee=0.00000000\nblockmintxfee=0.00000000\n\nThen, when using getblocktemplate you will get transactions with the highest fees first anyway, and you include cheap or free transactions in the end, if there will be enough room for them.\n\nSo, all of those settings are in the hands of node operators, there is no need to change the source code, all you need is to convince nodes to change their settings.\n\n\nOn 2021-08-08 20:53:28 user Jeremy via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\nWe should remove the dust limit from Bitcoin. Five reasons:\n\n\n1) it's not our business what outputs people want to create\n2) dust outputs can be used in various authentication/delegation smart contracts\n3) dust sized htlcs in lightning (https://bitcoin.stackexchange.com/questions/46730/can-you-send-amounts-that-would-typically-be-considered-dust-through-the-light) force channels to operate in a semi-trusted mode which has implications (AFAIU) for the regulatory classification of channels in various jurisdictions; agnostic treatment of fund transfers\u00a0would simplify this (like getting a 0.01 cent dividend check in the mail)\n4) thinly divisible colored coin protocols might make use of sats as value markers for transactions.\n5) should we ever do confidential transactions we can't prevent it without compromising\u00a0privacy / allowed transfers\n\n\nThe main reasons I'm aware of not allow dust creation is that:\n\n\n1) dust is spam\n2) dust fingerprinting attacks\n\n\n1 is (IMO) not valid given the 5 reasons above, and 2 is preventable by well behaved wallets to not redeem outputs that cost more in fees than they are worth.\n\n\ncheers,\n\n\njeremy"
            }
        ],
        "thread_summary": {
            "title": "Removing the Dust Limit",
            "categories": [
                "Lightning-dev",
                "bitcoin-dev"
            ],
            "authors": [
                "vjudeu at gazeta.pl"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1881
        }
    },
    {
        "title": "[Lightning-dev] Transcript: Lightning Network in 2022 panel",
        "thread_messages": [
            {
                "author": "Michael Folkson",
                "date": "2022-03-18T09:04:55",
                "message_text_only": "Hi\n\nI thought I'd start posting transcripts that may be of interest to this mailing list.\n\nWe had a panel discussion on various topics in London before Advancing Bitcoin with Christian Decker (c-lightning), Bastien Teinturier (eclair) and Oliver Gugger (LND).\n\nThe transcript is here: https://github.com/bitcointranscripts/bitcointranscripts/blob/master/london-bitcoin-devs/2022-03-01-lightning-panel.md\n\nIt will eventually make it onto the btctranscripts.com website and there will be a video up at some point too.\n\nChristian did a demo of and discussed Greenlight (onboarding new Lightning nodes that run on external infrastructure whilst retaining control of keys), we contrasted the different approaches of running a Lightning node with the different Lightning implementations, discussed priorities of the individual implementations in the coming year and covered recent tensions in the spec (BOLT) process.\n\nThanks\nMichael\n\n--\nMichael Folkson\nEmail: michaelfolkson at [protonmail.com](http://protonmail.com/)\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220318/254c2014/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Transcript: Lightning Network in 2022 panel",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Michael Folkson"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1277
        }
    },
    {
        "title": "[Lightning-dev] A Proposal for Adding Bandwidth Metered Payment to Onion Messages",
        "thread_messages": [
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2022-03-23T00:25:40",
                "message_text_only": "Hi Rusty,\n\n> AMP seems to be a Lightning Labs proprietary extension.  You mean keysend,\n> which at least has a draft spec?\n\nI'm confused: the keyspend specification lives today as a very succinct bLIP\n[1] (use this record that contains the preimage settle the payment), and the\nAMP spec has had an open PR [2] (which admittedly needs to be mega-rebased)\nfor sometime now and is amongst the oldest PRs in the BOLT tracker. Your\ndefinition of a \"proprietary extension\" here would seem to cover just about\nevery open BOLT pull request that hasn't yet been widely adopted by multiple\nimplementations.\n\nWith that aside, AMP is used here instead of keysend as each payment split\nuses a different payment hash, which makes it harder to correlate payment\nsplits as both the amount and hash will differ. Also given that the e2e\npayment needs to drop off additional fees (to pay for the forwarding pass)\nto each hop, the receiver doesn't actually know how much to expect as total\npayment (no invoice exists in this scenario). With keysend, it's possible\nthe receiver accidentally pulls early, with the funds irrevocably sent,\nwhile no true onion messaging session has been constructed. With AMP the\nsender is the one that controls _when_ all the funds can be pulled, which\neliminates such edge cases.\n\n> Sure, let's keep encouraging people to use HTLCs for free to send data?  I\n> can certainly implement that if you prefer!\n\nI think the difference here is that HTLCs aren't that great for sending\ndata: any space you allocate to arbitrary data takes away from the total\nroute length. Also given the existence of the max_htlc parameters in channel\nupdates, nodes can increase this value which allows them to discourage\ncertain classes of behavior, as the HTLC that transmits the data serves\neffectively as an ephemeral bond, meaning that HTLC data transfer attempts\nincur an opportunity cost w.r.t the capital locked up.\n\nAs a result, what's deployed in the wild today that uses HTLCs to send data\n(importantly at no additional marginal cost, since you sling 1.3KB across\nthe wire anyway for normal payments), imo is mostly cases that only need a\nsmaller amount of data, or use it mainly as a signalling layer. In contrast,\nonion messaging would allow users to send 32 KB (and beyond?), which makes\ncertain classes of applications more feasible.\n\n> I suggest naively limiting to 10/sec for peers with channels, and 1/sec\n> for peers without for now.\n\nCurious to see what a fleshed out version of this would look like. IMO if\nthis rate limiting event doesn't somehow make it back to the sender, then it\nisn't clear exactly why they're unable to even fetch an invoice to _attempt_\na payment. I think one issue with defaulting to always fetching invoices in\na multi-hop manner is that a node failure may mean that a user can't even\nobtain an invoice, compared to just hitting an API or scanning a QR code\nthat encodes one (pretty much 99% chance of success).\n\n(I think there's also a side discussion here w.r.t if tightly coupling an\ninvoice/offer to a single node is compatible with the multi-node receive\narchitectures that some services are beginning to develop/deploy, but we can\nmake another thread for that.)\n\n> You mean:\n> 1. Don't use an even TLV field.\n\nI don't see why one _wouldn't_ use an even field here: if the node doesn't\nknow what this means, then I want them to reject the payment, as otherwise\nthey aren't able to properly do the bandwidth metering. Even assuming a\nfeature bit, I think an even field would make sense, but ofc I'm open to\nfurther reasoning.\n\n> I think it's a premature optimization?  Make standard duration 2016\n> blocks; then they can request multiples if they want?  Reduces\n> node_announcement size.\n\nHmm, potentially but if I just need this to ask a node about their latest\nephemeral onion service, do I really need the session to persist for 2016\nblocks? At just 8 bytes here, it's smaller than a tor v3 onion address and\neven most feature bit distributions. Also there's nothing stopping someone\nfrom advertising optional feature bit 12,324 which means that nodes need to\nlug around and store all those extra bytes.\n\n> I'd be tempted to use 16 bytes?  Collisions here are not really a thing\n> since you'd need a network packet per probe, and you're time limited.\n\nYeah if the 32 bytes for each hop ends up eating too much into the total\nroute length, then most use cases can get by with 16 bytes.\n\n> AFAICT this is easy to implement on top of onion_messages as they stand\n> today (if you don't want to fwd free onion messages at all, don't set\n> that bit?).\n\nCorrect! The idea is to give nodes that don't want to forward for free an\noption to charge for forwarding and supplement their existing routing node\nrevenue.\n\nOn Twitter, Lisa brought up that as the data forwarding and payment aren't\n\"atomic\", you're reliant on the nodes to actually \"do the thing\" once the\npayment has been completed. This is correct, and imo can be mitigated mainly\nvia a \"tit for tat approach\", so paying for a smol session and seeing if\nthey actually cooperate. Incentive wise, if the node continues to operate as\nexpected, then they can expect more messaging sessions to be created, which\nmeans mo sats as revenue. I think we're seeing a similar dynamic play out to\nday in the network, as path finding algorithms continue to evolve to factor\nin \"reliability\" information (usually some derived probability of success),\nwhich can tend to favor well managed and responsive nodes.\n\n[1]: https://github.com/lightning/blips/blob/master/blip-0003.md\n[2]: https://github.com/lightning/bolts/pull/658\n\n-- Laolu\n\n\nOn Wed, Feb 23, 2022 at 8:37 PM Rusty Russell <rusty at rustcorp.com.au> wrote:\n\n> Olaoluwa Osuntokun <laolu32 at gmail.com> writes:\n> > Hi y'all,\n> >\n> > (TL;DR: a way to nodes to get paid to forward onion messages by adding an\n> > upfront session creation phase that uses AMP tender a messaging session\n> to a\n> > receiver, with nodes being paid upfront for purchase of forwarding\n> > bandwidth, and a session identifier being transmitted alongside onion\n> > messages to identify paid sessions)\n>\n> AMP seems to be a Lightning Labs proprietary extension.  You mean\n> keysend, which at least has a draft spec?\n>\n> > Onion messaging has been proposed as a way to do things like fetch\n> invoices\n> > directly from a potential receiver _directly_ over the existing LN. The\n> > current proposal (packaged under the BOLT 12 umbrella) uses a new message\n> > (`onion_message`) that inherits the design of the existing Sphinx-based\n> > onion blob included in htlc_add messages as a way to propagate arbitrary\n> > messages across the network. Blinded paths which are effectively an\n> unrolled\n> > Sphinx SURB (single use reply block), are used to support reply messages\n> in\n> > a more private manner. Compared to SURBs, blinded paths are more\n> flexible as\n> > they don't lock in things like fees or CLTV values.\n> >\n> > A direct outcome of widespread adoption of the proposal is that the\n> scope of\n> > LN is expanded beyond \"just\" a decentralized p2p payment system, with the\n>\n> Sure, let's keep encouraging people to use HTLCs for free to send data?\n> I can certainly implement that if you prefer!\n>\n> >  1. As there's no explicit session creation/acceptance, a node can be\n> >  spammed with unsolicited messages with no way to deny unwanted messages\n> nor\n> >  explicitly allow messages from certain senders.\n> >\n> >  2. Nodes that forward these messages (up to 32 KB per message) receive\n> no\n> >  compensation for the network bandwidth their expend, effectively\n> shuffling\n> >  around messages for free.\n> >\n> >  3. Rate limiting isn't concretely addressed, which may result in\n> >  heterogeneous rate limiting policies enforced around the network, which\n> can\n> >  degrade the developer/user experience (why are my packets being randomly\n> >  dropped?).\n>\n> Sure, this is a fun one!  I can post separately on ratelimiting; I\n> suggest naively limiting to 10/sec for peers with channels, and 1/sec\n> for peers without for now.\n>\n> (In practice, spamming with HTLCs is infinitely more satisfying...)\n>\n> > In this email I propose a way to address the issues mentioned above by\n> > adding explicit onion messaging session creation as well as a way for\n> nodes\n> > to be (optionally) paid for any onion messages they forward. In short, an\n> > explicit session creation phase is introduced, with the receiver being\n> able\n> > to accept/deny the session. If the session is accepted, then all nodes\n> that\n> > comprise the session route are compensated for allotting a certain\n> amount of\n> > bandwidth to the session (which is ephemeral by nature).\n>\n> It's an interesting layer on top (esp if you want to stream movies), but\n> I never proposed this because it seems to add a source-identifying\n> session id, which is a huge privacy step backwards.\n>\n> You really *do not want* to use this for independent transmissions.\n>\n> I flirted with using blinded tokens, but it gets complex fast; ideas\n> welcome!\n>\n> > ## Node Announcement TLV Extension\n> >\n> > In order to allow nodes to signal that they want to be paid to forward\n> onion\n> > messages and also specify their pricing, we add two new TLV to the\n> node_ann\n> > message:\n> >\n> >   * type: 1 (`sats_per_byte`)\n> >    * data:\n> >       * [`uint64`:`forwarding_rate`]\n> >   * type: 2 (`sats_per_block`)\n> >    * data:\n> >       * [`uint64`:`per_block_rate`]\n>\n> You mean:\n>\n>    * type: 1 (`sats_per_byte`)\n>    * data:\n>        * [`tu64`:`forwarding_rate`]\n>    * type: 3 (`sats_per_block`)\n>    * data:\n>        * [`tu64`:`per_block_rate`]\n>\n> 1. Don't use an even TLV field.\n> 2. Might as well use truncated u64.\n>\n> > The `sats_per_byte` field allows nodes to price their bandwidth, ensuring\n> > that they get paid for each chunk of allocated bandwidth. As sessions\n> have a\n> > fixed time frame and nodes need to store additional data within that time\n> > frame, the `sats_per_block` allows nodes to price this cost, as they'll\n> hold\n> > onto the session identifier information until the specified block height\n> > (detailed below).\n> >\n> > As onion messages will _typically_ be fixed sized we may want to use\n> > coursers\n> > metering here instead of bytes, possibly paying for 1.3KB or 32 KB chunks\n> > instead.\n>\n> I think it's a premature optimization?  Make standard duration 2016\n> blocks; then they can request multiples if they want?  Reduces\n> node_announcement size.\n>\n> > With the above nodes are able to express that they're willing to forward\n> > messages for sats, and how much they charge per byte as well as per\n> block.\n> > Next we add a new TLV in the _existing_ HTLC onion blob that allows a\n> > sending node to tender paid onion message session creation. A sketch of\n> this\n> > type would look something like:\n> >\n> >   * type: 14 (`onion_session_id`)\n> >     * data:\n> >       * [`32*byte`:`session_id`]\n>\n> I'd be tempted to use 16 bytes?  Collisions here are not really a thing\n> since you'd need a network packet per probe, and you're time limited.\n>\n> > After session creation succeeds, nodes will forward onion messages that\n> > include that `onion_session_id`. The set of `encrypted_data_tlv` for\n> onion\n> > messages is extended to also specify a new type that stores the session\n> ID:\n> >\n> >   * type: 10 (`onion_session_id`)\n> >     * data:\n> >       * [`32*byte`:`session_id`]\n> >\n> > When forwarding an onion message that includes an `onion_session_id` (a\n> node\n> > may only forward messages that contain such an ID), nodes do the\n> necessary\n> > bookkeeping to tally how much bandwidth if left in this session, and also\n> > check that the session hasn't expired before forwarding.\n>\n> This is good because it doesn't require a db write (if you crash and\n> forget to charge, it's OK).\n>\n> AFAICT this is easy to implement on top of onion_messages as they stand\n> today (if you don't want to fwd free onion messages at all, don't set\n> that bit?).\n>\n> Cheers,\n> Rusty.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220322/fc6fcf6b/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "A Proposal for Adding Bandwidth Metered Payment to Onion Messages",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Olaoluwa Osuntokun"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 12163
        }
    },
    {
        "title": "[Lightning-dev] Taproot-aware Channel Announcements + Proof Verification",
        "thread_messages": [
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2022-03-23T01:10:00",
                "message_text_only": "Hi y'all,\n\nOn the lnd side we've nearly finished fully integrating taproot into the\ncodebase [1] (which includes the btcsuite set of libraries and also full\nbtcwallet support), scheduled to ship in 0.15 (~April), which will enable\nexisting users of lnd's on-chain wallet and APIs to start getting taprooty\nwit it. As any flavor of taproot will mean a different on-chain funding\noutput, the _existing_ gossip layer needs some sort of modification, as the\nBOLTs today don't define how to validate that a given output is actually a\ntaproot channel. Discussions during the prior spec meetings seem to have\ncentered in on two possible paths:\n\n  1. Use this as an opportunity to entirely redesign the channel validation\n  portion of the gossip layer (ring sigs? zkps? less validation? better\n  privacy?).\n\n  2. Defer the above, and instead go with a more minimal mostly the same\n  channel_announcement-like message for taproot channels.\n\nIn this mail, I want to explore the second option in detail, as Rusty has\nalready started a thread on what option #1 may look like [2].\n\n## A new taproot-aware `channel_announcement2` message\n\nA simple `channel_announcement2` message that was taproot aware could look\nsomething like:\n\n1. type: xxx (`channel_announcement2`) 2. data:\n    * [`signature`:`announcement_sig`]\n    * [`u16`:`len`]\n    * [`len*byte`:`features`]\n    * [`chain_hash`:`chain_hash`]\n    * [`short_channel_id`:`short_channel_id`]\n    * [`point`:`node_id_1`]\n    * [`point`:`node_id_2`]\n    * [`point`:`bitcoin_key_1`]\n    * [`point`:`bitcoin_key_2`]\n\n(can assume it'll be just native TLV prob also)\n\nThis is pretty much the same as the _existing_ `channel_announcement`\nmessage but instead of us carrying around 4 signatures, we'd use musig2 to\ngenerate a _single_ signature under the aggregate public key\n(`node_id_1`+`node_id_2`+`bitcoin_key_1`+`bitcoin_key_2`).\n\nWhile were here, similar to what's been proposed in [2], it likely makes\nsense to add an optional (?) merkle proof here to make channel validation\nmore feasible for constrained/mobile clients (they don't need to fetch all\nthose blocks any longer). The tradeoff here is that the merkle proof would\npotentially be the largest part of the message, which means more on-chain\ndata needed to store the full channel graph. Alternatively, we could make\nthis into another gossip query option, with nodes only fetching the proofs\nif they actually need it (full nodes with a txindex and just fetch the\ntransaction).\n\n### Should the proof+verification strongly bind to the LN context?\n\nAs far as on-chain output validation, the main difference would be how nodes\nactually validate the Bitcoin output (referenced by the `short_channel_id`)\non chain. Today, nodes use the two bitcoin keys and construct a p2wsh\nmulti-sig script and then verify that the script matches what has been\nconfirmed on chain. With taproot, the output is actually just a single key.\nSo if we want to maintain the same level of binding (which makes it harder\nto advertise fake channels using just a change output have or something),\nthen we'd specify that nodes reconstruct the aggregated bitcoin public key\n(Q = a_1*B_1 + a_2*_B_2, where a_i is a blinding factor derived using the\ntarget key and every other key in the signing set) and assert that this\nmatches the pkScript on chain.\n\nBy verifying that this output key is just an aggregated key, then we can\nalso ensure that there's no actual committed script root (a la BIP 86 [3])\nwhich binds the output to our context further. However maybe we want to also\ninclude a `tapscript_root` field as well (so use the musig2 key as the\ninternal key and then apply the tweaking operations and verify things match\nup), which would enable more elaborate unlocking/multi-sig functionality for\nthe normal funding output.\n\nAlternatively, if we decided that this strong binding isn't as desirable\n(starting to get into option 1 territory), then we'd specify just a single\nBitcoin key and look for that directly in the on chain script. IMO, if we're\ngoing the route of something that very closely resembles what we have today,\nthen we shouldn't drop the strong binding, and fully verify that the key is\nindeed a musig2 aggregated public key.\n\n## `announcement_signatures2` and musig2 awareness\n\nThe `announcement_signatures` message would also need to change as we'd only\nbe sending a single signature across the wire: the musig2 _partial_\nsignature.\n\n1. type: xxx (`announcement_signatures2`) 2. data:\n    * [`channel_id`:`channel_id`]\n    * [`short_channel_id`:`short_channel_id`]\n    * [`signature`:`partial_chan_proof_sig`]\n\nOnce both sides exchange this, as normal either party can generate the\n`channel_announcement` message to broadcast to the network.\n\nThe addition of musig2 carries with it an additional dependency: before\nthese signatures can be generated, both sides need to exchange their public\nnonce (in practice it's two nonces points R_1 and R_2), which is then used\nto generate the aggregated nonce using for signing. Thankfully, I don't\nthink we actually need an additional message here, and instead we can piggy\nback the public nonces on top of the _existing_ funding message flow. So in\nthis case, we'd just add a `66*byte`:`public_nonce` (the public nonces use\nthe full compressed encoding for the points, which is why it isn't jut 64\nbytes) field to the open+accept channel messages, which MUST be present if\nthe channel was to be advertised.\n\n## Conclusion\n\nIn this mail, I've sketched out a mostly mechanical mapping of taproot\nawareness to our existing gossip message flow. This contrasts the approach\nto re-design the channel advertisement as it doesn't deviate too much from\nthe current flow (same verification with a slight twist), which may make it\neasier to deploy (but ofc the devil is in the details as always).\n\nThoughts?\n\n-- Laolu\n\n[1]: https://github.com/lightningnetwork/lnd/pull/6263\n[2]:\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2022-February/003470.html\n[3]:\nhttps://github.com/bitcoin/bips/blob/master/bip-0086.mediawiki#address-derivation\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220322/e6936024/attachment.html>"
            },
            {
                "author": "David A. Harding",
                "date": "2022-03-23T21:02:12",
                "message_text_only": "On 22.03.2022 15:10, Olaoluwa Osuntokun wrote:\n> ### Should the proof+verification strongly bind to the LN context?\n> Today, nodes use the two bitcoin keys and construct a p2wsh\n> multi-sig script and then verify that the script matches what has been\n> confirmed on chain. With taproot, the output is actually just a single\n> key. So if we want to maintain the same level of binding (which makes \n> it\n> harder to advertise fake channels using just a change output have or\n> something), then we'd specify that nodes reconstruct the aggregated \n> bitcoin public\n> key\n\nI think there's a significant difference between P2WSH and P2TR that's\nnot being considered here.  With P2WSH, if I want to fake the creation\nof a channel by making my change outputs OP_CMS(2-of-2) with myself, I\npay for that deception by incurring extra fee costs at spend time due to\nthe need to provide more witness data over plain single-sig.  With \nP2TR/MuSig2,\nI can make my change outputs MuSig2(2-of-2) with myself without \nincurring\nany additional onchain spend costs.\n\nIn short, I don't believe that you can maintain the same level of\nbinding-to-LN-usage currently provided by P2WSH when P2TR keypath\nspends are allowed.\n\nThanks,\n\n-Dave"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2022-03-23T22:30:40",
                "message_text_only": "Hi Harding,\n\nThat's a really good point: the false signal is more costly with witness v0\noutputs, as they need to pay for the extra bytes in the witness.\n\nI agree we can't 100% maintain the same level of binding for pure taproot\nchannels. However by having validators verify the final key derivation, we\nstill effectively further restrict the _type_ of outputs that can be used to\nadvertise channels, as this means that someone can't use \"normal\" P2TR\nwallet outputs for channel proofs (barring the existence of some new\nthreshold schnorr wallet, but that would use different key aggregation\n(FROST?) all together).\n\n-- Laolu\n\nOn Wed, Mar 23, 2022 at 2:02 PM David A. Harding <dave at dtrt.org> wrote:\n\n> On 22.03.2022 15:10, Olaoluwa Osuntokun wrote:\n> > ### Should the proof+verification strongly bind to the LN context?\n> > Today, nodes use the two bitcoin keys and construct a p2wsh\n> > multi-sig script and then verify that the script matches what has been\n> > confirmed on chain. With taproot, the output is actually just a single\n> > key. So if we want to maintain the same level of binding (which makes\n> > it\n> > harder to advertise fake channels using just a change output have or\n> > something), then we'd specify that nodes reconstruct the aggregated\n> > bitcoin public\n> > key\n>\n> I think there's a significant difference between P2WSH and P2TR that's\n> not being considered here.  With P2WSH, if I want to fake the creation\n> of a channel by making my change outputs OP_CMS(2-of-2) with myself, I\n> pay for that deception by incurring extra fee costs at spend time due to\n> the need to provide more witness data over plain single-sig.  With\n> P2TR/MuSig2,\n> I can make my change outputs MuSig2(2-of-2) with myself without\n> incurring\n> any additional onchain spend costs.\n>\n> In short, I don't believe that you can maintain the same level of\n> binding-to-LN-usage currently provided by P2WSH when P2TR keypath\n> spends are allowed.\n>\n> Thanks,\n>\n> -Dave\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220323/90c12121/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Taproot-aware Channel Announcements + Proof Verification",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "David A. Harding",
                "Olaoluwa Osuntokun"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 9618
        }
    },
    {
        "title": "[Lightning-dev] [RFC] Lightning gossip alternative",
        "thread_messages": [
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2022-03-23T01:38:40",
                "message_text_only": "Hi Rusty,\n\n> Timestamps are replaced with block heights.\n\nThis is a conceptually small change, but would actually make things like\nrate limiting updates for implementations easier and more uniform. A simple\nrule would be only allowing an update per block, which cuts down a lot on\npotential chatter traffic, but maybe it's _too_ restrictive? Based on my\nnetwork observations, these days some many power user nodes more\naggressively\nupdate their fee schedules in response to liquidity imbalances or as an\nattempt to incentive usage of some channels vs others.\n\n> 1. Nodes send out weekly node_announcement_v2, proving they own some\n> UTXOs.\n\nIf a node doesn't send out this announcement, then will others start to\nignore their \"channels\"?\n\n> 3. This uses UTXOs for anti-spam, but doesn't tie them to channels\n> directly.\n\nAs I hinted a bit in prior discussion, and also my recent ML [1] post\noutlining a possible \"do most of the same things\" stop gap, this has the\npotentially undesirable effect of allowing parties on the network to utilize\n_existing_ outputs to advertise false channels and inflate the \"total\nnetwork capacity\" metric. We'd effectively be moving away from \"Alice and\nBob have N BTC of bound capacity between them to\", \"Bob has N BTC he can use\nfor signing these proofs\".\n\nAlso while we're at it, why not add a merkle proof here (independent of\nwhich direction we go with) which makes it possible for constrained/mobile\nnodes to more easily verify gossip data (could be an optional query flag).\n\n> FIXME: what about tapscripts?\n\nYeah, one side effect of moving to nodes advertising how much BTC they can\nsign w/ vs the _actual_ BTC they have in \"channels\", is that to extend\nvalidation here, the verifiers would need to fully verify possible script\npath spends (assuming a scenario where a NUMs point is used as the internal\nkey).\n\n> Legacy proofs are two signatures, similar to the existing\n> channel_announcement.\n\nWhy not musig2? We'd be able to get away with just a single sig with this\nmodified `node_announcement_v2` and if we go the `channel_announcement2`\nroute, we'd be able to compress those 4 sigs into one.\n\n> - If two node_announcement_v2 claim the same UTXO, use the first seen,\n> discard any others.\n\nSo then this would mean that nodes that _actually_ have a channel between\nthem can't _individually_ claim the capacity?\n\n> - node_announcement_v2 are discarded after a week (1000 blocks).  - Nodes\n> do not need to monitor existence of UTXOs after initial check (since they\n> will automatically prune them after a week).\n\nA side effect of this would be _continual_ gossip churn in order to keep\nchannels alive. Today we do have the 2 week `channel_update` heart beat, but\nchannel updates are relatively small compared to this `node_announcement_v2`\nmessage.\n\n> - The total proved utxo value (not counting any utxos which are spent) is\n> multiplied by 10 to give the \"announcable_channel_capacity\" for that node.\n\nI don't see how this is at all useful in practice. We'd end up with inflated\nnumbers for the total node capacity, and path finding would be more\ndifficult as it isn't clear exactly how large an HTLC I can send over the\n\"channel\". Sure there's the existence of max_htlc, but in that case why add\nthis \"leverage\" factor in the first place?\n\n> 1. type: 273 (`channel_update_v2`)\n\nThis seems to allow the advertisement of channels which aren't actually\nanchored in the chain, which I *think* is a cool thing to have? On the other\nhand, the graph at the _edge_ level would be far more dynamic than it is\ntoday (Bob can advertise an entirely distinct topology from one day to\nanother). Would need to think about the implications here for path finding\nalgorithms and nodes that want to maintain an update to date view of the\nnetwork...\n\n> - `channel_id_and_claimant` is a 31-bit per-node channel_id which can be\n> used in onion_messages, and a one bit stolen for the `claim` flag.\n\nWhere would this `channel_id` be derived from? FWIW, using this value in the\nonion means we get a form of pubkey based routing [3] depending on how these\nare derived.\n\n> This simplifies gossip, requiring only two messages instead of three,\n> and reducing the UTXO validation requirements to per-node instead of\n> per-channel.\n\nI'm not sure this would actually _simplify_ gossip in practice, given that\nwe'd be moving to a channel graph that isn't entirely based in the reality\nof what's routable, and would be far more dynamic than it is today.\n\n> We can use a convention that a channel_update_v2 with no `capacity` is a\n> permanent close.\n\nOn the neutrino side, we tried to do something where if we see both channels\nbe disabled, then we'd mark the channel as closed. But in practice if you're\nnot syncing _every_ channel update every transmitted, then you'll end up\nactually missing them.\n\n> It also allows \"leasing\" of UTXOs: you could pay someone to sign their\n> UTXO for your node_announcement, with some level of trust.\n\nI'm not sure this is entirely a *good* thing, as the graph becomes more\ndecoupled with the _actual_ on-chain relationships...\n\n-- Laolu\n\n[1]:\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2022-March/003526.html\n[2]: https://github.com/lightning/bolts/pull/814\n\n\nOn Tue, Feb 15, 2022 at 12:46 AM Rusty Russell <rusty at blockstream.com>\nwrote:\n\n> Hi all,\n>\n>         I've floated this idea before, but this is a more concrete\n> proposal for a \"v2\" gossip protocol.\n>\n> It assumes x-only pubkeys (aka point32) and BIP-340 signatures, and uses\n> modern TLVs for all optional or extensible fields.  Timestamps are\n> replaced with block heights.\n>\n> 1. Nodes send out weekly node_announcement_v2, proving they own some\n>    UTXOs.\n> 2. This entitles them to broadcast channels, using channel_update_v2; a\n>    channel_update_v2 from both peers means the channel exists.\n> 3. This uses UTXOs for anti-spam, but doesn't tie them to channels\n>    directly.\n> 4. Future ZKP proofs are could be added.\n>\n> 1. type: 271 (`node_announcement_v2`)\n> 2. data:\n>     * [`bip340sig`:`signature`]\n>     * [`point32`:`node_id`]\n>     * [`u32`:`blockheight`]\n>     * [`node_announcement_v2_tlvs`:`tlvs`]\n>\n> 1. `tlv_stream`: `node_announcement_v2_tlvs`\n> 2. types:\n>     1. type: 2 (`features`)\n>     2. data:\n>         * [`...*byte`:`featurebits`]\n>     1. type: 3 (`chain_hash`)\n>     2. data:\n>         * [`chain_hash`:`chain`]\n>     1. type: 4 (`taproot_utxo_proofs`)\n>     2. data:\n>         * [`...*taproot_utxo_proof`:`proofs`]\n>     1. type: 6 (`legacy_utxo_proofs`)\n>     2. data:\n>         * [`...*legacy_utxo_proof`:`proofs`]\n>     1. type: 127 (`ipv4_addresses`)\n>     2. data:\n>         * [`...*ipv4`:`addresses`]\n>     1. type: 129 (`ipv6_addresses`)\n>     2. data:\n>         * [`...*ipv6`:`addresses`]\n>     1. type: 131 (`torv3_addresses`)\n>     2. data:\n>         * [`...*torv3`:`addr`]\n> # Maybe alias, color, etc?\n>\n> Taproot proofs are a signature of the v1 output over the `node_id`,\n> `utxo` and `blockheight` with prefix \"lightingtaproot_utxo_proofsig\"\n> (using the tagged signatures as per BOLT12). (FIXME: what about\n> tapscripts?).\n>\n> Legacy proofs are two signatures, similar to the existing\n> channel_announcement.\n>\n> 1. subtype: `taproot_utxo_proof`\n> 2. data:\n>     * [`short_channel_id`:`utxo`]\n>     * [`signature`:`sig`]\n>\n> 1. subtype: `legacy_utxo_proof`\n> 2. data:\n>     * [`short_channel_id`:`utxo`]\n>     * [`point`:`bitcoin_key_1`]\n>     * [`point`:`bitcoin_key_2`]\n>     * [`signature`:`sig_1`]\n>     * [`signature`:`sig_2`]\n>\n> - node_announcement_v2 are discarded after a week (1000 blocks).\n> - If two node_announcement_v2 claim the same UTXO, use the first seen,\n>   discard any others.\n> - Nodes do not need to monitor existence of UTXOs after initial check\n> (since\n>   they will automatically prune them after a week).\n> - The total proved utxo value (not counting any utxos which are spent)\n>   is multiplied by 10 to give the \"announcable_channel_capacity\" for that\n> node.\n>\n> 1. type: 273 (`channel_update_v2`)\n> 2. data:\n>     * [`bip340sig`:`signature`]\n>     * [`point32`:`local_node_id`]\n>     * [`point32`:`remote_node_id`]\n>     * [`u32`:`blockheight`]\n>     * [`u32`:`channel_id_and_claimant`]\n>     * [`channel_update_v2_tlvs`:`tlvs`]\n>\n> 1. `tlv_stream`: `channel_update_v2_tlvs`\n> 2. types:\n>     1. type: 2 (`features`)\n>     2. data:\n>         * [`...*byte`:`featurebits`]\n>     1. type: 3 (`chain_hash`)\n>     2. data:\n>         * [`chain_hash`:`chain`]\n>     1. type: 4 (`capacity`)\n>     2. data:\n>         * [`tu64`:`satoshis`]\n>     1. type: 5 (`cost`)\n>     2. data:\n>        * [`u16`:`cltv_expiry_delta`]\n>        * [`u32`:`fee_proportional_millionths`]\n>        * [`tu32`:`fee_base_msat`]\n>     1. type: 6 (`min_msat`)\n>     2. data:\n>         * [`tu64`:`min_htlc_sats`]\n>\n> - `channel_id_and_claimant` is a 31-bit per-node channel_id which can be\n>   used in onion_messages, and a one bit stolen for the `claim` flag.\n> - A channel is not considered to exist until both peers have sent a\n>   channel_update_v2, at least one of which must set the `claim` flag.\n> - If a node sets `claim`, the capacity of the channel is subtracted from\n>   the remaining announcable_channel_capacity for that node (minimum\n>   10,000 sats).\n> - If there is insufficient total `announcable_channel_capacity` for a\n>   node, it is used by the lower `channel_id`s first.\n>\n> Implications\n> ------------\n>\n> This simplifies gossip, requiring only two messages instead of three,\n> and reducing the UTXO validation requirements to per-node instead of\n> per-channel.  We can use a convention that a channel_update_v2 with no\n> `capacity` is a permanent close.\n>\n> We might want to add a taproot_utxo_delegated_proof where UTXOs can\n> sign over to another key, so cold storage only needs to sign once, and\n> the other key can sign weekly.\n>\n> It also allows \"leasing\" of UTXOs: you could pay someone to sign their\n> UTXO for your node_announcement, with some level of trust.  They could\n> spend the UTXO, which gives you a slow degredation as new nodes don't\n> accept your channels but existing nodes don't check until it's due for a\n> refresh).  Or they could sign one UTXO for multiple node_announcements,\n> which is why preference is given to the first-seen.  But it's a weekly\n> business so there's incentive for them not to.\n>\n> Between nodes there's the question of \"who claims this new channel?\",\n> which I didn't address here.  Opener claims is logical, but leaks\n> information (though you could definitely pay for the peer to claim it).\n> With dual-funding, it's more complex (some kind of proportional coinflip\n> protocol is possible), but basically you can always wait for your peer,\n> and if they don't set the claim bit you can.\n>\n> Cheers!\n> Rusty.\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220322/23c944d8/attachment-0001.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2022-03-23T23:50:00",
                "message_text_only": "Olaoluwa Osuntokun <laolu32 at gmail.com> writes:\n> Hi Rusty,\n>\n>> Timestamps are replaced with block heights.\n>\n> This is a conceptually small change, but would actually make things like\n> rate limiting updates for implementations easier and more uniform. A simple\n> rule would be only allowing an update per block, which cuts down a lot on\n> potential chatter traffic, but maybe it's _too_ restrictive? Based on my\n> network observations, these days some many power user nodes more\n> aggressively\n> update their fee schedules in response to liquidity imbalances or as an\n> attempt to incentive usage of some channels vs others.\n\nAs an aside, Alex Myers is looking at using minisketch for gossip\nreconciliation, and there's enough spam in the network that it probably\nrequires some consensus on filtering.  When he's got some better\nanalysis I expect a post here...\n\n>> 1. Nodes send out weekly node_announcement_v2, proving they own some\n>> UTXOs.\n>\n> If a node doesn't send out this announcement, then will others start to\n> ignore their \"channels\"?\n\nYes, this would be the anchor (reverse of now, ignore node_announcement\nunless you've seen a channel_announcement).\n\n>> 3. This uses UTXOs for anti-spam, but doesn't tie them to channels\n>> directly.\n>\n> As I hinted a bit in prior discussion, and also my recent ML [1] post\n> outlining a possible \"do most of the same things\" stop gap, this has the\n> potentially undesirable effect of allowing parties on the network to utilize\n> _existing_ outputs to advertise false channels and inflate the \"total\n> network capacity\" metric. We'd effectively be moving away from \"Alice and\n> Bob have N BTC of bound capacity between them to\", \"Bob has N BTC he can use\n> for signing these proofs\".\n\nSince the aim is to defeat onchain analysis, this seems inevitable in\nalmost any non-zero-knowledge scheme?\n\n> Also while we're at it, why not add a merkle proof here (independent of\n> which direction we go with) which makes it possible for constrained/mobile\n> nodes to more easily verify gossip data (could be an optional query flag).\n\nYeah, we should do that anyway (the merkle proof doesn't need to be\nsigned, so can be added by anyone).\n\n>> FIXME: what about tapscripts?\n>\n> Yeah, one side effect of moving to nodes advertising how much BTC they can\n> sign w/ vs the _actual_ BTC they have in \"channels\", is that to extend\n> validation here, the verifiers would need to fully verify possible script\n> path spends (assuming a scenario where a NUMs point is used as the internal\n> key).\n\nYech, you're right :(\n\n>> Legacy proofs are two signatures, similar to the existing\n>> channel_announcement.\n>\n> Why not musig2? We'd be able to get away with just a single sig with this\n> modified `node_announcement_v2` and if we go the `channel_announcement2`\n> route, we'd be able to compress those 4 sigs into one.\n\nGood point: I was thinking this would allow nodes to advertize v2 even\nif their peer doesn't support it, but it doesn't.\n\n>> - If two node_announcement_v2 claim the same UTXO, use the first seen,\n>> discard any others.\n>\n> So then this would mean that nodes that _actually_ have a channel between\n> them can't _individually_ claim the capacity?\n\nYeah, it's kind of arbitrary who gets the UTXO in that case, but for\nsimplicitly I think we need to enforce uniqueness.  They can flip a coin?\n\n>> - node_announcement_v2 are discarded after a week (1000 blocks).  - Nodes\n>> do not need to monitor existence of UTXOs after initial check (since they\n>> will automatically prune them after a week).\n>\n> A side effect of this would be _continual_ gossip churn in order to keep\n> channels alive. Today we do have the 2 week `channel_update` heart beat, but\n> channel updates are relatively small compared to this `node_announcement_v2`\n> message.\n\nWe could increase this to 2000 blocks, but #nodes << #channels, so it's\nstill probably a win?  I'd have to run some real numbers though...\n\n>> - The total proved utxo value (not counting any utxos which are spent) is\n>> multiplied by 10 to give the \"announcable_channel_capacity\" for that node.\n>\n> I don't see how this is at all useful in practice. We'd end up with inflated\n> numbers for the total node capacity, and path finding would be more\n> difficult as it isn't clear exactly how large an HTLC I can send over the\n> \"channel\". Sure there's the existence of max_htlc, but in that case why add\n> this \"leverage\" factor in the first place?\n\nBecause not every node will have enough onchain funds, they'll end up\nrevealing some channels' UTXOs.  The larger the leverage, the less\nrevealed...\n\n>> 1. type: 273 (`channel_update_v2`)\n>\n> This seems to allow the advertisement of channels which aren't actually\n> anchored in the chain, which I *think* is a cool thing to have? On the other\n> hand, the graph at the _edge_ level would be far more dynamic than it is\n> today (Bob can advertise an entirely distinct topology from one day to\n> another). Would need to think about the implications here for path finding\n> algorithms and nodes that want to maintain an update to date view of the\n> network...\n>\n>> - `channel_id_and_claimant` is a 31-bit per-node channel_id which can be\n>> used in onion_messages, and a one bit stolen for the `claim` flag.\n>\n> Where would this `channel_id` be derived from? FWIW, using this value in the\n> onion means we get a form of pubkey based routing [3] depending on how these\n> are derived.\n\nYeah, we could drop this altogether if we wanted; its just a unique\nidentifier for that specific node to refer to a peer.  It saves space in\nthe onion, that is all.\n\n>> This simplifies gossip, requiring only two messages instead of three,\n>> and reducing the UTXO validation requirements to per-node instead of\n>> per-channel.\n>\n> I'm not sure this would actually _simplify_ gossip in practice, given that\n> we'd be moving to a channel graph that isn't entirely based in the reality\n> of what's routable, and would be far more dynamic than it is today.\n>\n>> We can use a convention that a channel_update_v2 with no `capacity` is a\n>> permanent close.\n>\n> On the neutrino side, we tried to do something where if we see both channels\n> be disabled, then we'd mark the channel as closed. But in practice if you're\n> not syncing _every_ channel update every transmitted, then you'll end up\n> actually missing them.\n\nYeah, we may also want the 2week / 2000 block refresh here as well?\n\n>> It also allows \"leasing\" of UTXOs: you could pay someone to sign their\n>> UTXO for your node_announcement, with some level of trust.\n>\n> I'm not sure this is entirely a *good* thing, as the graph becomes more\n> decoupled with the _actual_ on-chain relationships...\n\nWhich is the aim (though not sure how many ppl would do it in\npractice?).\n\nCheers,\nRusty.\n\n> -- Laolu\n>\n> [1]:\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-March/003526.html\n> [2]: https://github.com/lightning/bolts/pull/814\n>\n>\n> On Tue, Feb 15, 2022 at 12:46 AM Rusty Russell <rusty at blockstream.com>\n> wrote:\n>\n>> Hi all,\n>>\n>>         I've floated this idea before, but this is a more concrete\n>> proposal for a \"v2\" gossip protocol.\n>>\n>> It assumes x-only pubkeys (aka point32) and BIP-340 signatures, and uses\n>> modern TLVs for all optional or extensible fields.  Timestamps are\n>> replaced with block heights.\n>>\n>> 1. Nodes send out weekly node_announcement_v2, proving they own some\n>>    UTXOs.\n>> 2. This entitles them to broadcast channels, using channel_update_v2; a\n>>    channel_update_v2 from both peers means the channel exists.\n>> 3. This uses UTXOs for anti-spam, but doesn't tie them to channels\n>>    directly.\n>> 4. Future ZKP proofs are could be added.\n>>\n>> 1. type: 271 (`node_announcement_v2`)\n>> 2. data:\n>>     * [`bip340sig`:`signature`]\n>>     * [`point32`:`node_id`]\n>>     * [`u32`:`blockheight`]\n>>     * [`node_announcement_v2_tlvs`:`tlvs`]\n>>\n>> 1. `tlv_stream`: `node_announcement_v2_tlvs`\n>> 2. types:\n>>     1. type: 2 (`features`)\n>>     2. data:\n>>         * [`...*byte`:`featurebits`]\n>>     1. type: 3 (`chain_hash`)\n>>     2. data:\n>>         * [`chain_hash`:`chain`]\n>>     1. type: 4 (`taproot_utxo_proofs`)\n>>     2. data:\n>>         * [`...*taproot_utxo_proof`:`proofs`]\n>>     1. type: 6 (`legacy_utxo_proofs`)\n>>     2. data:\n>>         * [`...*legacy_utxo_proof`:`proofs`]\n>>     1. type: 127 (`ipv4_addresses`)\n>>     2. data:\n>>         * [`...*ipv4`:`addresses`]\n>>     1. type: 129 (`ipv6_addresses`)\n>>     2. data:\n>>         * [`...*ipv6`:`addresses`]\n>>     1. type: 131 (`torv3_addresses`)\n>>     2. data:\n>>         * [`...*torv3`:`addr`]\n>> # Maybe alias, color, etc?\n>>\n>> Taproot proofs are a signature of the v1 output over the `node_id`,\n>> `utxo` and `blockheight` with prefix \"lightingtaproot_utxo_proofsig\"\n>> (using the tagged signatures as per BOLT12). (FIXME: what about\n>> tapscripts?).\n>>\n>> Legacy proofs are two signatures, similar to the existing\n>> channel_announcement.\n>>\n>> 1. subtype: `taproot_utxo_proof`\n>> 2. data:\n>>     * [`short_channel_id`:`utxo`]\n>>     * [`signature`:`sig`]\n>>\n>> 1. subtype: `legacy_utxo_proof`\n>> 2. data:\n>>     * [`short_channel_id`:`utxo`]\n>>     * [`point`:`bitcoin_key_1`]\n>>     * [`point`:`bitcoin_key_2`]\n>>     * [`signature`:`sig_1`]\n>>     * [`signature`:`sig_2`]\n>>\n>> - node_announcement_v2 are discarded after a week (1000 blocks).\n>> - If two node_announcement_v2 claim the same UTXO, use the first seen,\n>>   discard any others.\n>> - Nodes do not need to monitor existence of UTXOs after initial check\n>> (since\n>>   they will automatically prune them after a week).\n>> - The total proved utxo value (not counting any utxos which are spent)\n>>   is multiplied by 10 to give the \"announcable_channel_capacity\" for that\n>> node.\n>>\n>> 1. type: 273 (`channel_update_v2`)\n>> 2. data:\n>>     * [`bip340sig`:`signature`]\n>>     * [`point32`:`local_node_id`]\n>>     * [`point32`:`remote_node_id`]\n>>     * [`u32`:`blockheight`]\n>>     * [`u32`:`channel_id_and_claimant`]\n>>     * [`channel_update_v2_tlvs`:`tlvs`]\n>>\n>> 1. `tlv_stream`: `channel_update_v2_tlvs`\n>> 2. types:\n>>     1. type: 2 (`features`)\n>>     2. data:\n>>         * [`...*byte`:`featurebits`]\n>>     1. type: 3 (`chain_hash`)\n>>     2. data:\n>>         * [`chain_hash`:`chain`]\n>>     1. type: 4 (`capacity`)\n>>     2. data:\n>>         * [`tu64`:`satoshis`]\n>>     1. type: 5 (`cost`)\n>>     2. data:\n>>        * [`u16`:`cltv_expiry_delta`]\n>>        * [`u32`:`fee_proportional_millionths`]\n>>        * [`tu32`:`fee_base_msat`]\n>>     1. type: 6 (`min_msat`)\n>>     2. data:\n>>         * [`tu64`:`min_htlc_sats`]\n>>\n>> - `channel_id_and_claimant` is a 31-bit per-node channel_id which can be\n>>   used in onion_messages, and a one bit stolen for the `claim` flag.\n>> - A channel is not considered to exist until both peers have sent a\n>>   channel_update_v2, at least one of which must set the `claim` flag.\n>> - If a node sets `claim`, the capacity of the channel is subtracted from\n>>   the remaining announcable_channel_capacity for that node (minimum\n>>   10,000 sats).\n>> - If there is insufficient total `announcable_channel_capacity` for a\n>>   node, it is used by the lower `channel_id`s first.\n>>\n>> Implications\n>> ------------\n>>\n>> This simplifies gossip, requiring only two messages instead of three,\n>> and reducing the UTXO validation requirements to per-node instead of\n>> per-channel.  We can use a convention that a channel_update_v2 with no\n>> `capacity` is a permanent close.\n>>\n>> We might want to add a taproot_utxo_delegated_proof where UTXOs can\n>> sign over to another key, so cold storage only needs to sign once, and\n>> the other key can sign weekly.\n>>\n>> It also allows \"leasing\" of UTXOs: you could pay someone to sign their\n>> UTXO for your node_announcement, with some level of trust.  They could\n>> spend the UTXO, which gives you a slow degredation as new nodes don't\n>> accept your channels but existing nodes don't check until it's due for a\n>> refresh).  Or they could sign one UTXO for multiple node_announcements,\n>> which is why preference is given to the first-seen.  But it's a weekly\n>> business so there's incentive for them not to.\n>>\n>> Between nodes there's the question of \"who claims this new channel?\",\n>> which I didn't address here.  Opener claims is logical, but leaks\n>> information (though you could definitely pay for the peer to claim it).\n>> With dual-funding, it's more complex (some kind of proportional coinflip\n>> protocol is possible), but basically you can always wait for your peer,\n>> and if they don't set the claim bit you can.\n>>\n>> Cheers!\n>> Rusty.\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>"
            }
        ],
        "thread_summary": {
            "title": "Lightning gossip alternative",
            "categories": [
                "Lightning-dev",
                "RFC"
            ],
            "authors": [
                "Rusty Russell",
                "Olaoluwa Osuntokun"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 23925
        }
    },
    {
        "title": "[Lightning-dev] Dynamic Commitments Part 2: Taprooty Edition",
        "thread_messages": [
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2022-03-24T22:52:42",
                "message_text_only": "Hi y'all,\n\n## Dynamic Commitments Retrospective\n\nTwo years-ish ago I made a mailing list post on some ideas re dynamic\ncommitments [1], and how the concept can be used to allow us to upgrade\nchannel types on the fly, and also remove pesky hard coded limits like the\n483 HTLC in-flight limit that's present today. Back then my main target was\nupgrading all the existing channels over to the anchor output commitment\nvariant, so the core internal routing network would be more resilient in a\npersistent high fee environment (which hasn't really happened over the past\n2 years for various reasons tbh). Fast forward to today, and with taproot\nnow active on mainnet, and some initial design work/sketches for\ntaproot-native channels underway, I figure it would be good to bump this\nconcept as it gives us a way to upgrade all 80k+ public channels to taproot\nwithout any on chain transactions.\n\n## Updating Across Witness Versions w/ Adapter Commitments\n\nIn my original mail, I incorrectly concluded that the dynamic commitments\nconcept would only really work within the confines of a \"static\" multi-sig\noutput, meaning that it couldn't be used to help channels upgrade to future\nsegwit witness versions.  Thankfully this reply [2] by ZmnSCPxj, outlined a\nway to achieve this in practice. At a high level he proposes an \"adaptor\ncommitment\" (similar to the kickoff transaction in eltoo/duplex), which is\nbasically an upgrade transaction that spends one witness version type, and\nproduces an output with the next (upgraded) type. In the context of\nconverting from segwit v0 to v1 (taproot), two peers would collaboratively\ncreate a new adapter commitment that spends the old v0 multi-sig output, and\nproduces a _new_ v1 multi-sig output. The new commitment transaction would\nthen be anchored using this new output.\n\nHere's a rough sequence diagram of the before and after state to better\nconvey the concept:\n\n  * Before: fundingOutputV0 -> commitmentTransaction\n\n  * After fundingOutputV0 -> fundingOutputV1 (the adapter) ->\n    commitmentTransaction\n\nIt *is* still the case that _ultimately_ the two transactions to close the\nold segwit v0 funding output, and re-open the channel with a new segwit v1\nfunding output are unavoidable. However this adapter commitment lets peers\n_defer_ these two transactions until closing time. When force closing two\ntransactions need to be confirmed before the commitment outputs can be\nresolved. However, for co-op close, you can just spend the v0 output, and\ndeliver to the relevant P2TR outputs. The adapter commitment can leverage\nsighash anyonecanpay to let both parties (assuming it's symmetric) attach\nadditional inputs for fees (to avoid introducing the old update_fee related\nstatic fee issues), or alternatively inherit the anchor output pattern at\nthis level.\n\n## Existing Dynamic Commitments Proposals\n\nAssuming this concept holds up, then we need an actual concrete protocol to\nallow for dynamic commitment updates. Last year, Rusty made a spec PR\noutlining a way to upgrade the commitment type (leveraging the new\ncommitment type feature bits) upon channel re-establish [3]. The proposal\nrelies on another message that both sides send (`stfu`) to clear the\ncommitment (similar to the shutdown semantics) before the switch over\nhappens. However as this is tied to the channel re-establish flow, it\ndoesn't allow both sides to do things like only allow your peer to attach N\nHTLCs to start with, slowing increasing their allotted slots and possibly\nreducing them (TCP AIMD style).\n\n## A Two-Phase Dynamic Commitment Update Protocol\n\nIMO if we're adding in a way to do commitment/channel upgrades, then it may\nbe worthwhile to go with a more generalized, but slightly more involved\nroute instead. In the remainder of this mail, I'll describe an alternative\napproach that would allow upgrading nearly all channel/commitment related\nvalues (dust limit, max in flight, etc), which is inspired by the way the\nRaft consensus protocol handles configuration/member changes.\n\nFor those that aren't aware, Raft is a consensus protocol analogous to Paxos\n(but isn't byzantine fault tolerant out of the box) that was designed as a\nmore understandable alternative to Paxos for a pedagogical environment.\nTypically the algorithm is run in the context of a fixed cluster with N\nmachines, but supports adding/removing machines from the cluster with a\nconfiguration update protocol. At a high level the way this works is that a\nnew config is sent to the leader, with the leader synchronizing the config\nchange with the other members of the cluster. Once a majority threshold is\nreached, the leader then commits the config change with the acknowledged\nparties using the new config (basically a two phase commit). I'm skipping\nover some edge cases here that can arise if the new nodes participate\nconsensus too early, which can cause a split majority leading to two leaders\nbeing elected.\n\nApplying this to the LN context is a bit simpler than a generalized\nprotocol, as we typically just have two parties involved. The initiator is\nalready naturally a \"leader\" in our context, as they're the only ones that\ncan do things like trigger fee updates.\n\n### Message Structure\n\nAt a high level I propose we introduce two new messages, with the fields\nlooking something like this for `commitment_update_propose`:\n * type: 0 (`channel_id`)\n   * value: [`32*byte`:`chan_id`]\n * type: 1 (`propose_sig`)\n   * value: [`64*byte`:`sig`]\n * type: 2 (`update_payload`)\n   * value: [`*byte`:`tlv_payload`]\n\nand this `commitment_update_apply`:\n * type: 0 (`channel_id`)\n   * value: [`32*byte`:`chan_id`]\n * type: 1 (`local_propose`)\n   * value: [`*byte`:`commitment_update_propose`]\n * type: 2 (`remote_propose`)\n   * value: [`*byte`:`commitment_update_propose`]\n\n### Protocol Flow\n\nThe core idea here is that either party can propose a commitment/channel\nparam update, but only the initiator can actually apply it. The\n`commitment_update_propose` encodes the new set of updates, with a signature\ncovering the TLV blob for the new params (more on why that's needed later).\nThe `commitment_update_apply` includes up to _two_\n`commitment_update_propose` messages (one for the initiator and one for the\nresponder, as nested TLV messages). The `commitment_update_propose` message\nwould be treated like any other `update_*` message, in that it takes a new\ncommitment signature to properly commit/apply it.\n\nThe normal flow takes the form of both sides sending a\n`commitment_update_propose` message, with the initiator finally committing\nboth by sending a `commitment_update_apply` message. In the event that only\nthe responder wants to apply a param change/update, then the initiator can\nreply immediately with a `commitment_update_apply` message that doesn't\ninclude a param change for their commitment (or they just echo the\nparameters if they're acceptable).\n\n### Handling Retransmissions\n\nThe role of the signature it to prevent \"spoofing\" by one of the parties\n(authenticate the param change), and also it serves to convince a party that\nthey actually sent a prior commitment propose update during the\nretransmission phase. As the `commitment_update_propose` message would be\nretransmitted like any other message, if the initiator attempts to commit\nthe update but the connection dies, they'll retransmit it as normal along\nwith their latest signature.\n\n### Nested TLV Param Generality\n\nThe messages as sketched out here just have an opaque nested TLV field which\nmakes it extensible to add in other things like tweaking the total number of\nmax HTLCs, the current dust values, min/max HTLCs, etc (all things that are\ncurrently hard coded for the lifetime of the channel). An initial target\nwould likely just be a `chan_type` field, with future feature bits governing\n_what_ type of commitment updates both parties understand in the future.\n\nIn the past, when ideas like this were brought up, some were concerned that\nit wouldn't really be possible to do this type of updates while existing\nHTLCs were in flight (hence some of the ideas to clear out the commitment\nbeforehand). I don't see a reason why this fundamentally _shouldn't_ be\nallowed, as from the point of view of the channel update state machine, all\nupdates (adds/removes) get applied as normal, but with this _new_ commitment\ntype/params. The main edge case we'll need to consider is cases where the\nnew params make older HTLCs invalid for some reason.\n\n## Conclusion\n\nUsing the adapter commitment idea combined with a protocol for updating\ncommitments on the fly, would potentially allow us to update all 80k+ segwit\nv0 channels to the base level of taprooty channels without any on chain\ntransactions. The two transactions (open+close) must happen eventually, but\nby holding another layer of spends off-chain we can defer them (potentially\nindefinitely, as we have channels today that have been opened for over a\nyear).\n\nDeploying a generalised on-the-fly dynamic commitment update protocol gives\nus a tool to future proof the _existing_ anchored multi-sig outputs in the\nchain, and also a way to remove many of the hard coded parameters we have\ntoday in the protocol. One overly inflexible parameter we have today in the\nnetwork is the 483 HTLC limit. Allowing this value to float would allow\npeers to apply similar congestion avoidance algorithm that are used in TCP\ntoday, and also give us a way to protect the network against future\nunforeseen widespread policy changes (like a raising of the dust limit).\n\n-- Laolu\n\n[1]:\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2020-July/002763.html\n[2]:\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2020-July/002770.html\n[3]: https://github.com/lightning/bolts/pull/868\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220324/bbeddde8/attachment.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2022-03-24T23:36:51",
                "message_text_only": "Oh one other thing I forgot to mention is that switching over the _existing_\nchannels using w/e update protocol presents an accelerated path to\ntaproot/PTLC for the existing channels, as they don't necessarily need any\nnew gossip announcement messages. Instead, after making the switch over,\nthey can start to advertise the new relevant set of feature bits for\nPTLC-like stuff.\n\nThis gives us a bit more freedom as moving the existing channels over to\ntaproot isn't necessarily blocked on deciding which path we want to go re\nchannel announcement evolution.\n\n-- Laolu\n\n\nOn Thu, Mar 24, 2022 at 3:52 PM Olaoluwa Osuntokun <laolu32 at gmail.com>\nwrote:\n\n> Hi y'all,\n>\n> ## Dynamic Commitments Retrospective\n>\n> Two years-ish ago I made a mailing list post on some ideas re dynamic\n> commitments [1], and how the concept can be used to allow us to upgrade\n> channel types on the fly, and also remove pesky hard coded limits like the\n> 483 HTLC in-flight limit that's present today. Back then my main target was\n> upgrading all the existing channels over to the anchor output commitment\n> variant, so the core internal routing network would be more resilient in a\n> persistent high fee environment (which hasn't really happened over the past\n> 2 years for various reasons tbh). Fast forward to today, and with taproot\n> now active on mainnet, and some initial design work/sketches for\n> taproot-native channels underway, I figure it would be good to bump this\n> concept as it gives us a way to upgrade all 80k+ public channels to taproot\n> without any on chain transactions.\n>\n> ## Updating Across Witness Versions w/ Adapter Commitments\n>\n> In my original mail, I incorrectly concluded that the dynamic commitments\n> concept would only really work within the confines of a \"static\" multi-sig\n> output, meaning that it couldn't be used to help channels upgrade to future\n> segwit witness versions.  Thankfully this reply [2] by ZmnSCPxj, outlined a\n> way to achieve this in practice. At a high level he proposes an \"adaptor\n> commitment\" (similar to the kickoff transaction in eltoo/duplex), which is\n> basically an upgrade transaction that spends one witness version type, and\n> produces an output with the next (upgraded) type. In the context of\n> converting from segwit v0 to v1 (taproot), two peers would collaboratively\n> create a new adapter commitment that spends the old v0 multi-sig output,\n> and\n> produces a _new_ v1 multi-sig output. The new commitment transaction would\n> then be anchored using this new output.\n>\n> Here's a rough sequence diagram of the before and after state to better\n> convey the concept:\n>\n>   * Before: fundingOutputV0 -> commitmentTransaction\n>\n>   * After fundingOutputV0 -> fundingOutputV1 (the adapter) ->\n>     commitmentTransaction\n>\n> It *is* still the case that _ultimately_ the two transactions to close the\n> old segwit v0 funding output, and re-open the channel with a new segwit v1\n> funding output are unavoidable. However this adapter commitment lets peers\n> _defer_ these two transactions until closing time. When force closing two\n> transactions need to be confirmed before the commitment outputs can be\n> resolved. However, for co-op close, you can just spend the v0 output, and\n> deliver to the relevant P2TR outputs. The adapter commitment can leverage\n> sighash anyonecanpay to let both parties (assuming it's symmetric) attach\n> additional inputs for fees (to avoid introducing the old update_fee related\n> static fee issues), or alternatively inherit the anchor output pattern at\n> this level.\n>\n> ## Existing Dynamic Commitments Proposals\n>\n> Assuming this concept holds up, then we need an actual concrete protocol to\n> allow for dynamic commitment updates. Last year, Rusty made a spec PR\n> outlining a way to upgrade the commitment type (leveraging the new\n> commitment type feature bits) upon channel re-establish [3]. The proposal\n> relies on another message that both sides send (`stfu`) to clear the\n> commitment (similar to the shutdown semantics) before the switch over\n> happens. However as this is tied to the channel re-establish flow, it\n> doesn't allow both sides to do things like only allow your peer to attach N\n> HTLCs to start with, slowing increasing their allotted slots and possibly\n> reducing them (TCP AIMD style).\n>\n> ## A Two-Phase Dynamic Commitment Update Protocol\n>\n> IMO if we're adding in a way to do commitment/channel upgrades, then it may\n> be worthwhile to go with a more generalized, but slightly more involved\n> route instead. In the remainder of this mail, I'll describe an alternative\n> approach that would allow upgrading nearly all channel/commitment related\n> values (dust limit, max in flight, etc), which is inspired by the way the\n> Raft consensus protocol handles configuration/member changes.\n>\n> For those that aren't aware, Raft is a consensus protocol analogous to\n> Paxos\n> (but isn't byzantine fault tolerant out of the box) that was designed as a\n> more understandable alternative to Paxos for a pedagogical environment.\n> Typically the algorithm is run in the context of a fixed cluster with N\n> machines, but supports adding/removing machines from the cluster with a\n> configuration update protocol. At a high level the way this works is that a\n> new config is sent to the leader, with the leader synchronizing the config\n> change with the other members of the cluster. Once a majority threshold is\n> reached, the leader then commits the config change with the acknowledged\n> parties using the new config (basically a two phase commit). I'm skipping\n> over some edge cases here that can arise if the new nodes participate\n> consensus too early, which can cause a split majority leading to two\n> leaders\n> being elected.\n>\n> Applying this to the LN context is a bit simpler than a generalized\n> protocol, as we typically just have two parties involved. The initiator is\n> already naturally a \"leader\" in our context, as they're the only ones that\n> can do things like trigger fee updates.\n>\n> ### Message Structure\n>\n> At a high level I propose we introduce two new messages, with the fields\n> looking something like this for `commitment_update_propose`:\n>  * type: 0 (`channel_id`)\n>    * value: [`32*byte`:`chan_id`]\n>  * type: 1 (`propose_sig`)\n>    * value: [`64*byte`:`sig`]\n>  * type: 2 (`update_payload`)\n>    * value: [`*byte`:`tlv_payload`]\n>\n> and this `commitment_update_apply`:\n>  * type: 0 (`channel_id`)\n>    * value: [`32*byte`:`chan_id`]\n>  * type: 1 (`local_propose`)\n>    * value: [`*byte`:`commitment_update_propose`]\n>  * type: 2 (`remote_propose`)\n>    * value: [`*byte`:`commitment_update_propose`]\n>\n> ### Protocol Flow\n>\n> The core idea here is that either party can propose a commitment/channel\n> param update, but only the initiator can actually apply it. The\n> `commitment_update_propose` encodes the new set of updates, with a\n> signature\n> covering the TLV blob for the new params (more on why that's needed later).\n> The `commitment_update_apply` includes up to _two_\n> `commitment_update_propose` messages (one for the initiator and one for the\n> responder, as nested TLV messages). The `commitment_update_propose` message\n> would be treated like any other `update_*` message, in that it takes a new\n> commitment signature to properly commit/apply it.\n>\n> The normal flow takes the form of both sides sending a\n> `commitment_update_propose` message, with the initiator finally committing\n> both by sending a `commitment_update_apply` message. In the event that only\n> the responder wants to apply a param change/update, then the initiator can\n> reply immediately with a `commitment_update_apply` message that doesn't\n> include a param change for their commitment (or they just echo the\n> parameters if they're acceptable).\n>\n> ### Handling Retransmissions\n>\n> The role of the signature it to prevent \"spoofing\" by one of the parties\n> (authenticate the param change), and also it serves to convince a party\n> that\n> they actually sent a prior commitment propose update during the\n> retransmission phase. As the `commitment_update_propose` message would be\n> retransmitted like any other message, if the initiator attempts to commit\n> the update but the connection dies, they'll retransmit it as normal along\n> with their latest signature.\n>\n> ### Nested TLV Param Generality\n>\n> The messages as sketched out here just have an opaque nested TLV field\n> which\n> makes it extensible to add in other things like tweaking the total number\n> of\n> max HTLCs, the current dust values, min/max HTLCs, etc (all things that are\n> currently hard coded for the lifetime of the channel). An initial target\n> would likely just be a `chan_type` field, with future feature bits\n> governing\n> _what_ type of commitment updates both parties understand in the future.\n>\n> In the past, when ideas like this were brought up, some were concerned that\n> it wouldn't really be possible to do this type of updates while existing\n> HTLCs were in flight (hence some of the ideas to clear out the commitment\n> beforehand). I don't see a reason why this fundamentally _shouldn't_ be\n> allowed, as from the point of view of the channel update state machine, all\n> updates (adds/removes) get applied as normal, but with this _new_\n> commitment\n> type/params. The main edge case we'll need to consider is cases where the\n> new params make older HTLCs invalid for some reason.\n>\n> ## Conclusion\n>\n> Using the adapter commitment idea combined with a protocol for updating\n> commitments on the fly, would potentially allow us to update all 80k+\n> segwit\n> v0 channels to the base level of taprooty channels without any on chain\n> transactions. The two transactions (open+close) must happen eventually, but\n> by holding another layer of spends off-chain we can defer them (potentially\n> indefinitely, as we have channels today that have been opened for over a\n> year).\n>\n> Deploying a generalised on-the-fly dynamic commitment update protocol gives\n> us a tool to future proof the _existing_ anchored multi-sig outputs in the\n> chain, and also a way to remove many of the hard coded parameters we have\n> today in the protocol. One overly inflexible parameter we have today in the\n> network is the 483 HTLC limit. Allowing this value to float would allow\n> peers to apply similar congestion avoidance algorithm that are used in TCP\n> today, and also give us a way to protect the network against future\n> unforeseen widespread policy changes (like a raising of the dust limit).\n>\n> -- Laolu\n>\n> [1]:\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-July/002763.html\n> [2]:\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-July/002770.html\n> [3]: https://github.com/lightning/bolts/pull/868\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220324/19cab603/attachment-0001.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2022-03-26T00:27:19",
                "message_text_only": "Hi Laolu,\n\nThanks for the proposal, quick feedback.\n\n> It *is* still the case that _ultimately_ the two transactions to close the\n> old segwit v0 funding output, and re-open the channel with a new segwit v1\n> funding output are unavoidable. However this adapter commitment lets peers\n> _defer_ these two transactions until closing time.\n\nI think there is one downside coming with adapter commitment, which is the\nuncertainty of the fee overhead at the closing time. Instead of closing\nyour segwit v0 channel _now_ with known fees, when your commitment is empty\nof time-sensitive HTLCs, you're taking the risk of closing during fees\nspikes, due a move triggered by your counterparty, when you might have\nHTLCs at stake.\n\nIt might be more economically rational for a LN node operator to pay the\nupgrade cost now if they wish  to benefit from the taproot upgrade early,\nespecially if long-term we expect block fees to increase, or wait when\nthere is a \"normal\" cooperative closing.\n\nSo it's unclear to me what the economic gain of adapter commitments ?\n\n> In the remainder of this mail, I'll describe an alternative\n> approach that would allow upgrading nearly all channel/commitment related\n> values (dust limit, max in flight, etc), which is inspired by the way the\n> Raft consensus protocol handles configuration/member changes.\n\nLong-term, I think we'll likely need a consensus protocol anyway for\nmulti-party constructions (channel factories/payment pools). AFAIU this\nproposal doesn't aim to roll out a full-fledged consensus protocol *now*\nthough it could be wise to ensure what we're building slowly moves in this\ndirection. Less critical code to maintain across bitcoin\ncodebases/toolchains.\n\n> The role of the signature it to prevent \"spoofing\" by one of the parties\n> (authenticate the param change), and also it serves to convince a party\nthat\n> they actually sent a prior commitment propose update during the\n> retransmission phase.\n\nWhat's the purpose of data origin authentication if we assume only\ntwo-parties running over Noise_XK ?\n\nI think it's already a security property we have. Though if we think we're\ngoing to reuse these dynamic upgrades for N counterparties communicating\nthrough a coordinator, yes I think it's useful.\n\n> In the past, when ideas like this were brought up, some were concerned\nthat\n> it wouldn't really be possible to do this type of updates while existing\n> HTLCs were in flight (hence some of the ideas to clear out the commitment\n> beforehand).\n\nThe dynamic upgrade might serve in an emergency context where we don't have\nthe leisury to wait for the settlement of the pending HTLCs. The timing of\nthose ones might be beyond the coordination of link counterparties. Thus,\nwe have to allow upgrade of non-empty commitments (and if there are\nundesirable interferences between new commitment types and HTLCs/PTLCs\npresent, deal case-by-case).\n\nAntoine\n\nLe jeu. 24 mars 2022 \u00e0 18:53, Olaoluwa Osuntokun <laolu32 at gmail.com> a\n\u00e9crit :\n\n> Hi y'all,\n>\n> ## Dynamic Commitments Retrospective\n>\n> Two years-ish ago I made a mailing list post on some ideas re dynamic\n> commitments [1], and how the concept can be used to allow us to upgrade\n> channel types on the fly, and also remove pesky hard coded limits like the\n> 483 HTLC in-flight limit that's present today. Back then my main target was\n> upgrading all the existing channels over to the anchor output commitment\n> variant, so the core internal routing network would be more resilient in a\n> persistent high fee environment (which hasn't really happened over the past\n> 2 years for various reasons tbh). Fast forward to today, and with taproot\n> now active on mainnet, and some initial design work/sketches for\n> taproot-native channels underway, I figure it would be good to bump this\n> concept as it gives us a way to upgrade all 80k+ public channels to taproot\n> without any on chain transactions.\n>\n> ## Updating Across Witness Versions w/ Adapter Commitments\n>\n> In my original mail, I incorrectly concluded that the dynamic commitments\n> concept would only really work within the confines of a \"static\" multi-sig\n> output, meaning that it couldn't be used to help channels upgrade to future\n> segwit witness versions.  Thankfully this reply [2] by ZmnSCPxj, outlined a\n> way to achieve this in practice. At a high level he proposes an \"adaptor\n> commitment\" (similar to the kickoff transaction in eltoo/duplex), which is\n> basically an upgrade transaction that spends one witness version type, and\n> produces an output with the next (upgraded) type. In the context of\n> converting from segwit v0 to v1 (taproot), two peers would collaboratively\n> create a new adapter commitment that spends the old v0 multi-sig output,\n> and\n> produces a _new_ v1 multi-sig output. The new commitment transaction would\n> then be anchored using this new output.\n>\n> Here's a rough sequence diagram of the before and after state to better\n> convey the concept:\n>\n>   * Before: fundingOutputV0 -> commitmentTransaction\n>\n>   * After fundingOutputV0 -> fundingOutputV1 (the adapter) ->\n>     commitmentTransaction\n>\n> It *is* still the case that _ultimately_ the two transactions to close the\n> old segwit v0 funding output, and re-open the channel with a new segwit v1\n> funding output are unavoidable. However this adapter commitment lets peers\n> _defer_ these two transactions until closing time. When force closing two\n> transactions need to be confirmed before the commitment outputs can be\n> resolved. However, for co-op close, you can just spend the v0 output, and\n> deliver to the relevant P2TR outputs. The adapter commitment can leverage\n> sighash anyonecanpay to let both parties (assuming it's symmetric) attach\n> additional inputs for fees (to avoid introducing the old update_fee related\n> static fee issues), or alternatively inherit the anchor output pattern at\n> this level.\n>\n> ## Existing Dynamic Commitments Proposals\n>\n> Assuming this concept holds up, then we need an actual concrete protocol to\n> allow for dynamic commitment updates. Last year, Rusty made a spec PR\n> outlining a way to upgrade the commitment type (leveraging the new\n> commitment type feature bits) upon channel re-establish [3]. The proposal\n> relies on another message that both sides send (`stfu`) to clear the\n> commitment (similar to the shutdown semantics) before the switch over\n> happens. However as this is tied to the channel re-establish flow, it\n> doesn't allow both sides to do things like only allow your peer to attach N\n> HTLCs to start with, slowing increasing their allotted slots and possibly\n> reducing them (TCP AIMD style).\n>\n> ## A Two-Phase Dynamic Commitment Update Protocol\n>\n> IMO if we're adding in a way to do commitment/channel upgrades, then it may\n> be worthwhile to go with a more generalized, but slightly more involved\n> route instead. In the remainder of this mail, I'll describe an alternative\n> approach that would allow upgrading nearly all channel/commitment related\n> values (dust limit, max in flight, etc), which is inspired by the way the\n> Raft consensus protocol handles configuration/member changes.\n>\n> For those that aren't aware, Raft is a consensus protocol analogous to\n> Paxos\n> (but isn't byzantine fault tolerant out of the box) that was designed as a\n> more understandable alternative to Paxos for a pedagogical environment.\n> Typically the algorithm is run in the context of a fixed cluster with N\n> machines, but supports adding/removing machines from the cluster with a\n> configuration update protocol. At a high level the way this works is that a\n> new config is sent to the leader, with the leader synchronizing the config\n> change with the other members of the cluster. Once a majority threshold is\n> reached, the leader then commits the config change with the acknowledged\n> parties using the new config (basically a two phase commit). I'm skipping\n> over some edge cases here that can arise if the new nodes participate\n> consensus too early, which can cause a split majority leading to two\n> leaders\n> being elected.\n>\n> Applying this to the LN context is a bit simpler than a generalized\n> protocol, as we typically just have two parties involved. The initiator is\n> already naturally a \"leader\" in our context, as they're the only ones that\n> can do things like trigger fee updates.\n>\n> ### Message Structure\n>\n> At a high level I propose we introduce two new messages, with the fields\n> looking something like this for `commitment_update_propose`:\n>  * type: 0 (`channel_id`)\n>    * value: [`32*byte`:`chan_id`]\n>  * type: 1 (`propose_sig`)\n>    * value: [`64*byte`:`sig`]\n>  * type: 2 (`update_payload`)\n>    * value: [`*byte`:`tlv_payload`]\n>\n> and this `commitment_update_apply`:\n>  * type: 0 (`channel_id`)\n>    * value: [`32*byte`:`chan_id`]\n>  * type: 1 (`local_propose`)\n>    * value: [`*byte`:`commitment_update_propose`]\n>  * type: 2 (`remote_propose`)\n>    * value: [`*byte`:`commitment_update_propose`]\n>\n> ### Protocol Flow\n>\n> The core idea here is that either party can propose a commitment/channel\n> param update, but only the initiator can actually apply it. The\n> `commitment_update_propose` encodes the new set of updates, with a\n> signature\n> covering the TLV blob for the new params (more on why that's needed later).\n> The `commitment_update_apply` includes up to _two_\n> `commitment_update_propose` messages (one for the initiator and one for the\n> responder, as nested TLV messages). The `commitment_update_propose` message\n> would be treated like any other `update_*` message, in that it takes a new\n> commitment signature to properly commit/apply it.\n>\n> The normal flow takes the form of both sides sending a\n> `commitment_update_propose` message, with the initiator finally committing\n> both by sending a `commitment_update_apply` message. In the event that only\n> the responder wants to apply a param change/update, then the initiator can\n> reply immediately with a `commitment_update_apply` message that doesn't\n> include a param change for their commitment (or they just echo the\n> parameters if they're acceptable).\n>\n> ### Handling Retransmissions\n>\n> The role of the signature it to prevent \"spoofing\" by one of the parties\n> (authenticate the param change), and also it serves to convince a party\n> that\n> they actually sent a prior commitment propose update during the\n> retransmission phase. As the `commitment_update_propose` message would be\n> retransmitted like any other message, if the initiator attempts to commit\n> the update but the connection dies, they'll retransmit it as normal along\n> with their latest signature.\n>\n> ### Nested TLV Param Generality\n>\n> The messages as sketched out here just have an opaque nested TLV field\n> which\n> makes it extensible to add in other things like tweaking the total number\n> of\n> max HTLCs, the current dust values, min/max HTLCs, etc (all things that are\n> currently hard coded for the lifetime of the channel). An initial target\n> would likely just be a `chan_type` field, with future feature bits\n> governing\n> _what_ type of commitment updates both parties understand in the future.\n>\n> In the past, when ideas like this were brought up, some were concerned that\n> it wouldn't really be possible to do this type of updates while existing\n> HTLCs were in flight (hence some of the ideas to clear out the commitment\n> beforehand). I don't see a reason why this fundamentally _shouldn't_ be\n> allowed, as from the point of view of the channel update state machine, all\n> updates (adds/removes) get applied as normal, but with this _new_\n> commitment\n> type/params. The main edge case we'll need to consider is cases where the\n> new params make older HTLCs invalid for some reason.\n>\n> ## Conclusion\n>\n> Using the adapter commitment idea combined with a protocol for updating\n> commitments on the fly, would potentially allow us to update all 80k+\n> segwit\n> v0 channels to the base level of taprooty channels without any on chain\n> transactions. The two transactions (open+close) must happen eventually, but\n> by holding another layer of spends off-chain we can defer them (potentially\n> indefinitely, as we have channels today that have been opened for over a\n> year).\n>\n> Deploying a generalised on-the-fly dynamic commitment update protocol gives\n> us a tool to future proof the _existing_ anchored multi-sig outputs in the\n> chain, and also a way to remove many of the hard coded parameters we have\n> today in the protocol. One overly inflexible parameter we have today in the\n> network is the 483 HTLC limit. Allowing this value to float would allow\n> peers to apply similar congestion avoidance algorithm that are used in TCP\n> today, and also give us a way to protect the network against future\n> unforeseen widespread policy changes (like a raising of the dust limit).\n>\n> -- Laolu\n>\n> [1]:\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-July/002763.html\n> [2]:\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-July/002770.html\n> [3]: https://github.com/lightning/bolts/pull/868\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220325/9c2be8e0/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Dynamic Commitments Part 2: Taprooty Edition",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "Olaoluwa Osuntokun",
                "Antoine Riard"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 34236
        }
    },
    {
        "title": "[Lightning-dev] lightning channels, stablecoins and fifty shades of privacy",
        "thread_messages": [
            {
                "author": "pushd",
                "date": "2022-03-26T00:36:25",
                "message_text_only": "Good morning,\n\nThings that affect privacy particularly when large sums of money are involved in bitcoin:\n\nLiquidity, Anonymity set, Amounts, Type of addresses/scripts, Block, Locktime and Version\n\nI have left out things that aren't part of bitcoin protocol or blockchain like KYC. It is difficult for users to move large sums of BTC without being observed because bitcoin does not have confidential transactions to hide amounts. Coinjoin implementations have their own issues, trade-offs, some might even censor transactions and big amounts will still be a problem. Coinswap might be an alternative in future however I wanted to share one solution that could be helpful in improving privacy.\n\nSynonym did first [stablecoin transaction][1] in a lightning channel using Omni BOLT. Consider Alice starts a bitcoin project in which a lightning channel is used for assets like stablecoin. Bob wants to use 1000 BTC linked with an incident. He opens channels with Alice, gets stablecoin which can be used in any project that supports Omni BOLT assets.\n\nQuestions:\n\nWhat is the lightning channel capacity when using Omni BOLT?\n\nWhat else can be improved in this setup? Anything else that I maybe missing?\n\nI added 'fifty shades of privacy' in subject because it was the first thing that came to my mind when I look at privacy in bitcoin and lightning.\n\n[1]: https://youtu.be/MfaqYeyake8\n\npushd\n---\nparallel lines meet at infinity?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/lightning-dev/attachments/20220326/7def2c9b/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "lightning channels, stablecoins and fifty shades of privacy",
            "categories": [
                "Lightning-dev"
            ],
            "authors": [
                "pushd"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1608
        }
    }
]