[
    {
        "title": "[bitcoin-dev] Simple Bitcoin Payment Channel Protocol v0.1 draft (request for comments)",
        "thread_messages": [
            {
                "author": "Rusty Russell",
                "date": "2016-05-01T04:16:20",
                "message_text_only": "Rune Kj\u00e6r Svendsen via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> writes:\n> Dear list\n>\n> I've spent the past couple of months developing a simple protocol for\n> working with payment channels. I've written up a specification of how\n> it operates, in an attempt to standardize the operations of opening,\n> paying and closing.\n\nHi!\n\n        CHECKLOCKTIMEVERIFY [...] allows payment channel\n        setup to be risk free [...] something that was\n        not the case before, when the refund Bitcoin transaction\n        depended on another, unconfirmed Bitcoin transaction. Building\n        on unconfirmed transactions is currently not safe in Bitcoin\n\nWith Segregated Witness, this is now safe.  With that expected soon, I'd\nencourage you to take advantage of it.\n\nCheers,\nRusty."
            }
        ],
        "thread_summary": {
            "title": "Simple Bitcoin Payment Channel Protocol v0.1 draft (request for comments)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Rusty Russell"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 788
        }
    },
    {
        "title": "[bitcoin-dev] segwit subsidy and multi-sender (coinjoin) transactions",
        "thread_messages": [
            {
                "author": "Gregory Maxwell",
                "date": "2016-05-01T16:21:40",
                "message_text_only": "On Fri, Apr 29, 2016 at 6:22 PM, Kristov Atlas via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Has anyone thought about the effects of the 75% Segregated Witness subsidy\n> on CoinJoin transactions and CoinJoin-like transactions?\n\nYes.\n\n> My expectation from the above is that this will serve as a financial\n> disincentive against CoinJoin transactions.\n\nThis does not appear to be the case.\n\nCoinjoin doesn't necessitate any particular behavior that is relevant\nhere -- normal transactions spend a coin and create a payment of an\nexternally specified amount and change; CoinJoins are not special\nin this regard.\n\nUsers may sometimes split up their outputs in an effort to improve\nprivacy, which would have the \"more outputs\" effect you're describing,\nbut more outputs in and of itself would not increase costs under segwit:\n\nThe total cost to a user for creating an output paying themselves is both\nthe cost of the creation and the cost of eventually spending it.\n\nSegwit's cost calculation improvements shifts some relative cost from\nspending to creation, but in these cases same user is paying both.\n\n-- unless you want to assume the user is going to create it and never\nspend it.  In which case, ... they have other issues than transaction\nfees.  And in that case these outputs are creating a perpetual cost on\nthe system, it's prudent that the user creating the additional load\ntake on that cost.\n\n> A sample of the 16 transaction id's posted in the JoinMarket thread on\n> BitcoinTalk shows an average ratio of 1.38 or outputs to inputs\n[...]\n> As we know, a \"traditional\" CoinJoin transaction creates roughly 2x UTXOs\n> for everyone 1 it consumes\n\nIt's odd to state something like that as fact immediately after a providing\nfigure that disproves it...\n\nAlthough for self-sends the output to input ratio doesn't matter for total\ncosts (as I described above), you're missing the important bit of context:\nwhere are other transactions. In block 409711 (current height of my\ntxindex node on my laptop), I see an average of 1.4647 outputs per input.\nThis figure is all over the map in different blocks, however.\n\n> Please refrain from bringing up Schnorr signatures in your reply, since\n> they are not on any immediate roadmap.\n\nSchnorr signatures for Bitcoin have been in the works for  years, and are\none of the first proposed uses of the segwit versioning.\n\n[Comments like this last one from you make it hard to see your message\n as a good-faith inquiry: Schnorr multisignature signature aggregates\n would make CoinJoins massively less expensive, ... that you'd demand\n that your dismissal of it be the final word on the subject leaves\n the impression that you're intentionally calling for a misleading\n presentation of the trade-offs -- there doesn't appear to be a\n disincentive here, but if there were it would be far beyond eliminated\n by a planned use of segwit versioning.]"
            },
            {
                "author": "Peter Todd",
                "date": "2016-05-03T02:05:11",
                "message_text_only": "On Fri, Apr 29, 2016 at 02:22:32PM -0400, Kristov Atlas via bitcoin-dev wrote:\n> A sample of the 16 transaction id's posted in the JoinMarket thread on\n> BitcoinTalk shows an average ratio of 1.38 or outputs to inputs:\n> \n> https://docs.google.com/spreadsheets/d/1p9jZYXxX1HDtKCxTy79Zj5PrQaF20mxbD7BAuz0KC8s/edit?usp=sharing\n> \n> As we know, a \"traditional\" CoinJoin transaction creates roughly 2x UTXOs\n> for everyone 1 it consumes -- 1 spend and 1 change -- unless address reuse\n> comes into play.\n\nNote how this is obviously an unsustainable situation - at some point that\nchange needs to be combined again, or you're throwing away money in the form of\nUTXO's that aren't ever getting spent.\n\nMeanwhile, if you put it another way the segwit discount is an obvious\nadvantage for coinjoin: by making spending UTXO's cheaper, we can recover those\nfunds that would otherwise get lost to dust, becoming ever more difficult to\nspend.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160502/9e82576b/attachment-0001.sig>"
            }
        ],
        "thread_summary": {
            "title": "segwit subsidy and multi-sender (coinjoin) transactions",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Gregory Maxwell",
                "Peter Todd"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 4168
        }
    },
    {
        "title": "[bitcoin-dev] Compact Block Relay BIP",
        "thread_messages": [
            {
                "author": "Matt Corallo",
                "date": "2016-05-02T22:13:22",
                "message_text_only": "Hi all,\n\nThe following is a BIP-formatted design spec for compact block relay\ndesigned to limit on wire bytes during block relay. You can find the\nlatest version of this document at\nhttps://github.com/TheBlueMatt/bips/blob/master/bip-TODO.mediawiki.\n\nThere are several TODO items left on the document as indicated.\nAdditionally, the implementation linked at the bottom of the document\nhas a few remaining TODO items as well:\n\n * Only request compact-block-announcement from one or two peers at a\ntime, as the spec requires.\n * Request new blocks using MSG_CMPCT_BLOCK where appropriate.\n * Fill prefilledtxn with more than just the coinbase, as noted by the\nspec, up to 10K in transactions.\n\nLuke (CC'd): Can you assign a BIP number?\n\nThanks,\nMatt\n\n<pre>\n  BIP: TODO\n  Title: Compact block relay\n  Author: Matt Corallo <bip at bluematt.me>\n  Status: Draft\n  Type: Standards Track\n  Created: 2016-04-27\n</pre>\n\n==Abstract==\n\nCompact blocks on the wire as a way to save bandwidth for nodes on the\nP2P network.\n\nThe key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n\"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this\ndocument are to be interpreted as described in RFC 2119.\n\n==Motivation==\n\nHistorically, the Bitcoin P2P protocol has not been very bandwidth\nefficient for block relay. Every transaction in a block is included when\nrelayed, even though a large number of the transactions in a given block\nare already available to nodes before the block is relayed. This causes\nmoderate inbound bandwidth spikes for nodes when receiving blocks, but\ncan cause very significant outbound bandwidth spikes for some nodes\nwhich receive a block before their peers. When such spikes occur, buffer\nbloat can make consumer-grade internet connections temporarily unusable,\nand can delay the relay of blocks to remote peers who may choose to wait\ninstead of redundantly requesting the same block from other, less\ncongested, peers.\n\nThus, decreasing the bandwidth used during block relay is very useful\nfor many individuals running nodes.\n\nWhile the goal of this work is explicitly not to reduce block transfer\nlatency, it does, as a side effect reduce block transfer latencies in\nsome rather significant ways. Additionally, this work forms a foundation\nfor future work explicitly targeting low-latency block transfer.\n\n==Specification==\n\n===Intended Protocol Flow===\nTODO: Diagrams\n\nThe protocol is intended to be used in two ways, depending on the peers\nand bandwidth available, as discussed [[#Implementation_Details|later]].\nThe \"high-bandwidth\" mode, which nodes may only enable for a few of\ntheir peers, is enabled by setting the first boolean to 1 in a\n\"sendcmpct\" message. In this mode, peers send new block announcements\nwith the short transaction IDs already, possibly even before fully\nvalidating the block. In some cases no further round-trip is needed, and\nthe receiver can reconstruct the block and process it as usual\nimmediately. When some transactions were not available from local\nsources (ie mempool), a getblocktxn/blocktxn roundtrip is neccessary,\nbringing the best-case latency to the same 1.5*RTT minimum time that\nnodes take today, though with significantly less bandwidth usage.\n\nThe \"low-bandwidth\" mode is enabled by setting the first boolean to 0 in\na \"sendcmpct\" message. In this mode, peers send new block announcements\nwith the usual inv/headers announcements (as per BIP130, and after fully\nvalidating the block). The receiving peer may then request the block\nusing a MSG_CMPCT_BLOCK getdata reqeuest, which will receive a response\nof the header and short transaction IDs. In some cases no further\nround-trip is needed, and the receiver can reconstruct the block and\nprocess it as usual, taking the same 1.5*RTT minimum time that nodes\ntake today, though with significantly less bandwidth usage. When some\ntransactions were not available from local sources (ie mempool), a\ngetblocktxn/blocktxn roundtrip is neccessary, bringing the best-case\nlatency to 2.5*RTT, again with significantly less bandwidth usage than\ntoday. Because TCP often exhibits worse transfer latency for larger data\nsizes (as a multiple of RTT), total latency is expected to be reduced\neven when full the 2.5*RTT transfer mechanism is used.\n\n===New data structures===\nSeveral new data structures are added to the P2P network to relay\ncompact blocks: PrefilledTransaction, HeaderAndShortIDs,\nBlockTransactionsRequest, and BlockTransactions. Additionally, we\nintroduce a new variable-length integer encoding for use in these data\nstructures.\n\nFor the purposes of this section, CompactSize refers to the\nvariable-length integer encoding used across the existing P2P protocol\nto encode array lengths, among other things, in 1, 3, 5 or 9 bytes.\n\n====New VarInt====\nTODO: I just copied this out of the src...Something that is\nwiki-formatted and more descriptive should be used here isntead.\n\nVariable-length integers: bytes are a MSB base-128 encoding of the number.\nThe high bit in each byte signifies whether another digit follows. To make\nsure the encoding is one-to-one, one is subtracted from all but the last\ndigit.\nThus, the byte sequence a[] with length len, where all but the last byte\nhas bit 128 set, encodes the number:\n\n(a[len-1] & 0x7F) + sum(i=1..len-1, 128^i*((a[len-i-1] & 0x7F)+1))\n\nProperties:\n* Very small (0-127: 1 byte, 128-16511: 2 bytes, 16512-2113663: 3 bytes)\n* Every integer has exactly one encoding\n* Encoding does not depend on size of original integer type\n* No redundancy: every (infinite) byte sequence corresponds to a list\n  of encoded integers.\n\n0:         [0x00]  256:        [0x81 0x00]\n1:         [0x01]  16383:      [0xFE 0x7F]\n127:       [0x7F]  16384:      [0xFF 0x00]\n128:  [0x80 0x00]  16511: [0x80 0xFF 0x7F]\n255:  [0x80 0x7F]  65535: [0x82 0xFD 0x7F]\n2^32:           [0x8E 0xFE 0xFE 0xFF 0x00]\n\nSeveral uses of New VarInts below are \"differentially encoded\". For\nthese, instead of using raw indexes, the number encoded is the\ndifference between the current index and the previous index, minus one.\nFor example, a first index of 0 implies a real index of 0, a second\nindex of 0 thereafter refers to a real index of 1, etc.\n\n====PrefilledTransaction====\nA PrefilledTransaction structure is used in HeaderAndShortIDs to provide\na list of a few transactions explicitly.\n\n{|\n|Field Name||Type||Size||Encoding||Purpose\n|-\n|index||New VarInt||1-3 bytes||[[#New_VarInt|New VarInt]],\ndifferentially encoded since the last PrefilledTransaction in a\nlist||The index into the block at which this transaction is\n|-\n|tx||Transaction||variable||As encoded in \"tx\" messages||The transaction\nwhich is in the block at index index.\n|}\n\n====HeaderAndShortIDs====\nA HeaderAndShortIDs structure is used to relay a block header, the short\ntransactions IDs used for matching already-available transactions, and a\nselect few transactions which we expect a peer may be missing.\n\n{|\n|Field Name||Type||Size||Encoding||Purpose\n|-\n|header||Block header||80 bytes||First 80 bytes of the block as defined\nby the encoding used by \"block\" messages||The header of the block being\nprovided\n|-\n|nonce||uint64_t||8 bytes||Little Endian||A nonce for use in short\ntransaction ID calculations\n|-\n|shortids_length||CompactSize||1, 3, 5, or 9 bytes||As used elsewhere to\nencode array lengths||The number of short transaction IDs in shortids\n|-\n|shortids||List of uint64_ts||8*shortids_length bytes||Little\nEndian||The short transaction IDs calculated from the transactions which\nwere not provided explicitly in prefilledtxn\n|-\n|prefilledtxn_length||CompactSize||1, 3, 5, or 9 bytes||As used\nelsewhere to encode array lengths||The number of prefilled transactions\nin prefilledtxn\n|-\n|prefilledtxn||List of PrefilledTransactions||variable\nsize*prefilledtxn_length||As defined by PrefilledTransaction definition,\nabove||Used to provide the coinbase transaction and a select few which\nwe expect a peer may be missing\n|}\n\n====BlockTransactionsRequest====\nA BlockTransactionsRequest structure is used to list transaction indexes\nin a block being requested.\n\n{|\n|Field Name||Type||Size||Encoding||Purpose\n|-\n|blockhash||Binary blob||32 bytes||The output from a double-SHA256 of\nthe block header, as used elsewhere||The blockhash of the block which\nthe transactions being requested are in\n|-\n|indexes_length||New VarInt||1-3 bytes||As defined in [[#New_VarInt|New\nVarInt]]||The number of transactions being requested\n|-\n|indexes||List of New VarInts||1-3 bytes*indexes_length||As defined in\n[[#New_VarInt|New VarInt]], differentially encoded||The indexes of the\ntransactions being requested in the block\n|}\n\n====BlockTransactions====\nA BlockTransactions structure is used to provide some of the\ntransactions in a block, as requested.\n\n{|\n|Field Name||Type||Size||Encoding||Purpose\n|-\n|blockhash||Binary blob||32 bytes||The output from a double-SHA256 of\nthe block header, as used elsewhere||The blockhash of the block which\nthe transactions being provided are in\n|-\n|transactions_length||New VarInt||1-3 bytes||As defined in\n[[#New_VarInt|New VarInt]]||The number of transactions provided\n|-\n|transactions||List of Transactions||variable||As encoded in \"tx\"\nmessages||The transactions provided\n|}\n\n====Short transaction IDs====\nShort transaction IDs are used to represent a transaction without\nsending a full 256-bit hash. They are calculated by:\n# single-SHA256 hashing the block header with the nonce appended (in\nlittle-endian)\n# XORing each 8-byte chunk of the double-SHA256 transaction hash with\neach corresponding 8-byte chunk of the hash from the previous step\n# Adding each of the XORed 8-byte chunks together (in little-endian)\niteratively to find the short transaction ID\n\n===New messages===\nA new inv type (MSG_CMPCT_BLOCK == 4) and several new protocol messages\nare added: sendcmpct, cmpctblock, getblocktxn, and blocktxn.\n\n====sendcmpct====\n# The sendcmpct message is defined as a message containing a 1-byte\ninteger followed by a 8-byte integer where pchCommand == \"sendcmpct\".\n# The first integer SHALL be interpreted as a boolean (and MUST have a\nvalue of either 1 or 0)\n# The second integer SHALL be interpreted as a little-endian version\nnumber. Nodes sending a sendcmpct message MUST currently set this value\nto 1.\n# Upon receipt of a \"sendcmpct\" message with the first and second\nintegers set to 1, the node SHOULD announce new blocks by sending a\ncmpctblock message.\n# Upon receipt of a \"sendcmpct\" message with the first integer set to 0,\nthe node SHOULD NOT announce new blocks by sending a cmpctblock message,\nbut SHOULD announce new blocks by sending invs or headers, as defined by\nBIP130.\n# Upon receipt of a \"sendcmpct\" message with the second integer set to\nsomething other than 1, nodes SHOULD treat the peer as if they had not\nreceived the message (as it indicates the peer will provide an\nunexpected encoding in cmpctblock, and/or other, messages)\n# Nodes SHOULD check for a protocol version of >= 70014 before sending\nsendcmpct messages.\n# Nodes MUST NOT send a request for a MSG_CMPCT_BLOCK object to a peer\nbefore having received a sendcmpct message from that peer.\n\n====MSG_CMPCT_BLOCK====\n# getdata messages may now contain requests for MSG_CMPCT_BLOCK objects.\n# Upon receipt of a getdata containing a request for a MSG_CMPCT_BLOCK\nobject with the hash of a block which was recently announced and after\nhaving sent the requesting peer a sendcmpct message, nodes MUST respond\nwith a cmpctblock message containing appropriate data representing the\nblock being requested.\n# MSG_CMPCT_BLOCK inv objects MUST NOT appear anywhere except for in\ngetdata messages.\n\n====cmpctblock====\n# The cmpctblock message is defined as as a message containing a\nserialized HeaderAndShortIDs message and pchCommand == \"cmpctblock\".\n# Upon receipt of a cmpctblock message after sending a sendcmpct\nmessage, nodes SHOULD calculate the short transaction ID for each\nunconfirmed transaction they have available (ie in their mempool) and\ncompare each to each short transaction ID in the cmpctblock message.\n# After finding already-available transactions, nodes which do not have\nall transactions available to reconstruct the full block SHOULD request\nthe missing transactions using a getblocktxn message.\n# A node MUST NOT send a cmpctblock message unless they are able to\nrespond to a getblocktxn message which requests every transaction in the\nblock.\n# A node MUST NOT send a cmpctblock message without having validated\nthat the header properly commits to each transaction in the block, and\nproperly builds on top of the existing chain with a valid proof-of-work.\nA node MAY send a cmpctblock before validating that each transaction in\nthe block validly spends existing UTXO set entries.\n\n====getblocktxn====\n# The getblocktxn message is defined as as a message containing a\nserialized BlockTransactionsRequest message and pchCommand == \"getblocktxn\".\n# Upon receipt of a properly-formatted getblocktxnmessage, nodes which\nrecently provided the sender of such a message a cmpctblock for the\nblock hash identified in this message MUST respond with an appropriate\nblocktxn message. Such a blocktxn message MUST contain exactly and only\neach transaction which is present in the appropriate block at the index\nspecified in the getblocktxn indexes list, in the order requested.\n\n====blocktxn====\n# The blocktxn message is defined as as a message containing a\nserialized BlockTransactions message and pchCommand == \"blocktxn\".\n# Upon receipt of a properly-formatted requested blocktxn message, nodes\nSHOULD attempt to reconstruct the full block by:\n## Taking the prefilledtxn transactions from the original cmpctblock and\nplacing them in the marked positions.\n## For each short transaction ID from the original cmpctblock, in order,\nfind the corresponding transaction either from the blocktxn message or\nfrom other sources and place it in the first available position in the\nblock.\n# Once the block has been reconstructed, it shall be processed as\nnormal, keeping in mind that short transaction IDs are expected to\noccasionally collide, and that nodes MUST NOT be penalized for such\ncollisions, wherever they appear.\n\n===Implementation Notes===\n# For nodes which have sufficient inbound bandwidth, sending a sendcmpct\nmessage with the first integer set to 1 to up to three peers is\nRECOMMENDED. If possible, it is RECOMMENDED that those peers be selected\nbased on their past performance in providing blocks quickly. This will\nallow them to receive some blocks in only 0.5*RTT between them and the\nsending peer. It will also reduce their block transfer latency in other\ncases due to the smaller amount of data transmitted. Nodes MUST NOT send\nsuch sendcmpct messages to all peers, as it encourages wasting outbound\nbandwidth across the network.\n\n# All nodes SHOULD send a sendcmpct message to all appropriate peers.\nThis will reduce their outbound bandwidth usage by allowing their peers\nto request compact blocks instead of full blocks.\n\n# Nodes with limited inbound bandwidth SHOULD request blocks using\nMSG_CMPCT_BLOCK/getblocktxn requests, when possible. While this\nincreases worst-case message round-trips, it is expected to reduce\noverall transfer latency as TCP is more likely to exhibit poor\nthroughput on low-bandwidth nodes.\n\n# Nodes sending cmpctblock messages SHOULD make an attempt to not place\ntoo many transactions into prefilledtxn (ie should limit prefilledtxn to\nonly around 10KB of transactions). When in doubt, nodes SHOULD only\ninclude the coinbase transaction in prefilledtxn.\n\n# Nodes MAY pick one nonce per block they wish to send, and only build a\ncmpctblock message once for all peers which they wish to send a given\nblock to. Nodes SHOULD NOT use the same nonce across multiple different\nblocks.\n\n# Nodes MAY impose additional requirements on when they announce new\nblocks by sending cmpctblock messages. For example, nodes with limited\noutbound bandwidth MAY choose to announce new blocks using inv/header\nmessages (as per BIP130) to conserve outbound bandwidth.\n\n# Note that the MSG_CMPCT_BLOCK section does not require that nodes\nrespond to MSG_CMPCT_BLOCK getdata requests for blocks which they did\nnot recently announce. This allows nodes to calculate cmpctblock\nmessages at announce-time instead of at request-time. Thus, nodes MUST\nNOT request blocks using MSG_CMPCT_BLOCK getdatas unless it is in\nresponse to an inv/headers block announcement (as per BIP130), and MUST\nNOT request blocks using MSG_CMPCT_BLOCK getdatas in response to headers\nmessages which were, themselves, responses to getheaders requests.\n\n# While the current version sends transactions with the same encodings\nas is used in tx messages and elsewhere in the protocol, the version\nfield in sendcmpct is intended to allow this to change in the future.\nFor this reason, it is recommended that the code used to decode\nPrefilledTransaction and BlockTransactions messages be prepared to take\na different transaction encoding, if and when the version field in\nsendcmpct changes in a future BIP.\n\n==Justification==\n\n====Protocol design====\nThere have been many proposals to save wire bytes when relaying blocks.\nMany of them have a two-fold goal of reducing block relay time and thus\nrely on the use of significant processing power in order to avoid\nintroducing additional worst-case RTTs. Because this work is not focused\nprimarily on reducing block relay time, its design is much simpler (ie\ndoes not rely on set reconciliation protocols). Still, in testing at the\ntime of writing, nodes are able to relay blocks without the extra\ngetblocktxn/blocktxn RTT around 90% of the time. With a smart\ncompact-block-announcement policy, it is thus expected that this work\nmight allow blocks to be relayed between nodes in 0.5*RTT instead of\n1.5*RTT at least 75% of the time.\n\n====Use of New VarInts====\nBitcoin has long had a variable-length integer implementation (referred\nto as CompactSize in this document), making a second a strange protocol\nquirk. However, in this protocol most of our variable-length integers\nare between 0 and 2000. For both encodings, small numbers (<100) are\nencoded as 1-byte. For numbers over 250, the CompactSize encoding begins\nto use 3 bytes instead of 1, whereas the New VarInt encoding uses 2.\nBecause the primary motivation for this work is to save bytes during\nblock relay, the extra byte of saving per transaction-difference is\nconsidered worth the extra design complexity.\n\n====Short transaction ID calculation====\nThe short transaction ID calculation is designed to take absolutely\nminimal processing time during block compaction to avoid introducing\nserious DoS vulnerabilities such as those introduced by the\nbloom-filtering in BIP 37. As such, it is possible for a node to\nconstruct one compact-block representation of a block for relay to\nmultiple peers. Additionally, only one cryptographic hash (2 SHA rounds)\nis used when calculating the short transaction IDs for an entire block.\n\nThe XOR-and-add method is used for calculating short transaction IDs\nprimarily because it is fast and is reasonably able to limit the ability\nof an attacker who does not know the block hash or nonce to cause\ncollisions in short transaction IDs. If an attacker were able to cause\nsuch collisions, filling mempools (and, thus, blocks) with them would\ncause poor network propagation of new (or non-attacker, in the case of a\nminer) blocks.\n\nThe 8-byte nonce in short transaction ID calculation is used to\nintroduce additional entropy on a per-node level. While the use of 8\nbytes is sufficient for an attacker to maliciously cause short\ntransaction ID collisions in their own block relay, this would have less\nof an effect than if such an attacker were relaying headers/invs and not\nresponding to requests for the full block.\n\n==Backward compatibility==\n\nOlder clients remain fully compatible and interoperable after this change.\n\n==Implementation==\n\nhttps://github.com/TheBlueMatt/bitcoin/tree/udp\n\n==Acknowledgements==\n\nThanks to Gregory Maxwell for the initial suggestion as well as a lot of\nback-and-forth design and significant testing.\n\n==Copyright==\n\nThis document is placed in the public domain."
            },
            {
                "author": "Gregory Maxwell",
                "date": "2016-05-03T05:02:28",
                "message_text_only": "On Mon, May 2, 2016 at 10:13 PM, Matt Corallo via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Hi all,\n>\n> The following is a BIP-formatted design spec for compact block relay\n> designed to limit on wire bytes during block relay. You can find the\n> latest version of this document at\n> https://github.com/TheBlueMatt/bips/blob/master/bip-TODO.mediawiki.\n\nThanks Matt!\n\nI've been testing this for a couple weeks (in various forms).  I've\nbeen getting over 96% reduction in block-bytes sent. I don't have a\ngood metric for it, but bandwidth spikes are greatly reduced. The\nlargest blocktxn message I've seen on a node that has been up for at\nleast a day is 475736 bytes. 94% of the blocks less than 100kb must be\nsent in total.\n\nIn the opportunistic mode my measurements are showing 73% of blocks\ntransferred with 0.5 RTT even without prediction, 87% if up to 4\nadditional transactions are predicted, and 91% for 30 transactions (my\nrough estimate for the 10k maximum prediction suggested in the BIP."
            },
            {
                "author": "Matt Corallo",
                "date": "2016-05-06T03:09:14",
                "message_text_only": "Thanks Greg for the testing!\n\nNote that to those who are reviewing the doc, a few minor tweaks to\nwording and clarification have been made to the git version, so please\nreview there.\n\nOn 05/03/16 05:02, Gregory Maxwell wrote:\n> On Mon, May 2, 2016 at 10:13 PM, Matt Corallo via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> Hi all,\n>>\n>> The following is a BIP-formatted design spec for compact block relay\n>> designed to limit on wire bytes during block relay. You can find the\n>> latest version of this document at\n>> https://github.com/TheBlueMatt/bips/blob/master/bip-TODO.mediawiki.\n> \n> Thanks Matt!\n> \n> I've been testing this for a couple weeks (in various forms).  I've\n> been getting over 96% reduction in block-bytes sent. I don't have a\n> good metric for it, but bandwidth spikes are greatly reduced. The\n> largest blocktxn message I've seen on a node that has been up for at\n> least a day is 475736 bytes. 94% of the blocks less than 100kb must be\n> sent in total.\n> \n> In the opportunistic mode my measurements are showing 73% of blocks\n> transferred with 0.5 RTT even without prediction, 87% if up to 4\n> additional transactions are predicted, and 91% for 30 transactions (my\n> rough estimate for the 10k maximum prediction suggested in the BIP.\n>"
            },
            {
                "author": "Johnathan Corgan",
                "date": "2016-05-08T00:40:25",
                "message_text_only": "There was some confusion over the following email which was posted to the list\nwhich appears to have been cancelled before a decision could be reached.\n\nPlease note the email seems inflammatory in the \"acknowledgement\" section and\nreally should have been rewritten to contain specific details of the objection\nand corrections expected.\n\nTo be clear posts to the mailing list are either approved, or rejected for not\nmeeting the posting standards. This allows the author to make a quick correction\nand resubmit. All rejections are cc'd to\nhttps://lists.ozlabs.org/pipermail/bitcoin-dev-moderation/\nfor transparency. Sometimes moderators get delayed - this week has been a busy\nwith lots of distractions one for everyone :)\n\nI'm copying the entire message below:\n\n---------- Forwarded message ----------\nFrom: Tom <tomz at freedommail.ch>\nTo: bitcoin-dev at lists.linuxfoundation.org, Matt Corallo\n<lf-lists at mattcorallo.com>\nCc:\nDate: Fri, 06 May 2016 13:31:15 +0100\nSubject: Re: [bitcoin-dev] Compact Block Relay BIP\nOn Monday 02 May 2016 22:13:22 Matt Corallo via bitcoin-dev wrote:\n\nThanks for putting in the time to make a spec!\n\nIt looks good already, but I do think some more improvements can be made.\n\n\n> ===Intended Protocol Flow===\nI'm not a fan of the solution that a CNode should keep state and talk to\nits remote nodes differently while announcing new blocks.\nIts too complicated and ultimately counter-productive.\n\nThe problem is that an individual node needs to predict network behaviour in\nadvance. With the downside that if it guesses wrong that both nodes end up\npaying for the wrong guess.\nThis is not a good way to design a p2p layer.\n\n\n\nI would suggest that a new block is announced to all nodes equally and then\nindividual nodes can respond with a request of either a 'compact' or a\nnormal block.\nThis is much more in line with the current design as well.\n\nDetection if remote nodes support compact blocks, for the purpose of\nrequesting a compact-block, can be done either via a network-bit or just a\nprotocol version. Or something else entirely, if you have better\nsuggestions.\n\n\n\n> Variable-length integers: bytes are a MSB base-128 encoding of the\n> number.\n> The high bit in each byte signifies whether another digit follows.\n> [snip bitwise spec]\n\nI suggest just referring to UTF-8 which describes this just fine.\nit is good practice to refer to existing specs when possible and not copy\nthe details.\n\n> ====Short transaction IDs====\n> Short transaction IDs are used to represent a transaction without\n> sending a full 256-bit hash. They are calculated by:\n> # single-SHA256 hashing the block header with the nonce appended (in\n> little-endian)\n> # XORing each 8-byte chunk of the double-SHA256 transaction hash with\n> each corresponding 8-byte chunk of the hash from the previous step\n> # Adding each of the XORed 8-byte chunks together (in little-endian)\n> iteratively to find the short transaction ID\n\nI don't think this is needed. Just use the first 8 bytes.\nThe reason to do xor-ing doesn't hold up and extra complexity is unneeded.\nEspecially since you mention some lines down;\n\n> The short transaction ID calculation is designed to take absolutely\n> minimal processing time during block compaction to avoid introducing\n> serious DoS vulnerabilities\n\n\n==Acknowledgements==\n\nI think you need to acknowledge some more people, or just remove this\nparagraph.\n\nCheers\n\n\n---------- Forwarded message ----------\nFrom: bitcoin-dev-request at lists.linuxfoundation.org\nTo:\nCc:\nDate: Fri, 06 May 2016 12:31:23 +0000\nSubject: confirm 37d25406a07ab77823fba5f9b450438c410ccd75\nIf you reply to this message, keeping the Subject: header intact,\nMailman will discard the held message.  Do this if the message is\nspam.  If you reply to this message and include an Approved: header\nwith the list password in it, the message will be approved for posting\nto the list.  The Approved: header can also appear in the first line\nof the body of the reply.\n\n\nOn Mon, May 2, 2016 at 3:13 PM, Matt Corallo via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi all,\n>\n> The following is a BIP-formatted design spec for compact block relay\n> designed to limit on wire bytes during block relay. You can find the\n> latest version of this document at\n> https://github.com/TheBlueMatt/bips/blob/master/bip-TODO.mediawiki.\n>\n> There are several TODO items left on the document as indicated.\n> Additionally, the implementation linked at the bottom of the document\n> has a few remaining TODO items as well:\n>\n>  * Only request compact-block-announcement from one or two peers at a\n> time, as the spec requires.\n>  * Request new blocks using MSG_CMPCT_BLOCK where appropriate.\n>  * Fill prefilledtxn with more than just the coinbase, as noted by the\n> spec, up to 10K in transactions.\n>\n> Luke (CC'd): Can you assign a BIP number?\n>\n> Thanks,\n> Matt\n>\n> <pre>\n>   BIP: TODO\n>   Title: Compact block relay\n>   Author: Matt Corallo <bip at bluematt.me>\n>   Status: Draft\n>   Type: Standards Track\n>   Created: 2016-04-27\n> </pre>\n>\n> ==Abstract==\n>\n> Compact blocks on the wire as a way to save bandwidth for nodes on the\n> P2P network.\n>\n> The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n> \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this\n> document are to be interpreted as described in RFC 2119.\n>\n> ==Motivation==\n>\n> Historically, the Bitcoin P2P protocol has not been very bandwidth\n> efficient for block relay. Every transaction in a block is included when\n> relayed, even though a large number of the transactions in a given block\n> are already available to nodes before the block is relayed. This causes\n> moderate inbound bandwidth spikes for nodes when receiving blocks, but\n> can cause very significant outbound bandwidth spikes for some nodes\n> which receive a block before their peers. When such spikes occur, buffer\n> bloat can make consumer-grade internet connections temporarily unusable,\n> and can delay the relay of blocks to remote peers who may choose to wait\n> instead of redundantly requesting the same block from other, less\n> congested, peers.\n>\n> Thus, decreasing the bandwidth used during block relay is very useful\n> for many individuals running nodes.\n>\n> While the goal of this work is explicitly not to reduce block transfer\n> latency, it does, as a side effect reduce block transfer latencies in\n> some rather significant ways. Additionally, this work forms a foundation\n> for future work explicitly targeting low-latency block transfer.\n>\n> ==Specification==\n>\n> ===Intended Protocol Flow===\n> TODO: Diagrams\n>\n> The protocol is intended to be used in two ways, depending on the peers\n> and bandwidth available, as discussed [[#Implementation_Details|later]].\n> The \"high-bandwidth\" mode, which nodes may only enable for a few of\n> their peers, is enabled by setting the first boolean to 1 in a\n> \"sendcmpct\" message. In this mode, peers send new block announcements\n> with the short transaction IDs already, possibly even before fully\n> validating the block. In some cases no further round-trip is needed, and\n> the receiver can reconstruct the block and process it as usual\n> immediately. When some transactions were not available from local\n> sources (ie mempool), a getblocktxn/blocktxn roundtrip is neccessary,\n> bringing the best-case latency to the same 1.5*RTT minimum time that\n> nodes take today, though with significantly less bandwidth usage.\n>\n> The \"low-bandwidth\" mode is enabled by setting the first boolean to 0 in\n> a \"sendcmpct\" message. In this mode, peers send new block announcements\n> with the usual inv/headers announcements (as per BIP130, and after fully\n> validating the block). The receiving peer may then request the block\n> using a MSG_CMPCT_BLOCK getdata reqeuest, which will receive a response\n> of the header and short transaction IDs. In some cases no further\n> round-trip is needed, and the receiver can reconstruct the block and\n> process it as usual, taking the same 1.5*RTT minimum time that nodes\n> take today, though with significantly less bandwidth usage. When some\n> transactions were not available from local sources (ie mempool), a\n> getblocktxn/blocktxn roundtrip is neccessary, bringing the best-case\n> latency to 2.5*RTT, again with significantly less bandwidth usage than\n> today. Because TCP often exhibits worse transfer latency for larger data\n> sizes (as a multiple of RTT), total latency is expected to be reduced\n> even when full the 2.5*RTT transfer mechanism is used.\n>\n> ===New data structures===\n> Several new data structures are added to the P2P network to relay\n> compact blocks: PrefilledTransaction, HeaderAndShortIDs,\n> BlockTransactionsRequest, and BlockTransactions. Additionally, we\n> introduce a new variable-length integer encoding for use in these data\n> structures.\n>\n> For the purposes of this section, CompactSize refers to the\n> variable-length integer encoding used across the existing P2P protocol\n> to encode array lengths, among other things, in 1, 3, 5 or 9 bytes.\n>\n> ====New VarInt====\n> TODO: I just copied this out of the src...Something that is\n> wiki-formatted and more descriptive should be used here isntead.\n>\n> Variable-length integers: bytes are a MSB base-128 encoding of the number.\n> The high bit in each byte signifies whether another digit follows. To make\n> sure the encoding is one-to-one, one is subtracted from all but the last\n> digit.\n> Thus, the byte sequence a[] with length len, where all but the last byte\n> has bit 128 set, encodes the number:\n>\n> (a[len-1] & 0x7F) + sum(i=1..len-1, 128^i*((a[len-i-1] & 0x7F)+1))\n>\n> Properties:\n> * Very small (0-127: 1 byte, 128-16511: 2 bytes, 16512-2113663: 3 bytes)\n> * Every integer has exactly one encoding\n> * Encoding does not depend on size of original integer type\n> * No redundancy: every (infinite) byte sequence corresponds to a list\n>   of encoded integers.\n>\n> 0:         [0x00]  256:        [0x81 0x00]\n> 1:         [0x01]  16383:      [0xFE 0x7F]\n> 127:       [0x7F]  16384:      [0xFF 0x00]\n> 128:  [0x80 0x00]  16511: [0x80 0xFF 0x7F]\n> 255:  [0x80 0x7F]  65535: [0x82 0xFD 0x7F]\n> 2^32:           [0x8E 0xFE 0xFE 0xFF 0x00]\n>\n> Several uses of New VarInts below are \"differentially encoded\". For\n> these, instead of using raw indexes, the number encoded is the\n> difference between the current index and the previous index, minus one.\n> For example, a first index of 0 implies a real index of 0, a second\n> index of 0 thereafter refers to a real index of 1, etc.\n>\n> ====PrefilledTransaction====\n> A PrefilledTransaction structure is used in HeaderAndShortIDs to provide\n> a list of a few transactions explicitly.\n>\n> {|\n> |Field Name||Type||Size||Encoding||Purpose\n> |-\n> |index||New VarInt||1-3 bytes||[[#New_VarInt|New VarInt]],\n> differentially encoded since the last PrefilledTransaction in a\n> list||The index into the block at which this transaction is\n> |-\n> |tx||Transaction||variable||As encoded in \"tx\" messages||The transaction\n> which is in the block at index index.\n> |}\n>\n> ====HeaderAndShortIDs====\n> A HeaderAndShortIDs structure is used to relay a block header, the short\n> transactions IDs used for matching already-available transactions, and a\n> select few transactions which we expect a peer may be missing.\n>\n> {|\n> |Field Name||Type||Size||Encoding||Purpose\n> |-\n> |header||Block header||80 bytes||First 80 bytes of the block as defined\n> by the encoding used by \"block\" messages||The header of the block being\n> provided\n> |-\n> |nonce||uint64_t||8 bytes||Little Endian||A nonce for use in short\n> transaction ID calculations\n> |-\n> |shortids_length||CompactSize||1, 3, 5, or 9 bytes||As used elsewhere to\n> encode array lengths||The number of short transaction IDs in shortids\n> |-\n> |shortids||List of uint64_ts||8*shortids_length bytes||Little\n> Endian||The short transaction IDs calculated from the transactions which\n> were not provided explicitly in prefilledtxn\n> |-\n> |prefilledtxn_length||CompactSize||1, 3, 5, or 9 bytes||As used\n> elsewhere to encode array lengths||The number of prefilled transactions\n> in prefilledtxn\n> |-\n> |prefilledtxn||List of PrefilledTransactions||variable\n> size*prefilledtxn_length||As defined by PrefilledTransaction definition,\n> above||Used to provide the coinbase transaction and a select few which\n> we expect a peer may be missing\n> |}\n>\n> ====BlockTransactionsRequest====\n> A BlockTransactionsRequest structure is used to list transaction indexes\n> in a block being requested.\n>\n> {|\n> |Field Name||Type||Size||Encoding||Purpose\n> |-\n> |blockhash||Binary blob||32 bytes||The output from a double-SHA256 of\n> the block header, as used elsewhere||The blockhash of the block which\n> the transactions being requested are in\n> |-\n> |indexes_length||New VarInt||1-3 bytes||As defined in [[#New_VarInt|New\n> VarInt]]||The number of transactions being requested\n> |-\n> |indexes||List of New VarInts||1-3 bytes*indexes_length||As defined in\n> [[#New_VarInt|New VarInt]], differentially encoded||The indexes of the\n> transactions being requested in the block\n> |}\n>\n> ====BlockTransactions====\n> A BlockTransactions structure is used to provide some of the\n> transactions in a block, as requested.\n>\n> {|\n> |Field Name||Type||Size||Encoding||Purpose\n> |-\n> |blockhash||Binary blob||32 bytes||The output from a double-SHA256 of\n> the block header, as used elsewhere||The blockhash of the block which\n> the transactions being provided are in\n> |-\n> |transactions_length||New VarInt||1-3 bytes||As defined in\n> [[#New_VarInt|New VarInt]]||The number of transactions provided\n> |-\n> |transactions||List of Transactions||variable||As encoded in \"tx\"\n> messages||The transactions provided\n> |}\n>\n> ====Short transaction IDs====\n> Short transaction IDs are used to represent a transaction without\n> sending a full 256-bit hash. They are calculated by:\n> # single-SHA256 hashing the block header with the nonce appended (in\n> little-endian)\n> # XORing each 8-byte chunk of the double-SHA256 transaction hash with\n> each corresponding 8-byte chunk of the hash from the previous step\n> # Adding each of the XORed 8-byte chunks together (in little-endian)\n> iteratively to find the short transaction ID\n>\n> ===New messages===\n> A new inv type (MSG_CMPCT_BLOCK == 4) and several new protocol messages\n> are added: sendcmpct, cmpctblock, getblocktxn, and blocktxn.\n>\n> ====sendcmpct====\n> # The sendcmpct message is defined as a message containing a 1-byte\n> integer followed by a 8-byte integer where pchCommand == \"sendcmpct\".\n> # The first integer SHALL be interpreted as a boolean (and MUST have a\n> value of either 1 or 0)\n> # The second integer SHALL be interpreted as a little-endian version\n> number. Nodes sending a sendcmpct message MUST currently set this value\n> to 1.\n> # Upon receipt of a \"sendcmpct\" message with the first and second\n> integers set to 1, the node SHOULD announce new blocks by sending a\n> cmpctblock message.\n> # Upon receipt of a \"sendcmpct\" message with the first integer set to 0,\n> the node SHOULD NOT announce new blocks by sending a cmpctblock message,\n> but SHOULD announce new blocks by sending invs or headers, as defined by\n> BIP130.\n> # Upon receipt of a \"sendcmpct\" message with the second integer set to\n> something other than 1, nodes SHOULD treat the peer as if they had not\n> received the message (as it indicates the peer will provide an\n> unexpected encoding in cmpctblock, and/or other, messages)\n> # Nodes SHOULD check for a protocol version of >= 70014 before sending\n> sendcmpct messages.\n> # Nodes MUST NOT send a request for a MSG_CMPCT_BLOCK object to a peer\n> before having received a sendcmpct message from that peer.\n>\n> ====MSG_CMPCT_BLOCK====\n> # getdata messages may now contain requests for MSG_CMPCT_BLOCK objects.\n> # Upon receipt of a getdata containing a request for a MSG_CMPCT_BLOCK\n> object with the hash of a block which was recently announced and after\n> having sent the requesting peer a sendcmpct message, nodes MUST respond\n> with a cmpctblock message containing appropriate data representing the\n> block being requested.\n> # MSG_CMPCT_BLOCK inv objects MUST NOT appear anywhere except for in\n> getdata messages.\n>\n> ====cmpctblock====\n> # The cmpctblock message is defined as as a message containing a\n> serialized HeaderAndShortIDs message and pchCommand == \"cmpctblock\".\n> # Upon receipt of a cmpctblock message after sending a sendcmpct\n> message, nodes SHOULD calculate the short transaction ID for each\n> unconfirmed transaction they have available (ie in their mempool) and\n> compare each to each short transaction ID in the cmpctblock message.\n> # After finding already-available transactions, nodes which do not have\n> all transactions available to reconstruct the full block SHOULD request\n> the missing transactions using a getblocktxn message.\n> # A node MUST NOT send a cmpctblock message unless they are able to\n> respond to a getblocktxn message which requests every transaction in the\n> block.\n> # A node MUST NOT send a cmpctblock message without having validated\n> that the header properly commits to each transaction in the block, and\n> properly builds on top of the existing chain with a valid proof-of-work.\n> A node MAY send a cmpctblock before validating that each transaction in\n> the block validly spends existing UTXO set entries.\n>\n> ====getblocktxn====\n> # The getblocktxn message is defined as as a message containing a\n> serialized BlockTransactionsRequest message and pchCommand ==\n> \"getblocktxn\".\n> # Upon receipt of a properly-formatted getblocktxnmessage, nodes which\n> recently provided the sender of such a message a cmpctblock for the\n> block hash identified in this message MUST respond with an appropriate\n> blocktxn message. Such a blocktxn message MUST contain exactly and only\n> each transaction which is present in the appropriate block at the index\n> specified in the getblocktxn indexes list, in the order requested.\n>\n> ====blocktxn====\n> # The blocktxn message is defined as as a message containing a\n> serialized BlockTransactions message and pchCommand == \"blocktxn\".\n> # Upon receipt of a properly-formatted requested blocktxn message, nodes\n> SHOULD attempt to reconstruct the full block by:\n> ## Taking the prefilledtxn transactions from the original cmpctblock and\n> placing them in the marked positions.\n> ## For each short transaction ID from the original cmpctblock, in order,\n> find the corresponding transaction either from the blocktxn message or\n> from other sources and place it in the first available position in the\n> block.\n> # Once the block has been reconstructed, it shall be processed as\n> normal, keeping in mind that short transaction IDs are expected to\n> occasionally collide, and that nodes MUST NOT be penalized for such\n> collisions, wherever they appear.\n>\n> ===Implementation Notes===\n> # For nodes which have sufficient inbound bandwidth, sending a sendcmpct\n> message with the first integer set to 1 to up to three peers is\n> RECOMMENDED. If possible, it is RECOMMENDED that those peers be selected\n> based on their past performance in providing blocks quickly. This will\n> allow them to receive some blocks in only 0.5*RTT between them and the\n> sending peer. It will also reduce their block transfer latency in other\n> cases due to the smaller amount of data transmitted. Nodes MUST NOT send\n> such sendcmpct messages to all peers, as it encourages wasting outbound\n> bandwidth across the network.\n>\n> # All nodes SHOULD send a sendcmpct message to all appropriate peers.\n> This will reduce their outbound bandwidth usage by allowing their peers\n> to request compact blocks instead of full blocks.\n>\n> # Nodes with limited inbound bandwidth SHOULD request blocks using\n> MSG_CMPCT_BLOCK/getblocktxn requests, when possible. While this\n> increases worst-case message round-trips, it is expected to reduce\n> overall transfer latency as TCP is more likely to exhibit poor\n> throughput on low-bandwidth nodes.\n>\n> # Nodes sending cmpctblock messages SHOULD make an attempt to not place\n> too many transactions into prefilledtxn (ie should limit prefilledtxn to\n> only around 10KB of transactions). When in doubt, nodes SHOULD only\n> include the coinbase transaction in prefilledtxn.\n>\n> # Nodes MAY pick one nonce per block they wish to send, and only build a\n> cmpctblock message once for all peers which they wish to send a given\n> block to. Nodes SHOULD NOT use the same nonce across multiple different\n> blocks.\n>\n> # Nodes MAY impose additional requirements on when they announce new\n> blocks by sending cmpctblock messages. For example, nodes with limited\n> outbound bandwidth MAY choose to announce new blocks using inv/header\n> messages (as per BIP130) to conserve outbound bandwidth.\n>\n> # Note that the MSG_CMPCT_BLOCK section does not require that nodes\n> respond to MSG_CMPCT_BLOCK getdata requests for blocks which they did\n> not recently announce. This allows nodes to calculate cmpctblock\n> messages at announce-time instead of at request-time. Thus, nodes MUST\n> NOT request blocks using MSG_CMPCT_BLOCK getdatas unless it is in\n> response to an inv/headers block announcement (as per BIP130), and MUST\n> NOT request blocks using MSG_CMPCT_BLOCK getdatas in response to headers\n> messages which were, themselves, responses to getheaders requests.\n>\n> # While the current version sends transactions with the same encodings\n> as is used in tx messages and elsewhere in the protocol, the version\n> field in sendcmpct is intended to allow this to change in the future.\n> For this reason, it is recommended that the code used to decode\n> PrefilledTransaction and BlockTransactions messages be prepared to take\n> a different transaction encoding, if and when the version field in\n> sendcmpct changes in a future BIP.\n>\n> ==Justification==\n>\n> ====Protocol design====\n> There have been many proposals to save wire bytes when relaying blocks.\n> Many of them have a two-fold goal of reducing block relay time and thus\n> rely on the use of significant processing power in order to avoid\n> introducing additional worst-case RTTs. Because this work is not focused\n> primarily on reducing block relay time, its design is much simpler (ie\n> does not rely on set reconciliation protocols). Still, in testing at the\n> time of writing, nodes are able to relay blocks without the extra\n> getblocktxn/blocktxn RTT around 90% of the time. With a smart\n> compact-block-announcement policy, it is thus expected that this work\n> might allow blocks to be relayed between nodes in 0.5*RTT instead of\n> 1.5*RTT at least 75% of the time.\n>\n> ====Use of New VarInts====\n> Bitcoin has long had a variable-length integer implementation (referred\n> to as CompactSize in this document), making a second a strange protocol\n> quirk. However, in this protocol most of our variable-length integers\n> are between 0 and 2000. For both encodings, small numbers (<100) are\n> encoded as 1-byte. For numbers over 250, the CompactSize encoding begins\n> to use 3 bytes instead of 1, whereas the New VarInt encoding uses 2.\n> Because the primary motivation for this work is to save bytes during\n> block relay, the extra byte of saving per transaction-difference is\n> considered worth the extra design complexity.\n>\n> ====Short transaction ID calculation====\n> The short transaction ID calculation is designed to take absolutely\n> minimal processing time during block compaction to avoid introducing\n> serious DoS vulnerabilities such as those introduced by the\n> bloom-filtering in BIP 37. As such, it is possible for a node to\n> construct one compact-block representation of a block for relay to\n> multiple peers. Additionally, only one cryptographic hash (2 SHA rounds)\n> is used when calculating the short transaction IDs for an entire block.\n>\n> The XOR-and-add method is used for calculating short transaction IDs\n> primarily because it is fast and is reasonably able to limit the ability\n> of an attacker who does not know the block hash or nonce to cause\n> collisions in short transaction IDs. If an attacker were able to cause\n> such collisions, filling mempools (and, thus, blocks) with them would\n> cause poor network propagation of new (or non-attacker, in the case of a\n> miner) blocks.\n>\n> The 8-byte nonce in short transaction ID calculation is used to\n> introduce additional entropy on a per-node level. While the use of 8\n> bytes is sufficient for an attacker to maliciously cause short\n> transaction ID collisions in their own block relay, this would have less\n> of an effect than if such an attacker were relaying headers/invs and not\n> responding to requests for the full block.\n>\n> ==Backward compatibility==\n>\n> Older clients remain fully compatible and interoperable after this change.\n>\n> ==Implementation==\n>\n> https://github.com/TheBlueMatt/bitcoin/tree/udp\n>\n> ==Acknowledgements==\n>\n> Thanks to Gregory Maxwell for the initial suggestion as well as a lot of\n> back-and-forth design and significant testing.\n>\n> ==Copyright==\n>\n> This document is placed in the public domain.\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n\n\n\n-- \nJohnathan Corgan\nCorgan Labs - SDR Training and Development Services\nhttp://corganlabs.com\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160507/028cac26/attachment-0001.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2016-05-08T03:24:22",
                "message_text_only": "(This response was originally off-list as moderators were still\ndeciding, here it is for those interested).\n\nHi Tom,\n\nThanks for reading the draft text and commenting! Replies inline.\n\nMatt\n\nOn 05/08/16 00:40, Johnathan Corgan wrote:\n> ---------- Forwarded message ----------\n> From: Tom <tomz at freedommail.ch <mailto:tomz at freedommail.ch>>\n> To: bitcoin-dev at lists.linuxfoundation.org\n> <mailto:bitcoin-dev at lists.linuxfoundation.org>, Matt Corallo <lf-lists at mattcorallo.com <mailto:lf-lists at mattcorallo.com>>\n> Cc: \n> Date: Fri, 06 May 2016 13:31:15 +0100\n> Subject: Re: [bitcoin-dev] Compact Block Relay BIP\n> On Monday 02 May 2016 22:13:22 Matt Corallo via bitcoin-dev wrote:\n> \n> Thanks for putting in the time to make a spec!\n> \n> It looks good already, but I do think some more improvements can be made.\n> \n> \n>> ===Intended Protocol Flow===\n> I'm not a fan of the solution that a CNode should keep state and talk to\n> its remote nodes differently while announcing new blocks.\n> Its too complicated and ultimately counter-productive.\n> \n> The problem is that an individual node needs to predict network behaviour in\n> advance. With the downside that if it guesses wrong that both nodes end up\n> paying for the wrong guess.\n> This is not a good way to design a p2p layer.\n\nNodes don't need to predict much in advance, and the cost for predicting\nwrong is 0 if your peers receive blocks with a few hundred ms between\nthem (as we should expect) and you haven't set the announce bit on more\nthan a few peers (as the spec requires for this reason). As for\ncomplexity of keeping state, think of it as a version flag in much the\nsame way sendheaders operates.\n\nIt seems I forgot to add a suggested peer-preforwarding-selection\nalgorithm in the text, but the intended use-case is to set the bit on\npeers which recently provided you blocks faster than other peers, up to\nonly one or three peers. This is both simple and should be incredibly\neffective.\n\n[This has now been clarified in the BIP text]\n\n> I would suggest that a new block is announced to all nodes equally and then\n> individual nodes can respond with a request of either a 'compact' or a\n> normal block.\n> This is much more in line with the current design as well.\n> \n> Detection if remote nodes support compact blocks, for the purpose of\n> requesting a compact-block, can be done either via a network-bit or just a\n> protocol version. Or something else entirely, if you have better\n> suggestions.\n\nIn line with recent trends, neither service bits nor protocol versions\nare particularly well-suited for this purpose. Protocol versions are\nimpossible to handle sanely across different nodes on the network, as\nthey cannot indicate optional features. Service bits, while somewhat\nmore appropriate for this purpose, are a very limited resource which is\ngenerally better suited to indicating significant new features which\nnodes might need for correct operation, and thus might wish to actively\nseek out when making connections. I'm not sure anyone is suggesting that\nhere, and absent that recent agreement preferred message-based feature\nindication instead of version-message-extension.\n\n>> Variable-length integers: bytes are a MSB base-128 encoding of the\n>> number.\n>> The high bit in each byte signifies whether another digit follows.\n>> [snip bitwise spec]\n> \n> I suggest just referring to UTF-8 which describes this just fine.\n> it is good practice to refer to existing specs when possible and not copy\n> the details.\n\nHmm? There is no UTF anywhere in this protocol. Indeed this section\nneeds to be rewritten, as indicated. I'd recommend you read the code\nuntil I update the section with better text if you're confused.\n\n>> ====Short transaction IDs====\n>> Short transaction IDs are used to represent a transaction without\n>> sending a full 256-bit hash. They are calculated by:\n>> # single-SHA256 hashing the block header with the nonce appended (in\n>> little-endian)\n>> # XORing each 8-byte chunk of the double-SHA256 transaction hash with\n>> each corresponding 8-byte chunk of the hash from the previous step\n>> # Adding each of the XORed 8-byte chunks together (in little-endian)\n>> iteratively to find the short transaction ID\n> \n> I don't think this is needed. Just use the first 8 bytes.\n> The reason to do xor-ing doesn't hold up and extra complexity is unneeded.\n> Especially since you mention some lines down;\n> \n>> The short transaction ID calculation is designed to take absolutely\n>> minimal processing time during block compaction to avoid introducing\n>> serious DoS vulnerabilities\n\nI'm confused as to what, specifically, you're proposing this be changed\nto. I'm pretty sure the proposed protocol is about as simple as you can\nget while retaining some reasonable collision resistance. I might,\nhowever, decide to switch to siphash with a very low round count, given\nthat it's probably faster than the cache-fill-time taken by just\niterating over the mempool. Needs a bit further investigation.\n\n> ==Acknowledgements==\n> \n> I think you need to acknowledge some more people, or just remove this\n> paragraph.\n> \n> Cheers\n\nGreg was the only large contributor to the document (and was a very\nlarge contributor, as mentioned - the work is based hugely on a protocol\nrecommendation he wrote up several years ago) don't see why this should\nmean he doesn't get credit.\n\n[For those interested, I'm referring here to\nhttps://en.bitcoin.it/wiki/User:Gmaxwell/block_network_coding. This\nBIP/the implementation is a precursor to an implementation that looks\nsimilar to what Greg proposes there which can be found on my udp-wip\nbranch, which is based on and uses the data structures involved here.]"
            },
            {
                "author": "Tom Zander",
                "date": "2016-05-09T09:35:32",
                "message_text_only": "On Sunday, May 08, 2016 03:24:22 AM Matt Corallo wrote:\n> >> ===Intended Protocol Flow===\n> > \n> > I'm not a fan of the solution that a CNode should keep state and talk to\n> > its remote nodes differently while announcing new blocks.\n> > Its too complicated and ultimately counter-productive.\n> > \n> > The problem is that an individual node needs to predict network behaviour\n> > in advance. With the downside that if it guesses wrong that both nodes\n> > end up paying for the wrong guess.\n> > This is not a good way to design a p2p layer.\n> \n> Nodes don't need to predict much in advance, and the cost for predicting\n> wrong is 0 if your peers receive blocks with a few hundred ms between\n> them (as we should expect) and you haven't set the announce bit on more\n> than a few peers (as the spec requires for this reason).\n\nYou misunderstand the networking effects.\nThe fact that your node is required to choose which one to set the announce \nbit on implies that it needs to predict which node will have the best data in \nthe future.\nIt needs to predict which nodes will not start being incommunicado and it \nrequires them to predict all the things that are not possible to predict in a \nnetwork.\nIn networking it is even more true than in stocks; results of the past are no \nguarantee for the future.\n\nThis means you are creating a fragile system. Your system will only work in \nlaboratory situations.  It will fail spectacularly when the network or the \ninternet is under stress or some parts fall away.\n\n\nAnother problem with your solution is that nodes send a much larger amount of \nunsolicited data to peers in the form of the thin-block compared to the normal \ninv or header-first data.\n\nSaying this is mitigated by only subscribing on this data from a small \nsubsection of nodes means you position yourself in a situation that I \ndisplayed above. A tradeoff of fragile and fast.  With no possible way to make \na node automatically decide on a good equilibrium.\n\n\n> It seems I forgot to add a suggested peer-preforwarding-selection\n> algorithm in the text, but the intended use-case is to set the bit on\n> peers which recently provided you blocks faster than other peers, up to\n> only one or three peers. This is both simple and should be incredibly\n> effective.\n\nNetwork autorepair systems have been researched for decades, no real solution \nhas as of yet appeared. \nPHDs are written on the subject and you want to make this a design for Bitcoin \nbased on \"[it] should be incredibly effective\", I think you are underestimating \nthe subject matter you are dealing with.\n\n\n> > I would suggest that a new block is announced to all nodes equally and\n> > then\n> > individual nodes can respond with a request of either a 'compact' or a\n> > normal block.\n> > This is much more in line with the current design as well.\n> > \n> > Detection if remote nodes support compact blocks, for the purpose of\n> > requesting a compact-block, can be done either via a network-bit or just a\n> > protocol version. Or something else entirely, if you have better\n> > suggestions.\n> \n> In line with recent trends, neither service bits nor protocol versions\n> are particularly well-suited for this purpose.\n\nAm I to understand that you choose the solution based on the fact that service \nbits are too expensive to extend? (if not, please respond to my previous \nquestion actually answering the suggestion)\n\nThat sounds like a rather bad way of doing design. Maybe you can add a second \nservice bits field of message instead and then do the compact blocks correctly.\n\n\n> >> Variable-length integers: bytes are a MSB base-128 encoding of the\n> >> number.\n> >> The high bit in each byte signifies whether another digit follows.\n> >> [snip bitwise spec]\n> > \n> > I suggest just referring to UTF-8 which describes this just fine.\n> > it is good practice to refer to existing specs when possible and not copy\n> > the details.\n> \n> Hmm? There is no UTF anywhere in this protocol. Indeed this section\n> needs to be rewritten, as indicated. I'd recommend you read the code\n> until I update the section with better text if you're confused.\n\nWait, you didn't steal the variable length encoding from an existing standard \nand you programmed a new one?\nI strongly suggest you don't reinvent this kind of protocol level encodings \nbut instead steal from something like UTF8. Which has been around for decades.\n\nPlease base your standard on other standards where possible.\n\nLook at UTF-8 on wikipedia, you may have \"invented\" the same encoding that IBM \npublished in 1992.\n\n\n> >> ====Short transaction IDs====\n> >> Short transaction IDs are used to represent a transaction without\n> >> sending a full 256-bit hash. They are calculated by:\n> >> # single-SHA256 hashing the block header with the nonce appended (in\n> >> little-endian)\n> >> # XORing each 8-byte chunk of the double-SHA256 transaction hash with\n> >> each corresponding 8-byte chunk of the hash from the previous step\n> >> # Adding each of the XORed 8-byte chunks together (in little-endian)\n> >> iteratively to find the short transaction ID\n> > \n> > I don't think this is needed. Just use the first 8 bytes.\n> > The reason to do xor-ing doesn't hold up and extra complexity is unneeded.\n> > Especially since you mention some lines down;\n> > \n> >> The short transaction ID calculation is designed to take absolutely\n> >> minimal processing time during block compaction to avoid introducing\n> >> serious DoS vulnerabilities\n> \n> I'm confused as to what, specifically, you're proposing this be changed\n> to.\n\nJust the first (highest) 8 bytes of a sha256 hash.\n\nThe amount of collisions will not be less if you start xoring the rest.\nThe whole reason for doing this extra work is also irrelevant as a spam \nprotection. \n\n-- \nTom Zander"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2016-05-09T10:43:02",
                "message_text_only": "On Mon, May 9, 2016 at 9:35 AM, Tom Zander via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> You misunderstand the networking effects.\n> The fact that your node is required to choose which one to set the announce\n> bit on implies that it needs to predict which node will have the best data in\n> the future.\n\nNot required. It may. If it chooses fortunately, latency is reduced--\nto 0.5 RTT in many cases. If not-- nothing harmful happens.\n\nTesting on actual nodes in the actual network (not a \"lab\") shows that\nblocks are normally requested from one of the last three peers they\nwere requested from 70% of the time, with no special affordances or\nskipping samples when peers disconnected.\n\n(77% for last 4, 88% for last 8)\n\nThis also _increases_ robustness. Right now a single peer failing at\nthe wrong time will delay blocks with a long time out. In high\nbandwidth mode the redundancy means that node will be much more likely\nto make progress without timeout delays-- so long at least one of the\nthe selected opportunistic mode peers was successful.\n\nBecause the decision is non-normative to the protocol, nodes can\ndecide based on better criteria if better criteria is discovered in\nthe future.\n\n> Another problem with your solution is that nodes send a much larger amount of\n> unsolicited data to peers in the form of the thin-block compared to the normal\n> inv or header-first data.\n\n\"High bandwidth\" mode uses somewhat more bandwidth than low\nbandwidth... but still >>10 times less than an ordinary getdata relay\nwhich is used ubiquitously today.\n\nIf a node is trying to minimize bandwidth usage, it can choose to not\nrequest the \"high bandwidth\" mode.\n\nThe latency bound cannot be achieved without unsolicited data. The\nbest we can while achieving 0.5 RTT is try to arrange things so that\nthe information received is maximally useful and as small as\nreasonably possible.\n\nIf receivers implemented joint decoding (combining multiple\ncomprblocks in the event of faild decoding) 4 byte IDs would be\ncompletely reasonable, and were what I originally suggested (along\nwith forward error correction data, in that case).\n\n> Am I to understand that you choose the solution based on the fact that service\n> bits are too expensive to extend? (if not, please respond to my previous\n> question actually answering the suggestion)\n>\n> That sounds like a rather bad way of doing design. Maybe you can add a second\n> service bits field of message instead and then do the compact blocks correctly.\n\nService bits are not generally a good mechanism for negating optional\npeer-local parameters.\n\nThe settings for compactblocks can change at runtime, having to\nreconnect to change them would be obnoxious.\n\n> Wait, you didn't steal the variable length encoding from an existing standard\n> and you programmed a new one?\n\nThis is one of the two variable length encodings used for years in\nBitcoin Core. This is just the first time it's shown up in a BIP.\n\n[It's a little disconcerting that you appear to be maintaining a fork\nand are unaware of this.]\n\n> Look at UTF-8 on wikipedia, you may have \"invented\" the same encoding that IBM\n> published in 1992.\n\nThe similarity with UTF-8 is that both are variable length and some\ncontrol information is in the high bits. The similarity ends there.\n\nUTF-8 is more complex and less efficient for this application (coding\nsmall numbers), as it has to handle things like resynchronization\nwhich are critical in text but irrelevant in our framed, checksummed,\nreliably transported binary protocol.\n\n> Just the first (highest) 8 bytes of a sha256 hash.\n>\n> The amount of collisions will not be less if you start xoring the rest.\n> The whole reason for doing this extra work is also irrelevant as a spam\n> protection.\n\nThen you expose it to a trivial collision attack:  To find two 64 bit\nhashes that collide I need perform only roughly 2^32 computation. Then\nI can send them to the network.  You cannot reason about these systems\njust by assuming that bad things happen only according to pure chance.\n\nThis issue is eliminated by salting the hash.  Moreover, with\nper-source randomization of the hash, when a rare chance collision\nhappens it only impacts a single node at a time, so the propagation\ndoesn't stall network wide on an unlucky block; it just goes slower on\na tiny number of links a tiny percent of the time (instead of breaking\neverywhere an even tinyer amount of the time)-- in the non-attacker,\nchance event case."
            },
            {
                "author": "Tom",
                "date": "2016-05-09T11:32:59",
                "message_text_only": "On Monday 09 May 2016 10:43:02 Gregory Maxwell wrote:\n> On Mon, May 9, 2016 at 9:35 AM, Tom Zander via bitcoin-dev\n> \n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > You misunderstand the networking effects.\n> > The fact that your node is required to choose which one to set the\n> > announce\n> > bit on implies that it needs to predict which node will have the best data\n> > in the future.\n> \n> Not required. It may. \n\nIt is required, in the reference of wanting to actually use compact block \nrelay.\n\n\n> Testing on actual nodes in the actual network (not a \"lab\") shows\n\nApologies, I thought that the term was wider known.  \"Laboratory situations\" \nis used where I am from as the opposite of real-world messy and unpredictable \nsituations.\n\nSo, your measurements may be true, but are not useful to decide how well it \nbehaves under less optimal situations. aka \"the real world\".\n\n> This also _increases_ robustness. Right now a single peer failing at\n> the wrong time will delay blocks with a long time out.\n\nIf your peers that were supposed to send you a compact block fail, then you'll \nend up in exactly that same situation again.  Only with various timeouts in \nbetween before you get your block making it a magnitude slower.\n\nIn networking this is solved by reacting instead of predicting. The network is \nnot stable. Your protocol design assumes it to be.\n\n\n> > Another problem with your solution is that nodes send a much larger amount\n> > of unsolicited data to peers in the form of the thin-block compared to\n> > the normal inv or header-first data.\n> \n> \"High bandwidth\" mode \n\nAnother place where I may have explained better.\nThis is not about the difference about the two modes of your design.\nThis is about the design as a whole. As compared to current.\n\n\n> > Am I to understand that you choose the solution based on the fact that\n> > service bits are too expensive to extend? (if not, please respond to my\n> > previous question actually answering the suggestion)\n> > \n> > That sounds like a rather bad way of doing design. Maybe you can add a\n> > second service bits field of message instead and then do the compact\n> > blocks correctly.\n> Service bits are not generally a good mechanism for negating optional\n> peer-local parameters.\n\nService bits are exactly the right solution to indicate additional p2p \nfeature-support.\n\n\n> [It's a little disconcerting that you appear to be maintaining a fork\n> and are unaware of this.]\n\nehm...\n\n\n> > Wait, you didn't steal the variable length encoding from an existing\n> > standard and you programmed a new one?\n> \n> This is one of the two variable length encodings used for years in\n> Bitcoin Core. This is just the first time it's shown up in a BIP.\n>\n> > Look at UTF-8 on wikipedia, you may have \"invented\" the same encoding that\n> > IBM published in 1992.\n> \n> The similarity with UTF-8 is that both are variable length and some\n> control information is in the high bits. The similarity ends there.\n\nThat's all fine and well, it doesn't at any point take away from my point that \nany specification should NOT invent something new that has for decades had a \ngreat specification already.\n\nIf you make a spec to be used by all nodes, on the wire, don't base it on your \nproprietary implementation. Please.\n\n\n> > Just the first (highest) 8 bytes of a sha256 hash.\n> > \n> > The amount of collisions will not be less if you start xoring the rest.\n> > The whole reason for doing this extra work is also irrelevant as a spam\n> > protection.\n> \n> Then you expose it to a trivial collision attack:  To find two 64 bit\n> hashes that collide I need perform only roughly 2^32 computation. Then\n> I can send them to the network.\n\nNo, you still need to have done a POW.\n\nNext to that, your scheme is 2^32 computations *and* some XORs. The XORs are \npercentage wise a rounding error on the total time. So your argument also \ndestroys your own addition.\n\n> This issue is eliminated by salting the hash. \n\nThe issue is better eliminated by not allowing nodes to send uninvited large \nmessages.\n\nI don't think we're getting anywhere.\n\nI'm not sold on your design and I explained why. I tried explaining in this \nemail some misconceptions that may have appeared after my initial emails. I \nhope things are more clear."
            },
            {
                "author": "Peter Todd",
                "date": "2016-05-09T13:40:55",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA512\n\n\n\nOn 9 May 2016 07:32:59 GMT-04:00, Tom via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>On Monday 09 May 2016 10:43:02 Gregory Maxwell wrote:\n>> Service bits are not generally a good mechanism for negating optional\n>> peer-local parameters.\n>\n>Service bits are exactly the right solution to indicate additional p2p\n>feature-support.\n>\n>\n>> [It's a little disconcerting that you appear to be maintaining a fork\n>> and are unaware of this.]\n>\n>ehm...\n\nCan you please explain why you moved the above part of gmaxwell's reply to here, when previously it was right after:\n\n>> > Wait, you didn't steal the variable length encoding from an\n>existing\n>> > standard and you programmed a new one?\n>>\n>> This is one of the two variable length encodings used for years in\n>> Bitcoin Core. This is just the first time it's shown up in a BIP.\n\nhere?\n\nEditing gmaxwells reply like that changes the tone of the message significantly.\n-----BEGIN PGP SIGNATURE-----\n\niQE9BAEBCgAnIBxQZXRlciBUb2RkIDxwZXRlQHBldGVydG9kZC5vcmc+BQJXMJNd\nAAoJEGOZARBE6K+yz4MH/0fQNM8SQdT7a1zljOSJW17ZLs6cEwVXZc/fOtvrNnOa\nCkzXqylPrdT+BWBhPOwDlrzRa/2w5JAJDHRFoR8ZEidasxNDuSfhT3PwulBxmBqs\nqoXhg0ujzRv9736vKENzMI4y2HbfHmqOrlLSZrlk8zqBGmlp1fMqVjFriQN66dnV\n6cYFVyMVz0x/e4mXw8FigSQxkDAJ6gnfSInecQuZLT7H4g2xomIs6kQbqULHAylS\nsFaK4uXy7Vr/sgBbitEQPDHGwywRoA+7EhExb2XpvL6hdyQbL1G1i6SPxGkwKg7R\nMAuBPku/FraGo+qfcaA8R7eYKmyP4qZfZly317Aoo6Q=\n=NtSN\n-----END PGP SIGNATURE-----"
            },
            {
                "author": "Tom",
                "date": "2016-05-09T13:57:10",
                "message_text_only": "On Monday 09 May 2016 13:40:55 Peter Todd wrote:\n> >> [It's a little disconcerting that you appear to be maintaining a fork\n> >> and are unaware of this.]\n> >\n> >ehm...\n> \n> Can you please explain why you moved the above part of gmaxwell's reply to\n> here,\n\nA personal attack had no place in the technical discussion, I moved it out.\n\n\n\nInitially I asked him to please avoid personal attacks, but I thought better \nof it and edited my reply to just \"ehm...\".\n\n\nThe moderators failed to catch his aggressive tone while moderating my post \n(see archives) for being too aggressive.\n\nI'm sure this message will also not be allowed through. I would not even blame \nthe moderators since this, and Peters, messages were both off-topic.\n\nI thank you for todays talks, it makes me certain of the thing I said this \nweekend on Reddit that this list is not a suitable place for all the different \nstakeholders to talk on a level playing field.\n\nIf any of you agree, please urge the approach that we replace the entire \nmoderation team with a new one. This will be the least painful solution for \neveryone in the ecosystem.\n\nThanks again."
            },
            {
                "author": "Bryan Bishop",
                "date": "2016-05-09T14:04:20",
                "message_text_only": "On Mon, May 9, 2016 at 8:57 AM, Tom via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> The moderators failed to catch his aggressive tone while moderating my post\n> (see archives) for being too aggressive.\n>\n\nIIRC you were previously informed by moderators (on the same reddit thread\nto which you refer) that it would seem you had canceled your email from the\nmoderation queue, contrary to your retelling above. This is now reaching\nfar into off-topic and further posts on this subject should be sent to\nbitcoin-discuss at lists.linuxfoundation.org or\nbitcoin-dev-owners at lists.linuxfoundation.org instead of the bitcoin-dev\nmailing list.\n\n- Bryan\nhttp://heybryan.org/\n1 512 203 0507\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160509/041d1aa5/attachment-0001.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2016-05-09T17:06:06",
                "message_text_only": "On 05/03/2016 12:13 AM, lf-lists at mattcorallo.com (Matt Corallo) wrote:\n> Hi all,\n> \n> The following is a BIP-formatted design spec for compact block relay\n> designed to limit on wire bytes during block relay. You can find the\n> latest version of this document at\n> https://github.com/TheBlueMatt/bips/blob/master/bip-TODO.mediawiki.\n\nHi Matt,\n\nthank you for working on this!\n\n> ===New data structures===\n> Several new data structures are added to the P2P network to relay\n> compact blocks: PrefilledTransaction, HeaderAndShortIDs,\n> BlockTransactionsRequest, and BlockTransactions. Additionally, we\n> introduce a new variable-length integer encoding for use in these data\n> structures.\n> \n> For the purposes of this section, CompactSize refers to the\n> variable-length integer encoding used across the existing P2P protocol\n> to encode array lengths, among other things, in 1, 3, 5 or 9 bytes.\n\nThis is a not, but I think it's a bit strange to have two separate\nvariable length integers in the same specification. I understand is one\nis already the default for variable-length integers currently, and there\nare reasons to use the other one for efficiency reasons in some places,\nbut perhaps we should aim to get everything using the latter?\n\n> ====New VarInt====\n> Variable-length integers: bytes are a MSB base-128 encoding of the number.\n> The high bit in each byte signifies whether another digit follows. To make\n> sure the encoding is one-to-one, one is subtracted from all but the last\n> digit.\n\nMaybe it's worth mentioning that it is based on ASN.1 BER's compressed\ninteger format (see\nhttps://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf\nsection 8.1.3.5), though with a small modification to make every integer\nhave a single unique encoding.\n\n> ====HeaderAndShortIDs====\n> A HeaderAndShortIDs structure is used to relay a block header, the short\n> transactions IDs used for matching already-available transactions, and a\n> select few transactions which we expect a peer may be missing.\n> \n> |shortids||List of uint64_ts||8*shortids_length bytes||Little\n> Endian||The short transaction IDs calculated from the transactions which\n> were not provided explicitly in prefilledtxn\n\nI tried to derive what length of short ids is actually necessary (some\nwrite-up is on\nhttps://gist.github.com/sipa/b2eb2e486156b5509ac711edd16153ed but it's\nincomplete).\n\nFor any reasonable numbers I can come up with (in a very wide range),\nthe number of bits needed is very well approximated by:\n\n  log2(#receiver_mempool_txn * #block_txn_not_in_receiver_mempool /\nacceptable_per_block_failure_rate)\n\nFor example, with 20000 mempool transactions, 2500 transactions in a\nblock, 95% hitrate, and a chance of 1 in 10000 blocks to fail to\nreconstruct, needed_bits = log2(20000 * 2500 * (1 - 0.95) / 0.0001) =\n34.54, or 5 byte txids would suffice.\n\nNote that 1 in 10000 failures may sound like a lot, but this is for each\nindividual connection, and since every transmission uses separately\nsalted identifiers, occasional failures should not affect global\npropagation. Given that transmission failures due to timeouts, network\nconnectivity, ... already occur much more frequently than once every few\ngigabytes (what 10000 blocks corresponds to), that's probably already\nmore than enough.\n\nIn short: I believe 5 or 6 byte txids should be enough, but perhaps it\nmakes sense to allow the sender to choose (so he can weigh trying\nmultiple nonces against increasing the short txid length).\n\n> ====Short transaction IDs====\n> Short transaction IDs are used to represent a transaction without\n> sending a full 256-bit hash. They are calculated by:\n> # single-SHA256 hashing the block header with the nonce appended (in\n> little-endian)\n> # XORing each 8-byte chunk of the double-SHA256 transaction hash with\n> each corresponding 8-byte chunk of the hash from the previous step\n> # Adding each of the XORed 8-byte chunks together (in little-endian)\n> iteratively to find the short transaction ID\n\nAn alternative would be using SipHash-1-3 (a form of SipHash with\nreduced iteration counts; the default is SipHash-2-4). SipHash was\ndesigned as a Message Authentication Code, where the security\nrequirements are much stronger than in our case (in particular, we don't\ncare about observers being able to finding the key, as the key is just\npublic knowledge here). One of the designers of SipHash has commented\nthat SipHash-1-3 for collision resistance in hash tables may be enough:\nhttps://github.com/rust-lang/rust/issues/29754#issuecomment-156073946\n\nUsing SipHash-1-3 on modern hardware would take ~32 CPU cycles per txid.\n\n> ===Implementation Notes===\n\nThere are a few more heuristics that MAY be used to improve performance:\n\n* Receivers should treat short txids in blocks that match multiple\nmempool transactions as non-matches, and request the transactions. This\nsignificantly reduces the failure to reconstruct.\n\n* When constructing a compact block to send, the sender can verify it\nagainst its own mempool to check for collisions, and if so, choose to\neither try another nonce, or increase the short txid length.\n\nCheers,\n\n-- \nPieter"
            },
            {
                "author": "Peter R",
                "date": "2016-05-09T18:34:47",
                "message_text_only": "Hi Pieter,\n\n> I tried to derive what length of short ids is actually necessary (some\n> write-up is on\n> https://gist.github.com/sipa/b2eb2e486156b5509ac711edd16153ed but it's\n> incomplete).\n> \n> For any reasonable numbers I can come up with (in a very wide range),\n> the number of bits needed is very well approximated by:\n> \n>  log2(#receiver_mempool_txn * #block_txn_not_in_receiver_mempool /\n> acceptable_per_block_failure_rate)\n> \n> For example, with 20000 mempool transactions, 2500 transactions in a\n> block, 95% hitrate, and a chance of 1 in 10000 blocks to fail to\n> reconstruct, needed_bits = log2(20000 * 2500 * (1 - 0.95) / 0.0001) =\n> 34.54, or 5 byte txids would suffice.\n> \n> Note that 1 in 10000 failures may sound like a lot, but this is for each\n> individual connection, and since every transmission uses separately\n> salted identifiers, occasional failures should not affect global\n> propagation. Given that transmission failures due to timeouts, network\n> connectivity, ... already occur much more frequently than once every few\n> gigabytes (what 10000 blocks corresponds to), that's probably already\n> more than enough.\n> \n> In short: I believe 5 or 6 byte txids should be enough, but perhaps it\n> makes sense to allow the sender to choose (so he can weigh trying\n> multiple nonces against increasing the short txid length).\n\n[9 May 16 @ 11am PDT]  \n\nWe worked on this with respect to \u201cXthin\" for Bitcoin Unlimited, and came to a similar conclusion.  \n\nBut we (I think it was theZerg) also noticed another trick: if the node receiving the thin blocks has a small number of collisions with transactions in its mempool (e.g., 1 or 2), then it can test each possible block against the Merkle root in the block header to determine the correct one.  Using this technique, it should be possible to further reduce the number of bytes used for the txids.  That being said, even thin blocks built from 64-bit short IDs represent a tremendous savings compared to standard block propagation.  So we (Bitcoin Unlimited) decided not to pursue this optimization any further at that time.\n\n***\n\nIt\u2019s also interesting to ask what the information-theoretic minimum amount of information necessary for a node to re-construct a block is. The way I\u2019m thinking about this currently[1] is that the node needs all of the transactions in the block that were not initially part of its mempool, plus enough information to select and ordered subset from that mempool that represents the block.  If m is the number of transactions in mempool and n is the number of transactions in the block, then the number of possible subsets (C') is given by the binomial coefficient:\n\n  C' =  m! / [n! (m - n)!]\n\nSince there are n! possible orderings for each subset, the total number of possible blocks (C) of size n from a mempool of size m is\n\n  C = n! C\u2019 = m! / (m-n)!\n\nAssuming that all possible blocks are equally likely, the Shannon entropy (the information that must be communicated) is the base-2 logarithm of the number of possible blocks.  After making some approximations, this works out very close to\n\n   minimum information ~= n * log2(m),\n\nwhich for your case of 20,000 transactions in mempool (m = 20,000) and a 2500-transaction block (n = 2500), yields\n\n   minimum information = 2500 * log2(20,000) ~ 2500 * 15 bits.\n\nIn other words, a lower bound on the information required is about 2 bytes per transactions for every transaction in the block that the node is already aware of, as well as all the missing transactions in full. \n\nOf course, this assumes an unlimited number of round trips, and it is probably complicated by other factors that I haven\u2019t considered (queue the \u201cspherical cow\u201d jokes :), but I thought it was interesting that a technique like Xthin or compact blocks is already pretty close to this limit.  \n\nCheers,\nPeter \n\n[1] There are still some things that I can\u2019t wrap my mind around that I\u2019d love to discuss with another math geek :)"
            },
            {
                "author": "Rusty Russell",
                "date": "2016-05-10T05:28:19",
                "message_text_only": "Pieter Wuille via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> writes:\n> On 05/03/2016 12:13 AM, lf-lists at mattcorallo.com (Matt Corallo) wrote:\n>> Hi all,\n>> \n>> The following is a BIP-formatted design spec for compact block relay\n>> designed to limit on wire bytes during block relay. You can find the\n>> latest version of this document at\n>> https://github.com/TheBlueMatt/bips/blob/master/bip-TODO.mediawiki.\n>\n> Hi Matt,\n>\n> thank you for working on this!\n\nIndeed!  Sorry for the delayed feedback.\n\n>> |shortids||List of uint64_ts||8*shortids_length bytes||Little\n>> Endian||The short transaction IDs calculated from the transactions which\n>> were not provided explicitly in prefilledtxn\n>\n> I tried to derive what length of short ids is actually necessary (some\n> write-up is on\n> https://gist.github.com/sipa/b2eb2e486156b5509ac711edd16153ed but it's\n> incomplete).\n\nI did this for IBLT testing.\n\nI used variable-length bit encodings, and used the shortest encoding\nwhich is unique to you (including mempool).  It's a little more work,\nbut for an average node transmitting a block with 1300 txs and another\n~3000 in the mempool, you expect about 12 bits per transaction.  IOW,\nabout 1/5 of your current size.  Critically, we might be able to fit in\ntwo or three TCP packets.\n\nThe wire encoding of all those bit arrays was:\n  [varint-min-numbits] - Shortest bit array length\n  [varint-array-size]  - Number of bit arrays.\n          [varint-num].... - Number of entries in array N (x varint-array-size)\n  [packed-bit-arrays...]\n\n  Last byte was padded with zeros.\n  See: https://github.com/rustyrussell/bitcoin-iblt/blob/master/wire_encode.cpp#L12\n\nI would also avoid the nonce to save recalculating for each node, and\ninstead define an id as:\n\n        [<64-bit-short-id>][txid]\n\nSince you only ever send as many bits as needed to distinguish, this only\nmakes a difference if there actually are collisions.\n\nAs Peter R points out, we could later enhance receiver to brute force\ncollisions (you could speed that by sending a XOR of all the txids, but\nreally if there are more than a few collisions, give up).\n\nAnd a prototype could just always send 64-bit ids to start.\n\nCheers,\nRusty."
            },
            {
                "author": "Gregory Maxwell",
                "date": "2016-05-10T10:07:27",
                "message_text_only": "On Tue, May 10, 2016 at 5:28 AM, Rusty Russell via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> I used variable-length bit encodings, and used the shortest encoding\n> which is unique to you (including mempool).  It's a little more work,\n> but for an average node transmitting a block with 1300 txs and another\n> ~3000 in the mempool, you expect about 12 bits per transaction.  IOW,\n> about 1/5 of your current size.  Critically, we might be able to fit in\n> two or three TCP packets.\n\nHm. 12 bits sounds very small even giving those figures. Why failure\nrate were you targeting?\n\nI've mostly been thing in terms of 3000 txn, and 20k mempools, and\nblocks which are 90% consistent with the remote mempool, targeting\n1/100000 failure rates (which is roughly where it should be to put it\nwell below link failure levels).\n\nIf going down the path of more complexity, set reconciliation is\nenormously more efficient (e.g. 90% reduction), which no amount of\npacking/twiddling can achieve.\n\nBut the savings of going from 20kb to 3kb is not interesting enough to\njustify it*.  My expectation is that later we'll deploy set\nreconciliation to fix relay efficiency, where the savings is _much_\nlarger,  and then with the infrastructure in place we could define\nanother compactblock mode that used it.\n\n(*Not interesting because it mostly reduces exposure to loss and the\ngods of TCP, but since those are the long poles in the latency tent,\nit's best to escape them entirely, see Matt's udp_wip branch.)\n\n> I would also avoid the nonce to save recalculating for each node, and\n> instead define an id as:\n\nDoing this would greatly increase the cost of a collision though, as\nit would happen in many places in the network at once over the on the\nnetwork at once, rather than just happening on a single link, thus\nhardly impacting overall propagation.\n\n(The downside of the nonce is that you get an exponential increase in\nthe rate that a collision happens \"somewhere\", but links fail\n\"somewhere\" all the time-- propagation overall doesn't care about\nthat.)\n\nUsing the same nonce means you also would not get a recovery gain from\njointly decoding using compact blocks sent from multiple peers (which\nyou'll have anyways in high bandwidth mode).\n\nWith a nonce a sender does have the option of reusing what they got--\nbut the actual encoding cost is negligible, for a 2500 transaction\nblock its 27 microseconds (once per block, shared across all peers)\nusing Pieter's suggestion of siphash 1-3 instead of the cheaper\nconstruct in the current draft.\n\nOf course, if you're going to check your whole mempool to reroll the\nnonce, thats another matter-- but that seems wasteful compared to just\nusing a table driven size with a known negligible failure rate.\n\n64-bits as a maximum length is high enough that the collision rate\nwould be negligible even under fairly unrealistic assumptions-- so\nlong as it's salted. :)\n\n> As Peter R points out, we could later enhance receiver to brute force\n> collisions (you could speed that by sending a XOR of all the txids, but\n> really if there are more than a few collisions, give up).\n\nThe band between \"no collisions\" and \"infeasible many\" is fairly\nnarrow.  You can add a small amount more space to the ids and\nimmediately be in the no collision zone.\n\nSome earlier work we had would send small amount of erasure coding\ndata of the next couple bytes of the IDs.  E.g. the receiver in all\nthe IDs you know, mark totally unknown IDs as erased and the let the\nerror correction fix the rest. This let you algebraically resolve\ncollisions _far_ beyond what could be feasibly bruteforced. Pieter\nwent and implemented... but the added cost of encoding and software\ncomplexity seem not worth it."
            },
            {
                "author": "Rusty Russell",
                "date": "2016-05-10T21:23:55",
                "message_text_only": "Gregory Maxwell <greg at xiph.org> writes:\n> On Tue, May 10, 2016 at 5:28 AM, Rusty Russell via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> I used variable-length bit encodings, and used the shortest encoding\n>> which is unique to you (including mempool).  It's a little more work,\n>> but for an average node transmitting a block with 1300 txs and another\n>> ~3000 in the mempool, you expect about 12 bits per transaction.  IOW,\n>> about 1/5 of your current size.  Critically, we might be able to fit in\n>> two or three TCP packets.\n>\n> Hm. 12 bits sounds very small even giving those figures. Why failure\n> rate were you targeting?\n\nThat's a good question; I was assuming a best-case in which we have\nmempool set reconciliation (handwave) thus know they are close.  But\nthere's also an alterior motive: any later more sophisticated approach\nwill want variable-length IDs, and I'd like Matt to do the work :)\n\nIn particular, you can significantly narrow the possibilities for a\nblock by sending the min-fee-per-kb and a list of \"txs in my mempool\nwhich didn't get in\" and \"txs which did despite not making the\nfee-per-kb\".  Those turn out to be tiny, and often make set\nreconciliation trivial.  That's best done with variable-length IDs.\n\n> (*Not interesting because it mostly reduces exposure to loss and the\n> gods of TCP, but since those are the long poles in the latency tent,\n> it's best to escape them entirely, see Matt's udp_wip branch.)\n\nI'm not convinced on UDP; it always looks impressive, but then ends up\nreimplementing TCP in practice.  We should be well within a TCP window\nfor these, so it's hard to see where we'd win.\n\n>> I would also avoid the nonce to save recalculating for each node, and\n>> instead define an id as:\n>\n> Doing this would greatly increase the cost of a collision though, as\n> it would happen in many places in the network at once over the on the\n> network at once, rather than just happening on a single link, thus\n> hardly impacting overall propagation.\n\n\"Greatly increase\"?  I don't see that.\n\nLet's assume an attacker grinds out 10,000 txs with 128 bits of the same\nTXID, and gets them all in a block.  They then win the lottery and get a\ncollision.  Now we have to transmit ~48 bytes more than expected.\n\n> Using the same nonce means you also would not get a recovery gain from\n> jointly decoding using compact blocks sent from multiple peers (which\n> you'll have anyways in high bandwidth mode).\n\nNot quite true, since if their mempools differ they'll use different\nencoding lengths, but yes, you'll get less of this.\n\n> With a nonce a sender does have the option of reusing what they got--\n> but the actual encoding cost is negligible, for a 2500 transaction\n> block its 27 microseconds (once per block, shared across all peers)\n> using Pieter's suggestion of siphash 1-3 instead of the cheaper\n> construct in the current draft.\n>\n> Of course, if you're going to check your whole mempool to reroll the\n> nonce, thats another matter-- but that seems wasteful compared to just\n> using a table driven size with a known negligible failure rate.\n\nI'm not worried about the sender: The recipient needs to encode all the\nmempool.\n\n>> As Peter R points out, we could later enhance receiver to brute force\n>> collisions (you could speed that by sending a XOR of all the txids, but\n>> really if there are more than a few collisions, give up).\n>\n> The band between \"no collisions\" and \"infeasible many\" is fairly\n> narrow.  You can add a small amount more space to the ids and\n> immediately be in the no collision zone.\n\nIndeed, I would be adding extra bits in the sender and not implementing\nbrute force in the receiver.  But I welcome someone else to do so.\n\nCheers,\nRusty."
            },
            {
                "author": "Matt Corallo",
                "date": "2016-05-11T01:12:32",
                "message_text_only": "Replies inline.\n\nOn May 10, 2016 5:23:55 PM EDT, Rusty Russell via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>Gregory Maxwell <greg at xiph.org> writes:\n>> On Tue, May 10, 2016 at 5:28 AM, Rusty Russell via bitcoin-dev\n>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>> I used variable-length bit encodings, and used the shortest encoding\n>>> which is unique to you (including mempool).  It's a little more\n>work,\n>>> but for an average node transmitting a block with 1300 txs and\n>another\n>>> ~3000 in the mempool, you expect about 12 bits per transaction. \n>IOW,\n>>> about 1/5 of your current size.  Critically, we might be able to fit\n>in\n>>> two or three TCP packets.\n>>\n>> Hm. 12 bits sounds very small even giving those figures. Why failure\n>> rate were you targeting?\n>\n>That's a good question; I was assuming a best-case in which we have\n>mempool set reconciliation (handwave) thus know they are close.  But\n>there's also an alterior motive: any later more sophisticated approach\n>will want variable-length IDs, and I'd like Matt to do the work :)\n\nYea, there's already an ongoing discussion of that, and the UDP stuff will definitely want something different than the current proposals.\n\n>In particular, you can significantly narrow the possibilities for a\n>block by sending the min-fee-per-kb and a list of \"txs in my mempool\n>which didn't get in\" and \"txs which did despite not making the\n>fee-per-kb\".  Those turn out to be tiny, and often make set\n>reconciliation trivial.  That's best done with variable-length IDs.\n>\n>> (*Not interesting because it mostly reduces exposure to loss and the\n>> gods of TCP, but since those are the long poles in the latency tent,\n>> it's best to escape them entirely, see Matt's udp_wip branch.)\n>\n>I'm not convinced on UDP; it always looks impressive, but then ends up\n>reimplementing TCP in practice.  We should be well within a TCP window\n>for these, so it's hard to see where we'd win.\n\nNot at all. The goal with the UDP stuff I've been working on is not to provide reliable transport. Like the relay network, it is assumed some percent of blocks will fail to transit properly, and you will use some other transport to figure out how to get the block. Indeed, a big part of my desire for diversity in network protocols is to enable them to make tradeoffs in reliability/privacy/etc.\n\n>>> I would also avoid the nonce to save recalculating for each node,\n>and\n>>> instead define an id as:\n>>\n>> Doing this would greatly increase the cost of a collision though, as\n>> it would happen in many places in the network at once over the on the\n>> network at once, rather than just happening on a single link, thus\n>> hardly impacting overall propagation.\n>\n>\"Greatly increase\"?  I don't see that.\n>\n>Let's assume an attacker grinds out 10,000 txs with 128 bits of the\n>same\n>TXID, and gets them all in a block.  They then win the lottery and get\n>a\n>collision.  Now we have to transmit ~48 bytes more than expected.\n\nI assume what Greg was referring to the idea that if there is a conflict, a given block will require an extra round trip when being broadcast between roughly each peer, compounding the effect across each hop.\n\n>> Using the same nonce means you also would not get a recovery gain\n>from\n>> jointly decoding using compact blocks sent from multiple peers (which\n>> you'll have anyways in high bandwidth mode).\n>\n>Not quite true, since if their mempools differ they'll use different\n>encoding lengths, but yes, you'll get less of this.\n\n... Assuming different encoding lengths aren't just truncated, but ok :).\n\n>> With a nonce a sender does have the option of reusing what they got--\n>> but the actual encoding cost is negligible, for a 2500 transaction\n>> block its 27 microseconds (once per block, shared across all peers)\n>> using Pieter's suggestion of siphash 1-3 instead of the cheaper\n>> construct in the current draft.\n>>\n>> Of course, if you're going to check your whole mempool to reroll the\n>> nonce, thats another matter-- but that seems wasteful compared to\n>just\n>> using a table driven size with a known negligible failure rate.\n>\n>I'm not worried about the sender: The recipient needs to encode all the\n>mempool.\n>\n>>> As Peter R points out, we could later enhance receiver to brute\n>force\n>>> collisions (you could speed that by sending a XOR of all the txids,\n>but\n>>> really if there are more than a few collisions, give up).\n>>\n>> The band between \"no collisions\" and \"infeasible many\" is fairly\n>> narrow.  You can add a small amount more space to the ids and\n>> immediately be in the no collision zone.\n>\n>Indeed, I would be adding extra bits in the sender and not implementing\n>brute force in the receiver.  But I welcome someone else to do so.\n>\n>Cheers,\n>Rusty.\n>_______________________________________________\n>bitcoin-dev mailing list\n>bitcoin-dev at lists.linuxfoundation.org\n>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Matt Corallo",
                "date": "2016-05-18T01:49:10",
                "message_text_only": "Implemented a few of your suggestions.\n\nAlso opened a formal pull request for the BIP at\nhttps://github.com/bitcoin/bips/pull/389 and the code at\nhttps://github.com/bitcoin/bitcoin/pull/8068.\n\nOn 05/09/16 17:06, Pieter Wuille via bitcoin-dev wrote:\n> On 05/03/2016 12:13 AM, lf-lists at mattcorallo.com (Matt Corallo) wrote:\n>> Hi all,\n>>\n>> The following is a BIP-formatted design spec for compact block relay\n>> designed to limit on wire bytes during block relay. You can find the\n>> latest version of this document at\n>> https://github.com/TheBlueMatt/bips/blob/master/bip-TODO.mediawiki.\n> \n> Hi Matt,\n> \n> thank you for working on this!\n> \n>> ===New data structures===\n>> Several new data structures are added to the P2P network to relay\n>> compact blocks: PrefilledTransaction, HeaderAndShortIDs,\n>> BlockTransactionsRequest, and BlockTransactions. Additionally, we\n>> introduce a new variable-length integer encoding for use in these data\n>> structures.\n>>\n>> For the purposes of this section, CompactSize refers to the\n>> variable-length integer encoding used across the existing P2P protocol\n>> to encode array lengths, among other things, in 1, 3, 5 or 9 bytes.\n> \n> This is a not, but I think it's a bit strange to have two separate\n> variable length integers in the same specification. I understand is one\n> is already the default for variable-length integers currently, and there\n> are reasons to use the other one for efficiency reasons in some places,\n> but perhaps we should aim to get everything using the latter?\n\nFixed, the whole thing now uses New Varints.\n\n>> ====New VarInt====\n>> Variable-length integers: bytes are a MSB base-128 encoding of the number.\n>> The high bit in each byte signifies whether another digit follows. To make\n>> sure the encoding is one-to-one, one is subtracted from all but the last\n>> digit.\n> \n> Maybe it's worth mentioning that it is based on ASN.1 BER's compressed\n> integer format (see\n> https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf\n> section 8.1.3.5), though with a small modification to make every integer\n> have a single unique encoding.\n> \n>> ====HeaderAndShortIDs====\n>> A HeaderAndShortIDs structure is used to relay a block header, the short\n>> transactions IDs used for matching already-available transactions, and a\n>> select few transactions which we expect a peer may be missing.\n>>\n>> |shortids||List of uint64_ts||8*shortids_length bytes||Little\n>> Endian||The short transaction IDs calculated from the transactions which\n>> were not provided explicitly in prefilledtxn\n> \n> I tried to derive what length of short ids is actually necessary (some\n> write-up is on\n> https://gist.github.com/sipa/b2eb2e486156b5509ac711edd16153ed but it's\n> incomplete).\n> \n> For any reasonable numbers I can come up with (in a very wide range),\n> the number of bits needed is very well approximated by:\n> \n>   log2(#receiver_mempool_txn * #block_txn_not_in_receiver_mempool /\n> acceptable_per_block_failure_rate)\n> \n> For example, with 20000 mempool transactions, 2500 transactions in a\n> block, 95% hitrate, and a chance of 1 in 10000 blocks to fail to\n> reconstruct, needed_bits = log2(20000 * 2500 * (1 - 0.95) / 0.0001) =\n> 34.54, or 5 byte txids would suffice.\n> \n> Note that 1 in 10000 failures may sound like a lot, but this is for each\n> individual connection, and since every transmission uses separately\n> salted identifiers, occasional failures should not affect global\n> propagation. Given that transmission failures due to timeouts, network\n> connectivity, ... already occur much more frequently than once every few\n> gigabytes (what 10000 blocks corresponds to), that's probably already\n> more than enough.\n> \n> In short: I believe 5 or 6 byte txids should be enough, but perhaps it\n> makes sense to allow the sender to choose (so he can weigh trying\n> multiple nonces against increasing the short txid length).\n\nI switched to 6-byte short txids.\n\n>> ====Short transaction IDs====\n>> Short transaction IDs are used to represent a transaction without\n>> sending a full 256-bit hash. They are calculated by:\n>> # single-SHA256 hashing the block header with the nonce appended (in\n>> little-endian)\n>> # XORing each 8-byte chunk of the double-SHA256 transaction hash with\n>> each corresponding 8-byte chunk of the hash from the previous step\n>> # Adding each of the XORed 8-byte chunks together (in little-endian)\n>> iteratively to find the short transaction ID\n> \n> An alternative would be using SipHash-1-3 (a form of SipHash with\n> reduced iteration counts; the default is SipHash-2-4). SipHash was\n> designed as a Message Authentication Code, where the security\n> requirements are much stronger than in our case (in particular, we don't\n> care about observers being able to finding the key, as the key is just\n> public knowledge here). One of the designers of SipHash has commented\n> that SipHash-1-3 for collision resistance in hash tables may be enough:\n> https://github.com/rust-lang/rust/issues/29754#issuecomment-156073946\n> \n> Using SipHash-1-3 on modern hardware would take ~32 CPU cycles per txid.\n\nSwitched to SipHash2-4.\n\n>> ===Implementation Notes===\n> \n> There are a few more heuristics that MAY be used to improve performance:\n> \n> * Receivers should treat short txids in blocks that match multiple\n> mempool transactions as non-matches, and request the transactions. This\n> significantly reduces the failure to reconstruct.\n\nDone.\n\n> * When constructing a compact block to send, the sender can verify it\n> against its own mempool to check for collisions, and if so, choose to\n> either try another nonce, or increase the short txid length.\n\nAdditionally we should compare to the orphan pool (which apparently\nhelps a lot)."
            },
            {
                "author": "Nicolas Dorier",
                "date": "2016-05-08T10:25:49",
                "message_text_only": "Interesting, can you provide some historical context around it so I\nunderstand better ?\nActually I know that your relay's protocol (and about what I see in\nabstract) was about optimizing propagation time and not bandwidth.\n\nAnd I agree that bandwidth is what need to be optimized for nodes.\nSo far there was two other proposal that I know only from name and theory\nwhich is xthin block and ILBT which would also have decreased bandwidth.\n\nCan you quickly describe how does it compares to them ?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160508/eaaba9dc/attachment.html>"
            },
            {
                "author": "Peter R",
                "date": "2016-05-09T23:37:00",
                "message_text_only": "Greg Maxwell wrote:\n\n> What are you talking about? You seem profoundly confused here...\n> \n> I obtain some txouts. I write a transaction spending them in malleable\n> form (e.g. sighash single and an op_return output).. then grind the\n> extra output to produce different hashes.  After doing this 2^32 times\n> I am likely to find two which share the same initial 8 bytes of txid.\n\n[9 May 16 @ 4:30 PDT]\n\nI\u2019m trying to understand the collision attack that you're explaining to Tom Zander.  \n\nMathematica is telling me that if I generated 2^32 random transactions, that the chances that the initial 64-bits on one of the pairs of transactions is about 40%.  So I am following you up to this point.  Indeed, there is a good chance that a pair of transactions from a set of 2^32 will have a collision in the first 64 bits.  \n\nBut how do you actually find that pair from within your large set?  The only way I can think of is to check if the first 64-bits is equal for every possible pair until I find it.  How many possible pairs are there?  \n\nIt is a standard result that there are \n\n    m! / [n! (m-n)!] \n\nways of picking n numbers from a set of m numbers, so there are\n\n    (2^32)! / [2! (2^32 - 2)!] ~ 2^63\n\npossible pairs in a set of 2^32 transactions.  So wouldn\u2019t you have to perform approximately 2^63 comparisons in order to identify which pair of transactions are the two that collide?\n\nPerhaps I made an error or there is a faster way to scan your set to find the collision.  Happy to be corrected\u2026\n\nBest regards,\nPeter"
            },
            {
                "author": "Peter R",
                "date": "2016-05-10T01:42:45",
                "message_text_only": "[9 May 16 @ 6:40 PDT]\n\nFor those interested in the hash collision attack discussion, it turns out there is a faster way to scan your set to find the collision:  you\u2019d keep a sorted list of the hashes for each TX you generate and then use binary search to check that list for a collision for each new TX you randomly generate. Performing these operations can probably be reduced to N lg N complexity, which is doable for N ~2^32.   In other words, I now agree that the attack is feasible.  \n\nCheers,\nPeter\n\nhat tip to egs\n\n> On May 9, 2016, at 4:37 PM, Peter R via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> Greg Maxwell wrote:\n> \n>> What are you talking about? You seem profoundly confused here...\n>> \n>> I obtain some txouts. I write a transaction spending them in malleable\n>> form (e.g. sighash single and an op_return output).. then grind the\n>> extra output to produce different hashes.  After doing this 2^32 times\n>> I am likely to find two which share the same initial 8 bytes of txid.\n> \n> [9 May 16 @ 4:30 PDT]\n> \n> I\u2019m trying to understand the collision attack that you're explaining to Tom Zander.  \n> \n> Mathematica is telling me that if I generated 2^32 random transactions, that the chances that the initial 64-bits on one of the pairs of transactions is about 40%.  So I am following you up to this point.  Indeed, there is a good chance that a pair of transactions from a set of 2^32 will have a collision in the first 64 bits.  \n> \n> But how do you actually find that pair from within your large set?  The only way I can think of is to check if the first 64-bits is equal for every possible pair until I find it.  How many possible pairs are there?  \n> \n> It is a standard result that there are \n> \n>    m! / [n! (m-n)!] \n> \n> ways of picking n numbers from a set of m numbers, so there are\n> \n>    (2^32)! / [2! (2^32 - 2)!] ~ 2^63\n> \n> possible pairs in a set of 2^32 transactions.  So wouldn\u2019t you have to perform approximately 2^63 comparisons in order to identify which pair of transactions are the two that collide?\n> \n> Perhaps I made an error or there is a faster way to scan your set to find the collision.  Happy to be corrected\u2026\n> \n> Best regards,\n> Peter\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2016-05-10T02:12:03",
                "message_text_only": "On Mon, May 9, 2016 at 11:37 PM, Peter R <peter_r at gmx.com> wrote:\n> It is a standard result that there are\n>     m! / [n! (m-n)!]\n> ways of picking n numbers from a set of m numbers, so there are\n>\n>     (2^32)! / [2! (2^32 - 2)!] ~ 2^63\n> possible pairs in a set of 2^32 transactions.  So wouldn\u2019t you have to perform approximately 2^63 comparisons in order to identify which pair of transactions are the two that collide?\n>\n> Perhaps I made an error or there is a faster way to scan your set to find the collision.  Happy to be corrected\u2026\n\n$ echo -n Perhaps. 00000000f2736d91 |sha256sum\n359dfa6d4c2eb2ac81535392d68af4b5e1cb6d9c6321e8f111d3244329b6a4d8\n$ echo -n Perhaps. 0000000011ac0388 |sha256sum\n359dfa6d4c2eb2ac44d54d0ceeb2212500cb34617b9360695432f6c0fde9b006\n\nTry search term \"collision\", or there may be an undergrad Data\nstructures and algorithms coarse online-- you want something covering\n\"cycle finding\".\n\n(Though even ignoring efficient cycle finding, your factorial argument\ndoesn't hold... you can simply sort the data... Search term\n\"quicksort\" for a relevant algorithm)."
            }
        ],
        "thread_summary": {
            "title": "Compact Block Relay BIP",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Rusty Russell",
                "Bryan Bishop",
                "Tom",
                "Nicolas Dorier",
                "Tom Zander",
                "Peter Todd",
                "Johnathan Corgan",
                "Gregory Maxwell",
                "Matt Corallo",
                "Pieter Wuille",
                "Peter R"
            ],
            "messages_count": 22,
            "total_messages_chars_count": 106683
        }
    },
    {
        "title": "[bitcoin-dev] BIP75 update & PR - Simplification",
        "thread_messages": [
            {
                "author": "James MacWhyte",
                "date": "2016-05-06T23:51:58",
                "message_text_only": "Hi all,\n\nWe've made some significant changes to BIP75 which we think simplify things\ngreatly:\n\nInstead of introducing encrypted versions of all BIP70 messages\n(EncryptedPaymentRequest, EncryptedPayment, etc), we have defined a generic\nEncryptedProtocolMessage type which is essentially a wrapper that enables\nencryption for all existing BIP70 messages. This reduces the number of new\nmessages we are defining and makes it easier to add new message types in\nthe future.\n\nWe've also decided to use AES-GCM instead of AES-CBC, which eliminates the\nneed for the verification hash.\n\nA pull request has been submitted, which can be seen here:\nhttps://github.com/bitcoin/bips/pull/385\n\nAll comments are welcome. Thank you!\n\nJames\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160506/ff43d148/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "BIP75 update & PR - Simplification",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "James MacWhyte"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 905
        }
    },
    {
        "title": "[bitcoin-dev] Proposal to update BIP-32",
        "thread_messages": [
            {
                "author": "Pavol Rusnak",
                "date": "2016-05-08T10:07:52",
                "message_text_only": "On 21/04/16 14:08, Marek Palatinus via bitcoin-dev wrote:\n> Sipa, you are probably the most competent to answer this.\n> Could you please tell us your opinion? For me, this is\n> straightforward, backward compatible fix and I like it a lot.\n> Not sure about the process of changing \"Final\" BIP though.\n\nSipa: Marek told me you posted your answer and he received it, but it\nnever reached the list. Could you please resend after figuring out what\nwent wrong?\n\n-- \nBest Regards / S pozdravom,\n\nPavol \"stick\" Rusnak\nSatoshiLabs.com"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2016-05-08T11:09:45",
                "message_text_only": "On Sun, May 8, 2016 at 10:07 AM, Pavol Rusnak via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> On 21/04/16 14:08, Marek Palatinus via bitcoin-dev wrote:\n>> Sipa, you are probably the most competent to answer this.\n>> Could you please tell us your opinion? For me, this is\n>> straightforward, backward compatible fix and I like it a lot.\n>> Not sure about the process of changing \"Final\" BIP though.\n>\n> Sipa: Marek told me you posted your answer and he received it, but it\n> never reached the list. Could you please resend after figuring out what\n> went wrong?\n\nAFAIK Sipa has not been on this list for some time."
            }
        ],
        "thread_summary": {
            "title": "Proposal to update BIP-32",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Pavol Rusnak",
                "Gregory Maxwell"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 1157
        }
    },
    {
        "title": "[bitcoin-dev] Fwd:  Proposal to update BIP-32",
        "thread_messages": [
            {
                "author": "Marek Palatinus",
                "date": "2016-05-08T13:48:27",
                "message_text_only": "I received this:\n\n---------- Forwarded message ----------\nFrom: Pieter Wuille <pieter.wuille at gmail.com>\nDate: Fri, Apr 22, 2016 at 6:44 PM\nSubject: Re: [bitcoin-dev] Proposal to update BIP-32\nTo: Marek Palatinus <marek at palatinus.cz>\nCc: Bitcoin Dev <bitcoin-dev at lists.linuxfoundation.org>\n\n\nOn Thu, Apr 21, 2016 at 2:08 PM, Marek Palatinus <marek at palatinus.cz> wrote:\n\n> On Wed, Apr 20, 2016 at 6:32 PM, Jochen Hoenicke via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Hello Bitcoin Developers,\n>>\n>> I would like to make a proposal to update BIP-32 in a small way.\n>>\n>> I think the backward compatibility issues are minimal.  The chance\n>> that this affects anyone is less than 10^-30.  Even if it happens, it\n>> would only create some additional addresses (that are not seen if the\n>> user downgrades).  The main reason for suggesting a change is that we\n>> want a similar method for different curves where a collision is much\n>> more likely.\n>>\n>\nI think I change like this makes a lot of sense technically, and I wish I\nhad known how BIP-32 would end up being used inside higher level mechanisms\nthat automatically increment the position beyond the control of the\napplication generating them. The inclusion of the requirement was there\nbecause ECDSA is notorious for security problems under biased secret keys,\nthough it's really only a certificational issue for secp256k1 (due to its\ngroup order being so close to 2^256).\n\n>\n>> #QUESTIONS:\n>>\n>> What is the procedure to update the BIP?  Is it still possible to\n>> change the existing BIP-32 even though it is marked as final?  Or\n>> should I make a new BIP for this that obsoletes BIP-32?\n>>\n>\nBIPs are not supposed to be updated with new ideas, only\nremarks/links/typos/clarifications/..., so that their bumbers can\nunambiguously be used to refer to an idea. My suggestion would be to write\na new BIP that overrides parts of BIP32, and then put a note in BIP32 that\na better mechanism is available that is unlikely to change things in\nreality for the secp256k1 curve.\n\nI guess\n\n\n> What algorithm is preferred? (bike-shedding)  My suggestion:\n>>\n>> ---\n>>\n>> Change the last step of the private -> private derivation functions to:\n>>\n>>  . In case parse(I_L) >= n or k_i = 0, the procedure is repeated\n>>    at step 2 with\n>>     I = HMAC-SHA512(Key = c_par, Data = 0x01 || I_R || ser32(i))\n>\n>\n>> ---\n>>\n>> I think this suggestion is simple to implement (a bit harder to unit\n>> test) and the string to hash with HMAC-SHA512 always has the same\n>> length.  I use I_R, since I_L is obviously not very random if I_L >= n.\n>> There is a minimal chance that it will lead to an infinite loop if I_R\n>> is the same in two consecutive iterations, but that has only a chance\n>> of 1 in 2^512 (if the algorithm is used for different curves that make\n>> I_L >= n more likely, the chance is still less than 1 in 2^256).  In\n>> theory, this loop can be avoided by incrementing i in every iteration,\n>> but this would make an implementation error in the \"hard to test\" path\n>> of the program more likely.\n>>\n>\nThe chance for failure is a bit higher than that, as it only requires a\nfailed key (one in 2^128) in the first step, followed by an iteration that\nresults in the same I_R to cause a cycle. If you take multiple failures\nbefore the cycle starts into account, the combined chance for failure is\np/(1-p)^2 / 2^256 (with p the chance for a random inadmissable key), which\nis not much better than 1 in 2^128 for high values of p.\n\nAn alternative that always converges is to retry with an appended iteration\ncount is possible:\n{\n  I = HMAC-SHA512(Key = c_par, Data = 0x01 ||  || ser32(i)) for the first\niteration\n  I = HMAC-SHA512(Key = c_par, Data = 0x01 ||  || ser32(i) || ser32(j)) for\niteration number j, with j > 0\n}\n\nCheers,\n\n-- \nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160508/f42d5ac4/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Fwd:  Proposal to update BIP-32",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Marek Palatinus"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 4010
        }
    },
    {
        "title": "[bitcoin-dev] Fwd: Proposal to update BIP-32",
        "thread_messages": [
            {
                "author": "Pavol Rusnak",
                "date": "2016-05-08T22:21:02",
                "message_text_only": "On 08/05/16 15:48, Marek Palatinus via bitcoin-dev wrote:\n> unambiguously be used to refer to an idea. My suggestion would be to write\n> a new BIP that overrides parts of BIP32, and then put a note in BIP32 that\n> a better mechanism is available that is unlikely to change things in\n> reality for the secp256k1 curve.\n\nI guess, we'll write that down to SLIP-0032 then.\n\n-- \nBest Regards / S pozdravom,\n\nPavol \"stick\" Rusnak\nSatoshiLabs.com"
            }
        ],
        "thread_summary": {
            "title": "Fwd: Proposal to update BIP-32",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Pavol Rusnak"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 439
        }
    },
    {
        "title": "[bitcoin-dev] Committed bloom filters for improved wallet performance and SPV security",
        "thread_messages": [
            {
                "author": "bfd at cock.lu",
                "date": "2016-05-09T08:26:06",
                "message_text_only": "We introduce several concepts that rework the lightweight Bitcoin\nclient model in a manner which is secure, efficient and privacy\ncompatible.\n\nThea properties of BIP37 SPV [0] are unfortunately not as strong as\noriginally thought:\n\n     * The expected privacy of the probabilistic nature of bloom\n       filters does not exist [1][2], any user with a BIP37 SPV wallet\n       should be operating under no expectation of privacy.\n       Implementation flaws make this effect significantly worse, the\n       behavior meaning that no matter how high the false positive\n       rate (up to simply downloading the whole blocks verbatim) the\n       intent of the client connection is recoverable.\n\n     * Significant processing load is placed on nodes in the Bitcoin\n       network by lightweight clients, a single syncing wallet causes\n       (at the time of writing) 80GB of disk reads and a large amount\n       of CPU time to be consumed processing this data. This carries\n       significant denial of service risk [3], non-distinguishable\n       clients can repeatedly request taxing blocks causing\n       reprocessing on every request. Processed data is unique to every\n       client, and can not be cached or made more efficient while\n       staying within specification.\n\n     * Wallet clients can not have strong consistency or security\n       expectations, BIP37 merkle paths allow for a wallet to validate\n       that an output was spendable at some point in time but does not\n       prove that this output is not spent today.\n\n     * Nodes in the network can denial of service attack all BIP37 SPV\n       wallet clients by simply returning null filter results for\n       requests, the wallet has no way of discerning if it has been\n       lied to and may be made simply unaware that any payment has been\n       made to them. Many nodes can be queried in a probabilistic manor\n       but this increases the already heavy network load with little\n       benefit.\n\n\n\nWe propose a new concept which can work towards addressing these\nshortcomings.\n\n\nA Bloom Filter Digest is deterministically created of every block\nencompassing the inputs and outputs of the containing transactions,\nthe filter parameters being tuned such that the filter is a small\nportion of the size of the total block data. To determine if a block\nhas contents which may be interesting a second bloom filter of all\nrelevant key material is created. A binary comparison between the two\nfilters returns true if there is probably matching transactions, and\nfalse if there is certainly no matching transactions. Any matched\nblocks can be downloaded in full and processed for transactions which\nmay be relevant.\n\nThe BFD can be used verbatim in replacement of BIP37, where the filter\ncan be cached between clients without needing to be recomputed. It can\nalso be used by normal pruned nodes to do re-scans locally of their\nwallet without needing to have the block data available to scan, or\nwithout reading the entire block chain from disk.\n\n-\n\nFor improved probabilistic security the bloom filters can be presented\nto lightweight clients by semi-trusted oracles. A client wallet makes\nan assumption that they trust a set, or subset of remote parties\n(wallet vendors, services) which all all sign the BFD for each block.\nThe BFD can be downloaded from a single remote source, and the hash of\nthe filters compared against others in the trust set. Agreement is a\nweak suggestion that the filter has not been tampered with, assuming\nthat these parties are not conspiring to defraud the client.\n\nThe oracles do not learn any additional information about the client\nwallet, the client can download the block data from either nodes on\nthe network, HTTP services, NTTP, or any other out of band\ncommunication method that provides the privacy desired by the client.\n\n-\n\nThe security model of the oracle bloom filter can be vastly improved\nby instead committing a hash of the BFD inside every block as a soft-\nfork consensus rule change. After this, every node in the network would\nbuild the filter and validate that the hash in the block is correct,\nthen make a conscious choice discard it for space savings or cache the\ndata to disk.\n\nWith a commitment to the filter it becomes impossible to lie to\nlightweight clients by omission. Lightweight clients are provided with\na block header, merkle path, and the BFD. Altering the BFD invalidates\nthe merkle proof, it's validity is a strong indicator that the client\nhas an unadulterated picture of the UTXO condition without needing to\nbuild one itself. A strong assurance that the hash of the BFD means\nthat the filters can be downloaded out of band along with the block\ndata at the leisure of the client, allowing for significantly greater\nprivacy and taking load away from the P2P Bitcoin network.\n\nCommitting the BFD is not a hard forking change, and does not require\nalterations to mining software so long as the coinbase transaction\nscriptSig is not included in the bloom filter.\n\n\n[0] https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki\n[1] https://eprint.iacr.org/2014/763.pdf\n[2] https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/\n[3] https://github.com/petertodd/bloom-io-attack"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2016-05-09T08:57:08",
                "message_text_only": "On Mon, May 9, 2016 at 8:26 AM, bfd--- via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> We introduce several concepts that rework the lightweight Bitcoin\n> client model in a manner which is secure, efficient and privacy\n> compatible.\n[...]\n> A Bloom Filter Digest is deterministically created of every block\n\nI think this is a fantastic idea.\n\nSome napkin work shows that it has pretty good communications\nbandwidth so long as you assume that the wallet has many keys (e.g.\nmore than the number of the outputs in the block)-- otherwise BIP37\nuses less bandwidth, but you note its terrible privacy problems.\n\nYou should be aware that when the filter is transmitted but not\nupdated, as it is in these filtering applications, the bloom filter is\nnot the most communication efficient data structure.\n\nThe most efficient data structure is similar to a bloom filter, but\nyou use more bits and only one hash function. The result will be\nmostly zero bits. Then you entropy code it using RLE+Rice coding or an\noptimal binomial packer (e.g.\nhttps://people.xiph.org/~greg/binomial_codec.c).  This is about 45%\nmore space efficient than a bloom filter. ... it's just a PITA to\nupdate, though that is inapplicable here.  Entropy coding for this can\nbe quite fast, if many lookups are done the decompression could even\nbe faster than having to use two dozen hash functions for each lookup.\n\nThe intuition is that this kind of simple hash-bitmap is great, but\nspace inefficient if you don't have compression since most of the bits\nare 0 you end up spending a bit to send less than a bit of\ninformation. A bloom filter improve the situation by using the\nmultiple filters to increase the ones density to 50%, but the\nincreased collisions create overhead. This is important when its a\nin-memory data-structure that you're updating often, but not here.\n\nOne thing to do with matching blocks is after finding the matches the\nnode could potentially consult some PIR to get the blocks it cares\nabout... thus preventing a leak of which blocks it was interested in,\nbut not taking PIR costs for the whole chain or requiring the\nimplementation of PIR tree search (which is theoretically simple but\nin practice hard to implement)."
            },
            {
                "author": "Bob McElrath",
                "date": "2016-05-11T20:06:48",
                "message_text_only": "I like this idea, but let's run some numbers...\n\nbfd--- via bitcoin-dev [bitcoin-dev at lists.linuxfoundation.org] wrote:\n> A Bloom Filter Digest is deterministically created of every block\n\nBloom filters completely obfuscate the required size of the filter for a desired\nfalse-positive rate.  But, an optimal filter is linear in the number of elements\nit contains for fixed false-positive rate, and logarithmic in the false-positive\nrate.  (This comment applies to a RLL encoded Bloom filter Greg mentioned, but\nthat's not the only way)  That is for N elements and false positive rate\n\\epsilon:\n\n    filter size = - N \\log_2 \\epsilon\n\nGiven that the data that would be put into this particular filter is *already*\nhashed, it makes more sense and is faster to use a Cuckoo[1] filter, choosing a\nfixed false-positive rate, given expected wallet sizes.  For Bloom filters,\nmultiply the above formula by 1.44.\n\nTo prevent light clients from downloading more blocks than necessary, the\nfalse-positive rate should be roughly less than 1/(block height).  If we take\nthe false positive rate to be 1e-6 for today's block height ~ 410000, this is\nabout 20 bits per element.  So for todays block's, this is a 30kb filter, for a\n3% increase in block size, if blocks commit to the filter.  Thus the required\nsize of the filter commitment is roughly:\n\n    filter size = N \\log_2 H\n\nwhere H is the block height.  If bitcoin had these filters from the beginning, a\nlight client today would have to download about 12MB of data in filters.  My\npersonal SPV wallet is using 31MB currently.  It's not clear this is a bandwidth\nwin, though it's definitely a win for computing load on full nodes.\n\n\n[1] https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf\n\n--\nCheers, Bob McElrath\n\n\"For every complex problem, there is a solution that is simple, neat, and wrong.\"\n    -- H. L. Mencken \n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 198 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160511/90715bc8/attachment.sig>"
            },
            {
                "author": "Bob McElrath",
                "date": "2016-05-11T20:29:33",
                "message_text_only": "Eerrrr....let me revise that last paragraph.  That's 12 *GB* of filters at\ntoday's block height (at fixed false-positive rate 1e-6.  Compared to block\nheaders only which are about 33 MB today.  So this proposal is not really\ncompatible with such a wallet being \"light\"...\n\nDamn units...\n\nBob McElrath via bitcoin-dev [bitcoin-dev at lists.linuxfoundation.org] wrote:\n> I like this idea, but let's run some numbers...\n> \n> bfd--- via bitcoin-dev [bitcoin-dev at lists.linuxfoundation.org] wrote:\n> > A Bloom Filter Digest is deterministically created of every block\n> \n> Bloom filters completely obfuscate the required size of the filter for a desired\n> false-positive rate.  But, an optimal filter is linear in the number of elements\n> it contains for fixed false-positive rate, and logarithmic in the false-positive\n> rate.  (This comment applies to a RLL encoded Bloom filter Greg mentioned, but\n> that's not the only way)  That is for N elements and false positive rate\n> \\epsilon:\n> \n>     filter size = - N \\log_2 \\epsilon\n> \n> Given that the data that would be put into this particular filter is *already*\n> hashed, it makes more sense and is faster to use a Cuckoo[1] filter, choosing a\n> fixed false-positive rate, given expected wallet sizes.  For Bloom filters,\n> multiply the above formula by 1.44.\n> \n> To prevent light clients from downloading more blocks than necessary, the\n> false-positive rate should be roughly less than 1/(block height).  If we take\n> the false positive rate to be 1e-6 for today's block height ~ 410000, this is\n> about 20 bits per element.  So for todays block's, this is a 30kb filter, for a\n> 3% increase in block size, if blocks commit to the filter.  Thus the required\n> size of the filter commitment is roughly:\n> \n>     filter size = N \\log_2 H\n> \n> where H is the block height.  If bitcoin had these filters from the beginning, a\n> light client today would have to download about 12MB of data in filters.  My\n> personal SPV wallet is using 31MB currently.  It's not clear this is a bandwidth\n> win, though it's definitely a win for computing load on full nodes.\n> \n> \n> [1] https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf\n> \n> --\n> Cheers, Bob McElrath\n> \n> \"For every complex problem, there is a solution that is simple, neat, and wrong.\"\n>     -- H. L. Mencken \n> \n> \n> \n> !DSPAM:5733934b206851108912031!\n\n\n\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> \n> !DSPAM:5733934b206851108912031!\n\n--\nCheers, Bob McElrath\n\n\"For every complex problem, there is a solution that is simple, neat, and wrong.\"\n    -- H. L. Mencken \n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 198 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160511/ac766f2e/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "Committed bloom filters for improved wallet performance and SPV security",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Bob McElrath",
                "bfd at cock.lu",
                "Gregory Maxwell"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 12580
        }
    },
    {
        "title": "[bitcoin-dev] Fwd:  Compact Block Relay BIP",
        "thread_messages": [
            {
                "author": "Gregory Maxwell",
                "date": "2016-05-09T12:12:14",
                "message_text_only": "On Mon, May 9, 2016 at 11:32 AM, Tom <tomz at freedommail.ch> wrote:\n> On Monday 09 May 2016 10:43:02 Gregory Maxwell wrote:\n>> On Mon, May 9, 2016 at 9:35 AM, Tom Zander via bitcoin-dev\n>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> > You misunderstand the networking effects.\n>> > The fact that your node is required to choose which one to set the\n>> > announce\n>> > bit on implies that it needs to predict which node will have the best data\n>> > in the future.\n>>\n>> Not required. It may.\n>\n> It is required, in the reference of wanting to actually use compact block\n> relay.\n\nI cannot parse this sentence.\n\nA node implementing this does not have to ask peers to send blocks\nwithout further solicitation.\n\nIf they don't, their minimum transfer time increases to the current\n1.5 RTT (but sending massively less data).\n\n> Apologies, I thought that the term was wider known.  \"Laboratory situations\"\n> is used where I am from as the opposite of real-world messy and unpredictable\n> situations.\n>\n> So, your measurements may be true, but are not useful to decide how well it\n> behaves under less optimal situations. aka \"the real world\".\n\nMy measurements were made in the real world, on a collection of nodes\naround the network which were not setup for this purpose and are\nrunning standard configurations, over many weeks of logs.\n\nThis doesn't guarantee that they're representative of everything-- but\nthey don't need to be.\n\n>> This also _increases_ robustness. Right now a single peer failing at\n>> the wrong time will delay blocks with a long time out.\n>\n> If your peers that were supposed to send you a compact block fail, then you'll\n> end up in exactly that same situation again.  Only with various timeouts in\n> between before you get your block making it a magnitude slower.\n\nThat is incorrect.\n\nIf a header shows up and a compact block has not shown up, a compact\nblock will be requested.\n\nIf compactblock shows up reconstruction will be attempted.\n\nIf any of the requested compact blocks show up (the three in advance,\nif high bandwidth mode is used, or a requested one, if there was one)\nthen reconstruction proceeds without delay.\n\nThe addition of the unsolicited input causes no additional timeouts or\ndelays (ignoring bandwidth usage). It does use some more bandwidth\nthan not having it, but still massively less than the status quo.\n\n>> > Another problem with your solution is that nodes send a much larger amount\n>> > of unsolicited data to peers in the form of the thin-block compared to\n>> > the normal inv or header-first data.\n>>\n>> \"High bandwidth\" mode\n>\n> Another place where I may have explained better.\n> This is not about the difference about the two modes of your design.\n> This is about the design as a whole. As compared to current.\n\nIt is massively more efficient than the current protocol, even under\nfairly poor conditions. In the absolute worst possible case (miner\nsends a block of completely unexpected transactions, and three peers\nsend compact blocks, it adds about 6% overhead)\n\n> Service bits are exactly the right solution to indicate additional p2p\n> feature-support.\n\nWith this kind of unsubstantiated axiomatic assertion, I don't think\nfurther discussion with you is likely to be productive-- at least I\ngave a reason.\n\n> That's all fine and well, it doesn't at any point take away from my point that\n> any specification should NOT invent something new that has for decades had a\n> great specification already.\n\nUTF-8 would be a poor fit here for the reasons I explained and others\nless significant ones (including the additional error cases that must\nbe handled resulting from the inefficient encoding; -- poor handing of\ninvalid UTF-8 have even resulted in security issues in some\napplications).\n\nI am a bit baffled that you'd suggest using UTF-8 as a general compact\ninteger encoding in a binary protocol in the first place.\n\n>> > Just the first (highest) 8 bytes of a sha256 hash.\n>> >\n>> > The amount of collisions will not be less if you start xoring the rest.\n>> > The whole reason for doing this extra work is also irrelevant as a spam\n>> > protection.\n>>\n>> Then you expose it to a trivial collision attack:  To find two 64 bit\n>> hashes that collide I need perform only roughly 2^32 computation. Then\n>> I can send them to the network.\n>\n> No, you still need to have done a POW.\n>\n> Next to that, your scheme is 2^32 computations *and* some XORs. The XORs are\n> percentage wise a rounding error on the total time. So your argument also\n> destroys your own addition.\n>\n>> This issue is eliminated by salting the hash.\n>\n> The issue is better eliminated by not allowing nodes to send uninvited large\n> messages.\n\nWhat are you talking about? You seem profoundly confused here. There\nis no proof of work involved anywhere.\n\nI obtain some txouts. I write a transaction spending them in malleable\nform (e.g. sighash single and an op_return output).. then grind the\nextra output to produce different hashes.  After doing this 2^32 times\nI am likely to find two which share the same initial 8 bytes of txid.\n\nI send one to half the nodes, the other to half the nodes.  When a\nblock shows up carrying one or the other of my transactions\nreconstruction will fail on half the nodes in the network in a\nprotocol with a simple truncated hash.\n\nOf course, doing this is easy, so I can keep it going persistently. If\nI am a miner, I can be sure to filter these transactions from my own\nblocks-- causing all my competition to suffer higher orphaning.\n\nThe salted short-ids do not have this easily exploited, and gratuitous\nvulnerability. This was obvious enough that it this feature was in the\nvery earliest descriptions of these techniques in 2013/2014. The\nsalted short-ids cannot be collided in pre-computation, and cannot be\ncollided with respect to multiple nodes at once."
            }
        ],
        "thread_summary": {
            "title": "Fwd:  Compact Block Relay BIP",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Gregory Maxwell"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 5824
        }
    },
    {
        "title": "[bitcoin-dev] Making AsicBoost irrelevant",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2016-05-10T18:57:28",
                "message_text_only": "As part of the hard-fork proposed in the HK agreement(1) we'd like to make the\npatented AsicBoost optimisation useless, and hopefully make further similar\noptimizations useless as well.\n\nWhat's the best way to do this? Ideally this would be SPV compatible, but if it\nrequires changes from SPV clients that's ok too. Also the fix this should be\ncompatible with existing mining hardware.\n\n\n1) https://medium.com/@bitcoinroundtable/bitcoin-roundtable-consensus-266d475a61ff\n\n2) http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-April/012596.html\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160510/2eaafd23/attachment.sig>"
            },
            {
                "author": "Tier Nolan",
                "date": "2016-05-10T20:27:28",
                "message_text_only": "The various chunks in the double SHA256 are\n\nChunk 1: 64 bytes\nversion\nprevious_block_digest\nmerkle_root[31:4]\n\nChunk 2: 64 bytes\nmerkle_root[3:0]\nnonce\ntimestamp\ntarget\n\nChunk 3: 64 bytes\ndigest from first sha pass\n\nTheir improvement requires that all data in Chunk 2 is identical except for\nthe nonce.  With 4 bytes, the birthday paradox means collisions can be\nfound reasonable easily.\n\nIf hard forks are allowed, then moving more of the merkle root into the 2nd\nchunk would make things harder.  The timestamp and target could be moved\ninto chunk 1.  This increases the merkle root to 12 bytes in the 2nd\nchunk.  Finding collisions would be made much more difficult.\n\nIf ASIC limitations mean that the nonce must stay where it is, this would\nmean that the merkle root would be split into two pieces.\n\nOn Tue, May 10, 2016 at 7:57 PM, Peter Todd via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> As part of the hard-fork proposed in the HK agreement(1) we'd like to make\n> the\n> patented AsicBoost optimisation useless, and hopefully make further similar\n> optimizations useless as well.\n>\n> What's the best way to do this? Ideally this would be SPV compatible, but\n> if it\n> requires changes from SPV clients that's ok too. Also the fix this should\n> be\n> compatible with existing mining hardware.\n>\n>\n> 1)\n> https://medium.com/@bitcoinroundtable/bitcoin-roundtable-consensus-266d475a61ff\n>\n> 2)\n> http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-April/012596.html\n>\n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160510/cb6a2690/attachment.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2016-05-10T21:35:39",
                "message_text_only": "Yea, I think in any hardfork that we should be talking about, a part of\nit should include 1) fix the version field so its a static constant, 2)\nthe merkle root becomes hash of the real block header 3) swap first 2\nbytes of the merkle root with the timestamp's two high-order bits, 4)\nswap the next 4 bytes of the merkle root with the difficulty field.\n\nI believe this should be compatible with all existing ASICs, with the\nexception, possibly, of some 21 Inc hardware. I believe this fixes\nAsicBoost (without thinking about it tooo much, so please critique).\n\nWhile this is somewhat nasty, the risks of AsicBoost and the precedent\nthat should be set necessitates a response, and it should be included in\nany hardfork.\n\nMatt\n\nOn 05/10/16 20:27, Tier Nolan via bitcoin-dev wrote:\n> The various chunks in the double SHA256 are\n> \n> Chunk 1: 64 bytes\n> version\n> previous_block_digest\n> merkle_root[31:4]\n> \n> Chunk 2: 64 bytes\n> merkle_root[3:0]\n> nonce\n> timestamp\n> target\n> \n> Chunk 3: 64 bytes\n> digest from first sha pass\n> \n> Their improvement requires that all data in Chunk 2 is identical except\n> for the nonce.  With 4 bytes, the birthday paradox means collisions can\n> be found reasonable easily.\n> \n> If hard forks are allowed, then moving more of the merkle root into the\n> 2nd chunk would make things harder.  The timestamp and target could be\n> moved into chunk 1.  This increases the merkle root to 12 bytes in the\n> 2nd chunk.  Finding collisions would be made much more difficult.\n> \n> If ASIC limitations mean that the nonce must stay where it is, this\n> would mean that the merkle root would be split into two pieces.\n> \n> On Tue, May 10, 2016 at 7:57 PM, Peter Todd via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org\n> <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n> \n>     As part of the hard-fork proposed in the HK agreement(1) we'd like\n>     to make the\n>     patented AsicBoost optimisation useless, and hopefully make further\n>     similar\n>     optimizations useless as well.\n> \n>     What's the best way to do this? Ideally this would be SPV\n>     compatible, but if it\n>     requires changes from SPV clients that's ok too. Also the fix this\n>     should be\n>     compatible with existing mining hardware.\n> \n> \n>     1)\n>     https://medium.com/@bitcoinroundtable/bitcoin-roundtable-consensus-266d475a61ff\n> \n>     2)\n>     http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-April/012596.html\n> \n>     --\n>     https://petertodd.org 'peter'[:-1]@petertodd.org <http://petertodd.org>\n> \n>     _______________________________________________\n>     bitcoin-dev mailing list\n>     bitcoin-dev at lists.linuxfoundation.org\n>     <mailto:bitcoin-dev at lists.linuxfoundation.org>\n>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> \n> \n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Sergio Demian Lerner",
                "date": "2016-05-10T21:43:01",
                "message_text_only": "Your idea of moving the Merkle root to the second chunk does not work.\n\nThe AsicBoost can change the version bits and it does not need to find a\ncollision.\n(However *Spondoolies patent *only mentions Merkle collisions:\nhttps://patentscope.wipo.int/search/docservicepdf_pct/id00000032873338/PAMPH/WO2016046820.pdf\n)\n\nBack in 2014 I designed a ASIC-compatible block header that prevents\nAsicBoost in all its forms.\n\nYou can find it here:\nhttps://bitslog.wordpress.com/2014/03/18/the-re-design-of-the-bitcoin-block-header/\n\nBasically, the idea is to put in the first 64 bytes a 4 byte hash of the\nsecond 64-byte chunk. That design also allows increased nonce space in the\nfirst 64 bytes.\n\nBut it you want to do a simpler change, you can more easily use the first\n32 bits of the Parent Block Hash (now currently zero) to store the first 4\nbytes of the SHA256 of the last 16 bytes of the header. That way to \"tie\"\nthe two header chunks. It's a minimal change (but a hard-fork)\n\nBut some ASIC companies already have cores that are better (on power, cost,\nrate, temperature, etc.) than competing companies ASICs. Why do you think a\n10% improvement from AsicBoost is different from many of other improvements\nthey already have (secretly) added? Maybe we (?) should only allow ASICs\nthat have a 100% open source designs?\n\nIf we change the protocol then the message to the ecosystem is that ASIC\noptimizations should be kept secret. It is fair to change the protocol\nbecause we don't like that certain ASIC manufacturer has better chips, if\nthe chips are sold in the market and anyone can buy them? And what about\nusing approximate adders (30% improvement), or dual rail asynchronous\nadders (also more than 10% improvement) ? How do we repair those?\n\nDisclaimer: I have stake in AsicBoost, but I'm not sure about this.\n\n\nOn Tue, May 10, 2016 at 5:27 PM, Tier Nolan via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> The various chunks in the double SHA256 are\n>\n> Chunk 1: 64 bytes\n> version\n> previous_block_digest\n> merkle_root[31:4]\n>\n> Chunk 2: 64 bytes\n> merkle_root[3:0]\n> nonce\n> timestamp\n> target\n>\n> Chunk 3: 64 bytes\n> digest from first sha pass\n>\n> Their improvement requires that all data in Chunk 2 is identical except\n> for the nonce.  With 4 bytes, the birthday paradox means collisions can be\n> found reasonable easily.\n>\n> If hard forks are allowed, then moving more of the merkle root into the\n> 2nd chunk would make things harder.  The timestamp and target could be\n> moved into chunk 1.  This increases the merkle root to 12 bytes in the 2nd\n> chunk.  Finding collisions would be made much more difficult.\n>\n> If ASIC limitations mean that the nonce must stay where it is, this would\n> mean that the merkle root would be split into two pieces.\n>\n> On Tue, May 10, 2016 at 7:57 PM, Peter Todd via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> As part of the hard-fork proposed in the HK agreement(1) we'd like to\n>> make the\n>> patented AsicBoost optimisation useless, and hopefully make further\n>> similar\n>> optimizations useless as well.\n>>\n>> What's the best way to do this? Ideally this would be SPV compatible, but\n>> if it\n>> requires changes from SPV clients that's ok too. Also the fix this should\n>> be\n>> compatible with existing mining hardware.\n>>\n>>\n>> 1)\n>> https://medium.com/@bitcoinroundtable/bitcoin-roundtable-consensus-266d475a61ff\n>>\n>> 2)\n>> http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-April/012596.html\n>>\n>> --\n>> https://petertodd.org 'peter'[:-1]@petertodd.org\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160510/e98fa60e/attachment-0001.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2016-05-10T22:59:52",
                "message_text_only": "Replies inline.\n\nOn 05/10/16 21:43, Sergio Demian Lerner via bitcoin-dev wrote:\n-snip-\n\n> But some ASIC companies already have cores that are better (on power,\n> cost, rate, temperature, etc.) than competing companies ASICs. Why do\n> you think a 10% improvement from AsicBoost is different from many of\n> other improvements they already have (secretly) added? Maybe we (?)\n> should only allow ASICs that have a 100% open source designs?\n\nOne is patented and requires paying a license fee to a group, or more\nlikely, ends up with it being impossible to import hardware from other\njurisdictions into the US/western world. The other requires more\ninvestment in R&D, and over the long run, there is no guaranteed\nadvantage to such groups.\n\n> If we change the protocol then the message to the ecosystem is that ASIC\n> optimizations should be kept secret.\n\nTo some extent, this is the case, but there is a strong difference\nbetween a guaranteed advantage enforced by the legal system and one that\nis true due to intellectual superiority. In the long run, I am confident\nthe second will not remain the case. For example, AsicBoost was\nindependently discovered by at least two companies/individuals within a\nyear or two.\n\n> It is fair to change the protocol\n> because we don't like that certain ASIC manufacturer has better chips,\n> if the chips are sold in the market and anyone can buy them? And what\n> about using approximate adders (30% improvement), or dual rail\n> asynchronous adders (also more than 10% improvement) ? How do we repair\n> those?\n\nAs far as I'm aware neither of these are patented. Is this not the case?\n\n> Disclaimer: I have stake in AsicBoost, but I'm not sure about this.\n>  \n> \n> On Tue, May 10, 2016 at 5:27 PM, Tier Nolan via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org\n> <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n> \n>     The various chunks in the double SHA256 are\n> \n>     Chunk 1: 64 bytes\n>     version\n>     previous_block_digest\n>     merkle_root[31:4]\n> \n>     Chunk 2: 64 bytes\n>     merkle_root[3:0]\n>     nonce\n>     timestamp\n>     target\n> \n>     Chunk 3: 64 bytes\n>     digest from first sha pass\n> \n>     Their improvement requires that all data in Chunk 2 is identical\n>     except for the nonce.  With 4 bytes, the birthday paradox means\n>     collisions can be found reasonable easily.\n> \n>     If hard forks are allowed, then moving more of the merkle root into\n>     the 2nd chunk would make things harder.  The timestamp and target\n>     could be moved into chunk 1.  This increases the merkle root to 12\n>     bytes in the 2nd chunk.  Finding collisions would be made much more\n>     difficult.\n> \n>     If ASIC limitations mean that the nonce must stay where it is, this\n>     would mean that the merkle root would be split into two pieces.\n> \n>     On Tue, May 10, 2016 at 7:57 PM, Peter Todd via bitcoin-dev\n>     <bitcoin-dev at lists.linuxfoundation.org\n>     <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n> \n>         As part of the hard-fork proposed in the HK agreement(1) we'd\n>         like to make the\n>         patented AsicBoost optimisation useless, and hopefully make\n>         further similar\n>         optimizations useless as well.\n> \n>         What's the best way to do this? Ideally this would be SPV\n>         compatible, but if it\n>         requires changes from SPV clients that's ok too. Also the fix\n>         this should be\n>         compatible with existing mining hardware.\n> \n> \n>         1)\n>         https://medium.com/@bitcoinroundtable/bitcoin-roundtable-consensus-266d475a61ff\n> \n>         2)\n>         http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-April/012596.html\n> \n>         --\n>         https://petertodd.org 'peter'[:-1]@petertodd.org\n>         <http://petertodd.org>\n> \n>         _______________________________________________\n>         bitcoin-dev mailing list\n>         bitcoin-dev at lists.linuxfoundation.org\n>         <mailto:bitcoin-dev at lists.linuxfoundation.org>\n>         https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> \n> \n>     _______________________________________________\n>     bitcoin-dev mailing list\n>     bitcoin-dev at lists.linuxfoundation.org\n>     <mailto:bitcoin-dev at lists.linuxfoundation.org>\n>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> \n> \n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Sergio Demian Lerner",
                "date": "2016-05-11T12:20:55",
                "message_text_only": "On Tue, May 10, 2016 at 6:43 PM, Sergio Demian Lerner <\nsergio.d.lerner at gmail.com> wrote:\n\n>\n>\n> You can find it here:\n> https://bitslog.wordpress.com/2014/03/18/the-re-design-of-the-bitcoin-block-header/\n>\n> Basically, the idea is to put in the first 64 bytes a 4 byte hash of the\n> second 64-byte chunk. That design also allows increased nonce space in the\n> first 64 bytes.\n>\n> My mistake here. I didn't recalled correctly my own idea. The idea is to\ninclude in the second 64-byte chunk a 4-byte hash of the first chunk, not\nthe opposite.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160511/b0e9c92b/attachment.html>"
            },
            {
                "author": "Marek Palatinus",
                "date": "2016-05-11T13:08:57",
                "message_text_only": "Ehm, I though those discussions about \"ASICs are bad, because X\" ended\nyears ago by starting \"ASIC unfriendly\" altcoins. ASIC industry is twisted\neven without AsicBoost. I don't see any particular reason why to change\nrules just because of 10% edge.\n\nThis is opening Pandora box and it is potentially extremely dangerous for\nthe health of the network. You cannot know in advance what you'll break by\nchanging the rules.\n\nDisclaimer: I don't have any stake in any ASIC company/facility.\n\nslush\n\nOn Wed, May 11, 2016 at 2:20 PM, Sergio Demian Lerner via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n>\n> On Tue, May 10, 2016 at 6:43 PM, Sergio Demian Lerner <\n> sergio.d.lerner at gmail.com> wrote:\n>\n>>\n>>\n>> You can find it here:\n>> https://bitslog.wordpress.com/2014/03/18/the-re-design-of-the-bitcoin-block-header/\n>>\n>> Basically, the idea is to put in the first 64 bytes a 4 byte hash of the\n>> second 64-byte chunk. That design also allows increased nonce space in the\n>> first 64 bytes.\n>>\n>> My mistake here. I didn't recalled correctly my own idea. The idea is to\n> include in the second 64-byte chunk a 4-byte hash of the first chunk, not\n> the opposite.\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160511/9940ac75/attachment.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2016-05-11T21:01:57",
                "message_text_only": "Indeed, I think the \"ASICs are bad, because 1-CPU-1-vote\" arguments\nmostly died out long ago, and, indeed, the goal that many making those\narguments had of building \"unoptimizeable\" ASICs failed with them.\n\nI think everyone understands that there will always be some ability to\niterate on ASIC designs, however, a patented optimization breaks that\nassumption. Instead of being freely able to optimize their ASIC design,\npatented optimizations require that people who discover such\noptimizations themselves do not use them, giving one\nmanufacturer/licenser a huge influence in who is successful in a market\nthat we're all relying on remaining rather flat. Indeed, with AsicBoost,\nwe saw Spondoolies independently discover the same optimization, but\nwith the current legal system they would not have been able to sell such\nsystems without licensing AsicBoost.\n\nMatt\n\nOn 05/11/16 13:08, Marek Palatinus via bitcoin-dev wrote:\n> Ehm, I though those discussions about \"ASICs are bad, because X\" ended\n> years ago by starting \"ASIC unfriendly\" altcoins. ASIC industry is\n> twisted even without AsicBoost. I don't see any particular reason why to\n> change rules just because of 10% edge.\n> \n> This is opening Pandora box and it is potentially extremely dangerous\n> for the health of the network. You cannot know in advance what you'll\n> break by changing the rules.\n> \n> Disclaimer: I don't have any stake in any ASIC company/facility.\n> \n> slush\n> \n> On Wed, May 11, 2016 at 2:20 PM, Sergio Demian Lerner via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org\n> <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n> \n> \n> \n>     On Tue, May 10, 2016 at 6:43 PM, Sergio Demian Lerner\n>     <sergio.d.lerner at gmail.com <mailto:sergio.d.lerner at gmail.com>> wrote:\n> \n> \n> \n>         You can find it here:\n>         https://bitslog.wordpress.com/2014/03/18/the-re-design-of-the-bitcoin-block-header/\n> \n>         Basically, the idea is to put in the first 64 bytes a 4 byte\n>         hash of the second 64-byte chunk. That design also allows\n>         increased nonce space in the first 64 bytes.\n> \n>     My mistake here. I didn't recalled correctly my own idea. The idea\n>     is to include in the second 64-byte chunk a 4-byte hash of the first\n>     chunk, not the opposite.\n> \n> \n>     _______________________________________________\n>     bitcoin-dev mailing list\n>     bitcoin-dev at lists.linuxfoundation.org\n>     <mailto:bitcoin-dev at lists.linuxfoundation.org>\n>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> \n> \n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Simon Liu",
                "date": "2016-05-11T22:16:58",
                "message_text_only": "On 05/11/2016 02:01 PM, Matt Corallo via bitcoin-dev wrote:\n> Indeed, I think the \"ASICs are bad, because 1-CPU-1-vote\" arguments\n> mostly died out long ago, and, indeed, the goal that many making those\n> arguments had of building \"unoptimizeable\" ASICs failed with them.\n\nDiscussion quietened down but never went away.  With centralization of\nmining in China, the topic is up for discussion again.  For example,\nZ.Cash will now use Equihash as their proof-of-work scheme.\n\n> giving one\n> manufacturer/licenser a huge influence in who is successful in a market\n> that we're all relying on remaining rather flat.\n\nCentral planning is a slippery slope.  Let the market decide the winners\nand losers.  It's not feasible to hard fork every time an innovation or\nperceived unfair advantage appears in the space.\n\n--Simon"
            },
            {
                "author": "Peter Todd",
                "date": "2016-05-11T22:50:30",
                "message_text_only": "On Wed, May 11, 2016 at 03:16:58PM -0700, Simon Liu via bitcoin-dev wrote:\n> > giving one\n> > manufacturer/licenser a huge influence in who is successful in a market\n> > that we're all relying on remaining rather flat.\n> \n> Central planning is a slippery slope.  Let the market decide the winners\n> and losers.  It's not feasible to hard fork every time an innovation or\n> perceived unfair advantage appears in the space.\n\nThat's why we're asking the market right now, and any actual hard-fork to make\nAsicBoost irrelevant would be voted on by miners themselves and in turn, the\neconomic majority, again letting the market collectively decide.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160511/2ab40e5b/attachment.sig>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2016-05-11T14:28:24",
                "message_text_only": "On Wednesday, May 11, 2016 12:20:55 PM Sergio Demian Lerner via bitcoin-dev \nwrote:\n> On Tue, May 10, 2016 at 6:43 PM, Sergio Demian Lerner <\n> sergio.d.lerner at gmail.com> wrote:\n> > You can find it here:\n> > https://bitslog.wordpress.com/2014/03/18/the-re-design-of-the-bitcoin-blo\n> > ck-header/\n> > \n> > Basically, the idea is to put in the first 64 bytes a 4 byte hash of the\n> > second 64-byte chunk. That design also allows increased nonce space in\n> > the first 64 bytes.\n> \n> My mistake here. I didn't recalled correctly my own idea. The idea is to\n> include in the second 64-byte chunk a 4-byte hash of the first chunk, not\n> the opposite.\n\nWhat if we XOR bytes 64..76 with the first 12 bytes of the SHA2 midstate? \nWould that work?\n\nLuke"
            },
            {
                "author": "Timo Hanke",
                "date": "2016-05-11T16:24:13",
                "message_text_only": "Luke, do you mean to replace the first 4 bytes of the second chunk (bytes\n64..67 in 0-based counting) by the XOR of those 4 bytes with the first 4\nbytes of the midstate? (I assume you don't care about 12 bytes but rather\nthose 4 bytes.)\n\nThis does not work. All it does is adding another computational step before\nyou can check for a collision in those 4 bytes. It makes finding a\ncollision only marginally harder.\n\nOn Wed, May 11, 2016 at 7:28 AM, Luke Dashjr via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Wednesday, May 11, 2016 12:20:55 PM Sergio Demian Lerner via bitcoin-dev\n> wrote:\n> > On Tue, May 10, 2016 at 6:43 PM, Sergio Demian Lerner <\n> > sergio.d.lerner at gmail.com> wrote:\n> > > You can find it here:\n> > >\n> https://bitslog.wordpress.com/2014/03/18/the-re-design-of-the-bitcoin-blo\n> > > ck-header/\n> > >\n> > > Basically, the idea is to put in the first 64 bytes a 4 byte hash of\n> the\n> > > second 64-byte chunk. That design also allows increased nonce space in\n> > > the first 64 bytes.\n> >\n> > My mistake here. I didn't recalled correctly my own idea. The idea is to\n> > include in the second 64-byte chunk a 4-byte hash of the first chunk, not\n> > the opposite.\n>\n> What if we XOR bytes 64..76 with the first 12 bytes of the SHA2 midstate?\n> Would that work?\n>\n> Luke\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160511/499e1347/attachment.html>"
            },
            {
                "author": "Timo Hanke",
                "date": "2016-05-11T18:28:42",
                "message_text_only": "Sorry, you must have meant all 12 bytes. That makes finding a collision\nsubstantially harder. However, you may have to restrict yourself to 10\nbytes because you don't know if any hardware does timestamp rolling\non-chip. Also you create an incentive to mess around with the version bits\ninstead, so you would have to fix that as well. So it basically means a new\nmining header with the real blockheader as a child header.\n\nOn Wed, May 11, 2016 at 9:24 AM, Timo Hanke <timo.hanke at web.de> wrote:\n\n> Luke, do you mean to replace the first 4 bytes of the second chunk (bytes\n> 64..67 in 0-based counting) by the XOR of those 4 bytes with the first 4\n> bytes of the midstate? (I assume you don't care about 12 bytes but rather\n> those 4 bytes.)\n>\n> This does not work. All it does is adding another computational step\n> before you can check for a collision in those 4 bytes. It makes finding a\n> collision only marginally harder.\n>\n> On Wed, May 11, 2016 at 7:28 AM, Luke Dashjr via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> On Wednesday, May 11, 2016 12:20:55 PM Sergio Demian Lerner via\n>> bitcoin-dev\n>> wrote:\n>> > On Tue, May 10, 2016 at 6:43 PM, Sergio Demian Lerner <\n>> > sergio.d.lerner at gmail.com> wrote:\n>> > > You can find it here:\n>> > >\n>> https://bitslog.wordpress.com/2014/03/18/the-re-design-of-the-bitcoin-blo\n>> > > ck-header/\n>> > >\n>> > > Basically, the idea is to put in the first 64 bytes a 4 byte hash of\n>> the\n>> > > second 64-byte chunk. That design also allows increased nonce space in\n>> > > the first 64 bytes.\n>> >\n>> > My mistake here. I didn't recalled correctly my own idea. The idea is to\n>> > include in the second 64-byte chunk a 4-byte hash of the first chunk,\n>> not\n>> > the opposite.\n>>\n>> What if we XOR bytes 64..76 with the first 12 bytes of the SHA2 midstate?\n>> Would that work?\n>>\n>> Luke\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160511/6cb58d22/attachment.html>"
            },
            {
                "author": "Timo Hanke",
                "date": "2016-05-11T22:49:25",
                "message_text_only": "Ups, I forgot that you take the midstate which of course depends on the\nversion number. So forget everything I said about the version bits. You are\nright. But why take the midstate? It can be any hash of the first chunk. So\nyou probably want to take a hash function that's available in standard\nsoftware libraries. And I suppose midstate() is not.\n\n\nOn Wed, May 11, 2016 at 11:28 AM, Timo Hanke <timo.hanke at web.de> wrote:\n\n> Sorry, you must have meant all 12 bytes. That makes finding a collision\n> substantially harder. However, you may have to restrict yourself to 10\n> bytes because you don't know if any hardware does timestamp rolling\n> on-chip. Also you create an incentive to mess around with the version bits\n> instead, so you would have to fix that as well. So it basically means a new\n> mining header with the real blockheader as a child header.\n>\n> On Wed, May 11, 2016 at 9:24 AM, Timo Hanke <timo.hanke at web.de> wrote:\n>\n>> Luke, do you mean to replace the first 4 bytes of the second chunk (bytes\n>> 64..67 in 0-based counting) by the XOR of those 4 bytes with the first 4\n>> bytes of the midstate? (I assume you don't care about 12 bytes but rather\n>> those 4 bytes.)\n>>\n>> This does not work. All it does is adding another computational step\n>> before you can check for a collision in those 4 bytes. It makes finding a\n>> collision only marginally harder.\n>>\n>> On Wed, May 11, 2016 at 7:28 AM, Luke Dashjr via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> On Wednesday, May 11, 2016 12:20:55 PM Sergio Demian Lerner via\n>>> bitcoin-dev\n>>> wrote:\n>>> > On Tue, May 10, 2016 at 6:43 PM, Sergio Demian Lerner <\n>>> > sergio.d.lerner at gmail.com> wrote:\n>>> > > You can find it here:\n>>> > >\n>>> https://bitslog.wordpress.com/2014/03/18/the-re-design-of-the-bitcoin-blo\n>>> > > ck-header/\n>>> > >\n>>> > > Basically, the idea is to put in the first 64 bytes a 4 byte hash of\n>>> the\n>>> > > second 64-byte chunk. That design also allows increased nonce space\n>>> in\n>>> > > the first 64 bytes.\n>>> >\n>>> > My mistake here. I didn't recalled correctly my own idea. The idea is\n>>> to\n>>> > include in the second 64-byte chunk a 4-byte hash of the first chunk,\n>>> not\n>>> > the opposite.\n>>>\n>>> What if we XOR bytes 64..76 with the first 12 bytes of the SHA2 midstate?\n>>> Would that work?\n>>>\n>>> Luke\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160511/faae91b2/attachment.html>"
            },
            {
                "author": "Tom Harding",
                "date": "2016-05-12T02:27:09",
                "message_text_only": "On 5/10/2016 2:43 PM, Sergio Demian Lerner via bitcoin-dev wrote:\n>\n> If we change the protocol then the message to the ecosystem is that\n> ASIC optimizations should be kept secret.\n\nFurther to that point, if THIS optimization had been kept secret, nobody\nwould be talking about doing anything, as with countless other\noptimizations."
            },
            {
                "author": "Allen Piscitello",
                "date": "2016-05-12T02:31:33",
                "message_text_only": "And anyone who would have discovered it independently would have been free\nto implement it.  That's the issue, not that there's an optimization.\n\nOn Wed, May 11, 2016 at 9:27 PM, Tom Harding via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On 5/10/2016 2:43 PM, Sergio Demian Lerner via bitcoin-dev wrote:\n> >\n> > If we change the protocol then the message to the ecosystem is that\n> > ASIC optimizations should be kept secret.\n>\n> Further to that point, if THIS optimization had been kept secret, nobody\n> would be talking about doing anything, as with countless other\n> optimizations.\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160511/e364cf71/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2016-05-12T02:33:13",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA512\n\n\n\nOn 11 May 2016 22:27:09 GMT-04:00, Tom Harding via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>On 5/10/2016 2:43 PM, Sergio Demian Lerner via bitcoin-dev wrote:\n>>\n>> If we change the protocol then the message to the ecosystem is that\n>> ASIC optimizations should be kept secret.\n>\n>Further to that point, if THIS optimization had been kept secret,\n>nobody\n>would be talking about doing anything, as with countless other\n>optimizations.\n\nThe optimisation has been independently discovered two or three times (Spondoolies and maybe Bitmain).\n-----BEGIN PGP SIGNATURE-----\n\niQE9BAEBCgAnIBxQZXRlciBUb2RkIDxwZXRlQHBldGVydG9kZC5vcmc+BQJXM+tK\nAAoJEGOZARBE6K+yz4MH/j9TstqbVNG3nU+SJ9+Q9aZ0mZSQfR+4qgybGridjo7H\nTzGCnBVCLHt0LnbmZheFv/k9p+m2PojvGGKfODLIDFDHVPHv2wKflKIANIqxpXh/\nBl1SObDoKlRyby4fT22dW5SVSJsjVwTrYwTr2fmRfroeCLgJrHrr03AD7qmMf7CN\nMPrlpitLHZiEoSThTas3pTEEgL2EBgfZnxaaj96jQaMJloz0WjQaocllahl/gsme\n40BQ9TnSHZ02bBf9iEN/FqGhrEN8m2JL7AEyOCuGwrWJtfQ5b9kSpL2QSpuXSfQ7\n1d+OialY2G2L3QMPlnBMKdWGscUyapkYax3FmyA6wxI=\n=j9k+\n-----END PGP SIGNATURE-----"
            },
            {
                "author": "Tom Harding",
                "date": "2016-05-12T04:01:29",
                "message_text_only": "On May 11, 2016 7:33 PM, \"Peter Todd\" <pete at petertodd.org> wrote:\n\n> The optimisation has been independently discovered two or three times\n(Spondoolies and maybe Bitmain).\n\nThe idea that a precedent can be set, whereby those who seek or are awarded\nmining optimization patents risk retaliatory consensus changes, is very\nunrealistic, and such a precedent would actually encode a dependency on the\ninsane patent systems of the world into the protocol development process.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160511/3698300a/attachment-0001.html>"
            },
            {
                "author": "Marco Pontello",
                "date": "2016-05-10T21:49:25",
                "message_text_only": "On Tue, May 10, 2016 at 8:57 PM, Peter Todd via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> As part of the hard-fork proposed in the HK agreement(1) we'd like to make\n> the\n> patented AsicBoost optimisation useless, and hopefully make further similar\n> optimizations useless as well.\n>\n\nJust in the interest of clarity, I think you should clarify who you are\nincluding in the \"we\".\n\nBye!\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160510/2edd63b6/attachment.html>"
            },
            {
                "author": "Sergio Demian Lerner",
                "date": "2016-05-10T22:17:42",
                "message_text_only": "On Tue, May 10, 2016 at 3:57 PM, Peter Todd via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> As part of the hard-fork proposed in the HK agreement(1) we'd like to make\n> the\n> patented AsicBoost optimisation useless, and hopefully make further similar\n> optimizations useless as well.\n>\n>\n> You say that you want to make patented optimization useless, but you point\nto a link that doesn't say anything about ASIC improvements or patents,\nwhich means that you have been planning to change the protocol rules with\nsome miners (but not all the community).\n\nAll changes to the protocol should be discussed in public here. If you want\nto make \"further similar optimizations useless as well\" then maybe you\nshould propose a switch to EquiHash.\n\n\n\n>\n> 1)\n> https://medium.com/@bitcoinroundtable/bitcoin-roundtable-consensus-266d475a61ff\n>\n> 2)\n> http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-April/012596.html\n>\n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160510/a24f3b78/attachment.html>"
            },
            {
                "author": "Chris Riley",
                "date": "2016-05-10T22:27:54",
                "message_text_only": "The second like \"2)\" has a link to the paper:\nhttp://www.math.rwth-aachen.de/~Timo.Hanke/AsicBoostWhitepaperrev5.pdf\n\nwhich does discuss the fact that it is \"patent-pending\".   Likewise it\ndiscusses ASIC improvements.  Avoiding patents that impact bitcoin and are\nnot freely licensed, is something that is worthwhile for discussion.\n\n\nOn Tue, May 10, 2016 at 6:17 PM, Sergio Demian Lerner via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n>\n> On Tue, May 10, 2016 at 3:57 PM, Peter Todd via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> As part of the hard-fork proposed in the HK agreement(1) we'd like to\n>> make the\n>> patented AsicBoost optimisation useless, and hopefully make further\n>> similar\n>> optimizations useless as well.\n>>\n>>\n>> You say that you want to make patented optimization useless, but you\n> point to a link that doesn't say anything about ASIC improvements or\n> patents, which means that you have been planning to change the protocol\n> rules with some miners (but not all the community).\n>\n\n> All changes to the protocol should be discussed in public here. If you\n> want to make \"further similar optimizations useless as well\" then maybe you\n> should propose a switch to EquiHash.\n>\n>\n>\n>>\n>> 1)\n>> https://medium.com/@bitcoinroundtable/bitcoin-roundtable-consensus-266d475a61ff\n>>\n>> 2)\n>> http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-April/012596.html\n>>\n>> --\n>> https://petertodd.org 'peter'[:-1]@petertodd.org\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160510/cd50e919/attachment.html>"
            },
            {
                "author": "Timo Hanke",
                "date": "2016-05-11T03:14:33",
                "message_text_only": "There is no way to tell from a block if it was mined with AsicBoost or not.\nSo you don\u2019t know what percentage of the hashrate uses AsicBoost at any\npoint in time. How can you risk forking that percentage out? Note that this\nwould be a GUARANTEED chain fork. Meaning that after you change the block\nmining algorithm some percentage of hardware will no longer be able to\nproduce valid blocks. That hardware cannot \u201cswitch over\u201d to the majority\nchain even if it wanted to. Hence you are guaranteed to have two\nco-existing bitcoin blockchains afterwards.\n\nAgain: this is unlike the hypothetical persistence of two chains after a\nhardfork that is only contentious but doesn\u2019t change the mining algorithm,\nthe kind of hardfork you are proposing would guarantee the persistence of\ntwo chains.\n\nNote that \u201cAsicBoost\u201d above is replaceable with \u201coptimization X\u201d. It\u2019s\nsimply a logical argument: If you want to make optimization X impossible\nand someone is already using optimization X you end up with two chains. So\nunless you know exactly which optimizations are in use (and therefore also\nknow which ones are not in use) you can\u2019t make these kind of changes.\nAsicBoost is known at least since middle of 2013.\n\nTo be more precise, if you change the block validation ruleset R to block\nvalidation ruleset S you have to make sure that every hardware that was\ncapable of mining R-valid blocks is also capable of mining S-valid blocks.\n\nThe problem is that chip manufacturers will not tell you which\noptimizations they use. You would have to threaten to irreversibly fork\ntheir hardware out by a rule change, only then would they start shouting\nand reveal their optimization. It seems extremely dangerous to set the\nprecedence of a hardfork that irreversibly forks out a certain type of\nmining hardware.\n\nThe part \"Also the fix should be compatible with existing mining hardware.\"\nis impossible to achieve because it's unclear what \"existing mining\nhardware\" is. There has never been a specification of what mining hardware\nshould do. There are only acceptance rules.\n\nThe only way out is to go the exact opposite way and to embrace as many\noptimizations as possible to the point where there are no more\noptimizations left to do, or hopefully getting very close to that point.\n\nTimo\n\n\n\nOn Tue, May 10, 2016 at 11:57 AM, Peter Todd via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> As part of the hard-fork proposed in the HK agreement(1) we'd like to make\n> the\n> patented AsicBoost optimisation useless, and hopefully make further similar\n> optimizations useless as well.\n>\n> What's the best way to do this? Ideally this would be SPV compatible, but\n> if it\n> requires changes from SPV clients that's ok too. Also the fix this should\n> be\n> compatible with existing mining hardware.\n>\n>\n> 1)\n> https://medium.com/@bitcoinroundtable/bitcoin-roundtable-consensus-266d475a61ff\n>\n> 2)\n> http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-April/012596.html\n>\n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160510/26eb0bad/attachment.html>"
            },
            {
                "author": "Jannes Faber",
                "date": "2016-05-11T09:21:10",
                "message_text_only": "On 11 May 2016 at 05:14, Timo Hanke via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> There is no way to tell from a block if it was mined with AsicBoost or\n> not. So you don\u2019t know what percentage of the hashrate uses AsicBoost at\n> any point in time. How can you risk forking that percentage out? Note that\n> this would be a GUARANTEED chain fork. Meaning that after you change the\n> block mining algorithm some percentage of hardware will no longer be able\n> to produce valid blocks. That hardware cannot \u201cswitch over\u201d to the majority\n> chain even if it wanted to. Hence you are guaranteed to have two\n> co-existing bitcoin blockchains afterwards.\n>\n> Again: this is unlike the hypothetical persistence of two chains after a\n> hardfork that is only contentious but doesn\u2019t change the mining algorithm,\n> the kind of hardfork you are proposing would guarantee the persistence of\n> two chains.\n>\n\nAssuming AsicBoost miners are in the minority, their chain will constantly\nget overtaken. So it will not be one endless hard fork as you claim, but\nrather AsicBoost blocks will continue to be ignored (orphaned) until they\nstop making them.\n\nThat hardware cannot \u201cswitch over\u201d to the majority chain even if it wanted\n> to.\n>\n\nThey will in fact continually \"switch over\" to the majority, they just are\nunable to extend that majority chain themselves.\n\n--\nJannes\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160511/97d6a71f/attachment.html>"
            },
            {
                "author": "Henning Kopp",
                "date": "2016-05-11T10:36:01",
                "message_text_only": "On Wed, May 11, 2016 at 11:21:10AM +0200, Jannes Faber via bitcoin-dev wrote:\n> On 11 May 2016 at 05:14, Timo Hanke via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> > There is no way to tell from a block if it was mined with AsicBoost or\n> > not. So you don\u2019t know what percentage of the hashrate uses AsicBoost at\n> > any point in time. How can you risk forking that percentage out? Note that\n> > this would be a GUARANTEED chain fork. Meaning that after you change the\n> > block mining algorithm some percentage of hardware will no longer be able\n> > to produce valid blocks. That hardware cannot \u201cswitch over\u201d to the majority\n> > chain even if it wanted to. Hence you are guaranteed to have two\n> > co-existing bitcoin blockchains afterwards.\n> >\n> > Again: this is unlike the hypothetical persistence of two chains after a\n> > hardfork that is only contentious but doesn\u2019t change the mining algorithm,\n> > the kind of hardfork you are proposing would guarantee the persistence of\n> > two chains.\n> >\n> \n> Assuming AsicBoost miners are in the minority, their chain will constantly\n> get overtaken. So it will not be one endless hard fork as you claim, but\n> rather AsicBoost blocks will continue to be ignored (orphaned) until they\n> stop making them.\n\nAt least until a difficulty adjustment on the AsicBoost chain takes\nplace. From that point on, both chains, the AsicBoost one and the\nforked one will grow approximately at the same speed.\n\nAll the best\nHenning\n\n\n-- \nHenning Kopp\nInstitute of Distributed Systems\nUlm University, Germany\n\nOffice: O27 - 3402\nPhone: +49 731 50-24138\nWeb: http://www.uni-ulm.de/in/vs/~kopp"
            },
            {
                "author": "Jannes Faber",
                "date": "2016-05-11T10:47:58",
                "message_text_only": "On 11 May 2016 at 12:36, Henning Kopp <henning.kopp at uni-ulm.de> wrote:\n\n> On Wed, May 11, 2016 at 11:21:10AM +0200, Jannes Faber via bitcoin-dev\n> wrote:\n> > On 11 May 2016 at 05:14, Timo Hanke via bitcoin-dev <\n> > bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >\n> > > There is no way to tell from a block if it was mined with AsicBoost or\n> > > not. So you don\u2019t know what percentage of the hashrate uses AsicBoost\n> at\n> > > any point in time. How can you risk forking that percentage out? Note\n> that\n> > > this would be a GUARANTEED chain fork. Meaning that after you change\n> the\n> > > block mining algorithm some percentage of hardware will no longer be\n> able\n> > > to produce valid blocks. That hardware cannot \u201cswitch over\u201d to the\n> majority\n> > > chain even if it wanted to. Hence you are guaranteed to have two\n> > > co-existing bitcoin blockchains afterwards.\n> > >\n> > > Again: this is unlike the hypothetical persistence of two chains after\n> a\n> > > hardfork that is only contentious but doesn\u2019t change the mining\n> algorithm,\n> > > the kind of hardfork you are proposing would guarantee the persistence\n> of\n> > > two chains.\n> > >\n> >\n> > Assuming AsicBoost miners are in the minority, their chain will\n> constantly\n> > get overtaken. So it will not be one endless hard fork as you claim, but\n> > rather AsicBoost blocks will continue to be ignored (orphaned) until they\n> > stop making them.\n>\n> At least until a difficulty adjustment on the AsicBoost chain takes\n> place. From that point on, both chains, the AsicBoost one and the\n> forked one will grow approximately at the same speed.\n>\n>\nNo: you are still assuming AsicBoost miners would reject normal blocks.\nThey don't now and they would have to specifically code for that as a reply\nto AsicBoost being banned. So there won't be two chains at all, only the\nmain chain with a lot (more than usual) of short (few blocks) forks. Each\nforks starts anew, it's not one long fork. Therefore there is no\n\"difficulty adjustment on the AiscBoost chain\".\n\nNow if they do decide to ban non-AsicBoost blocks as a response to being\nbanned themselves, they're just another altcoin with a different PoW and no\none would have a reason to use them over Bitcoin (apart from maybe selling\nthose forked coins asap).\n\nYou're confused about what \"longest\" means as well: it's not just the\nnumber of blocks, it's the aggregate difficulty that counts: so AsicBoost\nwould never become \"longer\" (more total work) either.\n\nHope this helps clear things up.\n\n--\nJannes\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160511/9f723302/attachment.html>"
            },
            {
                "author": "Timo Hanke",
                "date": "2016-05-11T22:42:35",
                "message_text_only": "On Wed, May 11, 2016 at 3:47 AM, Jannes Faber <jannes.faber at gmail.com>\nwrote:\n\n> On 11 May 2016 at 12:36, Henning Kopp <henning.kopp at uni-ulm.de> wrote:\n>\n>> On Wed, May 11, 2016 at 11:21:10AM +0200, Jannes Faber via bitcoin-dev\n>> wrote:\n>> > On 11 May 2016 at 05:14, Timo Hanke via bitcoin-dev <\n>> > bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> >\n>> > > There is no way to tell from a block if it was mined with AsicBoost or\n>> > > not. So you don\u2019t know what percentage of the hashrate uses AsicBoost\n>> at\n>> > > any point in time. How can you risk forking that percentage out? Note\n>> that\n>> > > this would be a GUARANTEED chain fork. Meaning that after you change\n>> the\n>> > > block mining algorithm some percentage of hardware will no longer be\n>> able\n>> > > to produce valid blocks. That hardware cannot \u201cswitch over\u201d to the\n>> majority\n>> > > chain even if it wanted to. Hence you are guaranteed to have two\n>> > > co-existing bitcoin blockchains afterwards.\n>> > >\n>> > > Again: this is unlike the hypothetical persistence of two chains\n>> after a\n>> > > hardfork that is only contentious but doesn\u2019t change the mining\n>> algorithm,\n>> > > the kind of hardfork you are proposing would guarantee the\n>> persistence of\n>> > > two chains.\n>> > >\n>> >\n>> > Assuming AsicBoost miners are in the minority, their chain will\n>> constantly\n>> > get overtaken. So it will not be one endless hard fork as you claim, but\n>> > rather AsicBoost blocks will continue to be ignored (orphaned) until\n>> they\n>> > stop making them.\n>>\n>> At least until a difficulty adjustment on the AsicBoost chain takes\n>> place. From that point on, both chains, the AsicBoost one and the\n>> forked one will grow approximately at the same speed.\n>>\n>>\n> No: you are still assuming AsicBoost miners would reject normal blocks.\n> They don't now and they would have to specifically code for that as a reply\n> to AsicBoost being banned. So there won't be two chains at all, only the\n> main chain with a lot (more than usual) of short (few blocks) forks. Each\n> forks starts anew, it's not one long fork. Therefore there is no\n> \"difficulty adjustment on the AiscBoost chain\".\n>\n> Now if they do decide to ban non-AsicBoost blocks as a response to being\n> banned themselves, they're just another altcoin with a different PoW and no\n> one would have a reason to use them over Bitcoin (apart from maybe selling\n> those forked coins asap).\n>\n\nThis is what I meant. If existing hardware gets forked-out it will\ninevitably lead to the creation of an altcoin. Simply because the hardware\nexists and can't be used for anything else both chains will survive. I was\nonly comparing the situation to a contentious hardfork that does not fork\nout any hardware. If the latter one is suspected to lead to the permanent\nexistence of two chains then a hardfork that forks out hardware is even\nmore likely to do so (I claim it's guaranteed).\n\n\n> You're confused about what \"longest\" means as well: it's not just the\n> number of blocks, it's the aggregate difficulty that counts: so AsicBoost\n> would never become \"longer\" (more total work) either.\n>\n> Hope this helps clear things up.\n>\n> --\n> Jannes\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160511/7b9660b1/attachment-0001.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2016-05-11T22:58:48",
                "message_text_only": "On Wed, May 11, 2016 at 10:42 PM, Timo Hanke via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> This is what I meant. If existing hardware gets forked-out it will\n> inevitably lead to the creation of an altcoin. Simply because the hardware\n> exists and can't be used for anything else both chains will survive. I was\n> only comparing the situation to a contentious hardfork that does not fork\n> out any hardware. If the latter one is suspected to lead to the permanent\n> existence of two chains then a hardfork that forks out hardware is even more\n> likely to do so (I claim it's guaranteed).\n\nThere are already many altcoins out there, we could not prevent that\neven if we wanted to. New ones are created all the time.\n\nA 20% inherent advantage, in perfect competition, is likely to lead to\nan eventual monopoly of mining if monopoly patent right prohibit\ncompetitions-- if mining profits go are under the level of that\nenhancement everyone without it would be operating at a loss.\n\nPreserving a vulnerability that will ultimately harm the system's\ndecentralization for just the betterment of some miners does not seem\nlike a rational decision for the users of Bitcoin-- no more than it\nwould reasonable to add a rule that all blocks must be signed by a\nparticular private key.\n\nAs an altcoin the \"asicboost\" altcoin would be one of the least\ninteresting altcoins ever created... after all, no other altcoin has\never been created that required licensing in order to mine.\n\nI don't know if forking it out is the best move here and now, but I'm\nhappy some people are thinking carefully about what it would take to\ndo that."
            },
            {
                "author": "Tom",
                "date": "2016-05-12T07:29:13",
                "message_text_only": "On Wednesday 11 May 2016 22:58:48 Gregory Maxwell via bitcoin-dev wrote:\n> On Wed, May 11, 2016 at 10:42 PM, Timo Hanke via bitcoin-dev\n> \n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > This is what I meant. If existing hardware gets forked-out it will\n> > inevitably lead to the creation of an altcoin. Simply because the hardware\n> > exists and can't be used for anything else both chains will survive. I was\n> > only comparing the situation to a contentious hardfork that does not fork\n> > out any hardware. If the latter one is suspected to lead to the permanent\n> > existence of two chains then a hardfork that forks out hardware is even\n> > more likely to do so (I claim it's guaranteed).\n> \n> There are already many altcoins out there, we could not prevent that\n> even if we wanted to. New ones are created all the time.\n\nComparing apples and oranges.\n\nAltcoins have their own genesis block, the example Timo was talking about was \na fork in the Bitcoin blockchain.\n\nBut its good to know you don't mind a fork in the Bitcoin chain, I'll remember \nthat."
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2016-05-12T11:05:51",
                "message_text_only": "On May 12, 2016 00:43, \"Timo Hanke via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n> This is what I meant. If existing hardware gets forked-out it will\ninevitably lead to the creation of an altcoin. Simply because the hardware\nexists and can't be used for anything else both chains will survive. I was\nonly comparing the situation to a contentious hardfork that does not fork\nout any hardware. If the latter one is suspected to lead to the permanent\nexistence of two chains then a hardfork that forks out hardware is even\nmore likely to do so (I claim it's guaranteed).\n\nYou are wrong. Whether 2 chains survive in parallel or not depends SOLELY\nin whether both chains maintain demand (aka users).\nAnyway, this is a discussion I had with Gavin and Rusty on bitcoin-discuss\nalready. I suggest we move this particular point there since it is more\nphilosophical than technical.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160512/9203e278/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2016-05-11T14:07:26",
                "message_text_only": "On May 11, 2016 05:15, \"Timo Hanke via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> Again: this is unlike the hypothetical persistence of two chains after a\nhardfork that is only contentious but doesn\u2019t change the mining algorithm,\nthe kind of hardfork you are proposing would guarantee the persistence of\ntwo chains.\n\nIf all users abandon the old rules, why would asicboost miners continue to\nspend energy on a chain that everybody else is ignoring?\n\n> To be more precise, if you change the block validation ruleset R to block\nvalidation ruleset S you have to make sure that every hardware that was\ncapable of mining R-valid blocks is also capable of mining S-valid blocks.\n\nWhy?\nNo, this proposal, for example, may make patented asicboost hardware\nobsolete.\nI don't accept this claim as true, this is just your opinion.\n\n>\n> The only way out is to go the exact opposite way and to embrace as many\noptimizations as possible to the point where there are no more\noptimizations left to do, or hopefully getting very close to that point.\n\nWhat do you mean by \"embrace\" in the context of a patented optimization\nthat one miner can prevent the rest from using?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160511/d39670b3/attachment.html>"
            },
            {
                "author": "Sergio Demian Lerner",
                "date": "2016-05-11T14:18:34",
                "message_text_only": "Jorge Tim\u00f3n said..\n> What do you mean by \"embrace\" in the context of a patented optimization\nthat one miner can prevent the rest from using?\n\nEveryone seems to assume that one ASIC manufacturer will get the advantage\nof AsicBoost while others won't. If a patent license is non-exclusive, then\nall can.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160511/63abfd40/attachment.html>"
            },
            {
                "author": "Jannes Faber",
                "date": "2016-05-11T14:30:04",
                "message_text_only": "On 11 May 2016 at 16:18, Sergio Demian Lerner via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Jorge Tim\u00f3n said..\n> > What do you mean by \"embrace\" in the context of a patented optimization\n> that one miner can prevent the rest from using?\n>\n> Everyone seems to assume that one ASIC manufacturer will get the advantage\n> of AsicBoost while others won't. If a patent license is non-exclusive, then\n> all can.\n>\n>\n\n1. Whatever way you look at it, it will be an extra barrier of entry (cost,\nlegal hassle, more complex chip design) for any new ASIC manufacturer\ntrying to enter the market. That counters free competition and thus\ndecentralization.\n\n2. Why would you want to put yourself in the central spot of the big\ndecider on who gets access to the technology (and therefore the whole\nmining game) and who doesn't. You're not afraid of NSA knocking on your\ndoor to politely hand you their blacklist? You don't think this counters\nall the years of hard work that went into Bitcoin exactly to avoid any such\ncentral points of authority?\n\nP.S. I'm not decided yet on being for or against a HF to ban AsicBoost\nmyself, nor does my opinion count for much. But I think I do see real\nproblems, like the above.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160511/58f1e77b/attachment-0001.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2016-05-11T20:50:10",
                "message_text_only": "That's the reason for this post! All current major ASIC manufacturers\nhave made warrants that they are not using AsicBoost (with the exception\nof the 21 Inc Bitcoin computer).\n\nThe fact that the optimization was patented is what has required that we\nwork to hardfork it out, not that people might have such private\noptimizations. The fact that AsicBoost was independently discovered by\nat least two (if not three) organizations seems to lend credence to the\nidea that private optimizations will only provide a temporary win over\ncompetitors.\n\nMatt\n\nOn 05/11/16 03:14, Timo Hanke via bitcoin-dev wrote:\n> There is no way to tell from a block if it was mined with AsicBoost or\n> not. So you don\u2019t know what percentage of the hashrate uses AsicBoost at\n> any point in time. How can you risk forking that percentage out? Note\n> that this would be a GUARANTEED chain fork. Meaning that after you\n> change the block mining algorithm some percentage of hardware will no\n> longer be able to produce valid blocks. That hardware cannot \u201cswitch\n> over\u201d to the majority chain even if it wanted to. Hence you are\n> guaranteed to have two co-existing bitcoin blockchains afterwards.\n> \n> Again: this is unlike the hypothetical persistence of two chains after a\n> hardfork that is only contentious but doesn\u2019t change the mining\n> algorithm, the kind of hardfork you are proposing would guarantee the\n> persistence of two chains.\n> \n> Note that \u201cAsicBoost\u201d above is replaceable with \u201coptimization X\u201d. It\u2019s\n> simply a logical argument: If you want to make optimization X impossible\n> and someone is already using optimization X you end up with two chains.\n> So unless you know exactly which optimizations are in use (and therefore\n> also know which ones are not in use) you can\u2019t make these kind of\n> changes. AsicBoost is known at least since middle of 2013.\n> \n> To be more precise, if you change the block validation ruleset R to\n> block validation ruleset S you have to make sure that every hardware\n> that was capable of mining R-valid blocks is also capable of mining\n> S-valid blocks. \n> \n> The problem is that chip manufacturers will not tell you which\n> optimizations they use. You would have to threaten to irreversibly fork\n> their hardware out by a rule change, only then would they start shouting\n> and reveal their optimization. It seems extremely dangerous to set the\n> precedence of a hardfork that irreversibly forks out a certain type of\n> mining hardware.\n> \n> The part \"Also the fix should be compatible with existing mining\n> hardware.\" is impossible to achieve because it's unclear what \"existing\n> mining hardware\" is. There has never been a specification of what mining\n> hardware should do. There are only acceptance rules.\n> \n> The only way out is to go the exact opposite way and to embrace as many\n> optimizations as possible to the point where there are no more\n> optimizations left to do, or hopefully getting very close to that point. \n> \n> Timo\n> \n> \n> \n> On Tue, May 10, 2016 at 11:57 AM, Peter Todd via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org\n> <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n> \n>     As part of the hard-fork proposed in the HK agreement(1) we'd like\n>     to make the\n>     patented AsicBoost optimisation useless, and hopefully make further\n>     similar\n>     optimizations useless as well.\n> \n>     What's the best way to do this? Ideally this would be SPV\n>     compatible, but if it\n>     requires changes from SPV clients that's ok too. Also the fix this\n>     should be\n>     compatible with existing mining hardware.\n> \n> \n>     1)\n>     https://medium.com/@bitcoinroundtable/bitcoin-roundtable-consensus-266d475a61ff\n> \n>     2)\n>     http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-April/012596.html\n> \n>     --\n>     https://petertodd.org 'peter'[:-1]@petertodd.org <http://petertodd.org>\n> \n>     _______________________________________________\n>     bitcoin-dev mailing list\n>     bitcoin-dev at lists.linuxfoundation.org\n>     <mailto:bitcoin-dev at lists.linuxfoundation.org>\n>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> \n> \n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "James Hilliard",
                "date": "2016-05-11T22:00:59",
                "message_text_only": "I was told that the patent appears to be owned exclusively by Bitmain\nin China https://www.google.com/patents/CN105245327A?cl=en\n\nOn Wed, May 11, 2016 at 4:50 PM, Matt Corallo via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> That's the reason for this post! All current major ASIC manufacturers\n> have made warrants that they are not using AsicBoost (with the exception\n> of the 21 Inc Bitcoin computer).\n>\n> The fact that the optimization was patented is what has required that we\n> work to hardfork it out, not that people might have such private\n> optimizations. The fact that AsicBoost was independently discovered by\n> at least two (if not three) organizations seems to lend credence to the\n> idea that private optimizations will only provide a temporary win over\n> competitors.\n>\n> Matt\n>\n> On 05/11/16 03:14, Timo Hanke via bitcoin-dev wrote:\n>> There is no way to tell from a block if it was mined with AsicBoost or\n>> not. So you don\u2019t know what percentage of the hashrate uses AsicBoost at\n>> any point in time. How can you risk forking that percentage out? Note\n>> that this would be a GUARANTEED chain fork. Meaning that after you\n>> change the block mining algorithm some percentage of hardware will no\n>> longer be able to produce valid blocks. That hardware cannot \u201cswitch\n>> over\u201d to the majority chain even if it wanted to. Hence you are\n>> guaranteed to have two co-existing bitcoin blockchains afterwards.\n>>\n>> Again: this is unlike the hypothetical persistence of two chains after a\n>> hardfork that is only contentious but doesn\u2019t change the mining\n>> algorithm, the kind of hardfork you are proposing would guarantee the\n>> persistence of two chains.\n>>\n>> Note that \u201cAsicBoost\u201d above is replaceable with \u201coptimization X\u201d. It\u2019s\n>> simply a logical argument: If you want to make optimization X impossible\n>> and someone is already using optimization X you end up with two chains.\n>> So unless you know exactly which optimizations are in use (and therefore\n>> also know which ones are not in use) you can\u2019t make these kind of\n>> changes. AsicBoost is known at least since middle of 2013.\n>>\n>> To be more precise, if you change the block validation ruleset R to\n>> block validation ruleset S you have to make sure that every hardware\n>> that was capable of mining R-valid blocks is also capable of mining\n>> S-valid blocks.\n>>\n>> The problem is that chip manufacturers will not tell you which\n>> optimizations they use. You would have to threaten to irreversibly fork\n>> their hardware out by a rule change, only then would they start shouting\n>> and reveal their optimization. It seems extremely dangerous to set the\n>> precedence of a hardfork that irreversibly forks out a certain type of\n>> mining hardware.\n>>\n>> The part \"Also the fix should be compatible with existing mining\n>> hardware.\" is impossible to achieve because it's unclear what \"existing\n>> mining hardware\" is. There has never been a specification of what mining\n>> hardware should do. There are only acceptance rules.\n>>\n>> The only way out is to go the exact opposite way and to embrace as many\n>> optimizations as possible to the point where there are no more\n>> optimizations left to do, or hopefully getting very close to that point.\n>>\n>> Timo\n>>\n>>\n>>\n>> On Tue, May 10, 2016 at 11:57 AM, Peter Todd via bitcoin-dev\n>> <bitcoin-dev at lists.linuxfoundation.org\n>> <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n>>\n>>     As part of the hard-fork proposed in the HK agreement(1) we'd like\n>>     to make the\n>>     patented AsicBoost optimisation useless, and hopefully make further\n>>     similar\n>>     optimizations useless as well.\n>>\n>>     What's the best way to do this? Ideally this would be SPV\n>>     compatible, but if it\n>>     requires changes from SPV clients that's ok too. Also the fix this\n>>     should be\n>>     compatible with existing mining hardware.\n>>\n>>\n>>     1)\n>>     https://medium.com/@bitcoinroundtable/bitcoin-roundtable-consensus-266d475a61ff\n>>\n>>     2)\n>>     http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-April/012596.html\n>>\n>>     --\n>>     https://petertodd.org 'peter'[:-1]@petertodd.org <http://petertodd.org>\n>>\n>>     _______________________________________________\n>>     bitcoin-dev mailing list\n>>     bitcoin-dev at lists.linuxfoundation.org\n>>     <mailto:bitcoin-dev at lists.linuxfoundation.org>\n>>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Peter Todd",
                "date": "2016-05-11T23:01:44",
                "message_text_only": "On Tue, May 10, 2016 at 08:14:33PM -0700, Timo Hanke wrote:\n> There is no way to tell from a block if it was mined with AsicBoost or not.\n> So you don\u2019t know what percentage of the hashrate uses AsicBoost at any\n> point in time. How can you risk forking that percentage out? Note that this\n> would be a GUARANTEED chain fork. Meaning that after you change the block\n> mining algorithm some percentage of hardware will no longer be able to\n> produce valid blocks. That hardware cannot \u201cswitch over\u201d to the majority\n> chain even if it wanted to. Hence you are guaranteed to have two\n> co-existing bitcoin blockchains afterwards.\n\nFirst of all, we can easily do this in a way where miners show their support\nfor this change, say with the usual 95% approval threshold we've been using for\nsoft-forks. That gets the % of hashing power on a AsicBoost chain fork down to\n5% at most.\n\nSecondly, we can probably make the consensus PoW allow blocks to be mined using\nboth the existing PoW algorithm, and a very slightly tweaked version where\nimplementing AsicBoost gives no advantage. That removes any incentive to\nimplement AsicBoost, without making any hardware obsolete (such as 21inc's\nhardware). This means that no hashing power at all needs to use the AsicBoost\npatent.\n\nObviously, the fact that miners can support such a change (assuming of course\nthe economic majority approves it as well) changes the negotiation position re:\nlicensing fees; the actual outcome may simply be you guys make the patent 100%\npublic for all to use at a much reduced price, given you're lack of negotiation\nstrength.\n\n> Note that \u201cAsicBoost\u201d above is replaceable with \u201coptimization X\u201d. It\u2019s\n> simply a logical argument: If you want to make optimization X impossible\n> and someone is already using optimization X you end up with two chains. So\n> unless you know exactly which optimizations are in use (and therefore also\n> know which ones are not in use) you can\u2019t make these kind of changes.\n> AsicBoost is known at least since middle of 2013.\n\nI think _patented_ optimizations where one party has a monopoly are very\ndifferent than optimizations that anyone can independently rediscover -\nAsicBoost itself looks to be something that two or three parties independently\ndiscovered.\n\n> The only way out is to go the exact opposite way and to embrace as many\n> optimizations as possible to the point where there are no more\n> optimizations left to do, or hopefully getting very close to that point.\n\n...which is a scenario that may result in a dozen patented optimizations, with\nnew ASIC manufacturers needing a dozen licenses, from potentially hostile\nentities.\n\nFor instance, it's not clear to me if you actually own this patent, or\nCointerra's creditors. Obviously in the latter case, it'd be quite possible\nthat some kind of bankrupcy court ruling results in the patent getting sold to\na hostile entity who will use it against all of Bitcoin. Equally, even if it is\n100% owned by you and Sergio, it'd be very easy for a personal bankrupcy to\nresult in the same scenario (suppose you get into a car accident and lose a\nnegligence lawsuit over it).\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160511/67142ba6/attachment.sig>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2016-05-12T00:02:08",
                "message_text_only": "On Wed, May 11, 2016 at 11:01 PM, Peter Todd via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Secondly, we can probably make the consensus PoW allow blocks to be mined using\n> both the existing PoW algorithm, and a very slightly tweaked version where\n> implementing AsicBoost gives no advantage. That removes any incentive to\n> implement AsicBoost, without making any hardware obsolete\n\nTaking that a step further, the old POW could continue to be accepted\nbut with a 20% target penalty. (or vice versa, with the new POW having\na 20% target boost.)"
            },
            {
                "author": "Russell O'Connor",
                "date": "2016-05-12T01:23:21",
                "message_text_only": "Is the design and manufacturing processes for the most power efficient\nASICs otherwise patent unencumbered?  If not, why do we care so much about\nthis one patent over all the others that stand on the road between pen and\npaper computation and thermodynamically ideal computation?\n\nOn Wed, May 11, 2016 at 8:02 PM, Gregory Maxwell via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Wed, May 11, 2016 at 11:01 PM, Peter Todd via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > Secondly, we can probably make the consensus PoW allow blocks to be\n> mined using\n> > both the existing PoW algorithm, and a very slightly tweaked version\n> where\n> > implementing AsicBoost gives no advantage. That removes any incentive to\n> > implement AsicBoost, without making any hardware obsolete\n>\n> Taking that a step further, the old POW could continue to be accepted\n> but with a 20% target penalty. (or vice versa, with the new POW having\n> a 20% target boost.)\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160511/5acf1e0a/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2016-05-12T01:58:06",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA512\n\n\n\nOn 11 May 2016 21:23:21 GMT-04:00, Russell O'Connor via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>Is the design and manufacturing processes for the most power efficient\n>ASICs otherwise patent unencumbered?  If not, why do we care so much\n>about\n>this one patent over all the others that stand on the road between pen\n>and\n>paper computation and thermodynamically ideal computation?\n\nIf others are found that are significant I think we'd definitely consider fighting them as well.\n-----BEGIN PGP SIGNATURE-----\n\niQE9BAEBCgAnIBxQZXRlciBUb2RkIDxwZXRlQHBldGVydG9kZC5vcmc+BQJXM+Mh\nAAoJEGOZARBE6K+yz4MH/RwBknvWv+/sXLcJop59gTgfphMlt2KRRDs37bOm+ptc\n7eUK+70K6kT64gNEUqZPnYrdV/u1qMad6bo+5Xb3VYEN9jkaQfw6FnKbVJ2oRVSz\n2iDgO+bAe92n72bEJobmMxBpvD8lv+OjCMkWANHT8wr2/toFa2+V7JPipeXkZzvq\nE5qxhfCHNgoIS55S3LkgAI1cUFMVeYf5yc0MsSzmU3sO29OPuqEWTOgVeDwKF3GS\naNvMSEJeyZb0D4C7XPfwQmqhH6aWsno/7no/D7qYppgSWaP8JpwPW/ULGzfU9Fr9\nWdwgD2bX3zgAA3dcNM1nJ4lkoqCuEm2I0dO6Cj39HjE=\n=M5NE\n-----END PGP SIGNATURE-----"
            },
            {
                "author": "Matt Corallo",
                "date": "2016-05-12T01:58:42",
                "message_text_only": "Aside from patents related to the silicon manufacturing process itself and patents not yet published, yes, the process is unencumbered, and setting the correct precedent (that the community will fight large centralization risks) is important in the first case.\n\nMatt\n\nOn May 11, 2016 9:23:21 PM EDT, Russell O'Connor via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>Is the design and manufacturing processes for the most power efficient\n>ASICs otherwise patent unencumbered?  If not, why do we care so much\n>about\n>this one patent over all the others that stand on the road between pen\n>and\n>paper computation and thermodynamically ideal computation?\n>\n>On Wed, May 11, 2016 at 8:02 PM, Gregory Maxwell via bitcoin-dev <\n>bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> On Wed, May 11, 2016 at 11:01 PM, Peter Todd via bitcoin-dev\n>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> > Secondly, we can probably make the consensus PoW allow blocks to be\n>> mined using\n>> > both the existing PoW algorithm, and a very slightly tweaked\n>version\n>> where\n>> > implementing AsicBoost gives no advantage. That removes any\n>incentive to\n>> > implement AsicBoost, without making any hardware obsolete\n>>\n>> Taking that a step further, the old POW could continue to be accepted\n>> but with a 20% target penalty. (or vice versa, with the new POW\n>having\n>> a 20% target boost.)\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n>\n>------------------------------------------------------------------------\n>\n>_______________________________________________\n>bitcoin-dev mailing list\n>bitcoin-dev at lists.linuxfoundation.org\n>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160512/953527e5/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Making AsicBoost irrelevant",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Tier Nolan",
                "Allen Piscitello",
                "Tom Harding",
                "Timo Hanke",
                "Peter Todd",
                "Jorge Tim\u00f3n",
                "Gregory Maxwell",
                "Matt Corallo",
                "Henning Kopp",
                "Chris Riley",
                "James Hilliard",
                "Simon Liu",
                "Luke Dashjr",
                "Marek Palatinus",
                "Jannes Faber",
                "Marco Pontello",
                "Tom",
                "Russell O'Connor",
                "Sergio Demian Lerner"
            ],
            "messages_count": 39,
            "total_messages_chars_count": 73195
        }
    },
    {
        "title": "[bitcoin-dev] Bip44 extension for P2SH/P2WSH/...",
        "thread_messages": [
            {
                "author": "Daniel Weigl",
                "date": "2016-05-13T13:16:20",
                "message_text_only": "Hello List,\n\nWith SegWit approaching it would make sense to define a common derivation scheme how BIP44 compatible wallets will handle P2(W)SH (and later on P2WPKH) receiving addresses.\nI was thinking about starting a BIP for it, but I wanted to get some feedback from other wallets devs first.\n\nIn my opinion there are two(?) different options: \n\n1) Stay with the current Bip44 account, give the user for each public key the option to show it as a P2PKH-Address or a P2SH address and also scan the blockchain for both representation of each public key.\n\t+) This has the advantage, that the user does not need to decide or have to understand that he needs to migrate to a new account type\n\t-) The downside is that the wallet has to scan/look for ever twice as much addresses. In the future when we have a P2WPKH, it will be three times as much.\n\t-) If you have the same xPub/xPriv key in different wallets, you need to be sure both take care for the different address types\n\n2) Define a new derivation path, parallel to Bip44, but a different  'purpose' (eg. <BipNumber-of-this-BIP>' instead of 44'). Let the user choose which account he want to add (\"Normal account\", \"Witness account\").  \n\n\tm / purpose' / coin_type' / account' / change / address_index\n\n\t+) Wallet needs only to take care of 1 address per public key\n\t+) If you use more than one wallet on the same xPub/xPriv it will work or fail completely. You will notice it immediately that there is something wrong\n\t-) User has to understand that (s)he needs to migrate to a new account to get the benefits of SegWit\n\t+) Thus, its easier to make a staged roll-out, only user actively deciding to use SegWit will get it and we can catch bugs earlier.\n\t\n3) other ideas?\n\nMy personal favourite is pt2.\n\nHas any Bip44 compliant wallet already done any integration at this point?\n\nThx,\nDaniel/Mycelium"
            },
            {
                "author": "Pavol Rusnak",
                "date": "2016-05-13T15:00:39",
                "message_text_only": "On 13/05/16 15:16, Daniel Weigl via bitcoin-dev wrote:\n> 2) Define a new derivation path, parallel to Bip44, but a different  'purpose' (eg. <BipNumber-of-this-BIP>' instead of 44'). Let the user choose which account he want to add (\"Normal account\", \"Witness account\").  \n\nWe had quite a long discussion in our team some time ago and we agreed\non that option #2 is much better and we'd like to implement this way in\nmyTREZOR.\n\n> \t+) Wallet needs only to take care of 1 address per public key\n\nTrue, if this BIP only supports P2WPKH.\n\nP2WSH should probably be handled by another account type and another\nBIP, anyway.\n\n> Has any Bip44 compliant wallet already done any integration at this point?\n\nWe have something in the pipeline, but no visible results yet.\n\n-- \nBest Regards / S pozdravom,\n\nPavol \"stick\" Rusnak\nSatoshiLabs.com"
            },
            {
                "author": "Aaron Voisine",
                "date": "2016-05-13T16:03:11",
                "message_text_only": "We use the default BIP32 wallet layout, mentioned in BIP43 as purpose \"0\".\nWe were thinking of of having 4 chains below the \"account\" level, the\noriginal 0 and 1 for receive and change addresses, and then 0x40000000 and\n0x40000001 for P2WPKH-in-P2SH versions of receive and change addresses.\n\nI like the idea of specifying the type of address as a bit field flag.\n0x80000000 is already used to specify hardened derivation, so 0x40000000\nwould be the next available to specify witness addresses. This is\ncompatible with existing accounts and wallet layouts.\n\nAs Daniel mentioned, the downside is that trying to recover on non-segwit\nsoftware will miss segwit receives, however it does avoid the problem of\nhaving to check multiple address types for each key.\n\nAaron Voisine\nco-founder and CEO\nbreadwallet <http://breadwallet.com>\n\nOn Fri, May 13, 2016 at 8:00 AM, Pavol Rusnak via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On 13/05/16 15:16, Daniel Weigl via bitcoin-dev wrote:\n> > 2) Define a new derivation path, parallel to Bip44, but a different\n> 'purpose' (eg. <BipNumber-of-this-BIP>' instead of 44'). Let the user\n> choose which account he want to add (\"Normal account\", \"Witness account\").\n>\n> We had quite a long discussion in our team some time ago and we agreed\n> on that option #2 is much better and we'd like to implement this way in\n> myTREZOR.\n>\n> >       +) Wallet needs only to take care of 1 address per public key\n>\n> True, if this BIP only supports P2WPKH.\n>\n> P2WSH should probably be handled by another account type and another\n> BIP, anyway.\n>\n> > Has any Bip44 compliant wallet already done any integration at this\n> point?\n>\n> We have something in the pipeline, but no visible results yet.\n>\n> --\n> Best Regards / S pozdravom,\n>\n> Pavol \"stick\" Rusnak\n> SatoshiLabs.com\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160513/656973a5/attachment.html>"
            },
            {
                "author": "Pavol Rusnak",
                "date": "2016-05-13T16:11:05",
                "message_text_only": "On 13/05/16 18:03, Aaron Voisine wrote:\n> I like the idea of specifying the type of address as a bit field flag.\n> 0x80000000 is already used to specify hardened derivation, so 0x40000000\n> would be the next available to specify witness addresses. This is\n> compatible with existing accounts and wallet layouts.\n\nI think this is over-optimization. What is the advantage of\n\nm/0'/0x40000000 instead of m/whatever'/0 ?\n\nBut this is off-topic anyway, as we are discussing multiple-accounts per\nwallet layout here, not one-account-per-wallet design.\n\n-- \nBest Regards / S pozdravom,\n\nPavol \"stick\" Rusnak\nSatoshiLabs.com"
            },
            {
                "author": "Aaron Voisine",
                "date": "2016-05-13T16:59:35",
                "message_text_only": "This scheme is independent of the number of accounts. It works with BIP44\nas well as BIP43 purpose 0, or any other BIP43 purpose/layout. Instead of\noverloading the account index to indicate the type of address, you use the\nchain index, which is already being used to indicate what the specific\naddress chain is to be used for, i.e. receive vs change addresses.\n\n\nAaron Voisine\nco-founder and CEO\nbreadwallet <http://breadwallet.com>\n\nOn Fri, May 13, 2016 at 9:11 AM, Pavol Rusnak <stick at satoshilabs.com> wrote:\n\n> On 13/05/16 18:03, Aaron Voisine wrote:\n> > I like the idea of specifying the type of address as a bit field flag.\n> > 0x80000000 is already used to specify hardened derivation, so 0x40000000\n> > would be the next available to specify witness addresses. This is\n> > compatible with existing accounts and wallet layouts.\n>\n> I think this is over-optimization. What is the advantage of\n>\n> m/0'/0x40000000 instead of m/whatever'/0 ?\n>\n> But this is off-topic anyway, as we are discussing multiple-accounts per\n> wallet layout here, not one-account-per-wallet design.\n>\n> --\n> Best Regards / S pozdravom,\n>\n> Pavol \"stick\" Rusnak\n> SatoshiLabs.com\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160513/53135073/attachment.html>"
            },
            {
                "author": "Pavol Rusnak",
                "date": "2016-05-13T17:57:11",
                "message_text_only": "On 13/05/16 18:59, Aaron Voisine wrote:\n> This scheme is independent of the number of accounts. It works with BIP44\n> as well as BIP43 purpose 0, or any other BIP43 purpose/layout. Instead of\n> overloading the account index to indicate the type of address, you use the\n> chain index, which is already being used to indicate what the specific\n> address chain is to be used for, i.e. receive vs change addresses.\n\nI see the advantage here. But there is a major problem here.\n\nWe came up with BIP44 so a wallet can claim it is BIP44 compatible and\nyou can be 100% sure that you can migrate accounts from one wallet\nimplementation to another. This was not previously possible when a\nwallet claimed it is BIP32 compatible.\n\nNow we have a similar problem. When there is a BIP44 wallet, does it\nmean it supports segwit or not? For this reason I would like to see\nanother BIPXX for segwit, so a wallet can claim it is BIP44, BIP44+BIPXX\nor BIPXX compatible and you'll know what other wallets are compatible\nwith it.\n\n-- \nBest Regards / S pozdravom,\n\nPavol \"stick\" Rusnak\nSatoshiLabs.com"
            },
            {
                "author": "Aaron Voisine",
                "date": "2016-05-13T21:42:19",
                "message_text_only": "That's a valid concern, but I don't see the conflict here. In order to\nrecover funds from a wallet conforming to BIPXX, you must have wallet\nsoftware that handles BIPXX. Simply making BIPXX backwards compatible with\npreviously created BIP44 or BIP43 purpose 0 wallets doesn't change this at\nall.\n\n\nAaron Voisine\nco-founder and CEO\nbreadwallet <http://breadwallet.com>\n\nOn Fri, May 13, 2016 at 10:57 AM, Pavol Rusnak <stick at satoshilabs.com>\nwrote:\n\n> On 13/05/16 18:59, Aaron Voisine wrote:\n> > This scheme is independent of the number of accounts. It works with BIP44\n> > as well as BIP43 purpose 0, or any other BIP43 purpose/layout. Instead of\n> > overloading the account index to indicate the type of address, you use\n> the\n> > chain index, which is already being used to indicate what the specific\n> > address chain is to be used for, i.e. receive vs change addresses.\n>\n> I see the advantage here. But there is a major problem here.\n>\n> We came up with BIP44 so a wallet can claim it is BIP44 compatible and\n> you can be 100% sure that you can migrate accounts from one wallet\n> implementation to another. This was not previously possible when a\n> wallet claimed it is BIP32 compatible.\n>\n> Now we have a similar problem. When there is a BIP44 wallet, does it\n> mean it supports segwit or not? For this reason I would like to see\n> another BIPXX for segwit, so a wallet can claim it is BIP44, BIP44+BIPXX\n> or BIPXX compatible and you'll know what other wallets are compatible\n> with it.\n>\n> --\n> Best Regards / S pozdravom,\n>\n> Pavol \"stick\" Rusnak\n> SatoshiLabs.com\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160513/1fbda030/attachment.html>"
            },
            {
                "author": "Jonas Schnelli",
                "date": "2016-05-14T08:16:42",
                "message_text_only": "Hi\n\n> That's a valid concern, but I don't see the conflict here. In order to\n> recover funds from a wallet conforming to BIPXX, you must have wallet\n> software that handles BIPXX. Simply making BIPXX backwards compatible\n> with previously created BIP44 or BIP43 purpose 0 wallets doesn't change\n> this at all.\n\nMaybe I'm going a bit offtopic. Sorry for that.\n\nImporting a bip32 wallet (bip44 or not) is still an expert job IMO.\nAlso importing can lead to bad security practice (especially without a\nsweep).\n\nUsers will send around xpriv or import an seed over a compromised\ncomputer to a cold storage, etc.\n\nI don't think users want to import private keys.\nThey probably want to import the transaction history and send all funds\ncovered by that seed to a new wallet.\n\nI often though that task is better covered by a little GUI tool or\ncli-app/script:\n-> Accept different bip32 schematics (bip32 native, bip44, etc.)\n-> Accept different bip39 (like) implementation\n-> Create large lookup windows\n-> Create a sweep transaction to a new address/wallet and sign/broadcast it.\n-> Export transaction history (CSV)\n\nBut maybe I'm over-complicating things.\n\n--\n</jonas>\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 819 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160514/fb88c8d0/attachment.sig>"
            },
            {
                "author": "Jochen Hoenicke",
                "date": "2016-05-14T12:26:34",
                "message_text_only": "Am 14.05.2016 um 10:16 schrieb Jonas Schnelli via bitcoin-dev:\n> \n> Importing a bip32 wallet (bip44 or not) is still an expert job IMO.\n> Also importing can lead to bad security practice (especially without a\n> sweep).\n\nOne important use case is importing xpubs for watch-only accounts. This\nis necessary for hardware wallets and there are other valid use cases\nfor this.\n\n> \n> Users will send around xpriv or import an seed over a compromised\n> computer to a cold storage, etc.\n>\n> I don't think users want to import private keys.\n> They probably want to import the transaction history and send all funds\n> covered by that seed to a new wallet.\n>\n\nYes, in general it is not a good idea to import private keys and many\nwallets don't even have an option to give out the xprv (except\nindirectly via the backup mechanism).  But even when sweeping a\nbip-44+segwit wallet you need to know where the segwit addresses are.\n\n  Jochen"
            },
            {
                "author": "Pavol Rusnak",
                "date": "2016-05-14T14:07:18",
                "message_text_only": "On 14/05/16 10:16, Jonas Schnelli via bitcoin-dev wrote:\n> Importing a bip32 wallet (bip44 or not) is still an expert job IMO.\n\nThat's simply not true. All reasonable wallets (reasonable = user\noriented) now use BIP39 mnemonic for doing exactly this.\n\n-- \nBest Regards / S pozdravom,\n\nPavol \"stick\" Rusnak\nSatoshiLabs.com\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 819 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160514/0d9c8040/attachment.sig>"
            },
            {
                "author": "Jonas Schnelli",
                "date": "2016-05-14T16:14:43",
                "message_text_only": "> On 14/05/16 10:16, Jonas Schnelli via bitcoin-dev wrote:\n>> Importing a bip32 wallet (bip44 or not) is still an expert job IMO.\n> \n> That's simply not true. All reasonable wallets (reasonable = user\n> oriented) now use BIP39 mnemonic for doing exactly this.\n\nAFAIK: Bip39 import (cross-wallet) is not supported by Schildbachs\nandroid wallet [1] and Electrum [2] and Breadwallet [3].\n\nBut I think forming a BIP39 mnemonic into a extended master private key\nis not the problem here.\n\nThe problems I see:\n* What if the \"old\" wallet has used more then 1000 addresses? I guess\nsome wallets do not even create a lookup window up to 1000 addresses.\nThere is a high chance of loosing funds when doing sweep (move all funds\nto a new wallet) operation.\n\n* I guess most or maybe all wallets will keep all keys (the\n\"lookup-window\" keys) in the wallet database which could affect\nperformance [4]\n\n* I guess most wallets do not offer \"moving the funds to a new seed\" [5]\nwhich results in not solving the problem of a \"lost\" or \"compromised\"\nwallet and implies wrong security to the enduser.\n\n* If I import a bip39 mnemonic into a hardware wallet (assume Trezor or\nKeepkey) I have to type in the words into my computer which bypasses\nsome of the security my hardware wallet provides me (MITM seed attack).\nTogether with the point above this reduces the security of a wallet (in\nparticular cold storage significant).\n\nPlease correct me if I'm wrong.\n\nI just wanted to point out that importing a wallet is a tricky step\nespecially cross-wallet imports (I think cross wallet imports is an\nexperts job without further improvements).\n\n[1] https://github.com/bitcoin-wallet/bitcoin-wallet/issues/245\n[2] http://docs.electrum.org/en/latest/seedphrase.html\n[3] https://github.com/voisine/breadwallet/issues/360\n[4] https://github.com/bitcoin-wallet/bitcoin-wallet/issues/158\n[5]\nhttps://github.com/voisine/breadwallet/blob/master/BreadWallet/BRRestoreViewController.m#L225\n\n</jonas>\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 819 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160514/22aca5af/attachment.sig>"
            },
            {
                "author": "Kenneth Heutmaker",
                "date": "2016-05-14T17:37:02",
                "message_text_only": "> * What if the \"old\" wallet has used more then 1000 addresses? I guess\n> some wallets do not even create a lookup window up to 1000 addresses.\n> There is a high chance of loosing funds when doing sweep (move all funds\n> to a new wallet) operation.\n\nIf that is the case, the wallet is not following the standard. The wallet hierarchy standards like BIP44 specify how to walk an address chain. They all specify that you should keep going until you don\u2019t find any more used keys within the lookup window. If a wallet leaves gaps that are too big, that is also not compliant.\n\nIn any case, if the sweeping wallet understands how the \u201cold\u201d wallet uses the hierarchy, it can be safely swept without a potential loss of funds.\n\n> * I guess most or maybe all wallets will keep all keys (the\n> \"lookup-window\" keys) in the wallet database which could affect\n> performance [4]\n\nYes, wallets with more addresses take more time to process.\n\n> * I guess most wallets do not offer \"moving the funds to a new seed\" [5]\n> which results in not solving the problem of a \"lost\" or \"compromised\"\n> wallet and implies wrong security to the enduser.\n\nSome wallets do and for those that don\u2019t, implementing it is straight forward if it already implements BIP32. It\u2019s just a matter of knowing how the old wallet uses the hierarchy and prioritizing the work.\n\n> * If I import a bip39 mnemonic into a hardware wallet (assume Trezor or\n> Keepkey) I have to type in the words into my computer which bypasses\n> some of the security my hardware wallet provides me (MITM seed attack).\n> Together with the point above this reduces the security of a wallet (in\n> particular cold storage significant).\n\nBoth TREZOR and KeepKey have developed strategies to prevent MITM attacks during seed recovery. TREZOR asks for the words in a random order and in some cases, adds \u2019noise\u2019 words. KeepKey uses a rotating substitution cipher.\n\n> I just wanted to point out that importing a wallet is a tricky step\n> especially cross-wallet imports (I think cross wallet imports is an\n> experts job without further improvements).\n\nI don\u2019t think it is as hard as you think. If a wallet uses BIP32 HD, all of the hard code is already implemented. It is just a matter of stringing the correct sequence of steps together.\n\nAlso, if the new hierarchy is under a separate purpose code as specified in BIP43, there is no need to create new seed. The BIP44 hierarchy and the new hierarchy can be extended from the same seed.\n\n\u2014\nKen Heutmaker, KeepKey"
            },
            {
                "author": "Thomas Voegtlin",
                "date": "2016-05-15T08:53:06",
                "message_text_only": "Le 14/05/2016 18:14, Jonas Schnelli via bitcoin-dev a \u00e9crit :\n> \n> AFAIK: Bip39 import (cross-wallet) is not supported by [...] Electrum [2] .\n> \n\nThat is correct. There are several reasons why I decided not to use\nBIP39 in Electrum. One of them was that BIP39 seed phrases do not\ninclude a version number. A version number is needed in order to\nmaintain backward compatibility, everytime you change the address\nderivation.\n\nElectrum will allocate a new version number for seed phrases that should\nbe derived to segwit addresses.\n\nI guess BIP39 designers will have to change the semantics of their\nchecksum bits, in order to encode a version number for segwit.\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 836 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160515/05a23ff8/attachment.sig>"
            },
            {
                "author": "Pavol Rusnak",
                "date": "2016-05-15T10:04:01",
                "message_text_only": "On 14/05/16 18:14, Jonas Schnelli wrote:\n> AFAIK: Bip39 import (cross-wallet) is not supported by Schildbachs\n> android wallet [1] and Electrum [2] and Breadwallet [3].\n\nThey are not BIP44 compatible wallets. This thread is about BIP44.\n\n> * What if the \"old\" wallet has used more then 1000 addresses? I guess\n\nThey are not following the spec and are thus not BIP44 compatible.\n\n> * If I import a bip39 mnemonic into a hardware wallet (assume Trezor or\n> Keepkey) I have to type in the words into my computer which bypasses\n> some of the security my hardware wallet provides me (MITM seed attack).\n> Together with the point above this reduces the security of a wallet (in\n> particular cold storage significant).\n\nYou should send all your coins to the new seed anyway, but I agree this\nmight be tricky for non-power users.\n\n-- \nBest Regards / S pozdravom,\n\nPavol \"stick\" Rusnak\nSatoshiLabs.com\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 819 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160515/81461096/attachment.sig>"
            },
            {
                "author": "Daniel Weigl",
                "date": "2016-05-15T12:08:14",
                "message_text_only": "Hi,\n\n> 0x40000000 would be the next available to specify witness addresses.\n> This is compatible with existing accounts and wallet layouts.\n\nmy main concern here is that\n -) every Bip<this-bip>-compatible wallet in the future will have to implement all (then probably) legacy derivation and tx schemes.\n -) it does not fail in a deterministic way, if I import a seed or xPriv/xPub across different capable wallets.\n\tIt is more visible if one account has [no funds/does not show up] at all after an import than if something shows up but you need to make sure that the balance is what you might expect.\n\n\nDaniel/Mycelium\n\n\nOn 2016-05-13 18:03, Aaron Voisine wrote:\n> We use the default BIP32 wallet layout, mentioned in BIP43 as purpose\n> \"0\". We were thinking of of having 4 chains below the \"account\"\n> level, the original 0 and 1 for receive and change addresses, and\n> then 0x40000000 and 0x40000001 for P2WPKH-in-P2SH versions of receive\n> and change addresses.\n> \n> I like the idea of specifying the type of address as a bit field\n> flag. 0x80000000 is already used to specify hardened derivation, so\n> 0x40000000 would be the next available to specify witness addresses.\n> This is compatible with existing accounts and wallet layouts.\n> \n> As Daniel mentioned, the downside is that trying to recover on\n> non-segwit software will miss segwit receives, however it does avoid\n> the problem of having to check multiple address types for each key.\n> \n> Aaron Voisine co-founder and CEO breadwallet\n> <http://breadwallet.com>\n> \n> On Fri, May 13, 2016 at 8:00 AM, Pavol Rusnak via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org\n> <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n> \n> On 13/05/16 15:16, Daniel Weigl via bitcoin-dev wrote:\n>> 2) Define a new derivation path, parallel to Bip44, but a different\n>> 'purpose' (eg. <BipNumber-of-this-BIP>' instead of 44'). Let the\n>> user choose which account he want to add (\"Normal account\",\n>> \"Witness account\").\n> \n> We had quite a long discussion in our team some time ago and we\n> agreed on that option #2 is much better and we'd like to implement\n> this way in myTREZOR.\n> \n>> +) Wallet needs only to take care of 1 address per public key\n> \n> True, if this BIP only supports P2WPKH.\n> \n> P2WSH should probably be handled by another account type and another \n> BIP, anyway.\n> \n>> Has any Bip44 compliant wallet already done any integration at this\n>> point?\n> \n> We have something in the pipeline, but no visible results yet.\n> \n> -- Best Regards / S pozdravom,\n> \n> Pavol \"stick\" Rusnak SatoshiLabs.com \n> _______________________________________________ bitcoin-dev mailing\n> list bitcoin-dev at lists.linuxfoundation.org\n> <mailto:bitcoin-dev at lists.linuxfoundation.org> \n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n>"
            },
            {
                "author": "Aaron Voisine",
                "date": "2016-05-15T17:36:54",
                "message_text_only": "On Sun, May 15, 2016 at 5:08 AM, Daniel Weigl via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n> > 0x40000000 would be the next available to specify witness addresses.\n> > This is compatible with existing accounts and wallet layouts.\n>\n> my main concern here is that\n>  -) every Bip<this-bip>-compatible wallet in the future will have to\n> implement all (then probably) legacy derivation and tx schemes.\n>\n\nI can see the advantage of a segwit only scheme, but we will need to\nsupport old derivations anyway for many decades if not indefinitely. People\nare using it to store value for the long term.\n\n\n>  -) it does not fail in a deterministic way, if I import a seed or\n> xPriv/xPub across different capable wallets.\n>         It is more visible if one account has [no funds/does not show up]\n> at all after an import than if something shows up but you need to make sure\n> that the balance is what you might expect.\n>\n\nThis is certainly a downside. It has to be weighed against the benefit of\nbeing able to upgrade existing wallets in place. Asking users to create a\nnew wallet, and replace their recovery phrase backups is an even bigger\nproblem in my estimation.\n\nWhat do you think of doing both? A new BIP43 purpose number for segwit only\nwallets, but that also specifies 0x40000000/1 for the change/receive index\nso that the scheme is compatible with other schemes for upgrade existing\nwallets in place? There will certainly be wallet developers who decide to\nupgrade in place, but we can standardized both how to indicate segwit\nchains, independent of segwit only schemes or upgrade schemes, and still\nhave the advantages of a new segwit only BIP43 purpose number.\n\nAaron Voisine\nco-founder and CEO\nbreadwallet <http://breadwallet.com/>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160515/447aa364/attachment.html>"
            },
            {
                "author": "Andreas Schildbach",
                "date": "2016-05-14T07:00:27",
                "message_text_only": "The whole idea of BIP43 (which BIP44 bases on) is that how these BIPs\ndefine balance retrieval never changes. This is to make sure you always\nsee the same balance on \"same BIP\" wallets (and same seed of course).\n\nSo if you want to add paths, it has to be a new BIP.\n\n\nOn 05/13/2016 03:16 PM, Daniel Weigl via bitcoin-dev wrote:\n> Hello List,\n> \n> With SegWit approaching it would make sense to define a common derivation scheme how BIP44 compatible wallets will handle P2(W)SH (and later on P2WPKH) receiving addresses.\n> I was thinking about starting a BIP for it, but I wanted to get some feedback from other wallets devs first.\n> \n> In my opinion there are two(?) different options: \n> \n> 1) Stay with the current Bip44 account, give the user for each public key the option to show it as a P2PKH-Address or a P2SH address and also scan the blockchain for both representation of each public key.\n> \t+) This has the advantage, that the user does not need to decide or have to understand that he needs to migrate to a new account type\n> \t-) The downside is that the wallet has to scan/look for ever twice as much addresses. In the future when we have a P2WPKH, it will be three times as much.\n> \t-) If you have the same xPub/xPriv key in different wallets, you need to be sure both take care for the different address types\n> \n> 2) Define a new derivation path, parallel to Bip44, but a different  'purpose' (eg. <BipNumber-of-this-BIP>' instead of 44'). Let the user choose which account he want to add (\"Normal account\", \"Witness account\").  \n> \n> \tm / purpose' / coin_type' / account' / change / address_index\n> \n> \t+) Wallet needs only to take care of 1 address per public key\n> \t+) If you use more than one wallet on the same xPub/xPriv it will work or fail completely. You will notice it immediately that there is something wrong\n> \t-) User has to understand that (s)he needs to migrate to a new account to get the benefits of SegWit\n> \t+) Thus, its easier to make a staged roll-out, only user actively deciding to use SegWit will get it and we can catch bugs earlier.\n> \t\n> 3) other ideas?\n> \n> My personal favourite is pt2.\n> \n> Has any Bip44 compliant wallet already done any integration at this point?\n> \n> Thx,\n> Daniel/Mycelium\n>"
            },
            {
                "author": "Pavol Rusnak",
                "date": "2016-05-14T14:08:23",
                "message_text_only": "On 14/05/16 09:00, Andreas Schildbach via bitcoin-dev wrote:\n> The whole idea of BIP43 (which BIP44 bases on) is that how these BIPs\n> define balance retrieval never changes. This is to make sure you always\n> see the same balance on \"same BIP\" wallets (and same seed of course).\n\nThis! Thanks Andreas for formulating my thought that I was not able to\narticulate earlier.\n\n-- \nBest Regards / S pozdravom,\n\nPavol \"stick\" Rusnak\nSatoshiLabs.com"
            },
            {
                "author": "Aaron Voisine",
                "date": "2016-05-14T17:09:42",
                "message_text_only": "On Sat, May 14, 2016 at 7:08 AM, Pavol Rusnak via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On 14/05/16 09:00, Andreas Schildbach via bitcoin-dev wrote:\n> > The whole idea of BIP43 (which BIP44 bases on) is that how these BIPs\n> > define balance retrieval never changes. This is to make sure you always\n> > see the same balance on \"same BIP\" wallets (and same seed of course).\n>\n> This! Thanks Andreas for formulating my thought that I was not able to\n> articulate earlier.\n>\n\nIndeed, this would still be the case when using a new BIPXX to define\nadding segwit chains to what were previously BIP43/44 wallets. In this case\nretrieval of a BIP44 wallet remains exactly the same as it did before. A\nBIP44 wallet can still be recovered with any BIP44 compatible wallet\nsoftware. After you upgrade an existing BIP44 wallet to a BIPXX wallet, now\nit is no longer a BIP44 wallet. It is now a BIPXX wallet, and can only be\nrecovered using BIPXX compatible wallet software.\n\nIf you are concerned about making a new BIP that fits in the BIP43\nframework, i.e. a new purpose number, there's no reason this can't also be\ndone. You could create a new purpose number YY. Wallets that follow BIPYY\nlook just like BIPXX, except that they may only contain segwit address\nchains, no standard P2PKH address chains.\n\nOn Sat, May 14, 2016 at 9:14 AM, Jonas Schnelli via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> AFAIK: Bip39 import (cross-wallet) is not supported by Schildbachs\n> android wallet [1] and Electrum [2] and Breadwallet [3].\n\n\nBreadwallet is BIP39, with the BIP43 purpose 0 derivation path, and I\nbelieve Schlindbachs is as well. Electrum has their own format. I don't\nknow if it also supports sweeping other mnemonics and wallet layouts.\n\nAaron Voisine\nco-founder and CEO\nbreadwallet <http://breadwallet.com/>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160514/270a10da/attachment-0001.html>"
            },
            {
                "author": "Jochen Hoenicke",
                "date": "2016-05-14T12:15:16",
                "message_text_only": "Am 13.05.2016 um 15:16 schrieb Daniel Weigl via bitcoin-dev:\n> \n> With SegWit approaching it would make sense to define a common derivation scheme how BIP44 compatible wallets will handle P2(W)SH (and later on P2WPKH) receiving addresses.\n> I was thinking about starting a BIP for it, but I wanted to get some feedback from other wallets devs first.\n>\n\nThe discussion so far shows that starting a new BIP is a very good idea.\n Otherwise everyone would do it slightly different.\n\nWith P2(W)SH you mean P2WPKH embedded in P2SH, right?  P2WSH is\ncompletely different and used for example for multisig.\n\n\n> In my opinion there are two(?) different options: \n\nTo summarize, option 1 means one account that supports both non-segwit\nand segwit addresses.  With option 2 you have one p2pkh-only account and\none segwit-only account, which are completely separated.\n\nI personally would vote for option 1.  Scanning twice the addresses can\nbe avoided with Aaron's trick.  The second disadvantage remains:\n\n> \t-) If you have the same xPub/xPriv key in different wallets, you need to be sure both take care for the different address types\n\nA non-segwit wallet would ignore all segwit outputs, which means that\nthe balance it shows is smaller (and it doesn't show transactions that\nspend from previous segwit outputs).  I don't see that this can lead to\nlosing money except maybe when sweeping the account with a p2pkh-only\nwallet and then throwing the xprv away.\n\nOf course, you can also do option 2 and let it appear to the user as if\nit was only one account, but what is the advantage over option 1 in that\ncase?  Also you need two xpubs to watch this joined account.\n\n  Jochen"
            }
        ],
        "thread_summary": {
            "title": "Bip44 extension for P2SH/P2WSH/...",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Thomas Voegtlin",
                "Andreas Schildbach",
                "Pavol Rusnak",
                "Kenneth Heutmaker",
                "Daniel Weigl",
                "Jochen Hoenicke",
                "Aaron Voisine",
                "Jonas Schnelli"
            ],
            "messages_count": 20,
            "total_messages_chars_count": 30661
        }
    },
    {
        "title": "[bitcoin-dev] Making UTXO Set Growth Irrelevant With Low-Latency Delayed TXO Commitments",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2016-05-17T13:23:11",
                "message_text_only": "# Motivation\n\nUTXO growth is a serious concern for Bitcoin's long-term decentralization. To\nrun a competitive mining operation potentially the entire UTXO set must be in\nRAM to achieve competitive latency; your larger, more centralized, competitors\nwill have the UTXO set in RAM. Mining is a zero-sum game, so the extra latency\nof not doing so if they do directly impacts your profit margin. Secondly,\nhaving possession of the UTXO set is one of the minimum requirements to run a\nfull node; the larger the set the harder it is to run a full node.\n\nCurrently the maximum size of the UTXO set is unbounded as there is no\nconsensus rule that limits growth, other than the block-size limit itself; as\nof writing the UTXO set is 1.3GB in the on-disk, compressed serialization,\nwhich expands to significantly more in memory. UTXO growth is driven by a\nnumber of factors, including the fact that there is little incentive to merge\ninputs, lost coins, dust outputs that can't be economically spent, and\nnon-btc-value-transfer \"blockchain\" use-cases such as anti-replay oracles and\ntimestamping.\n\nWe don't have good tools to combat UTXO growth. Segregated Witness proposes to\ngive witness space a 75% discount, in part of make reducing the UTXO set size\nby spending txouts cheaper. While this may change wallets to more often spend\ndust, it's hard to imagine an incentive sufficiently strong to discourage most,\nlet alone all, UTXO growing behavior.\n\nFor example, timestamping applications often create unspendable outputs due to\nease of implementation, and because doing so is an easy way to make sure that\nthe data required to reconstruct the timestamp proof won't get lost - all\nBitcoin full nodes are forced to keep a copy of it. Similarly anti-replay\nuse-cases like using the UTXO set for key rotation piggyback on the uniquely\nstrong security and decentralization guarantee that Bitcoin provides; it's very\ndifficult - perhaps impossible - to provide these applications with\nalternatives that are equally secure. These non-btc-value-transfer use-cases\ncan often afford to pay far higher fees per UTXO created than competing\nbtc-value-transfer use-cases; many users could afford to spend $50 to register\na new PGP key, yet would rather not spend $50 in fees to create a standard two\noutput transaction. Effective techniques to resist miner censorship exist, so\nwithout resorting to whitelists blocking non-btc-value-transfer use-cases as\n\"spam\" is not a long-term, incentive compatible, solution.\n\nA hard upper limit on UTXO set size could create a more level playing field in\nthe form of fixed minimum requirements to run a performant Bitcoin node, and\nmake the issue of UTXO \"spam\" less important. However, making any coins\nunspendable, regardless of age or value, is a politically untenable economic\nchange.\n\n\n# TXO Commitments\n\nA merkle tree committing to the state of all transaction outputs, both spent\nand unspent, we can provide a method of compactly proving the current state of\nan output. This lets us \"archive\" less frequently accessed parts of the UTXO\nset, allowing full nodes to discard the associated data, still providing a\nmechanism to spend those archived outputs by proving to those nodes that the\noutputs are in fact unspent.\n\nSpecifically TXO commitments proposes a Merkle Mountain Range\u00b9 (MMR), a\ntype of deterministic, indexable, insertion ordered merkle tree, which allows\nnew items to be cheaply appended to the tree with minimal storage requirements,\njust log2(n) \"mountain tips\". Once an output is added to the TXO MMR it is\nnever removed; if an output is spent its status is updated in place. Both the\nstate of a specific item in the MMR, as well the validity of changes to items\nin the MMR, can be proven with log2(n) sized proofs consisting of a merkle path\nto the tip of the tree.\n\nAt an extreme, with TXO commitments we could even have no UTXO set at all,\nentirely eliminating the UTXO growth problem. Transactions would simply be\naccompanied by TXO commitment proofs showing that the outputs they wanted to\nspend were still unspent; nodes could update the state of the TXO MMR purely\nfrom TXO commitment proofs. However, the log2(n) bandwidth overhead per txin is\nsubstantial, so a more realistic implementation is be to have a UTXO cache for\nrecent transactions, with TXO commitments acting as a alternate for the (rare)\nevent that an old txout needs to be spent.\n\nProofs can be generated and added to transactions without the involvement of\nthe signers, even after the fact; there's no need for the proof itself to\nsigned and the proof is not part of the transaction hash. Anyone with access to\nTXO MMR data can (re)generate missing proofs, so minimal, if any, changes are\nrequired to wallet software to make use of TXO commitments.\n\n\n## Delayed Commitments\n\nTXO commitments aren't a new idea - the author proposed them years ago in\nresponse to UTXO commitments. However it's critical for small miners' orphan\nrates that block validation be fast, and so far it has proven difficult to\ncreate (U)TXO implementations with acceptable performance; updating and\nrecalculating cryptographicly hashed merkelized datasets is inherently more\nwork than not doing so. Fortunately if we maintain a UTXO set for recent\noutputs, TXO commitments are only needed when spending old, archived, outputs.\nWe can take advantage of this by delaying the commitment, allowing it to be\ncalculated well in advance of it actually being used, thus changing a\nlatency-critical task into a much easier average throughput problem.\n\nConcretely each block B_i commits to the TXO set state as of block B_{i-n}, in\nother words what the TXO commitment would have been n blocks ago, if not for\nthe n block delay. Since that commitment only depends on the contents of the\nblockchain up until block B_{i-n}, the contents of any block after are\nirrelevant to the calculation.\n\n\n## Implementation\n\nOur proposed high-performance/low-latency delayed commitment full-node\nimplementation needs to store the following data:\n\n1) UTXO set\n\n    Low-latency K:V map of txouts definitely known to be unspent. Similar to\n    existing UTXO implementation, but with the key difference that old,\n    unspent, outputs may be pruned from the UTXO set.\n\n\n2) STXO set\n\n    Low-latency set of transaction outputs known to have been spent by\n    transactions after the most recent TXO commitment, but created prior to the\n    TXO commitment.\n\n\n3) TXO journal\n\n    FIFO of outputs that need to be marked as spent in the TXO MMR. Appends\n    must be low-latency; removals can be high-latency.\n\n\n4) TXO MMR list\n\n    Prunable, ordered list of TXO MMR's, mainly the highest pending commitment,\n    backed by a reference counted, cryptographically hashed object store\n    indexed by digest (similar to how git repos work). High-latency ok. We'll\n    cover this in more in detail later.\n\n\n### Fast-Path: Verifying a Txout Spend In a Block\n\nWhen a transaction output is spent by a transaction in a block we have two\ncases:\n\n1) Recently created output\n\n    Output created after the most recent TXO commitment, so it should be in the\n    UTXO set; the transaction spending it does not need a TXO commitment proof.\n    Remove the output from the UTXO set and append it to the TXO journal.\n\n2) Archived output\n\n    Output created prior to the most recent TXO commitment, so there's no\n    guarantee it's in the UTXO set; transaction will have a TXO commitment\n    proof for the most recent TXO commitment showing that it was unspent.\n    Check that the output isn't already in the STXO set (double-spent), and if\n    not add it. Append the output and TXO commitment proof to the TXO journal.\n\nIn both cases recording an output as spent requires no more than two key:value\nupdates, and one journal append. The existing UTXO set requires one key:value\nupdate per spend, so we can expect new block validation latency to be within 2x\nof the status quo even in the worst case of 100% archived output spends.\n\n\n### Slow-Path: Calculating Pending TXO Commitments\n\nIn a low-priority background task we flush the TXO journal, recording the\noutputs spent by each block in the TXO MMR, and hashing MMR data to obtain the\nTXO commitment digest. Additionally this background task removes STXO's that\nhave been recorded in TXO commitments, and prunes TXO commitment data no longer\nneeded.\n\nThroughput for the TXO commitment calculation will be worse than the existing\nUTXO only scheme. This impacts bulk verification, e.g. initial block download.\nThat said, TXO commitments provides other possible tradeoffs that can mitigate\nimpact of slower validation throughput, such as skipping validation of old\nhistory, as well as fraud proof approaches.\n\n\n### TXO MMR Implementation Details\n\nEach TXO MMR state is a modification of the previous one with most information\nshared, so we an space-efficiently store a large number of TXO commitments\nstates, where each state is a small delta of the previous state, by sharing\nunchanged data between each state; cycles are impossible in merkelized data\nstructures, so simple reference counting is sufficient for garbage collection.\nData no longer needed can be pruned by dropping it from the database, and\nunpruned by adding it again. Since everything is committed to via cryptographic\nhash, we're guaranteed that regardless of where we get the data, after\nunpruning we'll have the right data.\n\nLet's look at how the TXO MMR works in detail. Consider the following TXO MMR\nwith two txouts, which we'll call state #0:\n\n      0\n     / \\\n    a   b\n\nIf we add another entry we get state #1:\n\n        1\n       / \\\n      0   \\\n     / \\   \\\n    a   b   c\n\nNote how it 100% of the state #0 data was reused in commitment #1. Let's\nadd two more entries to get state #2:\n\n            2\n           / \\\n          2   \\\n         / \\   \\\n        /   \\   \\\n       /     \\   \\\n      0       2   \\\n     / \\     / \\   \\\n    a   b   c   d   e\n\nThis time part of state #1 wasn't reused - it's wasn't a perfect binary\ntree - but we've still got a lot of re-use.\n\nNow suppose state #2 is committed into the blockchain by the most recent block.\nFuture transactions attempting to spend outputs created as of state #2 are\nobliged to prove that they are unspent; essentially they're forced to provide\npart of the state #2 MMR data. This lets us prune that data, discarding it,\nleaving us with only the bare minimum data we need to append new txouts to the\nTXO MMR, the tips of the perfect binary trees (\"mountains\") within the MMR:\n\n            2\n           / \\\n          2   \\\n               \\\n                \\\n                 \\\n                  \\\n                   \\\n                    e\n\nNote that we're glossing over some nuance here about exactly what data needs to\nbe kept; depending on the details of the implementation the only data we need\nfor nodes \"2\" and \"e\" may be their hash digest.\n\nAdding another three more txouts results in state #3:\n\n                  3\n                 / \\\n                /   \\\n               /     \\\n              /       \\\n             /         \\\n            /           \\\n           /             \\\n          2               3\n                         / \\\n                        /   \\\n                       /     \\\n                      3       3\n                     / \\     / \\\n                    e   f   g   h\n\nSuppose recently created txout f is spent. We have all the data required to\nupdate the MMR, giving us state #4. It modifies two inner nodes and one leaf\nnode:\n\n                  4\n                 / \\\n                /   \\\n               /     \\\n              /       \\\n             /         \\\n            /           \\\n           /             \\\n          2               4\n                         / \\\n                        /   \\\n                       /     \\\n                      4       3\n                     / \\     / \\\n                    e  (f)  g   h\n\nIf an archived txout is spent requires the transaction to provide the merkle\npath to the most recently committed TXO, in our case state #2. If txout b is\nspent that means the transaction must provide the following data from state #2:\n\n            2\n           /\n          2\n         /\n        /\n       /\n      0\n       \\\n        b\n\nWe can add that data to our local knowledge of the TXO MMR, unpruning part of\nit:\n\n                  4\n                 / \\\n                /   \\\n               /     \\\n              /       \\\n             /         \\\n            /           \\\n           /             \\\n          2               4\n         /               / \\\n        /               /   \\\n       /               /     \\\n      0               4       3\n       \\             / \\     / \\\n        b           e  (f)  g   h\n\nRemember, we haven't _modified_ state #4 yet; we just have more data about it.\nWhen we mark txout b as spent we get state #5:\n\n                  5\n                 / \\\n                /   \\\n               /     \\\n              /       \\\n             /         \\\n            /           \\\n           /             \\\n          5               4\n         /               / \\\n        /               /   \\\n       /               /     \\\n      5               4       3\n       \\             / \\     / \\\n       (b)          e  (f)  g   h\n\nSecondly by now state #3 has been committed into the chain, and transactions\nthat want to spend txouts created as of state #3 must provide a TXO proof\nconsisting of state #3 data. The leaf nodes for outputs g and h, and the inner\nnode above them, are part of state #3, so we prune them:\n\n                  5\n                 / \\\n                /   \\\n               /     \\\n              /       \\\n             /         \\\n            /           \\\n           /             \\\n          5               4\n         /               /\n        /               /\n       /               /\n      5               4\n       \\             / \\\n       (b)          e  (f)\n\nFinally, lets put this all together, by spending txouts a, c, and g, and\ncreating three new txouts i, j, and k. State #3 was the most recently committed\nstate, so the transactions spending a and g are providing merkle paths up to\nit. This includes part of the state #2 data:\n\n                  3\n                 / \\\n                /   \\\n               /     \\\n              /       \\\n             /         \\\n            /           \\\n           /             \\\n          2               3\n         / \\               \\\n        /   \\               \\\n       /     \\               \\\n      0       2               3\n     /       /               /\n    a       c               g\n\nAfter unpruning we have the following data for state #5:\n\n                  5\n                 / \\\n                /   \\\n               /     \\\n              /       \\\n             /         \\\n            /           \\\n           /             \\\n          5               4\n         / \\             / \\\n        /   \\           /   \\\n       /     \\         /     \\\n      5       2       4       3\n     / \\     /       / \\     /\n    a  (b)  c       e  (f)  g\n\nThat's sufficient to mark the three outputs as spent and add the three new\ntxouts, resulting in state #6:\n\n                        6\n                       / \\\n                      /   \\\n                     /     \\\n                    /       \\\n                   /         \\\n                  6           \\\n                 / \\           \\\n                /   \\           \\\n               /     \\           \\\n              /       \\           \\\n             /         \\           \\\n            /           \\           \\\n           /             \\           \\\n          6               6           \\\n         / \\             / \\           \\\n        /   \\           /   \\           6\n       /     \\         /     \\         / \\\n      6       6       4       6       6   \\\n     / \\     /       / \\     /       / \\   \\\n   (a) (b) (c)      e  (f) (g)      i   j   k\n\nAgain, state #4 related data can be pruned. In addition, depending on how the\nSTXO set is implemented may also be able to prune data related to spent txouts\nafter that state, including inner nodes where all txouts under them have been\nspent (more on pruning spent inner nodes later).\n\n\n### Consensus and Pruning\n\nIt's important to note that pruning behavior is consensus critical: a full node\nthat is missing data due to pruning it too soon will fall out of consensus, and\na miner that fails to include a merkle proof that is required by the consensus\nis creating an invalid block. At the same time many full nodes will have\nsignificantly more data on hand than the bare minimum so they can help wallets\nmake transactions spending old coins; implementations should strongly consider\nseparating the data that is, and isn't, strictly required for consensus.\n\nA reasonable approach for the low-level cryptography may be to actually treat\nthe two cases differently, with the TXO commitments committing too what data\ndoes and does not need to be kept on hand by the UTXO expiration rules. On the\nother hand, leaving that uncommitted allows for certain types of soft-forks\nwhere the protocol is changed to require more data than it previously did.\n\n\n### Consensus Critical Storage Overheads\n\nOnly the UTXO and STXO sets need to be kept on fast random access storage.\nSince STXO set entries can only be created by spending a UTXO - and are smaller\nthan a UTXO entry - we can guarantee that the peak size of the UTXO and STXO\nsets combined will always be less than the peak size of the UTXO set alone in\nthe existing UTXO-only scheme (though the combined size can be temporarily\nhigher than what the UTXO set size alone would be when large numbers of\narchived txouts are spent).\n\nTXO journal entries and unpruned entries in the TXO MMR have log2(n) maximum\noverhead per entry: a unique merkle path to a TXO commitment (by \"unique\" we\nmean that no other entry shares data with it). On a reasonably fast system the\nTXO journal will be flushed quickly, converting it into TXO MMR data; the TXO\njournal will never be more than a few blocks in size.\n\nTransactions spending non-archived txouts are not required to provide any TXO\ncommitment data; we must have that data on hand in the form of one TXO MMR\nentry per UTXO. Once spent however the TXO MMR leaf node associated with that\nnon-archived txout can be immediately pruned - it's no longer in the UTXO set\nso any attempt to spend it will fail; the data is now immutable and we'll never\nneed it again. Inner nodes in the TXO MMR can also be pruned if all leafs under\nthem are fully spent; detecting this is easy the TXO MMR is a merkle-sum tree,\nwith each inner node committing to the sum of the unspent txouts under it.\n\nWhen a archived txout is spent the transaction is required to provide a merkle\npath to the most recent TXO commitment. As shown above that path is sufficient\ninformation to unprune the necessary nodes in the TXO MMR and apply the spend\nimmediately, reducing this case to the TXO journal size question (non-consensus\ncritical overhead is a different question, which we'll address in the next\nsection).\n\nTaking all this into account the only significant storage overhead of our TXO\ncommitments scheme when compared to the status quo is the log2(n) merkle path\noverhead; as long as less than 1/log2(n) of the UTXO set is active,\nnon-archived, UTXO's we've come out ahead, even in the unrealistic case where\nall storage available is equally fast. In the real world that isn't yet the\ncase - even SSD's significantly slower than RAM.\n\n\n### Non-Consensus Critical Storage Overheads\n\nTransactions spending archived txouts pose two challenges:\n\n1) Obtaining up-to-date TXO commitment proofs\n\n2) Updating those proofs as blocks are mined\n\nThe first challenge can be handled by specialized archival nodes, not unlike\nhow some nodes make transaction data available to wallets via bloom filters or\nthe Electrum protocol. There's a whole variety of options available, and the\nthe data can be easily sharded to scale horizontally; the data is\nself-validating allowing horizontal scaling without trust.\n\nWhile miners and relay nodes don't need to be concerned about the initial\ncommitment proof, updating that proof is another matter. If a node aggressively\nprunes old versions of the TXO MMR as it calculates pending TXO commitments, it\nwon't have the data available to update the TXO commitment proof to be against\nthe next block, when that block is found; the child nodes of the TXO MMR tip\nare guaranteed to have changed, yet aggressive pruning would have discarded that\ndata.\n\nRelay nodes could ignore this problem if they simply accept the fact that\nthey'll only be able to fully relay the transaction once, when it is initially\nbroadcast, and won't be able to provide mempool functionality after the initial\nrelay. Modulo high-latency mixnets, this is probably acceptable; the author has\npreviously argued that relay nodes don't need a mempool\u00b2 at all.\n\nFor a miner though not having the data necessary to update the proofs as blocks\nare found means potentially losing out on transactions fees. So how much extra\ndata is necessary to make this a non-issue?\n\nSince the TXO MMR is insertion ordered, spending a non-archived txout can only\ninvalidate the upper nodes in of the archived txout's TXO MMR proof (if this\nisn't clear, imagine a two-level scheme, with a per-block TXO MMRs, committed\nby a master MMR for all blocks). The maximum number of relevant inner nodes\nchanged is log2(n) per block, so if there are n non-archival blocks between the\nmost recent TXO commitment and the pending TXO MMR tip, we have to store\nlog2(n)*n inner nodes - on the order of a few dozen MB even when n is a\n(seemingly ridiculously high) year worth of blocks.\n\nArchived txout spends on the other hand can invalidate TXO MMR proofs at any\nlevel - consider the case of two adjacent txouts being spent. To guarantee\nsuccess requires storing full proofs. However, they're limited by the blocksize\nlimit, and additionally are expected to be relatively uncommon. For example, if\n1% of 1MB blocks was archival spends, our hypothetical year long TXO commitment\ndelay is only a few hundred MB of data with low-IO-performance requirements.\n\n\n## Security Model\n\nOf course, a TXO commitment delay of a year sounds ridiculous. Even the slowest\nimaginable computer isn't going to need more than a few blocks of TXO\ncommitment delay to keep up ~100% of the time, and there's no reason why we\ncan't have the UTXO archive delay be significantly longer than the TXO\ncommitment delay.\n\nHowever, as with UTXO commitments, TXO commitments raise issues with Bitcoin's\nsecurity model by allowing relatively miners to profitably mine transactions\nwithout bothering to validate prior history. At the extreme, if there was no\ncommitment delay at all at the cost of a bit of some extra network bandwidth\n\"full\" nodes could operate and even mine blocks completely statelessly by\nexpecting all transactions to include \"proof\" that their inputs are unspent; a\nTXO commitment proof for a commitment you haven't verified isn't a proof that a\ntransaction output is unspent, it's a proof that some miners claimed the txout\nwas unspent.\n\nAt one extreme, we could simply implement TXO commitments in a \"virtual\"\nfashion, without miners actually including the TXO commitment digest in their\nblocks at all. Full nodes would be forced to compute the commitment from\nscratch, in the same way they are forced to compute the UTXO state, or total\nwork. Of course a full node operator who doesn't want to verify old history can\nget a copy of the TXO state from a trusted source - no different from how you\ncould get a copy of the UTXO set from a trusted source.\n\nA more pragmatic approach is to accept that people will do that anyway, and\ninstead assume that sufficiently old blocks are valid. But how old is\n\"sufficiently old\"? First of all, if your full node implementation comes \"from\nthe factory\" with a reasonably up-to-date minimum accepted total-work\nthreshold\u2071 - in other words it won't accept a chain with less than that amount\nof total work - it may be reasonable to assume any Sybil attacker with\nsufficient hashing power to make a forked chain meeting that threshold with,\nsay, six months worth of blocks has enough hashing power to threaten the main\nchain as well.\n\nThat leaves public attempts to falsify TXO commitments, done out in the open by\nthe majority of hashing power. In this circumstance the \"assumed valid\"\nthreshold determines how long the attack would have to go on before full nodes\nstart accepting the invalid chain, or at least, newly installed/recently reset\nfull nodes. The minimum age that we can \"assume valid\" is tradeoff between\npolitical/social/technical concerns; we probably want at least a few weeks to\nguarantee the defenders a chance to organise themselves.\n\nWith this in mind, a longer-than-technically-necessary TXO commitment delay\u02b2\nmay help ensure that full node software actually validates some minimum number\nof blocks out-of-the-box, without taking shortcuts. However this can be\nachieved in a wide variety of ways, such as the author's prev-block-proof\nproposal\u00b3, fraud proofs, or even a PoW with an inner loop dependent on\nblockchain data. Like UTXO commitments, TXO commitments are also potentially\nvery useful in reducing the need for SPV wallet software to trust third parties\nproviding them with transaction data.\n\ni) Checkpoints that reject any chain without a specific block are a more\n   common, if uglier, way of achieving this protection.\n\nj) A good homework problem is to figure out how the TXO commitment could be\n   designed such that the delay could be reduced in a soft-fork.\n\n\n## Further Work\n\nWhile we've shown that TXO commitments certainly could be implemented without\nincreasing peak IO bandwidth/block validation latency significantly with the\ndelayed commitment approach, we're far from being certain that they should be\nimplemented this way (or at all).\n\n1) Can a TXO commitment scheme be optimized sufficiently to be used directly\nwithout a commitment delay? Obviously it'd be preferable to avoid all the above\ncomplexity entirely.\n\n2) Is it possible to use a metric other than age, e.g. priority? While this\ncomplicates the pruning logic, it could use the UTXO set space more\nefficiently, especially if your goal is to prioritise bitcoin value-transfer\nover other uses (though if \"normal\" wallets nearly never need to use TXO\ncommitments proofs to spend outputs, the infrastructure to actually do this may\nrot).\n\n3) Should UTXO archiving be based on a fixed size UTXO set, rather than an\nage/priority/etc. threshold?\n\n4) By fixing the problem (or possibly just \"fixing\" the problem) are we\nencouraging/legitimising blockchain use-cases other than BTC value transfer?\nShould we?\n\n5) Instead of TXO commitment proofs counting towards the blocksize limit, can\nwe use a different miner fairness/decentralization metric/incentive? For\ninstance it might be reasonable for the TXO commitment proof size to be\ndiscounted, or ignored entirely, if a proof-of-propagation scheme (e.g.\nthinblocks) is used to ensure all miners have received the proof in advance.\n\n6) How does this interact with fraud proofs? Obviously furthering dependency on\nnon-cryptographically-committed STXO/UTXO databases is incompatible with the\nmodularized validation approach to implementing fraud proofs.\n\n\n# References\n\n1) \"Merkle Mountain Ranges\",\n   Peter Todd, OpenTimestamps, Mar 18 2013,\n   https://github.com/opentimestamps/opentimestamps-server/blob/master/doc/merkle-mountain-range.md\n\n2) \"Do we really need a mempool? (for relay nodes)\",\n   Peter Todd, bitcoin-dev mailing list, Jul 18th 2015,\n   https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-July/009479.html\n\n3) \"Segregated witnesses and validationless mining\",\n   Peter Todd, bitcoin-dev mailing list, Dec 23rd 2015,\n   https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012103.html\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160517/33f69665/attachment-0001.sig>"
            },
            {
                "author": "Jameson Lopp",
                "date": "2016-05-17T14:03:22",
                "message_text_only": "Great post, Peter.\n\n4) By fixing the problem (or possibly just \"fixing\" the problem) are\nwe encouraging/legitimising blockchain use-cases other than BTC value\ntransfer? Should we?\n\nI don't think it would encourage non-value-transfer usage more\nbecause, as you noted, many such use cases are valuable enough that\npeople are willing to pay much higher transaction fees in order to\nhave their data timestamped. I think it's more an issue of the block\nspace / transaction fee market since the cost of making a transaction\nis directly borne by users, as opposed to the cost of the UTXO set\nwhich may not be borne by them if they don't run a full node.\n\nI'm of the opinion that if the world decides that Bitcoin is more\nvaluable as a trustworthy generalized timestamping mechanism than as a\nvalue transfer system, protocol developers shouldn't try to steer the\nship against the wind. As more people and use cases enter the\necosystem, the most valuable ones ought to survive - I hope that this\nmarket will be fostered by the developers.\n\n- Jameson\n\n\nOn Tue, May 17, 2016 at 9:23 AM, Peter Todd via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> # Motivation\n>\n> UTXO growth is a serious concern for Bitcoin's long-term decentralization.\n> To\n> run a competitive mining operation potentially the entire UTXO set must be\n> in\n> RAM to achieve competitive latency; your larger, more centralized,\n> competitors\n> will have the UTXO set in RAM. Mining is a zero-sum game, so the extra\n> latency\n> of not doing so if they do directly impacts your profit margin. Secondly,\n> having possession of the UTXO set is one of the minimum requirements to\n> run a\n> full node; the larger the set the harder it is to run a full node.\n>\n> Currently the maximum size of the UTXO set is unbounded as there is no\n> consensus rule that limits growth, other than the block-size limit itself;\n> as\n> of writing the UTXO set is 1.3GB in the on-disk, compressed serialization,\n> which expands to significantly more in memory. UTXO growth is driven by a\n> number of factors, including the fact that there is little incentive to\n> merge\n> inputs, lost coins, dust outputs that can't be economically spent, and\n> non-btc-value-transfer \"blockchain\" use-cases such as anti-replay oracles\n> and\n> timestamping.\n>\n> We don't have good tools to combat UTXO growth. Segregated Witness\n> proposes to\n> give witness space a 75% discount, in part of make reducing the UTXO set\n> size\n> by spending txouts cheaper. While this may change wallets to more often\n> spend\n> dust, it's hard to imagine an incentive sufficiently strong to discourage\n> most,\n> let alone all, UTXO growing behavior.\n>\n> For example, timestamping applications often create unspendable outputs\n> due to\n> ease of implementation, and because doing so is an easy way to make sure\n> that\n> the data required to reconstruct the timestamp proof won't get lost - all\n> Bitcoin full nodes are forced to keep a copy of it. Similarly anti-replay\n> use-cases like using the UTXO set for key rotation piggyback on the\n> uniquely\n> strong security and decentralization guarantee that Bitcoin provides; it's\n> very\n> difficult - perhaps impossible - to provide these applications with\n> alternatives that are equally secure. These non-btc-value-transfer\n> use-cases\n> can often afford to pay far higher fees per UTXO created than competing\n> btc-value-transfer use-cases; many users could afford to spend $50 to\n> register\n> a new PGP key, yet would rather not spend $50 in fees to create a standard\n> two\n> output transaction. Effective techniques to resist miner censorship exist,\n> so\n> without resorting to whitelists blocking non-btc-value-transfer use-cases\n> as\n> \"spam\" is not a long-term, incentive compatible, solution.\n>\n> A hard upper limit on UTXO set size could create a more level playing\n> field in\n> the form of fixed minimum requirements to run a performant Bitcoin node,\n> and\n> make the issue of UTXO \"spam\" less important. However, making any coins\n> unspendable, regardless of age or value, is a politically untenable\n> economic\n> change.\n>\n>\n> # TXO Commitments\n>\n> A merkle tree committing to the state of all transaction outputs, both\n> spent\n> and unspent, we can provide a method of compactly proving the current\n> state of\n> an output. This lets us \"archive\" less frequently accessed parts of the\n> UTXO\n> set, allowing full nodes to discard the associated data, still providing a\n> mechanism to spend those archived outputs by proving to those nodes that\n> the\n> outputs are in fact unspent.\n>\n> Specifically TXO commitments proposes a Merkle Mountain Range\u00b9 (MMR), a\n> type of deterministic, indexable, insertion ordered merkle tree, which\n> allows\n> new items to be cheaply appended to the tree with minimal storage\n> requirements,\n> just log2(n) \"mountain tips\". Once an output is added to the TXO MMR it is\n> never removed; if an output is spent its status is updated in place. Both\n> the\n> state of a specific item in the MMR, as well the validity of changes to\n> items\n> in the MMR, can be proven with log2(n) sized proofs consisting of a merkle\n> path\n> to the tip of the tree.\n>\n> At an extreme, with TXO commitments we could even have no UTXO set at all,\n> entirely eliminating the UTXO growth problem. Transactions would simply be\n> accompanied by TXO commitment proofs showing that the outputs they wanted\n> to\n> spend were still unspent; nodes could update the state of the TXO MMR\n> purely\n> from TXO commitment proofs. However, the log2(n) bandwidth overhead per\n> txin is\n> substantial, so a more realistic implementation is be to have a UTXO cache\n> for\n> recent transactions, with TXO commitments acting as a alternate for the\n> (rare)\n> event that an old txout needs to be spent.\n>\n> Proofs can be generated and added to transactions without the involvement\n> of\n> the signers, even after the fact; there's no need for the proof itself to\n> signed and the proof is not part of the transaction hash. Anyone with\n> access to\n> TXO MMR data can (re)generate missing proofs, so minimal, if any, changes\n> are\n> required to wallet software to make use of TXO commitments.\n>\n>\n> ## Delayed Commitments\n>\n> TXO commitments aren't a new idea - the author proposed them years ago in\n> response to UTXO commitments. However it's critical for small miners'\n> orphan\n> rates that block validation be fast, and so far it has proven difficult to\n> create (U)TXO implementations with acceptable performance; updating and\n> recalculating cryptographicly hashed merkelized datasets is inherently more\n> work than not doing so. Fortunately if we maintain a UTXO set for recent\n> outputs, TXO commitments are only needed when spending old, archived,\n> outputs.\n> We can take advantage of this by delaying the commitment, allowing it to be\n> calculated well in advance of it actually being used, thus changing a\n> latency-critical task into a much easier average throughput problem.\n>\n> Concretely each block B_i commits to the TXO set state as of block\n> B_{i-n}, in\n> other words what the TXO commitment would have been n blocks ago, if not\n> for\n> the n block delay. Since that commitment only depends on the contents of\n> the\n> blockchain up until block B_{i-n}, the contents of any block after are\n> irrelevant to the calculation.\n>\n>\n> ## Implementation\n>\n> Our proposed high-performance/low-latency delayed commitment full-node\n> implementation needs to store the following data:\n>\n> 1) UTXO set\n>\n>     Low-latency K:V map of txouts definitely known to be unspent. Similar\n> to\n>     existing UTXO implementation, but with the key difference that old,\n>     unspent, outputs may be pruned from the UTXO set.\n>\n>\n> 2) STXO set\n>\n>     Low-latency set of transaction outputs known to have been spent by\n>     transactions after the most recent TXO commitment, but created prior\n> to the\n>     TXO commitment.\n>\n>\n> 3) TXO journal\n>\n>     FIFO of outputs that need to be marked as spent in the TXO MMR. Appends\n>     must be low-latency; removals can be high-latency.\n>\n>\n> 4) TXO MMR list\n>\n>     Prunable, ordered list of TXO MMR's, mainly the highest pending\n> commitment,\n>     backed by a reference counted, cryptographically hashed object store\n>     indexed by digest (similar to how git repos work). High-latency ok.\n> We'll\n>     cover this in more in detail later.\n>\n>\n> ### Fast-Path: Verifying a Txout Spend In a Block\n>\n> When a transaction output is spent by a transaction in a block we have two\n> cases:\n>\n> 1) Recently created output\n>\n>     Output created after the most recent TXO commitment, so it should be\n> in the\n>     UTXO set; the transaction spending it does not need a TXO commitment\n> proof.\n>     Remove the output from the UTXO set and append it to the TXO journal.\n>\n> 2) Archived output\n>\n>     Output created prior to the most recent TXO commitment, so there's no\n>     guarantee it's in the UTXO set; transaction will have a TXO commitment\n>     proof for the most recent TXO commitment showing that it was unspent.\n>     Check that the output isn't already in the STXO set (double-spent),\n> and if\n>     not add it. Append the output and TXO commitment proof to the TXO\n> journal.\n>\n> In both cases recording an output as spent requires no more than two\n> key:value\n> updates, and one journal append. The existing UTXO set requires one\n> key:value\n> update per spend, so we can expect new block validation latency to be\n> within 2x\n> of the status quo even in the worst case of 100% archived output spends.\n>\n>\n> ### Slow-Path: Calculating Pending TXO Commitments\n>\n> In a low-priority background task we flush the TXO journal, recording the\n> outputs spent by each block in the TXO MMR, and hashing MMR data to obtain\n> the\n> TXO commitment digest. Additionally this background task removes STXO's\n> that\n> have been recorded in TXO commitments, and prunes TXO commitment data no\n> longer\n> needed.\n>\n> Throughput for the TXO commitment calculation will be worse than the\n> existing\n> UTXO only scheme. This impacts bulk verification, e.g. initial block\n> download.\n> That said, TXO commitments provides other possible tradeoffs that can\n> mitigate\n> impact of slower validation throughput, such as skipping validation of old\n> history, as well as fraud proof approaches.\n>\n>\n> ### TXO MMR Implementation Details\n>\n> Each TXO MMR state is a modification of the previous one with most\n> information\n> shared, so we an space-efficiently store a large number of TXO commitments\n> states, where each state is a small delta of the previous state, by sharing\n> unchanged data between each state; cycles are impossible in merkelized data\n> structures, so simple reference counting is sufficient for garbage\n> collection.\n> Data no longer needed can be pruned by dropping it from the database, and\n> unpruned by adding it again. Since everything is committed to via\n> cryptographic\n> hash, we're guaranteed that regardless of where we get the data, after\n> unpruning we'll have the right data.\n>\n> Let's look at how the TXO MMR works in detail. Consider the following TXO\n> MMR\n> with two txouts, which we'll call state #0:\n>\n>       0\n>      / \\\n>     a   b\n>\n> If we add another entry we get state #1:\n>\n>         1\n>        / \\\n>       0   \\\n>      / \\   \\\n>     a   b   c\n>\n> Note how it 100% of the state #0 data was reused in commitment #1. Let's\n> add two more entries to get state #2:\n>\n>             2\n>            / \\\n>           2   \\\n>          / \\   \\\n>         /   \\   \\\n>        /     \\   \\\n>       0       2   \\\n>      / \\     / \\   \\\n>     a   b   c   d   e\n>\n> This time part of state #1 wasn't reused - it's wasn't a perfect binary\n> tree - but we've still got a lot of re-use.\n>\n> Now suppose state #2 is committed into the blockchain by the most recent\n> block.\n> Future transactions attempting to spend outputs created as of state #2 are\n> obliged to prove that they are unspent; essentially they're forced to\n> provide\n> part of the state #2 MMR data. This lets us prune that data, discarding it,\n> leaving us with only the bare minimum data we need to append new txouts to\n> the\n> TXO MMR, the tips of the perfect binary trees (\"mountains\") within the MMR:\n>\n>             2\n>            / \\\n>           2   \\\n>                \\\n>                 \\\n>                  \\\n>                   \\\n>                    \\\n>                     e\n>\n> Note that we're glossing over some nuance here about exactly what data\n> needs to\n> be kept; depending on the details of the implementation the only data we\n> need\n> for nodes \"2\" and \"e\" may be their hash digest.\n>\n> Adding another three more txouts results in state #3:\n>\n>                   3\n>                  / \\\n>                 /   \\\n>                /     \\\n>               /       \\\n>              /         \\\n>             /           \\\n>            /             \\\n>           2               3\n>                          / \\\n>                         /   \\\n>                        /     \\\n>                       3       3\n>                      / \\     / \\\n>                     e   f   g   h\n>\n> Suppose recently created txout f is spent. We have all the data required to\n> update the MMR, giving us state #4. It modifies two inner nodes and one\n> leaf\n> node:\n>\n>                   4\n>                  / \\\n>                 /   \\\n>                /     \\\n>               /       \\\n>              /         \\\n>             /           \\\n>            /             \\\n>           2               4\n>                          / \\\n>                         /   \\\n>                        /     \\\n>                       4       3\n>                      / \\     / \\\n>                     e  (f)  g   h\n>\n> If an archived txout is spent requires the transaction to provide the\n> merkle\n> path to the most recently committed TXO, in our case state #2. If txout b\n> is\n> spent that means the transaction must provide the following data from\n> state #2:\n>\n>             2\n>            /\n>           2\n>          /\n>         /\n>        /\n>       0\n>        \\\n>         b\n>\n> We can add that data to our local knowledge of the TXO MMR, unpruning part\n> of\n> it:\n>\n>                   4\n>                  / \\\n>                 /   \\\n>                /     \\\n>               /       \\\n>              /         \\\n>             /           \\\n>            /             \\\n>           2               4\n>          /               / \\\n>         /               /   \\\n>        /               /     \\\n>       0               4       3\n>        \\             / \\     / \\\n>         b           e  (f)  g   h\n>\n> Remember, we haven't _modified_ state #4 yet; we just have more data about\n> it.\n> When we mark txout b as spent we get state #5:\n>\n>                   5\n>                  / \\\n>                 /   \\\n>                /     \\\n>               /       \\\n>              /         \\\n>             /           \\\n>            /             \\\n>           5               4\n>          /               / \\\n>         /               /   \\\n>        /               /     \\\n>       5               4       3\n>        \\             / \\     / \\\n>        (b)          e  (f)  g   h\n>\n> Secondly by now state #3 has been committed into the chain, and\n> transactions\n> that want to spend txouts created as of state #3 must provide a TXO proof\n> consisting of state #3 data. The leaf nodes for outputs g and h, and the\n> inner\n> node above them, are part of state #3, so we prune them:\n>\n>                   5\n>                  / \\\n>                 /   \\\n>                /     \\\n>               /       \\\n>              /         \\\n>             /           \\\n>            /             \\\n>           5               4\n>          /               /\n>         /               /\n>        /               /\n>       5               4\n>        \\             / \\\n>        (b)          e  (f)\n>\n> Finally, lets put this all together, by spending txouts a, c, and g, and\n> creating three new txouts i, j, and k. State #3 was the most recently\n> committed\n> state, so the transactions spending a and g are providing merkle paths up\n> to\n> it. This includes part of the state #2 data:\n>\n>                   3\n>                  / \\\n>                 /   \\\n>                /     \\\n>               /       \\\n>              /         \\\n>             /           \\\n>            /             \\\n>           2               3\n>          / \\               \\\n>         /   \\               \\\n>        /     \\               \\\n>       0       2               3\n>      /       /               /\n>     a       c               g\n>\n> After unpruning we have the following data for state #5:\n>\n>                   5\n>                  / \\\n>                 /   \\\n>                /     \\\n>               /       \\\n>              /         \\\n>             /           \\\n>            /             \\\n>           5               4\n>          / \\             / \\\n>         /   \\           /   \\\n>        /     \\         /     \\\n>       5       2       4       3\n>      / \\     /       / \\     /\n>     a  (b)  c       e  (f)  g\n>\n> That's sufficient to mark the three outputs as spent and add the three new\n> txouts, resulting in state #6:\n>\n>                         6\n>                        / \\\n>                       /   \\\n>                      /     \\\n>                     /       \\\n>                    /         \\\n>                   6           \\\n>                  / \\           \\\n>                 /   \\           \\\n>                /     \\           \\\n>               /       \\           \\\n>              /         \\           \\\n>             /           \\           \\\n>            /             \\           \\\n>           6               6           \\\n>          / \\             / \\           \\\n>         /   \\           /   \\           6\n>        /     \\         /     \\         / \\\n>       6       6       4       6       6   \\\n>      / \\     /       / \\     /       / \\   \\\n>    (a) (b) (c)      e  (f) (g)      i   j   k\n>\n> Again, state #4 related data can be pruned. In addition, depending on how\n> the\n> STXO set is implemented may also be able to prune data related to spent\n> txouts\n> after that state, including inner nodes where all txouts under them have\n> been\n> spent (more on pruning spent inner nodes later).\n>\n>\n> ### Consensus and Pruning\n>\n> It's important to note that pruning behavior is consensus critical: a full\n> node\n> that is missing data due to pruning it too soon will fall out of\n> consensus, and\n> a miner that fails to include a merkle proof that is required by the\n> consensus\n> is creating an invalid block. At the same time many full nodes will have\n> significantly more data on hand than the bare minimum so they can help\n> wallets\n> make transactions spending old coins; implementations should strongly\n> consider\n> separating the data that is, and isn't, strictly required for consensus.\n>\n> A reasonable approach for the low-level cryptography may be to actually\n> treat\n> the two cases differently, with the TXO commitments committing too what\n> data\n> does and does not need to be kept on hand by the UTXO expiration rules. On\n> the\n> other hand, leaving that uncommitted allows for certain types of soft-forks\n> where the protocol is changed to require more data than it previously did.\n>\n>\n> ### Consensus Critical Storage Overheads\n>\n> Only the UTXO and STXO sets need to be kept on fast random access storage.\n> Since STXO set entries can only be created by spending a UTXO - and are\n> smaller\n> than a UTXO entry - we can guarantee that the peak size of the UTXO and\n> STXO\n> sets combined will always be less than the peak size of the UTXO set alone\n> in\n> the existing UTXO-only scheme (though the combined size can be temporarily\n> higher than what the UTXO set size alone would be when large numbers of\n> archived txouts are spent).\n>\n> TXO journal entries and unpruned entries in the TXO MMR have log2(n)\n> maximum\n> overhead per entry: a unique merkle path to a TXO commitment (by \"unique\"\n> we\n> mean that no other entry shares data with it). On a reasonably fast system\n> the\n> TXO journal will be flushed quickly, converting it into TXO MMR data; the\n> TXO\n> journal will never be more than a few blocks in size.\n>\n> Transactions spending non-archived txouts are not required to provide any\n> TXO\n> commitment data; we must have that data on hand in the form of one TXO MMR\n> entry per UTXO. Once spent however the TXO MMR leaf node associated with\n> that\n> non-archived txout can be immediately pruned - it's no longer in the UTXO\n> set\n> so any attempt to spend it will fail; the data is now immutable and we'll\n> never\n> need it again. Inner nodes in the TXO MMR can also be pruned if all leafs\n> under\n> them are fully spent; detecting this is easy the TXO MMR is a merkle-sum\n> tree,\n> with each inner node committing to the sum of the unspent txouts under it.\n>\n> When a archived txout is spent the transaction is required to provide a\n> merkle\n> path to the most recent TXO commitment. As shown above that path is\n> sufficient\n> information to unprune the necessary nodes in the TXO MMR and apply the\n> spend\n> immediately, reducing this case to the TXO journal size question\n> (non-consensus\n> critical overhead is a different question, which we'll address in the next\n> section).\n>\n> Taking all this into account the only significant storage overhead of our\n> TXO\n> commitments scheme when compared to the status quo is the log2(n) merkle\n> path\n> overhead; as long as less than 1/log2(n) of the UTXO set is active,\n> non-archived, UTXO's we've come out ahead, even in the unrealistic case\n> where\n> all storage available is equally fast. In the real world that isn't yet the\n> case - even SSD's significantly slower than RAM.\n>\n>\n> ### Non-Consensus Critical Storage Overheads\n>\n> Transactions spending archived txouts pose two challenges:\n>\n> 1) Obtaining up-to-date TXO commitment proofs\n>\n> 2) Updating those proofs as blocks are mined\n>\n> The first challenge can be handled by specialized archival nodes, not\n> unlike\n> how some nodes make transaction data available to wallets via bloom\n> filters or\n> the Electrum protocol. There's a whole variety of options available, and\n> the\n> the data can be easily sharded to scale horizontally; the data is\n> self-validating allowing horizontal scaling without trust.\n>\n> While miners and relay nodes don't need to be concerned about the initial\n> commitment proof, updating that proof is another matter. If a node\n> aggressively\n> prunes old versions of the TXO MMR as it calculates pending TXO\n> commitments, it\n> won't have the data available to update the TXO commitment proof to be\n> against\n> the next block, when that block is found; the child nodes of the TXO MMR\n> tip\n> are guaranteed to have changed, yet aggressive pruning would have\n> discarded that\n> data.\n>\n> Relay nodes could ignore this problem if they simply accept the fact that\n> they'll only be able to fully relay the transaction once, when it is\n> initially\n> broadcast, and won't be able to provide mempool functionality after the\n> initial\n> relay. Modulo high-latency mixnets, this is probably acceptable; the\n> author has\n> previously argued that relay nodes don't need a mempool\u00b2 at all.\n>\n> For a miner though not having the data necessary to update the proofs as\n> blocks\n> are found means potentially losing out on transactions fees. So how much\n> extra\n> data is necessary to make this a non-issue?\n>\n> Since the TXO MMR is insertion ordered, spending a non-archived txout can\n> only\n> invalidate the upper nodes in of the archived txout's TXO MMR proof (if\n> this\n> isn't clear, imagine a two-level scheme, with a per-block TXO MMRs,\n> committed\n> by a master MMR for all blocks). The maximum number of relevant inner nodes\n> changed is log2(n) per block, so if there are n non-archival blocks\n> between the\n> most recent TXO commitment and the pending TXO MMR tip, we have to store\n> log2(n)*n inner nodes - on the order of a few dozen MB even when n is a\n> (seemingly ridiculously high) year worth of blocks.\n>\n> Archived txout spends on the other hand can invalidate TXO MMR proofs at\n> any\n> level - consider the case of two adjacent txouts being spent. To guarantee\n> success requires storing full proofs. However, they're limited by the\n> blocksize\n> limit, and additionally are expected to be relatively uncommon. For\n> example, if\n> 1% of 1MB blocks was archival spends, our hypothetical year long TXO\n> commitment\n> delay is only a few hundred MB of data with low-IO-performance\n> requirements.\n>\n>\n> ## Security Model\n>\n> Of course, a TXO commitment delay of a year sounds ridiculous. Even the\n> slowest\n> imaginable computer isn't going to need more than a few blocks of TXO\n> commitment delay to keep up ~100% of the time, and there's no reason why we\n> can't have the UTXO archive delay be significantly longer than the TXO\n> commitment delay.\n>\n> However, as with UTXO commitments, TXO commitments raise issues with\n> Bitcoin's\n> security model by allowing relatively miners to profitably mine\n> transactions\n> without bothering to validate prior history. At the extreme, if there was\n> no\n> commitment delay at all at the cost of a bit of some extra network\n> bandwidth\n> \"full\" nodes could operate and even mine blocks completely statelessly by\n> expecting all transactions to include \"proof\" that their inputs are\n> unspent; a\n> TXO commitment proof for a commitment you haven't verified isn't a proof\n> that a\n> transaction output is unspent, it's a proof that some miners claimed the\n> txout\n> was unspent.\n>\n> At one extreme, we could simply implement TXO commitments in a \"virtual\"\n> fashion, without miners actually including the TXO commitment digest in\n> their\n> blocks at all. Full nodes would be forced to compute the commitment from\n> scratch, in the same way they are forced to compute the UTXO state, or\n> total\n> work. Of course a full node operator who doesn't want to verify old\n> history can\n> get a copy of the TXO state from a trusted source - no different from how\n> you\n> could get a copy of the UTXO set from a trusted source.\n>\n> A more pragmatic approach is to accept that people will do that anyway, and\n> instead assume that sufficiently old blocks are valid. But how old is\n> \"sufficiently old\"? First of all, if your full node implementation comes\n> \"from\n> the factory\" with a reasonably up-to-date minimum accepted total-work\n> threshold\u2071 - in other words it won't accept a chain with less than that\n> amount\n> of total work - it may be reasonable to assume any Sybil attacker with\n> sufficient hashing power to make a forked chain meeting that threshold\n> with,\n> say, six months worth of blocks has enough hashing power to threaten the\n> main\n> chain as well.\n>\n> That leaves public attempts to falsify TXO commitments, done out in the\n> open by\n> the majority of hashing power. In this circumstance the \"assumed valid\"\n> threshold determines how long the attack would have to go on before full\n> nodes\n> start accepting the invalid chain, or at least, newly installed/recently\n> reset\n> full nodes. The minimum age that we can \"assume valid\" is tradeoff between\n> political/social/technical concerns; we probably want at least a few weeks\n> to\n> guarantee the defenders a chance to organise themselves.\n>\n> With this in mind, a longer-than-technically-necessary TXO commitment\n> delay\u02b2\n> may help ensure that full node software actually validates some minimum\n> number\n> of blocks out-of-the-box, without taking shortcuts. However this can be\n> achieved in a wide variety of ways, such as the author's prev-block-proof\n> proposal\u00b3, fraud proofs, or even a PoW with an inner loop dependent on\n> blockchain data. Like UTXO commitments, TXO commitments are also\n> potentially\n> very useful in reducing the need for SPV wallet software to trust third\n> parties\n> providing them with transaction data.\n>\n> i) Checkpoints that reject any chain without a specific block are a more\n>    common, if uglier, way of achieving this protection.\n>\n> j) A good homework problem is to figure out how the TXO commitment could be\n>    designed such that the delay could be reduced in a soft-fork.\n>\n>\n> ## Further Work\n>\n> While we've shown that TXO commitments certainly could be implemented\n> without\n> increasing peak IO bandwidth/block validation latency significantly with\n> the\n> delayed commitment approach, we're far from being certain that they should\n> be\n> implemented this way (or at all).\n>\n> 1) Can a TXO commitment scheme be optimized sufficiently to be used\n> directly\n> without a commitment delay? Obviously it'd be preferable to avoid all the\n> above\n> complexity entirely.\n>\n> 2) Is it possible to use a metric other than age, e.g. priority? While this\n> complicates the pruning logic, it could use the UTXO set space more\n> efficiently, especially if your goal is to prioritise bitcoin\n> value-transfer\n> over other uses (though if \"normal\" wallets nearly never need to use TXO\n> commitments proofs to spend outputs, the infrastructure to actually do\n> this may\n> rot).\n>\n> 3) Should UTXO archiving be based on a fixed size UTXO set, rather than an\n> age/priority/etc. threshold?\n>\n> 4) By fixing the problem (or possibly just \"fixing\" the problem) are we\n> encouraging/legitimising blockchain use-cases other than BTC value\n> transfer?\n> Should we?\n>\n> 5) Instead of TXO commitment proofs counting towards the blocksize limit,\n> can\n> we use a different miner fairness/decentralization metric/incentive? For\n> instance it might be reasonable for the TXO commitment proof size to be\n> discounted, or ignored entirely, if a proof-of-propagation scheme (e.g.\n> thinblocks) is used to ensure all miners have received the proof in\n> advance.\n>\n> 6) How does this interact with fraud proofs? Obviously furthering\n> dependency on\n> non-cryptographically-committed STXO/UTXO databases is incompatible with\n> the\n> modularized validation approach to implementing fraud proofs.\n>\n>\n> # References\n>\n> 1) \"Merkle Mountain Ranges\",\n>    Peter Todd, OpenTimestamps, Mar 18 2013,\n>\n> https://github.com/opentimestamps/opentimestamps-server/blob/master/doc/merkle-mountain-range.md\n>\n> 2) \"Do we really need a mempool? (for relay nodes)\",\n>    Peter Todd, bitcoin-dev mailing list, Jul 18th 2015,\n>\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-July/009479.html\n>\n> 3) \"Segregated witnesses and validationless mining\",\n>    Peter Todd, bitcoin-dev mailing list, Dec 23rd 2015,\n>\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012103.html\n>\n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160517/b3c7219f/attachment-0001.html>"
            },
            {
                "author": "Eric Lombrozo",
                "date": "2016-05-17T14:25:22",
                "message_text_only": "Nice!\n\nWe\u2019ve been talking about doing this forever and it\u2019s so desperately needed.\n\n> On May 17, 2016, at 3:23 PM, Peter Todd via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> # Motivation\n> \n> UTXO growth is a serious concern for Bitcoin's long-term decentralization. To\n> run a competitive mining operation potentially the entire UTXO set must be in\n> RAM to achieve competitive latency; your larger, more centralized, competitors\n> will have the UTXO set in RAM. Mining is a zero-sum game, so the extra latency\n> of not doing so if they do directly impacts your profit margin. Secondly,\n> having possession of the UTXO set is one of the minimum requirements to run a\n> full node; the larger the set the harder it is to run a full node.\n> \n> Currently the maximum size of the UTXO set is unbounded as there is no\n> consensus rule that limits growth, other than the block-size limit itself; as\n> of writing the UTXO set is 1.3GB in the on-disk, compressed serialization,\n> which expands to significantly more in memory. UTXO growth is driven by a\n> number of factors, including the fact that there is little incentive to merge\n> inputs, lost coins, dust outputs that can't be economically spent, and\n> non-btc-value-transfer \"blockchain\" use-cases such as anti-replay oracles and\n> timestamping.\n> \n> We don't have good tools to combat UTXO growth. Segregated Witness proposes to\n> give witness space a 75% discount, in part of make reducing the UTXO set size\n> by spending txouts cheaper. While this may change wallets to more often spend\n> dust, it's hard to imagine an incentive sufficiently strong to discourage most,\n> let alone all, UTXO growing behavior.\n> \n> For example, timestamping applications often create unspendable outputs due to\n> ease of implementation, and because doing so is an easy way to make sure that\n> the data required to reconstruct the timestamp proof won't get lost - all\n> Bitcoin full nodes are forced to keep a copy of it. Similarly anti-replay\n> use-cases like using the UTXO set for key rotation piggyback on the uniquely\n> strong security and decentralization guarantee that Bitcoin provides; it's very\n> difficult - perhaps impossible - to provide these applications with\n> alternatives that are equally secure. These non-btc-value-transfer use-cases\n> can often afford to pay far higher fees per UTXO created than competing\n> btc-value-transfer use-cases; many users could afford to spend $50 to register\n> a new PGP key, yet would rather not spend $50 in fees to create a standard two\n> output transaction. Effective techniques to resist miner censorship exist, so\n> without resorting to whitelists blocking non-btc-value-transfer use-cases as\n> \"spam\" is not a long-term, incentive compatible, solution.\n> \n> A hard upper limit on UTXO set size could create a more level playing field in\n> the form of fixed minimum requirements to run a performant Bitcoin node, and\n> make the issue of UTXO \"spam\" less important. However, making any coins\n> unspendable, regardless of age or value, is a politically untenable economic\n> change.\n> \n> \n> # TXO Commitments\n> \n> A merkle tree committing to the state of all transaction outputs, both spent\n> and unspent, we can provide a method of compactly proving the current state of\n> an output. This lets us \"archive\" less frequently accessed parts of the UTXO\n> set, allowing full nodes to discard the associated data, still providing a\n> mechanism to spend those archived outputs by proving to those nodes that the\n> outputs are in fact unspent.\n> \n> Specifically TXO commitments proposes a Merkle Mountain Range\u00b9 (MMR), a\n> type of deterministic, indexable, insertion ordered merkle tree, which allows\n> new items to be cheaply appended to the tree with minimal storage requirements,\n> just log2(n) \"mountain tips\". Once an output is added to the TXO MMR it is\n> never removed; if an output is spent its status is updated in place. Both the\n> state of a specific item in the MMR, as well the validity of changes to items\n> in the MMR, can be proven with log2(n) sized proofs consisting of a merkle path\n> to the tip of the tree.\n> \n> At an extreme, with TXO commitments we could even have no UTXO set at all,\n> entirely eliminating the UTXO growth problem. Transactions would simply be\n> accompanied by TXO commitment proofs showing that the outputs they wanted to\n> spend were still unspent; nodes could update the state of the TXO MMR purely\n> from TXO commitment proofs. However, the log2(n) bandwidth overhead per txin is\n> substantial, so a more realistic implementation is be to have a UTXO cache for\n> recent transactions, with TXO commitments acting as a alternate for the (rare)\n> event that an old txout needs to be spent.\n> \n> Proofs can be generated and added to transactions without the involvement of\n> the signers, even after the fact; there's no need for the proof itself to\n> signed and the proof is not part of the transaction hash. Anyone with access to\n> TXO MMR data can (re)generate missing proofs, so minimal, if any, changes are\n> required to wallet software to make use of TXO commitments.\n> \n> \n> ## Delayed Commitments\n> \n> TXO commitments aren't a new idea - the author proposed them years ago in\n> response to UTXO commitments. However it's critical for small miners' orphan\n> rates that block validation be fast, and so far it has proven difficult to\n> create (U)TXO implementations with acceptable performance; updating and\n> recalculating cryptographicly hashed merkelized datasets is inherently more\n> work than not doing so. Fortunately if we maintain a UTXO set for recent\n> outputs, TXO commitments are only needed when spending old, archived, outputs.\n> We can take advantage of this by delaying the commitment, allowing it to be\n> calculated well in advance of it actually being used, thus changing a\n> latency-critical task into a much easier average throughput problem.\n> \n> Concretely each block B_i commits to the TXO set state as of block B_{i-n}, in\n> other words what the TXO commitment would have been n blocks ago, if not for\n> the n block delay. Since that commitment only depends on the contents of the\n> blockchain up until block B_{i-n}, the contents of any block after are\n> irrelevant to the calculation.\n> \n> \n> ## Implementation\n> \n> Our proposed high-performance/low-latency delayed commitment full-node\n> implementation needs to store the following data:\n> \n> 1) UTXO set\n> \n>    Low-latency K:V map of txouts definitely known to be unspent. Similar to\n>    existing UTXO implementation, but with the key difference that old,\n>    unspent, outputs may be pruned from the UTXO set.\n> \n> \n> 2) STXO set\n> \n>    Low-latency set of transaction outputs known to have been spent by\n>    transactions after the most recent TXO commitment, but created prior to the\n>    TXO commitment.\n> \n> \n> 3) TXO journal\n> \n>    FIFO of outputs that need to be marked as spent in the TXO MMR. Appends\n>    must be low-latency; removals can be high-latency.\n> \n> \n> 4) TXO MMR list\n> \n>    Prunable, ordered list of TXO MMR's, mainly the highest pending commitment,\n>    backed by a reference counted, cryptographically hashed object store\n>    indexed by digest (similar to how git repos work). High-latency ok. We'll\n>    cover this in more in detail later.\n> \n> \n> ### Fast-Path: Verifying a Txout Spend In a Block\n> \n> When a transaction output is spent by a transaction in a block we have two\n> cases:\n> \n> 1) Recently created output\n> \n>    Output created after the most recent TXO commitment, so it should be in the\n>    UTXO set; the transaction spending it does not need a TXO commitment proof.\n>    Remove the output from the UTXO set and append it to the TXO journal.\n> \n> 2) Archived output\n> \n>    Output created prior to the most recent TXO commitment, so there's no\n>    guarantee it's in the UTXO set; transaction will have a TXO commitment\n>    proof for the most recent TXO commitment showing that it was unspent.\n>    Check that the output isn't already in the STXO set (double-spent), and if\n>    not add it. Append the output and TXO commitment proof to the TXO journal.\n> \n> In both cases recording an output as spent requires no more than two key:value\n> updates, and one journal append. The existing UTXO set requires one key:value\n> update per spend, so we can expect new block validation latency to be within 2x\n> of the status quo even in the worst case of 100% archived output spends.\n> \n> \n> ### Slow-Path: Calculating Pending TXO Commitments\n> \n> In a low-priority background task we flush the TXO journal, recording the\n> outputs spent by each block in the TXO MMR, and hashing MMR data to obtain the\n> TXO commitment digest. Additionally this background task removes STXO's that\n> have been recorded in TXO commitments, and prunes TXO commitment data no longer\n> needed.\n> \n> Throughput for the TXO commitment calculation will be worse than the existing\n> UTXO only scheme. This impacts bulk verification, e.g. initial block download.\n> That said, TXO commitments provides other possible tradeoffs that can mitigate\n> impact of slower validation throughput, such as skipping validation of old\n> history, as well as fraud proof approaches.\n> \n> \n> ### TXO MMR Implementation Details\n> \n> Each TXO MMR state is a modification of the previous one with most information\n> shared, so we an space-efficiently store a large number of TXO commitments\n> states, where each state is a small delta of the previous state, by sharing\n> unchanged data between each state; cycles are impossible in merkelized data\n> structures, so simple reference counting is sufficient for garbage collection.\n> Data no longer needed can be pruned by dropping it from the database, and\n> unpruned by adding it again. Since everything is committed to via cryptographic\n> hash, we're guaranteed that regardless of where we get the data, after\n> unpruning we'll have the right data.\n> \n> Let's look at how the TXO MMR works in detail. Consider the following TXO MMR\n> with two txouts, which we'll call state #0:\n> \n>      0\n>     / \\\n>    a   b\n> \n> If we add another entry we get state #1:\n> \n>        1\n>       / \\\n>      0   \\\n>     / \\   \\\n>    a   b   c\n> \n> Note how it 100% of the state #0 data was reused in commitment #1. Let's\n> add two more entries to get state #2:\n> \n>            2\n>           / \\\n>          2   \\\n>         / \\   \\\n>        /   \\   \\\n>       /     \\   \\\n>      0       2   \\\n>     / \\     / \\   \\\n>    a   b   c   d   e\n> \n> This time part of state #1 wasn't reused - it's wasn't a perfect binary\n> tree - but we've still got a lot of re-use.\n> \n> Now suppose state #2 is committed into the blockchain by the most recent block.\n> Future transactions attempting to spend outputs created as of state #2 are\n> obliged to prove that they are unspent; essentially they're forced to provide\n> part of the state #2 MMR data. This lets us prune that data, discarding it,\n> leaving us with only the bare minimum data we need to append new txouts to the\n> TXO MMR, the tips of the perfect binary trees (\"mountains\") within the MMR:\n> \n>            2\n>           / \\\n>          2   \\\n>               \\\n>                \\\n>                 \\\n>                  \\\n>                   \\\n>                    e\n> \n> Note that we're glossing over some nuance here about exactly what data needs to\n> be kept; depending on the details of the implementation the only data we need\n> for nodes \"2\" and \"e\" may be their hash digest.\n> \n> Adding another three more txouts results in state #3:\n> \n>                  3\n>                 / \\\n>                /   \\\n>               /     \\\n>              /       \\\n>             /         \\\n>            /           \\\n>           /             \\\n>          2               3\n>                         / \\\n>                        /   \\\n>                       /     \\\n>                      3       3\n>                     / \\     / \\\n>                    e   f   g   h\n> \n> Suppose recently created txout f is spent. We have all the data required to\n> update the MMR, giving us state #4. It modifies two inner nodes and one leaf\n> node:\n> \n>                  4\n>                 / \\\n>                /   \\\n>               /     \\\n>              /       \\\n>             /         \\\n>            /           \\\n>           /             \\\n>          2               4\n>                         / \\\n>                        /   \\\n>                       /     \\\n>                      4       3\n>                     / \\     / \\\n>                    e  (f)  g   h\n> \n> If an archived txout is spent requires the transaction to provide the merkle\n> path to the most recently committed TXO, in our case state #2. If txout b is\n> spent that means the transaction must provide the following data from state #2:\n> \n>            2\n>           /\n>          2\n>         /\n>        /\n>       /\n>      0\n>       \\\n>        b\n> \n> We can add that data to our local knowledge of the TXO MMR, unpruning part of\n> it:\n> \n>                  4\n>                 / \\\n>                /   \\\n>               /     \\\n>              /       \\\n>             /         \\\n>            /           \\\n>           /             \\\n>          2               4\n>         /               / \\\n>        /               /   \\\n>       /               /     \\\n>      0               4       3\n>       \\             / \\     / \\\n>        b           e  (f)  g   h\n> \n> Remember, we haven't _modified_ state #4 yet; we just have more data about it.\n> When we mark txout b as spent we get state #5:\n> \n>                  5\n>                 / \\\n>                /   \\\n>               /     \\\n>              /       \\\n>             /         \\\n>            /           \\\n>           /             \\\n>          5               4\n>         /               / \\\n>        /               /   \\\n>       /               /     \\\n>      5               4       3\n>       \\             / \\     / \\\n>       (b)          e  (f)  g   h\n> \n> Secondly by now state #3 has been committed into the chain, and transactions\n> that want to spend txouts created as of state #3 must provide a TXO proof\n> consisting of state #3 data. The leaf nodes for outputs g and h, and the inner\n> node above them, are part of state #3, so we prune them:\n> \n>                  5\n>                 / \\\n>                /   \\\n>               /     \\\n>              /       \\\n>             /         \\\n>            /           \\\n>           /             \\\n>          5               4\n>         /               /\n>        /               /\n>       /               /\n>      5               4\n>       \\             / \\\n>       (b)          e  (f)\n> \n> Finally, lets put this all together, by spending txouts a, c, and g, and\n> creating three new txouts i, j, and k. State #3 was the most recently committed\n> state, so the transactions spending a and g are providing merkle paths up to\n> it. This includes part of the state #2 data:\n> \n>                  3\n>                 / \\\n>                /   \\\n>               /     \\\n>              /       \\\n>             /         \\\n>            /           \\\n>           /             \\\n>          2               3\n>         / \\               \\\n>        /   \\               \\\n>       /     \\               \\\n>      0       2               3\n>     /       /               /\n>    a       c               g\n> \n> After unpruning we have the following data for state #5:\n> \n>                  5\n>                 / \\\n>                /   \\\n>               /     \\\n>              /       \\\n>             /         \\\n>            /           \\\n>           /             \\\n>          5               4\n>         / \\             / \\\n>        /   \\           /   \\\n>       /     \\         /     \\\n>      5       2       4       3\n>     / \\     /       / \\     /\n>    a  (b)  c       e  (f)  g\n> \n> That's sufficient to mark the three outputs as spent and add the three new\n> txouts, resulting in state #6:\n> \n>                        6\n>                       / \\\n>                      /   \\\n>                     /     \\\n>                    /       \\\n>                   /         \\\n>                  6           \\\n>                 / \\           \\\n>                /   \\           \\\n>               /     \\           \\\n>              /       \\           \\\n>             /         \\           \\\n>            /           \\           \\\n>           /             \\           \\\n>          6               6           \\\n>         / \\             / \\           \\\n>        /   \\           /   \\           6\n>       /     \\         /     \\         / \\\n>      6       6       4       6       6   \\\n>     / \\     /       / \\     /       / \\   \\\n>   (a) (b) (c)      e  (f) (g)      i   j   k\n> \n> Again, state #4 related data can be pruned. In addition, depending on how the\n> STXO set is implemented may also be able to prune data related to spent txouts\n> after that state, including inner nodes where all txouts under them have been\n> spent (more on pruning spent inner nodes later).\n> \n> \n> ### Consensus and Pruning\n> \n> It's important to note that pruning behavior is consensus critical: a full node\n> that is missing data due to pruning it too soon will fall out of consensus, and\n> a miner that fails to include a merkle proof that is required by the consensus\n> is creating an invalid block. At the same time many full nodes will have\n> significantly more data on hand than the bare minimum so they can help wallets\n> make transactions spending old coins; implementations should strongly consider\n> separating the data that is, and isn't, strictly required for consensus.\n> \n> A reasonable approach for the low-level cryptography may be to actually treat\n> the two cases differently, with the TXO commitments committing too what data\n> does and does not need to be kept on hand by the UTXO expiration rules. On the\n> other hand, leaving that uncommitted allows for certain types of soft-forks\n> where the protocol is changed to require more data than it previously did.\n> \n> \n> ### Consensus Critical Storage Overheads\n> \n> Only the UTXO and STXO sets need to be kept on fast random access storage.\n> Since STXO set entries can only be created by spending a UTXO - and are smaller\n> than a UTXO entry - we can guarantee that the peak size of the UTXO and STXO\n> sets combined will always be less than the peak size of the UTXO set alone in\n> the existing UTXO-only scheme (though the combined size can be temporarily\n> higher than what the UTXO set size alone would be when large numbers of\n> archived txouts are spent).\n> \n> TXO journal entries and unpruned entries in the TXO MMR have log2(n) maximum\n> overhead per entry: a unique merkle path to a TXO commitment (by \"unique\" we\n> mean that no other entry shares data with it). On a reasonably fast system the\n> TXO journal will be flushed quickly, converting it into TXO MMR data; the TXO\n> journal will never be more than a few blocks in size.\n> \n> Transactions spending non-archived txouts are not required to provide any TXO\n> commitment data; we must have that data on hand in the form of one TXO MMR\n> entry per UTXO. Once spent however the TXO MMR leaf node associated with that\n> non-archived txout can be immediately pruned - it's no longer in the UTXO set\n> so any attempt to spend it will fail; the data is now immutable and we'll never\n> need it again. Inner nodes in the TXO MMR can also be pruned if all leafs under\n> them are fully spent; detecting this is easy the TXO MMR is a merkle-sum tree,\n> with each inner node committing to the sum of the unspent txouts under it.\n> \n> When a archived txout is spent the transaction is required to provide a merkle\n> path to the most recent TXO commitment. As shown above that path is sufficient\n> information to unprune the necessary nodes in the TXO MMR and apply the spend\n> immediately, reducing this case to the TXO journal size question (non-consensus\n> critical overhead is a different question, which we'll address in the next\n> section).\n> \n> Taking all this into account the only significant storage overhead of our TXO\n> commitments scheme when compared to the status quo is the log2(n) merkle path\n> overhead; as long as less than 1/log2(n) of the UTXO set is active,\n> non-archived, UTXO's we've come out ahead, even in the unrealistic case where\n> all storage available is equally fast. In the real world that isn't yet the\n> case - even SSD's significantly slower than RAM.\n> \n> \n> ### Non-Consensus Critical Storage Overheads\n> \n> Transactions spending archived txouts pose two challenges:\n> \n> 1) Obtaining up-to-date TXO commitment proofs\n> \n> 2) Updating those proofs as blocks are mined\n> \n> The first challenge can be handled by specialized archival nodes, not unlike\n> how some nodes make transaction data available to wallets via bloom filters or\n> the Electrum protocol. There's a whole variety of options available, and the\n> the data can be easily sharded to scale horizontally; the data is\n> self-validating allowing horizontal scaling without trust.\n> \n> While miners and relay nodes don't need to be concerned about the initial\n> commitment proof, updating that proof is another matter. If a node aggressively\n> prunes old versions of the TXO MMR as it calculates pending TXO commitments, it\n> won't have the data available to update the TXO commitment proof to be against\n> the next block, when that block is found; the child nodes of the TXO MMR tip\n> are guaranteed to have changed, yet aggressive pruning would have discarded that\n> data.\n> \n> Relay nodes could ignore this problem if they simply accept the fact that\n> they'll only be able to fully relay the transaction once, when it is initially\n> broadcast, and won't be able to provide mempool functionality after the initial\n> relay. Modulo high-latency mixnets, this is probably acceptable; the author has\n> previously argued that relay nodes don't need a mempool\u00b2 at all.\n> \n> For a miner though not having the data necessary to update the proofs as blocks\n> are found means potentially losing out on transactions fees. So how much extra\n> data is necessary to make this a non-issue?\n> \n> Since the TXO MMR is insertion ordered, spending a non-archived txout can only\n> invalidate the upper nodes in of the archived txout's TXO MMR proof (if this\n> isn't clear, imagine a two-level scheme, with a per-block TXO MMRs, committed\n> by a master MMR for all blocks). The maximum number of relevant inner nodes\n> changed is log2(n) per block, so if there are n non-archival blocks between the\n> most recent TXO commitment and the pending TXO MMR tip, we have to store\n> log2(n)*n inner nodes - on the order of a few dozen MB even when n is a\n> (seemingly ridiculously high) year worth of blocks.\n> \n> Archived txout spends on the other hand can invalidate TXO MMR proofs at any\n> level - consider the case of two adjacent txouts being spent. To guarantee\n> success requires storing full proofs. However, they're limited by the blocksize\n> limit, and additionally are expected to be relatively uncommon. For example, if\n> 1% of 1MB blocks was archival spends, our hypothetical year long TXO commitment\n> delay is only a few hundred MB of data with low-IO-performance requirements.\n> \n> \n> ## Security Model\n> \n> Of course, a TXO commitment delay of a year sounds ridiculous. Even the slowest\n> imaginable computer isn't going to need more than a few blocks of TXO\n> commitment delay to keep up ~100% of the time, and there's no reason why we\n> can't have the UTXO archive delay be significantly longer than the TXO\n> commitment delay.\n> \n> However, as with UTXO commitments, TXO commitments raise issues with Bitcoin's\n> security model by allowing relatively miners to profitably mine transactions\n> without bothering to validate prior history. At the extreme, if there was no\n> commitment delay at all at the cost of a bit of some extra network bandwidth\n> \"full\" nodes could operate and even mine blocks completely statelessly by\n> expecting all transactions to include \"proof\" that their inputs are unspent; a\n> TXO commitment proof for a commitment you haven't verified isn't a proof that a\n> transaction output is unspent, it's a proof that some miners claimed the txout\n> was unspent.\n> \n> At one extreme, we could simply implement TXO commitments in a \"virtual\"\n> fashion, without miners actually including the TXO commitment digest in their\n> blocks at all. Full nodes would be forced to compute the commitment from\n> scratch, in the same way they are forced to compute the UTXO state, or total\n> work. Of course a full node operator who doesn't want to verify old history can\n> get a copy of the TXO state from a trusted source - no different from how you\n> could get a copy of the UTXO set from a trusted source.\n> \n> A more pragmatic approach is to accept that people will do that anyway, and\n> instead assume that sufficiently old blocks are valid. But how old is\n> \"sufficiently old\"? First of all, if your full node implementation comes \"from\n> the factory\" with a reasonably up-to-date minimum accepted total-work\n> threshold\u2071 - in other words it won't accept a chain with less than that amount\n> of total work - it may be reasonable to assume any Sybil attacker with\n> sufficient hashing power to make a forked chain meeting that threshold with,\n> say, six months worth of blocks has enough hashing power to threaten the main\n> chain as well.\n> \n> That leaves public attempts to falsify TXO commitments, done out in the open by\n> the majority of hashing power. In this circumstance the \"assumed valid\"\n> threshold determines how long the attack would have to go on before full nodes\n> start accepting the invalid chain, or at least, newly installed/recently reset\n> full nodes. The minimum age that we can \"assume valid\" is tradeoff between\n> political/social/technical concerns; we probably want at least a few weeks to\n> guarantee the defenders a chance to organise themselves.\n> \n> With this in mind, a longer-than-technically-necessary TXO commitment delay\u02b2\n> may help ensure that full node software actually validates some minimum number\n> of blocks out-of-the-box, without taking shortcuts. However this can be\n> achieved in a wide variety of ways, such as the author's prev-block-proof\n> proposal\u00b3, fraud proofs, or even a PoW with an inner loop dependent on\n> blockchain data. Like UTXO commitments, TXO commitments are also potentially\n> very useful in reducing the need for SPV wallet software to trust third parties\n> providing them with transaction data.\n> \n> i) Checkpoints that reject any chain without a specific block are a more\n>   common, if uglier, way of achieving this protection.\n> \n> j) A good homework problem is to figure out how the TXO commitment could be\n>   designed such that the delay could be reduced in a soft-fork.\n> \n> \n> ## Further Work\n> \n> While we've shown that TXO commitments certainly could be implemented without\n> increasing peak IO bandwidth/block validation latency significantly with the\n> delayed commitment approach, we're far from being certain that they should be\n> implemented this way (or at all).\n> \n> 1) Can a TXO commitment scheme be optimized sufficiently to be used directly\n> without a commitment delay? Obviously it'd be preferable to avoid all the above\n> complexity entirely.\n> \n> 2) Is it possible to use a metric other than age, e.g. priority? While this\n> complicates the pruning logic, it could use the UTXO set space more\n> efficiently, especially if your goal is to prioritise bitcoin value-transfer\n> over other uses (though if \"normal\" wallets nearly never need to use TXO\n> commitments proofs to spend outputs, the infrastructure to actually do this may\n> rot).\n> \n> 3) Should UTXO archiving be based on a fixed size UTXO set, rather than an\n> age/priority/etc. threshold?\n> \n> 4) By fixing the problem (or possibly just \"fixing\" the problem) are we\n> encouraging/legitimising blockchain use-cases other than BTC value transfer?\n> Should we?\n> \n> 5) Instead of TXO commitment proofs counting towards the blocksize limit, can\n> we use a different miner fairness/decentralization metric/incentive? For\n> instance it might be reasonable for the TXO commitment proof size to be\n> discounted, or ignored entirely, if a proof-of-propagation scheme (e.g.\n> thinblocks) is used to ensure all miners have received the proof in advance.\n> \n> 6) How does this interact with fraud proofs? Obviously furthering dependency on\n> non-cryptographically-committed STXO/UTXO databases is incompatible with the\n> modularized validation approach to implementing fraud proofs.\n> \n> \n> # References\n> \n> 1) \"Merkle Mountain Ranges\",\n>   Peter Todd, OpenTimestamps, Mar 18 2013,\n>   https://github.com/opentimestamps/opentimestamps-server/blob/master/doc/merkle-mountain-range.md\n> \n> 2) \"Do we really need a mempool? (for relay nodes)\",\n>   Peter Todd, bitcoin-dev mailing list, Jul 18th 2015,\n>   https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-July/009479.html\n> \n> 3) \"Segregated witnesses and validationless mining\",\n>   Peter Todd, bitcoin-dev mailing list, Dec 23rd 2015,\n>   https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012103.html\n> \n> -- \n> https://petertodd.org 'peter'[:-1]@petertodd.org\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Chris Priest",
                "date": "2016-05-17T18:01:23",
                "message_text_only": "On 5/17/16, Eric Lombrozo via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Nice!\n>\n> We\u2019ve been talking about doing this forever and it\u2019s so desperately needed.\n>\n\n\"So desperately needed\"? How do you figure? The UTXO set is currently\n1.5 GB. What kind of computer these days doesn't have 1.5 GB of\nmemory? Since you people insist on keeping the blocksize limit at 1MB,\nthe UTXO set growth is stuck growing at a tiny rate. Most consumer\nhardware sold thee days has 8GB or more RAM, it'll take decades before\nthe UTXO set come close to not fitting into 8 GB of memory.\n\nMaybe 30 or 40 years from not I can see this change being \"so\ndesperately needed\" when nodes are falling off because the UTXO set is\nto large, but that day is not today."
            },
            {
                "author": "Nick ODell",
                "date": "2016-05-19T22:23:28",
                "message_text_only": "What if two people create transactions from oupoints  within the same MMR\ntree tip, at the same time?\n\nFor example, I create transaction A plus an MMR proof that MMR tip X will\nbecome Y.\n\nOn the other side of the planet, someone else creates transaction B, plus\nan MMR proof that tip X will become Z.\n\nCan a miner who receives A and B put both into a block, without access to\nthe outputs that were pruned?\n# Motivation\n\nUTXO growth is a serious concern for Bitcoin's long-term decentralization.\nTo\nrun a competitive mining operation potentially the entire UTXO set must be\nin\nRAM to achieve competitive latency; your larger, more centralized,\ncompetitors\nwill have the UTXO set in RAM. Mining is a zero-sum game, so the extra\nlatency\nof not doing so if they do directly impacts your profit margin. Secondly,\nhaving possession of the UTXO set is one of the minimum requirements to run\na\nfull node; the larger the set the harder it is to run a full node.\n\nCurrently the maximum size of the UTXO set is unbounded as there is no\nconsensus rule that limits growth, other than the block-size limit itself;\nas\nof writing the UTXO set is 1.3GB in the on-disk, compressed serialization,\nwhich expands to significantly more in memory. UTXO growth is driven by a\nnumber of factors, including the fact that there is little incentive to\nmerge\ninputs, lost coins, dust outputs that can't be economically spent, and\nnon-btc-value-transfer \"blockchain\" use-cases such as anti-replay oracles\nand\ntimestamping.\n\nWe don't have good tools to combat UTXO growth. Segregated Witness proposes\nto\ngive witness space a 75% discount, in part of make reducing the UTXO set\nsize\nby spending txouts cheaper. While this may change wallets to more often\nspend\ndust, it's hard to imagine an incentive sufficiently strong to discourage\nmost,\nlet alone all, UTXO growing behavior.\n\nFor example, timestamping applications often create unspendable outputs due\nto\nease of implementation, and because doing so is an easy way to make sure\nthat\nthe data required to reconstruct the timestamp proof won't get lost - all\nBitcoin full nodes are forced to keep a copy of it. Similarly anti-replay\nuse-cases like using the UTXO set for key rotation piggyback on the uniquely\nstrong security and decentralization guarantee that Bitcoin provides; it's\nvery\ndifficult - perhaps impossible - to provide these applications with\nalternatives that are equally secure. These non-btc-value-transfer use-cases\ncan often afford to pay far higher fees per UTXO created than competing\nbtc-value-transfer use-cases; many users could afford to spend $50 to\nregister\na new PGP key, yet would rather not spend $50 in fees to create a standard\ntwo\noutput transaction. Effective techniques to resist miner censorship exist,\nso\nwithout resorting to whitelists blocking non-btc-value-transfer use-cases as\n\"spam\" is not a long-term, incentive compatible, solution.\n\nA hard upper limit on UTXO set size could create a more level playing field\nin\nthe form of fixed minimum requirements to run a performant Bitcoin node, and\nmake the issue of UTXO \"spam\" less important. However, making any coins\nunspendable, regardless of age or value, is a politically untenable economic\nchange.\n\n\n# TXO Commitments\n\nA merkle tree committing to the state of all transaction outputs, both spent\nand unspent, we can provide a method of compactly proving the current state\nof\nan output. This lets us \"archive\" less frequently accessed parts of the UTXO\nset, allowing full nodes to discard the associated data, still providing a\nmechanism to spend those archived outputs by proving to those nodes that the\noutputs are in fact unspent.\n\nSpecifically TXO commitments proposes a Merkle Mountain Range\u00b9 (MMR), a\ntype of deterministic, indexable, insertion ordered merkle tree, which\nallows\nnew items to be cheaply appended to the tree with minimal storage\nrequirements,\njust log2(n) \"mountain tips\". Once an output is added to the TXO MMR it is\nnever removed; if an output is spent its status is updated in place. Both\nthe\nstate of a specific item in the MMR, as well the validity of changes to\nitems\nin the MMR, can be proven with log2(n) sized proofs consisting of a merkle\npath\nto the tip of the tree.\n\nAt an extreme, with TXO commitments we could even have no UTXO set at all,\nentirely eliminating the UTXO growth problem. Transactions would simply be\naccompanied by TXO commitment proofs showing that the outputs they wanted to\nspend were still unspent; nodes could update the state of the TXO MMR purely\nfrom TXO commitment proofs. However, the log2(n) bandwidth overhead per\ntxin is\nsubstantial, so a more realistic implementation is be to have a UTXO cache\nfor\nrecent transactions, with TXO commitments acting as a alternate for the\n(rare)\nevent that an old txout needs to be spent.\n\nProofs can be generated and added to transactions without the involvement of\nthe signers, even after the fact; there's no need for the proof itself to\nsigned and the proof is not part of the transaction hash. Anyone with\naccess to\nTXO MMR data can (re)generate missing proofs, so minimal, if any, changes\nare\nrequired to wallet software to make use of TXO commitments.\n\n\n## Delayed Commitments\n\nTXO commitments aren't a new idea - the author proposed them years ago in\nresponse to UTXO commitments. However it's critical for small miners' orphan\nrates that block validation be fast, and so far it has proven difficult to\ncreate (U)TXO implementations with acceptable performance; updating and\nrecalculating cryptographicly hashed merkelized datasets is inherently more\nwork than not doing so. Fortunately if we maintain a UTXO set for recent\noutputs, TXO commitments are only needed when spending old, archived,\noutputs.\nWe can take advantage of this by delaying the commitment, allowing it to be\ncalculated well in advance of it actually being used, thus changing a\nlatency-critical task into a much easier average throughput problem.\n\nConcretely each block B_i commits to the TXO set state as of block B_{i-n},\nin\nother words what the TXO commitment would have been n blocks ago, if not for\nthe n block delay. Since that commitment only depends on the contents of the\nblockchain up until block B_{i-n}, the contents of any block after are\nirrelevant to the calculation.\n\n\n## Implementation\n\nOur proposed high-performance/low-latency delayed commitment full-node\nimplementation needs to store the following data:\n\n1) UTXO set\n\n    Low-latency K:V map of txouts definitely known to be unspent. Similar to\n    existing UTXO implementation, but with the key difference that old,\n    unspent, outputs may be pruned from the UTXO set.\n\n\n2) STXO set\n\n    Low-latency set of transaction outputs known to have been spent by\n    transactions after the most recent TXO commitment, but created prior to\nthe\n    TXO commitment.\n\n\n3) TXO journal\n\n    FIFO of outputs that need to be marked as spent in the TXO MMR. Appends\n    must be low-latency; removals can be high-latency.\n\n\n4) TXO MMR list\n\n    Prunable, ordered list of TXO MMR's, mainly the highest pending\ncommitment,\n    backed by a reference counted, cryptographically hashed object store\n    indexed by digest (similar to how git repos work). High-latency ok.\nWe'll\n    cover this in more in detail later.\n\n\n### Fast-Path: Verifying a Txout Spend In a Block\n\nWhen a transaction output is spent by a transaction in a block we have two\ncases:\n\n1) Recently created output\n\n    Output created after the most recent TXO commitment, so it should be in\nthe\n    UTXO set; the transaction spending it does not need a TXO commitment\nproof.\n    Remove the output from the UTXO set and append it to the TXO journal.\n\n2) Archived output\n\n    Output created prior to the most recent TXO commitment, so there's no\n    guarantee it's in the UTXO set; transaction will have a TXO commitment\n    proof for the most recent TXO commitment showing that it was unspent.\n    Check that the output isn't already in the STXO set (double-spent), and\nif\n    not add it. Append the output and TXO commitment proof to the TXO\njournal.\n\nIn both cases recording an output as spent requires no more than two\nkey:value\nupdates, and one journal append. The existing UTXO set requires one\nkey:value\nupdate per spend, so we can expect new block validation latency to be\nwithin 2x\nof the status quo even in the worst case of 100% archived output spends.\n\n\n### Slow-Path: Calculating Pending TXO Commitments\n\nIn a low-priority background task we flush the TXO journal, recording the\noutputs spent by each block in the TXO MMR, and hashing MMR data to obtain\nthe\nTXO commitment digest. Additionally this background task removes STXO's that\nhave been recorded in TXO commitments, and prunes TXO commitment data no\nlonger\nneeded.\n\nThroughput for the TXO commitment calculation will be worse than the\nexisting\nUTXO only scheme. This impacts bulk verification, e.g. initial block\ndownload.\nThat said, TXO commitments provides other possible tradeoffs that can\nmitigate\nimpact of slower validation throughput, such as skipping validation of old\nhistory, as well as fraud proof approaches.\n\n\n### TXO MMR Implementation Details\n\nEach TXO MMR state is a modification of the previous one with most\ninformation\nshared, so we an space-efficiently store a large number of TXO commitments\nstates, where each state is a small delta of the previous state, by sharing\nunchanged data between each state; cycles are impossible in merkelized data\nstructures, so simple reference counting is sufficient for garbage\ncollection.\nData no longer needed can be pruned by dropping it from the database, and\nunpruned by adding it again. Since everything is committed to via\ncryptographic\nhash, we're guaranteed that regardless of where we get the data, after\nunpruning we'll have the right data.\n\nLet's look at how the TXO MMR works in detail. Consider the following TXO\nMMR\nwith two txouts, which we'll call state #0:\n\n      0\n     / \\\n    a   b\n\nIf we add another entry we get state #1:\n\n        1\n       / \\\n      0   \\\n     / \\   \\\n    a   b   c\n\nNote how it 100% of the state #0 data was reused in commitment #1. Let's\nadd two more entries to get state #2:\n\n            2\n           / \\\n          2   \\\n         / \\   \\\n        /   \\   \\\n       /     \\   \\\n      0       2   \\\n     / \\     / \\   \\\n    a   b   c   d   e\n\nThis time part of state #1 wasn't reused - it's wasn't a perfect binary\ntree - but we've still got a lot of re-use.\n\nNow suppose state #2 is committed into the blockchain by the most recent\nblock.\nFuture transactions attempting to spend outputs created as of state #2 are\nobliged to prove that they are unspent; essentially they're forced to\nprovide\npart of the state #2 MMR data. This lets us prune that data, discarding it,\nleaving us with only the bare minimum data we need to append new txouts to\nthe\nTXO MMR, the tips of the perfect binary trees (\"mountains\") within the MMR:\n\n            2\n           / \\\n          2   \\\n               \\\n                \\\n                 \\\n                  \\\n                   \\\n                    e\n\nNote that we're glossing over some nuance here about exactly what data\nneeds to\nbe kept; depending on the details of the implementation the only data we\nneed\nfor nodes \"2\" and \"e\" may be their hash digest.\n\nAdding another three more txouts results in state #3:\n\n                  3\n                 / \\\n                /   \\\n               /     \\\n              /       \\\n             /         \\\n            /           \\\n           /             \\\n          2               3\n                         / \\\n                        /   \\\n                       /     \\\n                      3       3\n                     / \\     / \\\n                    e   f   g   h\n\nSuppose recently created txout f is spent. We have all the data required to\nupdate the MMR, giving us state #4. It modifies two inner nodes and one leaf\nnode:\n\n                  4\n                 / \\\n                /   \\\n               /     \\\n              /       \\\n             /         \\\n            /           \\\n           /             \\\n          2               4\n                         / \\\n                        /   \\\n                       /     \\\n                      4       3\n                     / \\     / \\\n                    e  (f)  g   h\n\nIf an archived txout is spent requires the transaction to provide the merkle\npath to the most recently committed TXO, in our case state #2. If txout b is\nspent that means the transaction must provide the following data from state\n#2:\n\n            2\n           /\n          2\n         /\n        /\n       /\n      0\n       \\\n        b\n\nWe can add that data to our local knowledge of the TXO MMR, unpruning part\nof\nit:\n\n                  4\n                 / \\\n                /   \\\n               /     \\\n              /       \\\n             /         \\\n            /           \\\n           /             \\\n          2               4\n         /               / \\\n        /               /   \\\n       /               /     \\\n      0               4       3\n       \\             / \\     / \\\n        b           e  (f)  g   h\n\nRemember, we haven't _modified_ state #4 yet; we just have more data about\nit.\nWhen we mark txout b as spent we get state #5:\n\n                  5\n                 / \\\n                /   \\\n               /     \\\n              /       \\\n             /         \\\n            /           \\\n           /             \\\n          5               4\n         /               / \\\n        /               /   \\\n       /               /     \\\n      5               4       3\n       \\             / \\     / \\\n       (b)          e  (f)  g   h\n\nSecondly by now state #3 has been committed into the chain, and transactions\nthat want to spend txouts created as of state #3 must provide a TXO proof\nconsisting of state #3 data. The leaf nodes for outputs g and h, and the\ninner\nnode above them, are part of state #3, so we prune them:\n\n                  5\n                 / \\\n                /   \\\n               /     \\\n              /       \\\n             /         \\\n            /           \\\n           /             \\\n          5               4\n         /               /\n        /               /\n       /               /\n      5               4\n       \\             / \\\n       (b)          e  (f)\n\nFinally, lets put this all together, by spending txouts a, c, and g, and\ncreating three new txouts i, j, and k. State #3 was the most recently\ncommitted\nstate, so the transactions spending a and g are providing merkle paths up to\nit. This includes part of the state #2 data:\n\n                  3\n                 / \\\n                /   \\\n               /     \\\n              /       \\\n             /         \\\n            /           \\\n           /             \\\n          2               3\n         / \\               \\\n        /   \\               \\\n       /     \\               \\\n      0       2               3\n     /       /               /\n    a       c               g\n\nAfter unpruning we have the following data for state #5:\n\n                  5\n                 / \\\n                /   \\\n               /     \\\n              /       \\\n             /         \\\n            /           \\\n           /             \\\n          5               4\n         / \\             / \\\n        /   \\           /   \\\n       /     \\         /     \\\n      5       2       4       3\n     / \\     /       / \\     /\n    a  (b)  c       e  (f)  g\n\nThat's sufficient to mark the three outputs as spent and add the three new\ntxouts, resulting in state #6:\n\n                        6\n                       / \\\n                      /   \\\n                     /     \\\n                    /       \\\n                   /         \\\n                  6           \\\n                 / \\           \\\n                /   \\           \\\n               /     \\           \\\n              /       \\           \\\n             /         \\           \\\n            /           \\           \\\n           /             \\           \\\n          6               6           \\\n         / \\             / \\           \\\n        /   \\           /   \\           6\n       /     \\         /     \\         / \\\n      6       6       4       6       6   \\\n     / \\     /       / \\     /       / \\   \\\n   (a) (b) (c)      e  (f) (g)      i   j   k\n\nAgain, state #4 related data can be pruned. In addition, depending on how\nthe\nSTXO set is implemented may also be able to prune data related to spent\ntxouts\nafter that state, including inner nodes where all txouts under them have\nbeen\nspent (more on pruning spent inner nodes later).\n\n\n### Consensus and Pruning\n\nIt's important to note that pruning behavior is consensus critical: a full\nnode\nthat is missing data due to pruning it too soon will fall out of consensus,\nand\na miner that fails to include a merkle proof that is required by the\nconsensus\nis creating an invalid block. At the same time many full nodes will have\nsignificantly more data on hand than the bare minimum so they can help\nwallets\nmake transactions spending old coins; implementations should strongly\nconsider\nseparating the data that is, and isn't, strictly required for consensus.\n\nA reasonable approach for the low-level cryptography may be to actually\ntreat\nthe two cases differently, with the TXO commitments committing too what data\ndoes and does not need to be kept on hand by the UTXO expiration rules. On\nthe\nother hand, leaving that uncommitted allows for certain types of soft-forks\nwhere the protocol is changed to require more data than it previously did.\n\n\n### Consensus Critical Storage Overheads\n\nOnly the UTXO and STXO sets need to be kept on fast random access storage.\nSince STXO set entries can only be created by spending a UTXO - and are\nsmaller\nthan a UTXO entry - we can guarantee that the peak size of the UTXO and STXO\nsets combined will always be less than the peak size of the UTXO set alone\nin\nthe existing UTXO-only scheme (though the combined size can be temporarily\nhigher than what the UTXO set size alone would be when large numbers of\narchived txouts are spent).\n\nTXO journal entries and unpruned entries in the TXO MMR have log2(n) maximum\noverhead per entry: a unique merkle path to a TXO commitment (by \"unique\" we\nmean that no other entry shares data with it). On a reasonably fast system\nthe\nTXO journal will be flushed quickly, converting it into TXO MMR data; the\nTXO\njournal will never be more than a few blocks in size.\n\nTransactions spending non-archived txouts are not required to provide any\nTXO\ncommitment data; we must have that data on hand in the form of one TXO MMR\nentry per UTXO. Once spent however the TXO MMR leaf node associated with\nthat\nnon-archived txout can be immediately pruned - it's no longer in the UTXO\nset\nso any attempt to spend it will fail; the data is now immutable and we'll\nnever\nneed it again. Inner nodes in the TXO MMR can also be pruned if all leafs\nunder\nthem are fully spent; detecting this is easy the TXO MMR is a merkle-sum\ntree,\nwith each inner node committing to the sum of the unspent txouts under it.\n\nWhen a archived txout is spent the transaction is required to provide a\nmerkle\npath to the most recent TXO commitment. As shown above that path is\nsufficient\ninformation to unprune the necessary nodes in the TXO MMR and apply the\nspend\nimmediately, reducing this case to the TXO journal size question\n(non-consensus\ncritical overhead is a different question, which we'll address in the next\nsection).\n\nTaking all this into account the only significant storage overhead of our\nTXO\ncommitments scheme when compared to the status quo is the log2(n) merkle\npath\noverhead; as long as less than 1/log2(n) of the UTXO set is active,\nnon-archived, UTXO's we've come out ahead, even in the unrealistic case\nwhere\nall storage available is equally fast. In the real world that isn't yet the\ncase - even SSD's significantly slower than RAM.\n\n\n### Non-Consensus Critical Storage Overheads\n\nTransactions spending archived txouts pose two challenges:\n\n1) Obtaining up-to-date TXO commitment proofs\n\n2) Updating those proofs as blocks are mined\n\nThe first challenge can be handled by specialized archival nodes, not unlike\nhow some nodes make transaction data available to wallets via bloom filters\nor\nthe Electrum protocol. There's a whole variety of options available, and the\nthe data can be easily sharded to scale horizontally; the data is\nself-validating allowing horizontal scaling without trust.\n\nWhile miners and relay nodes don't need to be concerned about the initial\ncommitment proof, updating that proof is another matter. If a node\naggressively\nprunes old versions of the TXO MMR as it calculates pending TXO\ncommitments, it\nwon't have the data available to update the TXO commitment proof to be\nagainst\nthe next block, when that block is found; the child nodes of the TXO MMR tip\nare guaranteed to have changed, yet aggressive pruning would have discarded\nthat\ndata.\n\nRelay nodes could ignore this problem if they simply accept the fact that\nthey'll only be able to fully relay the transaction once, when it is\ninitially\nbroadcast, and won't be able to provide mempool functionality after the\ninitial\nrelay. Modulo high-latency mixnets, this is probably acceptable; the author\nhas\npreviously argued that relay nodes don't need a mempool\u00b2 at all.\n\nFor a miner though not having the data necessary to update the proofs as\nblocks\nare found means potentially losing out on transactions fees. So how much\nextra\ndata is necessary to make this a non-issue?\n\nSince the TXO MMR is insertion ordered, spending a non-archived txout can\nonly\ninvalidate the upper nodes in of the archived txout's TXO MMR proof (if this\nisn't clear, imagine a two-level scheme, with a per-block TXO MMRs,\ncommitted\nby a master MMR for all blocks). The maximum number of relevant inner nodes\nchanged is log2(n) per block, so if there are n non-archival blocks between\nthe\nmost recent TXO commitment and the pending TXO MMR tip, we have to store\nlog2(n)*n inner nodes - on the order of a few dozen MB even when n is a\n(seemingly ridiculously high) year worth of blocks.\n\nArchived txout spends on the other hand can invalidate TXO MMR proofs at any\nlevel - consider the case of two adjacent txouts being spent. To guarantee\nsuccess requires storing full proofs. However, they're limited by the\nblocksize\nlimit, and additionally are expected to be relatively uncommon. For\nexample, if\n1% of 1MB blocks was archival spends, our hypothetical year long TXO\ncommitment\ndelay is only a few hundred MB of data with low-IO-performance requirements.\n\n\n## Security Model\n\nOf course, a TXO commitment delay of a year sounds ridiculous. Even the\nslowest\nimaginable computer isn't going to need more than a few blocks of TXO\ncommitment delay to keep up ~100% of the time, and there's no reason why we\ncan't have the UTXO archive delay be significantly longer than the TXO\ncommitment delay.\n\nHowever, as with UTXO commitments, TXO commitments raise issues with\nBitcoin's\nsecurity model by allowing relatively miners to profitably mine transactions\nwithout bothering to validate prior history. At the extreme, if there was no\ncommitment delay at all at the cost of a bit of some extra network bandwidth\n\"full\" nodes could operate and even mine blocks completely statelessly by\nexpecting all transactions to include \"proof\" that their inputs are\nunspent; a\nTXO commitment proof for a commitment you haven't verified isn't a proof\nthat a\ntransaction output is unspent, it's a proof that some miners claimed the\ntxout\nwas unspent.\n\nAt one extreme, we could simply implement TXO commitments in a \"virtual\"\nfashion, without miners actually including the TXO commitment digest in\ntheir\nblocks at all. Full nodes would be forced to compute the commitment from\nscratch, in the same way they are forced to compute the UTXO state, or total\nwork. Of course a full node operator who doesn't want to verify old history\ncan\nget a copy of the TXO state from a trusted source - no different from how\nyou\ncould get a copy of the UTXO set from a trusted source.\n\nA more pragmatic approach is to accept that people will do that anyway, and\ninstead assume that sufficiently old blocks are valid. But how old is\n\"sufficiently old\"? First of all, if your full node implementation comes\n\"from\nthe factory\" with a reasonably up-to-date minimum accepted total-work\nthreshold\u2071 - in other words it won't accept a chain with less than that\namount\nof total work - it may be reasonable to assume any Sybil attacker with\nsufficient hashing power to make a forked chain meeting that threshold with,\nsay, six months worth of blocks has enough hashing power to threaten the\nmain\nchain as well.\n\nThat leaves public attempts to falsify TXO commitments, done out in the\nopen by\nthe majority of hashing power. In this circumstance the \"assumed valid\"\nthreshold determines how long the attack would have to go on before full\nnodes\nstart accepting the invalid chain, or at least, newly installed/recently\nreset\nfull nodes. The minimum age that we can \"assume valid\" is tradeoff between\npolitical/social/technical concerns; we probably want at least a few weeks\nto\nguarantee the defenders a chance to organise themselves.\n\nWith this in mind, a longer-than-technically-necessary TXO commitment delay\u02b2\nmay help ensure that full node software actually validates some minimum\nnumber\nof blocks out-of-the-box, without taking shortcuts. However this can be\nachieved in a wide variety of ways, such as the author's prev-block-proof\nproposal\u00b3, fraud proofs, or even a PoW with an inner loop dependent on\nblockchain data. Like UTXO commitments, TXO commitments are also potentially\nvery useful in reducing the need for SPV wallet software to trust third\nparties\nproviding them with transaction data.\n\ni) Checkpoints that reject any chain without a specific block are a more\n   common, if uglier, way of achieving this protection.\n\nj) A good homework problem is to figure out how the TXO commitment could be\n   designed such that the delay could be reduced in a soft-fork.\n\n\n## Further Work\n\nWhile we've shown that TXO commitments certainly could be implemented\nwithout\nincreasing peak IO bandwidth/block validation latency significantly with the\ndelayed commitment approach, we're far from being certain that they should\nbe\nimplemented this way (or at all).\n\n1) Can a TXO commitment scheme be optimized sufficiently to be used directly\nwithout a commitment delay? Obviously it'd be preferable to avoid all the\nabove\ncomplexity entirely.\n\n2) Is it possible to use a metric other than age, e.g. priority? While this\ncomplicates the pruning logic, it could use the UTXO set space more\nefficiently, especially if your goal is to prioritise bitcoin value-transfer\nover other uses (though if \"normal\" wallets nearly never need to use TXO\ncommitments proofs to spend outputs, the infrastructure to actually do this\nmay\nrot).\n\n3) Should UTXO archiving be based on a fixed size UTXO set, rather than an\nage/priority/etc. threshold?\n\n4) By fixing the problem (or possibly just \"fixing\" the problem) are we\nencouraging/legitimising blockchain use-cases other than BTC value transfer?\nShould we?\n\n5) Instead of TXO commitment proofs counting towards the blocksize limit,\ncan\nwe use a different miner fairness/decentralization metric/incentive? For\ninstance it might be reasonable for the TXO commitment proof size to be\ndiscounted, or ignored entirely, if a proof-of-propagation scheme (e.g.\nthinblocks) is used to ensure all miners have received the proof in advance.\n\n6) How does this interact with fraud proofs? Obviously furthering\ndependency on\nnon-cryptographically-committed STXO/UTXO databases is incompatible with the\nmodularized validation approach to implementing fraud proofs.\n\n\n# References\n\n1) \"Merkle Mountain Ranges\",\n   Peter Todd, OpenTimestamps, Mar 18 2013,\n\nhttps://github.com/opentimestamps/opentimestamps-server/blob/master/doc/merkle-mountain-range.md\n\n2) \"Do we really need a mempool? (for relay nodes)\",\n   Peter Todd, bitcoin-dev mailing list, Jul 18th 2015,\n\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-July/009479.html\n\n3) \"Segregated witnesses and validationless mining\",\n   Peter Todd, bitcoin-dev mailing list, Dec 23rd 2015,\n\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012103.html\n\n--\nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160519/985c4b60/attachment-0001.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2016-05-20T08:45:35",
                "message_text_only": "On Thu, May 19, 2016 at 04:23:28PM -0600, Nick ODell wrote:\n> What if two people create transactions from oupoints  within the same MMR\n> tree tip, at the same time?\n> \n> For example, I create transaction A plus an MMR proof that MMR tip X will\n> become Y.\n> \n> On the other side of the planet, someone else creates transaction B, plus\n> an MMR proof that tip X will become Z.\n> \n> Can a miner who receives A and B put both into a block, without access to\n> the outputs that were pruned?\n\nThe MMR proofs provided by transactions aren't proofs of *how* the MMR should\nbe be changd; they're just proofs that the MMR is in a certain state right now.\nYou're situation is just an example of a double-spend, that miners have to\ndetect if they don't want to create invalid blocks. Specifically, if I\nunderstand your example correctly, they'd be rejected by the STXO set.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160520/e23f355a/attachment.sig>"
            },
            {
                "author": "Johnson Lau",
                "date": "2016-05-20T09:46:32",
                "message_text_only": "How is this compared to my earlier proposal: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/011952.html <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/011952.html> ?\n\nIn my proposal, only the (pruned) UTXO set, and 32 bytes per archived block, are required for mining. But it is probably more difficult for people to spend an archived output. They need to know the status of other archived outputs from the same block. A full re-scan of the blockchain may be needed to generate the proof but this could be done by a third party archival node.\n\n> \n> \n> \n> ## Implementation\n> \n> Our proposed high-performance/low-latency delayed commitment full-node\n> implementation needs to store the following data:\n> \n> 1) UTXO set\n> \n>    Low-latency K:V map of txouts definitely known to be unspent. Similar to\n>    existing UTXO implementation, but with the key difference that old,\n>    unspent, outputs may be pruned from the UTXO set.\n> \n> \n> 2) STXO set\n> \n>    Low-latency set of transaction outputs known to have been spent by\n>    transactions after the most recent TXO commitment, but created prior to the\n>    TXO commitment.\n> \n> \n> 3) TXO journal\n> \n>    FIFO of outputs that need to be marked as spent in the TXO MMR. Appends\n>    must be low-latency; removals can be high-latency.\n> \n> \n> 4) TXO MMR list\n> \n>    Prunable, ordered list of TXO MMR's, mainly the highest pending commitment,\n>    backed by a reference counted, cryptographically hashed object store\n>    indexed by digest (similar to how git repos work). High-latency ok. We'll\n>    cover this in more in detail later.\n> \n> \n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160520/532a993f/attachment.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 671 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160520/532a993f/attachment.sig>"
            },
            {
                "author": "Peter Todd",
                "date": "2016-05-22T08:55:33",
                "message_text_only": "On Fri, May 20, 2016 at 11:46:32AM +0200, Johnson Lau wrote:\n> How is this compared to my earlier proposal: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/011952.html <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/011952.html> ?\n> \n> In my proposal, only the (pruned) UTXO set, and 32 bytes per archived block, are required for mining. But it is probably more difficult for people to spend an archived output. They need to know the status of other archived outputs from the same block. A full re-scan of the blockchain may be needed to generate the proof but this could be done by a third party archival node.\n\nWe're working along the same lines, but my proposal is much better fleshed out;\nI think you'll find you missed a few details if you flesh out yours in more\ndetail. For instance, since your dormant UTXO list is indexed by UTXO\nexpiration order, it's not possible to do any kind of verification that the\ncontents of that commitment are correct without the global state of all UTXO\ndata - you have no ability to locally verify as nothing commits to the contents\nof the UTXO set.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160522/bb29364c/attachment.sig>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2016-05-18T11:14:59",
                "message_text_only": "On May 17, 2016 15:23, \"Peter Todd via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n> # TXO Commitments\n>\n\n> Specifically TXO commitments proposes a Merkle Mountain Range\u00b9 (MMR), a\n> type of deterministic, indexable, insertion ordered merkle tree, which\nallows\n> new items to be cheaply appended to the tree with minimal storage\nrequirements,\n> just log2(n) \"mountain tips\". Once an output is added to the TXO MMR it is\n> never removed; if an output is spent its status is updated in place. Both\nthe\n> state of a specific item in the MMR, as well the validity of changes to\nitems\n> in the MMR, can be proven with log2(n) sized proofs consisting of a\nmerkle path\n> to the tip of the tree.\n\nHow expensive it is to update a leaf from this tree from unspent to spent?\n\nWouldn't it be better to have both an append-only TXO and an append-only\nSTXO (with all spent outputs, not only the latest ones like in your \"STXO\")?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160518/798bf65c/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2016-05-18T23:53:36",
                "message_text_only": "On Wed, May 18, 2016 at 01:14:59PM +0200, Jorge Tim\u00f3n wrote:\n> On May 17, 2016 15:23, \"Peter Todd via bitcoin-dev\" <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > # TXO Commitments\n> >\n> \n> > Specifically TXO commitments proposes a Merkle Mountain Range\u00b9 (MMR), a\n> > type of deterministic, indexable, insertion ordered merkle tree, which\n> allows\n> > new items to be cheaply appended to the tree with minimal storage\n> requirements,\n> > just log2(n) \"mountain tips\". Once an output is added to the TXO MMR it is\n> > never removed; if an output is spent its status is updated in place. Both\n> the\n> > state of a specific item in the MMR, as well the validity of changes to\n> items\n> > in the MMR, can be proven with log2(n) sized proofs consisting of a\n> merkle path\n> > to the tip of the tree.\n> \n> How expensive it is to update a leaf from this tree from unspent to spent?\n\nlog2(n) operations.\n\nI wrote a full MMR implementation with pruning support as part of my\nproofchains work:\n\nhttps://github.com/proofchains/python-proofmarshal/blob/master/proofmarshal/mmr.py\n\nDocumentation is a bit lacking, but I'd suggest reading the above source code\nand the unit tests(1) to understand what's going on. As of writing item\nretrieval by index is implemented(2), and if you follow how that works you'll\nsee it's log2(n) operations; changing elements in-place isn't yet\nimplemented(3) but would be a fun homework problem. I'll bet anyone a beer that\nyou'll find it can be done in k*log2(n) operations, with a reasonably small k. :)\n\nAdditionally, I also have a merkelized key:value prefix tree implementation\ncalled a \"merbinner tree\" in the same library, again with pruning support. It\ndoes implement changing elements in place(4) with log2(n) operations.\n\nIncidentally, something I probably should have made more clear in my TXO\ncommitments post is that the original MMR scheme I developed for OpenTimestamps\n(and independently reinvented for Certificate Transparency) is insufficient:\nwhile you can easily extract a proof that an element is present in the MMR,\nthat inclusion proof doesn't do a good job of proving the position in the tree\nvery well. OpenTimestamps didn't need that kind of proof, and I don't think\nCertificate Transparency needs it either. However many other MMR applications\ndo, including many types of TXO commitments.\n\nMy proofchains MMR scheme fixes this problem by making each inner node in the\nMMR commit to the total number of elements under it(5) - basically it's a\nmerkle-sum-tree with the size of the tree being what's summed. There may be\nmore efficient ways to do this, but a committed sum-length is easy to\nimplement, and the space overhead is only 25% even in the least optimised\nimplementation possible.\n\n1) https://github.com/proofchains/python-proofmarshal/blob/3f0ba0a9d46f36377ad6c1901de19273604e6fbc/proofmarshal/test/test_mmr.py\n2) https://github.com/proofchains/python-proofmarshal/blob/3f0ba0a9d46f36377ad6c1901de19273604e6fbc/proofmarshal/mmr.py#L294\n3) https://github.com/proofchains/python-proofmarshal/blob/3f0ba0a9d46f36377ad6c1901de19273604e6fbc/proofmarshal/mmr.py#L230\n4) https://github.com/proofchains/python-proofmarshal/blob/3f0ba0a9d46f36377ad6c1901de19273604e6fbc/proofmarshal/merbinnertree.py#L140\n5) https://github.com/proofchains/python-proofmarshal/blob/3f0ba0a9d46f36377ad6c1901de19273604e6fbc/proofmarshal/mmr.py#L139\n\n> Wouldn't it be better to have both an append-only TXO and an append-only\n> STXO (with all spent outputs, not only the latest ones like in your \"STXO\")?\n\nNope. The reason why this doesn't work is apparent when you ask how will the\nSTXO be indexed?\n\nIf it's indexed by outpoint - that is H(txid:n) - to update the STXO you need\nhe entire thing, as the position of any new STXO that you need to add to the\nSTXO tree is random.\n\nOTOH, if you index the STXO by txout creation order, with the first txout ever\ncreated having position #0, the second #1, etc. the data you may need to update\nthe STXO later has predictable locality... but now you have something that's\nbasically identical to my proposed insertion-ordered TXO commitment anyway.\n\nIncidentally, it's interesting how if a merbinner tree is insertion-order\nindexed you end up with a datastructure that's almost identical to a MMR.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160518/5d585b5c/attachment.sig>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2016-05-19T09:31:26",
                "message_text_only": "On May 19, 2016 01:53, \"Peter Todd\" <pete at petertodd.org> wrote:\ntip of the tree.\n> >\n> > How expensive it is to update a leaf from this tree from unspent to\nspent?\n>\n> log2(n) operations.\n\nUpdating a leaf is just as expensive as adding a new one?\nThat's not what I expected.\nOr is adding a new one O (1) ?\n\nAnyway, thanks, I'll read this in more detail.\n\n> > Wouldn't it be better to have both an append-only TXO and an append-only\n> > STXO (with all spent outputs, not only the latest ones like in your\n\"STXO\")?\n>\n> Nope. The reason why this doesn't work is apparent when you ask how will\nthe\n> STXO be indexed?\n\nJust the same way the TXO is (you just stop updating the txo leafs from\nunspent to spent.\n\n> If it's indexed by outpoint - that is H(txid:n) - to update the STXO you\nneed\n> he entire thing, as the position of any new STXO that you need to add to\nthe\n> STXO tree is random.\n>\n> OTOH, if you index the STXO by txout creation order, with the first txout\never\n> created having position #0, the second #1, etc. the data you may need to\nupdate\n> the STXO later has predictable locality... but now you have something\nthat's\n> basically identical to my proposed insertion-ordered TXO commitment\nanyway.\n\nYeah, that's what I want. Like your append only TXO but for STXO (that way\nwe avoid ever updating leafs in the TXO, and I suspect there are other\nadvantages for fraud proofs).\n\n> Incidentally, it's interesting how if a merbinner tree is insertion-order\n> indexed you end up with a datastructure that's almost identical to a MMR.\n\nNo complain with MMR. My point is having 2 of them separated: one for the\nTXO (entries unmutable) and one for the STXO (again, entries unmutable).\n\nMaybe it doesn't make sense, but I would like to understand why.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160519/91d33897/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Making UTXO Set Growth Irrelevant With Low-Latency Delayed TXO Commitments",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Eric Lombrozo",
                "Nick ODell",
                "Chris Priest",
                "Peter Todd",
                "Johnson Lau",
                "Jorge Tim\u00f3n",
                "Jameson Lopp"
            ],
            "messages_count": 11,
            "total_messages_chars_count": 130582
        }
    },
    {
        "title": "[bitcoin-dev] p2p authentication and encryption BIPs",
        "thread_messages": [
            {
                "author": "Jonas Schnelli",
                "date": "2016-05-18T08:00:44",
                "message_text_only": "Hi Lee\n\nThank you very much for the valuable input.\nI'm still processing your feedback....\n\n> \n> *Key Revocation*\n> This is probably too complicated, but an additional public key would\n> allow for cold-storage key revocation. Spreading the knowledge of such\n> an event is always painful, but it could be stored in the blockchain. I\n> think this is likely too complicated, but having these long-term keys\n> constantly in memory/disk is unfortunate.\n> \n\nYes. This could be something that could be extended once the BIP is\nstable and/or implemented.\n\n\n\n>> <code>K_1</code> must be used to only encrypt the payload size of the\n>> encrypted message to avoid leaking information by revealing the\n>> message size. \n>>\n>> <code>K_2</code> must be used in conjunction with poly1305 to build\n>> an AEAD.\n> \n> Chacha20 is a stream cipher, so only a single encryption key is needed.\n> The first 32 bytes of the keystream would be used for the Poly1305 key,\n> the next 4 bytes would be used to encrypt the length field, and the\n> remaining keystream would be used to encrypt the payload. Poly1305\n> would then generate a tag over the length and payload. The receiver\n> would generate the same keystream to decrypt the length which\n> identifies the length of the message and the MAC offset, then\n> authenticate the length and payload, then decypt with the remaining\n> keystream.\n> \n\nRight. The AEAD construct I though of is probably called\nchacha20-poly1305 at openssh.com and specified in\nhttps://github.com/openssh/openssh-portable/blob/05855bf2ce7d5cd0a6db18bc0b4214ed5ef7516d/PROTOCOL.chacha20poly1305#L34\n\nI think this construct has already serval implementations and is widely\nused.\n\nI have updated the BIP to mention the chacha20-poly1305 at openssh.com\nspecification.\n\n> Is it safer to define two keys to prevent implementations from screwing\n> this up? You have to split the decryption and authentication, so the\n> basic modes of libsodium cannot be used for instance. If a custom tag\n> generation scheme is being used, then the basic modes are already\n> unusable ...\n> \n> *Failed Authentication*\n> What happens on a failed MAC attempt? Connection closure is the\n> easiest way to handle the situation.\n\nYes. I think closing would make sense.\n\n>> After a successful <code>encinit</code>/<code>encack</code>\n>> interaction from both sides, the messages format must use the\n>> \"encrypted messages structure\". Non-encrypted messages from the\n>> requesting peer must lead to a connection termination (can be\n>> detected by the 4 byte network magic in the unencrypted message\n>> structure).\n> \n> The magic bytes are at the same offset and size as the encrypted length\n> field in the encrypted messages structure. So the magic bytes are not a\n> reliable way to identify unencrypted messages, although the probability\n> of collision is low.\n\nYes. This is a good point.\nThe implementation should probably also accept messages that contain the\n4 byte network magic from unencrypted messages (to avoid possible\ncollisions).\nIf the message is unencrypted, the length check or the unsuccessful\nauthentication check will lead to a disconnect.\n\n>> {|class=\"wikitable\"\n>> ! Field Size !! Description !! Data type !! Comments\n>> |-\n>> | 4 || length || uint32_t || Length of ciphertext payload in number\n>> of bytes\n>> |-\n>> | ? || ciphertext payload || ? || One or many ciphertext command &\n>> message data\n>> |-\n>> | 8 || MAC tag || ? || MAC-tag truncated to 8 bytes\n>> |}\n> \n> Why have a fixed MAC length? I think the MAC length should be inferred\n> from the cipher + authentication mode. And the Poly1305 tag is 16 bytes.\n> \n> *Unauthenticated Buffering*\n> Implementations are unlikely to (i.e. should not) process the payload\n> until authentication succeeds. Since the length field is 4 bytes, this\n> means an implementation may have to buffer up to 4 GiB of data _per\n> connection_ before it can authenticate the length field. If the outter\n> length field were reduced to 2 or 3 bytes, the unauthenticated\n> buffering requirements drop to 64 KiB and 16 MiB respectively. Inner\n> messages already have their own length, so they can span multiple\n> encrypted blocks without other changes. This will increase the\n> bandwidth requirements when the size of a single message exceeds 64 KiB\n> or 16 MiB, since it will require multiple authentication tags for that\n> message. I think an additional 16 bytes per 16 MiB seems like a good\n> tradeoff.\n> \n\nGood point.\nI have mentioned this now in the BIP but I think the BIP should allow\nmessage > 16 MiB.\nI leave the max. message length up to the implementation while keeping\nthe 4 byte length on the protocol level.\n\n> \n>> A responding peer can inform the requesting peer over a re-keying\n>> with a <code>encack</code> message containing 33byte of zeros to\n>> indicate that all encrypted message following after this\n>> <code>encack</code> message will be encrypted with ''the next\n>> symmetric cipher key''.\n>>\n>> The new symmetric cipher key will be calculated by\n>> <code>SHA256(SHA256(old_symetric_cipher_key))</code>.\n>>\n>> Re-Keying interval is a peer policy with a minimum timespan of 600\n>> seconds.\n> \n> Should the int64_t message count be reset to 0 on a re-key? Or should\n> the value reset to zero after 2^63-1? Hopefully the peer re-keys before\n> that rollover, or keystream reusage will occur. Unlikely that many\n> messages are sent on a single connection though. And presumably this\n> only re-keys the senders side? Bi-directional re-keying would be racy.\n\nI just added the RFC4253 recommendation as a must (re-key after every\n1GB of data sent or received).\n\n\n</jonas>\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 819 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160518/b974c034/attachment.sig>"
            },
            {
                "author": "Lee Clagett",
                "date": "2016-05-25T00:22:50",
                "message_text_only": "On Wed, 18 May 2016 10:00:44 +0200\nJonas Schnelli via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>\nwrote:\n\n> Hi Lee\n> \n> Thank you very much for the valuable input.\n> I'm still processing your feedback....\n\n[...]\n\n> > Why have a fixed MAC length? I think the MAC length should be\n> > inferred from the cipher + authentication mode. And the Poly1305\n> > tag is 16 bytes.\n> > \n> > *Unauthenticated Buffering*\n> > Implementations are unlikely to (i.e. should not) process the\n> > payload until authentication succeeds. Since the length field is 4\n> > bytes, this means an implementation may have to buffer up to 4 GiB\n> > of data _per connection_ before it can authenticate the length\n> > field. If the outter length field were reduced to 2 or 3 bytes, the\n> > unauthenticated buffering requirements drop to 64 KiB and 16 MiB\n> > respectively. Inner messages already have their own length, so they\n> > can span multiple encrypted blocks without other changes. This will\n> > increase the bandwidth requirements when the size of a single\n> > message exceeds 64 KiB or 16 MiB, since it will require multiple\n> > authentication tags for that message. I think an additional 16\n> > bytes per 16 MiB seems like a good tradeoff.\n> >   \n> \n> Good point.\n> I have mentioned this now in the BIP but I think the BIP should allow\n> message > 16 MiB.\n> I leave the max. message length up to the implementation while keeping\n> the 4 byte length on the protocol level.\n\nI expect the implementation defined max size to work (SSH 2.0 does this\nafter all), but I want to make sure my suggestion is understood\ncompletely.\n\nThere is a length field for the encrypted data, and length field(s)\ninside of the encrypted data to indicate the length of the plaintext\nBitcoin messages. I am suggesting that the outter (encrypted) length\nfield be reduced, which will _not limit_ the length of Bitcoin\nmessages. For example, if a 1 GiB Bitcoin message needed to be sent\nand the encrypted length field was 3 bytes - the sender is forced to\nsend a minimum of 64 MACs for this message. The tradeoff is allowing\nthe receiver to detect malformed data sooner and have a lower max\nbuffering window **against** slightly higher bandwidth and CPU\nrequirements due to the additional headers+MACs (the CPU requirements\nshould primarily be in \"finalizing each Poly1305\").\n\nAn alternative way to think about the suggestion is tunnelling Bitcoin\nmessages over TLS or SSH. TLS 1.2 has a 2-byte length field and SSH 2.0\na 4-byte length field, but neither prevents larger Bitcoin messages from\nbeing tunnelled; the lengths are independent.\n\n[...]\n\n> \n> </jonas>\n> \n\nLee"
            },
            {
                "author": "Jonas Schnelli",
                "date": "2016-05-25T09:36:24",
                "message_text_only": ">> Good point.\n>> I have mentioned this now in the BIP but I think the BIP should allow\n>> message > 16 MiB.\n>> I leave the max. message length up to the implementation while keeping\n>> the 4 byte length on the protocol level.\n> \n> I expect the implementation defined max size to work (SSH 2.0 does this\n> after all), but I want to make sure my suggestion is understood\n> completely.\n> \n> There is a length field for the encrypted data, and length field(s)\n> inside of the encrypted data to indicate the length of the plaintext\n> Bitcoin messages. I am suggesting that the outter (encrypted) length\n> field be reduced, which will _not limit_ the length of Bitcoin\n> messages. For example, if a 1 GiB Bitcoin message needed to be sent\n> and the encrypted length field was 3 bytes - the sender is forced to\n> send a minimum of 64 MACs for this message. The tradeoff is allowing\n> the receiver to detect malformed data sooner and have a lower max\n> buffering window **against** slightly higher bandwidth and CPU\n> requirements due to the additional headers+MACs (the CPU requirements\n> should primarily be in \"finalizing each Poly1305\").\n\nOkay. Got your point.\nThe current BIPs assumption is that an encrypted package/message can\ncontain 1..n bitcoin messages (a single bitcoin message distributed over\nmultiple encrypted messages/packages was not specified).\n\nBut right, this could make sense.\nLet me think this through....\n\n> An alternative way to think about the suggestion is tunnelling Bitcoin\n> messages over TLS or SSH. TLS 1.2 has a 2-byte length field and SSH 2.0\n> a 4-byte length field, but neither prevents larger Bitcoin messages from\n> being tunnelled; the lengths are independent.\n\nTLS/SSH tunneling is already possible with third party software like\nstunnel.\nAlso there is promising projects that would encrypt the traffic \"on a\ndeeper layer\" (see CurveCP).\n\nI think what we want is a simple, openssl-independent traffic encryption\nbuilt into the core p2p layer.\n\nIMO the risk of screwing up the implementation is moderate.\n\nThe implementation is not utterly-complex:\nOpenSSH chacha20:\nhttps://github.com/openssh/openssh-portable/blob/0235a5fa67fcac51adb564cba69011a535f86f6b/chacha.c\n\nChacha20-Poly1305:\nhttps://github.com/openssh/openssh-portable/blob/0235a5fa67fcac51adb564cba69011a535f86f6b/cipher-chachapoly.c\n\nSure. Before an implementation will be deployed to the endusers it will\nrequire intense cryptoanalysis first.\n\n</jonas>\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 819 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160525/3aed6567/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "p2p authentication and encryption BIPs",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Lee Clagett",
                "Jonas Schnelli"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 11253
        }
    },
    {
        "title": "[bitcoin-dev] RFC for BIP: Best Practices for Heterogeneous Input Script Transactions",
        "thread_messages": [
            {
                "author": "Kristov Atlas",
                "date": "2016-05-19T04:18:15",
                "message_text_only": "I've updated the language of the BIP. New version:\n\n<pre>\n  BIP: TBD\n  Title: Best Practices for Heterogeneous Input Script Transactions\n  Author: Kristov Atlas <kristov at openbitcoinprivacyproject.org>\n  Status: Draft\n  Type: Informational\n  Created: 2016-02-10\n</pre>\n\n==Abstract==\n\nThe privacy of Bitcoin users with respect to graph analysis is reduced when\na transaction is created that contains inputs composed from different\nscripts. However, creating such transactions is often unavoidable.\n\nThis document proposes a set of best practice guidelines which minimize the\nadverse privacy consequences of such unavoidable transaction situations\nwhile simultaneously maximising the effectiveness of user protection\nprotocols.\n\n==Copyright==\n\nThis BIP is in the public domain.\n\n==Definitions==\n\n* '''Heterogenous input script transaction (HIT)''': A transaction\ncontaining multiple inputs where not all inputs have identical scripts\n(e.g. a transaction spending from more than one Bitcoin address)\n* '''Unavoidable heterogenous input script transaction''': An HIT created\nas a result of a user\u2019s desire to create a new output with a value larger\nthan the value of his wallet's largest existing unspent output\n* '''Intentional heterogenous input script transaction''': An HIT created\nas part of a user protection protocol for reducing uncontrolled disclosure\nof personally-identifying information (PII)\n\n==Motivations==\n\nThe recommendations in this document are designed to accomplish three goals:\n\n# Maximise the effectiveness of user-protecting protocols: Users may find\nthat protection protocols are counterproductive if such transactions have a\ndistinctive fingerprint which renders them ineffective.\n# Minimise the adverse consequences of unavoidable heterogenous input\ntransactions: If unavoidable HITs are indistinguishable from intentional\nHITs, a user creating an unavoidable HIT benefits from ambiguity with\nrespect to graph analysis.\n# Limiting the effect on UTXO set growth: To date, non-standardized\nintentional HITs tend to increase the network's UTXO set with each\ntransaction; this standard attempts to minimize this effect by\nstandardizing unavoidable and intentional HITs to limit UTXO set growth.\n\nIn order to achieve these goals, this specification proposes a set of best\npractices for heterogenous input script transaction creation. These\npractices accommodate all applicable requirements of both intentional and\nunavoidable HITs while maximising the effectiveness of both in terms of\npreventing uncontrolled disclosure of PII.\n\nIn order to achieve this, two forms of HIT are proposed: Standard form and\nalternate form.\n\n==Standard form heterogenous input script transaction==\n\n===Rules===\n\nAn HIT is Standard form if it adheres to all of the following rules:\n\n# The number of unique output scripts must be equal to the number of unique\ninputs scripts (irrespective of the number of inputs and outputs).\n# All output scripts must be unique.\n# At least one pair of outputs must be of equal value.\n# The largest output in the transaction is a member of a set containing at\nleast two identically-sized outputs.\n\n===Rationale===\n\nThe requirement for equal numbers of unique input/output scripts instead of\nequal number of inputs/outputs accommodates user-protecting UTXO selection\nbehavior. Wallets may contain spendable outputs with identical scripts due\nto intentional or accidental address reuse, or due to dusting attacks. In\norder to minimise the adverse consequences of address reuse, any time a\nUTXO is included in a transaction as an input, all UTXOs with the same\nspending script should also be included in the transaction.\n\nThe requirement that all output scripts are unique prevents address reuse.\nRestricting the number of outputs to the number of unique input scripts\nprevents this policy from growing the network\u2019s UTXO set. A standard form\nHIT transaction will always have a number of inputs greater than or equal\nto the number of outputs.\n\nThe requirement for at least one pair of outputs in an intentional HIT to\nbe of equal value results in optimal behavior, and causes intentional HITs\nto resemble unavoidable HITs.\n\n==Alternate form heterogenous input script transactions==\n\nThe formation of a standard form HIT is not possible in the following cases:\n\n# The HIT is unavoidable, and the user\u2019s wallet contains an insufficient\nnumber or size of UTXOs to create a standard form HIT.\n# The user wishes to reduce the number of utxos in their wallet, and does\nnot have any sets of utxos with identical scripts.\n\nWhen one of the following cases exist, a compliant implementation may\ncreate an alternate form HIT by constructing a transaction as follows:\n\n===Procedure===\n\n# Find the smallest combination of inputs whose value is at least the value\nof the desired spend.\n## Add these inputs to the transaction.\n## Add a spend output to the transaction.\n## Add a change output to the transaction containing the difference between\nthe current set of inputs and the desired spend.\n# Repeat step 1 to create a second spend output and change output.\n# Adjust the change outputs as necessary to pay the desired transaction fee.\n\nClients which create intentional HITs must have the capability to form\nalternate form HITs, and must do so for a non-zero fraction of the\ntransactions they create.\n\n==Non-compliant heterogenous input script transactions==\n\nIf a user wishes to create an output that is larger than half the total\nsize of their spendable outputs, or if their inputs are not distributed in\na manner in which the alternate form procedure can be completed, then the\nuser can not create a transaction which is compliant with this procedure.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160519/4f01587b/attachment-0001.html>"
            },
            {
                "author": "T. DeV D",
                "date": "2016-05-23T17:44:05",
                "message_text_only": "ACK\n\nWe have already started work on Coinjoin simulated transactions and are\nvery interested in working on an implementation of this proposal with a\nview towards making wallet footprints less identifiable.\n\nOn Thu, May 19, 2016 at 5:18 AM, Kristov Atlas via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> I've updated the language of the BIP. New version:\n>\n> <pre>\n>   BIP: TBD\n>   Title: Best Practices for Heterogeneous Input Script Transactions\n>   Author: Kristov Atlas <kristov at openbitcoinprivacyproject.org>\n>   Status: Draft\n>   Type: Informational\n>   Created: 2016-02-10\n> </pre>\n>\n> ==Abstract==\n>\n> The privacy of Bitcoin users with respect to graph analysis is reduced\n> when a transaction is created that contains inputs composed from different\n> scripts. However, creating such transactions is often unavoidable.\n>\n> This document proposes a set of best practice guidelines which minimize\n> the adverse privacy consequences of such unavoidable transaction situations\n> while simultaneously maximising the effectiveness of user protection\n> protocols.\n>\n> ==Copyright==\n>\n> This BIP is in the public domain.\n>\n> ==Definitions==\n>\n> * '''Heterogenous input script transaction (HIT)''': A transaction\n> containing multiple inputs where not all inputs have identical scripts\n> (e.g. a transaction spending from more than one Bitcoin address)\n> * '''Unavoidable heterogenous input script transaction''': An HIT created\n> as a result of a user\u2019s desire to create a new output with a value larger\n> than the value of his wallet's largest existing unspent output\n> * '''Intentional heterogenous input script transaction''': An HIT created\n> as part of a user protection protocol for reducing uncontrolled disclosure\n> of personally-identifying information (PII)\n>\n> ==Motivations==\n>\n> The recommendations in this document are designed to accomplish three\n> goals:\n>\n> # Maximise the effectiveness of user-protecting protocols: Users may find\n> that protection protocols are counterproductive if such transactions have a\n> distinctive fingerprint which renders them ineffective.\n> # Minimise the adverse consequences of unavoidable heterogenous input\n> transactions: If unavoidable HITs are indistinguishable from intentional\n> HITs, a user creating an unavoidable HIT benefits from ambiguity with\n> respect to graph analysis.\n> # Limiting the effect on UTXO set growth: To date, non-standardized\n> intentional HITs tend to increase the network's UTXO set with each\n> transaction; this standard attempts to minimize this effect by\n> standardizing unavoidable and intentional HITs to limit UTXO set growth.\n>\n> In order to achieve these goals, this specification proposes a set of best\n> practices for heterogenous input script transaction creation. These\n> practices accommodate all applicable requirements of both intentional and\n> unavoidable HITs while maximising the effectiveness of both in terms of\n> preventing uncontrolled disclosure of PII.\n>\n> In order to achieve this, two forms of HIT are proposed: Standard form and\n> alternate form.\n>\n> ==Standard form heterogenous input script transaction==\n>\n> ===Rules===\n>\n> An HIT is Standard form if it adheres to all of the following rules:\n>\n> # The number of unique output scripts must be equal to the number of\n> unique inputs scripts (irrespective of the number of inputs and outputs).\n> # All output scripts must be unique.\n> # At least one pair of outputs must be of equal value.\n> # The largest output in the transaction is a member of a set containing at\n> least two identically-sized outputs.\n>\n> ===Rationale===\n>\n> The requirement for equal numbers of unique input/output scripts instead\n> of equal number of inputs/outputs accommodates user-protecting UTXO\n> selection behavior. Wallets may contain spendable outputs with identical\n> scripts due to intentional or accidental address reuse, or due to dusting\n> attacks. In order to minimise the adverse consequences of address reuse,\n> any time a UTXO is included in a transaction as an input, all UTXOs with\n> the same spending script should also be included in the transaction.\n>\n> The requirement that all output scripts are unique prevents address reuse.\n> Restricting the number of outputs to the number of unique input scripts\n> prevents this policy from growing the network\u2019s UTXO set. A standard form\n> HIT transaction will always have a number of inputs greater than or equal\n> to the number of outputs.\n>\n> The requirement for at least one pair of outputs in an intentional HIT to\n> be of equal value results in optimal behavior, and causes intentional HITs\n> to resemble unavoidable HITs.\n>\n> ==Alternate form heterogenous input script transactions==\n>\n> The formation of a standard form HIT is not possible in the following\n> cases:\n>\n> # The HIT is unavoidable, and the user\u2019s wallet contains an insufficient\n> number or size of UTXOs to create a standard form HIT.\n> # The user wishes to reduce the number of utxos in their wallet, and does\n> not have any sets of utxos with identical scripts.\n>\n> When one of the following cases exist, a compliant implementation may\n> create an alternate form HIT by constructing a transaction as follows:\n>\n> ===Procedure===\n>\n> # Find the smallest combination of inputs whose value is at least the\n> value of the desired spend.\n> ## Add these inputs to the transaction.\n> ## Add a spend output to the transaction.\n> ## Add a change output to the transaction containing the difference\n> between the current set of inputs and the desired spend.\n> # Repeat step 1 to create a second spend output and change output.\n> # Adjust the change outputs as necessary to pay the desired transaction\n> fee.\n>\n> Clients which create intentional HITs must have the capability to form\n> alternate form HITs, and must do so for a non-zero fraction of the\n> transactions they create.\n>\n> ==Non-compliant heterogenous input script transactions==\n>\n> If a user wishes to create an output that is larger than half the total\n> size of their spendable outputs, or if their inputs are not distributed in\n> a manner in which the alternate form procedure can be completed, then the\n> user can not create a transaction which is compliant with this procedure.\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n\n\n-- \n\ndev at samouraiwallet.com\n\nPGP public key fingerprint:\n\nED1A 1280 DEFC A603 14CD  15BF 72B5 BACD FEDF 39D7\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160523/4f125450/attachment.html>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2016-05-26T00:00:37",
                "message_text_only": "On Thursday, May 19, 2016 4:18:15 AM Kristov Atlas via bitcoin-dev wrote:\n>   BIP: TBD\n\nThis is assigned BIP 126.\n\n> * '''Heterogenous input script transaction (HIT)''': A transaction\n> containing multiple inputs where not all inputs have identical scripts\n> (e.g. a transaction spending from more than one Bitcoin address)\n\nTransactions are never from Bitcoin addresses, and inputs almost never have \nidentical scripts (although the UTXOs they are spending often do).\n\nLuke"
            }
        ],
        "thread_summary": {
            "title": "RFC for BIP: Best Practices for Heterogeneous Input Script Transactions",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "T. DeV D",
                "Luke Dashjr",
                "Kristov Atlas"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 13048
        }
    },
    {
        "title": "[bitcoin-dev] BIP: OP_PRANDOM",
        "thread_messages": [
            {
                "author": "Matthew Roberts",
                "date": "2016-05-20T10:57:46",
                "message_text_only": "== Background\n\nOP_PRANDOM is a new op code for Bitcoin that pushes a pseudo-random number\nto the top of the stack based on the next N block hashes. The source of the\npseudo-random number is defined as the XOR of the next N block hashes after\nconfirmation of a transaction containing the OP_PRANDOM encumbered output.\nWhen a transaction containing the op code is redeemed, the transaction\nreceives a pseudo-random number based on the next N block hashes after\nconfirmation of the redeeming input. This means that transactions are also\neffectively locked until at least N new blocks have been found.\n\n\n== Rational\n\nMaking deterministic, verifiable, and trustless pseudo-random numbers\navailable for use in the Script language makes it possible to support a\nnumber of new smart contracts. OP_PRANDOM would allow for the simplistic\ncreation of purely decentralized lotteries without the need for complicated\nmulti-party computation protocols. Gambling is also another possibility as\ncontracts can be written based on hashed commitments, with the winner\nchosen if a given commitment is closest to the pseudo-random number.\nOP_PRANDOM could also be used for cryptographically secure virtual asset\nmanagement such as rewards in video games and in other applications.\n\n\n== Security\n\nPay-to-script-hash can be used to protect the details of contracts that use\nOP_PRANDOM from the prying eyes of miners. However, since there is also a\nnon-zero risk that a participant in a contract may attempt to bribe a miner\nthe inclusion of multiple block hashes as a source of randomness is a must.\nEvery miner would effectively need to be bribed to ensure control over the\nresults of the random numbers, which is already very unlikely. The risk\napproaches zero as N goes up.\n\nThere is however another issue: since the random numbers are based on a\nchanging blockchain, its problematic to use the next immediate block hashes\nbefore the state is \u201cfinal.\u201d A safe default for accepting the blockchain\nstate as final would need to be agreed upon beforehand, otherwise you could\nhave multiple random outputs becoming valid simultaneously on different\nforks.\n\nA simple solution is not to reveal any commitments before the chain height\nsurpasses a certain point but this might not be an issue since only one\nversion will eventually make it into the final chain anyway -- though it is\nsomething to think about.\n\n\n== Outro\n\nI'm not sure how secure this is or whether its a good idea so posting it\nhere for feedback\n\nThoughts?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160520/951fcc41/attachment.html>"
            },
            {
                "author": "Johnson Lau",
                "date": "2016-05-20T11:34:03",
                "message_text_only": "Using the hash of multiple blocks does not make it any safer. The miner of the last block always determines the results, by knowing the hashes of all previous blocks.\n\n> \n> == Security\n> Pay-to-script-hash can be used to protect the details of contracts that use OP_PRANDOM from the prying eyes of miners. However, since there is also a non-zero risk that a participant in a contract may attempt to bribe a miner the inclusion of multiple block hashes as a source of randomness is a must. Every miner would effectively need to be bribed to ensure control over the results of the random numbers, which is already very unlikely. The risk approaches zero as N goes up.\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160520/18fa58d6/attachment.html>"
            },
            {
                "author": "James MacWhyte",
                "date": "2016-05-20T14:30:52",
                "message_text_only": "Matthew,\n\nOther than gambling, do you have any specific examples of how this could be\nuseful?\n\nOn Fri, May 20, 2016, 20:34 Johnson Lau via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Using the hash of multiple blocks does not make it any safer. The miner of\n> the last block always determines the results, by knowing the hashes of all\n> previous blocks.\n>\n>\n> == Security\n>\n> Pay-to-script-hash can be used to protect the details of contracts that\n> use OP_PRANDOM from the prying eyes of miners. However, since there is also\n> a non-zero risk that a participant in a contract may attempt to bribe a\n> miner the inclusion of multiple block hashes as a source of randomness is a\n> must. Every miner would effectively need to be bribed to ensure control\n> over the results of the random numbers, which is already very unlikely. The\n> risk approaches zero as N goes up.\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160520/5a5078fc/attachment.html>"
            },
            {
                "author": "Matthew Roberts",
                "date": "2016-05-20T15:05:36",
                "message_text_only": "Good point, to be honest. Maybe there's a better way to combine the block\nhashes like taking the first N bits from each block hash to produce a\nsingle number but the direction that this is going in doesn't seem ideal.\n\nI just asked a friend about this problem and he mentioned using the hash of\nthe proof of work hash as part of the number so you have to throw away a\nvalid POW if it doesn't give you the hash you want. I suppose its possible\nto make it infinitely expensive to manipulate the number but I can't think\nof anything better than that for now.\n\nI need to sleep on this for now but let me know if anyone has any better\nideas.\n\n\n\nOn Fri, May 20, 2016 at 6:34 AM, Johnson Lau <jl2012 at xbt.hk> wrote:\n\n> Using the hash of multiple blocks does not make it any safer. The miner of\n> the last block always determines the results, by knowing the hashes of all\n> previous blocks.\n>\n>\n> == Security\n>\n> Pay-to-script-hash can be used to protect the details of contracts that\n> use OP_PRANDOM from the prying eyes of miners. However, since there is also\n> a non-zero risk that a participant in a contract may attempt to bribe a\n> miner the inclusion of multiple block hashes as a source of randomness is a\n> must. Every miner would effectively need to be bribed to ensure control\n> over the results of the random numbers, which is already very unlikely. The\n> risk approaches zero as N goes up.\n>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160520/6d2ee1d5/attachment.html>"
            },
            {
                "author": "Eric Martindale",
                "date": "2016-05-20T18:32:07",
                "message_text_only": "Matthew,\n\nYou should take a look at OP_DETERMINISTICRANDOM [1] from the Elements\nProject.  It aims to achieve a similar goal.\n\nCode is in the `alpha` branch [2].\n\n[1]: https://www.elementsproject.org/elements/opcodes/\n[2]:\nhttps://github.com/ElementsProject/elements/blob/alpha/src/script/interpreter.cpp#L1252-L1305\n\nOn Fri, May 20, 2016 at 8:29 AM Matthew Roberts via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Good point, to be honest. Maybe there's a better way to combine the block\n> hashes like taking the first N bits from each block hash to produce a\n> single number but the direction that this is going in doesn't seem ideal.\n>\n> I just asked a friend about this problem and he mentioned using the hash\n> of the proof of work hash as part of the number so you have to throw away a\n> valid POW if it doesn't give you the hash you want. I suppose its possible\n> to make it infinitely expensive to manipulate the number but I can't think\n> of anything better than that for now.\n>\n> I need to sleep on this for now but let me know if anyone has any better\n> ideas.\n>\n>\n>\n> On Fri, May 20, 2016 at 6:34 AM, Johnson Lau <jl2012 at xbt.hk> wrote:\n>\n>> Using the hash of multiple blocks does not make it any safer. The miner\n>> of the last block always determines the results, by knowing the hashes of\n>> all previous blocks.\n>>\n>>\n>> == Security\n>>\n>> Pay-to-script-hash can be used to protect the details of contracts that\n>> use OP_PRANDOM from the prying eyes of miners. However, since there is also\n>> a non-zero risk that a participant in a contract may attempt to bribe a\n>> miner the inclusion of multiple block hashes as a source of randomness is a\n>> must. Every miner would effectively need to be bribed to ensure control\n>> over the results of the random numbers, which is already very unlikely. The\n>> risk approaches zero as N goes up.\n>>\n>>\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160520/b5dd03dd/attachment.html>"
            },
            {
                "author": "Jeremy",
                "date": "2016-05-22T13:30:53",
                "message_text_only": "nack -- not secure.\n\nOP_PRANDOM also adds extra validation overhead on a block potentially\ncomposed of transactions all spending an OP_PRANDOM output from all\ndifferent blocks.\n\nI do agree that random numbers are highly desirable though.\n\nI think it would be much better for these use cases to add OP_XOR back and\nthen use something like Blum's fair coin-flipping over the phone. OP_XOR\nmay have other uses too.\n\nI have a write-up from a while back which does Blum's without OP_XOR using\nOP_SIZE for off-chain probabilistic payments if anyone is interested. No\nfork needed, but of course it is more limited and broken in a number of\nways.\n\n(sorry to those of you seeing this twice, my first email bounced the list)\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n\nOn Fri, May 20, 2016 at 2:32 PM, Eric Martindale via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Matthew,\n>\n> You should take a look at OP_DETERMINISTICRANDOM [1] from the Elements\n> Project.  It aims to achieve a similar goal.\n>\n> Code is in the `alpha` branch [2].\n>\n> [1]: https://www.elementsproject.org/elements/opcodes/\n> [2]:\n> https://github.com/ElementsProject/elements/blob/alpha/src/script/interpreter.cpp#L1252-L1305\n>\n> On Fri, May 20, 2016 at 8:29 AM Matthew Roberts via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Good point, to be honest. Maybe there's a better way to combine the block\n>> hashes like taking the first N bits from each block hash to produce a\n>> single number but the direction that this is going in doesn't seem ideal.\n>>\n>> I just asked a friend about this problem and he mentioned using the hash\n>> of the proof of work hash as part of the number so you have to throw away a\n>> valid POW if it doesn't give you the hash you want. I suppose its possible\n>> to make it infinitely expensive to manipulate the number but I can't think\n>> of anything better than that for now.\n>>\n>> I need to sleep on this for now but let me know if anyone has any better\n>> ideas.\n>>\n>>\n>>\n>> On Fri, May 20, 2016 at 6:34 AM, Johnson Lau <jl2012 at xbt.hk> wrote:\n>>\n>>> Using the hash of multiple blocks does not make it any safer. The miner\n>>> of the last block always determines the results, by knowing the hashes of\n>>> all previous blocks.\n>>>\n>>>\n>>> == Security\n>>>\n>>> Pay-to-script-hash can be used to protect the details of contracts that\n>>> use OP_PRANDOM from the prying eyes of miners. However, since there is also\n>>> a non-zero risk that a participant in a contract may attempt to bribe a\n>>> miner the inclusion of multiple block hashes as a source of randomness is a\n>>> must. Every miner would effectively need to be bribed to ensure control\n>>> over the results of the random numbers, which is already very unlikely. The\n>>> risk approaches zero as N goes up.\n>>>\n>>>\n>>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160522/22a4184e/attachment.html>"
            },
            {
                "author": "Sergio Demian Lerner",
                "date": "2016-05-24T14:30:30",
                "message_text_only": "Bitcoin Beacon paper relevant here\n\nBasically is suggest using deciding a random bit on the majority 1s or 0s\nof lsb bits taken from last block hashes.\n\nIddo Bentov\u2217 Technion, Ariel Gabizon,  David Zuckerman\n\nWe examine a protocol \u03c0beacon that outputs unpredictable and publicly\nverifiable randomness, meaning that the output is unknown at the time that\n\u03c0beacon starts, yet everyone can verify that the output is close to uniform\nafter \u03c0beacon terminates. We show that \u03c0beacon can be instantiated via\nBitcoin under sensible assumptions; in particular we consider an adversary\nwith an arbitrarily large initial budget who may not operate at a loss\nindefinitely.\nIn case the adversary has an infinite budget, we provide an impossibility\nresult that stems from the similarity between the Bitcoin model and\nSantha-Vazirani sources. We also give a hybrid protocol that combines\ntrusted parties and a Bitcoin-based beacon.\n\nOn Sun, May 22, 2016 at 10:30 AM, Jeremy via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> nack -- not secure.\n>\n> OP_PRANDOM also adds extra validation overhead on a block potentially\n> composed of transactions all spending an OP_PRANDOM output from all\n> different blocks.\n>\n> I do agree that random numbers are highly desirable though.\n>\n> I think it would be much better for these use cases to add OP_XOR back and\n> then use something like Blum's fair coin-flipping over the phone. OP_XOR\n> may have other uses too.\n>\n> I have a write-up from a while back which does Blum's without OP_XOR using\n> OP_SIZE for off-chain probabilistic payments if anyone is interested. No\n> fork needed, but of course it is more limited and broken in a number of\n> ways.\n>\n> (sorry to those of you seeing this twice, my first email bounced the list)\n>\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n> <https://twitter.com/JeremyRubin>\n>\n> On Fri, May 20, 2016 at 2:32 PM, Eric Martindale via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Matthew,\n>>\n>> You should take a look at OP_DETERMINISTICRANDOM [1] from the Elements\n>> Project.  It aims to achieve a similar goal.\n>>\n>> Code is in the `alpha` branch [2].\n>>\n>> [1]: https://www.elementsproject.org/elements/opcodes/\n>> [2]:\n>> https://github.com/ElementsProject/elements/blob/alpha/src/script/interpreter.cpp#L1252-L1305\n>>\n>> On Fri, May 20, 2016 at 8:29 AM Matthew Roberts via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> Good point, to be honest. Maybe there's a better way to combine the\n>>> block hashes like taking the first N bits from each block hash to produce a\n>>> single number but the direction that this is going in doesn't seem ideal.\n>>>\n>>> I just asked a friend about this problem and he mentioned using the hash\n>>> of the proof of work hash as part of the number so you have to throw away a\n>>> valid POW if it doesn't give you the hash you want. I suppose its possible\n>>> to make it infinitely expensive to manipulate the number but I can't think\n>>> of anything better than that for now.\n>>>\n>>> I need to sleep on this for now but let me know if anyone has any better\n>>> ideas.\n>>>\n>>>\n>>>\n>>> On Fri, May 20, 2016 at 6:34 AM, Johnson Lau <jl2012 at xbt.hk> wrote:\n>>>\n>>>> Using the hash of multiple blocks does not make it any safer. The miner\n>>>> of the last block always determines the results, by knowing the hashes of\n>>>> all previous blocks.\n>>>>\n>>>>\n>>>> == Security\n>>>>\n>>>> Pay-to-script-hash can be used to protect the details of contracts that\n>>>> use OP_PRANDOM from the prying eyes of miners. However, since there is also\n>>>> a non-zero risk that a participant in a contract may attempt to bribe a\n>>>> miner the inclusion of multiple block hashes as a source of randomness is a\n>>>> must. Every miner would effectively need to be bribed to ensure control\n>>>> over the results of the random numbers, which is already very unlikely. The\n>>>> risk approaches zero as N goes up.\n>>>>\n>>>>\n>>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160524/2251ddba/attachment.html>"
            },
            {
                "author": "Sergio Demian Lerner",
                "date": "2016-05-24T14:36:35",
                "message_text_only": "Missing link to paper: https://arxiv.org/abs/1605.04559\n\nAnother relevant paper:\n\nOn Bitcoin as a public randomness source\nJoseph Bonneau, Jeremy Clark, and Steven Goldfeder\nhttps://eprint.iacr.org/2015/1015.pdf\n\nOn Tue, May 24, 2016 at 11:30 AM, Sergio Demian Lerner <\nsergio.d.lerner at gmail.com> wrote:\n\n> Bitcoin Beacon paper relevant here\n>\n> Basically is suggest using deciding a random bit on the majority 1s or 0s\n> of lsb bits taken from last block hashes.\n>\n> Iddo Bentov\u2217 Technion, Ariel Gabizon,  David Zuckerman\n>\n> We examine a protocol \u03c0beacon that outputs unpredictable and publicly\n> verifiable randomness, meaning that the output is unknown at the time that\n> \u03c0beacon starts, yet everyone can verify that the output is close to uniform\n> after \u03c0beacon terminates. We show that \u03c0beacon can be instantiated via\n> Bitcoin under sensible assumptions; in particular we consider an adversary\n> with an arbitrarily large initial budget who may not operate at a loss\n> indefinitely.\n> In case the adversary has an infinite budget, we provide an impossibility\n> result that stems from the similarity between the Bitcoin model and\n> Santha-Vazirani sources. We also give a hybrid protocol that combines\n> trusted parties and a Bitcoin-based beacon.\n>\n> On Sun, May 22, 2016 at 10:30 AM, Jeremy via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> nack -- not secure.\n>>\n>> OP_PRANDOM also adds extra validation overhead on a block potentially\n>> composed of transactions all spending an OP_PRANDOM output from all\n>> different blocks.\n>>\n>> I do agree that random numbers are highly desirable though.\n>>\n>> I think it would be much better for these use cases to add OP_XOR back\n>> and then use something like Blum's fair coin-flipping over the phone.\n>> OP_XOR may have other uses too.\n>>\n>> I have a write-up from a while back which does Blum's without OP_XOR\n>> using OP_SIZE for off-chain probabilistic payments if anyone is interested.\n>> No fork needed, but of course it is more limited and broken in a number of\n>> ways.\n>>\n>> (sorry to those of you seeing this twice, my first email bounced the list)\n>>\n>> --\n>> @JeremyRubin <https://twitter.com/JeremyRubin>\n>> <https://twitter.com/JeremyRubin>\n>>\n>> On Fri, May 20, 2016 at 2:32 PM, Eric Martindale via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> Matthew,\n>>>\n>>> You should take a look at OP_DETERMINISTICRANDOM [1] from the Elements\n>>> Project.  It aims to achieve a similar goal.\n>>>\n>>> Code is in the `alpha` branch [2].\n>>>\n>>> [1]: https://www.elementsproject.org/elements/opcodes/\n>>> [2]:\n>>> https://github.com/ElementsProject/elements/blob/alpha/src/script/interpreter.cpp#L1252-L1305\n>>>\n>>> On Fri, May 20, 2016 at 8:29 AM Matthew Roberts via bitcoin-dev <\n>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>>> Good point, to be honest. Maybe there's a better way to combine the\n>>>> block hashes like taking the first N bits from each block hash to produce a\n>>>> single number but the direction that this is going in doesn't seem ideal.\n>>>>\n>>>> I just asked a friend about this problem and he mentioned using the\n>>>> hash of the proof of work hash as part of the number so you have to throw\n>>>> away a valid POW if it doesn't give you the hash you want. I suppose its\n>>>> possible to make it infinitely expensive to manipulate the number but I\n>>>> can't think of anything better than that for now.\n>>>>\n>>>> I need to sleep on this for now but let me know if anyone has any\n>>>> better ideas.\n>>>>\n>>>>\n>>>>\n>>>> On Fri, May 20, 2016 at 6:34 AM, Johnson Lau <jl2012 at xbt.hk> wrote:\n>>>>\n>>>>> Using the hash of multiple blocks does not make it any safer. The\n>>>>> miner of the last block always determines the results, by knowing the\n>>>>> hashes of all previous blocks.\n>>>>>\n>>>>>\n>>>>> == Security\n>>>>>\n>>>>> Pay-to-script-hash can be used to protect the details of contracts\n>>>>> that use OP_PRANDOM from the prying eyes of miners. However, since there is\n>>>>> also a non-zero risk that a participant in a contract may attempt to bribe\n>>>>> a miner the inclusion of multiple block hashes as a source of randomness is\n>>>>> a must. Every miner would effectively need to be bribed to ensure control\n>>>>> over the results of the random numbers, which is already very unlikely. The\n>>>>> risk approaches zero as N goes up.\n>>>>>\n>>>>>\n>>>>>\n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>\n>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160524/ce08f20a/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "BIP: OP_PRANDOM",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeremy",
                "Johnson Lau",
                "Eric Martindale",
                "Sergio Demian Lerner",
                "James MacWhyte",
                "Matthew Roberts"
            ],
            "messages_count": 8,
            "total_messages_chars_count": 21986
        }
    },
    {
        "title": "[bitcoin-dev] BIP Number Request: Open Asset",
        "thread_messages": [
            {
                "author": "Nicolas Dorier",
                "date": "2016-05-26T02:50:26",
                "message_text_only": "Open Asset is a simple and well known colored coin protocol made by Flavien\nCharlon, which has been around for more than two years ago.\nOpen Asset is OP_RETURN to store coin's color. Since then, the only\nmodification to the protocol has been for allowing OA data to be into any\npush into an OP_RETURN.\n\nThe protocol is here:\nhttps://github.com/OpenAssets/open-assets-protocol/blob/master/specification.mediawiki\n\nI asked to Flavien Charlon if he was OK if I submit the protocol to the\nmailing list before posting.\n\nAdditional BIP number might be required to cover for example the \"colored\naddress\" format:\nhttps://github.com/OpenAssets/open-assets-protocol/blob/master/address-format.mediawiki\nBut I will do it in a separate request.\n\nHere is the core of the Open Asset specification:\n\n<pre>\n  Title: Open Assets Protocol (OAP/1.0)\n  Author: Flavien Charlon <flavien at charlon.net>\n  Created: 2013-12-12\n</pre>\n\n==Abstract==\n\nThis document describes a protocol used for storing and transferring\ncustom, non-native assets on the Blockchain. Assets are represented by\ntokens called colored coins.\n\nAn issuer would first issue colored coins and associate them with a\nformal or informal promise that he will redeem the coins according to\nterms he has defined. Colored coins can then be transferred using\ntransactions that preserve the quantity of every asset.\n\n==Motivation==\n\nIn the current Bitcoin implementation, outputs represent a quantity of\nBitcoin, secured by an output script. With the Open Assets Protocol,\noutputs can encapsulate a quantity of a user-defined asset on top of\nthat Bitcoin amount.\n\nThere are many applications:\n\n* A company could issue colored coins representing shares. The shares\ncould then be traded frictionlessly through the Bitcoin\ninfrastructure.\n* A bank could issue colored coins backed by a cash reserve. People\ncould withdraw and deposit money in colored coins, and trade those, or\nuse them to pay for goods and services. The Blockchain becomes a\nsystem allowing to transact not only in Bitcoin, but in any currency.\n* Locks on cars or houses could be associated with a particular type\nof colored coins. The door would only open when presented with a\nwallet containing that specific coin.\n\n==Protocol Overview==\n\nOutputs using the Open Assets Protocol to store an asset have two new\ncharacteristics:\n* The '''asset ID''' is a 160 bits hash, used to uniquely identify the\nasset stored on the output.\n* The '''asset quantity''' is an unsigned integer representing how\nmany units of that asset are stored on the output.\n\nThis document describes how the asset ID and asset quantity of an\noutput are calculated.\n\nEach output in the Blockchain can be either colored or uncolored:\n* Uncolored outputs have no asset ID and no asset quantity (they are\nboth undefined).\n* Colored outputs have a strictly positive asset quantity, and a\nnon-null asset ID.\n\nThe ID of an asset is the RIPEMD-160 hash of the SHA-256 hash of the\noutput script referenced by the first input of the transaction that\ninitially issued that asset (<code>script_hash =\nRIPEMD160(SHA256(script))</code>). An issuer can reissue more of an\nalready existing asset as long as they retain the private key for that\nasset ID. Assets on two different outputs can only be mixed together\nif they have the same asset ID.\n\nLike addresses, asset IDs can be represented in base 58. They must use\nversion byte 23 (115 in TestNet3) when represented in base 58. The\nbase 58 representation of an asset ID therefore starts with the\ncharacter 'A' in MainNet.\n\nThe process to generate an asset ID and the matching private key is\ndescribed in the following example:\n# The issuer first generates a private key:\n<code>18E14A7B6A307F426A94F8114701E7C8E774E7F9A47E2C2035DB29A206321725</code>.\n# He calculates the corresponding address:\n<code>16UwLL9Risc3QfPqBUvKofHmBQ7wMtjvM</code>.\n# Next, he builds the Pay-to-PubKey-Hash script associated to that\naddress: <code>OP_DUP OP_HASH160\n010966776006953D5567439E5E39F86A0D273BEE OP_EQUALVERIFY\nOP_CHECKSIG</code>.\n# The script is hashed: <code>36e0ea8e93eaa0285d641305f4c81e563aa570a2</code>\n# Finally, the hash is converted to a base 58 string with checksum\nusing version byte 23:\n<code>ALn3aK1fSuG27N96UGYB1kUYUpGKRhBuBC</code>.\n\nThe private key from the first step is required to issue assets\nidentified by the asset ID\n<code>ALn3aK1fSuG27N96UGYB1kUYUpGKRhBuBC</code>. This acts as a\ndigital signature, and gives the guarantee that nobody else but the\noriginal issuer is able to issue assets identified by this specific\nasset ID.\n\n==Open Assets Transactions==\n\nTransactions relevant to the Open Assets Protocol must have a special\noutput called the marker output. This allows clients to recognize such\ntransactions. Open Assets transactions can be used to issue new\nassets, or transfer ownership of assets.\n\nTransactions that are not recognized as an Open Assets transaction are\nconsidered as having all their outputs uncolored.\n\n===Marker output===\n\nThe marker output can have a zero or non-zero value. The marker output\nstarts with the OP_RETURN opcode, and can be followed by any sequence\nof opcodes, but it must contain a PUSHDATA opcode containing a\nparsable Open Assets marker payload. If multiple parsable PUSHDATA\nopcodes exist in the same output, the first one is used, and the other\nones are ignored.\n\nIf multiple valid marker outputs exist in the same transaction, the\nfirst one is used and the other ones are considered as regular\noutputs. If no valid marker output exists in the transaction, all\noutputs are considered uncolored.\n\nThe payload as defined by the Open Assets protocol has the following format:\n\n{|\n! Field                !! Description !! Size\n|-\n! OAP Marker           || A tag indicating that this transaction is an\nOpen Assets transaction. It is always 0x4f41. || 2 bytes\n|-\n! Version number       || The major revision number of the Open Assets\nProtocol. For this version, it is 1 (0x0100). || 2 bytes\n|-\n! Asset quantity count || A\n[https://en.bitcoin.it/wiki/Protocol_specification#Variable_length_integer\nvar-integer] representing the number of items in the <code>asset\nquantity list</code> field. || 1-9 bytes\n|-\n! Asset quantity list  || A list of zero or more\n[http://en.wikipedia.org/wiki/LEB128 LEB128-encoded] unsigned integers\nrepresenting the asset quantity of every output in order (excluding\nthe marker output). || Variable\n|-\n! Metadata length      || The\n[https://en.bitcoin.it/wiki/Protocol_specification#Variable_length_integer\nvar-integer] encoded length of the <code>metadata</code> field. || 1-9\nbytes\n|-\n! Metadata             || Arbitrary metadata to be associated with\nthis transaction. This can be empty. || Variable\n|}\n\nPossible formats for the <code>metadata</code> field are outside of\nscope of this protocol, and may be described in separate protocol\nspecifications building on top of this one.\n\nThe <code>asset quantity list</code> field is used to determine the\nasset quantity of each output. Each integer is encoded using variable\nlength [http://en.wikipedia.org/wiki/LEB128 LEB128] encoding (also\nused in [https://developers.google.com/protocol-buffers/docs/encoding#varints\nGoogle Protocol Buffers]). If the LEB128-encoded asset quantity of any\noutput exceeds 9 bytes, the marker output is deemed invalid. The\nmaximum valid asset quantity for an output is 2<sup>63</sup> - 1\nunits.\n\nIf the marker output is malformed, it is considered non-parsable.\nCoinbase transactions and transactions with zero inputs cannot have a\nvalid marker output, even if it would be otherwise considered valid.\n\nIf there are less items in the <code>asset quantity list</code> than\nthe number of colorable outputs (all the outputs except the marker\noutput), the outputs in excess receive an asset quantity of zero. If\nthere are more items in the <code>asset quantity list</code> than the\nnumber of colorable outputs, the marker output is deemed invalid. The\nmarker output is always uncolored.\n\nAfter the <code>asset quantity list</code> has been used to assign an\nasset quantity to every output, asset IDs are assigned to outputs.\nOutputs before the marker output are used for asset issuance, and\noutputs after the marker output are used for asset transfer.\n\n====Example====\n\nThis example illustrates how a marker output is decoded. Assuming the\nmarker output is output 1:\n\n    Data in the marker output      Description\n    -----------------------------\n-------------------------------------------------------------------\n    0x6a                           The OP_RETURN opcode.\n    0x10                           The PUSHDATA opcode for a 16 bytes payload.\n    0x4f 0x41                      The Open Assets Protocol tag.\n    0x01 0x00                      Version 1 of the protocol.\n    0x03                           There are 3 items in the asset quantity list.\n    0xac 0x02 0x00 0xe5 0x8e 0x26  The asset quantity list:\n                                   - '0xac 0x02' means output 0 has an\nasset quantity of 300.\n                                   - Output 1 is skipped and has an\nasset quantity of 0\n                                     because it is the marker output.\n                                   - '0x00' means output 2 has an\nasset quantity of 0.\n                                   - '0xe5 0x8e 0x26' means output 3\nhas an asset quantity of 624,485.\n                                   - Outputs after output 3 (if any)\nhave an asset quantity of 0.\n    0x04                           The metadata is 4 bytes long.\n    0x12 0x34 0x56 0x78            Some arbitrary metadata.\n\n===Asset issuance outputs===\n\nAll the outputs before the marker output are used for asset issuance.\n\nAll outputs preceding the marker output and with a non-zero asset\nquantity get assigned the asset ID defined as the RIPEMD-160 hash of\nthe SHA-256 hash of the output script referenced by the first input of\nthe transaction. Outputs that have an asset quantity of zero are\nuncolored.\n\n===Asset transfer outputs===\n\nAll the outputs after the marker output are used for asset transfer.\n\nThe asset IDs of those outputs are determined using a method called\norder-based coloring.\n\nInputs are seen as a sequence of asset units, each having an asset ID.\nSimilarly, outputs are seen as a sequence of asset units to be\nassigned an asset ID. These two sequences are built by taking each\ninput or output in order, each of them adding a number of asset units\nequal to their asset quantity. The process starts with the first input\nof the transaction and the first output after the marker output.\n\nAfter the sequences have been built, the asset ID of every asset unit\nin the input sequence is assigned to the asset unit at the same\nposition in the output sequence until all the asset units in the\noutput sequence have received an asset ID. If there are less asset\nunits in the input sequence than in the output sequence, the marker\noutput is considered invalid.\n\nFinally, for each transfer output, if the asset units forming that\noutput all have the same asset ID, the output gets assigned that asset\nID. If any output is mixing units with more than one distinct asset\nID, the marker output is considered invalid. Outputs with an asset\nquantity of zero are always considered uncolored.\n\n===Example===\n\nThis is an example of an Open Assets transaction.\n\nThe coloring process starts by retrieving the asset quantities and\nasset IDs of the outputs referenced by each input of the transaction.\nThen, the marker output is identified. In this example, it is output\n2, and the <code>asset quantity list</code> field contains the\nfollowing values:\n\n    0, 10, 6, 0, 7, 3\n\nThis list is used to assign asset quantities to outputs.\n\n\n    Inputs                          Outputs - Initial state\nOutputs - Final result\n    =============================   =============================\n=============================\n    Input 0                         Output 0 (Issuance)\nOutput 0 (Issuance)\n      Asset quantity:     3           Asset quantity:     0\nAsset quantity:     <NULL>\n      Asset ID:           A1          Asset ID:\nAsset ID:           <NULL>\n    -----------------------------   -----------------------------\n-----------------------------\n    Input 1                         Output 1 (Issuance)\nOutput 1 (Issuance)\n      Asset quantity:     2           Asset quantity:     10\nAsset quantity:     10\n      Asset ID:           A1          Asset ID:\nAsset ID:           H\n    -----------------------------   -----------------------------\n-----------------------------\n    Input 2                         Output 2 (Marker)\nOutput 2 (Marker)\n      Asset quantity:     <NULL>      Asset quantity:     <NULL>\nAsset quantity:     <NULL>\n      Asset ID:           <NULL>      Asset ID:           <NULL>\nAsset ID:           <NULL>\n    -----------------------------   -----------------------------\n-----------------------------\n    Input 3                         Output 3 (Transfer)\nOutput 3 (Transfer)\n      Asset quantity:     5           Asset quantity:     6\nAsset quantity:     6\n      Asset ID:           A1          Asset ID:\nAsset ID:           A1\n    -----------------------------   -----------------------------\n-----------------------------\n    Input 4                         Output 4 (Transfer)\nOutput 4 (Transfer)\n      Asset quantity:     3           Asset quantity:     0\nAsset quantity:     <NULL>\n      Asset ID:           A1          Asset ID:\nAsset ID:           <NULL>\n    -----------------------------   -----------------------------\n-----------------------------\n    Input 5                         Output 5 (Transfer)\nOutput 5 (Transfer)\n      Asset quantity:     9           Asset quantity:     7\nAsset quantity:     7\n      Asset ID:           A2          Asset ID:\nAsset ID:           A1\n    =============================   -----------------------------\n-----------------------------\n                                    Output 6 (Transfer)\nOutput 6 (Transfer)\n                                      Asset quantity:     3\nAsset quantity:     3\n                                      Asset ID:\nAsset ID:           A2\n                                    =============================\n=============================\n\nOutputs are colored from the first to the last. Outputs before the\nmarker output are issuance outputs:\n* Output 0 has an asset quantity of zero, so it is considered uncolored.\n* Output 1 gets assigned the asset ID defined by <code>H =\nRIPEMD160(SHA256((S))</code> where <code>S</code> is the output script\nreferenced by the first input of the transaction (input 0).\n\nOutput 2 is the marker output, separating issuance outputs from\ntransfer outputs. The marker output is always uncolored.\n\nTransfer outputs are then colored:\n* Output 3 receives 3 units from input 0, 2 units from input 1, 0 unit\nfrom input 2 and 1 unit from input 3. All the 6 units have the same\nasset ID <code>A1</code>, so the asset ID <code>A1</code> is assigned\nto output 3.\n* Output 4 has an asset quantity of zero, so it is considered uncolored.\n* Output 5 receives the remaining 4 units of input 3, and 3 units from\ninput 4. All the 7 units have the same asset ID <code>A1</code>, so\nthe asset ID <code>A1</code> is assigned to output 5.\n* Output 6 receives the first 3 units of input 5. Input 5 has the\nasset ID <code>A2</code> so the asset ID <code>A2</code> is assigned\nto output 6.\n\n==Rationale==\n\nThis approach offers a number of desirable characteristics:\n\n# Economical: The cost of issuing or transferring an asset is\ncompletely independent from the quantity issued or transferred.\n# Clients have a way to identify colored outputs simply by traversing\nthe Blockchain, without needing to be fed external data. Transactions\nrelevant to the Open Assets Protocol are identified by the special\nmarker output.\n# It is possible to determine the asset ID and asset quantity of an\noutput by traversing only a limited number of transactions.\n# Assets are pseudonymous. They are represented by an asset ID, which\nis enough to identify each asset uniquely, while still providing an\nadequate level of anonymity for both the issuer and users of the\nasset.\n# This approach uses the recommended way to embed data in the\nBlockchain (OP_RETURN), and therefore does not pollute the UTXO.\n# The whole cryptographic infrastructure that Bitcoin provides for\nsecuring the spending of outputs is reused for securing the ability to\nissue assets. There is a symmetry between ''an address + private key''\nas a way to spend Bitcoins, and ''an address + private key'' as a way\nto issue assets.\n# Generating a new type of asset is as simple as generating an\naddress, can be done offline, and for free.\n# Reissuing more of an existing asset is easy and can be done quickly\nand at no cost (except for the transaction fee) as long as the issuer\nretains the private key for the asset ID.\n# Single-issuance assets can be achieved by destroying the private key\nused to issue the asset immediately after issuing it.\n# Since issuance is based on standard Bitcoin output scripts, it is\npossible to create an asset that requires multiple signatures for\nissuance.\n\n==Compatibility==\n\nFor backward compatibility reasons, we consider than an older client\nis allowed to see a colored output as uncolored.\n\n===Backward compatibility with existing Bitcoin protocol===\n\nThe Open Assets Protocol sits on top of the Bitcoin protocol. It does\nnot require any change to the existing Bitcoin protocol. Existing\nclients that don't support the Open Assets Protocol will see all\noutputs as uncolored, and will not be able to perform transfer\ntransactions.\n\n===Compatibility between different versions of OAP===\n\nNew versions with the same major version number (e.g. 1.1) should be\nbackwards compatible. New versions with a different major version\nnumber (e.g. 2.0) can introduce breaking changes, but transactions\ncreated by newer clients will be identifiable by a different version\nnumber in the output 0 of genesis and transfer transactions.\n\n==Copyright==\n\nThis document has been placed in the public domain.\n\n\nNicolas Dorier,\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160526/7b874290/attachment-0001.html>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2016-05-26T03:53:04",
                "message_text_only": "On Thursday, May 26, 2016 2:50:26 AM Nicolas Dorier via bitcoin-dev wrote:\n>   Author: Flavien Charlon <flavien at charlon.net>\n\nIs he the author of this BIP, or merely the protocol described in it?\nWould it perhaps make sense to include yourself in the author list?\n\n> The ID of an asset is the RIPEMD-160 hash of the SHA-256 hash of the\n> output script referenced by the first input of the transaction that\n> initially issued that asset (<code>script_hash =\n> RIPEMD160(SHA256(script))</code>). An issuer can reissue more of an\n> already existing asset as long as they retain the private key for that\n> asset ID. Assets on two different outputs can only be mixed together\n> if they have the same asset ID.\n\nQuite a bit ugly, giving a meaning to an input's pubkey script like that.\nBut more problematically: how can this work with other pubkey scripts? \nParticularly relevant now that this old script format is being deprecated.\n\nAnother possible problem is that I don't see a way to provably guarantee an \nasset issuance is final.\n\n> Transactions that are not recognized as an Open Assets transaction are\n> considered as having all their outputs uncolored.\n\nAnd the assets attached to its inputs are destroyed? Or?\n\n> If multiple parsable PUSHDATA opcodes exist in the same output, the\n> first one is used, and the other ones are ignored.\n> \n> If multiple valid marker outputs exist in the same transaction, the\n> first one is used and the other ones are considered as regular\n> outputs.\n\nIs it intentional that the first case is \"parsable\", and the second \"valid\"?\nI think these need to be better specified; for example, it is not so clear how \nto reach if the OAP version number is something other than 1: is that \nparsable? valid?\n\n> ! Asset quantity count || A\n> [https://en.bitcoin.it/wiki/Protocol_specification#Variable_length_integer\n> var-integer] representing the number of items in the <code>asset quantity\n> list</code> field. || 1-9 bytes\n> \n> |-\n> \n> ! Asset quantity list  || A list of zero or more\n> [http://en.wikipedia.org/wiki/LEB128 LEB128-encoded] unsigned integers\n> representing the asset quantity of every output in order (excluding the\n> marker output). || Variable\n\nWhat determines the asset id? How would one issue and/or transfer multiple \nasset ids in the same transaction?\n\n> The marker output is always uncolored.\n\nWhat if I have a transaction with 5 outputs, the marker output at position 3, \nand all 4 other outputs are to receive assets? Does the marker output get \nskipped in the list (ie, the list is 4 elements long) or must it be set to \nzero quantity (ie, the list is 5 elements long)?\n\n> Inputs are seen as a sequence of asset units, each having an asset ID.\n> Similarly, outputs are seen as a sequence of asset units to be\n> assigned an asset ID. These two sequences are built by taking each\n> input or output in order, each of them adding a number of asset units\n> equal to their asset quantity. The process starts with the first input\n> of the transaction and the first output after the marker output.\n> \n> After the sequences have been built, the asset ID of every asset unit\n> in the input sequence is assigned to the asset unit at the same\n> position in the output sequence until all the asset units in the\n> output sequence have received an asset ID. If there are less asset\n> units in the input sequence than in the output sequence, the marker\n> output is considered invalid.\n> \n> Finally, for each transfer output, if the asset units forming that\n> output all have the same asset ID, the output gets assigned that asset\n> ID. If any output is mixing units with more than one distinct asset\n> ID, the marker output is considered invalid. Outputs with an asset\n> quantity of zero are always considered uncolored.\n\nI don't understand this.\n\n> # This approach uses the recommended way to embed data in the Blockchain\n> (OP_RETURN), and therefore does not pollute the UTXO.\n\nEmbedding data is not recommended at all. It seems a better way to have done \nthis would be to put the info in an OP_DROP within a P2SH or witness script.\n\n> # The whole cryptographic infrastructure that Bitcoin provides for\n> securing the spending of outputs is reused for securing the ability to\n> issue assets. There is a symmetry between ''an address + private key''\n> as a way to spend Bitcoins, and ''an address + private key'' as a way\n> to issue assets.\n\nAddresses are not used for spending bitcoins, only for receiving them. The way \nthis BIP uses inputs' pubkey script is extremely unusual and probably a bad \nidea.\n\n> # Reissuing more of an existing asset is easy and can be done quickly\n> and at no cost (except for the transaction fee) as long as the issuer\n> retains the private key for the asset ID.\n\nAs I understand it, this would require address reuse to setup, which is not \nsupported behaviour and insecure.\n\n> For backward compatibility reasons, we consider than an older client\n> is allowed to see a colored output as uncolored.\n\nHow is this compatible? Won't an older client then accidentally destroy \nassets?\n\nLuke"
            }
        ],
        "thread_summary": {
            "title": "BIP Number Request: Open Asset",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Luke Dashjr",
                "Nicolas Dorier"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 23261
        }
    },
    {
        "title": "[bitcoin-dev] Zurich engineering meeting transcript and notes (2016-05-20)",
        "thread_messages": [
            {
                "author": "Bryan Bishop",
                "date": "2016-05-27T21:24:36",
                "message_text_only": "It has occurred to me that some folks may not have seen the link floating\naround the other day on IRC.\n\nTranscript:\nhttps://bitcoincore.org/logs/2016-05-zurich-meeting-notes.html\nhttps://bitcoincore.org/logs/2016-05-zurich-meeting-notes.txt\n\nMeeting notes summary:\nhttps://bitcoincore.org/en/meetings/2016/05/20/\n\nTopics discussed and documented include mostly obscure details about\nsegwit, segwit code review, error correcting codes for future address\ntypes, encryption for the p2p network protocol, compact block relay,\nSchnorr signatures and signature aggregation, networking library, encrypted\ntransactions, UTXO commitments, MAST stuff, and many other topics. I think\nthis is highly useful reading material.\n\nAny errors in transcription are very likely my own as it is difficult to\ncapture everything with high accuracy in real-time. Another thing to keep\nin mind is that there are many different parallel conversations and I only\ndo linear serialization at best... and finally, I also want to mention that\nthis is the result of collaboration with many colleagues and this should\nnot be considered merely the work of just myself.\n\n- Bryan\nhttp://heybryan.org/\n1 512 203 0507\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160527/43e8aecb/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Zurich engineering meeting transcript and notes (2016-05-20)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Bryan Bishop"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1362
        }
    },
    {
        "title": "[bitcoin-dev] Towards Massive On-Chain Scaling: Presenting Our Block Propagation Results With Xthin",
        "thread_messages": [
            {
                "author": "Peter R",
                "date": "2016-05-30T15:41:15",
                "message_text_only": "Dear all,\n\nFor the past two months, Andrew Clifford, Andrew Stone, @sickpig, Peter Tschipper and I have been collecting empirical data regarding block propagation with Xthin\u200a\u2014\u200aboth across the normal P2P network and over the Great Firewall of China. We have six Bitcoin Unlimited (BU) nodes running, including one located in Shenzhen and another in Shanghai, and we have collected data on the transmission and reception for over nine thousand blocks.\n\nHere is a link to Part 1(Methodology) of our 5 part article series on the testing we performed for this exciting new block relay technology:\n\nhttps://medium.com/@peter_r/towards-massive-on-chain-scaling-presenting-our-block-propagation-results-with-xthin-da54e55dc0e4 <https://medium.com/@peter_r/towards-massive-on-chain-scaling-presenting-our-block-propagation-results-with-xthin-da54e55dc0e4>\n\nWe thank Jihan Wu from AntPool for the block source within Mainland China and @cypherdoc for the funding.  \n\nBest regards,\nPeter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20160530/31f7effc/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Towards Massive On-Chain Scaling: Presenting Our Block Propagation Results With Xthin",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Peter R"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1159
        }
    }
]