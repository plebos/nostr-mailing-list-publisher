[
    {
        "title": "[bitcoin-dev] [BIP Draft] Datastream compression of Blocks and Transactions",
        "thread_messages": [
            {
                "author": "Matt Corallo",
                "date": "2015-12-01T05:28:42",
                "message_text_only": "I'm really not a fan of this at all. To start with, adding a compression library that is directly accessible to the network on financial software is a really, really scary idea. If there were a massive improvement, I'd find it acceptable, but the improvement you've shown really isn't all that much. The numbers you recently posted show it improving the very beginning of IBD somewhat over high-latency connections, but if we're throughput-limited after the very beginning of IBD, we should fix that, not compress the blocks. Additionally, I'd be very surprised if this had any significant effect on the speed at which new blocks traverse the network (do you have any simulations or other thoughts on this?).\n\nAll that said, I'd love a proposal that allows clients to download compressed blocks via an external daemon, especially during IBD. This could help people with very restrictive data caps do IBD instead of being pushed to revert to SPV. Additionally, I think we need more chain sync protocols so that the current P2P protocol isn't consensus-critical anymore.\n\nOn November 30, 2015 4:12:24 PM MST, Peter Tschipper via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>@gmaxwell Bip Editor, and the Bitcoin Dev Community,\n>\n>After several weeks of experimenting and testing with various\n>compression libraries I think there is enough evidence to show that\n>compressing blocks and transactions is not only beneficial in reducing\n>network bandwidth but is also provides a small performance boost when\n>there is latency on the network.\n>\n>The following is a BIP Draft document for your review. \n>(The alignment of the columns in the tables doesn't come out looking\n>right in this email but if you cut and paste into a text document they\n>are just fine)\n>\n>\n><pre>\n>  BIP: ?\n>  Title: Datastream compression of Blocks and Tx's\n>  Author: Peter Tschipper <peter.tschipper at gmail.com>\n>  Status: Draft\n>  Type: Standards Track\n>  Created: 2015-11-30\n></pre>\n>\n>==Abstract==\n>\n>To compress blocks and transactions, and to concatenate them together\n>when possible, before sending.\n>\n>==Motivation==\n>\n>Bandwidth is an issue for users that run nodes in regions where\n>bandwidth is expensive and subject to caps, in addition network latency\n>in some regions can also be quite high. By compressing data we can\n>reduce daily bandwidth used in a significant way while at the same time\n>speed up the transmission of data throughout the network. This should\n>encourage users to keep their nodes running longer and allow for more\n>peer connections with less need for bandwidth throttling and in\n>addition, may also encourage users in areas of marginal internet\n>connectivity to run nodes where in the past they would not have been\n>able to.\n>\n>==Specification==\n>\n>Advertise compression using a service bit.  Both peers must have\n>compression turned on in order for data to be compressed, sent, and\n>decompressed.\n>\n>Blocks will be sent compressed.\n>\n>Transactions will be sent compressed with the exception of those less\n>than 500 bytes.\n>\n>Blocks will be concatenated when possible.\n>\n>Transactions will be concatenated when possible or when a\n>MSG_FILTERED_BLOCK is requested.\n>\n>Compression levels to be specified in \"bitcoin.conf\".\n>\n>Compression and decompression can be completely turned off.\n>\n>Although unlikely, if compression should fail then data will be sent\n>uncompressed.\n>\n>The code for compressing and decompressing will be located in class\n>CDataStream.\n>\n>Compression library LZO1x will be used.\n>\n>==Rationale==\n>\n>By using a service bit, compression and decompression can be turned\n>on/off completely at both ends with a simple configuration setting. It\n>is important to be able to easily turn off compression/decompression as\n>a fall back mechanism.  Using a service bit also makes the code fully\n>compatible with any node that does not currently support compression. A\n>node that do not present the correct service bit will simply receive\n>data in standard uncompressed format.\n>\n>All blocks will be compressed. Even small blocks have been found to\n>benefit from compression.\n> \n>Multiple block requests that are in queue will be concatenated together\n>when possible to increase compressibility of smaller blocks.\n>Concatenation will happen only if there are multiple block requests\n>from\n>the same remote peer.  For example, if peer1 is requesting two blocks\n>and they are both in queue then those two blocks will be concatenated.\n>However, if peer1 is requesting 1 block and peer2 also one block, and\n>they are both in queue, then each peer is sent only its block and no\n>concatenation will occur. Up to 16 blocks (the max blocks in flight)\n>can\n>be concatenated but not exceeding the MAX_PROTOCOL_MESSAGE_LENGTH.\n>Concatenated blocks compress better and further reduce bandwidth.\n>\n>Transactions below 500 bytes do not compress well and will be sent\n>uncompressed unless they can be concatenated (see Table 3).\n>\n>Multiple transaction requests that are in queue will be concatenated\n>when possible.  This further reduces bandwidth needs and speeds the\n>transfer of large requests for many transactions, such as with\n>MSG_FILTERED_BLOCK requests, or when the system gets busy and is\n>flooded\n>with transactions.  Concatenation happens in the same way as for\n>blocks,\n>described above.\n>\n>By allowing for differing compression levels which can be specified in\n>the bitcoin.conf file, a node operator can tailor their compression to\n>a\n>level suitable for their system.\n>\n>Although unlikely, if compression fails for any reason then blocks and\n>transactions will be sent uncompressed.  Therefore, even with\n>compression turned on, a node will be able to handle both compressed\n>and\n>uncompressed data from another peer.\n>\n>By Abstracting the compression/decompression code into class\n>\"CDataStream\", compression can be easily applied to any datastream.\n>\n>The compression library LZO1x-1 does not compress to the extent that\n>Zlib does but it is clearly the better performer (particularly as file\n>sizes get larger), while at the same time providing very good\n>compression (see Tables 1 and 2).  Furthermore, LZO1x-999 can provide\n>and almost Zlib like compression for those who wish to have more\n>compression, although at a cost.\n>\n>==Test Results==\n>\n>With the LZO library, current test results show up to a 20% compression\n>using LZO1x-1 and up to 27% when using LZO1x-999.  In addition there is\n>a marked performance improvement when there is latency on the network.\n>From the test results, with a latency of 60ms there is an almost 30%\n>improvement in performance when comparing LZO1x-1 compressed blocks\n>with\n>uncompressed blocks (see Table 5).\n>\n>The following table shows the percentage that blocks were compressed,\n>using two different Zlib and LZO1x compression level settings.\n>\n>TABLE 1:\n>range = data size range\n>range           Zlib-1  Zlib-6  LZO1x-1 LZO1x-999\n>-----------     ------  ------  ------- --------\n>0-250           12.44   12.86   10.79   14.34\n>250-500         19.33   12.97   10.34   11.11   \n>600-700         16.72   n/a     12.91   17.25\n>700-800         6.37    7.65    4.83    8.07\n>900-1KB         6.54    6.95    5.64    7.9\n>1KB-10KB        25.08   25.65   21.21   22.65\n>10KB-100KB      19.77   21.57   4.37    19.02\n>100KB-200KB     21.49   23.56   15.37   21.55\n>200KB-300KB     23.66   24.18   16.91   22.76\n>300KB-400KB     23.4    23.7    16.5    21.38\n>400KB-500KB     24.6    24.85   17.56   22.43\n>500KB-600KB     25.51   26.55   18.51   23.4\n>600KB-700KB     27.25   28.41   19.91   25.46\n>700KB-800KB     27.58   29.18   20.26   27.17\n>800KB-900KB     27      29.11   20      27.4\n>900KB-1MB       28.19   29.38   21.15   26.43\n>1MB -2MB        27.41   29.46   21.33   27.73\n>\n>The following table shows the time in seconds that a block of data\n>takes\n>to compress using different compression levels.  One can clearly see\n>that LZO1x-1 is the fastest and is not as affected when data sizes get\n>larger.\n>\n>TABLE 2:\n>range = data size range\n>range           Zlib-1  Zlib-6  LZO1x-1 LZO1x-999\n>-----------     ------  ------  ------- ---------\n>0-250           0.001   0       0       0\n>250-500         0       0       0       0.001\n>500-1KB         0       0       0       0.001\n>1KB-10KB        0.001   0.001   0       0.002\n>10KB-100KB      0.004   0.006   0.001   0.017\n>100KB-200KB     0.012   0.017   0.002   0.054\n>200KB-300KB     0.018   0.024   0.003   0.087\n>300KB-400KB     0.022   0.03    0.003   0.121\n>400KB-500KB     0.027   0.037   0.004   0.151\n>500KB-600KB     0.031   0.044   0.004   0.184\n>600KB-700KB     0.035   0.051   0.006   0.211\n>700KB-800KB     0.039   0.057   0.006   0.243\n>800KB-900KB     0.045   0.064   0.006   0.27\n>900KB-1MB       0.049   0.072   0.006   0.307\n>\n>TABLE 3:\n>Compression of Transactions (without concatenation)\n>range = block size range\n>ubytes = average size of uncompressed transactions\n>cbytes = average size of compressed transactions\n>cmp% = the percentage amount that the transaction was compressed\n>datapoints = number of datapoints taken\n>\n>range       ubytes    cbytes    cmp%    datapoints\n>----------  ------    ------    ------  ----------    \n>0-250       220       227       -3.16   23780\n>250-500     356       354       0.68    20882\n>500-600     534       505       5.29    2772\n>600-700     653       608       6.95    1853\n>700-800     757       649       14.22   578\n>800-900     822       758       7.77    661\n>900-1KB     954       862       9.69    906\n>1KB-10KB    2698      2222      17.64   3370\n>10KB-100KB  15463     12092     21.80   15429\n>\n>The above table shows that transactions don't compress well below 500\n>bytes but do very well beyond 1KB where there are a great deal of those\n>large spam type transactions.   However, most transactions happen to be\n>in the < 500 byte range.  So the next step was to appy concatenation\n>for\n>those smaller transactions.  Doing that yielded some very good\n>compression results.  Some examples as follows:\n>\n>The best one that was seen was when 175 transactions were concatenated\n>before being compressed.  That yielded a 20% compression ratio, but\n>that\n>doesn't take into account the savings from the unneeded 174 message\n>headers (24 bytes each) as well as 174 TCP ACKs of 52 bytes each which\n>yields and additional 76*174 = 13224 byte savings, making for an\n>overall\n>bandwidth savings of 32%:\n>\n>     2015-11-18 01:09:09.002061 compressed data from 79890 to 67426\n>txcount:175\n>\n>However, that was an extreme example.  Most transaction aggregates were\n>in the 2 to 10 transaction range.  Such as the following:\n>\n>2015-11-17 21:08:28.469313 compressed data from 3199 to 2876 txcount:10\n>\n>But even here the savings of 10% was far better than the \"nothing\" we\n>would get without concatenation, but add to that the 76 byte * 9\n>transaction savings and we have a total 20% savings in bandwidth for\n>transactions that otherwise would not be compressible.  Therefore the\n>concatenation of small transactions can also save bandwidth and speed\n>up\n>the transmission of those transactions through the network while\n>keeping\n>network and message queue chatter to a minimum.\n>\n>==Choice of Compression library==\n>\n>LZO was chosen over Zlib.  LZO is the fastest most scalable option when\n>used at the lowest compression setting which will be a performance\n>boost\n>for users that prefer performance over bandwidth savings. And at the\n>higher end, LZO provides good compression (although at a higher cost)\n>which approaches that of Zlib.\n>\n>Other compression libraries investigated were Snappy, LZOf, fastZlib\n>and\n>LZ4 however none of these were found to be suitable, either because\n>they\n>were not portable, lacked the flexibility to set compression levels or\n>did not provide a useful compression ratio.\n>\n>The following two tables show results in seconds for syncing the first\n>200,000 blocks. Tests were run on a high-speed wireless LAN with very\n>little latency, and also run with a 60ms latency which was induced with\n>\"Netbalancer\".\n>               \n>TABLE 4:\n>Results shown in seconds on highspeed wireless LAN (no induced latency)\n>Num blks sync'd  Uncmp  Zlib-1  Zlib-6  LZO1x-1  LZO1x-999\n>---------------  -----  ------  ------  -------  ---------\n>10000            255    232     233     231      257      \n>20000            464    414     420     407      453      \n>30000            677    594     611     585      650      \n>40000            887    787     795     760      849     \n>50000            1099   961     977     933      1048   \n>60000            1310   1145    1167    1110     1259  \n>70000            1512   1330    1362    1291     1470  \n>80000            1714   1519    1552    1469     1679   \n>90000            1917   1707    1747    1650     1882  \n>100000           2122   1905    1950    1843     2111    \n>110000           2333   2107    2151    2038     2329  \n>120000           2560   2333    2376    2256     2580   \n>130000           2835   2656    2679    2558     2921 \n>140000           3274   3259    3161    3051     3466   \n>150000           3662   3793    3547    3440     3919   \n>160000           4040   4172    3937    3767     4416   \n>170000           4425   4625    4379    4215     4958   \n>180000           4860   5149    4895    4781     5560    \n>190000           5855   6160    5898    5805     6557    \n>200000           7004   7234    7051    6983     7770   \n>\n>TABLE 5:\n>Results shown in seconds with 60ms of induced latency\n>Num blks sync'd  Uncmp  Zlib-1  Zlib-6  LZO1x-1  LZO1x-999\n>---------------  -----  ------  ------  -------  ---------\n>10000            219    299     296     294      291\n>20000            432    568     565     558      548\n>30000            652    835     836     819      811\n>40000            866    1106    1107    1081     1071\n>50000            1082   1372    1381    1341     1333\n>60000            1309   1644    1654    1605     1600\n>70000            1535   1917    1936    1873     1875\n>80000            1762   2191    2210    2141     2141\n>90000            1992   2463    2486    2411     2411\n>100000           2257   2748    2780    2694     2697\n>110000           2627   3034    3076    2970     2983\n>120000           3226   3416    3397    3266     3302\n>130000           4010   3983    3773    3625     3703\n>140000           4914   4503    4292    4127     4287\n>150000           5806   4928    4719    4529     4821\n>160000           6674   5249    5164    4840     5314\n>170000           7563   5603    5669    5289     6002\n>180000           8477   6054    6268    5858     6638\n>190000           9843   7085    7278    6868     7679\n>200000           11338  8215    8433    8044     8795\n>\n>==Backward compatibility==\n>\n>Being unable to present the correct service bit, older clients will\n>continue to receive standard uncompressed data and will be fully\n>compatible with this change.\n>\n>==Fallback==\n>\n>It is important to be able to entirely and easily turn off compression\n>and decompression as a fall back mechanism. This can be done with a\n>simple bitcoin.conf setting of \"compressionlevel=0\". Only one of the\n>two\n>connected peers need to set compressionlevel=0 in order to turn off\n>compression and decompression completely.\n>\n>==Deployment==\n>\n>This enhancement does not require a hard or soft fork.\n>\n>==Service Bit==\n>\n>During the testing of this implementation, service bit 28 was used,\n>however this enhancement will require a permanently assigned service\n>bit.\n>\n>==Implementation==\n>\n>This implementation depends on the LZO compression library: lzo-2.09\n>\n>     https://github.com/ptschip/bitcoin/tree/compress\n>\n>==Copyright==\n>\n>This document is placed in the public domain.\n>\n>\n>_______________________________________________\n>bitcoin-dev mailing list\n>bitcoin-dev at lists.linuxfoundation.org\n>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Pavel Jan\u00edk",
                "date": "2015-12-01T20:06:53",
                "message_text_only": "> On 01 Dec 2015, at 06:28, Matt Corallo via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> I'm really not a fan of this at all. To start with, adding a compression library that is directly accessible to the network on financial software is a really, really scary idea.\n\nI have the same opinion.\n\nOn the other hand, I can imagine using compression on local blocks storage (be it compressed filesystem, or compression in the user space/in the application - compare with https://github.com/bitcoin/bitcoin/issues/2278). Now that we support pruning and obfuscating, this could be another option. Saving ~20% can be interesting in some usecases.\n--  \nPavel Jan\u00edk"
            },
            {
                "author": "Emin G\u00fcn Sirer",
                "date": "2015-12-02T18:57:46",
                "message_text_only": "Thanks Peter for the careful, quantitative work.\n\nI want to bring one additional issue to everyone's consideration, related\nto the choice of the Lempel-Ziv family of compressors.\n\nWhile I'm not familiar with every single compression engine tested, the\nLempel-Ziv family of compressors are generally based on \"compression\ntables.\" Essentially, they assign a short unique number to every new\nsubsequence they encounter, and when they re-encounter a sequence like \"ab\"\nin \"abcdfdcdabcdfabcdf\" they replace it with that short integer (say, in\nthis case, 9-bit constant 256). So this example sequence may turn into\n\"abcdfd<258 for cd><256 for ab><258 for cd>f<261 for abc><259 for df>\"\nwhich is slightly shorter than the original (I'm doing this off the top of\nmy head so the counts may be off, but it's meant to be illustrative). Note\nthat the sequence \"abc\" got added into the table only after it was\nencountered twice in the input.\n\nThis is nice and generic and works well for English text where certain\nletter sequences (e.g. \"it\" \"th\" \"the\" \"this\" \"are\" \"there\" etc) are\nrepeated often, but it is nowhere as compact as it could possibly be for\nmostly binary data -- there are opportunities for much better compression,\nmade possible by the structured reuse of certain byte sequences in the\nBitcoin wire protocol.\n\nOn a Bitcoin wire connection, we might see several related transactions\nreorganizing cash in a set of addresses, and therefore, several reuses of a\n20-byte address. Or we might see a 200-byte transaction get transmitted,\nfollowed by the same transaction, repeated in a block. Ideally, we'd learn\nthe sequence that may be repeated later on, all at once (e.g. a Bitcoin\naddress or a transaction), and replace it with a short number, referring\nback to the long sequence. In the example above, if we knew that \"abcdf\"\nwas a UNIT that would likely be repeated, we would put it into the\ncompression table as a whole, instead of relying on repetition to get it\ninto the table one extra byte at a time. That may let us compress the\noriginal sequence down to \"abcdfd<257 for cd><256 for abcdf><256 for\nabcdf>\" from the get go.\n\nYet the LZ variants I know of will need to see a 200-byte sequence repeated\n**199 times** in order to develop a single, reusable, 200-byte long\nsubsequence in the compression table.\n\nSo, a Bitcoin-specific compressor can perhaps do significantly better, but\nis it a good idea? Let's argue both sides.\n\nCons:\n\nOn the one hand, Bitcoin-specific compressors will be closely tied to the\ncontents of messages, which might make it difficult to change the wire\nformat later on -- changes to the wire format may need corresponding\nchanges to the compressor.  If the compressor cannot be implemented\ncleanly, then the protocol-agnostic, off-the-shelf compressors have a\nmaintainability edge, which comes at the expense of the compression ratio.\n\nAnother argument is that compression algorithms of any kind should be\ntested thoroughly before inclusion, and brand new code may lack the\nmaturity required. While this argument has some merit, all outputs are\nverified separately later on during processing, so\ncompression/decompression errors can potentially be detected. If the\ncompressor/decompressor can be structured in a way that isolates bitcoind\nfrom failure (e.g. as a separate process for starters), this concern can be\nremedied.\n\nPros:\n\nThe nature of LZ compressors leads me to believe that much higher\ncompression ratios are possible by building a custom, Bitcoin-aware\ncompressor. If I had to guess, I would venture that compression ratios of\n2X or more are possible in some cases. In some sense, the \"O(1) block\npropagation\" idea that Gavin proposed a while ago can be seen as extreme\nexample of a Bitcoin-specific compressor, albeit one that constrains the\norder of transactions in a block.\n\nCompression can buy us some additional throughput at zero cost, modulo code\ncomplexity.\nGiven the amount of acrimonious debate over the block size we have all had\nto endure, it seems\ncriminal to leave potentially free improvements on the table. Even if the\nresulting code is\ndeemed too complex to include in the production client right now, it would\nbe good to understand\nthe potential for improvement.\n\nHow to Do It\n\nIf we want to compress Bitcoin, a programming challenge/contest would be\none of the best ways to find the best possible, Bitcoin-specific\ncompressor. This is the kind of self-contained exercise that bright young\nhackers love to tackle. It'd bring in new programmers into the ecosystem,\nand many of us would love to discover the limits of compressibility for\nBitcoin bits on a wire. And the results would be interesting even if the\nfinal compression engine is not enabled by default, or not even merged.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151202/32591f84/attachment.html>"
            },
            {
                "author": "Peter Tschipper",
                "date": "2015-12-02T20:16:19",
                "message_text_only": "Building a compressor from scratch may yeild some better compression\nratios, or not, but having trust and faith in whether it will stand up\nagainst attack vectors another matter.  LZO has been around for 20 years\nwith very few problems and no current issues.  Maybe something better\ncan be built, but when and how much testing will need to be done before\nit can be trusted?  Right now there is something that provides a benefit\nand in the future if something better is found it's not that difficult\nto add it.  We could easily support multiple compression libraries.\n\n\nOn 02/12/2015 10:57 AM, Emin G\u00fcn Sirer wrote:\n> Thanks Peter for the careful, quantitative work.\n>\n> I want to bring one additional issue to everyone's consideration,\n> related to the choice of the Lempel-Ziv family of compressors. \n>\n> While I'm not familiar with every single compression engine tested,\n> the Lempel-Ziv family of compressors are generally based on\n> \"compression tables.\" Essentially, they assign a short unique number\n> to every new subsequence they encounter, and when they re-encounter a\n> sequence like \"ab\" in \"abcdfdcdabcdfabcdf\" they replace it with that\n> short integer (say, in this case, 9-bit constant 256). So this example\n> sequence may turn into \"abcdfd<258 for cd><256 for ab><258 for\n> cd>f<261 for abc><259 for df>\" which is slightly shorter than the\n> original (I'm doing this off the top of my head so the counts may be\n> off, but it's meant to be illustrative). Note that the sequence \"abc\"\n> got added into the table only after it was encountered twice in the\n> input. \n>\n> This is nice and generic and works well for English text where certain\n> letter sequences (e.g. \"it\" \"th\" \"the\" \"this\" \"are\" \"there\" etc) are\n> repeated often, but it is nowhere as compact as it could possibly be\n> for mostly binary data -- there are opportunities for much better\n> compression, made possible by the structured reuse of certain byte\n> sequences in the Bitcoin wire protocol.\n>\n> On a Bitcoin wire connection, we might see several related\n> transactions reorganizing cash in a set of addresses, and therefore,\n> several reuses of a 20-byte address. Or we might see a 200-byte\n> transaction get transmitted, followed by the same transaction,\n> repeated in a block. Ideally, we'd learn the sequence that may be\n> repeated later on, all at once (e.g. a Bitcoin address or a\n> transaction), and replace it with a short number, referring back to\n> the long sequence. In the example above, if we knew that \"abcdf\" was a\n> UNIT that would likely be repeated, we would put it into the\n> compression table as a whole, instead of relying on repetition to get\n> it into the table one extra byte at a time. That may let us compress\n> the original sequence down to \"abcdfd<257 for cd><256 for abcdf><256\n> for abcdf>\" from the get go.\n>\n> Yet the LZ variants I know of will need to see a 200-byte sequence\n> repeated **199 times** in order to develop a single, reusable,\n> 200-byte long subsequence in the compression table. \n>\n> So, a Bitcoin-specific compressor can perhaps do significantly better,\n> but is it a good idea? Let's argue both sides.\n>\n> Cons:\n>\n> On the one hand, Bitcoin-specific compressors will be closely tied to\n> the contents of messages, which might make it difficult to change the\n> wire format later on -- changes to the wire format may need\n> corresponding changes to the compressor.  If the compressor cannot be\n> implemented cleanly, then the protocol-agnostic, off-the-shelf\n> compressors have a maintainability edge, which comes at the expense of\n> the compression ratio. \n>\n> Another argument is that compression algorithms of any kind should be\n> tested thoroughly before inclusion, and brand new code may lack the\n> maturity required. While this argument has some merit, all outputs are\n> verified separately later on during processing, so\n> compression/decompression errors can potentially be detected. If the\n> compressor/decompressor can be structured in a way that isolates\n> bitcoind from failure (e.g. as a separate process for starters), this\n> concern can be remedied.\n>\n> Pros:\n>\n> The nature of LZ compressors leads me to believe that much higher\n> compression ratios are possible by building a custom, Bitcoin-aware\n> compressor. If I had to guess, I would venture that compression ratios\n> of 2X or more are possible in some cases. In some sense, the \"O(1)\n> block propagation\" idea that Gavin proposed a while ago can be seen as\n> extreme example of a Bitcoin-specific compressor, albeit one that\n> constrains the order of transactions in a block.\n>\n> Compression can buy us some additional throughput at zero cost, modulo\n> code complexity. \n> Given the amount of acrimonious debate over the block size we have all\n> had to endure, it seems \n> criminal to leave potentially free improvements on the table. Even if\n> the resulting code is\n> deemed too complex to include in the production client right now, it\n> would be good to understand\n> the potential for improvement.\n>\n> How to Do It\n>\n> If we want to compress Bitcoin, a programming challenge/contest would\n> be one of the best ways to find the best possible, Bitcoin-specific\n> compressor. This is the kind of self-contained exercise that bright\n> young hackers love to tackle. It'd bring in new programmers into the\n> ecosystem, and many of us would love to discover the limits of\n> compressibility for Bitcoin bits on a wire. And the results would be\n> interesting even if the final compression engine is not enabled by\n> default, or not even merged.\n>\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151202/ddbc5702/attachment-0001.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2015-12-02T22:23:47",
                "message_text_only": "My issue is more that its additional complexity and attack surface, and \nfor a very minor gain which should disappear with further optimization \nelsewhere and less that we absolutely shouldn't add compression because \nwe're definitely gonna have issues.\n\nOn 12/02/15 20:16, Peter Tschipper via bitcoin-dev wrote:\n> Building a compressor from scratch may yeild some better compression\n> ratios, or not, but having trust and faith in whether it will stand up\n> against attack vectors another matter.  LZO has been around for 20 years\n> with very few problems and no current issues.  Maybe something better\n> can be built, but when and how much testing will need to be done before\n> it can be trusted?  Right now there is something that provides a benefit\n> and in the future if something better is found it's not that difficult\n> to add it.  We could easily support multiple compression libraries.\n>\n>\n> On 02/12/2015 10:57 AM, Emin G\u00fcn Sirer wrote:\n>> Thanks Peter for the careful, quantitative work.\n>>\n>> I want to bring one additional issue to everyone's consideration,\n>> related to the choice of the Lempel-Ziv family of compressors.\n>>\n>> While I'm not familiar with every single compression engine tested,\n>> the Lempel-Ziv family of compressors are generally based on\n>> \"compression tables.\" Essentially, they assign a short unique number\n>> to every new subsequence they encounter, and when they re-encounter a\n>> sequence like \"ab\" in \"abcdfdcdabcdfabcdf\" they replace it with that\n>> short integer (say, in this case, 9-bit constant 256). So this example\n>> sequence may turn into \"abcdfd<258 for cd><256 for ab><258 for\n>> cd>f<261 for abc><259 for df>\" which is slightly shorter than the\n>> original (I'm doing this off the top of my head so the counts may be\n>> off, but it's meant to be illustrative). Note that the sequence \"abc\"\n>> got added into the table only after it was encountered twice in the\n>> input.\n>>\n>> This is nice and generic and works well for English text where certain\n>> letter sequences (e.g. \"it\" \"th\" \"the\" \"this\" \"are\" \"there\" etc) are\n>> repeated often, but it is nowhere as compact as it could possibly be\n>> for mostly binary data -- there are opportunities for much better\n>> compression, made possible by the structured reuse of certain byte\n>> sequences in the Bitcoin wire protocol.\n>>\n>> On a Bitcoin wire connection, we might see several related\n>> transactions reorganizing cash in a set of addresses, and therefore,\n>> several reuses of a 20-byte address. Or we might see a 200-byte\n>> transaction get transmitted, followed by the same transaction,\n>> repeated in a block. Ideally, we'd learn the sequence that may be\n>> repeated later on, all at once (e.g. a Bitcoin address or a\n>> transaction), and replace it with a short number, referring back to\n>> the long sequence. In the example above, if we knew that \"abcdf\" was a\n>> UNIT that would likely be repeated, we would put it into the\n>> compression table as a whole, instead of relying on repetition to get\n>> it into the table one extra byte at a time. That may let us compress\n>> the original sequence down to \"abcdfd<257 for cd><256 for abcdf><256\n>> for abcdf>\" from the get go.\n>>\n>> Yet the LZ variants I know of will need to see a 200-byte sequence\n>> repeated **199 times** in order to develop a single, reusable,\n>> 200-byte long subsequence in the compression table.\n>>\n>> So, a Bitcoin-specific compressor can perhaps do significantly better,\n>> but is it a good idea? Let's argue both sides.\n>>\n>> Cons:\n>>\n>> On the one hand, Bitcoin-specific compressors will be closely tied to\n>> the contents of messages, which might make it difficult to change the\n>> wire format later on -- changes to the wire format may need\n>> corresponding changes to the compressor.  If the compressor cannot be\n>> implemented cleanly, then the protocol-agnostic, off-the-shelf\n>> compressors have a maintainability edge, which comes at the expense of\n>> the compression ratio.\n>>\n>> Another argument is that compression algorithms of any kind should be\n>> tested thoroughly before inclusion, and brand new code may lack the\n>> maturity required. While this argument has some merit, all outputs are\n>> verified separately later on during processing, so\n>> compression/decompression errors can potentially be detected. If the\n>> compressor/decompressor can be structured in a way that isolates\n>> bitcoind from failure (e.g. as a separate process for starters), this\n>> concern can be remedied.\n>>\n>> Pros:\n>>\n>> The nature of LZ compressors leads me to believe that much higher\n>> compression ratios are possible by building a custom, Bitcoin-aware\n>> compressor. If I had to guess, I would venture that compression ratios\n>> of 2X or more are possible in some cases. In some sense, the \"O(1)\n>> block propagation\" idea that Gavin proposed a while ago can be seen as\n>> extreme example of a Bitcoin-specific compressor, albeit one that\n>> constrains the order of transactions in a block.\n>>\n>> Compression can buy us some additional throughput at zero cost, modulo\n>> code complexity.\n>> Given the amount of acrimonious debate over the block size we have all\n>> had to endure, it seems\n>> criminal to leave potentially free improvements on the table. Even if\n>> the resulting code is\n>> deemed too complex to include in the production client right now, it\n>> would be good to understand\n>> the potential for improvement.\n>>\n>> How to Do It\n>>\n>> If we want to compress Bitcoin, a programming challenge/contest would\n>> be one of the best ways to find the best possible, Bitcoin-specific\n>> compressor. This is the kind of self-contained exercise that bright\n>> young hackers love to tackle. It'd bring in new programmers into the\n>> ecosystem, and many of us would love to discover the limits of\n>> compressibility for Bitcoin bits on a wire. And the results would be\n>> interesting even if the final compression engine is not enabled by\n>> default, or not even merged.\n>>\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Peter Tschipper",
                "date": "2015-12-02T23:02:20",
                "message_text_only": "On 02/12/2015 2:23 PM, Matt Corallo wrote:\n> My issue is more that its additional complexity and attack surface,\n> and for a very minor gain \nWhat is a minor gain?  15 to 27% compression sounds good to me and the\nlarger the data the better the compression.  And although there is a\ndecent peformance gain in proportion to the % of compression, the\noriginal motivation of the BIP was to reduce bandwidth for users in\nregions where they are subject to caps. \n> which should disappear with further optimization elsewhere \nWhy would the benefit of compressing data disappear with further\noptimizations elsewhere, I'm not following you?.  The compression of\ndata mainly has benefit in the sending of packets over the network.  I\nwould think the performance gain would be cumulative.  Why would this go\naway by optimizing elsewhere?\n\n> and less that we absolutely shouldn't add compression because we're\n> definitely gonna have issues.\nIt's not that difficult to add compression.  Even if there was an issue,\nthe compression feature can be completely turned off. \n\n>\n> On 12/02/15 20:16, Peter Tschipper via bitcoin-dev wrote:\n>> Building a compressor from scratch may yeild some better compression\n>> ratios, or not, but having trust and faith in whether it will stand up\n>> against attack vectors another matter.  LZO has been around for 20 years\n>> with very few problems and no current issues.  Maybe something better\n>> can be built, but when and how much testing will need to be done before\n>> it can be trusted?  Right now there is something that provides a benefit\n>> and in the future if something better is found it's not that difficult\n>> to add it.  We could easily support multiple compression libraries.\n>>\n>>\n>> On 02/12/2015 10:57 AM, Emin G\u00fcn Sirer wrote:\n>>> Thanks Peter for the careful, quantitative work.\n>>>\n>>> I want to bring one additional issue to everyone's consideration,\n>>> related to the choice of the Lempel-Ziv family of compressors.\n>>>\n>>> While I'm not familiar with every single compression engine tested,\n>>> the Lempel-Ziv family of compressors are generally based on\n>>> \"compression tables.\" Essentially, they assign a short unique number\n>>> to every new subsequence they encounter, and when they re-encounter a\n>>> sequence like \"ab\" in \"abcdfdcdabcdfabcdf\" they replace it with that\n>>> short integer (say, in this case, 9-bit constant 256). So this example\n>>> sequence may turn into \"abcdfd<258 for cd><256 for ab><258 for\n>>> cd>f<261 for abc><259 for df>\" which is slightly shorter than the\n>>> original (I'm doing this off the top of my head so the counts may be\n>>> off, but it's meant to be illustrative). Note that the sequence \"abc\"\n>>> got added into the table only after it was encountered twice in the\n>>> input.\n>>>\n>>> This is nice and generic and works well for English text where certain\n>>> letter sequences (e.g. \"it\" \"th\" \"the\" \"this\" \"are\" \"there\" etc) are\n>>> repeated often, but it is nowhere as compact as it could possibly be\n>>> for mostly binary data -- there are opportunities for much better\n>>> compression, made possible by the structured reuse of certain byte\n>>> sequences in the Bitcoin wire protocol.\n>>>\n>>> On a Bitcoin wire connection, we might see several related\n>>> transactions reorganizing cash in a set of addresses, and therefore,\n>>> several reuses of a 20-byte address. Or we might see a 200-byte\n>>> transaction get transmitted, followed by the same transaction,\n>>> repeated in a block. Ideally, we'd learn the sequence that may be\n>>> repeated later on, all at once (e.g. a Bitcoin address or a\n>>> transaction), and replace it with a short number, referring back to\n>>> the long sequence. In the example above, if we knew that \"abcdf\" was a\n>>> UNIT that would likely be repeated, we would put it into the\n>>> compression table as a whole, instead of relying on repetition to get\n>>> it into the table one extra byte at a time. That may let us compress\n>>> the original sequence down to \"abcdfd<257 for cd><256 for abcdf><256\n>>> for abcdf>\" from the get go.\n>>>\n>>> Yet the LZ variants I know of will need to see a 200-byte sequence\n>>> repeated **199 times** in order to develop a single, reusable,\n>>> 200-byte long subsequence in the compression table.\n>>>\n>>> So, a Bitcoin-specific compressor can perhaps do significantly better,\n>>> but is it a good idea? Let's argue both sides.\n>>>\n>>> Cons:\n>>>\n>>> On the one hand, Bitcoin-specific compressors will be closely tied to\n>>> the contents of messages, which might make it difficult to change the\n>>> wire format later on -- changes to the wire format may need\n>>> corresponding changes to the compressor.  If the compressor cannot be\n>>> implemented cleanly, then the protocol-agnostic, off-the-shelf\n>>> compressors have a maintainability edge, which comes at the expense of\n>>> the compression ratio.\n>>>\n>>> Another argument is that compression algorithms of any kind should be\n>>> tested thoroughly before inclusion, and brand new code may lack the\n>>> maturity required. While this argument has some merit, all outputs are\n>>> verified separately later on during processing, so\n>>> compression/decompression errors can potentially be detected. If the\n>>> compressor/decompressor can be structured in a way that isolates\n>>> bitcoind from failure (e.g. as a separate process for starters), this\n>>> concern can be remedied.\n>>>\n>>> Pros:\n>>>\n>>> The nature of LZ compressors leads me to believe that much higher\n>>> compression ratios are possible by building a custom, Bitcoin-aware\n>>> compressor. If I had to guess, I would venture that compression ratios\n>>> of 2X or more are possible in some cases. In some sense, the \"O(1)\n>>> block propagation\" idea that Gavin proposed a while ago can be seen as\n>>> extreme example of a Bitcoin-specific compressor, albeit one that\n>>> constrains the order of transactions in a block.\n>>>\n>>> Compression can buy us some additional throughput at zero cost, modulo\n>>> code complexity.\n>>> Given the amount of acrimonious debate over the block size we have all\n>>> had to endure, it seems\n>>> criminal to leave potentially free improvements on the table. Even if\n>>> the resulting code is\n>>> deemed too complex to include in the production client right now, it\n>>> would be good to understand\n>>> the potential for improvement.\n>>>\n>>> How to Do It\n>>>\n>>> If we want to compress Bitcoin, a programming challenge/contest would\n>>> be one of the best ways to find the best possible, Bitcoin-specific\n>>> compressor. This is the kind of self-contained exercise that bright\n>>> young hackers love to tackle. It'd bring in new programmers into the\n>>> ecosystem, and many of us would love to discover the limits of\n>>> compressibility for Bitcoin bits on a wire. And the results would be\n>>> interesting even if the final compression engine is not enabled by\n>>> default, or not even merged.\n>>>\n>>\n>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>"
            },
            {
                "author": "Matt Corallo",
                "date": "2015-12-04T13:30:33",
                "message_text_only": "On December 3, 2015 7:02:20 AM GMT+08:00, Peter Tschipper <peter.tschipper at gmail.com> wrote:\n>On 02/12/2015 2:23 PM, Matt Corallo wrote:\n>> My issue is more that its additional complexity and attack surface,\n>> and for a very minor gain \n>What is a minor gain?  15 to 27% compression sounds good to me and the\n>larger the data the better the compression.  And although there is a\n>decent peformance gain in proportion to the % of compression, the\n>original motivation of the BIP was to reduce bandwidth for users in\n>regions where they are subject to caps.\n\nOk. It wasn't clear to me that you weren't also claiming at latency reduction as a result. In any case, the point I was making is that the p2p protocol isn't for every use-case. Indeed, I agree (as noted previously) that we should support people who have very restrictive data usage limits, but I don't think we need to do this in the p2p protocol. Considering we're in desperate need of more ways to sync, supporting syncing over slow and/or very restrictive connections is something maybe better addressed by a sync-over-http-via-cdn protocol than the p2p protocol.\n\n>> which should disappear with further optimization elsewhere \n>Why would the benefit of compressing data disappear with further\n>optimizations elsewhere, I'm not following you?.  The compression of\n>data mainly has benefit in the sending of packets over the network.  I\n>would think the performance gain would be cumulative.  Why would this\n>go\n>away by optimizing elsewhere?\n\nMy point is that, with limited further optimization, and especially after the first hundred thousand blocks, block download should nearly never be the thing limiting IBD speed.\n\n>> and less that we absolutely shouldn't add compression because we're\n>> definitely gonna have issues.\n>It's not that difficult to add compression.  Even if there was an\n>issue,\n>the compression feature can be completely turned off. \n\nNo matter how easily you can implement something, complexity always has cost. This is especially true in complicated, incredibly security critical applications exposed to the internet.\n\n>>\n>> On 12/02/15 20:16, Peter Tschipper via bitcoin-dev wrote:\n>>> Building a compressor from scratch may yeild some better compression\n>>> ratios, or not, but having trust and faith in whether it will stand\n>up\n>>> against attack vectors another matter.  LZO has been around for 20\n>years\n>>> with very few problems and no current issues.  Maybe something\n>better\n>>> can be built, but when and how much testing will need to be done\n>before\n>>> it can be trusted?  Right now there is something that provides a\n>benefit\n>>> and in the future if something better is found it's not that\n>difficult\n>>> to add it.  We could easily support multiple compression libraries.\n>>>\n>>>\n>>> On 02/12/2015 10:57 AM, Emin G\u00fcn Sirer wrote:\n>>>> Thanks Peter for the careful, quantitative work.\n>>>>\n>>>> I want to bring one additional issue to everyone's consideration,\n>>>> related to the choice of the Lempel-Ziv family of compressors.\n>>>>\n>>>> While I'm not familiar with every single compression engine tested,\n>>>> the Lempel-Ziv family of compressors are generally based on\n>>>> \"compression tables.\" Essentially, they assign a short unique\n>number\n>>>> to every new subsequence they encounter, and when they re-encounter\n>a\n>>>> sequence like \"ab\" in \"abcdfdcdabcdfabcdf\" they replace it with\n>that\n>>>> short integer (say, in this case, 9-bit constant 256). So this\n>example\n>>>> sequence may turn into \"abcdfd<258 for cd><256 for ab><258 for\n>>>> cd>f<261 for abc><259 for df>\" which is slightly shorter than the\n>>>> original (I'm doing this off the top of my head so the counts may\n>be\n>>>> off, but it's meant to be illustrative). Note that the sequence\n>\"abc\"\n>>>> got added into the table only after it was encountered twice in the\n>>>> input.\n>>>>\n>>>> This is nice and generic and works well for English text where\n>certain\n>>>> letter sequences (e.g. \"it\" \"th\" \"the\" \"this\" \"are\" \"there\" etc)\n>are\n>>>> repeated often, but it is nowhere as compact as it could possibly\n>be\n>>>> for mostly binary data -- there are opportunities for much better\n>>>> compression, made possible by the structured reuse of certain byte\n>>>> sequences in the Bitcoin wire protocol.\n>>>>\n>>>> On a Bitcoin wire connection, we might see several related\n>>>> transactions reorganizing cash in a set of addresses, and\n>therefore,\n>>>> several reuses of a 20-byte address. Or we might see a 200-byte\n>>>> transaction get transmitted, followed by the same transaction,\n>>>> repeated in a block. Ideally, we'd learn the sequence that may be\n>>>> repeated later on, all at once (e.g. a Bitcoin address or a\n>>>> transaction), and replace it with a short number, referring back to\n>>>> the long sequence. In the example above, if we knew that \"abcdf\"\n>was a\n>>>> UNIT that would likely be repeated, we would put it into the\n>>>> compression table as a whole, instead of relying on repetition to\n>get\n>>>> it into the table one extra byte at a time. That may let us\n>compress\n>>>> the original sequence down to \"abcdfd<257 for cd><256 for\n>abcdf><256\n>>>> for abcdf>\" from the get go.\n>>>>\n>>>> Yet the LZ variants I know of will need to see a 200-byte sequence\n>>>> repeated **199 times** in order to develop a single, reusable,\n>>>> 200-byte long subsequence in the compression table.\n>>>>\n>>>> So, a Bitcoin-specific compressor can perhaps do significantly\n>better,\n>>>> but is it a good idea? Let's argue both sides.\n>>>>\n>>>> Cons:\n>>>>\n>>>> On the one hand, Bitcoin-specific compressors will be closely tied\n>to\n>>>> the contents of messages, which might make it difficult to change\n>the\n>>>> wire format later on -- changes to the wire format may need\n>>>> corresponding changes to the compressor.  If the compressor cannot\n>be\n>>>> implemented cleanly, then the protocol-agnostic, off-the-shelf\n>>>> compressors have a maintainability edge, which comes at the expense\n>of\n>>>> the compression ratio.\n>>>>\n>>>> Another argument is that compression algorithms of any kind should\n>be\n>>>> tested thoroughly before inclusion, and brand new code may lack the\n>>>> maturity required. While this argument has some merit, all outputs\n>are\n>>>> verified separately later on during processing, so\n>>>> compression/decompression errors can potentially be detected. If\n>the\n>>>> compressor/decompressor can be structured in a way that isolates\n>>>> bitcoind from failure (e.g. as a separate process for starters),\n>this\n>>>> concern can be remedied.\n>>>>\n>>>> Pros:\n>>>>\n>>>> The nature of LZ compressors leads me to believe that much higher\n>>>> compression ratios are possible by building a custom, Bitcoin-aware\n>>>> compressor. If I had to guess, I would venture that compression\n>ratios\n>>>> of 2X or more are possible in some cases. In some sense, the \"O(1)\n>>>> block propagation\" idea that Gavin proposed a while ago can be seen\n>as\n>>>> extreme example of a Bitcoin-specific compressor, albeit one that\n>>>> constrains the order of transactions in a block.\n>>>>\n>>>> Compression can buy us some additional throughput at zero cost,\n>modulo\n>>>> code complexity.\n>>>> Given the amount of acrimonious debate over the block size we have\n>all\n>>>> had to endure, it seems\n>>>> criminal to leave potentially free improvements on the table. Even\n>if\n>>>> the resulting code is\n>>>> deemed too complex to include in the production client right now,\n>it\n>>>> would be good to understand\n>>>> the potential for improvement.\n>>>>\n>>>> How to Do It\n>>>>\n>>>> If we want to compress Bitcoin, a programming challenge/contest\n>would\n>>>> be one of the best ways to find the best possible, Bitcoin-specific\n>>>> compressor. This is the kind of self-contained exercise that bright\n>>>> young hackers love to tackle. It'd bring in new programmers into\n>the\n>>>> ecosystem, and many of us would love to discover the limits of\n>>>> compressibility for Bitcoin bits on a wire. And the results would\n>be\n>>>> interesting even if the final compression engine is not enabled by\n>>>> default, or not even merged.\n>>>>\n>>>\n>>>\n>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-12-03T19:14:55",
                "message_text_only": "On Wed, Dec 2, 2015 at 1:57 PM, Emin G\u00fcn Sirer <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> How to Do It\n>\n> If we want to compress Bitcoin, a programming challenge/contest would be\n> one of the best ways to find the best possible, Bitcoin-specific\n> compressor. This is the kind of self-contained exercise that bright young\n> hackers love to tackle. It'd bring in new programmers into the ecosystem,\n> and many of us would love to discover the limits of compressibility for\n> Bitcoin bits on a wire. And the results would be interesting even if the\n> final compression engine is not enabled by default, or not even merged.\n>\n\nI love this idea. Lets build a standardized data set to test against using\nreal data from the network (has anybody done this yet?).\n\nSomething like:\n\nStarting network topology:\nlist of:  nodeid, nodeid, network latency between the two peers\n\nChanges to network topology:\nlist of:  nodeid, add/remove nodeid, time of change\n\nTransaction broadcasts:\nlist of :  transaction, node id that first broadcast, time first broadcast\n\nBlock broadcasts:\nlist of :  block, node id that first broadcast, time first broadcast\n\nProposed transaction/block optimizations could then be measured against\nthis standard data set.\n\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151203/fd6a0dcd/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2015-12-03T23:07:56",
                "message_text_only": "Gavin Andresen via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>\nwrites:\n> On Wed, Dec 2, 2015 at 1:57 PM, Emin G\u00fcn Sirer <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> How to Do It\n>>\n>> If we want to compress Bitcoin, a programming challenge/contest would be\n>> one of the best ways to find the best possible, Bitcoin-specific\n>> compressor. This is the kind of self-contained exercise that bright young\n>> hackers love to tackle. It'd bring in new programmers into the ecosystem,\n>> and many of us would love to discover the limits of compressibility for\n>> Bitcoin bits on a wire. And the results would be interesting even if the\n>> final compression engine is not enabled by default, or not even merged.\n>>\n>\n> I love this idea. Lets build a standardized data set to test against using\n> real data from the network (has anybody done this yet?).\n\nhttps://github.com/rustyrussell/bitcoin-corpus\n\nIt includes mempool contents and tx receipt logs for 1 week across 4\nnodes.  I vaguely plan to update it every year.\n\nA more ambitious version would add some topology information, but we\nneed to figure out some anonymization strategy for the data.\n\nCheers,\nRusty."
            },
            {
                "author": "Peter Tschipper",
                "date": "2015-12-02T23:05:10",
                "message_text_only": "On 30/11/2015 9:28 PM, Matt Corallo wrote:\n> I'm really not a fan of this at all. To start with, adding a compression library that is directly accessible to the network on financial software is a really, really scary idea. \nWhy scary?  LZO has no current security issues, and it will be\nconfigureable by each node operator so it can be turned off completely\nif needed or desired. \n> If there were a massive improvement, I'd find it acceptable, but the improvement you've shown really isn't all that much.\nWhy is 15% at the low end, to 27% at the high end not good?  It sounds\nlike a very good boost.   \n>  The numbers you recently posted show it improving the very beginning of IBD somewhat over high-latency connections, but if we're throughput-limited after the very beginning of IBD, we should fix that, not compress the blocks. \nI only did the compression up to the 200,000 block to better isolate the\ntransmission of data from the post processing of blocks and determine\nwhether the compressing of data was adding to much to the total\ntransmission time.\n\nI think it's clear from the data that as the data (blocks, transactions)\nincrease in size that (1) they compress better and (2) they have a\nbigger and positive impact on improving performance when compressed.\n\n> Additionally, I'd be very surprised if this had any significant effect on the speed at which new blocks traverse the network (do you have any simulations or other thoughts on this?).\n>From the table below, at 120000 blocks the time to sync the chain was\nroughly the same for compressed vs. uncompressed however after that\npoint as block sizes start increasing, all compression libraries\npeformed much faster than uncompressed. The data provided in this\ntesting clearly shows that as block size increases, the performance\nimprovement by compressing data also increases.\n\nTABLE 5:\nResults shown in seconds with 60ms of induced latency\nNum blks sync'd  Uncmp  Zlib-1  Zlib-6  LZO1x-1  LZO1x-999\n---------------  -----  ------  ------  -------  ---------\n120000           3226   3416    3397    3266     3302\n130000           4010   3983    3773    3625     3703\n140000           4914   4503    4292    4127     4287\n150000           5806   4928    4719    4529     4821\n160000           6674   5249    5164    4840     5314\n170000           7563   5603    5669    5289     6002\n180000           8477   6054    6268    5858     6638\n190000           9843   7085    7278    6868     7679\n200000           11338  8215    8433    8044     8795\n\n\nAs far as, what happens after the block is received, then obviously\ncompression isn't going to help in post processing and validating the\nblock, but in the pure transmission of the object it most certainly and\nlogically does and in a fairly direct proportion to the file size (a\nfile that is 20% smaller will be transmited \"at least\" 20% faster, you\ncan use any data transfer time calculator\n<http://www.calctool.org/CALC/prof/computing/transfer_time> for that). \nThe only issue, that I can see that required testing was to show how\nmuch compression there would be, and how much time the compression of\nthe data would add to the sending of the data.\n\n \n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151202/9212a042/attachment-0001.html>"
            },
            {
                "author": "Dave Scotese",
                "date": "2015-12-03T05:52:20",
                "message_text_only": "Emin's email presents to me the idea of dictionaries that already contain\nthe data we'd want to compress.  With 8 bytes of indexing data, we can\nrefer to a TxID or a Public Key or any existing part of the blockchain.\nThere are also data sequences like scripts that contain a few variable\nchunks and are otherwise identical.  Often, the receiver has the\nblockchain, which contains a lot of the data that is in the message being\ntransmitted.\n\nFirst, the receiver must indicate that compressed data is preferred and the\nheight of latest valid block it holds, and the sender must express the\nability to send compressed data.  From this state, the sender sends\nmessages that are compressed.  Compressed messages are the same as\nuncompressed messages except that:\n\n   1. Data read is copied into the decompressed message until the first\n   occurrence of 0x00, which is discarded and is followed by compressed data.\n   2. Compressed data can use as a dictionary the first 16,777,215 blocks,\n   or the last 4,244,635,647 ending with the block at the tip of the\n   receiver's chain, or it can specify a run of zero bytes.  The sender and\n   receiver must agree on the *receiver's* current block height in order to\n   use the last 4B blocks as the dictionary.\n   3. Within compressed data, the first byte identifies how to decompress:\n      1. 0xFF indicates that the following three bytes are a block height\n      with most significant byte 0x00 in network byte order.\n      2. 0xFE indicates that the following byte indicates how many zero\n      bytes to add to the decompressed data.\n      3. 0xFD is an error, so compressed messages are turned off and the\n      recipient fails the decompression process.\n      4. 0x00 indicates that the zero byte by itself should be added to the\n      decompressed data, and the data following is not compressed\n(return to step\n      1).\n      5. All other values represent the most significant byte of a number\n      to be subtracted from the receiver's current block height to identify a\n      block height (not available until there are least 16,777,216\nblocks so that\n      this byte can be at least 0x01, since 0x00 would indicate a single zero\n      byte, end compressed data, and return to step 1).\n   4. If decompression has identified a block height (previous byte was not\n   0xFD, 0x00, or 0xFE), then the next four bytes identify a *size *(one\n   byte) and a byte index into the block's data (three bytes), and *size *bytes\n   from that block are added to the decompressed data.\n   5. Steps 3 and 4 process a chunk of compressed data.  If the next byte\n   is 0xFD, then decompression goes back to step 1 (add raw bytes until it\n   hits a 0x00).  Otherwise, it proceeds through steps 3 (and maybe 4) again.\n\nIn Step 3.3, 0xFD causes an error, but it could be used to indicate a\nparameterized dictionary entry, for example 0xFD, 0x01 followed by eight\nmore bytes to be interpreted according to steps 3.1 or 3.5 could mean\nOP_DUP OP_HASH160 (20 bytes from the blockchain dictionary) OP_EQUALVERIFY\nOP_CHECKSIG, replacing that very common occurrence of 24 bytes with 10\nbytes.  Well, 11 if you include the 0x00 required by step5.  But that only\nworks on addresses that have spent inputs.  Or 0xFD, 0x02 could be\nshorthand for the four zeroes of lock_time, followed by Version (1),\nfollowed by 0x01 (for one-input transactions), turning nine bytes into two\nfor the data at the end of a normal (lock_time = 0) Txn and the beginning\nof a single-input Txn.  But I left 0xFD as an error because those gains\ndidn't seem as frequent as the others.\n\nDave.\n\nOn Wed, Dec 2, 2015 at 3:05 PM, Peter Tschipper via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n> On 30/11/2015 9:28 PM, Matt Corallo wrote:\n>\n> I'm really not a fan of this at all. To start with, adding a compression library that is directly accessible to the network on financial software is a really, really scary idea.\n>\n> Why scary?  LZO has no current security issues, and it will be\n> configureable by each node operator so it can be turned off completely if\n> needed or desired.\n>\n> If there were a massive improvement, I'd find it acceptable, but the improvement you've shown really isn't all that much.\n>\n> Why is 15% at the low end, to 27% at the high end not good?  It sounds\n> like a very good boost.\n>\n>  The numbers you recently posted show it improving the very beginning of IBD somewhat over high-latency connections, but if we're throughput-limited after the very beginning of IBD, we should fix that, not compress the blocks.\n>\n> I only did the compression up to the 200,000 block to better isolate the\n> transmission of data from the post processing of blocks and determine\n> whether the compressing of data was adding to much to the total\n> transmission time.\n>\n> I think it's clear from the data that as the data (blocks, transactions)\n> increase in size that (1) they compress better and (2) they have a bigger\n> and positive impact on improving performance when compressed.\n>\n> Additionally, I'd be very surprised if this had any significant effect on the speed at which new blocks traverse the network (do you have any simulations or other thoughts on this?).\n>\n> From the table below, at 120000 blocks the time to sync the chain was\n> roughly the same for compressed vs. uncompressed however after that point\n> as block sizes start increasing, all compression libraries peformed much\n> faster than uncompressed. The data provided in this testing clearly shows\n> that as block size increases, the performance improvement by compressing\n> data also increases.\n>\n> TABLE 5:\n> Results shown in seconds with 60ms of induced latency\n> Num blks sync'd  Uncmp  Zlib-1  Zlib-6  LZO1x-1  LZO1x-999\n> ---------------  -----  ------  ------  -------  ---------\n> 120000           3226   3416    3397    3266     3302\n> 130000           4010   3983    3773    3625     3703\n> 140000           4914   4503    4292    4127     4287\n> 150000           5806   4928    4719    4529     4821\n> 160000           6674   5249    5164    4840     5314\n> 170000           7563   5603    5669    5289     6002\n> 180000           8477   6054    6268    5858     6638\n> 190000           9843   7085    7278    6868     7679\n> 200000           11338  8215    8433    8044     8795\n>\n>\n> As far as, what happens after the block is received, then obviously\n> compression isn't going to help in post processing and validating the\n> block, but in the pure transmission of the object it most certainly and\n> logically does and in a fairly direct proportion to the file size (a file\n> that is 20% smaller will be transmited \"at least\" 20% faster, you can use\n> any data transfer time calculator\n> <http://www.calctool.org/CALC/prof/computing/transfer_time> for that).\n> The only issue, that I can see that required testing was to show how much\n> compression there would be, and how much time the compression of the data\n> would add to the sending of the data.\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n\n\n-- \nI like to provide some work at no charge to prove my value. Do you need a\ntechie?\nI own Litmocracy <http://www.litmocracy.com> and Meme Racing\n<http://www.memeracing.net> (in alpha).\nI'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which\nnow accepts Bitcoin.\nI also code for The Dollar Vigilante <http://dollarvigilante.com/>.\n\"He ought to find it more profitable to play by the rules\" - Satoshi\nNakamoto\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151202/c8405e2f/attachment.html>"
            },
            {
                "author": "Pavel Jan\u00edk",
                "date": "2015-12-02T06:47:28",
                "message_text_only": "> On 02 Dec 2015, at 00:44, Simon Liu <simon at bitcartel.com> wrote:\n> \n> Hi Matt/Pavel,\n> \n> Why is it scary/undesirable?  Thanks.\n\nSelect your preferable compression library and google for it with +CVE.\n\nE.g. in zlib:\n\nhttp://www.cvedetails.com/vulnerability-list/vendor_id-72/product_id-1820/GNU-Zlib.html\n\n\u2026allows remote attackers to cause a denial of service (crash) via a crafted compressed stream\u2026\n\u2026allows remote attackers to cause a denial of service (application crash)\u2026\netc.\n\nDo you want to expose such lib to the potential attacker?\n--  \nPavel Jan\u00edk"
            },
            {
                "author": "Simon Liu",
                "date": "2015-12-02T07:33:27",
                "message_text_only": "Hi Pavel,\n\n(my earlier email was moderated, so the list can only see it via your\nreply),\n\nYes, an attacker could try and send malicious data to take advantage of\na compression library vulnerability...  but is it that much worse than\nexisting attack vectors which might also result in denial of service,\ncrashes, remote execution?\n\nPeter, perhaps your BIP can look at possible ways to isolate the\ndecompression phase, such as having incoming compressed blocks be saved\nto a quarantine folder and an external process/daemon decompress and\nverify the block's hash?\n\nRegards,\nSimon\n\n\nOn 12/01/2015 10:47 PM, Pavel Jan\u00edk wrote:\n> \n>> On 02 Dec 2015, at 00:44, Simon Liu <simon at bitcartel.com> wrote:\n>>\n>> Hi Matt/Pavel,\n>>\n>> Why is it scary/undesirable?  Thanks.\n> \n> Select your preferable compression library and google for it with +CVE.\n> \n> E.g. in zlib:\n> \n> http://www.cvedetails.com/vulnerability-list/vendor_id-72/product_id-1820/GNU-Zlib.html\n> \n> \u2026allows remote attackers to cause a denial of service (crash) via a crafted compressed stream\u2026\n> \u2026allows remote attackers to cause a denial of service (application crash)\u2026\n> etc.\n> \n> Do you want to expose such lib to the potential attacker?\n> --  \n> Pavel Jan\u00edk\n> \n> \n> \n>"
            },
            {
                "author": "Patrick Strateman",
                "date": "2015-12-02T18:45:23",
                "message_text_only": "If compression is to be used a custom compression algorithm should be\nwritten.\n\nBitcoin data is largely incompressible outside of a tiny subset of fields.\n\nOn 12/01/2015 11:33 PM, Simon Liu via bitcoin-dev wrote:\n> Hi Pavel,\n>\n> (my earlier email was moderated, so the list can only see it via your\n> reply),\n>\n> Yes, an attacker could try and send malicious data to take advantage of\n> a compression library vulnerability...  but is it that much worse than\n> existing attack vectors which might also result in denial of service,\n> crashes, remote execution?\n>\n> Peter, perhaps your BIP can look at possible ways to isolate the\n> decompression phase, such as having incoming compressed blocks be saved\n> to a quarantine folder and an external process/daemon decompress and\n> verify the block's hash?\n>\n> Regards,\n> Simon\n>\n>\n> On 12/01/2015 10:47 PM, Pavel Jan\u00edk wrote:\n>>> On 02 Dec 2015, at 00:44, Simon Liu <simon at bitcartel.com> wrote:\n>>>\n>>> Hi Matt/Pavel,\n>>>\n>>> Why is it scary/undesirable?  Thanks.\n>> Select your preferable compression library and google for it with +CVE.\n>>\n>> E.g. in zlib:\n>>\n>> http://www.cvedetails.com/vulnerability-list/vendor_id-72/product_id-1820/GNU-Zlib.html\n>>\n>> \u2026allows remote attackers to cause a denial of service (crash) via a crafted compressed stream\u2026\n>> \u2026allows remote attackers to cause a denial of service (application crash)\u2026\n>> etc.\n>>\n>> Do you want to expose such lib to the potential attacker?\n>> --  \n>> Pavel Jan\u00edk\n>>\n>>\n>>\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "Datastream compression of Blocks and Transactions",
            "categories": [
                "bitcoin-dev",
                "BIP Draft"
            ],
            "authors": [
                "Rusty Russell",
                "Emin G\u00fcn Sirer",
                "Pavel Jan\u00edk",
                "Patrick Strateman",
                "Peter Tschipper",
                "Dave Scotese",
                "Simon Liu",
                "Gavin Andresen",
                "Matt Corallo"
            ],
            "messages_count": 14,
            "total_messages_chars_count": 65977
        }
    },
    {
        "title": "[bitcoin-dev] Opt-in Full Replace-By-Fee (Full-RBF)",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2015-12-02T09:27:39",
                "message_text_only": "On Sun, Nov 29, 2015 at 10:32:34PM -0500, Chris via bitcoin-dev wrote:\n> On 11/16/2015 07:42 PM, Peter Todd via bitcoin-dev wrote:\n> > Sequence is used for opting in as it is the only \"free-form\" field\n> > available for that purpose. Opt-in per output was proposed as well by\n> > Luke-Jr, however the CTxOut data structure simply doesn't contain any\n> > extra fields to use for that purpose.\n> What is wrong with using they same scheme as sighash_single?\n> \n> If input 0 has nSequence < maxint-1 then output 0 is replaceable.\n> \n> For fee bumps you would just stick the change in position zero and\n> reduce the value.\n> \n> You get FFS functionality without the hassle of addition other inputs.\n\nAgain, you're giving the whole world information about what's your\nchange address; that's simply unacceptable for privacy.\n\nThe only way to solve this is by a scheme where you pre-commit via a\nhash, and reveal that later, which is extremely complex and not easily\nfeasible given the current tx data structure.\n\n-- \n'peter'[:-1]@petertodd.org\n0000000000000000019a7c015d7b61baa25e8afd4f1dcade4133d8a1d6b7445d\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151202/2b58fcc7/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "Opt-in Full Replace-By-Fee (Full-RBF)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Peter Todd"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1378
        }
    },
    {
        "title": "[bitcoin-dev] Scaling Bitcoin - summarizing non-jgarzik block size BIPs",
        "thread_messages": [
            {
                "author": "Jeff Garzik",
                "date": "2015-12-02T16:39:57",
                "message_text_only": "To collect things into one place, I was asked by Kanzure to cover\nnon-jgarzik block size BIPs in a quick summary, and the Scaling Bitcoin\nconf folks have graciously allocated a bit of extra time for this.\n\ne.g. BIP 100.5, 103, 105, 106 - \"the serious ones\"\n\nIf there is some input people would like to add to the meat grinder prior\nto Dec 7, email jeff at bloq.com\n\nThanks!\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151202/bd1e71c8/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Scaling Bitcoin - summarizing non-jgarzik block size BIPs",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeff Garzik"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 556
        }
    },
    {
        "title": "[bitcoin-dev] Blockchain verification flag (BIP draft)",
        "thread_messages": [
            {
                "author": "Gregory Maxwell",
                "date": "2015-12-04T08:26:22",
                "message_text_only": "For discussion,\n\nA significant fraction of hashrate currently mines blocks without\nverifying them for a span of time after a new block shows up on the\nnetwork for economically rational reasons. This otherwise harmful\nbehavior can be made a beneficial to the whole network; but only if it\nis communicated.\n\nThis BIP proposal suggests a communication channel and describes its\nuse and the motivations for it.  I wrote it in response to suggestions\nthat Bitcoin Core add explicit support for this kind of mining, which\ncould also implement best in class risk mitigations. I believe\nsignaling the behavior is a necessary component for risk mitigation\nhere.\n\n-----------------------------------------------------------------\n\n<pre>\n  BIP: draft-maxwell-flagverify\n  Title: Blockchain verification flag\n  Author: Greg Maxwell <greg at xiph.org>\n  Status: Draft\n  Type: Standards Track\n  Created: 2015-12-02\n</pre>\n\n==Abstract==\n\nThis BIP describes a flag that the authors of blocks can use to voluntarily\nsignal that they have completely validated the content of their\nblock and the blocks before it.\n\nCorrect use of this signaling is not enforced internally to the network\nbut if used it can act as a hint allowing more intelligent risk analysis.\n\nIf deployed and adhered to, this mechanism turns otherwise harmful\nvalidation skipping by miners into a behavior which benefits the public.\n\n==Summary==\n\nThe version field in a Bitcoin block header is a 32-bit signed integer.\n\nThe most significant bit (30) of the block version is defined to signal that\nthe author of the block has validated the whole chain up to and including the\ncontent of the block.\n\nConforming miners MUST NOT set this flag when they have not completely\nvalidated the prior block(s) or the content of their own block.  Miners\nshould continue to try to minimize the amount of time spent mining\non a non-validated chain.  Blocks which extend an invalid chain will\ncontinue to be rejected and ultimately orphaned as validation catches up.\n\nIt is recommended, but not required, that miners also not set the flag on blocks\ncreated by the same device which created the block immediately prior.  This\nwill reduce the incorrect implication of independent validation when the two\nmost recent blocks are both the product of the same, single, faulty system.\n\nThe set state for the bit is defined as verified so that that\nun(der)maintained systems do not falsely signal validation.\n\nNon-verifying clients of the network may check this bit (e.g. checking\nthat the version is >= 1073741824) and use it as an input to their risk\nmodeling.  It is recommended that once this BIP is widely accepted by the\nnetwork that non-full-node wallets refrain from counting confirmations on\nblocks where the bit is not set.\n\nThe authors of non-verifying clients should keep in mind that this flag\nis only correct with the cooperation of the block author, and even then\na validating miner may still accidentally accept or produce an invalid\nblock due to faulty hardware or software.  Additionally, any miner which\ncorrectly uses this flag could stop doing so at any time, and might\ndo so intentionally in order to increase the effectiveness of an attack.\nAs a result of misunderstanding, misconfiguration, laziness, or other\nhuman factors some miners may falsely set the flag.  Because invalid\nblocks are rare it may take a long time to detect misuse of the flag.\n\nAs such, the accuracy of this field MUST NOT be strongly relied upon.\n\nEspecially due to the non-enforceability of the flag, the user community\nshould keep in mind that both setting the flag correctly and mining\nwithout verification (for brief periods of time) are healthy for the\nnetwork.  If participants are punished for following this specification\nthey will simply lie, and its utility will be diminished.\n\n==Motivation==\n\nSome applications of the Bitcoin system such as thin-client wallets make\na strong assumption that all the authors of the blocks have faithfully\nverified the blockchain.  Because many of these applications also take\nirreversible actions based on only one or two confirmations and the time\nbetween blocks is often very short, these clients are vulnerable to\neven small and short-duration violations of this assumption.\n\nProcessing and propagation delays resulting from increased transaction\nload contribute to block orphaning when multiple blocks are found at\nclose to the same time. This has caused some miners to work on extending\nthe chain with the most proof-of-work prior to validating the latest\nblock(s).\n\nAlthough this validation skipping undermines the security assumptions\nof thin clients, it also has a beneficial effect: these delays also\nmake the mining process unfair and cause increased rewards for the\nlargest miners relative to other miners, resulting in a centralization\npressure.  Deferring validation can reduce this pressure and improve\nthe security of the Bitcoin system long term.\n\nThis BIP seeks to mitigate the harm of breaking the thin client\nassumption by allowing miners to efficiently provide additional\ninformation on their level of validation.  By doing so the\nnetwork can take advantage of the benefits of bypassed\nvalidation with minimal collateral damage.\n\n==Deployment==\n\nBecause there is no consensus enforced behavior there is no special\ndeployment strategy required.  [BIP 9 will need to be updated.]\n\n==Credits==\n\nThanks goes to Jeremy Rubin for his two-phase mining suggestion\nwhich inspired this simplified proposal.\n\n==Copyright==\n\nThis document is placed in the public domain."
            },
            {
                "author": "Jannes Faber",
                "date": "2015-12-04T12:44:52",
                "message_text_only": "1) (I would assume this is already current default behaviour, but just in\ncase.) Would it not make sense to *never* send a blockheader to an SPV\nclient unless the node itself fully validated that block? Regardless of who\nmined the block and whether this verification flag has been set or not.\n\n2) Besides having your verification flag in the block, would it not also\nmake sense to have such a flag in the P2P protocol when blocks (or headers)\nare communicated? That way a node could simply do some quick sanity checks\n(difficulty as anti-DOS) on an incoming block and then immediately\npropagate it to the next (non-SPV) node, but with a flag \"Looks good, but I\nhaven't fully validated it myself, so please don't blame me\". And if the\nblock does turn out to be invalid, the node does not get banned if it was\nhonest about it.\n\n3) With the above implemented, I can imagine miners running 2 (or more)\nnodes side by side, one of them doesn't validate in order to reduce latency\nand orphan rates, but the other one does validate and quickly signals the\nfirst one if there's a problem. Both nodes don't necessarily need to be in\nthe same network or even on the same side of the Great Firewall. Of course\nthey would be whitelisting each other for trust, or the signal would need\nto include some sort of proof.\n\nThis probably has been suggested many times already, sorry if this is a\ndumb idea.\n\n--\nJannes\n\nOn 4 December 2015 at 09:26, Gregory Maxwell via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> For discussion,\n>\n> A significant fraction of hashrate currently mines blocks without\n> verifying them for a span of time after a new block shows up on the\n> network for economically rational reasons. This otherwise harmful\n> behavior can be made a beneficial to the whole network; but only if it\n> is communicated.\n>\n> This BIP proposal suggests a communication channel and describes its\n> use and the motivations for it.  I wrote it in response to suggestions\n> that Bitcoin Core add explicit support for this kind of mining, which\n> could also implement best in class risk mitigations. I believe\n> signaling the behavior is a necessary component for risk mitigation\n> here.\n>\n> -----------------------------------------------------------------\n>\n> <pre>\n>   BIP: draft-maxwell-flagverify\n>   Title: Blockchain verification flag\n>   Author: Greg Maxwell <greg at xiph.org>\n>   Status: Draft\n>   Type: Standards Track\n>   Created: 2015-12-02\n> </pre>\n>\n> ==Abstract==\n>\n> This BIP describes a flag that the authors of blocks can use to voluntarily\n> signal that they have completely validated the content of their\n> block and the blocks before it.\n>\n> Correct use of this signaling is not enforced internally to the network\n> but if used it can act as a hint allowing more intelligent risk analysis.\n>\n> If deployed and adhered to, this mechanism turns otherwise harmful\n> validation skipping by miners into a behavior which benefits the public.\n>\n> ==Summary==\n>\n> The version field in a Bitcoin block header is a 32-bit signed integer.\n>\n> The most significant bit (30) of the block version is defined to signal\n> that\n> the author of the block has validated the whole chain up to and including\n> the\n> content of the block.\n>\n> Conforming miners MUST NOT set this flag when they have not completely\n> validated the prior block(s) or the content of their own block.  Miners\n> should continue to try to minimize the amount of time spent mining\n> on a non-validated chain.  Blocks which extend an invalid chain will\n> continue to be rejected and ultimately orphaned as validation catches up.\n>\n> It is recommended, but not required, that miners also not set the flag on\n> blocks\n> created by the same device which created the block immediately prior.  This\n> will reduce the incorrect implication of independent validation when the\n> two\n> most recent blocks are both the product of the same, single, faulty system.\n>\n> The set state for the bit is defined as verified so that that\n> un(der)maintained systems do not falsely signal validation.\n>\n> Non-verifying clients of the network may check this bit (e.g. checking\n> that the version is >= 1073741824) and use it as an input to their risk\n> modeling.  It is recommended that once this BIP is widely accepted by the\n> network that non-full-node wallets refrain from counting confirmations on\n> blocks where the bit is not set.\n>\n> The authors of non-verifying clients should keep in mind that this flag\n> is only correct with the cooperation of the block author, and even then\n> a validating miner may still accidentally accept or produce an invalid\n> block due to faulty hardware or software.  Additionally, any miner which\n> correctly uses this flag could stop doing so at any time, and might\n> do so intentionally in order to increase the effectiveness of an attack.\n> As a result of misunderstanding, misconfiguration, laziness, or other\n> human factors some miners may falsely set the flag.  Because invalid\n> blocks are rare it may take a long time to detect misuse of the flag.\n>\n> As such, the accuracy of this field MUST NOT be strongly relied upon.\n>\n> Especially due to the non-enforceability of the flag, the user community\n> should keep in mind that both setting the flag correctly and mining\n> without verification (for brief periods of time) are healthy for the\n> network.  If participants are punished for following this specification\n> they will simply lie, and its utility will be diminished.\n>\n> ==Motivation==\n>\n> Some applications of the Bitcoin system such as thin-client wallets make\n> a strong assumption that all the authors of the blocks have faithfully\n> verified the blockchain.  Because many of these applications also take\n> irreversible actions based on only one or two confirmations and the time\n> between blocks is often very short, these clients are vulnerable to\n> even small and short-duration violations of this assumption.\n>\n> Processing and propagation delays resulting from increased transaction\n> load contribute to block orphaning when multiple blocks are found at\n> close to the same time. This has caused some miners to work on extending\n> the chain with the most proof-of-work prior to validating the latest\n> block(s).\n>\n> Although this validation skipping undermines the security assumptions\n> of thin clients, it also has a beneficial effect: these delays also\n> make the mining process unfair and cause increased rewards for the\n> largest miners relative to other miners, resulting in a centralization\n> pressure.  Deferring validation can reduce this pressure and improve\n> the security of the Bitcoin system long term.\n>\n> This BIP seeks to mitigate the harm of breaking the thin client\n> assumption by allowing miners to efficiently provide additional\n> information on their level of validation.  By doing so the\n> network can take advantage of the benefits of bypassed\n> validation with minimal collateral damage.\n>\n> ==Deployment==\n>\n> Because there is no consensus enforced behavior there is no special\n> deployment strategy required.  [BIP 9 will need to be updated.]\n>\n> ==Credits==\n>\n> Thanks goes to Jeremy Rubin for his two-phase mining suggestion\n> which inspired this simplified proposal.\n>\n> ==Copyright==\n>\n> This document is placed in the public domain.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151204/bbdcc1da/attachment.html>"
            },
            {
                "author": "Thomas Kerin",
                "date": "2015-12-04T16:46:34",
                "message_text_only": "1. Not relaying can cause problems. Gossip networks operate by\npropagating new information (like a single new header), and refuse to\nrelay information if it's obviously invalid.\n\n>From the POV of a full node, which will normally hear about the header\nfirst, there's no point to not telling peers about this information.\nIt's likely in the interest of SPV wallets to hear about EVERY\ncontending chain, so they can go about their business deciding which is\ncorrect.\n\n\n2. The only difference between a block and it's header is the list of\ntransactions. There isn't anywhere else to put the flag but the header's\nversion. Which is good, because clients usually receive headers first.\n\n\n3. \"Signal would need to include some sort of proof\" That's not the\npoint of this BIP. You can't prove the miner has or hasn't verified the\nchain. What purpose would it even serve? If clients accepted this\n'proof', they might ignore blocks they should pay attention to.\n\nThe BIP doesn't involve proof at all, it's just an indicator you can\nchose to use or ignore.\n\n\nOn 04/12/15 12:44, Jannes Faber via bitcoin-dev wrote:\n> nodes side by side, one of them doesn't validate in order to reduce latency"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-12-04T17:34:27",
                "message_text_only": "Overall, good idea.\n\nIs there a write-up somewhere describing in detail the 'accidental selfish\nmining' problem that this mitigates? I think a link in the BIP to a fuller\ndescription of the problem and how validation-skipping makes it go away\nwould be helpful.\n\nRE: which bit to use:  the draft versionbits BIP and BIP101 use bit 30; to\navoid confusion, I think it would be better to use bit 0.\n\nI agree with Jannes Faber, behavior with respect to SPV clients should be\nto only tell them about fully validated headers. And I also agree that\nimmediately relaying full-proof-of-work blocks before validation (with an\nindication that they haven't been fully validated) is a good idea, but that\ndiscussion didn't reach consensus when I brought it up two years ago (\nhttps://github.com/bitcoin/bitcoin/pull/3580).\n\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151204/ad61bf07/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2015-12-04T22:43:16",
                "message_text_only": "Gavin Andresen via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>\nwrites:\n> Overall, good idea.\n>\n> Is there a write-up somewhere describing in detail the 'accidental selfish\n> mining' problem that this mitigates? I think a link in the BIP to a fuller\n> description of the problem and how validation-skipping makes it go away\n> would be helpful.\n>\n> RE: which bit to use:  the draft versionbits BIP and BIP101 use bit 30; to\n> avoid confusion, I think it would be better to use bit 0.\n\nYes, BIP9 need to be adjusted (setting bit 30 means BIP9 counts it as a\nvote against all softforks).  BIP101 uses bits 0,1,2 AFAICT, so perhaps\nstart from the other end and use bit 29?  We can bikeshed that later\nthough...\n\n> I agree with Jannes Faber, behavior with respect to SPV clients should be\n> to only tell them about fully validated headers.\n\nA delicate balance.  If we penalize these blocks too much, it's\ndisincentive to set the bit.  Fortunately it's easy for SPV clients to\ndecide for themselves, I think?\n\nCheers,\nRusty."
            },
            {
                "author": "James Hilliard",
                "date": "2015-12-06T02:47:01",
                "message_text_only": "I think something that anyone who isn't validating should be aware of is\nthat cgminer(which powers the vast majority of the current mining network)\ndoesn't allow for a pool to revert to mining on the previous block due to\nthe way chain tracking is implemented.\n\nhttps://github.com/ckolivas/cgminer/blob/master/cgminer.c#L4727\n\nOn Fri, Dec 4, 2015 at 4:43 PM, Rusty Russell via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Gavin Andresen via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>\n> writes:\n> > Overall, good idea.\n> >\n> > Is there a write-up somewhere describing in detail the 'accidental\n> selfish\n> > mining' problem that this mitigates? I think a link in the BIP to a\n> fuller\n> > description of the problem and how validation-skipping makes it go away\n> > would be helpful.\n> >\n> > RE: which bit to use:  the draft versionbits BIP and BIP101 use bit 30;\n> to\n> > avoid confusion, I think it would be better to use bit 0.\n>\n> Yes, BIP9 need to be adjusted (setting bit 30 means BIP9 counts it as a\n> vote against all softforks).  BIP101 uses bits 0,1,2 AFAICT, so perhaps\n> start from the other end and use bit 29?  We can bikeshed that later\n> though...\n>\n> > I agree with Jannes Faber, behavior with respect to SPV clients should be\n> > to only tell them about fully validated headers.\n>\n> A delicate balance.  If we penalize these blocks too much, it's\n> disincentive to set the bit.  Fortunately it's easy for SPV clients to\n> decide for themselves, I think?\n>\n> Cheers,\n> Rusty.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151205/4ed6a987/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-12-06T06:26:15",
                "message_text_only": "On Sun, Dec 6, 2015 at 2:47 AM, James Hilliard via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> I think something that anyone who isn't validating should be aware of is\n> that cgminer(which powers the vast majority of the current mining network)\n> doesn't allow for a pool to revert to mining on the previous block due to\n> the way chain tracking is implemented.\n\nAn interesting potential use for the flag suggested in this draft\nwould be allowing non-monotone mining for non-verified blocks.\n\nI could add a recommendation for that as well."
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-12-06T05:13:15",
                "message_text_only": "On Fri, Dec 4, 2015 at 10:43 PM, Rusty Russell <rusty at rustcorp.com.au> wrote:\n>> I agree with Jannes Faber, behavior with respect to SPV clients should be\n>> to only tell them about fully validated headers.\n>\n> A delicate balance.  If we penalize these blocks too much, it's\n> disincentive to set the bit.  Fortunately it's easy for SPV clients to\n> decide for themselves, I think?\n\nI think this is orthogonal: You should only tell SPV clients* about\nblocks you've fully validated yourself.  The bit in the header doesn't\nmatter with respect to that. (Effectively, the wallet risk model gets\nas input the fact that they got given the block in the first place as\nwell as the flag where the miner said they were validating things or\nnot.)\n\nWhatever the ideal behavior is for network nodes towards lite clients;\nit's insanely cheap to just spin up a lot of 'nodes' that have\narbitrary behavior; so it shouldn't be relied on too much; but\nabsolutely they should be filtering to things they've verified\nthemselves... unlike the mining case, there is no reason not to.\n\n[Specific attacks to consider: You get a broken miner to include both\nyour payment, and some invalid transaction. Other miners extend it\nwithout verifying. To avoid the fact that nodes sensibly filter\ninvalid blocks from their lite clients, you simply just run a lot of\n'nodes' so that virtually every lite client has a connection to you]\n\n(More specifically, peers should be able to specify that they want to\nknow about pre-validated blocks and you should be able to fetch blocks\nfrom them which haven't been validated... but no one should get fed\nunverified blocks by surprise.)"
            }
        ],
        "thread_summary": {
            "title": "Blockchain verification flag (BIP draft)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Rusty Russell",
                "Thomas Kerin",
                "James Hilliard",
                "Gregory Maxwell",
                "Jannes Faber",
                "Gavin Andresen"
            ],
            "messages_count": 8,
            "total_messages_chars_count": 20523
        }
    },
    {
        "title": "[bitcoin-dev] Some transcripts from the Scaling Bitcoin workshops",
        "thread_messages": [
            {
                "author": "Bryan Bishop",
                "date": "2015-12-07T06:50:23",
                "message_text_only": "Hey while I was listening to the talks I also typed most of the words down.\n\nHere are some talks from the Hong Kong workshop:\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/bip99-and-uncontroversial-hard-forks/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/fungibility-and-scalability/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/zero-knowledge-proofs-for-bitcoin-scalability-and-beyond/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/security-assumptions/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/in-adversarial-environments-blockchains-dont-scale/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/why-miners-will-not-voluntarily-individually-produce-smaller-blocks/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/invertible-bloom-lookup-tables-and-weak-block-propagation-performance/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/bip101-block-propagation-data-from-testnet/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/segregated-witness-and-its-impact-on-scalability/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/overview-of-bips-necessary-for-lightning/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/network-topologies-and-their-scalability-implications-on-decentralized-off-chain-networks/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/a-bevy-of-block-size-proposals-bip100-bip102-and-more/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/a-flexible-limit-trading-subsidy-for-larger-blocks/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/validation-cost-metric/\n\nAlso, here are some talks from the Montreal workshop:\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/alternatives-to-block-size-as-aggregate-resource-limits/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/bitcoin-block-propagation-iblt-rusty-russell/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/bitcoin-failure-modes-and-the-role-of-the-lightning-network/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/bitcoin-load-spike-simulation/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/block-synchronization-time/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/coinscope-andrew-miller/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/competitive-fee-market-urgency/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/issues-impacting-block-size-proposals/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/overview-of-security-concerns/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/relay-network/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/reworking-bitcoin-core-p2p-code-for-robustness-and-event-driven/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/sharding-the-blockchain/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/transaction-fee-estimation/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/validation-costs/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/roundgroup-roundup-1/\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/roundgroup-roundup-2/\n\nThese are not always exact transcripts because I am typing while I am\nlistening, thus there are mistakes including typos and listening errors, so\nplease keep this discrepancy in mind between what's said and what's typed.\n\n- Bryan\nhttp://heybryan.org/\n1 512 203 0507\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151207/0d5da746/attachment.html>"
            },
            {
                "author": "Johnathan Corgan",
                "date": "2015-12-07T07:20:19",
                "message_text_only": "On Sun, Dec 6, 2015 at 10:50 PM, Bryan Bishop via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n\n> Hey while I was listening to the talks I also typed most of the words down\n>\n\n\u200bThis was clearly a lot of work...thanks again.\u200b\n\n-- \nJohnathan Corgan\nCorgan Labs - SDR Training and Development Services\nhttp://corganlabs.com\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151206/426750af/attachment.html>"
            },
            {
                "author": "Pindar Wong",
                "date": "2015-12-07T07:21:58",
                "message_text_only": "Awesome...Thank you Bryan!\n\nP.\n\nOn Sunday, December 6, 2015, Bryan Bishop via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hey while I was listening to the talks I also typed most of the words down.\n>\n> Here are some talks from the Hong Kong workshop:\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/bip99-and-uncontroversial-hard-forks/\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/fungibility-and-scalability/\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/zero-knowledge-proofs-for-bitcoin-scalability-and-beyond/\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/security-assumptions/\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/in-adversarial-environments-blockchains-dont-scale/\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/why-miners-will-not-voluntarily-individually-produce-smaller-blocks/\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/invertible-bloom-lookup-tables-and-weak-block-propagation-performance/\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/bip101-block-propagation-data-from-testnet/\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/segregated-witness-and-its-impact-on-scalability/\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/overview-of-bips-necessary-for-lightning/\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/network-topologies-and-their-scalability-implications-on-decentralized-off-chain-networks/\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/a-bevy-of-block-size-proposals-bip100-bip102-and-more/\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/a-flexible-limit-trading-subsidy-for-larger-blocks/\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/validation-cost-metric/\n>\n> Also, here are some talks from the Montreal workshop:\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/alternatives-to-block-size-as-aggregate-resource-limits/\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/bitcoin-block-propagation-iblt-rusty-russell/\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/bitcoin-failure-modes-and-the-role-of-the-lightning-network/\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/bitcoin-load-spike-simulation/\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/block-synchronization-time/\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/coinscope-andrew-miller/\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/competitive-fee-market-urgency/\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/issues-impacting-block-size-proposals/\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/overview-of-security-concerns/\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/relay-network/\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/reworking-bitcoin-core-p2p-code-for-robustness-and-event-driven/\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/sharding-the-blockchain/\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/transaction-fee-estimation/\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/validation-costs/\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/roundgroup-roundup-1/\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/roundgroup-roundup-2/\n>\n> These are not always exact transcripts because I am typing while I am\n> listening, thus there are mistakes including typos and listening errors, so\n> please keep this discrepancy in mind between what's said and what's typed.\n>\n> - Bryan\n> http://heybryan.org/\n> 1 512 203 0507\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151207/a2df4e9f/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Some transcripts from the Scaling Bitcoin workshops",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Bryan Bishop",
                "Johnathan Corgan",
                "Pindar Wong"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 7765
        }
    },
    {
        "title": "[bitcoin-dev] Capacity increases for the Bitcoin system.",
        "thread_messages": [
            {
                "author": "Gregory Maxwell",
                "date": "2015-12-07T22:02:17",
                "message_text_only": "The Scaling Bitcoin Workshop in HK is just wrapping up. Many fascinating\nproposals were presented. I think this would be a good time to share my\nview of the near term arc for capacity increases in the Bitcoin system. I\nbelieve we\u2019re in a fantastic place right now and that the community\nis ready to deliver on a clear forward path with a shared vision that\naddresses the needs of the system while upholding its values.\n\nI think it\u2019s important to first clearly express some of the relevant\nprinciples that I think should guide the ongoing development of the\nBitcoin system.\n\nBitcoin is P2P electronic cash that is valuable over legacy systems\nbecause of the monetary autonomy it brings to its users through\ndecentralization. Bitcoin seeks to address the root problem with\nconventional currency: all the trust that's required to make it work--\n\n-- Not that justified trust is a bad thing, but trust makes systems\nbrittle, opaque, and costly to operate. Trust failures result in systemic\ncollapses, trust curation creates inequality and monopoly lock-in, and\nnaturally arising trust choke-points can be abused to deny access to\ndue process. Through the use of cryptographic proof and decentralized\nnetworks Bitcoin minimizes and replaces these trust costs.\n\nWith the available technology, there are fundamental trade-offs between\nscale and decentralization. If the system is too costly people will be\nforced to trust third parties rather than independently enforcing the\nsystem's rules. If the Bitcoin blockchain\u2019s resource usage, relative\nto the available technology, is too great, Bitcoin loses its competitive\nadvantages compared to legacy systems because validation will be too\ncostly (pricing out many users), forcing trust back into the system.\nIf capacity is too low and our methods of transacting too inefficient,\naccess to the chain for dispute resolution will be too costly, again\npushing trust back into the system.\n\nSince Bitcoin is an electronic cash, it _isn't_ a generic database;\nthe demand for cheap highly-replicated perpetual storage is unbounded,\nand Bitcoin cannot and will not satisfy that demand for non-ecash\n(non-Bitcoin) usage, and there is no shame in that. Fortunately, Bitcoin\ncan interoperate with other systems that address other applications,\nand--with luck and hard work--the Bitcoin system can and will satisfy\nthe world's demand for electronic cash.\n\nFortunately, a lot of great technology is in the works that make\nnavigating the trade-offs easier.\n\nFirst up: after several years in the making Bitcoin Core has recently\nmerged libsecp256k1, which results in a huge increase in signature\nvalidation performance. Combined with other recent work we're now getting\nConnectTip performance 7x higher in 0.12 than in prior versions. This\nhas been a long time coming, and without its anticipation and earlier\nwork such as headers-first I probably would have been arguing for a\nblock size decrease last year.  This improvement in the state of the\nart for widely available production Bitcoin software sets a stage for\nsome capacity increases while still catching up on our decentralization\ndeficit. This shifts the bottlenecks off of CPU and more strongly onto\npropagation latency and bandwidth.\n\nVersionbits (BIP9) is approaching maturity and will allow the Bitcoin\nnetwork to have multiple in-flight soft-forks. Up until now we\u2019ve had to\ncompletely serialize soft-fork work, and also had no real way to handle\na soft-fork that was merged in core but rejected by the network. All\nthat is solved in BIP9, which should allow us to pick up the pace of\nimprovements in the network. It looks like versionbits will be ready\nfor use in the next soft-fork performed on the network.\n\nThe next thing is that, at Scaling Bitcoin Hong Kong, Pieter Wuille\npresented on bringing Segregated Witness to Bitcoin. What is proposed\nis a _soft-fork_ that increases Bitcoin's scalability and capacity by\nreorganizing data in blocks to handle the signatures separately, and in\ndoing so takes them outside the scope of the current blocksize limit.\n\nThe particular proposal amounts to a 4MB blocksize increase at worst. The\nseparation allows new security models, such as skipping downloading data\nyou're not going to check and improved performance for lite clients\n(especially ones with high privacy). The proposal also includes fraud\nproofs which make violations of the Bitcoin system provable with a compact\nproof. This completes the vision of \"alerts\" described in the \"Simplified\nPayment Verification\" section of the Bitcoin whitepaper, and would make it\npossible for lite clients to enforce all the rules of the system (under\na new strong assumption that they're not partitioned from someone who\nwould generate the proofs). The design has numerous other features like\nmaking further enhancements safer and eliminating signature malleability\nproblems. If widely used this proposal gives a 2x capacity increase\n(more if multisig is widely used), but most importantly it makes that\nadditional capacity--and future capacity beyond it--safer by increasing\nefficiency and allowing more trade-offs (in particular, you can use much\nless bandwidth in exchange for a strong non-partitioning assumption).\n\nThere is a working implementation (though it doesn't yet have the fraud\nproofs) at https://github.com/sipa/bitcoin/commits/segwit\n\n(Pieter's talk is at:  transcript:\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/segregated-witness-and-its-impact-on-scalability/\nslides:\nhttps://prezi.com/lyghixkrguao/segregated-witness-and-deploying-it-for-bitcoin/\nVideo: https://www.youtube.com/watch?v=fst1IK_mrng#t=36m )\n\nI had good success deploying an earlier (hard-fork) version of segwit\nin the Elements Alpha sidechain; the soft-fork segwit now proposed\nis a second-generation design. And I think it's quite reasonable to\nget this deployed in a relatively short time frame. The segwit design\ncalls for a future bitcoinj compatible hardfork to further increase its\nefficiency--but it's not necessary to reap most of the benefits,and that\nmeans it can happen on its own schedule and in a non-contentious manner.\n\nGoing beyond segwit, there has been some considerable activity brewing\naround more efficient block relay.  There is a collection of proposals,\nsome stemming from a p2pool-inspired informal sketch of mine and some\nindependently invented, called \"weak blocks\", \"thin blocks\" or \"soft\nblocks\".  These proposals build on top of efficient relay techniques\n(like the relay network protocol or IBLT) and move virtually all the\ntransmission time of a block to before the block is found, eliminating\nsize from the orphan race calculation. We already desperately need this\nat the current block sizes. These have not yet been implemented, but\nfortunately the path appears clear. I've seen at least one more or less\ncomplete specification, and I expect to see things running using this in a\nfew months. This tool will remove propagation latency from being a problem\nin the absence of strategic behavior by miners.  Better understanding\ntheir behavior when miners behave strategically is an open question.\n\nConcurrently, there is a lot of activity ongoing related to\n\u201cnon-bandwidth\u201d scaling mechanisms. Non-bandwidth scaling mechanisms\nare tools like transaction cut-through and bidirectional payment channels\nwhich increase Bitcoin\u2019s capacity and speed using clever smart contracts\nrather than increased bandwidth. Critically, these approaches strike right\nat the heart of the capacity vs autotomy trade-off, and may allow us to\nachieve very high capacity and very high decentralization. CLTV (BIP65),\ndeployed a month ago and now active on the network, is very useful for\nthese techniques (essential for making hold-up refunds work); CSV (BIP68\n/ BIP112) is in the pipeline for merge in core and making good progress\n(and will likely be ready ahead of segwit). Further Bitcoin protocol\nimprovements for non-bandwidth scaling are in the works: Many of these\nproposals really want anti-malleability fixes (which would be provided\nby segwit), and there are checksig flag improvements already tendered and\nmore being worked on, which would be much easier to deploy with segwit. I\nexpect that within six months we could have considerably more features\nready for deployment to enable these techniques. Even without them I\nbelieve we\u2019ll be in an acceptable position with respect to capacity\nin the near term, but it\u2019s important to enable them for the future.\n\n(http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/overview-of-bips-necessary-for-lightning\nis a relevant talk for some of the wanted network features for Lightning,\na bidirectional payment channel proposal which many parties are working\non right now; other non-bandwidth improvements discussed in the past\ninclude transaction cut-through, which I consider a must-read for the\nbasic intuition about how transaction capacity can be greater than\nblockchain capacity: https://bitcointalk.org/index.php?topic=281848.0 ,\nthough there are many others.)\n\nFurther out, there are several proposals related to flex caps or\nincentive-aligned dynamic block size controls based on allowing miners\nto produce larger blocks at some cost. These proposals help preserve\nthe alignment of incentives between miners and general node operators,\nand prevent defection between the miners from undermining the fee\nmarket behavior that will eventually fund security. I think that right\nnow capacity is high enough and the needed capacity is low enough that\nwe don't immediately need these proposals, but they will be critically\nimportant long term. I'm planning to help out and drive towards a more\nconcrete direction out of these proposals in the following months.\n\n(Relevant talks include\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/a-flexible-limit-trading-subsidy-for-larger-blocks/\n)\n\nFinally--at some point the capacity increases from the above may not\nbe enough.  Delivery on relay improvements, segwit fraud proofs, dynamic\nblock size controls, and other advances in technology will reduce the risk\nand therefore controversy around moderate block size increase proposals\n(such as 2/4/8 rescaled to respect segwit's increase). Bitcoin will\nbe able to move forward with these increases when improvements and\nunderstanding render their risks widely acceptable relative to the\nrisks of not deploying them. In Bitcoin Core we should keep patches\nready to implement them as the need and the will arises, to keep the\nbasic software engineering from being the limiting factor.\n\nOur recent and current progress has well positioned the Bitcoin ecosystem\nto handle its current capacity needs. I think the above sets out some\nclear achievable milestones to continue to advance the art in Bitcoin\ncapacity while putting us in a good position for further improvement and\nevolution.\n\nTL;DR:  I propose we work immediately towards the segwit 4MB block\nsoft-fork which increases capacity and scalability, and recent speedups\nand incoming relay improvements make segwit a reasonable risk. BIP9\nand segwit will also make further improvements easier and faster to\ndeploy. We\u2019ll continue to set the stage for non-bandwidth-increase-based\nscaling, while building additional tools that would make bandwidth\nincreases safer long term. Further work will prepare Bitcoin for further\nincreases, which will become possible when justified, while also providing\nthe groundwork to make them justifiable.\n\nThanks for your time,"
            },
            {
                "author": "Bryan Bishop",
                "date": "2015-12-07T22:54:07",
                "message_text_only": "On Mon, Dec 7, 2015 at 4:02 PM, Gregory Maxwell wrote:\n> The Scaling Bitcoin Workshop in HK is just wrapping up. Many fascinating\n> proposals were presented. I think this would be a good time to share my\n> view of the near term arc for capacity increases in the Bitcoin system. I\n> believe we\u2019re in a fantastic place right now and that the community\n> is ready to deliver on a clear forward path with a shared vision that\n> addresses the needs of the system while upholding its values.\n\nACK.\n\nOne of the interesting take-aways from the workshops for me has been\nthat there is a large discrepancy between what developers are doing\nand what's more widely known. When I was doing initial research and\nwork for my keynote at the Montreal conference (\nhttp://diyhpl.us/~bryan/irc/bitcoin/scalingbitcoin-review.pdf -- an\nattempt at being exhaustive, prior to seeing the workshop proposals ),\nwhat I was most surprised by was the discrepancy between what we think\nis being talked about versus what has been emphasized or socially\nprocessed (lots of proposals appear in text, but review efforts are\nsometimes \"hidden\" in corners of github pull request comments, for\nexample). As another example, the libsecp256k1 testing work reached a\nlevel unseen except perhaps in the aerospace industry, but these sorts\nof details are not apparent if you are reading bitcoin-dev archives.\nIt is very hard to listen to all ideas and find great ideas.\nSometimes, our time can be almost completely exhausted by evaluating\ninefficient proposals, so it's not surprising that rough consensus\nbuilding could take time. I suspect we will see consensus moving in\npositive directions around the proposals you have highlighted.\n\nWhen Satoshi originally released the Bitcoin whitepaper, practically\neveryone-- somehow with the exception of Hal Finney-- didn't look,\nbecause the costs of evaluating cryptographic system proposals is so\nhigh and everyone was jaded and burned out for the past umpteen\ndecades. (I have IRC logs from January 10th 2009 where I immediately\ndismissed Bitcoin after I had seen its announcement on the\np2pfoundation mailing list, perhaps in retrospect I should not let\nfamily tragedy so greatly impact my evaluation of proposals...). It's\nhard to evaluate these proposals. Sometimes it may feel like random\nproposals are review-resistant, or designed to burn our time up. But I\nthink this is more reflective of the simple fact that consensus takes\neffort, and it's hard work, and this is to be expected in this sort of\nsystem design.\n\nYour email contains a good summary of recent scaling progress and of\nefforts presented at the Hong Kong workshop. I like summaries. I have\npreviously recommended making more summaries and posting them to the\nmailing list. In general, it would be good if developers were to write\nsummaries of recent work and efforts and post them to the bitcoin-dev\nmailing list. BIP drafts are excellent. Long-term proposals are\nexcellent. Short-term coordination happens over IRC, and that makes\nsense to me. But I would point out that many of the developments even\nfrom, say, the Montreal workshop were notably absent from the mailing\nlist. Unless someone was paying close attention, they wouldn't have\nnoticed some of those efforts which, in some cases, haven't been\nmentioned since. I suspect most of this is a matter of attention,\nreview and keeping track of loose ends, which can be admittedly\ndifficult.\n\nShort (or even long) summaries in emails are helpful because they\nincrease the ability of the community to coordinate and figure out\nwhat's going on. Often I will write an email that summarizes some\ncontent simply because I estimate that I am going to forget the\ndetails in the near future, and if I am going to forget them then it\nseems likely that others might.... This creates a broad base of\nproposals and content to build from when we're doing development work\nin the future, making for a much richer community as a consequence.\nThe contributions from the scalingbitcoin.org workshops are a welcome\naddition, and the proposal outlined in the above email contains a good\nsummary of recent progress. We need more of this sort of synthesis,\nwe're richer for it. I am excitedly looking forward to the impending\nonslaught of Bitcoin progress.\n\n- Bryan\nhttp://heybryan.org/\n1 512 203 0507"
            },
            {
                "author": "Anthony Towns",
                "date": "2015-12-08T02:42:24",
                "message_text_only": "On Mon, Dec 07, 2015 at 10:02:17PM +0000, Gregory Maxwell via bitcoin-dev wrote:\n> ... bringing Segregated Witness to Bitcoin.\n> The particular proposal amounts to a 4MB blocksize increase at worst.\n\nBit ambiguous what \"worst\" means here; lots of people would say the\nsmallest increase is the worst option. :)\n\nBy my count, P2PKH transactions get 2x space saving with segwit [0],\nwhile 2-of-2 multisig P2SH transactions (and hence most of the on-chain\nlightning transactions) get a 3x space saving [1]. An on-chain HTLC (for\na cross-chain atomic swap eg) would also get 3x space saving [2]. The most\nextreme lightning transactions (uncooperative close with bonus anonymity)\ncould get a 6x saving, but would probably run into SIGOP limits [3].\n\n> If widely used this proposal gives a 2x capacity increase\n> (more if multisig is widely used),\n\nSo I think it's fair to say that on its own it gives up to a 2x increase\nfor ordinary pay to public key transactions, and a 3x increase for 2/2\nmultisig and (on-chain) lightning transactions (which would mean lightning\ncould scale to ~20M users with 1MB block sizes based on the estimates\nfrom Tadge Dryja's talk). More complicated smart contracts (even just 3\nof 5 multisig) presumably benefit even more from this, which seems like\nan interesting approach to (part of) jgarzik's \"Fidelity problem\".\n\nAveraging those numbers as a 2.5x improvement, means that combining\nsegwit with other proposals would allow you to derate them by a factor\nof 2.5, giving:\n\n BIP-100: maximum of 12.8MB\n BIP-101: 3.2MB in 2016, 6.4MB in 2018, 12.8MB in 2020, 25.6MB in 2022..\n 2-4-8:   800kB in 2016, 1.6MB in 2018, 3.2MB in 2020\n BIP-103: 400kB in 2016, 470kB in 2018, 650kB in 2020, 1MB in 2023...\n\n(ie, if BIP-103 had been the \"perfect\" approach, then post segwit,\nit would make sense to put non-consensus soft-limits back in place\nfor quite a while)\n\n> TL;DR:  I propose we work immediately towards the segwit 4MB block\n> soft-fork which increases capacity and scalability, and recent speedups\n> and incoming relay improvements make segwit a reasonable risk.\n\nI guess segwit effectively introduces two additional dimensions for\nworking out how to optimally pack transactions into a block -- there's\nthe existing constraints on block bytes (<=1MB) and sigops (<=20k), but\nthere are problably additional constraints on witness bytes (<=3MB) and\nthere *could* be a different constraint for sigops in witnesses (<=3*20k?\n<=4*20k?) compared to sigops in the block while remaining a soft-fork.\n\nIt could also be an opportunity to combine the constraints, ie\n(segwit_bytes + 50*segwit_sigs < 6M) which would make it easier to avoid\nattacks where people try sending transactions with lots of sigops in very\nfew bytes, filling up blocks by sigops, but only paying fees proportional\nto their byte count.\n\nHmm, after a quick look, I'm not sure if the current segwit branch\nactually accounts for sigops in segregated witnesses? If it does, afaics\nit simply applies the existing 20k limit to the total, which seems\ntoo low to me?\n\nHaving segwit with the current 1MB limit on the traditional block\ncontents plus an additional 3MB for witness data seems like it would\nalso give a somewhat gradual increase in transaction volume from the\ncurrent 1x rate to an eventual 2x or 3x rate as wallet software upgrades\nto support segregated witness transactions. So if problems were found\nwhen block+witness data hit 1.5MB, there'd still be time to roll out\nfixes before it got to 1.8MB or 2MB or 3MB. ie this further reduces the\nrisk compared to a single step increase to 2x capacity.\n\nBTW, it's never been quite clear to me what the risks are precisely.\nHere are some:\n\n - sometime soon, blockchain supply can't meet demand\n\n    + I've never worked out how you'd tell if this is the case;\n      there's potentially infinite demand if everything free, so at\n      one level it's trivially true, but that's not helpful.\n\n    + Presumably if this were happening in a way that \"matters\", fees\n      would rise precipitously. Perhaps median fees of $2 USD/kB would\n      indicate this is happening? If so, it's not here yet and seems\n      like it's still a ways off.\n\n    + If it were happening, then, presumably, people become would be\n      less optimistic about bitcoin and the price of BTC would drop/not\n      rise, but that seems pretty hard to interpret.\n\n - it becomes harder to build on blocks found by other miners,\n   encouraging mining centralisation (which then makes censorship easier,\n   and fungibility harder) or forcing trust between miners (eg SPV mining\n   empty blocks)\n\n    + latency/bandwidth limitations means miners can't get block\n      information quickly enough (mitigated by weak blocks and IBLT)\n\n    + blocks can't be verified quickly enough (due to too many crypto\n      ops per block, or because the UTXO set can't be kept in RAM)\n      (mitigated by libsecp256k1 improvements, ..?)\n\n    + constructing a new block to mine takes too long\n\n - it becomes harder to maintain a validating, but non-mining node,\n   which in turn makes non-validating nodes harder to run safely (ie,\n   Sybil attacks become easier)\n\n    + increased CPU to verify bigger/more complicated blocks (can't keep\n      up on a raspberry pi)\n\n    + increased storage (60GB of blockchain might mean it won't fit on\n      your laptop)\n\n    + increased bandwidth\n\n    + increased initial sync time (delayed reward = less likely to\n      bother)\n\nCheers,\naj\n\n[0] AIUI, segwit would make the \"in block\" transactions look like:\n\n     * (4) version\n     * (1) input count\n     * for each input:\n       - (32) tx hash\n       - (4) txout index\n       - (1) script length = 0\n       - (4) sequence number\n     * (1) output count\n     * for each output:\n       - (8) value\n       - (1) script length = 34\n       - (34) <33 byte push>\n     * (4) locktime\n\n    So about 10+41i+43o bytes (with the other information being external to\n    the block and the 1MB limit, but committed to via the coinbase).\n\n    A standard pay to public key hash would have a 25 byte output script\n    instead of 34 bytes, but also a 105 bytes of input script, so about\n    10+146i+34o bytes.\n\n    Over enough transactions inputs and outputs are about equal, so that's\n    10+84o versus 10+180o, so a factor of 2x-2.14x in the usual case.\n\n[1] With a P2SH to a 2-of-2 multisig address, the output script would\n    be 23 bytes, and the input script would be a 71B redeem script, plus\n    two signatures and an OP_0 for about 215B, so totalling 10+256i+32o.\n\n    Again treating i=o over the long term, that's 10+84o version 10+288o,\n    so that's a 3.2x-3.4x improvement. 2-of-2 multisig payment would\n    cover the normal case for on-chain lightning channel transactions,\n    ie where both sides are able to cooperatively close the channel.\n\n[2] A basic HTLC, ie: \"pay to A if they know the preimage for X, or pay\n    to B after a timeout of T\", done by P2SH has about 98B of redeem script\n    and either ~105B of signature or ~72B of signature for a total of 203B\n    or 170B of input script. So that comes to 10+244i+32o or 10+211i+32o.\n    Segwit gives an improvement of 3x-3.3x or 2.7x-2.9x there.\n\n[3] A lightning-style HTLC, which adds a third option of \", or pay to\n    B if A was trying to cheat\" adds an extra 25 bytes or so to the\n    redeem script, changing those numbers to 10+270i+32o and 10+236i+32o,\n    and an improvement of 3.3x-3.6x or 2.9x-3.2x.\n\n    A lightning-style HTLC that also uses ecc private keys as the secret\n    preimages to be revealed [4] might use an additional ~260 bytes of\n    redeem script / script signature, which would make the worst case\n    numbers be 10+530i+32o, so 10+562o versus 10+84o, which would be a\n    6x-6.7x improvement. But those particular scripts would be constrained\n    by consensus sigop limits before the filled up much more than a quarter\n    of a block in a segwit/1MB world anyway.\n\n[4] http://lists.linuxfoundation.org/pipermail/lightning-dev/2015-November/000344.html"
            },
            {
                "author": "Anthony Towns",
                "date": "2015-12-08T04:58:03",
                "message_text_only": "> On Mon, Dec 07, 2015 at 10:02:17PM +0000, Gregory Maxwell wrote:\n> > If widely used this proposal gives a 2x capacity increase\n> > (more if multisig is widely used),\n\nSo from IRC, this doesn't seem quite right -- capacity is constrained as\n\n  base_size + witness_size/4 <= 1MB\n\nrather than\n\n  base_size <= 1MB and base_size + witness_size <= 4MB\n\nor similar. So if you have a 500B transaction and move 250B into the\nwitness, you're still using up 250B+250B/4 of the 1MB limit, rather than\njust 250B of the 1MB limit.\n\nIn particular, if you use as many p2pkh transactions as possible, you'd\nhave 800kB of base data plus 800kB of witness data, and for a block\nfilled with 2-of-2 multisig p2sh transactions, you'd hit the limit at\n670kB of base data and 1.33MB of witness data.\n\nThat would be 1.6MB and 2MB of total actual data if you hit the limits\nwith real transactions, so it's more like a 1.8x increase for real\ntransactions afaics, even with substantial use of multisig addresses.\n\nThe 4MB consensus limit could only be hit by having a single trivial\ntransaction using as little base data as possible, then a single huge\n4MB witness. So people trying to abuse the system have 4x the blocksize\nfor 1 block's worth of fees, while people using it as intended only get\n1.6x or 2x the blocksize... That seems kinda backwards.\n\nHaving a cost function rather than separate limits does make it easier to\nbuild blocks (approximately) optimally, though (ie, just divide the fee by\n(base_bytes+witness_bytes/4) and sort). Are there any other benefits?\n\nBut afaics, you could just have fixed consensus limits and use the cost\nfunction for building blocks, though? ie sort txs by fee divided by [B +\nS*50 + W/3] (where B is base bytes, S is sigops and W is witness bytes)\nthen just fill up the block until one of the three limits (1MB base,\n20k sigops, 3MB witness) is hit?\n\n(Doing a hard fork to make *all* the limits -- base data, witness data,\nand sigop count -- part of a single cost function might be a win; I'm\njust not seeing the gain in forcing witness data to trade off against\nblock data when filling blocks is already a 2D knapsack problem)\n\nCheers,\naj"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-12-08T05:21:18",
                "message_text_only": "On Tue, Dec 8, 2015 at 4:58 AM, Anthony Towns via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Having a cost function rather than separate limits does make it easier to\n> build blocks (approximately) optimally, though (ie, just divide the fee by\n> (base_bytes+witness_bytes/4) and sort). Are there any other benefits?\n\nActually being able to compute fees for your transaction: If there are\nmultiple limits that are \"at play\" then how you need to pay would\ndepend on the entire set of other candidate transactions, which is\nunknown to you. Avoiding the need for a fancy solver in the miner is\nalso virtuous, because requiring software complexity there can make\nfor centralization advantages or divert development/maintenance cycles\nin open source software off to other ends... The multidimensional\noptimization is harder to accommodate for improved relay schemes, this\nis the same as the \"build blocks\" but much more critical both because\nof the need for consistency and the frequency in which you do it.\n\nThese don't, however, apply all that strongly if only one limit is\nlikely to be the limiting limit... though I am unsure about counting\non that; after all if the other limits wouldn't be limiting, why have\nthem?\n\n> That seems kinda backwards.\n\nIt can seem that way, but all limiting schemes have pathological cases\nwhere someone runs up against the limit in the most costly way.  Keep\nin mind that casual pathological behavior can be suppressed via\nIsStandard like rules without baking them into consensus; so long as\nthe candidate attacker isn't miners themselves. Doing so where\npossible can help avoid cases like the current sigops limiting which\nis just ... pretty broken."
            },
            {
                "author": "Anthony Towns",
                "date": "2015-12-08T06:54:48",
                "message_text_only": "On Tue, Dec 08, 2015 at 05:21:18AM +0000, Gregory Maxwell via bitcoin-dev wrote:\n> On Tue, Dec 8, 2015 at 4:58 AM, Anthony Towns via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > Having a cost function rather than separate limits does make it easier to\n> > build blocks (approximately) optimally, though (ie, just divide the fee by\n> > (base_bytes+witness_bytes/4) and sort). Are there any other benefits?\n> Actually being able to compute fees for your transaction: If there are\n> multiple limits that are \"at play\" then how you need to pay would\n> depend on the entire set of other candidate transactions, which is\n> unknown to you.\n\nIsn't that solvable in the short term, if miners just agree to order\ntransactions via a cost function, without enforcing it at consensus\nlevel until a later hard fork that can also change the existing limits\nto enforce that balance?\n\n(1MB base + 3MB witness + 20k sigops) with segwit initially, to something\nlike (B + W + 200*U + 40*S < 5e6) where B is base bytes, W is witness\nbytes, U is number of UTXOs added (or removed) and S is number of sigops,\nor whatever factors actually make sense.\n\nI guess segwit does allow soft-forking more sigops immediately -- segwit\ntransactions only add sigops into the segregated witness, which doesn't\nget counted for existing consensus. So it would be possible to take the\nopposite approach, and make the rule immediately be something like:\n\n  50*S < 1M\n  B + W/4 + 25*S' < 1M\n\n(where S is sigops in base data, and S' is sigops in witness) and\njust rely on S trending to zero (or soft-fork in a requirement that\nnon-segregated witness transactions have fewer than B/50 sigops) so that\nthere's only one (linear) equation to optimise, when deciding fees or\ncreating a block. (I don't see how you could safely set the coefficient\nfor S' too much smaller though)\n\nB+W/4+25*S' for a 2-in/2-out p2pkh would still be 178+206/4+25*2=280\nthough, which would allow 3570 transactions per block, versus 2700 now,\nwhich would only be a 32% increase...\n\n> These don't, however, apply all that strongly if only one limit is\n> likely to be the limiting limit... though I am unsure about counting\n> on that; after all if the other limits wouldn't be limiting, why have\n> them?\n\nSure, but, at least for now, there's already two limits that are being\nhit. Having one is *much* better than two, but I don't think two is a\nlot better than three?\n\n(Also, the ratio between the parameters doesn't necessary seem like a\nconstant; it's not clear to me that hardcoding a formula with a single\nlimit is actually better than hardcoding separate limits, and letting\nminers/the market work out coefficients that match the sort of contracts\nthat are actually being used)\n\n> > That seems kinda backwards.\n> It can seem that way, but all limiting schemes have pathological cases\n> where someone runs up against the limit in the most costly way. Keep\n> in mind that casual pathological behavior can be suppressed via\n> IsStandard like rules without baking them into consensus; so long as\n> the candidate attacker isn't miners themselves. Doing so where\n> possible can help avoid cases like the current sigops limiting which\n> is just ... pretty broken.\n\nSure; it just seems to be halving the increase in block space (60% versus\n100% extra for p2pkh, 100% versus 200% for 2/2 multisig p2sh) for what\ndoesn't actually look like that much of a benefit in fee comparisons?\n\nI mean, as far as I'm concerned, segwit is great even if it doesn't buy\nany improvement in transactions/block, so even a 1% gain is brilliant.\nI'd just rather the 100%-200% gain I was expecting. :)\n\nCheers,\naj"
            },
            {
                "author": "Wladimir J. van der Laan",
                "date": "2015-12-08T11:07:53",
                "message_text_only": "On Mon, Dec 07, 2015 at 10:02:17PM +0000, Gregory Maxwell via bitcoin-dev wrote:\n> The Scaling Bitcoin Workshop in HK is just wrapping up. Many fascinating\n> proposals were presented. I think this would be a good time to share my\n> view of the near term arc for capacity increases in the Bitcoin system. I\n> believe we\u2019re in a fantastic place right now and that the community\n> is ready to deliver on a clear forward path with a shared vision that\n> addresses the needs of the system while upholding its values.\n\nThanks for writing this up. Putting the progress, ongoing work and plans related\nto scaling in context, in one place, was badly needed.\n\n> TL;DR:  I propose we work immediately towards the segwit 4MB block\n> soft-fork which increases capacity and scalability, and recent speedups\n> and incoming relay improvements make segwit a reasonable risk. BIP9\n> and segwit will also make further improvements easier and faster to\n> deploy. We\u2019ll continue to set the stage for non-bandwidth-increase-based\n> scaling, while building additional tools that would make bandwidth\n> increases safer long term. Further work will prepare Bitcoin for further\n> increases, which will become possible when justified, while also providing\n> the groundwork to make them justifiable.\n\nSounds good to me.\n\nThere are multiple ways to get involved in ongoing work, where the community\ncan help to make this happen sooner:\n\n- Review the versionbits BIP https://github.com/bitcoin/bips/blob/master/bip-0009.mediawiki:\n\n  - Compare and test with implementation: https://github.com/bitcoin/bitcoin/pull/6816\n\n- Review CSV BIPs (BIP68 https://github.com/bitcoin/bips/blob/master/bip-0068.mediawiki / \n       BIP112 https://github.com/bitcoin/bips/blob/master/bip-0112.mediawiki),\n\n  - Compare and test implementation: \n\n    https://github.com/bitcoin/bitcoin/pull/6564  BIP-112: Mempool-only CHECKSEQUENCEVERIFY\n    https://github.com/bitcoin/bitcoin/pull/6312  BIP-68: Mempool-only sequence number constraint verification \n    https://github.com/bitcoin/bitcoin/pull/7184  [WIP] Implement SequenceLocks functions for BIP 68\n\n- Segwit BIP is being written, but has not yet been published.\n\n  - Gregory linked to an implementation but as he mentions it is not completely\n    finished yet. ETA for a Segwit testnet is later this month, then you can test as well.\n\nWladimir"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-12-08T11:14:32",
                "message_text_only": "On Dec 8, 2015 7:08 PM, \"Wladimir J. van der Laan via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n>   - Gregory linked to an implementation but as he mentions it is not\ncompletely\n>     finished yet. ETA for a Segwit testnet is later this month, then you\ncan test as well.\n\nTestnet4 ?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/bcac0700/attachment.html>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-12-08T15:12:10",
                "message_text_only": "Thanks for laying out a road-map, Greg.\n\nI'll need to think about it some more, but just a couple of initial\nreactions:\n\nWhy segwitness as a soft fork? Stuffing the segwitness merkle tree in the\ncoinbase is messy and will just complicate consensus-critical code (as\nopposed to making the right side of the merkle tree in block.version=5\nblocks the segwitness data).\n\nIt will also make any segwitness fraud proofs significantly larger (merkle\npath versus  merkle path to coinbase transactions, plus ENTIRE coinbase\ntransaction, which might be quite large, plus merkle path up to root).\n\n\nWe also need to fix the O(n^2) sighash problem as an additional BIP for ANY\nblocksize increase. That also argues for a hard fork-- it is much easier to\nfix it correctly and simplify the consensus code than to continue to apply\nband-aid fixes on top of something fundamentally broken.\n\n\nSegwitness will require a hard or soft-fork rollout, then a significant\nfraction of the transaction-producing wallets to upgrade and start\nsupporting segwitness-style transactions.  I think it will be much quicker\nthan the P2SH rollout, because the biggest transaction producers have a\nstrong motivation to lower their fees, and it won't require a new type of\nbitcoin address to fund wallets.  But it still feels like it'll be six\nmonths to a year at the earliest before any relief from the current\nproblems we're seeing from blocks filling up.\n\nSegwitness will make the current bottleneck (block propagation) a little\nworse in the short term, because of the extra fraud-proof data.  Benefits\nwell worth the costs.\n\n------------------\n\nI think a barrier to quickly getting consensus might be a fundamental\ndifference of opinion on this:\n   \"Even without them I believe we\u2019ll be in an acceptable position with\nrespect to capacity in the near term\"\n\nThe heaviest users of the Bitcoin network (businesses who generate tens of\nthousands of transactions per day on behalf of their customers) would\nstrongly disgree; the current state of affairs is NOT acceptable to them.\n\n\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/386ffdbc/attachment.html>"
            },
            {
                "author": "Justus Ranvier",
                "date": "2015-12-08T15:55:57",
                "message_text_only": "On 12/08/2015 09:12 AM, Gavin Andresen via bitcoin-dev wrote:\n> Stuffing the segwitness merkle tree in the coinbase\n\nIf such a change is going to be deployed via a soft fork instead of a\nhard fork, then the coinbase is the worst place to put the segwitness\nmerkle root.\n\nInstead, put it in the first output of the generation transaction as an\nOP_RETURN script.\n\nThis is a better pattern because coinbase space is limited while output\nspace is not. The next time there's a good reason to tie another merkle\ntree to a block, that proposal can be designated for the second output\nof the generation transaction.\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: 0xEAD9E623.asc\nType: application/pgp-keys\nSize: 23337 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/a7f58417/attachment-0001.bin>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 801 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/a7f58417/attachment-0001.sig>"
            },
            {
                "author": "Mark Friedenbach",
                "date": "2015-12-08T17:41:23",
                "message_text_only": "A far better place than the generation transaction (which I assume means\ncoinbase transaction?) is the last transaction in the block. That allows\nyou to save, on average, half of the hashes in the Merkle tree.\n\nOn Tue, Dec 8, 2015 at 11:55 PM, Justus Ranvier via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On 12/08/2015 09:12 AM, Gavin Andresen via bitcoin-dev wrote:\n> > Stuffing the segwitness merkle tree in the coinbase\n>\n> If such a change is going to be deployed via a soft fork instead of a\n> hard fork, then the coinbase is the worst place to put the segwitness\n> merkle root.\n>\n> Instead, put it in the first output of the generation transaction as an\n> OP_RETURN script.\n>\n> This is a better pattern because coinbase space is limited while output\n> space is not. The next time there's a good reason to tie another merkle\n> tree to a block, that proposal can be designated for the second output\n> of the generation transaction.\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/040dbf84/attachment.html>"
            },
            {
                "author": "Justus Ranvier",
                "date": "2015-12-08T18:43:40",
                "message_text_only": "On 12/08/2015 11:41 AM, Mark Friedenbach wrote:\n> A far better place than the generation transaction (which I assume means\n> coinbase transaction?) is the last transaction in the block. That allows\n> you to save, on average, half of the hashes in the Merkle tree.\n\nI don't care what color that bikeshed is painted.\n\nIn whatever transaction it is placed, the hash should be on the output\nside, That way is more future-proof since it does not crowd out other\nhashes which might be equally valuable to commit someday.\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: 0xEAD9E623.asc\nType: application/pgp-keys\nSize: 23337 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/fc970ace/attachment-0001.bin>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 801 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/fc970ace/attachment-0001.sig>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-12-08T19:08:57",
                "message_text_only": "On Tue, Dec 8, 2015 at 5:41 PM, Mark Friedenbach via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> A far better place than the generation transaction (which I assume means\n> coinbase transaction?) is the last transaction in the block. That allows\n> you to save, on average, half of the hashes in the Merkle tree.\n>\n\nThis trick can be improved by only using certain tx counts.  If the number\nof transactions is limited to a power of 2 (other than the extra\ntransactions), then you get a path of length zero.\n\nThe number of non-zero bits in the tx count determings how many digests are\nrequired.\n\nhttps://github.com/TierNolan/bips/blob/aux_header/bip-aux-header.mediawiki\n\nThis gets the benefit of a soft-fork, while also keeping the proof lengths\nsmall.  The linked bip has a 105 byte overhead for the path.\n\nThe cost is that only certain transaction counts are allowed.  In the worst\ncase, 12.5% of transactions would have to be left in the memory pool.  This\nmeans around 7% of transactions would be delayed until the next block.\n\nBlank transactions (or just transactions with low latency requirements)\ncould be used to increase the count so that it is raised to one of the\nvalid numbers.\n\nManaging the UTXO set to ensure that there is at least one output that pays\nto OP_TRUE is also a hassle.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/99821607/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-12-08T19:31:27",
                "message_text_only": "On Tue, Dec 8, 2015 at 3:55 PM, Justus Ranvier via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Instead, put it in the first output of the generation transaction as an\n> OP_RETURN script.\n\nPieter was originally putting it in a different location; so it's no\nbig deal to do so.\n\nBut there exists deployed mining hardware that imposes constraints on\nthe coinbase outputs, unfortunately."
            },
            {
                "author": "Jonathan Toomim",
                "date": "2015-12-08T23:40:42",
                "message_text_only": "Agree. This data does not belong in the coinbase. That space is for miners to use, not devs.\n\nI also think that a hard fork is better for SegWit, as it reduces the size of fraud proofs considerably, makes the whole design more elegant and less kludgey, and is safer for clients who do not upgrade in a timely fashion. I don't like the idea that SegWit would invalidate the security assumptions of non-upgraded clients (including SPV wallets). I think that for these clients, no data is better than invalid data. Better to force them to upgrade by cutting them off the network than to let them think they're validating transactions when they're not.\n\n\nOn Dec 8, 2015, at 11:55 PM, Justus Ranvier via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> If such a change is going to be deployed via a soft fork instead of a\n> hard fork, then the coinbase is the worst place to put the segwitness\n> merkle root.\n> \n> Instead, put it in the first output of the generation transaction as an\n> OP_RETURN script.\n> \n> This is a better pattern because coinbase space is limited while output\n> space is not. The next time there's a good reason to tie another merkle\n> tree to a block, that proposal can be designated for the second output\n> of the generation transaction.\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/a4037777/attachment.sig>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2015-12-08T23:48:53",
                "message_text_only": "On Tuesday, December 08, 2015 11:40:42 PM Jonathan Toomim via bitcoin-dev \nwrote:\n> Agree. This data does not belong in the coinbase. That space is for miners\n> to use, not devs.\n\nThis has never been guaranteed, nor are softforks a \"dev action\" in the first \nplace.\n\n> I also think that a hard fork is better for SegWit, as it reduces the size\n> of fraud proofs considerably, makes the whole design more elegant and less\n> kludgey, and is safer for clients who do not upgrade in a timely fashion.\n\nHow about we pursue the SegWit softfork, and at the same time* work on a \nhardfork which will simplify the proofs and reduce the kludgeyness of merge-\nmining in general? Then, if the hardfork is ready before the softfork, they \ncan both go together, but if not, we aren't stuck delaying the improvements of \nSegWit until the hardfork is completed.\n\n* I have been in fact working on such a proposal for a while now, since before \nSegWit.\n\n> I don't like the idea that SegWit would invalidate the security\n> assumptions of non-upgraded clients (including SPV wallets). I think that\n> for these clients, no data is better than invalid data. Better to force\n> them to upgrade by cutting them off the network than to let them think\n> they're validating transactions when they're not.\n\nThere isn't an option for \"no data\", as non-upgraded nodes in a hardfork are \nleft completely vulnerable to attacking miners, even much lower hashrate than \nthe 51% attack risk. So the alternatives are:\n- hardfork: complete loss of all security for the old nodes\n- softfork: degraded security for old nodes\n\nLuke"
            },
            {
                "author": "Jonathan Toomim",
                "date": "2015-12-09T00:54:38",
                "message_text_only": "On Dec 9, 2015, at 7:48 AM, Luke Dashjr <luke at dashjr.org> wrote:\n\n> How about we pursue the SegWit softfork, and at the same time* work on a\n> hardfork which will simplify the proofs and reduce the kludgeyness of merge-\n> mining in general? Then, if the hardfork is ready before the softfork, they\n> can both go together, but if not, we aren't stuck delaying the improvements of\n> SegWit until the hardfork is completed.\n\nSo that all our code that parses the blockchain needs to be able to find the sigwit data in both places? That doesn't really sound like an improvement to me. Why not just do it as a hard fork? They're really not that hard to do.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/96a8d6a7/attachment.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/96a8d6a7/attachment.sig>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-12-08T23:50:35",
                "message_text_only": "On Dec 9, 2015 7:41 AM, \"Jonathan Toomim via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> I also think that a hard fork is better for SegWit, as it reduces the\nsize of fraud proofs considerably, makes the whole design more elegant and\nless kludgey, and is safer for clients who do not upgrade in a timely\nfashion.\n\nI agree, although I disagree with the last reason.\n\n> I don't like the idea that SegWit would invalidate the security\nassumptions of non-upgraded clients (including SPV wallets). I think that\nfor these clients, no data is better than invalid data. Better to force\nthem to upgrade by cutting them off the network than to let them think\nthey're validating transactions when they're not.\n\nI don't undesrtand. SPV nodes won't think they are validating transactions\nwith the new version unless they adapt to the new format. They will be\nsimply unable to receive payments using the new format if it is a softfork\n(although as said I agree with making it a hardfork on the simpler design\nand smaller fraud proofs grounds alone).\n\n>\n> On Dec 8, 2015, at 11:55 PM, Justus Ranvier via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> > If such a change is going to be deployed via a soft fork instead of a\n> > hard fork, then the coinbase is the worst place to put the segwitness\n> > merkle root.\n> >\n> > Instead, put it in the first output of the generation transaction as an\n> > OP_RETURN script.\n> >\n> > This is a better pattern because coinbase space is limited while output\n> > space is not. The next time there's a good reason to tie another merkle\n> > tree to a block, that proposal can be designated for the second output\n> > of the generation transaction.\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/2af9dc6d/attachment.html>"
            },
            {
                "author": "Jonathan Toomim",
                "date": "2015-12-09T00:56:25",
                "message_text_only": "On Dec 9, 2015, at 7:50 AM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n\n> I don't undesrtand. SPV nodes won't think they are validating transactions with the new version unless they adapt to the new format. They will be simply unable to receive payments using the new format if it is a softfork (although as said I agree with making it a hardfork on the simpler design and smaller fraud proofs grounds alone).\n> \nOkay, I might just not understand how a sigwit payment would look to current software yet. I'll add learning about that to my to-do list...\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/b3c8f971/attachment.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/b3c8f971/attachment.sig>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-12-08T23:59:33",
                "message_text_only": "On Tue, Dec 8, 2015 at 3:12 PM, Gavin Andresen via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Why segwitness as a soft fork? Stuffing the segwitness merkle tree in the\n> coinbase is messy and will just complicate consensus-critical code (as\n> opposed to making the right side of the merkle tree in block.version=5\n> blocks the segwitness data).\n\nIt's nearly complexity-costless to put it in the coinbase transaction.\nExploring the costs is one of the reasons why this was implemented\nfirst.\n\nWe already have consensus critical enforcement there, the height,\nwhich has almost never been problematic. (A popular block explorer\nrecently misimplemented the var-int decode and suffered an outage).\n\nAnd most but not all prior commitment proposals have suggested the\nsame or similar.  The exact location is not that critical, however,\nand we do have several soft-fork compatible options.\n\n> It will also make any segwitness fraud proofs significantly larger (merkle\n> path versus  merkle path to coinbase transactions, plus ENTIRE coinbase\n> transaction, which might be quite large, plus merkle path up to root).\n\nYes, it will make them larger by log2() the number of transaction in a\nblock which is-- say-- 448 bytes.\n\nWith the coinbase transaction thats another couple kilobytes, I think\nthis is negligible.\n\n>From a risk reduction perspective, I think it is much preferable to\nperform the primary change in a backwards compatible manner, and pick\nup the data reorganization in a hardfork if anyone even cares.\n\nI think thats generally a nice cadence to split up risks that way; and\navoid controversy.\n\n> We also need to fix the O(n^2) sighash problem as an additional BIP for ANY\n> blocksize increase.\n\nThe witness data is never an input to sighash, so no, I don't agree\nthat this holds for \"any\" increase.\n\n> Segwitness will make the current bottleneck (block propagation) a little\n> worse in the short term, because of the extra fraud-proof data.  Benefits\n> well worth the costs.\n\nThe fraud proof data is deterministic, full nodes could skip sending\nit between each other, if anyone cared; but the overhead is pretty\ntiny in any case.\n\n> I think a barrier to quickly getting consensus might be a fundamental\n> difference of opinion on this:\n>    \"Even without them I believe we\u2019ll be in an acceptable position with\n> respect to capacity in the near term\"\n>\n> The heaviest users of the Bitcoin network (businesses who generate tens of\n> thousands of transactions per day on behalf of their customers) would\n> strongly disgree; the current state of affairs is NOT acceptable to them.\n\nMy message lays out a plan for several different complementary\ncapacity advances; it's not referring to the current situation--\nthough the current capacity situation is no emergency.\n\nI believe it already reflects the emerging consensus in the Bitcoin\nCore project; in terms of the overall approach and philosophy, if not\nevery specific technical detail. It's not a forever plan, but a\npragmatic one that understand that the future is uncertain no matter\nwhat we do; one that trusts that we'll respond to whatever\ncontingencies surprise us on the road to success."
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-12-09T00:58:06",
                "message_text_only": "On Wed, Dec 9, 2015 at 12:59 AM, Gregory Maxwell via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> On Tue, Dec 8, 2015 at 3:12 PM, Gavin Andresen via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> We already have consensus critical enforcement there, the height,\n> which has almost never been problematic. (A popular block explorer\n> recently misimplemented the var-int decode and suffered an outage).\n\nIt would be also a nice opportunity to move the height to a more\naccessible place.\nFor example CBlockHeader::hashMerkleRoot (and CBlockIndex's) could be\nreplaced with a hash of the following struct:\n\nstruct hashRootStruct\n{\nuint256 hashMerkleRoot;\nuint256 hashWitnessesRoot;\nint32_t nHeight;\n}\n\n> From a risk reduction perspective, I think it is much preferable to\n> perform the primary change in a backwards compatible manner, and pick\n> up the data reorganization in a hardfork if anyone even cares.\n\n\nBut then all wallet software will need to adapt their software twice.\nWhy introduce technical debt for no good reason?\n\n> I think thats generally a nice cadence to split up risks that way; and\n> avoid controversy.\n\nUncontroversial hardforks can also be deployed with small risks as\ndescribed in BIP99."
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-12-09T01:02:58",
                "message_text_only": "On Wed, Dec 9, 2015 at 1:58 AM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n> struct hashRootStruct\n> {\n> uint256 hashMerkleRoot;\n> uint256 hashWitnessesRoot;\n> int32_t nHeight;\n> }\n\nOr better, for forward compatibility (we may want to include more\nthings apart from nHeight and hashWitnessesRoot in the future):\n\nstruct hashRootStruct\n{\n uint256 hashMerkleRoot;\n uint256 hashWitnessesRoot;\n uint256 hashextendedHeader;\n}\n\nFor example, we may want to chose to add an extra nonce there."
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-12-09T01:09:16",
                "message_text_only": "On Tue, Dec 8, 2015 at 6:59 PM, Gregory Maxwell <greg at xiph.org> wrote:\n\n> > We also need to fix the O(n^2) sighash problem as an additional BIP for\n> ANY\n> > blocksize increase.\n>\n> The witness data is never an input to sighash, so no, I don't agree\n> that this holds for \"any\" increase.\n>\n\nHere's the attack:\n\nCreate a 1-megabyte transaction, with all of it's inputs spending\nsegwitness-spending SIGHASH_ALL inputs.\n\nBecause the segwitness inputs are smaller in the block, you can fit more of\nthem into 1 megabyte. Each will hash very close to one megabyte of data.\n\nThat will be O(n^2) worse than the worst case of a 1-megabyte transaction\nwith signatures in the scriptSigs.\n\nDid I misunderstand something or miss something about the 1-mb transaction\ndata and 3-mb segwitness data proposal that would make this attack not\npossible?\n\nRE: fraud proof data being deterministic:  yes, I see, the data can be\ncomputed instead of broadcast with the block.\n\nRE: emerging consensus of Core:\n\nI think it is a huge mistake not to \"design for success\" (see\nhttp://gavinandresen.ninja/designing-for-success ).\n\nI think it is a huge mistake to pile on technical debt in\nconsensus-critical code. I think we should be working harder to make things\nsimpler, not more complex, whenever possible.\n\nAnd I think there are pretty big self-inflicted current problems because\nworries about theoretical future problems have prevented us from coming to\nconsensus on simple solutions.\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/58a8269d/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-12-09T01:31:51",
                "message_text_only": "On Wed, Dec 9, 2015 at 1:09 AM, Gavin Andresen <gavinandresen at gmail.com> wrote:\n> Create a 1-megabyte transaction, with all of it's inputs spending\n> segwitness-spending SIGHASH_ALL inputs.\n>\n> Because the segwitness inputs are smaller in the block, you can fit more of\n> them into 1 megabyte. Each will hash very close to one megabyte of data.\n\nWitness size comes out of the 1MB at a factor of 0.25. It is not\npossible to make a block which has signatures with the full 1MB of\ndata under the sighash while also having signatures externally.  So\nevery byte moved into the witness and thus only counted as 25% comes\nout of the data being hashed and is hashed nInputs (*checksigs) less\ntimes.\n\n> I think it is a huge mistake not to \"design for success\" (see\n> http://gavinandresen.ninja/designing-for-success ).\n\nWe are designing for success; including the success of being able to\nadapt and cope with uncertainty-- which is the most critical kind of\nsuccess we can have in a world where nothing is and can be\npredictable.\n\n> I think it is a huge mistake to pile on technical debt in consensus-critical\n> code. I think we should be working harder to make things simpler, not more\n> complex, whenever possible.\n\nI agree, but nothing I have advocated creates significant technical\ndebt. It is also a bad engineering practice to combine functional\nchanges (especially ones with poorly understood system wide\nconsequences and low user autonomy) with structural tidying.\n\n> And I think there are pretty big self-inflicted current problems because\n> worries about theoretical future problems have prevented us from coming to\n> consensus on simple solutions.\n\nThat isn't my perspective. I believe we've suffered delays because of\na strong desire to be inclusive and hear out all ideas, and not\nforestall market adoption, even for ideas that eschewed pragmatism and\ntried to build for forever in a single step and which in our hear of\nhearts we knew were not the right path today. It's time to move past\nthat and get back on track with the progress can make and have been\nmaking, in terms of capacity as well as many other areas. I think that\nis designing for success."
            },
            {
                "author": "Ryan Butler",
                "date": "2015-12-09T04:44:09",
                "message_text_only": ">I agree, but nothing I have advocated creates significant technical\n>debt. It is also a bad engineering practice to combine functional\n>changes (especially ones with poorly understood system wide\n>consequences and low user autonomy) with structural tidying.\n\nI don't think I would classify placing things in consensus critical code\nwhen it doesn't need to be as \"structural tidying\".  Gavin said \"pile on\"\nwhich you took as implying \"a lot\", he can correct me, but I believe he\nmeant \"add to\".\n\n> (especially ones with poorly understood system wide consequences and low\nuser autonomy)\n\nThis implies there you have no confidence in the unit tests and functional\ntesting around Bitcoin and should not be a reason to avoid refactoring.\nIt's more a reason to increase testing so that you will have confidence\nwhen you refactor.\n\nAlso I don't think Martin Fowler would agree with you...\n\n\"Refactoring should be done in conjunction with adding new features.\"\n\n\"Always leave the code better than when you found it.\"\n\n\"Often you start working on adding new functionality and you realize the\nexisting structures don't play well with what you're about to do.\n\nIn this situation it usually pays to begin by refactoring the existing code\ninto the shape you now know is the right shape for what you're about to do.\"\n\n-Martin Fowler\n\n\n\n\n\n\n\n\nOn Tue, Dec 8, 2015 at 7:31 PM, Gregory Maxwell via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Wed, Dec 9, 2015 at 1:09 AM, Gavin Andresen <gavinandresen at gmail.com>\n> wrote:\n> > Create a 1-megabyte transaction, with all of it's inputs spending\n> > segwitness-spending SIGHASH_ALL inputs.\n> >\n> > Because the segwitness inputs are smaller in the block, you can fit more\n> of\n> > them into 1 megabyte. Each will hash very close to one megabyte of data.\n>\n> Witness size comes out of the 1MB at a factor of 0.25. It is not\n> possible to make a block which has signatures with the full 1MB of\n> data under the sighash while also having signatures externally.  So\n> every byte moved into the witness and thus only counted as 25% comes\n> out of the data being hashed and is hashed nInputs (*checksigs) less\n> times.\n>\n> > I think it is a huge mistake not to \"design for success\" (see\n> > http://gavinandresen.ninja/designing-for-success ).\n>\n> We are designing for success; including the success of being able to\n> adapt and cope with uncertainty-- which is the most critical kind of\n> success we can have in a world where nothing is and can be\n> predictable.\n>\n> > I think it is a huge mistake to pile on technical debt in\n> consensus-critical\n> > code. I think we should be working harder to make things simpler, not\n> more\n> > complex, whenever possible.\n>\n> I agree, but nothing I have advocated creates significant technical\n> debt. It is also a bad engineering practice to combine functional\n> changes (especially ones with poorly understood system wide\n> consequences and low user autonomy) with structural tidying.\n>\n> > And I think there are pretty big self-inflicted current problems because\n> > worries about theoretical future problems have prevented us from coming\n> to\n> > consensus on simple solutions.\n>\n> That isn't my perspective. I believe we've suffered delays because of\n> a strong desire to be inclusive and hear out all ideas, and not\n> forestall market adoption, even for ideas that eschewed pragmatism and\n> tried to build for forever in a single step and which in our hear of\n> hearts we knew were not the right path today. It's time to move past\n> that and get back on track with the progress can make and have been\n> making, in terms of capacity as well as many other areas. I think that\n> is designing for success.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/ac78b75c/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-12-09T06:29:53",
                "message_text_only": "On Wed, Dec 9, 2015 at 4:44 AM, Ryan Butler <rryananizer at gmail.com> wrote:\n>>I agree, but nothing I have advocated creates significant technical\n>>debt. It is also a bad engineering practice to combine functional\n>>changes (especially ones with poorly understood system wide\n>>consequences and low user autonomy) with structural tidying.\n>\n> I don't think I would classify placing things in consensus critical code\n> when it doesn't need to be as \"structural tidying\".  Gavin said \"pile on\"\n> which you took as implying \"a lot\", he can correct me, but I believe he\n> meant \"add to\".\n\nNothing being discussed would move something from consensus critical\ncode to not consensus critical.\n\nWhat was being discussed was the location of the witness commitment;\nwhich is consensus critical regardless of where it is placed. Should\nit be placed in an available location which is compatible with the\nexisting network, or should the block hashing data structure\nimmediately be changed in an incompatible way to accommodate it in\norder to satisfy an ascetic sense of purity and to make fraud proofs\nsomewhat smaller?\n\nI argue that the size difference in the fraud proofs is not\ninteresting, the disruption to the network in an incompatible upgrade\nis interesting; and that if it really were desirable reorganization to\nmove the commitment point could be done as part of a separate change\nthat changes only the location of things (and/or other trivial\nadjustments); and that proceeding int this fashion would minimize\ndisruption and risk... by making the incompatible changes that will\nforce network wide software updates be as small and as simple as\npossible.\n\n>> (especially ones with poorly understood system wide consequences and low\n>> user autonomy)\n>\n> This implies there you have no confidence in the unit tests and functional\n> testing around Bitcoin and should not be a reason to avoid refactoring.\n> It's more a reason to increase testing so that you will have confidence when\n> you refactor.\n\nI am speaking from our engineering experience in a  public,\nworld-wide, multi-vendor, multi-version, inter-operable, distributed\nsystem which is constantly changing and in production contains private\ncode, unknown and assorted hardware, mixtures of versions, unreliable\nnetworks, undisclosed usage patterns, and more sources of complex\nbehavior than can be counted-- including complex economic incentives\nand malicious participants.\n\nEven if we knew the complete spectrum of possible states for the\nsystem the combinatioric explosion makes complete testing infeasible.\n\nThough testing is essential one cannot \"unit test\" away all the risks\nrelated to deploying a new behavior in the network."
            },
            {
                "author": "Ryan Butler",
                "date": "2015-12-09T06:36:22",
                "message_text_only": "I see, thanks for clearing that up, I misread what Gavin stated.\n\nOn Wed, Dec 9, 2015 at 12:29 AM, Gregory Maxwell <greg at xiph.org> wrote:\n\n> On Wed, Dec 9, 2015 at 4:44 AM, Ryan Butler <rryananizer at gmail.com> wrote:\n> >>I agree, but nothing I have advocated creates significant technical\n> >>debt. It is also a bad engineering practice to combine functional\n> >>changes (especially ones with poorly understood system wide\n> >>consequences and low user autonomy) with structural tidying.\n> >\n> > I don't think I would classify placing things in consensus critical code\n> > when it doesn't need to be as \"structural tidying\".  Gavin said \"pile on\"\n> > which you took as implying \"a lot\", he can correct me, but I believe he\n> > meant \"add to\".\n>\n> Nothing being discussed would move something from consensus critical\n> code to not consensus critical.\n>\n> What was being discussed was the location of the witness commitment;\n> which is consensus critical regardless of where it is placed. Should\n> it be placed in an available location which is compatible with the\n> existing network, or should the block hashing data structure\n> immediately be changed in an incompatible way to accommodate it in\n> order to satisfy an ascetic sense of purity and to make fraud proofs\n> somewhat smaller?\n>\n> I argue that the size difference in the fraud proofs is not\n> interesting, the disruption to the network in an incompatible upgrade\n> is interesting; and that if it really were desirable reorganization to\n> move the commitment point could be done as part of a separate change\n> that changes only the location of things (and/or other trivial\n> adjustments); and that proceeding int this fashion would minimize\n> disruption and risk... by making the incompatible changes that will\n> force network wide software updates be as small and as simple as\n> possible.\n>\n> >> (especially ones with poorly understood system wide consequences and low\n> >> user autonomy)\n> >\n> > This implies there you have no confidence in the unit tests and\n> functional\n> > testing around Bitcoin and should not be a reason to avoid refactoring.\n> > It's more a reason to increase testing so that you will have confidence\n> when\n> > you refactor.\n>\n> I am speaking from our engineering experience in a  public,\n> world-wide, multi-vendor, multi-version, inter-operable, distributed\n> system which is constantly changing and in production contains private\n> code, unknown and assorted hardware, mixtures of versions, unreliable\n> networks, undisclosed usage patterns, and more sources of complex\n> behavior than can be counted-- including complex economic incentives\n> and malicious participants.\n>\n> Even if we knew the complete spectrum of possible states for the\n> system the combinatioric explosion makes complete testing infeasible.\n>\n> Though testing is essential one cannot \"unit test\" away all the risks\n> related to deploying a new behavior in the network.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/21a08ea0/attachment.html>"
            },
            {
                "author": "Mark Friedenbach",
                "date": "2015-12-09T06:59:43",
                "message_text_only": "Greg, if you have actual data showing that putting the commitment in the\nlast transaction would be disruptive, and how disruptive, that would be\nappreciated. Of the mining hardware I have looked at, none of it cared at\nall what transactions other than the coinbase are. You need to provide a\npath to the coinbase for extranonce rolling, but the witness commitment\nwouldn't need to be updated.\n\nI'm sorry but it's not clear how this would be an incompatible upgrade,\ndisruptive to anything other than the transaction selection code. Maybe I'm\nmissing something? I'm not familiar with all the hardware or pooling setups\nout there.\n\nOn Wed, Dec 9, 2015 at 2:29 PM, Gregory Maxwell via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Wed, Dec 9, 2015 at 4:44 AM, Ryan Butler <rryananizer at gmail.com> wrote:\n> >>I agree, but nothing I have advocated creates significant technical\n> >>debt. It is also a bad engineering practice to combine functional\n> >>changes (especially ones with poorly understood system wide\n> >>consequences and low user autonomy) with structural tidying.\n> >\n> > I don't think I would classify placing things in consensus critical code\n> > when it doesn't need to be as \"structural tidying\".  Gavin said \"pile on\"\n> > which you took as implying \"a lot\", he can correct me, but I believe he\n> > meant \"add to\".\n>\n> Nothing being discussed would move something from consensus critical\n> code to not consensus critical.\n>\n> What was being discussed was the location of the witness commitment;\n> which is consensus critical regardless of where it is placed. Should\n> it be placed in an available location which is compatible with the\n> existing network, or should the block hashing data structure\n> immediately be changed in an incompatible way to accommodate it in\n> order to satisfy an ascetic sense of purity and to make fraud proofs\n> somewhat smaller?\n>\n> I argue that the size difference in the fraud proofs is not\n> interesting, the disruption to the network in an incompatible upgrade\n> is interesting; and that if it really were desirable reorganization to\n> move the commitment point could be done as part of a separate change\n> that changes only the location of things (and/or other trivial\n> adjustments); and that proceeding int this fashion would minimize\n> disruption and risk... by making the incompatible changes that will\n> force network wide software updates be as small and as simple as\n> possible.\n>\n> >> (especially ones with poorly understood system wide consequences and low\n> >> user autonomy)\n> >\n> > This implies there you have no confidence in the unit tests and\n> functional\n> > testing around Bitcoin and should not be a reason to avoid refactoring.\n> > It's more a reason to increase testing so that you will have confidence\n> when\n> > you refactor.\n>\n> I am speaking from our engineering experience in a  public,\n> world-wide, multi-vendor, multi-version, inter-operable, distributed\n> system which is constantly changing and in production contains private\n> code, unknown and assorted hardware, mixtures of versions, unreliable\n> networks, undisclosed usage patterns, and more sources of complex\n> behavior than can be counted-- including complex economic incentives\n> and malicious participants.\n>\n> Even if we knew the complete spectrum of possible states for the\n> system the combinatioric explosion makes complete testing infeasible.\n>\n> Though testing is essential one cannot \"unit test\" away all the risks\n> related to deploying a new behavior in the network.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/5924c67f/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-12-09T07:17:08",
                "message_text_only": "On Wed, Dec 9, 2015 at 6:59 AM, Mark Friedenbach <mark at friedenbach.org> wrote:\n> Greg, if you have actual data showing that putting the commitment in the\n> last transaction would be disruptive, and how disruptive, that would be\n> appreciated. Of the mining hardware I have looked at, none of it cared at\n> all what transactions other than the coinbase are. You need to provide a\n> path to the coinbase for extranonce rolling, but the witness commitment\n> wouldn't need to be updated.\n>\n> I'm sorry but it's not clear how this would be an incompatible upgrade,\n> disruptive to anything other than the transaction selection code. Maybe I'm\n> missing something? I'm not familiar with all the hardware or pooling setups\n> out there.\n\nI didn't comment on the transaction output. I have commented on\ncoinbase outputs and on a hard-fork.\n\nUsing an output in the last transaction would break the assumption\nthat you can truncate a block and still have a valid block. This is\nused by some mining setups currently, because GBT does not generate\nthe coinbase transaction and so cannot know its size; and you may have\nto drop the last transaction(s) to make room for it.\n\nThat a block can be truncated and still result in a valid block also\nseems like a useful property to me.\n\nIf the input for that transaction is supposed to be generated from a\ncoinbase output some blocks earlier, then this may again run into\nhardware output constraints in coinbase transactions. (But it may be\nbetter since it wouldn't matter which output created it.). This could\nlikely be escaped by creating a zero value output only once and just\nrolling it forward."
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-12-09T07:54:49",
                "message_text_only": "On Wed, Dec 9, 2015 at 7:29 AM, Gregory Maxwell via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> What was being discussed was the location of the witness commitment;\n> which is consensus critical regardless of where it is placed. Should\n> it be placed in an available location which is compatible with the\n> existing network, or should the block hashing data structure\n> immediately be changed in an incompatible way to accommodate it in\n> order to satisfy an ascetic sense of purity and to make fraud proofs\n> somewhat smaller?\n\n>From this question one could think that when you said \"we can do the\ncleanup hardfork later\" earlier you didn't really meant it. And that\nyou will oppose to that hardfork later just like you are opposing to\nit now.\nAs said I disagree that making a softfork first and then move the\ncommitment is less disruptive (because people will need to adapt their\nsoftware twice), but if the intention is to never do the second part\nthen of course I agree it would be less disruptive.\nHow long after the softfork would you like to do the hardfork?\n1 year after the softfork? 2 years? never?"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-12-09T08:03:45",
                "message_text_only": "On Wed, Dec 9, 2015 at 7:54 AM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n> From this question one could think that when you said \"we can do the\n> cleanup hardfork later\" earlier you didn't really meant it. And that\n> you will oppose to that hardfork later just like you are opposing to\n> it now.\n> As said I disagree that making a softfork first and then move the\n> commitment is less disruptive (because people will need to adapt their\n> software twice), but if the intention is to never do the second part\n> then of course I agree it would be less disruptive.\n> How long after the softfork would you like to do the hardfork?\n> 1 year after the softfork? 2 years? never?\n\nI think it would be logical to do as part of a hardfork that moved\ncommitments generally; e.g. a better position for merged mining (such\na hardfork was suggested in 2010 as something that could be done if\nmerged mining was used), room for commitments to additional block\nback-references for compact SPV proofs, and/or UTXO set commitments.\nPart of the reason to not do it now is that the requirements for the\nother things that would be there are not yet well defined. For these\nother applications, the additional overhead is actually fairly\nmeaningful; unlike the fraud proofs."
            },
            {
                "author": "Mark Friedenbach",
                "date": "2015-12-09T08:46:31",
                "message_text_only": "My apologies for the apparent miscommunication earlier. It is of interest\nto me that the soft-fork be done which is necessary to put a commitment in\nthe most efficient spot possible, in part because that commitment could be\nused for other data such as the merged mining auxiliary blocks, which are\nvery sensitive to proof size.\n\nPerhaps we have a different view of how the commitment transaction would be\ngenerated. Just as GBT doesn't create the coinbase, it was my expectation\nthat it wouldn't generate the commitment transaction either -- but\ngeneration of the commitment would be easy, requiring either the coinbase\ntxid 100 blocks back, or the commitment txid of the prior transaction (note\nthis impacts SPV mining). The truncation shouldn't be an issue because the\ncommitment txn would not be part of the list of transactions selected by\nGBT, and in any case the truncation would change the witness data which\nchanges the commitment.\n\nOn Wed, Dec 9, 2015 at 4:03 PM, Gregory Maxwell via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Wed, Dec 9, 2015 at 7:54 AM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n> > From this question one could think that when you said \"we can do the\n> > cleanup hardfork later\" earlier you didn't really meant it. And that\n> > you will oppose to that hardfork later just like you are opposing to\n> > it now.\n> > As said I disagree that making a softfork first and then move the\n> > commitment is less disruptive (because people will need to adapt their\n> > software twice), but if the intention is to never do the second part\n> > then of course I agree it would be less disruptive.\n> > How long after the softfork would you like to do the hardfork?\n> > 1 year after the softfork? 2 years? never?\n>\n> I think it would be logical to do as part of a hardfork that moved\n> commitments generally; e.g. a better position for merged mining (such\n> a hardfork was suggested in 2010 as something that could be done if\n> merged mining was used), room for commitments to additional block\n> back-references for compact SPV proofs, and/or UTXO set commitments.\n> Part of the reason to not do it now is that the requirements for the\n> other things that would be there are not yet well defined. For these\n> other applications, the additional overhead is actually fairly\n> meaningful; unlike the fraud proofs.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/0736595c/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-12-09T11:08:14",
                "message_text_only": "Fair enough.\nOn Dec 9, 2015 4:03 PM, \"Gregory Maxwell\" <greg at xiph.org> wrote:\n\n> On Wed, Dec 9, 2015 at 7:54 AM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n> > From this question one could think that when you said \"we can do the\n> > cleanup hardfork later\" earlier you didn't really meant it. And that\n> > you will oppose to that hardfork later just like you are opposing to\n> > it now.\n> > As said I disagree that making a softfork first and then move the\n> > commitment is less disruptive (because people will need to adapt their\n> > software twice), but if the intention is to never do the second part\n> > then of course I agree it would be less disruptive.\n> > How long after the softfork would you like to do the hardfork?\n> > 1 year after the softfork? 2 years? never?\n>\n> I think it would be logical to do as part of a hardfork that moved\n> commitments generally; e.g. a better position for merged mining (such\n> a hardfork was suggested in 2010 as something that could be done if\n> merged mining was used), room for commitments to additional block\n> back-references for compact SPV proofs, and/or UTXO set commitments.\n> Part of the reason to not do it now is that the requirements for the\n> other things that would be there are not yet well defined. For these\n> other applications, the additional overhead is actually fairly\n> meaningful; unlike the fraud proofs.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/0845be33/attachment.html>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-12-09T16:40:34",
                "message_text_only": "On Wed, Dec 9, 2015 at 3:03 AM, Gregory Maxwell via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> I think it would be logical to do as part of a hardfork that moved\n> commitments generally; e.g. a better position for merged mining (such\n> a hardfork was suggested in 2010 as something that could be done if\n> merged mining was used), room for commitments to additional block\n> back-references for compact SPV proofs, and/or UTXO set commitments.\n> Part of the reason to not do it now is that the requirements for the\n> other things that would be there are not yet well defined. For these\n> other applications, the additional overhead is actually fairly\n> meaningful; unlike the fraud proofs.\n>\n\nSo just design ahead for those future uses. Make the merkle tree:\n\n\n             root_in_block_header\n                     /      \\\n  tx_data_root      other_root\n                               /       \\\n        segwitness_root     reserved_for_future_use_root\n\n... where reserved_for_future_use is zero until some future block version\n(or perhaps better, is just chosen arbitrarily by the miner and sent along\nwith the block data until some future block version).\n\nThat would minimize future disruption of any code that produced or consumed\nmerkle proofs of the transaction data or segwitness data, especially if the\nreserved_for_future_use_root is allowed to be any arbitrary 256-bit value\nand not a constant that would get hard-coded into segwitness-proof-checking\ncode.\n\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/89ff9b06/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-12-11T16:18:48",
                "message_text_only": "On Dec 9, 2015 5:40 PM, \"Gavin Andresen\" <gavinandresen at gmail.com> wrote:\n>\n> On Wed, Dec 9, 2015 at 3:03 AM, Gregory Maxwell via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>> I think it would be logical to do as part of a hardfork that moved\n>> commitments generally; e.g. a better position for merged mining (such\n>> a hardfork was suggested in 2010 as something that could be done if\n>> merged mining was used), room for commitments to additional block\n>> back-references for compact SPV proofs, and/or UTXO set commitments.\n>> Part of the reason to not do it now is that the requirements for the\n>> other things that would be there are not yet well defined. For these\n>> other applications, the additional overhead is actually fairly\n>> meaningful; unlike the fraud proofs.\n>\n>\n> So just design ahead for those future uses. Make the merkle tree:\n>\n>\n>              root_in_block_header\n>                      /      \\\n>   tx_data_root      other_root\n>                                /       \\\n>         segwitness_root     reserved_for_future_use_root\n\nThis is basically what I meant by\n\nstruct hashRootStruct\n{\nuint256 hashMerkleRoot;\nuint256 hashWitnessesRoot;\nuint256 hashextendedHeader;\n}\n\nbut my design doesn't calculate other_root as it appears in your tree (is\nnot necessary).\n\nSince stop requiring bip34 (height in coinbase) is also a hardfork (and a\ntrivial one) I suggested to move it at the same time. But thinking more\nabout it, since BIP34 also elegantly solves BIP30, I would keep the height\nin the coinbase (even if we move it to the extented header tree as well for\nconvenience).\nThat should be able to include future consensus-enforced commitments (extra\nback-refs for compact proofs, txo/utxo commitments, etc) or non-consensus\ndata (merged mining data, miner-published data).\nGreg Maxwell suggested to move those later and I answered fair enough. But\nthinking more about it, if the extra commitments field is extensible, we\ndon't need to move anything now, and therefore we don't need for those\ndesigns (extra back-refs for compact proofs, txo/utxo commitments, etc) to\nbe ready to deploy a hardfork segregated witness: you just need to make\nsure that your format is extensible via softfork in the future.\n\nI'm therefore back to the \"let's better deploy segregated witness as a\nhardfork\" position.\nThe change required to the softfork segregated witnesses implementation\nwould be relatively small.\n\nAnother option would be to deploy both parts (sw and the movement from the\ncoinbase to the extra header) at the same time but with different\nactivation conditions, for example:\n\n- For sw: deploy as soon as possible with bip9.\n- For the hardfork codebase to extra header movement: 1 year grace + bip9\nfor later miner upgrade confirmation.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151211/2a7a0a4a/attachment.html>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-12-11T16:43:40",
                "message_text_only": "On Fri, Dec 11, 2015 at 11:18 AM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n\n> This is basically what I meant by\n>\n> struct hashRootStruct\n> {\n> uint256 hashMerkleRoot;\n> uint256 hashWitnessesRoot;\n> uint256 hashextendedHeader;\n> }\n>\n> but my design doesn't calculate other_root as it appears in your tree (is\n> not necessary).\n>\n> It is necessary to maintain compatibility with SPV nodes/wallets.\n\nAny code that just checks merkle paths up into the block header would have\nto change if the structure of the merkle tree changed to be three-headed at\nthe top.\n\nIf it remains a binary tree, then it doesn't need to change at all-- the\ncode that produces the merkle paths will just send a path that is one step\ndeeper.\n\nPlus, it's just weird to have a merkle tree that isn't a binary tree.....\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151211/2f95a032/attachment-0001.html>"
            },
            {
                "author": "digitsu at gmail.com",
                "date": "2015-12-12T05:13:48",
                "message_text_only": "If this means essentially that a soft fork deployment of SegWit will require SPV wallet servers to change their logic (or risk not being able to send payments) then it does seem to me that a hard fork to deploy this non controversial change is not only cleaner (on the data structure side) but safer in terms of the potential to affect the user experience.\u00a0\n\n\n\n\n\n\n\u2014\nRegards,\n\nOn Sat, Dec 12, 2015 at 1:43 AM, Gavin Andresen via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Fri, Dec 11, 2015 at 11:18 AM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n>> This is basically what I meant by\n>>\n>> struct hashRootStruct\n>> {\n>> uint256 hashMerkleRoot;\n>> uint256 hashWitnessesRoot;\n>> uint256 hashextendedHeader;\n>> }\n>>\n>> but my design doesn't calculate other_root as it appears in your tree (is\n>> not necessary).\n>>\n>> It is necessary to maintain compatibility with SPV nodes/wallets.\n> Any code that just checks merkle paths up into the block header would have\n> to change if the structure of the merkle tree changed to be three-headed at\n> the top.\n> If it remains a binary tree, then it doesn't need to change at all-- the\n> code that produces the merkle paths will just send a path that is one step\n> deeper.\n> Plus, it's just weird to have a merkle tree that isn't a binary tree.....\n> -- \n> --\n> Gavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151211/63cf45aa/attachment.html>"
            },
            {
                "author": "Mark Friedenbach",
                "date": "2015-12-12T15:18:46",
                "message_text_only": "A segwit supporting server would be required to support relaying segwit\ntransactions, although a non-segwit server could at least inform a wallet\nof segwit txns observed, even if it doesn't relay all information necessary\nto validate.\n\nNon segwit servers and wallets would continue operations as if nothing had\noccurred.\nIf this means essentially that a soft fork deployment of SegWit will\nrequire SPV wallet servers to change their logic (or risk not being able to\nsend payments) then it does seem to me that a hard fork to deploy this non\ncontroversial change is not only cleaner (on the data structure side) but\nsafer in terms of the potential to affect the user experience.\n\n\n\u2014 Regards,\n\n\nOn Sat, Dec 12, 2015 at 1:43 AM, Gavin Andresen via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Fri, Dec 11, 2015 at 11:18 AM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n>\n>> This is basically what I meant by\n>>\n>> struct hashRootStruct\n>> {\n>> uint256 hashMerkleRoot;\n>> uint256 hashWitnessesRoot;\n>> uint256 hashextendedHeader;\n>> }\n>>\n>> but my design doesn't calculate other_root as it appears in your tree (is\n>> not necessary).\n>>\n>> It is necessary to maintain compatibility with SPV nodes/wallets.\n>\n> Any code that just checks merkle paths up into the block header would have\n> to change if the structure of the merkle tree changed to be three-headed at\n> the top.\n>\n> If it remains a binary tree, then it doesn't need to change at all-- the\n> code that produces the merkle paths will just send a path that is one step\n> deeper.\n>\n> Plus, it's just weird to have a merkle tree that isn't a binary tree.....\n>\n> --\n> --\n> Gavin Andresen\n>\n\n\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151212/68adbb6a/attachment.html>"
            },
            {
                "author": "Jonathan Toomim",
                "date": "2015-12-14T11:21:43",
                "message_text_only": "This means that a server supporting SW might only hear of the tx data and not get the signature data for some transactions, depending on how the relay rules worked (e.g. if the SW peers had higher minrelaytxfee settings than the legacy peers). This would complicate fast block relay code like IBLTs, since we now have to check to see that the recipient has both the tx data and the witness/sig data.\n\nThe same issue might happen with block relay if we do SW as a soft fork. A SW node might see a block inv from a legacy node first, and might start downloading the block from that node. This block would then be marked as in-flight, and the witness data might not get downloaded. This shouldn't be too hard to fix by creating an inv for the witness data as a separate object, so that a node could download the block from e.g. Peer 1 and the segwit data from Peer 2.\n\nOf course, the code would be simpler if we did this as a hard fork and we could rely on everyone on the segwit fork supporting the segwit data. Although maybe we want to write the interfaces in a way that supports some nodes not downloading the segwit data anyway, just because not every node will want that data.\n\nI haven't had time to read sipa's code yet. I apologize for talking out of a position of ignorance. For anyone who has, do you feel like sharing how it deals with these network relay issues?\n\nBy the way, since this thread is really about SegWit and not about any other mechanism for increasing Bitcoin capacity, perhaps we should rename it accordingly?\n\n\nOn Dec 12, 2015, at 11:18 PM, Mark Friedenbach via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> A segwit supporting server would be required to support relaying segwit transactions, although a non-segwit server could at least inform a wallet of segwit txns observed, even if it doesn't relay all information necessary to validate.\n> \n> Non segwit servers and wallets would continue operations as if nothing had occurred.\n> \n> If this means essentially that a soft fork deployment of SegWit will require SPV wallet servers to change their logic (or risk not being able to send payments) then it does seem to me that a hard fork to deploy this non controversial change is not only cleaner (on the data structure side) but safer in terms of the potential to affect the user experience.\n> \n> \n> \u2014 Regards,\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151214/b6bcb6cb/attachment.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151214/b6bcb6cb/attachment.sig>"
            },
            {
                "author": "Adam Back",
                "date": "2015-12-14T12:44:57",
                "message_text_only": "I think someone, maybe Pieter, commented on this relay issue that it\nwould be likely very transitory, as a lot of stuff would be fairly\nquickly upgraded in practice from previous deployment experience, and\nI think anyway there is a huge excess connectivity and capacity in the\np2p network vs having a connected network of various versions, and\nsupporting SPV client load (SPV load is quite low relative to\ncapacity, even one respectable node can support a large number of SPV\nclients).\n\n(Ie so two classes of network node and connectivity wouldnt be a\nproblem in practice even if it did persist; also the higher capacity\nbetter run nodes are more likely to upgrade due to having more clued\nin power user, miner, pool or company operators).\n\nMaybe someone more detailed knowledge could clarify further.\n\nAdam\n\nOn 14 December 2015 at 19:21, Jonathan Toomim via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> This means that a server supporting SW might only hear of the tx data and\n> not get the signature data for some transactions, depending on how the relay\n> rules worked (e.g. if the SW peers had higher minrelaytxfee settings than\n> the legacy peers). This would complicate fast block relay code like IBLTs,\n> since we now have to check to see that the recipient has both the tx data\n> and the witness/sig data.\n>\n> The same issue might happen with block relay if we do SW as a soft fork. A\n> SW node might see a block inv from a legacy node first, and might start\n> downloading the block from that node. This block would then be marked as\n> in-flight, and the witness data might not get downloaded. This shouldn't be\n> too hard to fix by creating an inv for the witness data as a separate\n> object, so that a node could download the block from e.g. Peer 1 and the\n> segwit data from Peer 2.\n>\n> Of course, the code would be simpler if we did this as a hard fork and we\n> could rely on everyone on the segwit fork supporting the segwit data.\n> Although maybe we want to write the interfaces in a way that supports some\n> nodes not downloading the segwit data anyway, just because not every node\n> will want that data.\n>\n> I haven't had time to read sipa's code yet. I apologize for talking out of a\n> position of ignorance. For anyone who has, do you feel like sharing how it\n> deals with these network relay issues?\n>\n> By the way, since this thread is really about SegWit and not about any other\n> mechanism for increasing Bitcoin capacity, perhaps we should rename it\n> accordingly?\n>\n>\n> On Dec 12, 2015, at 11:18 PM, Mark Friedenbach via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> A segwit supporting server would be required to support relaying segwit\n> transactions, although a non-segwit server could at least inform a wallet of\n> segwit txns observed, even if it doesn't relay all information necessary to\n> validate.\n>\n> Non segwit servers and wallets would continue operations as if nothing had\n> occurred.\n>\n> If this means essentially that a soft fork deployment of SegWit will require\n> SPV wallet servers to change their logic (or risk not being able to send\n> payments) then it does seem to me that a hard fork to deploy this non\n> controversial change is not only cleaner (on the data structure side) but\n> safer in terms of the potential to affect the user experience.\n>\n>\n> \u2014 Regards,\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Anthony Towns",
                "date": "2015-12-09T04:51:39",
                "message_text_only": "On Wed, Dec 09, 2015 at 01:31:51AM +0000, Gregory Maxwell via bitcoin-dev wrote:\n> On Wed, Dec 9, 2015 at 1:09 AM, Gavin Andresen <gavinandresen at gmail.com> wrote:\n> > Create a 1-megabyte transaction, with all of it's inputs spending\n> > segwitness-spending SIGHASH_ALL inputs.\n> > Because the segwitness inputs are smaller in the block, you can fit more of\n> > them into 1 megabyte. Each will hash very close to one megabyte of data.\n> Witness size comes out of the 1MB at a factor of 0.25. It is not\n> possible to make a block which has signatures with the full 1MB of\n> data under the sighash while also having signatures externally.  So\n> every byte moved into the witness and thus only counted as 25% comes\n> out of the data being hashed and is hashed nInputs (*checksigs) less\n> times.\n\nSo the worst case script I can come up with is:\n\n      <pubkey> 1 0 {2OVER CHECKSIG ADD CODESEP} OP_EQUAL\n\nwhich (if I didn't mess it up) would give you a redeem script of about\n36B plus 4B per sigop, redeemable via a single signature that's valid\nfor precisely one of the checksigs.\n\nMaxing out 20k sigops gives 80kB of redeemscript in that case; so you\ncould have to hash 19.9GB of data to fully verify the script with\ncurrent bitcoin rules.\n\nSegwit with the 75% factor and the same sigop limit would make that very\nslightly worse -- it'd up the hashed data by maybe 1MB in total. Without\na sigop limit at all it'd be severely worse of course -- you could fit\nalmost 500k sigops in 2MB of witness data, leaving 500kB of base data,\nfor a total of 250GB of data to hash to verify your 3MB block...\n\nSegwit without the 75% factor, but with a 3MB of witness data limit,\nmakes that up to three times worse (750k sigops in 3MB of witness data,\nwith 1MB of base data for 750GB of data to hash), but with any reasonable\nsigop limit, afaics it's pretty much the same.\n\nHowever I think you could add some fairly straightforward (maybe\nsoft-forking) optimisations to just rule out that sort of (deliberate)\nabuse; eg disallowing more than a dozen sigops per input, or just failing\nchecksigs with the same key in a single input, maybe. So maybe that's\nnot sufficiently realistic?\n\nI think the only realistic transactions that would cause lots of sigs and\nhashing are ones that have lots of inputs that each require a signature\nor two, so might happen if a miner is cleaning up dust. In that case,\nyour 1MB transaction is a single output with a bunch of 41B inputs. If you\nhave 10k such inputs, that's only 410kB. If each input is a legitimate\n2 of 2 multisig, that's about 210 bytes of witness data per input, or\n2.1MB, leaving 475kB of base data free, which matches up. 20k sigops by\n475kB of data is 9.5GB of hashing.\n\nSwitching from 2-of-2 multisig to just a single public key would prevent\nyou from hitting the sigop limit; I think you could hit 14900 signatures\nwith about 626kB of base data and 1488kB of witness data, for about\n9.3GB of hashed data.\n\nThat's a factor of 2x improvement over the deliberately malicious exploit\ncase above, but it's /only/ a factor of 2x.\n\nI think Rusty's calculation http://rusty.ozlabs.org/?p=522 was that\nthe worst case for now is hashing about 406kB, 3300 times for 1.34GB of\nhashed data [0].\n\nSo that's still almost a factor of 4 or 5 worse than what's possible now?\nUnless I messed up the maths somewhere?\n\nCheers,\naj\n\n[0] Though I'm not sure that's correct? Seems like with a 1MB\n    transaction with i inputs, each with s bytes of scriptsig, that you're\n    hashing (1MB-s*i), and the scriptsig for a p2pkh should only be about\n    105B, not 180B.  So maximising i*(1MB-s*i) = 1e6*i - 105*i^2 gives i =\n    1e6/210, so 4762 inputs, and hashing 500kB of data each time,\n    for about 2.4GB of hashed data total."
            },
            {
                "author": "Chris",
                "date": "2015-12-09T14:51:36",
                "message_text_only": "On 12/08/2015 10:12 AM, Gavin Andresen via bitcoin-dev wrote:\n> Why segwitness as a soft fork? Stuffing the segwitness merkle tree in\n> the coinbase is messy and will just complicate consensus-critical code\n> (as opposed to making the right side of the merkle tree in\n> block.version=5 blocks the segwitness data).\nAgreed. I thought the rule was no contentious hark forks. It seems\nhardly anyone opposes this change and there seems to be widespread\nagreement that the hardfork version would be much cleaner."
            },
            {
                "author": "Jonathan Toomim",
                "date": "2015-12-08T23:48:58",
                "message_text_only": "On Dec 8, 2015, at 6:02 AM, Gregory Maxwell via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> The particular proposal amounts to a 4MB blocksize increase at worst.\n\nI understood that SegWit would allow about 1.75 MB of data in the average case while also allowing up to 4 MB of data in the worst case. This means that the mining and block distribution network would need a larger safety factor to deal with worst-case situations, right? If you want to make sure that nothing goes wrong when everything is at its worst, you need to size your network pipes to handle 4 MB in a timely (DoS-resistant) fashion, but you'd normally only be able to use 1.75 MB of it. It seems to me that it would be safer to use a 3 MB limit, and that way you'd also be able to use 3 MB of actual transactions.\n\nAs an accounting trick to bypass the 1 MB limit, SegWit sounds like it might make things less well accounted for.\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/a0eca647/attachment.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/a0eca647/attachment.sig>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-12-09T00:23:27",
                "message_text_only": "On Tue, Dec 8, 2015 at 11:48 PM, Jonathan Toomim <j at toom.im> wrote:\n> I understood that SegWit would allow about 1.75 MB of data in the average\n> case while also allowing up to 4 MB of data in the worst case. This means\n> that the mining and block distribution network would need a larger safety\n> factor to deal with worst-case situations, right? If you want to make sure\n\nBy contrast it does not reduce the safety factor for the UTXO set at\nall; which most hold as a much greater concern in general; and that\nisn't something you can say for a block size increase.\n\nWith respect to witness safety factor; it's only needed in the case of\nstrategic or malicious behavior by miners-- both concerns which\nseveral people promoting large block size increases have not only\ndisregarded but portrayed as unrealistic fear-mongering. Are you\nconcerned about it?  In any case-- the other improvements described in\nmy post give me reason to believe that risks created by that\npossibility will be addressable."
            },
            {
                "author": "Jonathan Toomim",
                "date": "2015-12-09T00:40:46",
                "message_text_only": "On Dec 9, 2015, at 8:09 AM, Gregory Maxwell <gmaxwell at gmail.com> wrote:\n\n> On Tue, Dec 8, 2015 at 11:48 PM, Jonathan Toomim <j at toom.im> wrote:\n> \n> By contrast it does not reduce the safety factor for the UTXO set at\n> all; which most hold as a much greater concern in general;\n\nI don't agree that \"most\" hold UTXO as a much greater concern in general. I think that it's a concern that has been addressed less, which means it is a more unsolved concern. But it is not currently a bottleneck on block size. Miners can afford way more RAM than 1 GB, and non-mining full nodes don't need to store the UTXO in memory.I think that at the moment, block propagation time is the bottleneck, not UTXO size. It confuses me that SigWit is being pushed as a short-term fix to the capacity issue when it does not address the short-term bottleneck at all.\n\n> and that\n> isn't something you can say for a block size increase.\n\nTrue.\n\nI'd really like to see a grand unified cost metric that includes UTXO expansion. In the mean time, I think miners can use a bit more RAM.\n\n> With respect to witness safety factor; it's only needed in the case of\n> strategic or malicious behavior by miners-- both concerns which\n> several people promoting large block size increases have not only\n> disregarded but portrayed as unrealistic fear-mongering. Are you\n> concerned about it?\n\nSome. Much less than e.g. Peter Todd, for example, but when other people see something as a concern that I don't, I try to pay attention to it. I expect Peter wouldn't like the safety factor issue, and I'm surprised he didn't bring it up.\n\nEven if I didn't care about adversarial conditions, it would still interest me to pay attention to the safety factor for political reasons, as it would make subsequent blocksize increases much more difficult. Conspiracy theorists might have a field day with that one...\n\n> In any case-- the other improvements described in\n> my post give me reason to believe that risks created by that\n> possibility will be addressable.\n\nI'll take a look and try to see which of the worst-case concerns can and cannot be addressed by those improvements.\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/c3a9894f/attachment.sig>"
            },
            {
                "author": "Daniele Pinna",
                "date": "2015-12-09T12:28:52",
                "message_text_only": "If SegWit were implemented as a hardfork, could the entire blockchain be\nreorganized starting from the Genesis block to free up historical space?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/685bb9ff/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-12-21T04:33:16",
                "message_text_only": "On Tue, Dec 8, 2015 at 6:07 AM, Wladimir J. van der Laan wrote:\n> On Mon, Dec 07, 2015 at 10:02:17PM +0000, Gregory Maxwell via bitcoin-dev\nwrote:\n>> TL;DR: I propose we work immediately towards the segwit 4MB block\n>> soft-fork which increases capacity and scalability, and recent speedups\n>> and incoming relay improvements make segwit a reasonable risk. BIP9\n>> and segwit will also make further improvements easier and faster to\n>> deploy. We\u2019ll continue to set the stage for non-bandwidth-increase-based\n>> scaling, while building additional tools that would make bandwidth\n>> increases safer long term. Further work will prepare Bitcoin for further\n>> increases, which will become possible when justified, while also\nproviding\n>> the groundwork to make them justifiable.\n>\n> Sounds good to me.\n\nBetter late than never, let me comment on why I believe pursuing this plan\nis important.\n\nFor months, the block size debate, and the apparent need for agreement on a\nhardfork has distracted from needed engineering work, fed the external\nimpression that nothing is being done, and generally created a toxic\nenvironment to work in. It has affected my own productivity and health, and\nI do not think I am alone.\n\nI believe that soft-fork segwit can help us out of this deadlock and get us\ngoing again. It does not require the pervasive assumption that the entire\nworld will simultaneously switch to new consensus rules like a hardfork\ndoes, while at the same time:\n* Give a short-term capacity bump\n* Show the world that scalability is being worked on\n* Actually improve scalability (as opposed to just scale) by reducing\nbandwidth/storage and indirectly improving the effectiveness of systems\nlike Lightning.\n* Solve several unrelated problems at the same time (fraud proofs, script\nextensibility, malleability, ...).\n\nSo I'd like to ask the community that we work towards this plan, as it\nallows to make progress without being forced to make a possibly divisive\nchoice for one hardfork or another yet.\n\n-- \nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151221/30bc71df/attachment.html>"
            },
            {
                "author": "Justus Ranvier",
                "date": "2015-12-21T04:42:03",
                "message_text_only": "On 12/20/2015 10:33 PM, Pieter Wuille via bitcoin-dev wrote:\n> Solve several unrelated problems at the same time (fraud proofs, script\n> extensibility, malleability, ...).\n\nBy \"solve\" do you mean, \"actually implement\", or do you mean \"make\nfuture implementation theoretically possible?\"\n\nIn other words, would a deployment of SW involve the creation of new\nnetwork message for relaying fraud proofs, a specification that SPV\nwallet developers can use to validate these messages and so know when to\nignore the highest (but invalid) PoW chain, and the ability to\nautomatically generate and broadcast these proofs in Bitcoin Core?\n\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: 0xEAD9E623.asc\nType: application/pgp-keys\nSize: 23337 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/8a874ec1/attachment-0001.bin>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 801 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/8a874ec1/attachment-0001.sig>"
            },
            {
                "author": "Alex Morcos",
                "date": "2015-12-21T04:44:49",
                "message_text_only": "I'm also strongly in favor of moving forward with this plan.\n\nA couple of points:\n1) There has been too much confusion in looking at segwit as an alternative\nway to increase the block size and I think that is incorrect.  It should\nnot be drawn into the block size debate as it brings many needed\nimprovements and tools we'd want even if no one were worried about block\nsize now.\n2) The full capacity increase plan Greg lays out makes it clear that we can\naccomplish a tremendous amount without a contentious hard fork at this\npoint.\n3) Let's stop arguing endlessly and actually do work that will benefit\neveryone.\n\n\n\n\nOn Sun, Dec 20, 2015 at 11:33 PM, Pieter Wuille via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Tue, Dec 8, 2015 at 6:07 AM, Wladimir J. van der Laan wrote:\n> > On Mon, Dec 07, 2015 at 10:02:17PM +0000, Gregory Maxwell via\n> bitcoin-dev wrote:\n> >> TL;DR: I propose we work immediately towards the segwit 4MB block\n> >> soft-fork which increases capacity and scalability, and recent speedups\n> >> and incoming relay improvements make segwit a reasonable risk. BIP9\n> >> and segwit will also make further improvements easier and faster to\n> >> deploy. We\u2019ll continue to set the stage for non-bandwidth-increase-based\n> >> scaling, while building additional tools that would make bandwidth\n> >> increases safer long term. Further work will prepare Bitcoin for further\n> >> increases, which will become possible when justified, while also\n> providing\n> >> the groundwork to make them justifiable.\n> >\n> > Sounds good to me.\n>\n> Better late than never, let me comment on why I believe pursuing this plan\n> is important.\n>\n> For months, the block size debate, and the apparent need for agreement on\n> a hardfork has distracted from needed engineering work, fed the external\n> impression that nothing is being done, and generally created a toxic\n> environment to work in. It has affected my own productivity and health, and\n> I do not think I am alone.\n>\n> I believe that soft-fork segwit can help us out of this deadlock and get\n> us going again. It does not require the pervasive assumption that the\n> entire world will simultaneously switch to new consensus rules like a\n> hardfork does, while at the same time:\n> * Give a short-term capacity bump\n> * Show the world that scalability is being worked on\n> * Actually improve scalability (as opposed to just scale) by reducing\n> bandwidth/storage and indirectly improving the effectiveness of systems\n> like Lightning.\n> * Solve several unrelated problems at the same time (fraud proofs, script\n> extensibility, malleability, ...).\n>\n> So I'd like to ask the community that we work towards this plan, as it\n> allows to make progress without being forced to make a possibly divisive\n> choice for one hardfork or another yet.\n>\n> --\n> Pieter\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/727153dd/attachment.html>"
            },
            {
                "author": "Mark Friedenbach",
                "date": "2015-12-21T04:50:03",
                "message_text_only": "I am fully in support of the plan laid out in \"Capacity increases for the\nbitcoin system\".\n\nThis plan provides real benefit to the ecosystem in solving a number of\nlongstanding problems in bitcoin. It improves the scalability of bitcoin\nconsiderably.\n\nFurthermore it is time that we stop bikeshedding, start implementing, and\nmove forward, lest we lose more developers to the toxic atmosphere this\nhard-fork debacle has created.\n\nOn Mon, Dec 21, 2015 at 12:33 PM, Pieter Wuille via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Tue, Dec 8, 2015 at 6:07 AM, Wladimir J. van der Laan wrote:\n> > On Mon, Dec 07, 2015 at 10:02:17PM +0000, Gregory Maxwell via\n> bitcoin-dev wrote:\n> >> TL;DR: I propose we work immediately towards the segwit 4MB block\n> >> soft-fork which increases capacity and scalability, and recent speedups\n> >> and incoming relay improvements make segwit a reasonable risk. BIP9\n> >> and segwit will also make further improvements easier and faster to\n> >> deploy. We\u2019ll continue to set the stage for non-bandwidth-increase-based\n> >> scaling, while building additional tools that would make bandwidth\n> >> increases safer long term. Further work will prepare Bitcoin for further\n> >> increases, which will become possible when justified, while also\n> providing\n> >> the groundwork to make them justifiable.\n> >\n> > Sounds good to me.\n>\n> Better late than never, let me comment on why I believe pursuing this plan\n> is important.\n>\n> For months, the block size debate, and the apparent need for agreement on\n> a hardfork has distracted from needed engineering work, fed the external\n> impression that nothing is being done, and generally created a toxic\n> environment to work in. It has affected my own productivity and health, and\n> I do not think I am alone.\n>\n> I believe that soft-fork segwit can help us out of this deadlock and get\n> us going again. It does not require the pervasive assumption that the\n> entire world will simultaneously switch to new consensus rules like a\n> hardfork does, while at the same time:\n> * Give a short-term capacity bump\n> * Show the world that scalability is being worked on\n> * Actually improve scalability (as opposed to just scale) by reducing\n> bandwidth/storage and indirectly improving the effectiveness of systems\n> like Lightning.\n> * Solve several unrelated problems at the same time (fraud proofs, script\n> extensibility, malleability, ...).\n>\n> So I'd like to ask the community that we work towards this plan, as it\n> allows to make progress without being forced to make a possibly divisive\n> choice for one hardfork or another yet.\n>\n> --\n> Pieter\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151221/fa4c9349/attachment.html>"
            },
            {
                "author": "Douglas Roark",
                "date": "2015-12-21T05:29:16",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA512\n\nOn 2015/12/20 20:50, Mark Friedenbach via bitcoin-dev wrote:\n> I am fully in support of the plan laid out in \"Capacity increases \n> for the bitcoin system\".\n> \n> This plan provides real benefit to the ecosystem in solving a \n> number of longstanding problems in bitcoin. It improves the \n> scalability of bitcoin considerably.\n> \n> Furthermore it is time that we stop bikeshedding, start \n> implementing, and move forward, lest we lose more developers to\n> the toxic atmosphere this hard-fork debacle has created.\n\nAnother +1 here. While I'd still like to see some sort of short-term\nbump happen this year - good points have been raised about SegWit\nuptake by wallet devs, for one thing - I really do think this is one\nof the last pieces of the puzzle that'll make Bitcoin reasonably\nstable and robust. If people have legitimate concerns, that's great,\nand they should be addressed. I just worry that more navel-gazing and\nbikeshedding will play into the hands of those with less than noble\nintentions. That and, due to the somewhat complicated nature of\nSegWit, it may take time to get skeptical miners and wallet devs on-boar\nd.\n\nWhile we're talking about capacity increases, I'd like to reiterate\nthat I do think there should be some sort of short-term bump (Jeff's\nBIP 102 or his \"BIP 202\" variant, Dr. Back's 2/4/8 proposal (\"BIP\n248\"), etc.), hopefully chosen by this summer so that everybody can\nstart to prepare. I believe the KISS theory will work best. I talked\nto a couple of miners at Scaling Bitcoin. It was obvious they\ngenerally prefer simple solutions. (For that matter, if I put my\nminer's cap on, I prefer simple solutions too!) The research presented\nat Scaling Bitcoin regarding block size formulas was quite interesting\nand worthy of discussion. The research was also, IMO, nowhere near\nready for consensus. Work and discussions on that front should\ncertainly continue and push for a more permanent (final?) block size\nsolution. I just think that, barring some extraordinary solution that\nhasn't been widely discussed yet, a permanent solution isn't feasible\nright now. A temporary bump isn't ideal. It's just the only thing I've\nseen that strikes me as having any real shot at consensus.\n\n- -- \n- ---\nDouglas Roark\nCryptocurrency, network security, travel, and art.\nhttps://onename.com/droark\njoroark at vt.edu\nPGP key ID: 26623924\n-----BEGIN PGP SIGNATURE-----\nComment: GPGTools - https://gpgtools.org\n\niQIcBAEBCgAGBQJWd44sAAoJEEOBHRomYjkklUkP/AqnD4+oiNNNYRGDY3m0bQSG\nnoUoRmWG/h86AW+2LuNYtn72UVefWJscUcmXWeOOem1KX49KdtCRWz3UZcrmfPUF\nh/ilOpYpjCN69nFBhpJPp+0Jqr/PjQpoZkUQ2G1BznGIcIo3jwh7H7dQeI6PMtLB\nqTbfdYEqPawb2kIhrCKVVQqsf7dLjg0Hlzvnq+xqyggZ1+k89kXSMEHJaybras7q\nDFj1lOhzktzAtxquzAMcctkZM3JvFMnKUwOP6zC+ke9YlmvU0Yhu74F+30/EClLc\nXGL5GMvUtvJcC0VRxDlh4pIW3m+eWjLWxvPQGe58eLE2u2Ja2MNjcuVtJdRgouLI\nVSPBrUKoGOGfNfsqJH9U9jsvRuQMvT6JFS3jjxiapgi+ip1O7+Pkbq6tO55Mz7Gd\nWMG71HdrLzZtjOzRmOFL5q3CkTpZp75tsXOYxn7jVcJlYJUh/jrnVMvSbPAT/VAY\nyJIPtWRj+jtMKAR9m4Lx+9N4F56OC3g0M749v31luoYZkKMl7ohgkONgpKhrDRBU\nuVmWH0pUIvaScsJxrUtgZdqn2AUqRowq6nM0YNDKo4go5/LyAkYYi1mICb0O0JJG\nmt+3fabix6biBPHZDAvKxKX5CAPDapno2adTBx7vY36evGdhI9sWA1jw91He8Zmw\n8hwnRV7R8bPdkoIfnc8e\n=jJzD\n-----END PGP SIGNATURE-----"
            },
            {
                "author": "Btc Drak",
                "date": "2015-12-21T05:21:55",
                "message_text_only": "On Mon, Dec 21, 2015 at 4:33 AM, Pieter Wuille via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Tue, Dec 8, 2015 at 6:07 AM, Wladimir J. van der Laan wrote:\n> > On Mon, Dec 07, 2015 at 10:02:17PM +0000, Gregory Maxwell via\n> bitcoin-dev wrote:\n> >> TL;DR: I propose we work immediately towards the segwit 4MB block\n> >> soft-fork which increases capacity and scalability, and recent speedups\n> >> and incoming relay improvements make segwit a reasonable risk. BIP9\n> >> and segwit will also make further improvements easier and faster to\n> >> deploy. We\u2019ll continue to set the stage for non-bandwidth-increase-based\n> >> scaling, while building additional tools that would make bandwidth\n> >> increases safer long term. Further work will prepare Bitcoin for further\n> >> increases, which will become possible when justified, while also\n> providing\n> >> the groundwork to make them justifiable.\n> >\n> > Sounds good to me.\n>\n> Better late than never, let me comment on why I believe pursuing this plan\n> is important.\n>\n> For months, the block size debate, and the apparent need for agreement on\n> a hardfork has distracted from needed engineering work, fed the external\n> impression that nothing is being done, and generally created a toxic\n> environment to work in. It has affected my own productivity and health, and\n> I do not think I am alone.\n>\n> I believe that soft-fork segwit can help us out of this deadlock and get\n> us going again. It does not require the pervasive assumption that the\n> entire world will simultaneously switch to new consensus rules like a\n> hardfork does, while at the same time:\n> * Give a short-term capacity bump\n> * Show the world that scalability is being worked on\n> * Actually improve scalability (as opposed to just scale) by reducing\n> bandwidth/storage and indirectly improving the effectiveness of systems\n> like Lightning.\n> * Solve several unrelated problems at the same time (fraud proofs, script\n> extensibility, malleability, ...).\n>\n> So I'd like to ask the community that we work towards this plan, as it\n> allows to make progress without being forced to make a possibly divisive\n> choice for one hardfork or another yet.\n>\nThank you for saying this. I also think the plan is solid and delivers\nmultiple benefits without being contentious. The number of wins are so\nnumerous, it's frankly a no-brainer.\n\nI guess the next step for segwit is a BIP and deployment on a testnet?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151221/c11d08e4/attachment.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2015-12-21T08:07:47",
                "message_text_only": "On Mon, Dec 21, 2015 at 05:21:55AM +0000, Btc Drak via bitcoin-dev wrote:\n> On Mon, Dec 21, 2015 at 4:33 AM, Pieter Wuille via bitcoin-dev <\n> > So I'd like to ask the community that we work towards this plan, as it\n> > allows to make progress without being forced to make a possibly divisive\n> > choice for one hardfork or another yet.\n> Thank you for saying this. I also think the plan is solid and delivers\n> multiple benefits without being contentious. The number of wins are so\n> numerous, it's frankly a no-brainer.\n\n+1's are off-topic, but... +1. My impression is that each of libsecp256k1,\nversionbits, segregated witness, IBLT, weak blocks, and OP_CSV have\nbeen demonstrated to be significant improvements that are implementable,\nand don't introduce any new attacks or risks [0]. There's some freaking\nawesome engineering that's gone into all of those.\n\n> I guess the next step for segwit is a BIP and deployment on a testnet?\n\nI think the following proposed features are as yet missing from Pieter's\nsegwit branch, and I'm guessing patches for them would be appreciated:\n\n - enforcing the proposed base+witness/4 < 1MB calculation\n - applying limits to sigops seen in witness signatures\n\nI guess there might be other things that still need to be implemented\nas well (and presumably bugs of course)?\n\nI think I'm convinced that the proposed plan is the best approach (as\nopposed to separate base<1MB, witness<3MB limits, or done as a hard fork,\nor without committing to a merkle head for the witnesses, eg), though.\n\njl2012 already pointed to a draft segwit BIP in another thread, repeated\nhere though:\n\n https://github.com/jl2012/bips/blob/segwit/bip-segwit.mediawiki\n\nCheers,\naj (hoping that was enough content after the +1 to not get modded ;)\n\n[0] I'm still not persuaded that even a small increase in blocksize\n    doesn't introduce unacceptable risks (frankly, I'm not entirely\n    persuaded the *current* limits don't have unacceptable risk) and that\n    frustrates me no end. But I guess (even after six months of reading\n    arguments about it!) I'm equally unpersuaded that there's actually\n    more to the intense desire for more blocksize is anything other than\n    fear/uncertainty/doubt mixed with a desire for transactions to be\n    effectively free, rather than costing even a few cents each... So,\n    personally, since the above doesn't really resolve that quandry\n    for me, it doesn't really resolve the blocksize debate for me\n    either. YMMV."
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-12-21T09:56:54",
                "message_text_only": "To clarify, although I have defended the deployment of segwit as a\nhardfork, I have no strong opinion on whether to do that or do it as a\nsoftfork first and then do a hardfork to move things out of the\ncoinbase to a better place.\nI have a strong opinion against never doing the later hardfork though.\nI would have supported segwit for Bitcoin even if it was only possible\nas a hardfork, but there's a softfork version and that will hopefully\naccelerate its deployment.\nSince the plan seems to be to do a softfork first and a hardfork\nmoving the witness tree (and probably more things) outside of the\ncoinbase later, I support the plan for segwit deployment.\nIn fact, the plan is very exciting to me."
            }
        ],
        "thread_summary": {
            "title": "Capacity increases for the Bitcoin system.",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Anthony Towns",
                "digitsu at gmail.com",
                "Tier Nolan",
                "Jonathan Toomim",
                "Wladimir J. van der Laan",
                "Justus Ranvier",
                "Adam Back",
                "Ryan Butler",
                "Jorge Tim\u00f3n",
                "Gregory Maxwell",
                "Btc Drak",
                "Mark Friedenbach",
                "Bryan Bishop",
                "Daniele Pinna",
                "Luke Dashjr",
                "Gavin Andresen",
                "Douglas Roark",
                "Alex Morcos",
                "Pieter Wuille",
                "Chris"
            ],
            "messages_count": 54,
            "total_messages_chars_count": 123704
        }
    },
    {
        "title": "[bitcoin-dev] Coalescing Transactions BIP Draft",
        "thread_messages": [
            {
                "author": "Chris Priest",
                "date": "2015-12-07T23:39:12",
                "message_text_only": "I made a post a few days ago where I laid out a scheme for\nimplementing \"coalescing transactions\" using a new opcode. I have\nsince come to the realization that an opcode is not the best way to do\nthis. A much better approach I think is a new \"transaction type\" field\nthat is split off from the version field. Other uses can come out of\nthis type field, wildcard inputs is just the first one.\n\nThere are two unresolved issues. First, there might need to be a limit\non how many inputs are included in the \"coalesce\". Lets say you have\nan address that has 100,000,000 inputs. If you were to coalesce them\nall into one single input, that means that every node has to count of\nthese 100,000,000 inputs, which could take a long time. But then\nagain, the total number of inputs a wildcard can cover is limited to\nthe actual number of UTXOs in the pool, which is very much a\nfinite/constrained number.\n\nOne solution is to limit all wildcard inputs to, say, 10,000 items. If\nyou have more inputs that you want coalesced, you have to do it in\n10,000 chunks, starting from the beginning. I want wildcard inputs to\nlook as much like normal inputs as much as possible to facilitate\nimplementation, so embedding a \"max search\" inside the transaction I\ndon't think is the best idea. I think if there is going to be a limit,\nit should be implied.\n\nThe other issue is with limiting wildcard inputs to only inputs that\nare confirmed into a fixed number of blocks. Sort of like how coinbase\nhas to be a certain age before it can be spent, maybe wildcard inputs\nshould only work on inputs older than a certain block age. Someone\nbrought up in the last thread that re-orgs can cause problems. I don't\nquite see how that could happen, as re-orgs don't really affect\naddress balances, only block header values, which coalescing\ntransactions have nothing to do with.\n\nHere is the draft:\nhttps://github.com/priestc/bips/blob/master/bip-coalesc-wildcard.mediawiki"
            }
        ],
        "thread_summary": {
            "title": "Coalescing Transactions BIP Draft",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Chris Priest"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1937
        }
    },
    {
        "title": "[bitcoin-dev] Key.run: BIP-70 Payments and OP_RETURN",
        "thread_messages": [
            {
                "author": "Toby Padilla",
                "date": "2015-12-08T02:10:22",
                "message_text_only": "Hi all. I've been working on a new publication platform based on Bitcoin\ncalled key.run: http://key.run\n\nThe high level overview is that key.run stores BitTorrent info hashes\n(magnet links) in the blockchain by sending transactions to a \"namespace\"\nBitcoin address. Using SPV, I reconstitute the key.run db from the\nblockchain. This is meant to be an open source and distributed system so\nanyone can run the key.run server (and change namespace keys). More info\nhere: https://git.playgrub.com/toby/keyrun\n\nNow to my issue...\n\nOne of the main use cases I wanted to support was people using their\n*existing* Bitcoin wallet to encode the key.run transactions on the\nblockchain. Practically speaking this meant building the transactions with\nthe proper OP_RETURN value server-side then passing them to the end user's\nwallet via BIP-70. I have this working with Bitcoin Core (there are issues\nwith other wallets I've tested with BIP-70).\n\nThe problem I'm having is that Bitcoin Core will not allow BIP-70\nPaymentDetails to contain outputs with zero value. As stated in\nhttps://github.com/bitcoin/bips/blob/master/bip-0070.mediawiki:\n\n\"if there are more than one; Outputs with zero amounts shall be ignored\"\n\nBitcoin Core doesn't seem to ignore the output though, it rejects the\ntransaction and doesn't allow the user to submit it. The key.run\ntransactions currently work by giving the OP_RETURN outputs a non-zero >\ndust value. This value is presumably lost forever.\n\nI think ideally OP_RETURN outputs with zero value WOULD be allowed since\nthey are valid transactions. Allowing OP_RETURN outputs to be constructed\nwith BIP-70 Payments is also the only way I can think of to extend the\nfunctionality of existing wallets.\n\nI would love to get feedback on if there is an alternative way to do what I\npropose or ideally if BIP-70 could be tweaked to allow OP_RETURN outputs\nwith zero value.\n\nI'd also love feedback on key.run but this probably isn't the best venue\nfor that. I've setup #keyrun on Freenode if anyone is interested in\ndiscussing the project.\n\nRegards,\nToby\n\n--\nhttp://twitter.com/toby\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151207/ad78a3e4/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Key.run: BIP-70 Payments and OP_RETURN",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Toby Padilla"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2279
        }
    },
    {
        "title": "[bitcoin-dev] BIP 9 style version bits for txns",
        "thread_messages": [
            {
                "author": "Vincent Truong",
                "date": "2015-12-08T12:27:27",
                "message_text_only": "So I have been told more than once that the version announcement in blocks\nis not a vote, but a signal for readiness, used in isSupermajority().\nBasically, if soft forks (and hard forks) won't activate unless a certain %\nof blocks have been flagged with the version up (or bit flipped when\nversionbits go live) to signal their readiness, that is a vote against\nimplementation if they never follow up. I don't like this politically\ncorrect speech because in reality it is a vote... But I'm not here to argue\nabout that... I would like to see if there are any thoughts on\nextending/copying isSupermajority() for a new secondary/non-critical\nfunction to also look for a similar BIP 9 style version bit in txns.\nApologies if already proposed, haven't heard of it anywhere.\n\nIf we are looking for a signal of readiness, it is unfair to wallet\ndevelopers and exchanges that they are unable to signal if they too are\nready for a change. As more users are going into use SPV or SPV-like\nwallets, when a change occurs that makes them incompatible/in need of\nupgrade we need to make sure they aren't going to break or introduce\nsecurity flaws for users.\n\nIf a majority of transactions have been sent are flagged ready, we know\nthat they're also good to go.\n\nWould you implement the same versionbits for txn's version field, using 3\nbits for versioning and 29 bits for flags? This indexing of every txn might\nsound insane and computationally expensive for bitcoin Core to run, but if\nit isn't critical to upgrade with soft forks, then it can be watched\noutside the network by enthusiasts. I believe this is the most politically\ncorrect way to get wallet devs and exchanges involved. (If it were me I\nwould absolutely try figure out a way to stick it in isSupermajority...)\n\nMiners can watch for readiness flagged by wallets before they themselves\nflag ready. We will have to trust miners to not jump the gun, but that's\nthe trade off.\n\nThoughts?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/7274423d/attachment.html>"
            },
            {
                "author": "Chris Priest",
                "date": "2015-12-08T19:40:36",
                "message_text_only": "I proposed in my Wildcard Inputs BIP that the version field be split\nin two. The first 4 bytes are version number (which in practice is\nbeing used for script version), and the second 4 bits are used for\ntransaction type.\n\nI don't think the BIP9 mechanism really applies to transactions. A\nblock is essentially a collection of transactions, therefore voting on\nthe block applies to the many parties who have transactions in the\nblock. A transaction on the other hand only effects at most two\nparties (the sender and the receiver). In other words, block are\n\"communal\" data structures, transactions are individual data\nstructures. Also, the nature of soft forks are that wallets can choose\nto implement a new feature or not. For instance, if no wallets\nimplement RBF or SW, then those features effectively don't exist,\nregardless of how many nodes have upgraded to handle the feature.\n\nAny new transaction feature should get a new \"type\" number. A new\ntransaction feature can't happen until the nodes support it.\n\nOn 12/8/15, Vincent Truong via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> So I have been told more than once that the version announcement in blocks\n> is not a vote, but a signal for readiness, used in isSupermajority().\n> Basically, if soft forks (and hard forks) won't activate unless a certain %\n> of blocks have been flagged with the version up (or bit flipped when\n> versionbits go live) to signal their readiness, that is a vote against\n> implementation if they never follow up. I don't like this politically\n> correct speech because in reality it is a vote... But I'm not here to argue\n> about that... I would like to see if there are any thoughts on\n> extending/copying isSupermajority() for a new secondary/non-critical\n> function to also look for a similar BIP 9 style version bit in txns.\n> Apologies if already proposed, haven't heard of it anywhere.\n>\n> If we are looking for a signal of readiness, it is unfair to wallet\n> developers and exchanges that they are unable to signal if they too are\n> ready for a change. As more users are going into use SPV or SPV-like\n> wallets, when a change occurs that makes them incompatible/in need of\n> upgrade we need to make sure they aren't going to break or introduce\n> security flaws for users.\n>\n> If a majority of transactions have been sent are flagged ready, we know\n> that they're also good to go.\n>\n> Would you implement the same versionbits for txn's version field, using 3\n> bits for versioning and 29 bits for flags? This indexing of every txn might\n> sound insane and computationally expensive for bitcoin Core to run, but if\n> it isn't critical to upgrade with soft forks, then it can be watched\n> outside the network by enthusiasts. I believe this is the most politically\n> correct way to get wallet devs and exchanges involved. (If it were me I\n> would absolutely try figure out a way to stick it in isSupermajority...)\n>\n> Miners can watch for readiness flagged by wallets before they themselves\n> flag ready. We will have to trust miners to not jump the gun, but that's\n> the trade off.\n>\n> Thoughts?\n>"
            },
            {
                "author": "Vincent Truong",
                "date": "2015-12-08T21:02:55",
                "message_text_only": "I suppose whether the wallet devs want to implement the soft fork or not is\nirrelevant. They only need to indicate if they are ready i.e. they've\ntested the new soft fork, hard fork or feature and validated that it\ndoesn't break their nodes or wallet software.\nOn Dec 9, 2015 6:40 AM, \"Chris Priest\" <cp368202 at ohiou.edu> wrote:\n\n> I proposed in my Wildcard Inputs BIP that the version field be split\n> in two. The first 4 bytes are version number (which in practice is\n> being used for script version), and the second 4 bits are used for\n> transaction type.\n>\n> I don't think the BIP9 mechanism really applies to transactions. A\n> block is essentially a collection of transactions, therefore voting on\n> the block applies to the many parties who have transactions in the\n> block. A transaction on the other hand only effects at most two\n> parties (the sender and the receiver). In other words, block are\n> \"communal\" data structures, transactions are individual data\n> structures. Also, the nature of soft forks are that wallets can choose\n> to implement a new feature or not. For instance, if no wallets\n> implement RBF or SW, then those features effectively don't exist,\n> regardless of how many nodes have upgraded to handle the feature.\n>\n> Any new transaction feature should get a new \"type\" number. A new\n> transaction feature can't happen until the nodes support it.\n>\n> On 12/8/15, Vincent Truong via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > So I have been told more than once that the version announcement in\n> blocks\n> > is not a vote, but a signal for readiness, used in isSupermajority().\n> > Basically, if soft forks (and hard forks) won't activate unless a\n> certain %\n> > of blocks have been flagged with the version up (or bit flipped when\n> > versionbits go live) to signal their readiness, that is a vote against\n> > implementation if they never follow up. I don't like this politically\n> > correct speech because in reality it is a vote... But I'm not here to\n> argue\n> > about that... I would like to see if there are any thoughts on\n> > extending/copying isSupermajority() for a new secondary/non-critical\n> > function to also look for a similar BIP 9 style version bit in txns.\n> > Apologies if already proposed, haven't heard of it anywhere.\n> >\n> > If we are looking for a signal of readiness, it is unfair to wallet\n> > developers and exchanges that they are unable to signal if they too are\n> > ready for a change. As more users are going into use SPV or SPV-like\n> > wallets, when a change occurs that makes them incompatible/in need of\n> > upgrade we need to make sure they aren't going to break or introduce\n> > security flaws for users.\n> >\n> > If a majority of transactions have been sent are flagged ready, we know\n> > that they're also good to go.\n> >\n> > Would you implement the same versionbits for txn's version field, using 3\n> > bits for versioning and 29 bits for flags? This indexing of every txn\n> might\n> > sound insane and computationally expensive for bitcoin Core to run, but\n> if\n> > it isn't critical to upgrade with soft forks, then it can be watched\n> > outside the network by enthusiasts. I believe this is the most\n> politically\n> > correct way to get wallet devs and exchanges involved. (If it were me I\n> > would absolutely try figure out a way to stick it in isSupermajority...)\n> >\n> > Miners can watch for readiness flagged by wallets before they themselves\n> > flag ready. We will have to trust miners to not jump the gun, but that's\n> > the trade off.\n> >\n> > Thoughts?\n> >\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/86112498/attachment-0001.html>"
            },
            {
                "author": "Chris Priest",
                "date": "2015-12-08T22:27:48",
                "message_text_only": "A wallet doesn't receive transactions from other wallets. That is what\na node does. Wallets just make transactions and then sends them to the\nnodes. Nodes then send them to other nodes.\n\nIn the early days of bitcoin, all wallets were nodes, but now a lot of\nwallets are just wallets with out any specific node. For instance, SPV\nwallets, they don't get their UTXO data from any one node that can or\ncan not support a feature. They get UTXO data from many nodes, some of\nwhich could support said feature, others may not.\n\nThe nature of the work that nodes perform, they *should* broadcast\nwhat features they support. The only nodes that matter to the network\nare nodes that produce blocks. Nodes that don't produce blocks are\nkind of just there, serving whoever happens to connect... I guess\nnodes could broadcast their supported implementations of via part of\nthe version message that is part of the p2p handshake process...\n\nOn 12/8/15, Vincent Truong <vincent.truong at procabiak.com> wrote:\n> I suppose whether the wallet devs want to implement the soft fork or not is\n> irrelevant. They only need to indicate if they are ready i.e. they've\n> tested the new soft fork, hard fork or feature and validated that it\n> doesn't break their nodes or wallet software.\n> On Dec 9, 2015 6:40 AM, \"Chris Priest\" <cp368202 at ohiou.edu> wrote:\n>\n>> I proposed in my Wildcard Inputs BIP that the version field be split\n>> in two. The first 4 bytes are version number (which in practice is\n>> being used for script version), and the second 4 bits are used for\n>> transaction type.\n>>\n>> I don't think the BIP9 mechanism really applies to transactions. A\n>> block is essentially a collection of transactions, therefore voting on\n>> the block applies to the many parties who have transactions in the\n>> block. A transaction on the other hand only effects at most two\n>> parties (the sender and the receiver). In other words, block are\n>> \"communal\" data structures, transactions are individual data\n>> structures. Also, the nature of soft forks are that wallets can choose\n>> to implement a new feature or not. For instance, if no wallets\n>> implement RBF or SW, then those features effectively don't exist,\n>> regardless of how many nodes have upgraded to handle the feature.\n>>\n>> Any new transaction feature should get a new \"type\" number. A new\n>> transaction feature can't happen until the nodes support it.\n>>\n>> On 12/8/15, Vincent Truong via bitcoin-dev\n>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> > So I have been told more than once that the version announcement in\n>> blocks\n>> > is not a vote, but a signal for readiness, used in isSupermajority().\n>> > Basically, if soft forks (and hard forks) won't activate unless a\n>> certain %\n>> > of blocks have been flagged with the version up (or bit flipped when\n>> > versionbits go live) to signal their readiness, that is a vote against\n>> > implementation if they never follow up. I don't like this politically\n>> > correct speech because in reality it is a vote... But I'm not here to\n>> argue\n>> > about that... I would like to see if there are any thoughts on\n>> > extending/copying isSupermajority() for a new secondary/non-critical\n>> > function to also look for a similar BIP 9 style version bit in txns.\n>> > Apologies if already proposed, haven't heard of it anywhere.\n>> >\n>> > If we are looking for a signal of readiness, it is unfair to wallet\n>> > developers and exchanges that they are unable to signal if they too are\n>> > ready for a change. As more users are going into use SPV or SPV-like\n>> > wallets, when a change occurs that makes them incompatible/in need of\n>> > upgrade we need to make sure they aren't going to break or introduce\n>> > security flaws for users.\n>> >\n>> > If a majority of transactions have been sent are flagged ready, we know\n>> > that they're also good to go.\n>> >\n>> > Would you implement the same versionbits for txn's version field, using\n>> > 3\n>> > bits for versioning and 29 bits for flags? This indexing of every txn\n>> might\n>> > sound insane and computationally expensive for bitcoin Core to run, but\n>> if\n>> > it isn't critical to upgrade with soft forks, then it can be watched\n>> > outside the network by enthusiasts. I believe this is the most\n>> politically\n>> > correct way to get wallet devs and exchanges involved. (If it were me I\n>> > would absolutely try figure out a way to stick it in\n>> > isSupermajority...)\n>> >\n>> > Miners can watch for readiness flagged by wallets before they\n>> > themselves\n>> > flag ready. We will have to trust miners to not jump the gun, but\n>> > that's\n>> > the trade off.\n>> >\n>> > Thoughts?\n>> >\n>>\n>"
            }
        ],
        "thread_summary": {
            "title": "BIP 9 style version bits for txns",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Chris Priest",
                "Vincent Truong"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 13628
        }
    },
    {
        "title": "[bitcoin-dev] Can kick",
        "thread_messages": [
            {
                "author": "Jonathan Toomim",
                "date": "2015-12-08T12:59:51",
                "message_text_only": "I am leaning towards supporting a can kick proposal. Features I think are desirable for this can kick:\n\n0. Block size limit around 2 to 4 MB. Maybe 3 MB? Based on my testnet data, I think 3 MB should be pretty safe.\n1. Hard fork with a consensus mechanisms similar to BIP101\n2. Approximately 1 or 2 month delay before activation to allow for miners to upgrade their infrastructure.\n3. Some form of validation cost metric. BIP101's validation cost metric would be the minimum strictness that I would support, but it would be nice if there were a good UTXO growth metric too. (I do not know enough about the different options to evaluate them right now.)\n\nI will be working on a few improvements to block propagation (especially from China) over the next few months, like blocktorrent and stratum-based GFW penetration. I hope to have these working within a few months. Depending on how those efforts and others (e.g. IBLTs) go, we can look at increasing the block size further, and possibly enacting a long-term scaling roadmap like BIP101.\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/75222ee6/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "Can kick",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jonathan Toomim"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1340
        }
    },
    {
        "title": "[bitcoin-dev] Scaling by Partitioning",
        "thread_messages": [
            {
                "author": "Akiva Lichtner",
                "date": "2015-12-08T16:27:18",
                "message_text_only": "Hello,\n\nI am seeking some expert feedback on an idea for scaling Bitcoin. As a\nbrief introduction: I work in the payment industry and I have twenty years'\nexperience in development. I have some experience with process groups and\nordering protocols too. I think I understand Satoshi's paper but I admit I\nhave not read the source code.\n\nThe idea is to run more than one simultaneous chain, each chain defeating\ndouble spending on only part of the coin. The coin would be partitioned by\nradix (or modulus, not sure what to call it.) For example in order to\nmultiply throughput by a factor of ten you could run ten parallel chains,\none would work on coin that ends in \"0\", one on coin that ends in \"1\", and\nso on up to \"9\".\n\nThe number of chains could increase automatically over time based on the\nmoving average of transaction volume.\n\nBlocks would have to contain the number of the partition they belong to,\nand miners would have to round-robin through partitions so that an attacker\nwould not have an unfair advantage working on just one partition.\n\nI don't think there is much impact to miners, but clients would have to\nsend more than one message in order to spend money. Client messages will\nneed to enumerate coin using some sort of compression, to save space. This\nseems okay to me since often in computing client software does have to\nbreak things up in equal parts (e.g. memory pages, file system blocks,) and\nthe client software could hide the details.\n\nBest wishes for continued success to the project.\n\nRegards,\nAkiva\n\nP.S. I found a funny anagram for SATOSHI NAKAMOTO: \"NSA IS OOOK AT MATH\"\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/37780f8d/attachment.html>"
            },
            {
                "author": "Bryan Bishop",
                "date": "2015-12-08T16:45:32",
                "message_text_only": "On Tue, Dec 8, 2015 at 10:27 AM, Akiva Lichtner via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> and miners would have to round-robin through partitions\n\nAt first glance this proposal seems most similar to the sharding proposals:\n\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/sharding-the-blockchain/\nhttps://github.com/vbuterin/scalability_paper/blob/master/scalability.pdf\nhttps://www.reddit.com/r/Bitcoin/comments/3u1m36/why_arent_we_as_a_community_talking_about/cxbamhn\nhttp://eprint.iacr.org/2015/1168.pdf (committee approach)\n\n> but clients would have to send more than one message in order to spend money\n\nOffloading work to the client for spends has in the past been a\nwell-received concept, such as the linearized coin history idea.\n\n- Bryan\nhttp://heybryan.org/\n1 512 203 0507"
            },
            {
                "author": "Akiva Lichtner",
                "date": "2015-12-08T18:30:01",
                "message_text_only": "Thanks for your response and links.\n\nI think the difference is that those proposals all shard the mining work,\nwhereas what I wrote in my post shards the coin itself. In other words\ndifferent parts of the coin space are forever segregated, never ending up\nin the same block. It's a big difference conceptually because I could spend\nmoney and a fraction of it makes it into a block in ten minutes and the\nrest two hours later.\n\nAnd I think that's where the potential for the scalability comes in. I am\nnot really scaling Bitcoin's present requirements, I am relaxing the\nrequirements in a way that leaves the users and the miners happy, but that\ncould provoke resistance by the part of of all of us that doesn't want\ndigital cash as much as it wants to make history.\n\nAll the best,\nAkiva\n\nP.S. Thanks for pointing out that I hit \"reply\" instead of \"reply all\"\nearlier ...\n\nOn Tue, Dec 8, 2015 at 11:45 AM, Bryan Bishop <kanzure at gmail.com> wrote:\n\n> On Tue, Dec 8, 2015 at 10:27 AM, Akiva Lichtner via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > and miners would have to round-robin through partitions\n>\n> At first glance this proposal seems most similar to the sharding proposals:\n>\n> http://diyhpl.us/wiki/transcripts/scalingbitcoin/sharding-the-blockchain/\n> https://github.com/vbuterin/scalability_paper/blob/master/scalability.pdf\n>\n> https://www.reddit.com/r/Bitcoin/comments/3u1m36/why_arent_we_as_a_community_talking_about/cxbamhn\n> http://eprint.iacr.org/2015/1168.pdf (committee approach)\n>\n> > but clients would have to send more than one message in order to spend\n> money\n>\n> Offloading work to the client for spends has in the past been a\n> well-received concept, such as the linearized coin history idea.\n>\n> - Bryan\n> http://heybryan.org/\n> 1 512 203 0507\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/fb4dadb7/attachment.html>"
            },
            {
                "author": "Patrick Strateman",
                "date": "2015-12-08T20:50:08",
                "message_text_only": "Payment recipients would need to operate a daemon for each chain, thus\nguaranteeing no scaling advantage.\n\n(There are other issues, but I believe that to be enough of a show\nstopper not to continue).\n\nOn 12/08/2015 08:27 AM, Akiva Lichtner via bitcoin-dev wrote:\n> Hello,\n>\n> I am seeking some expert feedback on an idea for scaling Bitcoin. As a\n> brief introduction: I work in the payment industry and I have twenty\n> years' experience in development. I have some experience with process\n> groups and ordering protocols too. I think I understand Satoshi's\n> paper but I admit I have not read the source code.\n>\n> The idea is to run more than one simultaneous chain, each chain\n> defeating double spending on only part of the coin. The coin would be\n> partitioned by radix (or modulus, not sure what to call it.) For\n> example in order to multiply throughput by a factor of ten you could\n> run ten parallel chains, one would work on coin that ends in \"0\", one\n> on coin that ends in \"1\", and so on up to \"9\".\n>\n> The number of chains could increase automatically over time based on\n> the moving average of transaction volume.\n>\n> Blocks would have to contain the number of the partition they belong\n> to, and miners would have to round-robin through partitions so that an\n> attacker would not have an unfair advantage working on just one partition.\n>\n> I don't think there is much impact to miners, but clients would have\n> to send more than one message in order to spend money. Client messages\n> will need to enumerate coin using some sort of compression, to save\n> space. This seems okay to me since often in computing client software\n> does have to break things up in equal parts (e.g. memory pages, file\n> system blocks,) and the client software could hide the details.\n>\n> Best wishes for continued success to the project.\n>\n> Regards,\n> Akiva\n>\n> P.S. I found a funny anagram for SATOSHI NAKAMOTO: \"NSA IS OOOK AT MATH\"\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/82e0a25d/attachment.html>"
            },
            {
                "author": "Akiva Lichtner",
                "date": "2015-12-08T21:23:12",
                "message_text_only": "It's true that miners would have to be prepared to work on any partition. I\ndon't see where the number affects defeating double spending, what matters\nis the nonce in the block that keeps the next successful miner random.\n\nI expect that the number of miners would be ten times larger as well, so an\nattacker would have no advantage working on one partition.\n\nOn Tue, Dec 8, 2015 at 3:50 PM, Patrick Strateman via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Payment recipients would need to operate a daemon for each chain, thus\n> guaranteeing no scaling advantage.\n>\n> (There are other issues, but I believe that to be enough of a show stopper\n> not to continue).\n>\n> On 12/08/2015 08:27 AM, Akiva Lichtner via bitcoin-dev wrote:\n>\n> Hello,\n>\n> I am seeking some expert feedback on an idea for scaling Bitcoin. As a\n> brief introduction: I work in the payment industry and I have twenty years'\n> experience in development. I have some experience with process groups and\n> ordering protocols too. I think I understand Satoshi's paper but I admit I\n> have not read the source code.\n>\n> The idea is to run more than one simultaneous chain, each chain defeating\n> double spending on only part of the coin. The coin would be partitioned by\n> radix (or modulus, not sure what to call it.) For example in order to\n> multiply throughput by a factor of ten you could run ten parallel chains,\n> one would work on coin that ends in \"0\", one on coin that ends in \"1\", and\n> so on up to \"9\".\n>\n> The number of chains could increase automatically over time based on the\n> moving average of transaction volume.\n>\n> Blocks would have to contain the number of the partition they belong to,\n> and miners would have to round-robin through partitions so that an attacker\n> would not have an unfair advantage working on just one partition.\n>\n> I don't think there is much impact to miners, but clients would have to\n> send more than one message in order to spend money. Client messages will\n> need to enumerate coin using some sort of compression, to save space. This\n> seems okay to me since often in computing client software does have to\n> break things up in equal parts (e.g. memory pages, file system blocks,) and\n> the client software could hide the details.\n>\n> Best wishes for continued success to the project.\n>\n> Regards,\n> Akiva\n>\n> P.S. I found a funny anagram for SATOSHI NAKAMOTO: \"NSA IS OOOK AT MATH\"\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing listbitcoin-dev at lists.linuxfoundation.orghttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/9de15dd2/attachment.html>"
            },
            {
                "author": "Patrick Strateman",
                "date": "2015-12-08T21:29:13",
                "message_text_only": "If partition is selected from a random key (the hash of the output for\nexample) then payment recipients would need to operate a full node on\neach of the chains.\n\nWhat's the point of partitioning if virtually everybody needs to operate\neach partition?\n\nThe mining aspect has it's own set of issues, but I'm not going to get\ninto those.\n\nOn 12/08/2015 01:23 PM, Akiva Lichtner wrote:\n> It's true that miners would have to be prepared to work on any\n> partition. I don't see where the number affects defeating double\n> spending, what matters is the nonce in the block that keeps the next\n> successful miner random.\n>\n> I expect that the number of miners would be ten times larger as well,\n> so an attacker would have no advantage working on one partition.\n>\n> On Tue, Dec 8, 2015 at 3:50 PM, Patrick Strateman via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org\n> <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n>\n>     Payment recipients would need to operate a daemon for each chain,\n>     thus guaranteeing no scaling advantage.\n>\n>     (There are other issues, but I believe that to be enough of a show\n>     stopper not to continue).\n>\n>     On 12/08/2015 08:27 AM, Akiva Lichtner via bitcoin-dev wrote:\n>>     Hello,\n>>\n>>     I am seeking some expert feedback on an idea for scaling Bitcoin.\n>>     As a brief introduction: I work in the payment industry and I\n>>     have twenty years' experience in development. I have some\n>>     experience with process groups and ordering protocols too. I\n>>     think I understand Satoshi's paper but I admit I have not read\n>>     the source code.\n>>\n>>     The idea is to run more than one simultaneous chain, each chain\n>>     defeating double spending on only part of the coin. The coin\n>>     would be partitioned by radix (or modulus, not sure what to call\n>>     it.) For example in order to multiply throughput by a factor of\n>>     ten you could run ten parallel chains, one would work on coin\n>>     that ends in \"0\", one on coin that ends in \"1\", and so on up to \"9\".\n>>\n>>     The number of chains could increase automatically over time based\n>>     on the moving average of transaction volume.\n>>\n>>     Blocks would have to contain the number of the partition they\n>>     belong to, and miners would have to round-robin through\n>>     partitions so that an attacker would not have an unfair advantage\n>>     working on just one partition.\n>>\n>>     I don't think there is much impact to miners, but clients would\n>>     have to send more than one message in order to spend money.\n>>     Client messages will need to enumerate coin using some sort of\n>>     compression, to save space. This seems okay to me since often in\n>>     computing client software does have to break things up in equal\n>>     parts (e.g. memory pages, file system blocks,) and the client\n>>     software could hide the details.\n>>\n>>     Best wishes for continued success to the project.\n>>\n>>     Regards,\n>>     Akiva\n>>\n>>     P.S. I found a funny anagram for SATOSHI NAKAMOTO: \"NSA IS OOOK\n>>     AT MATH\"\n>>\n>>\n>>\n>>     _______________________________________________\n>>     bitcoin-dev mailing list\n>>     bitcoin-dev at lists.linuxfoundation.org\n>>     <mailto:bitcoin-dev at lists.linuxfoundation.org>\n>>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n>     _______________________________________________\n>     bitcoin-dev mailing list\n>     bitcoin-dev at lists.linuxfoundation.org\n>     <mailto:bitcoin-dev at lists.linuxfoundation.org>\n>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/e19fe53b/attachment.html>"
            },
            {
                "author": "Akiva Lichtner",
                "date": "2015-12-08T21:41:07",
                "message_text_only": "If the system is modified to scale up that means the number of transactions\nis going up. That means the number of miners can also go up, and so will\nthe portion of malicious nodes. At least this seems reasonable. The problem\nwith partitions is that an attacker can focus on one partition. However\nbecause the number of miners also increases any attacks will fail as long\nas the miners are willing to work on any partition, which is easily\naccomplished by round-robin.\n\nSince there are N times more miners each miner still does the same amount\nof work. The system scales by partitioning the money supply and increasing\nthe number of miners.\n\n\n\nOn Tue, Dec 8, 2015 at 4:29 PM, Patrick Strateman via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> If partition is selected from a random key (the hash of the output for\n> example) then payment recipients would need to operate a full node on each\n> of the chains.\n>\n> What's the point of partitioning if virtually everybody needs to operate\n> each partition?\n>\n> The mining aspect has it's own set of issues, but I'm not going to get\n> into those.\n>\n> On 12/08/2015 01:23 PM, Akiva Lichtner wrote:\n>\n> It's true that miners would have to be prepared to work on any partition.\n> I don't see where the number affects defeating double spending, what\n> matters is the nonce in the block that keeps the next successful miner\n> random.\n>\n> I expect that the number of miners would be ten times larger as well, so\n> an attacker would have no advantage working on one partition.\n>\n> On Tue, Dec 8, 2015 at 3:50 PM, Patrick Strateman via bitcoin-dev <\n> <bitcoin-dev at lists.linuxfoundation.org>\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Payment recipients would need to operate a daemon for each chain, thus\n>> guaranteeing no scaling advantage.\n>>\n>> (There are other issues, but I believe that to be enough of a show\n>> stopper not to continue).\n>>\n>> On 12/08/2015 08:27 AM, Akiva Lichtner via bitcoin-dev wrote:\n>>\n>> Hello,\n>>\n>> I am seeking some expert feedback on an idea for scaling Bitcoin. As a\n>> brief introduction: I work in the payment industry and I have twenty years'\n>> experience in development. I have some experience with process groups and\n>> ordering protocols too. I think I understand Satoshi's paper but I admit I\n>> have not read the source code.\n>>\n>> The idea is to run more than one simultaneous chain, each chain defeating\n>> double spending on only part of the coin. The coin would be partitioned by\n>> radix (or modulus, not sure what to call it.) For example in order to\n>> multiply throughput by a factor of ten you could run ten parallel chains,\n>> one would work on coin that ends in \"0\", one on coin that ends in \"1\", and\n>> so on up to \"9\".\n>>\n>> The number of chains could increase automatically over time based on the\n>> moving average of transaction volume.\n>>\n>> Blocks would have to contain the number of the partition they belong to,\n>> and miners would have to round-robin through partitions so that an attacker\n>> would not have an unfair advantage working on just one partition.\n>>\n>> I don't think there is much impact to miners, but clients would have to\n>> send more than one message in order to spend money. Client messages will\n>> need to enumerate coin using some sort of compression, to save space. This\n>> seems okay to me since often in computing client software does have to\n>> break things up in equal parts (e.g. memory pages, file system blocks,) and\n>> the client software could hide the details.\n>>\n>> Best wishes for continued success to the project.\n>>\n>> Regards,\n>> Akiva\n>>\n>> P.S. I found a funny anagram for SATOSHI NAKAMOTO: \"NSA IS OOOK AT MATH\"\n>>\n>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing listbitcoin-dev at lists.linuxfoundation.orghttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/bbd37f43/attachment-0001.html>"
            },
            {
                "author": "Loi Luu",
                "date": "2015-12-09T06:30:24",
                "message_text_only": "Dear Akiva,\n\nIts Loi Luu, one of the authors of the SCP protocol (\nhttp://eprint.iacr.org/2015/1168.pdf ).\n\nBefore SCP, we had been thinking hard about how to do sharding efficiently\nwithout degrading any security guarantee. A simple solution which splits\nthe coins, or TXs in to several partitions will just not work. You have to\nanswer more questions to have a good solutions. For example, I wonder in\nyour proposal, if a transaction spends a \"coin\" that ends in \"1\" and\ncreates a new coin that ends in \"1\", which partition should process the\ntransaction? What is the prior data needed to validate that kind of TXs?\n\nThe problem with other proposals, and probably yours as well,  that we see\nis that the amount of data that you need to broadcast immediately to the\nnetwork increases linearly with the number of TXs that the network can\nprocess. Thus, sharding does not bring any advantage than simply using\nother techniques to publish more blocks in one epoch (like Bitcoin-NG,\nGhost). The whole point of using sharding/ partition is to localize\nthe bandwidth used, and only broadcast only a minimal data to the network.\n\nClearly we are able to localize the bandwidth used with our SCP protocol.\nThe cost is that now recipients need to  themselves verify whether a\ntransaction is double spending. However, we think that it is a reasonable\ntradeoff, given the potential scalability that SCP can provides.\n\nThanks,\nLoi Luu.\n\nOn Wed, Dec 9, 2015 at 12:27 AM, Akiva Lichtner via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hello,\n>\n> I am seeking some expert feedback on an idea for scaling Bitcoin. As a\n> brief introduction: I work in the payment industry and I have twenty years'\n> experience in development. I have some experience with process groups and\n> ordering protocols too. I think I understand Satoshi's paper but I admit I\n> have not read the source code.\n>\n> The idea is to run more than one simultaneous chain, each chain defeating\n> double spending on only part of the coin. The coin would be partitioned by\n> radix (or modulus, not sure what to call it.) For example in order to\n> multiply throughput by a factor of ten you could run ten parallel chains,\n> one would work on coin that ends in \"0\", one on coin that ends in \"1\", and\n> so on up to \"9\".\n>\n> The number of chains could increase automatically over time based on the\n> moving average of transaction volume.\n>\n> Blocks would have to contain the number of the partition they belong to,\n> and miners would have to round-robin through partitions so that an attacker\n> would not have an unfair advantage working on just one partition.\n>\n> I don't think there is much impact to miners, but clients would have to\n> send more than one message in order to spend money. Client messages will\n> need to enumerate coin using some sort of compression, to save space. This\n> seems okay to me since often in computing client software does have to\n> break things up in equal parts (e.g. memory pages, file system blocks,) and\n> the client software could hide the details.\n>\n> Best wishes for continued success to the project.\n>\n> Regards,\n> Akiva\n>\n> P.S. I found a funny anagram for SATOSHI NAKAMOTO: \"NSA IS OOOK AT MATH\"\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/65affaa5/attachment-0001.html>"
            },
            {
                "author": "Akiva Lichtner",
                "date": "2015-12-09T18:26:06",
                "message_text_only": "Thanks for giving serious consideration to my post.\n\nWith regard to your question \"if a transaction spends a \"coin\" that\nends in \"1\" and creates a new coin that ends in \"1\", which partition\nshould process the transaction?\", I would answer that only one\npartition is involved. In other words, there are N independent block\nchains that never cross paths.\n\nWith regard to your question \"what is the prior data needed to\nvalidate that kind of TXs?\" I do not understand what this means. If\nyou can dumb it down a bit that would be good because there could be\nsome interesting concern in this question.\n\nSince partitions are completely segregated, there is no need for a\nnode to work on multiple partitions simultaneously. For attacks to be\ndefeated a node needs to be able to work on multiple partitions in\nturn, not at the same time. The reason is because if the computing\npower of the good-faith nodes is unbalanced this gives attackers an\nunfair advantage.\n\nOn 12/9/15, Loi Luu via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Dear Akiva,\n>\n> Its Loi Luu, one of the authors of the SCP protocol (\n> http://eprint.iacr.org/2015/1168.pdf ).\n>\n> Before SCP, we had been thinking hard about how to do sharding efficiently\n> without degrading any security guarantee. A simple solution which splits\n> the coins, or TXs in to several partitions will just not work. You have to\n> answer more questions to have a good solutions. For example, I wonder in\n> your proposal, if a transaction spends a \"coin\" that ends in \"1\" and\n> creates a new coin that ends in \"1\", which partition should process the\n> transaction? What is the prior data needed to validate that kind of TXs?\n>\n> The problem with other proposals, and probably yours as well,  that we see\n> is that the amount of data that you need to broadcast immediately to the\n> network increases linearly with the number of TXs that the network can\n> process. Thus, sharding does not bring any advantage than simply using\n> other techniques to publish more blocks in one epoch (like Bitcoin-NG,\n> Ghost). The whole point of using sharding/ partition is to localize\n> the bandwidth used, and only broadcast only a minimal data to the network.\n>\n> Clearly we are able to localize the bandwidth used with our SCP protocol.\n> The cost is that now recipients need to  themselves verify whether a\n> transaction is double spending. However, we think that it is a reasonable\n> tradeoff, given the potential scalability that SCP can provides.\n>\n> Thanks,\n> Loi Luu.\n>\n> On Wed, Dec 9, 2015 at 12:27 AM, Akiva Lichtner via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Hello,\n>>\n>> I am seeking some expert feedback on an idea for scaling Bitcoin. As a\n>> brief introduction: I work in the payment industry and I have twenty\n>> years'\n>> experience in development. I have some experience with process groups and\n>> ordering protocols too. I think I understand Satoshi's paper but I admit\n>> I\n>> have not read the source code.\n>>\n>> The idea is to run more than one simultaneous chain, each chain defeating\n>> double spending on only part of the coin. The coin would be partitioned\n>> by\n>> radix (or modulus, not sure what to call it.) For example in order to\n>> multiply throughput by a factor of ten you could run ten parallel chains,\n>> one would work on coin that ends in \"0\", one on coin that ends in \"1\",\n>> and\n>> so on up to \"9\".\n>>\n>> The number of chains could increase automatically over time based on the\n>> moving average of transaction volume.\n>>\n>> Blocks would have to contain the number of the partition they belong to,\n>> and miners would have to round-robin through partitions so that an\n>> attacker\n>> would not have an unfair advantage working on just one partition.\n>>\n>> I don't think there is much impact to miners, but clients would have to\n>> send more than one message in order to spend money. Client messages will\n>> need to enumerate coin using some sort of compression, to save space.\n>> This\n>> seems okay to me since often in computing client software does have to\n>> break things up in equal parts (e.g. memory pages, file system blocks,)\n>> and\n>> the client software could hide the details.\n>>\n>> Best wishes for continued success to the project.\n>>\n>> Regards,\n>> Akiva\n>>\n>> P.S. I found a funny anagram for SATOSHI NAKAMOTO: \"NSA IS OOOK AT MATH\"\n>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>"
            },
            {
                "author": "Loi Luu",
                "date": "2015-12-09T21:16:00",
                "message_text_only": "I guess the most basic question is how do you define a coin here?\n\nThanks,\nLoi Luu\nOn 10 Dec 2015 2:26 a.m., \"Akiva Lichtner\" <akiva.lichtner at gmail.com> wrote:\n\n> Thanks for giving serious consideration to my post.\n>\n> With regard to your question \"if a transaction spends a \"coin\" that\n> ends in \"1\" and creates a new coin that ends in \"1\", which partition\n> should process the transaction?\", I would answer that only one\n> partition is involved. In other words, there are N independent block\n> chains that never cross paths.\n>\n> With regard to your question \"what is the prior data needed to\n> validate that kind of TXs?\" I do not understand what this means. If\n> you can dumb it down a bit that would be good because there could be\n> some interesting concern in this question.\n>\n> Since partitions are completely segregated, there is no need for a\n> node to work on multiple partitions simultaneously. For attacks to be\n> defeated a node needs to be able to work on multiple partitions in\n> turn, not at the same time. The reason is because if the computing\n> power of the good-faith nodes is unbalanced this gives attackers an\n> unfair advantage.\n>\n> On 12/9/15, Loi Luu via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > Dear Akiva,\n> >\n> > Its Loi Luu, one of the authors of the SCP protocol (\n> > http://eprint.iacr.org/2015/1168.pdf ).\n> >\n> > Before SCP, we had been thinking hard about how to do sharding\n> efficiently\n> > without degrading any security guarantee. A simple solution which splits\n> > the coins, or TXs in to several partitions will just not work. You have\n> to\n> > answer more questions to have a good solutions. For example, I wonder in\n> > your proposal, if a transaction spends a \"coin\" that ends in \"1\" and\n> > creates a new coin that ends in \"1\", which partition should process the\n> > transaction? What is the prior data needed to validate that kind of TXs?\n> >\n> > The problem with other proposals, and probably yours as well,  that we\n> see\n> > is that the amount of data that you need to broadcast immediately to the\n> > network increases linearly with the number of TXs that the network can\n> > process. Thus, sharding does not bring any advantage than simply using\n> > other techniques to publish more blocks in one epoch (like Bitcoin-NG,\n> > Ghost). The whole point of using sharding/ partition is to localize\n> > the bandwidth used, and only broadcast only a minimal data to the\n> network.\n> >\n> > Clearly we are able to localize the bandwidth used with our SCP protocol.\n> > The cost is that now recipients need to  themselves verify whether a\n> > transaction is double spending. However, we think that it is a reasonable\n> > tradeoff, given the potential scalability that SCP can provides.\n> >\n> > Thanks,\n> > Loi Luu.\n> >\n> > On Wed, Dec 9, 2015 at 12:27 AM, Akiva Lichtner via bitcoin-dev <\n> > bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >\n> >> Hello,\n> >>\n> >> I am seeking some expert feedback on an idea for scaling Bitcoin. As a\n> >> brief introduction: I work in the payment industry and I have twenty\n> >> years'\n> >> experience in development. I have some experience with process groups\n> and\n> >> ordering protocols too. I think I understand Satoshi's paper but I admit\n> >> I\n> >> have not read the source code.\n> >>\n> >> The idea is to run more than one simultaneous chain, each chain\n> defeating\n> >> double spending on only part of the coin. The coin would be partitioned\n> >> by\n> >> radix (or modulus, not sure what to call it.) For example in order to\n> >> multiply throughput by a factor of ten you could run ten parallel\n> chains,\n> >> one would work on coin that ends in \"0\", one on coin that ends in \"1\",\n> >> and\n> >> so on up to \"9\".\n> >>\n> >> The number of chains could increase automatically over time based on the\n> >> moving average of transaction volume.\n> >>\n> >> Blocks would have to contain the number of the partition they belong to,\n> >> and miners would have to round-robin through partitions so that an\n> >> attacker\n> >> would not have an unfair advantage working on just one partition.\n> >>\n> >> I don't think there is much impact to miners, but clients would have to\n> >> send more than one message in order to spend money. Client messages will\n> >> need to enumerate coin using some sort of compression, to save space.\n> >> This\n> >> seems okay to me since often in computing client software does have to\n> >> break things up in equal parts (e.g. memory pages, file system blocks,)\n> >> and\n> >> the client software could hide the details.\n> >>\n> >> Best wishes for continued success to the project.\n> >>\n> >> Regards,\n> >> Akiva\n> >>\n> >> P.S. I found a funny anagram for SATOSHI NAKAMOTO: \"NSA IS OOOK AT MATH\"\n> >>\n> >>\n> >> _______________________________________________\n> >> bitcoin-dev mailing list\n> >> bitcoin-dev at lists.linuxfoundation.org\n> >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> >>\n> >>\n> >\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151210/949b328b/attachment-0001.html>"
            },
            {
                "author": "Akiva Lichtner",
                "date": "2015-12-10T04:04:17",
                "message_text_only": "It's an interval (a,b) where a, b are between 0 and 21*10^6*10^8 .\n\nSomebody pointed out that this is not easily accomplished using the current\ncode because there are no coin ids.\n\n\nOn Wed, Dec 9, 2015 at 4:16 PM, Loi Luu <loi.luuthe at gmail.com> wrote:\n\n> I guess the most basic question is how do you define a coin here?\n>\n> Thanks,\n> Loi Luu\n> On 10 Dec 2015 2:26 a.m., \"Akiva Lichtner\" <akiva.lichtner at gmail.com>\n> wrote:\n>\n>> Thanks for giving serious consideration to my post.\n>>\n>> With regard to your question \"if a transaction spends a \"coin\" that\n>> ends in \"1\" and creates a new coin that ends in \"1\", which partition\n>> should process the transaction?\", I would answer that only one\n>> partition is involved. In other words, there are N independent block\n>> chains that never cross paths.\n>>\n>> With regard to your question \"what is the prior data needed to\n>> validate that kind of TXs?\" I do not understand what this means. If\n>> you can dumb it down a bit that would be good because there could be\n>> some interesting concern in this question.\n>>\n>> Since partitions are completely segregated, there is no need for a\n>> node to work on multiple partitions simultaneously. For attacks to be\n>> defeated a node needs to be able to work on multiple partitions in\n>> turn, not at the same time. The reason is because if the computing\n>> power of the good-faith nodes is unbalanced this gives attackers an\n>> unfair advantage.\n>>\n>> On 12/9/15, Loi Luu via bitcoin-dev\n>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> > Dear Akiva,\n>> >\n>> > Its Loi Luu, one of the authors of the SCP protocol (\n>> > http://eprint.iacr.org/2015/1168.pdf ).\n>> >\n>> > Before SCP, we had been thinking hard about how to do sharding\n>> efficiently\n>> > without degrading any security guarantee. A simple solution which splits\n>> > the coins, or TXs in to several partitions will just not work. You have\n>> to\n>> > answer more questions to have a good solutions. For example, I wonder in\n>> > your proposal, if a transaction spends a \"coin\" that ends in \"1\" and\n>> > creates a new coin that ends in \"1\", which partition should process the\n>> > transaction? What is the prior data needed to validate that kind of TXs?\n>> >\n>> > The problem with other proposals, and probably yours as well,  that we\n>> see\n>> > is that the amount of data that you need to broadcast immediately to the\n>> > network increases linearly with the number of TXs that the network can\n>> > process. Thus, sharding does not bring any advantage than simply using\n>> > other techniques to publish more blocks in one epoch (like Bitcoin-NG,\n>> > Ghost). The whole point of using sharding/ partition is to localize\n>> > the bandwidth used, and only broadcast only a minimal data to the\n>> network.\n>> >\n>> > Clearly we are able to localize the bandwidth used with our SCP\n>> protocol.\n>> > The cost is that now recipients need to  themselves verify whether a\n>> > transaction is double spending. However, we think that it is a\n>> reasonable\n>> > tradeoff, given the potential scalability that SCP can provides.\n>> >\n>> > Thanks,\n>> > Loi Luu.\n>> >\n>> > On Wed, Dec 9, 2015 at 12:27 AM, Akiva Lichtner via bitcoin-dev <\n>> > bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> >\n>> >> Hello,\n>> >>\n>> >> I am seeking some expert feedback on an idea for scaling Bitcoin. As a\n>> >> brief introduction: I work in the payment industry and I have twenty\n>> >> years'\n>> >> experience in development. I have some experience with process groups\n>> and\n>> >> ordering protocols too. I think I understand Satoshi's paper but I\n>> admit\n>> >> I\n>> >> have not read the source code.\n>> >>\n>> >> The idea is to run more than one simultaneous chain, each chain\n>> defeating\n>> >> double spending on only part of the coin. The coin would be partitioned\n>> >> by\n>> >> radix (or modulus, not sure what to call it.) For example in order to\n>> >> multiply throughput by a factor of ten you could run ten parallel\n>> chains,\n>> >> one would work on coin that ends in \"0\", one on coin that ends in \"1\",\n>> >> and\n>> >> so on up to \"9\".\n>> >>\n>> >> The number of chains could increase automatically over time based on\n>> the\n>> >> moving average of transaction volume.\n>> >>\n>> >> Blocks would have to contain the number of the partition they belong\n>> to,\n>> >> and miners would have to round-robin through partitions so that an\n>> >> attacker\n>> >> would not have an unfair advantage working on just one partition.\n>> >>\n>> >> I don't think there is much impact to miners, but clients would have to\n>> >> send more than one message in order to spend money. Client messages\n>> will\n>> >> need to enumerate coin using some sort of compression, to save space.\n>> >> This\n>> >> seems okay to me since often in computing client software does have to\n>> >> break things up in equal parts (e.g. memory pages, file system blocks,)\n>> >> and\n>> >> the client software could hide the details.\n>> >>\n>> >> Best wishes for continued success to the project.\n>> >>\n>> >> Regards,\n>> >> Akiva\n>> >>\n>> >> P.S. I found a funny anagram for SATOSHI NAKAMOTO: \"NSA IS OOOK AT\n>> MATH\"\n>> >>\n>> >>\n>> >> _______________________________________________\n>> >> bitcoin-dev mailing list\n>> >> bitcoin-dev at lists.linuxfoundation.org\n>> >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>> >>\n>> >>\n>> >\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/8f725842/attachment.html>"
            },
            {
                "author": "Andrew",
                "date": "2015-12-09T22:35:07",
                "message_text_only": "Hi Akiva\n\nI sketched out a similar proposal here:\nhttps://bitcointalk.org/index.php?topic=1083345.0\n\nIt's good to see people talking about this :). I'm not quite convinced with\nsegregated witness, as it might mess up some things, but will take a closer\nlook.\nOn Dec 9, 2015 7:32 AM, \"Loi Luu via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Dear Akiva,\n>\n> Its Loi Luu, one of the authors of the SCP protocol (\n> http://eprint.iacr.org/2015/1168.pdf ).\n>\n> Before SCP, we had been thinking hard about how to do sharding efficiently\n> without degrading any security guarantee. A simple solution which splits\n> the coins, or TXs in to several partitions will just not work. You have to\n> answer more questions to have a good solutions. For example, I wonder in\n> your proposal, if a transaction spends a \"coin\" that ends in \"1\" and\n> creates a new coin that ends in \"1\", which partition should process the\n> transaction? What is the prior data needed to validate that kind of TXs?\n>\n> The problem with other proposals, and probably yours as well,  that we see\n> is that the amount of data that you need to broadcast immediately to the\n> network increases linearly with the number of TXs that the network can\n> process. Thus, sharding does not bring any advantage than simply using\n> other techniques to publish more blocks in one epoch (like Bitcoin-NG,\n> Ghost). The whole point of using sharding/ partition is to localize\n> the bandwidth used, and only broadcast only a minimal data to the network.\n>\n> Clearly we are able to localize the bandwidth used with our SCP protocol.\n> The cost is that now recipients need to  themselves verify whether a\n> transaction is double spending. However, we think that it is a reasonable\n> tradeoff, given the potential scalability that SCP can provides.\n>\n> Thanks,\n> Loi Luu.\n>\n> On Wed, Dec 9, 2015 at 12:27 AM, Akiva Lichtner via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Hello,\n>>\n>> I am seeking some expert feedback on an idea for scaling Bitcoin. As a\n>> brief introduction: I work in the payment industry and I have twenty years'\n>> experience in development. I have some experience with process groups and\n>> ordering protocols too. I think I understand Satoshi's paper but I admit I\n>> have not read the source code.\n>>\n>> The idea is to run more than one simultaneous chain, each chain defeating\n>> double spending on only part of the coin. The coin would be partitioned by\n>> radix (or modulus, not sure what to call it.) For example in order to\n>> multiply throughput by a factor of ten you could run ten parallel chains,\n>> one would work on coin that ends in \"0\", one on coin that ends in \"1\", and\n>> so on up to \"9\".\n>>\n>> The number of chains could increase automatically over time based on the\n>> moving average of transaction volume.\n>>\n>> Blocks would have to contain the number of the partition they belong to,\n>> and miners would have to round-robin through partitions so that an attacker\n>> would not have an unfair advantage working on just one partition.\n>>\n>> I don't think there is much impact to miners, but clients would have to\n>> send more than one message in order to spend money. Client messages will\n>> need to enumerate coin using some sort of compression, to save space. This\n>> seems okay to me since often in computing client software does have to\n>> break things up in equal parts (e.g. memory pages, file system blocks,) and\n>> the client software could hide the details.\n>>\n>> Best wishes for continued success to the project.\n>>\n>> Regards,\n>> Akiva\n>>\n>> P.S. I found a funny anagram for SATOSHI NAKAMOTO: \"NSA IS OOOK AT MATH\"\n>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/19e592c7/attachment.html>"
            },
            {
                "author": "Akiva Lichtner",
                "date": "2015-12-10T03:58:10",
                "message_text_only": "Hi Andrew,\n\nWhat you suggested is much more sophisticated than what I suggested. I am\nstrictly talking about independent chains - that's all.\n\nI am struck by the fact that the topic of \"scaling bitcoin\" seems to be a\nmix of different problems, and when people are really trying to solve\ndifferent problems, or arguing about applying the same solution in\ndifferent settings, it's easy to argue back and forth endlessly. Your post\ntalks about validating transactions without seeing all transactions. This\nis a different problem than what I am addressing. My view of Bitcoin is\ncolored by my experience with process groups and total ordering. I view\nBitcoin as a timestamp service on all transactions, a total order. A total\norder is difficult to scale. Period.\n\nI am just addressing how to change the system so that blocks can be\ngenerated faster if a) the transaction volume increases and b) you are\nwilling to have more miners. But you are also talking about transaction\nverification and making sure that you don't need to verify everything. I\nthink these are two problems that should have two different names.\n\nCorrect me if I am wrong, but the dream of a virtual currency where\neverybody is equal and runs the client on their mobile device went out the\nwindow long ago. I think that went out with the special mining hardware. If\nmy organization had to accept bitcoin payments I would assume that we'll\nneed a small server farm for transaction verification, and that we would\nsee all the transactions. Figure 10,000 transactions per second, like VISA.\nAs far as small organizations or private individuals are concerned, I think\nit would be entirely okay for a guy on a smartphone to delegate\nverification to a trusted party, as long as the trust chain stops there and\nthere is plenty of choice.\n\nI am guessing the trustless virtual currency police would get pretty upset\nabout such a pragmatic approach, but it's not really a choice, the failure\nto scale has already occurred. All things considered I think that Bitcoin\nwill only scale when pragmatic considerations take center stage and the\nacademic goals take a lower priority. I would think companies would make a\ngood living out of running trusted verification services.\n\nOnce again, it doesn't mean that there is a bank, the network still allows\nmalicious nodes, but there can be pockets of trust. This is only natural,\nmost people trust at least one other person, so it's not that weird.\n\nAkiva\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/0844f5e2/attachment-0001.html>"
            },
            {
                "author": "Bryan Bishop",
                "date": "2015-12-10T04:31:42",
                "message_text_only": "On Wed, Dec 9, 2015 at 9:58 PM, Akiva Lichtner wrote:\n> Correct me if I am wrong, but the dream of a virtual currency where\n> everybody is equal and runs the client on their mobile device went out the\n> window long ago. I think that went out with the special mining hardware. If\n\nMining equipment isn't for transaction verification. The mining\nequipment is used to work on Proof-of-Work. Thanks.\n\n> my organization had to accept bitcoin payments I would assume that we'll\n> need a small server farm for transaction verification, and that we would see\n\nUnfortunately Bitcoin does not work like those centralized systems; it\nshould not be surprising that a system focused so much on\ndecentralized and independent verification would have developers\nworking on so many non-bandwidth scaling solutions. These other\nproposals seek to preserve existing properties of Bitcoin (such as\ncheap verification, low-bandwidth) while also increasing the amount of\nactivity that can enjoy the decentralized fruits of Proof-of-Work\nlabor. But not helpful to assume this can only look like Visa or any\ndatabase on a cluster etc...\n\n> would be entirely okay for a guy on a smartphone to delegate verification to\n> a trusted party, as long as the trust chain stops there and there is plenty\n> of choice.\n\nI don't suppose I could tempt you with probabilistically checkable\nproofs, could I? These verify in milliseconds, grow sublinear in size\nof the total data, but have no near-term proposal available yet.\n\n> I am guessing the trustless virtual currency police would get pretty upset\n> about such a pragmatic approach, but it's not really a choice, the failure\n> to scale has already occurred. All things considered I think that Bitcoin\n\nPerhaps instead of failure-to-scale you mean \"failure to apply\ntraditional scaling has already failed\", which shouldn't be so\nsurprising given the different security model that Bitcoin operates\non.\n\n> most people trust at least one other person, so it's not that weird.\n\nsee the following recent text,\n\"\"\"\nBitcoin is P2P electronic cash that is valuable over legacy systems\nbecause of the monetary autonomy it brings to its users through\ndecentralization. Bitcoin seeks to address the root problem with\nconventional currency: all the trust that's required to make it work--\n\n-- Not that justified trust is a bad thing, but trust makes systems\nbrittle, opaque, and costly to operate. Trust failures result in systemic\ncollapses, trust curation creates inequality and monopoly lock-in, and\nnaturally arising trust choke-points can be abused to deny access to\ndue process. Through the use of cryptographic proof and decentralized\nnetworks Bitcoin minimizes and replaces these trust costs.\n\nWith the available technology, there are fundamental trade-offs between\nscale and decentralization. If the system is too costly people will be\nforced to trust third parties rather than independently enforcing the\nsystem's rules. If the Bitcoin blockchain\u2019s resource usage, relative\nto the available technology, is too great, Bitcoin loses its competitive\nadvantages compared to legacy systems because validation will be too\ncostly (pricing out many users), forcing trust back into the system.\nIf capacity is too low and our methods of transacting too inefficient,\naccess to the chain for dispute resolution will be too costly, again\npushing trust back into the system.\n\"\"\"\n\n- Bryan\nhttp://heybryan.org/\n1 512 203 0507"
            },
            {
                "author": "Dave Scotese",
                "date": "2015-12-10T04:08:04",
                "message_text_only": "If we partition the work using bits from the TxID (once it is no longer\nmalleable) or even bits from whatever definition we use for \"coin,\" then\nevery transaction may have to use all the other partitions to verify that\nthe incoming coin is good.\n\nIf all partitions are involved in validating and storing every transaction,\nthen we may be doing more work in total, but any one node will only have to\ndo (and store) a fraction of what it is now.  We would want the current\nsituation to be identical to one in which all the participants are handling\nall the partitions.  So how can we break up the work so that any\nparticipant can handle whatever fraction of the work he or she wants?  One\nidea is to use the last bits of the address that will receive the subsidy\nand fees.  You solve the block for your partition by determining that all\ntransactions in the block are valid against the subset of blocks whose\nhashes end with the same bits.\n\nThis solution is broadcast in the hope that others will start attempting to\nvalidate that same block on their own partition. If they are mining the\nsame partition, they simply change their subsidy address to work on a\ndifferent partition.  Each time a new-but-not-last partition is solved,\neveryone working on the block adds the new solver's output address to their\ngeneration transaction with the appropriate fraction of the\nreward-plus-subsidy.  In this way, several miners contribute to the\nsolution of a single block and need only store those blocks that match the\npartitions they want to work on.\n\nSuppose we use eight bits so that there are 256 partitions and a miner\nwishes to do about 1/5 of the work. That would be 51 partitions.  This is\nsignaled in the generation transaction, where the bit-pattern of the last\nbyte of the public key identifies the first partition, and the proportion\nof the total reward for the block (51/256) indicates how many partitions a\nsolution will cover.\n\nSuppose that the last byte of the subsidy address is 0xF0.  This means\nthere are only 16 partitions left, so we define partition selection to wrap\naround.  This 51/256 miner must cover partitions 0xF0 - 0xFF and 0x00 -\n0x23. In this way, all mining to date has covered all partitions.\n\nThe number of bits to be used might be able to be abstracted out to a\ncertain level.  Perhaps a miner can indicate how many bits B the\npartitioning should use in the CoinBase. The blocks for which a partition\nminer claims responsibility are all those with a bit pattern of length B at\nthe end of their hash matching the the bits at the end of the first\noutput's public key in the generation transaction, as well as those blocks\nwith hashes for which the last B bits match any of the next N bit patterns\nwhere for the largest integer N for which the claimed output is not less\nthan (subsidy+fees)*(N/(2^B)).\n\nIf you only store and validate against one partition, and that partition\nhas a solution already, then you would start working on the next block\n(once you've validated the current one against your subset of the\nblockchain).  You could even broadcast a solution for that next block\nbefore the previous block is fully solved, thus claiming a piece of the\nnext block reward (assuming the current block is valid on all partitions).\n\nIt seems that a miner who covers only one partition will be at a serious\ndisadvantage, but as the rate of incoming transactions increases, the\nfraction of time he must spend validating (being about half of all other\nminers who cover just one more partition) makes up for this disadvantage\nsomewhat.  He is a \"spry\" miner and therefore wins more rewards during\ntimes of very dense transaction volume.  If we wish to encourage miners to\nwork on smaller partitions, we can provide a difficulty break for smaller\nfractions of the work.  In fact, the difficulty can be adjusted down for\nthe first solution, and then slowly back up to full for the last\npartition(s).\n\nThis proposal has the added benefit of encouraging the assembly of blocks\nby miners who work on single partitions to get them out there with a\none-partition solution.\n\nOn Wed, Dec 9, 2015 at 2:35 PM, Andrew via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi Akiva\n>\n> I sketched out a similar proposal here:\n> https://bitcointalk.org/index.php?topic=1083345.0\n>\n> It's good to see people talking about this :). I'm not quite convinced\n> with segregated witness, as it might mess up some things, but will take a\n> closer look.\n> On Dec 9, 2015 7:32 AM, \"Loi Luu via bitcoin-dev\" <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Dear Akiva,\n>>\n>> Its Loi Luu, one of the authors of the SCP protocol (\n>> http://eprint.iacr.org/2015/1168.pdf ).\n>>\n>> Before SCP, we had been thinking hard about how to do sharding\n>> efficiently without degrading any security guarantee. A simple solution\n>> which splits the coins, or TXs in to several partitions will just not work.\n>> You have to answer more questions to have a good solutions. For example, I\n>> wonder in your proposal, if a transaction spends a \"coin\" that ends in \"1\"\n>> and creates a new coin that ends in \"1\", which partition should process the\n>> transaction? What is the prior data needed to validate that kind of TXs?\n>>\n>> The problem with other proposals, and probably yours as well,  that we\n>> see is that the amount of data that you need to broadcast immediately to\n>> the network increases linearly with the number of TXs that the network can\n>> process. Thus, sharding does not bring any advantage than simply using\n>> other techniques to publish more blocks in one epoch (like Bitcoin-NG,\n>> Ghost). The whole point of using sharding/ partition is to localize\n>> the bandwidth used, and only broadcast only a minimal data to the network.\n>>\n>> Clearly we are able to localize the bandwidth used with our SCP protocol.\n>> The cost is that now recipients need to  themselves verify whether a\n>> transaction is double spending. However, we think that it is a reasonable\n>> tradeoff, given the potential scalability that SCP can provides.\n>>\n>> Thanks,\n>> Loi Luu.\n>>\n>> On Wed, Dec 9, 2015 at 12:27 AM, Akiva Lichtner via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> Hello,\n>>>\n>>> I am seeking some expert feedback on an idea for scaling Bitcoin. As a\n>>> brief introduction: I work in the payment industry and I have twenty years'\n>>> experience in development. I have some experience with process groups and\n>>> ordering protocols too. I think I understand Satoshi's paper but I admit I\n>>> have not read the source code.\n>>>\n>>> The idea is to run more than one simultaneous chain, each chain\n>>> defeating double spending on only part of the coin. The coin would be\n>>> partitioned by radix (or modulus, not sure what to call it.) For example in\n>>> order to multiply throughput by a factor of ten you could run ten parallel\n>>> chains, one would work on coin that ends in \"0\", one on coin that ends in\n>>> \"1\", and so on up to \"9\".\n>>>\n>>> The number of chains could increase automatically over time based on the\n>>> moving average of transaction volume.\n>>>\n>>> Blocks would have to contain the number of the partition they belong to,\n>>> and miners would have to round-robin through partitions so that an attacker\n>>> would not have an unfair advantage working on just one partition.\n>>>\n>>> I don't think there is much impact to miners, but clients would have to\n>>> send more than one message in order to spend money. Client messages will\n>>> need to enumerate coin using some sort of compression, to save space. This\n>>> seems okay to me since often in computing client software does have to\n>>> break things up in equal parts (e.g. memory pages, file system blocks,) and\n>>> the client software could hide the details.\n>>>\n>>> Best wishes for continued success to the project.\n>>>\n>>> Regards,\n>>> Akiva\n>>>\n>>> P.S. I found a funny anagram for SATOSHI NAKAMOTO: \"NSA IS OOOK AT MATH\"\n>>>\n>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n\n\n-- \nI like to provide some work at no charge to prove my value. Do you need a\ntechie?\nI own Litmocracy <http://www.litmocracy.com> and Meme Racing\n<http://www.memeracing.net> (in alpha).\nI'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which\nnow accepts Bitcoin.\nI also code for The Dollar Vigilante <http://dollarvigilante.com/>.\n\"He ought to find it more profitable to play by the rules\" - Satoshi\nNakamoto\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/7030e17e/attachment-0001.html>"
            },
            {
                "author": "Dave Scotese",
                "date": "2015-12-10T04:14:17",
                "message_text_only": "Edit:\n\"... as well as those blocks with hashes for which the last B bits match\nany of the next N bit patterns where *N is largest* integer for which the\nclaimed output is not *greater* than (subsidy+fees)*(N/(2^B)).\n\nOn Wed, Dec 9, 2015 at 8:08 PM, Dave Scotese <dscotese at litmocracy.com>\nwrote:\n\n> If we partition the work using bits from the TxID (once it is no longer\n> malleable) or even bits from whatever definition we use for \"coin,\" then\n> every transaction may have to use all the other partitions to verify that\n> the incoming coin is good.\n>\n> If all partitions are involved in validating and storing every\n> transaction, then we may be doing more work in total, but any one node will\n> only have to do (and store) a fraction of what it is now.  We would want\n> the current situation to be identical to one in which all the participants\n> are handling all the partitions.  So how can we break up the work so that\n> any participant can handle whatever fraction of the work he or she wants?\n> One idea is to use the last bits of the address that will receive the\n> subsidy and fees.  You solve the block for your partition by determining\n> that all transactions in the block are valid against the subset of blocks\n> whose hashes end with the same bits.\n>\n> This solution is broadcast in the hope that others will start attempting\n> to validate that same block on their own partition. If they are mining the\n> same partition, they simply change their subsidy address to work on a\n> different partition.  Each time a new-but-not-last partition is solved,\n> everyone working on the block adds the new solver's output address to their\n> generation transaction with the appropriate fraction of the\n> reward-plus-subsidy.  In this way, several miners contribute to the\n> solution of a single block and need only store those blocks that match the\n> partitions they want to work on.\n>\n> Suppose we use eight bits so that there are 256 partitions and a miner\n> wishes to do about 1/5 of the work. That would be 51 partitions.  This is\n> signaled in the generation transaction, where the bit-pattern of the last\n> byte of the public key identifies the first partition, and the proportion\n> of the total reward for the block (51/256) indicates how many partitions a\n> solution will cover.\n>\n> Suppose that the last byte of the subsidy address is 0xF0.  This means\n> there are only 16 partitions left, so we define partition selection to wrap\n> around.  This 51/256 miner must cover partitions 0xF0 - 0xFF and 0x00 -\n> 0x23. In this way, all mining to date has covered all partitions.\n>\n> The number of bits to be used might be able to be abstracted out to a\n> certain level.  Perhaps a miner can indicate how many bits B the\n> partitioning should use in the CoinBase. The blocks for which a partition\n> miner claims responsibility are all those with a bit pattern of length B at\n> the end of their hash matching the the bits at the end of the first\n> output's public key in the generation transaction, as well as those blocks\n> with hashes for which the last B bits match any of the next N bit patterns\n> where for the largest integer N for which the claimed output is not less\n> than (subsidy+fees)*(N/(2^B)).\n>\n> If you only store and validate against one partition, and that partition\n> has a solution already, then you would start working on the next block\n> (once you've validated the current one against your subset of the\n> blockchain).  You could even broadcast a solution for that next block\n> before the previous block is fully solved, thus claiming a piece of the\n> next block reward (assuming the current block is valid on all partitions).\n>\n> It seems that a miner who covers only one partition will be at a serious\n> disadvantage, but as the rate of incoming transactions increases, the\n> fraction of time he must spend validating (being about half of all other\n> miners who cover just one more partition) makes up for this disadvantage\n> somewhat.  He is a \"spry\" miner and therefore wins more rewards during\n> times of very dense transaction volume.  If we wish to encourage miners to\n> work on smaller partitions, we can provide a difficulty break for smaller\n> fractions of the work.  In fact, the difficulty can be adjusted down for\n> the first solution, and then slowly back up to full for the last\n> partition(s).\n>\n> This proposal has the added benefit of encouraging the assembly of blocks\n> by miners who work on single partitions to get them out there with a\n> one-partition solution.\n>\n> On Wed, Dec 9, 2015 at 2:35 PM, Andrew via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Hi Akiva\n>>\n>> I sketched out a similar proposal here:\n>> https://bitcointalk.org/index.php?topic=1083345.0\n>>\n>> It's good to see people talking about this :). I'm not quite convinced\n>> with segregated witness, as it might mess up some things, but will take a\n>> closer look.\n>> On Dec 9, 2015 7:32 AM, \"Loi Luu via bitcoin-dev\" <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> Dear Akiva,\n>>>\n>>> Its Loi Luu, one of the authors of the SCP protocol (\n>>> http://eprint.iacr.org/2015/1168.pdf ).\n>>>\n>>> Before SCP, we had been thinking hard about how to do sharding\n>>> efficiently without degrading any security guarantee. A simple solution\n>>> which splits the coins, or TXs in to several partitions will just not work.\n>>> You have to answer more questions to have a good solutions. For example, I\n>>> wonder in your proposal, if a transaction spends a \"coin\" that ends in \"1\"\n>>> and creates a new coin that ends in \"1\", which partition should process the\n>>> transaction? What is the prior data needed to validate that kind of TXs?\n>>>\n>>> The problem with other proposals, and probably yours as well,  that we\n>>> see is that the amount of data that you need to broadcast immediately to\n>>> the network increases linearly with the number of TXs that the network can\n>>> process. Thus, sharding does not bring any advantage than simply using\n>>> other techniques to publish more blocks in one epoch (like Bitcoin-NG,\n>>> Ghost). The whole point of using sharding/ partition is to localize\n>>> the bandwidth used, and only broadcast only a minimal data to the network.\n>>>\n>>> Clearly we are able to localize the bandwidth used with our SCP\n>>> protocol. The cost is that now recipients need to  themselves verify\n>>> whether a transaction is double spending. However, we think that it is a\n>>> reasonable tradeoff, given the potential scalability that SCP can provides.\n>>>\n>>> Thanks,\n>>> Loi Luu.\n>>>\n>>> On Wed, Dec 9, 2015 at 12:27 AM, Akiva Lichtner via bitcoin-dev <\n>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>>> Hello,\n>>>>\n>>>> I am seeking some expert feedback on an idea for scaling Bitcoin. As a\n>>>> brief introduction: I work in the payment industry and I have twenty years'\n>>>> experience in development. I have some experience with process groups and\n>>>> ordering protocols too. I think I understand Satoshi's paper but I admit I\n>>>> have not read the source code.\n>>>>\n>>>> The idea is to run more than one simultaneous chain, each chain\n>>>> defeating double spending on only part of the coin. The coin would be\n>>>> partitioned by radix (or modulus, not sure what to call it.) For example in\n>>>> order to multiply throughput by a factor of ten you could run ten parallel\n>>>> chains, one would work on coin that ends in \"0\", one on coin that ends in\n>>>> \"1\", and so on up to \"9\".\n>>>>\n>>>> The number of chains could increase automatically over time based on\n>>>> the moving average of transaction volume.\n>>>>\n>>>> Blocks would have to contain the number of the partition they belong\n>>>> to, and miners would have to round-robin through partitions so that an\n>>>> attacker would not have an unfair advantage working on just one partition.\n>>>>\n>>>> I don't think there is much impact to miners, but clients would have to\n>>>> send more than one message in order to spend money. Client messages will\n>>>> need to enumerate coin using some sort of compression, to save space. This\n>>>> seems okay to me since often in computing client software does have to\n>>>> break things up in equal parts (e.g. memory pages, file system blocks,) and\n>>>> the client software could hide the details.\n>>>>\n>>>> Best wishes for continued success to the project.\n>>>>\n>>>> Regards,\n>>>> Akiva\n>>>>\n>>>> P.S. I found a funny anagram for SATOSHI NAKAMOTO: \"NSA IS OOOK AT MATH\"\n>>>>\n>>>>\n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>\n>>>>\n>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n>\n> --\n> I like to provide some work at no charge to prove my value. Do you need a\n> techie?\n> I own Litmocracy <http://www.litmocracy.com> and Meme Racing\n> <http://www.memeracing.net> (in alpha).\n> I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com>\n> which now accepts Bitcoin.\n> I also code for The Dollar Vigilante <http://dollarvigilante.com/>.\n> \"He ought to find it more profitable to play by the rules\" - Satoshi\n> Nakamoto\n>\n\n\n\n-- \nI like to provide some work at no charge to prove my value. Do you need a\ntechie?\nI own Litmocracy <http://www.litmocracy.com> and Meme Racing\n<http://www.memeracing.net> (in alpha).\nI'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which\nnow accepts Bitcoin.\nI also code for The Dollar Vigilante <http://dollarvigilante.com/>.\n\"He ought to find it more profitable to play by the rules\" - Satoshi\nNakamoto\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/45701a43/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Scaling by Partitioning",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Bryan Bishop",
                "Andrew",
                "Patrick Strateman",
                "Dave Scotese",
                "Akiva Lichtner",
                "Loi Luu"
            ],
            "messages_count": 16,
            "total_messages_chars_count": 66485
        }
    },
    {
        "title": "[bitcoin-dev] Impacts of Segregated Witness softfork",
        "thread_messages": [
            {
                "author": "jl2012 at xbt.hk",
                "date": "2015-12-09T14:30:23",
                "message_text_only": "Although the plan is to implement SW with softfork, I think many \nimportant (but non-consensus critical) components of the network would \nbe broken and many things have to be redefined.\n\n1. Definition of \"Transaction ID\". Currently, \"Transaction ID\" is simply \na hash of a tx. With SW, we may need to deal with 2 or 3 IDs for each \ntx. Firstly we have the \"backward-compatible txid\" (bctxid), which has \nexactly the same meaning of the original txid. We also have a \"witness \nID\" (wid), which is the hash of the witness. And finally we may need a \n\"global txid\" (gtxid), which is a hash of bctxid|wid. A gtxid is needed \nmainly for the relay of txs between full nodes. bctxid and wid are \nconsensus critical while gtxid is for relay network only.\n\n2. IBLT / Bitcoin relay network: As the \"backward-compatible txid\" \ndefines only part of a tx, any relay protocols between full nodes have \nto use the \"global txid\" to identify a tx. Malleability attack targeting \nrelay network is still possible as the witness is malleable.\n\n3. getblocktemplete has to be upgraded to deal with witness data and \nwitness IDs. (Stratum seems to be not affected? I'm not sure)\n\n4. Protocols relying on the coinbase tx (e.g. P2Pool, merged mining): \ndepends on the location of witness commitment, these protocols may be \nbroken.\n\nFeel free to correct me and add more to the list."
            }
        ],
        "thread_summary": {
            "title": "Impacts of Segregated Witness softfork",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "jl2012 at xbt.hk"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1357
        }
    },
    {
        "title": "[bitcoin-dev] \"Subsidy fraud\" ?",
        "thread_messages": [
            {
                "author": "xor",
                "date": "2015-12-09T21:26:05",
                "message_text_only": "Pieter Wuille mentions \"subsidy fraud\" in his recent talk:\nhttps://youtu.be/fst1IK_mrng?t=57m2s\n\nI was unable to google what this is, and the Bitcoin Wiki also does not seem \nto explain it.\n\nIf this is a well-known problem, perhaps it would be a good idea to explain it \nsomewhere?\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 836 bytes\nDesc: This is a digitally signed message part.\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/26a29606/attachment.sig>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-12-09T21:43:22",
                "message_text_only": "I meant a miner claiming more in the coinbase's output than subsidy + fees\nallow.\nOn Dec 10, 2015 5:26 AM, \"xor\" <xor at freenetproject.org> wrote:\n\n> Pieter Wuille mentions \"subsidy fraud\" in his recent talk:\n> https://youtu.be/fst1IK_mrng?t=57m2s\n>\n> I was unable to google what this is, and the Bitcoin Wiki also does not\n> seem\n> to explain it.\n>\n> If this is a well-known problem, perhaps it would be a good idea to\n> explain it\n> somewhere?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/1ba8a058/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "\"Subsidy fraud\" ?",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Pieter Wuille",
                "xor"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 1210
        }
    },
    {
        "title": "[bitcoin-dev] Standard BIP Draft: Turing Pseudo-Completeness",
        "thread_messages": [
            {
                "author": "Luke Durback",
                "date": "2015-12-10T01:35:17",
                "message_text_only": "Hello Bitcoin-Dev,\n\nI hope this isn't out of line, but I joined the mailing list to try to\nstart a discussion on adding opcodes to make Script Turing Pseudo-Complete\nas Wright suggested is possible.\n\n---\n\nIn line with Wright's suggestion, I propose adding a return stack alongside\nthe, already existing, control stack.\n\nThe principle opcodes (excluding conditional versions of call and\nreturn_from) needed are\n\nOP_DEFINITION_START FunctionName:  The code that follows is the definition\nof a new function to be named TransactionSenderAddress.FunctionName.  If\nthis function name is already taken, the transaction is marked invalid.\nWithin the transaction, the function can be called simply as FunctionName.\n\nOP_DEFINITION_END:  This ends a function definition\n\nOP_FUNCTION_NAME FunctionName:  Gives the current transaction the name\nFunctionName (this is necessary to build recursive functions)\n\n---\n\nOP_CALL Namespace.FunctionName Value TransactionFee:  This marks the\ntransaction as valid.  It also pushes the current execution location onto\nthe return stack, debits the calling transaction by the TransactionFee and\nValue, and creates a new transaction specified by Namespace.FunctionName\nwith both stacks continued from before (this may be dangerous, but I see no\nway around it) with the specified value.\n\nOP_RETURN_FROM_CALL_AND_CONTINUE:  This pops the top value off the return\nstack and continues from the specified location with both stacks in tact.\n\n---\n\nIt would also be useful if a transaction can create another transaction\narbitrarily, so to prepare for that, I additionally propose\n\nOP_NAMESPACE:  Pushes the current namespace onto the control stack\n\nThis, combined with the ability to make new transactions arbitrarily would\nallow a function to pay its creator.\n\n\n\nI understand that this isn't all that is needed, but I think it's a start.\nI hope this proposal has met you all well,\n\nLuke Durback\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/a880be67/attachment.html>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-12-10T04:03:30",
                "message_text_only": "There is no need for a BIP draft.  \"Turing complete\" is just a fancy,\nexecutive-impressing term for \"it can run any computer program\", or put\neven more simply, \"it can loop\"\n\nFurthermore, the specification of such a language is trivial.  It is the\neconomics of validation that is the complex piece.  Proving whether or not\na program will halt as expected - The Halting Problem - is near impossible\nfor most complex programs.  As a result, your proof is... running the\nprogram.  That produces enormous validation consequences and costs for\ngeneric-execution scripts when applied to a decentralized network of\nvalidation P2P nodes.\n\nIf you need that capability, it is just as easy to use a normal C/C++/etc.\ncomputer language, with your preferred algorithm libraries and development\ntools.\n\nSee https://github.com/jgarzik/moxiebox for a working example of provable\nexecution.\n\n\n\nOn Thu, Dec 10, 2015 at 9:35 AM, Luke Durback via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hello Bitcoin-Dev,\n>\n> I hope this isn't out of line, but I joined the mailing list to try to\n> start a discussion on adding opcodes to make Script Turing Pseudo-Complete\n> as Wright suggested is possible.\n>\n> ---\n>\n> In line with Wright's suggestion, I propose adding a return stack\n> alongside the, already existing, control stack.\n>\n> The principle opcodes (excluding conditional versions of call and\n> return_from) needed are\n>\n> OP_DEFINITION_START FunctionName:  The code that follows is the definition\n> of a new function to be named TransactionSenderAddress.FunctionName.  If\n> this function name is already taken, the transaction is marked invalid.\n> Within the transaction, the function can be called simply as FunctionName.\n>\n> OP_DEFINITION_END:  This ends a function definition\n>\n> OP_FUNCTION_NAME FunctionName:  Gives the current transaction the name\n> FunctionName (this is necessary to build recursive functions)\n>\n> ---\n>\n> OP_CALL Namespace.FunctionName Value TransactionFee:  This marks the\n> transaction as valid.  It also pushes the current execution location onto\n> the return stack, debits the calling transaction by the TransactionFee and\n> Value, and creates a new transaction specified by Namespace.FunctionName\n> with both stacks continued from before (this may be dangerous, but I see no\n> way around it) with the specified value.\n>\n> OP_RETURN_FROM_CALL_AND_CONTINUE:  This pops the top value off the return\n> stack and continues from the specified location with both stacks in tact.\n>\n> ---\n>\n> It would also be useful if a transaction can create another transaction\n> arbitrarily, so to prepare for that, I additionally propose\n>\n> OP_NAMESPACE:  Pushes the current namespace onto the control stack\n>\n> This, combined with the ability to make new transactions arbitrarily would\n> allow a function to pay its creator.\n>\n>\n>\n> I understand that this isn't all that is needed, but I think it's a\n> start.  I hope this proposal has met you all well,\n>\n> Luke Durback\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151210/1c482e7c/attachment.html>"
            },
            {
                "author": "Luke Durback",
                "date": "2015-12-10T04:23:26",
                "message_text_only": "Mr. Garzik,\n\nThank you for the prompt response.  I should have explained my proposal a\nlittle better.\n\nFirst of all, this is not Turing completeness, nor is it pseudo-complete in\nthe sense of Ethereum's gas economics.\n\nInstead, whenever a function call is encountered, the transaction is\nvalidated and can be included in a block.  The code actually halts many\ntimes.  A new transaction is then produced with the 2 stacks stored in the\ntransaction data (so that the 2 stacks are saved and execution can be\ncontinued later).  When OP_RETURN_FROM_CALL_AND_CONTINUE is encountered,\nthe top value of the Return stack is popped and execution continues from\nthat location until validation/invalidation is reached.  It's not necessary\nto check the code to see that it has no infinite loops because any\ntransaction with infinite loops will run out of BTC with which to fund the\ntransaction fees of additional function calls.\n\nTo reiterate the most important point:  Execution halts every time a\nfunction call is encountered and the transaction can be included in a\nblock.  A new transaction is then produced that can (if included in a\nblock) continue execution.\n\n\nLuke Durback\n\nOn Wed, Dec 9, 2015 at 11:03 PM, Jeff Garzik <jgarzik at gmail.com> wrote:\n\n> There is no need for a BIP draft.  \"Turing complete\" is just a fancy,\n> executive-impressing term for \"it can run any computer program\", or put\n> even more simply, \"it can loop\"\n>\n> Furthermore, the specification of such a language is trivial.  It is the\n> economics of validation that is the complex piece.  Proving whether or not\n> a program will halt as expected - The Halting Problem - is near impossible\n> for most complex programs.  As a result, your proof is... running the\n> program.  That produces enormous validation consequences and costs for\n> generic-execution scripts when applied to a decentralized network of\n> validation P2P nodes.\n>\n> If you need that capability, it is just as easy to use a normal C/C++/etc.\n> computer language, with your preferred algorithm libraries and development\n> tools.\n>\n> See https://github.com/jgarzik/moxiebox for a working example of provable\n> execution.\n>\n>\n>\n> On Thu, Dec 10, 2015 at 9:35 AM, Luke Durback via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Hello Bitcoin-Dev,\n>>\n>> I hope this isn't out of line, but I joined the mailing list to try to\n>> start a discussion on adding opcodes to make Script Turing Pseudo-Complete\n>> as Wright suggested is possible.\n>>\n>> ---\n>>\n>> In line with Wright's suggestion, I propose adding a return stack\n>> alongside the, already existing, control stack.\n>>\n>> The principle opcodes (excluding conditional versions of call and\n>> return_from) needed are\n>>\n>> OP_DEFINITION_START FunctionName:  The code that follows is the\n>> definition of a new function to be named\n>> TransactionSenderAddress.FunctionName.  If this function name is already\n>> taken, the transaction is marked invalid.  Within the transaction, the\n>> function can be called simply as FunctionName.\n>>\n>> OP_DEFINITION_END:  This ends a function definition\n>>\n>> OP_FUNCTION_NAME FunctionName:  Gives the current transaction the name\n>> FunctionName (this is necessary to build recursive functions)\n>>\n>> ---\n>>\n>> OP_CALL Namespace.FunctionName Value TransactionFee:  This marks the\n>> transaction as valid.  It also pushes the current execution location onto\n>> the return stack, debits the calling transaction by the TransactionFee and\n>> Value, and creates a new transaction specified by Namespace.FunctionName\n>> with both stacks continued from before (this may be dangerous, but I see no\n>> way around it) with the specified value.\n>>\n>> OP_RETURN_FROM_CALL_AND_CONTINUE:  This pops the top value off the return\n>> stack and continues from the specified location with both stacks in tact.\n>>\n>> ---\n>>\n>> It would also be useful if a transaction can create another transaction\n>> arbitrarily, so to prepare for that, I additionally propose\n>>\n>> OP_NAMESPACE:  Pushes the current namespace onto the control stack\n>>\n>> This, combined with the ability to make new transactions arbitrarily\n>> would allow a function to pay its creator.\n>>\n>>\n>>\n>> I understand that this isn't all that is needed, but I think it's a\n>> start.  I hope this proposal has met you all well,\n>>\n>> Luke Durback\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/e1428fac/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-12-10T05:38:01",
                "message_text_only": "On Dec 10, 2015 10:10 AM, \"Luke Durback via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n> This, combined with the ability to make new transactions arbitrarily\nwould allow a function to pay its creator.\n\nI don't understand what you mean by \"a function\" in this context, I assume\nyou mean a scriptSig, but then \"paying its creator\" doesn't make much sense\nto me .\n\nCould you provide some high level examples of the use cases you would like\nto support with this?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151210/6fab13f9/attachment-0001.html>"
            },
            {
                "author": "Luke Durback",
                "date": "2015-12-10T06:36:28",
                "message_text_only": "Tomorrow, I'll work on writing a way to do voting on proposals with BTC\nused as voting shares (This will be difficult as I do not know FORTH).\nThat seems like a fairly simple, useful example that will require loops and\nreused functions.  I'll add a fee that goes to the creator.\n\nIMO, if you write a complicated system of scripts that's used frequently,\nit makes sense to charge a fee for its usage.  A decentralized exchange\nbetween colored coins, for instance might take a small fee on each trade.\n\n\nOn Dec 10, 2015 10:10 AM, \"Luke Durback via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n> This, combined with the ability to make new transactions arbitrarily\nwould allow a function to pay its creator.\n\nI don't understand what you mean by \"a function\" in this context, I assume\nyou mean a scriptSig, but then \"paying its creator\" doesn't make much sense\nto me .\n\nCould you provide some high level examples of the use cases you would like\nto support with this?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151210/cad0b36c/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-12-11T15:36:48",
                "message_text_only": "On Dec 10, 2015 7:36 AM, \"Luke Durback\" <luke.durback at gmail.com> wrote:\n>\n> Tomorrow, I'll work on writing a way to do voting on proposals with BTC\nused as voting shares (This will be difficult as I do not know FORTH).\nThat seems like a fairly simple, useful example that will require loops and\nreused functions.  I'll add a fee that goes to the creator.\n\nIf it's voting for something consensus, you will need something special. If\nit's not consensus (ie external) thw voting doesn't have to hit the chain\nat all.\nI don't see how \"loops and reused functions\" are needed in the scripting\nlanguage for this use case, but I'm probably missing some details. Please,\nthe more concrete you make your example, the easiest it will be for me to\nunderstand.\n\n> IMO, if you write a complicated system of scripts that's used frequently,\nit makes sense to charge a fee for its usage.\n\nBut each scriptSig is only executed once with its corresponding\nscriptPubKey. Are you proposing we change that?\n\n>  A decentralized exchange between colored coins, for instance might take\na small fee on each trade.\n\nI've been researching the topic of decentralized exchange from before the\nterm \"colored coins\" was first used (now there's multiple designs and\nimplementations); contributed to and reviewed many designs: none of them\n(colored coins or not) required turing completeness.\nI'm sorry, but what you are saying here is too vague for me to concretely\nbe able to refute the low level \"needs\" you claim your use cases to have.\n\n> On Dec 10, 2015 10:10 AM, \"Luke Durback via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n> > This, combined with the ability to make new transactions arbitrarily\nwould allow a function to pay its creator.\n>\n> I don't understand what you mean by \"a function\" in this context, I\nassume you mean a scriptSig, but then \"paying its creator\" doesn't make\nmuch sense to me .\n>\n> Could you provide some high level examples of the use cases you would\nlike to support with this?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151211/af6b721c/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-12-11T15:38:37",
                "message_text_only": "well \"only executed once\" (every time someone verifies that transaction)...\nOn Dec 11, 2015 4:36 PM, \"Jorge Tim\u00f3n\" <jtimon at jtimon.cc> wrote:\n\n>\n> On Dec 10, 2015 7:36 AM, \"Luke Durback\" <luke.durback at gmail.com> wrote:\n> >\n> > Tomorrow, I'll work on writing a way to do voting on proposals with BTC\n> used as voting shares (This will be difficult as I do not know FORTH).\n> That seems like a fairly simple, useful example that will require loops and\n> reused functions.  I'll add a fee that goes to the creator.\n>\n> If it's voting for something consensus, you will need something special.\n> If it's not consensus (ie external) thw voting doesn't have to hit the\n> chain at all.\n> I don't see how \"loops and reused functions\" are needed in the scripting\n> language for this use case, but I'm probably missing some details. Please,\n> the more concrete you make your example, the easiest it will be for me to\n> understand.\n>\n> > IMO, if you write a complicated system of scripts that's used\n> frequently, it makes sense to charge a fee for its usage.\n>\n> But each scriptSig is only executed once with its corresponding\n> scriptPubKey. Are you proposing we change that?\n>\n> >  A decentralized exchange between colored coins, for instance might take\n> a small fee on each trade.\n>\n> I've been researching the topic of decentralized exchange from before the\n> term \"colored coins\" was first used (now there's multiple designs and\n> implementations); contributed to and reviewed many designs: none of them\n> (colored coins or not) required turing completeness.\n> I'm sorry, but what you are saying here is too vague for me to concretely\n> be able to refute the low level \"needs\" you claim your use cases to have.\n>\n> > On Dec 10, 2015 10:10 AM, \"Luke Durback via bitcoin-dev\" <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > > This, combined with the ability to make new transactions arbitrarily\n> would allow a function to pay its creator.\n> >\n> > I don't understand what you mean by \"a function\" in this context, I\n> assume you mean a scriptSig, but then \"paying its creator\" doesn't make\n> much sense to me .\n> >\n> > Could you provide some high level examples of the use cases you would\n> like to support with this?\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151211/7e6ef177/attachment.html>"
            },
            {
                "author": "Luke Durback",
                "date": "2015-12-11T21:45:44",
                "message_text_only": ">If it's voting for something consensus, you will need something special.\nIf it's not consensus (ie external) thw voting doesn't have to hit the\nchain at all.\n\nI had in mind voting for something that can't be trusted if done\nexternally:  Perhaps BIPs for instance.  People would somehow \"mark\" their\nBTC as being \"For Proposition X\" (as opposed to all other propositions) and\nthe vote would be canceled as soon as the BTC is spent again.\n\nUnfortunately, I've spent the past 2 days trying to find a design that\nwould allow this (I don't think my original suggestion made sense in the\ncontext of how transactions work), and I haven't gotten much yet.\n\n>But each scriptSig is only executed once with its corresponding\nscriptPubKey. Are you proposing we change that?\n\nSorry, I didn't understand Bitcoin's transaction model well enough when I\nfirst made the proposal.  If Turing Pseudo-Completeness is possible with\nBitcoin, then I understand now that it could not require you to execute a\nscript more than once.  My current thought is that recursion can be\naccomplished via checking if the next output's scriptPubKey is identical in\nevery way to the current scriptPubKey.  Unfortunately, a lot more is needed\nthan just recursion in order to do on-chain BTC voting the way I have in\nmind.  I'll keep working on this.\n\nOn Fri, Dec 11, 2015 at 10:36 AM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n\n>\n> On Dec 10, 2015 7:36 AM, \"Luke Durback\" <luke.durback at gmail.com> wrote:\n> >\n> > Tomorrow, I'll work on writing a way to do voting on proposals with BTC\n> used as voting shares (This will be difficult as I do not know FORTH).\n> That seems like a fairly simple, useful example that will require loops and\n> reused functions.  I'll add a fee that goes to the creator.\n>\n> If it's voting for something consensus, you will need something special.\n> If it's not consensus (ie external) thw voting doesn't have to hit the\n> chain at all.\n> I don't see how \"loops and reused functions\" are needed in the scripting\n> language for this use case, but I'm probably missing some details. Please,\n> the more concrete you make your example, the easiest it will be for me to\n> understand.\n>\n> > IMO, if you write a complicated system of scripts that's used\n> frequently, it makes sense to charge a fee for its usage.\n>\n> But each scriptSig is only executed once with its corresponding\n> scriptPubKey. Are you proposing we change that?\n>\n> >  A decentralized exchange between colored coins, for instance might take\n> a small fee on each trade.\n>\n> I've been researching the topic of decentralized exchange from before the\n> term \"colored coins\" was first used (now there's multiple designs and\n> implementations); contributed to and reviewed many designs: none of them\n> (colored coins or not) required turing completeness.\n> I'm sorry, but what you are saying here is too vague for me to concretely\n> be able to refute the low level \"needs\" you claim your use cases to have.\n>\n> > On Dec 10, 2015 10:10 AM, \"Luke Durback via bitcoin-dev\" <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > > This, combined with the ability to make new transactions arbitrarily\n> would allow a function to pay its creator.\n> >\n> > I don't understand what you mean by \"a function\" in this context, I\n> assume you mean a scriptSig, but then \"paying its creator\" doesn't make\n> much sense to me .\n> >\n> > Could you provide some high level examples of the use cases you would\n> like to support with this?\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151211/20ba8605/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-12-12T20:00:43",
                "message_text_only": "On Fri, Dec 11, 2015 at 10:45 PM, Luke Durback <luke.durback at gmail.com> wrote:\n>>If it's voting for something consensus, you will need something special. If\n>> it's not consensus (ie external) thw voting doesn't have to hit the chain at\n>> all.\n>\n> I had in mind voting for something that can't be trusted if done externally:\n> Perhaps BIPs for instance.  People would somehow \"mark\" their BTC as being\n> \"For Proposition X\" (as opposed to all other propositions) and the vote\n> would be canceled as soon as the BTC is spent again.\n>\n> Unfortunately, I've spent the past 2 days trying to find a design that would\n> allow this (I don't think my original suggestion made sense in the context\n> of how transactions work), and I haven't gotten much yet.\n\nWell, as said, if it's for consensus, you will need to adapt the\nsystem in a special way anyway, but I still don't see why turing\ncompleteness is required.\nThis type of idea is not new. Since miners can censor votes (and\nthat's undetectable for consensus), several solutions have been\nproposed, time lock the votes, for example.\n\n>>But each scriptSig is only executed once with its corresponding\n>> scriptPubKey. Are you proposing we change that?\n>\n> Sorry, I didn't understand Bitcoin's transaction model well enough when I\n> first made the proposal.  If Turing Pseudo-Completeness is possible with\n> Bitcoin, then I understand now that it could not require you to execute a\n> script more than once.  My current thought is that recursion can be\n> accomplished via checking if the next output's scriptPubKey is identical in\n> every way to the current scriptPubKey.  Unfortunately, a lot more is needed\n> than just recursion in order to do on-chain BTC voting the way I have in\n> mind.  I'll keep working on this.\n\nWhat you call \"recursion\" seems similar to what we usually call \"covenants\", see\n\nhttps://bitcointalk.org/index.php?topic=278122.0\n\nAlthough the thread says \"an amusingly bad idea\", I think it's\nactually a great idea and there's some use cases that are very hard to\nsupport without covenants.\nAgain, no Turing completeness required for this.\n\n> On Fri, Dec 11, 2015 at 10:36 AM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n>>\n>>\n>> On Dec 10, 2015 7:36 AM, \"Luke Durback\" <luke.durback at gmail.com> wrote:\n>> >\n>> > Tomorrow, I'll work on writing a way to do voting on proposals with BTC\n>> > used as voting shares (This will be difficult as I do not know FORTH).  That\n>> > seems like a fairly simple, useful example that will require loops and\n>> > reused functions.  I'll add a fee that goes to the creator.\n>>\n>> If it's voting for something consensus, you will need something special.\n>> If it's not consensus (ie external) thw voting doesn't have to hit the chain\n>> at all.\n>> I don't see how \"loops and reused functions\" are needed in the scripting\n>> language for this use case, but I'm probably missing some details. Please,\n>> the more concrete you make your example, the easiest it will be for me to\n>> understand.\n>>\n>> > IMO, if you write a complicated system of scripts that's used\n>> > frequently, it makes sense to charge a fee for its usage.\n>>\n>> But each scriptSig is only executed once with its corresponding\n>> scriptPubKey. Are you proposing we change that?\n>>\n>> >  A decentralized exchange between colored coins, for instance might take\n>> > a small fee on each trade.\n>>\n>> I've been researching the topic of decentralized exchange from before the\n>> term \"colored coins\" was first used (now there's multiple designs and\n>> implementations); contributed to and reviewed many designs: none of them\n>> (colored coins or not) required turing completeness.\n>> I'm sorry, but what you are saying here is too vague for me to concretely\n>> be able to refute the low level \"needs\" you claim your use cases to have.\n>>\n>> > On Dec 10, 2015 10:10 AM, \"Luke Durback via bitcoin-dev\"\n>> > <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> > > This, combined with the ability to make new transactions arbitrarily\n>> > > would allow a function to pay its creator.\n>> >\n>> > I don't understand what you mean by \"a function\" in this context, I\n>> > assume you mean a scriptSig, but then \"paying its creator\" doesn't make much\n>> > sense to me .\n>> >\n>> > Could you provide some high level examples of the use cases you would\n>> > like to support with this?\n>\n>"
            },
            {
                "author": "Emin G\u00fcn Sirer",
                "date": "2015-12-12T21:01:45",
                "message_text_only": ".,\n,\n/.\n, /,\n\n,.\n   / ,\n..\n,,,  . // .,      .\n\n_. ...  ..   ._.\n\n,    _\n\n\n,\n\n\n\n,\n,\n  , , ...     _  _.\n\n,.\n\n.  ,.,    _.\n.,    ,  ..\n,\n\n,,\n\n._\n\n.  .\n\n_\n.\n,\n,     ,    ,   /..,,\n\n/ ,\n\n.     .\n\n_\n.,. _.. ,\n,\n\n.. _\n   ..\n\n,.,, _\n, _\n,\n///\n. ,\n\n   / . ,.\n  ,\n,.,\n. ,\n, .,   ,. ._ ,  ,,,//\n\n,        ,\n.\n\n,\n\n,\n  . . ,\n\n, //  .\n,  ,\n/\n\n      _,.\n\n, . ,, .\n\n..\n  /,/ .\n.\n\n\n  .   .,,_//\n,,\n.,  .\n\n.  /_. ,\n/\n.\n  /\n.._\n.\n,, / .\n   . _ ,\n,  ,\n/     ,    _ .,\n, ,,, ..  ,\n  ,\n\n  /.,.\n  /. /\n. ,/  ,\n\n. .   /,\n/,\n._\n   ,/.\n_\n.,\n,//\n, .,,, , ,    , ,\n,\n\n,.   ,.,.  .\n\n,  .    ,.  .,   ,\n/   _\n.\n/\n  ,.,. ,\n,._\n\n\n,,\n\n, _ _ ,\n\n,\n. ,,   ,  _\n\n\n_..,\n\n  ,\n// ,\n__ /\n!;\"$'''. b\n    __\n\nOn Sat, Dec 12, 2015, 3:01 PM Jorge Tim\u00f3n <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Fri, Dec 11, 2015 at 10:45 PM, Luke Durback <luke.durback at gmail.com>\n> wrote:\n> >>If it's voting for something consensus, you will need something special.\n> If\n> >> it's not consensus (ie external) thw voting doesn't have to hit the\n> chain at\n> >> all.\n> >\n> > I had in mind voting for something that can't be trusted if done\n> externally:\n> > Perhaps BIPs for instance.  People would somehow \"mark\" their BTC as\n> being\n> > \"For Proposition X\" (as opposed to all other propositions) and the vote\n> > would be canceled as soon as the BTC is spent again.\n> >\n> > Unfortunately, I've spent the past 2 days trying to find a design that\n> would\n> > allow this (I don't think my original suggestion made sense in the\n> context\n> > of how transactions work), and I haven't gotten much yet.\n>\n> Well, as said, if it's for consensus, you will need to adapt the\n> system in a special way anyway, but I still don't see why turing\n> completeness is required.\n> This type of idea is not new. Since miners can censor votes (and\n> that's undetectable for consensus), several solutions have been\n> proposed, time lock the votes, for example.\n>\n> >>But each scriptSig is only executed once with its corresponding\n> >> scriptPubKey. Are you proposing we change that?\n> >\n> > Sorry, I didn't understand Bitcoin's transaction model well enough when I\n> > first made the proposal.  If Turing Pseudo-Completeness is possible with\n> > Bitcoin, then I understand now that it could not require you to execute a\n> > script more than once.  My current thought is that recursion can be\n> > accomplished via checking if the next output's scriptPubKey is identical\n> in\n> > every way to the current scriptPubKey.  Unfortunately, a lot more is\n> needed\n> > than just recursion in order to do on-chain BTC voting the way I have in\n> > mind.  I'll keep working on this.\n>\n> What you call \"recursion\" seems similar to what we usually call\n> \"covenants\", see\n>\n> https://bitcointalk.org/index.php?topic=278122.0\n>\n> Although the thread says \"an amusingly bad idea\", I think it's\n> actually a great idea and there's some use cases that are very hard to\n> support without covenants.\n> Again, no Turing completeness required for this.\n>\n> > On Fri, Dec 11, 2015 at 10:36 AM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n> >>\n> >>\n> >> On Dec 10, 2015 7:36 AM, \"Luke Durback\" <luke.durback at gmail.com> wrote:\n> >> >\n> >> > Tomorrow, I'll work on writing a way to do voting on proposals with\n> BTC\n> >> > used as voting shares (This will be difficult as I do not know\n> FORTH).  That\n> >> > seems like a fairly simple, useful example that will require loops and\n> >> > reused functions.  I'll add a fee that goes to the creator.\n> >>\n> >> If it's voting for something consensus, you will need something special.\n> >> If it's not consensus (ie external) thw voting doesn't have to hit the\n> chain\n> >> at all.\n> >> I don't see how \"loops and reused functions\" are needed in the scripting\n> >> language for this use case, but I'm probably missing some details.\n> Please,\n> >> the more concrete you make your example, the easiest it will be for me\n> to\n> >> understand.\n> >>\n> >> > IMO, if you write a complicated system of scripts that's used\n> >> > frequently, it makes sense to charge a fee for its usage.\n> >>\n> >> But each scriptSig is only executed once with its corresponding\n> >> scriptPubKey. Are you proposing we change that?\n> >>\n> >> >  A decentralized exchange between colored coins, for instance might\n> take\n> >> > a small fee on each trade.\n> >>\n> >> I've been researching the topic of decentralized exchange from before\n> the\n> >> term \"colored coins\" was first used (now there's multiple designs and\n> >> implementations); contributed to and reviewed many designs: none of them\n> >> (colored coins or not) required turing completeness.\n> >> I'm sorry, but what you are saying here is too vague for me to\n> concretely\n> >> be able to refute the low level \"needs\" you claim your use cases to\n> have.\n> >>\n> >> > On Dec 10, 2015 10:10 AM, \"Luke Durback via bitcoin-dev\"\n> >> > <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >> > > This, combined with the ability to make new transactions arbitrarily\n> >> > > would allow a function to pay its creator.\n> >> >\n> >> > I don't understand what you mean by \"a function\" in this context, I\n> >> > assume you mean a scriptSig, but then \"paying its creator\" doesn't\n> make much\n> >> > sense to me .\n> >> >\n> >> > Could you provide some high level examples of the use cases you would\n> >> > like to support with this?\n> >\n> >\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151212/a45f8dbe/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Standard BIP Draft: Turing Pseudo-Completeness",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeff Garzik",
                "Jorge Tim\u00f3n",
                "Luke Durback",
                "Emin G\u00fcn Sirer"
            ],
            "messages_count": 10,
            "total_messages_chars_count": 30267
        }
    },
    {
        "title": "[bitcoin-dev] Segregated Witness features wish list",
        "thread_messages": [
            {
                "author": "jl2012 at xbt.hk",
                "date": "2015-12-10T06:47:35",
                "message_text_only": "It seems the current consensus is to implement Segregated Witness. SW \nopens many new possibilities but we need a balance between new features \nand deployment time frame. I'm listing by my priority:\n\n1-2 are about scalability and have highest priority\n\n1. Witness size limit: with SW we should allow a bigger overall block \nsize. It seems 2MB is considered to be safe for many people. However, \nthe exact size and growth of block size should be determined based on \ntesting and reasonable projection.\n\n2. Deployment time frame: I prefer as soon as possible, even if none of \nthe following new features are implemented. This is not only a technical \nissue but also a response to the community which has been waiting for a \nscaling solution for years\n\n3-6 promote safety and reduce level of trust (higher priority)\n\n3. SIGHASH_WITHINPUTVALUE [1]: there are many SIGHASH proposals but this \none has the highest priority as it makes offline signing much easier.\n\n4. Sum of fee, sigopcount, size etc as part of the witness hash tree: \nfor compact proof of violations in these parameters. I prefer to have \nthis feature in SWv1. Otherwise, that would become an ugly softfork in \nSWv2 as we need to maintain one more hash tree\n\n5. Height and position of an input as part of witness will allow compact \nproof of non-existing UTXO. We need this eventually. If it is not done \nin SWv1, we could softfork it nicely in SWv2. I prefer this earlier as \nthis is the last puzzle for compact fraud proof.\n\n6. BIP62 and OP_IF malleability fix [2] as standardness rules: \ninvoluntary malleability may still be a problem in the relay network and \nmay make the relay less efficient (need more research)\n\n7-15 are new features and long-term goals (lower priority)\n\n7. Enable OP_CAT etc:\nOP_CAT will allow tree signatures described by [3]. Even without Schnorr \nsignature, m-of-n multisig will become more efficient if m < n.\n\nOP_SUBSTR/OP_LEFT/OP_RIGHT will allow people to shorten a payment \naddress, while sacrificing security.\n\nI'm not sure how those disabled bitwise logic codes could be useful\n\nMultiplication and division may still considered to be risky and not \nvery useful?\n\n8. Schnorr signature: for very efficient multisig [3] but could be \nintroduced later.\n\n9. Per-input lock-time and relative lock-time: define lock-time and \nrelative lock-time in witness, and signed by user. BIP68 is not a very \nideal solution due to limited lock time length and resolution\n\n10. OP_PUSHLOCKTIME and OP_PUSHRELATIVELOCKTIME: push the lock-time and \nrelative lock-time to stack. Will allow more flexibility than OP_CLTV \nand OP_CSV\n\n11. OP_RETURNTURE which allows softfork of any new OP codes [4]. It is \nnot really necessary with the version byte design but with OP_RETURNTURE \nwe don't need to pump the version byte too frequently.\n\n12. OP_EVAL (BIP12), which enables Merkleized Abstract Syntax Trees \n(MAST) with OP_CAT [5]. This will also obsolete BIP16. Further \nrestrictions should be made to make it safe [6]:\na) We may allow at most one OP_EVAL in the scriptPubKey\nb) Not allow any OP_EVAL in the serialized script, nor anywhere else in \nthe witness (therefore not Turing-complete)\nc) In order to maintain the ability to statically analyze scripts, the \nserialized script must be the last push of the witness (or script \nfails), and OP_EVAL must be the last OP code in the scriptPubKey\n\n13. Combo OP codes for more compact scripts, for example:\n\nOP_MERKLEHASH160, if executed, is equivalent to OP_SWAP OP_IF OP_SWAP \nOP_ENDIF OP_CAT OP_HASH160 [3]. Allowing more compact tree-signature and \nMAST scripts.\n\nOP_DUPTOALTSTACK, OP_DUPFROMALTSTACK: copy to / from alt stack without \nremoving the item\n\n14. UTXO commitment: good but not in near future\n\n15. Starting as a softfork, moving to a hardfork? SW Softfork is a quick \nbut dirty solution. I believe a hardfork is unavoidable in the future, \nas the 1MB limit has to be increased someday. If we could plan it ahead, \nwe could have a much cleaner SW hardfork in the future, with codes \npre-announced for 2 years.\n\n\n[1] https://bitcointalk.org/index.php?topic=181734.0\n[2] \nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-November/011679.html\n[3] https://blockstream.com/2015/08/24/treesignatures/\n[4] https://bitcointalk.org/index.php?topic=1106586.0\n[5] \nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-September/010977.html\n[6] https://bitcointalk.org/index.php?topic=58579.msg690093#msg690093"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-12-10T08:26:04",
                "message_text_only": "On Thu, Dec 10, 2015 at 6:47 AM, jl2012--- via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> It seems the current consensus is to implement Segregated Witness. SW opens\n> many new possibilities but we need a balance between new features and\n> deployment time frame. I'm listing by my priority:\n\n> 2. Deployment time frame: I prefer as soon as possible, even if none of the following new features are implemented.\n\nThanks, I agree there.\n\nA point to keep in mind:  Segregated Witness was specifically designed\nto make script changes / improvements / additions / total rewrites no\nharder to do _after_ SW then they would be do do along with it.  For\nmany people the \"ah ha! lets do this\" was realizing it could be a\npretty clean soft-fork.  For me, it was realizing that we could\nstructure Segwit in a way that radically simply future script updates\n... and in doing so avoid a getting trapped by a rush to put in every\nscript update someone wants.\n\nThis is achieved by having the 'version' byte(s) at the start of the\nwitness program. If the witness program prefix is unrecognized it\nmeans RETURN TRUE.  This recaptures the behavior that seems to have\nbeen intended by OP_VER in the earliest versions of the software, but\nactually works instead of giving every user the power to hardfork the\nsystem at any time. :)  This escapes much of the risk in script\nchanges, as we no longer need to worry about negation, or other\ninteractions potentially breaking things.  A new version flag can have\nits whole design crafted as if it were being created on a clean slate.\n\nOptimizing layout and such I think makes sense, but I think we should\nconsider any script enhancements completely off the table for SW;\notherwise the binding will delay deployment and increase complexity. I\nwant most of those things too (a couple I disagree with) and a few of\nthem we could do quite quickly-- but no need to bind them up; post SW\nand esp with version bits we could deploy them quite rapidly and on\ntheir own timeframes.\n\n\n> Multiplication and division may still considered to be risky and not very useful?\n\nOperations like these make sense with fixed with types, when they are\nover arbitrary bignums, they're a complexity nightmare...  as\ndemonstrated by Bitcoin. :)\n\n\nRE: OP_DUPTOALTSTACK  yea, I've wanted that several times (really I've\nbeen sad that there isn't just a stack flag on every manipulation\ninstruction)."
            },
            {
                "author": "Bryan Bishop",
                "date": "2015-12-10T08:28:49",
                "message_text_only": "On Thu, Dec 10, 2015 at 12:47 AM, jl2012 wrote:\n> 3. SIGHASH_WITHINPUTVALUE [1]: there are many SIGHASH proposals but this one\n> has the highest priority as it makes offline signing much easier.\n\nnhashtype proposal:\nhttps://github.com/scmorse/bitcoin-misc/blob/master/sighash_proposal.md\n\nOP_CODESEPARATOR:\nhttp://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-April/007802.html\n\nsummary email about sighash type proposals (which IIRC you saw, so\nleaving this link here is mainly for the benefit of others):\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-August/010759.html\n\n- Bryan\nhttp://heybryan.org/\n1 512 203 0507"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-12-10T09:51:23",
                "message_text_only": "On Thu, Dec 10, 2015 at 6:47 AM, jl2012--- via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> 4. Sum of fee, sigopcount, size etc as part of the witness hash tree: for\n\nI should have also commented on this: the block can indicate how many\nsum criteria there are; and then additional ones could be soft-forked\nin. Haven't tried implementing it yet, but there you go. :)"
            },
            {
                "author": "jl2012 at xbt.hk",
                "date": "2015-12-13T15:25:10",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\n\nI'm trying to list the minimal consensus rule changes needed for segwit \nsoftfork. The list does not cover the changes in non-consensus critical \nbehaviors, such as relay of witness data.\n\n1. OP_NOP4 is renamed as OP_SEGWIT\n2. A script with OP_SEGWIT must fail if the scriptSig is not completely \nempty\n3. If OP_SEGWIT is used in the scriptPubKey, it must be the only and the \nlast OP code in the scriptPubKey, or the script must fail\n4. The OP_SEGWIT must be preceded by exactly one data push (the \n\"serialized script\") with at least one byte, or the script must fail\n5. The most significant byte of serialized script is the version byte, \nan unsigned number\n6. If the version byte is 0x00, the script must fail\n7. If the version byte is 0x02 to 0xff, the rest of the serialized \nscript is ignored and the output is spendable with any form of witness \n(even if the witness contains something invalid in the current script \nsystem, e.g. OP_RETURN)\n8. If the version byte is 0x01,\n8a. rest of the serialized script is deserialized, and be interpreted as \nthe scriptPubKey.\n8b. the witness is interpreted as the scriptSig.\n8c. the script runs with existing rules (including P2SH)\n9. If the script fails when OP_SEGWIT is interpreted as a NOP, the \nscript must fail. However, this is guaranteed by the rules 2, 3, 4, 6 so \nno additional check is needed.\n10. The calculation of Merkle root in the block header remains unchanged\n11. The witness commitment is placed somewhere in the block, either in \ncoinbase or an OP_RETURN output in a specific tx\n\nFormat of the witness commitment:\nThe witness commitment could be as simple as a hash tree of all witness \nin a block. However, this is bad for further development of sum tree for \ncompact SPV fraud proof as we have to maintain one more tree in the \nfuture. Even if we are not going to implement any sum checking in first \nversion of segwit, it's better to make it easier for future softforks. \n(credit: gmaxwell)\n12. The block should indicate how many sum criteria there are by \ncommitting the number following the witness commitment\n13. The witness commitment is a hash-sum tree with the number of sum \ncriteria committed in rule 12\n14. Each sum criterion is a fixed 8 byte signed number (Negative value \nis allowed for use like counting delta-UTXO. 8 bytes is needed for fee \ncommitment. Multiple smaller criteria may share a 8 byte slot, as long \nas they do not allow negative value)\n15. Nodes will ignore the sum criteria that they do not understand, as \nlong as the sum is correctly calculated\n\nSize limit:\n16. We need to determine the size limit of witness\n17. We also need to have an upper limit for the number of sum criteria, \nor a malicious miner may find a block with many unknown sum criteria and \nfeed unlimited amount of garbage to other nodes.\n\nAll other functions I mentioned in my wish list could be softforked \nlater to this system\n\nTo mitigate the risk described in rule 17, we may ask miners to vote for \nan increase (but not decrease) in the number of sum criteria. Initially \nthere should be 0 sum criteria. If we would like to introduce a new \ncriteria, miners will support the proposal by setting the number of sum \ncriteria as 1. However, until 95% of miners support the increase, all \nvalues of the extra sum criteria must be 0. Therefore, even a malicious \nminer may declare any amount of sum criteria, those criteria unknown to \nthe network must be 0, and no extra data is needed to construct the \nblock. This voting mechanism could be a softfork over rule 12 and 13, \nand is not necessary in the initial segwit deployment.\n-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v2\n\niQGcBAEBCAAGBQJWbY0jAAoJEO6eVSA0viTSU1oMAJIrYWerwk84poZBL/ezEsIp\n9fCLnFZ4lyO2ijAm5UmwLXGijY03kqp29b0zmyIWV2WuoeW2lN64tLHQRilT0+5R\nn5/viQOMv0C0MYs525+/dpNkk2q2MiFmyyozdbU6zcyfdkrkYdChCFOQ9GsxzQHk\nn4lL4/RSKdqJZg4x2yEGgdyKA6XrQHaFirdr/K2bhhbk4Q0SOuYjy8Wxa2oCHFCC\nWG4K2NBnKCI7DuVXQK+ZC8dYXMwbemeFfPHY6dZVti7j/OFsllyxno/CFKO3rsCs\n+uko4XJk6pH0Ncjrc1n0l0v9xIKF5hTqSxFs+GVvhkiBdTDZVe7CdedO9qJWf1hE\nbbmLXTURCDQUFe9F3uKsnYfMoD5eniWHx2OQcJcNPlLMJd9zObB3HdgFMW6N53KN\nQXLmxobU/xFhmFknz1ShGEIdGSaH0gqnb+WEkO5v5vBO4L6Cikc+lcp7zXqQzWpW\nuqm3QSrbKcbR6JEwDFoGQpDkcqpwpTIrOAk4B1jJRg==\n=J2KF\n-----END PGP SIGNATURE-----\n\n\nGregory Maxwell \u65bc 2015-12-10 04:51 \u5beb\u5230:\n> On Thu, Dec 10, 2015 at 6:47 AM, jl2012--- via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> 4. Sum of fee, sigopcount, size etc as part of the witness hash tree: \n>> for\n> \n> I should have also commented on this: the block can indicate how many\n> sum criteria there are; and then additional ones could be soft-forked\n> in. Haven't tried implementing it yet, but there you go. :)"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-12-13T18:07:01",
                "message_text_only": "On Sun, Dec 13, 2015 at 4:25 PM, jl2012--- via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> I'm trying to list the minimal consensus rule changes needed for segwit\n> softfork. The list does not cover the changes in non-consensus critical\n> behaviors, such as relay of witness data.\n>\n> 1. OP_NOP4 is renamed as OP_SEGWIT\n> 2. A script with OP_SEGWIT must fail if the scriptSig is not completely\n> empty\n> 3. If OP_SEGWIT is used in the scriptPubKey, it must be the only and the\n> last OP code in the scriptPubKey, or the script must fail\n> 4. The OP_SEGWIT must be preceded by exactly one data push (the \"serialized\n> script\") with at least one byte, or the script must fail\n\nThe use of a NOP opcode to indicate a witness script was something I\nconsidered at first too, but it's not really needed. You wouldn't be\nable to use that opcode in any place a normal opcode could occur, as\nit needs to be able to inspect the full scriptSig (rather than just\nits resulting stack) anyway. So both in practice and conceptually it\nis only really working as a template that gets assigned a special\nmeaning (like P2SH did). We don't need an opcode for that, and instead\nwe could say that any scriptPubKey (or redeemscript) that consists of\na single push is a witness program.\n\n> 5. The most significant byte of serialized script is the version byte, an\n> unsigned number\n> 6. If the version byte is 0x00, the script must fail\n\nWhat is that good for?\n\n> 7. If the version byte is 0x02 to 0xff, the rest of the serialized script is\n> ignored and the output is spendable with any form of witness (even if the\n> witness contains something invalid in the current script system, e.g.\n> OP_RETURN)\n\nDo you mean the scriptPubKey itself, or the script that follows after\nthe version byte?\n* The scriptPubKey itself: that's in contradiction with your rule 4,\nas segwit scripts are by definition only a push (+ opcode), so they\ncan't be an OP_RETURN.\n* The script after the version byte: agree - though it doesn't\nactually need to be a script at all even (see further).\n\n> 8. If the version byte is 0x01,\n> 8a. rest of the serialized script is deserialized, and be interpreted as the\n> scriptPubKey.\n> 8b. the witness is interpreted as the scriptSig.\n> 8c. the script runs with existing rules (including P2SH)\n\nI don't think it's very useful to allow P2SH inside segwit, as we can\nactually do better and allow segwit scripts to push the (perhaps 256\nbit) hash of the redeemscript in the scriptPubKey, and have the full\nredeemscript in the witness. See further for details. The numbers I\nshowed in the presentation were created using a simulation that used\nthat model already.\n\nIt is useful however to allow segwit inside P2SH (so the witness\nprogram including version byte goes into the redeemscript, inside the\nscriptSig). This allows old clients to send to new wallets without any\nmodifications (at slightly lower efficiency). The rules in this case\nmust say that the scriptSig is exactly a push of the redeemscript\n(which itself contains the witness program), to provide both\ncompatibility with old consensus rules and malleability protection.\n\nSo let me summarize by giving an equivalent to your list above,\nreflecting how my current prototype works:\nA) A scriptPubKey or P2SH redeemscript that consists of a single push\nof 2 to 41 bytes gets a new special meaning, and the byte vector\npushed by it is called the witness program.\nA.1) In case the scriptPubKey pushes a witness program directly, the\nscriptSig must be exactly empty.\nA.2) In case the redeemscript pushes a witness program, the scriptSig\nmust be exactly the single push of the redeemscript.\nB) The first byte of a witness program is the version byte.\nB.1) If the witness version byte is 0, the rest of the witness program\nis the actual script, which is executed after normal script evaluation\nbut with data from the witness rather than the scriptSig. The program\nmust not fail and result in a single TRUE on the stack, and nothing\nelse (to prevent stuffing the witness with pointless data during relay\nof transactions).\nB.2) if the witness version byte is 1, the rest of the witness program\nmust be 32 bytes, and a SHA256 hash of the actual script. The witness\nmust consist of an input stack to feed to the program, followed by the\nserialized program itself (whose hash must match the hash pushed in\nthe witness program). It is executed after normal script evluation,\nand must not fail and result in a single TRUE on the stack, and\nnothing else.\nB.3) if the witness version byte is 2 or higher, no further\ninterpretation of the data happens, but can be softforked in.\n\nI'll write a separate mail on the block commitment structure.\n\n-- \nPieter"
            },
            {
                "author": "jl2012 at xbt.hk",
                "date": "2015-12-13T18:41:44",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\n\nPieter Wuille 2015-12-13 13:07 :\n\n> The use of a NOP opcode to indicate a witness script was something I\n> considered at first too, but it's not really needed. You wouldn't be\n> able to use that opcode in any place a normal opcode could occur, as\n> it needs to be able to inspect the full scriptSig (rather than just\n> its resulting stack) anyway. So both in practice and conceptually it\n> is only really working as a template that gets assigned a special\n> meaning (like P2SH did). We don't need an opcode for that, and instead\n> we could say that any scriptPubKey (or redeemscript) that consists of\n> a single push is a witness program.\n> \n>> 5. The most significant byte of serialized script is the version byte, \n>> an\n>> unsigned number\n>> 6. If the version byte is 0x00, the script must fail\n> \n> What is that good for?\n\nJust to make sure a script like OP_0 OP_SEGWIT will fail.\n\nAnyway, your design may be better so forget it\n\n>> 7. If the version byte is 0x02 to 0xff, the rest of the serialized \n>> script is\n>> ignored and the output is spendable with any form of witness (even if \n>> the\n>> witness contains something invalid in the current script system, e.g.\n>> OP_RETURN)\n> \n> Do you mean the scriptPubKey itself, or the script that follows after\n> the version byte?\n> * The scriptPubKey itself: that's in contradiction with your rule 4,\n> as segwit scripts are by definition only a push (+ opcode), so they\n> can't be an OP_RETURN.\n> * The script after the version byte: agree - though it doesn't\n> actually need to be a script at all even (see further).\n\nI am not referring to the serialized script, but the witness. Basically,\nit doesn't care what the content look like.\n\n\n> It is useful however to allow segwit inside P2SH\n\nAgree\n\n> So let me summarize by giving an equivalent to your list above,\n> reflecting how my current prototype works:\n> A) A scriptPubKey or P2SH redeemscript that consists of a single push\n> of 2 to 41 bytes gets a new special meaning, and the byte vector\n> pushed by it is called the witness program.\n\nWhy 41 bytes? Do you expect all witness program to be P2SH-like?\n\n> The program\n> must not fail and result in a single TRUE on the stack, and nothing\n> else (to prevent stuffing the witness with pointless data during relay\n> of transactions).\n\nCould we just implement this as standardness rule? It is always possible\nto stuff the scriptSig with pointless data so I don't think it's a new\nattack vector. What if we want to include the height and tx index of\nthe input for compact fraud proof? Such fraud proof should not be an\nopt-in function and not be dependent on the version byte\n\nFor the same reason, we should also allow traditional tx to have data\nin the witness field, for any potential softfork upgrade\n-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v2\n\niQGcBAEBCAAGBQJWbbugAAoJEO6eVSA0viTSD8oMAKFvd/+KZgH13tErEA+iXzF5\npwT4/eoQWSTvxIDVrFN+9wV79ogO4/aiCDEdmNF2IZD3QqmhKl7iOPw2SEseRTbe\ne1r5z67yuudXyEQocZvy5+NOUp3N978b8weuRsHWG1HXgxTRmgZTrEeNtbEUs0X2\nn5l6e0scnZAu70svBXr8X9HnOm2P/QLxtAqyNW19caCi+Dg/4Curx48tXQ/I9IxT\nSYFVzB++FIoua49Cf1RJN+dUfywg67wT5l9NX4uWAX0qNB+p6BPP8df/72G/u564\nNIaJs3IFiUaNktXz9aDM4s7pSzR6PlCK6LFKjE52sBY5uREHGU4PnfX9YqtwiEXA\nHr3YoFiepxAwl6icJi3wHKa6i0NGvj1fR1h6xuJ7ulzNv5mwuzXPOgvTDK4wpejl\nee8wsQZwmzchAfgyfPsgSaPh/jjBwm2S+WDMbL4HDmnWqVDl8dG3I/b3XP0aegY9\n4RxPhLOA1qToNDGhnm+JNqT60OKgatpDN/4bRgRscA==\n=4B1D\n-----END PGP SIGNATURE-----"
            },
            {
                "author": "Tamas Blummer",
                "date": "2015-12-10T12:54:30",
                "message_text_only": "Note that the unused space in coin base input script allows us to soft-fork an additional SW Merkle tree root into the design,\ntherefore please make sure the new SW data structure also has a new slot for future extension.\n\nTamas Blummer"
            },
            {
                "author": "Jannes Faber",
                "date": "2015-12-12T00:43:11",
                "message_text_only": "Segregated IBLT\n\nI was just wondering if it would make sense when we have SW to also make\nSegregated IBLT? Segregating transactions from signatures and then tune the\nparameters such that transactions have a slightly higher guarantee and save\na bit of space on the signatures side.\n\nIBLT should of course, most of the time, convey all transactions _and_ all\nsignatures. However, in suboptimal situations, at least the receiving miner\nwill be more likely to have all the transactions, just possibly not all the\nsignatures.\n\nAssuming the miner was already planning on SPV mining anyway, at least now\nshe knows which transactions to remove from her mempool, taking away an\nexcuse to mine an empty block. And she can still verify most of the\nsignatures too (whatever % could be recovered from the IBLT).\n\nI guess this does not improve the worst adversarial case for IBLT block\npropagation, but it should improve the effectiveness in cases where the\n\"normal\" IBLT would fail to deliver all transactions. Transactions without\nsignatures is better than no transactions at all, for a miner that's eager\nto start on the next block, right? In \"optimal\" cases it would reduce the\nsize of the IBLT.\n\nSorry if this was already suggested.\n\n\n--\nJannes\n\nOn 10 December 2015 at 13:54, Tamas Blummer via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Note that the unused space in coin base input script allows us to\n> soft-fork an additional SW Merkle tree root into the design,\n> therefore please make sure the new SW data structure also has a new slot\n> for future extension.\n>\n> Tamas Blummer\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151212/80703517/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2015-12-13T20:34:48",
                "message_text_only": "Jannes Faber via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> writes:\n> Segregated IBLT\n>\n> I was just wondering if it would make sense when we have SW to also make\n> Segregated IBLT? Segregating transactions from signatures and then tune the\n> parameters such that transactions have a slightly higher guarantee and save\n> a bit of space on the signatures side.\n\nIt just falls out naturally.  If the peer doesn't want the witnesses,\nthey don't get serialized into the IBLT.\n\nCheers,\nRusty."
            },
            {
                "author": "Jonathan Toomim",
                "date": "2015-12-14T11:44:34",
                "message_text_only": "1. I think we should limit the sum of the block and witness data to nBlockMaxSize*7/4 per block, for a maximum of 1.75 MB total. I don't like the idea that SegWit would give us 1.75 MB of capacity in the typical case, but we have to have hardware capable of 4 MB in adversarial conditions (i.e. intentional multisig). I think a limit to the segwit size allays that concern.\n\n2. I think that segwit is a substantial change to how Bitcoin works, and I very strongly believe that we should not rush this. It changes the block structure, it changes the transaction structure, it changes the network protocol, it changes SPV wallet software, it changes block explorers, and it has changes that affect most other parts of the Bitcoin ecosystem. After we decide to implement it, and have a final version of the code that will be merged, we should give developers of other Bitcoin software time to implement code that supports the new transaction/witness formats.\n\nWhen you guys say \"as soon as possible,\" what do you mean exactly?\n\nOn Dec 10, 2015, at 2:47 PM, jl2012--- via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> It seems the current consensus is to implement Segregated Witness. SW opens many new possibilities but we need a balance between new features and deployment time frame. I'm listing by my priority:\n> \n> 1-2 are about scalability and have highest priority\n> \n> 1. Witness size limit: with SW we should allow a bigger overall block size. It seems 2MB is considered to be safe for many people. However, the exact size and growth of block size should be determined based on testing and reasonable projection.\n> \n> 2. Deployment time frame: I prefer as soon as possible, even if none of the following new features are implemented. This is not only a technical issue but also a response to the community which has been waiting for a scaling solution for years\n> \n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151214/7a48cdf9/attachment-0001.sig>"
            },
            {
                "author": "Adam Back",
                "date": "2015-12-14T12:32:01",
                "message_text_only": "I think people have a variety of sequences in mind, eg some would\nprefer to deploy versionBits first, so that subsequent features can be\ndeployed in parallel (currently soft-fork deployment is necessarily\nserialised).\n\nOthers think do seg-witness first because scale is more important.\n\nSome want to do seg-witness as a hard-fork (personally I think that\nwould take a bit longer to deploy & activate - the advantage of\nsoft-fork is that it's lower risk and we have more experience of it).\n\nI've seen a few people want to do BIP 102 first (straight move to 2MB\nand only that) and then do seg-witness and other scaling work later.\nThat's possible also and before Luke observed that you could do a\nseg-witness based block-size increase via soft-fork, people had been\nworking following the summary from the montreal workshop discussion\nposted on this list about a loose plan of action, people had been\nworking on something like BIP 102 to 2-4-8 kind of space, plus\nvalidation cost accounting.\n\nSo I think personally soft-fork seg-witness first, but hey I'm not\nwriting code at present and I'm very happy for wiser and more code and\ndeployment detail aware minds to figure out the best deployment\nstrategy, I wouldnt mind any of the above, just think seg-witness\nsoft-fork is the safest and fastest.  The complexity risk - well on\nthe plus side it is implemented and it reduces deployment risk, and\nit's anyway needed work to have a robust malleability fix which is\nneeded for a whole range of bitcoin smart-contract, and scaling\nfeatures, including for example greenAddress like faster transactions\nas used by BitPay?, BitGo and GreenAddress as well as lightning\nrelated proposals and basically any smart-contract use that involves\nmultiple clauses and dependent transactions.  Also re complexity risk\nGreg has highlighted that the complexity and overhead difference is\nreally minor.  About knock on code changes needed, a bunch of the next\nsteps for Bitcoin are going to need code changes, I think our scope to\nimprove and scale Bitcoin are going to be severely hampered if we\nrestricted ourselves with the pre-condition that we cant make protocol\nimprovements.  I think people in core would be happy to, and have done\nthis kind of thing multiple times in the past, to help people for free\non volunteer time integrate and fix up related code in various\nlanguages and FOSS and commercial software that is affected.\n\nAs to time-line Luke I saw guestimated by march 2016 on reddit.\nOthers maybe be more or less conservative.  Probably a BIP and testing\nare the main thing, and that can be helped by more eyes.  The one\nadvantage of BIP 102 like proposal is simplicity if one had to do a\nmore emergency hard-fork.  Maybe companies and power users, miners and\npool operators could help define some requirements and metrics about\nan industry wide service level they are receiving relative to fees.\n\nThe other thing which is not protocol related, is that companies can\nhelp themselves and help Bitcoin developers help them, by working to\nimprove decentralisation with better configurations, more use of\nself-hosted and secured full nodes, and decentralisation of policy\ncontrol over hashrate.  That might even include buying a nominal (to a\nreasonably funded startup) amount of mining equipment.  Or for power\nusers to do more of that.  Some developers are doing mining.\nBlockstream and some employees have a little bit of hashrate.  If we\ncould define some metrics and best practices and measure the\nimprovements, that would maybe reduce miners concerns about\ncentralisation risk and allow a bigger block faster, alongside the\nIBLT & weak block network protocol improvements.\n\nAdam\n\nOn 14 December 2015 at 19:44, Jonathan Toomim via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> 1. I think we should limit the sum of the block and witness data to nBlockMaxSize*7/4 per block, for a maximum of 1.75 MB total. I don't like the idea that SegWit would give us 1.75 MB of capacity in the typical case, but we have to have hardware capable of 4 MB in adversarial conditions (i.e. intentional multisig). I think a limit to the segwit size allays that concern.\n>\n> 2. I think that segwit is a substantial change to how Bitcoin works, and I very strongly believe that we should not rush this. It changes the block structure, it changes the transaction structure, it changes the network protocol, it changes SPV wallet software, it changes block explorers, and it has changes that affect most other parts of the Bitcoin ecosystem. After we decide to implement it, and have a final version of the code that will be merged, we should give developers of other Bitcoin software time to implement code that supports the new transaction/witness formats.\n>\n> When you guys say \"as soon as possible,\" what do you mean exactly?\n>\n> On Dec 10, 2015, at 2:47 PM, jl2012--- via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> It seems the current consensus is to implement Segregated Witness. SW opens many new possibilities but we need a balance between new features and deployment time frame. I'm listing by my priority:\n>>\n>> 1-2 are about scalability and have highest priority\n>>\n>> 1. Witness size limit: with SW we should allow a bigger overall block size. It seems 2MB is considered to be safe for many people. However, the exact size and growth of block size should be determined based on testing and reasonable projection.\n>>\n>> 2. Deployment time frame: I prefer as soon as possible, even if none of the following new features are implemented. This is not only a technical issue but also a response to the community which has been waiting for a scaling solution for years"
            },
            {
                "author": "Jonathan Toomim",
                "date": "2015-12-14T12:50:51",
                "message_text_only": "Off-topic: If you want to decentralize hashing, the best solution is probably to redesign p2pool to use DAGs. p2pool would be great except for the fact that the 30 sec share times are (a) long enough to cause significant reward variance for miners, but (b) short enough to cause hashrate loss from frequent switching on hardware that wasn't designed for it (e.g. Antminers, KNC) and (c) uneven rewards to different miners due to share orphan rates. DAGs can fix all of those issues. I had a talk with some medium-sized Chinese miners on Thursday in which I told them about p2pool, and I got the impression that they would prefer it over their existing pools due to the 0% fees and trustless design if the performance issues were fixed. If anybody is interested in helping with this work, ping me or Bob McElrath backchannel to be included in our conversation.\n\n\nOn Dec 14, 2015, at 8:32 PM, Adam Back <adam at cypherspace.org> wrote:\n\n> The other thing which is not protocol related, is that companies can\n> help themselves and help Bitcoin developers help them, by working to\n> improve decentralisation with better configurations, more use of\n> self-hosted and secured full nodes, and decentralisation of policy\n> control over hashrate.  That might even include buying a nominal (to a\n> reasonably funded startup) amount of mining equipment.  Or for power\n> users to do more of that.  Some developers are doing mining.\n> Blockstream and some employees have a little bit of hashrate.  If we\n> could define some metrics and best practices and measure the\n> improvements, that would maybe reduce miners concerns about\n> centralisation risk and allow a bigger block faster, alongside the\n> IBLT & weak block network protocol improvements.\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151214/8a06afe6/attachment-0001.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151214/8a06afe6/attachment-0001.sig>"
            }
        ],
        "thread_summary": {
            "title": "Segregated Witness features wish list",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Rusty Russell",
                "Bryan Bishop",
                "Tamas Blummer",
                "Adam Back",
                "Jonathan Toomim",
                "Gregory Maxwell",
                "Jannes Faber",
                "jl2012 at xbt.hk",
                "Pieter Wuille"
            ],
            "messages_count": 13,
            "total_messages_chars_count": 33605
        }
    },
    {
        "title": "[bitcoin-dev] Not this again.",
        "thread_messages": [
            {
                "author": "satoshi at vistomail.com",
                "date": "2015-12-10T06:54:46",
                "message_text_only": "I am not Craig Wright. We are all Satoshi."
            }
        ],
        "thread_summary": {
            "title": "Not this again.",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "satoshi at vistomail.com"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 42
        }
    },
    {
        "title": "[bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin",
        "thread_messages": [
            {
                "author": "jl2012 at xbt.hk",
                "date": "2015-12-12T20:09:03",
                "message_text_only": "It is a common practice in commercial banks that a dormant account might \nbe confiscated. Confiscating or deleting dormant UTXOs might be too \ncontroversial, but allowing the UTXOs set growing without any limit \nmight not be a sustainable option. People lose their private keys. \nPeople do stupid things like sending bitcoin to 1BitcoinEater. We \nshouldn\u2019t be obliged to store everything permanently. This is my \nproposal:\n\nDormant UTXOs are those UTXOs with 420000 confirmations. In every block \nX after 420000, it will commit to a hash for all UTXOs generated in \nblock X-420000. The UTXOs are first serialized into the form: \ntxid|index|value|scriptPubKey, then a sorted Merkle hash is calculated. \nAfter some confirmations, nodes may safely delete the UTXO records of \nblock X permanently.\n\nIf a user is trying to redeem a dormant UTXO, in addition the signature, \nthey have to provide the scriptPubKey, height (X), and UTXO value as \npart of the witness. They also need to provide the Merkle path to the \ndormant UTXO commitment.\n\nTo confirm this tx, the miner will calculate a new Merkle hash for the \nblock X, with the hash of the spent UTXO replaced by 1, and commit the \nhash to the current block. All full nodes will keep an index of latest \ndormant UTXO commitments so double spending is not possible. (a \n\"meta-UTXO set\")\n\nIf all dormant UTXOs under a Merkle branch are spent, hash of the branch \nwill become 1. If all dormant UTXOs in a block are spent, the record for \nthis block could be forgotten. Full nodes do not need to remember which \nparticular UTXO is spent or not, since any person trying to redeem a \ndormant UTXO has to provide such information.\n\nIt becomes the responsibility of dormant coin holders to scan the \nblockchain for the current status of the UTXO commitment for their coin. \nThey may also need to pay extra fee for the increased tx size.\n\nThis is a softfork if there is no hash collision but this is a \nfundamental assumption in Bitcoin anyway. The proposal also works \nwithout segregated witness, just by replacing \"witness\" with \"scriptSig\""
            },
            {
                "author": "gb",
                "date": "2015-12-12T23:01:09",
                "message_text_only": "The general concept has merit and the basic outline here seems sound\nenough. I have harboured a notion for having \"archived UTXO\" for some\ntime, this is essentially it. The retrieval from archive cost is on the\nUTXO holder not the entire storage network, which is then only bearing\nfull 'instant' retrieval costs for N blocks.\n\nOn Sat, 2015-12-12 at 15:09 -0500, jl2012--- via bitcoin-dev wrote:\n> It is a common practice in commercial banks that a dormant account might \n> be confiscated. Confiscating or deleting dormant UTXOs might be too \n> controversial, but allowing the UTXOs set growing without any limit \n> might not be a sustainable option. People lose their private keys. \n> People do stupid things like sending bitcoin to 1BitcoinEater. We \n> shouldn\u2019t be obliged to store everything permanently. This is my \n> proposal:\n> \n> Dormant UTXOs are those UTXOs with 420000 confirmations. In every block \n> X after 420000, it will commit to a hash for all UTXOs generated in \n> block X-420000. The UTXOs are first serialized into the form: \n> txid|index|value|scriptPubKey, then a sorted Merkle hash is calculated. \n> After some confirmations, nodes may safely delete the UTXO records of \n> block X permanently.\n> \n> If a user is trying to redeem a dormant UTXO, in addition the signature, \n> they have to provide the scriptPubKey, height (X), and UTXO value as \n> part of the witness. They also need to provide the Merkle path to the \n> dormant UTXO commitment.\n> \n> To confirm this tx, the miner will calculate a new Merkle hash for the \n> block X, with the hash of the spent UTXO replaced by 1, and commit the \n> hash to the current block. All full nodes will keep an index of latest \n> dormant UTXO commitments so double spending is not possible. (a \n> \"meta-UTXO set\")\n> \n> If all dormant UTXOs under a Merkle branch are spent, hash of the branch \n> will become 1. If all dormant UTXOs in a block are spent, the record for \n> this block could be forgotten. Full nodes do not need to remember which \n> particular UTXO is spent or not, since any person trying to redeem a \n> dormant UTXO has to provide such information.\n> \n> It becomes the responsibility of dormant coin holders to scan the \n> blockchain for the current status of the UTXO commitment for their coin. \n> They may also need to pay extra fee for the increased tx size.\n> \n> This is a softfork if there is no hash collision but this is a \n> fundamental assumption in Bitcoin anyway. The proposal also works \n> without segregated witness, just by replacing \"witness\" with \"scriptSig\"\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Vincent Truong",
                "date": "2015-12-13T01:00:26",
                "message_text_only": "Dormant threshold is way too low. There's many news articles about people\nforgetting that they used to mine bitcoins and then suddenly remembered.\nThis will continue to happen for much longer than 8 years as people\nrediscover bitcoin when it goes further mainstream. You can't expect them\nto have run a node/kept their utxo before they were aware of this change\nand then realise miners have discarded their utxo. Oops?\n\nSince we can't predict when mainstream will happen, you instead need a\nthreshold where the key holder is likely dead. That should be like 80 years\nor 120 years, so 4.2m to 6.3m confirmations.\n\nNext paragraph is off topic:\n\nIMO it would be even better for these dormant & dead key holder's utxos to\nalso re-enter the economy as miner fees; let 1 dormant utxo to be mined per\nblock. It would need a hard fork. But then maybe people would stop being so\nstupid with burning bitcoins/sending it to 1BitcoinEater, or mining a\nmillion bitcoins from day 1 and leaving it, if they know it'll eventually\ngo into another miner's pockets. This could be used to fund cheap\ntransactions forever, and miners would be incentivised to hold copies of\nthese dormant utxos since it could become theirs one day. But this would be\neven more controversial than just expiring them as we are in no short\nsupply of people who believe in Bitcoin's deflationary, fossil fuel\n(burnable) economy, rather than a cyclical economy that better resembles\nhow we treat lost gold today...\nOn Dec 13, 2015 10:29 AM, \"gb via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> The general concept has merit and the basic outline here seems sound\n> enough. I have harboured a notion for having \"archived UTXO\" for some\n> time, this is essentially it. The retrieval from archive cost is on the\n> UTXO holder not the entire storage network, which is then only bearing\n> full 'instant' retrieval costs for N blocks.\n>\n> On Sat, 2015-12-12 at 15:09 -0500, jl2012--- via bitcoin-dev wrote:\n> > It is a common practice in commercial banks that a dormant account might\n> > be confiscated. Confiscating or deleting dormant UTXOs might be too\n> > controversial, but allowing the UTXOs set growing without any limit\n> > might not be a sustainable option. People lose their private keys.\n> > People do stupid things like sending bitcoin to 1BitcoinEater. We\n> > shouldn\u2019t be obliged to store everything permanently. This is my\n> > proposal:\n> >\n> > Dormant UTXOs are those UTXOs with 420000 confirmations. In every block\n> > X after 420000, it will commit to a hash for all UTXOs generated in\n> > block X-420000. The UTXOs are first serialized into the form:\n> > txid|index|value|scriptPubKey, then a sorted Merkle hash is calculated.\n> > After some confirmations, nodes may safely delete the UTXO records of\n> > block X permanently.\n> >\n> > If a user is trying to redeem a dormant UTXO, in addition the signature,\n> > they have to provide the scriptPubKey, height (X), and UTXO value as\n> > part of the witness. They also need to provide the Merkle path to the\n> > dormant UTXO commitment.\n> >\n> > To confirm this tx, the miner will calculate a new Merkle hash for the\n> > block X, with the hash of the spent UTXO replaced by 1, and commit the\n> > hash to the current block. All full nodes will keep an index of latest\n> > dormant UTXO commitments so double spending is not possible. (a\n> > \"meta-UTXO set\")\n> >\n> > If all dormant UTXOs under a Merkle branch are spent, hash of the branch\n> > will become 1. If all dormant UTXOs in a block are spent, the record for\n> > this block could be forgotten. Full nodes do not need to remember which\n> > particular UTXO is spent or not, since any person trying to redeem a\n> > dormant UTXO has to provide such information.\n> >\n> > It becomes the responsibility of dormant coin holders to scan the\n> > blockchain for the current status of the UTXO commitment for their coin.\n> > They may also need to pay extra fee for the increased tx size.\n> >\n> > This is a softfork if there is no hash collision but this is a\n> > fundamental assumption in Bitcoin anyway. The proposal also works\n> > without segregated witness, just by replacing \"witness\" with \"scriptSig\"\n> >\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151213/8ec46cd1/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-12-13T02:07:36",
                "message_text_only": "On Sun, Dec 13, 2015 at 1:00 AM, Vincent Truong via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> have run a node/kept their utxo before they were aware of this change and\n> then realise miners have discarded their utxo. Oops?\n\nI believe you have misunderstood jl2012's post.  His post does not\ncause the outputs to become discarded. They are still spendable,\nbut the transactions must carry a membership proof to spend them.\nThey don't have to have stored the data themselves, but they must\nget it from somewhere-- including archive nodes that serve this\npurpose rather than having every full node carry all that data forever.\n\nPlease be conservative with the send button. The list loses its\nutility if every moderately complex idea is hit with reflexive\nopposition by people who don't understand it.\n\nPeter Todd has proposed something fairly similar with \"STXO\ncommitments\". The primary argument against this kind of approach that\nI'm aware of is that the membership proofs get pretty big, and if too\naggressive this trades bandwidth for storage, and storage is usually\nthe cheaper resource. Though at least the membership proofs could be\nomitted when transmitting to a node which has signaled that it has\nkept the historical data anyways."
            },
            {
                "author": "Chris Priest",
                "date": "2015-12-13T08:13:42",
                "message_text_only": "I don't like this scheme at all. It doesn't seem to make bitcoin\nbetter, it makes it worse.\n\nLets say it's 2050 and I want to sweep a paper wallet I created in\n2013. I can't just make the TX and send it to the network, I have to\nfirst contact an \"archive node\" to get the UTXO data in order to make\nthe TX. How is this better than how the system works today?\n\nSince many people are going to be holding BTC long term (store of\nvalue of a first-class feature of bitcoin), this scheme is going to\neffect pretty much all users.\n\nThese archive nodes will be essential to network's operation. If there\nare no running archive nodes, the effect on the network is the same as\nthe network today without any full nodes.\n\nAnyways, UTXO size is a function of number of users, rather than a\nfunction of time. If tons of people join the network, UTXO still will\nincrease no matter what. All this change is going to do is make it\nharder for people to use bitcoin. A person can still generate 1GB of\nUTXO data, but as long as they spend those UTXOs within the amount\nthey are still using those resources.\n\nIMO, wildcard inputs is still the best way to limit the UTXO set.\n\n\nOn 12/12/15, Gregory Maxwell via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> On Sun, Dec 13, 2015 at 1:00 AM, Vincent Truong via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> have run a node/kept their utxo before they were aware of this change and\n>> then realise miners have discarded their utxo. Oops?\n>\n> I believe you have misunderstood jl2012's post.  His post does not\n> cause the outputs to become discarded. They are still spendable,\n> but the transactions must carry a membership proof to spend them.\n> They don't have to have stored the data themselves, but they must\n> get it from somewhere-- including archive nodes that serve this\n> purpose rather than having every full node carry all that data forever.\n>\n> Please be conservative with the send button. The list loses its\n> utility if every moderately complex idea is hit with reflexive\n> opposition by people who don't understand it.\n>\n> Peter Todd has proposed something fairly similar with \"STXO\n> commitments\". The primary argument against this kind of approach that\n> I'm aware of is that the membership proofs get pretty big, and if too\n> aggressive this trades bandwidth for storage, and storage is usually\n> the cheaper resource. Though at least the membership proofs could be\n> omitted when transmitting to a node which has signaled that it has\n> kept the historical data anyways.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-12-13T08:18:57",
                "message_text_only": "On Sun, Dec 13, 2015 at 8:13 AM, Chris Priest <cp368202 at ohiou.edu> wrote:\n> Lets say it's 2050 and I want to sweep a paper wallet I created in\n> 2013. I can't just make the TX and send it to the network, I have to\n> first contact an \"archive node\" to get the UTXO data in order to make\n> the TX. How is this better than how the system works today?\n\nYou already are in that boat. If your paper wallet has only the\nprivate key (as 100% of them do today). You'll have no idea what coins\nhave been assigned to it, or what their TXids are. You'll need to\ncontact a public index (which isn't a service existing nodes provide)\nor synchronize the full blockchain history to find it. Both are also\nsufficient for jl2012's (/Petertodd's STXO), they'd only be providing\nyou with somewhat more data.  If instead, you insist that you'd\nalready be running a full node and not have to wait for the sync, then\nagain you'd also be your own archive. In none of these cases do you\nlose anything."
            },
            {
                "author": "Chris Priest",
                "date": "2015-12-13T09:17:38",
                "message_text_only": "> In none of these cases do you lose anything.\n\nNor do you gain anything. Archive nodes will still need to exist\nprecisely because paper wallets don't include UTXO data. This is like\nadding the ability to partially seed a movie with bittorrent. You\nstill need someone who has the whole thing has to be participating in\norder for anyone to play the movie.\n\nThis isn't going to kill bitcoin, but it won't make it any better.\nEvery paper wallet would have to be re-printed with UTXO data\nincluded. It doesn't even solve the core problem because someone can\nstill flood the network with lots of UTXOs, as long as they spend them\nquickly.\n\nOn 12/13/15, Gregory Maxwell <greg at xiph.org> wrote:\n> On Sun, Dec 13, 2015 at 8:13 AM, Chris Priest <cp368202 at ohiou.edu> wrote:\n>> Lets say it's 2050 and I want to sweep a paper wallet I created in\n>> 2013. I can't just make the TX and send it to the network, I have to\n>> first contact an \"archive node\" to get the UTXO data in order to make\n>> the TX. How is this better than how the system works today?\n>\n> You already are in that boat. If your paper wallet has only the\n> private key (as 100% of them do today). You'll have no idea what coins\n> have been assigned to it, or what their TXids are. You'll need to\n> contact a public index (which isn't a service existing nodes provide)\n> or synchronize the full blockchain history to find it. Both are also\n> sufficient for jl2012's (/Petertodd's STXO), they'd only be providing\n> you with somewhat more data.  If instead, you insist that you'd\n> already be running a full node and not have to wait for the sync, then\n> again you'd also be your own archive. In none of these cases do you\n> lose anything.\n>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-12-13T09:24:57",
                "message_text_only": "On Sun, Dec 13, 2015 at 9:17 AM, Chris Priest <cp368202 at ohiou.edu> wrote:\n>> In none of these cases do you lose anything.\n>\n> Nor do you gain anything. Archive nodes will still need to exist\n\nNot every node is an archive node; that's even the case today.\nLowering the resource requirements to independently enforce the rules\nof the system is highly virtuous.\n\n> precisely because paper wallets don't include UTXO data. This is like\n> adding the ability to partially seed a movie with bittorrent.\n[...]\n> Every paper wallet would have to be re-printed with UTXO data\n\nThey are not printed now with UTXO data\n(txid:vout:scriptpubkey:amount), and unless you start and fully\nsynchronize (or are running a full node) you already cannot author a\ntransaction without that data. The private key is already not enough,\nand no Bitcoin node will just give you what you need to know.\n\nThe only additional information JL2012's scheme would add would be the\nhash tree fragments to show membership; and the same places that\ncurrently give you what is required to author a transaction could\nprovide it for you.\n\n> included. It doesn't even solve the core problem because someone can\n> still flood the network with lots of UTXOs, as long as they spend them\n> quickly.\n\nThe system already inhibits the rate new UTXO can be added; but we're\nstill left with the perpetually growing history that contains many\nlost and otherwise unspendable outputs."
            },
            {
                "author": "jl2012 at xbt.hk",
                "date": "2015-12-13T18:11:41",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\n\nOn Mon, Dec 14, 2015 at 12:14 AM, Danny Thorpe <danny.thorpe at gmail.com> \nwrote:\n> What is the current behavior / cost that this proposal is trying to \n> avoid? Are ancient utxos required to be kept in memory always in a \n> fully validating node, or can ancient utxos get pushed out of memory \n> like a normal LRU caching db?\n\nI don't see why it must be kept in memory. But storage is still a \nproblem. With the 8 year limit and a fixed max block size, it indirectly \nsets an upper limit for UTXO set.\n\n\nChris Priest via bitcoin-dev :\n> This isn't going to kill bitcoin, but it won't make it any better.\n\nDo you believe that thousands of volunteer full nodes are obliged to \nstore an UTXO record, just because one paid US$0.01 to an anonymous \nminer 100 years ago? It sounds insanely cheap, isn't it? My proposal (or \nsimilar proposal by Peter Todd) is to solve this problem. Many \ncommercial banks have a dormant threshold less than 8 years so I believe \nit is a balanced choice.\n\nBack to the topic, I would like to further elaborate my proposal.\n\nWe have 3 types of full nodes:\n\nArchive nodes: full nodes that store the whole blockchain\nFull UTXO nodes: full nodes that fully store the latest UTXO state, but \nnot the raw blockchain\nLite UTXO nodes: full nodes that store only UTXO created in that past \n420000 blocks\n\nCurrently, if one holds nothing but a private key, he must consult \neither an archive node or a full UTXO node for the latest UTXO state to \nspend his coin. We currently do not have any lite UTXO node, and such \nnode would not work properly beyond block 420000.\n\nWith the softfork I described in my original post, if the UTXO is \ncreated within the last 420000 blocks, the key holder may consult any \ntype of full node, including a lite UTXO node, to create the \ntransaction.\n\nIf the UTXO has been confirmed by more than 420000 blocks, a lite UTXO \nnode obviously can't provide the necessary information to spend the \ncoin. However, not even a full UTXO node may do so. A full UTXO node \ncould tell the position of the UTXO in the blockchain, but can't provide \nall the information required by my specification. Only an archive node \nmay do so.\n\nWhat extra information is needed?\n\n(1) If your UTXO was generated in block Y, you first need to know the \nTXO state (spent / unspent) of all outputs in block Y at block (Y + \n420000). Only UTXOs at that time are relevant.\n\n(2) You also need to know if there was any spending of any block Y UTXOs \nafter block (Y + 420000).\n\nIt is not possible to construct the membership prove I require without \nthese information. It is designed this way, so that lite UTXO nodes \nwon't need to store any dormant UTXO records: not even the hash of \nindividual dormant UTXO records. If the blockchain grows to insanely \nbig, it may take days or weeks to retrieve to records. However, I don't \nthink this is relevant as one has already left his coins dormant for >8 \nyears. Actually, you don't even need the full blockchain. For (1), all \nyou need is the 420000 blocks from Y to Y+420000 minus any witness data, \nas you don't need to do any validation. For (2), you just need the \ncoinbase of Y+420001 to present, where any spending would have been \ncommitted, and retrieve the full block only if a spending is found.\n\nSo the Bitcoin Bank (miners) is not going to shred your record and \nconfiscate your money. Instead, the Bank throws your record to the \ngarage (raw blockchain). You can search for your record by yourself, or \nemploy someone (archive node) to search it for you. In any case it \nincurs costs. But as thousands of bankers have kept your record on their \nlimited desk space for 8 years for free (though one of them might \nreceive a fraction of a penny from you), you shouldn't complain with any \nmoral, technical, or legal reason. And no matter what users say, I \nbelieve something like this will happen when miners and full nodes can't \nhandle the UTXO set.\n\nI'd like to see more efficient proposals that archive the same goals.\n\np.s. there were some typos in my original. The second sentence of the \nsecond paragraph should be read as \"For every block X+420000, it will \ncommit to a hash for all UTXOs generated in block X.\"\n-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v2\n\niQGcBAEBCAAGBQJWbbR2AAoJEO6eVSA0viTScEoL/RPlsxr0A5wTtgdi+9i4AFlV\nSw/He89+YPGe5VCG74YNAPLEUF1/rICzUJ4DulvNTOo/5xtmkv5ok4bD7v1JZnH3\nDE2PExMQYs2X4Qm6mkcwi8IWlMR2U5j5ebUq21Kj4AqVFj9UcQmYGhPehB2f+cM9\nWki/TDwNj5fV8AZ4uR9pPgaf+bvVQQ9BOOLiIMiTbphNCx1hfGfYcsqmXlCbGk9A\nPatGR88aQTxpa7PhbCZwwf76cKuOaYYZeHr9jRR9RL5rZVXgE1SI/niBytJhXaP8\nlwYtk4Bpz0IGd23v1dArNQQoOp5Xycbeq1l1qyv/qtxju65No+dhqiEcFBZVI1AS\nVcndMQ+yvNuxVgib2Ifh9YjXelWAqqLzzoVcz2RxXh6HJ0tVKxBokwdAcsclZb93\nzQ1JhDR4vBpLquytZA8lDIxJraNCdB/KEAOAey6ljP3zL7fBLBp1oZw4DDDtFy8V\nEMjrOSVnjyuyfey2YXsGnnHuQS0mpwmSroV2400uGQ==\n=2xRy\n-----END PGP SIGNATURE-----"
            },
            {
                "author": "Ricardo Filipe",
                "date": "2015-12-13T21:20:21",
                "message_text_only": "I really like ideas that tackle this issue. The question imho is what is\nthe incentive to run a \"Full UTXO node\" instead of a pruned or archive node.\nFor starters, it would be nice to know what would be the savings for Full\nUTXO nodes over archive nodes right now.\nAlso, what advantages would this have over \"archive pruned nodes: nodes\nthat store X blocks of the whole blockchain before 420000\". Seems like an\ninteresting intermediate use case to me too.\n\n2015-12-13 18:11 GMT+00:00 jl2012--- via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org>:\n\n> -----BEGIN PGP SIGNED MESSAGE-----\n> Hash: SHA256\n>\n> On Mon, Dec 14, 2015 at 12:14 AM, Danny Thorpe <danny.thorpe at gmail.com>\n> wrote:\n>\n>> What is the current behavior / cost that this proposal is trying to\n>> avoid? Are ancient utxos required to be kept in memory always in a fully\n>> validating node, or can ancient utxos get pushed out of memory like a\n>> normal LRU caching db?\n>>\n>\n> I don't see why it must be kept in memory. But storage is still a problem.\n> With the 8 year limit and a fixed max block size, it indirectly sets an\n> upper limit for UTXO set.\n>\n>\n> Chris Priest via bitcoin-dev :\n>\n>> This isn't going to kill bitcoin, but it won't make it any better.\n>>\n>\n> Do you believe that thousands of volunteer full nodes are obliged to store\n> an UTXO record, just because one paid US$0.01 to an anonymous miner 100\n> years ago? It sounds insanely cheap, isn't it? My proposal (or similar\n> proposal by Peter Todd) is to solve this problem. Many commercial banks\n> have a dormant threshold less than 8 years so I believe it is a balanced\n> choice.\n>\n> Back to the topic, I would like to further elaborate my proposal.\n>\n> We have 3 types of full nodes:\n>\n> Archive nodes: full nodes that store the whole blockchain\n> Full UTXO nodes: full nodes that fully store the latest UTXO state, but\n> not the raw blockchain\n> Lite UTXO nodes: full nodes that store only UTXO created in that past\n> 420000 blocks\n>\n> Currently, if one holds nothing but a private key, he must consult either\n> an archive node or a full UTXO node for the latest UTXO state to spend his\n> coin. We currently do not have any lite UTXO node, and such node would not\n> work properly beyond block 420000.\n>\n> With the softfork I described in my original post, if the UTXO is created\n> within the last 420000 blocks, the key holder may consult any type of full\n> node, including a lite UTXO node, to create the transaction.\n>\n> If the UTXO has been confirmed by more than 420000 blocks, a lite UTXO\n> node obviously can't provide the necessary information to spend the coin.\n> However, not even a full UTXO node may do so. A full UTXO node could tell\n> the position of the UTXO in the blockchain, but can't provide all the\n> information required by my specification. Only an archive node may do so.\n>\n> What extra information is needed?\n>\n> (1) If your UTXO was generated in block Y, you first need to know the TXO\n> state (spent / unspent) of all outputs in block Y at block (Y + 420000).\n> Only UTXOs at that time are relevant.\n>\n> (2) You also need to know if there was any spending of any block Y UTXOs\n> after block (Y + 420000).\n>\n> It is not possible to construct the membership prove I require without\n> these information. It is designed this way, so that lite UTXO nodes won't\n> need to store any dormant UTXO records: not even the hash of individual\n> dormant UTXO records. If the blockchain grows to insanely big, it may take\n> days or weeks to retrieve to records. However, I don't think this is\n> relevant as one has already left his coins dormant for >8 years. Actually,\n> you don't even need the full blockchain. For (1), all you need is the\n> 420000 blocks from Y to Y+420000 minus any witness data, as you don't need\n> to do any validation. For (2), you just need the coinbase of Y+420001 to\n> present, where any spending would have been committed, and retrieve the\n> full block only if a spending is found.\n>\n> So the Bitcoin Bank (miners) is not going to shred your record and\n> confiscate your money. Instead, the Bank throws your record to the garage\n> (raw blockchain). You can search for your record by yourself, or employ\n> someone (archive node) to search it for you. In any case it incurs costs.\n> But as thousands of bankers have kept your record on their limited desk\n> space for 8 years for free (though one of them might receive a fraction of\n> a penny from you), you shouldn't complain with any moral, technical, or\n> legal reason. And no matter what users say, I believe something like this\n> will happen when miners and full nodes can't handle the UTXO set.\n>\n> I'd like to see more efficient proposals that archive the same goals.\n>\n> p.s. there were some typos in my original. The second sentence of the\n> second paragraph should be read as \"For every block X+420000, it will\n> commit to a hash for all UTXOs generated in block X.\"\n> -----BEGIN PGP SIGNATURE-----\n> Version: GnuPG v2\n>\n> iQGcBAEBCAAGBQJWbbR2AAoJEO6eVSA0viTScEoL/RPlsxr0A5wTtgdi+9i4AFlV\n> Sw/He89+YPGe5VCG74YNAPLEUF1/rICzUJ4DulvNTOo/5xtmkv5ok4bD7v1JZnH3\n> DE2PExMQYs2X4Qm6mkcwi8IWlMR2U5j5ebUq21Kj4AqVFj9UcQmYGhPehB2f+cM9\n> Wki/TDwNj5fV8AZ4uR9pPgaf+bvVQQ9BOOLiIMiTbphNCx1hfGfYcsqmXlCbGk9A\n> PatGR88aQTxpa7PhbCZwwf76cKuOaYYZeHr9jRR9RL5rZVXgE1SI/niBytJhXaP8\n> lwYtk4Bpz0IGd23v1dArNQQoOp5Xycbeq1l1qyv/qtxju65No+dhqiEcFBZVI1AS\n> VcndMQ+yvNuxVgib2Ifh9YjXelWAqqLzzoVcz2RxXh6HJ0tVKxBokwdAcsclZb93\n> zQ1JhDR4vBpLquytZA8lDIxJraNCdB/KEAOAey6ljP3zL7fBLBp1oZw4DDDtFy8V\n> EMjrOSVnjyuyfey2YXsGnnHuQS0mpwmSroV2400uGQ==\n> =2xRy\n> -----END PGP SIGNATURE-----\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151213/67eaba28/attachment.html>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-12-13T21:36:06",
                "message_text_only": "On Sun, Dec 13, 2015 at 6:11 PM, jl2012--- via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Back to the topic, I would like to further elaborate my proposal.\n>\n> We have 3 types of full nodes:\n>\n> Archive nodes: full nodes that store the whole blockchain\n> Full UTXO nodes: full nodes that fully store the latest UTXO state, but\n> not the raw blockchain\n> Lite UTXO nodes: full nodes that store only UTXO created in that past\n> 420000 blocks\n>\n\nThere is a risk that miners would eventually react by just refusing to\naccept blocks that spend dormant outputs.  This is a risk even without the\nprotocol, but I think if there are already lots of UTXO-lite nodes\ndeployed, it would be much easier to just define them as the new\n(soft-forked) consensus rule.\n\nThere is a precedent for things to be disabled rather than fixed when\nsecurity problems arise.\n\nImagine a crisis caused by a security related bug with the revival proofs.\nDisabling them is much lower risk than trying to find/fix the bug and then\ndeploy the fix.  The longer it takes, the longer the security problem\nremains.\n\n\n>\n> What extra information is needed?\n>\n> (1) If your UTXO was generated in block Y, you first need to know the TXO\n> state (spent / unspent) of all outputs in block Y at block (Y + 420000).\n> Only UTXOs at that time are relevant.\n>\n> (2) You also need to know if there was any spending of any block Y UTXOs\n> after block (Y + 420000).\n>\n\nIs this how it works?\n\nSource transaction is included in block Y.\n\nIf the output is spent before Y + 420,000, then no further action is taken.\n\nThe miner for block Y + 420,000 will include a commitment to\nmerkle_hash(Block Y's unspent outputs).\n\nIt is possible for someone to prove that they didn't spend their\ntransaction before Y + 420,000.\n\nI think the miners have to remember the \"live\" UTXO merkle root for every\nblock?\n\nWith the path to the UTXO and the miner can recalculate the root for that\nblock.\n\nIf there were 20 dormant outputs being spent, then the miner would have to\ncommit to 20 updates.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151213/a4cf6d21/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-12-20T11:24:54",
                "message_text_only": "On Sun, Dec 13, 2015 at 02:07:36AM +0000, Gregory Maxwell via bitcoin-dev wrote:\n> On Sun, Dec 13, 2015 at 1:00 AM, Vincent Truong via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > have run a node/kept their utxo before they were aware of this change and\n> > then realise miners have discarded their utxo. Oops?\n> \n> I believe you have misunderstood jl2012's post.  His post does not\n> cause the outputs to become discarded. They are still spendable,\n> but the transactions must carry a membership proof to spend them.\n> They don't have to have stored the data themselves, but they must\n> get it from somewhere-- including archive nodes that serve this\n> purpose rather than having every full node carry all that data forever.\n> \n> Please be conservative with the send button. The list loses its\n> utility if every moderately complex idea is hit with reflexive\n> opposition by people who don't understand it.\n> \n> Peter Todd has proposed something fairly similar with \"STXO\n> commitments\". The primary argument against this kind of approach that\n\nThat's incorrect terminology - what I proposed are \"TXO commitments\". I\nproposed that a MMR of all prior transaction outputs's, spent and\nunspent, be committed too in blocks along with a spentness flag, not\njust spent transaction outputs.\n\nThat's why I often use the term (U)TXO commitments to refer to both\nclasses of proposals.\n\n> I'm aware of is that the membership proofs get pretty big, and if too\n> aggressive this trades bandwidth for storage, and storage is usually\n> the cheaper resource. Though at least the membership proofs could be\n> omitted when transmitting to a node which has signaled that it has\n> kept the historical data anyways.\n\nWhat I proprosed is that a consensus-critical maximum UTXO age be part\nof the protocol; UTXO's younger than that age are expected to be cached.\nFor UTXO's older than that age, they can be dropped from the cache,\nhowever to spend them you are required to provide the proof, and that\nproof counts as blockchain space to account for the fact that they do\nneed to be broadcast on the network.\n\nThe proofs are relatively large, but not so much larger than a CTxIn as\nto make paying for that data infeasible.\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/ee109d4b/attachment.sig>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-12-20T11:34:29",
                "message_text_only": "On Sun, Dec 20, 2015 at 6:24 AM, Peter Todd via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> What I proprosed is that a consensus-critical maximum UTXO age be part\n> of the protocol; UTXO's younger than that age are expected to be cached.\n> For UTXO's older than that age, they can be dropped from the cache,\n> however to spend them you are required to provide the proof, and that\n> proof counts as blockchain space to account for the fact that they do\n> need to be broadcast on the network.\n\n\nYes, this is almost what -has- to happen in the long term.\n\nIdeally we should start having wallets generate those proofs now, and then\nintroduce the max-age as a second step as a planned hard fork a couple\nyears down the line.\n\nHowever,\n1) There is also the open question of \"grandfathered\" UTXOs - for those\nwallets generated in 2009, buried in a landfill and then dug out 10 years\nago\n\n2) This reverses the useful minimization attribute of HD wallets - \"just\nbackup the seed\"\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/849d17b4/attachment.html>"
            },
            {
                "author": "s7r",
                "date": "2015-12-20T11:43:15",
                "message_text_only": "What will be the actual effect over wallets?\n\nSay I have the private key for a dormant UTXO older than the\nconsensus-critical maximum UTXO age. The UTXO is not part of the cache.\nSo I setup a full node and import my old private key (wallet.dat). Will\nI even see the correct balance (where will it get if from, since it's\ndropped from the cache), or it will show me a 0 balance?\n\nIf I can see the correct balance, where can I get the proof I need and\nwhat ensures I'll always be able to get that proof?\n\nOn 12/20/2015 1:34 PM, Jeff Garzik via bitcoin-dev wrote:\n> On Sun, Dec 20, 2015 at 6:24 AM, Peter Todd via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org\n> <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n> \n>     What I proprosed is that a consensus-critical maximum UTXO age be part\n>     of the protocol; UTXO's younger than that age are expected to be cached.\n>     For UTXO's older than that age, they can be dropped from the cache,\n>     however to spend them you are required to provide the proof, and that\n>     proof counts as blockchain space to account for the fact that they do\n>     need to be broadcast on the network.\n> \n> \n> Yes, this is almost what -has- to happen in the long term.\n> \n> Ideally we should start having wallets generate those proofs now, and\n> then introduce the max-age as a second step as a planned hard fork a\n> couple years down the line.\n> \n> However,\n> 1) There is also the open question of \"grandfathered\" UTXOs - for those\n> wallets generated in 2009, buried in a landfill and then dug out 10\n> years ago\n> \n> 2) This reverses the useful minimization attribute of HD wallets - \"just\n> backup the seed\"\n> \n>"
            },
            {
                "author": "Chris Pacia",
                "date": "2015-12-20T16:30:45",
                "message_text_only": "On Dec 20, 2015 6:34 AM, \"Jeff Garzik via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> 2) This reverses the useful minimization attribute of HD wallets - \"just\nbackup the seed\"\n\nIt would be nice if the bip37 filter matching algorithm was extended to\nserve up the proof.\n\nAnd if server-based wallets did the same it would preserve the \"just backup\nthe seed\" functionality.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/5a7f7f61/attachment.html>"
            },
            {
                "author": "Tom Harding",
                "date": "2015-12-21T03:23:40",
                "message_text_only": "On 12/20/2015 3:34 AM, Jeff Garzik via bitcoin-dev wrote:\n> Ideally we should start having wallets generate those proofs now, and\n> then introduce the max-age as a second step as a planned hard fork a\n> couple years down the line.\n>\n> However,\n> 1) There is also the open question of \"grandfathered\" UTXOs - for\n> those wallets generated in 2009, buried in a landfill and then dug out\n> 10 years ago\n>\n> 2) This reverses the useful minimization attribute of HD wallets -\n> \"just backup the seed\"\n\nAlso, a change (#6550) has been merged to bitcoin core that removes\nmerkle branches from the wallet, and if pruning gets turned on (possible\nin 0.12 with #6057), it would become quite a bit more difficult to spend\nolder coins under a change like this.\n\nAs a solution I would favor not removing wallet merkle branches."
            },
            {
                "author": "Danny Thorpe",
                "date": "2015-12-13T16:14:21",
                "message_text_only": "What is the current behavior / cost that this proposal is trying to avoid?\nAre ancient utxos required to be kept in memory always in a fully\nvalidating node, or can ancient utxos get pushed out of memory like a\nnormal LRU caching db?\n\nThanks,\n-Danny\nOn Dec 12, 2015 1:55 PM, \"jl2012--- via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> It is a common practice in commercial banks that a dormant account might\n> be confiscated. Confiscating or deleting dormant UTXOs might be too\n> controversial, but allowing the UTXOs set growing without any limit might\n> not be a sustainable option. People lose their private keys. People do\n> stupid things like sending bitcoin to 1BitcoinEater. We shouldn\u2019t be\n> obliged to store everything permanently. This is my proposal:\n>\n> Dormant UTXOs are those UTXOs with 420000 confirmations. In every block X\n> after 420000, it will commit to a hash for all UTXOs generated in block\n> X-420000. The UTXOs are first serialized into the form:\n> txid|index|value|scriptPubKey, then a sorted Merkle hash is calculated.\n> After some confirmations, nodes may safely delete the UTXO records of block\n> X permanently.\n>\n> If a user is trying to redeem a dormant UTXO, in addition the signature,\n> they have to provide the scriptPubKey, height (X), and UTXO value as part\n> of the witness. They also need to provide the Merkle path to the dormant\n> UTXO commitment.\n>\n> To confirm this tx, the miner will calculate a new Merkle hash for the\n> block X, with the hash of the spent UTXO replaced by 1, and commit the hash\n> to the current block. All full nodes will keep an index of latest dormant\n> UTXO commitments so double spending is not possible. (a \"meta-UTXO set\")\n>\n> If all dormant UTXOs under a Merkle branch are spent, hash of the branch\n> will become 1. If all dormant UTXOs in a block are spent, the record for\n> this block could be forgotten. Full nodes do not need to remember which\n> particular UTXO is spent or not, since any person trying to redeem a\n> dormant UTXO has to provide such information.\n>\n> It becomes the responsibility of dormant coin holders to scan the\n> blockchain for the current status of the UTXO commitment for their coin.\n> They may also need to pay extra fee for the increased tx size.\n>\n> This is a softfork if there is no hash collision but this is a fundamental\n> assumption in Bitcoin anyway. The proposal also works without segregated\n> witness, just by replacing \"witness\" with \"scriptSig\"\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151213/5df532a2/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-12-20T16:35:24",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA512\n\n\n\nOn 20 December 2015 08:30:45 GMT-08:00, Chris Pacia <ctpacia at gmail.com> wrote:\n>On Dec 20, 2015 6:34 AM, \"Jeff Garzik via bitcoin-dev\" <\n>bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> 2) This reverses the useful minimization attribute of HD wallets -\n>\"just\n>backup the seed\"\n>\n>It would be nice if the bip37 filter matching algorithm was extended to\n>serve up the proof.\n>\n>And if server-based wallets did the same it would preserve the \"just\n>backup\n>the seed\" functionality.\n\nExactly! The information will be out there - \"just backup the seed\" requires someone to have the exact same data needed to generate the TXO-unspent proof that my proposal requires to spend an old txout.\n\ntl;dr: jgarzik is incorrect; theres no difference at all from the status quo.\n-----BEGIN PGP SIGNATURE-----\n\niQE9BAEBCgAnIBxQZXRlciBUb2RkIDxwZXRlQHBldGVydG9kZC5vcmc+BQJWdtjA\nAAoJEMCF8hzn9Lncz4MH/iwwa7xlbrJJiYqe7Hccr3Vm5D/qbv60eYi2btPDHFo9\nmRnijzqFtt60e1AoFr9NwnCrAhhGmYkWsxLcA2oF+38H12DS9qsb9oT+pckJX34V\nv06+Uap89v9VTHcxVrOEQUCx+9xQO7WkpFw/OTZJA4nmFZ8lvbgDGWE9q7mjQKof\nQU1FiOM7mI6QCU8xTjRvVX5pTwvYNB7PAie/UoCfWU7/QdvsgTHRe4pq0XwJqHKy\nxCr0DbfMZ2TvLFXitS5ZgTbDHURljjHlE7Qdmk9AcFNpI0PCR9YrZ5Mgb10sMygr\nkX3V3uzjx2YBjKEpX9fqk6Kf/aufUqQ1PRBHF3m6bSE=\n=mtj/\n-----END PGP SIGNATURE-----"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-12-21T03:34:06",
                "message_text_only": "On Sun, Dec 20, 2015 at 11:35 AM, Peter Todd <pete at petertodd.org> wrote:\n\n> -----BEGIN PGP SIGNED MESSAGE-----\n> Hash: SHA512\n>\n>\n>\n> On 20 December 2015 08:30:45 GMT-08:00, Chris Pacia <ctpacia at gmail.com>\n> wrote:\n> >On Dec 20, 2015 6:34 AM, \"Jeff Garzik via bitcoin-dev\" <\n> >bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >\n> >> 2) This reverses the useful minimization attribute of HD wallets -\n> >\"just\n> >backup the seed\"\n> >\n> >It would be nice if the bip37 filter matching algorithm was extended to\n> >serve up the proof.\n> >\n> >And if server-based wallets did the same it would preserve the \"just\n> >backup\n> >the seed\" functionality.\n>\n> Exactly! The information will be out there - \"just backup the seed\"\n> requires someone to have the exact same data needed to generate the\n> TXO-unspent proof that my proposal requires to spend an old txout.\n>\n> tl;dr: jgarzik is incorrect; theres no difference at all from the status\n> quo.\n>\n\nThe data stored in the legacy case makes it possible to sign and send a\ntransaction without any connection to a network.  The data stored in the\nupgraded case, absent grandfathering, requires significant network sync at\na minimum.\n\nThe user experience and security parameters are different.\n\nThus, issue/recommendation #1.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/e8707e97/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Forget dormant UTXOs without confiscating bitcoin",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeff Garzik",
                "Chris Priest",
                "Danny Thorpe",
                "s7r",
                "Tom Harding",
                "Peter Todd",
                "Tier Nolan",
                "Gregory Maxwell",
                "Ricardo Filipe",
                "jl2012 at xbt.hk",
                "Chris Pacia",
                "Vincent Truong",
                "gb"
            ],
            "messages_count": 19,
            "total_messages_chars_count": 43219
        }
    },
    {
        "title": "[bitcoin-dev] Block size: It's economics & user preparation & moral hazard",
        "thread_messages": [
            {
                "author": "Jeff Garzik",
                "date": "2015-12-16T14:53:58",
                "message_text_only": "All,\n\nFollowing the guiding WP principle of Assume Good Faith, I've been trying\nto boil down the essence of the message following Scaling Bitcoin.  There\nare key bitcoin issues that remain outstanding and pressing, that are*\northogonal to LN & SW*.\n\nI create multiple proposals and try multiple angles because of a few,\nnotable systemic economic and analysis issues - multiple tries at solving\nthe same problems.  Why do I do what I do -- Why not try to reboot... just\nlist those problems?\n\nDefinitions:\n\nFE - \"Fee Event\", the condition where main chain MSG_BLOCK is 95+% to hard\nlimit for 7 or more days in row, \"blocks generally full\"   This can also be\ninduced by a miner squeeze (collective soft limit reduction).\n\n\nService - a view of bitcoin as a decentralized, faceless, multi-celled,\namorphous automaton cloud, that provides services in exchange for payment\n\nUsers - total [current | future] set of economic actors that pay money to\nthe Service, and receive value (figuratively or literally) in return\n\nBlock Size - This is short hand for MAX_BLOCK_SIZE, the hard limit that\nrequires, today, a hard fork to increase (excl. extension blocks etc.)\n\n\nGuiding Principle:\n\nKeep the Service alive, secure, decentralized, and censorship resistant for\nas many Users as possible.\n\n\nObservations on block size (shorthand for MAX_BLOCK_SIZE as noted above):\n\nThis is economically modeled as a supply limited resource over time.  On\naverage, 1M capacity is available every 10 minutes, with variance.\n\n\nObservations on Users, block size and modern bidding process:\n\nA supermajority of hashpower currently evaluates for block inclusion based,\nfirst-pass, on tx-fee/KB.  Good.\n\nThe Service is therefore responsive to the free market and some classes of\nDoS.  Good.\n\nRecent mempool changes float relay fee, making the Service more responsive\nto fast moving markets and DoS's.  Good progress.\n\n\nService provided to Users can be modeled at the bandwidth resource level as\nbidding for position in a virtual priority queue, where up-to-1M bursts are\ncleared every 10 min (on avg etc.).  Not a perfectly fixed supply,\ndefinitionally, but constrained within a fixed range.\n\n\nObservations on the state of today's fee market:\n\nOn average, blocks are not full.  Economically, this means that fees trend\ntowards zero, due to theoretically unlimited supply at <1M levels.\n\nOf course, fees are not zero.  The network relay anti-flood limits serve as\nan average lower limit for most transactions (excl direct-to-miner).\nWallet software also introduces fee variance in interesting ways.  All this\nfee activity is range-bound on the low end.\n\nLet the current set of Users + transaction fee market behavior be TFM\n(today's fee market).\nLet the post-Fee-Event set of Users + transaction fee market behavior be\nFFM (future fee market).\n\n*Key observation:   A Bitcoin Fee Event (see def. at top) is an Economic\nChange Event.*\n\nAn Economic Change Event is a period of market chaos, where large changes\nto prices and sets of economic actors occurs over a short time period.\n\nA Fee Event is a notable Economic Change Event, where a realistic\nprojection forsees higher fee/KB on average, pricing some economic actors\n(bitcoin projects and businesses) out of the system.\n\n*It is a major change to how current Users experience and pay for the\nService*, state change from TFM to FFM.\n\nThe game theory bidding behavior is different for a mostly-empty resource\nversus a usually-full resource.  Prices are different.  Profitable business\nmodels are different.  Users (the set of economic actors on the network)\nare different.\n\n\nObservation:  Contentious hard fork is an Economic Change Event.\n\nSimilarly, a fork that partitions economic actors for an extended period or\npermanently is also an Economic Change Event, shuffling prices and economic\nactors as the Service dynamically readjusts on both sides of the partition,\nand Users-A and Users-B populations change their behavior.\n\n\n\nShort-Term Problem #1:  No-action on block size increase leads to an\nEconomic Change Event.\n\n\nFailure to increase block size is not obviously-conservative, it is a\nconscious choice, electing for one economic state and set of actors and\nprices over another.  Choosing FFM over TFM.\n\n*It is rational to reason that maintaining TFM is more conservative* than\nenduring an Economic Change Event from TFM to FFM.\n\n*It is rational to reason that maintaining similar prices and economic\nactors is less disruptive.*\n\nFailure to increase block size will lead to a Fee Event sooner rather than\nlater.\n\nFailure to plan ahead for a Fee Event will lead to greater market chaos and\nUser pain.\n\n\nShort-Term Problem #2:  Some Developers wish to accelerate the Fee Event,\nand a veto can accomplish that.\n\nIn the current developer dynamics, 1-2 key developers can and very likely\nwould veto any block size increase.\n\nThus a veto (e.g. no-action) can lead to a Fee Event, which leads to\npricing actors out of the system.\n\nA block size veto wields outsize economic power, because it can accelerate\nECE.\n\n*This is an extreme moral hazard:  A few Bitcoin Core committers can veto\nincrease and thereby reshape bitcoin economics, price some businesses out\nof the system.  It is less of a moral hazard to keep the current economics\n[by raising block size] and not exercise such power.*\n\n\nShort-Term Problem #3:  User communication and preparation\n\nThe current trajectory of no-block-size-increase can lead to short time\nmarket chaos, actor chaos, businesses no longer viable.\n\nIn a $6.6B economy, it is criminal to let the Service undergo an ECE\nwithout warning users loudly, months in advance:  \"Dear users, ECE has\naccelerated potential due to developers preferring a transition from TFM to\nFFM.\"\n\nAs stated, *it is a conscious choice to change bitcoin economics and User\nexperience* if block size is not advanced with a healthy buffer above\nactual average traffic levels.\n\n*Raising block size today, at TFM, produces a smaller fee market delta.*\n\nFurther, wallet software User experience is very, very poor in a\nhyper-competitive fee market.   (This can and will be improved; that's just\nthe state of things today)\n\n\nShort-Term Problem #4:  User/Dev disconnect:   Large mass of users wishes\nto push Fee Event into future\n\nAlmost all bitcoin businesses, exchanges and miners have stated they want a\nblock size increase.  See the many media articles, BIP 101 letter, and wiki\ne.g.\nhttps://en.bitcoin.it/wiki/Block_size_limit_controversy#Entities_positions\n\nThe current apparent-veto on block size increase runs contra to the desires\nof many Users.  (note language: \"many\", not claiming \"all\")\n\n*It is a valid and rational economic choice to subsidize the system with\nlower fees in the beginning*.  Many miners, for example, openly state they\nprefer long term system growth over maximizing tiny amounts of current day\nincome.\n\nVetoing a block size increase has the effect of eliminating that economic\nchoice as an option.\n\n\nIt is difficult to measure Users; projecting beyond \"businesses and miners\"\nis near impossible.\n\nWithout exaggeration, I have never seen this much disconnect between user\nwishes and dev outcomes in 20+ years of open source.\n\n\nShort-Term Problem #5:  Higher Service prices can negatively impact system\nsecurity\n\nBitcoin depends on a virtuous cycle of users boosting and maintaining\nbitcoin's network effect, incentivizing miners, increasing security.\n\nHigher prices that reduce bitcoin's user count and network effect can have\nthe opposite impact.\n\n(Obviously this is a dynamic system, users and miners react to higher\nprices... including actions that then reduce the price)\n\nShort-Term Problem #6:  Post-Fee-Event market reboot problem + general lack\nof planning\n\nGame it out:   Blocks are now full (FFM).  Block size kept at 1M.\n\nHow full is too full - who and what dictates when 1M should be increased?\n\nThe same question remains, yet now economic governance issues are\ncompounded:  In FFM, the fees are very tightly bound to the upper bound of\nthe block size.  In TFM, fees are much less sensitive to the upper bound of\nblock size.\n\n\nChanging block size, when blocks are full, has a more dramatic effect on\nthe market - suddenly new supply is magically brought online, and a minor\nEconomic Change Event occurs.\n\nMore generally, the post-Fee-Event next step has not been agreed upon.  Is\nit flexcap?  This key \"step #2\" is just barely at whiteboard stage.\n\n\nShort-Term Problem #7:   Fee Event timing is unpredictable.\n\nAs block size free space gets tighter - that is the trend - and block size\nremains at 1M, Users are ever more likely to hit an Economic Change Event.\nIt could happen in the next 2-6 months.\n\nToday, Users and wallets are not prepared.\n\nIt is also understandably a very touchy subject to say \"your business or\nuse case might get priced out of bitcoin\"\n\n\nBut it is even worse to let worse let Users run into a Fee Event without\ninforming the market that the block size will remain at 1M.\n\nMarkets function best with maximum knowledge - when they are informed well\nin advance of market shifting news and events, giving economic actors time\nto prepare.\n\n\nShort-Term Problem #8:   Very little testing, data, effort put into\nblocks-mostly-full economics\n\n*We only know for certain that blocks-mostly-not-full works.*  We do not\nknow that changing to blocks-mostly-full works.\n\nChanging to a new economic system includes boatloads of risk.\n\nVery little data has been forthcoming from any party on what FFM might look\nlike, following a Fee Event.\n\n\nObservation:   In the long run, it is assumed we need a \"healthy fee market\"\n\nYes, absolutely.  In the long run, bitcoin was intended to be supported by\ntransaction fees and not the minting of new supply, and the design of the\nsystem is to slowly wean Users off new supply and onto transaction fees for\nsupporting the Service.\n\nWhile agreeing with the goal, it must be acknowledge that this is a vague\nand untested goal with many open economic questions -- more of a hope,\nreally.\n\nIt is more conservative to preserve current economics than to change to a\nnew system with new economics and no notion of what-comes-next (flexcap?)\nin terms of system security, healthy sustainable market levels, and impact\nof changes during and following an ECE.\n\n\n\nCore recommendations:\n\n1) \"Short term bump\"  Block size increase to maintain buffer.  I've no\nspecial BIP preference.\n\nThis avoids moral hazard and avoids a major Economic Change Event, as well\nmany other risks.\n\n\n2) If block size stays at 1M, the Bitcoin Core developer team should sign a\ncollective note stating their desire to transition to a new economic\npolicy, that of \"healthy fee market\" and strongly urge users to examine\ntheir fee policies, wallet software, transaction volumes and other possible\nUser impacting outcomes.\n\n\n3) Even if can is kicked down the road, Fee Event will come eventually.\nDirect research, testing and simulations into the economics and user impact\nside of the equation.  Research and experiment with pay-for-burst (pay to\nfuture miner), flexcap and other solutions ASAP.\n\n\nThe worst possible outcome is letting the ecosystem randomly drift into the\nfirst Fee Event without openly stating the new economic policy choices and\nconsequences.\n\nThe simple fact is *inaction* on this supply-limited resource, block size,\nwill change bitcoin to a new economic shape and with different economic\nactors, selecting some and not others.\n\nIt is better to kick the can and gather crucial field data, because\nnext-step (FFM) is very much not fleshed out.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/024095fb/attachment-0001.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-12-16T18:34:32",
                "message_text_only": "On Wed, Dec 16, 2015 at 3:53 PM, Jeff Garzik via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> 2) If block size stays at 1M, the Bitcoin Core developer team should sign a\n> collective note stating their desire to transition to a new economic policy,\n> that of \"healthy fee market\" and strongly urge users to examine their fee\n> policies, wallet software, transaction volumes and other possible User\n> impacting outcomes.\n\nYou present this as if the Bitcoin Core development team is in charge\nof deciding the network consensus rules, and is responsible for making\nchanges to it in order to satisfy economic demand. If that is the\ncase, Bitcoin has failed, in my opinion.\n\nWhat the Bitcoin Core team should do, in my opinion, is merge any\nconsensus change that is uncontroversial. We can certainly -\nindividually or not - propose solutions, and express opinions, but as\nfar as maintainers of the software goes our responsibility is keeping\nthe system running, and risking either a fork or establishing\nourselves as the de-facto central bank that can make any change to the\nsystem would greatly undermine the system's value.\n\nHard forking changes require that ultimately every participant in the\nsystem adopts the new rules. I find it immoral and dangerous to merge\nsuch a change without extremely widespread agreement. I am personally\nfine with a short-term small block size bump to kick the can down the\nroad if that is what the ecosystem desires, but I can only agree with\nmerging it in Core if I'm convinced that there is no strong opposition\nto it from others.\n\nSoft forks on the other hand only require a majority of miners to\naccept them, and everyone else can upgrade at their leisure or not at\nall. Yes, old full nodes after a soft fork are not able to fully\nvalidate the rules new miners enforce anymore, but they do still\nverify the rules that their operators opted to enforce. Furthermore,\nthey can't be prevented. For that reason, I've proposed, and am\nworking hard, on an approach that includes Segregated Witness as a\nfirst step. It shows the ecosystem that something is being done, it\nkicks the can down the road, it solves/issues half a dozen other\nissues at the same time, and it does not require the degree of\ncertainty needed for a hardfork.\n\n-- \nPieter"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-12-16T21:08:29",
                "message_text_only": "On Wed, Dec 16, 2015 at 1:34 PM, Pieter Wuille <pieter.wuille at gmail.com>\nwrote:\n\n> On Wed, Dec 16, 2015 at 3:53 PM, Jeff Garzik via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > 2) If block size stays at 1M, the Bitcoin Core developer team should\n> sign a\n> > collective note stating their desire to transition to a new economic\n> policy,\n> > that of \"healthy fee market\" and strongly urge users to examine their fee\n> > policies, wallet software, transaction volumes and other possible User\n> > impacting outcomes.\n>\n> You present this as if the Bitcoin Core development team is in charge\n> of deciding the network consensus rules, and is responsible for making\n> changes to it in order to satisfy economic demand. If that is the\n> case, Bitcoin has failed, in my opinion.\n>\n\nThis circles back to Problem #1:   Avoidance of a choice is a still a\nchoice - failing to ACK a MAX_BLOCK_SIZE increase still creates very real\nEconomic Change Event risk.\n\nAnd #3:  If the likely predicted course is that Bitcoin Core will not\naccept a protocol change changing MAX_BLOCK_SIZE via hard fork in the short\nterm, the core dev team should communicate that position clearly to users\nand media.\n\nHitting a Fee Event is market changing, potentially reshuffling economic\nactors to a notable degree.  Maintaining a short term economic policy of\nfixed 1M supply in the face of rising transaction volume carries risks that\nshould be analyzed and communicated.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/117c35a7/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-12-16T21:11:52",
                "message_text_only": "On Wed, Dec 16, 2015 at 10:08 PM, Jeff Garzik <jgarzik at gmail.com> wrote:\n>> You present this as if the Bitcoin Core development team is in charge\n>> of deciding the network consensus rules, and is responsible for making\n>> changes to it in order to satisfy economic demand. If that is the\n>> case, Bitcoin has failed, in my opinion.\n>\n>\n> This circles back to Problem #1:   Avoidance of a choice is a still a choice\n> - failing to ACK a MAX_BLOCK_SIZE increase still creates very real Economic\n> Change Event risk.\n\nWe are not avoiding a choice. We don't have the authority to make a choice.\n\n> And #3:  If the likely predicted course is that Bitcoin Core will not accept\n> a protocol change changing MAX_BLOCK_SIZE via hard fork in the short term,\n> the core dev team should communicate that position clearly to users and\n> media.\n\nI indeed think we can communicate much better that deciding consensus\nrules is not within our power.\n\n-- \nPieter"
            },
            {
                "author": "Jameson Lopp",
                "date": "2015-12-17T02:06:08",
                "message_text_only": "On Wed, Dec 16, 2015 at 1:11 PM, Pieter Wuille via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Wed, Dec 16, 2015 at 10:08 PM, Jeff Garzik <jgarzik at gmail.com> wrote:\n> >> You present this as if the Bitcoin Core development team is in charge\n> >> of deciding the network consensus rules, and is responsible for making\n> >> changes to it in order to satisfy economic demand. If that is the\n> >> case, Bitcoin has failed, in my opinion.\n> >\n> >\n> > This circles back to Problem #1:   Avoidance of a choice is a still a\n> choice\n> > - failing to ACK a MAX_BLOCK_SIZE increase still creates very real\n> Economic\n> > Change Event risk.\n>\n> We are not avoiding a choice. We don't have the authority to make a choice.\n>\n> > And #3:  If the likely predicted course is that Bitcoin Core will not\n> accept\n> > a protocol change changing MAX_BLOCK_SIZE via hard fork in the short\n> term,\n> > the core dev team should communicate that position clearly to users and\n> > media.\n>\n> I indeed think we can communicate much better that deciding consensus\n> rules is not within our power.\n>\n\nIndeed, because I sometimes find these statements to be confusing as well -\nI can completely understand what you mean if you're speaking from a moral\nstandpoint. If you're saying that it's unacceptable for the Bitcoin Core\ndevelopers to force consensus changes upon the system, I agree. But\nthankfully the design of the system does not allow the developers to do so.\nDevelopers can commit amazing code or terrible code, but it must be\nvoluntarily adopted by the rest of the ecosystem. Core developers can't\ndecide these changes, they merely propose them to the ecosystem by writing\nand releasing code.\n\nI agree that Core developers have no authority to make these decisions on\nbehalf of all of the network participants. However, they are in a position\nof authority when it comes to proposing changes. One of my takeaways from\nHong Kong was that most miners have little interest in taking\nresponsibility for consensus changes - they trust the Core developers to\nuse their expertise to propose changes that will result in the continued\noperation of the network and not endanger their business operations.\n\nA non-trivial portion of the ecosystem is requesting that the Core\ndevelopers make a proposal so that the network participants can make a\nchoice. Jeff noted that we can expect for the economic conditions of the\nnetwork to change significantly in 2016, barring higher throughput\ncapacity. If the year+ deployment timeframe for hard forks proposed by Matt\non another thread is what we can expect for any proposed consensus change,\nthen it should be non-contentious to announce that there will be no hard\nfork in 2016. This will give clarity to the rest of the ecosystem as to how\nthey should prepare.\n\n\n> --\n> Pieter\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/ba83fcba/attachment.html>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-12-17T16:58:02",
                "message_text_only": "On Wed, Dec 16, 2015 at 9:11 PM, Pieter Wuille via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> We are not avoiding a choice. We don't have the authority to make a choice.\n>\n\nThis is really the most important question.\n\nBitcoin is kind of like a republic where there is separation of powers\nbetween various groups.\n\nThe power blocs in the process include\n\n- Core Devs\n- Miners\n- Exchanges\n- Merchants\n- Customers\n\nComplete agreement is not required for a change.  If merchants and their\ncustomers were to switch to different software, then there is little any of\nthe other groups could do.\n\nConsensus is nice, certainly, and it is a good social norm to seek\nwidespread agreement before committing to a decision above objection.\nCommitting to no block increase is also committing to a decision against\nobjections.\n\nHaving said that, each of the groups are not equal in power and\norganisation.\n\nMerchants and their customers have potentially a large amount of power, but\nthey are disorganised.  There is little way for them to formally express a\nview, much less put their power behind making a change.  Their potential\npower is crippled by public action problems.\n\nOn the other extreme is the core devs. Their power is based on legitimacy\ndue to having a line of succession starting with Satoshi and respect gained\ndue to technical and political competence.  Being a small group, they are\norganised and they are also more directly involved.\n\nThe miners are less centralised, but statements supported by the majority\nof the hashing power are regularly made.  The miners' position is that they\nwant dev consensus.  This means that they have delegated their decision\nmaking to the core devs.\n\nThe means that the two most powerful groups in Bitcoin have given the core\ndevs the authority to make the decision.  They don't have carte blanche\nfrom the miners.\n\nIf the core devs made the 2MB hard-fork with a 75% miner threshold, it is\nhighly likely that the other groups would accept it.\n\nThat is the only authority that exists in Bitcoin.  The check is that if\nthe authority is abused, the other groups can simply leave (or use\ncheckpointing)\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151217/35172889/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-12-17T19:44:08",
                "message_text_only": "On Thu, Dec 17, 2015 at 04:58:02PM +0000, Tier Nolan via bitcoin-dev wrote:\n> This is really the most important question.\n> \n> Bitcoin is kind of like a republic where there is separation of powers\n> between various groups.\n> \n> The power blocs in the process include\n> \n> - Core Devs\n> - Miners\n> - Exchanges\n> - Merchants\n> - Customers\n> \n> Complete agreement is not required for a change.  If merchants and their\n> customers were to switch to different software, then there is little any of\n> the other groups could do.\n\nIf Bitcoin remains decentralized, miners have veto power over any\nblocksize increases. You can always soft-fork in a blocksize reduction\nin a decentralized blockchain that actually works.\n\n-- \n'peter'[:-1]@petertodd.org\n000000000000000001bd68962863e6fa34e9776df361d4926912f52fc5f4b618\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151217/704b0752/attachment.sig>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-12-18T05:23:25",
                "message_text_only": "On Thu, Dec 17, 2015 at 8:44 PM, Peter Todd via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> If Bitcoin remains decentralized, miners have veto power over any\n> blocksize increases. You can always soft-fork in a blocksize reduction\n> in a decentralized blockchain that actually works.\n\nYou can always schism hardfork miners out..."
            },
            {
                "author": "Tier Nolan",
                "date": "2015-12-18T09:44:36",
                "message_text_only": "On Thu, Dec 17, 2015 at 7:44 PM, Peter Todd <pete at petertodd.org> wrote:\n\n> If Bitcoin remains decentralized, miners have veto power over any\n> blocksize increases. You can always soft-fork in a blocksize reduction\n> in a decentralized blockchain that actually works.\n>\n\nThe actual users of the system have significant power, if they (could)\nchoose to use it.  There are \"chicken\" effects though.  They can impose\ncosts on the other participants but using those options harms themselves.\nIf the cost of inaction is greater than the costs of action, then the\nchicken effects go away.\n\nIn the extreme, they could move away from decentralisation and the concept\nof miners and have a centralised checkpointing system.  This would be a\nbankrupting cost to miners but at the cost to the users of the\ndecentralised nature of the system.\n\nAt a lower extreme, they could change the mining hash function.  This would\ndevalue all of the miner's investments.  A whole new program of ASIC\ninvestments would have to happen and the new miners would be significantly\ndifferent.  It would also establish that merchants and users are not to be\nignored.  On the other hand, bankrupting miners would make it harder to\nconvince new miners to make the actual investments in ASICs required to\nestablish security.\n\nAs a gesture, if merchants and exchanges wanted to get their \"seat\" at the\ntable, they could create a representative group that insists on a trivial\nsoft fork.  For example, they could say that they will not accept any block\nfrom block N to block N + 5000 that doesn't have a specific bit set in the\nversion.\n\nMiners have an advantage where they can say that they have the majority of\nthe hashing power.  As part of the public action problem that merchants\nface, there is no equivalent metric.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/52161768/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-12-16T21:24:38",
                "message_text_only": "On Dec 16, 2015 10:08 PM, \"Jeff Garzik via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> On Wed, Dec 16, 2015 at 1:34 PM, Pieter Wuille <pieter.wuille at gmail.com>\nwrote:\n>>\n>> On Wed, Dec 16, 2015 at 3:53 PM, Jeff Garzik via bitcoin-dev\n>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> > 2) If block size stays at 1M, the Bitcoin Core developer team should\nsign a\n>> > collective note stating their desire to transition to a new economic\npolicy,\n>> > that of \"healthy fee market\" and strongly urge users to examine their\nfee\n>> > policies, wallet software, transaction volumes and other possible User\n>> > impacting outcomes.\n>>\n>> You present this as if the Bitcoin Core development team is in charge\n>> of deciding the network consensus rules, and is responsible for making\n>> changes to it in order to satisfy economic demand. If that is the\n>> case, Bitcoin has failed, in my opinion.\n>\n>\n> This circles back to Problem #1:   Avoidance of a choice is a still a\nchoice - failing to ACK a MAX_BLOCK_SIZE increase still creates very real\nEconomic Change Event risk.\n\nUnless the community is going to always avoid this \"economic change event\"\nforever (effectively eliminating MAX_BLOCK_SIZE), this is going to happen\nat some point. I assume those concerned with the \"economic change\" are only\nscared about it because \"nitcoin is still very young\" of something like\nthat.\nSince you advocate for delaying this event from happening, can you be\nclearer about when do you think it would be ok to let the event happen?\nWhat other event makes this event ok?\n\n> Hitting a Fee Event is market changing, potentially reshuffling economic\nactors to a notable degree.  Maintaining a short term economic policy of\nfixed 1M supply in the face of rising transaction volume carries risks that\nshould be analyzed and communicated.\n\nAssuming we adopt bip102, eventually you will be able to say exactly the\nsame about 2 MB. When does this \"let's not change the economics\" finishes\n(if ever)?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/24def18f/attachment.html>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-12-16T21:36:18",
                "message_text_only": "On Wed, Dec 16, 2015 at 4:24 PM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n\n> Assuming we adopt bip102, eventually you will be able to say exactly the\n> same about 2 MB. When does this \"let's not change the economics\" finishes\n> (if ever)?\n>\n\nThe question is answered in the original email.\n\nIn the short term, the risk of a Fee Event and lack of solid post-Fee-Event\neconomic plan implies \"short term bump\" reduces those risks.\n\nIt is also true - as noted in [1], a bump does create the danger of long\nterm moral hazard.\n\nThis is why a bump should be accompanied with at least a weak public\ncommitment to flexcap or a similar long term proposal, as the original\nemail recommended.  Communicate clearly the conditions for future change,\nthen iterate as needs and feedback indicate.\n\n\n\n[1] http://gtf.org/garzik/bitcoin/BIP100-blocksizechangeproposal.pdf\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/eecf9243/attachment.html>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-12-18T05:11:48",
                "message_text_only": "On Wed, Dec 16, 2015 at 1:34 PM, Pieter Wuille <pieter.wuille at gmail.com>\nwrote:\n\n> On Wed, Dec 16, 2015 at 3:53 PM, Jeff Garzik via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > 2) If block size stays at 1M, the Bitcoin Core developer team should\n> sign a\n> > collective note stating their desire to transition to a new economic\n> policy,\n> > that of \"healthy fee market\" and strongly urge users to examine their fee\n> > policies, wallet software, transaction volumes and other possible User\n> > impacting outcomes.\n>\n> You present this as if the Bitcoin Core development team is in charge\n> of deciding the network consensus rules, and is responsible for making\n> changes to it in order to satisfy economic demand. If that is the\n> case, Bitcoin has failed, in my opinion.\n>\n\nDiverging from the satoshi block size change plan[1] and current economics\nwould seem to require a high level of months-ahead communication to users.\n\n\n\n\n> all. Yes, old full nodes after a soft fork are not able to fully\n> validate the rules new miners enforce anymore, but they do still\n> verify the rules that their operators opted to enforce. Furthermore,\n> they can't be prevented. For that reason, I've proposed, and am\n> working hard, on an approach that includes Segregated Witness as a\n> first step. It shows the ecosystem that something is being done, it\n> kicks the can down the road, it solves/issues half a dozen other\n> issues at the same time, and it does not require the degree of\n> certainty needed for a hardfork.\n>\n\nSegregated Witness does not kick the can, it solves none of the problems\n#1, #3 - #8 explicitly defined and listed in email #1.\n\n1)  A plan of \"SW + no hard fork\" is gambling with ECE risk, gambling there\nwill be no Fee Event, because the core block size is still heavily\ncontended -- 100% contended at time out SW rollout.\n\n2) We are only 100% certain that bitcoin works in the\nblocks-not-full-on-avg state, where there is a healthy buffer between the\nhard limit and the average block size.\n\nThere is remains major ECE risk due to the core block size freeze, possibly\npushing the system into a new, untried economic state and causing major\nmarket and actor disruption.  Users of the Service can still drift randomly\nand unpredictably into a Fee Event.\n\nSW mitigates this\n- only after several months\n- only assuming robust adoption rates by up-layer ecosystem software, and\n- only assuming transaction volume growth is flat or sub-linear\n\nThose conditions *must* go as planned to fulfill \"SW kicked the can\" -- a\nlot of if's.\n\nAs stated, SW is orthogonal to the drift-into-uncharted-waters problem\noutlined in email #1, which a short term bump does address.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/df9746ce/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-12-18T07:56:58",
                "message_text_only": "On Fri, Dec 18, 2015 at 6:11 AM, Jeff Garzik <jgarzik at gmail.com> wrote:\n>> You present this as if the Bitcoin Core development team is in charge\n>> of deciding the network consensus rules, and is responsible for making\n>> changes to it in order to satisfy economic demand. If that is the\n>> case, Bitcoin has failed, in my opinion.\n>\n> Diverging from the satoshi block size change plan[1] and current economics\n> would seem to require a high level of months-ahead communication to users.\n\nI don't see any plan, but will you say the same thing when the subsidy\ndwindles, and mining income seems to become uncertain? It will equally\nbe an economic change, which equally well will have been predictable,\nand it will equally well be treatable with a hardfork to increase the\nsubsidy.\n\nYes, I'm aware the argument above is a straw man, because there was\nclear expectation that the subsidy would go down asymptotically, and\nmuch less an expectation that the blocksize would remain fixed\nforever.\n\nBut I am not against a block size increase hard fork. My talk on\nsegregated witness even included proposed pursuing a hard fork at a\nslightly later stage.\n\nBut what you're arguing for is that - despite being completely\nexpected - blocks grew fuller, and people didn't adapt to block size\npressure and a fee market, so the Core committee now needs to kick the\ncan down the road, because we can't accept the risk of economic\nchange. That sounds very much like a bailout to me.\n\nAgain. I am not against growth, but increasing in response to fear of\neconomic change is the wrong approach. Economic change is inevitable.\n\n> Segregated Witness does not kick the can, it solves none of the problems #1,\n> #3 - #8 explicitly defined and listed in email #1.\n>\n> 1)  A plan of \"SW + no hard fork\" is gambling with ECE risk, gambling there\n> will be no Fee Event, because the core block size is still heavily contended\n> -- 100% contended at time out SW rollout.\n\nThat is an assumption. I expect demand for transactions at a given\nfeerate to stop growing at a certain contention level (and we've\nreached some level of contention already, with mempools not being\ncleared for significant amounts of time already).\n\n> SW mitigates this\n> - only after several months\n\nThat is assuming a hard fork consensus forming, deployment,\nactivation, ... goes faster than a softfork.\n\n> - only assuming robust adoption rates by up-layer ecosystem software, and\n\nThat's not required. Everyone who individually switches to new\ntransactions gets to do 1.75x more transactions for the same price\n(and at the same time gets safer contracts, better script\nupgradability, and more security models at their disposal), completely\nindependent of whether anyone else in the ecosystem does the same.\n\n> - only assuming transaction volume growth is flat or sub-linear\n\nThe only question is how many transactions for what price. Contention\nalways happens at a specific feerate level anyway.\n\n> Those conditions must go as planned to fulfill \"SW kicked the can\" -- a lot\n> of if's.\n>\n> As stated, SW is orthogonal to the drift-into-uncharted-waters problem\n> outlined in email #1, which a short term bump does address.\n\nBoth SW and a short bump (which is apparently not what BIP102 does\nanymore?) increase capacity available per price, and yes, they are\ncompletely orthogonal.\n\nMy only disagreement is the motivation (avoiding economic change, as\nopposed to aiming for safe growth) and the claim that a capacity\nincrease hardfork is easier and safe(r) to roll out quickly than\nsortfork SW.\n\nApart from that, we clearly need to do both at some point.\n\n-- \nPieter"
            },
            {
                "author": "sickpig at gmail.com",
                "date": "2015-12-18T10:13:33",
                "message_text_only": "On Fri, Dec 18, 2015 at 8:56 AM, Pieter Wuille via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n> > - only assuming robust adoption rates by up-layer ecosystem software, and\n>\n> That's not required. Everyone who individually switches to new\n> transactions gets to do 1.75x more transactions for the same price\n> (and at the same time gets safer contracts, better script\n> upgradability, and more security models at their disposal), completely\n> independent of whether anyone else in the ecosystem does the same.\n>\n>\nSo hypothetically if wallets/payments processors/full nodes adoption\nwill take 6 month to get to 50% after the segwit soft-fork activation, this\nmeans that actual network capacity will be increased by:\n\n1.75 x 0.5 + 1 x 0.5 = 1.375\n\nafter six month.\n\nAn hard-fork on the others side would bring 1.75 since the activation, am I\nright?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/79231514/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-12-18T15:48:43",
                "message_text_only": "On Dec 18, 2015 2:13 AM, \"sickpig at gmail.com\" <sickpig at gmail.com> wrote:\n> 1.75 x 0.5 + 1 x 0.5 = 1.375\n>\n> after six month.\n>\n> An hard-fork on the others side would bring 1.75 since the activation, am\nI right?\n\nYes.\n\nHowever, SW immediately gives a 1.75 capacity increase for anyone who\nadopts it, after the softfork, instantly. They don't need to wait for\nanyone else.\n\nA hard fork is an orthogonal improvement, which is also needed if we don't\nwant to be stuck with a constant maximum ultimately.\n\nHardforks can however only be deployed at a time when all full node\nsoftware can reasonably have agreed to upgrade, while a softfork can be\ndeployed much earlier.\n\nThey are independent improvements, and we need both. I am however of the\nopinion that hard forks need a much clearer consensus and much longer\nrollout timeframes to be safe (see my thread on the security of softforks).\n\n-- \nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/5a9ace37/attachment.html>"
            },
            {
                "author": "Dave Scotese",
                "date": "2015-12-19T19:04:01",
                "message_text_only": "I've already publicly declared that I offer one bitcoin to \"those who\nsuffer from a May 5, 2016 hardfork to 2MB blocks\" but that's probably way\ntoo sloppy.  Here's a better idea that transmits the economic power of\nmerchants and customers (well, anyone with bitcoin) to the miners on whom\nthey must rely for confirmations:\n\nThe bitcoin I offer is part of a fund that, when it reaches 25 BTC, will be\npledged to a miner.  Here is how that miner earns the reward:\n\n   1. Wait until a core dev signs a release of bitcoin core in which the\n   limit is double it's current level.\n   2. Use the new release to mine, but use a soft limit on the blocksize to\n   produce only blocks that are valid according to the old software.\n   3. Wait until May 5th, 2016.\n   4. Remove the soft limit on blocksize.\n   5. Create a block that breaks the old limit and is valid according to\n   the new signed release.\n   6. Wait for the new large block to be orphaned.\n\nHopefully, the reward will be greater than 25 bitcoins and therefore cover\nthe transaction fees.  Of course, if they wait until after the halving in\nstep 3, then they will get twice the (new, 12.5 btc) reward if they can\narrange for the orphaning of their own block.\n\nAny core dev could do this but I guess it would be playing with fire.  So\nmaybe Satoshi will do it.  He played with fire (right?) and look how that\nworked out.  Come on, someone, be a hero.  Mike and Gavin tried, but I\nthink they went a little overboard.\n\nAnother way to do this is to identify the position in each binary where the\nhard limit is stored, and write a little script that will (check the date\nfirst, and then) alter the data at that position so that currently running\nbitcoin software can be hot-patched on May 5th without the help of any core\ndevs (if that would work).  Obviously, the little script should be signed\nby a competent programmer whom the user trusts, a slightly less stringent\nrequirement than being an actual core dev.\n\nnotplato\n\nOn Fri, Dec 18, 2015 at 7:48 AM, Pieter Wuille via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n> On Dec 18, 2015 2:13 AM, \"sickpig at gmail.com\" <sickpig at gmail.com> wrote:\n> > 1.75 x 0.5 + 1 x 0.5 = 1.375\n> >\n> > after six month.\n> >\n> > An hard-fork on the others side would bring 1.75 since the activation,\n> am I right?\n>\n> Yes.\n>\n> However, SW immediately gives a 1.75 capacity increase for anyone who\n> adopts it, after the softfork, instantly. They don't need to wait for\n> anyone else.\n>\n> A hard fork is an orthogonal improvement, which is also needed if we don't\n> want to be stuck with a constant maximum ultimately.\n>\n> Hardforks can however only be deployed at a time when all full node\n> software can reasonably have agreed to upgrade, while a softfork can be\n> deployed much earlier.\n>\n> They are independent improvements, and we need both. I am however of the\n> opinion that hard forks need a much clearer consensus and much longer\n> rollout timeframes to be safe (see my thread on the security of softforks).\n>\n> --\n> Pieter\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n\n\n-- \nI like to provide some work at no charge to prove my value. Do you need a\ntechie?\nI own Litmocracy <http://www.litmocracy.com> and Meme Racing\n<http://www.memeracing.net> (in alpha).\nI'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which\nnow accepts Bitcoin.\nI also code for The Dollar Vigilante <http://dollarvigilante.com/>.\n\"He ought to find it more profitable to play by the rules\" - Satoshi\nNakamoto\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151219/81c76c41/attachment.html>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-12-18T13:56:45",
                "message_text_only": "On Fri, Dec 18, 2015 at 2:56 AM, Pieter Wuille <pieter.wuille at gmail.com>\nwrote:\n\n> On Fri, Dec 18, 2015 at 6:11 AM, Jeff Garzik <jgarzik at gmail.com> wrote:\n> >> You present this as if the Bitcoin Core development team is in charge\n> >> of deciding the network consensus rules, and is responsible for making\n> >> changes to it in order to satisfy economic demand. If that is the\n> >> case, Bitcoin has failed, in my opinion.\n> >\n> > Diverging from the satoshi block size change plan[1] and current\n> economics\n> > would seem to require a high level of months-ahead communication to\n> users.\n>\n> I don't see any plan, but will you say the same thing when the subsidy\n>\n\nYes, I forgot the link:\n\n[1] https://bitcointalk.org/index.php?topic=1347.msg15366#msg15366\n\n\n\n> dwindles, and mining income seems to become uncertain? It will equally\n> be an economic change, which equally well will have been predictable,\n> and it will equally well be treatable with a hardfork to increase the\n> subsidy.\n>\n\nThat is a red herring.  Nobody I know has proposed this, and I am opposed\nto changing that fundamental.\n\nIt is well known that the 1M limit was never intended to stay, unlike 21M\ncoin limit etc.\n\n1M was set high in the beginning because it is a DoS engineering limit, not\nan [accidental] economic policy tool.\n\n\n\n\n> But I am not against a block size increase hard fork. My talk on\n> segregated witness even included proposed pursuing a hard fork at a\n> slightly later stage.\n>\n\nGreat!\n\n\n\n> But what you're arguing for is that - despite being completely\n> expected - blocks grew fuller, and people didn't adapt to block size\n> pressure and a fee market, so the Core committee now needs to kick the\n> can down the road, because we can't accept the risk of economic\n> change. That sounds very much like a bailout to me.\n>\n\nI am arguing for continuing what we know works.  We are 100% certain\nblocks-not-full-on-avg works, where a \"buffer\" of space exists between avg\nblock size and hard limit.\n\nAny other avenue is by definition speculation and risk.  You _think_ you\nknow what a healthy fee market _should_ be.  Massive damage occurs to\nbitcoin if you are wrong - and I listed several\n\nvis expectation, there is clear consensus and expectation that block size\nwould increase, from 2010 onward.  It was always a question of _when_ not\nif.\n\nSticking with 1M presents clear risk of (a) economic fracture and (b)\ncommunity fracture.  It quite clearly risks massive change to an unknown\nsystem at an unknown, unpredictable date in the future.\n\nBIP 102 presents an expected upgrade at a predictable date in the future.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/3a05c528/attachment.html>"
            },
            {
                "author": "Aaron Voisine",
                "date": "2015-12-23T06:26:11",
                "message_text_only": "> You present this as if the Bitcoin Core development team is in charge\n> of deciding the network consensus rules, and is responsible for\n> making changes to it in order to satisfy economic demand. If that is\n> the case, Bitcoin has failed, in my opinion.\n\nPieter, what's actually happening is that the bitcoin-core release has\nbecome a Schelling point in the consensus game:\n\nhttps://en.wikipedia.org/wiki/Schelling_point\n\nDue to the strong incentives for consensus, everyone is looking for an\nobvious reference point that they think everyone else will also pick, even\nthough the point itself isn't critical, only that everyone agree on\nwhatever point is picked. Like it or not, the bitcoin-core release, and by\nextension it's committers have a great degree of influence over what the\ncommunity as a whole decides to do. If core screws things up badly enough,\nyes, the community will settle on some other focal point for consensus, but\nthe cost and risk of doing so is high, so there is indeed unavoidable moral\nhazard for whoever has control over any such focus point.\n\nAaron Voisine\nco-founder and CEO\nbreadwallet <http://breadwallet.com>\n\nOn Wed, Dec 16, 2015 at 10:34 AM, Pieter Wuille via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Wed, Dec 16, 2015 at 3:53 PM, Jeff Garzik via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > 2) If block size stays at 1M, the Bitcoin Core developer team should\n> sign a\n> > collective note stating their desire to transition to a new economic\n> policy,\n> > that of \"healthy fee market\" and strongly urge users to examine their fee\n> > policies, wallet software, transaction volumes and other possible User\n> > impacting outcomes.\n>\n> You present this as if the Bitcoin Core development team is in charge\n> of deciding the network consensus rules, and is responsible for making\n> changes to it in order to satisfy economic demand. If that is the\n> case, Bitcoin has failed, in my opinion.\n>\n> What the Bitcoin Core team should do, in my opinion, is merge any\n> consensus change that is uncontroversial. We can certainly -\n> individually or not - propose solutions, and express opinions, but as\n> far as maintainers of the software goes our responsibility is keeping\n> the system running, and risking either a fork or establishing\n> ourselves as the de-facto central bank that can make any change to the\n> system would greatly undermine the system's value.\n>\n> Hard forking changes require that ultimately every participant in the\n> system adopts the new rules. I find it immoral and dangerous to merge\n> such a change without extremely widespread agreement. I am personally\n> fine with a short-term small block size bump to kick the can down the\n> road if that is what the ecosystem desires, but I can only agree with\n> merging it in Core if I'm convinced that there is no strong opposition\n> to it from others.\n>\n> Soft forks on the other hand only require a majority of miners to\n> accept them, and everyone else can upgrade at their leisure or not at\n> all. Yes, old full nodes after a soft fork are not able to fully\n> validate the rules new miners enforce anymore, but they do still\n> verify the rules that their operators opted to enforce. Furthermore,\n> they can't be prevented. For that reason, I've proposed, and am\n> working hard, on an approach that includes Segregated Witness as a\n> first step. It shows the ecosystem that something is being done, it\n> kicks the can down the road, it solves/issues half a dozen other\n> issues at the same time, and it does not require the degree of\n> certainty needed for a hardfork.\n>\n> --\n> Pieter\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151222/2a316e8d/attachment.html>"
            },
            {
                "author": "jl2012",
                "date": "2015-12-16T18:36:00",
                "message_text_only": "I would also like to summarize my observation and thoughts after the \nHong Kong workshop.\n\n1. I'm so glad that I had this opportunity to meet so many smart \ndevelopers who are dedicated to make Bitcoin better. Regular conference \nlike this is very important for a young project, and it is particularly \nimportant for Bitcoin, with consensus as the core value. I hope such a \nconference could be conducted at least once in 2 years in Hong Kong, \nwhich is visa-friendly for most people in both East and West.\n\n2. I think some consensus has emerged at/after the conference. There is \nno doubt that segregated witness will be implemented. For block size, I \nbelieve 2MB as the first step is accepted by the super majority of \nminers, and is generally acceptable / tolerable for devs.\n\n3. Chinese miners are requesting consensus among devs nicely, instead of \nusing their majority hashing power to threaten the community. However, \nif I were allowed to speak for them, I think 2MB is what they really \nwant, and they believe it is for the best interest of themselves and the \nwhole community\n\n4. In the miners round table on the second day, one of the devs \nmentioned that he didn't want to be seen as the decision maker of \nBitcoin. On the other hand, Chinese miners repeatedly mentioned that \nthey want several concrete proposals from devs which they could choose. \nI see no contradiction between these 2 viewpoints.\n\nBelow are some of my personal views:\n\n5. Are we going to have a \"Fee Event\" / \"Economic Change Event\" in 2-6 \nmonths as Jeff mentioned? Frankly speaking I don't know. As the fee \nstarts to increase, spammers will first get squeezed --- which could be \na good thing. However, I have no idea how many txs on the blockchain are \nspam. We also need to consider the effect of halving in July, which may \nlead to speculation bubble and huge legitimate tx volume.\n\n6. I believe we should avoid a radical \"Economic Change Event\" at least \nin the next halving cycle, as Bitcoin was designed to bootstrap the \nadoption by high mining reward in the beginning. For this reason, I \nsupport an early and conservative increase, such as BIP102 or 2-4-8. 2MB \nis accepted by most people and it's better than nothing for BIP101 \nproponents. By \"early\" I mean to be effective by May, at least 2 months \nbefore the halving.\n\n7. Segregated witness must be done. However, it can't replace a \nshort-term block size hardfork for the following reasons:\n(a) SW softfork does not allow higher volume if users are not upgrading. \nIn order to bootstrap the new tx type, we may need the help of \naltruistic miners to provide a fee discount for SW tx.\n(b) In terms of block space saving, SW softfork is most efficient for \nmultisig tx, which is still very uncommon\n(c) My most optimistic guess is SW will be ready in 6 months, which will \nbe very close to halving and potential tx volume burst. And it may not \nbe done in 2016, as it does not only involve consensus code, but also \nchange in the p2p protocol and wallet design\n\n8. Duplex payment channel / Lightning Network may be viable solutions. \nHowever, they won't be fully functional until SW is done so they are \nirrelevant in this discussion\n\n9. No matter what is going to be done / not done, I believe we should \nnow have a clear road map and schedule for the community: a short-term \nhardfork or not? The timeline of SW? It is bad to leave everything \nuncertain and people can't well prepared for any potential radical \nchanges\n\n10. Finally, I hope this discussion remains educated and evidence-based, \nand no circling"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-12-16T22:27:40",
                "message_text_only": "On Wed, Dec 16, 2015 at 1:36 PM, jl2012 <jl2012 at xbt.hk> wrote:\n\n> 4. In the miners round table on the second day, one of the devs mentioned\n> that he didn't want to be seen as the decision maker of Bitcoin. On the\n> other hand, Chinese miners repeatedly mentioned that they want several\n> concrete proposals from devs which they could choose. I see no\n> contradiction between these 2 viewpoints.\n>\n\nThis was a very interesting dynamic, and seems fair (menu).\n\n\n\n> 6. I believe we should avoid a radical \"Economic Change Event\" at least in\n> the next halving cycle, as Bitcoin was designed to bootstrap the adoption\n> by high mining reward in the beginning. For this reason, I support an early\n> and conservative increase, such as BIP102 or 2-4-8. 2MB is accepted by most\n> people and it's better than nothing for BIP101 proponents. By \"early\" I\n> mean to be effective by May, at least 2 months before the halving.\n>\n\nThat was precisely my logic for picking May 5 as the hard fork date.  Some\nbuffer before halving, enough for caution and iteration in the meantime.\n\n\n\n\n\n\n>\n> (c) My most optimistic guess is SW will be ready in 6 months, which will\n> be very close to halving and potential tx volume burst. And it may not be\n> done in 2016, as it does not only involve consensus code, but also change\n> in the p2p protocol and wallet design\n>\n\nNot just wallet design -- you have to game through the standard steps of:\n update dev lib (bitcoin-core.js/bitcoinj) + release cycle, update app +\nrelease cycle, for most actors in the ecosystem, on top of the Bitcoin Core\nroll out.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/9123a94f/attachment.html>"
            },
            {
                "author": "Dave Scotese",
                "date": "2015-12-17T06:12:41",
                "message_text_only": "On Wed, Dec 16, 2015 at 1:11 PM, Pieter Wuille via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n> I indeed think we can communicate much better that deciding consensus\n> rules is not within our power.\n\nI'm not a core dev, so maybe I have the power to change the consensus\nrules.  No one has that power, actually, at least not legitimately.  All we\ncan do is build it and hope enough people find it acceptable to adopt.  Who\ndoesn't want to hard fork to 2MB blocks on May 5th and why not?\n\nI have a bitcoin to be split up among all those who suffer from a May 5,\n2016 hardfork to 2MB blocks if they can prove it to me, or prove it to\nenough engineers that I succumb to peer pressure.  I would have to respect\nthe engineers though.\n\nThere!\n\nNow that we've agreed to have a hard fork on May 5th, 2016, we might decide\nto implement some other methods of avoiding the FFM, like braiding the\nblockchain or flexcap, or just let anyone mining on top of a block make one\nthat is a five or ten Kb larger.\n\nnotplato\n\nOn Wed, Dec 16, 2015 at 2:27 PM, Jeff Garzik via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Wed, Dec 16, 2015 at 1:36 PM, jl2012 <jl2012 at xbt.hk> wrote:\n>\n>> 4. In the miners round table on the second day, one of the devs mentioned\n>> that he didn't want to be seen as the decision maker of Bitcoin. On the\n>> other hand, Chinese miners repeatedly mentioned that they want several\n>> concrete proposals from devs which they could choose. I see no\n>> contradiction between these 2 viewpoints.\n>>\n>\n> This was a very interesting dynamic, and seems fair (menu).\n>\n>\n>\n>> 6. I believe we should avoid a radical \"Economic Change Event\" at least\n>> in the next halving cycle, as Bitcoin was designed to bootstrap the\n>> adoption by high mining reward in the beginning. For this reason, I support\n>> an early and conservative increase, such as BIP102 or 2-4-8. 2MB is\n>> accepted by most people and it's better than nothing for BIP101 proponents.\n>> By \"early\" I mean to be effective by May, at least 2 months before the\n>> halving.\n>>\n>\n> That was precisely my logic for picking May 5 as the hard fork date.  Some\n> buffer before halving, enough for caution and iteration in the meantime.\n>\n>\n>\n>\n>\n>\n>>\n>> (c) My most optimistic guess is SW will be ready in 6 months, which will\n>> be very close to halving and potential tx volume burst. And it may not be\n>> done in 2016, as it does not only involve consensus code, but also change\n>> in the p2p protocol and wallet design\n>>\n>\n> Not just wallet design -- you have to game through the standard steps of:\n>  update dev lib (bitcoin-core.js/bitcoinj) + release cycle, update app +\n> release cycle, for most actors in the ecosystem, on top of the Bitcoin Core\n> roll out.\n>\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n\n\n-- \nI like to provide some work at no charge to prove my value. Do you need a\ntechie?\nI own Litmocracy <http://www.litmocracy.com> and Meme Racing\n<http://www.memeracing.net> (in alpha).\nI'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which\nnow accepts Bitcoin.\nI also code for The Dollar Vigilante <http://dollarvigilante.com/>.\n\"He ought to find it more profitable to play by the rules\" - Satoshi\nNakamoto\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/73119590/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-12-26T16:44:41",
                "message_text_only": "That's exactly the point: a hard fork does not just affect miners, and\ncannot just get decided by miners. All full nodes must have accepted the\nnew rules, or they will be forked off when the hashrate percentage triggers.\n\nFurthermore, 75% is pretty terrible as a switchover point, as it guarantees\nthat old nodes will still see a 25% forked off chain temporarily.\n\nMy opinion is that the role of Bitcoin Core maintainers is judging whether\nconsensus for a hard fork exists, and is technically necessary and safe. We\ndon't need a hashpower vote to decide whether a hardfork is accepted or\nnot, we need to be sure that full noded will accept it, and adopt it in\ntime. A hashpower vote can still be used to be sure that miners _also_\nagree.\n\nCurrently, a large amount of developers and others believe that the\ntreshhold for a hardfork is not reached, especially given the fact that we\nscale in the short term, as well as make many changes that long-term\nbenefit scalability, with just a softfork (which does not require forking\noff nodes that don't adopt the new rules, for whatever reason).\n\n-- \nPieter\nOn Dec 26, 2015 17:25, \"digitsu\" <jerry.d.chan at bittoku.co.jp> wrote:\n\n> So it seems that everyone is in agreement then, since a hard fork to 2M is\n> orthogonal to SW, and also that core should not be involved in dictating\n> what the consensus rules should be, then why not put BIP102 into play with\n> a 75% mining majority activation and let the market decide.\n>\n> As it\u2019s activation only involves the miners, and its implementation\n> doesn\u2019t affect users or exchanges, then it poses no complication to SW\n> which can do done in parallel.\n>\n>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/59c39d2e/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-12-26T17:20:22",
                "message_text_only": "On Dec 26, 2015 5:45 PM, \"Pieter Wuille via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n> My opinion is that the role of Bitcoin Core maintainers is judging\nwhether consensus for a hard fork exists, and is technically necessary and\nsafe. We don't need a hashpower vote to decide whether a hardfork is\naccepted or not, we need to be sure that full noded will accept it, and\nadopt it in time. A hashpower vote can still be used to be sure that miners\n_also_ agree.\n\nTo clarify, that's the role of Bitcoin Core maintainers because they decide\nwhat goes into Bitcoin Core, not because they decide the consensus rules of\nBitcoin. Other full node implementations (say, libbitcoin) will have to\ndecide on their own and Bitcoin Core mainteiners don't have any authority\nover libbitcoin (or other alternative implementations). Nobody has such\nauthority (not even the creator of the system if he was still maintaining\nBitcoin Core).\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/567065fc/attachment.html>"
            },
            {
                "author": "Jonathan Toomim",
                "date": "2015-12-26T22:55:48",
                "message_text_only": "On Dec 26, 2015, at 8:44 AM, Pieter Wuille via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Furthermore, 75% is pretty terrible as a switchover point, as it guarantees that old nodes will still see a 25% forked off chain temporarily.\n> \nYes, 75% plus a grace period is better. I prefer a grace period of about 4000 to 8000 blocks (1 to 2 months).\n\nFrom my discussions with miners, I think we will be able to create a hardfork proposal that reaches about 90% support among miners, or possibly higher. I'll post a summary of those discussions in the next 24 hours.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/ae50613f/attachment-0001.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/ae50613f/attachment-0001.sig>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-12-26T23:01:04",
                "message_text_only": "On Dec 26, 2015 23:55, \"Jonathan Toomim\" <j at toom.im> wrote:\n>\n> On Dec 26, 2015, at 8:44 AM, Pieter Wuille via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>> Furthermore, 75% is pretty terrible as a switchover point, as it\nguarantees that old nodes will still see a 25% forked off chain temporarily.\n>\n> Yes, 75% plus a grace period is better. I prefer a grace period of about\n4000 to 8000 blocks (1 to 2 months).\n\nI think that's extremely short, even assuming there is no controversy about\nchanging the rules at all. Things like BIP65 and BIP66 already took\nsignificantly longer than that, were uncontroversial, and only need miner\nadoption. Full node adoption is even slower.\n\nI think the shortest reasonable timeframe for an uncontroversial hardfork\nis somewhere in the range between 6 and 12 months.\n\nFor a controversial one, not at all.\n\n-- \nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151227/9446849d/attachment.html>"
            },
            {
                "author": "Jonathan Toomim",
                "date": "2015-12-26T23:07:17",
                "message_text_only": "On Dec 26, 2015, at 3:01 PM, Pieter Wuille <pieter.wuille at gmail.com> wrote:\n\n> I think that's extremely short, even assuming there is no controversy about changing the rules at all. Things like BIP65 and BIP66 already took significantly longer than that, were uncontroversial, and only need miner adoption. Full node adoption is even slower.\n> \n\nBIP65 and BIP66 were uncontroversial, but also generally uninteresting. Most people don't care about OP_CLTV right now, and they won't for quite a while longer. They neglect to upgrade their full nodes because there has been no reason to.\n\nGiven that a supermajority of users and miners have been asking for a hard fork to increase the blocksize for years, I do not think that mobilizing people to upgrade their nodes is going to be hard.\n\nWhen we do the hard fork, we will need to encourage people to upgrade their full nodes. We may want to request that miners not trigger the fork until some percentage of visible full nodes have upgraded.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/66d58a47/attachment.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/66d58a47/attachment.sig>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-12-26T23:16:17",
                "message_text_only": "On Dec 27, 2015 00:06, \"Jonathan Toomim\" <j at toom.im> wrote:\n\n> Given that a supermajority of users and miners have been asking for a\nhard fork to increase the blocksize for years, I do not think that\nmobilizing people to upgrade their nodes is going to be hard.\n>\n> When we do the hard fork, we will need to encourage people to upgrade\ntheir full nodes. We may want to request that miners not trigger the fork\nuntil some percentage of visible full nodes have upgraded.\n\nI am generally not interested in a system where we rely on miners to make\nthat judgement call to fork off nodes that don't pay attention and/or\ndisagree with the change. This is not because I don't trust them, but\nbecause I believe one of the principle values of the system is that its\nconsensus system should be hard to change.\n\nI can't tell you what code to run of course, but I can decide what system I\nfind interesting to build. And it seems many people have signed off on\nworking towards a plan that does not include a hard fork being scheduled\nright now: https://bitcoin.org/en/bitcoin-core/capacity-increases\n\nCheers,\n\n-- \nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151227/96ee711e/attachment.html>"
            },
            {
                "author": "Jonathan Toomim",
                "date": "2015-12-27T00:03:58",
                "message_text_only": "On Dec 26, 2015, at 3:16 PM, Pieter Wuille <pieter.wuille at gmail.com> wrote:\n\n> I am generally not interested in a system where we rely on miners to make that judgement call to fork off nodes that don't pay attention and/or disagree with the change. This is not because I don't trust them, but because I believe one of the principle values of the system is that its consensus system should be hard to change.\n> \nI'm not proposing that we rely on miners' assessments of full node counts. I'm simply proposing that they serve as an extra layer of safety.\n\nFor the users that don't pay attention, I don't think the miners should be the sole parties to make that judgment call. That's why I suggested the grace period. I think that 1 or 2 months more than enough time for a business or active bitcoin user to download a new version of the software. I think that 1 or 2 months after a majority of nodes and miners have upgraded is more than more than enough time. For non-active businesses and users, there is no risk from the hard fork. If you're not transacting, you can't be defrauded.\n\nNodes that disagree with the change are welcome to continue to run the old version and watch the small fork if they so choose. Their numbers should be small if indeed this is an uncontroversial hardfork, but they won't be zero, and that's fine. The software supports this (except for peer discovery, which can get a little bit tricky in hardfork scenarios for the minority fork). Miners have no ethical obligation to protect individuals who choose not to follow consensus.\n\nI think that use of the Alert System https://en.bitcoin.it/wiki/Alert_system would be justified in the weeks preceding the hard fork. Maybe we can create an \"Upgrade now!\" message that the new version would ignore, and set it to broadcast forever to all old nodes?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/f3f84eab/attachment.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/f3f84eab/attachment.sig>"
            },
            {
                "author": "Justus Ranvier",
                "date": "2015-12-26T23:15:06",
                "message_text_only": "On 12/26/2015 05:01 PM, Pieter Wuille via bitcoin-dev wrote:\n> I think the shortest reasonable timeframe for an uncontroversial\n> hardfork is somewhere in the range between 6 and 12 months.\n\nThis argument would hold more weight if it didn't looks like a stalling\ntactic in context.\n\n6 months ago, there was a concerted effort to being the process then,\nfor exactly this reason.\n\nAfter 6 months of denial, stonewalling, and generally unproductive\nfighting, the need for proactivity is being acknowledged with no\nreference to the delay.\n\nIf the network ever ends up making a hasty forced upgrade to solve a\ncapacity emergency the responsibility for that difficulty will not fall\non those who did their best to prevent emergency upgrades by planning ahead.\n\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: 0xEAD9E623.asc\nType: application/pgp-keys\nSize: 23337 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/218f1c1b/attachment-0001.bin>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 801 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/218f1c1b/attachment-0001.sig>"
            },
            {
                "author": "Bryan Bishop",
                "date": "2015-12-27T00:13:40",
                "message_text_only": "On Sat, Dec 26, 2015 at 5:15 PM, Justus Ranvier via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On 12/26/2015 05:01 PM, Pieter Wuille via bitcoin-dev wrote:\n> > I think the shortest reasonable timeframe for an uncontroversial\n> > hardfork is somewhere in the range between 6 and 12 months.\n>\n> This argument would hold more weight if it didn't looks like a stalling\n> tactic in context.\n>\n\nI think you'll find that there hasn't been stalling regarding an\nuncontroversial hard-fork deployment. You might be confusing an\nuncontroversial hard-fork decision instead with how developers have brought\nup many issues about various (hard-forking) block size proposals.... I\nsuspect this is what you're intending to mention instead, given your\nmention of \"capacity emergencies\" and also the subject line.\n\n\n> 6 months ago, there was a concerted effort to being the process then,\n> for exactly this reason.\n>\n\nThe uncontroversial hard-fork proposals from 6 months ago were mostly along\nthe lines of jtimon's proposals, which were not about capacity. (Although,\nI should say \"almost entirely uncontroversial\"-- obviously has been some\nminor (and in my opinion, entirely solvable) disagreement regarding\nprioritization of deploying a jtimon's uncontroversial hard-fork idea I\nguess, seeing as how it has not yet happened.)\n\n\n> After 6 months of denial, stonewalling, and generally unproductive\n> fighting, the need for proactivity is being acknowledged with no\n> reference to the delay.\n>\n\nThere wasn't 6 months of \"stonewalling\" or \"denial\" about an\nuncontroversial hard-fork proposal. There has been extensive discussion\nregarding the controversial (flawed?) properties of other (block size)\nproposals. But that's something else. Much of this has been rehashed ad\nnauseum on this mailing list already...  thankfully I think your future\nemails could be improved and made more useful if you were to read the\nmailing list archives, try to employ more careful reasoning, etc. Thanks.\n\n\n> If the network ever ends up making a hasty forced upgrade to solve a\n> capacity emergency the responsibility for that difficulty will not fall\n> on those who did their best to prevent emergency upgrades by planning\n> ahead.\n>\n\n(\"Capacity emergency\" is too ambiguous in this context because of the\ncompeting concerns and tradeoffs regarding transaction rate capacity\nexhaustion vs. p2p low-bandwidth node bandwidth exhaustion.)\n\n- Bryan\nhttp://heybryan.org/\n1 512 203 0507\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/181a6658/attachment.html>"
            },
            {
                "author": "Justus Ranvier",
                "date": "2015-12-27T00:33:58",
                "message_text_only": "On 12/26/2015 06:13 PM, Bryan Bishop wrote:\n> I think you'll find that there hasn't been stalling regarding an\n> uncontroversial hard-fork deployment. You might be confusing an\n> uncontroversial hard-fork decision instead with how developers have\n> brought up many issues about various (hard-forking) block size\n> proposals.... I suspect this is what you're intending to mention\n> instead, given your mention of \"capacity emergencies\" and also the\n> subject line.\n\nI think you'll find that writing in that tone makes one come across as a\ncomplete and utter douchebag.\n\nI suspect what you're intending to do is to use faux-polite\ncondescension to bait me into responding in a way to will justify my\nsubsequent banning from this mailing list so that the people who aren't\ninterested in answering certain uncomfortable questions will have a\nplausible excuse for preventing them from being asked here.\n\n> There wasn't 6 months of \"stonewalling\" or \"denial\" about an\n> uncontroversial hard-fork proposal. There has been extensive discussion\n> regarding the controversial (flawed?) properties of other (block size)\n> proposals. But that's something else. Much of this has been rehashed ad\n> nauseum on this mailing list already...  thankfully I think your future\n> emails could be improved and made more useful if you were to read the\n> mailing list archives, try to employ more careful reasoning, etc. Thanks.\n\nActually there's been 3+ years of stonewalling, deception, conflicts of\ninterest, and outright crimes, which have been generally ignored by\nthose who are desperately attempting to assume good faith.\n\nThe purpose of my email was to remind everyone that nobody is going to\nget away with avoiding ownership of the consequences of their actions.\n\nIf the network experiences a painful upgrade because six months of time\nthat could have been used to prepare a smooth upgrade was lost, the\nindividuals who squandered that time own the result. They can't get\naround it by demanding six additional months, as if they had nothing to\ndo with the six lost months.\n\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: 0xEAD9E623.asc\nType: application/pgp-keys\nSize: 23337 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/fc9ad844/attachment-0001.bin>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 801 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/fc9ad844/attachment-0001.sig>"
            }
        ],
        "thread_summary": {
            "title": "Block size: It's economics & user preparation & moral hazard",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeff Garzik",
                "Bryan Bishop",
                "sickpig at gmail.com",
                "Peter Todd",
                "Dave Scotese",
                "Tier Nolan",
                "Jorge Tim\u00f3n",
                "Jonathan Toomim",
                "Justus Ranvier",
                "Jameson Lopp",
                "jl2012",
                "Pieter Wuille",
                "Aaron Voisine"
            ],
            "messages_count": 31,
            "total_messages_chars_count": 73678
        }
    },
    {
        "title": "[bitcoin-dev] Segregated Witness in the context of Scaling Bitcoin",
        "thread_messages": [
            {
                "author": "Jeff Garzik",
                "date": "2015-12-16T20:38:30",
                "message_text_only": "1. Summary\n\nSegregated Witness (SegWitness, SW) is being presented in the context of\nScaling Bitcoin.  It has useful attributes, notably addressing a major\nmalleability vector, but is not a short term scaling solution.\n\n\n2. Definitions\n\nImport Fee Event, ECE, TFM, FFM from previous email.\n\nOlder clients - Any software not upgraded to SW\n\nNewer clients - Upgraded, SW aware software\n\n\nBlock size - refers to the core block economic resource limited by\nMAX_BLOCK_SIZE.  Witness data (or extension block data) is excluded.\nRequires a hard fork to change.\n\nCore block - Current bitcoin block, with upper bound MAX_BLOCK_SIZE.  Not\nchanged by SW.\n\n\nExtended transaction - Newer, upgraded version of transaction data format.\n\nExtended block - Newer, upgraded version of block data format.\n\n\nEBS - Extended block size.  Block size seen by newer clients.\n\n\n3. Context of analysis\n\nOne proposal presents SW *in lieu of* a hard fork block size increase.\nThis email focuses directly on that.\n\nUseful features outside block size context, such as anti-malleability or\nfraud proof features, are not covered in depth.\n\n\n4.1.  Observations on data structure formats and views\n\nSW creates two *views* of each transaction and block.  SW has blocks and\nextended blocks.  Similarly, there exists transactions and extended\ntransactions.\n\nThis view is rendered to clients depending on compatibility level.  Newer\nclients see extended blocks and extended transactions.  Older clients see\nblocks (limit 1M), and do not see extended blocks.  Older clients see\nupgraded transactions as unsigned, anyone-can-pay transactions.\n\nEach extended transaction exists in two states, one unsigned and one\nsigned, each of which passes validation as a valid bitcoin transaction.\n\n\n4.2.  Observations on behavior of older transaction creation\n\nTransactions created by older clients will not use the extended transaction\nformat.  All data is stored the standard 1M block as today.\n\n\n4.3.  Observations on new block economic model\n\nSW complicates block economics by creating two separate, supply limited\nresources.\n\nThe core block economic resource is heavily contended.  Older clients use\ncore blocks exclusively.  Newer clients use core blocks more\nconservatively, storing as much data as possible in extended blocks.\n\nThe extended block economic resource is less heavily contended, though that\nof course grows over time as clients upgrade.\n\nBecause core blocks are more heavily contended, it is presumed that older\nclients will pay a higher fee than newer clients (subject to elasticity\netc.).\n\n\n5.1.  Problem:  Pace of roll-out will be slow - Whole Ecosystem must be\nconsidered.\n\nThe current apparent proposal is to roll out Segregated Witness as a soft\nfork, and keep block size at 1M.\n\nThe roll-out pace cannot simply be judged by soft fork speed - which is\nmonths at best.  Analysis must the layers above:  Updating bitcoin-core\n(JS) and bitcoinj (Java), and then the timelines to roll out those updates\nto apps, and then the timeline to update those apps to create extended\ntransactions.\n\nOverall, wallet software and programmer libraries must be upgraded to make\nuse of this new format, adding many more months (12+ in some stacks) to the\nroll out timeline.  In the meantime, clients continue to contend entirely\nfor core block space.\n\n\n5.2.  Problem:   Hard fork to bigger block size Just Works(tm) with most\nsoftware, unlike SW.\n\nA simple hard fork such as BIP 102 is automatically compatible with the\nvast range of today's ecosystem software.\n\nSW requires merchants to upgrade almost immediately, requires wallet and\nother peripheral software upgrades to make use of.  Other updates are\nopt-in and occur more slowly.  BIP 70 processors need some updates.\n\nThe number of LOC that must change for BIP 102 is very small, and the\nproblem domain well known, versus SW.\n\n\n5.3.  Problem:   Due to pace, Fee Event not forestalled.\n\nEven presuming SW is merged into Bitcoin Core tomorrow, this does not\naddress the risk of a Fee Event and associated Economic Change in the\ncoming months.\n\n\n5.4.  Problem:   More complex economic policy, new game theory, new bidding\nstructure risks.\n\nSplitting blocks into two pieces, each with separate and distinct behaviors\nand resource values, creates *two fee markets.*\n\nHaving two pricing strata within each block has certainly feasible - that\nis the current mining policy of (1) fee/KB followed by (2) priority/age.\n\nValuable or not - e.g. incentivizing older clients to upgrade - the fact\nremains that SW creates a more-complex bidding structure by creating a\nsecond economic resource.\n\n*This is clearly a change to a new economic policy* with standard risks\nassociated with that.  Will that induce an Economic Change Event (see def\nlast email)?  *Unlikely*, due to slow rollout pace.\n\n\n5.5.  Problem:  Current SW mining algorithm needs improvement\n\nCurrent SW block template maker does a reasonable job, but makes some naive\nassumptions about the fee market across an entire extended block.  This is\na mismatch with the economic reality (just described).\n\n5.6.   Problem:  New, under-analyzed attack surfaces\n\nLess significant and fundamental but still worth noting.\n\nThis is not a fundamental SW problem, but simply standard complexity risk\nfactors:  splitting the signatures away from transactions, and creating a\nnew apparently-unsigned version of the transaction opens the possibility of\nsome network attacks which cause some clients to degrade down from extended\nblock to core block mode temporarily.\n\nThere is a chance of a failure mode that fools older clients into thinking\nfraudulent data is valid (judgement: unlikely vis hashpower but not\nimpossible)\n\n6. Conclusions and recommendations\n\nIt seems unlikely that SW provides scaling in the short term, and SW\nintroduces new economics complexities.\n\nA \"short term bump\" hard fork block size increase addresses economic and\necosystem risks that SW does not.\n\nBump + SW should proceed in parallel, independent tracks, as orthogonal\nissues.\n\n\n7. Appendix - Other SW comments\n\nHard forks provide much stronger validation, and ensure the network\noperates at a fully trustless level.\n\nSW hard fork is preferred, versus soft fork.  Soft forking SW places a huge\namount of trust on miners to validate transaction signatures, versus the\nrest of the network, as the network slowly upgrades to newer clients.\n\nAn SW hard fork could also add several zero-filled placeholders in a merkle\ntree for future use.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/1c334c8f/attachment-0001.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2015-12-16T20:50:15",
                "message_text_only": "A large part of your argument is that SW will take longer to deploy than a hard fork, but I completely disagree. Though I do not agree with some people claiming we can deploy SW significantly faster than a hard fork, once the code is ready (probably a six month affair) we can get it deployed very quickly. It's true the ecosystem may take some time to upgrade, but I see that as a feature, not a bug - we can build up some fee pressure with an immediate release valve available for people to use if they want to pay fewer fees.\n\nOn the other hand, a hard fork, while simpler for the ecosystem to upgrade to, is a  1-2 year affair (after the code is shipped, so at least 1.5-2.5 from today if we all put off heads down and work). One thing that has concerned me greatly through this whole debate is how quickly people seem to think we can roll out a hard fork. Go look at the distribution of node versions on the network today and work backwards to get nearly every node upgraded... Even with a year between fork-version-release and fork-activation, we'd still kill a bunch of nodes and instead of reducing their security model, lead them to be outright robbed.\n\nOn December 16, 2015 12:38:30 PM PST, Jeff Garzik via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>1. Summary\n>\n>Segregated Witness (SegWitness, SW) is being presented in the context\n>of\n>Scaling Bitcoin.  It has useful attributes, notably addressing a major\n>malleability vector, but is not a short term scaling solution.\n>\n>\n>2. Definitions\n>\n>Import Fee Event, ECE, TFM, FFM from previous email.\n>\n>Older clients - Any software not upgraded to SW\n>\n>Newer clients - Upgraded, SW aware software\n>\n>\n>Block size - refers to the core block economic resource limited by\n>MAX_BLOCK_SIZE.  Witness data (or extension block data) is excluded.\n>Requires a hard fork to change.\n>\n>Core block - Current bitcoin block, with upper bound MAX_BLOCK_SIZE. \n>Not\n>changed by SW.\n>\n>\n>Extended transaction - Newer, upgraded version of transaction data\n>format.\n>\n>Extended block - Newer, upgraded version of block data format.\n>\n>\n>EBS - Extended block size.  Block size seen by newer clients.\n>\n>\n>3. Context of analysis\n>\n>One proposal presents SW *in lieu of* a hard fork block size increase.\n>This email focuses directly on that.\n>\n>Useful features outside block size context, such as anti-malleability\n>or\n>fraud proof features, are not covered in depth.\n>\n>\n>4.1.  Observations on data structure formats and views\n>\n>SW creates two *views* of each transaction and block.  SW has blocks\n>and\n>extended blocks.  Similarly, there exists transactions and extended\n>transactions.\n>\n>This view is rendered to clients depending on compatibility level. \n>Newer\n>clients see extended blocks and extended transactions.  Older clients\n>see\n>blocks (limit 1M), and do not see extended blocks.  Older clients see\n>upgraded transactions as unsigned, anyone-can-pay transactions.\n>\n>Each extended transaction exists in two states, one unsigned and one\n>signed, each of which passes validation as a valid bitcoin transaction.\n>\n>\n>4.2.  Observations on behavior of older transaction creation\n>\n>Transactions created by older clients will not use the extended\n>transaction\n>format.  All data is stored the standard 1M block as today.\n>\n>\n>4.3.  Observations on new block economic model\n>\n>SW complicates block economics by creating two separate, supply limited\n>resources.\n>\n>The core block economic resource is heavily contended.  Older clients\n>use\n>core blocks exclusively.  Newer clients use core blocks more\n>conservatively, storing as much data as possible in extended blocks.\n>\n>The extended block economic resource is less heavily contended, though\n>that\n>of course grows over time as clients upgrade.\n>\n>Because core blocks are more heavily contended, it is presumed that\n>older\n>clients will pay a higher fee than newer clients (subject to elasticity\n>etc.).\n>\n>\n>5.1.  Problem:  Pace of roll-out will be slow - Whole Ecosystem must be\n>considered.\n>\n>The current apparent proposal is to roll out Segregated Witness as a\n>soft\n>fork, and keep block size at 1M.\n>\n>The roll-out pace cannot simply be judged by soft fork speed - which is\n>months at best.  Analysis must the layers above:  Updating bitcoin-core\n>(JS) and bitcoinj (Java), and then the timelines to roll out those\n>updates\n>to apps, and then the timeline to update those apps to create extended\n>transactions.\n>\n>Overall, wallet software and programmer libraries must be upgraded to\n>make\n>use of this new format, adding many more months (12+ in some stacks) to\n>the\n>roll out timeline.  In the meantime, clients continue to contend\n>entirely\n>for core block space.\n>\n>\n>5.2.  Problem:   Hard fork to bigger block size Just Works(tm) with\n>most\n>software, unlike SW.\n>\n>A simple hard fork such as BIP 102 is automatically compatible with the\n>vast range of today's ecosystem software.\n>\n>SW requires merchants to upgrade almost immediately, requires wallet\n>and\n>other peripheral software upgrades to make use of.  Other updates are\n>opt-in and occur more slowly.  BIP 70 processors need some updates.\n>\n>The number of LOC that must change for BIP 102 is very small, and the\n>problem domain well known, versus SW.\n>\n>\n>5.3.  Problem:   Due to pace, Fee Event not forestalled.\n>\n>Even presuming SW is merged into Bitcoin Core tomorrow, this does not\n>address the risk of a Fee Event and associated Economic Change in the\n>coming months.\n>\n>\n>5.4.  Problem:   More complex economic policy, new game theory, new\n>bidding\n>structure risks.\n>\n>Splitting blocks into two pieces, each with separate and distinct\n>behaviors\n>and resource values, creates *two fee markets.*\n>\n>Having two pricing strata within each block has certainly feasible -\n>that\n>is the current mining policy of (1) fee/KB followed by (2)\n>priority/age.\n>\n>Valuable or not - e.g. incentivizing older clients to upgrade - the\n>fact\n>remains that SW creates a more-complex bidding structure by creating a\n>second economic resource.\n>\n>*This is clearly a change to a new economic policy* with standard risks\n>associated with that.  Will that induce an Economic Change Event (see\n>def\n>last email)?  *Unlikely*, due to slow rollout pace.\n>\n>\n>5.5.  Problem:  Current SW mining algorithm needs improvement\n>\n>Current SW block template maker does a reasonable job, but makes some\n>naive\n>assumptions about the fee market across an entire extended block.  This\n>is\n>a mismatch with the economic reality (just described).\n>\n>5.6.   Problem:  New, under-analyzed attack surfaces\n>\n>Less significant and fundamental but still worth noting.\n>\n>This is not a fundamental SW problem, but simply standard complexity\n>risk\n>factors:  splitting the signatures away from transactions, and creating\n>a\n>new apparently-unsigned version of the transaction opens the\n>possibility of\n>some network attacks which cause some clients to degrade down from\n>extended\n>block to core block mode temporarily.\n>\n>There is a chance of a failure mode that fools older clients into\n>thinking\n>fraudulent data is valid (judgement: unlikely vis hashpower but not\n>impossible)\n>\n>6. Conclusions and recommendations\n>\n>It seems unlikely that SW provides scaling in the short term, and SW\n>introduces new economics complexities.\n>\n>A \"short term bump\" hard fork block size increase addresses economic\n>and\n>ecosystem risks that SW does not.\n>\n>Bump + SW should proceed in parallel, independent tracks, as orthogonal\n>issues.\n>\n>\n>7. Appendix - Other SW comments\n>\n>Hard forks provide much stronger validation, and ensure the network\n>operates at a fully trustless level.\n>\n>SW hard fork is preferred, versus soft fork.  Soft forking SW places a\n>huge\n>amount of trust on miners to validate transaction signatures, versus\n>the\n>rest of the network, as the network slowly upgrades to newer clients.\n>\n>An SW hard fork could also add several zero-filled placeholders in a\n>merkle\n>tree for future use.\n>\n>\n>------------------------------------------------------------------------\n>\n>_______________________________________________\n>bitcoin-dev mailing list\n>bitcoin-dev at lists.linuxfoundation.org\n>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/4c7fc0c2/attachment.html>"
            },
            {
                "author": "Jameson Lopp",
                "date": "2015-12-16T21:51:47",
                "message_text_only": "On Wed, Dec 16, 2015 at 12:50 PM, Matt Corallo via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> A large part of your argument is that SW will take longer to deploy than a\n> hard fork, but I completely disagree. Though I do not agree with some\n> people claiming we can deploy SW significantly faster than a hard fork,\n> once the code is ready (probably a six month affair) we can get it deployed\n> very quickly. It's true the ecosystem may take some time to upgrade, but I\n> see that as a feature, not a bug - we can build up some fee pressure with\n> an immediate release valve available for people to use if they want to pay\n> fewer fees.\n>\n> On the other hand, a hard fork, while simpler for the ecosystem to upgrade\n> to, is a 1-2 year affair (after the code is shipped, so at least 1.5-2.5\n> from today if we all put off heads down and work). One thing that has\n> concerned me greatly through this whole debate is how quickly people seem\n> to think we can roll out a hard fork. Go look at the distribution of node\n> versions on the network today and work backwards to get nearly every node\n> upgraded... Even with a year between fork-version-release and\n> fork-activation, we'd still kill a bunch of nodes and instead of reducing\n> their security model, lead them to be outright robbed.\n>\n>\nOver a year seems to be an extraordinarily long time frame is for deploying\na hard fork. It looks like <https://bitnodes.21.co/dashboard/?days=365> 75%\nof reachable nodes have upgraded in the past 6 months while as much as 25%\nmay not have been upgraded in over a year. However, viewing historical\nstats of version upgrades doesn't seem to be an appropriate comparison\nbecause node operators have never been faced with the same incentive to\nupgrade. We can point to unintentional forks in the past that have been\nresolved fairly quickly by reaching out to miners, but it's also a poor\ncomparison. Unfortunately, we have no way of knowing what percentage of\nnodes are economically important - a great deal of them may be running and\nnot even be used by the operators.\n\nPerhaps it would be better if we were to formalize the expectations for\nfull node operators, but it seems to me that node operators have a\nresponsibility to keep themselves informed and decide when it is\nappropriate to update their software. I'm not so sure that it's the rest of\nthe ecosystem's responsibility to wait around for laggards.\n\n- Jameson\n\nOn December 16, 2015 12:38:30 PM PST, Jeff Garzik via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>\n>> 1. Summary\n>>\n>> Segregated Witness (SegWitness, SW) is being presented in the context of\n>> Scaling Bitcoin.  It has useful attributes, notably addressing a major\n>> malleability vector, but is not a short term scaling solution.\n>>\n>>\n>> 2. Definitions\n>>\n>> Import Fee Event, ECE, TFM, FFM from previous email.\n>>\n>> Older clients - Any software not upgraded to SW\n>>\n>> Newer clients - Upgraded, SW aware software\n>>\n>>\n>> Block size - refers to the core block economic resource limit ed by\n>> MAX_BLOCK_SIZE.  Witness data (or extension block data) is excluded.\n>> Requires a hard fork to change.\n>>\n>> Core block - Current bitcoin block, with upper bound MAX_BLOCK_SIZE.  Not\n>> changed by SW.\n>>\n>>\n>> Extended transaction - Newer, upgraded version of transaction data format.\n>>\n>> Extended block - Newer, upgraded version of block data format.\n>>\n>>\n>> EBS - Extended block size.  Block size seen by newer clients.\n>>\n>>\n>> 3. Context of analysis\n>>\n>> One proposal presents SW *in lieu of* a hard fork block size increase.\n>> This email focuses directly on that.\n>>\n>> Useful features outside block size context, such as anti-malleability or\n>> fraud proof features, are not covered in depth.\n>>\n>>\n>> 4.1.  Observations on data structure formats and views\n>>\n>> SW creates two *views* of each transaction and block.  SW has blocks and\n>> extended blocks.  Similarly, there exists transactions and extended\n>> transactions.\n>>\n>> This view is rendered to clients depending on compatibility level.  Newer\n>> clients see extended blocks and extended transactions.  Older clients see\n>> blocks (limit 1M), and do not see extended blocks.  Older clients see\n>> upgraded transactions as unsigned, anyone-can-pay transactions.\n>>\n>> Each extended transaction exists in two states, one unsigned and one\n>> signed, each of which passes validation as a valid bitcoin transaction.\n>>\n>>\n>> 4.2.  Observations on behavior of older transaction creation\n>>\n>> Transactions created by older clients will not use the extended\n>> transaction format.  All data is stored the standard 1M block as today.\n>>\n>>\n>> 4.3.  Observations on new block economic model\n>>\n>> SW complicates block economics by creating two separate, supply limited\n>> resources.\n>>\n>> The core block economic resource is heavily contended.  Older clients use\n>> core blocks exclusively.  Newer clients use core block s more\n>> conservatively, storing as much data as possible in extended blocks.\n>>\n>> The extended block economic resource is less heavily contended, though\n>> that of course grows over time as clients upgrade.\n>>\n>> Because core blocks are more heavily contended, it is presumed that older\n>> clients will pay a higher fee than newer clients (subject to elasticity\n>> etc.).\n>>\n>>\n>> 5.1.  Problem:  Pace of roll-out will be slow - Whole Ecosystem must be\n>> considered.\n>>\n>> The current apparent proposal is to roll out Segregated Witness as a soft\n>> fork, and keep block size at 1M.\n>>\n>> The roll-out pace cannot simply be judged by soft fork speed - which is\n>> months at best.  Analysis must the layers above:  Updating bitcoin-core\n>> (JS) and bitcoinj (Java), and then the timelines to roll out those updates\n>> to apps, and then the timeline to update those apps to create extended\n>> transactions.\n>>\n>> Overall, wallet software and programmer libraries must be upgraded to\n>> make use of this new format, adding many more months (12+ in some stacks)\n>> to the roll out timeline.  In the meantime, clients continue to contend\n>> entirely for core block space.\n>>\n>>\n>> 5.2.  Problem:   Hard fork to bigger block size Just Works(tm) with most\n>> software, unlike SW.\n>>\n>> A simple hard fork such as BIP 102 is automatically compatible with the\n>> vast range of today's ecosystem software.\n>>\n>> SW requires merchants to upgrade almost immediately, requires wallet and\n>> other peripheral software upgrades to make use of.  Other updates are\n>> opt-in and occur more slowly.  BIP 70 processors need some updates.\n>>\n>> The number of LOC that must change for BIP 102 is very small, and the\n>> problem domain well known, versus SW.\n>>\n>>\n>> 5.3.  Problem:   Due to pace, Fee Event not forestalled.\n>>\n>> Even presuming SW is merged into Bitcoin Core tomorrow, this does not\n>> address the risk of a Fee Event and associated Economic Change in the\n>> coming months.\n>>\n>>\n>> 5.4.  Problem:   More complex economic policy, new game theory, new\n>> bidding structure risks.\n>>\n>> Splitting blocks into two pieces, each with separate and distinct\n>> behaviors and resource values, creates *two fee markets.*\n>>\n>> Having two pricing strata within each block has certainly feasible - that\n>> is the current mining policy of (1) fee/KB followed by (2) priority/age.\n>>\n>> Valuable or not - e.g. incentivizing older clients to upgrade - the fact\n>> remains that SW creates a more-complex bidding structure by creating a\n>> second economic resource.\n>>\n>> *This is clearly a change to a new economic policy* with standard risks\n>> associated with that.  Will that induce an Economic C hange Event (see def\n>> last email)?  *Unlikely*, due to slow rollout pace.\n>>\n>>\n>> 5.5.  Problem:  Current SW mining algorithm needs improvement\n>>\n>> Current SW block template maker does a reasonable job, but makes some\n>> naive assumptions about the fee market across an entire extended block.\n>> This is a mismatch with the economic reality (just described).\n>>\n>> 5.6.   Problem:  New, under-analyzed attack surfaces\n>>\n>> Less significant and fundamental but still worth noting.\n>>\n>> This is not a fundamental SW problem, but simply standard complexity risk\n>> factors:  splitting the signatures away from transactions, and creating a\n>> new apparently-unsigned version of the transaction opens t he possibility\n>> of some network attacks which cause some clients to degrade down from\n>> extended block to core block mode temporarily.\n>>\n>> There is a chance of a failure mode that fools older clients into\n>> thinking fraudulent data is valid (judgement: unlikely vis hashpower but\n>> not impossible)\n>>\n>> 6. Conclusions and recommendations\n>>\n>> It seems unlikely that SW provides scaling in the short term, and SW\n>> introduces new economics complexities.\n>>\n>> A \"short term bump\" hard fork block size increase addresses economic and\n>> ecosystem risks that SW does not.\n>>\n>> Bump + SW should proce ed in parallel, independent tracks, as orthogonal\n>> issues.\n>>\n>>\n>> 7. Appendix - Other SW comments\n>>\n>> Hard forks provide much stronger validation, and ensure the network\n>> operates at a fully trustless level.\n>>\n>> SW hard fork is preferred, versus soft fork.  Soft forking SW places a\n>> huge amount of trust on miners to validate transaction signatures, versus\n>> the rest of the network, as the network slowly upgrades to newer clients.\n>>\n>> An SW hard fork could also add several zero-filled placeholders in a\n>> merkle tree for future use.\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>> ------------------------------\n>>\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/3bfa78f6/attachment-0001.html>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-12-17T02:21:22",
                "message_text_only": "On Wed, Dec 16, 2015 at 3:50 PM, Matt Corallo <lf-lists at mattcorallo.com>\nwrote:\n\n> A large part of your argument is that SW will take longer to deploy than a\n> hard fork, but I completely disagree. Though I do not agree with some\n> people claiming we can deploy SW significantly faster than a hard fork,\n> once the code is ready (probably a six month affair) we can get it deployed\n> very quickly. It's true the ecosystem may take some time to upgrade, but I\n> see that as a feature, not a bug - we can build up some fee pressure with\n> an immediate release valve available for people to use if they want to pay\n> fewer fees.\n>\n\nThat's taking a big risk.  \"Build up some fee pressure\" is essentially\nrisking a Fee Event if uptake is slower than planned, or traffic is greater\nthan expected.\n\n\n\n>\n> On the other hand, a hard fork, while simpler for the ecosystem to upgrade\n> to, is a 1-2 year affair (after the code is shipped, so at least 1.5-2.5\n> from today if we all put off heads down and work). One thing that has\n> concerned me greatly through this whole debate is how quickly people seem\n> to think we can roll out a hard fork. Go look at the distribution of node\n> versions on the network today and work backwards to get nearly every node\n> upgraded... Even with a year between fork-version-release and\n> fork-activation, we'd still kill a bunch of nodes and instead of reducing\n> their security model, lead them to be outright robbed.\n>\n\nA hard fork will never achieve 100%  There are many credible folks and\nestimates who feel a May hard fork is reasonable and doable.\n\nFurther, hard forks restore the full trustless nature of the post-hard-fork\nnodes.  Soft forks continually erode that.  That's why SW should come via\nhard fork.  The end result is more secure - 100% validation of witness\ntransactions.\n\nIf regular hard fork plans are proposed in public, many months in advance,\nthere is plenty of time for the community to react.  Hard forks create a\nmore predictable market and environment for Users, and a more secure\nnetwork.\n\nFurther, even if you believe SW makes hard fork unnecessary, it is the\nresponsible thing to code and communicate to users the plan for a Fee Event\njust in case SW uptake and extension block use does not match theoretical\nprojections of SW proponents.\n\nFinally, SW does not eliminate and is orthogonal to Short Term Problem #1\n(orig. email - drift into ECE)\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/f044cec8/attachment-0001.html>"
            },
            {
                "author": "Eric Lombrozo",
                "date": "2015-12-17T02:44:56",
                "message_text_only": "There are no good short-term scaling solutions...this is a very hard problem that necessarily requires a lot of out-of-the-box thinking, something 2015 has seen a LOT of...and I'm optimistic about the ideas presented thus far.\n\nAt least SW *is* a scaling solution (albeit most of the important benefits are long term). The issue of fee events has nothing to do with scaling - it has to do with economics...specifically whether we should be subsidizing transactions, who should pay the bill for it, etc. My own personal opinion is that increasing validation costs works against adoption, not for it...even if it artificially keeps fees low - and we'll have to deal with a fee event sooner or later anyhow. You may disagree with my opinion, but please, let's stop confounding the economic issues with actual scaling.\n\nOn December 16, 2015 6:21:22 PM PST, Jeff Garzik via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>On Wed, Dec 16, 2015 at 3:50 PM, Matt Corallo\n><lf-lists at mattcorallo.com>\n>wrote:\n>\n>> A large part of your argument is that SW will take longer to deploy\n>than a\n>> hard fork, but I completely disagree. Though I do not agree with some\n>> people claiming we can deploy SW significantly faster than a hard\n>fork,\n>> once the code is ready (probably a six month affair) we can get it\n>deployed\n>> very quickly. It's true the ecosystem may take some time to upgrade,\n>but I\n>> see that as a feature, not a bug - we can build up some fee pressure\n>with\n>> an immediate release valve available for people to use if they want\n>to pay\n>> fewer fees.\n>>\n>\n>That's taking a big risk.  \"Build up some fee pressure\" is essentially\n>risking a Fee Event if uptake is slower than planned, or traffic is\n>greater\n>than expected.\n>\n>\n>\n>>\n>> On the other hand, a hard fork, while simpler for the ecosystem to\n>upgrade\n>> to, is a 1-2 year affair (after the code is shipped, so at least\n>1.5-2.5\n>> from today if we all put off heads down and work). One thing that has\n>> concerned me greatly through this whole debate is how quickly people\n>seem\n>> to think we can roll out a hard fork. Go look at the distribution of\n>node\n>> versions on the network today and work backwards to get nearly every\n>node\n>> upgraded... Even with a year between fork-version-release and\n>> fork-activation, we'd still kill a bunch of nodes and instead of\n>reducing\n>> their security model, lead them to be outright robbed.\n>>\n>\n>A hard fork will never achieve 100%  There are many credible folks and\n>estimates who feel a May hard fork is reasonable and doable.\n>\n>Further, hard forks restore the full trustless nature of the\n>post-hard-fork\n>nodes.  Soft forks continually erode that.  That's why SW should come\n>via\n>hard fork.  The end result is more secure - 100% validation of witness\n>transactions.\n>\n>If regular hard fork plans are proposed in public, many months in\n>advance,\n>there is plenty of time for the community to react.  Hard forks create\n>a\n>more predictable market and environment for Users, and a more secure\n>network.\n>\n>Further, even if you believe SW makes hard fork unnecessary, it is the\n>responsible thing to code and communicate to users the plan for a Fee\n>Event\n>just in case SW uptake and extension block use does not match\n>theoretical\n>projections of SW proponents.\n>\n>Finally, SW does not eliminate and is orthogonal to Short Term Problem\n>#1\n>(orig. email - drift into ECE)\n>\n>\n>------------------------------------------------------------------------\n>\n>_______________________________________________\n>bitcoin-dev mailing list\n>bitcoin-dev at lists.linuxfoundation.org\n>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n-- \nSent from my Android device with K-9 Mail. Please excuse my brevity.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/e2c4511c/attachment.html>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-12-17T02:58:35",
                "message_text_only": "On Wed, Dec 16, 2015 at 9:44 PM, Eric Lombrozo <elombrozo at gmail.com> wrote:\n\n> At least SW *is* a scaling solution (albeit most of the important benefits\n> are long term). The issue of fee events has nothing to do with scaling - it\n> has to do with economics...specifically whether we should be subsidizing\n> transactions, who should pay the bill for it, etc. My own personal opinion\n> is that increasing validation costs works against adoption, not for\n> it...even if it artificially keeps fees low - and we'll have to deal with a\n> fee event sooner or later anyhow. You may disagree with my opinion, but\n> please, let's stop confounding the economic issues with actual scaling.\n>\n\nAt least on my part, the title of the 1st email was \"It's economics & ...\"\nand focused on (a) economics and (b) transition issues.  There was no\nconfounding.  There was a list of real problems and risks taken when 1M is\nnot lifted in the short term.\n\nThus \"SW is orthogonal\" in these emails, because these problems remain\nregardless of SW or no, as the 1st email outlined.\n\nThe 2nd email addresses the specific assertion of \"no 1M hard fork needed,\nbecause SW.\"\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/f3dd723a/attachment.html>"
            },
            {
                "author": "Adam Back",
                "date": "2015-12-17T03:48:29",
                "message_text_only": "There are a range of opinions about input assumptions by different\npeople.  In each case, short of misunderstanding, if we have the same\ninput assumptions we're going to reach the same conclusions.  This is\nthe way of the world in a meritocracy.  The interesting point is to\ncompare the input assumptions and try to figure out which are more\nrealistic, pragmatic and achieve the best outcome.\n\nIt might be instructive to re-read Greg's roadmap and others to\nre-read Jeff's original post (I will).\n\nThere is a proposed roadmap and soft-fork block-size increase and code\nthat Pieter is working on.  There has been rationale described for\nthis approach, and it achieves many useful things both short, mid and\nlong term for scale and other issues.\n\nThere seem to be a range of opinions on the fee market, and one\nquestion is when do we deem it safe to aim to be prepared to support a\nfee market.\n\nHow elastic is block-size demand?  (I think there is evidence of some\nelasticity which indicates a partly working fee market already).  What\nI mean by elasticity of block-size demand is there are off-chain\ntransactions and people make an economic choice of whether to go on\nchain or not, and the vast majority of transactions, all told, are\noff-chain.  Clearly it is ideal if they all go on chain, scale\npermitting.\n\nIf we look at the roadmap at high-level:\n\n1) bump (seg-wit or ...)\n2) network improvements (IBLT/weak-block/other)\n3) longer term dynamic block-size (flexcap)\n4) write-cache (lightning)\n\nIt would probably be good to see some work on preparing for fee\nmarkets.  That has happened somewhat recently in response to the\nstress tests.  We do have an observed problem that if there is no\nincentive to prepare, the improvements dont happen, and so we can\nnever be ready for a fee market.  That's kind of how we got here,\npeople were talking about fee-estimation and dynamic fees several\nyears ago before the block-size went from 250kB to 750kB, and then\nlost interest as there was another 500kB to play with.  There could be\na best practice doc written asking people to prepare.  That might\nhelp.\n\nPresumably it's good if we do see the fee market more, for it to come\nin gradually.  Flexcap probably helps there because the block-size\nitself becomes elastic to demand (pay for bigger blocks).\n\nIf we want to avoid a fee market for the immediate term, are we more\nworried about period 1, or period 2 or 3.  Probably 2 is more of a\nworry as we're scaling in 1 where in period 2 we're preparing for\nscaling and more time has passed for demand to grow.  That might for\nexample argue for seg-wit because it brings us closer to 4) and if we\nspread things out we might delay the possibility to do lightning as\nthere is only so many cycles for forks (hard or soft) in testing,\ndeployment planning etc so it can be good to have a holistic view.\n\nAlso the question of time-frame that is safe for soft-forks or\nhard-forks is another input where views seem to vary.  I think some\npeople are more optimistic about being able to avoid people losing\nmoney in fast hard-forks.  One lesson on users, is users find failure\nmodes that testing cant, or do things you would expect them not to do.\n\nAlso we're calling hard-forks things that are really soft-forks to SPV\nclients, and hard-forks only to full-nodes.  If we wanted to make a\nreal economic choice, we could artificially make an SPV hard-fork,\nhowever that would make upgrade harder.\n\nAs I said in an earlier email I think everyone is empathetic to user\nrequirements, including economic desires - but Bitcoin has inherent\nconstraints that are complex to improve.  Each proposal is trying to\nbest meet those holistic user requirements.  There are no free lunches\nand we dont want to economically hurt anyone in total or as a group or\ntype of use.  Not all requirements can be met, they are in a trade\noff, so that calls for balance, planning and transparency.\n\nThis is also a market, we can discuss protocol tradeoffs without being\nmelodramatic - would be kind of undesirable if a dramatic or emotive\nway to express something as easily or more clearly expressed in\ntechnical constructive words is moving the price around.\n\nAdam\n\nOn 17 December 2015 at 03:58, Jeff Garzik via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> On Wed, Dec 16, 2015 at 9:44 PM, Eric Lombrozo <elombrozo at gmail.com> wrote:\n>>\n>> At least SW *is* a scaling solution (albeit most of the important benefits\n>> are long term). The issue of fee events has nothing to do with scaling - it\n>> has to do with economics...specifically whether we should be subsidizing\n>> transactions, who should pay the bill for it, etc. My own personal opinion\n>> is that increasing validation costs works against adoption, not for\n>> it...even if it artificially keeps fees low - and we'll have to deal with a\n>> fee event sooner or later anyhow. You may disagree with my opinion, but\n>> please, let's stop confounding the economic issues with actual scaling.\n>\n>\n> At least on my part, the title of the 1st email was \"It's economics & ...\"\n> and focused on (a) economics and (b) transition issues.  There was no\n> confounding.  There was a list of real problems and risks taken when 1M is\n> not lifted in the short term.\n>\n> Thus \"SW is orthogonal\" in these emails, because these problems remain\n> regardless of SW or no, as the 1st email outlined.\n>\n> The 2nd email addresses the specific assertion of \"no 1M hard fork needed,\n> because SW.\"\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "jl2012",
                "date": "2015-12-17T05:32:11",
                "message_text_only": "There are at least 2 proposals on the table:\n\n1. SWSF (segwit soft fork) with 1MB virtual block limit, approximately \nequals to 2MB actual limit\n\n2. BIP102: 2MB actual limit\n\nSince the actual limits for both proposals are approximately the same, \nit is not a determining factor in this discussion\n\nThe biggest advantage of SWSF is its softfork nature. However, its \ncomplexity is not comparable with any previous softforks we had. It is \nreasonable to doubt if it could be ready in 6 months\n\nFor BIP102, although it is a hardfork, it is a very simple one and could \nbe deployed with ISM in less than a month. It is even simpler than \nBIP34, 66, and 65.\n\nSo we have a very complicated softfork vs. a very simple hardfork. The \nonly reason makes BIP102 not easy is the fact that it's a hardfork.\n\nThe major criticism for a hardfork is requiring everyone to upgrade. Is \nthat really a big problem?\n\nFirst of all, hardfork is not a totally unknown territory. BIP50 was a \nhardfork. The accident happened on 13 March 2013. Bitcoind 0.8.1 was \nreleased on 18 March, which only gave 2 months of grace period for \neveryone to upgrade. The actual hardfork happened on 16 August. \nEverything completed in 5 months without any panic or chaos. This \nexperience strongly suggests that 5 months is already safe for a simple \nhardfork. (in terms of simplicity, I believe BIP102 is even simpler than \nBIP50)\n\nAnother experience is from BIP66. The 0.10.0 was released on 16 Feb \n2015, exactly 10 months ago. I analyze the data on \nhttps://bitnodes.21.co and found that 4600 out of 5090 nodes (90.4%) \nindicate BIP66 support. Considering this is a softfork, I consider this \nas very good adoption already.\n\nWith the evidence from BIP50 and BIP66, I believe a 5 months \npre-announcement is good enough for BIP102. As the vast majority of \nminers have declared their support for a 2MB solution, the legacy 1MB \nfork will certainly be abandoned and no one will get robbed.\n\n\nMy primary proposal:\n\nNow - 15 Jan 2016: formally consult the major miners and merchants if \nthey support an one-off rise to 2MB. I consider approximately 80% of \nmining power and 80% of trading volume would be good enough\n\n16 - 31 Jan 2016: release 0.11.3 with BIP102 with ISM vote requiring 80% \nof hashing power\n\n1 Jun 2016: the first day a 2MB block may be allowed\n\nBefore 31 Dec 2016: release SWSF\n\n\n\nMy secondary proposal:\n\nNow: Work on SWSF in a turbo mode and have a deadline of 1 Jun 2016\n\n1 Jun 2016: release SWSF\n\nWhat if the deadline is not met? Maybe pushing an urgent BIP102 if \nthings become really bad.\n\n\nIn any case, I hope a clear decision and road map could be made now. \nThis topic has been discussed to death. We are just bringing further \nuncertainty if we keep discussing.\n\n\nMatt Corallo via bitcoin-dev \u65bc 2015-12-16 15:50 \u5beb\u5230:\n> A large part of your argument is that SW will take longer to deploy\n> than a hard fork, but I completely disagree. Though I do not agree\n> with some people claiming we can deploy SW significantly faster than a\n> hard fork, once the code is ready (probably a six month affair) we can\n> get it deployed very quickly. It's true the ecosystem may take some\n> time to upgrade, but I see that as a feature, not a bug - we can build\n> up some fee pressure with an immediate release valve available for\n> people to use if they want to pay fewer fees.\n> \n>  On the other hand, a hard fork, while simpler for the ecosystem to\n> upgrade to, is a 1-2 year affair (after the code is shipped, so at\n> least 1.5-2.5 from today if we all put off heads down and work). One\n> thing that has concerned me greatly through this whole debate is how\n> quickly people seem to think we can roll out a hard fork. Go look at\n> the distribution of node versions on the network today and work\n> backwards to get nearly every node upgraded... Even with a year\n> between fork-version-release and fork-activation, we'd still kill a\n> bunch of nodes and instead of reducing their security model, lead them\n> to be outright robbed.\n>"
            },
            {
                "author": "Corey Haddad",
                "date": "2015-12-17T07:54:41",
                "message_text_only": "A planned hardfork, similar to certain softforks, leaves users with some\nreduction in security.  It does not leave them defenseless.  Consider the\nfollowing:\n\n1: Hard to be robbed on the basis of hashpower.\n\nIn reality the old chain will see mining all but stop, and blocks would be\nhours to days apart even if a couple percentage points of hashpower failed\nto switch over.  Six confirmations would certainly take days.  If the fork\ncan be scheduled at the beginning of a difficulty period, the old chain\nwould almost certainly not even ever make it to the next retargeting.\n\n2: Hard to be robber on the basis of awareness.\n\nExpect there to be fairly widespread coverage in the Bitcoin press, and as\nthe fork draws near, maybe coverage in business and tech publications.\nFurther, the alert keys will certainly be used, so node operators will get\nthe message directly.\n\n3: There still needs to be a targeted attack by a fraudster on an unaware\nnode operator.\n\nTo fall victim, one needs to give up something of value to an attacker in\nexchange for Bitcoins (on the old chain).  The typical uninitiated\nfull-node user (probably a small subset anyway) is typically going to be\nbuying bitcoin from a trusted source, and then saving or spending them, or\nperhaps gambling.  They are not, typically, going to be providing a service\nor selling goods in exchange for Bitcoin unless they are at least somewhat\naware of what is going on in the Bitcoin space.  It's possible, of course,\nbut we are talking about small numbers here of people who fit the above.\n\nAll three parts of the above would have to go perfectly wrong for someone\nto loose out.  Someone somewhere will probably get scammed as a result of a\nhardfork.  That stinks, and we should make reasonable efforts to help them\navoid that fate.  But at this point in Bitcoin's development, it is still\nin beta, it's still an economic experiment, and we can't allow the software\nto become hamstrung out of fear that some inattentive user might bungle\ntheir security.  If they merely waited for 6 confirmations, as is the\nstandard advice, they would be waiting for days.  If that along doesn't\ngive them a hint that something is wrong, it might still be too early days\nfor them to be playing with Bitcoin for anything important.\n\nI support a hardfork deployment that takes 80% of hashpower activate + a\n4-month delay.\n\nOn Wed, Dec 16, 2015 at 9:32 PM, jl2012 via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> There are at least 2 proposals on the table:\n>\n> 1. SWSF (segwit soft fork) with 1MB virtual block limit, approximately\n> equals to 2MB actual limit\n>\n> 2. BIP102: 2MB actual limit\n>\n> Since the actual limits for both proposals are approximately the same, it\n> is not a determining factor in this discussion\n>\n> The biggest advantage of SWSF is its softfork nature. However, its\n> complexity is not comparable with any previous softforks we had. It is\n> reasonable to doubt if it could be ready in 6 months\n>\n> For BIP102, although it is a hardfork, it is a very simple one and could\n> be deployed with ISM in less than a month. It is even simpler than BIP34,\n> 66, and 65.\n>\n> So we have a very complicated softfork vs. a very simple hardfork. The\n> only reason makes BIP102 not easy is the fact that it's a hardfork.\n>\n> The major criticism for a hardfork is requiring everyone to upgrade. Is\n> that really a big problem?\n>\n> First of all, hardfork is not a totally unknown territory. BIP50 was a\n> hardfork. The accident happened on 13 March 2013. Bitcoind 0.8.1 was\n> released on 18 March, which only gave 2 months of grace period for everyone\n> to upgrade. The actual hardfork happened on 16 August. Everything completed\n> in 5 months without any panic or chaos. This experience strongly suggests\n> that 5 months is already safe for a simple hardfork. (in terms of\n> simplicity, I believe BIP102 is even simpler than BIP50)\n>\n> Another experience is from BIP66. The 0.10.0 was released on 16 Feb 2015,\n> exactly 10 months ago. I analyze the data on https://bitnodes.21.co and\n> found that 4600 out of 5090 nodes (90.4%) indicate BIP66 support.\n> Considering this is a softfork, I consider this as very good adoption\n> already.\n>\n> With the evidence from BIP50 and BIP66, I believe a 5 months\n> pre-announcement is good enough for BIP102. As the vast majority of miners\n> have declared their support for a 2MB solution, the legacy 1MB fork will\n> certainly be abandoned and no one will get robbed.\n>\n>\n> My primary proposal:\n>\n> Now - 15 Jan 2016: formally consult the major miners and merchants if they\n> support an one-off rise to 2MB. I consider approximately 80% of mining\n> power and 80% of trading volume would be good enough\n>\n> 16 - 31 Jan 2016: release 0.11.3 with BIP102 with ISM vote requiring 80%\n> of hashing power\n>\n> 1 Jun 2016: the first day a 2MB block may be allowed\n>\n> Before 31 Dec 2016: release SWSF\n>\n>\n>\n> My secondary proposal:\n>\n> Now: Work on SWSF in a turbo mode and have a deadline of 1 Jun 2016\n>\n> 1 Jun 2016: release SWSF\n>\n> What if the deadline is not met? Maybe pushing an urgent BIP102 if things\n> become really bad.\n>\n>\n> In any case, I hope a clear decision and road map could be made now. This\n> topic has been discussed to death. We are just bringing further uncertainty\n> if we keep discussing.\n>\n>\n> Matt Corallo via bitcoin-dev \u65bc 2015-12-16 15:50 \u5beb\u5230:\n>\n>> A large part of your argument is that SW will take longer to deploy\n>> than a hard fork, but I completely disagree. Though I do not agree\n>> with some people claiming we can deploy SW significantly faster than a\n>> hard fork, once the code is ready (probably a six month affair) we can\n>> get it deployed very quickly. It's true the ecosystem may take some\n>> time to upgrade, but I see that as a feature, not a bug - we can build\n>> up some fee pressure with an immediate release valve available for\n>> people to use if they want to pay fewer fees.\n>>\n>>  On the other hand, a hard fork, while simpler for the ecosystem to\n>> upgrade to, is a 1-2 year affair (after the code is shipped, so at\n>> least 1.5-2.5 from today if we all put off heads down and work). One\n>> thing that has concerned me greatly through this whole debate is how\n>> quickly people seem to think we can roll out a hard fork. Go look at\n>> the distribution of node versions on the network today and work\n>> backwards to get nearly every node upgraded... Even with a year\n>> between fork-version-release and fork-activation, we'd still kill a\n>> bunch of nodes and instead of reducing their security model, lead them\n>> to be outright robbed.\n>>\n>>\n> _______________________________________________\n>\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/5e55d210/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-12-17T13:09:05",
                "message_text_only": "Although I agree that how safe a pre-hardfork upgrade period is depends on\nthe complexity of the changes (we should assume everyone may need time to\nreimplementat it themselves in their own implementations, not just upgrade\nbitcoin core) and bip102 is indeed a very simple hardfork, I think less\nthan 6 months for a hardfork is starting to push it too much.\nFor a more complex hardfork (say, a SW hardfork or a collection of many\nlittle fixes) I believe 1 year or more would make more sense.\n\nBIP99 recommends a time threshold (height or median time) + 95% miner\nupgrade confirmation with BIP9 (version bits).\nSo how about the following plan?\n\n1) Deploy BIP102 when its ready + 6 median time months + 95% miner upgrade\nconfirmation\n\n2) Deploy SW when it's ready + 95% miner upgrade confirmation via bip9.\n\nNote that both \"when it's ready\" depend on something we are not paying a\nlot of attention to: bip9's implementation (just like bip113, bip68-112,\nbip99, the coinbase-commitments-cleanup post-SW uncontroversial hardfork,\netc).\n\nUnless I'm missing something, 2 mb x4 = 8mb, so bip102 + SW is already\nequivalent to the 2-4-8 \"compromise\" proposal (which by the way I never\nliked, because I don't think anybody should be in a position to\n\"compromise\" anything and because I don't see how \"let's avoid an\nunavoidable economic change for a little bit longer\" arguments can\nreasoably claim that \"we need to kick the can down the road exactly 3 more\ntimes\" or whatever is the reasoning behind it).\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151217/e44914d7/attachment.html>"
            },
            {
                "author": "sickpig at gmail.com",
                "date": "2015-12-17T15:51:19",
                "message_text_only": "On Thu, Dec 17, 2015 at 2:09 PM, Jorge Tim\u00f3n <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n> Unless I'm missing something, 2 mb x4 = 8mb, so bip102 + SW is already\n> equivalent to the 2-4-8 \"compromise\" proposal (which by the way I never\n> liked, because I don't think anybody should be in a position to\n> \"compromise\" anything and because I don't see how \"let's avoid an\n> unavoidable economic change for a little bit longer\" arguments can\n> reasoably claim that \"we need to kick the can down the road exactly 3 more\n> times\" or whatever is the reasoning behind it).\n>\n\nisn't SegWit gain ~75%? hence 2mb x 1.75 = 3.5.\n\n4x is theoric gain you get in case of 2-2 multisig txs.\n\nam I missign something obvious?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151217/54f03105/attachment-0001.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2015-12-17T17:55:41",
                "message_text_only": "On Thu, Dec 17, 2015 at 04:51:19PM +0100, sickpig--- via bitcoin-dev wrote:\n> On Thu, Dec 17, 2015 at 2:09 PM, Jorge Tim\u00f3n wrote:\n> > Unless I'm missing something, 2 mb x4 = 8mb, so bip102 + SW is already\n> > equivalent to the 2-4-8 \"compromise\" proposal [...]\n> isn't SegWit gain ~75%? hence 2mb x 1.75 = 3.5.\n\nSegwit as proposed gives a 75% *discount* to witness data with the\nsame limit, so at a 1MB limit, that might give you (eg) 2.05MB made up\nof 650kB of base block data plus 1.4MB of witness data; where 650kB +\n1.4MB/4 = 1MB at the 1MB limit; or 4.1MB made up of 1.3MB of base plus\n2.8MB of witness, for 1.3MB+2.8MB/4 = 2MB at a 2MB limit.\n\n> 4x is theoric gain you get in case of 2-2 multisig txs.\n\nWith segregated witness, 2-2 multisig transactions are made up of 94B\nof base data, plus about 214B of witness data; discounting the witness\ndata by 75% gives 94+214/4=148 bytes. That compares to about 301B for\na 2-2 multisig transaction with P2SH rather than segwit, and 301/148\ngives about a 2.03x gain, not a 4x gain. A 2.05x gain is what I assumed\nto get the numbers above.\n\nYou get further improvements with, eg, 3-of-3 multisig, but to get\nthe full, theoretical 4x gain you'd need a fairly degenerate looking\ntransaction.\n\nPay to public key hash with segwit lets you move about half the\ntransaction data into the witness, giving about a 1.6x improvement by\nmy count (eg 1.6MB = 800kB of base data plus 800kB of witness data,\nwhere 800kB+800kB/4=1MB), so I think a gain of between 1.6 and 2.0 is\na reasonable expectation to have for the proposed segwit scheme overall.\n\nCheers,\naj"
            },
            {
                "author": "sickpig at gmail.com",
                "date": "2015-12-18T10:01:52",
                "message_text_only": "Anthony,\n\n\nOn Thu, Dec 17, 2015 at 6:55 PM, Anthony Towns via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Thu, Dec 17, 2015 at 04:51:19PM +0100, sickpig--- via bitcoin-dev wrote:\n> > On Thu, Dec 17, 2015 at 2:09 PM, Jorge Tim\u00f3n wrote:\n> > > Unless I'm missing something, 2 mb x4 = 8mb, so bip102 + SW is already\n> > > equivalent to the 2-4-8 \"compromise\" proposal [...]\n> > isn't SegWit gain ~75%? hence 2mb x 1.75 = 3.5.\n>\n> Segwit as proposed gives a 75% *discount* to witness data with the\n> same limit, so at a 1MB limit, that might give you (eg) 2.05MB made up\n> of 650kB of base block data plus 1.4MB of witness data; where 650kB +\n> 1.4MB/4 = 1MB at the 1MB limit; or 4.1MB made up of 1.3MB of base plus\n> 2.8MB of witness, for 1.3MB+2.8MB/4 = 2MB at a 2MB limit.\n>\n> > 4x is theoric gain you get in case of 2-2 multisig txs.\n>\n> With segregated witness, 2-2 multisig transactions are made up of 94B\n> of base data, plus about 214B of witness data; discounting the witness\n> data by 75% gives 94+214/4=148 bytes. That compares to about 301B for\n> a 2-2 multisig transaction with P2SH rather than segwit, and 301/148\n> gives about a 2.03x gain, not a 4x gain. A 2.05x gain is what I assumed\n> to get the numbers above.\n>\n> You get further improvements with, eg, 3-of-3 multisig, but to get\n> the full, theoretical 4x gain you'd need a fairly degenerate looking\n> transaction.\n>\n> Pay to public key hash with segwit lets you move about half the\n> transaction data into the witness, giving about a 1.6x improvement by\n> my count (eg 1.6MB = 800kB of base data plus 800kB of witness data,\n> where 800kB+800kB/4=1MB), so I think a gain of between 1.6 and 2.0 is\n> a reasonable expectation to have for the proposed segwit scheme overall.\n>\n>\nmany thanks for the explanation.\n\nso it should be fair to say that BIP 102 + SW would bring a gain between\n2*1.6 and 2*2.\n\nJust for the sake of simplicity if we take the middle of the interval we\ncould say\nthat BIP102 + SW will bring us a max block (virtual) size equal to 1MB * 2\n* 1.8 = 3.6\n\nIs it right?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/7a7b71da/attachment-0001.html>"
            },
            {
                "author": "Mark Friedenbach",
                "date": "2015-12-19T07:50:41",
                "message_text_only": "Not entirely correct, no. Edge cases also matter. Segwit is described as\n4MB because that is the largest possible combined block size that can be\nconstructed. BIP 102 + segwit would allow a maximum relay of 8MB. So you\nhave to be confident that an 8MB relay size would be acceptable, even if a\nblock full of actual transactions would be closer to 3.5MB.\n\nOn Fri, Dec 18, 2015 at 6:01 PM, sickpig--- via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Anthony,\n>\n>\n> On Thu, Dec 17, 2015 at 6:55 PM, Anthony Towns via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> On Thu, Dec 17, 2015 at 04:51:19PM +0100, sickpig--- via bitcoin-dev\n>> wrote:\n>> > On Thu, Dec 17, 2015 at 2:09 PM, Jorge Tim\u00f3n wrote:\n>> > > Unless I'm missing something, 2 mb x4 = 8mb, so bip102 + SW is already\n>> > > equivalent to the 2-4-8 \"compromise\" proposal [...]\n>> > isn't SegWit gain ~75%? hence 2mb x 1.75 = 3.5.\n>>\n>> Segwit as proposed gives a 75% *discount* to witness data with the\n>> same limit, so at a 1MB limit, that might give you (eg) 2.05MB made up\n>> of 650kB of base block data plus 1.4MB of witness data; where 650kB +\n>> 1.4MB/4 = 1MB at the 1MB limit; or 4.1MB made up of 1.3MB of base plus\n>> 2.8MB of witness, for 1.3MB+2.8MB/4 = 2MB at a 2MB limit.\n>>\n>> > 4x is theoric gain you get in case of 2-2 multisig txs.\n>>\n>> With segregated witness, 2-2 multisig transactions are made up of 94B\n>> of base data, plus about 214B of witness data; discounting the witness\n>> data by 75% gives 94+214/4=148 bytes. That compares to about 301B for\n>> a 2-2 multisig transaction with P2SH rather than segwit, and 301/148\n>> gives about a 2.03x gain, not a 4x gain. A 2.05x gain is what I assumed\n>> to get the numbers above.\n>>\n>> You get further improvements with, eg, 3-of-3 multisig, but to get\n>> the full, theoretical 4x gain you'd need a fairly degenerate looking\n>> transaction.\n>>\n>> Pay to public key hash with segwit lets you move about half the\n>> transaction data into the witness, giving about a 1.6x improvement by\n>> my count (eg 1.6MB = 800kB of base data plus 800kB of witness data,\n>> where 800kB+800kB/4=1MB), so I think a gain of between 1.6 and 2.0 is\n>> a reasonable expectation to have for the proposed segwit scheme overall.\n>>\n>>\n> many thanks for the explanation.\n>\n> so it should be fair to say that BIP 102 + SW would bring a gain between\n> 2*1.6 and 2*2.\n>\n> Just for the sake of simplicity if we take the middle of the interval we\n> could say\n> that BIP102 + SW will bring us a max block (virtual) size equal to 1MB * 2\n> * 1.8 = 3.6\n>\n> Is it right?\n>\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151219/991342c9/attachment-0001.html>"
            },
            {
                "author": "Dave Scotese",
                "date": "2015-12-19T23:03:20",
                "message_text_only": "A couple observations:\n\n   - The consensus block limit is different than the disk space required to\n   do validation.  Some participants are worried about one and some about the\n   other, and sometimes they feel what amounts to an imaginary contention\n   because they perceive these two different things as the same.  They are\n   both addressed by scaling solutions, but to different degrees.  This is the\n   most concrete I can get about my impression whenever someone writes \"not\n   correct.\"  Less concrete is my usual impression, \"you're both right.\"\n\n   - \"Kicking the can\" has value, but no one has connected the value to the\n   phrase, so here it is: The more time we have to make changes, the better\n   the changes will be.  Of course it's a trade-off (because we suffer through\n   that extra time with the unsolved problem), but using (or thinking of)\n   \"kicking the can\" as bad is a mistake.\n\n   - Whether or not there is a massive campaign targeting *current\n   bitcoiners* has a very strong effect on upgrade rates.\n\nIt seems that a hardfork to a 2MB limit on 5/5/16 is a tad more than one\nLOC, since we want an if-then around it so it doesn't happen til the agreed\ndate.  But I still support it.\n\nOn Fri, Dec 18, 2015 at 11:50 PM, Mark Friedenbach via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Not entirely correct, no. Edge cases also matter. Segwit is described as\n> 4MB because that is the largest possible combined block size that can be\n> constructed. BIP 102 + segwit would allow a maximum relay of 8MB. So you\n> have to be confident that an 8MB relay size would be acceptable, even if a\n> block full of actual transactions would be closer to 3.5MB.\n>\n> On Fri, Dec 18, 2015 at 6:01 PM, sickpig--- via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Anthony,\n>>\n>>\n>> On Thu, Dec 17, 2015 at 6:55 PM, Anthony Towns via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> On Thu, Dec 17, 2015 at 04:51:19PM +0100, sickpig--- via bitcoin-dev\n>>> wrote:\n>>> > On Thu, Dec 17, 2015 at 2:09 PM, Jorge Tim\u00f3n wrote:\n>>> > > Unless I'm missing something, 2 mb x4 = 8mb, so bip102 + SW is\n>>> already\n>>> > > equivalent to the 2-4-8 \"compromise\" proposal [...]\n>>> > isn't SegWit gain ~75%? hence 2mb x 1.75 = 3.5.\n>>>\n>>> Segwit as proposed gives a 75% *discount* to witness data with the\n>>> same limit, so at a 1MB limit, that might give you (eg) 2.05MB made up\n>>> of 650kB of base block data plus 1.4MB of witness data; where 650kB +\n>>> 1.4MB/4 = 1MB at the 1MB limit; or 4.1MB made up of 1.3MB of base plus\n>>> 2.8MB of witness, for 1.3MB+2.8MB/4 = 2MB at a 2MB limit.\n>>>\n>>> > 4x is theoric gain you get in case of 2-2 multisig txs.\n>>>\n>>> With segregated witness, 2-2 multisig transactions are made up of 94B\n>>> of base data, plus about 214B of witness data; discounting the witness\n>>> data by 75% gives 94+214/4=148 bytes. That compares to about 301B for\n>>> a 2-2 multisig transaction with P2SH rather than segwit, and 301/148\n>>> gives about a 2.03x gain, not a 4x gain. A 2.05x gain is what I assumed\n>>> to get the numbers above.\n>>>\n>>> You get further improvements with, eg, 3-of-3 multisig, but to get\n>>> the full, theoretical 4x gain you'd need a fairly degenerate looking\n>>> transaction.\n>>>\n>>> Pay to public key hash with segwit lets you move about half the\n>>> transaction data into the witness, giving about a 1.6x improvement by\n>>> my count (eg 1.6MB = 800kB of base data plus 800kB of witness data,\n>>> where 800kB+800kB/4=1MB), so I think a gain of between 1.6 and 2.0 is\n>>> a reasonable expectation to have for the proposed segwit scheme overall.\n>>>\n>>>\n>> many thanks for the explanation.\n>>\n>> so it should be fair to say that BIP 102 + SW would bring a gain between\n>> 2*1.6 and 2*2.\n>>\n>> Just for the sake of simplicity if we take the middle of the interval we\n>> could say\n>> that BIP102 + SW will bring us a max block (virtual) size equal to 1MB *\n>> 2 * 1.8 = 3.6\n>>\n>> Is it right?\n>>\n>>\n>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n\n\n-- \nI like to provide some work at no charge to prove my value. Do you need a\ntechie?\nI own Litmocracy <http://www.litmocracy.com> and Meme Racing\n<http://www.memeracing.net> (in alpha).\nI'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which\nnow accepts Bitcoin.\nI also code for The Dollar Vigilante <http://dollarvigilante.com/>.\n\"He ought to find it more profitable to play by the rules\" - Satoshi\nNakamoto\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151219/367b6d92/attachment.html>"
            },
            {
                "author": "Mark Friedenbach",
                "date": "2015-12-17T09:33:26",
                "message_text_only": "There are many reasons to support segwit beyond it being a soft-fork. For\nexample:\n\n* the limitation of non-witness data to no more than 1MB makes the\nquadratic scaling costs in large transaction validation no worse than they\ncurrently are;\n* redeem scripts in witness use a more accurate cost accounting than\nnon-witness data (further improvements to this beyond what Pieter has\nimplemented are possible); and\n* segwit provides features (e.g. opt-in malleability protection) which are\nrequired by higher-level scaling solutions.\n\nWith that in mind I really don't understand the viewpoint that it would be\nbetter to engage a strictly inferior proposal such as a simple adjustment\nof the block size to 2MB.\n\nOn Thu, Dec 17, 2015 at 1:32 PM, jl2012 via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> There are at least 2 proposals on the table:\n>\n> 1. SWSF (segwit soft fork) with 1MB virtual block limit, approximately\n> equals to 2MB actual limit\n>\n> 2. BIP102: 2MB actual limit\n>\n> Since the actual limits for both proposals are approximately the same, it\n> is not a determining factor in this discussion\n>\n> The biggest advantage of SWSF is its softfork nature. However, its\n> complexity is not comparable with any previous softforks we had. It is\n> reasonable to doubt if it could be ready in 6 months\n>\n> For BIP102, although it is a hardfork, it is a very simple one and could\n> be deployed with ISM in less than a month. It is even simpler than BIP34,\n> 66, and 65.\n>\n> So we have a very complicated softfork vs. a very simple hardfork. The\n> only reason makes BIP102 not easy is the fact that it's a hardfork.\n>\n> The major criticism for a hardfork is requiring everyone to upgrade. Is\n> that really a big problem?\n>\n> First of all, hardfork is not a totally unknown territory. BIP50 was a\n> hardfork. The accident happened on 13 March 2013. Bitcoind 0.8.1 was\n> released on 18 March, which only gave 2 months of grace period for everyone\n> to upgrade. The actual hardfork happened on 16 August. Everything completed\n> in 5 months without any panic or chaos. This experience strongly suggests\n> that 5 months is already safe for a simple hardfork. (in terms of\n> simplicity, I believe BIP102 is even simpler than BIP50)\n>\n> Another experience is from BIP66. The 0.10.0 was released on 16 Feb 2015,\n> exactly 10 months ago. I analyze the data on https://bitnodes.21.co and\n> found that 4600 out of 5090 nodes (90.4%) indicate BIP66 support.\n> Considering this is a softfork, I consider this as very good adoption\n> already.\n>\n> With the evidence from BIP50 and BIP66, I believe a 5 months\n> pre-announcement is good enough for BIP102. As the vast majority of miners\n> have declared their support for a 2MB solution, the legacy 1MB fork will\n> certainly be abandoned and no one will get robbed.\n>\n>\n> My primary proposal:\n>\n> Now - 15 Jan 2016: formally consult the major miners and merchants if they\n> support an one-off rise to 2MB. I consider approximately 80% of mining\n> power and 80% of trading volume would be good enough\n>\n> 16 - 31 Jan 2016: release 0.11.3 with BIP102 with ISM vote requiring 80%\n> of hashing power\n>\n> 1 Jun 2016: the first day a 2MB block may be allowed\n>\n> Before 31 Dec 2016: release SWSF\n>\n>\n>\n> My secondary proposal:\n>\n> Now: Work on SWSF in a turbo mode and have a deadline of 1 Jun 2016\n>\n> 1 Jun 2016: release SWSF\n>\n> What if the deadline is not met? Maybe pushing an urgent BIP102 if things\n> become really bad.\n>\n>\n> In any case, I hope a clear decision and road map could be made now. This\n> topic has been discussed to death. We are just bringing further uncertainty\n> if we keep discussing.\n>\n>\n> Matt Corallo via bitcoin-dev \u65bc 2015-12-16 15:50 \u5beb\u5230:\n>\n>> A large part of your argument is that SW will take longer to deploy\n>> than a hard fork, but I completely disagree. Though I do not agree\n>> with some people claiming we can deploy SW significantly faster than a\n>> hard fork, once the code is ready (probably a six month affair) we can\n>> get it deployed very quickly. It's true the ecosystem may take some\n>> time to upgrade, but I see that as a feature, not a bug - we can build\n>> up some fee pressure with an immediate release valve available for\n>> people to use if they want to pay fewer fees.\n>>\n>>  On the other hand, a hard fork, while simpler for the ecosystem to\n>> upgrade to, is a 1-2 year affair (after the code is shipped, so at\n>> least 1.5-2.5 from today if we all put off heads down and work). One\n>> thing that has concerned me greatly through this whole debate is how\n>> quickly people seem to think we can roll out a hard fork. Go look at\n>> the distribution of node versions on the network today and work\n>> backwards to get nearly every node upgraded... Even with a year\n>> between fork-version-release and fork-activation, we'd still kill a\n>> bunch of nodes and instead of reducing their security model, lead them\n>> to be outright robbed.\n>>\n>>\n> _______________________________________________\n>\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151217/49c13a41/attachment-0001.html>"
            },
            {
                "author": "jl2012",
                "date": "2015-12-17T10:00:24",
                "message_text_only": "I know my reply is a long one but please read before you hit send. I \nhave 2 proposals: fast BIP102 + slow SWSF and fast SWSF only. I guess no \none here is arguing for not doing segwit; and it is on the top of my \nwish list. My main argument (maybe also Jeff's) is that segwit is too \ncomplicated and may not be a viable short term solution (with the \nreasons I listed that I don't want to repeat)\n\nAnd also I don't agree with you that BIP102 is *strictly* inferior than \nsegwit. We never had a complex softfork like segwit, but we did have a \nsuccessful simple hardfork (BIP50), and BIP102 is very simple. (Details \nin my last post. I'm not going to repeat)\n\nMark Friedenbach \u65bc 2015-12-17 04:33 \u5beb\u5230:\n> There are many reasons to support segwit beyond it being a soft-fork.\n> For example:\n> \n> * the limitation of non-witness data to no more than 1MB makes the\n> quadratic scaling costs in large transaction validation no worse than\n> they currently are;\n> * redeem scripts in witness use a more accurate cost accounting than\n> non-witness data (further improvements to this beyond what Pieter has\n> implemented are possible); and\n> * segwit provides features (e.g. opt-in malleability protection) which\n> are required by higher-level scaling solutions.\n> \n> With that in mind I really don't understand the viewpoint that it\n> would be better to engage a strictly inferior proposal such as a\n> simple adjustment of the block size to 2MB."
            },
            {
                "author": "Anthony Towns",
                "date": "2015-12-17T10:57:13",
                "message_text_only": "On Thu, Dec 17, 2015 at 12:32:11AM -0500, jl2012 via bitcoin-dev wrote:\n> There are at least 2 proposals on the table:\n> 1. SWSF (segwit soft fork) with 1MB virtual block limit, approximately\n>    equals to 2MB actual limit\n> 2. BIP102: 2MB actual limit\n\nI think there's a few variants of (2) -- there's \"just 2MB\", \"2MB now,\n4MB in a while, 8MB after that\", \"1MB for a while longer, then 2MB,\nthen 4MB\" (halved from 2/4/8 since segwit gives 1.6x-2x benefit), and\nvariations of those with different dates, whether or not to smooth out\nthe increases to avoid economic shocks, and how to determine activation\n(flag day? miner consensus? combination?).\n\n> Since the actual limits for both proposals are approximately the same, it is\n> not a determining factor in this discussion\n\nThat's true on the benefit side (both give about double the number of\nordinary transactions per block; though segregated witness has other\nbenefits). On the cost side, the limits are different:\n\n * worst case block data size is 2x for BIP102, 4x for segwit (affecting\n   bandwidth, latency and storage costs for nodes)\n\n * worst case sigops is 2x for BIP102, but the same as today for segwit\n   (affecting block validation time)\n\n * worst case bytes to hash a block is 4x for BIP102 (doubling block\n   size and sigops), but the same as today for segwit (again affecting\n   block validation time)\n\n * worst case UTXO bloat is 2x for BIP102, but the same as today for\n   segwit (affecting memory usage, and validation time)\n\nIn the \"expected\" case (where people aren't attacking the blockchain)\nI think they're the same on all these metrics. But increasing the\nlimits could easily make attacks more common, especially if it makes\nthem more effective.\n\nI think the main attack vector of these is that increasing block\nvalidation time via too many (active) sigops or too many bytes hashed\nallows a selfish mining attack, but I'm not clear enough on how that\nwould work exactly to estimate where the boundary between acceptable and\nunacceptable risk is (and how feasible non-consensus-level countermeasures\nmight be).\n\nBut at 1x sigops, you can already (accidently!) construct blocks that\ntake minutes to verify; and at 4x you can probably already construct a\nblock that takes 10 minutes to verify, which would probably be pretty\nbad... But I'm not sure this isn't already exploitable, so maybe we're\nalready assuming a certain level of altruism and making things worse\ndoesn't actually make them worse?\n\nI think it would be good for BIP102 or similar to include an evaluation\nof that risk before being deployed [0].\n\n> The major criticism for a hardfork is requiring everyone to upgrade. Is that\n> really a big problem?\n\nYes. That doesn't mean it's not worth it, though.\n\n(The 2-month timeline for the BIP50 accidental hardfork to be accepted\non the network seems persuasive to me that it's possible to roll out a\ndeliberate, SPV-compatible, hardfork on today's network in 3-6 months)\n\n> My primary proposal:\n> 1 Jun 2016: the first day a 2MB block may be allowed\n> \n> My secondary proposal:\n> 1 Jun 2016: release SWSF\n\nI think it makes sense to just do both of these independently; ie:\n\n * release segwit via softfork ASAP (perhaps targetting March or April\n   to get it included in bitcoin, activation a month or three\n   afterwards?), with virtual block size calculated as proposed and\n   capped by MAX_BLOCK_SIZE [1]\n\n * increase MAX_BLOCK_SIZE via hardfork to 2MB after block 420,000\n   (phased in gradually? with future scheduled increases to 4MB or 8MB?)\n\nIf segwit gets delayed because it's complicated, that's okay; if\nit comes out earlier, that's okay too. If the hardfork gets delayed\nbecause miners aren't ready or because it's better to introduce it in a\nstaggered fashion, or because there's no clear evaluation of the risks,\nthat's okay too.\n\nBut more importantly, it allows evaluate the pros and cons of each\nimplementation separately and on its own merits, rather than arguing\nagainst working on one just because you're in favour of doing the\nother ASAP.\n\n\nThey have benefits if you combine them too; for instance, if the\nMAX_BLOCK_SIZE increase is phased in rather than done as a step increase\n(ie block x's limit is 1MB, block x+1's limit is 1.005MB or similar,\nand block x+2's limit is 1.01MB, etc) having segwit available in parallel\ncould provide a helpful escape valve: if an individual bitcoin user has\nbeen dying for more capacity, they can spend the time/effort to update\ntheir software for segwit and get it immediately without having to wait\nas the consensus limits rise.\n\nConversely, having both segwit and a phased increase to MAX_BLOCK_SIZE\nmeans that miners generally won't be immediately mining 2MB (or 4MB)\nblocks halfway through the year, which should avoid both technological\nshocks (bandwidth just doubled!) or economic shocks (supply has increased\nso fees have plummeted), which could be good.\n\n\nFWIW, the worst case scenarios are:\n\n * block data size:\n     BIP102:  2x   (worst/avg)\n     segwit:  4x   (worst, ~2x avg)\n     both:    8x   (worst, ~4x avg)\n     BIP101:  8x   (worst/avg)\n\n * sigops per block:\n     BIP102:  2x\n     segwit:  1x\n     both:    2x\n     BIP101:  8x\n\n * bytes hashed per block:\n     BIP102:  4x\n     segwit:  1x\n     both:    4x\n     BIP101:  64x\n\n * UTXO rate of increase:\n     BIP102:  2x\n     segwit:  1x\n     both:    2x\n     BIP101:  8x\n\nCompared to the (expected, eventual, near-term) benefits:\n\n * transactions per block:\n     BIP102:  2x\n     segwit:  1.6x-2x\n     both:    3.2x-4x\n     BIP101:  8x\n\n * misc:\n     BIP101/2: planned hardforks are possible, bitcoin community governance\n       is demonstrably working, etc\n     segwit: malleability fixes, script improvements, lightning\n       enablement, etc\n\nThe block data is the only case where the average case is already just\nabout the worst case; for the others, as long as the worst case doesn't\ninspire new attacks, the future average case should just increase in\nproportion to the additional transactions.\n\nCheers,\naj\n\n[0] (and segwit should actually account for sigops in witness data before\n     being deployed...)\n\n[1] If segwit warrants a hardfork to clean up data structures, I think\n    that should be deferred until well after the MAX_BLOCK_SIZE hardfork,\n    rather than trying to do it at the same time. As such, doing segwit by\n    soft fork in the short term seems to make sense, since it also helps\n    with transaction malleability and further improvements to script."
            },
            {
                "author": "Marcel Jamin",
                "date": "2015-12-17T06:14:13",
                "message_text_only": "Maybe we should first gather concrete estimates about and roughly agree on\n\n- how long SW (SF) development will probably take\n\n- how long the ecosystem needs to prepare for a hardfork (SW (HF) or a\nsimple can kicking block size increase)\n\nOpinions differ wildly from what it looks like, but maybe we can get to\nestimates that the majority here can accept.\n\n---\n\nPersonally, I think that the disclaimer \"Bitcoin is an experiment\" is\npervasive. It's still a pre-release, even with a $6bn vote of confidence.\nIf you don't follow developments in this phase, don't upgrade and then have\nan elevated risk of losing money by getting scammed, then that's a little\nbit your fault. I'd absolutely support a change in mentality on that once\n1.0.0 arrives, but until then is bitcoin a work-in-progress experiment and\na high risk investment.\n\nA planned hard-fork is an experiment that needs to be run anyway. When do\nwe want to do that, if not now?\n\n\n2015-12-16 21:50 GMT+01:00 Matt Corallo via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org>:\n\n> A large part of your argument is that SW will take longer to deploy than a\n> hard fork, but I completely disagree. Though I do not agree with some\n> people claiming we can deploy SW significantly faster than a hard fork,\n> once the code is ready (probably a six month affair) we can get it deployed\n> very quickly. It's true the ecosystem may take some time to upgrade, but I\n> see that as a feature, not a bug - we can build up some fee pressure with\n> an immediate release valve available for people to use if they want to pay\n> fewer fees.\n>\n> On the other hand, a hard fork, while simpler for the ecosystem to upgrade\n> to, is a 1-2 year affair (after the code is shipped, so at least 1.5-2.5\n> from today if we all put off heads down and work). One thing that has\n> concerned me greatly through this whole debate is how quickly people seem\n> to think we can roll out a hard fork. Go look at the distribution of node\n> versions on the network today and work backwards to get nearly every node\n> upgraded... Even with a year between fork-version-release and\n> fork-activation, we'd still kill a bunch of nodes and instead of reducing\n> their security model, lead them to be outright robbed.\n>\n> On December 16, 2015 12:38:30 PM PST, Jeff Garzik via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>\n>> 1. Summary\n>>\n>> Segregated Witness (SegWitness, SW) is being presented in the context of\n>> Scaling Bitcoin.  It has useful attributes, notably addressing a major\n>> malleability vector, but is not a short term scaling solution.\n>>\n>>\n>> 2. Definitions\n>>\n>> Import Fee Event, ECE, TFM, FFM from previous email.\n>>\n>> Older clients - Any software not upgraded to SW\n>>\n>> Newer clients - Upgraded, SW aware software\n>>\n>>\n>> Block size - refers to the core block economic resource limit ed by\n>> MAX_BLOCK_SIZE.  Witness data (or extension block data) is excluded.\n>> Requires a hard fork to change.\n>>\n>> Core block - Current bitcoin block, with upper bound MAX_BLOCK_SIZE.  Not\n>> changed by SW.\n>>\n>>\n>> Extended transaction - Newer, upgraded version of transaction data format.\n>>\n>> Extended block - Newer, upgraded version of block data format.\n>>\n>>\n>> EBS - Extended block size.  Block size seen by newer clients.\n>>\n>>\n>> 3. Context of analysis\n>>\n>> One proposal presents SW *in lieu of* a hard fork block size increase.\n>> This email focuses directly on that.\n>>\n>> Useful features outside block size context, such as anti-malleability or\n>> fraud proof features, are not covered in depth.\n>>\n>>\n>> 4.1.  Observations on data structure formats and views\n>>\n>> SW creates two *views* of each transaction and block.  SW has blocks and\n>> extended blocks.  Similarly, there exists transactions and extended\n>> transactions.\n>>\n>> This view is rendered to clients depending on compatibility level.  Newer\n>> clients see extended blocks and extended transactions.  Older clients see\n>> blocks (limit 1M), and do not see extended blocks.  Older clients see\n>> upgraded transactions as unsigned, anyone-can-pay transactions.\n>>\n>> Each extended transaction exists in two states, one unsigned and one\n>> signed, each of which passes validation as a valid bitcoin transaction.\n>>\n>>\n>> 4.2.  Observations on behavior of older transaction creation\n>>\n>> Transactions created by older clients will not use the extended\n>> transaction format.  All data is stored the standard 1M block as today.\n>>\n>>\n>> 4.3.  Observations on new block economic model\n>>\n>> SW complicates block economics by creating two separate, supply limited\n>> resources.\n>>\n>> The core block economic resource is heavily contended.  Older clients use\n>> core blocks exclusively.  Newer clients use core block s more\n>> conservatively, storing as much data as possible in extended blocks.\n>>\n>> The extended block economic resource is less heavily contended, though\n>> that of course grows over time as clients upgrade.\n>>\n>> Because core blocks are more heavily contended, it is presumed that older\n>> clients will pay a higher fee than newer clients (subject to elasticity\n>> etc.).\n>>\n>>\n>> 5.1.  Problem:  Pace of roll-out will be slow - Whole Ecosystem must be\n>> considered.\n>>\n>> The current apparent proposal is to roll out Segregated Witness as a soft\n>> fork, and keep block size at 1M.\n>>\n>> The roll-out pace cannot simply be judged by soft fork speed - which is\n>> months at best.  Analysis must the layers above:  Updating bitcoin-core\n>> (JS) and bitcoinj (Java), and then the timelines to roll out those updates\n>> to apps, and then the timeline to update those apps to create extended\n>> transactions.\n>>\n>> Overall, wallet software and programmer libraries must be upgraded to\n>> make use of this new format, adding many more months (12+ in some stacks)\n>> to the roll out timeline.  In the meantime, clients continue to contend\n>> entirely for core block space.\n>>\n>>\n>> 5.2.  Problem:   Hard fork to bigger block size Just Works(tm) with most\n>> software, unlike SW.\n>>\n>> A simple hard fork such as BIP 102 is automatically compatible with the\n>> vast range of today's ecosystem software.\n>>\n>> SW requires merchants to upgrade almost immediately, requires wallet and\n>> other peripheral software upgrades to make use of.  Other updates are\n>> opt-in and occur more slowly.  BIP 70 processors need some updates.\n>>\n>> The number of LOC that must change for BIP 102 is very small, and the\n>> problem domain well known, versus SW.\n>>\n>>\n>> 5.3.  Problem:   Due to pace, Fee Event not forestalled.\n>>\n>> Even presuming SW is merged into Bitcoin Core tomorrow, this does not\n>> address the risk of a Fee Event and associated Economic Change in the\n>> coming months.\n>>\n>>\n>> 5.4.  Problem:   More complex economic policy, new game theory, new\n>> bidding structure risks.\n>>\n>> Splitting blocks into two pieces, each with separate and distinct\n>> behaviors and resource values, creates *two fee markets.*\n>>\n>> Having two pricing strata within each block has certainly feasible - that\n>> is the current mining policy of (1) fee/KB followed by (2) priority/age.\n>>\n>> Valuable or not - e.g. incentivizing older clients to upgrade - the fact\n>> remains that SW creates a more-complex bidding structure by creating a\n>> second economic resource.\n>>\n>> *This is clearly a change to a new economic policy* with standard risks\n>> associated with that.  Will that induce an Economic C hange Event (see def\n>> last email)?  *Unlikely*, due to slow rollout pace.\n>>\n>>\n>> 5.5.  Problem:  Current SW mining algorithm needs improvement\n>>\n>> Current SW block template maker does a reasonable job, but makes some\n>> naive assumptions about the fee market across an entire extended block.\n>> This is a mismatch with the economic reality (just described).\n>>\n>> 5.6.   Problem:  New, under-analyzed attack surfaces\n>>\n>> Less significant and fundamental but still worth noting.\n>>\n>> This is not a fundamental SW problem, but simply standard complexity risk\n>> factors:  splitting the signatures away from transactions, and creating a\n>> new apparently-unsigned version of the transaction opens t he possibility\n>> of some network attacks which cause some clients to degrade down from\n>> extended block to core block mode temporarily.\n>>\n>> There is a chance of a failure mode that fools older clients into\n>> thinking fraudulent data is valid (judgement: unlikely vis hashpower but\n>> not impossible)\n>>\n>> 6. Conclusions and recommendations\n>>\n>> It seems unlikely that SW provides scaling in the short term, and SW\n>> introduces new economics complexities.\n>>\n>> A \"short term bump\" hard fork block size increase addresses economic and\n>> ecosystem risks that SW does not.\n>>\n>> Bump + SW should proce ed in parallel, independent tracks, as orthogonal\n>> issues.\n>>\n>>\n>> 7. Appendix - Other SW comments\n>>\n>> Hard forks provide much stronger validation, and ensure the network\n>> operates at a fully trustless level.\n>>\n>> SW hard fork is preferred, versus soft fork.  Soft forking SW places a\n>> huge amount of trust on miners to validate transaction signatures, versus\n>> the rest of the network, as the network slowly upgrades to newer clients.\n>>\n>> An SW hard fork could also add several zero-filled placeholders in a\n>> merkle tree for future use.\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>> ------------------------------\n>>\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151217/689f5cee/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-12-16T20:59:41",
                "message_text_only": "On Wed, Dec 16, 2015 at 9:38 PM, Jeff Garzik via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> 4.3.  Observations on new block economic model\n>\n> SW complicates block economics by creating two separate, supply limited\n> resources.\n\nNot correct. I propose defining the virtual_block_size as base_size +\nwitness_size * 0.25, and limiting virtual_block_size to 1M. This\ncreates a single variable to optimize for. If accepted, miners are\nincentived to maximize fee per virtual_block_size instead of per size.\n\nWallet software can individually choose whether to upgrade or not.\nOnce they upgrade, they get to perform 1.75x as many transactions for\nthe same fee (assuming non-complex transactions), and this is\nindependent of whether anyone else upgrades.\n\n> 5.1.  Problem:  Pace of roll-out will be slow - Whole Ecosystem must be\n> considered.\n>\n> The current apparent proposal is to roll out Segregated Witness as a soft\n> fork, and keep block size at 1M.\n>\n> The roll-out pace cannot simply be judged by soft fork speed - which is\n> months at best.  Analysis must the layers above:  Updating bitcoin-core (JS)\n> and bitcoinj (Java), and then the timelines to roll out those updates to\n> apps, and then the timeline to update those apps to create extended\n> transactions.\n\nAgree, however everyone can upgrade whenever they want, and get the\nreduced fees as soon as they do. This is contrary to a hard fork,\nwhich forces every full node to upgrade at once (though indeed, light\nclients are not necessarily forced to upgrade).\n\n> 5.2.  Problem:   Hard fork to bigger block size Just Works(tm) with most\n> software, unlike SW.\n>\n> A simple hard fork such as BIP 102 is automatically compatible with the vast\n> range of today's ecosystem software.\n>\n> SW requires merchants to upgrade almost immediately, requires wallet and\n> other peripheral software upgrades to make use of.  Other updates are opt-in\n> and occur more slowly.  BIP 70 processors need some updates.\n>\n> The number of LOC that must change for BIP 102 is very small, and the\n> problem domain well known, versus SW.\n\nIt multiplies all current DoS vectors by a factor equal to the\ncapacity increase factor. SW increases capacity while leaving several\nworst-case effects constant.\n\n> 5.4.  Problem:   More complex economic policy, new game theory, new bidding\n> structure risks.\n>\n> Splitting blocks into two pieces, each with separate and distinct behaviors\n> and resource values, creates two fee markets.\n\nI believe you have misunderstood the proposal in that case.\n\n> 5.5.  Problem:  Current SW mining algorithm needs improvement\n>\n> Current SW block template maker does a reasonable job, but makes some naive\n> assumptions about the fee market across an entire extended block.  This is a\n> mismatch with the economic reality (just described).\n\nAgain, I think you misunderstood. The proposal includes a single new\nformula for block size, and optimizes for it. In case the proposal is\naccepted, the mining code is automatically as optimal as it was\nbefore.\n\n> 6. Conclusions and recommendations\n>\n> A \"short term bump\" hard fork block size increase addresses economic and\n> ecosystem risks that SW does not.\n>\n> Bump + SW should proceed in parallel, independent tracks, as orthogonal\n> issues.\n\nI agree here. SW is not a replacement for a scale increase. However, I\nthink it can be adopted much more easily, as it doesn't require the\nmassively pervasive consensus that a hardfork requires to perform\nsafely.\n\n> 7. Appendix - Other SW comments\n>\n> Hard forks provide much stronger validation, and ensure the network operates\n> at a fully trustless level.\n>\n> SW hard fork is preferred, versus soft fork.  Soft forking SW places a huge\n> amount of trust on miners to validate transaction signatures, versus the\n> rest of the network, as the network slowly upgrades to newer clients.\n\nBut old clients may not care about the new rules, and they still\nvalidate the old ones they chose to enforce.\n\nFurthermore, soft forks cannot be prevented: miners can always choose\nto enforce stronger rules than the network demands from them.\n\n-- \nPieter"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-12-16T21:27:15",
                "message_text_only": "On Wed, Dec 16, 2015 at 3:59 PM, Pieter Wuille <pieter.wuille at gmail.com>\nwrote:\n\n> On Wed, Dec 16, 2015 at 9:38 PM, Jeff Garzik via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > 4.3.  Observations on new block economic model\n> >\n> > SW complicates block economics by creating two separate, supply limited\n> > resources.\n>\n> Not correct. I propose defining the virtual_block_size as base_size +\n> witness_size * 0.25, and limiting virtual_block_size to 1M. This\n> creates a single variable to optimize for. If accepted, miners are\n> incentived to maximize fee per virtual_block_size instead of per size.\n>\n\nIt is correct.  There are two separate sets of economic actors and levels\nof contention for each set of space.\n\nThat is true regardless of the proposed miner selection algorithm.\n\n\n\n> > 5.4.  Problem:   More complex economic policy, new game theory, new\n> bidding\n> > structure risks.\n> >\n> > Splitting blocks into two pieces, each with separate and distinct\n> behaviors\n> > and resource values, creates two fee markets.\n>\n> I believe you have misunderstood the proposal in that case.\n>\n\nSee above.  There are two separate and distinct resource velocities and\ndemand levels in reality.  That creates two markets regardless of miner\nselection algorithm in the block maker.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/17766776/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-12-16T21:36:09",
                "message_text_only": "On Wed, Dec 16, 2015 at 10:27 PM, Jeff Garzik <jgarzik at gmail.com> wrote:\n>> Not correct. I propose defining the virtual_block_size as base_size +\n>> witness_size * 0.25, and limiting virtual_block_size to 1M. This\n>> creates a single variable to optimize for. If accepted, miners are\n>> incentived to maximize fee per virtual_block_size instead of per size.\n>\n>\n> It is correct.  There are two separate sets of economic actors and levels of\n> contention for each set of space.\n>\n> That is true regardless of the proposed miner selection algorithm.\n\nMaybe I haven't explained this properly, so consider this example:\n\nA miner receives sets of 200 byte transactions with all identical\nfees. Non-witness ones (whose virtual size is thus 200 bytes) and a\nwitness one (where 120 of the 200 bytes are witness data, so its\nvirtual size is 80 + 120*0.25 = 110 bytes).\n\nThe consensus rules would limit 1) the base size to 1000000 bytes 2)\nthe virtual size to 1000000 bytes. However, as the virtual size is\ndefined as the sum of the base size plus a non-negative number,\nsatisfying (2) always implies satisfying (1).\n\nThus, the miners' best strategy is to accept the witness transactions,\nas it allows 1000000/110=9090 transactions rather than\n1000000/200=5000.\n\nIn fact, the optimal fee maximizing strategy is always to maximize fee\nper virtual size.\n\n-- \nPieter"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-12-16T22:09:58",
                "message_text_only": "Maybe a new analogy helps.\n\nSW presents a blended price and blended basket of two goods.  You can\ninteract with the Service through the blended price, but that does not\nerase the fact that the basket contains two separate from similar resources.\n\nA different set of economic actors uses one resource, and/or both.  There\nare explicit incentives to shift actors from solely using one resource to\nusing both.\n\nThe fact that separate sets of economic actors and incentives exist is\nsufficient to prove it is indeed a basket of goods, not a single good.\n\n\nOn Wed, Dec 16, 2015 at 4:36 PM, Pieter Wuille <pieter.wuille at gmail.com>\nwrote:\n\n> Thus, the miners' best strategy is to accept the witness transactions,\n> as it allows 1000000/110=9090 transactions rather than\n> 1000000/200=5000.\n>\n\nUnder your blended algorithm, this seems reasonable as a first pass.\n\n\n\n> In fact, the optimal fee maximizing strategy is always to maximize fee\n> per virtual size.\n>\n\nThis is a microscopic, not macroscopic analysis.  Externalities and long\nterm incentives can severely perturb or invalidate that line of thinking.\n\nTypical counter-example:  Many miners are perfectly happy with very low\nfees to encourage long term growth of their bitcoin income through network\neffect growth -- rendering fee micro-optimizations largely in the realm of\nDoS prevention rather than miner incentive.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/0f083114/attachment.html>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-12-16T22:10:53",
                "message_text_only": "On Wed, Dec 16, 2015 at 5:09 PM, Jeff Garzik <jgarzik at gmail.com> wrote:\n\n> SW presents a blended price and blended basket of two goods.  You can\n> interact with the Service through the blended price, but that does not\n> erase the fact that the basket contains two separate from similar resources.\n>\n\nseparate-but-similar\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/30baa066/attachment.html>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-12-17T18:27:13",
                "message_text_only": "On Wed, Dec 16, 2015 at 5:09 PM, Jeff Garzik <jgarzik at gmail.com> wrote:\n\n> SW presents a blended price and blended basket of two goods.  You can\n> interact with the Service through the blended price, but that does not\n> erase the fact that the basket contains two separate from similar resources.\n>\n> A different set of economic actors uses one resource, and/or both.  There\n> are explicit incentives to shift actors from solely using one resource to\n> using both.\n>\n\nIllustration:  If SW is deployed via soft fork, the count of nodes that\nvalidate witness data is significantly lower than the count of nodes that\nvalidate non-witness data.  Soft forks are not trustless operation, they\ndepend on miner trust, slowly eroding the trustless validation of older\nnodes over time.\n\nHigher security in one data area versus another produces another economic\nvalue distinction between the two goods in the basket, and creates a \"pay\nmore for higher security in core block, pay less for lower security in\nwitness\" dynamic.\n\nThis economic distinction is not present if SW is deployed via hard fork.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151217/f0a3b264/attachment.html>"
            },
            {
                "author": "jl2012",
                "date": "2015-12-17T18:46:06",
                "message_text_only": "This is not correct.\n\nAs only about 1/3 of nodes support BIP65 now, would you consider CLTV tx \nare less secure than others? I don't think so. Since one invalid CLTV tx \nwill make the whole block invalid. Having more nodes to fully validate \nnon-CLTV txs won't make them any safer. The same logic also applies to \nSW softfork.\n\nYou may argue that a softfork would make the network as a whole less \nsecure, as old nodes have to trust new nodes. However, the security of \nall content in the same block must be the same, by definition.\n\nAnyway, I support SW softfork at the beginning, and eventually (~2 \nyears) moving to a hardfork with higher block size limit and better \ncommitment structure.\n\nJeff Garzik via bitcoin-dev \u65bc 2015-12-17 13:27 \u5beb\u5230:\n\n> \n> Illustration:  If SW is deployed via soft fork, the count of nodes\n> that validate witness data is significantly lower than the count of\n> nodes that validate non-witness data.  Soft forks are not trustless\n> operation, they depend on miner trust, slowly eroding the trustless\n> validation of older nodes over time.\n> \n> Higher security in one data area versus another produces another\n> economic value distinction between the two goods in the basket, and\n> creates a \"pay more for higher security in core block, pay less for\n> lower security in witness\" dynamic.\n> \n> This economic distinction is not present if SW is deployed via hard\n> fork.\n> \n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Anthony Towns",
                "date": "2015-12-17T03:52:22",
                "message_text_only": "On Wed, Dec 16, 2015 at 10:36:09PM +0100, Pieter Wuille via bitcoin-dev wrote:\n> On Wed, Dec 16, 2015 at 10:27 PM, Jeff Garzik <jgarzik at gmail.com> wrote:\n> >> Not correct. I propose defining the virtual_block_size as base_size +\n> >> witness_size * 0.25, and limiting virtual_block_size to 1M. This\n> >> creates a single variable to optimize for. If accepted, miners are\n> >> incentived to maximize fee per virtual_block_size instead of per size.\n> > It is correct.  There are two separate sets of economic actors and levels of\n> > contention for each set of space.\n> > That is true regardless of the proposed miner selection algorithm.\n\nYou're right that the miner selection algorithm doesn't force it to be\nthe way Pieter describe. But your claim is still incorrect. :)\n\n> Maybe I haven't explained this properly, so consider this example:\n\nAlternatively:\n\nWith Pieter's segwit proposal (as it stands), there are two\nconsensus-limited resources: number of signature ops in the base block\nmust be no more than 20k, and the virtual block size must be no more\nthan 1MB (where virtual block size = base block size plus a quarter of\nthe witness data size).\n\nNodes and miners have other constraints -- bandwidth, storage, CPU, etc,\nsuch that they might not want to max out these limits for whatever reason,\nbut those limits aren't enforced by consensus, so can be adjusted as\ntechnology improves just by individual miner policy.\n\n> In fact, the optimal fee maximizing strategy is always to maximize fee\n> per virtual size.\n\n(modulo sigop constraints, same as today for fee per base block size)\n\nThat's on the \"supply\" side (ie, miners are forced to be a single group\nof economic actors with alighned constraints due to consensus rules).\n\nOn the demand side, there might be people who are able to trade off\nwitness data for base data at different ratios. For most, it's just 1:1\nup to a limit as they move scriptsig to witness data, and obviously if\nyou have to trade 1B of base data for more than 4B of witness data it's\nuneconomic. But since the ratio is fixed, there's no bartering to be\ndone, it's just the same simple calculation for everyone -- does 1B of\nbase convert to <4B of witness? then do it; otherwise, don't. But once\nthey've selected a tradeoff, all they can do is choose an absolute fee\nvalue for their transaction, and then you're just back to having some\npeople who are willing to pay higher fees per virtual block size, and\nothers who are only willing to pay lower fees.\n\nCheers,\naj"
            },
            {
                "author": "Matt Corallo",
                "date": "2015-12-16T22:29:39",
                "message_text_only": "We should probably start by defining \"economically important\". To me, it's pretty clear that every, or at least around 99% of, \" economically important\" node have upgraded by the time the fork kicks in, with way more than sufficient time given to everyone to upgrade (minding that this is not an emergency situation and that people have lives and many Bitcoin services are hobby projects and upgrading isn't always as easy as just restarting your node). I'd define \"economically important\" as any node that is used for anything more than simply \"being a node\" (ie people who started a node to provide resources to the network, and only using their node for that). Note, of course, that we should avoid breaking all such \"non-economically important\" nodes, but breaking many of them is not a big deal. Note that my proposal includes nodes such as the one doing transaction selection for the relay network. Though it is not used for payments, if it is not upgraded, things will break.\n\nWith this definition in mind, I think a year is an aggressive timeline.\n\nOn December 16, 2015 1:51:47 PM PST, Jameson Lopp <jameson.lopp at gmail.com> wrote:\n>On Wed, Dec 16, 2015 at 12:50 PM, Matt Corallo via bitcoin-dev <\n>bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> A large part of your argument is that SW will take longer to deploy\n>than a\n>> hard fork, but I completely disagree. Though I do not agree with some\n>> people claiming we can deploy SW significantly faster than a hard\n>fork,\n>> once the code is ready (probably a six month affair) we can get it\n>deployed\n>> very quickly. It's true the ecosystem may take some time to upgrade,\n>but I\n>> see that as a feature, not a bug - we can build up some fee pressure\n>with\n>> an immediate release valve available for people to use if they want\n>to pay\n>> fewer fees.\n>>\n>> On the other hand, a hard fork, while simpler for the ecosystem to\n>upgrade\n>> to, is a 1-2 year affair (after the code is shipped, so at least\n>1.5-2.5\n>> from today if we all put off heads down and work). One thing that has\n>> concerned me greatly through this whole debate is how quickly people\n>seem\n>> to think we can roll out a hard fork. Go look at the distribution of\n>node\n>> versions on the network today and work backwards to get nearly every\n>node\n>> upgraded... Even with a year between fork-version-release and\n>> fork-activation, we'd still kill a bunch of nodes and instead of\n>reducing\n>> their security model, lead them to be outright robbed.\n>>\n>>\n>Over a year seems to be an extraordinarily long time frame is for\n>deploying\n>a hard fork. It looks like <https://bitnodes.21.co/dashboard/?days=365>\n>75%\n>of reachable nodes have upgraded in the past 6 months while as much as\n>25%\n>may not have been upgraded in over a year. However, viewing historical\n>stats of version upgrades doesn't seem to be an appropriate comparison\n>because node operators have never been faced with the same incentive to\n>upgrade. We can point to unintentional forks in the past that have been\n>resolved fairly quickly by reaching out to miners, but it's also a poor\n>comparison. Unfortunately, we have no way of knowing what percentage of\n>nodes are economically important - a great deal of them may be running\n>and\n>not even be used by the operators.\n>\n>Perhaps it would be better if we were to formalize the expectations for\n>full node operators, but it seems to me that node operators have a\n>responsibility to keep themselves informed and decide when it is\n>appropriate to update their software. I'm not so sure that it's the\n>rest of\n>the ecosystem's responsibility to wait around for laggards.\n>\n>- Jameson\n>\n>On December 16, 2015 12:38:30 PM PST, Jeff Garzik via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>>\n>>> 1. Summary\n>>>\n>>> Segregated Witness (SegWitness, SW) is being presented in the\n>context of\n>>> Scaling Bitcoin.  It has useful attributes, notably addressing a\n>major\n>>> malleability vector, but is not a short term scaling solution.\n>>>\n>>>\n>>> 2. Definitions\n>>>\n>>> Import Fee Event, ECE, TFM, FFM from previous email.\n>>>\n>>> Older clients - Any software not upgraded to SW\n>>>\n>>> Newer clients - Upgraded, SW aware software\n>>>\n>>>\n>>> Block size - refers to the core block economic resource limit ed by\n>>> MAX_BLOCK_SIZE.  Witness data (or extension block data) is excluded.\n>>> Requires a hard fork to change.\n>>>\n>>> Core block - Current bitcoin block, with upper bound MAX_BLOCK_SIZE.\n> Not\n>>> changed by SW.\n>>>\n>>>\n>>> Extended transaction - Newer, upgraded version of transaction data\n>format.\n>>>\n>>> Extended block - Newer, upgraded version of block data format.\n>>>\n>>>\n>>> EBS - Extended block size.  Block size seen by newer clients.\n>>>\n>>>\n>>> 3. Context of analysis\n>>>\n>>> One proposal presents SW *in lieu of* a hard fork block size\n>increase.\n>>> This email focuses directly on that.\n>>>\n>>> Useful features outside block size context, such as\n>anti-malleability or\n>>> fraud proof features, are not covered in depth.\n>>>\n>>>\n>>> 4.1.  Observations on data structure formats and views\n>>>\n>>> SW creates two *views* of each transaction and block.  SW has blocks\n>and\n>>> extended blocks.  Similarly, there exists transactions and extended\n>>> transactions.\n>>>\n>>> This view is rendered to clients depending on compatibility level. \n>Newer\n>>> clients see extended blocks and extended transactions.  Older\n>clients see\n>>> blocks (limit 1M), and do not see extended blocks.  Older clients\n>see\n>>> upgraded transactions as unsigned, anyone-can-pay transactions.\n>>>\n>>> Each extended transaction exists in two states, one unsigned and one\n>>> signed, each of which passes validation as a valid bitcoin\n>transaction.\n>>>\n>>>\n>>> 4.2.  Observations on behavior of older transaction creation\n>>>\n>>> Transactions created by older clients will not use the extended\n>>> transaction format.  All data is stored the standard 1M block as\n>today.\n>>>\n>>>\n>>> 4.3.  Observations on new block economic model\n>>>\n>>> SW complicates block economics by creating two separate, supply\n>limited\n>>> resources.\n>>>\n>>> The core block economic resource is heavily contended.  Older\n>clients use\n>>> core blocks exclusively.  Newer clients use core block s more\n>>> conservatively, storing as much data as possible in extended blocks.\n>>>\n>>> The extended block economic resource is less heavily contended,\n>though\n>>> that of course grows over time as clients upgrade.\n>>>\n>>> Because core blocks are more heavily contended, it is presumed that\n>older\n>>> clients will pay a higher fee than newer clients (subject to\n>elasticity\n>>> etc.).\n>>>\n>>>\n>>> 5.1.  Problem:  Pace of roll-out will be slow - Whole Ecosystem must\n>be\n>>> considered.\n>>>\n>>> The current apparent proposal is to roll out Segregated Witness as a\n>soft\n>>> fork, and keep block size at 1M.\n>>>\n>>> The roll-out pace cannot simply be judged by soft fork speed - which\n>is\n>>> months at best.  Analysis must the layers above:  Updating\n>bitcoin-core\n>>> (JS) and bitcoinj (Java), and then the timelines to roll out those\n>updates\n>>> to apps, and then the timeline to update those apps to create\n>extended\n>>> transactions.\n>>>\n>>> Overall, wallet software and programmer libraries must be upgraded\n>to\n>>> make use of this new format, adding many more months (12+ in some\n>stacks)\n>>> to the roll out timeline.  In the meantime, clients continue to\n>contend\n>>> entirely for core block space.\n>>>\n>>>\n>>> 5.2.  Problem:   Hard fork to bigger block size Just Works(tm) with\n>most\n>>> software, unlike SW.\n>>>\n>>> A simple hard fork such as BIP 102 is automatically compatible with\n>the\n>>> vast range of today's ecosystem software.\n>>>\n>>> SW requires merchants to upgrade almost immediately, requires wallet\n>and\n>>> other peripheral software upgrades to make use of.  Other updates\n>are\n>>> opt-in and occur more slowly.  BIP 70 processors need some updates.\n>>>\n>>> The number of LOC that must change for BIP 102 is very small, and\n>the\n>>> problem domain well known, versus SW.\n>>>\n>>>\n>>> 5.3.  Problem:   Due to pace, Fee Event not forestalled.\n>>>\n>>> Even presuming SW is merged into Bitcoin Core tomorrow, this does\n>not\n>>> address the risk of a Fee Event and associated Economic Change in\n>the\n>>> coming months.\n>>>\n>>>\n>>> 5.4.  Problem:   More complex economic policy, new game theory, new\n>>> bidding structure risks.\n>>>\n>>> Splitting blocks into two pieces, each with separate and distinct\n>>> behaviors and resource values, creates *two fee markets.*\n>>>\n>>> Having two pricing strata within each block has certainly feasible -\n>that\n>>> is the current mining policy of (1) fee/KB followed by (2)\n>priority/age.\n>>>\n>>> Valuable or not - e.g. incentivizing older clients to upgrade - the\n>fact\n>>> remains that SW creates a more-complex bidding structure by creating\n>a\n>>> second economic resource.\n>>>\n>>> *This is clearly a change to a new economic policy* with standard\n>risks\n>>> associated with that.  Will that induce an Economic C hange Event\n>(see def\n>>> last email)?  *Unlikely*, due to slow rollout pace.\n>>>\n>>>\n>>> 5.5.  Problem:  Current SW mining algorithm needs improvement\n>>>\n>>> Current SW block template maker does a reasonable job, but makes\n>some\n>>> naive assumptions about the fee market across an entire extended\n>block.\n>>> This is a mismatch with the economic reality (just described).\n>>>\n>>> 5.6.   Problem:  New, under-analyzed attack surfaces\n>>>\n>>> Less significant and fundamental but still worth noting.\n>>>\n>>> This is not a fundamental SW problem, but simply standard complexity\n>risk\n>>> factors:  splitting the signatures away from transactions, and\n>creating a\n>>> new apparently-unsigned version of the transaction opens t he\n>possibility\n>>> of some network attacks which cause some clients to degrade down\n>from\n>>> extended block to core block mode temporarily.\n>>>\n>>> There is a chance of a failure mode that fools older clients into\n>>> thinking fraudulent data is valid (judgement: unlikely vis hashpower\n>but\n>>> not impossible)\n>>>\n>>> 6. Conclusions and recommendations\n>>>\n>>> It seems unlikely that SW provides scaling in the short term, and SW\n>>> introduces new economics complexities.\n>>>\n>>> A \"short term bump\" hard fork block size increase addresses economic\n>and\n>>> ecosystem risks that SW does not.\n>>>\n>>> Bump + SW should proce ed in parallel, independent tracks, as\n>orthogonal\n>>> issues.\n>>>\n>>>\n>>> 7. Appendix - Other SW comments\n>>>\n>>> Hard forks provide much stronger validation, and ensure the network\n>>> operates at a fully trustless level.\n>>>\n>>> SW hard fork is preferred, versus soft fork.  Soft forking SW places\n>a\n>>> huge amount of trust on miners to validate transaction signatures,\n>versus\n>>> the rest of the network, as the network slowly upgrades to newer\n>clients.\n>>>\n>>> An SW hard fork could also add several zero-filled placeholders in a\n>>> merkle tree for future use.\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>> ------------------------------\n>>>\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/d0d4287d/attachment-0001.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2015-12-16T22:32:57",
                "message_text_only": "As for \"the ecosystem waiting around for laggards\", yes, it is absolutely the ecosystems y responsibility to not take actions that will result in people losing money without providing them far more than enough opportunity to fix it. One of the absolute most important features of Bitcoin is that, if you're running a full node, you are provided reasonable security against accepting invalid transactions.\n\nOn December 16, 2015 1:51:47 PM PST, Jameson Lopp <jameson.lopp at gmail.com> wrote:\n>On Wed, Dec 16, 2015 at 12:50 PM, Matt Corallo via bitcoin-dev <\n>bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> A large part of your argument is that SW will take longer to deploy\n>than a\n>> hard fork, but I completely disagree. Though I do not agree with some\n>> people claiming we can deploy SW significantly faster than a hard\n>fork,\n>> once the code is ready (probably a six month affair) we can get it\n>deployed\n>> very quickly. It's true the ecosystem may take some time to upgrade,\n>but I\n>> see that as a feature, not a bug - we can build up some fee pressure\n>with\n>> an immediate release valve available for people to use if they want\n>to pay\n>> fewer fees.\n>>\n>> On the other hand, a hard fork, while simpler for the ecosystem to\n>upgrade\n>> to, is a 1-2 year affair (after the code is shipped, so at least\n>1.5-2.5\n>> from today if we all put off heads down and work). One thing that has\n>> concerned me greatly through this whole debate is how quickly people\n>seem\n>> to think we can roll out a hard fork. Go look at the distribution of\n>node\n>> versions on the network today and work backwards to get nearly every\n>node\n>> upgraded... Even with a year between fork-version-release and\n>> fork-activation, we'd still kill a bunch of nodes and instead of\n>reducing\n>> their security model, lead them to be outright robbed.\n>>\n>>\n>Over a year seems to be an extraordinarily long time frame is for\n>deploying\n>a hard fork. It looks like <https://bitnodes.21.co/dashboard/?days=365>\n>75%\n>of reachable nodes have upgraded in the past 6 months while as much as\n>25%\n>may not have been upgraded in over a year. However, viewing historical\n>stats of version upgrades doesn't seem to be an appropriate comparison\n>because node operators have never been faced with the same incentive to\n>upgrade. We can point to unintentional forks in the past that have been\n>resolved fairly quickly by reaching out to miners, but it's also a poor\n>comparison. Unfortunately, we have no way of knowing what percentage of\n>nodes are economically important - a great deal of them may be running\n>and\n>not even be used by the operators.\n>\n>Perhaps it would be better if we were to formalize the expectations for\n>full node operators, but it seems to me that node operators have a\n>responsibility to keep themselves informed and decide when it is\n>appropriate to update their software. I'm not so sure that it's the\n>rest of\n>the ecosystem's responsibility to wait around for laggards.\n>\n>- Jameson\n>\n>On December 16, 2015 12:38:30 PM PST, Jeff Garzik via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>>\n>>> 1. Summary\n>>>\n>>> Segregated Witness (SegWitness, SW) is being presented in the\n>context of\n>>> Scaling Bitcoin.  It has useful attributes, notably addressing a\n>major\n>>> malleability vector, but is not a short term scaling solution.\n>>>\n>>>\n>>> 2. Definitions\n>>>\n>>> Import Fee Event, ECE, TFM, FFM from previous email.\n>>>\n>>> Older clients - Any software not upgraded to SW\n>>>\n>>> Newer clients - Upgraded, SW aware software\n>>>\n>>>\n>>> Block size - refers to the core block economic resource limit ed by\n>>> MAX_BLOCK_SIZE.  Witness data (or extension block data) is excluded.\n>>> Requires a hard fork to change.\n>>>\n>>> Core block - Current bitcoin block, with upper bound MAX_BLOCK_SIZE.\n> Not\n>>> changed by SW.\n>>>\n>>>\n>>> Extended transaction - Newer, upgraded version of transaction data\n>format.\n>>>\n>>> Extended block - Newer, upgraded version of block data format.\n>>>\n>>>\n>>> EBS - Extended block size.  Block size seen by newer clients.\n>>>\n>>>\n>>> 3. Context of analysis\n>>>\n>>> One proposal presents SW *in lieu of* a hard fork block size\n>increase.\n>>> This email focuses directly on that.\n>>>\n>>> Useful features outside block size context, such as\n>anti-malleability or\n>>> fraud proof features, are not covered in depth.\n>>>\n>>>\n>>> 4.1.  Observations on data structure formats and views\n>>>\n>>> SW creates two *views* of each transaction and block.  SW has blocks\n>and\n>>> extended blocks.  Similarly, there exists transactions and extended\n>>> transactions.\n>>>\n>>> This view is rendered to clients depending on compatibility level. \n>Newer\n>>> clients see extended blocks and extended transactions.  Older\n>clients see\n>>> blocks (limit 1M), and do not see extended blocks.  Older clients\n>see\n>>> upgraded transactions as unsigned, anyone-can-pay transactions.\n>>>\n>>> Each extended transaction exists in two states, one unsigned and one\n>>> signed, each of which passes validation as a valid bitcoin\n>transaction.\n>>>\n>>>\n>>> 4.2.  Observations on behavior of older transaction creation\n>>>\n>>> Transactions created by older clients will not use the extended\n>>> transaction format.  All data is stored the standard 1M block as\n>today.\n>>>\n>>>\n>>> 4.3.  Observations on new block economic model\n>>>\n>>> SW complicates block economics by creating two separate, supply\n>limited\n>>> resources.\n>>>\n>>> The core block economic resource is heavily contended.  Older\n>clients use\n>>> core blocks exclusively.  Newer clients use core block s more\n>>> conservatively, storing as much data as possible in extended blocks.\n>>>\n>>> The extended block economic resource is less heavily contended,\n>though\n>>> that of course grows over time as clients upgrade.\n>>>\n>>> Because core blocks are more heavily contended, it is presumed that\n>older\n>>> clients will pay a higher fee than newer clients (subject to\n>elasticity\n>>> etc.).\n>>>\n>>>\n>>> 5.1.  Problem:  Pace of roll-out will be slow - Whole Ecosystem must\n>be\n>>> considered.\n>>>\n>>> The current apparent proposal is to roll out Segregated Witness as a\n>soft\n>>> fork, and keep block size at 1M.\n>>>\n>>> The roll-out pace cannot simply be judged by soft fork speed - which\n>is\n>>> months at best.  Analysis must the layers above:  Updating\n>bitcoin-core\n>>> (JS) and bitcoinj (Java), and then the timelines to roll out those\n>updates\n>>> to apps, and then the timeline to update those apps to create\n>extended\n>>> transactions.\n>>>\n>>> Overall, wallet software and programmer libraries must be upgraded\n>to\n>>> make use of this new format, adding many more months (12+ in some\n>stacks)\n>>> to the roll out timeline.  In the meantime, clients continue to\n>contend\n>>> entirely for core block space.\n>>>\n>>>\n>>> 5.2.  Problem:   Hard fork to bigger block size Just Works(tm) with\n>most\n>>> software, unlike SW.\n>>>\n>>> A simple hard fork such as BIP 102 is automatically compatible with\n>the\n>>> vast range of today's ecosystem software.\n>>>\n>>> SW requires merchants to upgrade almost immediately, requires wallet\n>and\n>>> other peripheral software upgrades to make use of.  Other updates\n>are\n>>> opt-in and occur more slowly.  BIP 70 processors need some updates.\n>>>\n>>> The number of LOC that must change for BIP 102 is very small, and\n>the\n>>> problem domain well known, versus SW.\n>>>\n>>>\n>>> 5.3.  Problem:   Due to pace, Fee Event not forestalled.\n>>>\n>>> Even presuming SW is merged into Bitcoin Core tomorrow, this does\n>not\n>>> address the risk of a Fee Event and associated Economic Change in\n>the\n>>> coming months.\n>>>\n>>>\n>>> 5.4.  Problem:   More complex economic policy, new game theory, new\n>>> bidding structure risks.\n>>>\n>>> Splitting blocks into two pieces, each with separate and distinct\n>>> behaviors and resource values, creates *two fee markets.*\n>>>\n>>> Having two pricing strata within each block has certainly feasible -\n>that\n>>> is the current mining policy of (1) fee/KB followed by (2)\n>priority/age.\n>>>\n>>> Valuable or not - e.g. incentivizing older clients to upgrade - the\n>fact\n>>> remains that SW creates a more-complex bidding structure by creating\n>a\n>>> second economic resource.\n>>>\n>>> *This is clearly a change to a new economic policy* with standard\n>risks\n>>> associated with that.  Will that induce an Economic C hange Event\n>(see def\n>>> last email)?  *Unlikely*, due to slow rollout pace.\n>>>\n>>>\n>>> 5.5.  Problem:  Current SW mining algorithm needs improvement\n>>>\n>>> Current SW block template maker does a reasonable job, but makes\n>some\n>>> naive assumptions about the fee market across an entire extended\n>block.\n>>> This is a mismatch with the economic reality (just described).\n>>>\n>>> 5.6.   Problem:  New, under-analyzed attack surfaces\n>>>\n>>> Less significant and fundamental but still worth noting.\n>>>\n>>> This is not a fundamental SW problem, but simply standard complexity\n>risk\n>>> factors:  splitting the signatures away from transactions, and\n>creating a\n>>> new apparently-unsigned version of the transaction opens t he\n>possibility\n>>> of some network attacks which cause some clients to degrade down\n>from\n>>> extended block to core block mode temporarily.\n>>>\n>>> There is a chance of a failure mode that fools older clients into\n>>> thinking fraudulent data is valid (judgement: unlikely vis hashpower\n>but\n>>> not impossible)\n>>>\n>>> 6. Conclusions and recommendations\n>>>\n>>> It seems unlikely that SW provides scaling in the short term, and SW\n>>> introduces new economics complexities.\n>>>\n>>> A \"short term bump\" hard fork block size increase addresses economic\n>and\n>>> ecosystem risks that SW does not.\n>>>\n>>> Bump + SW should proce ed in parallel, independent tracks, as\n>orthogonal\n>>> issues.\n>>>\n>>>\n>>> 7. Appendix - Other SW comments\n>>>\n>>> Hard forks provide much stronger validation, and ensure the network\n>>> operates at a fully trustless level.\n>>>\n>>> SW hard fork is preferred, versus soft fork.  Soft forking SW places\n>a\n>>> huge amount of trust on miners to validate transaction signatures,\n>versus\n>>> the rest of the network, as the network slowly upgrades to newer\n>clients.\n>>>\n>>> An SW hard fork could also add several zero-filled placeholders in a\n>>> merkle tree for future use.\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>> ------------------------------\n>>>\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/f5ad5c11/attachment.html>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-12-17T18:52:39",
                "message_text_only": "On Thu, Dec 17, 2015 at 1:46 PM, jl2012 <jl2012 at xbt.hk> wrote:\n\n> This is not correct.\n>\n> As only about 1/3 of nodes support BIP65 now, would you consider CLTV tx\n> are less secure than others? I don't think so. Since one invalid CLTV tx\n> will make the whole block invalid. Having more nodes to fully validate\n> non-CLTV txs won't make them any safer. The same logic also applies to SW\n> softfork.\n>\n\n\nYes - the logic applies to all soft forks.  Each soft fork degrades the\nsecurity of non-upgraded nodes.\n\nThe core design of bitcoin is that trustless nodes validate the work of\nminers, not trust them.\n\nSoft forks move in the opposite direction.  Each new soft-forked feature\nleans very heavily on miner trust rather than P2P network validation.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151217/b7b9e80f/attachment.html>"
            },
            {
                "author": "Eric Lombrozo",
                "date": "2015-12-17T21:18:57",
                "message_text_only": "Doesn't a good soft fork signaling mechanism along with an activation warning system for non-upgraded nodes (i.e. BIP9, or even block version ISM for that matter) essentially fix this? I don't quite get why this should be an issue.\n\nOn December 17, 2015 10:52:39 AM PST, Jeff Garzik via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>On Thu, Dec 17, 2015 at 1:46 PM, jl2012 <jl2012 at xbt.hk> wrote:\n>\n>> This is not correct.\n>>\n>> As only about 1/3 of nodes support BIP65 now, would you consider CLTV\n>tx\n>> are less secure than others? I don't think so. Since one invalid CLTV\n>tx\n>> will make the whole block invalid. Having more nodes to fully\n>validate\n>> non-CLTV txs won't make them any safer. The same logic also applies\n>to SW\n>> softfork.\n>>\n>\n>\n>Yes - the logic applies to all soft forks.  Each soft fork degrades the\n>security of non-upgraded nodes.\n>\n>The core design of bitcoin is that trustless nodes validate the work of\n>miners, not trust them.\n>\n>Soft forks move in the opposite direction.  Each new soft-forked\n>feature\n>leans very heavily on miner trust rather than P2P network validation.\n>\n>\n>------------------------------------------------------------------------\n>\n>_______________________________________________\n>bitcoin-dev mailing list\n>bitcoin-dev at lists.linuxfoundation.org\n>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151217/1cefd38f/attachment.html>"
            },
            {
                "author": "Adam Back",
                "date": "2015-12-17T21:31:07",
                "message_text_only": "While it is interesting to contemplate moving to a world with\nhard-fork only upgrades (deprecate soft-forks), now is possibly not\nthe time to consider that.  Someone can take that topic and make a\nwhat-if sketch for how it could work and put it on the wishlist wiki\nif its not already there.\n\nWe want to be pragmatic and constructive to reach consensus and that\ntakes not mixing in what-ifs or orthogonal long standing problems into\nthe mix, as needing to be fixed now.\n\nAdam\n\n\nOn 17 December 2015 at 19:52, Jeff Garzik via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>\n> On Thu, Dec 17, 2015 at 1:46 PM, jl2012 <jl2012 at xbt.hk> wrote:\n>>\n>> This is not correct.\n>>\n>> As only about 1/3 of nodes support BIP65 now, would you consider CLTV tx\n>> are less secure than others? I don't think so. Since one invalid CLTV tx\n>> will make the whole block invalid. Having more nodes to fully validate\n>> non-CLTV txs won't make them any safer. The same logic also applies to SW\n>> softfork.\n>\n>\n>\n> Yes - the logic applies to all soft forks.  Each soft fork degrades the\n> security of non-upgraded nodes.\n>\n> The core design of bitcoin is that trustless nodes validate the work of\n> miners, not trust them.\n>\n> Soft forks move in the opposite direction.  Each new soft-forked feature\n> leans very heavily on miner trust rather than P2P network validation.\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            }
        ],
        "thread_summary": {
            "title": "Segregated Witness in the context of Scaling Bitcoin",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeff Garzik",
                "Eric Lombrozo",
                "Corey Haddad",
                "Adam Back",
                "Anthony Towns",
                "sickpig at gmail.com",
                "Dave Scotese",
                "Marcel Jamin",
                "Jorge Tim\u00f3n",
                "Matt Corallo",
                "Mark Friedenbach",
                "Jameson Lopp",
                "Pieter Wuille",
                "jl2012"
            ],
            "messages_count": 32,
            "total_messages_chars_count": 128049
        }
    },
    {
        "title": "[bitcoin-dev] Fwd: Block size: It's economics & user preparation & moral hazard",
        "thread_messages": [
            {
                "author": "Martijn Meijering",
                "date": "2015-12-17T17:01:30",
                "message_text_only": "Appealing moderator's decision. If my post was off-topic, then so is the\nwhole thread. As for content-heavy, I made a very specific compromise\nproposal that I'd like to bring to the developers attention. If this isn't\nthe place to do that, then I don't know what is, but I'd be happy to repost\nto a different forum if you can suggest one that is more appropriate.\n\n===\n\n\nIt looks as if there has been movement on both sides of this debate and the\noutlines of a potential medium term truce are starting to appear. Given the\nenormous amount of unrest this whole debate has caused I think it would be\nhighly desirable if both sides could reach an honourable compromise that's\nready to be deployed next year.\n\nI'd like to make a compromise proposal, made up of of uncontroversial\nelements of other proposals.\n\nIt is intended to achieve the following goals:\n\n- Discourage a potentially disastrous contentious hard fork and\nconsequently only activate with overwhelming community support.\n- Provide immediate relief on both fees and growth potential once activated.\n- Provide immediate reassurance that there won't be radical growth for a\nnumber of years without another hard fork.\n- Lock in a temporary modest growth path that goes meaningfully beyond BIP\n102 so we have at least a few years worth of certainty for those who want\ngrowth.\n- Be no more than a kick-the-can-down-the-road solution and do not rule\nanything in or out after an interim period of a number of years.\n- One-off activation vote to avoid uncertainty hanging over the market\nindefinitely.\n- Be as simple as possible to allow for the earliest possible deployment.\n- Be as uncontroversial as possible to allow for the earliest possible\ndeployment.\n- Do not grow much beyond 2M for at least a year because of concerns\nexpressed by miners at Scaling Bitcoin.\n- Involve both a hard fork and a soft fork.\n- Have continuous, gradual growth instead of big step functions.\n\nIt's essentially a combination of a variant of 2-4-8 (necessarily a hard\nfork) and a soft fork version of Segregated Witness. The advantage of the\n2-4-8 hard fork is that it is very simple and can be coded and merged very\nquickly, probably in January. The SW soft fork on the other hand can\nprobably be activated sooner. Iherefore I'd like to see the hard fork\ncoded, merged and deployed first, then the soft fork merged and deployed\nand then the hard fork activated provided it passes its vote. In this way\nSW would be sandwiched between the deployment of an as yet inactive 2-4-8\nand its activation.\n\nIt does not preclude further hard forks at any time, either before or after\nthe proposed compromise hard fork, although it is specifically intended to\ndiscourage *contentious* hard forks. Non-contentious hard forks that become\npossible in the light of experience gained are only to be welcomed of\ncourse.\n\nThe details are as follows:\n\nHard fork with gradual growth:\n- +20kb each difficulty adjustment up to 8M.\n- Possibly a one-off jump to 2MB, but probably not given that SW will\nlikely be activated first.\n- 95% activation threshold hard-coded to a period of 1000 blocks before a\none-off, fixed block (\"election day\").\n- One month grace period, with a flag date at a fixed block height\n(\"inauguration day\") that will enable the growth mechanism supposing the\nthreshold was met by election day.\n- Pull request ready somewhere in January.\n- Merged in Q1.\n- Release around May.\n- Election day January 1st 2017.\n- Inauguration day February 1st 2017.\n\nSoft fork Segregated Witness:\n- Deployed as already suggested by the developers, but modified to be aware\nof the upcoming hard fork.\n- Limited to 1MB for the witness section for the first two years after\ndeployment to take miner concerns into account and allow time for research\ninto give weak blocks and IBLT research\n- Witness section size set to 1x or 2x the size of the main section of the\nblock after two years.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151217/633b9c14/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Fwd: Block size: It's economics & user preparation & moral hazard",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Martijn Meijering"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 4086
        }
    },
    {
        "title": "[bitcoin-dev] On the security of softforks",
        "thread_messages": [
            {
                "author": "Pieter Wuille",
                "date": "2015-12-18T02:30:38",
                "message_text_only": "Hello all,\n\nFor a long time, I was personally of the opinion that soft forks\nconstituted a mild security reduction for old full nodes, albeit one\nthat was preferable to hard forks due to being far less risky, easier,\nand less forceful to deploy.\n\nAfter thinking more about this, I'm not convinced that it is even that anymore.\n\nLet's analyze all failure modes (and feel free to let me know whether\nI've missed any specific ones):\n\n1) The risk of an old full node wallet accepting a transaction that is\ninvalid to the new rules.\n\nThe receiver wallet chooses what address/script to accept coins on.\nThey'll upgrade to the new softfork rules before creating an address\nthat depends on the softfork's features.\n\nSo, not a problem.\n\n2) The risk of an old full node wallet accepting a transaction whose\ncoins passed through a script that depends on the softforked rules.\n\nIt is reasonable that the receiver of a transaction places some trust\nin the sender, and on the basis of that, decides to reduce the number\nof confirmations before acceptance. In case the transaction indirectly\ndepends on a low-confirmation transaction using softforked rules, it\nmay be treated as an anyone-can-spend transaction. Obviously, no trust\ncan be placed in such a transactions not being reorged out and\nreplaced with an incompatible one.\n\nHowever, this problem is common for all anyonecanspend transactions,\nwhich are perfectly legal today in the blockchain. So, if this is a\nworry, we can solve it by marking incoming transactions as \"uncertain\nhistory\" in the wallet if they have an anyonecanspend transaction with\nless than 6 confirmations in its history. In fact, the same problem to\na lesser extent exists if coins pass through a 1-of-N multisig or so,\nbecause you're not only trusting the (indirect) senders, but also\ntheir potential cosigners.\n\n3) The risk of an SPV node wallet accepting an unconfirmed transaction\nwhich is invalid to new nodes.\n\nDefrauding an SPV wallet with an invalid unconfirmed transaction\ndoesn't change with the introduction of new consensus rules, as they\ndon't validate them anyway.\n\nIn the case the client trusts the full node peer(s) it is connected to\nto do validation before relay, nodes can either indicate (service bit\nor new p2p message) which softforks are accepted (as it only matters\nto SPV wallets that wish to accept transactions using new style script\nanyway), or wallets can rely on the new rules being non-standard even\nto old full nodes (which is typically aimed for in softforks).\n\n4) The risk of an SPV node wallet accepting a confirmed transaction\nwhich is invalid to new nodes\n\nMiners can of course construct an invalid block purely for defrauding\nSPV nodes, without intending to get that block accepted by full nodes.\nThat is expensive (no subsidy/fee income for those blocks) and more\nimportantly it isn't in any way affected by softforks.\n\nSo the only place where this matters is where miners create a block\nchain that violates the new rules, and still get it accepted. This\nrequires a hash rate majority, and sufficiently few economically\nimportant full nodes that forking them off is a viable approach.\n\nIt's interesting that even though it requires forking off full nodes\n(who will notice, there will be an invalid majority hash rate chain to\nthem), the attack only allows defrauding SPV nodes. It can't be used\nto bypass any of the economic properties of the system (as subsidy and\nother resource limits are still enforced by old nodes, and invalid\nscripts will either not be accepted by old full nodes wallets, or are\nas vulnerable as unrelated anyonecanspends).\n\nFurthermore, it's easily preventable by not using the feature in SPV\nwallets until a sufficient amount of economically relevant full nodes\nare known to have upgraded, or by just waiting for enough\nconfirmations.\n\n\n\nSo, we'd of course prefer to have all full nodes enforce all rules,\nbut the security reduction is not large. On the other hand, there are\nalso security advantages that softforks offer:\n\nA) Softforks do not require the pervasive consensus that hardforks\nneed. Soft forks can be deployed without knowing when all full nodes\nwill adopt the rule, or even whether they will ever adopt it at all.\n\nB) Keeping up with hard forking changes puts load on full node\noperators, who may choose to instead switch to delegating full\nvalidation to third parties, which is worse than just validating the\nold rules.\n\nC) Hardfork coordination has a centralizing effect on development. As\nhardforks can only be deployed with sufficient node deployment, they\ncan't just be triggered by miner votes. This requires central\ncoordination to determine flag times, which is incompatible with\nhaving multiple independent consensus changes being proposed. For\nsoftforks, something like BIP9 supports having multiple independent\nsoftforks in flight, that nodes can individually chose to accept or\nnot, only requiring coordination to not choose clashing bit numbers.\nFor hardforks, there is effectively no choice but having every\ncodebase deployed at a particular point in time to support every\npossible hard forks (there can still be an additional hashpower based\ntrigger conditions for hardforks, but all nodes need to support the\nfork at the earliest time it can happen, or risk being forked off).\n\nD) If you are concerned about the security degradation a soft fork\nmight bring, you can always configure your node to treat a (signalled)\nsoftfork as a hardfork, and stop processing blocks if a sortfork\ncondition is detected. The other direction is not possible.\n\n-- \nPieter"
            },
            {
                "author": "Jonathan Toomim",
                "date": "2015-12-18T02:47:14",
                "message_text_only": "On Dec 18, 2015, at 10:30 AM, Pieter Wuille via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> 1) The risk of an old full node wallet accepting a transaction that is\n> invalid to the new rules.\n> \n> The receiver wallet chooses what address/script to accept coins on.\n> They'll upgrade to the new softfork rules before creating an address\n> that depends on the softfork's features.\n> \n> So, not a problem.\n\n\nMallory wants to defraud Bob with a 1 BTC payment for some beer. Bob runs the old rules. Bob creates a p2pkh address for Mallory to use. Mallory takes 1 BTC, and creates an invalid SegWit transaction that Bob cannot properly validate and that pays into one of Mallory's wallets. Mallory then immediately spends the unconfirmed transaction into Bob's address. Bob sees what appears to be a valid transaction chain which is not actually valid.\n\nClueless Carol is one of the 4.9% of miners who forgot to upgrade her mining node. Carol sees that Mallory included an enormous fee in his transactions, so Carol makes sure to include both transactions in her block.\n\nMallory gets free beer.\n\nAnything I'm missing?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/4389e72a/attachment-0001.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/4389e72a/attachment-0001.sig>"
            },
            {
                "author": "Eric Lombrozo",
                "date": "2015-12-18T03:02:36",
                "message_text_only": "First of all, that's an expensive beer!\n\nSecond of all, any consensus rule change risks non-full-validating or \nnon-upgraded nodes seeing invalid confirmations...but assuming a large \nsupermajority (i.e. > 95%) of hashing power is behind the new rule, it \nis extremely unlikely that very many invalid confirmations will ever be \nseen by anyone. The number of confirmations you require depends on your \nuse case security requirements...and especially during a new rule \nactivation, it is probably not a good idea for non-validating nodes or \nnon-upgraded nodes to accept coins with low confirmation counts unless \nthe risk is accounted for in the use case (i.e. a web hosting provider \nthat can shut the user out if fraud is later detected).\n\nThird of all, as long as the rule change activation is signaled in \nblocks, even old nodes will be able to detect that something is fishy \nand warn users to be more cautious (i.e. wait more confirmations or \nimmediately upgrade or connect to a different node that has upgraded, \netc...)\n\nI honestly don't see an issue here - unless you're already violating \nfundamental security assumptions that would make you vulnerable to \nexploitation even without rule changes.\n\n- Eric\n\n------ Original Message ------\nFrom: \"Jonathan Toomim via bitcoin-dev\" \n<bitcoin-dev at lists.linuxfoundation.org>\nTo: \"Pieter Wuille\" <pieter.wuille at gmail.com>\nCc: \"Bitcoin Dev\" <bitcoin-dev at lists.linuxfoundation.org>\nSent: 12/17/2015 6:47:14 PM\nSubject: Re: [bitcoin-dev] On the security of softforks\n\n>\n>On Dec 18, 2015, at 10:30 AM, Pieter Wuille via bitcoin-dev \n><bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>>1) The risk of an old full node wallet accepting a transaction that is\n>>invalid to the new rules.\n>>\n>>The receiver wallet chooses what address/script to accept coins on.\n>>They'll upgrade to the new softfork rules before creating an address\n>>that depends on the softfork's features.\n>>\n>>So, not a problem.\n>\n>Mallory wants to defraud Bob with a 1 BTC payment for some beer. Bob \n>runs the old rules. Bob creates a p2pkh address for Mallory to use. \n>Mallory takes 1 BTC, and creates an invalid SegWit transaction that Bob \n>cannot properly validate and that pays into one of Mallory's wallets. \n>Mallory then immediately spends the unconfirmed transaction into Bob's \n>address. Bob sees what appears to be a valid transaction chain which is \n>not actually valid.\n>\n>Clueless Carol is one of the 4.9% of miners who forgot to upgrade her \n>mining node. Carol sees that Mallory included an enormous fee in his \n>transactions, so Carol makes sure to include both transactions in her \n>block.\n>\n>Mallory gets free beer.\n>\n>Anything I'm missing?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/a519b123/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-12-18T12:18:45",
                "message_text_only": "On Fri, Dec 18, 2015 at 03:02:36AM +0000, Eric Lombrozo via bitcoin-dev wrote:\n> First of all, that's an expensive beer!\n> \n> Second of all, any consensus rule change risks non-full-validating\n> or non-upgraded nodes seeing invalid confirmations...but assuming a\n> large supermajority (i.e. > 95%) of hashing power is behind the new\n> rule, it is extremely unlikely that very many invalid confirmations\n> will ever be seen by anyone. The number of confirmations you require\n\nTo clarify, because the 95% of upgraded hashing power is creating valid\nblocks from the point of view of the remaining 5%, that 95% majority\nwill continually reorg the 5% non-upgrading chain. This ensures that the\ninvalid chain remains short, and thus the # of invalid confirmations\npossible remains small. For instance, the chance of getting one invalid\nconfirmation is 0.05^1 = 5%, two invalid confirmations 0.05^2 = 0.25%, three\n0.05^3 = 0.01% etc.\n\nWhereas with a hard fork, the 5% of miners will continue mining on their\nown chain. While that chain's length will increase more slowly than\nnormal, the # of confirmations that non-upgraded clients will see on it\nare unbounded.\n\n\nAnyway, we should write this up as a BIP - there's been a tremendous\namount of misinformation, even flat out lies, floating around on this\nsubject.\n\n-- \n'peter'[:-1]@petertodd.org\n000000000000000001bd68962863e6fa34e9776df361d4926912f52fc5f4b618\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/6607180d/attachment.sig>"
            },
            {
                "author": "Bryan Bishop",
                "date": "2015-12-19T15:48:09",
                "message_text_only": "On Fri, Dec 18, 2015 at 6:18 AM, Peter Todd via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Anyway, we should write this up as a BIP - there's been a tremendous\n> amount of misinformation, even flat out lies, floating around on this\n> subject.\n>\n\nEr, this sounds like something that should go into bip99. Right?\n\n- Bryan\nhttp://heybryan.org/\n1 512 203 0507\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151219/a2ae7187/attachment.html>"
            },
            {
                "author": "jl2012",
                "date": "2015-12-18T03:10:02",
                "message_text_only": "Jonathan Toomim via bitcoin-dev \u65bc 2015-12-17 21:47 \u5beb\u5230:\n> Mallory wants to defraud Bob with a 1 BTC payment for some beer. Bob\n> runs the old rules. Bob creates a p2pkh address for Mallory to use.\n> Mallory takes 1 BTC, and creates an invalid SegWit transaction that\n> Bob cannot properly validate and that pays into one of Mallory's\n> wallets. Mallory then immediately spends the unconfirmed transaction\n> into Bob's address. Bob sees what appears to be a valid transaction\n> chain which is not actually valid.\n> \n> Clueless Carol is one of the 4.9% of miners who forgot to upgrade her\n> mining node. Carol sees that Mallory included an enormous fee in his\n> transactions, so Carol makes sure to include both transactions in her\n> block.\n> \n> Mallory gets free beer.\n> \n> Anything I'm missing?\n\nYou miss the fact that 0-conf is not safe, neither 1-conf. What you are \nsuggesting is just a variation of Finney attack."
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-12-18T05:32:31",
                "message_text_only": "To me it's getting clearer and clearer that th frintier between\nsoftforks and hardforks it's softer than we thought.\nAoftforks should start having a minimum median time deplayment day (be\nit height or median time, I don't care, just not header.nTime).\nTYDGFHdfthfg64565$%^$\n\nOn Fri, Dec 18, 2015 at 4:10 AM, jl2012 via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Jonathan Toomim via bitcoin-dev \u65bc 2015-12-17 21:47 \u5beb\u5230:\n>>\n>> Mallory wants to defraud Bob with a 1 BTC payment for some beer. Bob\n>> runs the old rules. Bob creates a p2pkh address for Mallory to use.\n>> Mallory takes 1 BTC, and creates an invalid SegWit transaction that\n>> Bob cannot properly validate and that pays into one of Mallory's\n>> wallets. Mallory then immediately spends the unconfirmed transaction\n>> into Bob's address. Bob sees what appears to be a valid transaction\n>> chain which is not actually valid.\n>>\n>> Clueless Carol is one of the 4.9% of miners who forgot to upgrade her\n>> mining node. Carol sees that Mallory included an enormous fee in his\n>> transactions, so Carol makes sure to include both transactions in her\n>> block.\n>>\n>> Mallory gets free beer.\n>>\n>> Anything I'm missing?\n>\n>\n> You miss the fact that 0-conf is not safe, neither 1-conf. What you are\n> suggesting is just a variation of Finney attack.\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Anthony Towns",
                "date": "2015-12-18T06:12:23",
                "message_text_only": "On Fri, Dec 18, 2015 at 10:47:14AM +0800, Jonathan Toomim via bitcoin-dev wrote:\n> On Dec 18, 2015, at 10:30 AM, Pieter Wuille via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > 1) The risk of an old full node wallet accepting a transaction that is\n> > invalid to the new rules.\n> > The receiver wallet chooses what address/script to accept coins on.\n> > They'll upgrade to the new softfork rules before creating an address\n> > that depends on the softfork's features.\n> > So, not a problem.\n> Mallory wants to defraud Bob with a 1 BTC payment for some beer. Bob\n> runs the old rules. Bob creates a p2pkh address for Mallory to\n> use. Mallory takes 1 BTC, and creates an invalid SegWit transaction\n> that Bob cannot properly validate and that pays into one of Mallory's\n> wallets. [...]\n> Clueless Carol is one of the 4.9% of miners who forgot to upgrade\n> her mining node. Carol sees that Mallory included an enormous fee in\n> his transactions, so Carol makes sure to include both transactions in\n> her block.\n\nFor it to be a \"safe\" soft fork, the \"invalid segwit transaction\" should\nlook non-standard to Carol, and as such she should refuse to mine it.\n\nI think the attack has to go like this:\n\n * segwit activates; 5% of miners fail to upgrade however\n\n * Mallory creates a transaction paying to a segwit script\n   (ie scriptPubKey is just a 33 byte push) [0]\n\n * non-upgraded nodes and miners will refuse to forward or mine\n   this transaction (a non-p2sh scriptPubKey that just pushes data is\n   non-standard) but the upgraded nodes and miners will forward and mine\n   it. it will be included in the blockchain by upgraded miners fairly\n   soon, and will then be in the UTXO set of non-upgraded miners and\n   nodes too.\n\n * Mallory creates a segwit-invalid spend back to himself (or directly\n   to Bob for the 1BTC), ie provides empty scriptSig, but no\n   witness data. Upgraded miners and nodes reject the transaction,\n   but non-upgraded nodes will relay and mine it afaics.\n\nI *think* that transaction will fail the AreInputsStandard() test on\nnon-upgraded nodes, and thus still won't be accepted to the mempool\nor mined by non-upgraded nodes, and thus no one will see it, or any\ndescendent transactions. (Upgraded nodes will reject it because it's\nsegwit invalid, of course)\n\nIf it is accepted by some old nodes, that transaction won't ever get many\nconfirmations -- if Carol mines it, her block will be orphaned by the\nupgraded mining majority after the next two or three blocks are found.\nWith only 5% of hashpower, it will take around three hours for Carol\nand friends to find a block in general.\n\nAlso, the fact that segwit outputs are \"anyone can spend\" maybe mitigates\nthis further -- you could have a vigilante node that creates invalid\nsegwit txns for every segwit output that just spends the entire thing\nto fees. Even if the vigilante's transactions get rejected by nodes who\nsee Mallory's attempt first, that should still be enough to trigger any\nsort of double-spend alerts I can think of, at least if anyone at all\nis altruistic enough to run a vigilante node like that in the first place.\n\n> Mallory gets free beer.\n> Anything I'm missing?\n\nSo I think the only way Mallory gets free beer from you with segwit\nsoft-fork is if:\n\n - you're running out of date software and you're ignoring warnings to\n   upgrade (block versions have bumped)\n - you've turned off standardness checks\n - you're accepting low-confirmation transactions\n - you're not using any double-spend detection service\n\nIf you're not accepting zero-confirmation transactions straight from\nthe mempool, (ie you require 1 or 2 confirmations) you also need:\n\n - some non-upgraded miners who have turned off standardness checks\n - your business is setup that an attacker can happily wait hours for\n   the transaction to be included in a block before trying to get beer\n   from you\n\nIn general (IMO), just leaving standardness checks turned on (and waiting\nfor 6 confirmations before accepting any non-standard transaction) should\nbe enough to keep you safe from any attack a soft-fork might enable.\n\nUpgrading your software regularly should also be enough to keep you safe\nfor any soft-fork, and also for any hard-fork, obviously.\n\nCheers,\naj\n\n[0] Actually, for this attack Mallory could use *any* segwit payment, it\n    doesn't have to be his bitcoins to start with, he just has to make\n    it look like they finish up with him, which is trivial if segwit\n    looks like anyone can spend. Having it be his segwit payment in the\n    first place makes it a little easier to ensure his payment is seen\n    as the original and not the doublespend though."
            },
            {
                "author": "Chris",
                "date": "2015-12-19T01:36:21",
                "message_text_only": "On Dec 18, 2015, at 10:30 AM, Pieter Wuille via bitcoin-dev \n<bitcoin-dev at lists.linuxfoundation.org \n<mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n> 2) The risk of an old full node wallet accepting a transaction whose\n> coins passed through a script that depends on the softforked rules.\n>\nThere's that, but there's also a case where an attacker creates a \nmajority chain that follows the old rules but not the new ones. \nNon-upgraded nodes would accept a transaction on what they believe to be \nthe consensus chain only to find that when they try to spend those coins \nno one accepts them because they were part of an invalid chain.\n\nThis has the effect of dropping non upgraded nodes to a form of spv \nsecurity without their consent.\n\nThis is in contrast to a hard fork where a full node operator could \nexplicitly set their node to accept higher version blocks that it can't \nvalidate. They get the soft fork functionality back but they have at \nleast consented to it rather than have it forced on them. Doing forks \nthat way would also have the benefit of notifying the user they are \naccepting unvalidated coins, whereas they wont know that in a soft fork.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/94b6565b/attachment.html>"
            },
            {
                "author": "Andrew",
                "date": "2015-12-19T17:46:02",
                "message_text_only": "On Fri, Dec 18, 2015 at 2:47 AM, Jonathan Toomim via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n> On Dec 18, 2015, at 10:30 AM, Pieter Wuille via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> 1) The risk of an old full node wallet accepting a transaction that is\n> invalid to the new rules.\n>\n> The receiver wallet chooses what address/script to accept coins on.\n> They'll upgrade to the new softfork rules before creating an address\n> that depends on the softfork's features.\n>\n> So, not a problem.\n>\n>\n> Mallory wants to defraud Bob with a 1 BTC payment for some beer. Bob runs\n> the old rules. Bob creates a p2pkh address for Mallory to use. Mallory\n> takes 1 BTC, and creates an invalid SegWit transaction that Bob cannot\n> properly validate and that pays into one of Mallory's wallets. Mallory then\n> immediately spends the unconfirmed transaction into Bob's address. Bob sees\n> what appears to be a valid transaction chain which is not actually valid.\n>\n> What do you mean a valid transaction chain? If Bob is fully validating\n(even with old software), he should see that Mallory's signature is not on\na transaction with his address.\n\nDo you mean Mallory creates a regular transaction as well as an\nAnyone-can-spend segwit transaction that results in double spending in the\nsame block?\n\nSorry not sure what I'm missing...\n\n\n> Clueless Carol is one of the 4.9% of miners who forgot to upgrade her\n> mining node. Carol sees that Mallory included an enormous fee in his\n> transactions, so Carol makes sure to include both transactions in her\n> block.\n>\n> Mallory gets free beer.\n>\n> Anything I'm missing?\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n\n\n-- \nPGP: B6AC 822C 451D 6304 6A28  49E9 7DB7 011C D53B 5647\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151219/7dec3138/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2015-12-20T04:14:25",
                "message_text_only": "Jonathan Toomim via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>\nwrites:\n> On Dec 18, 2015, at 10:30 AM, Pieter Wuille via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> 1) The risk of an old full node wallet accepting a transaction that is\n>> invalid to the new rules.\n>> \n>> The receiver wallet chooses what address/script to accept coins on.\n>> They'll upgrade to the new softfork rules before creating an address\n>> that depends on the softfork's features.\n>> \n>> So, not a problem.\n>\n>\n> Mallory wants to defraud Bob with a 1 BTC payment for some beer. Bob\n> runs the old rules. Bob creates a p2pkh address for Mallory to\n> use. Mallory takes 1 BTC, and creates an invalid SegWit transaction\n> that Bob cannot properly validate and that pays into one of Mallory's\n> wallets. Mallory then immediately spends the unconfirmed transaction\n> into Bob's address. Bob sees what appears to be a valid transaction\n> chain which is not actually valid.\n\nPretty sure Bob's wallet will be looking for \"OP_DUP OP_HASH160\n<pubKeyHash> OP_EQUALVERIFY OP_CHECKSIG\" scriptSig.  The SegWit-usable\noutputs will (have to) look different, won't they?\n\nCheers,\nRusty."
            },
            {
                "author": "jl2012",
                "date": "2015-12-20T19:16:29",
                "message_text_only": "Rusty Russell via bitcoin-dev \u65bc 2015-12-19 23:14 \u5beb\u5230:\n> Jonathan Toomim via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>\n> writes:\n>> On Dec 18, 2015, at 10:30 AM, Pieter Wuille via bitcoin-dev \n>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> \n>>> 1) The risk of an old full node wallet accepting a transaction that \n>>> is\n>>> invalid to the new rules.\n>>> \n>>> The receiver wallet chooses what address/script to accept coins on.\n>>> They'll upgrade to the new softfork rules before creating an address\n>>> that depends on the softfork's features.\n>>> \n>>> So, not a problem.\n>> \n>> \n>> Mallory wants to defraud Bob with a 1 BTC payment for some beer. Bob\n>> runs the old rules. Bob creates a p2pkh address for Mallory to\n>> use. Mallory takes 1 BTC, and creates an invalid SegWit transaction\n>> that Bob cannot properly validate and that pays into one of Mallory's\n>> wallets. Mallory then immediately spends the unconfirmed transaction\n>> into Bob's address. Bob sees what appears to be a valid transaction\n>> chain which is not actually valid.\n> \n> Pretty sure Bob's wallet will be looking for \"OP_DUP OP_HASH160\n> <pubKeyHash> OP_EQUALVERIFY OP_CHECKSIG\" scriptSig.  The SegWit-usable\n> outputs will (have to) look different, won't they?\n> \n> Cheers,\n> Rusty.\n\nI think he means Mallory is paying with an invalid Segwit input, not \noutput (there is no \"invalid output\" anyway). However, this is not a \nissue if Bob waits for a few confirmations."
            }
        ],
        "thread_summary": {
            "title": "On the security of softforks",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Rusty Russell",
                "Eric Lombrozo",
                "Bryan Bishop",
                "Andrew",
                "Anthony Towns",
                "Peter Todd",
                "Jonathan Toomim",
                "Jorge Tim\u00f3n",
                "Pieter Wuille",
                "jl2012",
                "Chris"
            ],
            "messages_count": 12,
            "total_messages_chars_count": 25471
        }
    },
    {
        "title": "[bitcoin-dev] The increase of max block size should be determined by block height instead of block time",
        "thread_messages": [
            {
                "author": "Chun Wang",
                "date": "2015-12-18T19:17:03",
                "message_text_only": "In many BIPs we have seen, include the latest BIP202, it is the block\ntime that determine the max block size. From from pool's point of\nview, it cannot issue a job with a fixed ntime due to the existence of\nntime roll. It is hard to issue a job with the max block size unknown.\nFor developers, it is also easier to implement if max block size is a\nfunction of block height instead of time. Block height is also much\nmore simple and elegant than time."
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-12-18T19:52:19",
                "message_text_only": "I agree that nHeight is the simplest option and is my preference.\nAnother option is to use the median time from the previous block (thus you\nknow whether or not the next block should start the miner confirmation or\nnot). In fact, if we're going to use bip9  for 95% miner upgrade\nconfirmation, it would be nice to always pick a difficulty retarget block\n(ie block.nHeight % DifficultyAdjustmentInterval == 0).\nActually I would always have an initial height in bip9, for softforks too.\nI would also use the sign bit as the \"hardfork bit\" that gets activated for\nthe next diff interval after 95% is reached and a hardfork becomes active\n(that way even SPV nodes will notice when a softfork  or hardfork happens\nand also be able to tell which one is it).\nI should update bip99 with all this. And if the 2 mb bump is\nuncontroversial, maybe I can add that to the timewarp fix and th recovery\nof the other 2 bits in block.nVersion (given that bip102 doesn't seem to\nfollow bip99's recommendations and doesn't want to give 6 full months as\nthe pre activation grace period).\nOn Dec 18, 2015 8:17 PM, \"Chun Wang via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> In many BIPs we have seen, include the latest BIP202, it is the block\n> time that determine the max block size. From from pool's point of\n> view, it cannot issue a job with a fixed ntime due to the existence of\n> ntime roll. It is hard to issue a job with the max block size unknown.\n> For developers, it is also easier to implement if max block size is a\n> function of block height instead of time. Block height is also much\n> more simple and elegant than time.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/80de6201/attachment-0001.html>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-12-18T20:02:24",
                "message_text_only": ">From a code standpoint, based off height is easy.\n\nMy first internal version triggered on block 406,800 (~May 5), and each\nblock increased by 20 bytes thereafter.\n\nIt was changed to time, because time was the standard used in years past\nfor other changes; MTP flag day is more stable than block height.\n\nIt is preferred to have a single flag trigger (height or time), rather than\nthe more complex trigger-on-time, increment-on-height, but any combination\nof those will work.\n\nEasy to change code back to height-based...\n\n\n\nOn Fri, Dec 18, 2015 at 2:52 PM, Jorge Tim\u00f3n <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> I agree that nHeight is the simplest option and is my preference.\n> Another option is to use the median time from the previous block (thus you\n> know whether or not the next block should start the miner confirmation or\n> not). In fact, if we're going to use bip9  for 95% miner upgrade\n> confirmation, it would be nice to always pick a difficulty retarget block\n> (ie block.nHeight % DifficultyAdjustmentInterval == 0).\n> Actually I would always have an initial height in bip9, for softforks too.\n> I would also use the sign bit as the \"hardfork bit\" that gets activated\n> for the next diff interval after 95% is reached and a hardfork becomes\n> active (that way even SPV nodes will notice when a softfork  or hardfork\n> happens and also be able to tell which one is it).\n> I should update bip99 with all this. And if the 2 mb bump is\n> uncontroversial, maybe I can add that to the timewarp fix and th recovery\n> of the other 2 bits in block.nVersion (given that bip102 doesn't seem to\n> follow bip99's recommendations and doesn't want to give 6 full months as\n> the pre activation grace period).\n> On Dec 18, 2015 8:17 PM, \"Chun Wang via bitcoin-dev\" <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> In many BIPs we have seen, include the latest BIP202, it is the block\n>> time that determine the max block size. From from pool's point of\n>> view, it cannot issue a job with a fixed ntime due to the existence of\n>> ntime roll. It is hard to issue a job with the max block size unknown.\n>> For developers, it is also easier to implement if max block size is a\n>> function of block height instead of time. Block height is also much\n>> more simple and elegant than time.\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/770de789/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-12-18T20:10:02",
                "message_text_only": "Well, if it's not going to be height, I think median time of the previous\nblock is better than the time of the current one, and would also solve Chun\nWang's concerns.\nBut as said I prefer to use heights that correspond to diff recalculation\n(because that's the window that bip9 will use for the later 95%\nconfirmation anyway).\nOn Dec 18, 2015 9:02 PM, \"Jeff Garzik\" <jgarzik at gmail.com> wrote:\n\n> From a code standpoint, based off height is easy.\n>\n> My first internal version triggered on block 406,800 (~May 5), and each\n> block increased by 20 bytes thereafter.\n>\n> It was changed to time, because time was the standard used in years past\n> for other changes; MTP flag day is more stable than block height.\n>\n> It is preferred to have a single flag trigger (height or time), rather\n> than the more complex trigger-on-time, increment-on-height, but any\n> combination of those will work.\n>\n> Easy to change code back to height-based...\n>\n>\n>\n> On Fri, Dec 18, 2015 at 2:52 PM, Jorge Tim\u00f3n <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> I agree that nHeight is the simplest option and is my preference.\n>> Another option is to use the median time from the previous block (thus\n>> you know whether or not the next block should start the miner confirmation\n>> or not). In fact, if we're going to use bip9  for 95% miner upgrade\n>> confirmation, it would be nice to always pick a difficulty retarget block\n>> (ie block.nHeight % DifficultyAdjustmentInterval == 0).\n>> Actually I would always have an initial height in bip9, for softforks too.\n>> I would also use the sign bit as the \"hardfork bit\" that gets activated\n>> for the next diff interval after 95% is reached and a hardfork becomes\n>> active (that way even SPV nodes will notice when a softfork  or hardfork\n>> happens and also be able to tell which one is it).\n>> I should update bip99 with all this. And if the 2 mb bump is\n>> uncontroversial, maybe I can add that to the timewarp fix and th recovery\n>> of the other 2 bits in block.nVersion (given that bip102 doesn't seem to\n>> follow bip99's recommendations and doesn't want to give 6 full months as\n>> the pre activation grace period).\n>> On Dec 18, 2015 8:17 PM, \"Chun Wang via bitcoin-dev\" <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> In many BIPs we have seen, include the latest BIP202, it is the block\n>>> time that determine the max block size. From from pool's point of\n>>> view, it cannot issue a job with a fixed ntime due to the existence of\n>>> ntime roll. It is hard to issue a job with the max block size unknown.\n>>> For developers, it is also easier to implement if max block size is a\n>>> function of block height instead of time. Block height is also much\n>>> more simple and elegant than time.\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/20ab7c5a/attachment.html>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-12-18T20:15:54",
                "message_text_only": "My preference is height activation + one step per block (i.e. also\nheight).  Height seems KISS.\n\nAFAICT most of the attacks would occur around the already-heavily-watched\nflag day activation event, in a height based environment, a useful\nattribute.\n\nHowever I would like to hear from others about possible attacks with the\nvarious approaches, before diverging from the default community approach of\nswitch-based-on-time.\n\n\n\n\n\n\nOn Fri, Dec 18, 2015 at 3:10 PM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n\n> Well, if it's not going to be height, I think median time of the previous\n> block is better than the time of the current one, and would also solve Chun\n> Wang's concerns.\n> But as said I prefer to use heights that correspond to diff recalculation\n> (because that's the window that bip9 will use for the later 95%\n> confirmation anyway).\n> On Dec 18, 2015 9:02 PM, \"Jeff Garzik\" <jgarzik at gmail.com> wrote:\n>\n>> From a code standpoint, based off height is easy.\n>>\n>> My first internal version triggered on block 406,800 (~May 5), and each\n>> block increased by 20 bytes thereafter.\n>>\n>> It was changed to time, because time was the standard used in years past\n>> for other changes; MTP flag day is more stable than block height.\n>>\n>> It is preferred to have a single flag trigger (height or time), rather\n>> than the more complex trigger-on-time, increment-on-height, but any\n>> combination of those will work.\n>>\n>> Easy to change code back to height-based...\n>>\n>>\n>>\n>> On Fri, Dec 18, 2015 at 2:52 PM, Jorge Tim\u00f3n <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> I agree that nHeight is the simplest option and is my preference.\n>>> Another option is to use the median time from the previous block (thus\n>>> you know whether or not the next block should start the miner confirmation\n>>> or not). In fact, if we're going to use bip9  for 95% miner upgrade\n>>> confirmation, it would be nice to always pick a difficulty retarget block\n>>> (ie block.nHeight % DifficultyAdjustmentInterval == 0).\n>>> Actually I would always have an initial height in bip9, for softforks\n>>> too.\n>>> I would also use the sign bit as the \"hardfork bit\" that gets activated\n>>> for the next diff interval after 95% is reached and a hardfork becomes\n>>> active (that way even SPV nodes will notice when a softfork  or hardfork\n>>> happens and also be able to tell which one is it).\n>>> I should update bip99 with all this. And if the 2 mb bump is\n>>> uncontroversial, maybe I can add that to the timewarp fix and th recovery\n>>> of the other 2 bits in block.nVersion (given that bip102 doesn't seem to\n>>> follow bip99's recommendations and doesn't want to give 6 full months as\n>>> the pre activation grace period).\n>>> On Dec 18, 2015 8:17 PM, \"Chun Wang via bitcoin-dev\" <\n>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>>> In many BIPs we have seen, include the latest BIP202, it is the block\n>>>> time that determine the max block size. From from pool's point of\n>>>> view, it cannot issue a job with a fixed ntime due to the existence of\n>>>> ntime roll. It is hard to issue a job with the max block size unknown.\n>>>> For developers, it is also easier to implement if max block size is a\n>>>> function of block height instead of time. Block height is also much\n>>>> more simple and elegant than time.\n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>\n>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/42f35752/attachment-0001.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-12-18T20:20:13",
                "message_text_only": "I believe the attacks are the same for height or median time of the prev\nblock are equal, only the time of the current block has more edge cases.\nOn Dec 18, 2015 9:15 PM, \"Jeff Garzik\" <jgarzik at gmail.com> wrote:\n\n> My preference is height activation + one step per block (i.e. also\n> height).  Height seems KISS.\n>\n> AFAICT most of the attacks would occur around the already-heavily-watched\n> flag day activation event, in a height based environment, a useful\n> attribute.\n>\n> However I would like to hear from others about possible attacks with the\n> various approaches, before diverging from the default community approach of\n> switch-based-on-time.\n>\n>\n>\n>\n>\n>\n> On Fri, Dec 18, 2015 at 3:10 PM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n>\n>> Well, if it's not going to be height, I think median time of the previous\n>> block is better than the time of the current one, and would also solve Chun\n>> Wang's concerns.\n>> But as said I prefer to use heights that correspond to diff recalculation\n>> (because that's the window that bip9 will use for the later 95%\n>> confirmation anyway).\n>> On Dec 18, 2015 9:02 PM, \"Jeff Garzik\" <jgarzik at gmail.com> wrote:\n>>\n>>> From a code standpoint, based off height is easy.\n>>>\n>>> My first internal version triggered on block 406,800 (~May 5), and each\n>>> block increased by 20 bytes thereafter.\n>>>\n>>> It was changed to time, because time was the standard used in years past\n>>> for other changes; MTP flag day is more stable than block height.\n>>>\n>>> It is preferred to have a single flag trigger (height or time), rather\n>>> than the more complex trigger-on-time, increment-on-height, but any\n>>> combination of those will work.\n>>>\n>>> Easy to change code back to height-based...\n>>>\n>>>\n>>>\n>>> On Fri, Dec 18, 2015 at 2:52 PM, Jorge Tim\u00f3n <\n>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>>> I agree that nHeight is the simplest option and is my preference.\n>>>> Another option is to use the median time from the previous block (thus\n>>>> you know whether or not the next block should start the miner confirmation\n>>>> or not). In fact, if we're going to use bip9  for 95% miner upgrade\n>>>> confirmation, it would be nice to always pick a difficulty retarget block\n>>>> (ie block.nHeight % DifficultyAdjustmentInterval == 0).\n>>>> Actually I would always have an initial height in bip9, for softforks\n>>>> too.\n>>>> I would also use the sign bit as the \"hardfork bit\" that gets activated\n>>>> for the next diff interval after 95% is reached and a hardfork becomes\n>>>> active (that way even SPV nodes will notice when a softfork  or hardfork\n>>>> happens and also be able to tell which one is it).\n>>>> I should update bip99 with all this. And if the 2 mb bump is\n>>>> uncontroversial, maybe I can add that to the timewarp fix and th recovery\n>>>> of the other 2 bits in block.nVersion (given that bip102 doesn't seem to\n>>>> follow bip99's recommendations and doesn't want to give 6 full months as\n>>>> the pre activation grace period).\n>>>> On Dec 18, 2015 8:17 PM, \"Chun Wang via bitcoin-dev\" <\n>>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>>\n>>>>> In many BIPs we have seen, include the latest BIP202, it is the block\n>>>>> time that determine the max block size. From from pool's point of\n>>>>> view, it cannot issue a job with a fixed ntime due to the existence of\n>>>>> ntime roll. It is hard to issue a job with the max block size unknown.\n>>>>> For developers, it is also easier to implement if max block size is a\n>>>>> function of block height instead of time. Block height is also much\n>>>>> more simple and elegant than time.\n>>>>> _______________________________________________\n>>>>> bitcoin-dev mailing list\n>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>\n>>>>\n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>\n>>>>\n>>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/984ddbc3/attachment.html>"
            },
            {
                "author": "gb",
                "date": "2015-12-18T20:58:42",
                "message_text_only": "On Fri, 2015-12-18 at 15:15 -0500, Jeff Garzik via bitcoin-dev wrote:\n> My preference is height activation + one step per block (i.e. also\n> height).  Height seems KISS.\n> \n> \nUnder this scheme the size of the step-per-block increase could be\ndecreased every 210,000 blocks (at time of reward halvings). \n\nSo, a linear growth rate that decreases every ~4 years, ultimately\ngrandfathering max_block_size increases on the same block-schedule that\nreward decreases."
            },
            {
                "author": "Peter Todd",
                "date": "2015-12-18T20:43:35",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA512\n\n\n\nOn 18 December 2015 11:52:19 GMT-08:00, \"Jorge Tim\u00f3n via bitcoin-dev\" <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>I agree that nHeight is the simplest option and is my preference.\n>Another option is to use the median time from the previous block\n\n\nFWIW all these median time based schemes should be using median time past: the point is to use a time that the block creator has no direct control of, while still tying the rule to wall clock time for planning purposes.\n-----BEGIN PGP SIGNATURE-----\n\niQE9BAEBCgAnIBxQZXRlciBUb2RkIDxwZXRlQHBldGVydG9kZC5vcmc+BQJWdG/r\nAAoJEMCF8hzn9Lncz4MH/iYJv6aB9rvfvy1KuSSHAQDQ++6j7Flmk2n8f/S4jt4q\n92MZnKDw09HxUJiWvwREi81wHpq4JedgK1Z/+8m3wlK+jaIyWZ7Su+Jm+EqsoOSJ\nSx6oisbyFlhVEUAdaG/XOX/K0mqh01NSvGGpoQjHAYzcG3pI03OC4G7Qg4WGeZLx\nO0yb387DmK/of52JGJcei3TUx0w8Up/GdXDqerLxioH7fhGhtGCj0vyD4LugnNLQ\nhka5g+hri27YltfaRxncNQ0nZT4rAfgRgRH1Qi3kHnc6ZgRcRjjb36TyrWjZ34eb\n9+YDAirFwu8HGmi7lfxh9DDtVjPZCwKal7/rNeRI744=\n=7f+W\n-----END PGP SIGNATURE-----"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-12-18T22:58:29",
                "message_text_only": "On Dec 18, 2015 9:43 PM, \"Peter Todd\" <pete at petertodd.org> wrote:\n> FWIW all these median time based schemes should be using median time\npast: the point is to use a time that the block creator has no direct\ncontrol of, while still tying the rule to wall clock time for planning\npurposes.\n\nWell, if after the \"planned clock time\" you need to wait for the next diff\nretarget and then wait for 95% (bip9) I think the value of being able to\nuse \"human friendly clock time\" is very dubious (specially since median\ntime is different from real-world time anyway).\nBut yeah, not giving the creator of the current block direct control over\nwhether its block starts the activation process or not is achieved with\nmedian time of the previous block just as well as nHeight does.\nSo even if I disagree with the value that median time brings over the\nsimpler height approach, let's please decide on one and always use that for\nboth hardforks and softforks as part of bip9 (which we would need to\nmodify).\nAn initial time threshold is not necessary for uncontroversial softforks,\nbut it doesn't hurt (you can always set it in the past if you want to not\nuse it) and in fact it simplifies bip9's implementation.\nLet's please decide once and for all, update bip9 and bip99 and stop doing\nsomething different on every hardfork patch we write.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/ae398fa6/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-12-19T18:20:38",
                "message_text_only": "On Sat, Dec 19, 2015 at 03:17:03AM +0800, Chun Wang via bitcoin-dev wrote:\n> In many BIPs we have seen, include the latest BIP202, it is the block\n> time that determine the max block size. From from pool's point of\n> view, it cannot issue a job with a fixed ntime due to the existence of\n> ntime roll. It is hard to issue a job with the max block size unknown.\n> For developers, it is also easier to implement if max block size is a\n> function of block height instead of time. Block height is also much\n> more simple and elegant than time.\n\nIf size is calculated from the median time past, which is fixed for a\ngiven block and has no dependency on the block header's nTime field,\ndoes that solve your problem?\n\nBy \"median time past\" I mean the median time for the previous block.\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151219/519299d0/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "The increase of max block size should be determined by block height instead of block time",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeff Garzik",
                "Peter Todd",
                "Jorge Tim\u00f3n",
                "Chun Wang",
                "gb"
            ],
            "messages_count": 10,
            "total_messages_chars_count": 20952
        }
    },
    {
        "title": "[bitcoin-dev] Segregated witness softfork with moderate adoption has very small block size effect",
        "thread_messages": [
            {
                "author": "jl2012",
                "date": "2015-12-19T16:49:25",
                "message_text_only": "I have done some calculation for the effect of a SW softfork on the \nactual total block size.\n\nDefinitions:\n\nCore block size (CBS): The block size as seen by a non-upgrading full \nnode\nWitness size (WS): The total size of witness in a block\nTotal block size (TBS): CBS + WS\nWitness discount (WD): A discount factor for witness for calculation of \nVBS (1 = no discount)\nVirtual block size (VBS): CBS + (WS * WD)\nWitness adoption (WA): Proportion of new format transactions among all \ntransactions\nPrunable ratio (PR): Proportion of signature data size in a transaction\n\nWith some transformation it could be shown that:\n\n  TBS = CBS / (1 - WA * PR) = VBS / (1 - WA * PR * (1 - WD))\n\nsipa suggested a WD of 25%.\n\nThe PR heavily depends on the transaction script type and input-output \nratio. For example, the PR of 1-in 2-out P2PKH and 1-in 1-out 2-of-2 \nmultisig P2SH are about 47% and 72% respectively. According to sipa's \npresentation, the current average PR on the blockchain is about 60%.\n\nAssuming WD=25% and PR=60%, the MAX TBS with different MAX VBS and WA is \nlisted at:\n\nhttp://i.imgur.com/4bgTMRO.png\n\nThe highlight indicates whether the CBS or VBS is the limiting factor.\n\nWith moderate SW adoption at 40-60%, the total block size is 1.32-1.56MB \nwhen MAX VBS is 1.25MB, and 1.22-1.37MB when MAX VBS is 1.00MB.\n\nP2SH has been introduced for 3.5 years and only about 10% of bitcoin is \nstored this way (I can't find proportion of existing P2SH address). A \n1-year adoption rate of 40% for segwit is clearly over-optimistic unless \nthe tx fee becomes really high.\n\n(btw the PR of 60% may also be over-optimistic, as using SW nested in \nP2SH will decrease the PR, and therefore TBS becomes even lower)\n\nI am not convinced that SW softfork should be the *only* short term \nscalability solution"
            },
            {
                "author": "Peter Todd",
                "date": "2015-12-19T17:43:10",
                "message_text_only": "On Sat, Dec 19, 2015 at 11:49:25AM -0500, jl2012 via bitcoin-dev wrote:\n> I have done some calculation for the effect of a SW softfork on the\n> actual total block size.\n\nNote how the fact that segwit needs client-side adoption to enable an\nactual blocksize increase can be a good thing: it's a clear sign that\nthe ecosystem as a whole has opted-into a blocksize increase.\n\nNot as good as a direct proof-of-stake vote, and somewhat coercive as a\nvote as you pay lower fees, but it's an interesting side-effect.\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151219/4cda69f5/attachment.sig>"
            },
            {
                "author": "Santino Napolitano",
                "date": "2015-12-19T18:37:06",
                "message_text_only": "I disagree. I think all client-side adoption of SW reliably tells you is that those implementers saw value in it greater than the cost of implementation. It's possible what they valued was the malleability fix and didn't see the limited potential circumvention of MAX_BLOCK_SIZE material to their decision.\n\nThey could just as easily attach an OP_RETURN output to all of their transactions which pushes \"big blocks please\" which would more directly indicate their preference for larger blocks. You could also let hand-signed letters from the heads of businesses explicitly stating their desire speak for their intentions vs. any of this nonsense. Or the media interviews, forum comments, tweets, etc...\n\n19.12.2015, 20:43, \"Peter Todd via bitcoin-dev\" <bitcoin-dev at lists.linuxfoundation.org>:\n> On Sat, Dec 19, 2015 at 11:49:25AM -0500, jl2012 via bitcoin-dev wrote:\n>> \u00a0I have done some calculation for the effect of a SW softfork on the\n>> \u00a0actual total block size.\n>\n> Note how the fact that segwit needs client-side adoption to enable an\n> actual blocksize increase can be a good thing: it's a clear sign that\n> the ecosystem as a whole has opted-into a blocksize increase.\n>\n> Not as good as a direct proof-of-stake vote, and somewhat coercive as a\n> vote as you pay lower fees, but it's an interesting side-effect.\n>\n> --\n> 'peter'[:-1]@petertodd.org\n> 00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d\n> ,\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Chris Priest",
                "date": "2015-12-20T03:37:26",
                "message_text_only": "By that same logic, any wallet that implemented P2SH is also voting\nfor an increased block size, since P2SH results in smaller\ntransactions, in the same way SW results in smaller transactions.\n\nOn 12/19/15, Peter Todd via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> On Sat, Dec 19, 2015 at 11:49:25AM -0500, jl2012 via bitcoin-dev wrote:\n>> I have done some calculation for the effect of a SW softfork on the\n>> actual total block size.\n>\n> Note how the fact that segwit needs client-side adoption to enable an\n> actual blocksize increase can be a good thing: it's a clear sign that\n> the ecosystem as a whole has opted-into a blocksize increase.\n>\n> Not as good as a direct proof-of-stake vote, and somewhat coercive as a\n> vote as you pay lower fees, but it's an interesting side-effect.\n>\n> --\n> 'peter'[:-1]@petertodd.org\n> 00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d\n>"
            },
            {
                "author": "Justus Ranvier",
                "date": "2015-12-19T17:55:10",
                "message_text_only": "On 12/19/2015 10:49 AM, jl2012 via bitcoin-dev wrote:\n> I am not convinced that SW softfork should be the *only* short term\n> scalability solution\n\nI don't think SW is relevant at all with respect to scalability.\n\nFraud proofs are extremely important from a security perspective. The\nnetwork as it exists now places too much trust in miners. Creating a way\nfor non-full node clients to reject chains with contain invalid\ntransactions regardless of how much hashing power produces the invalid\nchains is essential for the security of the network.\n\nAdding a fraud proof system into blocks means that other features, like\ncommitted UTXO sets, become less unsafe to deploy.\n\nSolving transaction malleability is a very nice to have feature.\n\nA scalability solution, IMHO, is \"how do we buy some time to allow\ncontinue usage growth while working on creating a situation where it\nbecomes safe to eliminate maximum block size as a consensus rule?\"\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: 0xEAD9E623.asc\nType: application/pgp-keys\nSize: 23337 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151219/6fc064cf/attachment-0001.bin>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 801 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151219/6fc064cf/attachment-0001.sig>"
            },
            {
                "author": "Douglas Roark",
                "date": "2015-12-20T01:19:59",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA512\n\nOn 2015/12/19 08:49, jl2012 via bitcoin-dev wrote:\n> P2SH has been introduced for 3.5 years and only about 10% of\n> bitcoin is stored this way (I can't find proportion of existing\n> P2SH address). A 1-year adoption rate of 40% for segwit is clearly\n> over-optimistic unless the tx fee becomes really high.\n\nI don't think one can necessarily conflate P2SH and SegWit uptake. In\nthe case of P2SH, there's the \"if it ain't broke, don't fix it\"\nproblem. P2PKH works just fine for an awful lot of Bitcoin users. Why\nshould they switch over to P2SH? In the absence of a compelling\nreason, they'll probably stick to a proven solution. (You can see that\nline of thinking anywhere.) Even Armory, which values security and\nwhiz-bang features over usability, offers P2SH but keeps it off by\ndefault.\n\nMeanwhile, SegWit fixes multiple problems, or at least fixes some\nwhile also sticking a bit of gum on others. True, it'll rely on wallet\nuptake. I just think wallet developers will see the value in it. The\nbig question, of course, is when they'll enable it by default, which\nis the only way SegWit will gain serious traction. My personal,\nsemi-educated guess is that you'll see 3-6 months of wallet\nintegration and testnet tweaking, then another 3-6 months of mainnet\navailability if explicitly enabled by the user, and finally the switch\nbeing thrown for all wallet users. I'm hoping for the aggressive\ntimeframes. I'm expecting the conservative ones.\n\nIs 40% optimistic? Maybe, and I'd personally like to see it enabled in\nconcert with a minimal block size increase. I don't think 40% within a\nyear of deployment is out of the realm of possibility, though.\n\n- -- \n- ---\nDouglas Roark\nCryptocurrency, network security, travel, and art.\nhttps://onename.com/droark\njoroark at vt.edu\nPGP key ID: 26623924\n-----BEGIN PGP SIGNATURE-----\nComment: GPGTools - https://gpgtools.org\n\niQIcBAEBCgAGBQJWdgI/AAoJEEOBHRomYjkkZwsP/iZT+qa/Yp2xIE6ConcTrbbx\nIOmz6h4O+ro/Egx/6XrukBSRybn8gsjize279WQWjR0h7O3KS4YAGuR/TKx6IrOw\ncz7lZpwC08ZcZb83fUqEqz4q/Rbgp4cPU8WU1niLCYg2OCGqtTEhSSRatmO1ULXp\n6KJrBCaoVBzaoqFXx6nXiJF/K1dKZsb/IuFFJZisXEmoDkvTunE82sjHZ+JgmHk9\njhhlk+JU43C7lXXkk+3KPbD+z3dMZNDYrIopaWOUXfk6yXIp8cy7MUK/z58PCm8C\nV/pTk0edkMIRxu6ybJLKNNZHqhSQipyEMfG/CojVb6Qtt8zC0RIEUsYj5uDk9RQL\nITxql9RtPHQPx+uoxoCr7Zitx0448YFNpQs6S5/g81vDt7ilh5k5PgnILkMvuA7Y\nF6abFvsP/ahmAkisiyDzwMmcM+xzxvJkaY9aDvKy4NNiFw8kUxkAJ2VlMeQvwVTK\n2ePzyrFTOGFJRk/a1uTr0aUOxpCdI8rB1O6jcrhmLl2ENRMjN1I3Ksl79Q6h3P+F\nzj3hR9CyuXrzoPxAISYF58ot32fil9nEnLcchu2mdWArYKBY2IDWVv7JiK8RCJ8b\n2XymxccKsXIUHTrYJfrHg+AeRHkVuV8emyUp95A/8kb8meWbLbmpxOrt6JLEE4k9\nqsl9Zkg/0IsCr+JzE6Ko\n=696M\n-----END PGP SIGNATURE-----"
            },
            {
                "author": "Peter Todd",
                "date": "2015-12-19T18:48:33",
                "message_text_only": "On Sat, Dec 19, 2015 at 09:37:06PM +0300, Santino Napolitano wrote:\n> I disagree. I think all client-side adoption of SW reliably tells you is that those implementers saw value in it greater than the cost of implementation. It's possible what they valued was the malleability fix and didn't see the limited potential circumvention of MAX_BLOCK_SIZE material to their decision.\n> \n> They could just as easily attach an OP_RETURN output to all of their transactions which pushes \"big blocks please\" which would more directly indicate their preference for larger blocks. You could also let hand-signed letters from the heads of businesses explicitly stating their desire speak for their intentions vs. any of this nonsense. Or the media interviews, forum comments, tweets, etc...\n\nNote that English-language measures of Bitcoin usage/activity are very\nmisleading, as a significant - probably super majority - of economnic\nactivity happens outside the English language, Western world.\nCentralized forums such as twitter and reddit are easily censored and\nmanipulated. Finally, we can't discount the significant amount of\nnon-law-abiding Bitcoin economic activity that does happen, and I do not\nbelieve we should adopt consensus-building processes that shut those\nstakeholders out of the discussion.\n\nAs an aside, I have a friend of mine who made a Bitcoin related product\nwith non-culturally-specific appeal.  I asked where she was shipping her\nproduct, and it turned out that a super majority went to\nnon-English-speaking countries. (she might be willing to go on public\nrecord about this; I can ask)\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151219/4563cbc9/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "Segregated witness softfork with moderate adoption has very small block size effect",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Chris Priest",
                "Peter Todd",
                "Santino Napolitano",
                "jl2012",
                "Douglas Roark",
                "Justus Ranvier"
            ],
            "messages_count": 7,
            "total_messages_chars_count": 11372
        }
    },
    {
        "title": "[bitcoin-dev] We need to fix the block withholding attack",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2015-12-19T18:42:40",
                "message_text_only": "At the recent Scaling Bitcoin conference in Hong Kong we had a chatham\nhouse rules workshop session attending by representitives of a super\nmajority of the Bitcoin hashing power.\n\nOne of the issues raised by the pools present was block withholding\nattacks, which they said are a real issue for them. In particular, pools\nare receiving legitimate threats by bad actors threatening to use block\nwithholding attacks against them. Pools offering their services to the\ngeneral public without anti-privacy Know-Your-Customer have little\ndefense against such attacks, which in turn is a threat to the\ndecentralization of hashing power: without pools only fairly large\nhashing power installations are profitable as variance is a very real\nbusiness expense. P2Pool is often brought up as a replacement for pools,\nbut it itself is still relatively vulnerable to block withholding, and\nin any case has many other vulnerabilities and technical issues that has\nprevented widespread adoption of P2Pool.\n\nFixing block withholding is relatively simple, but (so far) requires a\nSPV-visible hardfork. (Luke-Jr's two-stage target mechanism) We should\ndo this hard-fork in conjunction with any blocksize increase, which will\nhave the desirable side effect of clearly show consent by the entire\necosystem, SPV clients included.\n\n\nNote that Ittay Eyal and Emin Gun Sirer have argued(1) that block\nwitholding attacks are a good thing, as in their model they can be used\nby small pools against larger pools, disincentivising large pools.\nHowever this argument is academic and not applicable to the real world,\nas a much simpler defense against block withholding attacks is to use\nanti-privacy KYC and the legal system combined with the variety of\nwithholding detection mechanisms only practical for large pools.\nEqually, large hashing power installations - a dangerous thing for\ndecentralization - have no block withholding attack vulnerabilities.\n\n1) http://hackingdistributed.com/2014/12/03/the-miners-dilemma/\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151219/8c0d100a/attachment.sig>"
            },
            {
                "author": "Bob McElrath",
                "date": "2015-12-19T19:30:45",
                "message_text_only": "Peter Todd via bitcoin-dev [bitcoin-dev at lists.linuxfoundation.org] wrote:\n> One of the issues raised by the pools present was block withholding\n> attacks, which they said are a real issue for them. In particular, pools\n> are receiving legitimate threats by bad actors threatening to use block\n> withholding attacks against them.\n\nThe only possible other bad actors are other miners.  So who are the \"bad actor\"\nminers?  It's a short list of candidates.\n\n> P2Pool is often brought up as a replacement for pools, but it itself is still\n> relatively vulnerable to block withholding, and in any case has many other\n> vulnerabilities and technical issues that has prevented widespread adoption of\n> P2Pool.\n\nI've been trying to understand this source of \"vulnerabilities and technical\nissues\" with p2pool and have received a lot of contradictory information.  Can\nsomeone in the know summarize what the problems with p2pool are?\n\nThe economic situation where miners can be deprived of profit due to the lack of\nsynchronicity in block updates is a physics problem due to the size of the Earth\nand will never be removed.  This is a design flaw in Bitcoin.  Therefore a\ndifferent, more comprehensive solution is called for.\n\nMy solution to this is somewhat longer term and needs more simulation but\nfundamentally removes the source of contention and fixes the design flaw, while\nremaining as close \"in spirit\" to bitcoin as possible:\n    https://scalingbitcoin.org/hongkong2015/presentations/DAY2/2_breaking_the_chain_1_mcelrath.pdf\nNot only does block withholding simply not work to deny other miners income due\nto the absence of orphans, but I explicitly added a dis-incentive against\nwithholding blocks in terms of the \"cohort difficulty\".  Other graph-theoretic\nquantities are in general possible in the reward function to better align the\nincentives of miners with the correct operation of the system.  Also by lowering\nthe target difficulty and increasing the block (bead) rate, one lowers the\nvariance of miner income.\n\nPart of the reason I ask is that there has been some interest in testing my\nideas in p2pool itself (or a new similar share pool), but I'm failing to\nunderstand the source of all the complaints about p2pool.\n\n--\nCheers, Bob McElrath\n\n\"For every complex problem, there is a solution that is simple, neat, and wrong.\"\n    -- H. L. Mencken"
            },
            {
                "author": "jl2012",
                "date": "2015-12-19T20:03:28",
                "message_text_only": "After the meeting I find a softfork solution. It is very inefficient and \nI am leaving it here just for record.\n\n1. In the first output of the second transaction of a block, mining pool \nwill commit a random nonce with an OP_RETURN.\n\n2. Mine as normal. When a block is found, the hash is concatenated with \nthe committed random nonce and hashed.\n\n3. The resulting hash must be smaller than 2 ^ (256 - 1/64) or the block \nis invalid. That means about 1% of blocks are discarded.\n\n4. For each difficulty retarget, the secondary target is decreased by 2 \n^ 1/64.\n\n5. After 546096 blocks or 10 years, the secondary target becomes 2 ^ \n252. Therefore only 1 in 16 hash returned by hasher is really valid. \nThis should make the detection of block withholding attack much easier.\n\nAll miners have to sacrifice 1% reward for 10 years. Confirmation will \nalso be 1% slower than it should be.\n\nIf a node (full or SPV) is not updated, it becomes more vulnerable as an \nattacker could mine a chain much faster without following the new rules. \nBut this is still a softfork, by definition.\n\n---------------\n\nok, back to topic. Do you mean this? \nhttp://lists.linuxfoundation.org/pipermail/bitcoin-dev/2012-June/001506.html\n\n\n\nPeter Todd via bitcoin-dev \u65bc 2015-12-19 13:42 \u5beb\u5230:\n> At the recent Scaling Bitcoin conference in Hong Kong we had a chatham\n> house rules workshop session attending by representitives of a super\n> majority of the Bitcoin hashing power.\n> \n> One of the issues raised by the pools present was block withholding\n> attacks, which they said are a real issue for them. In particular, \n> pools\n> are receiving legitimate threats by bad actors threatening to use block\n> withholding attacks against them. Pools offering their services to the\n> general public without anti-privacy Know-Your-Customer have little\n> defense against such attacks, which in turn is a threat to the\n> decentralization of hashing power: without pools only fairly large\n> hashing power installations are profitable as variance is a very real\n> business expense. P2Pool is often brought up as a replacement for \n> pools,\n> but it itself is still relatively vulnerable to block withholding, and\n> in any case has many other vulnerabilities and technical issues that \n> has\n> prevented widespread adoption of P2Pool.\n> \n> Fixing block withholding is relatively simple, but (so far) requires a\n> SPV-visible hardfork. (Luke-Jr's two-stage target mechanism) We should\n> do this hard-fork in conjunction with any blocksize increase, which \n> will\n> have the desirable side effect of clearly show consent by the entire\n> ecosystem, SPV clients included.\n> \n> \n> Note that Ittay Eyal and Emin Gun Sirer have argued(1) that block\n> witholding attacks are a good thing, as in their model they can be used\n> by small pools against larger pools, disincentivising large pools.\n> However this argument is academic and not applicable to the real world,\n> as a much simpler defense against block withholding attacks is to use\n> anti-privacy KYC and the legal system combined with the variety of\n> withholding detection mechanisms only practical for large pools.\n> Equally, large hashing power installations - a dangerous thing for\n> decentralization - have no block withholding attack vulnerabilities.\n> \n> 1) http://hackingdistributed.com/2014/12/03/the-miners-dilemma/\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Chris Priest",
                "date": "2015-12-20T03:34:26",
                "message_text_only": "Block witholding attacks are only possible if you have a majority of\nhashpower. If you only have 20% hashpower, you can't do this attack.\nCurrently, this attack is only a theoretical attack, as the ones with\nall the hashpower today are not engaging in this behavior. Even if\nsomeone who had a lot of hashpower decided to pull off this attack,\nthey wouldn't be able to disrupt much. Once that time comes, then I\nthink this problem should be solved, until then it should be a low\npriority. There are more important things to work on in the meantime.\n\nOn 12/19/15, Peter Todd via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> At the recent Scaling Bitcoin conference in Hong Kong we had a chatham\n> house rules workshop session attending by representitives of a super\n> majority of the Bitcoin hashing power.\n>\n> One of the issues raised by the pools present was block withholding\n> attacks, which they said are a real issue for them. In particular, pools\n> are receiving legitimate threats by bad actors threatening to use block\n> withholding attacks against them. Pools offering their services to the\n> general public without anti-privacy Know-Your-Customer have little\n> defense against such attacks, which in turn is a threat to the\n> decentralization of hashing power: without pools only fairly large\n> hashing power installations are profitable as variance is a very real\n> business expense. P2Pool is often brought up as a replacement for pools,\n> but it itself is still relatively vulnerable to block withholding, and\n> in any case has many other vulnerabilities and technical issues that has\n> prevented widespread adoption of P2Pool.\n>\n> Fixing block withholding is relatively simple, but (so far) requires a\n> SPV-visible hardfork. (Luke-Jr's two-stage target mechanism) We should\n> do this hard-fork in conjunction with any blocksize increase, which will\n> have the desirable side effect of clearly show consent by the entire\n> ecosystem, SPV clients included.\n>\n>\n> Note that Ittay Eyal and Emin Gun Sirer have argued(1) that block\n> witholding attacks are a good thing, as in their model they can be used\n> by small pools against larger pools, disincentivising large pools.\n> However this argument is academic and not applicable to the real world,\n> as a much simpler defense against block withholding attacks is to use\n> anti-privacy KYC and the legal system combined with the variety of\n> withholding detection mechanisms only practical for large pools.\n> Equally, large hashing power installations - a dangerous thing for\n> decentralization - have no block withholding attack vulnerabilities.\n>\n> 1) http://hackingdistributed.com/2014/12/03/the-miners-dilemma/\n>\n> --\n> 'peter'[:-1]@petertodd.org\n> 00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d\n>"
            },
            {
                "author": "Matt Corallo",
                "date": "2015-12-20T03:36:10",
                "message_text_only": "Peter was referring to pool-block-withholding, not selfish mining.\n\nOn December 19, 2015 7:34:26 PM PST, Chris Priest via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>Block witholding attacks are only possible if you have a majority of\n>hashpower. If you only have 20% hashpower, you can't do this attack.\n>Currently, this attack is only a theoretical attack, as the ones with\n>all the hashpower today are not engaging in this behavior. Even if\n>someone who had a lot of hashpower decided to pull off this attack,\n>they wouldn't be able to disrupt much. Once that time comes, then I\n>think this problem should be solved, until then it should be a low\n>priority. There are more important things to work on in the meantime.\n>\n>On 12/19/15, Peter Todd via bitcoin-dev\n><bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> At the recent Scaling Bitcoin conference in Hong Kong we had a\n>chatham\n>> house rules workshop session attending by representitives of a super\n>> majority of the Bitcoin hashing power.\n>>\n>> One of the issues raised by the pools present was block withholding\n>> attacks, which they said are a real issue for them. In particular,\n>pools\n>> are receiving legitimate threats by bad actors threatening to use\n>block\n>> withholding attacks against them. Pools offering their services to\n>the\n>> general public without anti-privacy Know-Your-Customer have little\n>> defense against such attacks, which in turn is a threat to the\n>> decentralization of hashing power: without pools only fairly large\n>> hashing power installations are profitable as variance is a very real\n>> business expense. P2Pool is often brought up as a replacement for\n>pools,\n>> but it itself is still relatively vulnerable to block withholding,\n>and\n>> in any case has many other vulnerabilities and technical issues that\n>has\n>> prevented widespread adoption of P2Pool.\n>>\n>> Fixing block withholding is relatively simple, but (so far) requires\n>a\n>> SPV-visible hardfork. (Luke-Jr's two-stage target mechanism) We\n>should\n>> do this hard-fork in conjunction with any blocksize increase, which\n>will\n>> have the desirable side effect of clearly show consent by the entire\n>> ecosystem, SPV clients included.\n>>\n>>\n>> Note that Ittay Eyal and Emin Gun Sirer have argued(1) that block\n>> witholding attacks are a good thing, as in their model they can be\n>used\n>> by small pools against larger pools, disincentivising large pools.\n>> However this argument is academic and not applicable to the real\n>world,\n>> as a much simpler defense against block withholding attacks is to use\n>> anti-privacy KYC and the legal system combined with the variety of\n>> withholding detection mechanisms only practical for large pools.\n>> Equally, large hashing power installations - a dangerous thing for\n>> decentralization - have no block withholding attack vulnerabilities.\n>>\n>> 1) http://hackingdistributed.com/2014/12/03/the-miners-dilemma/\n>>\n>> --\n>> 'peter'[:-1]@petertodd.org\n>> 00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d\n>>\n>_______________________________________________\n>bitcoin-dev mailing list\n>bitcoin-dev at lists.linuxfoundation.org\n>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Chris Priest",
                "date": "2015-12-20T03:43:59",
                "message_text_only": "Then shouldn't this be something the pool deals with, not the bitcoin protocol?\n\nOn 12/19/15, Matt Corallo <lf-lists at mattcorallo.com> wrote:\n> Peter was referring to pool-block-withholding, not selfish mining.\n>\n> On December 19, 2015 7:34:26 PM PST, Chris Priest via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>Block witholding attacks are only possible if you have a majority of\n>>hashpower. If you only have 20% hashpower, you can't do this attack.\n>>Currently, this attack is only a theoretical attack, as the ones with\n>>all the hashpower today are not engaging in this behavior. Even if\n>>someone who had a lot of hashpower decided to pull off this attack,\n>>they wouldn't be able to disrupt much. Once that time comes, then I\n>>think this problem should be solved, until then it should be a low\n>>priority. There are more important things to work on in the meantime.\n>>\n>>On 12/19/15, Peter Todd via bitcoin-dev\n>><bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>> At the recent Scaling Bitcoin conference in Hong Kong we had a\n>>chatham\n>>> house rules workshop session attending by representitives of a super\n>>> majority of the Bitcoin hashing power.\n>>>\n>>> One of the issues raised by the pools present was block withholding\n>>> attacks, which they said are a real issue for them. In particular,\n>>pools\n>>> are receiving legitimate threats by bad actors threatening to use\n>>block\n>>> withholding attacks against them. Pools offering their services to\n>>the\n>>> general public without anti-privacy Know-Your-Customer have little\n>>> defense against such attacks, which in turn is a threat to the\n>>> decentralization of hashing power: without pools only fairly large\n>>> hashing power installations are profitable as variance is a very real\n>>> business expense. P2Pool is often brought up as a replacement for\n>>pools,\n>>> but it itself is still relatively vulnerable to block withholding,\n>>and\n>>> in any case has many other vulnerabilities and technical issues that\n>>has\n>>> prevented widespread adoption of P2Pool.\n>>>\n>>> Fixing block withholding is relatively simple, but (so far) requires\n>>a\n>>> SPV-visible hardfork. (Luke-Jr's two-stage target mechanism) We\n>>should\n>>> do this hard-fork in conjunction with any blocksize increase, which\n>>will\n>>> have the desirable side effect of clearly show consent by the entire\n>>> ecosystem, SPV clients included.\n>>>\n>>>\n>>> Note that Ittay Eyal and Emin Gun Sirer have argued(1) that block\n>>> witholding attacks are a good thing, as in their model they can be\n>>used\n>>> by small pools against larger pools, disincentivising large pools.\n>>> However this argument is academic and not applicable to the real\n>>world,\n>>> as a much simpler defense against block withholding attacks is to use\n>>> anti-privacy KYC and the legal system combined with the variety of\n>>> withholding detection mechanisms only practical for large pools.\n>>> Equally, large hashing power installations - a dangerous thing for\n>>> decentralization - have no block withholding attack vulnerabilities.\n>>>\n>>> 1) http://hackingdistributed.com/2014/12/03/the-miners-dilemma/\n>>>\n>>> --\n>>> 'peter'[:-1]@petertodd.org\n>>> 00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d\n>>>\n>>_______________________________________________\n>>bitcoin-dev mailing list\n>>bitcoin-dev at lists.linuxfoundation.org\n>>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-12-20T04:44:51",
                "message_text_only": "On Sat, Dec 19, 2015 at 07:43:59PM -0800, Chris Priest via bitcoin-dev wrote:\n> Then shouldn't this be something the pool deals with, not the bitcoin protocol?\n\nThere is no known way for pools - especially ones that allow anonymous\nhashers - to effectively prevent block withholding attacks without\nchanging the Bitcoin protocol.\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151219/f7fbf6c9/attachment.sig>"
            },
            {
                "author": "Multipool Admin",
                "date": "2015-12-26T08:12:13",
                "message_text_only": "Any attempt to 'fix' this problem, would most likely require changes to all\nmining software, why not just make mining more decentralized in general?\n\nFor example, allow anyone to submit proofs of work to Bitcoind that are\nsome fraction of the network difficulty and receive payment for them if\nthey're valid.  This would also encourage the proliferation of full nodes\nsince anyone could solo mine again.  Then, the next coinbase transaction\ncould be split among, say, the top 100 proofs of work.\n\nEligius already does their miner payouts like this.\n\nIf you want to fix an issue with mining, fix the selfish mining issue first\nas it's a much larger and more dangerous potential issue.\n\nI don't believe it was ever clearly established whether Eligius suffered a\nblock withholding attack or was just the victim of a miner with (what was,\nat the time) a large amount of faulty hardware, however, from the\nBitcointalk threads at the time I believe it was assumed to be the latter.\n\n--Adam\n\n\nOn Sat, Dec 19, 2015 at 8:44 PM, Peter Todd via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Sat, Dec 19, 2015 at 07:43:59PM -0800, Chris Priest via bitcoin-dev\n> wrote:\n> > Then shouldn't this be something the pool deals with, not the bitcoin\n> protocol?\n>\n> There is no known way for pools - especially ones that allow anonymous\n> hashers - to effectively prevent block withholding attacks without\n> changing the Bitcoin protocol.\n>\n> --\n> 'peter'[:-1]@petertodd.org\n> 00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/d35657ba/attachment.html>"
            },
            {
                "author": "Geir Harald Hansen",
                "date": "2015-12-27T04:10:09",
                "message_text_only": "Last I heard it was believed the miner had made their own mining client\nand that the block withholding was a bug, not an intended feature.\n\nOn 26.12.2015 09:12, Multipool Admin via bitcoin-dev wrote:\n> Any attempt to 'fix' this problem, would most likely require changes to\n> all mining software, why not just make mining more decentralized in general?\n> \n> For example, allow anyone to submit proofs of work to Bitcoind that are\n> some fraction of the network difficulty and receive payment for them if\n> they're valid.  This would also encourage the proliferation of full\n> nodes since anyone could solo mine again.  Then, the next coinbase\n> transaction could be split among, say, the top 100 proofs of work.\n> \n> Eligius already does their miner payouts like this.\n> \n> If you want to fix an issue with mining, fix the selfish mining issue\n> first as it's a much larger and more dangerous potential issue.\n> \n> I don't believe it was ever clearly established whether Eligius suffered\n> a block withholding attack or was just the victim of a miner with (what\n> was, at the time) a large amount of faulty hardware, however, from the\n> Bitcointalk threads at the time I believe it was assumed to be the latter.\n> \n> --Adam\n> \n> \n> On Sat, Dec 19, 2015 at 8:44 PM, Peter Todd via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org\n> <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n> \n>     On Sat, Dec 19, 2015 at 07:43:59PM -0800, Chris Priest via\n>     bitcoin-dev wrote:\n>     > Then shouldn't this be something the pool deals with, not the bitcoin protocol?\n> \n>     There is no known way for pools - especially ones that allow anonymous\n>     hashers - to effectively prevent block withholding attacks without\n>     changing the Bitcoin protocol.\n> \n>     --\n>     'peter'[:-1]@petertodd.org <http://petertodd.org>\n>     00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d\n> \n>     _______________________________________________\n>     bitcoin-dev mailing list\n>     bitcoin-dev at lists.linuxfoundation.org\n>     <mailto:bitcoin-dev at lists.linuxfoundation.org>\n>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> \n> \n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-12-28T19:12:28",
                "message_text_only": "On Sat, Dec 26, 2015 at 12:12:13AM -0800, Multipool Admin wrote:\n> Any attempt to 'fix' this problem, would most likely require changes to all\n> mining software, why not just make mining more decentralized in general?\n> \n> For example, allow anyone to submit proofs of work to Bitcoind that are\n> some fraction of the network difficulty and receive payment for them if\n> they're valid.  This would also encourage the proliferation of full nodes\n> since anyone could solo mine again.  Then, the next coinbase transaction\n> could be split among, say, the top 100 proofs of work.\n\nThat's certainly be a good place to be, but the design of Bitcoin\ncurrently makes achieving that goal fundementally difficult. \n\n> Eligius already does their miner payouts like this.\n> \n> If you want to fix an issue with mining, fix the selfish mining issue first\n> as it's a much larger and more dangerous potential issue.\n\nDo you specifically mean selfish mining as defined in Emin G\u00fcn\nSirer/Ittay Eyal's paper? Keep in mind that attack is only a significant\nissue in a scenario - one malicious miner with >30% hashing power -\nwhere you're already very close to the margins anyway; the difference\nbetween a 50% attack threshold and a 30% attack threshold isn't very\nsignificant.\n\nFar more concerning is network propagation effects between large and\nsmall miners. For that class of issues, if you are in an environemnt\nwhere selfish mining is possible - a fairly flat, easily DoS/sybil\nattacked network topology - the profitability difference between small\nand large miners even *without* attacks going on is a hugely worrying\nproblem. OTOH, if you're blocksize is small enough that propagation time\nis negligable to profitability, then selfish mining attacks with <30%\nhashing power aren't much of a concern - they'll be naturally defeated\nby anti-DoS/anti-sybil measures.\n\n> I don't believe it was ever clearly established whether Eligius suffered a\n> block withholding attack or was just the victim of a miner with (what was,\n> at the time) a large amount of faulty hardware, however, from the\n> Bitcointalk threads at the time I believe it was assumed to be the latter.\n\nI think the latter was assumed as well, although ruling out of the\nformer is impossible.\n\nNote though that Eligius is *not* the only pool to have had problems\nwith block withholding, though AFAIK Eligius is the only one who has\ngone on record so far. (as I said in my original post, I'm relaying\ninformation given to me under condition of confidentiality)\n\n-- \n'peter'[:-1]@petertodd.org\n000000000000000004a36565fb282c4bd06dda61329fda2465b0bfeaf7241dab\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151228/1e23dd8a/attachment.sig>"
            },
            {
                "author": "Emin G\u00fcn Sirer",
                "date": "2015-12-28T19:30:14",
                "message_text_only": "On Mon, Dec 28, 2015 at 2:12 PM, Peter Todd via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Do you specifically mean selfish mining as defined in Emin G\u00fcn\n> Sirer/Ittay Eyal's paper? Keep in mind that attack is only a significant\n> issue in a scenario - one malicious miner with >30% hashing power -\n> where you're already very close to the margins anyway; the difference\n> between a 50% attack threshold and a 30% attack threshold isn't very\n> significant.\n>\n\nThis is not quite right: we know that selfish mining is a guaranteed win\nat 34%. We do not know when exactly it begins to pay off. The more\nconsolidated and centralized the other mining pools, the less of a threat\nit is below 34%; the more decentralized, the more likely it is to pay off\nat lower thresholds.\n\nFar more concerning is network propagation effects between large and\n> small miners.\n\n\nOn a related note, the Bitcoin-NG paper took a big step towards moving\nthese kinds of concerns out of the realm of gut-feelings and wavy hands\ninto science. In particular, it introduced metrics for fairness (i.e.\ndifferential\nrate in orphans experienced by small and large miners), hash power\nefficiency, as well as consensus delay.\n\n\n> For that class of issues, if you are in an environemnt\n> where selfish mining is possible - a fairly flat, easily DoS/sybil\n> attacked network topology - the profitability difference between small\n> and large miners even *without* attacks going on is a hugely worrying\n> problem.\n\n\nIndeed, there is a slight, quantifiable benefit to larger pools. Which is\nwhy\nwe need to be diligent about not letting pools get too big.\n\n\n> Note though that Eligius is *not* the only pool to have had problems\n>\nwith block withholding, though AFAIK Eligius is the only one who has\n> gone on record so far. (as I said in my original post, I'm relaying\n> information given to me under condition of confidentiality)\n>\n\nI can see why they don't want to go public with this: it means that they\nare less profitable than other pools.\n\nIt still looks to me like Ittay's discovery is doing exactly the right\nthing:\nthis pool will need to be more careful when signing up new people,\ncurbing its otherwise steady march towards the 51% boundary.\n\n- egs\n\n\n- egs\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151228/2e490a2b/attachment.html>"
            },
            {
                "author": "Multipool Admin",
                "date": "2015-12-28T19:35:34",
                "message_text_only": "On Mon, Dec 28, 2015 at 11:30 AM, Emin G\u00fcn Sirer <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n>\n> On Mon, Dec 28, 2015 at 2:12 PM, Peter Todd via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Do you specifically mean selfish mining as defined in Emin G\u00fcn\n>> Sirer/Ittay Eyal's paper? Keep in mind that attack is only a significant\n>> issue in a scenario - one malicious miner with >30% hashing power -\n>> where you're already very close to the margins anyway; the difference\n>> between a 50% attack threshold and a 30% attack threshold isn't very\n>> significant.\n>>\n>\n> This is not quite right: we know that selfish mining is a guaranteed win\n> at 34%. We do not know when exactly it begins to pay off. The more\n> consolidated and centralized the other mining pools, the less of a threat\n> it is below 34%; the more decentralized, the more likely it is to pay off\n> at lower thresholds.\n>\n\nExactly.\n\n\n> Far more concerning is network propagation effects between large and\n>> small miners.\n>\n>\n> On a related note, the Bitcoin-NG paper took a big step towards moving\n> these kinds of concerns out of the realm of gut-feelings and wavy hands\n> into science. In particular, it introduced metrics for fairness (i.e.\n> differential\n> rate in orphans experienced by small and large miners), hash power\n> efficiency, as well as consensus delay.\n>\n>\n>> For that class of issues, if you are in an environemnt\n>> where selfish mining is possible - a fairly flat, easily DoS/sybil\n>> attacked network topology - the profitability difference between small\n>> and large miners even *without* attacks going on is a hugely worrying\n>> problem.\n>\n>\n> Indeed, there is a slight, quantifiable benefit to larger pools. Which is\n> why\n> we need to be diligent about not letting pools get too big.\n>\n>\n>> Note though that Eligius is *not* the only pool to have had problems\n>>\n> with block withholding, though AFAIK Eligius is the only one who has\n>> gone on record so far. (as I said in my original post, I'm relaying\n>> information given to me under condition of confidentiality)\n>>\n>\n> I can see why they don't want to go public with this: it means that they\n> are less profitable than other pools.\n>\n\nThis I disagree with -- if they know that they have been attacked, then\nthere is every reason to come forward with this information.\n\nFirst of all, it offers an explanation for poor profits (this is better\nthan unexplained poor profits).\n\nSecond of all, if one pool can be attacked then any pool can be attacked --\nthis is not a reason not to mine on a particular pool.  If anything, it's a\nreason to diversify hashrate among many pools.\n\n--Adam\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151228/8c25dd48/attachment-0001.html>"
            },
            {
                "author": "Multipool Admin",
                "date": "2015-12-28T19:33:11",
                "message_text_only": "On Mon, Dec 28, 2015 at 11:12 AM, Peter Todd <pete at petertodd.org> wrote:\n\n> On Sat, Dec 26, 2015 at 12:12:13AM -0800, Multipool Admin wrote:\n> > Any attempt to 'fix' this problem, would most likely require changes to\n> all\n> > mining software, why not just make mining more decentralized in general?\n> >\n> > For example, allow anyone to submit proofs of work to Bitcoind that are\n> > some fraction of the network difficulty and receive payment for them if\n> > they're valid.  This would also encourage the proliferation of full nodes\n> > since anyone could solo mine again.  Then, the next coinbase transaction\n> > could be split among, say, the top 100 proofs of work.\n>\n> That's certainly be a good place to be, but the design of Bitcoin\n> currently makes achieving that goal fundementally difficult.\n>\n\nAgreed, however I don't think it would be impossible or even really that\ndifficult, and would be a great way to increase decentralization while\nsimultaneously fixing other issues with mining.\n\nProofs of work would be valid if they're built on top of the current block\nhash, and we could require (difficulty/N) proofs of work that are >=\n(difficulty/N) to assemble a valid block.  Same as mining shares work.\n\nThe block assembler who finds the final diff/N 'share' could get a small\nbonus as an incentive to complete the block as quickly as possible.  Or\nalternatively, a checksum could be computed of all the current diff/N\nshares in the mempool and that way only the final share would need to be\nbroadcasted to the entire network, and clients with the correct checksum\ncould assemble the block themselves without having to download the entire\nblock.  This would drastically decrease data usage on the network.\n\n> Eligius already does their miner payouts like this.\n> >\n> > If you want to fix an issue with mining, fix the selfish mining issue\n> first\n> > as it's a much larger and more dangerous potential issue.\n>\n> Do you specifically mean selfish mining as defined in Emin G\u00fcn\n> Sirer/Ittay Eyal's paper? Keep in mind that attack is only a significant\n> issue in a scenario - one malicious miner with >30% hashing power -\n> where you're already very close to the margins anyway; the difference\n> between a 50% attack threshold and a 30% attack threshold isn't very\n> significant.\n>\n\nYes, that's what I'm talking about.\n\n\n> Far more concerning is network propagation effects between large and\n> small miners. For that class of issues, if you are in an environemnt\n> where selfish mining is possible - a fairly flat, easily DoS/sybil\n> attacked network topology - the profitability difference between small\n> and large miners even *without* attacks going on is a hugely worrying\n> problem. OTOH, if you're blocksize is small enough that propagation time\n> is negligable to profitability, then selfish mining attacks with <30%\n> hashing power aren't much of a concern - they'll be naturally defeated\n> by anti-DoS/anti-sybil measures.\n>\n\nThe possibility that a previously 'good' actor with 30% of the hashpower\ngoing 'rogue' becomes more and more of a concern as the block subsidy\ndecreases.\n\n\n> > I don't believe it was ever clearly established whether Eligius suffered\n> a\n> > block withholding attack or was just the victim of a miner with (what\n> was,\n> > at the time) a large amount of faulty hardware, however, from the\n> > Bitcointalk threads at the time I believe it was assumed to be the\n> latter.\n>\n> I think the latter was assumed as well, although ruling out of the\n> former is impossible.\n>\n> Note though that Eligius is *not* the only pool to have had problems\n> with block withholding, though AFAIK Eligius is the only one who has\n> gone on record so far. (as I said in my original post, I'm relaying\n> information given to me under condition of confidentiality)\n>\n\nWhat is the incentive not to go on record about this?\n\n--Adam\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151228/46d76db5/attachment.html>"
            },
            {
                "author": "Ivan Brightly",
                "date": "2015-12-28T20:26:43",
                "message_text_only": ">\n> On Mon, Dec 28, 2015 at 2:12 PM, Peter Todd via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Far more concerning is network propagation effects between large and\n> small miners. For that class of issues, if you are in an environemnt\n> where selfish mining is possible - a fairly flat, easily DoS/sybil\n> attacked network topology - the profitability difference between small\n> and large miners even *without* attacks going on is a hugely worrying\n> problem. OTOH, if you're blocksize is small enough that propagation time\n> is negligable to profitability, then selfish mining attacks with <30%\n> hashing power aren't much of a concern - they'll be naturally defeated\n> by anti-DoS/anti-sybil measures.\n>\n\nLet's agree that one factor in mining profitability is bandwidth/network\nreliability/stability. Why focus on that vs electricity contracts or\nvertically integrated chip manufacturers? Surely, sufficient network\nbandwidth is a more broadly available commodity than <$0.02/kwh\nelectricity, for example. I'm not sure that your stranded hydroelectric\nminer is any more desirable than thousands of dorm room miners with access\nto 10gbit university connections and free electricity.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151228/abe958d6/attachment.html>"
            },
            {
                "author": "Dave Scotese",
                "date": "2015-12-29T18:59:58",
                "message_text_only": "There have been no decent objections to altering the block-selection\nmechanism (when two block solutions appear at nearly the same time) as\ndescribed at\n\nhttp://bitcoin.stackexchange.com/questions/39226\n\nKey components are:\n\n   - Compute BitcoinDaysDestroyed using only transactions that have been in\n   your mempool for some time as oBTCDD (\"old BTCDD\").\n   - Use \"nearly the same time\" to mean separated in time by your guess of\n   the average duration of block propagation times.\n   - When two block solutions come in at nearly the same time, build on the\n   one that has the most oBTCDD, rather than the one that came in first.\n\nThe goal of this change is to reduce the profitability of withholding block\nsolutions by severely reducing the chances that a block solved a while ago\ncan orphan one solved recently.  \"Came in first\" seems more easily gamed\nthan \"most oBTCDD\".  As I wrote there, \"*old coins* is always a dwindling\nresource and *global nodes willing to help cheat* is probably a growing\none.\"\n\nI will write a BIP if anyone agrees it's a good idea.\n\nOn Mon, Dec 28, 2015 at 12:26 PM, Ivan Brightly via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Mon, Dec 28, 2015 at 2:12 PM, Peter Todd via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> Far more concerning is network propagation effects between large and\n>> small miners. For that class of issues, if you are in an environemnt\n>> where selfish mining is possible - a fairly flat, easily DoS/sybil\n>> attacked network topology - the profitability difference between small\n>> and large miners even *without* attacks going on is a hugely worrying\n>> problem. OTOH, if you're blocksize is small enough that propagation time\n>> is negligable to profitability, then selfish mining attacks with <30%\n>> hashing power aren't much of a concern - they'll be naturally defeated\n>> by anti-DoS/anti-sybil measures.\n>>\n>\n> Let's agree that one factor in mining profitability is bandwidth/network\n> reliability/stability. Why focus on that vs electricity contracts or\n> vertically integrated chip manufacturers? Surely, sufficient network\n> bandwidth is a more broadly available commodity than <$0.02/kwh\n> electricity, for example. I'm not sure that your stranded hydroelectric\n> miner is any more desirable than thousands of dorm room miners with access\n> to 10gbit university connections and free electricity.\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n\n\n-- \nI like to provide some work at no charge to prove my value. Do you need a\ntechie?\nI own Litmocracy <http://www.litmocracy.com> and Meme Racing\n<http://www.memeracing.net> (in alpha).\nI'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which\nnow accepts Bitcoin.\nI also code for The Dollar Vigilante <http://dollarvigilante.com/>.\n\"He ought to find it more profitable to play by the rules\" - Satoshi\nNakamoto\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151229/654898a2/attachment.html>"
            },
            {
                "author": "Jonathan Toomim",
                "date": "2015-12-29T19:08:16",
                "message_text_only": "Ultimately, a self-interested miner will chose to build on the block that leaves the most transaction fees up for grabs. (This usually means the smallest block.) It's an interesting question whether the default behavior for Core should be the rational behavior (build on the \"smallest\" block in terms of fees) or some other supposedly altruistic behavior (most BTCDD). This also applies to the decision of the \"same time\" threshold -- a selfish miner will not care if the blocks arrived at about the same time or not.\n\nI currently do not have a strong opinion on what that behavior should be, although if the blocksize limit were increased substantially, I may prefer the selfish behavior because it ends up also being fail-safe (punishes selfish mining using large blocks or fee-stealing attempts).\n\n\nOn Dec 29, 2015, at 10:59 AM, Dave Scotese via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> There have been no decent objections to altering the block-selection mechanism (when two block solutions appear at nearly the same time) as described at\n> \n> http://bitcoin.stackexchange.com/questions/39226\n> \n> Key components are:\n> Compute BitcoinDaysDestroyed using only transactions that have been in your mempool for some time as oBTCDD (\"old BTCDD\").\n> Use \"nearly the same time\" to mean separated in time by your guess of the average duration of block propagation times.\n> When two block solutions come in at nearly the same time, build on the one that has the most oBTCDD, rather than the one that came in first.\n> The goal of this change is to reduce the profitability of withholding block solutions by severely reducing the chances that a block solved a while ago can orphan one solved recently.  \"Came in first\" seems more easily gamed than \"most oBTCDD\".  As I wrote there, \"old coins is always a dwindling resource and global nodes willing to help cheat is probably a growing one.\"\n> \n> I will write a BIP if anyone agrees it's a good idea.\n> \n> \n> On Mon, Dec 28, 2015 at 12:26 PM, Ivan Brightly via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> On Mon, Dec 28, 2015 at 2:12 PM, Peter Todd via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Far more concerning is network propagation effects between large and\n> small miners. For that class of issues, if you are in an environemnt\n> where selfish mining is possible - a fairly flat, easily DoS/sybil\n> attacked network topology - the profitability difference between small\n> and large miners even *without* attacks going on is a hugely worrying\n> problem. OTOH, if you're blocksize is small enough that propagation time\n> is negligable to profitability, then selfish mining attacks with <30%\n> hashing power aren't much of a concern - they'll be naturally defeated\n> by anti-DoS/anti-sybil measures.\n> \n> Let's agree that one factor in mining profitability is bandwidth/network reliability/stability. Why focus on that vs electricity contracts or vertically integrated chip manufacturers? Surely, sufficient network bandwidth is a more broadly available commodity than <$0.02/kwh electricity, for example. I'm not sure that your stranded hydroelectric miner is any more desirable than thousands of dorm room miners with access to 10gbit university connections and free electricity.\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> \n> \n> \n> --\n> I like to provide some work at no charge to prove my value. Do you need a techie?\n> I own Litmocracy and Meme Racing (in alpha).\n> I'm the webmaster for The Voluntaryist which now accepts Bitcoin.\n> I also code for The Dollar Vigilante.\n> \"He ought to find it more profitable to play by the rules\" - Satoshi Nakamoto\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151229/a62c256a/attachment-0001.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151229/a62c256a/attachment-0001.sig>"
            },
            {
                "author": "Allen Piscitello",
                "date": "2015-12-29T19:25:17",
                "message_text_only": "How could this possibly be enforced?\n\nOn Tue, Dec 29, 2015 at 12:59 PM, Dave Scotese via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> There have been no decent objections to altering the block-selection\n> mechanism (when two block solutions appear at nearly the same time) as\n> described at\n>\n> http://bitcoin.stackexchange.com/questions/39226\n>\n> Key components are:\n>\n>    - Compute BitcoinDaysDestroyed using only transactions that have been\n>    in your mempool for some time as oBTCDD (\"old BTCDD\").\n>    - Use \"nearly the same time\" to mean separated in time by your guess\n>    of the average duration of block propagation times.\n>    - When two block solutions come in at nearly the same time, build on\n>    the one that has the most oBTCDD, rather than the one that came in first.\n>\n> The goal of this change is to reduce the profitability of withholding\n> block solutions by severely reducing the chances that a block solved a\n> while ago can orphan one solved recently.  \"Came in first\" seems more\n> easily gamed than \"most oBTCDD\".  As I wrote there, \"*old coins* is\n> always a dwindling resource and *global nodes willing to help cheat* is\n> probably a growing one.\"\n>\n> I will write a BIP if anyone agrees it's a good idea.\n>\n> On Mon, Dec 28, 2015 at 12:26 PM, Ivan Brightly via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> On Mon, Dec 28, 2015 at 2:12 PM, Peter Todd via bitcoin-dev <\n>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>> Far more concerning is network propagation effects between large and\n>>> small miners. For that class of issues, if you are in an environemnt\n>>> where selfish mining is possible - a fairly flat, easily DoS/sybil\n>>> attacked network topology - the profitability difference between small\n>>> and large miners even *without* attacks going on is a hugely worrying\n>>> problem. OTOH, if you're blocksize is small enough that propagation time\n>>> is negligable to profitability, then selfish mining attacks with <30%\n>>> hashing power aren't much of a concern - they'll be naturally defeated\n>>> by anti-DoS/anti-sybil measures.\n>>>\n>>\n>> Let's agree that one factor in mining profitability is bandwidth/network\n>> reliability/stability. Why focus on that vs electricity contracts or\n>> vertically integrated chip manufacturers? Surely, sufficient network\n>> bandwidth is a more broadly available commodity than <$0.02/kwh\n>> electricity, for example. I'm not sure that your stranded hydroelectric\n>> miner is any more desirable than thousands of dorm room miners with access\n>> to 10gbit university connections and free electricity.\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n>\n> --\n> I like to provide some work at no charge to prove my value. Do you need a\n> techie?\n> I own Litmocracy <http://www.litmocracy.com> and Meme Racing\n> <http://www.memeracing.net> (in alpha).\n> I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com>\n> which now accepts Bitcoin.\n> I also code for The Dollar Vigilante <http://dollarvigilante.com/>.\n> \"He ought to find it more profitable to play by the rules\" - Satoshi\n> Nakamoto\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151229/01725358/attachment.html>"
            },
            {
                "author": "jl2012",
                "date": "2015-12-20T03:40:06",
                "message_text_only": "Chris Priest via bitcoin-dev \u65bc 2015-12-19 22:34 \u5beb\u5230:\n> Block witholding attacks are only possible if you have a majority of\n> hashpower. If you only have 20% hashpower, you can't do this attack.\n> Currently, this attack is only a theoretical attack, as the ones with\n> all the hashpower today are not engaging in this behavior. Even if\n> someone who had a lot of hashpower decided to pull off this attack,\n> they wouldn't be able to disrupt much. Once that time comes, then I\n> think this problem should be solved, until then it should be a low\n> priority. There are more important things to work on in the meantime.\n> \n\nThis is not true. For a pool with 5% total hash rate, an attacker only\nneeds 0.5% of hash rate to sabotage 10% of their income. It's already\nenough to kill the pool"
            },
            {
                "author": "Chris Priest",
                "date": "2015-12-20T03:47:12",
                "message_text_only": "On 12/19/15, jl2012 <jl2012 at xbt.hk> wrote:\n> Chris Priest via bitcoin-dev \u65bc 2015-12-19 22:34 \u5beb\u5230:\n>> Block witholding attacks are only possible if you have a majority of\n>> hashpower. If you only have 20% hashpower, you can't do this attack.\n>> Currently, this attack is only a theoretical attack, as the ones with\n>> all the hashpower today are not engaging in this behavior. Even if\n>> someone who had a lot of hashpower decided to pull off this attack,\n>> they wouldn't be able to disrupt much. Once that time comes, then I\n>> think this problem should be solved, until then it should be a low\n>> priority. There are more important things to work on in the meantime.\n>>\n>\n> This is not true. For a pool with 5% total hash rate, an attacker only\n> needs 0.5% of hash rate to sabotage 10% of their income. It's already\n> enough to kill the pool\n>\n>\n\nThis begs the question: If this is such a devastating attack, then why\nhasn't this attack brought down every pool in existence? As far as I'm\naware, there are many pools in operation despite this possibility."
            },
            {
                "author": "jl2012",
                "date": "2015-12-20T04:24:52",
                "message_text_only": "Chris Priest \u65bc 2015-12-19 22:47 \u5beb\u5230:\n> On 12/19/15, jl2012 <jl2012 at xbt.hk> wrote:\n>> Chris Priest via bitcoin-dev \u65bc 2015-12-19 22:34 \u5beb\u5230:\n>>> Block witholding attacks are only possible if you have a majority of\n>>> hashpower. If you only have 20% hashpower, you can't do this attack.\n>>> Currently, this attack is only a theoretical attack, as the ones with\n>>> all the hashpower today are not engaging in this behavior. Even if\n>>> someone who had a lot of hashpower decided to pull off this attack,\n>>> they wouldn't be able to disrupt much. Once that time comes, then I\n>>> think this problem should be solved, until then it should be a low\n>>> priority. There are more important things to work on in the meantime.\n>>> \n>> \n>> This is not true. For a pool with 5% total hash rate, an attacker only\n>> needs 0.5% of hash rate to sabotage 10% of their income. It's already\n>> enough to kill the pool\n>> \n>> \n> \n> This begs the question: If this is such a devastating attack, then why\n> hasn't this attack brought down every pool in existence? As far as I'm\n> aware, there are many pools in operation despite this possibility.\n\nIt did happen: \nhttps://www.reddit.com/r/Bitcoin/comments/28242v/eligius_falls_victim_to_blocksolution_withholding/\n\nThe worst thing is that the proof for such attack is probabilistic, not\ndeterministic.\n\nA smarter attacker may even pretend to be many small miners, make it\neven more difficult or impossible to prove who are attacking.\n\n\n> Then shouldn't this be something the pool deals with, not the bitcoin \n> protocol?\n\nThe only solution is to ask for KYC registration, unless one could \npropose\na cryptographic solution that does not require a consensus fork."
            },
            {
                "author": "Emin G\u00fcn Sirer",
                "date": "2015-12-20T05:12:49",
                "message_text_only": "There's quite a bit of confusion in this thread.\n\nPeter is referring to block withholding attacks. Ittay Eyal (as sole\nauthor -- I was not involved in this paper [1]) was the first\nto analyze these attacks and to discover a fascinating, paradoxical\nresult. An attacker pool (A) can take a certain portion of its hashpower,\nuse it to mine on behalf of victim pool (B), furnish partial proofs of work\nto B, but discard any full blocks it discovers. If A picks the amount of\nattacking hashpower judiciously, it can make more money using this\nattack, than it would if it were to use 100% of its hashpower for its own\nmining. This last sentence should sound non-sensical to most of you,\nat least, it did to me. Ittay did the math, and his paper can tell you\nexactly how much of your hashpower you need to peel off and use\nto attack another open pool, and you will come out ahead.\n\nChris Priest is confusing these attacks with selfish mining, and further,\nhis characterization of selfish mining is incorrect. Selfish Mining is\nguaranteed to yield profits for any pool over 33% (as a result, Nick\nSzabo has dubbed this the \"34% attack\") and it may pay off even\nbelow that point if the attacker is well-positioned in the network;\nor it may not, depending on the makeup of the rest of the pools\nas well as the network characteristics (the more centralized\nand bigger the other pools are, the less likely it is to pay off). There\nwas a lot of noise in the community when the SM paper came out,\nso there are tons of incorrect response narrative out there. By now,\neveryone who seems to be Bitcoin competent sees SM as a\nconcern, and Ethereum has already adopted our fix. I'd have hoped\nthat a poster to this list would be better informed than to repeat the\nclaim that \"majority will protect Bitcoin\" to refute a paper whose title\nis \"majority is not enough.\"\n\nBack to Ittay's paradoxical discovery:\n\nWe have seen pool-block withholding attacks before; I believe Eligius\ncaught one case. I don't believe that any miners will deploy strong KYC\nmeasures, and even if they did, I don't believe that these measures\nwill be effective, at least, as long as the attacker is somewhat savvy.\nThe problem with these attacks are that statistics favor the attackers.\nIs someone really discarding the blocks they find, or are they just\nunlucky? This is really hard to tell for small miners. Even with KYC,\none could break up one's servers, register them under different\npeople's names, and tunnel them through VPNs.\n\nKeep in mind that when an open pool gets big, like GHash did and\ntwo other pools did before them, the only thing at our disposal used\nto be to yell at people about centralization until they left the big\npools and reformed into smaller groups. Not only was such yelling\nkind of desperate looking, it wasn't incredibly effective, either.\nWe had no protocol mechanisms that put pressure on big pools to\nstop signing up people. Ittay's discovery changed that: pools that\nget to be very big by indiscriminately signing up miners are likely to\nbe infiltrated and their profitability will drop. And Peter's post is\nevidence that this is, indeed, happening as predicted. This is a\ngood outcome, it puts pressure on the big pools to not grow.\n\nPeter, you allude to a specific suggestion from Luke-Jr. Can you\nplease describe what it is?\n\nHope this is useful,\n- egs\n\n[1] https://www.cs.cornell.edu/~ie53/publications/btcPoolsSP15.pdf\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/5a7c575b/attachment-0001.html>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-12-20T11:38:34",
                "message_text_only": "On Sun, Dec 20, 2015 at 5:12 AM, Emin G\u00fcn Sirer <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>  An attacker pool (A) can take a certain portion of its hashpower,\n> use it to mine on behalf of victim pool (B), furnish partial proofs of work\n> to B, but discard any full blocks it discovers.\n>\n\nI wonder if part of the problem here is that there is no pool identity\nlinked to mining pools.\n\nIf the mining protocols were altered so that miners had to indicate their\nidentity, then a pool couldn't forward hashing power to their victim.\n\nIf the various mining protocols were updated, they could allow checking\nthat the work has the domain name of the pool included.  Pools would have\nto include their domain name in the block header.\n\nA pool which provides this service is publicly saying that they will not\nuse the block withholding attack.  Any two pools which are doing it cannot\nattack each other (since they have different domain names).  This creates\nan incentive for pools to start supporting the feature.\n\nOwners of hashing power also have an incentive to operate with pools which\noffer this identity.  It means that they can ensure that they get a payout\nfrom any blocks found.\n\nHosted mining is weaker, but even then, it is possible for mining hosts to\nprovide proof that they performed mining.  This proof would include the\nidentity of the mining pool.  Even if the pool was run by the host, it\nwould still need to have the name embedded.\n\nMining hosts might be able to figure out which of their customers actually\ncheck the identity info, and then they could redirect the mining power of\nthose who generally don't check.  If customers randomly ask for all of the\nhashing power, right back to when they joined, then this becomes expensive.\n\nMining power directly owned by the pool is also immune to this effect.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/feb0d395/attachment.html>"
            },
            {
                "author": "Natanael",
                "date": "2015-12-20T12:42:10",
                "message_text_only": "Den 20 dec 2015 12:38 skrev \"Tier Nolan via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org>:\n>\n> On Sun, Dec 20, 2015 at 5:12 AM, Emin G\u00fcn Sirer <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>  An attacker pool (A) can take a certain portion of its hashpower,\n>> use it to mine on behalf of victim pool (B), furnish partial proofs of\nwork\n>> to B, but discard any full blocks it discovers.\n>\n> I wonder if part of the problem here is that there is no pool identity\nlinked to mining pools.\n>\n> If the mining protocols were altered so that miners had to indicate their\nidentity, then a pool couldn't forward hashing power to their victim.\n\nOur approaches can be combined.\n\nEach pool (or solo miner) has a public key included in their blocks that\nidentifies them to their miners (solo miners can use their own unique\nrandom keys every time). This public key may be registered with DNSSEC+DANE\nand the pool could point to their domain in the block template as an\nidentifier.\n\nFor each block the pool generates a nonce, and for each of every miner's\nworkers it double-hashes that nonce with their own public key and that\nminer's worker ID and the previous block hash (to ensure no accidental\noverlapping work is done).\n\nThe double-hash is a commitment hash, the first hash is the committed value\nto be used by the pool as described below. Publishing the nonce reveals how\nthe hashes were derived to their miners.\n\nEach miner puts this commitment hash in their blocks, and also the public\nkey of the pool separately as mentioned above.\n\nHere's where it differs from standard mining: both the candidate block PoW\nhash and the pool's commitment value above determines block validity\ntogether.\n\nIf total difficulty is X and the ratio for full blocks to candidate blocks\nshared with the pool is Y, then the candidate block PoW now has to meet X/Y\nwhile hashing the candidate block PoW + the pool's commitment hash must\nmeet Y, which together makes for X/Y*Y and thus the same total difficulty.\n\nSo now miners don't know if their blocks are valid before the pool does, so\nwithholding isn't effective, and the public key identifiers simultaneously\nstops a pool from telling honest but naive miners to attack other pools\nusing whatever other schemes one might come up with.\n\nThe main differences are that there's a public key identifier the miners\nare told about in advance and expect to see in block templates, and that\nthat now the pool has to publish this commitment value together with the\nblock that also contains the commitment hash, and that this is verified\ntogether with the PoW.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/f952cf21/attachment.html>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-12-20T15:30:09",
                "message_text_only": "On Sun, Dec 20, 2015 at 12:42 PM, Natanael <natanael.l at gmail.com> wrote:\n\n> If total difficulty is X and the ratio for full blocks to candidate blocks\n> shared with the pool is Y, then the candidate block PoW now has to meet X/Y\n> while hashing the candidate block PoW + the pool's commitment hash must\n> meet Y, which together makes for X/Y*Y and thus the same total difficulty.\n\n\nThis gives the same total difficulty but miners are throwing away otherwise\nvalid blocks.\n\nThis means that it is technically a soft fork.  All new blocks are valid\naccording to the old rule.\n\nIn practice, it is kind of a hard fork.  If Y is 10, then all upgraded\nminers are throwing away 90% of the blocks that are valid under the old\nrules.\n\n>From the perspective of non-upgraded clients, the upgraded miners operate\nat a 10X disadvantage.\n\nThis means that someone with 15% of the network power has a majority of the\neffective hashing power, since 15% is greater than 8.5% (85% * 0.1).\n\nThe slow roll-out helps mitigate this though.  It gives non-upgraded\nclients time to react.  If there is only a 5% difference initially, then\nthe attacker doesn't get much benefit.\n\nThe main differences are that there's a public key identifier the miners\n> are told about in advance and expect to see in block templates, and that\n> that now the pool has to publish this commitment value together with the\n> block that also contains the commitment hash, and that this is verified\n> together with the PoW.\n\n\nI don't think public keys are strictly required.  Registering them with\nDNSSEC is way over the top.  They can just publish the key on their website\nand then use that for their identity.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/d23c9ae8/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-12-20T13:28:43",
                "message_text_only": "On Sun, Dec 20, 2015 at 12:12:49AM -0500, Emin G\u00fcn Sirer via bitcoin-dev wrote:\n> There's quite a bit of confusion in this thread.\n> \n> Peter is referring to block withholding attacks. Ittay Eyal (as sole\n> author -- I was not involved in this paper [1]) was the first\n\nAh, thanks for the correction.\n\n> to analyze these attacks and to discover a fascinating, paradoxical\n> result. An attacker pool (A) can take a certain portion of its hashpower,\n> use it to mine on behalf of victim pool (B), furnish partial proofs of work\n> to B, but discard any full blocks it discovers. If A picks the amount of\n> attacking hashpower judiciously, it can make more money using this\n> attack, than it would if it were to use 100% of its hashpower for its own\n> mining. This last sentence should sound non-sensical to most of you,\n> at least, it did to me. Ittay did the math, and his paper can tell you\n> exactly how much of your hashpower you need to peel off and use\n> to attack another open pool, and you will come out ahead.\n\nNow to be clear, I'm not saying any of the above isn't true - it's a\nfascinating result. But the hashing/mining ecosystem is significantly\nmore complex than just pools.\n\n> Back to Ittay's paradoxical discovery:\n> \n> We have seen pool-block withholding attacks before; I believe Eligius\n> caught one case. I don't believe that any miners will deploy strong KYC\n> measures, and even if they did, I don't believe that these measures\n> will be effective, at least, as long as the attacker is somewhat savvy.\n> The problem with these attacks are that statistics favor the attackers.\n> Is someone really discarding the blocks they find, or are they just\n> unlucky? This is really hard to tell for small miners. Even with KYC,\n> one could break up one's servers, register them under different\n> people's names, and tunnel them through VPNs.\n\nThere are a number of techniques that can be used to detect block\nwithholding attacks that you are not aware of. These techniques usually\nhave the characteristic that if known they can be avoided, so obviously\nthose who know about them are highly reluctant to reveal what exactly\nthey are. I personally know about some of them and have been asked to\nkeep that information secret, which I will.\n\nIn the context of KYC, this techniques would likely hold up in court,\nwhich means that if this stuff becomes a more serious problem it's\nperfectly viable for large, well-resourced, pools to prevent block\nwithholding attacks, in part by removing anonymity of hashing power.\nThis would not be a positive development for the ecosystem.\n\nSecondly, DRM tech can also easily be used to prevent block withholding\nattacks by attesting to the honest of the hashing power. This is being\ndiscussed in the industry, and again, this isn't a positive development\nfor the ecosystem.\n\n> Keep in mind that when an open pool gets big, like GHash did and\n> two other pools did before them, the only thing at our disposal used\n> to be to yell at people about centralization until they left the big\n> pools and reformed into smaller groups. Not only was such yelling\n> kind of desperate looking, it wasn't incredibly effective, either.\n> We had no protocol mechanisms that put pressure on big pools to\n> stop signing up people. Ittay's discovery changed that: pools that\n> get to be very big by indiscriminately signing up miners are likely to\n> be infiltrated and their profitability will drop. And Peter's post is\n> evidence that this is, indeed, happening as predicted. This is a\n> good outcome, it puts pressure on the big pools to not grow.\n\nGHash.io was not a pure pool - they owned and operated a significant\namount of physical hashing power, and it's not at all clear that their %\nof the network actually went down following that 51% debacle.\n\nCurrently a significant % of the hashing power - possibly a majority -\nis in the form of large hashing installations whose owners individually,\nand definitely in trusting groups, have enough hashing power to solo\nmine. Eyal's results indicate those miners have incentives to attack\npools, and additionally they have the incentive of killing off pools to\nmake it difficult for new competition to get established, yet they\nthemselves are not vulnerable to that attack.\n\nMoving to a state where new hashing power can't be brought online except\nin large quantities is not a positive development for the ecosystem.\n\nThis is also way I described the suggestion that Eyal's results are a\ngood thing as academic - while the idea hypothetically works in a pure\nopen pool vs. open pool scenario, the real world is significantly more\ncomplex than that simple model.\n\n> Peter, you allude to a specific suggestion from Luke-Jr. Can you\n> please describe what it is?\n\nBasically you have the pool pick a secret k for each share, and commit\nto H(k) in the share. Additionally the share commits to a target divider\nD. The PoW validity rule is then changed from H(block header) < T, to be\nH(block header) < T * D && H(H(block header) + k) < max_int / D\n\nBecause the hasher doesn't know what k is, they don't know which shares\nare valid blocks and thus can't selectively discard those shares.\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/ea290ba1/attachment.sig>"
            },
            {
                "author": "Emin G\u00fcn Sirer",
                "date": "2015-12-20T17:00:37",
                "message_text_only": "On Sun, Dec 20, 2015 at 8:28 AM, Peter Todd <pete at petertodd.org> wrote:\n\n> There are a number of techniques that can be used to detect block\n> withholding attacks that you are not aware of. These techniques usually\n> have the characteristic that if known they can be avoided, so obviously\n> those who know about them are highly reluctant to reveal what exactly\n> they are. I personally know about some of them and have been asked to\n> keep that information secret, which I will.\n>\n\nIndeed, there are lots of weak measures that one could employ against\nan uninformed attacker. As I mentioned before, these are unlikely to be\neffective against a savvy attacker, and this is a good thing.\n\n\n> In the context of KYC, this techniques would likely hold up in court,\n> which means that if this stuff becomes a more serious problem it's\n> perfectly viable for large, well-resourced, pools to prevent block\n> withholding attacks, in part by removing anonymity of hashing power.\n> This would not be a positive development for the ecosystem.\n>\n\nKYC has a particular financial-regulation connotation in Bitcoin circles,\nof which I'm sure you're aware, and which you're using as a spectre.\nYou don't mean government-regulated-KYC a la FINCEN and Bitcoin\nexchanges like Coinbase, you are just referring to a pool operator\ndemanding to know that its customer is not coming from its competitors'\ndata centers.\n\nAnd your prediction doesn't seem well-motivated or properly justified.\nThere are tons of conditionals in your prediction, starting with the premise\nthat every single open pool would implement some notion of identity\nchecking. I don't believe that will happen. Instead, we will have the bigger\npools become more suspicious of signing up new hash power, which is a\ngood thing. And we will have small groups of people who have some reason\nfor trusting each other (e.g. they know each other from IRC, conferences,\netc) band together into small pools. These are fantastic outcomes for\ndecentralization.\n\nSecondly, DRM tech can also easily be used to prevent block withholding\n> attacks by attesting to the honest of the hashing power. This is being\n> discussed in the industry, and again, this isn't a positive development\n> for the ecosystem.\n>\n\nDRM is a terrible application. Once again, I see that you're trying to use\nthose\nthree letters as a spectre as well, knowing that most people hate DRM, but\nkeep in mind that DRM is just an application -- it's like pointing to Adobe\nFlash\nto taint all browser plugins.\n\nThe tech behind DRM is called \"attestation,\" and it provides a technical\ncapability not possible by any other means. In essence, attestation can\nensure that\na remote node is indeed running the code that it purports to be running.\nSince\nmost problems in computer security and distributed systems stem from not\nknowing what protocol the attacker is going to follow, attestation is the\nonly\ntechnology we have that lets us step around this limitation.\n\nIt can ensure, for instance,\n  - that a node purporting to be Bitcoin Core (vLatest) is indeed running an\nunadulterated, latest version of Bitcoin Core\n  - that a node claiming that it does not harvest IP addresses from SPV\nclients indeed does not harvest IP addresses.\n  - that a cloud hashing outfit that rented out X terahashes to a user did\nindeed rent out X terahashes to that particular user,\n  - that a miner operating on behalf of some pool P will not misbehave and\ndiscard perfectly good blocks\nand so forth. All of these would be great for the ecosystem. Just getting\nrid\nof the cloudhashing scams would put an end to a lot of heartache.\n\n> Keep in mind that when an open pool gets big, like GHash did and\n> > two other pools did before them, the only thing at our disposal used\n> > to be to yell at people about centralization until they left the big\n> > pools and reformed into smaller groups. Not only was such yelling\n> > kind of desperate looking, it wasn't incredibly effective, either.\n> > We had no protocol mechanisms that put pressure on big pools to\n> > stop signing up people. Ittay's discovery changed that: pools that\n> > get to be very big by indiscriminately signing up miners are likely to\n> > be infiltrated and their profitability will drop. And Peter's post is\n> > evidence that this is, indeed, happening as predicted. This is a\n> > good outcome, it puts pressure on the big pools to not grow.\n>\n> GHash.io was not a pure pool - they owned and operated a significant\n> amount of physical hashing power, and it's not at all clear that their %\n> of the network actually went down following that 51% debacle.\n>\n\nRight, it's not clear at all that yelling at people has much effect. As much\nfun as I had going to that meeting with GHash in London to ask them to\nback down off of the 51% boundary, I am pretty sure that yelling at large\nopen pools will not scale. We needed better mechanisms for keeping pools\nin check.\n\nAnd Miner's Dilemma (MD) attacks are clearly quite effective. This is a\ntime when we should count our blessings, not work actively to render\nthem inoperable.\n\nCurrently a significant % of the hashing power - possibly a majority -\n> is in the form of large hashing installations whose owners individually,\n> and definitely in trusting groups, have enough hashing power to solo\n> mine. Eyal's results indicate those miners have incentives to attack\n> pools, and additionally they have the incentive of killing off pools to\n> make it difficult for new competition to get established, yet they\n> themselves are not vulnerable to that attack.\n>\n\nThere are indeed solo miners out there who can attack the big open\npools. The loss of the biggest open pools would not be a bad outcome.\nPools >25% pose a danger, and the home miner doesn't need a pool\n>25% for protection against variance.\n\n> Peter, you allude to a specific suggestion from Luke-Jr. Can you\n> > please describe what it is?\n>\n> Basically you have the pool pick a secret k for each share, and commit\n> to H(k) in the share. Additionally the share commits to a target divider\n> D. The PoW validity rule is then changed from H(block header) < T, to be\n> H(block header) < T * D && H(H(block header) + k) < max_int / D\n>\n\nThanks, this requires a change to the Bitcoin PoW. Good luck with that!\n\nOnce again, this suggestion would make the GHash-at-51% situation\npossible again. Working extra hard to re-enable those painful days\nsounds like a terrible idea.\n\n- egs\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/9b9bc3ea/attachment-0001.html>"
            },
            {
                "author": "Jannes Faber",
                "date": "2015-12-21T11:39:40",
                "message_text_only": "If you're saying a block withholding attack is a nice weapon to have to\ndissuade large pools, isn't that easily defeated by large pools simply\nmasquerading as multiple small pools? As, for all we know, ghash may have\ndone?\n\nIf you don't know who to attack there's no point in having the weapon.\nWhile that weapon is still dangerous in the hands of others that are\nindiscriminate, like the solo miners example of Peter Todd.\n\nSorry if i misunderstood your point.\n\n--\nJannes\n\nOn 20 December 2015 at 18:00, Emin G\u00fcn Sirer <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Sun, Dec 20, 2015 at 8:28 AM, Peter Todd <pete at petertodd.org> wrote:\n>\n>> There are a number of techniques that can be used to detect block\n>> withholding attacks that you are not aware of. These techniques usually\n>> have the characteristic that if known they can be avoided, so obviously\n>> those who know about them are highly reluctant to reveal what exactly\n>> they are. I personally know about some of them and have been asked to\n>> keep that information secret, which I will.\n>>\n>\n> Indeed, there are lots of weak measures that one could employ against\n> an uninformed attacker. As I mentioned before, these are unlikely to be\n> effective against a savvy attacker, and this is a good thing.\n>\n>\n>> In the context of KYC, this techniques would likely hold up in court,\n>> which means that if this stuff becomes a more serious problem it's\n>> perfectly viable for large, well-resourced, pools to prevent block\n>> withholding attacks, in part by removing anonymity of hashing power.\n>> This would not be a positive development for the ecosystem.\n>>\n>\n> KYC has a particular financial-regulation connotation in Bitcoin circles,\n> of which I'm sure you're aware, and which you're using as a spectre.\n> You don't mean government-regulated-KYC a la FINCEN and Bitcoin\n> exchanges like Coinbase, you are just referring to a pool operator\n> demanding to know that its customer is not coming from its competitors'\n> data centers.\n>\n> And your prediction doesn't seem well-motivated or properly justified.\n> There are tons of conditionals in your prediction, starting with the\n> premise\n> that every single open pool would implement some notion of identity\n> checking. I don't believe that will happen. Instead, we will have the\n> bigger\n> pools become more suspicious of signing up new hash power, which is a\n> good thing. And we will have small groups of people who have some reason\n> for trusting each other (e.g. they know each other from IRC, conferences,\n> etc) band together into small pools. These are fantastic outcomes for\n> decentralization.\n>\n> Secondly, DRM tech can also easily be used to prevent block withholding\n>> attacks by attesting to the honest of the hashing power. This is being\n>> discussed in the industry, and again, this isn't a positive development\n>> for the ecosystem.\n>>\n>\n> DRM is a terrible application. Once again, I see that you're trying to use\n> those\n> three letters as a spectre as well, knowing that most people hate DRM, but\n> keep in mind that DRM is just an application -- it's like pointing to\n> Adobe Flash\n> to taint all browser plugins.\n>\n> The tech behind DRM is called \"attestation,\" and it provides a technical\n> capability not possible by any other means. In essence, attestation can\n> ensure that\n> a remote node is indeed running the code that it purports to be running.\n> Since\n> most problems in computer security and distributed systems stem from not\n> knowing what protocol the attacker is going to follow, attestation is the\n> only\n> technology we have that lets us step around this limitation.\n>\n> It can ensure, for instance,\n>   - that a node purporting to be Bitcoin Core (vLatest) is indeed running\n> an\n> unadulterated, latest version of Bitcoin Core\n>   - that a node claiming that it does not harvest IP addresses from SPV\n> clients indeed does not harvest IP addresses.\n>   - that a cloud hashing outfit that rented out X terahashes to a user did\n> indeed rent out X terahashes to that particular user,\n>   - that a miner operating on behalf of some pool P will not misbehave and\n> discard perfectly good blocks\n> and so forth. All of these would be great for the ecosystem. Just getting\n> rid\n> of the cloudhashing scams would put an end to a lot of heartache.\n>\n> > Keep in mind that when an open pool gets big, like GHash did and\n>> > two other pools did before them, the only thing at our disposal used\n>> > to be to yell at people about centralization until they left the big\n>> > pools and reformed into smaller groups. Not only was such yelling\n>> > kind of desperate looking, it wasn't incredibly effective, either.\n>> > We had no protocol mechanisms that put pressure on big pools to\n>> > stop signing up people. Ittay's discovery changed that: pools that\n>> > get to be very big by indiscriminately signing up miners are likely to\n>> > be infiltrated and their profitability will drop. And Peter's post is\n>> > evidence that this is, indeed, happening as predicted. This is a\n>> > good outcome, it puts pressure on the big pools to not grow.\n>>\n>> GHash.io was not a pure pool - they owned and operated a significant\n>> amount of physical hashing power, and it's not at all clear that their %\n>> of the network actually went down following that 51% debacle.\n>>\n>\n> Right, it's not clear at all that yelling at people has much effect. As\n> much\n> fun as I had going to that meeting with GHash in London to ask them to\n> back down off of the 51% boundary, I am pretty sure that yelling at large\n> open pools will not scale. We needed better mechanisms for keeping pools\n> in check.\n>\n> And Miner's Dilemma (MD) attacks are clearly quite effective. This is a\n> time when we should count our blessings, not work actively to render\n> them inoperable.\n>\n> Currently a significant % of the hashing power - possibly a majority -\n>> is in the form of large hashing installations whose owners individually,\n>> and definitely in trusting groups, have enough hashing power to solo\n>> mine. Eyal's results indicate those miners have incentives to attack\n>> pools, and additionally they have the incentive of killing off pools to\n>> make it difficult for new competition to get established, yet they\n>> themselves are not vulnerable to that attack.\n>>\n>\n> There are indeed solo miners out there who can attack the big open\n> pools. The loss of the biggest open pools would not be a bad outcome.\n> Pools >25% pose a danger, and the home miner doesn't need a pool\n> >25% for protection against variance.\n>\n> > Peter, you allude to a specific suggestion from Luke-Jr. Can you\n>> > please describe what it is?\n>>\n>> Basically you have the pool pick a secret k for each share, and commit\n>> to H(k) in the share. Additionally the share commits to a target divider\n>> D. The PoW validity rule is then changed from H(block header) < T, to be\n>> H(block header) < T * D && H(H(block header) + k) < max_int / D\n>>\n>\n> Thanks, this requires a change to the Bitcoin PoW. Good luck with that!\n>\n> Once again, this suggestion would make the GHash-at-51% situation\n> possible again. Working extra hard to re-enable those painful days\n> sounds like a terrible idea.\n>\n> - egs\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151221/bf51d088/attachment.html>"
            },
            {
                "author": "Ittay",
                "date": "2015-12-25T11:15:27",
                "message_text_only": "Treating the pool block withholding attack as a weapon has bad\nconnotations, and I don't think anyone directly condones such an attack.\nNevertheless, the mere possibility of the attack could drive miners away\nfrom those overly-large open pools.\n\nAs for masquerading as multiple small pools -- that's a very good point,\nwith a surprising answer: it doesn't really matter. An attacker attacks all\nparts of the open pool proportionally to their size, and the result is\nbasically identical to that of attacking a single large pool.\n\nAll that being said -- it's not great to rely on the potential of attacks\nand on threats against the honest large pools out there (including GHash,\nwhich, afaik, did nothing more wrong than being successful).\n\nBest,\nIttay\n\n\nOn Mon, Dec 21, 2015 at 1:39 PM, Jannes Faber via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> If you're saying a block withholding attack is a nice weapon to have to\n> dissuade large pools, isn't that easily defeated by large pools simply\n> masquerading as multiple small pools? As, for all we know, ghash may have\n> done?\n>\n> If you don't know who to attack there's no point in having the weapon.\n> While that weapon is still dangerous in the hands of others that are\n> indiscriminate, like the solo miners example of Peter Todd.\n>\n> Sorry if i misunderstood your point.\n>\n> --\n> Jannes\n>\n> On 20 December 2015 at 18:00, Emin G\u00fcn Sirer <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> On Sun, Dec 20, 2015 at 8:28 AM, Peter Todd <pete at petertodd.org> wrote:\n>>\n>>> There are a number of techniques that can be used to detect block\n>>> withholding attacks that you are not aware of. These techniques usually\n>>> have the characteristic that if known they can be avoided, so obviously\n>>> those who know about them are highly reluctant to reveal what exactly\n>>> they are. I personally know about some of them and have been asked to\n>>> keep that information secret, which I will.\n>>>\n>>\n>> Indeed, there are lots of weak measures that one could employ against\n>> an uninformed attacker. As I mentioned before, these are unlikely to be\n>> effective against a savvy attacker, and this is a good thing.\n>>\n>>\n>>> In the context of KYC, this techniques would likely hold up in court,\n>>> which means that if this stuff becomes a more serious problem it's\n>>> perfectly viable for large, well-resourced, pools to prevent block\n>>> withholding attacks, in part by removing anonymity of hashing power.\n>>> This would not be a positive development for the ecosystem.\n>>>\n>>\n>> KYC has a particular financial-regulation connotation in Bitcoin circles,\n>> of which I'm sure you're aware, and which you're using as a spectre.\n>> You don't mean government-regulated-KYC a la FINCEN and Bitcoin\n>> exchanges like Coinbase, you are just referring to a pool operator\n>> demanding to know that its customer is not coming from its competitors'\n>> data centers.\n>>\n>> And your prediction doesn't seem well-motivated or properly justified.\n>> There are tons of conditionals in your prediction, starting with the\n>> premise\n>> that every single open pool would implement some notion of identity\n>> checking. I don't believe that will happen. Instead, we will have the\n>> bigger\n>> pools become more suspicious of signing up new hash power, which is a\n>> good thing. And we will have small groups of people who have some reason\n>> for trusting each other (e.g. they know each other from IRC, conferences,\n>> etc) band together into small pools. These are fantastic outcomes for\n>> decentralization.\n>>\n>> Secondly, DRM tech can also easily be used to prevent block withholding\n>>> attacks by attesting to the honest of the hashing power. This is being\n>>> discussed in the industry, and again, this isn't a positive development\n>>> for the ecosystem.\n>>>\n>>\n>> DRM is a terrible application. Once again, I see that you're trying to\n>> use those\n>> three letters as a spectre as well, knowing that most people hate DRM,\n>> but\n>> keep in mind that DRM is just an application -- it's like pointing to\n>> Adobe Flash\n>> to taint all browser plugins.\n>>\n>> The tech behind DRM is called \"attestation,\" and it provides a technical\n>> capability not possible by any other means. In essence, attestation can\n>> ensure that\n>> a remote node is indeed running the code that it purports to be running.\n>> Since\n>> most problems in computer security and distributed systems stem from not\n>> knowing what protocol the attacker is going to follow, attestation is the\n>> only\n>> technology we have that lets us step around this limitation.\n>>\n>> It can ensure, for instance,\n>>   - that a node purporting to be Bitcoin Core (vLatest) is indeed running\n>> an\n>> unadulterated, latest version of Bitcoin Core\n>>   - that a node claiming that it does not harvest IP addresses from SPV\n>> clients indeed does not harvest IP addresses.\n>>   - that a cloud hashing outfit that rented out X terahashes to a user\n>> did\n>> indeed rent out X terahashes to that particular user,\n>>   - that a miner operating on behalf of some pool P will not misbehave and\n>> discard perfectly good blocks\n>> and so forth. All of these would be great for the ecosystem. Just getting\n>> rid\n>> of the cloudhashing scams would put an end to a lot of heartache.\n>>\n>> > Keep in mind that when an open pool gets big, like GHash did and\n>>> > two other pools did before them, the only thing at our disposal used\n>>> > to be to yell at people about centralization until they left the big\n>>> > pools and reformed into smaller groups. Not only was such yelling\n>>> > kind of desperate looking, it wasn't incredibly effective, either.\n>>> > We had no protocol mechanisms that put pressure on big pools to\n>>> > stop signing up people. Ittay's discovery changed that: pools that\n>>> > get to be very big by indiscriminately signing up miners are likely to\n>>> > be infiltrated and their profitability will drop. And Peter's post is\n>>> > evidence that this is, indeed, happening as predicted. This is a\n>>> > good outcome, it puts pressure on the big pools to not grow.\n>>>\n>>> GHash.io was not a pure pool - they owned and operated a significant\n>>> amount of physical hashing power, and it's not at all clear that their %\n>>> of the network actually went down following that 51% debacle.\n>>>\n>>\n>> Right, it's not clear at all that yelling at people has much effect. As\n>> much\n>> fun as I had going to that meeting with GHash in London to ask them to\n>> back down off of the 51% boundary, I am pretty sure that yelling at large\n>> open pools will not scale. We needed better mechanisms for keeping pools\n>> in check.\n>>\n>> And Miner's Dilemma (MD) attacks are clearly quite effective. This is a\n>> time when we should count our blessings, not work actively to render\n>> them inoperable.\n>>\n>> Currently a significant % of the hashing power - possibly a majority -\n>>> is in the form of large hashing installations whose owners individually,\n>>> and definitely in trusting groups, have enough hashing power to solo\n>>> mine. Eyal's results indicate those miners have incentives to attack\n>>> pools, and additionally they have the incentive of killing off pools to\n>>> make it difficult for new competition to get established, yet they\n>>> themselves are not vulnerable to that attack.\n>>>\n>>\n>> There are indeed solo miners out there who can attack the big open\n>> pools. The loss of the biggest open pools would not be a bad outcome.\n>> Pools >25% pose a danger, and the home miner doesn't need a pool\n>> >25% for protection against variance.\n>>\n>> > Peter, you allude to a specific suggestion from Luke-Jr. Can you\n>>> > please describe what it is?\n>>>\n>>> Basically you have the pool pick a secret k for each share, and commit\n>>> to H(k) in the share. Additionally the share commits to a target divider\n>>> D. The PoW validity rule is then changed from H(block header) < T, to be\n>>> H(block header) < T * D && H(H(block header) + k) < max_int / D\n>>>\n>>\n>> Thanks, this requires a change to the Bitcoin PoW. Good luck with that!\n>>\n>> Once again, this suggestion would make the GHash-at-51% situation\n>> possible again. Working extra hard to re-enable those painful days\n>> sounds like a terrible idea.\n>>\n>> - egs\n>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151225/54346fe1/attachment.html>"
            },
            {
                "author": "Jonathan Toomim",
                "date": "2015-12-25T12:00:11",
                "message_text_only": "On Dec 25, 2015, at 3:15 AM, Ittay via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Treating the pool block withholding attack as a weapon has bad connotations, and I don't think anyone directly condones such an attack.\n\nI directly condone the use of block withholding attacks whenever pools get large enough to perform selfish mining attacks. Selfish mining and large, centralized pools also have bad connotations.\n\nIt's an attack against pools, not just large pools. Solo miners are immune. As such, the presence or use of block withholding attacks makes Bitcoin more similar to Satoshi's original vision. One of the issues with mining centralization via pools is that miners have a direct financial incentive to stay relatively small, but pools do not. Investing in mining is a zero-sum game, where each miner gains revenue by making investments at the expense of existing miners. This also means that miners take revenue from themselves when they upgrade their hashrate. If a miner already has 1/5 of the network hashrate, then the marginal revenue for that miner of adding 1 TH/s is only 4/5 of the marginal revenue for a miner with 0% of the network and who adds 1 TH/s. The bigger you get, the smaller your incentive to get bigger.\n\nThis incentive applies to miners, but it does not apply to pools. Pools have an incentive to get as big as possible (except for social backlash and altruistic punishment issues). Pools are the problem. I think we should be looking for ways of making pooled mining less profitable than solo mining or p2pool-style mining. Block withholding attacks are one such tool, and maybe the only usable tool we'll get. If we have to choose between making bitcoin viable long-term and avoiding things with bad connotations, it might be better to let our hands get a little bit dirty.\n\nI don't intend to perform any such attacks myself. I like to keep my hat a nice shiny white. However, if anyone else were to perform such an attack, I would condone it.\n\nP.S.: Sorry, pool operators. I have nothing against you personally. I just think pools are dangerous, and I wish they didn't exist.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151225/590e0d31/attachment.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151225/590e0d31/attachment.sig>"
            },
            {
                "author": "benevolent at cock.li",
                "date": "2015-12-25T12:02:10",
                "message_text_only": "On 2015-12-25 11:15, Ittay via bitcoin-dev wrote:\n> \n> All that being said -- it's not great to rely on the potential of\n> attacks and on threats against the honest large pools out there\n> (including GHash, which, afaik, did nothing more wrong than being\n> successful).\n\nGHash.IO and double-spending against BetCoin Dice\n\nhttps://bitcointalk.org/index.php?topic=327767.0"
            },
            {
                "author": "Jannes Faber",
                "date": "2015-12-25T16:11:18",
                "message_text_only": "On 25 Dec 2015 12:15 p.m., \"Ittay\" <ittay.eyal at cornell.edu> wrote:\n\n> As for masquerading as multiple small pools -- that's a very good point,\nwith a surprising answer: it doesn't really matter. An attacker attacks all\nparts of the open pool proportionally to their size, and the result is\nbasically identical to that of attacking a single large pool.\n\nWhile true, that's only relevant to the indiscriminate attacker! The\nvigilante attacker that wants to hurt only pools that are too large,\ndoesn't even know that there's a need to attack as all of them seem small.\n\nThat's what i was saying.\n\n>\n> On Mon, Dec 21, 2015 at 1:39 PM, Jannes Faber via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>> If you're saying a block withholding attack is a nice weapon to have to\ndissuade large pools, isn't that easily defeated by large pools simply\nmasquerading as multiple small pools? As, for all we know, ghash may have\ndone?\n>>\n>> If you don't know who to attack there's no point in having the weapon.\nWhile that weapon is still dangerous in the hands of others that are\nindiscriminate, like the solo miners example of Peter Todd.\n>>\n>> Sorry if i misunderstood your point.\n>>\n>>\n>> --\n>> Jannes\n>>\n>> On 20 December 2015 at 18:00, Emin G\u00fcn Sirer <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>> On Sun, Dec 20, 2015 at 8:28 AM, Peter Todd <pete at petertodd.org> wrote:\n>>>>\n>>>> There are a number of techniques that can be used to detect block\n>>>> withholding attacks that you are not aware of. These techniques usually\n>>>> have the characteristic that if known they can be avoided, so obviously\n>>>> those who know about them are highly reluctant to reveal what exactly\n>>>> they are. I personally know about some of them and have been asked to\n>>>> keep that information secret, which I will.\n>>>\n>>>\n>>> Indeed, there are lots of weak measures that one could employ against\n>>> an uninformed attacker. As I mentioned before, these are unlikely to be\n>>> effective against a savvy attacker, and this is a good thing.\n>>>\n>>>>\n>>>> In the context of KYC, this techniques would likely hold up in court,\n>>>> which means that if this stuff becomes a more serious problem it's\n>>>> perfectly viable for large, well-resourced, pools to prevent block\n>>>> withholding attacks, in part by removing anonymity of hashing power.\n>>>> This would not be a positive development for the ecosystem.\n>>>\n>>>\n>>> KYC has a particular financial-regulation connotation in Bitcoin\ncircles,\n>>> of which I'm sure you're aware, and which you're using as a spectre.\n>>> You don't mean government-regulated-KYC a la FINCEN and Bitcoin\n>>> exchanges like Coinbase, you are just referring to a pool operator\n>>> demanding to know that its customer is not coming from its competitors'\n>>> data centers.\n>>>\n>>> And your prediction doesn't seem well-motivated or properly justified.\n>>> There are tons of conditionals in your prediction, starting with the\npremise\n>>> that every single open pool would implement some notion of identity\n>>> checking. I don't believe that will happen. Instead, we will have the\nbigger\n>>> pools become more suspicious of signing up new hash power, which is a\n>>> good thing. And we will have small groups of people who have some reason\n>>> for trusting each other (e.g. they know each other from IRC,\nconferences,\n>>> etc) band together into small pools. These are fantastic outcomes for\n>>> decentralization.\n>>>\n>>>> Secondly, DRM tech can also easily be used to prevent block withholding\n>>>> attacks by attesting to the honest of the hashing power. This is being\n>>>> discussed in the industry, and again, this isn't a positive development\n>>>> for the ecosystem.\n>>>\n>>>\n>>> DRM is a terrible application. Once again, I see that you're trying to\nuse those\n>>> three letters as a spectre as well, knowing that most people hate DRM,\nbut\n>>> keep in mind that DRM is just an application -- it's like pointing to\nAdobe Flash\n>>> to taint all browser plugins.\n>>>\n>>> The tech behind DRM is called \"attestation,\" and it provides a\ntechnical\n>>> capability not possible by any other means. In essence, attestation can\nensure that\n>>> a remote node is indeed running the code that it purports to be\nrunning. Since\n>>> most problems in computer security and distributed systems stem from not\n>>> knowing what protocol the attacker is going to follow, attestation is\nthe only\n>>> technology we have that lets us step around this limitation.\n>>>\n>>> It can ensure, for instance,\n>>>   - that a node purporting to be Bitcoin Core (vLatest) is indeed\nrunning an\n>>> unadulterated, latest version of Bitcoin Core\n>>>   - that a node claiming that it does not harvest IP addresses from SPV\n>>> clients indeed does not harvest IP addresses.\n>>>   - that a cloud hashing outfit that rented out X terahashes to a user\ndid\n>>> indeed rent out X terahashes to that particular user,\n>>>   - that a miner operating on behalf of some pool P will not misbehave\nand\n>>> discard perfectly good blocks\n>>> and so forth. All of these would be great for the ecosystem. Just\ngetting rid\n>>> of the cloudhashing scams would put an end to a lot of heartache.\n>>>\n>>>> > Keep in mind that when an open pool gets big, like GHash did and\n>>>> > two other pools did before them, the only thing at our disposal used\n>>>> > to be to yell at people about centralization until they left the big\n>>>> > pools and reformed into smaller groups. Not only was such yelling\n>>>> > kind of desperate looking, it wasn't incredibly effective, either.\n>>>> > We had no protocol mechanisms that put pressure on big pools to\n>>>> > stop signing up people. Ittay's discovery changed that: pools that\n>>>> > get to be very big by indiscriminately signing up miners are likely\nto\n>>>> > be infiltrated and their profitability will drop. And Peter's post is\n>>>> > evidence that this is, indeed, happening as predicted. This is a\n>>>> > good outcome, it puts pressure on the big pools to not grow.\n>>>>\n>>>> GHash.io was not a pure pool - they owned and operated a significant\n>>>> amount of physical hashing power, and it's not at all clear that their\n%\n>>>> of the network actually went down following that 51% debacle.\n>>>\n>>>\n>>> Right, it's not clear at all that yelling at people has much effect. As\nmuch\n>>> fun as I had going to that meeting with GHash in London to ask them to\n>>> back down off of the 51% boundary, I am pretty sure that yelling at\nlarge\n>>> open pools will not scale. We needed better mechanisms for keeping pools\n>>> in check.\n>>>\n>>> And Miner's Dilemma (MD) attacks are clearly quite effective. This is a\n>>> time when we should count our blessings, not work actively to render\n>>> them inoperable.\n>>>\n>>>> Currently a significant % of the hashing power - possibly a majority -\n>>>> is in the form of large hashing installations whose owners\nindividually,\n>>>> and definitely in trusting groups, have enough hashing power to solo\n>>>> mine. Eyal's results indicate those miners have incentives to attack\n>>>> pools, and additionally they have the incentive of killing off pools to\n>>>> make it difficult for new competition to get established, yet they\n>>>> themselves are not vulnerable to that attack.\n>>>\n>>>\n>>> There are indeed solo miners out there who can attack the big open\n>>> pools. The loss of the biggest open pools would not be a bad outcome.\n>>> Pools >25% pose a danger, and the home miner doesn't need a pool\n>>> >25% for protection against variance.\n>>>\n>>>> > Peter, you allude to a specific suggestion from Luke-Jr. Can you\n>>>> > please describe what it is?\n>>>>\n>>>> Basically you have the pool pick a secret k for each share, and commit\n>>>> to H(k) in the share. Additionally the share commits to a target\ndivider\n>>>> D. The PoW validity rule is then changed from H(block header) < T, to\nbe\n>>>> H(block header) < T * D && H(H(block header) + k) < max_int / D\n>>>\n>>>\n>>> Thanks, this requires a change to the Bitcoin PoW. Good luck with that!\n>>>\n>>> Once again, this suggestion would make the GHash-at-51% situation\n>>> possible again. Working extra hard to re-enable those painful days\n>>> sounds like a terrible idea.\n>>>\n>>> - egs\n>>>\n>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151225/352db21d/attachment-0001.html>"
            },
            {
                "author": "Geir Harald Hansen",
                "date": "2015-12-26T00:38:06",
                "message_text_only": "On 20.12.2015 18:00, Emin G\u00fcn Sirer via bitcoin-dev wrote:\n> Instead, we will have the bigger\n> pools become more suspicious of signing up new hash power, which is a\n> good thing.\n\nBlock withholding attacks do not differentiate between small and large\npools. When Eligius and BTCGuild got hit with this, they were far from\nthe biggest pools at the time.\n\nWhen my pool, Bitminter, got a new large miner who found 1 block where\naverage luck would have had them find 3, one of the other miners claimed\nthey must be withholding blocks. Even if there is no logic or evidence\nbehind it, after one person cries wolf the others get nervous. This way\neven the possibility of block withholding can keep smaller pools from\ngrowing. It takes more hashpower to put a dent in a bigger pool, so you\nwill see less such panic.\n\n> And we will have small groups of people who have some reason\n> for trusting each other (e.g. they know each other from IRC, conferences,\n> etc) band together into small pools. These are fantastic outcomes for\n> decentralization.\n\nThree guys with 1 TH/s, 2 TH/s and 100 GH/s meet at a conference and\ndecide to start a private pool? Obviously that doesn't work. Maybe three\npeople with huge warehouses of miners would work together if they knew\nand trusted each other.\n\nThose small miners need to mine with people they don't know to get an\nacceptable variance.\n\nIf you kill off mining pools then small miners have no way to achieve\nacceptable variance and they will disappear. There will only be big\nwarehouse miners left, the ones who are big enough to solo mine.\n\nThat's not helping decentralization.\n\n> Right, it's not clear at all that yelling at people has much effect. As much\n> fun as I had going to that meeting with GHash in London to ask them to\n> back down off of the 51% boundary, I am pretty sure that yelling at large\n> open pools will not scale. We needed better mechanisms for keeping pools\n> in check.\n\nI agree. It's very disappointing how most miners and pools handle this\n(BTCGuild being the exception). But I do not think block withholding is\na good tool. It can easily destroy small pools, but it won't put a dent\nin a pool that goes over 50%.\n\nBlock withholding is a tool big pools can use to put smaller competitors\nout of business.\n\nAnd even if it was effective I would not use block withholding to attack\nother pools.\n\n> And Miner's Dilemma (MD) attacks are clearly quite effective. This is a\n> time when we should count our blessings, not work actively to render\n> them inoperable.\n\nIs it? Is there any example of block withholding leading to more\ndecentralized mining?\n\nIf I remember right, GHash being too big ended with BitFury moving some\nof their hashpower out of the pool. I don't know where that hashpower\nwent and whether the problem was solved or merely hidden.\n\nGHash profitability being very low for some time wasn't due to block\nwithholding, it was a bug that some miners abused to get paid for the\nsame work multiple times. This made it look like a lot of work was done\nwhile finding few blocks.\n\n>     Basically you have the pool pick a secret k for each share, and commit\n>     to H(k) in the share. Additionally the share commits to a target divider\n>     D. The PoW validity rule is then changed from H(block header) < T, to be\n>     H(block header) < T * D && H(H(block header) + k) < max_int / D\n> \n> \n> Thanks, this requires a change to the Bitcoin PoW. Good luck with that! \n> \n> Once again, this suggestion would make the GHash-at-51% situation \n> possible again. Working extra hard to re-enable those painful days \n> sounds like a terrible idea. \n\nBlock withholding didn't solve the problem back then. And guess what,\nthose painful days are here right now. China is at 65% and block\nwithholding isn't solving it.\n\nI was disappointed when GHash got too big and refused to do anything. It\nwas sad when their miners didn't do anything. Then they used\ndouble-spends to scam a casino. I was shocked that noone cared. Now two\nthirds of the bitcoin hashpower is within the control of a single\ngovernment. This time I expected noone would care - but I'm still\ndisappointed. I'm also surprised at the irrational behavior; there are\nso many who go out of their way to put their own investments in danger.\n\nFor a long time now many miners and pools have been irresponsible with\nthe hashpower. But block withholding just makes it worse.\n\nRegards,\nGeir H. Hansen, Bitminter mining pool"
            },
            {
                "author": "Peter Todd",
                "date": "2015-12-28T20:02:21",
                "message_text_only": "On Sun, Dec 20, 2015 at 12:00:37PM -0500, Emin G\u00fcn Sirer via bitcoin-dev wrote:\n> > In the context of KYC, this techniques would likely hold up in court,\n> > which means that if this stuff becomes a more serious problem it's\n> > perfectly viable for large, well-resourced, pools to prevent block\n> > withholding attacks, in part by removing anonymity of hashing power.\n> > This would not be a positive development for the ecosystem.\n> >\n> \n> KYC has a particular financial-regulation connotation in Bitcoin circles,\n> of which I'm sure you're aware, and which you're using as a spectre.\n> You don't mean government-regulated-KYC a la FINCEN and Bitcoin\n> exchanges like Coinbase, you are just referring to a pool operator\n> demanding to know that its customer is not coming from its competitors'\n> data centers.\n\nI mean Knowing Your Customer. The only way to know that a customer is\n*not* coming from a competitor's data center is to know their identity,\nwhich is precisely what KYC is.\n\nIn the financial world, KYC is used to refer to any time you take steps\nto determine the real identity/deanonymize your customers.\n\n> And your prediction doesn't seem well-motivated or properly justified.\n> There are tons of conditionals in your prediction, starting with the premise\n> that every single open pool would implement some notion of identity\n> checking. I don't believe that will happen. Instead, we will have the bigger\n> pools become more suspicious of signing up new hash power, which is a\n> good thing. And we will have small groups of people who have some reason\n> for trusting each other (e.g. they know each other from IRC, conferences,\n> etc) band together into small pools. These are fantastic outcomes for\n> decentralization.\n\nThat's a terrible outcomes for decentralization; we *want* people to be\nable to contribute hashing power to the network even if they don't\nalready have personal connections with existing miners. That's how we\nattract new players to the mining industry whose backgrounds are not\ncorrelated with the backgrounds of other miners - exactly what we want\nfor decentralization.\n\nKeep in mind that access to cheap power and cheap opportunities to get\nrid of waste heat is naturally decentralized by physics, economics, and\npolitics. Basically, the cheapest power, and cheapest ways to get rid of\nwaste heat, is in the form of unique opportunities that don't have\neconomies of scale. For example, much of the Chinese hashing power takes\nadvantage of stranded hydroelectric plants that are located far away\nfrom markets that would otherwise buy that power. These plants are\nlimited in size by the local rivers and there's no way to make them any\nbigger - there's a natural diseconomy of scale involved.\n\nNow, support if you have access to such a hydro plant - maybe a mine in\nthe middle of nowhere just closed and there's no-one else to sell the\npower too. Right now you can buy some hashing equipment(1) and start\nearning money immediately by pointing it at a pool of your choice. If\nthat pool fucks up, it's really easy for you to change a few lines in\nyour configs and point that hashing power to a different pool.\n\nHowever if block withholding attacks continue and kill off open access\npools the process becomes much more arduous. Chances are you won't even\nbother, and Bitcoin will end up with one less decentralized\nminer.\n\n\n1) If access to hashing equipment becomes a limiting factor/fails to\nimprove, Bitcoin itself will likely have to switch PoW functions to\nsucceed as a decentralized system.\n\n> Secondly, DRM tech can also easily be used to prevent block withholding\n> > attacks by attesting to the honest of the hashing power. This is being\n> > discussed in the industry, and again, this isn't a positive development\n> > for the ecosystem.\n> >\n> \n> DRM is a terrible application. Once again, I see that you're trying to use\n> those\n> three letters as a spectre as well, knowing that most people hate DRM, but\n> keep in mind that DRM is just an application -- it's like pointing to Adobe\n> Flash\n> to taint all browser plugins.\n> \n> The tech behind DRM is called \"attestation,\" and it provides a technical\n> capability not possible by any other means. In essence, attestation can\n> ensure that\n> a remote node is indeed running the code that it purports to be running.\n> Since\n> most problems in computer security and distributed systems stem from not\n> knowing what protocol the attacker is going to follow, attestation is the\n> only\n> technology we have that lets us step around this limitation.\n>\n> It can ensure, for instance,\n>   - that a node purporting to be Bitcoin Core (vLatest) is indeed running an\n> unadulterated, latest version of Bitcoin Core\n>   - that a node claiming that it does not harvest IP addresses from SPV\n> clients indeed does not harvest IP addresses.\n>   - that a cloud hashing outfit that rented out X terahashes to a user did\n> indeed rent out X terahashes to that particular user,\n>   - that a miner operating on behalf of some pool P will not misbehave and\n> discard perfectly good blocks\n> and so forth. All of these would be great for the ecosystem. Just getting\n> rid\n> of the cloudhashing scams would put an end to a lot of heartache.\n\nAgain, lets look at it from the perspective of someone with access to\ncheap power.\n\nWith DRM tech a likely implementation is the equipment manufacturer/pool\noperator sells you a locked down, tamper-resistant, box that only can\nsend hashing power to a specific pool. 21 for example has been\ninvestigating this model. If such equipment is common, even though the\nguy with a hydro plant in Siberia is physically and politically highly\ndecentralized, the control of the blocks created is highly centralized,\nrendering his contribution to the network's decentralization moot.\n\nAt best we might get general purpose attestation, but implementing that\nvs. locked down, single pool, boxes is more expensive and slower to\nmarket. Even then, we'd be much more likely to get fragile and\ndifficult-to-reverse-engineer hashing equipment that's always going to\nbe easier to add backdoors too.\n\nWe're better off with an ecosystem where DRM tech like attestation isn't\nneeded at all.\n\nAs for cloud hashing... those scams have mostly died off as the market\nhas become more efficient.\n\n> > GHash.io was not a pure pool - they owned and operated a significant\n> > amount of physical hashing power, and it's not at all clear that their %\n> > of the network actually went down following that 51% debacle.\n> >\n> \n> Right, it's not clear at all that yelling at people has much effect. As much\n> fun as I had going to that meeting with GHash in London to ask them to\n> back down off of the 51% boundary, I am pretty sure that yelling at large\n> open pools will not scale. We needed better mechanisms for keeping pools\n> in check.\n> \n> And Miner's Dilemma (MD) attacks are clearly quite effective. This is a\n> time when we should count our blessings, not work actively to render\n> them inoperable.\n\nWhat evidence do you have for them being \"clearly quite effective\"? Is\nthere any evidence that they were used against GHash.io for example?\n\nRemember that block withholding attacks give an advantage to those with\naccess to large amounts of physical hashing power, like GHash.IO did at\nthat time.\n\n> > Basically you have the pool pick a secret k for each share, and commit\n> > to H(k) in the share. Additionally the share commits to a target divider\n> > D. The PoW validity rule is then changed from H(block header) < T, to be\n> > H(block header) < T * D && H(H(block header) + k) < max_int / D\n> >\n> \n> Thanks, this requires a change to the Bitcoin PoW. Good luck with that!\n\nIt's not a change to the PoW, just a change to the definition of block\nvalidity; mining hardware does *not* need to change to implement\nLuke-Jr's idea. Also, as mentioned elsewhere in this thread, it can be\nimplemented slowly as a pseudo-soft-fork.\n\n-- \n'peter'[:-1]@petertodd.org\n000000000000000001d3c4acb7446f66482fb6aceb087d7601c9e0644cf60e9a\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151228/bac9774f/attachment.sig>"
            },
            {
                "author": "Eric Lombrozo",
                "date": "2015-12-26T08:23:38",
                "message_text_only": "Peter Todd wrote:\n  Fixing block withholding is relatively simple, but (so far) requires a\nSPV-visible hardfork. (Luke-Jr's two-stage target mechanism) We should\ndo this hard-fork in conjunction with any blocksize increase, which will\nhave the desirable side effect of clearly show consent by the entire\necosystem, SPV clients included.\n\nI think we can generalize this and argue that it is impossible fix this \nwithout reducing the visible difficulty and blinding the hasher to an \ninvisible difficulty. Unfortunately, changing the retargeting algo to \ncompute lower visible difficulty (leaving all else the same) or \ninterpreting the bits field in a way that yields a lower visible \ndifficulty is a hard fork by definition - blocks that didn't meet the \nvisible difficulty requirement before will now meet it.\n\njl2012 wrote:\n>After the meeting I find a softfork solution. It is very inefficient \n>and I am leaving it here just for record.\n>\n>1. In the first output of the second transaction of a block, mining \n>pool will commit a random nonce with an OP_RETURN.\n>\n>2. Mine as normal. When a block is found, the hash is concatenated with \n>the committed random nonce and hashed.\n>\n>3. The resulting hash must be smaller than 2 ^ (256 - 1/64) or the \n>block is invalid. That means about 1% of blocks are discarded.\n>\n>4. For each difficulty retarget, the secondary target is decreased by 2 \n>^ 1/64.\n>\n>5. After 546096 blocks or 10 years, the secondary target becomes 2 ^ \n>252. Therefore only 1 in 16 hash returned by hasher is really valid. \n>This should make the detection of block withholding attack much easier.\n>\n>All miners have to sacrifice 1% reward for 10 years. Confirmation will \n>also be 1% slower than it should be.\n>\n>If a node (full or SPV) is not updated, it becomes more vulnerable as \n>an attacker could mine a chain much faster without following the new \n>rules. But this is still a softfork, by definition.\njl2012's key discovery here is that if we add an invisible difficulty \nwhile keeping the retarget algo and bits semantics the same, the visible \ndifficulty will decrease automatically to compensate. In other words, we \ncan artificially increase the block time interval, allowing us to force \na lower visible difficulty at the next retarget without changing the \nretarget algo nor the bits semantics. There are no other free parameters \nwe can tweak, so it seems this is really the best we can do.\n\nUnfortunately, this also means longer confirmation times, lower \nthroughput, and lower miner revenue. Note, however, that confirmations \nwould (on average) represent more PoW, so fewer confirmations would be \nrequired to achieve the same level of security.\n\nWe can compensate for lower throughput and lower miner revenue by \nincreasing block size and increasing block rewards. Interestingly, it \nturns out we *can* do these things with soft forks by embedding new \nstructures into blocks and nesting their hash trees into existing \nstructures. Ideas such as extension blocks \n[https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-May/008356.html] \nhave been proposed before...but they add significant complications to \nthe protocol and require nontrivial app migration efforts. Old nodes \nwould not get forked off the network but backwards compatibility would \nstill be a problem as they would not be able to see at least some of the \ntransactions and some of the bitcoins in blocks. But if we're willing to \naccept this, even the \"sacred\" 21 million asymptotic limit can be raised \nvia soft fork!\n\nSo in conclusion, it *is* possible to fix this attack with a soft fork \nand without altering the basic economics...but it's almost surely a lot \nmore trouble than it's worth. Luke-Jr's solution is far simpler and more \nelegant and is perhaps one of the few examples of a new feature (as \nopposed to merely a structure cleanup) that would be better to deploy as \na hard fork since it's simple to implement and seems to stand a \nreasonable chance of near universal support...and soft fork alternatives \nare very, very ugly and significantly impact system usability...and I \nthink theory tells us we can't do any better.\n\n- Eric\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/5d48d72e/attachment-0001.html>"
            },
            {
                "author": "Eric Lombrozo",
                "date": "2015-12-26T08:26:54",
                "message_text_only": "Note: my stupid email client didn't indent Peter Todd's quote correctly. \nThe first paragraph is his, the second is my response.\n\n------ Original Message ------\nFrom: \"Eric Lombrozo\" <elombrozo at gmail.com>\nTo: \"Peter Todd\" <pete at petertodd.org>; \"Emin G\u00fcn Sirer\" \n<el33th4x0r at gmail.com>\nCc: nbvfour at gmail.com; \"Bitcoin Dev\" \n<bitcoin-dev at lists.linuxfoundation.org>\nSent: 12/26/2015 12:23:38 AM\nSubject: Re[2]: [bitcoin-dev] We need to fix the block withholding \nattack\n\n>Peter Todd wrote:\n>  Fixing block withholding is relatively simple, but (so far) requires a\n>SPV-visible hardfork. (Luke-Jr's two-stage target mechanism) We should\n>do this hard-fork in conjunction with any blocksize increase, which \n>will\n>have the desirable side effect of clearly show consent by the entire\n>ecosystem, SPV clients included.\n>\n>I think we can generalize this and argue that it is impossible fix this \n>without reducing the visible difficulty and blinding the hasher to an \n>invisible difficulty. Unfortunately, changing the retargeting algo to \n>compute lower visible difficulty (leaving all else the same) or \n>interpreting the bits field in a way that yields a lower visible \n>difficulty is a hard fork by definition - blocks that didn't meet the \n>visible difficulty requirement before will now meet it.\n>\n>jl2012 wrote:\n>>After the meeting I find a softfork solution. It is very inefficient \n>>and I am leaving it here just for record.\n>>\n>>1. In the first output of the second transaction of a block, mining \n>>pool will commit a random nonce with an OP_RETURN.\n>>\n>>2. Mine as normal. When a block is found, the hash is concatenated \n>>with the committed random nonce and hashed.\n>>\n>>3. The resulting hash must be smaller than 2 ^ (256 - 1/64) or the \n>>block is invalid. That means about 1% of blocks are discarded.\n>>\n>>4. For each difficulty retarget, the secondary target is decreased by \n>>2 ^ 1/64.\n>>\n>>5. After 546096 blocks or 10 years, the secondary target becomes 2 ^ \n>>252. Therefore only 1 in 16 hash returned by hasher is really valid. \n>>This should make the detection of block withholding attack much \n>>easier.\n>>\n>>All miners have to sacrifice 1% reward for 10 years. Confirmation will \n>>also be 1% slower than it should be.\n>>\n>>If a node (full or SPV) is not updated, it becomes more vulnerable as \n>>an attacker could mine a chain much faster without following the new \n>>rules. But this is still a softfork, by definition.\n>jl2012's key discovery here is that if we add an invisible difficulty \n>while keeping the retarget algo and bits semantics the same, the \n>visible difficulty will decrease automatically to compensate. In other \n>words, we can artificially increase the block time interval, allowing \n>us to force a lower visible difficulty at the next retarget without \n>changing the retarget algo nor the bits semantics. There are no other \n>free parameters we can tweak, so it seems this is really the best we \n>can do.\n>\n>Unfortunately, this also means longer confirmation times, lower \n>throughput, and lower miner revenue. Note, however, that confirmations \n>would (on average) represent more PoW, so fewer confirmations would be \n>required to achieve the same level of security.\n>\n>We can compensate for lower throughput and lower miner revenue by \n>increasing block size and increasing block rewards. Interestingly, it \n>turns out we *can* do these things with soft forks by embedding new \n>structures into blocks and nesting their hash trees into existing \n>structures. Ideas such as extension blocks \n>[https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-May/008356.html] \n>have been proposed before...but they add significant complications to \n>the protocol and require nontrivial app migration efforts. Old nodes \n>would not get forked off the network but backwards compatibility would \n>still be a problem as they would not be able to see at least some of \n>the transactions and some of the bitcoins in blocks. But if we're \n>willing to accept this, even the \"sacred\" 21 million asymptotic limit \n>can be raised via soft fork!\n>\n>So in conclusion, it *is* possible to fix this attack with a soft fork \n>and without altering the basic economics...but it's almost surely a lot \n>more trouble than it's worth. Luke-Jr's solution is far simpler and \n>more elegant and is perhaps one of the few examples of a new feature \n>(as opposed to merely a structure cleanup) that would be better to \n>deploy as a hard fork since it's simple to implement and seems to stand \n>a reasonable chance of near universal support...and soft fork \n>alternatives are very, very ugly and significantly impact system \n>usability...and I think theory tells us we can't do any better.\n>\n>- Eric\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/381a45ab/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-12-26T15:33:53",
                "message_text_only": "On Dec 26, 2015 9:24 AM, \"Eric Lombrozo via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Unfortunately, this also means longer confirmation times, lower\nthroughput, and lower miner revenue. Note, however, that confirmations\nwould (on average) represent more PoW, so fewer confirmations would be\nrequired to achieve the same level of security.\n>\n\nI'm not sure I understand this. If mining revenue per unit of time drops,\ntotal pow per unit of time should also drop. Even if the inter-block time\nis increased, it's not clear to me that the pow per block would necessarily\nbe higher.\nWhat am I missing?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/071e9e45/attachment.html>"
            },
            {
                "author": "Eric Lombrozo",
                "date": "2015-12-26T17:38:33",
                "message_text_only": "For simplicity, assume total network hashpower is constant. Also, assume the soft fork activates at the beginning of a retarget period.\n\nAt the moment the soft fork activates, the effective difficulty is increased (by adding a second independent PoW check that must also be satisfied) which means more hashes on average (and proportionally more time) are required to find a block. At the end of the retarget period,  the difficulty is lowered so that if the second PoW difficulty were to be kept constant the block interval would again average 10 mins.\n\nIf we were to keep the second PoW difficulty constant, we would restore the same total PoW-to-time-unit ratio and the retarget difficulty would stabilize again so each block would once more require the same number of hashes (and same amount of time) on average as before.\n\nBut we don't keep the second PoW difficulty constant - we increase it so once again more hashes on average are required to find a block by the same proportion as before. And we keep doing this.\n\nNow, the assumption that hashpower is constant is obviously unrealistic. If this is your bone of contention, then yes, I agree my model is overly simplistic.\n\nMy larger point was to explore the extent of what's possible with only a soft fork - and we can actually go pretty far and even compensate for these economic shifts by increasing block size and rewards. The whole thing is clearly a huge mess - and I wouldn't recommend actually doing it.\n\n\n\nOn December 26, 2015 7:33:53 AM PST, \"Jorge Tim\u00f3n\" <jtimon at jtimon.cc> wrote:\n>On Dec 26, 2015 9:24 AM, \"Eric Lombrozo via bitcoin-dev\" <\n>bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Unfortunately, this also means longer confirmation times, lower\n>throughput, and lower miner revenue. Note, however, that confirmations\n>would (on average) represent more PoW, so fewer confirmations would be\n>required to achieve the same level of security.\n>>\n>\n>I'm not sure I understand this. If mining revenue per unit of time\n>drops,\n>total pow per unit of time should also drop. Even if the inter-block\n>time\n>is increased, it's not clear to me that the pow per block would\n>necessarily\n>be higher.\n>What am I missing?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/91406f63/attachment-0001.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-12-26T18:01:59",
                "message_text_only": "The hashpower is a function of the block reward (subsidy + fees): it's\neconomically irrational to have costs greater than the reward (better just\nturn off your miners) and in a perfect competition (a theoretical model)\nprofits tend to zero. That is, the costs tend to equal revenue (block\nreward).\nOn Dec 26, 2015 6:38 PM, \"Eric Lombrozo\" <elombrozo at gmail.com> wrote:\n\n> For simplicity, assume total network hashpower is constant. Also, assume\n> the soft fork activates at the beginning of a retarget period.\n>\n> At the moment the soft fork activates, the effective difficulty is\n> increased (by adding a second independent PoW check that must also be\n> satisfied) which means more hashes on average (and proportionally more\n> time) are required to find a block. At the end of the retarget period, the\n> difficulty is lowered so that if the second PoW difficulty were to be kept\n> constant the block interval would again average 10 mins.\n>\n> If we were to keep the second PoW difficulty constant, we would restore\n> the same total PoW-to-time-unit ratio and the retarget difficulty would\n> stabilize again so each block would once more require the same number of\n> hashes (and same amount of time) on average as before.\n>\n> But we don't keep the second PoW difficulty constant - we increase it so\n> once again more hashes on average are required to find a block by the same\n> proportion as before. And we keep doing this.\n>\n> Now, the assumption that hashpower is constant is obviously unrealistic.\n> If this is your bone of contention, then yes, I agree my model is overly\n> simplistic.\n>\n> My larger point was to explore the extent of what's possible with only a\n> soft fork - and we can actually go pretty far and even compensate for these\n> economic shifts by increasing block size and rewards. The whole thing is\n> clearly a huge mess - and I wouldn't recommend actually doing it.\n>\n>\n>\n> On December 26, 2015 7:33:53 AM PST, \"Jorge Tim\u00f3n\" <jtimon at jtimon.cc>\n> wrote:\n>>\n>>\n>> On Dec 26, 2015 9:24 AM, \"Eric Lombrozo via bitcoin-dev\" <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>> > Unfortunately, this also means longer confirmation times, lower\n>> throughput, and lower miner revenue. Note, however, that confirmations\n>> would (on average) represent more PoW, so fewer confirmations would be\n>> required to achieve the same level of security.\n>> >\n>>\n>> I'm not sure I understand this. If mining revenue per unit of time drops,\n>> total pow per unit of time should also drop. Even if the inter-block time\n>> is increased, it's not clear to me that the pow per block would necessarily\n>> be higher.\n>> What am I missing?\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/f9473770/attachment.html>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-12-26T16:09:18",
                "message_text_only": "On Sat, Dec 26, 2015 at 8:23 AM, Eric Lombrozo via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Unfortunately, this also means longer confirmation times, lower\n> throughput, and lower miner revenue. Note, however, that confirmations\n> would (on average) represent more PoW, so fewer confirmations would be\n> required to achieve the same level of security.\n>\n\n\nNo, the re-target compensates so that the number of blocks in the last two\nweeks is 2016.  If a soft fork forces miners to throw away 25% of their\nblocks, then the difficulty will drop by 75% to keep things balanced.\nThrowing away 75% of blocks has the same effect on difficulty as destroying\n75% of mining hardware.\n\nThe block interval will only increase until the next re-target.\n\nSlowly increasing the fraction of blocks which are thrown away gives the\nre-target algorithm time to adjust, so it is another advantage.\n\nIf the rule was instantly changed so that 95% of blocks were thrown away,\nthen there could be up to 40 weeks until the next retarget and that would\ngive 200 minute block times until the adjustment.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/c9da101f/attachment.html>"
            },
            {
                "author": "Eric Lombrozo",
                "date": "2015-12-26T18:30:04",
                "message_text_only": "I should have stated that we're assuming the actual total hashrate remains constant. Obviously this is not what would actually happen - the rest of the post discusses ways to counter the economic forces at play pushing total hashrate down using only soft forks. The increased variance is still unaccounted for (pool operators would have to deal with this somehow)...and we still have larger block intervals even with compensation. And the practicality of deployment and usability are clearly problematic, to understate things.\n\nIt's merely an exercise seeking the theoretical limit of what's actually possible to do with a soft fork.\n\nOn December 26, 2015 8:09:18 AM PST, Tier Nolan via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>On Sat, Dec 26, 2015 at 8:23 AM, Eric Lombrozo via bitcoin-dev <\n>bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Unfortunately, this also means longer confirmation times, lower\n>> throughput, and lower miner revenue. Note, however, that\n>confirmations\n>> would (on average) represent more PoW, so fewer confirmations would\n>be\n>> required to achieve the same level of security.\n>>\n>\n>\n>No, the re-target compensates so that the number of blocks in the last\n>two\n>weeks is 2016.  If a soft fork forces miners to throw away 25% of their\n>blocks, then the difficulty will drop by 75% to keep things balanced.\n>Throwing away 75% of blocks has the same effect on difficulty as\n>destroying\n>75% of mining hardware.\n>\n>The block interval will only increase until the next re-target.\n>\n>Slowly increasing the fraction of blocks which are thrown away gives\n>the\n>re-target algorithm time to adjust, so it is another advantage.\n>\n>If the rule was instantly changed so that 95% of blocks were thrown\n>away,\n>then there could be up to 40 weeks until the next retarget and that\n>would\n>give 200 minute block times until the adjustment.\n>\n>\n>------------------------------------------------------------------------\n>\n>_______________________________________________\n>bitcoin-dev mailing list\n>bitcoin-dev at lists.linuxfoundation.org\n>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/c7075802/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-12-26T19:34:32",
                "message_text_only": "On Dec 26, 2015 7:30 PM, \"Eric Lombrozo via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> I should have stated that we're assuming the actual total hashrate\nremains constant.\n\nBut that's not reasonable if you are assuming that the total reward per\nunit of time drops, that's what confused me.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/130b7a07/attachment.html>"
            },
            {
                "author": "Jonathan Toomim",
                "date": "2015-12-26T21:22:36",
                "message_text_only": "Another option for how to deal with block withholding attacks: Give the miner who finds the block a bonus. This could even be part of the coinbase transaction.\n\nBlock withholding is effective because it costs the attacker 0% and costs the pool 100%. If the pool's coinbase tx was 95% to the pool, 5% (1.25 BTC) to the miner, that would make block withholding attacks much more expensive to the attacker without making a huge impact on reward variance for small miners. If your pool gets attacked by a block withholding attack, then you can respond by jacking up the bonus ratio. At some point, block withholding attacks become unfeasibly expensive to perform. This can work because the pool sacrifices a small amount of variance for its customers by increasing the bonus, but the block attacker sacrifices revenue. This should make the attacker give up before the pool does.\n\nThis system already exists in p2pool, although there the reward bonus for the block's finder is only 0.5%.\n\nThis must have been proposed before, right? Anyone know of a good analysis of the game theory math?\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/ff9f8ec9/attachment.sig>"
            },
            {
                "author": "Emin G\u00fcn Sirer",
                "date": "2015-12-27T04:33:25",
                "message_text_only": ">Another option for how to deal with block withholding attacks: Give the\nminer who finds the block a bonus.\n...\n>This must have been proposed before, right? Anyone know of a good analysis\nof the game theory math?\n\nYes, Section 8.D. in Ittay's paper discusses this countermeasure, as well\nas a few others:\n    https://www.cs.cornell.edu/~ie53/publications/btcPoolsSP15.pdf\n\n- egs\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/02893426/attachment.html>"
            },
            {
                "author": "Chris Priest",
                "date": "2015-12-20T07:39:07",
                "message_text_only": "On 12/19/15, Emin G\u00fcn Sirer <el33th4x0r at gmail.com> wrote:\n>\n> Chris Priest is confusing these attacks with selfish mining, and further,\n> his characterization of selfish mining is incorrect. Selfish Mining is\n> guaranteed to yield profits for any pool over 33% (as a result, Nick\n> Szabo has dubbed this the \"34% attack\") and it may pay off even\n> below that point if the attacker is well-positioned in the network;\n> or it may not, depending on the makeup of the rest of the pools\n> as well as the network characteristics (the more centralized\n> and bigger the other pools are, the less likely it is to pay off). There\n> was a lot of noise in the community when the SM paper came out,\n> so there are tons of incorrect response narrative out there. By now,\n> everyone who seems to be Bitcoin competent sees SM as a\n> concern, and Ethereum has already adopted our fix. I'd have hoped\n> that a poster to this list would be better informed than to repeat the\n> claim that \"majority will protect Bitcoin\" to refute a paper whose title\n> is \"majority is not enough.\"\n\nhttp://www.coindesk.com/bitcoin-mining-network-vulnerability/\n\njust sayin'...\n\nBut anyways, I agree with you on the rest of your email. This is only\nreally an attack from the perspective of the mining pool. From the\nuser's perspective, its not an attack at all. Imagine your aunt who\nhas bitcoin on a SPV wallet on her iphone. Does she care that two\nmining pools are attacking each other? Its has nothing to do with her,\nand it has nothing to do with most users or bitcoin either. From the\nbitcoin user's perspective, the mining pool landscape *should* be\nconstantly changing. Fixing this \"attack\" is promoting mining pool\nstatism. Existing mining pools will have an advantage over up and\ncoming mining pools. That is not an advantage that is best for bitcoin\nfrom the user's perspective.\n\nNow, on the other hand, if this technique is used so much, it results\nin too many pools getting shut down such that the difficulty starts to\ndecrease, *then* maybe it might be time to start thinking about fixing\nthis issue. The difficulty dropping means the security of the network\nis decreased, which *does* have an effect on every user."
            },
            {
                "author": "Emin G\u00fcn Sirer",
                "date": "2015-12-20T07:56:03",
                "message_text_only": "Initial reactions aren't always accurate, people's views change, and\nscience has its insurmountable way of convincing people. Gavin [1]\nand others [2] now cite selfish mining as a concern in the block size\ndebate, and more importantly, the paper has been peer-reviewed,\ncited, and even built-upon [3].\n\nLet's elevate the discussion, shall we?\n\n[1] Here's Gavin concerned about selfish mining:\nhttp://gavinandresen.ninja/are-bigger-blocks-better-for-bigger-miners\n\n[2] Here's Adam:\nhttp://bitcoin-development.narkive.com/mvI8Wpjp/dynamic-limit-to-the-block-size-bip-draft-discussion\n\n[3] This is a very nice extension of our work:\nAyelet Sapirshtein, Yonatan Sompolinsky, Aviv Zohar:\nhttp://arxiv.org/abs/1507.06183\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/35641b59/attachment-0001.html>"
            },
            {
                "author": "Natanael",
                "date": "2015-12-20T08:30:37",
                "message_text_only": "Wouldn't block withhold be fixed by not letting miners in pools know which\nblock candidates are valid before the pool knows? (Note: I haven't read any\nother proposals for how to fix it, this may already be known)\n\nAs an example, by having the pool use the unique per-miner nonces sent to\neach miner for effective division of labor as a kind of seed / commitment\nvalue, where one in X block candidates will be valid, where X is the\ncurrent ratio between partial PoW blocks sent as mining proofs and the full\ndifficulty?\n\nThe computational work of the pool remains low (checking this isn't harder\nthan the partial PoW validation already performed), they pool simply looks\nat which commitment value from the pool that the miner used, looks up the\ncorrect committed value and hashes that together with the partial PoW. If\nit hits the target, the block is valid.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/1fa589e5/attachment.html>"
            },
            {
                "author": "Dave Scotese",
                "date": "2015-12-29T21:51:29",
                "message_text_only": "It cannot possibly be enforced.  Enforcement is not important when you're\nsetting defaults.  In fact, you don't want to enforce defaults, but rather\nallow anyone who cares to deviate from them to do so.\n\nThe importance of default behavior is proportional to the number of folks\nwho mess with the defaults, and that, among miners, is pretty small as far\nas I know, at least in the area of deciding how to decide which block to\nbuild on when two show up at nearly the same time.\n\nOn Tue, Dec 29, 2015 at 11:25 AM, Allen Piscitello <\nallen.piscitello at gmail.com> wrote:\n\n> How could this possibly be enforced?\n>\n> On Tue, Dec 29, 2015 at 12:59 PM, Dave Scotese via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> There have been no decent objections to altering the block-selection\n>> mechanism (when two block solutions appear at nearly the same time) as\n>> described at\n>>\n>> http://bitcoin.stackexchange.com/questions/39226\n>>\n>> Key components are:\n>>\n>>    - Compute BitcoinDaysDestroyed using only transactions that have been\n>>    in your mempool for some time as oBTCDD (\"old BTCDD\").\n>>    - Use \"nearly the same time\" to mean separated in time by your guess\n>>    of the average duration of block propagation times.\n>>    - When two block solutions come in at nearly the same time, build on\n>>    the one that has the most oBTCDD, rather than the one that came in first.\n>>\n>> The goal of this change is to reduce the profitability of withholding\n>> block solutions by severely reducing the chances that a block solved a\n>> while ago can orphan one solved recently.  \"Came in first\" seems more\n>> easily gamed than \"most oBTCDD\".  As I wrote there, \"*old coins* is\n>> always a dwindling resource and *global nodes willing to help cheat* is\n>> probably a growing one.\"\n>>\n>> I will write a BIP if anyone agrees it's a good idea.\n>>\n>> On Mon, Dec 28, 2015 at 12:26 PM, Ivan Brightly via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> On Mon, Dec 28, 2015 at 2:12 PM, Peter Todd via bitcoin-dev <\n>>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>> Far more concerning is network propagation effects between large and\n>>>> small miners. For that class of issues, if you are in an environemnt\n>>>> where selfish mining is possible - a fairly flat, easily DoS/sybil\n>>>> attacked network topology - the profitability difference between small\n>>>> and large miners even *without* attacks going on is a hugely worrying\n>>>> problem. OTOH, if you're blocksize is small enough that propagation time\n>>>> is negligable to profitability, then selfish mining attacks with <30%\n>>>> hashing power aren't much of a concern - they'll be naturally defeated\n>>>> by anti-DoS/anti-sybil measures.\n>>>>\n>>>\n>>> Let's agree that one factor in mining profitability is bandwidth/network\n>>> reliability/stability. Why focus on that vs electricity contracts or\n>>> vertically integrated chip manufacturers? Surely, sufficient network\n>>> bandwidth is a more broadly available commodity than <$0.02/kwh\n>>> electricity, for example. I'm not sure that your stranded hydroelectric\n>>> miner is any more desirable than thousands of dorm room miners with access\n>>> to 10gbit university connections and free electricity.\n>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>>\n>>\n>>\n>> --\n>> I like to provide some work at no charge to prove my value. Do you need a\n>> techie?\n>> I own Litmocracy <http://www.litmocracy.com> and Meme Racing\n>> <http://www.memeracing.net> (in alpha).\n>> I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com>\n>> which now accepts Bitcoin.\n>> I also code for The Dollar Vigilante <http://dollarvigilante.com/>.\n>> \"He ought to find it more profitable to play by the rules\" - Satoshi\n>> Nakamoto\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n\n\n-- \nI like to provide some work at no charge to prove my value. Do you need a\ntechie?\nI own Litmocracy <http://www.litmocracy.com> and Meme Racing\n<http://www.memeracing.net> (in alpha).\nI'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which\nnow accepts Bitcoin.\nI also code for The Dollar Vigilante <http://dollarvigilante.com/>.\n\"He ought to find it more profitable to play by the rules\" - Satoshi\nNakamoto\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151229/35a0b842/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "We need to fix the block withholding attack",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Tier Nolan",
                "Geir Harald Hansen",
                "Jonathan Toomim",
                "Allen Piscitello",
                "Ittay",
                "benevolent at cock.li",
                "Peter Todd",
                "Multipool Admin",
                "Jorge Tim\u00f3n",
                "Matt Corallo",
                "Emin G\u00fcn Sirer",
                "Natanael",
                "Chris Priest",
                "Dave Scotese",
                "Jannes Faber",
                "Ivan Brightly",
                "Eric Lombrozo",
                "Bob McElrath",
                "jl2012"
            ],
            "messages_count": 47,
            "total_messages_chars_count": 144618
        }
    },
    {
        "title": "[bitcoin-dev] Increasing the blocksize as a (generalized) softfork.",
        "thread_messages": [
            {
                "author": "joe2015 at openmailbox.org",
                "date": "2015-12-20T10:56:33",
                "message_text_only": "This is a draft.\n\n--joe\n\nIntroduction\n============\n\nIt is generally assumed that increasing the blocksize limit requires a\nhardfork.  Instead we show that a increasing the limit can be achieved \nusing a\n\"generalized\" softfork.  Like standard softforks, generalized softforks \nneed a\nmere miner majority (>50% hashpower) rather than global consensus.\n\nStandard Softforks\n==================\n\nAfter a softfork two potential chains exist:\n\n* The new chain B3,B4,B5,... valid under the new rules and old rules.\n* The old chain B3',B4',B5',... valid under the old rules only.\n\nE.g.\n\n                       +-- B3 --- B4 --- B5\n                       |\n     ... -- B1 -- B2 --+\n                       |\n                       +-- B3' -- B4' -- B5' -- B6'\n\nAssuming that >50% of the hashpower follow the new rules, the old chain \nis\ndoomed to be orphaned:\n\n                       +-- B3 --- B4 --- B5 --- B6 --- B7 --- B8 --- ...\n                       |\n     ... -- B1 -- B2 --+\n                       |\n                       +-- B3' -- B4' -- B5' -- B6' (orphaned)\n\nHardforks may result in two chains that can co-exist indefinitely:\n\n                       +-- B3 --- B4 --- B5 --- B6 --- B7 --- B8 --- ...\n                       |\n     ... -- B1 -- B2 --+\n                       |\n                       +-- B3' -- B4' -- B5' -- B6' -- B7' -- B8' -- ...\n\nGeneralized Softforks\n=====================\n\nA *generalized* softfork introduces a transform function f(B)=B' that \nmaps a\nblock B valid under the new rules to a block B' valid under the old \nrules.\n\nAfter a generalized softfork three chains may exist:\n\n* The new chain B3,B4,B5,... valid under the new rules only.\n* The mapped chain f(B3),f(B4),f(B5),... valid under the old rules.\n* The old chain B3',B4',B5',... valid under the old rules only.\n\nE.g.\n\n                       +-- B3 ---- B4 ---- B5\n                       |\n     ... -- B1 -- B2 --+-- f(B3) - f(B4) - f(B5)\n                       |\n                       +-- B3' --- B4' --- B5' --- B6'\n\nThis is \"generalized\" softfork since defining f(B)=B (identity function)\nreduces to the standard softfork case above.\n\nAs with standard softforks, if the majority of the hashpower follow the \nnew\nrules then the old chain B3',B4',B5',... is doomed to be orphaned:\n\n                       +-- B3 ---- B4 ---- B5 ---- B6 ---- B7 ---- ...\n                       |\n     ... -- B1 -- B2 --+-- f(B3) - f(B4) - f(B5) - f(B6) - f(B7) - ...\n                       |\n                       +-- B3' --- B4' --- B5' --- B6' (orphaned)\n\nExample:\n--------\n\nSegregated Witness can be thought of as an example of a generalized \nsoftfork.\nHere the new block format consists of the combined old block and witness \ndata.\nThe function f() simply strips the witness data to reveal a valid block \nunder\nthe old rules:\n\n     NewBlock := OldBlock ++ Witness\n     f(NewBlock) = OldBlock\n\nAn Arbitrary Block-size Increase Via a Generalized Softfork\n===========================================================\n\nSegregated Witness only allows for a modest effective blocksize increase\n(although there can be other motivations for SW, but that is off-topic).\n\nInstead we engineer a generalized softfork that allows an arbitrarily \nincrease\nof the blocksize limit.  The proposal consists of two parts: (a) new \nrules for\nvalid blocks, and (b) a transformation function f().\n\nThe new block rules are very similar to the old block rules but with \nsome\nsmall changes.  In summary the changes are:\n\n* The MAX_BLOCK_SIZE limit is raised to some new limit\n   (e.g. 8Mb, BIP101, 2-4-8, BIP202, etc., or some other limit)\n* The MerkleRoot field in the header has been re-interpreted.\n* The CoinBaseTx must obey some additional new rules.\n\nAs with old blocks, a block under the new rules consists of a block \nheader\nfollowed by a vector of transactions [CoinBaseTx, Tx1, .., Txn], i.e.\n\n     NewBlock := BlockHeader ++ NumTx ++ CoinBaseTx ++ Tx1 ++ .. ++ Txn\n\nThe block header format is the same as under the old rules defined as \nfollows:\n\n     \n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n     |  Ver  |                        PrevHash                            \n    |\n     \n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n     |                           MerkleRoot                          | \nTime  |\n     \n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n     | Bits  | Nonce |\n     +-+-+-+-+-+-+-+-+\n\nUnder the old rules MerkleRoot is the Merkle root of the hashes of all\ntransactions included in the block, i.e.\n\n     MerkleRoot = merkleRoot([hash(CoinBaseTx), hash(Tx1), .., \nhash(Txn)])\n\nUnder the new rules we instead define:\n\n     MerkleRoot = merkleRoot([hash(CoinBaseTx)])\n\nThat is, under the new rules, MerkleRoot is the Merkle root of a \nsingleton\nvector containing the CoinBaseTx hash only.\n\nIn order to preserve the security properties of Bitcoin we additionally\nrequire that the CoinBaseTx somehow encodes the Merkle root of the \nremaining\ntransactions [Tx1, .., Txn].  For example, this could be achieved by \nrequiring\na mandatory OP_RETURN output that encodes this information, e.g.\n\n     OP_RETURN merkleRoot([hash(Tx1), .., hash(Txn)])\n\nAlternatively the Merkle root could be encoded in the coinbase itself.  \nThis\nensures that new transactions cannot be added/deleted from the block \nwithout\naltering the MerkleRoot field in the header.\n\nAside from these changes and the increased MAX_BLOCK_SIZE, the new block \nmust\nobey all the rules of the old block format, e.g. valid PoW, have valid \nblock\nreward, contain valid transactions, etc., etc.\n\nIn order to be a generalized softfork we also need to define a mapping \nf()\nfrom valid new blocks to valid blocks under the old rules.  We can \ndefine this\nas follows:\n\n     NewBlock    := BlockHeader ++ NumTx ++ CoinBaseTx ++ Tx1 ++ .. ++ \nTxn\n     f(NewBlock) := BlockHeader ++ 1 ++ CoinBaseTx\n\nThat is, function f() simply truncates the block so that it contains the\ncoinbase transaction only.  After truncation, the MerkleRoot field of \nthe\nblock header is valid under the old rules.\n\nThe proposed new rules combined with the transformation f() comprise a\ngeneralized softfork.  After the fork a new chain B3,B4,B5,... will be\ngenerated under the new rules defined above, including an increased \nblocksize\nlimit.  This new chain can be mapped to a valid chain \nf(B3),f(B4),f(B5),...\nunder the old rules.  Assuming that >50% of the hashpower has adopted \nthe new\nrules, the mapped chain will orphan any competing chain under the old \nrules,\njust like a standard softfork.\n\nAn interesting consequence of this design is that, since all mapped \nblocks are\nempty, old clients will never see transactions confirming.  This is be a\nstrong incentive for users to update their clients.\n\nConclusion\n==========\n\nConventional wisdoms suggests that increasing the blocksize limit \nrequires a\nhardfork.  We show that it can instead be achieved using a generalized\nsoftfork.  Like with a standard softfork, a generalized softfork merely\nrequires a majority (>50%) of hash power rather than global consensus.\nExperience has shown that the former is significantly easier to achieve.\n\nFuture Work\n-----------\n\nInvestigate other kinds of hardforks that can instead be implemented as\ngeneralized softforks, and the security implications of such...\n\n7943a2934d0be2f96589fdef2b2e00a2a7d8c3b782546bb37625d1669accb9b1\n72f018588572ca2786168cb531d10e79b81b86d3fada92298225a0f950eed3a5"
            },
            {
                "author": "joe2015 at openmailbox.org",
                "date": "2015-12-20T15:22:27",
                "message_text_only": "Link to better formatted version for web-users:\nhttps://bitcointalk.org/index.php?topic=1296628.0\n\n--joe"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-12-20T15:50:57",
                "message_text_only": "This is essentially the \"nuclear option\".  You are destroying the current\nchain (converting it to a chain of coinbases) and using the same POW to\nstart the new chain.  You are also giving everyone credit in the new chain\nequal to their credit in the old chain.\n\nIt would be better if the current chain wasn't destroyed.\n\nThis could be achieved by adding the hash of an extended block into the\ncoinbase but not requiring the coinbase to be the only transaction.\n\nThe new block is the legacy block plus the associated extended block.\n\nUsers would be allowed to move money to the extended block by spending it\nto a specific output template.\n\n<public key hash> OP_1 OP_TO_EXTENDED OP_TRUE\n\nOP_1 is the extended block index and initially, only one level is available.\n\nThis would work like P2SH.  Users could spend the money on the extended\nblock chain exactly as they could on the main chain.\n\nMoney can be brought back the same way.\n\n<public key hash> <txid1> <txid2> ... <txid-n> <N> OP_0 OP_UNLOCK OP_TRUE\n\nThe txids are for transactions that have been locked in root chain.  The\ntransaction is only valid if they are all fully funded.  The fee for the\ntransaction would be fee - (cost to fund unlocked txids).  A negative fee\ntx would be invalid.\n\nThis has the advantage that it keeps the main chain operating.  People can\nstill send money with their un-upgraded clients.  There is also an\nincentive to move funds to the extended block(s).  The new extended blocks\nare more complex, but potentially have lower fees.  Nobody is forced to\nchange.  If the large blocks aren't needed, nobody will both to use them.\n\nThe rule could be\n\nNow:\n0) 1 MB\n\nAfter change over\n0) 1 MB\n1) 2 MB\n\nAfter 2 years\n0) 1 MB\n1) 2 MB\n2) 4MB\n\nAfter 4 years\n0) 1 MB\n1) 2 MB\n2) 4MB\n3) 8MB\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/aba78371/attachment.html>"
            },
            {
                "author": "Bryan Bishop",
                "date": "2015-12-20T18:17:30",
                "message_text_only": "On Sun, Dec 20, 2015 at 4:56 AM, joe2015--- via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> An Arbitrary Block-size Increase Via a Generalized Softfork\n>\n\nThis seems conceptually similar to \"extension blocks\":\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-May/008356.html\nhttps://bitcointalk.org/index.php?topic=283746.0\nhttp://gnusha.org/bitcoin-wizards/2015-12-20.log\n\n\"Extended blocks\" are also mentioned over here too:\nhttps://bitcointalk.org/index.php?topic=1296628.msg13307275#msg13307275\n\n- Bryan\nhttp://heybryan.org/\n1 512 203 0507\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/124004d6/attachment.html>"
            },
            {
                "author": "joe2015 at openmailbox.org",
                "date": "2015-12-21T03:04:31",
                "message_text_only": "On 2015-12-21 02:17, Bryan Bishop wrote:\n> On Sun, Dec 20, 2015 at 4:56 AM, joe2015--- via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n>> An Arbitrary Block-size Increase Via a Generalized Softfork\n> \n> This seems conceptually similar to \"extension blocks\":\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-May/008356.html\n> [1]\n> https://bitcointalk.org/index.php?topic=283746.0 [2]\n> http://gnusha.org/bitcoin-wizards/2015-12-20.log [3]\n> \n> \"Extended blocks\" are also mentioned over here too:\n> https://bitcointalk.org/index.php?topic=1296628.msg13307275#msg13307275\n> [4]\n\nThe main difference is that my proposal does not introduce different \n\"tiers\" of blocks, and does not require uses to move coins to manually \nmove coins between these tiers.\n\nInstead, my proposal uses a single flat block format that is essentially \nthe same as the current block format; only bigger.\n\nThe main point is that such a change does not require a hardfork with \nglobal consensus, as is commonly assumed, but rather can be deployed \nlike a softfork using the method described in my original post.\n\n--joe."
            },
            {
                "author": "jl2012",
                "date": "2015-12-21T04:23:32",
                "message_text_only": "I proposed something very similar 2 years ago:\nhttps://bitcointalk.org/index.php?topic=283746.0\n\nThis is an interesting academic idea. But the way you implement it will \nimmediately kill all existing full and SPV nodes (not really dead, \nrather like zombie as they can't send and receive any tx).\n\njoe2015--- via bitcoin-dev \u65bc 2015-12-20 05:56 \u5beb\u5230:\n> This is a draft.\n> \n> --joe\n> \n> Introduction\n> ============\n> \n> It is generally assumed that increasing the blocksize limit requires a\n> hardfork.  Instead we show that a increasing the limit can be achieved \n> using a\n> \"generalized\" softfork.  Like standard softforks, generalized softforks \n> need a\n> mere miner majority (>50% hashpower) rather than global consensus.\n>"
            },
            {
                "author": "joe2015 at openmailbox.org",
                "date": "2015-12-21T04:41:54",
                "message_text_only": "On 2015-12-21 12:23, jl2012 wrote:\n> I proposed something very similar 2 years ago:\n> https://bitcointalk.org/index.php?topic=283746.0\n\nYes there are similarities but also some important differences.  See my \nresponse here: \nhttp://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012085.html\n\nIn short my proposal is compatible with conventional blocksize limit \nhardfork ideas, like BIP101, BIP202, 2-4-8 etc. etc.\n\n> This is an interesting academic idea. But the way you implement it\n> will immediately kill all existing full and SPV nodes (not really\n> dead, rather like zombie as they can't send and receive any tx).\n\nThat's the whole point.  After a conventional hardfork everyone needs to \nupgrade, but there is no way to force users to upgrade.  A user who is \nsimply unaware of the fork, or disagrees with the fork, uses the old \nclient and the currency splits.\n\nUnder this proposal old clients effectively enter \"zombie\" mode, forcing \nusers to upgrade.\n\n--joe"
            },
            {
                "author": "Bob McElrath",
                "date": "2015-12-30T19:00:43",
                "message_text_only": "joe2015--- via bitcoin-dev [bitcoin-dev at lists.linuxfoundation.org] wrote:\n> That's the whole point.  After a conventional hardfork everyone\n> needs to upgrade, but there is no way to force users to upgrade.  A\n> user who is simply unaware of the fork, or disagrees with the fork,\n> uses the old client and the currency splits.\n> \n> Under this proposal old clients effectively enter \"zombie\" mode,\n> forcing users to upgrade.\n\nThis is a very complex way to enter zombie mode.\n\nA simpler way is to track valid PoW chains by examining only the header, that\nare rejected for other reasons.\n\nOnce a chain is seen to be 6 or more blocks ahead of my chain tip, we should\nenter \"zombie mode\" and refuse to mine or relay, and alert the operator, because\nwe don't know what we're doing and we're out of date.  This way doesn't require\nany modifications to block structure at all.\n\n--\nCheers, Bob McElrath\n\n\"For every complex problem, there is a solution that is simple, neat, and wrong.\"\n    -- H. L. Mencken"
            },
            {
                "author": "Jonathan Toomim",
                "date": "2015-12-30T23:49:35",
                "message_text_only": "On Dec 30, 2015, at 11:00 AM, Bob McElrath via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> joe2015--- via bitcoin-dev [bitcoin-dev at lists.linuxfoundation.org] wrote:\n>> That's the whole point.  After a conventional hardfork everyone\n>> needs to upgrade, but there is no way to force users to upgrade.  A\n>> user who is simply unaware of the fork, or disagrees with the fork,\n>> uses the old client and the currency splits.\n>> \n>> Under this proposal old clients effectively enter \"zombie\" mode,\n>> forcing users to upgrade.\n> \n> This is a very complex way to enter zombie mode.\n\n\nAnother way you could make non-upgraded nodes enter zombie mode is to explicitly 51% attack the minority fork.\n\nAll soft forks are controlled, coordinated, developer-sanctioned 51% attacks against nodes that do not upgrade. The generalized softfork technique is a method of performing a soft fork that completely eliminates any usefulness to non-upgraded nodes while merge-mining another block structure to provide functionality to the nodes who have upgraded and know where to look for the new data.\n\nSoft forks are \"safe\" forks because you can trust the miners to censor blocks and transactions that do not conform to the new consensus rules. Since we've been relying on the trustworthiness of miners during soft forks in the past (and it only failed us once!), why not\n\nThe generalized softfork method has the advantage of being merge-mined, so miners don't have to lose any revenue while performing this 51% attack against non-upgraded nodes. But then you're stuck with all of your transactions in a merge-mined/commitment-based data structure, which is a bit awkward and ugly. But you could avoid all of that code ugliness by just convincing the miners to donate some hashrate (say, 5.1% if the IsSupermajority threshold is 95%, or you could make it dynamic to save some money) to ensuring that the minority fork never has any transactions in the chain. That way, you can replace the everlasting code ugliness with a little bit of temporary sociopolitical ugliness. Fortunately, angry people are easier to ignore than ugly code. /s\n\nMaybe we could call this a softly enforced hard fork? It's basically a combined hard fork for the supermajority and a soft fork to make the minority chain useless.\n\nI don't personally think that these 51% attacks are useful or necessary. This is one of the main reasons why I don't like soft forks. I find them distasteful, and think that leaving minorities free to practice their own religions and blockchain rules is a good thing. But I could see how this could address some of the objections that others have raised about the dangers of hardforks, so I'm putting it out there.\n\n> Once a chain is seen to be 6 or more blocks ahead of my chain tip, we should\n> enter \"zombie mode\" and refuse to mine or relay\n\nI like this method. However, it does have the problem of being voluntary. If nodes don't upgrade to a version that has the latent zombie gene long before a fork, then it does nothing.\n\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/5ebf647b/attachment-0001.sig>"
            },
            {
                "author": "Jonathan Toomim",
                "date": "2015-12-30T23:56:43",
                "message_text_only": "On Dec 30, 2015, at 3:49 PM, Jonathan Toomim <j at toom.im> wrote:\n\n> Since we've been relying on the trustworthiness of miners during soft forks in the past (and it only failed us once!), why not\n\nmake it explicit?\n\n(Sorry for the premature send.)\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/8d85c1b5/attachment.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/8d85c1b5/attachment.sig>"
            },
            {
                "author": "Bob McElrath",
                "date": "2015-12-31T00:04:42",
                "message_text_only": "Jonathan Toomim [j at toom.im] wrote:\n> \n> The generalized softfork method has the advantage of being merge-mined\n\nThat's an over-generalization.  There are two kinds of soft-forks WRT mining,\nthose which:\n\n1. involve new validation rules by data-hiding from non-upgraded modes\n    (e.g. extension blocks, generalized softfork)\n2. involve NO new validation logic (e.g. P2SH)\n\nMiners which are not validating transactions *should* be deprived of revenue,\nbecause their role is transaction validation, not simply brute forcing sha256d.\n\nSo I'm very strongly against this \"generalized softfork\" idea -- I also don't\nsee how upgraded nodes and non-upgraded nodes can possibly end up with the same\nUTXO set.\n\n> > Once a chain is seen to be 6 or more blocks ahead of my chain tip, we should\n> > enter \"zombie mode\" and refuse to mine or relay\n> \n> I like this method. However, it does have the problem of being voluntary. If\n> nodes don't upgrade to a version that has the latent zombie gene long before a\n> fork, then it does nothing.\n\nWhich is why it should be put into core long before forks.  ;-)\n\n--\nCheers, Bob McElrath\n\n\"For every complex problem, there is a solution that is simple, neat, and wrong.\"\n    -- H. L. Mencken"
            },
            {
                "author": "joe2015 at openmailbox.org",
                "date": "2015-12-31T04:39:25",
                "message_text_only": "> So I'm very strongly against this \"generalized softfork\" idea -- I also \n> don't\n> see how upgraded nodes and non-upgraded nodes can possibly end up with \n> the same\n> UTXO set.\n\nThe only way for non-upgraded nodes to get the correct UTXO set is to \nupgrade.\n\nIt is important to keep in mind this was proposed as an alternative to a \nhardfork.  With a hardfork the UTXOs also diverge as upgraded and \nnon-upgraded clients follow different chains.\n\n--joe."
            },
            {
                "author": "David Chan",
                "date": "2015-12-31T10:39:41",
                "message_text_only": "The UTXO sets may diverge but they actually will be strict subsets/supersets of each other as no transaction would be invalid on one fork vs another unless the hard fork lasts longer than 100 blocks. \nThis is of course specific to a block limit change hard fork. \n\n\n\nOn 2015/12/31, at 13:39, joe2015--- via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>> So I'm very strongly against this \"generalized softfork\" idea -- I also don't\n>> see how upgraded nodes and non-upgraded nodes can possibly end up with the same\n>> UTXO set.\n> \n> The only way for non-upgraded nodes to get the correct UTXO set is to upgrade.\n> \n> It is important to keep in mind this was proposed as an alternative to a hardfork.  With a hardfork the UTXOs also diverge as upgraded and non-upgraded clients follow different chains.\n> \n> --joe.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "joe2015 at openmailbox.org",
                "date": "2015-12-20T17:21:22",
                "message_text_only": "On 2015-12-20 23:50, Tier Nolan via bitcoin-dev wrote:\n> This is essentially the \"nuclear option\".\n\nRemember this is proposed as an alternative to hardforks, which is also \na \"nuclear option\".  Hardforks carry significant risks such as \npermanently splitting Bitcoin into two chains if global consensus is \nnever reached.  A (generalized) softfork avoids this problem.\n\n> This could be achieved by adding the hash of an extended block into\n> the coinbase but not requiring the coinbase to be the only\n> transaction.\n\nI think this can also be viewed as a generalized softfork if one so \nchooses, e.g.\n\n     NewBlock := OldBlock ++ ExtendedBlock\n     f(NewBlock) = OldBlock\n\nI do not think this is a bad idea but is more complex than my proposal, \ne.g. users having to move coins between different tiers of blocks.  \nUnder my proposal the Bitcoin works more or less the same except with a \nlarger limit.\n\n--joe"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-12-21T03:39:52",
                "message_text_only": "On Sun, Dec 20, 2015 at 12:21 PM, joe2015--- via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Remember this is proposed as an alternative to hardforks, which is also a\n> \"nuclear option\".  Hardforks carry significant risks such as permanently\n> splitting Bitcoin into two chains if global consensus is never reached.  A\n> (generalized) softfork avoids this problem.\n\n\nCurrent hard fork implementations include / will include miner lock-in,\njust like any soft fork.  They will not activate if global consensus is not\nreached.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/264fff1d/attachment.html>"
            },
            {
                "author": "joe2015 at openmailbox.org",
                "date": "2015-12-21T03:58:50",
                "message_text_only": "On 2015-12-21 11:39, Jeff Garzik wrote:\n> On Sun, Dec 20, 2015 at 12:21 PM, joe2015--- via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Current hard fork implementations include / will include miner\n> lock-in, just like any soft fork.  They will not activate if global\n> consensus is not reached.\n\nThat's not true at all. They activate with a miner majority (e.g. 75%, \n95%, etc.), not global consensus.  Here global really means global, i.e. \nminer, economic, all clients, etc.  In the case of a hardfork there is \nnothing stopping the miner minority from continuing the old chain.  With \na softfork the miner minority is forced to upgrade otherwise their \nblocks will be eventually orphaned.\n\nMy proposal achieves a hardfork-like blocksize limit increase but, like \na softfork, also forces the miner minority to upgrade.\n\n--joe."
            },
            {
                "author": "joe2015 at openmailbox.org",
                "date": "2015-12-31T11:32:20",
                "message_text_only": "On 2015-12-31 18:39, David Chan wrote:\n> The UTXO sets may diverge but they actually will be strict\n> subsets/supersets of each other as no transaction would be invalid on\n> one fork vs another unless the hard fork lasts longer than 100 blocks.\n\nThe UTXO sets can also diverge thanks to double spends, i.e. A->B is \nconfirmed on the old chain and A->C is confirmed on the new.\n\n--joe."
            }
        ],
        "thread_summary": {
            "title": "Increasing the blocksize as a (generalized) softfork.",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeff Garzik",
                "Bryan Bishop",
                "David Chan",
                "joe2015 at openmailbox.org",
                "Tier Nolan",
                "Jonathan Toomim",
                "Bob McElrath",
                "jl2012"
            ],
            "messages_count": 17,
            "total_messages_chars_count": 23762
        }
    },
    {
        "title": "[bitcoin-dev] A new payment address format for segregated witness or not?",
        "thread_messages": [
            {
                "author": "jl2012",
                "date": "2015-12-21T05:14:12",
                "message_text_only": "On the -dev IRC I asked the same question and people seem don't like it. \nI would like to further elaborate this topic and would like to consult \nmerchants, exchanges, wallet devs, and users for their preference\n\nBackground:\n\nPeople will be able to use segregated witness in 2 forms. They either \nput the witness program directly as the scriptPubKey, or hide the \nwitness program in a P2SH address. They are referred as \"native SW\" and \n\"SW in P2SH\" respectively\n\nExamples could be found in the draft BIP: \nhttps://github.com/jl2012/bips/blob/segwit/bip-segwit.mediawiki\n\nAs a tx malleability fix, native SW and SW in P2SH are equally good.\n\nThe SW in P2SH is better in terms of:\n1. It allows payment from any Bitcoin reference client since version \n0.6.0.\n2. Slightly better privacy by obscuration since people won't know \nwhether it is a traditional P2SH or a SW tx before it is spent. I don't \nconsider this is important since the type of tx will be revealed \neventually, and is irrelevant when native SW is more popular\n\nThe SW in P2SH is worse in terms of:\n1. It requires an additional push in scriptSig, which is not prunable in \ntransmission, and is counted as part of the core block size\n2. It requires an additional HASH160 operation than native SW\n3. It provides 160bit security, while native SW provides 256bit\n4. Since it is less efficient, the tx fee is likely to be higher than \nnative SW (but still lower than non-SW tx)\n---------------------------\n\nThe question: should we have a new payment address format for native SW?\n\nThe native SW address in my mind is basically same as existing P2PKH and \nP2SH addresses:\n\nBASE58(address_version|witness_program|checksum) , where checksum is the \nfirst 4 bytes of dSHA256(address_version|witness_program)\n\nWhy not a better checksum algorithm? Reusing the existing algorithm make \nthe implementation much easier and safe.\n\nPros for native SW address:\n1. Many people and services are still using BASE58 address\n2. Promote the use of native SW which allows lower fee, instead of the \nless efficient SW in P2SH\n3. Not all wallets and services support payment protocol (BIP70)\n4. Easy for wallets to implement\n5. Even if a wallet wants to only implement SW in P2SH, they need a new \nwallet format anyway. So there is not much exta cost to introduce a new \naddress format.\n6. Since SW is very flexible, this is very likely to be the last address \nformat to define.\n\nCons for native SW address:\n1. Addresses are bad and should not be used anymore (some arguments \ncould be found in BIP13)\n2. Payment protocol is better\n3. With SW in P2SH, it is not necessary to have a new address format\n4. Depends on the length of the witness program, the address length \ncould be a double of the existing address\n5. Old wallets won't be able to pay to a new address (but no money could \nbe lost this way)\n\n------------------------------\n\nSo I'd like to suggest 2 proposals:\n\nProposal 1:\n\nTo define a native SW address format, while people can still use payment \nprotocol or SW in P2SH if the want\n\nProposal 2:\n\nNo new address format is defined. If people want to pay as lowest fee as \npossible, they must use payment protocol. Otherwise, they may use SW in \nP2SH\n\nSince this topic is more relevant to user experience, in addition to \ncore devs, I would also like to consult merchants, exchanges, wallet \ndevs, and users for their preferences."
            },
            {
                "author": "Tier Nolan",
                "date": "2015-12-21T15:48:23",
                "message_text_only": "On Mon, Dec 21, 2015 at 5:14 AM, jl2012 via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> The SW in P2SH is worse in terms of:\n> 1. It requires an additional push in scriptSig, which is not prunable in\n> transmission, and is counted as part of the core block size\n>\n\n\"Prunable in transmission\" means that you have to include it when not\nsending the witnesses?\n\nThat is a name collision with UTXO set prunable.  My initial thought when\nreading that was \"but scriptSigs are inherently prunable, it is\nscriptPubKeys that have to be held in the UTXO database\" until I saw the\n\"in transmission\" clarification.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151221/7553ba52/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "A new payment address format for segregated witness or not?",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Tier Nolan",
                "jl2012"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 4185
        }
    },
    {
        "title": "[bitcoin-dev] Weekly developer meetings over holidays",
        "thread_messages": [
            {
                "author": "Wladimir J. van der Laan",
                "date": "2015-12-22T15:58:45",
                "message_text_only": "Next two weekly developer meetings would fall on:\n\n- Thursday December 24th\n- Thursday December 31th\n\nIn my timezone they're xmas eve and new year's eve respectively, so at least I\nwon't be there, and I'm sure they're inconvenient for most people.\n\nSo: let's have a two week hiatus, and continue January 7th.\n\nWladimir"
            }
        ],
        "thread_summary": {
            "title": "Weekly developer meetings over holidays",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Wladimir J. van der Laan"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 318
        }
    },
    {
        "title": "[bitcoin-dev] Segregated witnesses and validationless mining",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2015-12-23T01:31:19",
                "message_text_only": "# Summary\n\n1) Segregated witnesses separates transaction information about what\ncoins were transferred from the information proving those transfers were\nlegitimate.\n\n2) In its current form, segregated witnesses makes validationless mining\neasier and more profitable than the status quo, particularly as\ntransaction fees increase in relevance.\n\n3) This can be easily fixed by changing the protocol to make having a\ncopy of the previous block's (witness) data a precondition to creating a\nblock.\n\n\n# Background\n\n## Why should a miner publish the blocks they find?\n\nSuppose Alice has negligible hashing power. She finds a block. Should\nshe publish that block to the rest of the hashing power? Yes! If she\ndoesn't publish, the rest of the hashing power will build a longer chain\nthan her chain, and she won't be rewarded. Right?\n\nWell, can other miners build on top of Alice's block? If she publishes\nnothing at all, the answer is certainely no - block headers commit to\nthe previous block's hash, so without knowing at least the hash of\nAlice's block other miners can't build upon it.\n\n\n## Validationless mining\n\nSuppose Bob knows the hash of Alice's new block, as well as the height\nof it. This is sufficient information for Bob to create a new, valid,\nblock building upon Alice's block. The hash is needed because of the\nprevhash field in the block header; the height is needed because the\ncoinbase has to contain the block height. (technically he needs to know\nnTime as well to be 100% sure he's satisfying the median time rule) What\nBob is doing is validationless mining: he hasn't validated Alice's\nblock, and is assuming it is valid.\n\nIf Alice runs a pool her stratum or getblocktemplate interfaces give\nsufficient information for Bob to figure all this out. Miners today take\nadvantage of this to reduce their orphan rates - the sooner you can\nstart mining on top of the most recently found block the more money you\nearn. Pools have strong incentives to only publish work that's valid to\ntheir hashers, so as long as the target pool doesn't know who you are,\nyou have high assurance that the block hash you're building upon is\nreal.\n\nOf course, when this goes wrong it goes very wrong, greatly amplifying\nthe effect of 51% attacks and technical screwups, as seen by the July\n4th 2015 chain fork, where a majority of hashing power was building on\ntop of an invalid block.\n\n\n## Transactions\n\nHowever other than coinbase transactions, validationless mined blocks\nare nearly always empty: if Bob doesn't know what transactions Alice\nincluded in her block, he doesn't know what transaction outputs are\nstill unspent and can't safely include transactions in his block. In\nshort, Bob doesn't know what the current state of the UTXO set is. This\nhelps limit the danger of validationless mining by making it visible to\neveryone, as well as making it not as profitable due to the inability to\ncollect transaction fees. (among other reasons)\n\n\n# Segregated witnesses and validationless mining\n\nWith segregated witnesses the information required to update the UTXO\nset state is now separate from the information required to prove that\nthe new state is valid. We can fully expect miners to take advantage of\nthis to reduce latency and thus improve their profitability.\n\nWe can expect block relaying with segregated witnesses to separate block\npropagation into four different parts, from fastest to propagate to\nslowest:\n\n1) Stratum/getblocktemplate - status quo between semi-trusting miners\n\n2) Block header - bare minimum information needed to build upon a block.\nNot much trust required as creating an invalid header is expensive.\n\n3) Block w/o witness data - significant bandwidth savings, (~75%) and\nallows next miner to include transactions as normal. Again, not much\ntrust required as creating an invalid header is expensive.\n\n4) Witness data - proves that block is actually valid.\n\nThe problem is #4 is optional: the only case where not having the\nwitness data matters is when an invalid block is created, which is a\nvery rare event. It's also difficult to test in production, as creating\ninvalid blocks is extremely expensive - it would be surprising if an\nanyone had ever deliberately created an invalid block meeting the\ncurrent difficulty target in the past year or two.\n\n\n# The nightmare scenario - never tested code ~never works\n\nThe obvious implementation of highly optimised mining with segregated\nwitnesses will have the main codepath that creates blocks do no\nvalidation at all; if the current ecosystem's validationless mining is\nany indication the actual code doing this will be proprietary codebases\nwritten on a budget with little testing, and lots of bugs. At best the\ncodepaths that actually do validation will be rarely, if ever, tested in\nproduction.\n\nSecondly, as the UTXO set can be updated without the witness data, it\nwould not be surprising if at least some of the wallet ecosystem skips\nwitness validation.\n\nWith that in mind, what happens in the event of a validation failure?\nMining could continue indefinitely on an invalid chain, producing blocks\nthat in isolation appear totally normal and contain apparently valid\ntransactions. It's easy to imagine this happening from an engineering\nperspective: a simple implementation would be to have the main mining\ncodepaths be a separate, not-validating, process that receives \"invalid\nblock\" notifications from another process containing a validating\nimplementation of the Bitcoin protocol. If a bug/exploit is found that\ncauses that validation process to crash, what's to guarantee that the\nblock creation codepath will even notice? Quite likely it will continue\ncreating blocks unabated - the invalid block notification codepath is\nnever tested in production.\n\n\n# Easy solution: previous witness data proof\n\nTo return segregated witnesses to the status quo, we need to at least\nmake having the previous block's witness data be a precondition to\ncreating a block with transactions; ideally we would make it a\nprecondition to making any valid block, although going this far may\nreceive pushback from miners who are currently using validationless\nmining techniques.\n\nWe can require blocks to include the previous witness data, hashed with\na different hash function that the commitment in the previous block.\nWith witness data W, and H(W) the witness commitment in the previous\nblock, require the current block to include H'(W)\n\nA possible concrete implementation would be to compute the hash of the\ncurrent block's coinbase txouts (unique per miner for obvious reasons!)\nas well as the previous block hash. Then recompute the previous block's\nwitness data merkle tree (and optionally, transaction data merkle tree)\nwith that hash prepended to the serialized data for each witness.\n\nThis calculation can only be done by a trusted entity with access to all\nwitness data from the previous block, forcing miners to both publish\ntheir witness data promptly, as well as at least obtain witness data\nfrom other miners. (if not actually validate it!) This returns us to at\nleast the status quo, if not slightly better.\n\nThis solution is a soft-fork. As the calculation is only done once per\nblock, it is *not* a change to the PoW algorithm and is thus compatible\nwith existing miner/hasher setups. (modulo validationless mining\noptimizations, which are no longer possible)\n\n\n# Proofs of non-inflation vs. proofs of non-theft\n\nCurrently full nodes can easily verify both that inflation of the\ncurrency has no occured, as well as verify that theft of coins through\ninvalid scriptSigs has not occured. (though as an optimisation currently\nscriptSig's prior to checkpoints are not validated by default in Bitcoin\nCore)\n\nIt has been proposed that with segregated witnesses old witness data\nwill be discarded entirely. This makes it impossible to know if miner\ntheft has occured in the past; as a practical matter due to the\nsignificant amount of lost coins this also makes it possible to inflate\nthe currency.\n\nHow to fix this problem is an open question; it may be sufficient have\nthe previous witness data proof solution above require proving posession\nof not just the n-1 block, but a (random?) selection of other previous\nblocks as well. Adding this to the protocol could be done as soft-fork\nwith respect to the above previous witness data proof.\n\n-- \n'peter'[:-1]@petertodd.org\n000000000000000002c7cfc8455339de54444ac9798cad32cbfbcda77e0f2b09\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151222/6792b37a/attachment.sig>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-12-23T15:41:43",
                "message_text_only": "On Tue, Dec 22, 2015 at 05:31:19PM -0800, Peter Todd via bitcoin-dev wrote:\n> # Easy solution: previous witness data proof\n> \n> To return segregated witnesses to the status quo, we need to at least\n> make having the previous block's witness data be a precondition to\n> creating a block with transactions; ideally we would make it a\n> precondition to making any valid block, although going this far may\n> receive pushback from miners who are currently using validationless\n> mining techniques.\n> \n> We can require blocks to include the previous witness data, hashed with\n> a different hash function that the commitment in the previous block.\n> With witness data W, and H(W) the witness commitment in the previous\n> block, require the current block to include H'(W)\n> \n> A possible concrete implementation would be to compute the hash of the\n> current block's coinbase txouts (unique per miner for obvious reasons!)\n> as well as the previous block hash. Then recompute the previous block's\n> witness data merkle tree (and optionally, transaction data merkle tree)\n> with that hash prepended to the serialized data for each witness.\n> \n> This calculation can only be done by a trusted entity with access to all\n> witness data from the previous block, forcing miners to both publish\n> their witness data promptly, as well as at least obtain witness data\n> from other miners. (if not actually validate it!) This returns us to at\n> least the status quo, if not slightly better.\n> \n> This solution is a soft-fork. As the calculation is only done once per\n> block, it is *not* a change to the PoW algorithm and is thus compatible\n> with existing miner/hasher setups. (modulo validationless mining\n> optimizations, which are no longer possible)\n\nNote that this fix can be designed to retain the possibility of\nvalidationless mining, by allowing empty blocks to be created if the\nprevious witness data proof is omitted. This would achieve the same goal\nas Gregory Maxwell's blockchain verification flag(1) but with\nsignificantly less ability/reason to lie about the status of that flag.\n\n1) [bitcoin-dev] Blockchain verification flag (BIP draft),\n   Gregory Maxwell, Dec 4th 2015,\n   http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/011853.html\n\n-- \n'peter'[:-1]@petertodd.org\n000000000000000002c7cfc8455339de54444ac9798cad32cbfbcda77e0f2b09\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151223/f6274926/attachment.sig>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-12-31T23:48:48",
                "message_text_only": "On Tue, Dec 22, 2015 at 05:31:19PM -0800, Peter Todd via bitcoin-dev wrote:\n> # Summary\n\nUpdates from IRC discussion:\n\n1) There was some debate about what exactly should be required from the\ncurrent block to calculate the previous block posession proof. For\ninstance, requiring the coinbase outputs potentially restricts some\nmining setups; requiring a commitment to the current block's\n(non-coinbase) transaction outputs restricts tx selection outsourcing\nschemes.\n\nHowever, it appears that we can allow the nonce to be picked\narbitrarily. Equally, if the nonce is arbitrary, then a future soft-fork\ncan be add commitments to current block contents. Thus the previous\nblock proof can be simple H(<nonce> + <prev-block-contents>)\n\n\n2) Pieter Wuille brought up fraud proofs in relation to previous block\ncontent proofs - specifically how the simplest H(<nonce> +\n<prev-block-contents>) construction requires a large fraud proof to\nprove incorrect. This followed a bunch of debate over what exactly fraud\nproofs would be - a proof that some data is fraudulent, or a unmet\nchallenge that some data is correct?\n\nRegardless, if the posession proof is structured as a merkle tree, then\nfraud can be easily proven with a merkle path. In that model we'd take\nthe previous block contents and rehash it in its entirety with the\nnonce. The fraud proof then becomes two merkle paths - one in the\noriginal block with the original hash, and the second with the same\ndata, and same structure, but with the nonce mixed into the hashing\nalgorithm.\n\n\nTodo: writeup the difference between the fraud proof model, and the\nvalidity challenge model, to provide background to making this decision.\n\n\nIncidentally, based the positive response to fixing this issue w/\nsegregated witnesses - my main objection to the plan - I've signed the\nBitcoin Core capacity increases statement:\n\nhttps://github.com/bitcoin-dot-org/bitcoin.org/pull/1165#issuecomment-168263005\n\n-- \n'peter'[:-1]@petertodd.org\n000000000000000006808135a221edd19be6b5b966c4621c41004d3d719d18b7\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151231/1f2e6e9d/attachment-0001.sig>"
            }
        ],
        "thread_summary": {
            "title": "Segregated witnesses and validationless mining",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Peter Todd"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 13623
        }
    },
    {
        "title": "[bitcoin-dev] Segregated Witness BIPs",
        "thread_messages": [
            {
                "author": "Eric Lombrozo",
                "date": "2015-12-23T15:22:30",
                "message_text_only": "I've been working with jl2012 on some SEGWIT BIPs based on earlier \ndiscussions Pieter Wuille's implementation. We're considering submitting \nthree separate BIPs:\n\n\nCONSENSUS BIP: witness structures and how they're committed to blocks, \ncost metrics and limits, the scripting system (witness programs), and \nthe soft fork mechanism.\n\nPEER SERVICES BIP: relay message structures, witnesstx serialization, \nand other issues pertaining to the p2p protocol such as IBD, \nsynchronization, tx and block propagation, etc...\n\nAPPLICATIONS BIP: scriptPubKey encoding formats and other wallet \ninteroperability concerns.\n\n\nThe Consensus BIP is submitted as a draft and is pending BIP number \nassignment: https://github.com/bitcoin/bips/pull/265\nThe other two BIPS will be drafted soon.\n\n---\nEric\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151223/c301011d/attachment.html>"
            },
            {
                "author": "jl2012",
                "date": "2015-12-24T14:22:01",
                "message_text_only": "The SW payment address format BIP draft is ready and is pending BIP \nnumber assignment:\nhttps://github.com/bitcoin/bips/pull/267\n\nThis is the 3rd BIP for segwit. The 2nd one for Peer Services is being \nprepared by Eric Lombrozo\n\nEric Lombrozo via bitcoin-dev \u65bc 2015-12-23 10:22 \u5beb\u5230:\n> I've been working with jl2012 on some SEGWIT BIPs based on earlier\n> discussions Pieter Wuille's implementation. We're considering\n> submitting three separate BIPs:\n> \n> CONSENSUS BIP: witness structures and how they're committed to blocks,\n> cost metrics and limits, the scripting system (witness programs), and\n> the soft fork mechanism.\n> \n> PEER SERVICES BIP: relay message structures, witnesstx serialization,\n> and other issues pertaining to the p2p protocol such as IBD,\n> synchronization, tx and block propagation, etc...\n> \n> APPLICATIONS BIP: scriptPubKey encoding formats and other wallet\n> interoperability concerns.\n> \n> The Consensus BIP is submitted as a draft and is pending BIP number\n> assignment: https://github.com/bitcoin/bips/pull/265 [1]\n> The other two BIPS will be drafted soon.\n> \n> ---\n> Eric\n> \n> Links:\n> ------\n> [1] https://github.com/bitcoin/bips/pull/265\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "jl2012",
                "date": "2015-12-27T08:26:21",
                "message_text_only": "The SW payment address format BIP is completely rewritten to introduce 2 \ntypes of new addresses:\n\nhttps://github.com/bitcoin/bips/pull/267\n\njl2012 via bitcoin-dev \u65bc 2015-12-24 09:22 \u5beb\u5230:\n> The SW payment address format BIP draft is ready and is pending BIP\n> number assignment:\n> https://github.com/bitcoin/bips/pull/267\n> \n> This is the 3rd BIP for segwit. The 2nd one for Peer Services is being\n> prepared by Eric Lombrozo\n> \n> Eric Lombrozo via bitcoin-dev \u65bc 2015-12-23 10:22 \u5beb\u5230:\n>> I've been working with jl2012 on some SEGWIT BIPs based on earlier\n>> discussions Pieter Wuille's implementation. We're considering\n>> submitting three separate BIPs:\n>> \n>> CONSENSUS BIP: witness structures and how they're committed to blocks,\n>> cost metrics and limits, the scripting system (witness programs), and\n>> the soft fork mechanism.\n>> \n>> PEER SERVICES BIP: relay message structures, witnesstx serialization,\n>> and other issues pertaining to the p2p protocol such as IBD,\n>> synchronization, tx and block propagation, etc...\n>> \n>> APPLICATIONS BIP: scriptPubKey encoding formats and other wallet\n>> interoperability concerns.\n>> \n>> The Consensus BIP is submitted as a draft and is pending BIP number\n>> assignment: https://github.com/bitcoin/bips/pull/265 [1]\n>> The other two BIPS will be drafted soon.\n>> \n>> ---\n>> Eric\n>> \n>> Links:\n>> ------\n>> [1] https://github.com/bitcoin/bips/pull/265\n>> \n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "Segregated Witness BIPs",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Eric Lombrozo",
                "jl2012"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 4105
        }
    },
    {
        "title": "[bitcoin-dev] \"Hashpower liquidity\" is more important than \"mining centralization\"",
        "thread_messages": [
            {
                "author": "Chris Priest",
                "date": "2015-12-26T03:18:13",
                "message_text_only": "The term \"mining centralization\" is very common. It come up almost in\nevery single discussion relating to bitcoin these days. Some people\nsay \"mining is already centralized\" and other such things. I think\nthis is a very bad term, and people should stop saying those things.\nLet me explain:\n\nUnder normal operations, if every single miner in th network is under\none roof, nothing would happen. If there was oly one mining pool that\neveryone had to use, this would have no effect on the system\nwhatsoever. The only time this would be a problem is if that one pool\nwere to censor transactions, or in any other way operate out of the\nnormal.\n\nRight now, the network is in a period of peace. There are no\ngovernments trying to coerce mining pools into censoring transaction,\nor otherwise disrupting the network. For all we know, the next 500\nyears of bitcoin's history could be filled with complete peaceful\noperations with no government interference at all.\n\n*If* for some reason in the future a government were to decide that\nthey want to disrupt the bitcoin network, then all the hashpower being\nunder one control will be problematic, if and only if hashpower\nliquidity is very low. Hashpower liquidity is the measure of how\neasily hashpower can move from one pool to another. If all the mining\nhardware on the network is mining one one pool and **will never or can\nnever switch to another pool** then the hashpower liquidity is very\nlow. If all the hashpower on the network can very easily move to\nanother pool, then hashpower liquidity is very high.\n\nIf the one single mining pool were to start censoring transactions and\nthere is no other pool to move to, then hashpower liquidity is very\nhigh, and that would be very bad for bitcoin. If there was dozens of\nother pools in existence, and all the mining hardware owners could\nswitch to another pool easiely, then the hashpower liquidity is very\nhigh, and the censorship attack will end as soon as the hashpower\nmoves to other pools.\n\nMy argument is that hashpower liquidity is much more important of a\nmetric to think about than simply \"mining centralization\". The\ndifference between the two terms is that one term describes a\ntemporary condition, while the other one measures a more permanent\ncondition. Both terms are hard to measure in concrete terms.\n\nInstead of saying \"this change will increase mining centralization\" we\nshould instead be thinking \"will this change increase hashpower\nliquidity?\".\n\nHopefully people will understand this concept and the term \"mining\ncentralization\" will become archaic."
            }
        ],
        "thread_summary": {
            "title": "\"Hashpower liquidity\" is more important than \"mining centralization\"",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Chris Priest"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2558
        }
    },
    {
        "title": "[bitcoin-dev] Consensus census",
        "thread_messages": [
            {
                "author": "Jonathan Toomim",
                "date": "2015-12-28T00:10:36",
                "message_text_only": "I traveled around in China for a couple weeks after Hong Kong to visit with miners and confer on the blocksize increase and block propagation issues. I performed an informal survey of a few of the blocksize increase proposals that I thought would be likely to have widespread support. The results of the version 1.0 census are below.\n\nMy brother is working on a website for a version 2.0 census. You can view the beta version of it and participate in it at https://bitcoin.consider.it. If you have any requests for changes to the format, please CC him at m at toom.im.\n\nhttps://docs.google.com/spreadsheets/d/1Cg9Qo9Vl5PdJYD4EiHnIGMV3G48pWmcWI3NFoKKfIzU/edit#gid=0\n\nOr a snapshot for those behind the GFW without a VPN:\nhttp://toom.im/files/consensus_census.pdf\n\nHTML follows:\n\nMiner\tHashrate\tBIP103\t2 MB now (BIP102)\t2 MB now, 4 MB in 2 yr\t2-4-8 (Adam Back)\t3 MB now\t3 MB now, 10 MB in 3 yr\tBIP101\nF2Pool\t22%\tN/A\tAcceptable\tAcceptable\tPreferred\tAcceptable\tAcceptable\tToo fast\nAntPool\t23%\tToo slow\tAcceptable\tAcceptable\tAcceptable\tN/A\tN/A\tToo fast\nBitfury\t18%\tN/A\tAcceptable\tProbably/maybe\tMaybe\tN/A\tProbably too fast\tToo fast\nBTCC Pool\t11%\tN/A\tAcceptable\tAcceptable\tAcceptable\tAcceptable\tAcceptable, I think\tN/A\nKnCMiner\t7%\tN/A\tProbably?\tProbably?\t\"We like 2-4-8\"\tProbably?\tN/A\tN/A\nBW.com\t7%\tN/A\tN/A\tN/A\tN/A\tN/A\tN/A\tN/A\nSlush\t4%\tN/A\tN/A\tN/A\tN/A\tN/A\tN/A\tN/A\n21 Inc.\t3%\tN/A\tN/A\tN/A\tN/A\tN/A\tN/A\tN/A\nEligius\t1%\tN/A\tN/A\tN/A\tN/A\tN/A\tN/A\tN/A\nBitClub\t1%\tN/A\tN/A\tN/A\tN/A\tN/A\tN/A\tN/A\nGHash.io\t1%\tN/A\tN/A\tN/A\tN/A\tN/A\tN/A\tN/A\nMisc\t2%\tN/A\tN/A\tN/A\tN/A\tN/A\tN/A\tN/A\nCertainly in favor\t\t\t74%\t56%\t63%\t33%\t22%\nPossibly in favor\t\t\t81%\t81%\t81%\t40%\t33%\t0%\nTotal votes counted\t\t\t81%\t81%\t81%\t40%\t51%\t63%\nF2Pool: Blocksize increase could be phased in at block 400,000. No floating-point math. No timestamp-based forking (block height is okay). Conversation was with Wang Chun via IRC.\nAntPool/Bitmain: We should get miners and devs together for few rounds of voting to decide which plan to implement. (My brother is working on a tool which may be useful for this. More info soon.) The blocksize increase should be merged into Bitcoin Core, and should not be implemented in an alternate client like BitcoinXT. A timeline of about 3 months for the fork was discussed, though I don't know if that was acceptable or preferable to Bitmain. Conversation was mostly with Micree Zhan and Kevin Pan at the Bitmain HQ. Jihan Wu was absent.\nBitfury: We should fix performance issues in bitcoind before 4 MB, and we MUST fix performance issues before 8 MB. A plan that includes 8 MB blocks in the future and assumes the performance fixes will be implemented might be acceptable to us, but we'll have to evaluate it more before coming to a conclusion. 2-4-8 \"is like parachute basejumping - if you jump, and was unable to fix parachute during the 90sec drop - you will be 100% dead. plan A) [multiple hard forks] more safe.\" Conversation was with Alex Petrov at the conference and via email.\nKnC: I only had short conversations with Sam Cole, but from what I can tell, they would be okay with just about anything reasonable.\nBTCC: It would be much better to have the support of Core, but if Core doesn't include a blocksize increase soon in the master branch, we may be willing to start running a fork. Conversation was with Samson Mow and a few others at BTCC HQ.\nThe conversations I had with all of these entities were of an informal, non-binding nature. Positions are subject to change. BIP100 was not included in my talks because (a) coinbase voting already covers it pretty well, and (b) it is more complicated than the other proposals and currently does not seem likely to be implemented. I generally did not bring up SegWit during the conversations I had with miners, and neither did the miners, so it is also absent. (I thought that it was too early for miners to have an informed opinion of SegWit's relative merits.) I have not had any contact with BW.com or any of the smaller entities. Questions can be directed to j at toom.im.\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151227/824ec2c9/attachment-0001.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151227/824ec2c9/attachment-0001.sig>"
            }
        ],
        "thread_summary": {
            "title": "Consensus census",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jonathan Toomim"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 4498
        }
    },
    {
        "title": "[bitcoin-dev] We can trivially fix quadratic CHECKSIG with a simple soft-fork modifying just SignatureHash()",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2015-12-29T05:35:59",
                "message_text_only": "Occured to me that this hasn't been mentioned before...\n\nWe can trivially fix the quadratic CHECK(MULTI)SIG execution time issue\nby soft-forking in a limitation on just SignatureHash() to only return\ntrue if the tx size is <100KB. (or whatever limit makes sense)\n\nThis fix has the advantage over schemes that limit all txs, or try to\ncount sigops, of being trivial to implement, while still allowing for a\nfuture CHECKSIG2 soft-fork that properly fixes the quadratic hashing\nissue; >100KB txs would still be technically allowed, it's just that\n(for now) there'd be no way for them to spend coins that are\ncryptographically secured.\n\nFor example, if we had an issue with a major miner exploiting\nslow-to-propagate blocks(1) to harm their competitors, this simple fix\ncould be deployed as a soft-fork in a matter of days, stopping the\nattack quickly.\n\n1) www.mail-archive.com/bitcoin-development at lists.sourceforge.net/msg03200.html\n\n-- \n'peter'[:-1]@petertodd.org\n0000000000000000094afcbbad10aa6c82ddd8aad102020e553d50a60b6c678f\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151228/0f5f8fd2/attachment.sig>"
            },
            {
                "author": "jl2012",
                "date": "2015-12-29T07:47:22",
                "message_text_only": "Do we need to consider that someone may have a timelocked big tx, with \nprivate key lost?\n\nI think we need to tell people not to do this. Related discussion:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-November/011656.html\n\n\nPeter Todd via bitcoin-dev \u65bc 2015-12-29 00:35 \u5beb\u5230:\n> Occured to me that this hasn't been mentioned before...\n> \n> We can trivially fix the quadratic CHECK(MULTI)SIG execution time issue\n> by soft-forking in a limitation on just SignatureHash() to only return\n> true if the tx size is <100KB. (or whatever limit makes sense)\n> \n> This fix has the advantage over schemes that limit all txs, or try to\n> count sigops, of being trivial to implement, while still allowing for a\n> future CHECKSIG2 soft-fork that properly fixes the quadratic hashing\n> issue; >100KB txs would still be technically allowed, it's just that\n> (for now) there'd be no way for them to spend coins that are\n> cryptographically secured.\n> \n> For example, if we had an issue with a major miner exploiting\n> slow-to-propagate blocks(1) to harm their competitors, this simple fix\n> could be deployed as a soft-fork in a matter of days, stopping the\n> attack quickly.\n> \n> 1) \n> www.mail-archive.com/bitcoin-development at lists.sourceforge.net/msg03200.html\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Jonathan Toomim",
                "date": "2015-12-29T12:42:55",
                "message_text_only": "That sounds like a rather unlikely scenario. Unless you have a specific reason to suspect that might be the case, I think we don't need to worry about it too much. If we announce the intention to perform such a soft fork a couple of months before the soft fork becomes active, and if nobody complains about it destroying their secret stash, then I think that's fair enough and we could proceed.\n\nOn Dec 28, 2015, at 11:47 PM, jl2012 via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Do we need to consider that someone may have a timelocked big tx, with private key lost?\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151229/11fa10ce/attachment.sig>"
            },
            {
                "author": "jl2012",
                "date": "2015-12-29T12:55:28",
                "message_text_only": "What if someone complains? We can't even tell whether a complaint is \nlegit or just trolling. That's why I think we need some general \nconsensus rules which is not written in code, but as a social contract. \nBreaking those rules would be considered as a hardfork and is allowed \nonly in exceptional situation.\n\nJonathan Toomim via bitcoin-dev \u65bc 2015-12-29 07:42 \u5beb\u5230:\n> That sounds like a rather unlikely scenario. Unless you have a\n> specific reason to suspect that might be the case, I think we don't\n> need to worry about it too much. If we announce the intention to\n> perform such a soft fork a couple of months before the soft fork\n> becomes active, and if nobody complains about it destroying their\n> secret stash, then I think that's fair enough and we could proceed.\n> \n> On Dec 28, 2015, at 11:47 PM, jl2012 via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n>> Do we need to consider that someone may have a timelocked big tx, with \n>> private key lost?\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Jonathan Toomim",
                "date": "2015-12-29T13:00:45",
                "message_text_only": "I suggest we use short-circuit evaluation. If someone complains, we figure it out as we go, maybe depending on the nature of the complaint. If nobody complains, we get it done faster.\n\nWe're humans. We have the ability to respond to novel conditions without relying on predetermined rules and algorithms. I suggest we use that ability sometimes.\n\nOn Dec 29, 2015, at 4:55 AM, jl2012 <jl2012 at xbt.hk> wrote:\n\n> What if someone complains? We can't even tell whether a complaint is legit or just trolling. That's why I think we need some general consensus rules which is not written in code, but as a social contract. Breaking those rules would be considered as a hardfork and is allowed only in exceptional situation.\n> \n> Jonathan Toomim via bitcoin-dev \u65bc 2015-12-29 07:42 \u5beb\u5230:\n>> That sounds like a rather unlikely scenario. Unless you have a\n>> specific reason to suspect that might be the case, I think we don't\n>> need to worry about it too much. If we announce the intention to\n>> perform such a soft fork a couple of months before the soft fork\n>> becomes active, and if nobody complains about it destroying their\n>> secret stash, then I think that's fair enough and we could proceed.\n>> On Dec 28, 2015, at 11:47 PM, jl2012 via bitcoin-dev\n>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>> Do we need to consider that someone may have a timelocked big tx, with private key lost?\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151229/f1c4bd08/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "We can trivially fix quadratic CHECKSIG with a simple soft-fork modifying just SignatureHash()",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jonathan Toomim",
                "jl2012",
                "Peter Todd"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 6705
        }
    },
    {
        "title": "[bitcoin-dev] An implementation of BIP102 as a softfork.",
        "thread_messages": [
            {
                "author": "joe2015 at openmailbox.org",
                "date": "2015-12-30T05:46:01",
                "message_text_only": "Below is a proof-of-concept implementation of BIP102 as a softfork:\n\nhttps://github.com/ZoomT/bitcoin/tree/2015_2mb_blocksize\nhttps://github.com/jgarzik/bitcoin/compare/2015_2mb_blocksize...ZoomT:2015_2mb_blocksize?diff=split&name=2015_2mb_blocksize\n\nBIP102 is normally a hardfork.  The softfork version (unofficial\ncodename BIP102s) uses the idea described here:\nhttp://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012073.html\n\nThe basic idea is that post-fork blocks are constructed in such a way\nthey can be mapped to valid blocks under the pre-fork rules.  BIP102s\nis a softfork in the sense that post-fork miners are still creating a\nvalid chain under the old rules, albeit indirectly.\n\n From the POV of non-upgraded clients, BIP102s circumvents the\nblock-size limit by moving transaction validation data \"outside\" of\nthe block.  This is a similar trick used by Segregated Witness and\nExtension Blocks (both softfork proposals).\n\n From the POV of upgraded clients, the block layout is unchanged,\nexcept:\n- A larger 2MB block-size limit (=BIP102);\n- The header Merkle root has a new (backwards compatible)\n   interpretation;\n- The coinbase encodes the Merkle root of the remaining txs.\nAside from this, blocks maintain their original format, i.e. a block\nheader followed by a vector of transactions.  This keeps the\nimplementation simple, and is distinct from SW and EB.\n\nSince BIP102s is a softfork it means that:\n- A miner majority (e.g. 75%, 95%) force miner consensus (100%).  This\n   is not true for a hardfork.\n- Fraud risk is significantly reduced (6-conf unlikely depending on\n   activation threshold).\nThis should address some of the concerns with deploying a block-size\nincrease using a hardfork.\n\nNotes:\n\n- The same basic idea could be adapted to any of the other proposals\n   (BIP101, 2-4-8, BIP202, etc.).\n- I used Jeff Garzik's BIP102 implementation which is incomplete (?).\n   The activation logic is left unchanged.\n- I am not a Bitcoin dev so hopefully no embarrassing mistakes in my\n   code :-(\n\n--joe"
            },
            {
                "author": "Marco Falke",
                "date": "2015-12-30T10:33:39",
                "message_text_only": "This is an interesting approach but I don't see how this is a soft\nfork. (Just because something is not a hard fork, doesn't make it a\nsoft fork by definition)\nSoftforks don't require any nodes to upgrade. [1]\nNonetheless, as I understand your approach, it requires nodes to\nupgrade. Otherwise they are missing all transactions but the coinbase\ntransactions. Thus, they cannot update their utxoset and are easily\nsusceptible to double spends...\n\nAm I missing something obvious?\n\n-- Marco\n\n\n[1] https://en.bitcoin.it/wiki/Softfork#Implications"
            },
            {
                "author": "joe2015 at openmailbox.org",
                "date": "2015-12-30T16:27:50",
                "message_text_only": "On 2015-12-30 18:33, Marco Falke wrote:\n> This is an interesting approach but I don't see how this is a soft\n> fork. (Just because something is not a hard fork, doesn't make it a\n> soft fork by definition)\n> Softforks don't require any nodes to upgrade. [1]\n> Nonetheless, as I understand your approach, it requires nodes to\n> upgrade. Otherwise they are missing all transactions but the coinbase\n> transactions. Thus, they cannot update their utxoset and are easily\n> susceptible to double spends...\n> \n> Am I missing something obvious?\n> \n> -- Marco\n> \n> \n> [1] https://en.bitcoin.it/wiki/Softfork#Implications\n\nIt just depends how you define \"softfork\".  In my original write-up I \ncalled it a \"generalized\" softfork, Peter suggested a \"firm\" fork, and \nthere are some suggestions for other names.  Ultimately what you call it \nis not very important.\n\n--joe."
            },
            {
                "author": "Jonathan Toomim",
                "date": "2015-12-30T13:29:05",
                "message_text_only": "As a first impression, I think this proposal is intellectually interesting, but crufty and hackish and should never actually be deployed. Writing code for Bitcoin in a future in which we have deployed a few generalized softforks this way sounds terrifying.\n\nInstead of this:\n\n    CTransaction GetTransaction(CBlock block, unsigned int index) {\n        return block->vtx[index];\n    }\n\nWe might have this:\n\n    CTransaction GetTransaction(CBlock block, unsigned int index) {\n        if (!IsBIP102sBlock(block)) {\n            return block->vtx[index];\n        } else {\n            if (!IsOtherGeneralizedSoftforkBlock(block)) {\n                // hooray! only one generalized softfork level to deal with!\n                return LookupBlock(GetGSHashFromCoinbase(block->vtx[0].vin[0].scriptSig))->vtx[index];\n           } else {\n               throw NotImplementedError; // I'm too lazy to write pseudocode this complicated just to argue a point\n        }\n    }\n\nIt might be possible to make that a bit simpler with recursion, or by doing subsequent generalized softforks in a way that doesn't have multi-levels-deep block-within-a-block-within-a-block stuff. Still: ugh.\n\n\n\n\nOn Dec 29, 2015, at 9:46 PM, joe2015--- via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Below is a proof-of-concept implementation of BIP102 as a softfork:\n> \n> https://github.com/ZoomT/bitcoin/tree/2015_2mb_blocksize\n> https://github.com/jgarzik/bitcoin/compare/2015_2mb_blocksize...ZoomT:2015_2mb_blocksize?diff=split&name=2015_2mb_blocksize\n> \n> BIP102 is normally a hardfork.  The softfork version (unofficial\n> codename BIP102s) uses the idea described here:\n> http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012073.html\n> \n> The basic idea is that post-fork blocks are constructed in such a way\n> they can be mapped to valid blocks under the pre-fork rules.  BIP102s\n> is a softfork in the sense that post-fork miners are still creating a\n> valid chain under the old rules, albeit indirectly.\n> \n> From the POV of non-upgraded clients, BIP102s circumvents the\n> block-size limit by moving transaction validation data \"outside\" of\n> the block.  This is a similar trick used by Segregated Witness and\n> Extension Blocks (both softfork proposals).\n> \n> From the POV of upgraded clients, the block layout is unchanged,\n> except:\n> - A larger 2MB block-size limit (=BIP102);\n> - The header Merkle root has a new (backwards compatible)\n>  interpretation;\n> - The coinbase encodes the Merkle root of the remaining txs.\n> Aside from this, blocks maintain their original format, i.e. a block\n> header followed by a vector of transactions.  This keeps the\n> implementation simple, and is distinct from SW and EB.\n> \n> Since BIP102s is a softfork it means that:\n> - A miner majority (e.g. 75%, 95%) force miner consensus (100%).  This\n>  is not true for a hardfork.\n> - Fraud risk is significantly reduced (6-conf unlikely depending on\n>  activation threshold).\n> This should address some of the concerns with deploying a block-size\n> increase using a hardfork.\n> \n> Notes:\n> \n> - The same basic idea could be adapted to any of the other proposals\n>  (BIP101, 2-4-8, BIP202, etc.).\n> - I used Jeff Garzik's BIP102 implementation which is incomplete (?).\n>  The activation logic is left unchanged.\n> - I am not a Bitcoin dev so hopefully no embarrassing mistakes in my\n>  code :-(\n> \n> --joe\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/fd516c4a/attachment.sig>"
            },
            {
                "author": "Marcel Jamin",
                "date": "2015-12-30T13:57:08",
                "message_text_only": "I guess the same could be said about the softfork flavoured SW\nimplementation. In any case, the strategy pattern helps with code structure\nin situations like this.\n\n2015-12-30 14:29 GMT+01:00 Jonathan Toomim via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org>:\n\n> As a first impression, I think this proposal is intellectually\n> interesting, but crufty and hackish and should never actually be deployed.\n> Writing code for Bitcoin in a future in which we have deployed a few\n> generalized softforks this way sounds terrifying.\n>\n> Instead of this:\n>\n>     CTransaction GetTransaction(CBlock block, unsigned int index) {\n>         return block->vtx[index];\n>     }\n>\n> We might have this:\n>\n>     CTransaction GetTransaction(CBlock block, unsigned int index) {\n>         if (!IsBIP102sBlock(block)) {\n>             return block->vtx[index];\n>         } else {\n>             if (!IsOtherGeneralizedSoftforkBlock(block)) {\n>                 // hooray! only one generalized softfork level to deal\n> with!\n>                 return\n> LookupBlock(GetGSHashFromCoinbase(block->vtx[0].vin[0].scriptSig))->vtx[index];\n>            } else {\n>                throw NotImplementedError; // I'm too lazy to write\n> pseudocode this complicated just to argue a point\n>         }\n>     }\n>\n> It might be possible to make that a bit simpler with recursion, or by\n> doing subsequent generalized softforks in a way that doesn't have\n> multi-levels-deep block-within-a-block-within-a-block stuff. Still: ugh.\n>\n>\n>\n>\n> On Dec 29, 2015, at 9:46 PM, joe2015--- via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> > Below is a proof-of-concept implementation of BIP102 as a softfork:\n> >\n> > https://github.com/ZoomT/bitcoin/tree/2015_2mb_blocksize\n> >\n> https://github.com/jgarzik/bitcoin/compare/2015_2mb_blocksize...ZoomT:2015_2mb_blocksize?diff=split&name=2015_2mb_blocksize\n> >\n> > BIP102 is normally a hardfork.  The softfork version (unofficial\n> > codename BIP102s) uses the idea described here:\n> >\n> http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012073.html\n> >\n> > The basic idea is that post-fork blocks are constructed in such a way\n> > they can be mapped to valid blocks under the pre-fork rules.  BIP102s\n> > is a softfork in the sense that post-fork miners are still creating a\n> > valid chain under the old rules, albeit indirectly.\n> >\n> > From the POV of non-upgraded clients, BIP102s circumvents the\n> > block-size limit by moving transaction validation data \"outside\" of\n> > the block.  This is a similar trick used by Segregated Witness and\n> > Extension Blocks (both softfork proposals).\n> >\n> > From the POV of upgraded clients, the block layout is unchanged,\n> > except:\n> > - A larger 2MB block-size limit (=BIP102);\n> > - The header Merkle root has a new (backwards compatible)\n> >  interpretation;\n> > - The coinbase encodes the Merkle root of the remaining txs.\n> > Aside from this, blocks maintain their original format, i.e. a block\n> > header followed by a vector of transactions.  This keeps the\n> > implementation simple, and is distinct from SW and EB.\n> >\n> > Since BIP102s is a softfork it means that:\n> > - A miner majority (e.g. 75%, 95%) force miner consensus (100%).  This\n> >  is not true for a hardfork.\n> > - Fraud risk is significantly reduced (6-conf unlikely depending on\n> >  activation threshold).\n> > This should address some of the concerns with deploying a block-size\n> > increase using a hardfork.\n> >\n> > Notes:\n> >\n> > - The same basic idea could be adapted to any of the other proposals\n> >  (BIP101, 2-4-8, BIP202, etc.).\n> > - I used Jeff Garzik's BIP102 implementation which is incomplete (?).\n> >  The activation logic is left unchanged.\n> > - I am not a Bitcoin dev so hopefully no embarrassing mistakes in my\n> >  code :-(\n> >\n> > --joe\n> >\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/5c2838ea/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-12-30T14:19:55",
                "message_text_only": "On Wed, Dec 30, 2015 at 05:29:05AM -0800, Jonathan Toomim via bitcoin-dev wrote:\n> As a first impression, I think this proposal is intellectually interesting, but crufty and hackish and should never actually be deployed. Writing code for Bitcoin in a future in which we have deployed a few generalized softforks this way sounds terrifying.\n\n<snip>\n\n> It might be possible to make that a bit simpler with recursion, or by doing subsequent generalized softforks in a way that doesn't have multi-levels-deep block-within-a-block-within-a-block stuff. Still: ugh.\n\nYour fear is misplaced: it's trivial to avoid recursion with a bit of\nplanning.\n\nFor instance, if Bitcoin was redesigned to incorporate the forced fork\nconcept, instead of block headers committing to just a merkle root,\nthey could instead commit to H(version + digest)\n\nFor version == 0, digest would be a merkle root of all transactions. If\nthe version was > 0, any digest would be allowed and the block would be\ninterpreted as a NOP with no effect on the UTXO set.\n\nIn the event of a major change - e.g. what would otherwise be a\nhard-forking change to the way the merkle root was calculated - a\nsoft-fork would change the block validity rules to make version == 0\ninvalid, and verison == 1 blocks would interpret the digest according to\nthe new merkle root rules. Again, version > 1 blocks would be treated as\nNOPs.\n\nA good exercise is to apply the above to the existing Bitcoin ecosystem\nas a soft-fork - it certainely can be done, and done right is\ntechnically very simple.\n\n\nRegardless of how it's done - existing Bitcoin compatible or clean sheet\nredesign - you get the significant safety advantages soft-forks have\nover hard-forks in nearly all situations where you'd have to do a\nhard-fork. OTOH, it's kinda scary how this institutionalizes what could\nbe seen as 51% attacks, possibly giving miners significantly more\ncontrol over the system politically. I'm not sure I agree with that\nviewpoint - miners can do this anyway - but that has made people shy\naway from promoting this idea in the past. (previously it's been often\nreferred to as an \"evil\" soft-fork)\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000831fc2554d9370aeba2701fff09980123d24a615eee7416\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/38a95928/attachment.sig>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-12-30T14:31:37",
                "message_text_only": "On Wed, Dec 30, 2015 at 06:19:55AM -0800, Peter Todd via bitcoin-dev wrote:\n> On Wed, Dec 30, 2015 at 05:29:05AM -0800, Jonathan Toomim via bitcoin-dev wrote:\n> > As a first impression, I think this proposal is intellectually interesting, but crufty and hackish and should never actually be deployed. Writing code for Bitcoin in a future in which we have deployed a few generalized softforks this way sounds terrifying.\n> \n> <snip>\n> \n> > It might be possible to make that a bit simpler with recursion, or by doing subsequent generalized softforks in a way that doesn't have multi-levels-deep block-within-a-block-within-a-block stuff. Still: ugh.\n> \n> Your fear is misplaced: it's trivial to avoid recursion with a bit of\n> planning.\n> \n> For instance, if Bitcoin was redesigned to incorporate the forced fork\n\nActually, a better name is probably \"forced soft-fork\", making this\nclear we're using the soft-fork mechanism to force everyone to upgrade.\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000831fc2554d9370aeba2701fff09980123d24a615eee7416\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/a075a180/attachment-0001.sig>"
            },
            {
                "author": "Jonathan Toomim",
                "date": "2015-12-30T15:00:16",
                "message_text_only": "On Dec 30, 2015, at 6:19 AM, Peter Todd <pete at petertodd.org> wrote:\n\n> Your fear is misplaced: it's trivial to avoid recursion with a bit of\n> planning...\n\nThat makes some sense. I downgrade my emotions from \"a future in which we have deployed a few generalized softforks this way sounds terrifying\" to \"the idea of a future in which we have deployed at least one generalized softfork this way gives me the heebie jeebies.\"\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/5c219060/attachment.sig>"
            },
            {
                "author": "Martijn Meijering",
                "date": "2015-12-30T11:16:22",
                "message_text_only": "That looks very interesting. But is effectively blocking old clients from\nseeing transactions really safe? After all, such transactions are still\nconfirmed on the new chain. A person might try to send a similar\ntransaction several times, perhaps with increasing fees in an attempt to\nget it to confirm and end up paying someone several times.\n\nMaybe we could require the tx version number to be increased as well so\ntransactions sent from old clients would never confirm? Perhaps your code\nalready includes this idea, I need to look at it more closely.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/6a96deaf/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-12-30T14:28:37",
                "message_text_only": "On Wed, Dec 30, 2015 at 12:16:22PM +0100, Martijn Meijering via bitcoin-dev wrote:\n> That looks very interesting. But is effectively blocking old clients from\n> seeing transactions really safe? After all, such transactions are still\n> confirmed on the new chain. A person might try to send a similar\n> transaction several times, perhaps with increasing fees in an attempt to\n> get it to confirm and end up paying someone several times.\n\nIt's very dangerous to simply send multiple transactions in such a way\nthat they don't double-spend each other; you have no good way of knowing\nfor sure that you're seeing the longest block chain with software alone.\n\nCompetently designed software with fee-bumping wouldn't allow that\nmistake to be made; the UX should make it clear that txs sent are still\npending until confirmed or clearly double-spent.\n\n> Maybe we could require the tx version number to be increased as well so\n> transactions sent from old clients would never confirm? Perhaps your code\n> already includes this idea, I need to look at it more closely.\n\nThat can mess up pre-signed transations, e.g. refunds.\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000831fc2554d9370aeba2701fff09980123d24a615eee7416\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/43ec3a9c/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "An implementation of BIP102 as a softfork.",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Peter Todd",
                "joe2015 at openmailbox.org",
                "Marcel Jamin",
                "Jonathan Toomim",
                "Martijn Meijering",
                "Marco Falke"
            ],
            "messages_count": 10,
            "total_messages_chars_count": 18517
        }
    },
    {
        "title": "[bitcoin-dev]  [BIP Draft] Decentralized Improvement Proposals",
        "thread_messages": [
            {
                "author": "Tomas",
                "date": "2015-12-30T16:35:17",
                "message_text_only": "In an attempt to reduce developer centralization, and to reduce the risk\nof forks introduced by implementation other than bitcoin-core, I have\ndrafted a BIP to support changes to the protocol from different\nimplementations.\n\nThe BIP can be found at:\nhttps://github.com/tomasvdw/bips/blob/master/decentralized-improvement-proposals.mediawiki\n\nI believe this BIP could mitigate the risk of forks, and decentralize\nthe development of the protocol.\n\nIf you consider the proposal worthy of discussion, please assign a\nBIP-number.\n\nRegards,\nTomas van der Wansem"
            }
        ],
        "thread_summary": {
            "title": "Decentralized Improvement Proposals",
            "categories": [
                "bitcoin-dev",
                "BIP Draft"
            ],
            "authors": [
                "Tomas"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 555
        }
    },
    {
        "title": "[bitcoin-dev] [BIP Draft] Decentralized Improvement Proposals",
        "thread_messages": [
            {
                "author": "Luke Dashjr",
                "date": "2015-12-30T17:10:25",
                "message_text_only": "On Wednesday, December 30, 2015 4:35:17 PM Tomas via bitcoin-dev wrote:\n> In an attempt to reduce developer centralization, and to reduce the risk\n> of forks introduced by implementation other than bitcoin-core, I have\n> drafted a BIP to support changes to the protocol from different\n> implementations.\n\nThe premises in Motivation are false. BIPs are required to have a reference \nimplementation, but that implementation need not necessarily be for Bitcoin \nCore specifically.\n\nThe specification itself looks like an inefficient and bloaty reinvention of \nversion bits.\n\nLuke"
            },
            {
                "author": "Tomas",
                "date": "2015-12-30T18:22:59",
                "message_text_only": "> The specification itself looks like an inefficient and bloaty reinvention\n> of \n> version bits.\n\nThe actual assignment of version bits isn't clear from the\nspecification. Are you saying that any implementation that wants to\npropose a change is encouraged to pick a free version bit and use it?\n\nFurthermore, my proposal addresses the danger of forward-incompatible\nchanges; a hard-fork can no longer occur as every implementation will\nagree on the active the set of rules even if it has not implemented\nthem. This seems to be lacking in the version bits proposal.\n\nTomas"
            },
            {
                "author": "Luke Dashjr",
                "date": "2015-12-30T23:47:16",
                "message_text_only": "On Wednesday, December 30, 2015 6:22:59 PM Tomas wrote:\n> > The specification itself looks like an inefficient and bloaty reinvention\n> > of version bits.\n> \n> The actual assignment of version bits isn't clear from the\n> specification. Are you saying that any implementation that wants to\n> propose a change is encouraged to pick a free version bit and use it?\n\nThat should probably be clarified in the BIP, I agree. Perhaps it ought to be \nassigned the same as BIP numbers themselves, by the BIP editor? (Although as a \nlimited resource, maybe that's not the best solution.)\n\n> Furthermore, my proposal addresses the danger of forward-incompatible\n> changes; a hard-fork can no longer occur as every implementation will\n> agree on the active the set of rules even if it has not implemented\n> them. This seems to be lacking in the version bits proposal.\n\nI don't think that's possible. First of all, a hardfork can always occur, \nsince this is something done by the economy and not (even possibly opposed to) \nminers. Furthermore, consider the change affecting how further rule changes \nare made, such as a PoW algorithm change.\n\nLuke"
            }
        ],
        "thread_summary": {
            "title": "Decentralized Improvement Proposals",
            "categories": [
                "bitcoin-dev",
                "BIP Draft"
            ],
            "authors": [
                "Tomas",
                "Luke Dashjr"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 2282
        }
    },
    {
        "title": "[bitcoin-dev] BIP numbers",
        "thread_messages": [
            {
                "author": "Marco Pontello",
                "date": "2015-12-30T16:42:47",
                "message_text_only": "Sorry to ask again but... what's up with the BIP number assignments?\nI thought that it was just more or less a formality, to avoid conflicts and\nBIP spamming. And that would be perfectly fine.\nBut since I see that it's a process that can take months (just looking at\nthe PR request list), it seems that something different is going on. Maybe\nit's considered something that give an aura of officiality of sorts? But\nthat would make little sense, since that should come eventually with\nsubsequents steps (like adding a BIP to the main repo, and eventual\napprovation).\n\nHaving # 333 assigned to a BIP, should just mean that's easy to refer to a\nparticular BIP.\nThat seems something that could be done quick and easily.\n\nWhat I'm missing? Probably some historic context?\nThanks!\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/e6b4259e/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-12-31T23:14:40",
                "message_text_only": "On Wed, Dec 30, 2015 at 05:42:47PM +0100, Marco Pontello via bitcoin-dev wrote:\n> Sorry to ask again but... what's up with the BIP number assignments?\n> I thought that it was just more or less a formality, to avoid conflicts and\n> BIP spamming. And that would be perfectly fine.\n> But since I see that it's a process that can take months (just looking at\n> the PR request list), it seems that something different is going on. Maybe\n> it's considered something that give an aura of officiality of sorts? But\n> that would make little sense, since that should come eventually with\n> subsequents steps (like adding a BIP to the main repo, and eventual\n> approvation).\n> \n> Having # 333 assigned to a BIP, should just mean that's easy to refer to a\n> particular BIP.\n> That seems something that could be done quick and easily.\n> \n> What I'm missing? Probably some historic context?\n\nYou ever noticed how actually getting a BIP # assigned is the *last*\nthing the better known Bitcoin Core devs do? For instance, look at the\nsegregated witness draft BIPs.\n\nI think we have problem with peoples' understanding of the Bitcoin\nconsensus protocol development process being backwards: first write your\nprotocol specification - the code - and then write the human readable\nreference explaining it - the BIP.\n\nEqually, without people actually using that protocol, who cares about\nthe BIP?\n\n\nPersonally if I were assigning BIP numbers I'd be inclined to say \"fuck\nit\" and only assign BIP numbers to BIPs after they've had significant\nadoption... It'd might just cause a lot less headache than the current\nsystem.\n\n-- \n'peter'[:-1]@petertodd.org\n000000000000000006808135a221edd19be6b5b966c4621c41004d3d719d18b7\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151231/4b62cfd2/attachment.sig>"
            },
            {
                "author": "Adrian Macneil",
                "date": "2015-12-31T23:30:02",
                "message_text_only": "I'm not sure if anyone has suggested this in the past, but a novel approach\nwould be to simply let anyone open a pull request and use the PR # as the\nBIP #. This would avoid conflicts, and avoid the chore of having someone\nmanually assign them.\n\nDownside would be that some numbers will never get used (for example if PRs\nare opened to update existing BIPs), but this doesn't seem to be a huge\nproblem since already many numbers are going unused.\n\nThis process can still be independent from approving/merging the BIP into\nmaster, if it meets quality standards.\nOn Thu, Dec 31, 2015 at 3:14 PM Peter Todd via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Wed, Dec 30, 2015 at 05:42:47PM +0100, Marco Pontello via bitcoin-dev\n> wrote:\n> > Sorry to ask again but... what's up with the BIP number assignments?\n> > I thought that it was just more or less a formality, to avoid conflicts\n> and\n> > BIP spamming. And that would be perfectly fine.\n> > But since I see that it's a process that can take months (just looking at\n> > the PR request list), it seems that something different is going on.\n> Maybe\n> > it's considered something that give an aura of officiality of sorts? But\n> > that would make little sense, since that should come eventually with\n> > subsequents steps (like adding a BIP to the main repo, and eventual\n> > approvation).\n> >\n> > Having # 333 assigned to a BIP, should just mean that's easy to refer to\n> a\n> > particular BIP.\n> > That seems something that could be done quick and easily.\n> >\n> > What I'm missing? Probably some historic context?\n>\n> You ever noticed how actually getting a BIP # assigned is the *last*\n> thing the better known Bitcoin Core devs do? For instance, look at the\n> segregated witness draft BIPs.\n>\n> I think we have problem with peoples' understanding of the Bitcoin\n> consensus protocol development process being backwards: first write your\n> protocol specification - the code - and then write the human readable\n> reference explaining it - the BIP.\n>\n> Equally, without people actually using that protocol, who cares about\n> the BIP?\n>\n>\n> Personally if I were assigning BIP numbers I'd be inclined to say \"fuck\n> it\" and only assign BIP numbers to BIPs after they've had significant\n> adoption... It'd might just cause a lot less headache than the current\n> system.\n>\n> --\n> 'peter'[:-1]@petertodd.org\n> 000000000000000006808135a221edd19be6b5b966c4621c41004d3d719d18b7\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151231/5214ee14/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "BIP numbers",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Marco Pontello",
                "Adrian Macneil",
                "Peter Todd"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 5739
        }
    },
    {
        "title": "[bitcoin-dev] Generalized soft forks",
        "thread_messages": [
            {
                "author": "David Chan",
                "date": "2015-12-30T17:58:44",
                "message_text_only": "Please forgive the perhaps pedantic question but in the referred document\nhttp://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012073.html\nIt talks about how a soft fork with >50% support will doom the other fork to being orphaned eventually but that hard forks could persist forever.  I fail to see why the same logic that one side having the majority will eventually win wouldn't also apply to hard forks.  \n\nAdditionally it seems to me that a notable difference between a generalized soft fork as described and a hard fork majority is in the process by which they force the other fork to be orphaned. In a hard fork an unupgraded node would know you were in a forking situation due to your node getting a lot of blocks from the other fork and having to reject them (because they are invalid) whereas in a generalized soft fork you wouldn't know there was a fork going on so there would be less of an impetus to upgrade.  Of course the downside of the hard fork is that the losing side would potentially lose money in the orphaned chain, but presumably this discussion of generalized soft forks is with regards to non-mining nodes so it shouldn't come into consideration. \n\nIn fact if an non-upgraded miner were to start mining on top of that block which they cannot actually fully validate essentially this condones mining without verification (and trusting that others which have upgraded nodes to have validated the txns for you) as this situation can continue for a prolonged period of time does this not hurt network security ?\n\n\n\n>> On 2015/12/31, at 1:27, joe2015--- via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> \n>> On 2015-12-30 18:33, Marco Falke wrote:\n>> This is an interesting approach but I don't see how this is a soft\n>> fork. (Just because something is not a hard fork, doesn't make it a\n>> soft fork by definition)\n>> Softforks don't require any nodes to upgrade. [1]\n>> Nonetheless, as I understand your approach, it requires nodes to\n>> upgrade. Otherwise they are missing all transactions but the coinbase\n>> transactions. Thus, they cannot update their utxoset and are easily\n>> susceptible to double spends...\n>> Am I missing something obvious?\n>> -- Marco\n>> [1] https://en.bitcoin.it/wiki/Softfork#Implications\n> \n> It just depends how you define \"softfork\".  In my original write-up I called it a \"generalized\" softfork, Peter suggested a \"firm\" fork, and there are some suggestions for other names.  Ultimately what you call it is not very important.\n> \n> --joe.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151231/4526c922/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Generalized soft forks",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "David Chan"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2903
        }
    },
    {
        "title": "[bitcoin-dev] How to preserve the value of coins after a fork.",
        "thread_messages": [
            {
                "author": "Emin G\u00fcn Sirer",
                "date": "2015-12-30T20:08:36",
                "message_text_only": "Ittay Eyal and I just put together a writeup that we're informally calling\nBitcoin-United for preserving the value of coins following a permanent fork:\n\n\nhttp://hackingdistributed.com/2015/12/30/technique-to-unite-bitcoin-factions/\n\nHalf of the core idea is to eliminate double-spends (where someone spends a\nUTXO on chain A and the same UTXO on chain B, at separate merchants) by\nplacing transactions from A on chain B, and by taking the intersection of\ntransactions on chain A and chain B when considering whether a payment has\nbeen received.\n\nThe other half of the core idea is to enable minting of new coins and\ncollection of mining fees on both chains, while preserving the 21M maximum.\nThis is achieved by creating a one-to-one correspondence between coins on\none chain with coins on the other.\n\nGiven the level of the audience here, I'm keeping the description quite\nterse. Much more detail and discussion is at the link above, as well as the\nassumptions that need to hold for Bitcoin-United.\n\nThe high bit is that, with a few modest assumptions, it is possible to\ncreate a cohesive coin in the aftermath of a fork, even if the core devs\nare split, and even if one of the forks is (in the worst case) completely\nnon-cooperative. Bitcoin-United is a trick to create a cohesive coin even\nwhen there is no consensus at the lowest level.\n\nBitcoin-United opens up a lot of new, mostly game-theoretic questions: what\nhappens to native clients who prefer A or B? What will happen to the value\nof native-A or native-B coins? And so on.\n\nWe're actively working on these questions and more, but we wanted to share\nthe Bitcoin-United idea, mainly to receive feedback, and partly to provide\nsome hope about future consensus to the community. It turns out that it is\npossible to craft consensus at the network level even when there isn't one\nat the developer level.\n\nHappy New Year, and may 2016 be united,\n- egs & ittay\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/4b322fe8/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-12-30T20:16:12",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA512\n\nNote how transaction malleability can quickly sabotage naive notions of this idea.\n\nEqually, if this looks like it might ever be implemented, rather than using a hard fork, using a forced soft-fork to deploy changes becomes attractive.\n\n\nOn 30 December 2015 12:08:36 GMT-08:00, \"Emin G\u00fcn Sirer via bitcoin-dev\" <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>Ittay Eyal and I just put together a writeup that we're informally\n>calling\n>Bitcoin-United for preserving the value of coins following a permanent\n>fork:\n>\n>\n>http://hackingdistributed.com/2015/12/30/technique-to-unite-bitcoin-factions/\n>\n>Half of the core idea is to eliminate double-spends (where someone\n>spends a\n>UTXO on chain A and the same UTXO on chain B, at separate merchants) by\n>placing transactions from A on chain B, and by taking the intersection\n>of\n>transactions on chain A and chain B when considering whether a payment\n>has\n>been received.\n>\n>The other half of the core idea is to enable minting of new coins and\n>collection of mining fees on both chains, while preserving the 21M\n>maximum.\n>This is achieved by creating a one-to-one correspondence between coins\n>on\n>one chain with coins on the other.\n>\n>Given the level of the audience here, I'm keeping the description quite\n>terse. Much more detail and discussion is at the link above, as well as\n>the\n>assumptions that need to hold for Bitcoin-United.\n>\n>The high bit is that, with a few modest assumptions, it is possible to\n>create a cohesive coin in the aftermath of a fork, even if the core\n>devs\n>are split, and even if one of the forks is (in the worst case)\n>completely\n>non-cooperative. Bitcoin-United is a trick to create a cohesive coin\n>even\n>when there is no consensus at the lowest level.\n>\n>Bitcoin-United opens up a lot of new, mostly game-theoretic questions:\n>what\n>happens to native clients who prefer A or B? What will happen to the\n>value\n>of native-A or native-B coins? And so on.\n>\n>We're actively working on these questions and more, but we wanted to\n>share\n>the Bitcoin-United idea, mainly to receive feedback, and partly to\n>provide\n>some hope about future consensus to the community. It turns out that it\n>is\n>possible to craft consensus at the network level even when there isn't\n>one\n>at the developer level.\n>\n>Happy New Year, and may 2016 be united,\n>- egs & ittay\n>\n>\n>------------------------------------------------------------------------\n>\n>_______________________________________________\n>bitcoin-dev mailing list\n>bitcoin-dev at lists.linuxfoundation.org\n>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n- --\nSent from my Android device with K-9 Mail. Please excuse my brevity.\n-----BEGIN PGP SIGNATURE-----\n\niQE9BAEBCgAnIBxQZXRlciBUb2RkIDxwZXRlQHBldGVydG9kZC5vcmc+BQJWhDuA\nAAoJEMCF8hzn9Lncz4MIAIObFNbRRJ5g52H8yprqAjX76Lt7vw+cwCnICNzHra5h\niuTWxgbwED5fki2Q96ZzYAyUf7ju7rI45qBl8YuuVUlyxJgE6oV6h2oJoxGQNGz0\nWvrOjWMkmARNs0FM4GMsKQWcmIMgZxWnWTMOXv0EDBLySsm8WFRu9H4drGBB+Fmb\nwFRyi0XVDiXxsVUoNj6pCdcpekdnuq+V87IoweoxigfqgWIM31Vb9QK8Y/7vWO2b\n0lu0CvVdqvw5Npx55LWLF1tY8jbw6BYvgXwZGtUazKO+x8i3Qt6+tRm07+UXvkoR\n3erxzhnoZa3F66ufz+ImY7l0E/AyRE5ox+1W68hO6sk=\n=d0+L\n-----END PGP SIGNATURE-----"
            },
            {
                "author": "Emin G\u00fcn Sirer",
                "date": "2015-12-30T20:22:43",
                "message_text_only": "On Wed, Dec 30, 2015 at 3:16 PM, Peter Todd <pete at petertodd.org> wrote:\n\n> Note how transaction malleability can quickly sabotage naive notions of\n> this idea.\n>\n\nBitcoin-United relies on a notion of transaction equivalence that doesn't\ninvolve the transaction hash at all, so it should be immune to malleability\nissues and compatible with segwit.\n\n>From the post, two transactions are equal if they \"consume the same inputs\nand result in the same outputs, not counting the miner fee. Simple\npay-to-pubkey-hash and pay-to-script-hash transactions are straightforward.\nMultikey transactions are evaluated for equivalency by their inputs and\noutputs, so it is allowable for a 2-out-of-3 payment to be signed by one\nset of two keys on Dum and another set of two keys on Dee, as long as the\ntransaction consumes the same coins and produces the same outputs. Not that\nwe'll ever encounter such a case, but making this point helps pedagogically\nwith getting across the notion of transaction equivalence. What counts are\nthe consumed inputs and the destination and amounts of the outputs.\"\n\nBut you're right, if a naive implementation were to just use the\ntransaction hash, the result would be a mess.\n\n- egs\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/1672f2d0/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-12-30T20:32:43",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA512\n\n\n\nOn 30 December 2015 12:22:43 GMT-08:00, \"Emin G\u00fcn Sirer\" <el33th4x0r at gmail.com> wrote:\n>On Wed, Dec 30, 2015 at 3:16 PM, Peter Todd <pete at petertodd.org> wrote:\n>\n>> Note how transaction malleability can quickly sabotage naive notions\n>of\n>> this idea.\n>>\n>\n>Bitcoin-United relies on a notion of transaction equivalence that\n>doesn't\n>involve the transaction hash at all, so it should be immune to\n>malleability\n>issues and compatible with segwit.\n>\n>From the post, two transactions are equal if they \"consume the same\n>inputs\n>and result in the same outputs, not counting the miner fee. Simple\n>pay-to-pubkey-hash and pay-to-script-hash transactions are\n>straightforward.\n>Multikey transactions are evaluated for equivalency by their inputs and\n>outputs, so it is allowable for a 2-out-of-3 payment to be signed by\n>one\n>set of two keys on Dum and another set of two keys on Dee, as long as\n>the\n>transaction consumes the same coins and produces the same outputs. Not\n>that\n>we'll ever encounter such a case, but making this point helps\n>pedagogically\n>with getting across the notion of transaction equivalence. What counts\n>are\n>the consumed inputs and the destination and amounts of the outputs.\"\n\nYou seem to not be familiar with how multisig transactions on Bitcoin work - 99.9% of the time theyre hidden behind p2sh and there is no way to know what keys are involved. Equally, multisig is just one of many complex scripts possible.\n\nLook into what a segwit transaction hashes - that's a better notion of non-malleable transaction. But even then lots of transactions are malleable, and its easy to trigger those cases intentionally by third parties.\n\nMost likely any Bitcoin United scheme would quickly diverge and fail; much simpler and more predictable to achieve convincing consensus, e.g. via proof of stake voting, or Adam Bank's extension blocks suggestions. (or of course, not trying to force controversial forks in the first place)\n\n-----BEGIN PGP SIGNATURE-----\n\niQE9BAEBCgAnIBxQZXRlciBUb2RkIDxwZXRlQHBldGVydG9kZC5vcmc+BQJWhD9N\nAAoJEMCF8hzn9Lncz4MH/0JPGVc2JLtD5q0I2w0vqmBqsoSzSueCtnKa2K1Ea10g\nw9I4uhK7+cgfCLbofJznVHMChXu0uCxtWwqSj++uJx238TEupcu951gUhFfuPOeH\nEgye8jmDkDFiB1P40kUSVk9N64Zt3kWLk4xSsfjawVHz/WWpM24Fn8k/bmI7JiLl\nnmVwoBdRsTKffM/1dr8ix4U8YPSmJ7W+jAByNHUpSgc1R73YylqNT95pF8QD35df\ndQwSK9DIc+2N4CKnp22xLvYeCivFjeS2Fm4kbcKQwMjcqlJ1mWghP/c8q/lzhaGN\nAc15/pgeHp8dPP8c81zkN9ps14rrnXoHnrzjiY+TwKY=\n=FfK1\n-----END PGP SIGNATURE-----"
            },
            {
                "author": "Nick ODell",
                "date": "2015-12-30T23:13:56",
                "message_text_only": "Emin,\n\nI have two technical criticisms of your proposal, and one economic criticism.\n\n>Unified miners would make sure that they mine transactions on Dum first, then on Dee. Recall that Dee miners can always just take Dum transactions and stick them in their blocks.\nThis seems to contradict a later section that says that users can use\nDee natively, without paying fees necessary to get a transaction into\nDum. You can't have this both ways - either you can get a transaction\ninto Dee without getting it into Dum first, or you can't.\n\n>Such an attack would be quite visible, and it would leave Dum vulnerable. Unified clients could counter this launching a 51% counterattack on Dum.\nWhat if some other group that wants to hurt both Dum and Dee were to\nmake a false-flag attack against Dee? Mutually assured destruction\ndoesn't work if you can't accurately attribute attacks.\n\n>This would create some gentle pressure to explicitly unify the two chains (by merging Dee and Dum at some compromise and doing away with Unified)\nI don't think a compromise would be reachable at that point - suppose\none had a market cap of 1.2 billion, and the other had a market cap of\n0.8 billion. How would the coins on the unified chain be distributed?\nYou could give each chain an equal number of coins, but that's not\nfair to the people holding the more valuable coins. You could give\neach chain a number of coins proportional to the market cap, but that\ninvites price manipulation. In any case, if you had a way of reaching\ncompromise, why not use it instead of creating two chains?\n\n\nOverall, I think this proposal is a bad idea.\n\n\n>You seem to not be familiar with how multisig transactions on Bitcoin work - 99.9% of the time theyre hidden behind p2sh and there is no way to know what keys are involved. Equally, multisig is just one of many complex scripts possible.\nThat doesn't end up mattering, though, as I understand his proposal.\nThe unified client would just see that both validly spend an output\nwith a scriptPubKey of OP_HASH160 0xabcdef... OP_EQUAL.\n\nOn Wed, Dec 30, 2015 at 1:32 PM, Peter Todd via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> -----BEGIN PGP SIGNED MESSAGE-----\n> Hash: SHA512\n>\n>\n>\n> On 30 December 2015 12:22:43 GMT-08:00, \"Emin G\u00fcn Sirer\" <el33th4x0r at gmail.com> wrote:\n>>On Wed, Dec 30, 2015 at 3:16 PM, Peter Todd <pete at petertodd.org> wrote:\n>>\n>>> Note how transaction malleability can quickly sabotage naive notions\n>>of\n>>> this idea.\n>>>\n>>\n>>Bitcoin-United relies on a notion of transaction equivalence that\n>>doesn't\n>>involve the transaction hash at all, so it should be immune to\n>>malleability\n>>issues and compatible with segwit.\n>>\n> >From the post, two transactions are equal if they \"consume the same\n>>inputs\n>>and result in the same outputs, not counting the miner fee. Simple\n>>pay-to-pubkey-hash and pay-to-script-hash transactions are\n>>straightforward.\n>>Multikey transactions are evaluated for equivalency by their inputs and\n>>outputs, so it is allowable for a 2-out-of-3 payment to be signed by\n>>one\n>>set of two keys on Dum and another set of two keys on Dee, as long as\n>>the\n>>transaction consumes the same coins and produces the same outputs. Not\n>>that\n>>we'll ever encounter such a case, but making this point helps\n>>pedagogically\n>>with getting across the notion of transaction equivalence. What counts\n>>are\n>>the consumed inputs and the destination and amounts of the outputs.\"\n>\n> You seem to not be familiar with how multisig transactions on Bitcoin work - 99.9% of the time theyre hidden behind p2sh and there is no way to know what keys are involved. Equally, multisig is just one of many complex scripts possible.\n>\n> Look into what a segwit transaction hashes - that's a better notion of non-malleable transaction. But even then lots of transactions are malleable, and its easy to trigger those cases intentionally by third parties.\n>\n> Most likely any Bitcoin United scheme would quickly diverge and fail; much simpler and more predictable to achieve convincing consensus, e.g. via proof of stake voting, or Adam Bank's extension blocks suggestions. (or of course, not trying to force controversial forks in the first place)\n>\n> -----BEGIN PGP SIGNATURE-----\n>\n> iQE9BAEBCgAnIBxQZXRlciBUb2RkIDxwZXRlQHBldGVydG9kZC5vcmc+BQJWhD9N\n> AAoJEMCF8hzn9Lncz4MH/0JPGVc2JLtD5q0I2w0vqmBqsoSzSueCtnKa2K1Ea10g\n> w9I4uhK7+cgfCLbofJznVHMChXu0uCxtWwqSj++uJx238TEupcu951gUhFfuPOeH\n> Egye8jmDkDFiB1P40kUSVk9N64Zt3kWLk4xSsfjawVHz/WWpM24Fn8k/bmI7JiLl\n> nmVwoBdRsTKffM/1dr8ix4U8YPSmJ7W+jAByNHUpSgc1R73YylqNT95pF8QD35df\n> dQwSK9DIc+2N4CKnp22xLvYeCivFjeS2Fm4kbcKQwMjcqlJ1mWghP/c8q/lzhaGN\n> Ac15/pgeHp8dPP8c81zkN9ps14rrnXoHnrzjiY+TwKY=\n> =FfK1\n> -----END PGP SIGNATURE-----\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "How to preserve the value of coins after a fork.",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Nick ODell",
                "Emin G\u00fcn Sirer",
                "Peter Todd"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 14119
        }
    },
    {
        "title": "[bitcoin-dev] fork types (Re: An implementation of BIP102 as a softfork.)",
        "thread_messages": [
            {
                "author": "Adam Back",
                "date": "2015-12-30T23:05:57",
                "message_text_only": "> I guess the same could be said about the softfork flavoured SW implementation\n\nNo, segregated witness\nhttps://bitcoin.org/en/bitcoin-core/capacity-increases-faq is a\nsoft-fork maybe loosely similar to P2SH - particularly it is backwards\nand forwards compatible by design.\n\nThese firm forks have the advantage over hard forks that there is no\nleft-over weak chain that is at risk of losing money (because it\nbecomes a consensus rule that old transactions are blocked).\n\nThere is also another type of fork a firm hard fork that can do the\nsame but for format changes that are not possible with a soft-fork.\n\nExtension blocks show a more general backwards and forwards compatible\nsoft-fork is also possible.\nSegregated witness is simpler.\n\nAdam\n\nOn 30 December 2015 at 13:57, Marcel Jamin via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> I guess the same could be said about the softfork flavoured SW\n> implementation. In any case, the strategy pattern helps with code structure\n> in situations like this.\n>\n> 2015-12-30 14:29 GMT+01:00 Jonathan Toomim via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org>:"
            },
            {
                "author": "Bryan Bishop",
                "date": "2015-12-30T23:10:05",
                "message_text_only": "On Wed, Dec 30, 2015 at 5:05 PM, Adam Back via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> There is also another type of fork a firm hard fork that can do the\n> same but for format changes that are not possible with a soft-fork.\n>\n\nI was drafting an email for a new thread with some links about this topic,\ninstead I'll just send this as a reply now that we are writing down fork\ntypes...\n\nauxiliary blocks and evil soft-forks or forced soft-forks:\nhttps://bitcointalk.org/index.php?topic=283746.0\nhttps://bitcointalk.org/index.php?topic=874313.0\n\nsoft-fork block size increase using extension blocks:\nhttp://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-May/008356.html\n\ngeneralized soft-forks:\nhttp://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012073.html\n\nbip102 forced soft-fork:\nhttp://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012153.html\n\nextension blocks were also discussed in this interview:\nhttp://diyhpl.us/wiki/transcripts/bitcoin-sidechains-unchained-epicenter-adam3us-gmaxwell/\n.... also there was something about a \"soft-hard fork\".\n\nsome discussion from today re: origin of the term evil fork, evil\nsoft-fork, forced soft-fork:\nhttps://www.reddit.com/r/Bitcoin/comments/3yrsxt/bitcoindev_an_implementation_of_bip102_as_a/cyg2g7q\n\nsome much older discussion about extension blocks and sidechains:\nhttp://gnusha.org/bitcoin-wizards/2015-01-01.log\n\nsome discussion about \"generalized soft-forks\" and extension blocks and\nevil soft-forks:\nhttp://gnusha.org/bitcoin-wizards/2015-12-20.log\n\nsome discussion about evil forks and evil soft-forks and extension blocks:\nhttp://gnusha.org/bitcoin-wizards/2015-12-30.log\n\nsegwit soft-fork makes use of a similar idea:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/011865.html\nhttps://bitcoin.org/en/bitcoin-core/capacity-increases-faq\nhttp://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/segregated-witness-and-its-impact-on-scalability/\n\nNote: I am taking the term \"forced soft-fork\" from petertodd; it's pretty\nmuch the same thing as \"evil fork\" in every way but intent.\n\nThis is an x-post from\nhttps://bitcointalk.org/index.php?topic=1296628.msg13400092#msg13400092\n\n- Bryan\nhttp://heybryan.org/\n1 512 203 0507\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/7041b5c4/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "fork types (Re: An implementation of BIP102 as a softfork.)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Adam Back",
                "Bryan Bishop"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 3581
        }
    }
]