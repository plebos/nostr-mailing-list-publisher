[
    {
        "title": "[bitcoin-dev] Simple lock/unlock mechanism",
        "thread_messages": [
            {
                "author": "\u30a2\u30eb\u30e0\u3000\u30ab\u30fc\u30eb\u30e8\u30cf\u30f3",
                "date": "2018-03-01T05:11:54",
                "message_text_only": "On Wed, Feb 28, 2018 at 10:30 PM, Anthony Towns <aj at erisian.com.au> wrote:\n> On Wed, Feb 28, 2018 at 04:34:18AM +0000, \u30a2\u30eb\u30e0 \u30ab\u30fc\u30eb\u30e8\u30cf\u30f3 via bitcoin-dev wrote:\n>> 1. Graftroot probably breaks this (someone could just sign the\n>> time-locked output with a script that has no time-lock).\n>\n> Making the graftroot key be a 2-of-2 muSig with an independent third party\n> that commits to only signing CLTV scripts could avoid this. Making it\n> 3-of-3 or 5-of-5 could be even better if you can find multiple independent\n> services that will do it.\n\nThat kind of defeats the purpose. If you go through the trouble of\ndoing that, you can just do multisig and skip the freezing part\nentirely. A robber would have to get you and the cosigner to sign in\nboth cases, and the CLTV could be overridden with graftroot.\n\nOn Wed, Feb 28, 2018 at 11:36 PM, Adam Back <adam.back at gmail.com> wrote:\n> Coincidentally I had thought of something similar to what Kalle posted\n> about a kind of software only time-lock vault, and described the idea\n> to a few people off-list.  Re. Root incompatibility, if the key is\n> deleted (as it must be) then a delegated signature can not be made\n> that bypasses the CSV timeout restriction, so Root should not be\n> incompatible with this.  I think it would be disadvantageous to mark\n> keys as Rootable vs not in a sighash sense, because then that is\n> another privacy/fungibility loss eroding  the uniformity advantage of\n> Root when the delegate is not used.\n\n1. Create TX1=(tx, sig) from UTXO A to p2sh B which has a CSV\ntimelock. Discard privkey A.\n2. After broadcasting TX1, you need privkey B to spend it.\n3. Use graftroot and privkey B with a script without timelock to spend B.\n\nThe robber can simply force you to execute step 3, since you have the\nprivkey to B.\n\n> One drawback is deleting keys may itself be a bit difficult to assure\n> with HD wallet seeds setup-time backup model.\n\nThat's a good point. Even more of a reason to include as part of\n'freezing' a send to a new ephemeral key as 'initialization'. Sucks to\npay triple fees though (freeze ephemeral + unfreeze + actual use).\n\n> As Anthony described I think, a simpler though less robust model would\n> be to have a third party refuse to co-sign until a pre-arranged time,\n> and this would have the advantage of not requiring two on-chain\n> transactions.\n\nI was hoping there was a way for a person to simply lock-up the major\nportion of their coins easily.\n\nAs a sidenote: a security firm (e.g. one that comes to your house when\nthe alarm goes off) could have a service where seeing an unfreeze\ntransaction which you have told them about without you giving a heads\nup beforehand is equal to alarm going off.\n\n-Kalle."
            },
            {
                "author": "\u30a2\u30eb\u30e0\u3000\u30ab\u30fc\u30eb\u30e8\u30cf\u30f3",
                "date": "2018-03-05T14:53:16",
                "message_text_only": "On Thu, Mar 1, 2018 at 5:11 AM, \u30a2\u30eb\u30e0\u3000\u30ab\u30fc\u30eb\u30e8\u30cf\u30f3 <karl at dglab.com> wrote:\n> That kind of defeats the purpose. If you go through the trouble of\n> doing that, you can just do multisig and skip the freezing part\n> entirely. A robber would have to get you and the cosigner to sign in\n> both cases, and the CLTV could be overridden with graftroot.\n\nI think I'm confused on this. To use graftroot it has to be a pubkey,\nnot a p2sh thing.\n\n-Kalle."
            }
        ],
        "thread_summary": {
            "title": "Simple lock/unlock mechanism",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "\u30a2\u30eb\u30e0\u3000\u30ab\u30fc\u30eb\u30e8\u30cf\u30f3"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 3135
        }
    },
    {
        "title": "[bitcoin-dev] Revisiting BIP 125 RBF policy.",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2018-03-01T15:11:29",
                "message_text_only": "On Tue, Feb 27, 2018 at 11:25:59AM -0500, Russell O'Connor wrote:\n> On Mon, Feb 12, 2018 at 6:42 PM, Peter Todd <pete at petertodd.org> wrote:\n> \n> >\n> > Ah ok, I misunderstood and didn't realise you were talking about the case\n> > where\n> > Alice re-spends her unconfirmed payment. Unfortunately I don't think that\n> > case\n> > is possible to solve without putting some kind of restriction on spending\n> > unconfirmed outputs; with a restriction it's fairly simple to solve.\n> \n> \n> When you say that you don't think it is possible to solve, do you mean that\n> there is a specific problem with this proposal of replacing transactions\n> when offered a new transaction whose fee rate exceeds the package fee rate\n> of the original transaction (and ensuring that the fee increase covers the\n> size of the transactions being ejected)?  Is your concern only about the\n> ability to computing and track the package fee rate for transactions within\n> the mempool or is there some other issue you foresee?\n\nI mean, I think in general solving this problem is probably not possible.\nBasically, the fundamental problem is someone else has consumed network\nbandwidth that should be paid for with fees. What you're trying to do is\nreplace a transaction without paying those fees, which is identical to what an\nattacker is trying to do, and thus any such scheme will be as vulnerable to\nattack as not having that protection in the first place.\n\n...which does give you an out: maybe the attack isn't important enough to\nmatter. :)\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 488 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180301/3ef65ba8/attachment-0001.sig>"
            },
            {
                "author": "Russell O'Connor",
                "date": "2018-03-08T15:39:46",
                "message_text_only": "On Thu, Mar 1, 2018 at 10:11 AM, Peter Todd <pete at petertodd.org> wrote:\n\n> On Tue, Feb 27, 2018 at 11:25:59AM -0500, Russell O'Connor wrote:\n> > When you say that you don't think it is possible to solve, do you mean\n> that\n> > there is a specific problem with this proposal of replacing transactions\n> > when offered a new transaction whose fee rate exceeds the package fee\n> rate\n> > of the original transaction (and ensuring that the fee increase covers\n> the\n> > size of the transactions being ejected)?  Is your concern only about the\n> > ability to computing and track the package fee rate for transactions\n> within\n> > the mempool or is there some other issue you foresee?\n>\n> I mean, I think in general solving this problem is probably not possible.\n> Basically, the fundamental problem is someone else has consumed network\n> bandwidth that should be paid for with fees. What you're trying to do is\n> replace a transaction without paying those fees, which is identical to\n> what an\n> attacker is trying to do, and thus any such scheme will be as vulnerable to\n> attack as not having that protection in the first place.\n>\n> ...which does give you an out: maybe the attack isn't important enough to\n> matter. :)\n>\n\nThanks, that makes sense.\n\nI still think it is worthwhile pursuing this proposed change in RBF policy\nas it would seem that the current policy is problematic in practice today\nwhere participants are just performing normal transactions and are not\ntrying to attack each other.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180308/dfce6649/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2018-03-08T18:34:26",
                "message_text_only": "On Thu, Mar 08, 2018 at 10:39:46AM -0500, Russell O'Connor wrote:\n> On Thu, Mar 1, 2018 at 10:11 AM, Peter Todd <pete at petertodd.org> wrote:\n> > I mean, I think in general solving this problem is probably not possible.\n> > Basically, the fundamental problem is someone else has consumed network\n> > bandwidth that should be paid for with fees. What you're trying to do is\n> > replace a transaction without paying those fees, which is identical to\n> > what an\n> > attacker is trying to do, and thus any such scheme will be as vulnerable to\n> > attack as not having that protection in the first place.\n> >\n> > ...which does give you an out: maybe the attack isn't important enough to\n> > matter. :)\n> >\n> \n> Thanks, that makes sense.\n> \n> I still think it is worthwhile pursuing this proposed change in RBF policy\n> as it would seem that the current policy is problematic in practice today\n> where participants are just performing normal transactions and are not\n> trying to attack each other.\n\nBut that's not a good argument: whether or not normal users are trying to\nattack each other has nothing to do with whether or not you're opening up an\nattack by relaxing anti-DoS protections.\n\nEqually, how often are normal users who aren't attacking each other creating\nissues anyway? You can always have your wallet code just skip use of RBF\nreplacements in the event that someone does spend an unconfirmed output that\nyou sent them; how often does this actually happen in practice? Not many\nwallets let you spend unconfirmed outputs that you didn't create.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 614 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180308/dccea518/attachment.sig>"
            },
            {
                "author": "Russell O'Connor",
                "date": "2018-03-08T20:07:43",
                "message_text_only": "On Thu, Mar 8, 2018 at 1:34 PM, Peter Todd <pete at petertodd.org> wrote:\n\n> On Thu, Mar 08, 2018 at 10:39:46AM -0500, Russell O'Connor wrote:\n> > On Thu, Mar 1, 2018 at 10:11 AM, Peter Todd <pete at petertodd.org> wrote:\n> > > I mean, I think in general solving this problem is probably not\n> possible.\n> > > Basically, the fundamental problem is someone else has consumed network\n> > > bandwidth that should be paid for with fees. What you're trying to do\n> is\n> > > replace a transaction without paying those fees, which is identical to\n> > > what an\n> > > attacker is trying to do, and thus any such scheme will be as\n> vulnerable to\n> > > attack as not having that protection in the first place.\n> > >\n> > > ...which does give you an out: maybe the attack isn't important enough\n> to\n> > > matter. :)\n> > >\n> >\n> > Thanks, that makes sense.\n> >\n> > I still think it is worthwhile pursuing this proposed change in RBF\n> policy\n> > as it would seem that the current policy is problematic in practice today\n> > where participants are just performing normal transactions and are not\n> > trying to attack each other.\n>\n> But that's not a good argument: whether or not normal users are trying to\n> attack each other has nothing to do with whether or not you're opening up\n> an\n> attack by relaxing anti-DoS protections.\n>\n\nI'm not suggesting removing the anti-DoS protections.  I'm suggesting that\nreplaced transaction require a fee increase of at least the min-fee-rate\ntimes the size of all the transactions being ejected (in addition to the\nother proposed requirements).\n\n\n> Equally, how often are normal users who aren't attacking each other\n> creating\n> issues anyway? You can always have your wallet code just skip use of RBF\n>\nreplacements in the event that someone does spend an unconfirmed output that\n> you sent them; how often does this actually happen in practice?\n\n\nJust ask rhavar.  It happens regularly.\n\nNot many wallets let you spend unconfirmed outputs that you didn't create.\n>\n\nThe problem is with institutional wallets sweeping incoming payments.  It\nseems that in practice they are happy to sweep unconfirmed outputs.\n\nSetting all of the above aside for a moment.  We need to understand that\nrational miners are going to prefer to transactions with higher package fee\nrates regardless of whatever your personal preferred RBF policy is.  If we\ndo not bring the RBF policy to alignment with what is economically\nrational, then miners are going to change their own policies anyways,\nprobably all in slightly different ways.  It behooves everyone to develop a\nreasonable standard RBF policy, that is still robust against possible DoS\nvectors, and aligns with miner incentives, so that all participants know\nwhat behaviour they can reasonably expect.  It is simply a bonus that this\nchange in RBF policy also partially mitigates the problem of pinned\ntransactions.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180308/9193b079/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2018-03-09T18:28:03",
                "message_text_only": "On Thu, Mar 08, 2018 at 03:07:43PM -0500, Russell O'Connor wrote:\n> On Thu, Mar 8, 2018 at 1:34 PM, Peter Todd <pete at petertodd.org> wrote:\n> > But that's not a good argument: whether or not normal users are trying to\n> > attack each other has nothing to do with whether or not you're opening up\n> > an\n> > attack by relaxing anti-DoS protections.\n> >\n> \n> I'm not suggesting removing the anti-DoS protections.  I'm suggesting that\n> replaced transaction require a fee increase of at least the min-fee-rate\n> times the size of all the transactions being ejected (in addition to the\n> other proposed requirements).\n\nFair: you're not removing them entirely, but you are weakening them compared to\nthe status quo.\n\n> > Equally, how often are normal users who aren't attacking each other\n> > creating\n> > issues anyway? You can always have your wallet code just skip use of RBF\n> >\n> replacements in the event that someone does spend an unconfirmed output that\n> > you sent them; how often does this actually happen in practice?\n> \n> \n> Just ask rhavar.  It happens regularly.\n> \n> Not many wallets let you spend unconfirmed outputs that you didn't create.\n> >\n> \n> The problem is with institutional wallets sweeping incoming payments.  It\n> seems that in practice they are happy to sweep unconfirmed outputs.\n\nPity, that does sound like a problem. :(\n\n> Setting all of the above aside for a moment.  We need to understand that\n> rational miners are going to prefer to transactions with higher package fee\n> rates regardless of whatever your personal preferred RBF policy is.  If we\n> do not bring the RBF policy to alignment with what is economically\n> rational, then miners are going to change their own policies anyways,\n> probably all in slightly different ways.  It behooves everyone to develop a\n> reasonable standard RBF policy, that is still robust against possible DoS\n> vectors, and aligns with miner incentives, so that all participants know\n> what behaviour they can reasonably expect.  It is simply a bonus that this\n> change in RBF policy also partially mitigates the problem of pinned\n> transactions.\n\nMiners and full nodes have slightly different priorities here; it's not clear\nto me why it matters that they implement slightly different policies.\n\n\nStill, re-reading your initital post, I'm convinced that the weakening of the\nDoS protections is probably not a huge problem, so maybe lets try this in a\nrelease and see what happens.\n\nNotably, if people actually use this new replacement behavior, the institutions\ndoing these sweeps of unconfirmed outputs might stop doing that! That's\nprobably a good thing, as respends of potentially conflicted unconfirmed\noutputs can be dangerous in reorgs; we're better off if outputs are buried\ndeeply before being spent again.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 614 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180309/5a952ae2/attachment.sig>"
            },
            {
                "author": "rhavar at protonmail.com",
                "date": "2018-03-09T18:40:34",
                "message_text_only": "> Still, re-reading your initital post, I'm convinced that the weakening of the\n> DoS protections is probably not a huge problem, so maybe lets try this in a\n> release and see what happens.\n\nAwesome! I very much agree. The relaxation of some of these DoS prevention rules I think will really open up a lot of use cases and adoption \n\n> Notably, if people actually use this new replacement behavior, the institutions\n> doing these sweeps of unconfirmed outputs might stop doing that! \n\nAgree, I'm pretty sure it's unintentional. I know a lot of services struggle with coin selection, so what they do is conceptually have a receive wallet from which they can sweep to their hot wallet (or cold storage) to keep their utxo manageable.\n\nCurrently some of them are sweeping unconfirmed inputs with it, but I don't think it's a conscious design choice, just something that happens to be working well now.\n\n(FWIW I observed this behavior like 6+ months ago, I haven't kept track of if it's still happening or how often. But at the time I had to write off the idea of low-fee rbf batch transactions as it was happening too often to be feasible)\n\n\n\u200b-Ryan\u200b\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n\nOn March 9, 2018 1:28 PM, Peter Todd via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> \u200b\u200b\n> \n> On Thu, Mar 08, 2018 at 03:07:43PM -0500, Russell O'Connor wrote:\n> \n> > On Thu, Mar 8, 2018 at 1:34 PM, Peter Todd pete at petertodd.org wrote:\n> > \n> > > But that's not a good argument: whether or not normal users are trying to\n> > > \n> > > attack each other has nothing to do with whether or not you're opening up\n> > > \n> > > an\n> > > \n> > > attack by relaxing anti-DoS protections.\n> > \n> > I'm not suggesting removing the anti-DoS protections. I'm suggesting that\n> > \n> > replaced transaction require a fee increase of at least the min-fee-rate\n> > \n> > times the size of all the transactions being ejected (in addition to the\n> > \n> > other proposed requirements).\n> \n> Fair: you're not removing them entirely, but you are weakening them compared to\n> \n> the status quo.\n> \n> > > Equally, how often are normal users who aren't attacking each other\n> > > \n> > > creating\n> > > \n> > > issues anyway? You can always have your wallet code just skip use of RBF\n> > \n> > replacements in the event that someone does spend an unconfirmed output that\n> > \n> > > you sent them; how often does this actually happen in practice?\n> > \n> > Just ask rhavar. It happens regularly.\n> > \n> > Not many wallets let you spend unconfirmed outputs that you didn't create.\n> > \n> > > \n> > \n> > The problem is with institutional wallets sweeping incoming payments. It\n> > \n> > seems that in practice they are happy to sweep unconfirmed outputs.\n> \n> Pity, that does sound like a problem. :(\n> \n> > Setting all of the above aside for a moment. We need to understand that\n> > \n> > rational miners are going to prefer to transactions with higher package fee\n> > \n> > rates regardless of whatever your personal preferred RBF policy is. If we\n> > \n> > do not bring the RBF policy to alignment with what is economically\n> > \n> > rational, then miners are going to change their own policies anyways,\n> > \n> > probably all in slightly different ways. It behooves everyone to develop a\n> > \n> > reasonable standard RBF policy, that is still robust against possible DoS\n> > \n> > vectors, and aligns with miner incentives, so that all participants know\n> > \n> > what behaviour they can reasonably expect. It is simply a bonus that this\n> > \n> > change in RBF policy also partially mitigates the problem of pinned\n> > \n> > transactions.\n> \n> Miners and full nodes have slightly different priorities here; it's not clear\n> \n> to me why it matters that they implement slightly different policies.\n> \n> Still, re-reading your initital post, I'm convinced that the weakening of the\n> \n> DoS protections is probably not a huge problem, so maybe lets try this in a\n> \n> release and see what happens.\n> \n> Notably, if people actually use this new replacement behavior, the institutions\n> \n> doing these sweeps of unconfirmed outputs might stop doing that! That's\n> \n> probably a good thing, as respends of potentially conflicted unconfirmed\n> \n> outputs can be dangerous in reorgs; we're better off if outputs are buried\n> \n> deeply before being spent again.\n> \n> \n> -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n> \n> https://petertodd.org 'peter'\\[:-1\\]@petertodd.org\n> \n> bitcoin-dev mailing list\n> \n> bitcoin-dev at lists.linuxfoundation.org\n> \n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "Revisiting BIP 125 RBF policy.",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Russell O'Connor",
                "rhavar at protonmail.com",
                "Peter Todd"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 16796
        }
    },
    {
        "title": "[bitcoin-dev] BIP 117 Feedback",
        "thread_messages": [
            {
                "author": "Johnson Lau",
                "date": "2018-03-05T15:28:20",
                "message_text_only": "Altstack in v0 P2WSH should be left untouched. If anyone is already using altstack, BIP117 would very likely confiscate those UTXOs because the altstack would unlikely be executable.\n\nEven in v1 witness, I think altstack should remain be a temporary data storage.\n\nThe \u201c(many scripts) concatinated together in reverse order to form a serialized script\u201d in BIP117 is exactly the same security hole of Satoshi\u2019s scriptSig + OP_CODESAPARATOR + scriptPubKey . That means it is possible to skip execution of scriptPubKey by using a scriptSig with an invalid push operation, so the whole concatenated script becomes a simple push.\n\nFor SigOp limit, I think it\u2019d become more and more difficult to maintain the current statical analyzability model as we try to introduce more functions. I think we should just migrate to a model of limiting sigop per weight, and count the actual number of sigop during execution.  ( https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-February/015764.html ) Actually, this approach is cheaper to analyse, as you only need to look at the witness size, and don\u2019t need to look at the script at all.\n\n\n\n> On 9 Jan 2018, at 6:22 AM, Rusty Russell via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> I've just re-read BIP 117, and I'm concerned about its flexibility.  It\n> seems to be doing too much.\n> \n> The use of altstack is awkward, and makes me query this entire approach.\n> I understand that CLEANSTACK painted us into a corner here :(\n> \n> The simplest implementation of tail recursion would be a single blob: if\n> a single element is left on the altstack, pop and execute it.  That\n> seems trivial to specify.  The treatment of concatenation seems like\n> trying to run before we can walk.\n> \n> Note that if we restrict this for a specific tx version, we can gain\n> experience first and get fancier later.\n> \n> BIP 117 also drops SIGOP and opcode limits.  This requires more\n> justification, in particular, measurements and bounds on execution\n> times.  If this analysis has been done, I'm not aware of it.\n> \n> We could restore statically analyzability by rules like so:\n> 1.  Only applied for tx version 3 segwit txs.\n> 2.  For version 3, top element of stack is counted for limits (perhaps\n>    with discount).\n> 3.  The blob popped off for tail recursion must be identical to that top\n>    element of the stack (ie. the one counted above).\n> \n> Again, future tx versions could drop such restrictions.\n> \n> Cheers,\n> Rusty.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "BIP 117 Feedback",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Johnson Lau"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2668
        }
    },
    {
        "title": "[bitcoin-dev] BIP proposal: Reserved nversion bits in blockheader",
        "thread_messages": [
            {
                "author": "Btc Drak",
                "date": "2018-03-07T08:19:57",
                "message_text_only": "Hi,\n\nThe following proposal reduces the number of version-bits that can be used\nfor parallel soft-fork signalling, reserving 16 bits for non-specific use.\nThis would reduce the number of parallel soft-fork activations using\nversionbits to from 29 to 13 and prevent node software from emitting false\nwarnings about unknown signalling bits under the versionbits signalling\nsystem (BIP8/9). I chose the upper bits of the nVersion, because looking at\nthe versionbits implementation in the most widely deployed node software,\nit is easier to implement than say annexing the lower 2 bytes of the field.\n\nThe scope of the BIP is deliberately limited to reserving bits for general\nuse without specifying specific uses for each bit, although there have\npreviously been various discussions of some use-cases of nVersion bits\nincluding version-rolling AsicBoost[1], and nonce rolling to reduce CPU\nload on mining controllers because ntime-rolling can only be done for short\nperiods otherwise it could have negative side effects distorting time.\nHowever, specific use cases are not important for this BIP.\n\nI am reviving discussion on this topic now, specifically, because the new\nDragonMint miner uses version-rolling AsicBoost on mainnet[2]. It is\nimportant to bring up so node software can adapt the versionbits warning\nsystem to prevent false positives. This BIP has the added advantage that\nwhen a new use for bits is found, mining manufacturers can play in the\ndesignated area without causing disruption or inconvenience (as\nunfortuntely, the use of version-rolling will cause until BIP8/9 warning\nsystems are adapted). I appologise for the inconvenience in advance, but\nthis is the unfortunate result of restraints while negotiating to get the\npatent opened[3] and licensed defensively[4] in the first place.\n\nI believe there was a similar proposal[5] made some years ago, before the\nadvent of BIP9. This proposal differs in that it's primary purpose is to\nremove bits from the versionbits soft-fork activation system and earmark 16\nbits for general use without allocating fixed uses for each bit. The BIP\ncites a couple of usecases for good measure, but they are just\ninformational examples, not part of a specification laid down. For this\nreason, there no is mention of the version-rolling Stratum extension[6]\nspecifics within the BIP text other than a reference to the specification\nitself.\n\nRefs:\n\n[1] https://arxiv.org/pdf/1604.00575.pdf\n[2]\nhttps://halongmining.com/blog/2018/03/07/dragonmint-btc-miner-uses-version-rolling-asicboost/\n[3]\nhttps://www.asicboost.com/single-post/2018/03/01/opening-asicboost-for-defensive-use/\n[4] https://blockchaindpl.org/\n[5] https://github.com/BlockheaderNonce2/bitcoin/wiki\n[6] http://stratumprotocol.org/stratum-extensions\n\n<pre>\n  BIP: ?\n  Title: Reserved nversion bits in blockheader\n  Author: BtcDrak <btcdrak at gmail.com>\n  Comments-Summary: No comments yet.\n  Comments-URI: https://github.com/bitcoin/bips/wiki/Comments:BIP-????\n  Status: Draft\n  Type: Informational\n  Created: 2018-03-01\n  License: BSD-3-Clause\n           CC0-1.0\n</pre>\n\n==Abstract==\n\nThis BIP reserves 16 bits of the block header nVersion field for\ngeneral purpose use and removes their meaning for the purpose of\nversion bits soft-fork signalling.\n\n==Motivation==\n\nThere are a variety of things that miners may desire to use some of\nthe nVersion field bits for. However, due to their use to coordinate\nminer activated soft-forks, full node software will generate false\nwarnings about unknown soft forks if those bits are used for non soft\nfork signalling purposes. By reserving bits from the nVersion field\nfor general use, node software can be updated to ignore those bits and\ntherefore will not emit false warnings. Reserving 16 bits for general\nuse leaves enough for 13 parallel soft-forks using version bits.\n\n==Example Uses==\n\nThe following are example cases that would benefit from using some of\nthe bits from the nVersion field. This list is not exhaustive.\n\nBitcoin mining hardware currently can exhaust the 32 bit nonce field\nin less than 200ms requiring the controller to distribute new jobs\nvery frequently to each mining chip consuming a lot of bandwidth and\nCPU time. This can be greatly reduced by rolling more bits. Rolling\ntoo many bits from nTime is not ideal because it may distort the\ntimestamps over a longer period.\n\nVersion-rolling AsicBoost requires two bits from the nVersion field to\ncalculate 4-way collisions. Any two bits can be used and mining\nequipment can negotiate which bits are to be used with mining pools\nvia the Stratum \"version-rolling\" extension.\n\n==Specification==\n\nSixteen bits from the block header nVersion field, starting from 13\nand ending at 28 inclusive (0x1fffe000), are reserved for general use\nand removed from BIP8 and BIP9 specifications. A mask of 0xe0001fff\nshould be applied to nVersion bits so bits 13-28 inclusive will be\nignored for soft-fork signalling and unknown soft-fork warnings.\n\nThis specification does not reserve specific bits for specific purposes.\n\n==Backwards Compatibility==\n\nThis proposal is backwards compatible, and does not require a soft\nfork to implement.\n\n==References==\n\n[[bip-0008.mediawiki|BIP8]]\n[[bip-0009.mediawiki|BIP9]]\n[https://arxiv.org/pdf/1604.00575.pdf AsicBoost white paper]\n[https://github.com/BlockheaderNonce2/bitcoin/wiki nNonce2 proposal]\n[http://stratumprotocol.org/ Stratum protocol extension for version-rolling]\n\n==Copyright==\n\nThis document is dual licensed as BSD 3-clause, and Creative Commons\nCC0 1.0 Universal.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180307/363336e6/attachment-0001.html>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2018-03-07T14:43:11",
                "message_text_only": "Why are you posting this obsolete draft? You've already received review in \nprivate, and been given useful suggestions. There's even a shared Google Doc \nwith the current draft:\n\n    https://docs.google.com/document/d/1GedKia78NUAtylCzeRD3lMlLrpPVBFg9TV9LRqvStak/edit?usp=sharing\n\nAgain:\n\n* This is no different from what Timo and Sergio proposed years ago, and as \nsuch should be based on their work instead of outright not-invented-here \nrespecification. The current draft integrates their work while not trying to \nsteal credit for it (they are included as primary authors).\n\n* The specification should be complete, including updates for GBT and the \nStratum mining protocol. These are included in the current draft.\n\nAdditionally, it is not appropriate to begin using a draft BIP on mainnet \nbefore any discussion or consensus has been reached. Doing so seems quite \nmalicious, in fact. I hope DragonMint miners can still operate using the \n*current* Bitcoin protocol.\n\nLuke\n\n\nOn Wednesday 07 March 2018 8:19:57 AM Btc Drak via bitcoin-dev wrote:\n> Hi,\n> \n> The following proposal reduces the number of version-bits that can be used\n> for parallel soft-fork signalling, reserving 16 bits for non-specific use.\n> This would reduce the number of parallel soft-fork activations using\n> versionbits to from 29 to 13 and prevent node software from emitting false\n> warnings about unknown signalling bits under the versionbits signalling\n> system (BIP8/9). I chose the upper bits of the nVersion, because looking at\n> the versionbits implementation in the most widely deployed node software,\n> it is easier to implement than say annexing the lower 2 bytes of the field.\n> \n> The scope of the BIP is deliberately limited to reserving bits for general\n> use without specifying specific uses for each bit, although there have\n> previously been various discussions of some use-cases of nVersion bits\n> including version-rolling AsicBoost[1], and nonce rolling to reduce CPU\n> load on mining controllers because ntime-rolling can only be done for short\n> periods otherwise it could have negative side effects distorting time.\n> However, specific use cases are not important for this BIP.\n> \n> I am reviving discussion on this topic now, specifically, because the new\n> DragonMint miner uses version-rolling AsicBoost on mainnet[2]. It is\n> important to bring up so node software can adapt the versionbits warning\n> system to prevent false positives. This BIP has the added advantage that\n> when a new use for bits is found, mining manufacturers can play in the\n> designated area without causing disruption or inconvenience (as\n> unfortuntely, the use of version-rolling will cause until BIP8/9 warning\n> systems are adapted). I appologise for the inconvenience in advance, but\n> this is the unfortunate result of restraints while negotiating to get the\n> patent opened[3] and licensed defensively[4] in the first place.\n> \n> I believe there was a similar proposal[5] made some years ago, before the\n> advent of BIP9. This proposal differs in that it's primary purpose is to\n> remove bits from the versionbits soft-fork activation system and earmark 16\n> bits for general use without allocating fixed uses for each bit. The BIP\n> cites a couple of usecases for good measure, but they are just\n> informational examples, not part of a specification laid down. For this\n> reason, there no is mention of the version-rolling Stratum extension[6]\n> specifics within the BIP text other than a reference to the specification\n> itself.\n> \n> Refs:\n> \n> [1] https://arxiv.org/pdf/1604.00575.pdf\n> [2]\n> https://halongmining.com/blog/2018/03/07/dragonmint-btc-miner-uses-version-> rolling-asicboost/ [3]\n> https://www.asicboost.com/single-post/2018/03/01/opening-asicboost-for-defe\n> nsive-use/ [4] https://blockchaindpl.org/\n> [5] https://github.com/BlockheaderNonce2/bitcoin/wiki\n> [6] http://stratumprotocol.org/stratum-extensions\n> \n> <pre>\n>   BIP: ?\n>   Title: Reserved nversion bits in blockheader\n>   Author: BtcDrak <btcdrak at gmail.com>\n>   Comments-Summary: No comments yet.\n>   Comments-URI: https://github.com/bitcoin/bips/wiki/Comments:BIP-????\n>   Status: Draft\n>   Type: Informational\n>   Created: 2018-03-01\n>   License: BSD-3-Clause\n>            CC0-1.0\n> </pre>\n> \n> ==Abstract==\n> \n> This BIP reserves 16 bits of the block header nVersion field for\n> general purpose use and removes their meaning for the purpose of\n> version bits soft-fork signalling.\n> \n> ==Motivation==\n> \n> There are a variety of things that miners may desire to use some of\n> the nVersion field bits for. However, due to their use to coordinate\n> miner activated soft-forks, full node software will generate false\n> warnings about unknown soft forks if those bits are used for non soft\n> fork signalling purposes. By reserving bits from the nVersion field\n> for general use, node software can be updated to ignore those bits and\n> therefore will not emit false warnings. Reserving 16 bits for general\n> use leaves enough for 13 parallel soft-forks using version bits.\n> \n> ==Example Uses==\n> \n> The following are example cases that would benefit from using some of\n> the bits from the nVersion field. This list is not exhaustive.\n> \n> Bitcoin mining hardware currently can exhaust the 32 bit nonce field\n> in less than 200ms requiring the controller to distribute new jobs\n> very frequently to each mining chip consuming a lot of bandwidth and\n> CPU time. This can be greatly reduced by rolling more bits. Rolling\n> too many bits from nTime is not ideal because it may distort the\n> timestamps over a longer period.\n> \n> Version-rolling AsicBoost requires two bits from the nVersion field to\n> calculate 4-way collisions. Any two bits can be used and mining\n> equipment can negotiate which bits are to be used with mining pools\n> via the Stratum \"version-rolling\" extension.\n> \n> ==Specification==\n> \n> Sixteen bits from the block header nVersion field, starting from 13\n> and ending at 28 inclusive (0x1fffe000), are reserved for general use\n> and removed from BIP8 and BIP9 specifications. A mask of 0xe0001fff\n> should be applied to nVersion bits so bits 13-28 inclusive will be\n> ignored for soft-fork signalling and unknown soft-fork warnings.\n> \n> This specification does not reserve specific bits for specific purposes.\n> \n> ==Backwards Compatibility==\n> \n> This proposal is backwards compatible, and does not require a soft\n> fork to implement.\n> \n> ==References==\n> \n> [[bip-0008.mediawiki|BIP8]]\n> [[bip-0009.mediawiki|BIP9]]\n> [https://arxiv.org/pdf/1604.00575.pdf AsicBoost white paper]\n> [https://github.com/BlockheaderNonce2/bitcoin/wiki nNonce2 proposal]\n> [http://stratumprotocol.org/ Stratum protocol extension for\n> version-rolling]\n> \n> ==Copyright==\n> \n> This document is dual licensed as BSD 3-clause, and Creative Commons\n> CC0 1.0 Universal."
            }
        ],
        "thread_summary": {
            "title": "BIP proposal: Reserved nversion bits in blockheader",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Luke Dashjr",
                "Btc Drak"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 12558
        }
    },
    {
        "title": "[bitcoin-dev] BIP proposal: Reserved nversion bits in blockheader - stratum mining.configure",
        "thread_messages": [
            {
                "author": "Jan \u010capek",
                "date": "2018-03-07T15:43:49",
                "message_text_only": "Hello,\n\nOur reasoning for coming up with a new method for miner configuration\nwas stated here: https://github.com/slushpool/stratumprotocol/issues/1\n\nIt is primarily the determinism of expecting the response. That is\nthe reason why we chose a new method mining.configure instead of an\nexisting mining.capabilities that was not being very well documented or\nused.\n\n\nOn Wed, 7 Mar 2018 14:43:11 +0000 Luke Dashjr via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Why are you posting this obsolete draft? You've already received\n> review in private, and been given useful suggestions. There's even a\n> shared Google Doc with the current draft:\n> \n>     https://docs.google.com/document/d/1GedKia78NUAtylCzeRD3lMlLrpPVBFg9TV9LRqvStak/edit?usp=sharing\n> \n> Again:\n> \n> * This is no different from what Timo and Sergio proposed years ago,\n> and as such should be based on their work instead of outright\n> not-invented-here respecification. The current draft integrates their\n> work while not trying to steal credit for it (they are included as\n> primary authors).\n> \n> * The specification should be complete, including updates for GBT and\n> the Stratum mining protocol. These are included in the current draft.\n> \n> Additionally, it is not appropriate to begin using a draft BIP on\n> mainnet before any discussion or consensus has been reached. Doing so\n> seems quite malicious, in fact. I hope DragonMint miners can still\n> operate using the *current* Bitcoin protocol.\n> \n> Luke\n> \n> \n> On Wednesday 07 March 2018 8:19:57 AM Btc Drak via bitcoin-dev wrote:\n> > Hi,\n> > \n> > The following proposal reduces the number of version-bits that can\n> > be used for parallel soft-fork signalling, reserving 16 bits for\n> > non-specific use. This would reduce the number of parallel\n> > soft-fork activations using versionbits to from 29 to 13 and\n> > prevent node software from emitting false warnings about unknown\n> > signalling bits under the versionbits signalling system (BIP8/9). I\n> > chose the upper bits of the nVersion, because looking at the\n> > versionbits implementation in the most widely deployed node\n> > software, it is easier to implement than say annexing the lower 2\n> > bytes of the field.\n> > \n> > The scope of the BIP is deliberately limited to reserving bits for\n> > general use without specifying specific uses for each bit, although\n> > there have previously been various discussions of some use-cases of\n> > nVersion bits including version-rolling AsicBoost[1], and nonce\n> > rolling to reduce CPU load on mining controllers because\n> > ntime-rolling can only be done for short periods otherwise it could\n> > have negative side effects distorting time. However, specific use\n> > cases are not important for this BIP.\n> > \n> > I am reviving discussion on this topic now, specifically, because\n> > the new DragonMint miner uses version-rolling AsicBoost on\n> > mainnet[2]. It is important to bring up so node software can adapt\n> > the versionbits warning system to prevent false positives. This BIP\n> > has the added advantage that when a new use for bits is found,\n> > mining manufacturers can play in the designated area without\n> > causing disruption or inconvenience (as unfortuntely, the use of\n> > version-rolling will cause until BIP8/9 warning systems are\n> > adapted). I appologise for the inconvenience in advance, but this\n> > is the unfortunate result of restraints while negotiating to get\n> > the patent opened[3] and licensed defensively[4] in the first place.\n> > \n> > I believe there was a similar proposal[5] made some years ago,\n> > before the advent of BIP9. This proposal differs in that it's\n> > primary purpose is to remove bits from the versionbits soft-fork\n> > activation system and earmark 16 bits for general use without\n> > allocating fixed uses for each bit. The BIP cites a couple of\n> > usecases for good measure, but they are just informational\n> > examples, not part of a specification laid down. For this reason,\n> > there no is mention of the version-rolling Stratum extension[6]\n> > specifics within the BIP text other than a reference to the\n> > specification itself.\n> > \n> > Refs:\n> > \n> > [1] https://arxiv.org/pdf/1604.00575.pdf\n> > [2]\n> > https://halongmining.com/blog/2018/03/07/dragonmint-btc-miner-uses-version->\n> > rolling-asicboost/ [3]\n> > https://www.asicboost.com/single-post/2018/03/01/opening-asicboost-for-defe\n> > nsive-use/ [4] https://blockchaindpl.org/ [5]\n> > https://github.com/BlockheaderNonce2/bitcoin/wiki [6]\n> > http://stratumprotocol.org/stratum-extensions\n> > \n> > <pre>\n> >   BIP: ?\n> >   Title: Reserved nversion bits in blockheader\n> >   Author: BtcDrak <btcdrak at gmail.com>\n> >   Comments-Summary: No comments yet.\n> >   Comments-URI:\n> > https://github.com/bitcoin/bips/wiki/Comments:BIP-???? Status: Draft\n> >   Type: Informational\n> >   Created: 2018-03-01\n> >   License: BSD-3-Clause\n> >            CC0-1.0\n> > </pre>\n> > \n> > ==Abstract==\n> > \n> > This BIP reserves 16 bits of the block header nVersion field for\n> > general purpose use and removes their meaning for the purpose of\n> > version bits soft-fork signalling.\n> > \n> > ==Motivation==\n> > \n> > There are a variety of things that miners may desire to use some of\n> > the nVersion field bits for. However, due to their use to coordinate\n> > miner activated soft-forks, full node software will generate false\n> > warnings about unknown soft forks if those bits are used for non\n> > soft fork signalling purposes. By reserving bits from the nVersion\n> > field for general use, node software can be updated to ignore those\n> > bits and therefore will not emit false warnings. Reserving 16 bits\n> > for general use leaves enough for 13 parallel soft-forks using\n> > version bits.\n> > \n> > ==Example Uses==\n> > \n> > The following are example cases that would benefit from using some\n> > of the bits from the nVersion field. This list is not exhaustive.\n> > \n> > Bitcoin mining hardware currently can exhaust the 32 bit nonce field\n> > in less than 200ms requiring the controller to distribute new jobs\n> > very frequently to each mining chip consuming a lot of bandwidth and\n> > CPU time. This can be greatly reduced by rolling more bits. Rolling\n> > too many bits from nTime is not ideal because it may distort the\n> > timestamps over a longer period.\n> > \n> > Version-rolling AsicBoost requires two bits from the nVersion field\n> > to calculate 4-way collisions. Any two bits can be used and mining\n> > equipment can negotiate which bits are to be used with mining pools\n> > via the Stratum \"version-rolling\" extension.\n> > \n> > ==Specification==\n> > \n> > Sixteen bits from the block header nVersion field, starting from 13\n> > and ending at 28 inclusive (0x1fffe000), are reserved for general\n> > use and removed from BIP8 and BIP9 specifications. A mask of\n> > 0xe0001fff should be applied to nVersion bits so bits 13-28\n> > inclusive will be ignored for soft-fork signalling and unknown\n> > soft-fork warnings.\n> > \n> > This specification does not reserve specific bits for specific\n> > purposes.\n> > \n> > ==Backwards Compatibility==\n> > \n> > This proposal is backwards compatible, and does not require a soft\n> > fork to implement.\n> > \n> > ==References==\n> > \n> > [[bip-0008.mediawiki|BIP8]]\n> > [[bip-0009.mediawiki|BIP9]]\n> > [https://arxiv.org/pdf/1604.00575.pdf AsicBoost white paper]\n> > [https://github.com/BlockheaderNonce2/bitcoin/wiki nNonce2 proposal]\n> > [http://stratumprotocol.org/ Stratum protocol extension for\n> > version-rolling]\n> > \n> > ==Copyright==\n> > \n> > This document is dual licensed as BSD 3-clause, and Creative Commons\n> > CC0 1.0 Universal.  \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n\n\n-- \nCEO Braiins Systems | Slushpool.com\ntel: +420 604 566 382\nemail: jan.capek at braiins.cz\nhttp://braiins.cz\nhttp://slushpool.com\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: not available\nType: application/pgp-signature\nSize: 819 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180307/87b5929e/attachment-0001.sig>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2018-03-07T15:48:00",
                "message_text_only": "On Wednesday 07 March 2018 3:43:49 PM Jan \u010capek wrote:\n> Our reasoning for coming up with a new method for miner configuration\n> was stated here: https://github.com/slushpool/stratumprotocol/issues/1\n\nThis reasoning is not sound.\n\n> It is primarily the determinism of expecting the response. That is\n> the reason why we chose a new method mining.configure instead of an\n> existing mining.capabilities that was not being very well documented or\n> used.\n\nIt was as well documented as the original stratum protocol, and in use since \n2014.\n\nWhile the response type is admittedly undefined, simply defining that would \nhave been a better solution than to reinvent it incompatibly for no reason. \n(Although version rolling does not actually require a response at all.)\n\n> \n> \n> On Wed, 7 Mar 2018 14:43:11 +0000 Luke Dashjr via bitcoin-dev\n> \n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > Why are you posting this obsolete draft? You've already received\n> > review in private, and been given useful suggestions. There's even a\n> > \n> > shared Google Doc with the current draft:\n> >     https://docs.google.com/document/d/1GedKia78NUAtylCzeRD3lMlLrpPVBFg9T\n> >     V9LRqvStak/edit?usp=sharing\n> > \n> > Again:\n> > \n> > * This is no different from what Timo and Sergio proposed years ago,\n> > and as such should be based on their work instead of outright\n> > not-invented-here respecification. The current draft integrates their\n> > work while not trying to steal credit for it (they are included as\n> > primary authors).\n> > \n> > * The specification should be complete, including updates for GBT and\n> > the Stratum mining protocol. These are included in the current draft.\n> > \n> > Additionally, it is not appropriate to begin using a draft BIP on\n> > mainnet before any discussion or consensus has been reached. Doing so\n> > seems quite malicious, in fact. I hope DragonMint miners can still\n> > operate using the *current* Bitcoin protocol.\n> > \n> > Luke\n> > \n> > On Wednesday 07 March 2018 8:19:57 AM Btc Drak via bitcoin-dev wrote:\n> > > Hi,\n> > > \n> > > The following proposal reduces the number of version-bits that can\n> > > be used for parallel soft-fork signalling, reserving 16 bits for\n> > > non-specific use. This would reduce the number of parallel\n> > > soft-fork activations using versionbits to from 29 to 13 and\n> > > prevent node software from emitting false warnings about unknown\n> > > signalling bits under the versionbits signalling system (BIP8/9). I\n> > > chose the upper bits of the nVersion, because looking at the\n> > > versionbits implementation in the most widely deployed node\n> > > software, it is easier to implement than say annexing the lower 2\n> > > bytes of the field.\n> > > \n> > > The scope of the BIP is deliberately limited to reserving bits for\n> > > general use without specifying specific uses for each bit, although\n> > > there have previously been various discussions of some use-cases of\n> > > nVersion bits including version-rolling AsicBoost[1], and nonce\n> > > rolling to reduce CPU load on mining controllers because\n> > > ntime-rolling can only be done for short periods otherwise it could\n> > > have negative side effects distorting time. However, specific use\n> > > cases are not important for this BIP.\n> > > \n> > > I am reviving discussion on this topic now, specifically, because\n> > > the new DragonMint miner uses version-rolling AsicBoost on\n> > > mainnet[2]. It is important to bring up so node software can adapt\n> > > the versionbits warning system to prevent false positives. This BIP\n> > > has the added advantage that when a new use for bits is found,\n> > > mining manufacturers can play in the designated area without\n> > > causing disruption or inconvenience (as unfortuntely, the use of\n> > > version-rolling will cause until BIP8/9 warning systems are\n> > > adapted). I appologise for the inconvenience in advance, but this\n> > > is the unfortunate result of restraints while negotiating to get\n> > > the patent opened[3] and licensed defensively[4] in the first place.\n> > > \n> > > I believe there was a similar proposal[5] made some years ago,\n> > > before the advent of BIP9. This proposal differs in that it's\n> > > primary purpose is to remove bits from the versionbits soft-fork\n> > > activation system and earmark 16 bits for general use without\n> > > allocating fixed uses for each bit. The BIP cites a couple of\n> > > usecases for good measure, but they are just informational\n> > > examples, not part of a specification laid down. For this reason,\n> > > there no is mention of the version-rolling Stratum extension[6]\n> > > specifics within the BIP text other than a reference to the\n> > > specification itself.\n> > > \n> > > Refs:\n> > > \n> > > [1] https://arxiv.org/pdf/1604.00575.pdf\n> > > [2]\n> > > https://halongmining.com/blog/2018/03/07/dragonmint-btc-miner-uses-vers\n> > > ion-> rolling-asicboost/ [3]\n> > > https://www.asicboost.com/single-post/2018/03/01/opening-asicboost-for-> > > defe nsive-use/ [4] https://blockchaindpl.org/ [5]\n> > > https://github.com/BlockheaderNonce2/bitcoin/wiki [6]\n> > > http://stratumprotocol.org/stratum-extensions\n> > > \n> > > <pre>\n> > > \n> > >   BIP: ?\n> > >   Title: Reserved nversion bits in blockheader\n> > >   Author: BtcDrak <btcdrak at gmail.com>\n> > >   Comments-Summary: No comments yet.\n> > > \n> > >   Comments-URI:\n> > > https://github.com/bitcoin/bips/wiki/Comments:BIP-???? Status: Draft\n> > > \n> > >   Type: Informational\n> > >   Created: 2018-03-01\n> > >   License: BSD-3-Clause\n> > >   \n> > >            CC0-1.0\n> > > \n> > > </pre>\n> > > \n> > > ==Abstract==\n> > > \n> > > This BIP reserves 16 bits of the block header nVersion field for\n> > > general purpose use and removes their meaning for the purpose of\n> > > version bits soft-fork signalling.\n> > > \n> > > ==Motivation==\n> > > \n> > > There are a variety of things that miners may desire to use some of\n> > > the nVersion field bits for. However, due to their use to coordinate\n> > > miner activated soft-forks, full node software will generate false\n> > > warnings about unknown soft forks if those bits are used for non\n> > > soft fork signalling purposes. By reserving bits from the nVersion\n> > > field for general use, node software can be updated to ignore those\n> > > bits and therefore will not emit false warnings. Reserving 16 bits\n> > > for general use leaves enough for 13 parallel soft-forks using\n> > > version bits.\n> > > \n> > > ==Example Uses==\n> > > \n> > > The following are example cases that would benefit from using some\n> > > of the bits from the nVersion field. This list is not exhaustive.\n> > > \n> > > Bitcoin mining hardware currently can exhaust the 32 bit nonce field\n> > > in less than 200ms requiring the controller to distribute new jobs\n> > > very frequently to each mining chip consuming a lot of bandwidth and\n> > > CPU time. This can be greatly reduced by rolling more bits. Rolling\n> > > too many bits from nTime is not ideal because it may distort the\n> > > timestamps over a longer period.\n> > > \n> > > Version-rolling AsicBoost requires two bits from the nVersion field\n> > > to calculate 4-way collisions. Any two bits can be used and mining\n> > > equipment can negotiate which bits are to be used with mining pools\n> > > via the Stratum \"version-rolling\" extension.\n> > > \n> > > ==Specification==\n> > > \n> > > Sixteen bits from the block header nVersion field, starting from 13\n> > > and ending at 28 inclusive (0x1fffe000), are reserved for general\n> > > use and removed from BIP8 and BIP9 specifications. A mask of\n> > > 0xe0001fff should be applied to nVersion bits so bits 13-28\n> > > inclusive will be ignored for soft-fork signalling and unknown\n> > > soft-fork warnings.\n> > > \n> > > This specification does not reserve specific bits for specific\n> > > purposes.\n> > > \n> > > ==Backwards Compatibility==\n> > > \n> > > This proposal is backwards compatible, and does not require a soft\n> > > fork to implement.\n> > > \n> > > ==References==\n> > > \n> > > [[bip-0008.mediawiki|BIP8]]\n> > > [[bip-0009.mediawiki|BIP9]]\n> > > [https://arxiv.org/pdf/1604.00575.pdf AsicBoost white paper]\n> > > [https://github.com/BlockheaderNonce2/bitcoin/wiki nNonce2 proposal]\n> > > [http://stratumprotocol.org/ Stratum protocol extension for\n> > > version-rolling]\n> > > \n> > > ==Copyright==\n> > > \n> > > This document is dual licensed as BSD 3-clause, and Creative Commons\n> > > CC0 1.0 Universal.\n> > \n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "BIP proposal: Reserved nversion bits in blockheader - stratum mining.configure",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Luke Dashjr",
                "Jan \u010capek"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 16868
        }
    },
    {
        "title": "[bitcoin-dev] version.relay behavior change",
        "thread_messages": [
            {
                "author": "Eric Voskuil",
                "date": "2018-03-09T07:50:02",
                "message_text_only": "/Satoshi:0.15.0/ and later nodes appear to be no longer honoring the\nversion.relay=false flag (BIP37). Could someone familiar with the change\nplease explain the rational?\n\nThanks,\n\ne\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 490 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180308/ae314972/attachment-0001.sig>"
            },
            {
                "author": "Andrew Chow",
                "date": "2018-03-09T15:33:32",
                "message_text_only": "Looking through the code, I don't think that this behavior has changed.\nAre you sure that you are actually connected to Satoshi:0.15.0 nodes and\nnot a node that has simply set their user-agent to that (i.e. not a real\nSatoshi:0.15.0 node)?\n\nIf what you are seeing is true, it is likely a bug and not an\nintentional change. In that case, can you provide specific details on\nhow to reproduce?\n\nAndrew\n\n\nOn 03/09/2018 02:50 AM, Eric Voskuil via bitcoin-dev wrote:\n> /Satoshi:0.15.0/ and later nodes appear to be no longer honoring the\n> version.relay=false flag (BIP37). Could someone familiar with the change\n> please explain the rational?\n>\n> Thanks,\n>\n> e\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180309/5add8c1b/attachment.html>"
            },
            {
                "author": "Eric Voskuil",
                "date": "2018-03-15T09:17:22",
                "message_text_only": "Thanks for the reply Andrew. I\u2019ve reviewed the relevant Core sources and I do not see any problem. We have also synced against a Core node locally and not seen the problem.\n\nThe reason I suspected it was Core is that it is very common and all of the User Agents are consistent (with an occasional exception for forked nodes). So there\u2019s no easy way to determine what sort of nodes we are seeing. \n\nWe tend to cycle through many more connections during sync than a Core node, so may just be seeing it more frequently, but I assume Core would log this behavior as well. Even so, seeing that wouldn\u2019t help much. I\u2019m as certain as I can be at this point that we are setting the flag and version correctly (and that we do not set bip37 filters).\n\nThis behavior started infrequently with 0.14.0 peers and has become more common over time. Just wondering at this point what fork would report as Core and be that common? We used to drop peers that did this (for protocol noncompliance), and I\u2019m considering reinstating that behavior.\n\ne\n\n> On Mar 9, 2018, at 16:33, Andrew Chow via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> Looking through the code, I don't think that this behavior has changed. Are you sure that you are actually connected to Satoshi:0.15.0 nodes and not a node that has simply set their user-agent to that (i.e. not a real Satoshi:0.15.0 node)?\n> \n> If what you are seeing is true, it is likely a bug and not an intentional change. In that case, can you provide specific details on how to reproduce?\n> \n> Andrew\n> \n>> On 03/09/2018 02:50 AM, Eric Voskuil via bitcoin-dev wrote:\n>> /Satoshi:0.15.0/ and later nodes appear to be no longer honoring the\n>> version.relay=false flag (BIP37). Could someone familiar with the change\n>> please explain the rational?\n>> \n>> Thanks,\n>> \n>> e\n>> \n>> \n>> \n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180315/40bc2617/attachment-0001.html>"
            },
            {
                "author": "Andrew Chow",
                "date": "2018-03-15T15:44:43",
                "message_text_only": "I don't think the nodes that you are connecting to that have this\nbehavior are actually forked from Bitcoin Core. It seems more like fake\nnodes - nodes that don't actually do any verification or follow the\nprotocol. Such fake nodes can set whatever user agent they want, common\nones being Bitcoin Core's user agents.\n\nIMO your best solution would be to drop peers for protocol noncompliance.\n\nAndrew\n\n\nOn 03/15/2018 05:17 AM, Eric Voskuil wrote:\n> Thanks for the reply Andrew. I\u2019ve reviewed the relevant Core sources\n> and I do not see any problem. We have also synced against a Core node\n> locally and not seen the problem.\n>\n> The reason I suspected it was Core is that it is very common and all\n> of the User Agents are consistent (with an occasional exception for\n> forked nodes). So there\u2019s no easy way to determine what sort of nodes\n> we are seeing.\u00a0\n>\n> We tend to cycle through many more connections during sync than a Core\n> node, so may just be seeing it more frequently, but I assume Core\n> would log this behavior as well. Even so, seeing that wouldn\u2019t help\n> much. I\u2019m as certain as I can be at this point that we are setting the\n> flag and version correctly (and that we do not set bip37 filters).\n>\n> This behavior started infrequently with 0.14.0 peers and has become\n> more common over time. Just wondering at this point what fork would\n> report as Core and be that common? We used to drop peers that did this\n> (for protocol noncompliance), and I\u2019m considering reinstating that\n> behavior.\n>\n> e\n>\n> On Mar 9, 2018, at 16:33, Andrew Chow via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org\n> <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n>\n>> Looking through the code, I don't think that this behavior has\n>> changed. Are you sure that you are actually connected to\n>> Satoshi:0.15.0 nodes and not a node that has simply set their\n>> user-agent to that (i.e. not a real Satoshi:0.15.0 node)?\n>>\n>> If what you are seeing is true, it is likely a bug and not an\n>> intentional change. In that case, can you provide specific details on\n>> how to reproduce?\n>>\n>> Andrew\n>>\n>>\n>> On 03/09/2018 02:50 AM, Eric Voskuil via bitcoin-dev wrote:\n>>> /Satoshi:0.15.0/ and later nodes appear to be no longer honoring the\n>>> version.relay=false flag (BIP37). Could someone familiar with the change\n>>> please explain the rational?\n>>>\n>>> Thanks,\n>>>\n>>> e\n>>>\n>>>\n>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> <mailto:bitcoin-dev at lists.linuxfoundation.org>\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180315/831fdf48/attachment-0001.html>"
            },
            {
                "author": "Eric Voskuil",
                "date": "2018-03-16T08:27:47",
                "message_text_only": "Agree, thanks for the input Andrew.\n\ne\n\n> On Mar 15, 2018, at 16:44, Andrew Chow <achow101-lists at achow101.com> wrote:\n> \n> I don't think the nodes that you are connecting to that have this behavior are actually forked from Bitcoin Core. It seems more like fake nodes - nodes that don't actually do any verification or follow the protocol. Such fake nodes can set whatever user agent they want, common ones being Bitcoin Core's user agents.\n> \n> IMO your best solution would be to drop peers for protocol noncompliance.\n> \n> Andrew\n> \n>> On 03/15/2018 05:17 AM, Eric Voskuil wrote:\n>> Thanks for the reply Andrew. I\u2019ve reviewed the relevant Core sources and I do not see any problem. We have also synced against a Core node locally and not seen the problem.\n>> \n>> The reason I suspected it was Core is that it is very common and all of the User Agents are consistent (with an occasional exception for forked nodes). So there\u2019s no easy way to determine what sort of nodes we are seeing. \n>> \n>> We tend to cycle through many more connections during sync than a Core node, so may just be seeing it more frequently, but I assume Core would log this behavior as well. Even so, seeing that wouldn\u2019t help much. I\u2019m as certain as I can be at this point that we are setting the flag and version correctly (and that we do not set bip37 filters).\n>> \n>> This behavior started infrequently with 0.14.0 peers and has become more common over time. Just wondering at this point what fork would report as Core and be that common? We used to drop peers that did this (for protocol noncompliance), and I\u2019m considering reinstating that behavior.\n>> \n>> e\n>> \n>> On Mar 9, 2018, at 16:33, Andrew Chow via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> \n>>> Looking through the code, I don't think that this behavior has changed. Are you sure that you are actually connected to Satoshi:0.15.0 nodes and not a node that has simply set their user-agent to that (i.e. not a real Satoshi:0.15.0 node)?\n>>> \n>>> If what you are seeing is true, it is likely a bug and not an intentional change. In that case, can you provide specific details on how to reproduce?\n>>> \n>>> Andrew\n>>> \n>>>> On 03/09/2018 02:50 AM, Eric Voskuil via bitcoin-dev wrote:\n>>>> /Satoshi:0.15.0/ and later nodes appear to be no longer honoring the\n>>>> version.relay=false flag (BIP37). Could someone familiar with the change\n>>>> please explain the rational?\n>>>> \n>>>> Thanks,\n>>>> \n>>>> e\n>>>> \n>>>> \n>>>> \n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>> \n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180316/9d42def7/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "version.relay behavior change",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Andrew Chow",
                "Eric Voskuil"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 9984
        }
    },
    {
        "title": "[bitcoin-dev] Bulletproof CT as basis for election voting?",
        "thread_messages": [
            {
                "author": "JOSE FEMENIAS CA\u00d1UELO",
                "date": "2018-03-11T12:44:47",
                "message_text_only": "If I understand Bulletproof Confidential Transactions properly, their main virtue is being able to hide not the senders/receivers of a coin but the amount transferred.\nThat sounds to me like a perfect use case for an election.\nFor instance, in my country, every citizen is issued a National ID Card with a digital certificate. \nSo, a naive implementation could simply be that the Voting Authority, sends a coin (1 coin = 1 vote) to each citizen above 18. This would be an open transaction, so it is easily auditable.\nLater on, each voter sends her coin to her preferred party, as part of a Bulletproof CT, along with 0 coins to other parties to disguise her vote.\nIn the end, each party will accrue as may votes as coins received.\n\nIs there any gotcha I\u2019m missing here? Are there any missing features required in Bulletproof to support this use case?"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-03-12T04:14:42",
                "message_text_only": "Good morning Jose,\n\nBy my understanding, the sender needs to reveal some secrets to the receiver, and the receiver will then know if it received 0 or 1 coin from that sender.  (At least from my understanding of MimbleWimble; it might not be the case for CT, but MW is an extension of CT so...)\n\nIf voters send vote-coins directly to The Party, then The Party knows the votes of particular voters, and may then dispatch subcontractors to dispatch those voters.  It may be possible to have aggregators/mixers, but then you would have to trust the aggregators/mixers operate correctly and send to the correct destination party, and that the mixers are not recording voters.\n\nMaybe in combination with something like CoinSwap or CoinJoin protocol would work to obscure the source of coins: a voter would have to swap several times with many, many other voters to ensure increased anonymity set (and then maybe some voters may report their transactions to The Party).\n\nIn any case sending directly from the tx of the Voting Authority to another tx to your selected The Party would let The Party members who secretly control the Voting Authority records to figure out, which voters got which txouts of the Voting Authority (presumably the Voting Authority has strict public records of which txout went to which voter, in order to prevent the Voting Authority secretly giving multiple vote-coins to a single One Man, All Votes).\n\nRegards,\nZmnSCPxj\n\n\n\u200bSent with ProtonMail Secure Email.\u200b\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n\nOn March 11, 2018 8:44 PM, JOSE FEMENIAS CA\u00d1UELO via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> If I understand Bulletproof Confidential Transactions properly, their main virtue is being able to hide not the senders/receivers of a coin but the amount transferred.\n> \n> That sounds to me like a perfect use case for an election.\n> \n> For instance, in my country, every citizen is issued a National ID Card with a digital certificate.\n> \n> So, a naive implementation could simply be that the Voting Authority, sends a coin (1 coin = 1 vote) to each citizen above 18. This would be an open transaction, so it is easily auditable.\n> \n> Later on, each voter sends her coin to her preferred party, as part of a Bulletproof CT, along with 0 coins to other parties to disguise her vote.\n> \n> In the end, each party will accrue as may votes as coins received.\n> \n> Is there any gotcha I\u2019m missing here? Are there any missing features required in Bulletproof to support this use case?\n> \n> bitcoin-dev mailing list\n> \n> bitcoin-dev at lists.linuxfoundation.org\n> \n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-03-12T06:46:39",
                "message_text_only": "Good morning again Jose,\n\nAnother idea is that with sufficiently high stakes (i.e. control of the government of an entire country) it would be possible for a miner-strong The Party to censor transactions that do not give it non-zero amounts of coins.  If The Party has a strong enough power over miners (or is composed of miners) then it would be possible for The Party to censor transactions using some simple heuristics: (1) At least one output goes to The Party (2) the number of inputs equals the number of vote-coins that go to The Party output.  Since The Party must know how many vote-coins it received, it can know #2, and it assumes that each input has 1 coin, since that is what is issued by the Voting Authority.  This prevents mixing, too, since transactions that do not involve The Party cannot be confirmed.\n\nPresumably other parties may exist that have some miners, but if everyone starts censoring transactions then parties end up voting by their controlled hashpower rather than anything else (simply censor all transactions that fail the above heuristics and build the longest chain: as long as you get even 1 vote and all others get 0 votes on the longest chain, you win. since presumably you are also a valid voter, you can just give that single vote-coin issued to you-as-voter to you-as-party, then censor all other transactions in the blockchain so that other voters cannot give their coins to their preferred parties).  One could try using proof-of-stake if one has managed to create a solution to nothing-at-stake and stake-grinding that itself does not require proof-of-work (hint, there are none).\n\nThis can be mitigated by using a multi-asset international blockchain with confidential assets, such that no single The Party can control enough hashpower to censor, but that makes small blocks even more important to help fight against centralization (and control of cheap energy becomes even more important such that some international entity may very well bend elections in individual countries to its favor to get more energy with which to control more energy, and so on).\n\nYou can only trust the miners of the blockchain to the extent that you pay fees to those miners, effectively buying a portion of hashrate in a (mostly) fair auction.  You can expect that miners will attempt to charge as much as they can for the hashrate, and therefore that vote transfers (if they can be detected by miners) are likely to be charged at whatever is the going rate for that vote.  If what is being voted on is important enough, you can assure yourself, that miners will ally with politicians and use the fact that CT is confidential only between receiver and sender to discern preferred vote transfers.\n\nUncensorability may be possible though; I think Peter Todd was working on those.  A simple one is a two-step commitment, where an earlier miner only knows of a sealed commitment (a hash of a transaction), publishes it, then a future commitment shows the entire transaction and the earlier miner gets paid only if the second commitment pushes through (the fee gets split somehow between the earlier and later miner).  But once you reveal a transaction and it is not one of those desired by the later miner, if the vote is valuable enough then the miner might very well forgo its fee in favor of never confirming the second commitment.\n\nIt may be better to focus more on libertarian solutions (e.g. assurance contracts) on top of blockchains than attempting to shoehorn democractic ideals on top of blockchains.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Tim Ruffing",
                "date": "2018-03-12T09:32:55",
                "message_text_only": "You're right that this is a simple electronic voting scheme. The thing\nis that cryptographers are working on e-voting for decades and the idea\nto use homomorphic commitments (or encryption) and zero-knowledge\nproofs is not new in this area. It's rather the case that e-voting\ninspired a lot of work on homomorphic crypto and related zero-knowledge \nproofs. For example, range proofs are overkill in e-voting. You just\nneed to ensure that the sum of all my votes (over all candidates) is 1.\n \nE-voting protocols typically require some \"bulletin board\", where\nballots are stored. A blockchain could indeed be helpful in specific\ncases (but not in all cases)...\n\nIf you're interested in that stuff, I'd suggest you to read some\nliterature about e-voting. (For example, \nhttps://arxiv.org/pdf/1801.08064 looks interesting for the connection\nto blockchains -- I haven't read it though). There are pretty\nsophisticated protocols in the literature. And I think that this\nmailing list may not be the best place to discuss these.\n\nBest,\nTim \n\n\n\nOn Sun, 2018-03-11 at 13:44 +0100, JOSE FEMENIAS CA\u00d1UELO via bitcoin-\ndev wrote:\n> If I understand Bulletproof Confidential Transactions properly, their\n> main virtue is being able to hide not the senders/receivers of a coin\n> but the amount transferred.\n> That sounds to me like a perfect use case for an election.\n> For instance, in my country, every citizen is issued a National ID\n> Card with a digital certificate. \n> So, a naive implementation could simply be that the Voting Authority,\n> sends a coin (1 coin = 1 vote) to each citizen above 18. This would\n> be an open transaction, so it is easily auditable.\n> Later on, each voter sends her coin to her preferred party, as part\n> of a Bulletproof CT, along with 0 coins to other parties to disguise\n> her vote.\n> In the end, each party will accrue as may votes as coins received.\n> \n> Is there any gotcha I\u2019m missing here? Are there any missing features\n> required in Bulletproof to support this use case?\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "Bulletproof CT as basis for election voting?",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "Tim Ruffing",
                "JOSE FEMENIAS CA\u00d1UELO"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 9239
        }
    },
    {
        "title": "[bitcoin-dev] Sign / Verify message against SegWit P2SH and Bech32 addresses",
        "thread_messages": [
            {
                "author": "Damian Williamson",
                "date": "2018-03-13T13:26:17",
                "message_text_only": "Current implementation of sign/verify is broken for SegWit and Bech32 addresses.\n\n\nPlease add the following reference to the use cases:\n\n---\n\n# Does blockchain.info show balances for addresses that are in cold storage?\n\nYes.\n\n>... is there any way for me in another country to confirm that what my colleague views is actually accurate and correct?\n\nSince they use Bitcoin Core, yes, there is a way to verify that they hold the addresses that they claim. Have them sign a message with each address that they claim to have the holdings on, using Bitcoin Core you can verify that they indeed have those addresses and check them on blockchain.info to find the current balance.\n\nOnly works in Bitcoin Core currently for addresses starting with a '1' (not Segwit addresses starting with a '3' and not Bech32 addresses starting with 'bc1' - the developers are aware of this and I will remind them shortly.)\n\nIn Bitcoin Core, your transaction opposite goes to File -> Sign Message and signs any message with one of the holding addresses. Copy the message, address and signature and send to you via probably plain text format email is the easiest. Repeat for each additional address holding the balance of BTC that they are offering to sell.\n\nIn Bitcoin Core, you go to File -> Verify Message and key the details provided EXACTLY - spaces, new lines and all characters must be an EXACT match. Click on verify and voil\u00e0.\n\nI prefer the form of signed message as follows (don't key the top and bottom bar rows for the message, just the contents and you can check this yourself, the bottom row is the signature). I like to key the address used for verifying as a part of the message but that is not strictly necessary:\n\n    ------------------------------\n    Something that I want to sign.\n\n    bitcoin:1PMUf9aaQ41M4bgVbCAPVwAeuKvj8CwxJg\n    ------------------------------\n    Signture:\n    IGaXlQNRHHM6ferJ+Ocr3cN9dRJhIWxo+n9PGwgg1uPdOLVYIeCuaccEzDygVgYPJMXqmQeSaLaZVoG6FMHPJkg=\n\nThis contains all of the compact information necessary to verify the message.\n\nExample of verified message:\n![verified message][1]\n\n[1]: https://i.stack.imgur.com/zv1xq.png\n\n---\n\nhttps://bitcoin.stackexchange.com/a/72281/75001\n\n\n\nSolution seems to be straight-forward, as noted in Issue# [10542](https://github.com/bitcoin/bitcoin/issues/10542#issuecomment-306584383)\n\n\n>And it would in theory be possible to make signmessage work for a P2SH-P2WPKH address, in cases where the verifier knows the embedded pubkeyhash already. But in that case you don't need \"sign with a witness address\" functionality - *you could just sign with the embedded key (see validateaddress), and have the verifier check that*.\n\n\n>The point is to not further the misunderstanding that signmessage signs with an address - it never did. It signs with a keyhash, and verify with a keyhash.\n\n\nThis is an important feature, there are few other ways to verify that an address is held. Note that the linked issue is not currently labeld GUI and probably could be - unless a new issue should also be opened?\n\n\nRegards,\n\nDamian Williamson\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180313/7dbec559/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Sign / Verify message against SegWit P2SH and Bech32 addresses",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Damian Williamson"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 3257
        }
    },
    {
        "title": "[bitcoin-dev] Data structure for efficient proofs of non-inclusion",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2018-03-14T00:37:52",
                "message_text_only": "On Wed, Feb 14, 2018 at 09:09:18PM +0000, Daniel Robinson wrote:\n> Hi Peter,\n> \n> Thanks for the mind-expanding presentation on client-side validation at\n> Chaincode last week.\n> \n\nCCing bitcoin-dev because this is of general interest...\n\nFor background, Daniel is asking about my client-side verified single-use-seals\nvia proof-of-publication model, previously published here\u00b9, which creates an\nanti-double-spend primitive via a proof-of-publication, and many\nproofs-of-non-publication.\n\ntl;dr: A seal is closed by publishing a valid signature for the seal to a\nledger. The first signature is the valid one, so if Alice want to convince Bob\nyou she closed a seal correctly (e.g. to pay Bob), she has to supply Bob with a\nproof that the signature _was_ published - a proof-of-publication - as well as\nproof that prior to being published, no valid signature was previously\npublished - a proof-of-non-publication.\n\nIt's the proofs-of-non-publication that take up the most space.\n\n> I'm trying to sketch out a system that would use some of those ideas, and\n> am looking at what Merkle-tree-like structure for the \"transaction root\"\n> would best allow efficient proofs of non-inclusion in a block. The simplest\n> solution seems to just be a Merkle tree with the transactions sorted by\n> public key, but it seems like having intermediate information about ranges\n> higher up in the branches might help make non-inclusion proofs more\n> compact. Other solutions I've seen like Patricia trees and sparse Merkle\n> trees seem to be optimizing for easy recomputation on update, which doesn't\n> seem to be necessary for at least this version of the idea.\n>\n> Are there any verifiable data structures you've found that improve on\n> sorted Merkle trees with respect to proofs of non-inclusion?\n\nSo remember that the system I proposed\u00b9 used sorted merkle trees only within a\nblock; for blocks themselves you mostly can't do any better than a linear list.\nThough I think there may be some exceptions which deserves another email. :)\n\nAs you know, asymptotically merkle trees have excellent log2(n) proof size\ngrowth. But as you correctly suggest, their high overhead in the small-n case\nsuggests that we can do better. In fact, Bram Cohen previously proposed\u00b2 a \"TXO\nBitfield\" for the somewhat similar use-case of committing to the spentness of\noutputs efficiently.\n\n\n# Naive Analysis\n\nSo suppose at an intermediate node you commit to a simple bitfield where each\npossible value under that node is represented by a single bit. Thus for m\nvalues under that point in the tree, the marginal size of the non-inclusion\nproof is m bits. By comparison a naive merkle tree built from a hash function\nwith k bits takes approximately k*log2(m) bits to prove non-inclusion. For an\nrough, unsophisticated, analysis just solve:\n\n    m = k * log2(m)\n\nApparently you can do this analytically, but as this analysis is only\napproximate a much better idea is to just plot it on a graph: for k=256bits the\ncrossover point is roughly m=3000.\n\n\n# Merkle Tree Structure\n\nBut what is the bitfield competing against exactly? Just saying \"merkle tree\"\nisn't very precise. Most designs along these lines use something like a\nmerkelized patricia tree, where each bit of the key is compared individually\nand each node is a two-way (left vs right) branch. Better yet is the radix\ntree, where each inner node that has only one child is merged with its parent.\n\nRegardless, the logic of these kinds of trees can be thought of a recursive\nquery, where each type of node has a `get(key)` operation that returns either a\nvalue or None.\n\nSo let's define a new type of inner node that commits to a\nmerkle-mountain-range (MMR) tip and a 2^j element bitfield. `get(key)` is then\nthis pseudo-rust:\n\n    fn get(self, prefix) -> Option<Value> {\n        let idx = Int::from(prefix[0 .. j]);\n        if self.bitfield[idx] {\n            let mmr_idx = node.bitfield[0 .. idx].count_ones() - 1;\n            Some(node.mmr[mmr_idx].get(prefix)\n        } else {\n            None\n        }\n    }\n\nThe hard part with this is choosing when to use a bitfield-based inner node\ninstead of a standard one. Assuming keys are randomly distributed, it makes\nsense to do so when the bitfield table is partially empty, but how empty? It's\na trade-off between multiple parameters, including the size of\nproofs-of-publication - although the latter may be OK to ignore as the number\nof proof-of-non-publication needed should usually greatly outnumber\nproofs-of-publication.\n\nQuestion: is it OK for this choice to not be part of the deterministic\nconsensus? Is that even possible to enforce?\n\n\n# Security\n\nFor a proof-of-publication to be secure, it must ensure that any attempt to\nconstruct a false proof-of-non-publication will fail. In the pruning model,\nthat means that a proof-of-publication is simply the data necessary for the\nproof-of-non-publication verifier to return false. Concretely:\n\n    fn verify_pop(tree, key) -> bool {\n        !verify_non_pop(tree, key)\n    }\n\nHowever note the subtle difference in trust model with respect to censorship\nbetween the following two possible non-pop verifiers:\n\n    fn verify_non_pop(tree, key) -> bool {\n        !tree.exists(key)\n    }\n\n    fn verify_non_pop(tree, key) -> bool {\n        match tree.get(key) {\n            Some(value) => !verify_signature(value),\n            None => true,\n        }\n    }\n\n\n## False Positives\n\nNote how if we use the second `verify_non_pop()` function shown above we can\nalso use probabilistic data structures such as bloom filters in place of a\nbitfield. This works because a false-positive is acceptable, as it will still\nfail signature verification (or sooner if the key is committed in the leaf\nnode).\n\nFor example, it's plausible that a compressed bloom filter would be more space\nefficient than a bitfield, as the multiple hashing steps might use the bits in\nthe filter more efficiently. Investigating this further would be a good\nresearch topic.\n\n\n# References\n\n1) \"[bitcoin-dev] Scalable Semi-Trustless Asset Transfer via Single-Use-Seals and Proof-of-Publication\",\n   Peter Todd, Dec 5th 2017, https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-December/015350.html\n\n2) \"[bitcoin-dev] The TXO bitfield\",\n   Bram Cohen, Mar 31st 2017, https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-March/013928.html\n\n3) \"Bloom filters\",\n    Wikipedia, Jan 27th 2018, https://en.wikipedia.org/w/index.php?title=Bloom_filter&oldid=822632093\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 614 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180313/7f08d01f/attachment-0001.sig>"
            }
        ],
        "thread_summary": {
            "title": "Data structure for efficient proofs of non-inclusion",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Peter Todd"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 6792
        }
    },
    {
        "title": "[bitcoin-dev] {sign|verify}message replacement",
        "thread_messages": [
            {
                "author": "Karl Johan Alm",
                "date": "2018-03-14T08:09:20",
                "message_text_only": "Hello,\n\nI am considering writing a replacement for the message signing tools\nthat are currently broken for all but the legacy 1xx addresses. The\napproach (suggested by Pieter Wuille) is to do a script based\napproach. This does not seem to require a lot of effort for\nimplementing in Bitcoin Core*. Below is my proposal for this system:\n\nA new structure SignatureProof is added, which is a simple scriptSig &\nwitnessProgram container that can be serialized. This is passed out\nfrom/into the signer/verifier.\n\nRPC commands:\n\nsign <address> <message> [<prehashed>=false]\n\nGenerates a signature proof for <message> using the same method that\nwould be used to spend coins sent to <address>.**\n\nverify <address> <message> <proof> [<prehashed>=false]\n\nDeserializes and executes the proof using a custom signature checker\nwhose sighash is derived from <message>. Returns true if the check\nsucceeds, and false otherwise. The scriptPubKey is derived directly\nfrom <address>.**\n\nFeedback welcome.\n\n-Kalle.\n\n(*) Looks like you can simply use VerifyScript with a new signature\nchecker class. (h/t Nicolas Dorier)\n(**) If <prehashed> is true, <message> is the sighash, otherwise\nsighash=sha256d(message)."
            },
            {
                "author": "Kalle Rosenbaum",
                "date": "2018-03-14T09:46:55",
                "message_text_only": "Thank you.\n\nI can't really see from your proposal if you had thought of this: A soft\nfork can make old nodes accept invalid message signatures as valid. For\nexample, a \"signer\" can use a witness version unknown to the verifier to\nfool the verifier. Witness version is detectable (just reject unknown\nwitness versions)  but there may be more subtle changes. Segwit was not\n\"detectable\" in that way, for example.\n\nThis is the reason why I withdrew BIP120. If you have thought about the\nabove, I'd be very interested.\n\n/Kalle\n\nSent from my Sinclair ZX81\n\nDen 14 mars 2018 16:10 skrev \"Karl Johan Alm via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org>:\n\nHello,\n\nI am considering writing a replacement for the message signing tools\nthat are currently broken for all but the legacy 1xx addresses. The\napproach (suggested by Pieter Wuille) is to do a script based\napproach. This does not seem to require a lot of effort for\nimplementing in Bitcoin Core*. Below is my proposal for this system:\n\nA new structure SignatureProof is added, which is a simple scriptSig &\nwitnessProgram container that can be serialized. This is passed out\nfrom/into the signer/verifier.\n\nRPC commands:\n\nsign <address> <message> [<prehashed>=false]\n\nGenerates a signature proof for <message> using the same method that\nwould be used to spend coins sent to <address>.**\n\nverify <address> <message> <proof> [<prehashed>=false]\n\nDeserializes and executes the proof using a custom signature checker\nwhose sighash is derived from <message>. Returns true if the check\nsucceeds, and false otherwise. The scriptPubKey is derived directly\nfrom <address>.**\n\nFeedback welcome.\n\n-Kalle.\n\n(*) Looks like you can simply use VerifyScript with a new signature\nchecker class. (h/t Nicolas Dorier)\n(**) If <prehashed> is true, <message> is the sighash, otherwise\nsighash=sha256d(message).\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180314/a857358a/attachment.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2018-03-14T16:12:11",
                "message_text_only": "On 14 March 2018 5:46:55 AM GMT-04:00, Kalle Rosenbaum via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>Thank you.\n>\n>I can't really see from your proposal if you had thought of this: A\n>soft\n>fork can make old nodes accept invalid message signatures as valid. For\n>example, a \"signer\" can use a witness version unknown to the verifier\n>to\n>fool the verifier. Witness version is detectable (just reject unknown\n>witness versions)  but there may be more subtle changes. Segwit was not\n>\"detectable\" in that way, for example.\n>\n>This is the reason why I withdrew BIP120. If you have thought about the\n>above, I'd be very interested.\n>\n>/Kalle\n>\n>Sent from my Sinclair ZX81\n>\n>Den 14 mars 2018 16:10 skrev \"Karl Johan Alm via bitcoin-dev\" <\n>bitcoin-dev at lists.linuxfoundation.org>:\n>\n>Hello,\n>\n>I am considering writing a replacement for the message signing tools\n>that are currently broken for all but the legacy 1xx addresses. The\n>approach (suggested by Pieter Wuille) is to do a script based\n>approach. This does not seem to require a lot of effort for\n>implementing in Bitcoin Core*. Below is my proposal for this system:\n>\n>A new structure SignatureProof is added, which is a simple scriptSig &\n>witnessProgram container that can be serialized. This is passed out\n>from/into the signer/verifier.\n>\n>RPC commands:\n>\n>sign <address> <message> [<prehashed>=false]\n>\n>Generates a signature proof for <message> using the same method that\n>would be used to spend coins sent to <address>.**\n>\n>verify <address> <message> <proof> [<prehashed>=false]\n>\n>Deserializes and executes the proof using a custom signature checker\n>whose sighash is derived from <message>. Returns true if the check\n>succeeds, and false otherwise. The scriptPubKey is derived directly\n>from <address>.**\n>\n>Feedback welcome.\n>\n>-Kalle.\n>\n>(*) Looks like you can simply use VerifyScript with a new signature\n>checker class. (h/t Nicolas Dorier)\n>(**) If <prehashed> is true, <message> is the sighash, otherwise\n>sighash=sha256d(message).\n>_______________________________________________\n>bitcoin-dev mailing list\n>bitcoin-dev at lists.linuxfoundation.org\n>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\nWouldn't it be sufficient for old nodes to check for standardness of the spending script and report non-standard scripts as either invalid outright, or at least highly questionable? That should prevent confusion as long as soft forks are only making nonstandard behaviours invalid.\n\nCheers,\naj\n\n-- \nSent from my phone."
            },
            {
                "author": "Karl Johan Alm",
                "date": "2018-03-15T03:01:03",
                "message_text_only": "On Wed, Mar 14, 2018 at 5:46 AM, Kalle Rosenbaum <kalle at rosenbaum.se> wrote:\n> I can't really see from your proposal if you had thought of this: A soft\n> fork can make old nodes accept invalid message signatures as valid. For\n> example, a \"signer\" can use a witness version unknown to the verifier to\n> fool the verifier. Witness version is detectable (just reject unknown\n> witness versions)  but there may be more subtle changes. Segwit was not\n> \"detectable\" in that way, for example.\n>\n> This is the reason why I withdrew BIP120. If you have thought about the\n> above, I'd be very interested.\n\nI'm not sure I see the problem. The scriptPubKey is derived directly\nfrom the address in all cases, which means the unknown witness version\nwould have to be committed to in the address itself.\n\nSo yeah, I can make a P2SH address with a witness version > 0 and a to\nme unknown pubkey and then fool you into thinking I own it, but I\ndon't really see why you'd ultimately care. In other words, if I can\nSPEND funds sent to that address today, I can prove that I can spend\ntoday, which is the purpose of the tool, I think.\n\nFor the case where the witness version HAS been upgraded, the above\nstill applies, but I'm not sure it's a big issue. And it doesn't\nreally require an old node. I just need to set witness version >\ncurrent witness version and the problem applies to all nodes.\n\nOn Wed, Mar 14, 2018 at 8:36 AM, Luke Dashjr <luke at dashjr.org> wrote:\n> I don't see a need for a new RPC interface, just a new signature format.\n\nAll right.\n\n> Ideally, it should support not only just \"proof I receive at this address\",\n> but also \"proof of funds\" (as a separate feature) since this is a popular\n> misuse of the current message signing (which doesn't actually prove funds at\n> all). To do this, it needs to be capable of signing for multiple inputs.\n\nI assume by inputs you mean addresses/keys. The address field could\noptionally be an array. That'd be enough?\n\n> Preferably, it should also avoid disclosing the public key for existing or\n> future UTXOs. But I don't think it's possible to avoid this without something\n> MAST-like first. Perhaps it can be a MAST upgrade later on, but the new\n> signature scheme should probably be designed with it in mind.\n\nI'd love to not have to reveal the public key, but I'm not sure how it\nwould be done, even with MAST.\n\nOn Wed, Mar 14, 2018 at 12:12 PM, Anthony Towns <aj at erisian.com.au> wrote:\n> Wouldn't it be sufficient for old nodes to check for standardness of the spending script and report non-standard scripts as either invalid outright, or at least highly questionable? That should prevent confusion as long as soft forks are only making nonstandard behaviours invalid.\n\nThat seems sensible to me. A warning would probably be useful, in case\nthe verifier is running old software.\n\n-Kalle."
            },
            {
                "author": "Jim Posen",
                "date": "2018-03-15T06:43:21",
                "message_text_only": "I like this proposal, it seems sufficiently general.\n\nHow are scripts with OP_CLTV and OP_CSV handled by verifiers? Do they\nalways succeed? Or should an nLockTime and nSequence also be included in\nthe proof in a way that can be parsed out and displayed to verifiers?\n\nI assume any signatures in the scriptSig/witness data would have no sighash\ntype?\n\nOn Wed, Mar 14, 2018 at 8:01 PM, Karl Johan Alm via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Wed, Mar 14, 2018 at 5:46 AM, Kalle Rosenbaum <kalle at rosenbaum.se>\n> wrote:\n> > I can't really see from your proposal if you had thought of this: A soft\n> > fork can make old nodes accept invalid message signatures as valid. For\n> > example, a \"signer\" can use a witness version unknown to the verifier to\n> > fool the verifier. Witness version is detectable (just reject unknown\n> > witness versions)  but there may be more subtle changes. Segwit was not\n> > \"detectable\" in that way, for example.\n> >\n> > This is the reason why I withdrew BIP120. If you have thought about the\n> > above, I'd be very interested.\n>\n> I'm not sure I see the problem. The scriptPubKey is derived directly\n> from the address in all cases, which means the unknown witness version\n> would have to be committed to in the address itself.\n>\n> So yeah, I can make a P2SH address with a witness version > 0 and a to\n> me unknown pubkey and then fool you into thinking I own it, but I\n> don't really see why you'd ultimately care. In other words, if I can\n> SPEND funds sent to that address today, I can prove that I can spend\n> today, which is the purpose of the tool, I think.\n>\n> For the case where the witness version HAS been upgraded, the above\n> still applies, but I'm not sure it's a big issue. And it doesn't\n> really require an old node. I just need to set witness version >\n> current witness version and the problem applies to all nodes.\n>\n> On Wed, Mar 14, 2018 at 8:36 AM, Luke Dashjr <luke at dashjr.org> wrote:\n> > I don't see a need for a new RPC interface, just a new signature format.\n>\n> All right.\n>\n> > Ideally, it should support not only just \"proof I receive at this\n> address\",\n> > but also \"proof of funds\" (as a separate feature) since this is a popular\n> > misuse of the current message signing (which doesn't actually prove\n> funds at\n> > all). To do this, it needs to be capable of signing for multiple inputs.\n>\n> I assume by inputs you mean addresses/keys. The address field could\n> optionally be an array. That'd be enough?\n>\n> > Preferably, it should also avoid disclosing the public key for existing\n> or\n> > future UTXOs. But I don't think it's possible to avoid this without\n> something\n> > MAST-like first. Perhaps it can be a MAST upgrade later on, but the new\n> > signature scheme should probably be designed with it in mind.\n>\n> I'd love to not have to reveal the public key, but I'm not sure how it\n> would be done, even with MAST.\n>\n> On Wed, Mar 14, 2018 at 12:12 PM, Anthony Towns <aj at erisian.com.au> wrote:\n> > Wouldn't it be sufficient for old nodes to check for standardness of the\n> spending script and report non-standard scripts as either invalid outright,\n> or at least highly questionable? That should prevent confusion as long as\n> soft forks are only making nonstandard behaviours invalid.\n>\n> That seems sensible to me. A warning would probably be useful, in case\n> the verifier is running old software.\n>\n> -Kalle.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180314/0281bc39/attachment-0001.html>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2018-03-14T12:36:47",
                "message_text_only": "I don't see a need for a new RPC interface, just a new signature format.\n\nIdeally, it should support not only just \"proof I receive at this address\", \nbut also \"proof of funds\" (as a separate feature) since this is a popular \nmisuse of the current message signing (which doesn't actually prove funds at \nall). To do this, it needs to be capable of signing for multiple inputs.\n\nPreferably, it should also avoid disclosing the public key for existing or \nfuture UTXOs. But I don't think it's possible to avoid this without something \nMAST-like first. Perhaps it can be a MAST upgrade later on, but the new \nsignature scheme should probably be designed with it in mind.\n\nLuke\n\n\nOn Wednesday 14 March 2018 8:09:20 AM Karl Johan Alm via bitcoin-dev wrote:\n> Hello,\n> \n> I am considering writing a replacement for the message signing tools\n> that are currently broken for all but the legacy 1xx addresses. The\n> approach (suggested by Pieter Wuille) is to do a script based\n> approach. This does not seem to require a lot of effort for\n> implementing in Bitcoin Core*. Below is my proposal for this system:\n> \n> A new structure SignatureProof is added, which is a simple scriptSig &\n> witnessProgram container that can be serialized. This is passed out\n> from/into the signer/verifier.\n> \n> RPC commands:\n> \n> sign <address> <message> [<prehashed>=false]\n> \n> Generates a signature proof for <message> using the same method that\n> would be used to spend coins sent to <address>.**\n> \n> verify <address> <message> <proof> [<prehashed>=false]\n> \n> Deserializes and executes the proof using a custom signature checker\n> whose sighash is derived from <message>. Returns true if the check\n> succeeds, and false otherwise. The scriptPubKey is derived directly\n> from <address>.**\n> \n> Feedback welcome.\n> \n> -Kalle.\n> \n> (*) Looks like you can simply use VerifyScript with a new signature\n> checker class. (h/t Nicolas Dorier)\n> (**) If <prehashed> is true, <message> is the sighash, otherwise\n> sighash=sha256d(message).\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Karl Johan Alm",
                "date": "2018-03-15T07:36:48",
                "message_text_only": "On Wed, Mar 14, 2018 at 12:36 PM, Luke Dashjr <luke at dashjr.org> wrote:\n> Ideally, it should support not only just \"proof I receive at this address\",\n> but also \"proof of funds\" (as a separate feature) since this is a popular\n> misuse of the current message signing (which doesn't actually prove funds at\n> all). To do this, it needs to be capable of signing for multiple inputs.\n\nRe-reading this, I think what you mean is it should be possible to\ncreate a proof for (a) specific UTXO(s), hence \"inputs\". That sounds\npretty useful, yeah!\n\nSo you could provide a mix of addresses and inputs (as txid:vout) and\nit would generate a proof that signs the message for each input\n(taking scriptPubKey from address or from the UTXO data directly on\nthe blockchain).\n\n-Kalle."
            },
            {
                "author": "Luke Dashjr",
                "date": "2018-03-15T14:14:04",
                "message_text_only": "On Thursday 15 March 2018 7:36:48 AM Karl Johan Alm wrote:\n> On Wed, Mar 14, 2018 at 12:36 PM, Luke Dashjr <luke at dashjr.org> wrote:\n> > Ideally, it should support not only just \"proof I receive at this\n> > address\", but also \"proof of funds\" (as a separate feature) since this\n> > is a popular misuse of the current message signing (which doesn't\n> > actually prove funds at all). To do this, it needs to be capable of\n> > signing for multiple inputs.\n> \n> Re-reading this, I think what you mean is it should be possible to\n> create a proof for (a) specific UTXO(s), hence \"inputs\". That sounds\n> pretty useful, yeah!\n\nNot necessarily specific UTXOs (that would contradict fungibility, as well as \nbe impossible for hot/cold wallet separation), but just to prove funds are \navailable. The current sign message cannot be used to prove present possession \nof funds, only that you receive funds."
            },
            {
                "author": "Karl Johan Alm",
                "date": "2018-03-16T00:38:06",
                "message_text_only": "On Thu, Mar 15, 2018 at 2:14 PM, Luke Dashjr <luke at dashjr.org> wrote:\n> Not necessarily specific UTXOs (that would contradict fungibility, as well as\n> be impossible for hot/cold wallet separation), but just to prove funds are\n> available. The current sign message cannot be used to prove present possession\n> of funds, only that you receive funds.\n\nBy saying \"not necessarily specific UTXOs\", are you saying it may be\nspent outputs? I'm a little confused I think.\n\nOn Thu, Mar 15, 2018 at 8:53 PM, Jim Posen <jim.posen at gmail.com> wrote:\n> In this general signing-a-script context, I think a verifier might want to\n> see the time conditions under which it may be spent. The proof container\n> could include an optional nLockTime which defaults to 0 and nSequence which\n> defaults to 0xFFFF...\n\nGood point!\n\n>> I think it would just use the default (SIGHASH_ALL?) for simplicity.\n>> Is there a good reason to tweak it?\n>\n> I took another look and there should definitely be a byte appended to the\n> end of the sig so that the encoding checks pass, but I think it might as\n> well be a 0x00 byte since it's not actually a sighash flag.\n\nI think the sighash flag affects the outcome of the actual\nverification, but I could be mistaken.\n\n-Kalle."
            },
            {
                "author": "Greg Sanders",
                "date": "2018-03-16T01:59:45",
                "message_text_only": "Sorry if I missed the rationale earlier, but why not just do a transaction,\nwith a FORKID specifically for this? Then a node can have a mempool\nacceptance test that returns true even if the signature is not valid as per\nBitcoin consensus, but only due to the FORKID?\n\nThis way basically any wallet can support this provided generic FORKID\nsupport.\n\nOn Thu, Mar 15, 2018 at 8:38 PM, Karl Johan Alm via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Thu, Mar 15, 2018 at 2:14 PM, Luke Dashjr <luke at dashjr.org> wrote:\n> > Not necessarily specific UTXOs (that would contradict fungibility, as\n> well as\n> > be impossible for hot/cold wallet separation), but just to prove funds\n> are\n> > available. The current sign message cannot be used to prove present\n> possession\n> > of funds, only that you receive funds.\n>\n> By saying \"not necessarily specific UTXOs\", are you saying it may be\n> spent outputs? I'm a little confused I think.\n>\n> On Thu, Mar 15, 2018 at 8:53 PM, Jim Posen <jim.posen at gmail.com> wrote:\n> > In this general signing-a-script context, I think a verifier might want\n> to\n> > see the time conditions under which it may be spent. The proof container\n> > could include an optional nLockTime which defaults to 0 and nSequence\n> which\n> > defaults to 0xFFFF...\n>\n> Good point!\n>\n> >> I think it would just use the default (SIGHASH_ALL?) for simplicity.\n> >> Is there a good reason to tweak it?\n> >\n> > I took another look and there should definitely be a byte appended to the\n> > end of the sig so that the encoding checks pass, but I think it might as\n> > well be a 0x00 byte since it's not actually a sighash flag.\n>\n> I think the sighash flag affects the outcome of the actual\n> verification, but I could be mistaken.\n>\n> -Kalle.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180315/e55106e6/attachment.html>"
            },
            {
                "author": "Karl Johan Alm",
                "date": "2018-03-16T02:04:51",
                "message_text_only": "On Fri, Mar 16, 2018 at 1:59 AM, Greg Sanders <gsanders87 at gmail.com> wrote:\n> Sorry if I missed the rationale earlier, but why not just do a transaction,\n> with a FORKID specifically for this? Then a node can have a mempool\n> acceptance test that returns true even if the signature is not valid as per\n> Bitcoin consensus, but only due to the FORKID?\n>\n> This way basically any wallet can support this provided generic FORKID\n> support.\n\nYou'd basically have to provide an entire transaction rather than just\nthe signature, so there's some overhead. (Copy-pasting may become\nunwieldy quicker.)"
            },
            {
                "author": "Damian Williamson",
                "date": "2018-03-15T10:15:17",
                "message_text_only": "That is very helpful Luke. I would not have been concerned if it was necessary to sign multiple times for multiple utxo's on different addresses but, since it is a feature it may as well be best usable. Signing for multiple inputs verifying that you have the priv key for each in your wallet is certainly usable for this popular misuse.\n\n\n>Ideally, it should support not only just \"proof I receive at this address\",\nbut also \"proof of funds\" (as a separate feature) since this is a popular\nmisuse of the current message signing (which doesn't actually prove funds at\nall). To do this, it needs to be capable of signing for multiple inputs.\n\n________________________________\nFrom: bitcoin-dev-bounces at lists.linuxfoundation.org <bitcoin-dev-bounces at lists.linuxfoundation.org> on behalf of Luke Dashjr via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>\nSent: Wednesday, 14 March 2018 11:36:47 PM\nTo: Karl Johan Alm; Bitcoin Protocol Discussion\nSubject: Re: [bitcoin-dev] {sign|verify}message replacement\n\nI don't see a need for a new RPC interface, just a new signature format.\n\nIdeally, it should support not only just \"proof I receive at this address\",\nbut also \"proof of funds\" (as a separate feature) since this is a popular\nmisuse of the current message signing (which doesn't actually prove funds at\nall). To do this, it needs to be capable of signing for multiple inputs.\n\nPreferably, it should also avoid disclosing the public key for existing or\nfuture UTXOs. But I don't think it's possible to avoid this without something\nMAST-like first. Perhaps it can be a MAST upgrade later on, but the new\nsignature scheme should probably be designed with it in mind.\n\nLuke\n\n\nOn Wednesday 14 March 2018 8:09:20 AM Karl Johan Alm via bitcoin-dev wrote:\n> Hello,\n>\n> I am considering writing a replacement for the message signing tools\n> that are currently broken for all but the legacy 1xx addresses. The\n> approach (suggested by Pieter Wuille) is to do a script based\n> approach. This does not seem to require a lot of effort for\n> implementing in Bitcoin Core*. Below is my proposal for this system:\n>\n> A new structure SignatureProof is added, which is a simple scriptSig &\n> witnessProgram container that can be serialized. This is passed out\n> from/into the signer/verifier.\n>\n> RPC commands:\n>\n> sign <address> <message> [<prehashed>=false]\n>\n> Generates a signature proof for <message> using the same method that\n> would be used to spend coins sent to <address>.**\n>\n> verify <address> <message> <proof> [<prehashed>=false]\n>\n> Deserializes and executes the proof using a custom signature checker\n> whose sighash is derived from <message>. Returns true if the check\n> succeeds, and false otherwise. The scriptPubKey is derived directly\n> from <address>.**\n>\n> Feedback welcome.\n>\n> -Kalle.\n>\n> (*) Looks like you can simply use VerifyScript with a new signature\n> checker class. (h/t Nicolas Dorier)\n> (**) If <prehashed> is true, <message> is the sighash, otherwise\n> sighash=sha256d(message).\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180315/8fdff9f2/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2018-03-26T08:53:23",
                "message_text_only": "Hello,\n\nThanks for starting a discussion about this idea.\n\nA few comments inline:\n\nOn Wed, Mar 14, 2018 at 1:09 AM, Karl Johan Alm via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hello,\n>\n> I am considering writing a replacement for the message signing tools\n> that are currently broken for all but the legacy 1xx addresses. The\n> approach (suggested by Pieter Wuille) is to do a script based\n> approach. This does not seem to require a lot of effort for\n> implementing in Bitcoin Core*. Below is my proposal for this system:\n>\n> A new structure SignatureProof is added, which is a simple scriptSig &\n> witnessProgram container that can be serialized. This is passed out\n> from/into the signer/verifier.\n>\n\nYou need a bit more logic to deal with softforks and compatibility. The\nquestion is which script validation flags you verify with:\n* If you make them fixed, it means signatures can't evolve with new address\ntypes being introduced that rely on new features.\n* If you make it just consensus flags (following mainnet), it means that\npeople with old software will see future invalid signatures as always\nvalid; this is probably not acceptable.\n* If you make it standardness flags, you will get future valid signatures\nthat fail to verify.\n\nOne solution is to include a version number in the signature, which\nexplicitly corresponds to a set of validation flags. When the version\nnumber is something a verifier doesn't know, it can be reported as\ninconclusive (it's relying on features you don't know about).\n\nAn solution is to verify twice; once with all consensus rules you know\nabout, and once with standardness rules. If they're both valid, the\nsignature is valid. If they're both invalid, the signature is invalid. If\nthey're different (consensus valid but standardness invalid), you report\nthe signature validation as inconclusive (it's relying on features you\ndon't know about). This approach works as long as new features only use\nprevious standardness-invalid scripts, but perhaps a version number is\nstill needed to indicate the standardness flags.\n\nRPC commands:\n>\n> sign <address> <message> [<prehashed>=false]\n>\n\nWhy not extend the existing signmessage/verifymessage RPC? For legacy\naddresses it can fall back to the existing signature algorithm, while using\nthe script-based approach for all others.\n\n\n>\n> Generates a signature proof for <message> using the same method that\n> would be used to spend coins sent to <address>.**\n>\n> verify <address> <message> <proof> [<prehashed>=false]\n>\n> Deserializes and executes the proof using a custom signature checker\n> whose sighash is derived from <message>. Returns true if the check\n> succeeds, and false otherwise. The scriptPubKey is derived directly\n> from <address>.**\n>\n> Feedback welcome.\n>\n> -Kalle.\n>\n>\n(**) If <prehashed> is true, <message> is the sighash, otherwise\n> sighash=sha256d(message).\n>\n\nThat's very dangerous I'm afraid. It could be used to trick someone into\nsigning off on an actual transaction, if you get them to sign a \"random\nlooking\" prehashed message. Even if you have a prehashed message, there is\nno problem with treating it as hex input to a second hashing step, so I\nthink the prehashed option isn't needed. It's why the existing message\nsigning functionality always forcibly prefixes \"Bitcoin signed message:\",\nto avoid signing something that unintentionally corresponds to a message\nintended for another goal.\n\nCheers,\n\n-- \nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180326/8324b4f3/attachment.html>"
            },
            {
                "author": "Karl Johan Alm",
                "date": "2018-03-27T08:09:41",
                "message_text_only": "Pieter,\n\nThanks for the feedback. Comments below:\n\nOn Mon, Mar 26, 2018 at 5:53 PM, Pieter Wuille <pieter.wuille at gmail.com> wrote:\n> One solution is to include a version number in the signature, which\n> explicitly corresponds to a set of validation flags. When the version number\n> is something a verifier doesn't know, it can be reported as inconclusive\n> (it's relying on features you don't know about).\n>\n> An solution is to verify twice; once with all consensus rules you know\n> about, and once with standardness rules. If they're both valid, the\n> signature is valid. If they're both invalid, the signature is invalid. If\n> they're different (consensus valid but standardness invalid), you report the\n> signature validation as inconclusive (it's relying on features you don't\n> know about). This approach works as long as new features only use previous\n> standardness-invalid scripts, but perhaps a version number is still needed\n> to indicate the standardness flags.\n\nI think the double verify approach seems promising. I assume old nodes\nconsider new consensus rule enforcing transactions as non-standard but\nvalid. If this is always the case, it may be an idea to simply fail\nverification with a message indicating the node is unable to verify\ndue to unknown consensus rules.\n\n>> RPC commands:\n>>\n>> sign <address> <message> [<prehashed>=false]\n>\n> Why not extend the existing signmessage/verifymessage RPC? For legacy\n> addresses it can fall back to the existing signature algorithm, while using\n> the script-based approach for all others.\n\nYes, I initially thought it would be better to not use it as the\nlegacy behavior could be depended on god knows where, but I think\nadding a legacy mode or simply doing the old way for 1xx is\nsufficient.\n\n>> (**) If <prehashed> is true, <message> is the sighash, otherwise\n>> sighash=sha256d(message).\n>\n>\n> That's very dangerous I'm afraid. It could be used to trick someone into\n> signing off on an actual transaction, if you get them to sign a \"random\n> looking\" prehashed message. Even if you have a prehashed message, there is\n> no problem with treating it as hex input to a second hashing step, so I\n> think the prehashed option isn't needed. It's why the existing message\n> signing functionality always forcibly prefixes \"Bitcoin signed message:\", to\n> avoid signing something that unintentionally corresponds to a message\n> intended for another goal.\n\nEek.. good point..."
            },
            {
                "author": "Karl Johan Alm",
                "date": "2018-03-15T07:25:21",
                "message_text_only": "On Thu, Mar 15, 2018 at 6:43 AM, Jim Posen <jim.posen at gmail.com> wrote:\n> How are scripts with OP_CLTV and OP_CSV handled by verifiers? Do they always\n> succeed? Or should an nLockTime and nSequence also be included in the proof\n> in a way that can be parsed out and displayed to verifiers?\n\nGood question.. Since you don't really have the input(s), I think it's\nfine to always assume sufficient time/height on CLTV/CSV checks.\n\n> I assume any signatures in the scriptSig/witness data would have no sighash\n> type?\n\nI think it would just use the default (SIGHASH_ALL?) for simplicity.\nIs there a good reason to tweak it?\n\n-Kalle."
            },
            {
                "author": "Jim Posen",
                "date": "2018-03-15T20:53:34",
                "message_text_only": ">\n> Good question.. Since you don't really have the input(s), I think it's\n> fine to always assume sufficient time/height on CLTV/CSV checks.\n>\n\nIn this general signing-a-script context, I think a verifier might want to\nsee the time conditions under which it may be spent. The proof container\ncould include an optional nLockTime which defaults to 0 and nSequence which\ndefaults to 0xFFFF...\n\n\n> I think it would just use the default (SIGHASH_ALL?) for simplicity.\n> Is there a good reason to tweak it?\n>\n\nI took another look and there should definitely be a byte appended to the\nend of the sig so that the encoding checks pass, but I think it might as\nwell be a 0x00 byte since it's not actually a sighash flag.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180315/6046c7a8/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "{sign|verify}message replacement",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Anthony Towns",
                "Pieter Wuille",
                "Kalle Rosenbaum",
                "Damian Williamson",
                "Luke Dashjr",
                "Karl Johan Alm",
                "Jim Posen",
                "Greg Sanders"
            ],
            "messages_count": 16,
            "total_messages_chars_count": 31545
        }
    },
    {
        "title": "[bitcoin-dev] feature: Enhance privacy by change obfuscation",
        "thread_messages": [
            {
                "author": "Damian Williamson",
                "date": "2018-03-18T01:34:20",
                "message_text_only": "Application: Bitcoin Core\n\nFeature: Enhanced privacy by change obfuscation\n\nOperation: Provide a user selectable 'Enhanced privacy' option for transaction creation, when true the transaction randomly distributes change across up to twenty output addresses (minimum five?), provided each output is not dust.\n\nSuggestions: Perhaps limit the total random number of addresses to distribute to by change amount. Optionally: If necessary, additional inputs can be selected if available to increase change although consider if this may eventually result in a decrease in obfuscation in some cases when the outputs are spent.\n\nIssues: Transaction cost will be higher for the initial spend with the change due to increased outputs and, possibly for later spending the change depending on the future spend amount(s) and the number of inputs required.\n\nArgument: If transaction linkage is possible, it is still possible with the obfuscated change but, it is far more difficult to guess what was retained by the owner of the originating utxo's unless the new change outputs are spent together in the same transaction.\n\n\nRegards,\n\nDamian Williamson\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180318/f91f0fd9/attachment.html>"
            },
            {
                "author": "Evan Klitzke",
                "date": "2018-03-18T05:50:34",
                "message_text_only": "Damian Williamson via bitcoin-dev \n<bitcoin-dev at lists.linuxfoundation.org> writes:\n> Operation: Provide a user selectable 'Enhanced privacy' option for\n> transaction creation, when true the transaction randomly distributes\n> change across up to twenty output addresses (minimum five?), provided\n> each output is not dust.\n\nThis would be really expensive for the network due to the bloat in UTXO\nsize, a cost everyone has to pay for. Not to mention the fact that it\ndoesn't really seem that private, as the wallet is likely going to have\nto rejoin those inputs in future transactions (and the user will have to\npay a high transaction fee as a result).\n\n--\nEvan Klitzke\nhttps://eklitzke.org/"
            },
            {
                "author": "Damian Williamson",
                "date": "2018-03-18T07:07:58",
                "message_text_only": "Alright, but even if two (or more) of the change outputs were linked in a future transaction, no-one can tell if they are still linked to your wallet or not unless there is also an additional re-used address on the new transaction input side that has also been previously linked to one of the inputs on the transaction creating the change.\n\n\nYes, I understand the additional cost but still thought it worthy of consideration.\n\n\nRegards,\n\nDamian Williamson\n\n________________________________\nFrom: Evan Klitzke <evan at eklitzke.org>\nSent: Sunday, 18 March 2018 4:50:34 PM\nTo: Damian Williamson; Bitcoin Protocol Discussion\nSubject: Re: [bitcoin-dev] feature: Enhance privacy by change obfuscation\n\n\nDamian Williamson via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> writes:\n> Operation: Provide a user selectable 'Enhanced privacy' option for\n> transaction creation, when true the transaction randomly distributes\n> change across up to twenty output addresses (minimum five?), provided\n> each output is not dust.\n\nThis would be really expensive for the network due to the bloat in UTXO\nsize, a cost everyone has to pay for. Not to mention the fact that it\ndoesn't really seem that private, as the wallet is likely going to have\nto rejoin those inputs in future transactions (and the user will have to\npay a high transaction fee as a result).\n\n--\nEvan Klitzke\nhttps://eklitzke.org/\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180318/adae0170/attachment.html>"
            },
            {
                "author": "Eric Voskuil",
                "date": "2018-03-18T18:59:28",
                "message_text_only": "> This would be really expensive for the network due to the bloat in UTXO size, a cost everyone has to pay for.\n\nWithout commenting on the merits of this proposal, I\u2019d just like to correct this common misperception. There is no necessary additional cost to the network from the count of unspent outputs. This perception arises from an implementation detail of particular node software. There is no requirement for redundant indexing of unspent outputs.\n\ne"
            }
        ],
        "thread_summary": {
            "title": "feature: Enhance privacy by change obfuscation",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Damian Williamson",
                "Eric Voskuil",
                "Evan Klitzke"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 4037
        }
    },
    {
        "title": "[bitcoin-dev] Soft-forks and schnorr signature aggregation",
        "thread_messages": [
            {
                "author": "Anthony Towns",
                "date": "2018-03-21T04:06:18",
                "message_text_only": "Hello world,\n\nThere was a lot of discussion on Schnorr sigs and key and signature\naggregation at the recent core-dev-tech meeting (one relevant conversation\nis transcribed at [0]).\n\nQuick summary, with more background detail in the corresponding footnotes:\nsignature aggregation is awesome [1], and the possibility of soft-forking\nin new opcodes via OP_RETURN_VALID opcodes (instead of OP_NOP) is also\nawesome [2].\n\nUnfortunately doing both of these together may turn out to be awful.\n\nRETURN_VALID and Signature Aggregation\n--------------------------------------\n\nBumping segwit script versions and redefining OP_NOP opcodes are\nfairly straightforward to deal with even with signature aggregation,\nthe straightforward implementation of both combined is still a soft-fork.\n\nRETURN_VALID, unfortunately, has a serious potential pitfall: any\naggregatable signature operations that occur after it have to go into\nseparate buckets.\n\nAs an example of why this is the case, imagine introducing a covenant\nopcode that pulls a potentially complicated condition from the stack\n(perhaps, \"an output pays at least 50000 satoshi to address xyzzy\"),\nchecks the condition against the transaction, and then pushes 1 (or 0)\nback onto the stack indicating compliance with the covenant (or not).\nYou might then write a script allowing a single person to spend the coins\nif they comply with the covenant, and allow breaking the covenant with\nsomeone else's sign-off in addition. You could write this as:\n\n  pubkey1 CHECKSIGVERIFY\n  cond CHECKCOVENANT IFDUP NOTIF pubkey2 CHECKSIG ENDIF\n\nIf you pass the covenant, you supply \"SIGHASHALL|BUCKET_1\" and aggregate\nthe signature for pubkey1 into bucket1 and you're set; otherwise you supply\n\"SIGHASHALL|BUCKET_1 SIGHASHALL|BUCKET_1\" and aggregate signatures for both\npubkey1 and pubkey2 into bucket1 and you're set. Great!\n\nBut this isn't a soft-fork: old nodes would see this script as:\n\n  pubkey1 CHECKSIGVERIFY\n  cond RETURN_VALID IFDUP NOTIF pubkey2 CHECKSIG ENDIF\n\nwhich it would just interpret as:\n\n  pubkey1 CHECKSIGVERIFY cond RETURN_VALID\n\nwhich is fine if the covenant was passing; but no good if the covenant\ndidn't pass -- they'd be expecting the aggregted sig to just be for\npubkey1 when it's actually pubkey1+pubkey2, so old nodes would fail the\ntx and new nodes would accept it, making it a hard fork.\n\nSolution 0a / 0b\n----------------\n\nThere are two obvious solutions here:\n\n 0a) Just be very careful to ensure any aggregated signatures that\n     are conditional on an redefined RETURN_VALID opcode go into later\n     buckets, but be careful about having separate sets of buckets every\n     time a soft-fork introduces a new redefined opcode. Probably very\n     complicated to implement correctly, and essentially doubles the\n     number of buckets you have to potentially deal with every time you\n     soft fork in a new opcode.\n\n 0b) Alternatively, forget about the hope that RETURN_VALID\n     opcodes could be converted to anything, and just reserve OP_NOP\n     opcodes and convert them to CHECK_foo_VERIFY opcodes just as we\n     have been doing, and when we can't do that bump the segwit witness\n     version for a whole new version of script. Or in twitter speak:\n     \"non-verify upgrades should be done with new script versions\" [3]\n\nI think with a little care we can actually salvage RETURN_VALID though!\n\nSolution 1\n----------\n\nYou don't actually have to write your scripts in ways that can cause\nthis problem, as long as you're careful. In particular, the problem only\noccurs if you do aggregatable CHECKSIG operations after \"RETURN_VALID\"\n-- if you do all the CHECKSIGs first, then all nodes will be checking\nfor the same signatures, and there's no problem. So you could rewrite\nthe script above as:\n\n  pubkey1 CHECKSIGVERIFY\n  IF pubkey2 CHECKSIG ENDIF\n  cond CHECKCOVENANT OR\n\nwhich is redeemable either by:\n\n  sig1 0        [and covenant is met]\n  sig1 1 sig2   [covenant is not checked]\n\nThe witness in this case is essentially committing to the execution path\nthat would have been taken in the first script by a fully validating node,\nthen the new script checks all the signatures, and then validates that the\ncommitted execution path was in fact the one that was meant to be taken.\n\nIf people are clever enough to write scripts this way, I believe you\ncan make RETURN_VALID soft-fork safe simply by having every soft-forked\nRETURN_VALID operation set a state flag that makes every subsequent\nCHECKSIG operation require a non-aggregated sig.\n\nThe drawback of this approach is that if the script is complicated\n(eg it has multiple IF conditions, some of which are nested), it may be\ndifficult to write the script to ensure the signatures are checked in the\nsame combination as the later logic actually requires -- you might have\nto store the flag indicating whether you checked particular signatures\non the altstack, or use DUP and PICK/ROLL to organise it on the stack.\n\nSolution 2\n----------\n\nWe could make that simpler for script authors by making dedicated opcodes\nto help with \"do all the signatures first\" and \"check the committed\nexecution path against reality\" steps. I think a reasonable approach\nwould be something like:\n\n  0b01 pubkey2 pubkey1 2 CHECK_AGGSIG_VERIFY\n  cond CHECKCOVENANT 0b10 CHECK_AGG_SIGNERS OR\n\nwhich is redeemed either by:\n\n  sighash1 0   [and passing the covenant cond]\n  sighash2 sighash1 0b10\n\n(I'm using the notation 0b10110 to express numbers as binary bitfields;\n0b10110 = 22 eg)\n\nThat is, two new opcodes, namely:\n\nCHECK_AGGSIG_VERIFY which takes from the stack:\n    - N: a count of pubkeys\n    - pubkey1..pubkeyN: N pubkeys\n    - REQ: a bitmask of which pubkeys are required to sign\n    - OPT: a bitmask of which optional pubkeys have signed\n    - sighashes: M sighashes for the pubkeys corresponding to the set\n      bits of (REQ|OPT)\n\n  CHECK_AGGSIG_VERIFY fails if:\n    - the stack doesn't have enough elements\n    - the aggregated signature doesn't pass\n    - a redefined RETURN_VALID opcode has already been seen\n    - a previous CHECK_AGGSIG_VERIFY has already been seen in this script\n\n  REQ|OPT is stored as state\n\nCHECK_AGG_SIGNERS takes from the stack:\n    - B: a bitmask of which pubkeys are being queried\n  and it pushes to the stack 1 or 0 based on:\n    - (REQ|OPT) & B == B ? 1 : 0\n\nA possible way to make sure the \"no agg sigs after an upgraded\nRETURN_VALID\" behaviour works right might be to have \"RETURN_VALID\"\nfail if CHECK_AGGSIG_VERIFY hasn't already been seen. That way once you\nredefine RETURN_VALID in a soft-fork, if you have a CHECK_AGGSIG_VERIFY\nafter a RETURN_VALID you've either already failed (because the\nRETURN_VALID wasn't after a CHECK_AGGSIG_VERIFY), or you automatically\nfail (because you've already seen a CHECK_AGGSIG_VERIFY).\n\nThere would be no need to make CHECKSIG, CHECKSIGVERIFY, CHECKMULTISIG\nand CHECKMULTISIGVERIFY do signature aggregation in this case. They could\nbe left around to allow script authors to force non-aggregate signatures\nor could be dropped entirely, I think.\n\nThis construct would let you do M-of-N aggregated multisig in a fairly\nstraightforward manner without needing an explicit opcode, eg:\n\n  0 pubkey5 pubkey4 pubkey3 pubkey2 pubkey1 5 CHECK_AGGSIG_VERIFY\n  0b10000 CHECK_AGG_SIGNERS\n  0b01000 CHECK_AGG_SIGNERS ADD\n  0b00100 CHECK_AGG_SIGNERS ADD\n  0b00010 CHECK_AGG_SIGNERS ADD\n  0b00001 CHECK_AGG_SIGNERS ADD\n  3 NUMEQUAL\n\nredeemable by, eg:\n\n  0b10110 sighash5 sighash3 sighash2\n\nand a single aggregate signature by the private keys corresponding to\npubkey{2,3,5}.\n\nOf course, another way of getting M-of-N aggregated multisig is via MAST,\nwhich brings us to another approach...\n\nSolution 3\n----------\n\nAll we're doing above is committing to an execution path and validating\nsignatures for that path before checking the path was the right one. But\nMAST is a great way of committing to an execution path, so another\napproach would just be \"don't have alternative execution paths, just have\nMAST and CHECK/VERIFY codes\". Taking the example I've been running with,\nthat would be:\n\n  branch1: 2 pubkey2 pubkey1 2 CHECKMULTISIG\n  branch2: pubkey1 CHECKSIGVERIFY cond CHECKCOVENANT\n\nSo long as MAST is already supported when signature aggregation becomes\npossible, that works fine. The drawback is MAST can end up with lots of\nbranches, eg the 3-of-5 multisig check has 10 branches:\n\n  branch1: 3 pubkey3 pubkey2 pubkey1 3 CHECKMULTISIG\n  branch2: 3 pubkey4 pubkey2 pubkey1 3 CHECKMULTISIG\n  branch3: 3 pubkey5 pubkey2 pubkey1 3 CHECKMULTISIG\n  branch4: 3 pubkey4 pubkey3 pubkey1 3 CHECKMULTISIG\n  branch5: 3 pubkey5 pubkey3 pubkey1 3 CHECKMULTISIG\n  branch6: 3 pubkey5 pubkey4 pubkey1 3 CHECKMULTISIG\n  branch7: 3 pubkey4 pubkey3 pubkey2 3 CHECKMULTISIG\n  branch8: 3 pubkey5 pubkey3 pubkey2 3 CHECKMULTISIG\n  branch9: 3 pubkey5 pubkey4 pubkey2 3 CHECKMULTISIG\n  branch10: 3 pubkey5 pubkey4 pubkey3 3 CHECKMULTISIG\n\nwhile if you want, say, 6-of-11 multisig you get 462 branches, versus\njust:\n\n  0 pubkey11 pubkey10 pubkey9 pubkey8 pubkey7 pubkey6\n    pubkey5 pubkey4 pubkey3 pubkey2 pubkey1 11 CHECK_AGGSIG_VERIFY\n  0b10000000000 CHECK_AGG_SIGNERS\n  0b01000000000 CHECK_AGG_SIGNERS ADD\n  0b00100000000 CHECK_AGG_SIGNERS ADD\n  0b00010000000 CHECK_AGG_SIGNERS ADD\n  0b00001000000 CHECK_AGG_SIGNERS ADD\n  0b00000100000 CHECK_AGG_SIGNERS ADD\n  0b00000010000 CHECK_AGG_SIGNERS ADD\n  0b00000001000 CHECK_AGG_SIGNERS ADD\n  0b00000000100 CHECK_AGG_SIGNERS ADD\n  0b00000000010 CHECK_AGG_SIGNERS ADD\n  0b00000000001 CHECK_AGG_SIGNERS ADD\n  6 NUMEQUAL\n\nProvided doing lots of hashes to calculate merkle paths is cheaper than\npublishing to the blockchain, MAST will likely still be better though:\nyou'd be doing 6 pubkeys and 9 steps in the merkle path for about 15*32\nbytes in MAST, versus showing off all 11 pubkeys above for 11*(32+4)\nbytes, and the above is roughly the worst case for m-of-11 multisig\nvia MAST.\n\nIf everyone's happy to use MAST, then it could be the only solution:\ndrop OP_IF and friends, and require all the CHECKSIG ops to occur before\nany RETURN_VALID ops: since there's no branching, that's just a matter of\nreordering your script a bit and should be pretty easy for script authors.\n\nI think there's a couple of drawbacks to this approach that it shouldn't\nbe the only solution:\n\n a) we don't have a lot of experience with using MAST\n b) MAST is a bit more complicated than just dealing with branches in\n    a script (probably solvable once (a) is no longer the case) \n c) some useful scripts might be a bit cheaper expressed with \n    of branches and be better expressed without MAST\n\nIf other approaches than MAST are still desirable, then MAST works fine\nin combination with either of the earlier solutions as far as I can see.\n\nSummary\n-------\n\nI think something along the lines of solution 2 makes the most sense,\nso I think a good approach for aggregate signatures is:\n\n - introduce a new segwit witness version, which I'll call v2 (but which\n   might actually be v1 or v3 etc, of course)\n\n - v2 must support Schnorr signature verification.\n\n - v2 should have a \"pay to public key (hash?)\" witness format. direct\n   signatures of the transaction via the corresponding private key should\n   be aggregatable.\n\n - v2 should have a \"pay to script hash\" witness format: probably via\n   taproot+MAST, possibly via graftroot as well\n\n - v2 should support MAST scripts: again, probably via taproot+MAST\n\n - v2 taproot shouldn't have a separate script version (ie,\n   the pubkey shouldn't be P+H(P,version,scriptroot)), as signatures\n   for later-versioned scripts couldn't be aggregated, so there's no\n   advantage over bumping the segwit witness version\n\n - v2 scripts should have a CHECK_AGG_SIG_VERIFY opcode roughly as\n   described above for aggregating signatures, along with CHECK_AGG_SIGNERS\n\n - CHECK{MULTI,}SIG{VERIFY,} in v2 scripts shouldn't support aggregated\n   signatures, and possibly shouldn't be present at all?\n\n - v2 signers should be able to specify an aggregation bucket for each\n   signature, perhaps in the range 0-7 or so?\n\n - v2 scripts should have a bunch of RETURN_VALID opcodes for future\n   soft-forks, constrained so that CHECK_AGG_SIG_VERIFY doesn't appear\n   after them. the currently disabled opcodes should be redefined as\n   RETURN_VALID eg.\n\nFor soft-fork upgrades from that point:\n\n - introducing new opcodes just means redefining an RETURN_VALID opcode\n\n - introducing new sighash versions requires bumping the segwit witness\n   version (to v3, etc)\n\n - if non-interactive half-signature aggregation isn't ready to go, it\n   would likewise need a bump in the segwit witness version when\n   introduced\n\nI think it's worth considering bundling a hard-fork upgrade something\nlike:\n\n - ~5 years after v2 scripts are activated, existing p2pk/p2pkh UTXOs\n   (either matching the pre-segwit templates or v0 segwit p2wpkh) can\n   be spent via a v2-aggregated-signature (but not via taproot)\n   [4]\n\n - core will maintain a config setting that allows users to prevent\n   that hard fork from activating via UASF up until the next release\n   after activation (probably with UASF-enforced miner-signalling that\n   the hard-fork will not go ahead)\n\nThis is already very complicated of course, but note that there's still\n*more* things that need to be considered for signature aggregation:\n\n - whether to use Bellare-Neven or muSig in the consensus-critical\n   aggregation algorithm\n\n - whether to assign the aggregate sigs to inputs and plunk them in the\n   witness data somewhere, or to add a new structure and commitment and\n   worry about p2p impact\n\n - whether there are new sighash options that should go in at the same time\n\n - whether non-interactive half-sig aggregation can go in at the same time\n\nThat leads me to think that interactive signature aggregation is going to\ntake a lot of time and work, and it would make sense to do a v1-upgrade\nthat's \"just\" Schnorr (and taproot and MAST and re-enabling opcodes and\n...) in the meantime. YMMV.\n\nCheers,\naj\n\n[0] http://diyhpl.us/wiki/transcripts/bitcoin-core-dev-tech/2018-03-06-taproot-graftroot-etc/\n\n[1] Signature aggregation:\n\n    Signature aggregation is cool because it lets you post a transaction\n    spending many inputs, but only providing a single 64 byte signature\n    that proves authorisation by the holders of all the private keys\n    for all the inputs. So the witnesses for your inputs might be:\n\n     p2wpkh: pubkey1 SIGHASH_ALL\n     p2wpkh: pubkey2 SIGHASH_ALL\n     p2wsh: \"3 pubkey1 pubkey3 pubkey4 3 CHECKMULTISIG\" SIGHASH_ALL SIGHASH_ALL SIGHASH_ALL\n\n    where instead of including full 65-byte signature for each CHECKSIG\n    operation in each input witness, you just include the ~1-byte sighash,\n    and provide a single 64-byte signature elsewhere, calculated either\n    according to the Bellare-Neven algorithm, or the muSig algorithm.\n\n    In the above case, that means going from about 500 witness bytes\n    for 5 public keys and 5 signatures, to about 240 witness bytes for\n    5 public keys and just 1 signature.\n\n    A complication here is that because the signatures are aggregated,\n    in order to validate *any* signature you have to be able to validate\n    *every* signature.\n\n    It's possible to limit that a bit, and have aggregation\n    \"buckets\". This might be something you just choose when signing, eg:\n\n     p2wpkh: pubkey1 SIGHASH_ALL|BUCKET_1\n     p2wpkh: pubkey2 SIGHASH_ALL|BUCKET_2\n     p2wsh: \"3 pubkey1 pubkey3 pubkey4 3 CHECKMULTISIG\" SIGHASH_ALL|BUCKET_1 SIGHASH_ALL|BUCKET_2 SIGHASH_ALL|BUCKET_2\n\n     bucket1: 64 byte sig for (pubkey1, pubkey1)\n     bucket2: 64 byte sig for (pubkey2, pubkey3, pubkey4)\n\n    That way you get the choice to verify both of the pubkey1 signatures\n    or all of the pubkey{2,3,4} signatures or all the signatures (or\n    none of the signatures).\n\n    This might be useful if the private key for pubkey1 is essentially\n    offline, and can't easily participate in an interactive protocol\n    -- with separate buckets the separate signatures can be generated\n    independently at different times, while with only one bucket,\n    everyone has to coordinate to produce the signature)\n\n    (For clarity: each bucket corresponds to many CHECKSIG operations,\n    but only contains a single 64-byte signature)\n\n    Different buckets will also be necessary when dealing with new\n    segwit script versions: if there are any aggregated signatures for\n    v1 addresses that go into bucket X, then aggregate signatures for\n    v2 addresses cannot go into bucket X, as that would prevent nodes\n    that support v1 addresses but not v2 addresses from validating\n    bucket X, which would prevent them from validating the v1 addresses\n    corresponding to that bucket, which would make the v2 upgrade a hard\n    fork rather than a soft fork. So each segwit version will need to\n    introduce a new set of aggregation buckets, which in turn reduces\n    the benefit you get from signature aggregation.\n\n    Note that it's obviously fine to use an aggregated signature in\n    response to CHECKSIGVERIFY or n-of-n CHECKMULTISIGVERIFY -- when\n    processing the script you just assume it succeeds, relying on the\n    fact that the aggregated signature will fail the entire transaction\n    if there was a problem. However it's also fine to use an aggregated\n    signature in response to CHECKSIG for most plausible scripts, since:\n\n        sig key CHECKSIG\n\n    can be treated as equivalent to\n\n        sig DUP IF key CHECKSIGVERIFY OP_1 FI\n\n    provided invalid signatures are supplied as a \"false\" value. So\n    for the purpose of this email, I'll mostly be treating CHECKSIG and\n    n-of-n CHECKMULTISIG as if they support aggregation.\n\n[2] Soft-forks and RETURN_VALID:\n\n    There are two approaches for soft-forking in new opcodes that are\n    reasonably well understood:\n\n    1) We can bump the segwit script version, introducing a new class of\n       bc1 bech32 addresses, which behave however we like, but can't be\n       validated at all by existing nodes. This has the downside that it\n       effectively serialises upgrades.\n\n    2) We can redefine OP_NOP opcodes as OP_CHECK_foo_VERIFY\n       opcodes, along the same lines as OP_CHECKLOCKTIMEVERIFY or\n       OP_CHECKSEQUENCEVERIFY. This has the downside that it's pretty\n       restrictive in what new opcodes you can introduce.\n\n    A third approach seems possible as well though, which would combine\n    the benefits of both approaches: allowing any new opcode to be\n    introduced, and allowing different opcodes to be introduced in\n    concurrent soft-forks. Namely:\n\n    3) If we introduce some RETURN_VALID opcodes (in script for a new\n       segwit witness version), we can then redefine those as having any\n       behaviour we might want, including ones that manipulate the stack,\n       and have the change simply be a soft-fork. RETURN_VALID would\n       force the script to immediately succeed, in contrast to OP_RETURN\n       which forces the script to immediately fail.\n\n[3] https://twitter.com/bramcohen/status/972205820275388416\n\n[4] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-January/015580.html"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-03-21T07:53:59",
                "message_text_only": "Good morning aj,\n\nI am probably wrong, but could solution 2 be simplified by using the below opcodes for aggregated signatures?\n\nOP_ADD_AGG_PUBKEY - Adds a public key for verification of an aggregated signature.\n\nOP_CHECK_AGG_SIG[VERIFY] - Check that the gathered public keys matches the aggregated signature.\n\nThen:\n\n pubkey1 OP_ADD_AGG_PUBKEY\n OP_IF\n   pubkey2 OP_ADD_AGG_PUBKEY\n OP_ELSE\n   cond OP_CHECKCOVENANT\n OP_ENDIF\n OP_CHECK_AGG_SIG\n\n(omitting the existence of buckets)\n\nI imagine that aggregated signatures, being linear, would allow pubkey to be aggregated also by adding the pubkey points (but note that I am not a mathematician, I only parrot what better mathematicians say) so OP_ADD_AGG_PUBKEY would not require storing all public keys, just adding them linearly.\n\nThe effect is that in the OP_CHECKCOVENANT case, pre-softfork nodes will not actually do any checking.\n\nOP_CHECK_AGG_SIG might accept the signature on the stack (combined signature of pubkey1 and pubkey2 and from other inputs), or the bucket the signature is stored in.\n\nWe might even consider using the altstack: no more OP_ADD_AGG_PUBKEY (one less opcode to reserve!), just push pubkeys on the altstack, and OP_CHECK_AGG_SIG would take the entire altstack as all the public keys to be used in aggregated signature checking.\n\nThis way, rather than gathering signatures, we gather public keys for aggregate signature checking.  OP_RETURN_TRUE interacts with that by not performing aggregate signature checking at all if we encounter OP_RETURN_TRUE first (which makes sense: old nodes have no idea what OP_RETURN_TRUE is really doing, and would fail to understand all its details).\n\n\nI am very probably wrong but am willing to learn how to break the above, though.  I am probably making a mistake somewhere.\n\nRegards,\nZmnSCPxj\n\n\u200bSent with ProtonMail Secure Email.\u200b\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n\nOn March 21, 2018 12:06 PM, Anthony Towns via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hello world,\n> \n> There was a lot of discussion on Schnorr sigs and key and signature\n> \n> aggregation at the recent core-dev-tech meeting (one relevant conversation\n> \n> is transcribed at \\[0\\]).\n> \n> Quick summary, with more background detail in the corresponding footnotes:\n> \n> signature aggregation is awesome \\[1\\], and the possibility of soft-forking\n> \n> in new opcodes via OP\\_RETURN\\_VALID opcodes (instead of OP_NOP) is also\n> \n> awesome \\[2\\].\n> \n> Unfortunately doing both of these together may turn out to be awful.\n> \n> RETURN_VALID and Signature Aggregation\n> \n> \n> -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n> \n> Bumping segwit script versions and redefining OP_NOP opcodes are\n> \n> fairly straightforward to deal with even with signature aggregation,\n> \n> the straightforward implementation of both combined is still a soft-fork.\n> \n> RETURN_VALID, unfortunately, has a serious potential pitfall: any\n> \n> aggregatable signature operations that occur after it have to go into\n> \n> separate buckets.\n> \n> As an example of why this is the case, imagine introducing a covenant\n> \n> opcode that pulls a potentially complicated condition from the stack\n> \n> (perhaps, \"an output pays at least 50000 satoshi to address xyzzy\"),\n> \n> checks the condition against the transaction, and then pushes 1 (or 0)\n> \n> back onto the stack indicating compliance with the covenant (or not).\n> \n> You might then write a script allowing a single person to spend the coins\n> \n> if they comply with the covenant, and allow breaking the covenant with\n> \n> someone else's sign-off in addition. You could write this as:\n> \n> pubkey1 CHECKSIGVERIFY\n> \n> cond CHECKCOVENANT IFDUP NOTIF pubkey2 CHECKSIG ENDIF\n> \n> If you pass the covenant, you supply \"SIGHASHALL|BUCKET_1\" and aggregate\n> \n> the signature for pubkey1 into bucket1 and you're set; otherwise you supply\n> \n> \"SIGHASHALL|BUCKET\\_1 SIGHASHALL|BUCKET\\_1\" and aggregate signatures for both\n> \n> pubkey1 and pubkey2 into bucket1 and you're set. Great!\n> \n> But this isn't a soft-fork: old nodes would see this script as:\n> \n> pubkey1 CHECKSIGVERIFY\n> \n> cond RETURN_VALID IFDUP NOTIF pubkey2 CHECKSIG ENDIF\n> \n> which it would just interpret as:\n> \n> pubkey1 CHECKSIGVERIFY cond RETURN_VALID\n> \n> which is fine if the covenant was passing; but no good if the covenant\n> \n> didn't pass -- they'd be expecting the aggregted sig to just be for\n> \n> pubkey1 when it's actually pubkey1+pubkey2, so old nodes would fail the\n> \n> tx and new nodes would accept it, making it a hard fork.\n> \n> Solution 0a / 0b\n> \n> \n> ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n> \n> There are two obvious solutions here:\n> \n> 0a) Just be very careful to ensure any aggregated signatures that\n> \n> are conditional on an redefined RETURN_VALID opcode go into later\n> \n> buckets, but be careful about having separate sets of buckets every\n> \n> time a soft-fork introduces a new redefined opcode. Probably very\n> \n> complicated to implement correctly, and essentially doubles the\n> \n> number of buckets you have to potentially deal with every time you\n> \n> soft fork in a new opcode.\n> \n> 0b) Alternatively, forget about the hope that RETURN_VALID\n> \n> opcodes could be converted to anything, and just reserve OP_NOP\n> \n> opcodes and convert them to CHECK\\_foo\\_VERIFY opcodes just as we\n> \n> have been doing, and when we can't do that bump the segwit witness\n> \n> version for a whole new version of script. Or in twitter speak:\n> \n> \"non-verify upgrades should be done with new script versions\" \\[3\\]\n> \n> I think with a little care we can actually salvage RETURN_VALID though!\n> \n> Solution 1\n> \n> \n> ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n> \n> You don't actually have to write your scripts in ways that can cause\n> \n> this problem, as long as you're careful. In particular, the problem only\n> \n> occurs if you do aggregatable CHECKSIG operations after \"RETURN_VALID\"\n> \n> \\-\\- if you do all the CHECKSIGs first, then all nodes will be checking\n> \n> for the same signatures, and there's no problem. So you could rewrite\n> \n> the script above as:\n> \n> pubkey1 CHECKSIGVERIFY\n> \n> IF pubkey2 CHECKSIG ENDIF\n> \n> cond CHECKCOVENANT OR\n> \n> which is redeemable either by:\n> \n> sig1 0 \\[and covenant is met\\]\n> \n> sig1 1 sig2 \\[covenant is not checked\\]\n> \n> The witness in this case is essentially committing to the execution path\n> \n> that would have been taken in the first script by a fully validating node,\n> \n> then the new script checks all the signatures, and then validates that the\n> \n> committed execution path was in fact the one that was meant to be taken.\n> \n> If people are clever enough to write scripts this way, I believe you\n> \n> can make RETURN_VALID soft-fork safe simply by having every soft-forked\n> \n> RETURN_VALID operation set a state flag that makes every subsequent\n> \n> CHECKSIG operation require a non-aggregated sig.\n> \n> The drawback of this approach is that if the script is complicated\n> \n> (eg it has multiple IF conditions, some of which are nested), it may be\n> \n> difficult to write the script to ensure the signatures are checked in the\n> \n> same combination as the later logic actually requires -- you might have\n> \n> to store the flag indicating whether you checked particular signatures\n> \n> on the altstack, or use DUP and PICK/ROLL to organise it on the stack.\n> \n> Solution 2\n> \n> \n> ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n> \n> We could make that simpler for script authors by making dedicated opcodes\n> \n> to help with \"do all the signatures first\" and \"check the committed\n> \n> execution path against reality\" steps. I think a reasonable approach\n> \n> would be something like:\n> \n> 0b01 pubkey2 pubkey1 2 CHECK\\_AGGSIG\\_VERIFY\n> \n> cond CHECKCOVENANT 0b10 CHECK\\_AGG\\_SIGNERS OR\n> \n> which is redeemed either by:\n> \n> sighash1 0 \\[and passing the covenant cond\\]\n> \n> sighash2 sighash1 0b10\n> \n> (I'm using the notation 0b10110 to express numbers as binary bitfields;\n> \n> 0b10110 = 22 eg)\n> \n> That is, two new opcodes, namely:\n> \n> CHECK\\_AGGSIG\\_VERIFY which takes from the stack:\n> \n> \\- N: a count of pubkeys\n> \n> \\- pubkey1..pubkeyN: N pubkeys\n> \n> \\- REQ: a bitmask of which pubkeys are required to sign\n> \n> \\- OPT: a bitmask of which optional pubkeys have signed\n> \n> \\- sighashes: M sighashes for the pubkeys corresponding to the set\n> \n> bits of (REQ|OPT)\n> \n> CHECK\\_AGGSIG\\_VERIFY fails if:\n> \n> \\- the stack doesn't have enough elements\n> \n> \\- the aggregated signature doesn't pass\n> \n> \\- a redefined RETURN_VALID opcode has already been seen\n> \n> \\- a previous CHECK\\_AGGSIG\\_VERIFY has already been seen in this script\n> \n> REQ|OPT is stored as state\n> \n> CHECK\\_AGG\\_SIGNERS takes from the stack:\n> \n> \\- B: a bitmask of which pubkeys are being queried\n> \n> and it pushes to the stack 1 or 0 based on:\n> \n> \\- (REQ|OPT) & B == B ? 1 : 0\n> \n> A possible way to make sure the \"no agg sigs after an upgraded\n> \n> RETURN\\_VALID\" behaviour works right might be to have \"RETURN\\_VALID\"\n> \n> fail if CHECK\\_AGGSIG\\_VERIFY hasn't already been seen. That way once you\n> \n> redefine RETURN\\_VALID in a soft-fork, if you have a CHECK\\_AGGSIG_VERIFY\n> \n> after a RETURN_VALID you've either already failed (because the\n> \n> RETURN\\_VALID wasn't after a CHECK\\_AGGSIG_VERIFY), or you automatically\n> \n> fail (because you've already seen a CHECK\\_AGGSIG\\_VERIFY).\n> \n> There would be no need to make CHECKSIG, CHECKSIGVERIFY, CHECKMULTISIG\n> \n> and CHECKMULTISIGVERIFY do signature aggregation in this case. They could\n> \n> be left around to allow script authors to force non-aggregate signatures\n> \n> or could be dropped entirely, I think.\n> \n> This construct would let you do M-of-N aggregated multisig in a fairly\n> \n> straightforward manner without needing an explicit opcode, eg:\n> \n> 0 pubkey5 pubkey4 pubkey3 pubkey2 pubkey1 5 CHECK\\_AGGSIG\\_VERIFY\n> \n> 0b10000 CHECK\\_AGG\\_SIGNERS\n> \n> 0b01000 CHECK\\_AGG\\_SIGNERS ADD\n> \n> 0b00100 CHECK\\_AGG\\_SIGNERS ADD\n> \n> 0b00010 CHECK\\_AGG\\_SIGNERS ADD\n> \n> 0b00001 CHECK\\_AGG\\_SIGNERS ADD\n> \n> 3 NUMEQUAL\n> \n> redeemable by, eg:\n> \n> 0b10110 sighash5 sighash3 sighash2\n> \n> and a single aggregate signature by the private keys corresponding to\n> \n> pubkey{2,3,5}.\n> \n> Of course, another way of getting M-of-N aggregated multisig is via MAST,\n> \n> which brings us to another approach...\n> \n> Solution 3\n> \n> \n> ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n> \n> All we're doing above is committing to an execution path and validating\n> \n> signatures for that path before checking the path was the right one. But\n> \n> MAST is a great way of committing to an execution path, so another\n> \n> approach would just be \"don't have alternative execution paths, just have\n> \n> MAST and CHECK/VERIFY codes\". Taking the example I've been running with,\n> \n> that would be:\n> \n> branch1: 2 pubkey2 pubkey1 2 CHECKMULTISIG\n> \n> branch2: pubkey1 CHECKSIGVERIFY cond CHECKCOVENANT\n> \n> So long as MAST is already supported when signature aggregation becomes\n> \n> possible, that works fine. The drawback is MAST can end up with lots of\n> \n> branches, eg the 3-of-5 multisig check has 10 branches:\n> \n> branch1: 3 pubkey3 pubkey2 pubkey1 3 CHECKMULTISIG\n> \n> branch2: 3 pubkey4 pubkey2 pubkey1 3 CHECKMULTISIG\n> \n> branch3: 3 pubkey5 pubkey2 pubkey1 3 CHECKMULTISIG\n> \n> branch4: 3 pubkey4 pubkey3 pubkey1 3 CHECKMULTISIG\n> \n> branch5: 3 pubkey5 pubkey3 pubkey1 3 CHECKMULTISIG\n> \n> branch6: 3 pubkey5 pubkey4 pubkey1 3 CHECKMULTISIG\n> \n> branch7: 3 pubkey4 pubkey3 pubkey2 3 CHECKMULTISIG\n> \n> branch8: 3 pubkey5 pubkey3 pubkey2 3 CHECKMULTISIG\n> \n> branch9: 3 pubkey5 pubkey4 pubkey2 3 CHECKMULTISIG\n> \n> branch10: 3 pubkey5 pubkey4 pubkey3 3 CHECKMULTISIG\n> \n> while if you want, say, 6-of-11 multisig you get 462 branches, versus\n> \n> just:\n> \n> 0 pubkey11 pubkey10 pubkey9 pubkey8 pubkey7 pubkey6\n> \n> pubkey5 pubkey4 pubkey3 pubkey2 pubkey1 11 CHECK\\_AGGSIG\\_VERIFY\n> \n> 0b10000000000 CHECK\\_AGG\\_SIGNERS\n> \n> 0b01000000000 CHECK\\_AGG\\_SIGNERS ADD\n> \n> 0b00100000000 CHECK\\_AGG\\_SIGNERS ADD\n> \n> 0b00010000000 CHECK\\_AGG\\_SIGNERS ADD\n> \n> 0b00001000000 CHECK\\_AGG\\_SIGNERS ADD\n> \n> 0b00000100000 CHECK\\_AGG\\_SIGNERS ADD\n> \n> 0b00000010000 CHECK\\_AGG\\_SIGNERS ADD\n> \n> 0b00000001000 CHECK\\_AGG\\_SIGNERS ADD\n> \n> 0b00000000100 CHECK\\_AGG\\_SIGNERS ADD\n> \n> 0b00000000010 CHECK\\_AGG\\_SIGNERS ADD\n> \n> 0b00000000001 CHECK\\_AGG\\_SIGNERS ADD\n> \n> 6 NUMEQUAL\n> \n> Provided doing lots of hashes to calculate merkle paths is cheaper than\n> \n> publishing to the blockchain, MAST will likely still be better though:\n> \n> you'd be doing 6 pubkeys and 9 steps in the merkle path for about 1532bytes in MAST, versus showing off all 11 pubkeys above for 11(32+4)\n> \n> bytes, and the above is roughly the worst case for m-of-11 multisig\n> \n> via MAST.\n> \n> If everyone's happy to use MAST, then it could be the only solution:\n> \n> drop OP_IF and friends, and require all the CHECKSIG ops to occur before\n> \n> any RETURN_VALID ops: since there's no branching, that's just a matter of\n> \n> reordering your script a bit and should be pretty easy for script authors.\n> \n> I think there's a couple of drawbacks to this approach that it shouldn't\n> \n> be the only solution:\n> \n> a) we don't have a lot of experience with using MAST\n> \n> b) MAST is a bit more complicated than just dealing with branches in\n> \n> a script (probably solvable once (a) is no longer the case)\n> \n> c) some useful scripts might be a bit cheaper expressed with\n> \n> of branches and be better expressed without MAST\n> \n> If other approaches than MAST are still desirable, then MAST works fine\n> \n> in combination with either of the earlier solutions as far as I can see.\n> \n> Summary\n> \n> \n> ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n> \n> I think something along the lines of solution 2 makes the most sense,\n> \n> so I think a good approach for aggregate signatures is:\n> \n> -   introduce a new segwit witness version, which I'll call v2 (but which\n>     \n>     might actually be v1 or v3 etc, of course)\n>     \n> -   v2 must support Schnorr signature verification.\n> -   v2 should have a \"pay to public key (hash?)\" witness format. direct\n>     \n>     signatures of the transaction via the corresponding private key should\n>     \n>     be aggregatable.\n>     \n> -   v2 should have a \"pay to script hash\" witness format: probably via\n>     \n>     taproot+MAST, possibly via graftroot as well\n>     \n> -   v2 should support MAST scripts: again, probably via taproot+MAST\n> -   v2 taproot shouldn't have a separate script version (ie,\n>     \n>     the pubkey shouldn't be P+H(P,version,scriptroot)), as signatures\n>     \n>     for later-versioned scripts couldn't be aggregated, so there's no\n>     \n>     advantage over bumping the segwit witness version\n>     \n> -   v2 scripts should have a CHECK\\_AGG\\_SIG_VERIFY opcode roughly as\n>     \n>     described above for aggregating signatures, along with CHECK\\_AGG\\_SIGNERS\n>     \n> -   CHECK{MULTI,}SIG{VERIFY,} in v2 scripts shouldn't support aggregated\n>     \n>     signatures, and possibly shouldn't be present at all?\n>     \n> -   v2 signers should be able to specify an aggregation bucket for each\n>     \n>     signature, perhaps in the range 0-7 or so?\n>     \n> -   v2 scripts should have a bunch of RETURN_VALID opcodes for future\n>     \n>     soft-forks, constrained so that CHECK\\_AGG\\_SIG_VERIFY doesn't appear\n>     \n>     after them. the currently disabled opcodes should be redefined as\n>     \n>     RETURN_VALID eg.\n>     \n>     For soft-fork upgrades from that point:\n>     \n> -   introducing new opcodes just means redefining an RETURN_VALID opcode\n> -   introducing new sighash versions requires bumping the segwit witness\n>     \n>     version (to v3, etc)\n>     \n> -   if non-interactive half-signature aggregation isn't ready to go, it\n>     \n>     would likewise need a bump in the segwit witness version when\n>     \n>     introduced\n>     \n>     I think it's worth considering bundling a hard-fork upgrade something\n>     \n>     like:\n>     \n> -   ~5 years after v2 scripts are activated, existing p2pk/p2pkh UTXOs\n>     \n>     (either matching the pre-segwit templates or v0 segwit p2wpkh) can\n>     \n>     be spent via a v2-aggregated-signature (but not via taproot)\n>     \n>     \\[4\\]\n>     \n> -   core will maintain a config setting that allows users to prevent\n>     \n>     that hard fork from activating via UASF up until the next release\n>     \n>     after activation (probably with UASF-enforced miner-signalling that\n>     \n>     the hard-fork will not go ahead)\n>     \n>     This is already very complicated of course, but note that there's still\n>     \n>     more things that need to be considered for signature aggregation:\n>     \n> -   whether to use Bellare-Neven or muSig in the consensus-critical\n>     \n>     aggregation algorithm\n>     \n> -   whether to assign the aggregate sigs to inputs and plunk them in the\n>     \n>     witness data somewhere, or to add a new structure and commitment and\n>     \n>     worry about p2p impact\n>     \n> -   whether there are new sighash options that should go in at the same time\n> -   whether non-interactive half-sig aggregation can go in at the same time\n>     \n>     That leads me to think that interactive signature aggregation is going to\n>     \n>     take a lot of time and work, and it would make sense to do a v1-upgrade\n>     \n>     that's \"just\" Schnorr (and taproot and MAST and re-enabling opcodes and\n>     \n>     ...) in the meantime. YMMV.\n>     \n>     Cheers,\n>     \n>     aj\n>     \n>     \\[0\\] http://diyhpl.us/wiki/transcripts/bitcoin-core-dev-tech/2018-03-06-taproot-graftroot-etc/\n>     \n>     \\[1\\] Signature aggregation:\n>     \n>     Signature aggregation is cool because it lets you post a transaction\n>     \n>     spending many inputs, but only providing a single 64 byte signature\n>     \n>     that proves authorisation by the holders of all the private keys\n>     \n>     for all the inputs. So the witnesses for your inputs might be:\n>     \n>     p2wpkh: pubkey1 SIGHASH_ALL\n>     \n>     p2wpkh: pubkey2 SIGHASH_ALL\n>     \n>     p2wsh: \"3 pubkey1 pubkey3 pubkey4 3 CHECKMULTISIG\" SIGHASH\\_ALL SIGHASH\\_ALL SIGHASH_ALL\n>     \n>     where instead of including full 65-byte signature for each CHECKSIG\n>     \n>     operation in each input witness, you just include the ~1-byte sighash,\n>     \n>     and provide a single 64-byte signature elsewhere, calculated either\n>     \n>     according to the Bellare-Neven algorithm, or the muSig algorithm.\n>     \n>     In the above case, that means going from about 500 witness bytes\n>     \n>     for 5 public keys and 5 signatures, to about 240 witness bytes for\n>     \n>     5 public keys and just 1 signature.\n>     \n>     A complication here is that because the signatures are aggregated,\n>     \n>     in order to validate any signature you have to be able to validate\n>     \n>     every signature.\n>     \n>     It's possible to limit that a bit, and have aggregation\n>     \n>     \"buckets\". This might be something you just choose when signing, eg:\n>     \n>     p2wpkh: pubkey1 SIGHASH\\_ALL|BUCKET\\_1\n>     \n>     p2wpkh: pubkey2 SIGHASH\\_ALL|BUCKET\\_2\n>     \n>     p2wsh: \"3 pubkey1 pubkey3 pubkey4 3 CHECKMULTISIG\" SIGHASH\\_ALL|BUCKET\\_1 SIGHASH\\_ALL|BUCKET\\_2 SIGHASH\\_ALL|BUCKET\\_2\n>     \n>     bucket1: 64 byte sig for (pubkey1, pubkey1)\n>     \n>     bucket2: 64 byte sig for (pubkey2, pubkey3, pubkey4)\n>     \n>     That way you get the choice to verify both of the pubkey1 signatures\n>     \n>     or all of the pubkey{2,3,4} signatures or all the signatures (or\n>     \n>     none of the signatures).\n>     \n>     This might be useful if the private key for pubkey1 is essentially\n>     \n>     offline, and can't easily participate in an interactive protocol\n>     \n>     \\-\\- with separate buckets the separate signatures can be generated\n>     \n>     independently at different times, while with only one bucket,\n>     \n>     everyone has to coordinate to produce the signature)\n>     \n>     (For clarity: each bucket corresponds to many CHECKSIG operations,\n>     \n>     but only contains a single 64-byte signature)\n>     \n>     Different buckets will also be necessary when dealing with new\n>     \n>     segwit script versions: if there are any aggregated signatures for\n>     \n>     v1 addresses that go into bucket X, then aggregate signatures for\n>     \n>     v2 addresses cannot go into bucket X, as that would prevent nodes\n>     \n>     that support v1 addresses but not v2 addresses from validating\n>     \n>     bucket X, which would prevent them from validating the v1 addresses\n>     \n>     corresponding to that bucket, which would make the v2 upgrade a hard\n>     \n>     fork rather than a soft fork. So each segwit version will need to\n>     \n>     introduce a new set of aggregation buckets, which in turn reduces\n>     \n>     the benefit you get from signature aggregation.\n>     \n>     Note that it's obviously fine to use an aggregated signature in\n>     \n>     response to CHECKSIGVERIFY or n-of-n CHECKMULTISIGVERIFY -- when\n>     \n>     processing the script you just assume it succeeds, relying on the\n>     \n>     fact that the aggregated signature will fail the entire transaction\n>     \n>     if there was a problem. However it's also fine to use an aggregated\n>     \n>     signature in response to CHECKSIG for most plausible scripts, since:\n>     \n>     sig key CHECKSIG\n>     \n>     can be treated as equivalent to\n>     \n>     sig DUP IF key CHECKSIGVERIFY OP_1 FI\n>     \n>     provided invalid signatures are supplied as a \"false\" value. So\n>     \n>     for the purpose of this email, I'll mostly be treating CHECKSIG and\n>     \n>     n-of-n CHECKMULTISIG as if they support aggregation.\n>     \n>     \\[2\\] Soft-forks and RETURN_VALID:\n>     \n>     There are two approaches for soft-forking in new opcodes that are\n>     \n>     reasonably well understood:\n>     \n>     1.  We can bump the segwit script version, introducing a new class of\n>         \n>         bc1 bech32 addresses, which behave however we like, but can't be\n>         \n>         validated at all by existing nodes. This has the downside that it\n>         \n>         effectively serialises upgrades.\n>         \n>     2.  We can redefine OP\\_NOP opcodes as OP\\_CHECK\\_foo\\_VERIFY\n>         \n>         opcodes, along the same lines as OP_CHECKLOCKTIMEVERIFY or\n>         \n>         OP_CHECKSEQUENCEVERIFY. This has the downside that it's pretty\n>         \n>         restrictive in what new opcodes you can introduce.\n>         \n>         A third approach seems possible as well though, which would combine\n>         \n>         the benefits of both approaches: allowing any new opcode to be\n>         \n>         introduced, and allowing different opcodes to be introduced in\n>         \n>         concurrent soft-forks. Namely:\n>         \n>     3.  If we introduce some RETURN_VALID opcodes (in script for a new\n>         \n>         segwit witness version), we can then redefine those as having any\n>         \n>         behaviour we might want, including ones that manipulate the stack,\n>         \n>         and have the change simply be a soft-fork. RETURN_VALID would\n>         \n>         force the script to immediately succeed, in contrast to OP_RETURN\n>         \n>         which forces the script to immediately fail.\n>         \n>         \\[3\\] https://twitter.com/bramcohen/status/972205820275388416\n>         \n>         \\[4\\] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-January/015580.html\n>         \n> \n> bitcoin-dev mailing list\n> \n> bitcoin-dev at lists.linuxfoundation.org\n> \n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Andrew Poelstra",
                "date": "2018-03-21T12:45:21",
                "message_text_only": "On Wed, Mar 21, 2018 at 02:06:18PM +1000, Anthony Towns via bitcoin-dev wrote:\n> \n> That leads me to think that interactive signature aggregation is going to\n> take a lot of time and work, and it would make sense to do a v1-upgrade\n> that's \"just\" Schnorr (and taproot and MAST and re-enabling opcodes and\n> ...) in the meantime. YMMV.\n>\n\nUnfortunately I agree. Another complication with aggregate signatures is\nthat they complicate blind signature protocols such as [1]. In particular\nthey break the assumption \"one signature can spend at most one UTXO\"\nmeaning that a blind signer cannot tell how many coins they're authorizing\nwith a given signature, even if they've ensured that the key they're using\nonly controls UTXOs of a fixed value.\n\nThis seems solvable with creative use of ZKPs, but the fact that it's even\na problem caught me off guard, and makes me think that signature aggregation\nis much harder to think about than e.g. Taproot which does not change\nsignature semantics at all.\n\n\nAndrew\n\n\n\n[1] https://github.com/jonasnick/scriptless-scripts/blob/blind-swaps/md/partially-blind-swap.md\n\n\n\n-- \nAndrew Poelstra\nMathematics Department, Blockstream\nEmail: apoelstra at wpsoftware.net\nWeb:   https://www.wpsoftware.net/andrew\n\n\"A goose alone, I suppose, can know the loneliness of geese\n who can never find their peace,\n whether north or south or west or east\"\n       --Joanna Newsom\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180321/535daaca/attachment.sig>"
            },
            {
                "author": "Anthony Towns",
                "date": "2018-03-21T11:21:19",
                "message_text_only": "On Wed, Mar 21, 2018 at 03:53:59AM -0400, ZmnSCPxj wrote:\n> Good morning aj,\n\nGood evening Zeeman!\n\n[pulled from the bottom of your mail]\n> This way, rather than gathering signatures, we gather public keys for aggregate signature checking.  \n\nSorry, I probably didn't explain it well (or at all): during the script,\nyou're collecting public keys and messages (ie, BIP 143 style digests)\nwhich then go into the signing/verification algorithm to produce/check\nthe signature.\n\nYou do need to gather signatures from each private key holder when\nproducing the aggregate signature, but that happens at the wallet/p2p\nlevel, rather than the consensus level.\n\n> I am probably wrong, but could solution 2 be simplified by using the below opcodes for aggregated signatures?\n> \n> OP_ADD_AGG_PUBKEY - Adds a public key for verification of an aggregated signature.\n> OP_CHECK_AGG_SIG[VERIFY] - Check that the gathered public keys matches the aggregated signature.\n\nChecking the gathered public keys match the aggregated signature is\nsomething that only happens for the entire transaction as a whole, so\nyou don't need an opcode for it in the scripts, since they're per-input.\n\nOtherwise, I think that's pretty similar to what I was already saying;\nhaving:\n\n   SIGHASH_ALL|BUCKET_1 pubkey OP_CHECKSIG\n\nwould be adding \"pubkey\" and a message hash calculated via the SIGHASH_ALL\nhashing rules to the list of things that the signature for bucket 1 verifies.\n\nFWIW, the Bellare-Neven verification algorithm looks something like:\n\n   s*G = R + K   (s,R is the signature)\n   K = sum( H(R, L, i, m) * X_i )   for i corresponding to each pubkey X_i\n   L = the concatenation of all the pubkeys, X_0..X_n\n   m = the concatenation of all the message hashes, m_0..m_n\n\nSo the way I look at it is each input puts a public key and a message hash\n(X_i, m_i) into the bucket via a CHECKSIG operation (or similar), and once\nyou're done, you look into the bucket and there's just a single signature\n(s,R) left to verify. You can't start verifying any of it until you've\nlooked through all the scripts because you need to know L and m before\nyou can do anything, and both of those require info from every part of\nthe aggregation.\n\n[0] [1]\n\n> The effect is that in the OP_CHECKCOVENANT case, pre-softfork nodes will not actually do any checking.\n\nPre-softfork nodes not doing any checking doesn't work with cross-input\nsignature aggregation as far as I can see. If it did, all you would have\nto do to steal people's funds is mine a non-standard transaction:\n\n  inputs:\n   my-millions:\n     pay-to-pubkey pubkey1\n     witness=SIGHASH_ALL|BUCKET_1\n   your-two-cents:\n     pay-to-script-hash script=[1 OP_RETURN_TRUE pubkey2 CHECKSIG]\n     witness=SIGHASH_ALL|BUCKET_1\n\n   bucket1: 64-random-bytes\n  output:\n   all-the-money: you\n\nBecause there's no actual soft-fork at this point every node is an \"old\"\nnode, so they all see the OP_RETURN_TRUE and stop validating signatures,\naccepting the transaction as valid, and giving you all my money, despite\nyou being unable to actually produce my signature.\n\nMake sense?\n\nCheers,\naj\n\n[0] For completeness: constructing the signature for Bellare-Neven\n    requires two communication phases amongst the signers, and looks\n    roughly like:\n\n     1. each party generates a random variable r_i, and sharing the\n        corresponding curve point R_i=r_i*G and their sighash choice\n        (ie, m_i) with the other signers.\n\n     2. this allows each party to calculate R=sum(R_i) and m,\n        and hence H(R,L,i,m), at which point each party calculates a\n        partial signature using their respective private key, x_i:\n\n          s_i = r_i + H(R,L,i,m)*x_i\n\n        all these s_i values are then communicated to each signer.\n\n     3. these combine to give the final signature (s,R),\n        with s=sum(s_i), allowing each signer to verify that the signing\n        protocol completed successfully, and any signer can broadcast\n        the transaction to the blockchain\n\n[1] muSig differs in the details, but is basically the same."
            },
            {
                "author": "ZmnSCPxj at protonmail.com",
                "date": "2018-03-21T23:28:00",
                "message_text_only": "Good morning aj,\n\n\n\n\n\u200bSent with ProtonMail Secure Email.\u200b\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n\nOn March 21, 2018 7:21 PM, Anthony Towns <aj at erisian.com.au> wrote:\n\n> On Wed, Mar 21, 2018 at 03:53:59AM -0400, ZmnSCPxj wrote:\n> \n> > Good morning aj,\n> \n> Good evening Zeeman!\n> \n> [pulled from the bottom of your mail]\n> \n> > This way, rather than gathering signatures, we gather public keys for aggregate signature checking.\n> \n> Sorry, I probably didn't explain it well (or at all): during the script,\n> \n> you're collecting public keys and messages (ie, BIP 143 style digests)\n> \n> which then go into the signing/verification algorithm to produce/check\n> \n> the signature.\n\nYes, I think this is indeed what OP_CHECK_AGG_SIG really does.\n\nWhat I propose is that we have two places where we aggregate public keys: one at the script level, and one at the transaction level.  OP_ADD_AGG_PUBKEY adds to the script-level aggregate, then OP_CHECK_AGG_SIG adds the script-level aggregate to the transaction-level aggregate.\n\nUnfortunately it will not work since transaction-level aggregate (which is actually what gets checked) is different between pre-fork and post-fork nodes.\n\nIt looks like signature aggregation is difficult to reconcile with script...\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Bram Cohen",
                "date": "2018-03-22T00:47:01",
                "message_text_only": "Regarding the proposed segwit v2 with reclaiming most things as\nRETURN_VALID, the net result for what's being proposed in the near future\nfor supporting aggregated signatures in the not-so-near future is to punt.\nA number of strategies are possible for how to deal with new opcodes being\nadded later on, and the general strategy of making unused opcodes be\nRETURN_VALID for now and figuring out how to handle it later works for all\nof them. I think this is the right approach, but wanted to clarify that it\nis in fact the approach being proposed.\n\nThat said, there are some subtleties to getting it right which the last\nmessage doesn't really cover. Most unused opcodes should be reclaimed as\nRETURN_VALID, but there should still be one OP_NOP and there should be a\n'real' RETURN_VALID, which (a) is guaranteed to not be soft forked into\nsomething else in the future, and (b) doesn't have any parsing weirdness.\nThe parsing weirdness of all the unclaimed opcodes is interesting. Because\neverything in an IF clause needs to be parsed in order to find where the\nELSE is, you have a few options for dealing with an unknown opcode getting\nparsed in an unexecuted section of code. They are (a) avoid the problem\ncompletely by exterminating IF and MASTing (b) avoid the problem completely\nby getting rid of IF and adding IFJUMP, IFNJUMP, and JUMP which specify a\nnumber of bytes (this also allows for script merkleization) (c) require all\nnew opcodes have fixed length 1, even after they're soft forked, (d) do\nalmost like (c) but require that on new soft forks people hack their old\nscripts to still parse properly by avoiding the OP_ELSE in inopportune\nplaces (yuck!) (e) make it so that the unknown opcodes case a RETURN_VALID\neven when they're parsed, regardless of whether they're being executed.\n\nBy far the most expedient option is (e) cause a RETURN_VALID at parse time.\nThere's even precedent for this sort of behavior in the other direction\nwith disabled opcodes causing failure at parse time even if they aren't\nbeing executed.\n\nA lot can be said about all the options, but one thing I feel like snarking\nabout is that if you get rid of IFs using MAST, then it's highly unclear\nwhether OP_DEPTH should be nuked as well. My feeling is that it should and\nthat strict parsing should require that the bottom thing in the witness\ngets referenced at some point.\n\nHacking in a multisig opcode isn't a horrible idea, but it is very stuck\nspecifically on m-of-n and doesn't support more complex formulas for how\nsignatures can be combined, which makes it feel hacky and weird.\n\nAlso it may make sense to seriously consider BLS signatures, which have a\nlot of practical benefits starting with them being noninteractively\naggregatable so you can always assume that they're aggregated instead of\nrequiring complex semantics to specify what's aggregated with what. My team\nis working on an implementation which has several advantages over what's\ncurrently in the published literature but it isn't quite ready for public\nconsumption yet. This should probably go on the pile of reasons why it's\npremature to finalize a plan for aggregation at this point.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180321/c0b652a2/attachment.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2018-03-27T06:34:33",
                "message_text_only": "On Wed, Mar 21, 2018 at 05:47:01PM -0700, Bram Cohen via bitcoin-dev wrote:\n> [...] Most unused opcodes should be reclaimed as RETURN_VALID,\n> but there should still be one OP_NOP and there should be a 'real' RETURN_VALID,\n> which (a) is guaranteed to not be soft forked into something else in the\n> future, and (b) doesn't have any parsing weirdness.\n\nWhat's the reason for those? I could see an argument for RETURN_VALID, I guess:\n\n  confA IF condB IF condC IF [pathA] RETURN_VALID ENDIF ENDIF ENDIF [pathB]\n\nis probably simpler and saves 3 bytes compared to:\n\n  1 condA IF condB IF condC IF [pathA] NOT ENDIF ENDIF ENDIF IF [pathB] ENDIF\n\nbut that doesn't seem crazy compelling? I don't see a reason to just keep\none OP_NOP though.\n\n> The parsing weirdness of\n> all the unclaimed opcodes is interesting. Because everything in an IF clause\n> needs to be parsed in order to find where the ELSE is, you have a few options\n> for dealing with an unknown opcode getting parsed in an unexecuted section of\n> code. They are (a) avoid the problem completely by exterminating IF and MASTing\n> (b) avoid the problem completely by getting rid of IF and adding IFJUMP,\n> IFNJUMP, and JUMP which specify a number of bytes (this also allows for script\n> merkleization) (c) require all new opcodes have fixed length 1, even after\n> they're soft forked, (d) do almost like (c) but require that on new soft forks\n> people hack their old scripts to still parse properly by avoiding the OP_ELSE\n> in inopportune places (yuck!) (e) make it so that the unknown opcodes case a\n> RETURN_VALID even when they're parsed, regardless of whether they're being\n> executed.\n\nI was figuring (c), fwiw, and assuming that opcodes will just be about\nmanipulating stack values and marking the script as invalid, rather than,\nsay, introducing new flow control ops.\n\n> By far the most expedient option is (e) cause a RETURN_VALID at parse time.\n> There's even precedent for this sort of behavior in the other direction with\n> disabled opcodes causing failure at parse time even if they aren't being\n> executed.\n\nYou're probably right. That still doesn't let you implement intercal's\nCOMEFROM statement as a new opcode, of course. :)\n\n> A lot can be said about all the options, but one thing I feel like snarking\n> about is that if you get rid of IFs using MAST, then it's highly unclear\n> whether OP_DEPTH should be nuked as well. My feeling is that it should and that\n> strict parsing should require that the bottom thing in the witness gets\n> referenced at some point.\n\nI guess when passing the script you could perhaps check if each witness\nitem could have been replaced with OP_FALSE or OP_1 and still get the\nsame result, and consider the transaction non-standard if so?\n\n> Hacking in a multisig opcode isn't a horrible idea, but it is very stuck\n> specifically on m-of-n and doesn't support more complex formulas for how\n> signatures can be combined, which makes it feel hacky and weird.\n\nHmm? The opcode I suggested works just as easily with arbitrary formulas,\neg, \"There must be at least 1 signer from pka{1,2,3}, and 3 signers all\nup, except each of pkb{1,2,3,4,5,6} only counts for half\":\n\n  0 pkb6 pkb5 pkb4 pkb3 pkb2 pkb1 pka3 pka2 pka1 9 CHECK_AGGSIG_VERIFY\n    (declare pubkeys)\n  0b111 CHECK_AGG_SIGNERS VERIFY\n    (one of pka{1,2,3} must sign)\n  0b001 CHECK_AGG_SIGNERS\n  0b010 CHECK_AGG_SIGNERS ADD\n  0b100 CHECK_AGG_SIGNERS ADD\n  DUP ADD\n    (pka{1,2,3} count double)\n  0b000001000 CHECK_AGG_SIGNERS ADD\n  0b000010000 CHECK_AGG_SIGNERS ADD\n  0b000100000 CHECK_AGG_SIGNERS ADD\n  0b001000000 CHECK_AGG_SIGNERS ADD\n  0b010000000 CHECK_AGG_SIGNERS ADD\n  0b100000000 CHECK_AGG_SIGNERS ADD\n    (pkb{1..6} count single)\n  6 EQUAL\n    (summing to a total of 3 doubled)\n\nNot sure that saves it from being \"hacky and weird\" though...\n\n(There are different ways you could do \"CHECK_AGG_SIGNERS\": for\ninstance, take a bitmask of keys and return the bitwise-and with the\nkeys that signed, or take a bitmask and just return the number of keys\nmatching that bitmask that signed, or take a pubkey index and return a\nboolean whether that key signed)\n\nCheers,\naj"
            },
            {
                "author": "Bram Cohen",
                "date": "2018-03-28T03:19:48",
                "message_text_only": "On Mon, Mar 26, 2018 at 11:34 PM, Anthony Towns <aj at erisian.com.au> wrote:\n\n> On Wed, Mar 21, 2018 at 05:47:01PM -0700, Bram Cohen via bitcoin-dev wrote:\n> > [...] Most unused opcodes should be reclaimed as RETURN_VALID,\n> > but there should still be one OP_NOP and there should be a 'real'\n> RETURN_VALID,\n> > which (a) is guaranteed to not be soft forked into something else in the\n> > future, and (b) doesn't have any parsing weirdness.\n>\n> What's the reason for those? I could see an argument for RETURN_VALID, I\n> guess:\n>\n>   confA IF condB IF condC IF [pathA] RETURN_VALID ENDIF ENDIF ENDIF [pathB]\n>\n> is probably simpler and saves 3 bytes compared to:\n>\n>   1 condA IF condB IF condC IF [pathA] NOT ENDIF ENDIF ENDIF IF [pathB]\n> ENDIF\n>\n> but that doesn't seem crazy compelling?\n\n\nMostly yes it's for that case and also for:\n\n   condA IF RETURN_VALID ENDIF condb IF RETURN_VALID ENDIF condc\n\nTechnically that can be done with fewer opcodes using OP_BOOLOR but maybe\nin the future there will be some incentive for short circuit evaluation\n\nBut there's also the general principle that it's only one opcode and if\nthere are a lot of things which look like RETURN_VALID there should be one\nthing which actually is RETURN_VALID\n\n\n> I don't see a reason to just keep one OP_NOP though.\n>\n\nMostly based on momentum because there are several of them there right now.\nIf noone else wants to defend it I won't either.\n\n\n> > By far the most expedient option is (e) cause a RETURN_VALID at parse\n> time.\n> > There's even precedent for this sort of behavior in the other direction\n> with\n> > disabled opcodes causing failure at parse time even if they aren't being\n> > executed.\n>\n> You're probably right. That still doesn't let you implement intercal's\n> COMEFROM statement as a new opcode, of course. :)\n>\n\nThat can be in the hardfork wishlist :-)\n\n\n> > A lot can be said about all the options, but one thing I feel like\n> snarking\n> > about is that if you get rid of IFs using MAST, then it's highly unclear\n> > whether OP_DEPTH should be nuked as well. My feeling is that it should\n> and that\n> > strict parsing should require that the bottom thing in the witness gets\n> > referenced at some point.\n>\n> I guess when passing the script you could perhaps check if each witness\n> item could have been replaced with OP_FALSE or OP_1 and still get the\n> same result, and consider the transaction non-standard if so?\n>\n\nEssentially all opcodes including OP_PICK make clear at runtime how deep\nthey go and anything below the max depth can be safely eliminated (or used\nas grounds for rejecting in strict mode). The big exception is OP_DEPTH\nwhich totally mangles the assumptions. It's trivial to make scripts which\nuse OP_DEPTH which become invalid with things added below the stack then go\nback to being valid again with more things added even though the individual\nitems are never even accessed.\n\n\n>\n> > Hacking in a multisig opcode isn't a horrible idea, but it is very stuck\n> > specifically on m-of-n and doesn't support more complex formulas for how\n> > signatures can be combined, which makes it feel hacky and weird.\n>\n> Hmm? The opcode I suggested works just as easily with arbitrary formulas,\n> eg, \"There must be at least 1 signer from pka{1,2,3}, and 3 signers all\n> up, except each of pkb{1,2,3,4,5,6} only counts for half\":\n>\n>   0 pkb6 pkb5 pkb4 pkb3 pkb2 pkb1 pka3 pka2 pka1 9 CHECK_AGGSIG_VERIFY\n>     (declare pubkeys)\n>   0b111 CHECK_AGG_SIGNERS VERIFY\n>     (one of pka{1,2,3} must sign)\n>   0b001 CHECK_AGG_SIGNERS\n>   0b010 CHECK_AGG_SIGNERS ADD\n>   0b100 CHECK_AGG_SIGNERS ADD\n>   DUP ADD\n>     (pka{1,2,3} count double)\n>   0b000001000 CHECK_AGG_SIGNERS ADD\n>   0b000010000 CHECK_AGG_SIGNERS ADD\n>   0b000100000 CHECK_AGG_SIGNERS ADD\n>   0b001000000 CHECK_AGG_SIGNERS ADD\n>   0b010000000 CHECK_AGG_SIGNERS ADD\n>   0b100000000 CHECK_AGG_SIGNERS ADD\n>     (pkb{1..6} count single)\n>   6 EQUAL\n>     (summing to a total of 3 doubled)\n>\n> Not sure that saves it from being \"hacky and weird\" though...\n>\n\nThat is very hacky and weird. Doing MAST on lots of possibilities is always\nreasonably elegant, and it only gets problematic when the number of\npossibilities is truly massive.\n\nIt's also the case that BLS can support complex key agreement schemes\nwithout even giving away that it isn't a simple single signature. Just\nsaying.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180327/51bdea13/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Soft-forks and schnorr signature aggregation",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj at protonmail.com",
                "Anthony Towns",
                "ZmnSCPxj",
                "Andrew Poelstra",
                "Bram Cohen"
            ],
            "messages_count": 8,
            "total_messages_chars_count": 72056
        }
    },
    {
        "title": "[bitcoin-dev] Soft Fork Activation & Enforcement w/o Signaling?",
        "thread_messages": [
            {
                "author": "Samad Sajanlal",
                "date": "2018-03-21T22:04:35",
                "message_text_only": "Is it possible to activate soft forks such as BIP65 and BIP66 without prior\nsignaling from miners? I noticed in chainparams.cpp that there are block\nheights where the enforcement begins.\n\nI understand this is already active on bitcoin. I'm working on a project\nthat is a clone of a clone of bitcoin, and we currently do not have BIP65\nor BIP66 enforced - no signaling of these soft forks either (most of the\nnetwork is on a source code fork of bitcoin 0.9). This project does not and\nnever intends to attempt to replace bitcoin - we know that without bitcoin\nour project could never exist, so we owe a great deal of gratitude to the\nbitcoin developers.\n\nIf the entire network upgrades to the correct version of the software\n(based on bitcoin 0.15), which includes the block height that has\nenforcement, can we simply skip over the signaling and go straight into\nactivation/enforcement?\n\nAt this time we are lucky that our network is very small, so it is\nreasonable to assume that the whole network will upgrade their clients\nwithin a short window (~2 weeks). We would schedule the activation ~2\nmonths out from when the client is released, just to ensure everyone has\ntime to upgrade.\n\nWe have been stuck on the 0.9 code branch and my goal is to bring it up to\n0.15 at least, so that we can implement Segwit and other key features that\nbitcoin has introduced. The 0.15 client currently works with regards to\nsending and receiving transactions but the soft forks are not active. I\nunderstand that activating them will segregate the 0.15 clients onto their\nown fork, which is why I'd like to understand the repercussions of doing it\nwithout any signaling beforehand. I also would prefer not to have to make\nintermediate releases such as 0.10, 0.11.. etc to get the soft forks\nactivated.\n\nAnother related question - does the block version get bumped up\nautomatically at the time that a soft fork activates, or is there\nadditional stuff that I need to do within the code to ensure it bumps up at\nthe same time? From what I saw in the code it appears that it will bump up\nautomatically, but I would like some confirmation on that.\n\nRegards,\nSamad\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180321/bf373a90/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2018-03-28T12:55:26",
                "message_text_only": "Yes, you can activate softforks at a given height.\nI don't see any reason why you couldn't rebase to 0.16 directly.\nThe block version bumping was a mistake in bip34, you don't really\nneed to bump the version number. In any case, I would recommend\nreading bip34 and what it activates in the code. IIRC the last thing\nwas bip65.\n\nOn Wed, Mar 21, 2018 at 11:04 PM, Samad Sajanlal via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Is it possible to activate soft forks such as BIP65 and BIP66 without prior\n> signaling from miners? I noticed in chainparams.cpp that there are block\n> heights where the enforcement begins.\n>\n> I understand this is already active on bitcoin. I'm working on a project\n> that is a clone of a clone of bitcoin, and we currently do not have BIP65 or\n> BIP66 enforced - no signaling of these soft forks either (most of the\n> network is on a source code fork of bitcoin 0.9). This project does not and\n> never intends to attempt to replace bitcoin - we know that without bitcoin\n> our project could never exist, so we owe a great deal of gratitude to the\n> bitcoin developers.\n>\n> If the entire network upgrades to the correct version of the software (based\n> on bitcoin 0.15), which includes the block height that has enforcement, can\n> we simply skip over the signaling and go straight into\n> activation/enforcement?\n>\n> At this time we are lucky that our network is very small, so it is\n> reasonable to assume that the whole network will upgrade their clients\n> within a short window (~2 weeks). We would schedule the activation ~2 months\n> out from when the client is released, just to ensure everyone has time to\n> upgrade.\n>\n> We have been stuck on the 0.9 code branch and my goal is to bring it up to\n> 0.15 at least, so that we can implement Segwit and other key features that\n> bitcoin has introduced. The 0.15 client currently works with regards to\n> sending and receiving transactions but the soft forks are not active. I\n> understand that activating them will segregate the 0.15 clients onto their\n> own fork, which is why I'd like to understand the repercussions of doing it\n> without any signaling beforehand. I also would prefer not to have to make\n> intermediate releases such as 0.10, 0.11.. etc to get the soft forks\n> activated.\n>\n> Another related question - does the block version get bumped up\n> automatically at the time that a soft fork activates, or is there additional\n> stuff that I need to do within the code to ensure it bumps up at the same\n> time? From what I saw in the code it appears that it will bump up\n> automatically, but I would like some confirmation on that.\n>\n> Regards,\n> Samad\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Samad Sajanlal",
                "date": "2018-03-29T05:14:42",
                "message_text_only": "Excellent - Thanks for your response Jorge. This helps us plan out the\nfuture upgrades properly.\nSince I see 0.15 and 0.16 use block versions as 0x20000000, whereas the\ncurrent deployed codebase (based on bitcoin 0.9.4) makes versions\n0x00000002 (as seen by a 0.15 client), it appears safe to activate soft\nforks which require a minimum of version 3 and 4 blocks (0x00000003\nand 0x00000004,\nrespectively). Would you agree?\n\nOn Wed, Mar 28, 2018 at 7:55 AM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n\n> Yes, you can activate softforks at a given height.\n> I don't see any reason why you couldn't rebase to 0.16 directly.\n> The block version bumping was a mistake in bip34, you don't really\n> need to bump the version number. In any case, I would recommend\n> reading bip34 and what it activates in the code. IIRC the last thing\n> was bip65.\n>\n> On Wed, Mar 21, 2018 at 11:04 PM, Samad Sajanlal via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > Is it possible to activate soft forks such as BIP65 and BIP66 without\n> prior\n> > signaling from miners? I noticed in chainparams.cpp that there are block\n> > heights where the enforcement begins.\n> >\n> > I understand this is already active on bitcoin. I'm working on a project\n> > that is a clone of a clone of bitcoin, and we currently do not have\n> BIP65 or\n> > BIP66 enforced - no signaling of these soft forks either (most of the\n> > network is on a source code fork of bitcoin 0.9). This project does not\n> and\n> > never intends to attempt to replace bitcoin - we know that without\n> bitcoin\n> > our project could never exist, so we owe a great deal of gratitude to the\n> > bitcoin developers.\n> >\n> > If the entire network upgrades to the correct version of the software\n> (based\n> > on bitcoin 0.15), which includes the block height that has enforcement,\n> can\n> > we simply skip over the signaling and go straight into\n> > activation/enforcement?\n> >\n> > At this time we are lucky that our network is very small, so it is\n> > reasonable to assume that the whole network will upgrade their clients\n> > within a short window (~2 weeks). We would schedule the activation ~2\n> months\n> > out from when the client is released, just to ensure everyone has time to\n> > upgrade.\n> >\n> > We have been stuck on the 0.9 code branch and my goal is to bring it up\n> to\n> > 0.15 at least, so that we can implement Segwit and other key features\n> that\n> > bitcoin has introduced. The 0.15 client currently works with regards to\n> > sending and receiving transactions but the soft forks are not active. I\n> > understand that activating them will segregate the 0.15 clients onto\n> their\n> > own fork, which is why I'd like to understand the repercussions of doing\n> it\n> > without any signaling beforehand. I also would prefer not to have to make\n> > intermediate releases such as 0.10, 0.11.. etc to get the soft forks\n> > activated.\n> >\n> > Another related question - does the block version get bumped up\n> > automatically at the time that a soft fork activates, or is there\n> additional\n> > stuff that I need to do within the code to ensure it bumps up at the same\n> > time? From what I saw in the code it appears that it will bump up\n> > automatically, but I would like some confirmation on that.\n> >\n> > Regards,\n> > Samad\n> >\n> >\n> >\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> >\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180329/46f683eb/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2018-03-30T20:52:50",
                "message_text_only": "Yes, in fact, you don't need to lose those bits like bitcoin by\nimposing that the version is greater than that. But I guess just doing\nthe same is simpler.\n\nOn Thu, Mar 29, 2018 at 7:14 AM, Samad Sajanlal\n<samad.sajanlal at gmail.com> wrote:\n> Excellent - Thanks for your response Jorge. This helps us plan out the\n> future upgrades properly.\n> Since I see 0.15 and 0.16 use block versions as 0x20000000, whereas the\n> current deployed codebase (based on bitcoin 0.9.4) makes versions 0x00000002\n> (as seen by a 0.15 client), it appears safe to activate soft forks which\n> require a minimum of version 3 and 4 blocks (0x00000003 and 0x00000004,\n> respectively). Would you agree?\n>\n> On Wed, Mar 28, 2018 at 7:55 AM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n>>\n>> Yes, you can activate softforks at a given height.\n>> I don't see any reason why you couldn't rebase to 0.16 directly.\n>> The block version bumping was a mistake in bip34, you don't really\n>> need to bump the version number. In any case, I would recommend\n>> reading bip34 and what it activates in the code. IIRC the last thing\n>> was bip65.\n>>\n>> On Wed, Mar 21, 2018 at 11:04 PM, Samad Sajanlal via bitcoin-dev\n>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> > Is it possible to activate soft forks such as BIP65 and BIP66 without\n>> > prior\n>> > signaling from miners? I noticed in chainparams.cpp that there are block\n>> > heights where the enforcement begins.\n>> >\n>> > I understand this is already active on bitcoin. I'm working on a project\n>> > that is a clone of a clone of bitcoin, and we currently do not have\n>> > BIP65 or\n>> > BIP66 enforced - no signaling of these soft forks either (most of the\n>> > network is on a source code fork of bitcoin 0.9). This project does not\n>> > and\n>> > never intends to attempt to replace bitcoin - we know that without\n>> > bitcoin\n>> > our project could never exist, so we owe a great deal of gratitude to\n>> > the\n>> > bitcoin developers.\n>> >\n>> > If the entire network upgrades to the correct version of the software\n>> > (based\n>> > on bitcoin 0.15), which includes the block height that has enforcement,\n>> > can\n>> > we simply skip over the signaling and go straight into\n>> > activation/enforcement?\n>> >\n>> > At this time we are lucky that our network is very small, so it is\n>> > reasonable to assume that the whole network will upgrade their clients\n>> > within a short window (~2 weeks). We would schedule the activation ~2\n>> > months\n>> > out from when the client is released, just to ensure everyone has time\n>> > to\n>> > upgrade.\n>> >\n>> > We have been stuck on the 0.9 code branch and my goal is to bring it up\n>> > to\n>> > 0.15 at least, so that we can implement Segwit and other key features\n>> > that\n>> > bitcoin has introduced. The 0.15 client currently works with regards to\n>> > sending and receiving transactions but the soft forks are not active. I\n>> > understand that activating them will segregate the 0.15 clients onto\n>> > their\n>> > own fork, which is why I'd like to understand the repercussions of doing\n>> > it\n>> > without any signaling beforehand. I also would prefer not to have to\n>> > make\n>> > intermediate releases such as 0.10, 0.11.. etc to get the soft forks\n>> > activated.\n>> >\n>> > Another related question - does the block version get bumped up\n>> > automatically at the time that a soft fork activates, or is there\n>> > additional\n>> > stuff that I need to do within the code to ensure it bumps up at the\n>> > same\n>> > time? From what I saw in the code it appears that it will bump up\n>> > automatically, but I would like some confirmation on that.\n>> >\n>> > Regards,\n>> > Samad\n>> >\n>> >\n>> >\n>> > _______________________________________________\n>> > bitcoin-dev mailing list\n>> > bitcoin-dev at lists.linuxfoundation.org\n>> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>> >\n>\n>"
            }
        ],
        "thread_summary": {
            "title": "Soft Fork Activation & Enforcement w/o Signaling?",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jorge Tim\u00f3n",
                "Samad Sajanlal"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 12723
        }
    },
    {
        "title": "[bitcoin-dev] Lookinf for issues to contribute to",
        "thread_messages": [
            {
                "author": "Daniel R",
                "date": "2018-03-24T13:52:56",
                "message_text_only": "Hey guys,\n\nI want to contribute to bitcoin core. I am an intermediate programmer and\nwant to get started contributing fast. I have already cloned the git\nrepository. Can you maybe direct me to sources where I can learn more about\nthe structure of bitcoin core and specifically to problems where I can get\nexperience working with the source code?\n\nI know Python, C/C++ and a bit of Java. I have advanced knowledge of\ncryptographic concepts and procedures and try to teach myself some of the\nmath, especially around elliptic curves. I am currently composing a\nbachelor thesis around the question of Blockchain usage in the\ncar-industry, where I try to look at different aspects of protocol-design.\n\n\nBest Regards\nDaniel\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180324/648f2a0f/attachment-0001.html>"
            },
            {
                "author": "rhavar at protonmail.com",
                "date": "2018-03-24T14:50:54",
                "message_text_only": "Maybe:\nhttps://github.com/bitcoin/bitcoin/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22\n\nJust pick something small (even if it's not interesting), struggle with it, struggle with it some more, do a git blame on the parts you need to modify and try contact the person if there's something you need help with.\n\nI'd say start with simple and boring changes, and you'll organically get a better understanding.\n\nBut even better, go to:\nhttps://github.com/bitcoin/bitcoin/pulls\n\nAnd find some issues, reproduce the problem, test the fix -- and look at how the person did it. Post your results/feedback on the pull requests\n\nI think you'll find in bitcoin (and cryptocurrencies in general) there's a lot more demand for elbow grease than advanced maths stuff =)\n\n-Ryan\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\nOn March 24, 2018 8:52 AM, Daniel R via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hey guys,\n>\n> I want to contribute to bitcoin core. I am an intermediate programmer and want to get started contributing fast. I have already cloned the git repository. Can you maybe direct me to sources where I can learn more about the structure of bitcoin core and specifically to problems where I can get experience working with the source code?\n>\n> I know Python, C/C++ and a bit of Java. I have advanced knowledge of cryptographic concepts and procedures and try to teach myself some of the math, especially around elliptic curves. I am currently composing a bachelor thesis around the question of Blockchain usage in the car-industry, where I try to look at different aspects of protocol-design.\n>\n> Best Regards\n> Daniel\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180324/50b8c8b4/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Lookinf for issues to contribute to",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Daniel R",
                "rhavar at protonmail.com"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 2731
        }
    },
    {
        "title": "[bitcoin-dev] Optimized Header Sync",
        "thread_messages": [
            {
                "author": "Jim Posen",
                "date": "2018-03-27T23:31:58",
                "message_text_only": "Based on some ideas that were thrown around in this thread (\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-December/015385.html),\nI have been working on a P2P extension that will allow faster header sync\nmechanisms. The one-sentence summary is that by encoding headers more\nefficiently (eg. omitting prev_hash) and downloading evenly spaced\ncheckpoints throughout history (say every 1,000th) from all peers first, we\ncould speed up header sync, which would be a huge improvement for light\nclients. Here is a draft of the BIP:\nhttps://github.com/jimpo/bips/blob/headers-sync/headersv2.mediawiki. The\nfull text is below as well.\n\nI'd love to hear any feedback people have.\n\n----------------------------------------------------------\n\n== Abstract ==\n\nThis BIP describes a P2P network extension enabling faster, more\nreliable methods for syncing the block header chain. New P2P messages\nare proposed as more efficient replacements for\n<code>getheaders</code> and <code>headers</code> during initial block\ndownload. The proposed header download protocol reduces bandwidth\nusage by ~40%-50% and supports downloading headers ranges from\nmultiple peers in parallel, which is not possible with the current\nmechanism. This also enables sync strategies with better resistance to\ndenial-of-service attacks.\n\n== Motivation ==\n\nSince 2015, optimized Bitcoin clients fetch all block headers before\nblocks themselves in order to avoid downloading ones that are not part\nof the most work chain. The protocol currently in use for fetching\nheaders leaves room for further optimization, specifically by\ncompressing header data and downloading more headers\nsimulaneously<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-December/015385.html</ref>.\nAny savings here should have a large impact given that both full nodes\nand light clients must sync the header chain as a first step, and that\nthe time to validate and index the headers is negligible compared to\nthe time spent downloading them from the network. Furthermore, some\ncurrent implementations of headers syncing rely on preconfigured\ncheckpoints to discourage attackers attempting to fill up a victim's\ndisk space with low-work headers. The proposed messages enable sync\nstrategies that are resilient against these types of attacks. The P2P\nmessages are designed to be flexible, supporting multiple header sync\nstrategies and leaving room for future innovations, while also\ncompact.\n\n== Definitions ==\n\n''double-SHA256'' is a hash algorithm defined by two invocations of\nSHA-256: <code>double-SHA256(x) = SHA256(SHA256(x))</code>.\n\n== Specification ==\n\nThe key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n\"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this\ndocument are to be interpreted as described in RFC 2119.\n\n=== New Structures ===\n\n==== Compressed Headers ====\n\nBitcoin headers are serialized by default in 80 bytes as follows:\n\n{| class=\"wikitable\"\n! Field Name\n! Data Type\n! Byte Size\n! Description\n|-\n| version\n| int32_t\n| 4\n| Block version information\n|-\n| prev_block\n| uint256\n| 32\n| The hash of the previous block\n|-\n| merkle_root\n| uint256\n| 32\n| The root hash of the transaction Merkle tree\n|-\n| timestamp\n| uint32_t\n| 4\n| A Unix timestamp of the block creation time, as reported by the miner\n|-\n| bits\n| uint32_t\n| 4\n| The calculated difficulty target for this block\n|-\n| nonce\n| uint32_t\n| 4\n| A nonce that is set such that the header's hash matches the difficulty target\n|}\n\nWhen deserializing a correctly-formed sequence of block headers\nencoded in this way, it can be noted that:\n\n* The prev_block field should always match the double-SHA256 hash of\nthe previous header, making it redundant\n* According to Bitcoin consensus rules, the bits field only changes\nevery 2016 blocks\n* The version often matches that of a recent ancestor block\n* The timestamp is often a small delta from the preceding header's timestamp\n\nTo take advantage of these possible savings, this document defines a\nvariable-sized ''compressed encoding'' of block headers that occur in\na range. Note that no savings are possible when serializing a single\nheader; it should only be used for vectors of sequential headers. The\nfull headers are reconstructed using data from previous headers in the\nrange. The serialization begins with an ''encoding indicator'', which\nis a bitfield specifying how each field is serialized. The bits of the\nindicator have the following semantics:\n\n{| class=\"wikitable\"\n! Bit Index\n! Reconstruction\n! Description\n|-\n| 0\n| <code>prev_block[i] = DSHA256(header[i-1])</code>\n| The prev_block field is ommitted and assigned to the double-SHA256\nhash of the previous uncompressed header.\n|-\n| 1\n| <code>nbits[i] = nbits[i-1]</code>\n| The nbits field is omitted and matches that of the previous header.\n|-\n| 2\n| <code>timestamp[i] = timestamp[i-1] + value</code>\n| The timestamp is replaced by a 2-byte signed short int, representing\nan offset from the previous block's timestamp\n|-\n| 3\n|\n| Interpreted along with bits 4 & 5.\n|-\n| 4\n|\n| Interpreted along with bits 3 & 5.\n|-\n| 5\n| <code>version[i] = version[i - ((bit[3] << 2) + (bit[4] << 1) +\nbit[5])]</code>\n| Bits 3, 4, and 5 are first interpreted as a 3-bit offset, with bit\nindex 3 as the most significant and bit index 5 as the least\nsignificant. If the offset is non-zero, the version field is omitted\nand assigned to the version of the block at the offset number of\nblocks prior.\n|-\n| 6\n|\n| Reserved.\n|-\n| 7\n|\n| Reserved. May be used in a future encoding version to signal another\nindicator byte.\n|}\n\nThe compressed header format is versioned by a 256-bit unsigned\ninteger. This document defines version 0.\n\n==== VarInt ====\n\n''VarInt'' is a variable-length unsigned integer encoding that\nsupports a greater range of numbers than the standard ''CompactSize''.\nThis encoding was introduced at the database layer in Bitcoin\nCore<ref>https://github.com/bitcoin/bitcoin/commit/4d6144f97faf9d2a6c89f41d7d2360f21f0b71e2</ref>\nin 2012, but is new to the Bitcoin P2P layer.\n\nThis definition is per the code comments in Bitcoin Core written by\nPieter Wuille:\n\n<pre>\nVariable-length integers: bytes are a MSB base-128 encoding of the number.\nThe high bit in each byte signifies whether another digit follows. To make\nthe encoding is one-to-one, one is subtracted from all but the last digit.\nThus, the byte sequence a[] with length len, where all but the last byte\nhas bit 128 set, encodes the number:\n\n  (a[len-1] & 0x7F) + sum(i=1..len-1, 128^i*((a[len-i-1] & 0x7F)+1))\n\nProperties:\n* Very small (0-127: 1 byte, 128-16511: 2 bytes, 16512-2113663: 3 bytes)\n* Every integer has exactly one encoding\n* Encoding does not depend on size of original integer type\n* No redundancy: every (infinite) byte sequence corresponds to a list\n  of encoded integers.\n\n0:         [0x00]  256:        [0x81 0x00]\n1:         [0x01]  16383:      [0xFE 0x7F]\n127:       [0x7F]  16384:      [0xFF 0x00]\n128:  [0x80 0x00]  16511: [0x80 0xFF 0x7F]\n255:  [0x80 0x7F]  65535: [0x82 0xFD 0x7F]\n2^32:           [0x8E 0xFE 0xFE 0xFF 0x00]\n</pre>\n\n==== Checkpoints ====\n\nA ''checkpoint'' is defined for a block as a tuple of its hash and the\nchain work:\n\n{| class=\"wikitable\"\n! Field Name\n! Data Type\n! Byte Size\n! Description\n|-\n| block_hash\n| uint256\n| 32\n| The hash of the block\n|-\n| chain_work\n| VarInt\n| Variable(1-20)\n| A delta between the total work in the chain at the checkpoint block\nand a previous checkpoint, determined by context\n|}\n\n=== Service Bit ===\n\nThis BIP allocates a new service bit:\n\n{| class=\"wikitable\"\n|-\n| NODE_HEADERS_V2\n| <code>1 << ?</code>\n| If enabled, the node MUST respond to <code>getcheckpts</code> and\n<code>getheaders2</code> queries\n|}\n\n=== New Messages ===\n\n==== getcheckpts ====\n<code>getcheckpts</code> is used to request block headers at a\nspecified distance from each other which serve as checkpoints during\nparallel header download. The message contains the following fields:\n\n{| class=\"wikitable\"\n! Field Name\n! Data Type\n! Byte Size\n! Description\n|-\n| block_locator\n| uint256[]\n| Variable\n| A vector of block hashes in descending order by height used to\nidentify the header chain of the requesting node\n|-\n| interval\n| uint32_t\n| 4\n| The distance in block height between requested block hashes\n|}\n\n# Nodes SHOULD NOT send <code>getcheckpts</code> unless the peer has\nset the <code>NODE_HEADERS_V2</code> service bit\n# The hashes in <code>block_locator</code> MUST be in descending order\nby block height\n# The block locator SHOULD be generated as it is in\n<code>getheaders</code> requests\n# The receiving node MUST respond to valid requests with a\n<code>checkpts</code> response where the interval is the same as in\nthe request and the first checkpoint hash matches the first common\nblock hash in the block locator\n\n==== checkpts ====\n<code>checkpts</code> is sent in response to <code>getcheckpts</code>,\nlisting block hashes at the specified interval. The message contains\nthe following fields:\n\n{| class=\"wikitable\"\n! Field Name\n! Data Type\n! Byte Size\n! Description\n|-\n| start_height\n| uint32_t\n| 4\n| The height of the first block in the active chain matching the\nrequest's block locator\n|-\n| end_height\n| uint32_t\n| 4\n| The height of the last block in the active chain\n|-\n| start_checkpoint\n| Checkpoint\n| 48\n| The checkpoint structure for the block in the active chain at height\nstart_height\n|-\n| end_checkpoint\n| Checkpoint\n| 48\n| The checkpoint structure for the block in the active chain at height\nend_height\n|-\n| interval\n| uint32_t\n| 4\n| The distance in block height between checkpoints\n|-\n| checkpoints_length\n| CompactSize\n| Variable(1-5)\n| The number of checkpoints to follow\n|-\n| checkpoints\n| Checkpoint[]\n| checkoints_length * Variable(33-52)\n| The checkpoints as specified below\n|}\n\n# The interval SHOULD match the field in the <code>getcheckpts</code> request\n# The start_checkpoint SHOULD correspond to the first block hash in\nthe locator from the <code>getcheckpts</code> request that is part of\nthe active chain\n# The end_checkpoint SHOULD correspond to the tip of the node's active chain\n# The start_height MOST be set to the block height of the start_checkpoint\n# The end_height MOST be set to the block height of the end_checkpoint\n# If the interval is zero, the checkpoints vector MUST be empty\n# If the interval is non-zero, checkpoints MUST correspond to blocks\non the active chain between the start_checkpoint and the\nend_checkpoint (exclusive), where the difference in block height\nbetween each entry and the previous one is equal to the interval\n# The checkpoints_length MUST be less than or equal to 2,000\n# The node SHOULD include as many checkpoints on its active chain as\nare available, up to the limit of 2,000\n# The chain_work field in the first checkpoint MUST be the total work\nin the chain ending at that block\n# The chain_work field in each subsequent checkpoint MUST be the\ndifference in chain work between that block and the previous\ncheckpoint\n# The chain_work field in each checkpoint MUST be a properly-encoded\nVarInt, not exceeding 20 bytes\n\n==== getheaders2 ====\n<code>getheaders2</code> is used to request compressed headers for a\nrange of blocks. The message contains the following fields:\n\n{| class=\"wikitable\"\n! Field Name\n! Data Type\n! Byte Size\n! Description\n|-\n| max_version\n| uint8_t\n| 1\n| The maximum supported encoding version of the headers\n|-\n| flags\n| uint8_t\n| 1\n| A bitfield of message encoding flags\n|-\n| start_height\n| uint32_t\n| 4\n| The height of the first block header in the requested range\n|-\n| end_hash\n| uint256\n| 32\n| The hash of the last block header in the requested range\n|}\n\n# Nodes SHOULD NOT send <code>getheaders2</code> unless the peer has\nset the <code>NODE_HEADERS_V2</code> service bit\n# The height of the block with hash end_hash MUST be greater than or\nequal to start_height, and the difference MUST be strictly less than\n3,000\n# The end_hash SHOULD match one in a previously received\n<code>checkpts</code> message, otherwise the receiving node MAY\ndisconnect\n# The 0th bit (least significant order) of the flags field MAY be set\nto request the coinbase transaction and merkle branch for the block at\nheight start_height\n\n==== headers2 ====\n<code>headers2</code> is sent in response to <code>getheaders2</code>,\nlisting the compressed headers in the requested range. The message\ncontains the following fields:\n\n{| class=\"wikitable\"\n! Field Name\n! Data Type\n! Byte Size\n! Description\n|-\n| version\n| uint8_t\n| 1\n| The encoding version of the headers\n|-\n| flags\n| uint8_t\n| 1\n| A bitfield of message encoding flags\n|-\n| start_height\n| uint32_t\n| 4\n| The height of the first block header returned\n|-\n| headers_length\n| CompactSize\n| 1-3\n| The number of block headers to follow\n|-\n| headers\n| CompressedHeader[]\n| Variable\n| The compressed block headers\n|-\n| start_block_coinbase_tx\n| CTransaction\n| Variable\n| The coinbase transaction in the block at start_height\n|-\n| start_block_coinbase_branch\n| uint256[]\n| Variable\n| A merkle branch linking the coinbase transaction in the block at\nstart_height to its header\n|}\n\n# The version MUST be less than or equal to the max_version field of\nthe <code>getheaders2</code> request\n# Any bits set in the flags field of the <code>getheaders2</code>\nrequest MAY be set in the response field\n# Any bits not set in the flags field of the <code>getheaders2</code>\nrequest MUST NOT be set in the response field\n# The first header MUST be encoded with a 0-byte indicator (ie. the\nheader is uncompressed)\n# start_height MUST be set to the block height of the first header\n# The hash of the last block SHOULD equal the end_hash of the\n<code>getheaders2</code> request, ''even if the block is no longer\npart of the active chain''\n# The length of the headers vector MUST be less than or equal to 3,000\n# The headers MUST be sequential in order of height, with each header\na successor of the previous one\n# Each header SHOULD be optimally compressed\n# The start_block_coinbase_tx should be the serialized coinbase\ntransaction in the block corresponding to the first header\n# The start_block_coinbase_branch should be a vector of\nright-hand-side hashes in the merkle branch linking the coinbase\ntransaction to the first header, in order from bottom of the tree to\ntop\n# If the 0th bit (least significant order) of the flags field is\nunset, the start_block_coinbase_tx and start_block_coinbase_branch\nfields MUST be omitted\n\n=== Sync Strategies ===\n\nThe general header sync protocol for clients now is to first request\ncheckpoints from all peers with <code>getcheckpts</code>, then decide\nwhich peers to fetch ranges of headers from and download them with\n<code>getheaders2</code>.\n\n==== Forward Sequential Syncing ====\n\nSimilar to the current sync protocol, a client may choose one peer to\ndownload headers from, then fetch them in forward sequential order.\nOnce this peer is out of headers, the client performs the same routine\nwith any peers offering more headers.\n\nWith this strategy, the client is able to fully validate the block\nheaders in order and abort if the peer serves an invalid one. On the\nother hand, the peer may be able to serve a longer, lower-work chain\nthan the global active chain, wasting the client's time, memory, and\nstorage space.\n\n==== Parallel Header Download ====\n\nIn order to increase the throughput of header downloads, a node may\ndownload multiple header ranges in parallel from all peers serving the\nsame checkpoints, then validate them in sequential order.\n\n==== Random Sampling Proof-of-Work  ====\n\nSimilar the FlyClient<ref>https://www.youtube.com/watch?time_continue=8400&v=BPNs9EVxWrA</ref>\nheader download protocol, clients can select the peer claiming the\ngreatest total work chain and use random sampling to efficiently\ndetermine if the peer is likely to be reporting its chain work\nhonestly.\n\nThe client treats the checkpoint message as a commitment to chain work\nof intermediate ranges of headers, the client then randomly samples\nranges of headers weighted by total work to determine whether the\ntotal chain work is valid before downloading all headers. To defend\nagainst malicious peers attempting to reuse earlier headers later in\nthe chain to fake greater total work, the client should check the\nblock height in the coinbase transaction for all headers after the BIP\n34 activation height. If the peer is found to be dishonest, they can\nbe banned before the client downloads too many headers, otherwise the\nclient chooses this as the primary sync peer for forward sequential\nsync or parallel download.\n\n== Rationale ==\n\n* '''Why include the coinbase transaction in the headers messages?'''\nThe primary reason is that after BIP\n34<ref>https://github.com/bitcoin/bips/blob/master/bip-0034.mediawiki</ref>\nactivation at block height 227,835, coinbase transactions constitute\ncryptographic commitments to a block's height in the chain, which\nmitigates certain attacks during header sync. Furthermore, the\n<code>getheaders2</code> message can be used as a simple way of\nrequesting a coinbase transaction for a single header, which may be\nindependently useful.\n\n* '''Why not omit nBits entirely?''' The compression is designed to\npermit full decompression of all headers in a <code>headers2</code>\nmessage ''without'' requiring any other chain context. This is\ndesirable so that proofs of work may be validated for arbitrary header\nranges. While nBits can be computed knowing previous headers, this\nrequires block headers that may not be sent in the same message.\n\n== Compatibility ==\n\nThis is backwards compatible, as it defines new P2P messages which are\navailable if a service bit is signaled. There are no changes to\nconsensus rules.\n\n== Acknowledgements ==\n\nThanks to Gregory Maxwell for suggestions on the compressed header\nencoding and the DOS-resistant sync strategies. Thanks to Suhas\nDaftuar for helpful discussions.\n\nCredit for the VarInt encoding goes to Pieter Wuille.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180327/65c9ed79/attachment-0001.html>"
            },
            {
                "author": "Riccardo Casatta",
                "date": "2018-03-29T08:17:12",
                "message_text_only": "Hi Jim,\n\n| <code>version[i] = version[i - ((bit[3] << 2) + (bit[4] << 1) +\nbit[5])]</code>\n\n\nThought this wasn't effective in case overt asic boost get widely adopted,\nbut then I understood that at the moment only two bits of version get\nscrambled by that technique so this looks fine, maybe add a comment about\nthis so the reader doesn't get the same initial doubt I got.\n\n...downloading evenly spaced checkpoints throughout history (say every\n> 1,000th) from all peers first...\n\n\nMy feeling is that encoding of the headers and checkpoints/parallel\ndownload are separate subjects for two BIPS.\nAbout the checkpoints I don't grasp why they are useful since an attacker\ncould lie about them but maybe I am missing something...\n\nTo take advantage of these possible savings, this document defines a\n> variable-sized ''compressed encoding'' of block headers that occur in a\n> range. Note that no savings are possible when serializing a single header;\n> it should only be used for vectors of sequential headers. The full headers\n> are reconstructed using data from previous headers in the range. The\n> serialization begins with an ''encoding indicator'', which is a bitfield\n> specifying how each field is serialized. The bits of the indicator have the\n> following semantics:\n\n\nBitfield allows great savings, however the encoding depends on the headers\nheight a client ask for, this cause a little computational burden on the\nnode and the undesirable side effect of difficult caching. Variable length\nencoding cause caching difficulties too...\nA simpler approach could be to encode the headers in groups of 2016 headers\n(the difficulty period) where the first header is complete and the others\n2015 are missing the previous hash and the difficulty, this achieve\ncomparable savings ~45%, allows better caching and has fixed length\nencoding. This could be useful for the node by caching headers on a single\nfile on disk and simply stream out the relative range when requested or to\nserve the same encoded headers format in other context like http,\nleveraging http caching infrastructure.\n\n\n\n2018-03-28 1:31 GMT+02:00 Jim Posen via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org>:\n\n> Based on some ideas that were thrown around in this thread (https://lists.\n> linuxfoundation.org/pipermail/bitcoin-dev/2017-December/015385.html), I\n> have been working on a P2P extension that will allow faster header sync\n> mechanisms. The one-sentence summary is that by encoding headers more\n> efficiently (eg. omitting prev_hash) and downloading evenly spaced\n> checkpoints throughout history (say every 1,000th) from all peers first, we\n> could speed up header sync, which would be a huge improvement for light\n> clients. Here is a draft of the BIP: https://github.com/jimpo/\n> bips/blob/headers-sync/headersv2.mediawiki. The full text is below as\n> well.\n>\n> I'd love to hear any feedback people have.\n>\n> ----------------------------------------------------------\n>\n> == Abstract ==\n>\n> This BIP describes a P2P network extension enabling faster, more reliable methods for syncing the block header chain. New P2P messages are proposed as more efficient replacements for <code>getheaders</code> and <code>headers</code> during initial block download. The proposed header download protocol reduces bandwidth usage by ~40%-50% and supports downloading headers ranges from multiple peers in parallel, which is not possible with the current mechanism. This also enables sync strategies with better resistance to denial-of-service attacks.\n>\n> == Motivation ==\n>\n> Since 2015, optimized Bitcoin clients fetch all block headers before blocks themselves in order to avoid downloading ones that are not part of the most work chain. The protocol currently in use for fetching headers leaves room for further optimization, specifically by compressing header data and downloading more headers simulaneously<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-December/015385.html</ref>. Any savings here should have a large impact given that both full nodes and light clients must sync the header chain as a first step, and that the time to validate and index the headers is negligible compared to the time spent downloading them from the network. Furthermore, some current implementations of headers syncing rely on preconfigured checkpoints to discourage attackers attempting to fill up a victim's disk space with low-work headers. The proposed messages enable sync strategies that are resilient against these types of attacks. The P2P messages are designed to be flexible, supporting multiple header sync strategies and leaving room for future innovations, while also compact.\n>\n> == Definitions ==\n>\n> ''double-SHA256'' is a hash algorithm defined by two invocations of SHA-256: <code>double-SHA256(x) = SHA256(SHA256(x))</code>.\n>\n> == Specification ==\n>\n> The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in RFC 2119.\n>\n> === New Structures ===\n>\n> ==== Compressed Headers ====\n>\n> Bitcoin headers are serialized by default in 80 bytes as follows:\n>\n> {| class=\"wikitable\"\n> ! Field Name\n> ! Data Type\n> ! Byte Size\n> ! Description\n> |-\n> | version\n> | int32_t\n> | 4\n> | Block version information\n> |-\n> | prev_block\n> | uint256\n> | 32\n> | The hash of the previous block\n> |-\n> | merkle_root\n> | uint256\n> | 32\n> | The root hash of the transaction Merkle tree\n> |-\n> | timestamp\n> | uint32_t\n> | 4\n> | A Unix timestamp of the block creation time, as reported by the miner\n> |-\n> | bits\n> | uint32_t\n> | 4\n> | The calculated difficulty target for this block\n> |-\n> | nonce\n> | uint32_t\n> | 4\n> | A nonce that is set such that the header's hash matches the difficulty target\n> |}\n>\n> When deserializing a correctly-formed sequence of block headers encoded in this way, it can be noted that:\n>\n> * The prev_block field should always match the double-SHA256 hash of the previous header, making it redundant\n> * According to Bitcoin consensus rules, the bits field only changes every 2016 blocks\n> * The version often matches that of a recent ancestor block\n> * The timestamp is often a small delta from the preceding header's timestamp\n>\n> To take advantage of these possible savings, this document defines a variable-sized ''compressed encoding'' of block headers that occur in a range. Note that no savings are possible when serializing a single header; it should only be used for vectors of sequential headers. The full headers are reconstructed using data from previous headers in the range. The serialization begins with an ''encoding indicator'', which is a bitfield specifying how each field is serialized. The bits of the indicator have the following semantics:\n>\n> {| class=\"wikitable\"\n> ! Bit Index\n> ! Reconstruction\n> ! Description\n> |-\n> | 0\n> | <code>prev_block[i] = DSHA256(header[i-1])</code>\n> | The prev_block field is ommitted and assigned to the double-SHA256 hash of the previous uncompressed header.\n> |-\n> | 1\n> | <code>nbits[i] = nbits[i-1]</code>\n> | The nbits field is omitted and matches that of the previous header.\n> |-\n> | 2\n> | <code>timestamp[i] = timestamp[i-1] + value</code>\n> | The timestamp is replaced by a 2-byte signed short int, representing an offset from the previous block's timestamp\n> |-\n> | 3\n> |\n> | Interpreted along with bits 4 & 5.\n> |-\n> | 4\n> |\n> | Interpreted along with bits 3 & 5.\n> |-\n> | 5\n> | <code>version[i] = version[i - ((bit[3] << 2) + (bit[4] << 1) + bit[5])]</code>\n> | Bits 3, 4, and 5 are first interpreted as a 3-bit offset, with bit index 3 as the most significant and bit index 5 as the least significant. If the offset is non-zero, the version field is omitted and assigned to the version of the block at the offset number of blocks prior.\n> |-\n> | 6\n> |\n> | Reserved.\n> |-\n> | 7\n> |\n> | Reserved. May be used in a future encoding version to signal another indicator byte.\n> |}\n>\n> The compressed header format is versioned by a 256-bit unsigned integer. This document defines version 0.\n>\n> ==== VarInt ====\n>\n> ''VarInt'' is a variable-length unsigned integer encoding that supports a greater range of numbers than the standard ''CompactSize''. This encoding was introduced at the database layer in Bitcoin Core<ref>https://github.com/bitcoin/bitcoin/commit/4d6144f97faf9d2a6c89f41d7d2360f21f0b71e2</ref> in 2012, but is new to the Bitcoin P2P layer.\n>\n> This definition is per the code comments in Bitcoin Core written by Pieter Wuille:\n>\n> <pre>\n> Variable-length integers: bytes are a MSB base-128 encoding of the number.\n> The high bit in each byte signifies whether another digit follows. To make\n> the encoding is one-to-one, one is subtracted from all but the last digit.\n> Thus, the byte sequence a[] with length len, where all but the last byte\n> has bit 128 set, encodes the number:\n>\n>   (a[len-1] & 0x7F) + sum(i=1..len-1, 128^i*((a[len-i-1] & 0x7F)+1))\n>\n> Properties:\n> * Very small (0-127: 1 byte, 128-16511: 2 bytes, 16512-2113663: 3 bytes)\n> * Every integer has exactly one encoding\n> * Encoding does not depend on size of original integer type\n> * No redundancy: every (infinite) byte sequence corresponds to a list\n>   of encoded integers.\n>\n> 0:         [0x00]  256:        [0x81 0x00]\n> 1:         [0x01]  16383:      [0xFE 0x7F]\n> 127:       [0x7F]  16384:      [0xFF 0x00]\n> 128:  [0x80 0x00]  16511: [0x80 0xFF 0x7F]\n> 255:  [0x80 0x7F]  65535: [0x82 0xFD 0x7F]\n> 2^32:           [0x8E 0xFE 0xFE 0xFF 0x00]\n> </pre>\n>\n> ==== Checkpoints ====\n>\n> A ''checkpoint'' is defined for a block as a tuple of its hash and the chain work:\n>\n> {| class=\"wikitable\"\n> ! Field Name\n> ! Data Type\n> ! Byte Size\n> ! Description\n> |-\n> | block_hash\n> | uint256\n> | 32\n> | The hash of the block\n> |-\n> | chain_work\n> | VarInt\n> | Variable(1-20)\n> | A delta between the total work in the chain at the checkpoint block and a previous checkpoint, determined by context\n> |}\n>\n> === Service Bit ===\n>\n> This BIP allocates a new service bit:\n>\n> {| class=\"wikitable\"\n> |-\n> | NODE_HEADERS_V2\n> | <code>1 << ?</code>\n> | If enabled, the node MUST respond to <code>getcheckpts</code> and <code>getheaders2</code> queries\n> |}\n>\n> === New Messages ===\n>\n> ==== getcheckpts ====\n> <code>getcheckpts</code> is used to request block headers at a specified distance from each other which serve as checkpoints during parallel header download. The message contains the following fields:\n>\n> {| class=\"wikitable\"\n> ! Field Name\n> ! Data Type\n> ! Byte Size\n> ! Description\n> |-\n> | block_locator\n> | uint256[]\n> | Variable\n> | A vector of block hashes in descending order by height used to identify the header chain of the requesting node\n> |-\n> | interval\n> | uint32_t\n> | 4\n> | The distance in block height between requested block hashes\n> |}\n>\n> # Nodes SHOULD NOT send <code>getcheckpts</code> unless the peer has set the <code>NODE_HEADERS_V2</code> service bit\n> # The hashes in <code>block_locator</code> MUST be in descending order by block height\n> # The block locator SHOULD be generated as it is in <code>getheaders</code> requests\n> # The receiving node MUST respond to valid requests with a <code>checkpts</code> response where the interval is the same as in the request and the first checkpoint hash matches the first common block hash in the block locator\n>\n> ==== checkpts ====\n> <code>checkpts</code> is sent in response to <code>getcheckpts</code>, listing block hashes at the specified interval. The message contains the following fields:\n>\n> {| class=\"wikitable\"\n> ! Field Name\n> ! Data Type\n> ! Byte Size\n> ! Description\n> |-\n> | start_height\n> | uint32_t\n> | 4\n> | The height of the first block in the active chain matching the request's block locator\n> |-\n> | end_height\n> | uint32_t\n> | 4\n> | The height of the last block in the active chain\n> |-\n> | start_checkpoint\n> | Checkpoint\n> | 48\n> | The checkpoint structure for the block in the active chain at height start_height\n> |-\n> | end_checkpoint\n> | Checkpoint\n> | 48\n> | The checkpoint structure for the block in the active chain at height end_height\n> |-\n> | interval\n> | uint32_t\n> | 4\n> | The distance in block height between checkpoints\n> |-\n> | checkpoints_length\n> | CompactSize\n> | Variable(1-5)\n> | The number of checkpoints to follow\n> |-\n> | checkpoints\n> | Checkpoint[]\n> | checkoints_length * Variable(33-52)\n> | The checkpoints as specified below\n> |}\n>\n> # The interval SHOULD match the field in the <code>getcheckpts</code> request\n> # The start_checkpoint SHOULD correspond to the first block hash in the locator from the <code>getcheckpts</code> request that is part of the active chain\n> # The end_checkpoint SHOULD correspond to the tip of the node's active chain\n> # The start_height MOST be set to the block height of the start_checkpoint\n> # The end_height MOST be set to the block height of the end_checkpoint\n> # If the interval is zero, the checkpoints vector MUST be empty\n> # If the interval is non-zero, checkpoints MUST correspond to blocks on the active chain between the start_checkpoint and the end_checkpoint (exclusive), where the difference in block height between each entry and the previous one is equal to the interval\n> # The checkpoints_length MUST be less than or equal to 2,000\n> # The node SHOULD include as many checkpoints on its active chain as are available, up to the limit of 2,000\n> # The chain_work field in the first checkpoint MUST be the total work in the chain ending at that block\n> # The chain_work field in each subsequent checkpoint MUST be the difference in chain work between that block and the previous checkpoint\n> # The chain_work field in each checkpoint MUST be a properly-encoded VarInt, not exceeding 20 bytes\n>\n> ==== getheaders2 ====\n> <code>getheaders2</code> is used to request compressed headers for a range of blocks. The message contains the following fields:\n>\n> {| class=\"wikitable\"\n> ! Field Name\n> ! Data Type\n> ! Byte Size\n> ! Description\n> |-\n> | max_version\n> | uint8_t\n> | 1\n> | The maximum supported encoding version of the headers\n> |-\n> | flags\n> | uint8_t\n> | 1\n> | A bitfield of message encoding flags\n> |-\n> | start_height\n> | uint32_t\n> | 4\n> | The height of the first block header in the requested range\n> |-\n> | end_hash\n> | uint256\n> | 32\n> | The hash of the last block header in the requested range\n> |}\n>\n> # Nodes SHOULD NOT send <code>getheaders2</code> unless the peer has set the <code>NODE_HEADERS_V2</code> service bit\n> # The height of the block with hash end_hash MUST be greater than or equal to start_height, and the difference MUST be strictly less than 3,000\n> # The end_hash SHOULD match one in a previously received <code>checkpts</code> message, otherwise the receiving node MAY disconnect\n> # The 0th bit (least significant order) of the flags field MAY be set to request the coinbase transaction and merkle branch for the block at height start_height\n>\n> ==== headers2 ====\n> <code>headers2</code> is sent in response to <code>getheaders2</code>, listing the compressed headers in the requested range. The message contains the following fields:\n>\n> {| class=\"wikitable\"\n> ! Field Name\n> ! Data Type\n> ! Byte Size\n> ! Description\n> |-\n> | version\n> | uint8_t\n> | 1\n> | The encoding version of the headers\n> |-\n> | flags\n> | uint8_t\n> | 1\n> | A bitfield of message encoding flags\n> |-\n> | start_height\n> | uint32_t\n> | 4\n> | The height of the first block header returned\n> |-\n> | headers_length\n> | CompactSize\n> | 1-3\n> | The number of block headers to follow\n> |-\n> | headers\n> | CompressedHeader[]\n> | Variable\n> | The compressed block headers\n> |-\n> | start_block_coinbase_tx\n> | CTransaction\n> | Variable\n> | The coinbase transaction in the block at start_height\n> |-\n> | start_block_coinbase_branch\n> | uint256[]\n> | Variable\n> | A merkle branch linking the coinbase transaction in the block at start_height to its header\n> |}\n>\n> # The version MUST be less than or equal to the max_version field of the <code>getheaders2</code> request\n> # Any bits set in the flags field of the <code>getheaders2</code> request MAY be set in the response field\n> # Any bits not set in the flags field of the <code>getheaders2</code> request MUST NOT be set in the response field\n> # The first header MUST be encoded with a 0-byte indicator (ie. the header is uncompressed)\n> # start_height MUST be set to the block height of the first header\n> # The hash of the last block SHOULD equal the end_hash of the <code>getheaders2</code> request, ''even if the block is no longer part of the active chain''\n> # The length of the headers vector MUST be less than or equal to 3,000\n> # The headers MUST be sequential in order of height, with each header a successor of the previous one\n> # Each header SHOULD be optimally compressed\n> # The start_block_coinbase_tx should be the serialized coinbase transaction in the block corresponding to the first header\n> # The start_block_coinbase_branch should be a vector of right-hand-side hashes in the merkle branch linking the coinbase transaction to the first header, in order from bottom of the tree to top\n> # If the 0th bit (least significant order) of the flags field is unset, the start_block_coinbase_tx and start_block_coinbase_branch fields MUST be omitted\n>\n> === Sync Strategies ===\n>\n> The general header sync protocol for clients now is to first request checkpoints from all peers with <code>getcheckpts</code>, then decide which peers to fetch ranges of headers from and download them with <code>getheaders2</code>.\n>\n> ==== Forward Sequential Syncing ====\n>\n> Similar to the current sync protocol, a client may choose one peer to download headers from, then fetch them in forward sequential order. Once this peer is out of headers, the client performs the same routine with any peers offering more headers.\n>\n> With this strategy, the client is able to fully validate the block headers in order and abort if the peer serves an invalid one. On the other hand, the peer may be able to serve a longer, lower-work chain than the global active chain, wasting the client's time, memory, and storage space.\n>\n> ==== Parallel Header Download ====\n>\n> In order to increase the throughput of header downloads, a node may download multiple header ranges in parallel from all peers serving the same checkpoints, then validate them in sequential order.\n>\n> ==== Random Sampling Proof-of-Work  ====\n>\n> Similar the FlyClient<ref>https://www.youtube.com/watch?time_continue=8400&v=BPNs9EVxWrA</ref> header download protocol, clients can select the peer claiming the greatest total work chain and use random sampling to efficiently determine if the peer is likely to be reporting its chain work honestly.\n>\n> The client treats the checkpoint message as a commitment to chain work of intermediate ranges of headers, the client then randomly samples ranges of headers weighted by total work to determine whether the total chain work is valid before downloading all headers. To defend against malicious peers attempting to reuse earlier headers later in the chain to fake greater total work, the client should check the block height in the coinbase transaction for all headers after the BIP 34 activation height. If the peer is found to be dishonest, they can be banned before the client downloads too many headers, otherwise the client chooses this as the primary sync peer for forward sequential sync or parallel download.\n>\n> == Rationale ==\n>\n> * '''Why include the coinbase transaction in the headers messages?''' The primary reason is that after BIP 34<ref>https://github.com/bitcoin/bips/blob/master/bip-0034.mediawiki</ref> activation at block height 227,835, coinbase transactions constitute cryptographic commitments to a block's height in the chain, which mitigates certain attacks during header sync. Furthermore, the <code>getheaders2</code> message can be used as a simple way of requesting a coinbase transaction for a single header, which may be independently useful.\n>\n> * '''Why not omit nBits entirely?''' The compression is designed to permit full decompression of all headers in a <code>headers2</code> message ''without'' requiring any other chain context. This is desirable so that proofs of work may be validated for arbitrary header ranges. While nBits can be computed knowing previous headers, this requires block headers that may not be sent in the same message.\n>\n> == Compatibility ==\n>\n> This is backwards compatible, as it defines new P2P messages which are available if a service bit is signaled. There are no changes to consensus rules.\n>\n> == Acknowledgements ==\n>\n> Thanks to Gregory Maxwell for suggestions on the compressed header encoding and the DOS-resistant sync strategies. Thanks to Suhas Daftuar for helpful discussions.\n>\n> Credit for the VarInt encoding goes to Pieter Wuille.\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n\n\n-- \nRiccardo Casatta - @RCasatta <https://twitter.com/RCasatta>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180329/e96f51fb/attachment-0001.html>"
            },
            {
                "author": "Jim Posen",
                "date": "2018-03-30T00:50:30",
                "message_text_only": "Thanks for giving it a read and for sparking the discussion with your\nobservation about the 40% savings from dropping prev_hash!\n\n\n> Thought this wasn't effective in case overt asic boost get widely adopted,\n> but then I understood that at the moment only two bits of version get\n> scrambled by that technique so this looks fine, maybe add a comment about\n> this so the reader doesn't get the same initial doubt I got.\n>\n\nI still need to compute for historical blocks how many could have an\nomitted version. Will post back with that when I get results. If overt ASIC\nBoost made this less effective, that would be unfortunate, but so be it.\n\n\n> My feeling is that encoding of the headers and checkpoints/parallel\n> download are separate subjects for two BIPS.\n> About the checkpoints I don't grasp why they are useful since an attacker\n> could lie about them but maybe I am missing something...\n>\n\nYeah, I guess the background wasn't explained in the BIP itself. After your\noriginal post on the mailing list, there were suggestions that instead of\nmodifying the format of existing messages, it would be better do create a\nnew headers message. And as long as we're designing a new headers message,\nwe should change the semantics to allow parallel download. But if you want\nto download from peers in parallel, you need to get a summary of the blocks\nthat they have. Hence the checkpoints message. So that is why both of these\nmessages are in the same BIP -- only together can they perform an efficient\nsync.\n\nRegarding the reliability of the checkpoints, I think it's strictly better\nthan what we have now. Let's say a node is connected to 6 honest peers and\n2 malicious peers. Even if the node does not know which ones are good or\nbad until it validates the headers, it sees that 6 of the peers are on the\nsame chain, and can download those headers in parallel from 6 different\nsources. So that's already a win.\n\nTaken a step further though, I'm really interested in treating the\ncheckpoints as commitments to chain work and using random sampling to\ndetect lying peers before downloading all of their headers. So imagine you\nare connected to two peers, one good one bad, where the good one claims a\nchain with X total work and the bad one claims a chain with Y total work.\nTo determine quickly which is correct, you can randomly sample ranges of\nheaders and check the proofs of work to see whether it matches what the\npeer claimed. So basically you pick a checkpoint at random (weighted by the\nwork delta) which commits to a total amount of work from the last\ncheckpoint, then request all headers in between. If the peer responds with\nheaders with the correct start hash, end hash, and start height (from the\ncoinbase tx of the first header), then you can be somewhat more confident\ntheir total PoW matches the claimed amount.\n\nHow many times do you need to sample? I don't know yet, but I've heard\nBenedikt Bunz is exploring this question with his research on FlyClients\n[1], which was an inspiration for this.\n\n\n> Bitfield allows great savings, however the encoding depends on the headers\n> height a client ask for, this cause a little computational burden on the\n> node and the undesirable side effect of difficult caching. Variable length\n> encoding cause caching difficulties too...\n> A simpler approach could be to encode the headers in groups of 2016\n> headers (the difficulty period) where the first header is complete and the\n> others 2015 are missing the previous hash and the difficulty, this achieve\n> comparable savings ~45%, allows better caching and has fixed length\n> encoding. This could be useful for the node by caching headers on a single\n> file on disk and simply stream out the relative range when requested or to\n> serve the same encoded headers format in other context like http,\n> leveraging http caching infrastructure.\n>\n\nI don't see too much of a problem with caching. Most node implementations I\nknow of keep all headers in memory anyway, often in contiguous segments of\nRAM for historical headers, so it should be fairly inexpensive to serve\nqueries. Beyond that, the response for a particular query (start_height,\nend_hash, encoding version) can be cached, so if some service wants to\nprecompute max size responses for all start_height multiples of 1,000, they\ncould cache those.\n\n-Jim\n\n[1] https://www.youtube.com/watch?time_continue=8400&v=BPNs9EVxWrA\n\n\n> 2018-03-28 1:31 GMT+02:00 Jim Posen via bitcoin-dev <bitcoin-dev at lists.\n> linuxfoundation.org>:\n>\n>> Based on some ideas that were thrown around in this thread (\n>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/\n>> 2017-December/015385.html), I have been working on a P2P extension that\n>> will allow faster header sync mechanisms. The one-sentence summary is that\n>> by encoding headers more efficiently (eg. omitting prev_hash) and\n>> downloading evenly spaced checkpoints throughout history (say every\n>> 1,000th) from all peers first, we could speed up header sync, which would\n>> be a huge improvement for light clients. Here is a draft of the BIP:\n>> https://github.com/jimpo/bips/blob/headers-sync/headersv2.mediawiki. The\n>> full text is below as well.\n>>\n>> I'd love to hear any feedback people have.\n>>\n>> ----------------------------------------------------------\n>>\n>> == Abstract ==\n>>\n>> This BIP describes a P2P network extension enabling faster, more reliable methods for syncing the block header chain. New P2P messages are proposed as more efficient replacements for <code>getheaders</code> and <code>headers</code> during initial block download. The proposed header download protocol reduces bandwidth usage by ~40%-50% and supports downloading headers ranges from multiple peers in parallel, which is not possible with the current mechanism. This also enables sync strategies with better resistance to denial-of-service attacks.\n>>\n>> == Motivation ==\n>>\n>> Since 2015, optimized Bitcoin clients fetch all block headers before blocks themselves in order to avoid downloading ones that are not part of the most work chain. The protocol currently in use for fetching headers leaves room for further optimization, specifically by compressing header data and downloading more headers simulaneously<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-December/015385.html</ref>. Any savings here should have a large impact given that both full nodes and light clients must sync the header chain as a first step, and that the time to validate and index the headers is negligible compared to the time spent downloading them from the network. Furthermore, some current implementations of headers syncing rely on preconfigured checkpoints to discourage attackers attempting to fill up a victim's disk space with low-work headers. The proposed messages enable sync strategies that are resilient against these types of attacks. The P2P messages are designed to be flexible, supporting multiple header sync strategies and leaving room for future innovations, while also compact.\n>>\n>> == Definitions ==\n>>\n>> ''double-SHA256'' is a hash algorithm defined by two invocations of SHA-256: <code>double-SHA256(x) = SHA256(SHA256(x))</code>.\n>>\n>> == Specification ==\n>>\n>> The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in RFC 2119.\n>>\n>> === New Structures ===\n>>\n>> ==== Compressed Headers ====\n>>\n>> Bitcoin headers are serialized by default in 80 bytes as follows:\n>>\n>> {| class=\"wikitable\"\n>> ! Field Name\n>> ! Data Type\n>> ! Byte Size\n>> ! Description\n>> |-\n>> | version\n>> | int32_t\n>> | 4\n>> | Block version information\n>> |-\n>> | prev_block\n>> | uint256\n>> | 32\n>> | The hash of the previous block\n>> |-\n>> | merkle_root\n>> | uint256\n>> | 32\n>> | The root hash of the transaction Merkle tree\n>> |-\n>> | timestamp\n>> | uint32_t\n>> | 4\n>> | A Unix timestamp of the block creation time, as reported by the miner\n>> |-\n>> | bits\n>> | uint32_t\n>> | 4\n>> | The calculated difficulty target for this block\n>> |-\n>> | nonce\n>> | uint32_t\n>> | 4\n>> | A nonce that is set such that the header's hash matches the difficulty target\n>> |}\n>>\n>> When deserializing a correctly-formed sequence of block headers encoded in this way, it can be noted that:\n>>\n>> * The prev_block field should always match the double-SHA256 hash of the previous header, making it redundant\n>> * According to Bitcoin consensus rules, the bits field only changes every 2016 blocks\n>> * The version often matches that of a recent ancestor block\n>> * The timestamp is often a small delta from the preceding header's timestamp\n>>\n>> To take advantage of these possible savings, this document defines a variable-sized ''compressed encoding'' of block headers that occur in a range. Note that no savings are possible when serializing a single header; it should only be used for vectors of sequential headers. The full headers are reconstructed using data from previous headers in the range. The serialization begins with an ''encoding indicator'', which is a bitfield specifying how each field is serialized. The bits of the indicator have the following semantics:\n>>\n>> {| class=\"wikitable\"\n>> ! Bit Index\n>> ! Reconstruction\n>> ! Description\n>> |-\n>> | 0\n>> | <code>prev_block[i] = DSHA256(header[i-1])</code>\n>> | The prev_block field is ommitted and assigned to the double-SHA256 hash of the previous uncompressed header.\n>> |-\n>> | 1\n>> | <code>nbits[i] = nbits[i-1]</code>\n>> | The nbits field is omitted and matches that of the previous header.\n>> |-\n>> | 2\n>> | <code>timestamp[i] = timestamp[i-1] + value</code>\n>> | The timestamp is replaced by a 2-byte signed short int, representing an offset from the previous block's timestamp\n>> |-\n>> | 3\n>> |\n>> | Interpreted along with bits 4 & 5.\n>> |-\n>> | 4\n>> |\n>> | Interpreted along with bits 3 & 5.\n>> |-\n>> | 5\n>> | <code>version[i] = version[i - ((bit[3] << 2) + (bit[4] << 1) + bit[5])]</code>\n>> | Bits 3, 4, and 5 are first interpreted as a 3-bit offset, with bit index 3 as the most significant and bit index 5 as the least significant. If the offset is non-zero, the version field is omitted and assigned to the version of the block at the offset number of blocks prior.\n>> |-\n>> | 6\n>> |\n>> | Reserved.\n>> |-\n>> | 7\n>> |\n>> | Reserved. May be used in a future encoding version to signal another indicator byte.\n>> |}\n>>\n>> The compressed header format is versioned by a 256-bit unsigned integer. This document defines version 0.\n>>\n>> ==== VarInt ====\n>>\n>> ''VarInt'' is a variable-length unsigned integer encoding that supports a greater range of numbers than the standard ''CompactSize''. This encoding was introduced at the database layer in Bitcoin Core<ref>https://github.com/bitcoin/bitcoin/commit/4d6144f97faf9d2a6c89f41d7d2360f21f0b71e2</ref> in 2012, but is new to the Bitcoin P2P layer.\n>>\n>> This definition is per the code comments in Bitcoin Core written by Pieter Wuille:\n>>\n>> <pre>\n>> Variable-length integers: bytes are a MSB base-128 encoding of the number.\n>> The high bit in each byte signifies whether another digit follows. To make\n>> the encoding is one-to-one, one is subtracted from all but the last digit.\n>> Thus, the byte sequence a[] with length len, where all but the last byte\n>> has bit 128 set, encodes the number:\n>>\n>>   (a[len-1] & 0x7F) + sum(i=1..len-1, 128^i*((a[len-i-1] & 0x7F)+1))\n>>\n>> Properties:\n>> * Very small (0-127: 1 byte, 128-16511: 2 bytes, 16512-2113663: 3 bytes)\n>> * Every integer has exactly one encoding\n>> * Encoding does not depend on size of original integer type\n>> * No redundancy: every (infinite) byte sequence corresponds to a list\n>>   of encoded integers.\n>>\n>> 0:         [0x00]  256:        [0x81 0x00]\n>> 1:         [0x01]  16383:      [0xFE 0x7F]\n>> 127:       [0x7F]  16384:      [0xFF 0x00]\n>> 128:  [0x80 0x00]  16511: [0x80 0xFF 0x7F]\n>> 255:  [0x80 0x7F]  65535: [0x82 0xFD 0x7F]\n>> 2^32:           [0x8E 0xFE 0xFE 0xFF 0x00]\n>> </pre>\n>>\n>> ==== Checkpoints ====\n>>\n>> A ''checkpoint'' is defined for a block as a tuple of its hash and the chain work:\n>>\n>> {| class=\"wikitable\"\n>> ! Field Name\n>> ! Data Type\n>> ! Byte Size\n>> ! Description\n>> |-\n>> | block_hash\n>> | uint256\n>> | 32\n>> | The hash of the block\n>> |-\n>> | chain_work\n>> | VarInt\n>> | Variable(1-20)\n>> | A delta between the total work in the chain at the checkpoint block and a previous checkpoint, determined by context\n>> |}\n>>\n>> === Service Bit ===\n>>\n>> This BIP allocates a new service bit:\n>>\n>> {| class=\"wikitable\"\n>> |-\n>> | NODE_HEADERS_V2\n>> | <code>1 << ?</code>\n>> | If enabled, the node MUST respond to <code>getcheckpts</code> and <code>getheaders2</code> queries\n>> |}\n>>\n>> === New Messages ===\n>>\n>> ==== getcheckpts ====\n>> <code>getcheckpts</code> is used to request block headers at a specified distance from each other which serve as checkpoints during parallel header download. The message contains the following fields:\n>>\n>> {| class=\"wikitable\"\n>> ! Field Name\n>> ! Data Type\n>> ! Byte Size\n>> ! Description\n>> |-\n>> | block_locator\n>> | uint256[]\n>> | Variable\n>> | A vector of block hashes in descending order by height used to identify the header chain of the requesting node\n>> |-\n>> | interval\n>> | uint32_t\n>> | 4\n>> | The distance in block height between requested block hashes\n>> |}\n>>\n>> # Nodes SHOULD NOT send <code>getcheckpts</code> unless the peer has set the <code>NODE_HEADERS_V2</code> service bit\n>> # The hashes in <code>block_locator</code> MUST be in descending order by block height\n>> # The block locator SHOULD be generated as it is in <code>getheaders</code> requests\n>> # The receiving node MUST respond to valid requests with a <code>checkpts</code> response where the interval is the same as in the request and the first checkpoint hash matches the first common block hash in the block locator\n>>\n>> ==== checkpts ====\n>> <code>checkpts</code> is sent in response to <code>getcheckpts</code>, listing block hashes at the specified interval. The message contains the following fields:\n>>\n>> {| class=\"wikitable\"\n>> ! Field Name\n>> ! Data Type\n>> ! Byte Size\n>> ! Description\n>> |-\n>> | start_height\n>> | uint32_t\n>> | 4\n>> | The height of the first block in the active chain matching the request's block locator\n>> |-\n>> | end_height\n>> | uint32_t\n>> | 4\n>> | The height of the last block in the active chain\n>> |-\n>> | start_checkpoint\n>> | Checkpoint\n>> | 48\n>> | The checkpoint structure for the block in the active chain at height start_height\n>> |-\n>> | end_checkpoint\n>> | Checkpoint\n>> | 48\n>> | The checkpoint structure for the block in the active chain at height end_height\n>> |-\n>> | interval\n>> | uint32_t\n>> | 4\n>> | The distance in block height between checkpoints\n>> |-\n>> | checkpoints_length\n>> | CompactSize\n>> | Variable(1-5)\n>> | The number of checkpoints to follow\n>> |-\n>> | checkpoints\n>> | Checkpoint[]\n>> | checkoints_length * Variable(33-52)\n>> | The checkpoints as specified below\n>> |}\n>>\n>> # The interval SHOULD match the field in the <code>getcheckpts</code> request\n>> # The start_checkpoint SHOULD correspond to the first block hash in the locator from the <code>getcheckpts</code> request that is part of the active chain\n>> # The end_checkpoint SHOULD correspond to the tip of the node's active chain\n>> # The start_height MOST be set to the block height of the start_checkpoint\n>> # The end_height MOST be set to the block height of the end_checkpoint\n>> # If the interval is zero, the checkpoints vector MUST be empty\n>> # If the interval is non-zero, checkpoints MUST correspond to blocks on the active chain between the start_checkpoint and the end_checkpoint (exclusive), where the difference in block height between each entry and the previous one is equal to the interval\n>> # The checkpoints_length MUST be less than or equal to 2,000\n>> # The node SHOULD include as many checkpoints on its active chain as are available, up to the limit of 2,000\n>> # The chain_work field in the first checkpoint MUST be the total work in the chain ending at that block\n>> # The chain_work field in each subsequent checkpoint MUST be the difference in chain work between that block and the previous checkpoint\n>> # The chain_work field in each checkpoint MUST be a properly-encoded VarInt, not exceeding 20 bytes\n>>\n>> ==== getheaders2 ====\n>> <code>getheaders2</code> is used to request compressed headers for a range of blocks. The message contains the following fields:\n>>\n>> {| class=\"wikitable\"\n>> ! Field Name\n>> ! Data Type\n>> ! Byte Size\n>> ! Description\n>> |-\n>> | max_version\n>> | uint8_t\n>> | 1\n>> | The maximum supported encoding version of the headers\n>> |-\n>> | flags\n>> | uint8_t\n>> | 1\n>> | A bitfield of message encoding flags\n>> |-\n>> | start_height\n>> | uint32_t\n>> | 4\n>> | The height of the first block header in the requested range\n>> |-\n>> | end_hash\n>> | uint256\n>> | 32\n>> | The hash of the last block header in the requested range\n>> |}\n>>\n>> # Nodes SHOULD NOT send <code>getheaders2</code> unless the peer has set the <code>NODE_HEADERS_V2</code> service bit\n>> # The height of the block with hash end_hash MUST be greater than or equal to start_height, and the difference MUST be strictly less than 3,000\n>> # The end_hash SHOULD match one in a previously received <code>checkpts</code> message, otherwise the receiving node MAY disconnect\n>> # The 0th bit (least significant order) of the flags field MAY be set to request the coinbase transaction and merkle branch for the block at height start_height\n>>\n>> ==== headers2 ====\n>> <code>headers2</code> is sent in response to <code>getheaders2</code>, listing the compressed headers in the requested range. The message contains the following fields:\n>>\n>> {| class=\"wikitable\"\n>> ! Field Name\n>> ! Data Type\n>> ! Byte Size\n>> ! Description\n>> |-\n>> | version\n>> | uint8_t\n>> | 1\n>> | The encoding version of the headers\n>> |-\n>> | flags\n>> | uint8_t\n>> | 1\n>> | A bitfield of message encoding flags\n>> |-\n>> | start_height\n>> | uint32_t\n>> | 4\n>> | The height of the first block header returned\n>> |-\n>> | headers_length\n>> | CompactSize\n>> | 1-3\n>> | The number of block headers to follow\n>> |-\n>> | headers\n>> | CompressedHeader[]\n>> | Variable\n>> | The compressed block headers\n>> |-\n>> | start_block_coinbase_tx\n>> | CTransaction\n>> | Variable\n>> | The coinbase transaction in the block at start_height\n>> |-\n>> | start_block_coinbase_branch\n>> | uint256[]\n>> | Variable\n>> | A merkle branch linking the coinbase transaction in the block at start_height to its header\n>> |}\n>>\n>> # The version MUST be less than or equal to the max_version field of the <code>getheaders2</code> request\n>> # Any bits set in the flags field of the <code>getheaders2</code> request MAY be set in the response field\n>> # Any bits not set in the flags field of the <code>getheaders2</code> request MUST NOT be set in the response field\n>> # The first header MUST be encoded with a 0-byte indicator (ie. the header is uncompressed)\n>> # start_height MUST be set to the block height of the first header\n>> # The hash of the last block SHOULD equal the end_hash of the <code>getheaders2</code> request, ''even if the block is no longer part of the active chain''\n>> # The length of the headers vector MUST be less than or equal to 3,000\n>> # The headers MUST be sequential in order of height, with each header a successor of the previous one\n>> # Each header SHOULD be optimally compressed\n>> # The start_block_coinbase_tx should be the serialized coinbase transaction in the block corresponding to the first header\n>> # The start_block_coinbase_branch should be a vector of right-hand-side hashes in the merkle branch linking the coinbase transaction to the first header, in order from bottom of the tree to top\n>> # If the 0th bit (least significant order) of the flags field is unset, the start_block_coinbase_tx and start_block_coinbase_branch fields MUST be omitted\n>>\n>> === Sync Strategies ===\n>>\n>> The general header sync protocol for clients now is to first request checkpoints from all peers with <code>getcheckpts</code>, then decide which peers to fetch ranges of headers from and download them with <code>getheaders2</code>.\n>>\n>> ==== Forward Sequential Syncing ====\n>>\n>> Similar to the current sync protocol, a client may choose one peer to download headers from, then fetch them in forward sequential order. Once this peer is out of headers, the client performs the same routine with any peers offering more headers.\n>>\n>> With this strategy, the client is able to fully validate the block headers in order and abort if the peer serves an invalid one. On the other hand, the peer may be able to serve a longer, lower-work chain than the global active chain, wasting the client's time, memory, and storage space.\n>>\n>> ==== Parallel Header Download ====\n>>\n>> In order to increase the throughput of header downloads, a node may download multiple header ranges in parallel from all peers serving the same checkpoints, then validate them in sequential order.\n>>\n>> ==== Random Sampling Proof-of-Work  ====\n>>\n>> Similar the FlyClient<ref>https://www.youtube.com/watch?time_continue=8400&v=BPNs9EVxWrA</ref> header download protocol, clients can select the peer claiming the greatest total work chain and use random sampling to efficiently determine if the peer is likely to be reporting its chain work honestly.\n>>\n>> The client treats the checkpoint message as a commitment to chain work of intermediate ranges of headers, the client then randomly samples ranges of headers weighted by total work to determine whether the total chain work is valid before downloading all headers. To defend against malicious peers attempting to reuse earlier headers later in the chain to fake greater total work, the client should check the block height in the coinbase transaction for all headers after the BIP 34 activation height. If the peer is found to be dishonest, they can be banned before the client downloads too many headers, otherwise the client chooses this as the primary sync peer for forward sequential sync or parallel download.\n>>\n>> == Rationale ==\n>>\n>> * '''Why include the coinbase transaction in the headers messages?''' The primary reason is that after BIP 34<ref>https://github.com/bitcoin/bips/blob/master/bip-0034.mediawiki</ref> activation at block height 227,835, coinbase transactions constitute cryptographic commitments to a block's height in the chain, which mitigates certain attacks during header sync. Furthermore, the <code>getheaders2</code> message can be used as a simple way of requesting a coinbase transaction for a single header, which may be independently useful.\n>>\n>> * '''Why not omit nBits entirely?''' The compression is designed to permit full decompression of all headers in a <code>headers2</code> message ''without'' requiring any other chain context. This is desirable so that proofs of work may be validated for arbitrary header ranges. While nBits can be computed knowing previous headers, this requires block headers that may not be sent in the same message.\n>>\n>> == Compatibility ==\n>>\n>> This is backwards compatible, as it defines new P2P messages which are available if a service bit is signaled. There are no changes to consensus rules.\n>>\n>> == Acknowledgements ==\n>>\n>> Thanks to Gregory Maxwell for suggestions on the compressed header encoding and the DOS-resistant sync strategies. Thanks to Suhas Daftuar for helpful discussions.\n>>\n>> Credit for the VarInt encoding goes to Pieter Wuille.\n>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n>\n> --\n> Riccardo Casatta - @RCasatta <https://twitter.com/RCasatta>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180329/4817a551/attachment-0001.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2018-03-30T06:14:18",
                "message_text_only": "On Thu, Mar 29, 2018 at 05:50:30PM -0700, Jim Posen via bitcoin-dev wrote:\n> Taken a step further though, I'm really interested in treating the checkpoints\n> as commitments to chain work [...]\n\nIn that case, shouldn't the checkpoints just be every 2016 blocks and\ninclude the corresponding bits value for that set of blocks?\n\nThat way every node commits to (approximately) how much work their entire\nchain has by sending something like 10kB of data (currently), and you\ncould verify the deltas in each node's chain's target by downloading the\n2016 headers between those checkpoints (~80kB with the proposed compact\nencoding?) and checking the timestamps and proof of work match both the\nold target and the new target from adjacent checkpoints.\n\n(That probably still works fine even if there's a hardfork that allows\ndifficulty to adjust more frequently: a bits value at block n*2016 will\nstill enforce *some* lower limit on how much work blocks n*2016+{1..2016}\nwill have to contribute; so will still allow you to estimate how much work\nwill have been done, it may just be less precise than the estimate you could\ngenerate now)\n\nCheers,\naj"
            },
            {
                "author": "Riccardo Casatta",
                "date": "2018-03-30T08:06:24",
                "message_text_only": "Yes, I think the checkpoints and the compressed headers streams should be\nhandled in chunks of 2016 headers and queried by chunk number instead of\nheight, falling back to current method if the chunk is not full yet.\n\nThis is cache friendly and allows to avoid bit 0 and bit 1 in the bitfield\n(because they are always 1 after the first header in the chunk of 2016).\n\n2018-03-30 8:14 GMT+02:00 Anthony Towns <aj at erisian.com.au>:\n\n> On Thu, Mar 29, 2018 at 05:50:30PM -0700, Jim Posen via bitcoin-dev wrote:\n> > Taken a step further though, I'm really interested in treating the\n> checkpoints\n> > as commitments to chain work [...]\n>\n> In that case, shouldn't the checkpoints just be every 2016 blocks and\n> include the corresponding bits value for that set of blocks?\n>\n> That way every node commits to (approximately) how much work their entire\n> chain has by sending something like 10kB of data (currently), and you\n> could verify the deltas in each node's chain's target by downloading the\n> 2016 headers between those checkpoints (~80kB with the proposed compact\n> encoding?) and checking the timestamps and proof of work match both the\n> old target and the new target from adjacent checkpoints.\n>\n> (That probably still works fine even if there's a hardfork that allows\n> difficulty to adjust more frequently: a bits value at block n*2016 will\n> still enforce *some* lower limit on how much work blocks n*2016+{1..2016}\n> will have to contribute; so will still allow you to estimate how much work\n> will have been done, it may just be less precise than the estimate you\n> could\n> generate now)\n>\n> Cheers,\n> aj\n>\n>\n\n\n-- \nRiccardo Casatta - @RCasatta <https://twitter.com/RCasatta>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180330/0aa5cd7e/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Optimized Header Sync",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Anthony Towns",
                "Jim Posen",
                "Riccardo Casatta"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 66368
        }
    },
    {
        "title": "[bitcoin-dev] Electrum Personal Server beta release",
        "thread_messages": [
            {
                "author": "Chris Belcher",
                "date": "2018-03-29T12:07:04",
                "message_text_only": "Electrum Personal Server is an implementation of the Electrum wallet\nserver protocol that allows users to point their Electrum wallet at\ntheir own full node. It is compatible resource-saving features like\npruning, blocksonly and disabled txindex. It is much less\nresource-intensive than other Electrum servers because it only stores\nthe user's own addresses, instead of every address that was ever used.\nAs such it makes tradeoffs, namely losing Electrum's \"instant on\" feature.\n\nRight now using Electrum Personal Server is the easiest way to use a\nhardware wallet backed by your own full node. It is very lightweight,\nbeing a small python script that barely uses any CPU or RAM; much less\nthan the full node it's connected to. Hopefully Electrum Personal Server\ncan be part of the solution in putting full node wallets into the hands\nof as many people as possible.\n\nThe project is now in beta release:\nhttps://github.com/chris-belcher/electrum-personal-server\n\nIt now has all the essential features to make it practical for use;\nMerkle proofs, deterministic wallets, bech32 addresses, SSL, Core's\nmulti-wallet support. Along with the features that were in the alpha\nrelease of tracking new transactions, confirmations, block headers,\nimporting addresses.\n\nThere is a caveat about pruning. Electrum Personal Server obtains merkle\nproofs using the `gettxoutproof` RPC call, if pruning is enabled and\nthat block has been deleted then the RPC will return null and so the\nElectrum wallet will display `Not Verified`. Everything else will still\nwork, and this shouldn't be a problem in most situations because\nElectrum usually only requests merkle proofs for recent transactions and\npruning keeps recent blocks. But in the long term it needs some thought\non the best way to fix this. I've been thinking about adding code for\nBitcoin Core that stores merkle proofs for each of the wallet's own\ntransactions in wallet.dat.\n\nFurther Reading:\n*\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-February/015707.html\n* https://bitcointalk.org/index.php?topic=3167572.0"
            }
        ],
        "thread_summary": {
            "title": "Electrum Personal Server beta release",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Chris Belcher"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2069
        }
    }
]