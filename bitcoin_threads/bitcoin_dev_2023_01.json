[
    {
        "title": "[bitcoin-dev] Pseudocode for robust tail emission",
        "thread_messages": [
            {
                "author": "Alfie John",
                "date": "2023-01-01T12:42:50",
                "message_text_only": "On 31 Dec 2022, at 10:28 am, Peter Todd via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n>> This way:\n>> \n>> 1. system cannot be played\n>> 2. only in case of destructive halving: system waits for the recovery of network security\n> \n> The immediate danger we have with halvings is that in a competitive market,\n> profit margins tend towards marginal costs - the cost to produce an additional\n> unit of production - rather than total costs - the cost necessary to recover\n> prior and future expenses. Since the halving is a sudden shock to the system,\n> under the right conditions we could have a significant amount of hashing power\n> just barely able to afford to hash prior to the halving, resulting in all that\n> hashing power immediately having to shut down and fees increasing dramatically,\n> and likely, chaotically.  Your proposal does not address that problem as it can\n> only measure difficulty prior to the halving point.\n\n\n> ... Since the halving is a sudden shock to the system\n\nIs it though? Since everyone knows of the possible outcomes, wouldn't a possible halving be priced in? \n\n> resulting in all that hashing power immediately having to shut down and fees increasing dramatically\n\nWhich should cause that hashing power to come back because of this fee increases.\n\nAlfie\n\n--\nAlfie John\nhttps://www.alfie <https://www.alfie/>.wtf\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230101/383bbd29/attachment.html>"
            },
            {
                "author": "jk_14 at op.pl",
                "date": "2023-01-01T21:23:37",
                "message_text_only": "Yes, the idea is:\nif mining activity is growing - let's execute consecutive halvings\nbut if miner exodus has happened - let's delay next halving until mining activity is recovered to previous levels\nIf it gets to the point where a sudden drop in mining difficulty happens - delaying the next halving may be not sufficient to correct, but is surely better than not delaying it.\nWhile Bitcoin is better and better money with every halving in comparision to other types of money - there is non-zero risk that people will hoard it more and more, according to old Gresham's law (\"HODL\"). And this way decreasing liquidity / transactions volume. The positive feedback loop - is my real concern here.\nRegarding the relationship between difficulty and security - I fully agree.\nBut ASIC technology is already matured. And also any technology breakthrough is a short event within 4 years period.\nSo growth of difficulty could be gained by technology breakthrough, but any sudden drop of difficulty would be always an issue, while there is no such thing as: ASIC technology regression.\nObviously, not complicated solution would be better than complicated one.\n\u00a0\n\u00a0\nW dniu 2022-12-30 19:21:10 u\u017cytkownik Billy Tetrud <billy.tetrud at gmail.com> napisa\u0142:\nIf the idea is to ensure that a catastrophic miner exodus doesn't happen, the \"difference\" you're calculating should only care about downward differences. Upward differences indicate more mining activity and so shouldn't cause a halving skip. \u00a0\nBut I don't think any scheme like this that only acts on the basis of difficulty will be sufficient. If it gets to the point where a sudden drop in mining difficulty happens, it is very likely that simply delaying the next halving or even ending halving all together will not be sufficient to correct for whatever is causing hashrate to tank. There is also the danger of simple difficulty stagnation, which this mechanism wouldn't detect.\u00a0\n\u00a0\nThe relationship between difficulty and security becomes less and less predictable the longer you want to look ahead. There's no long term relation between difficulty and any reasonable security target. A security target might be something like \"no colluding group with less than $1 trillion dollars at their disposal could successfully 51% attack the network (with a probability of blah blah)\". There is no way to today program in any code that detects based on difficult alone when that criteria is violated. You would have to program in assumptions about the cost of hashrate projected into the future.\n\u00a0\nI can't think of any robust automatic way to do this. I think to a certain degree, it will have to be a change that happens in a fork of some kind (soft or hard) periodically (every 10 years? 30 years?). The basic relations needed is really the cost in Bitcoin of the security target (ie the minimum number of Bitcoin it should take to 51% attack the system) and the cost in Bitcoin of acquiring a unit of hashrate. This could be simply input into the code, or could use some complicated oracle system. But with that relation, the system could be programmed to calculate the difficulty necessary to keep the system secure.\n\u00a0\nOnce that is in place, the system could automatically adjust the subsidy up or down to attract more or less miners, or it could adjust the block size up or down to change the fee market such that more or less total fees are collected each block to attract more or less miners.\u00a0\nOn Tue, Dec 27, 2022, 09:41 Jaroslaw via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\nIt seems like the more elegant solution could be by using a chainwork parameter instead.\ni.e. comparison just before halving - if the last 210,000 block interval has a higher chainwork difference between the begining and the end of interval\nthan any other such inter-halving interval before.\nLIttle digression yet:\nA system in which all users participate in ensuring its security looks better than one in which only some (i.e. active) of them participate (and passive stakeholders are de facto free riders)\nIn my opinion this concept above is only the complement of currently missing mechanism: achieving equilibrium regarding costs of security between two parties with opposing interests.\nIt's easy to understand and - most important - it has no hardcoded value of tail emission - what is the clear proof it is based on a free market.\nAnd last but not least, if someone is 100% sure that income from transactions will takeover security support from block subsidy - accepting such proposal is like putting the money where the mouth is: this safety measure will never be triggered, then (no risk of fork)\nBest Regards\nJaroslaw\nW dniu 2022-12-23 20:29:20 u\u017cytkownik Jaroslaw via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> napisa\u0142:\n>\nNecessary or not - it doesn't hurt to plan the robust model, just in case. The proposal is:\nLet every 210,000 the code calculate the average difficulty of 100 last retargets (100 fit well in 210,000 / 2016 = 104.166)\nand compare with the maximum of all such values calculated before, every 210,000 blocks:\nif average_diff_of_last_100_retargets > maximum_of_all_previous_average_diffs\n\u00a0 \u00a0 \u00a0 \u00a0 do halving\nelse\n\u00a0 \u00a0 \u00a0 \u00a0 do nothing\nThis way:\n1. system cannot be played\n2. only in case of destructive halving: system waits for the recovery of network security\nBest Regards\nJaroslaw\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230101/b43f2863/attachment.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2023-01-02T04:53:39",
                "message_text_only": "> is surely better than not delaying it.\n\nI might agree, but I don't think it really solves the problem well enough\nto be worth it. Any solution that would solve the problem better would make\ndelaying halvings unnecessary.\n\n> there is non-zero risk that people will hoard it more and more, according\nto old Gresham's law\n\nGresham's law doesn't apply here. Gresham's law is about the interaction\nbetween two currencies with a fixed, usually government-enforced exchange\nrate. You seem to be saying that Bitcoin will be hoarded because Bitcoin\ninflation reduces every halving. But even with 0 inflation, it certainly\nwon't cause all Bitcoin to be hoarded. Also, \"hoarding\" is also known as\n\"saving\", and there's nothing wrong with saving. The spectre of deflation\ncomes from a misunderstanding of deflation and why it happens during bad\neconomic times. It is an effect, not a cause.\n\nOn Sun, Jan 1, 2023, 15:23 <jk_14 at op.pl> wrote:\n\n>\n> Yes, the idea is:\n> if mining activity is growing - let's execute consecutive halvings\n> but if miner exodus has happened - let's delay next halving until mining\n> activity is recovered to previous levels\n>\n> If it gets to the point where a sudden drop in mining difficulty happens -\n> delaying the next halving may be not sufficient to correct, but is surely\n> better than not delaying it.\n>\n> While Bitcoin is better and better money with every halving in comparision\n> to other types of money - there is non-zero risk that people will hoard it\n> more and more, according to old Gresham's law (\"HODL\"). And this way\n> decreasing liquidity / transactions volume. The positive feedback loop - is\n> my real concern here.\n>\n> Regarding the relationship between difficulty and security - I fully agree.\n> But ASIC technology is already matured. And also any technology\n> breakthrough is a short event within 4 years period.\n> So growth of difficulty could be gained by technology breakthrough, but\n> any sudden drop of difficulty would be always an issue, while there is no\n> such thing as: ASIC technology regression.\n>\n> Obviously, not complicated solution would be better than complicated one.\n>\n>\n> W dniu 2022-12-30 19:21:10 u\u017cytkownik Billy Tetrud <billy.tetrud at gmail.com>\n> napisa\u0142:\n>\n> If the idea is to ensure that a catastrophic miner exodus doesn't happen,\n> the \"difference\" you're calculating should only care about downward\n> differences. Upward differences indicate more mining activity and so\n> shouldn't cause a halving skip.\n>\n> But I don't think any scheme like this that only acts on the basis of\n> difficulty will be sufficient. If it gets to the point where a sudden drop\n> in mining difficulty happens, it is very likely that simply delaying the\n> next halving or even ending halving all together will not be sufficient to\n> correct for whatever is causing hashrate to tank. There is also the danger\n> of simple difficulty stagnation, which this mechanism wouldn't detect.\n>\n> The relationship between difficulty and security becomes less and less\n> predictable the longer you want to look ahead. There's no long term\n> relation between difficulty and any reasonable security target. A security\n> target might be something like \"no colluding group with less than $1\n> trillion dollars at their disposal could successfully 51% attack the\n> network (with a probability of blah blah)\". There is no way to today\n> program in any code that detects based on difficult alone when that\n> criteria is violated. You would have to program in assumptions about the\n> cost of hashrate projected into the future.\n>\n> I can't think of any robust automatic way to do this. I think to a certain\n> degree, it will have to be a change that happens in a fork of some kind\n> (soft or hard) periodically (every 10 years? 30 years?). The basic\n> relations needed is really the cost in Bitcoin of the security target (ie\n> the minimum number of Bitcoin it should take to 51% attack the system) and\n> the cost in Bitcoin of acquiring a unit of hashrate. This could be simply\n> input into the code, or could use some complicated oracle system. But with\n> that relation, the system could be programmed to calculate the difficulty\n> necessary to keep the system secure.\n>\n> Once that is in place, the system could automatically adjust the subsidy\n> up or down to attract more or less miners, or it could adjust the block\n> size up or down to change the fee market such that more or less total fees\n> are collected each block to attract more or less miners.\n>\n> On Tue, Dec 27, 2022, 09:41 Jaroslaw via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>>\n>> It seems like the more elegant solution could be by using a chainwork\n>> parameter instead.\n>> i.e. comparison just before halving - if the last 210,000 block interval\n>> has a higher chainwork difference between the begining and the end of\n>> interval\n>> than any other such inter-halving interval before.\n>>\n>> LIttle digression yet:\n>> A system in which all users participate in ensuring its security looks\n>> better than one in which only some (i.e. active) of them participate (and\n>> passive stakeholders are de facto free riders)\n>> In my opinion this concept above is only the complement of currently\n>> missing mechanism: achieving equilibrium regarding costs of security\n>> between two parties with opposing interests.\n>> It's easy to understand and - most important - it has no hardcoded value\n>> of tail emission - what is the clear proof it is based on a free market.\n>> And last but not least, if someone is 100% sure that income from\n>> transactions will takeover security support from block subsidy - accepting\n>> such proposal is like putting the money where the mouth is: this safety\n>> measure will never be triggered, then (no risk of fork)\n>>\n>>\n>> Best Regards\n>> Jaroslaw\n>>\n>>\n>>\n>> W dniu 2022-12-23 20:29:20 u\u017cytkownik Jaroslaw via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> napisa\u0142:\n>> >\n>> Necessary or not - it doesn't hurt to plan the robust model, just in\n>> case. The proposal is:\n>>\n>> Let every 210,000 the code calculate the average difficulty of 100 last\n>> retargets (100 fit well in 210,000 / 2016 = 104.166)\n>> and compare with the maximum of all such values calculated before, every\n>> 210,000 blocks:\n>>\n>>\n>> if average_diff_of_last_100_retargets >\n>> maximum_of_all_previous_average_diffs\n>>         do halving\n>> else\n>>         do nothing\n>>\n>>\n>> This way:\n>>\n>> 1. system cannot be played\n>> 2. only in case of destructive halving: system waits for the recovery of\n>> network security\n>>\n>>\n>> Best Regards\n>> Jaroslaw\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230101/6da1a895/attachment.html>"
            },
            {
                "author": "jk_14 at op.pl",
                "date": "2023-01-01T22:27:38",
                "message_text_only": "Is a storage fee averaged out over many future blocks - but not hardcoded value and regulated by a free market?\n\n\nThe problem with demurrage I see is that the fee is taken when you spend. There is no additional income for miners if people are still hoarding.\nIn tail emission even if people are still hoarding - the fee is taken immediately and is distributed to miners.\n\nWe have a hope there is still the global adoption ahead (most of countries are like El Salvador). It may increase price and marketcap of Bitcoin by order of magnitude.\nAnd that's why hoarding in demurrage may still exist: due to extremely appealing long-term risk/reward (i.e. relatively small, delayed tax versus huge possible profit)\n\n\n\n\nW dniu 2022-12-31 00:29:08 u\u017cytkownik Peter Todd <pete at petertodd.org> napisa\u0142:\n> On Fri, Dec 23, 2022 at 07:43:36PM +0100, jk_14 at op.pl wrote:\n> \n> Necessary or not - it doesn't hurt to plan the robust model, just in case. The proposal is:\n> \n> Let every 210,000 the code calculate the average difficulty of 100 last retargets (100 fit well in 210,000 / 2016 = 104.166)\n> and compare with the maximum of all such values calculated before, every 210,000 blocks:\n> \n> \n> if average_diff_of_last_100_retargets > maximum_of_all_previous_average_diffs\n> \tdo halving\n> else\n> \tdo nothing\n> \n> \n> This way:\n> \n> 1. system cannot be played\n> 2. only in case of destructive halving: system waits for the recovery of network security\n\nFirst of all - while I suspct you already understand this issue - I should\npoint out the following:\n\nThe immediate danger we have with halvings is that in a competitive market,\nprofit margins tend towards marginal costs - the cost to produce an additional\nunit of production - rather than total costs - the cost necessary to recover\nprior and future expenses. Since the halving is a sudden shock to the system,\nunder the right conditions we could have a significant amount of hashing power\njust barely able to afford to hash prior to the halving, resulting in all that\nhashing power immediately having to shut down and fees increasing dramatically,\nand likely, chaotically.  Your proposal does not address that problem as it can\nonly measure difficulty prior to the halving point.\n\n\nOther than that problem, I agree that this proposal would, at least in theory,\nbe a positive improvement on the status quo. But it is a hard fork and I don't\nthink there is much hope for such hard forks to be implemented. I believe that\na demmurrage soft-fork, implemented via a storage fee averaged out over many\nfuture blocks, has a much more plausible route towards implementation.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org"
            },
            {
                "author": "jk_14 at op.pl",
                "date": "2023-01-02T23:02:48",
                "message_text_only": "Right now security comes from almost fully from ~1.8% inflation.\nIn November mempool was inflated to ~150MB and people were rather waiting for cheap transactions back.\nInstead of being happy that system is closer for a while to default working area.\n\nDeflation in Bitcoin is not 1:1 matter like in gold, for example.\nIf all plain gold available to mine would be finished - gold mines as unprofitable enterprices are immediately closed.\nAnd it doesn't affect security of gold already in circulation.\nIn Bitcoin \"the show must go on\" and someone must pay for it.\nActive and passive users together (balanced by market play) or: only active users (in current scenario, long-term).\n\nDeflation (or more precisely: tiny inflation) in Bitcoin is more complex issue with more repercussions than in gold.\nIn case of drop of network security - the tax will be paid anyway, in Bitcoin price.\nSo, there is an self-regulating mechanism here. The harsh one, but still.\n\n\n\nW dniu 2023-01-02 05:53:57 u\u017cytkownik Billy Tetrud <billy.tetrud at gmail.com> napisa\u0142:\n>\u00a0is surely better than not delaying it.\n\u00a0\nI might agree, but I don't think it really solves the problem well enough to be worth it. Any solution that would solve the problem better would make delaying halvings unnecessary.\u00a0\n\u00a0\n>\u00a0there is non-zero risk that people will hoard it more and more, according to old Gresham's law\n\n\nGresham's law doesn't apply here. Gresham's law is about the interaction between two currencies with a fixed, usually government-enforced exchange rate. You seem to be saying that Bitcoin will be hoarded because Bitcoin inflation reduces every halving. But even with 0 inflation, it certainly won't cause all Bitcoin to be hoarded. Also, \"hoarding\" is also known as \"saving\", and there's nothing wrong with saving. The spectre of deflation comes from a misunderstanding of deflation and why it happens during bad economic times. It is an effect, not a cause.\n\n\nOn Sun, Jan 1, 2023, 15:23 <jk_14 at op.pl> wrote:\n\nYes, the idea is:\nif mining activity is growing - let's execute consecutive halvings\nbut if miner exodus has happened - let's delay next halving until mining activity is recovered to previous levels\n\nIf it gets to the point where a sudden drop in mining difficulty happens - delaying the next halving may be not sufficient to correct, but is surely better than not delaying it.\n\nWhile Bitcoin is better and better money with every halving in comparision to other types of money - there is non-zero risk that people will hoard it more and more, according to old Gresham's law (\"HODL\"). And this way decreasing liquidity / transactions volume. The positive feedback loop - is my real concern here.\n\nRegarding the relationship between difficulty and security - I fully agree.\nBut ASIC technology is already matured. And also any technology breakthrough is a short event within 4 years period.\nSo growth of difficulty could be gained by technology breakthrough, but any sudden drop of difficulty would be always an issue, while there is no such thing as: ASIC technology regression.\n\nObviously, not complicated solution would be better than complicated one.\n\n\n\n\nW dniu 2022-12-30 19:21:10 u\u017cytkownik Billy Tetrud <billy.tetrud at gmail.com> napisa\u0142:\nIf the idea is to ensure that a catastrophic miner exodus doesn't happen, the \"difference\" you're calculating should only care about downward differences. Upward differences indicate more mining activity and so shouldn't cause a halving skip.\n\n\nBut I don't think any scheme like this that only acts on the basis of difficulty will be sufficient. If it gets to the point where a sudden drop in mining difficulty happens, it is very likely that simply delaying the next halving or even ending halving all together will not be sufficient to correct for whatever is causing hashrate to tank. There is also the danger of simple difficulty stagnation, which this mechanism wouldn't detect.\u00a0\n\n\nThe relationship between difficulty and security becomes less and less predictable the longer you want to look ahead. There's no long term relation between difficulty and any reasonable security target. A security target might be something like \"no colluding group with less than $1 trillion dollars at their disposal could successfully 51% attack the network (with a probability of blah blah)\". There is no way to today program in any code that detects based on difficult alone when that criteria is violated. You would have to program in assumptions about the cost of hashrate projected into the future.\n\n\nI can't think of any robust automatic way to do this. I think to a certain degree, it will have to be a change that happens in a fork of some kind (soft or hard) periodically (every 10 years? 30 years?). The basic relations needed is really the cost in Bitcoin of the security target (ie the minimum number of Bitcoin it should take to 51% attack the system) and the cost in Bitcoin of acquiring a unit of hashrate. This could be simply input into the code, or could use some complicated oracle system. But with that relation, the system could be programmed to calculate the difficulty necessary to keep the system secure.\n\n\nOnce that is in place, the system could automatically adjust the subsidy up or down to attract more or less miners, or it could adjust the block size up or down to change the fee market such that more or less total fees are collected each block to attract more or less miners.\u00a0\n\n\nOn Tue, Dec 27, 2022, 09:41 Jaroslaw via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\nIt seems like the more elegant solution could be by using a chainwork parameter instead.\ni.e. comparison just before halving - if the last 210,000 block interval has a higher chainwork difference between the begining and the end of interval\nthan any other such inter-halving interval before.\n\nLIttle digression yet:\nA system in which all users participate in ensuring its security looks better than one in which only some (i.e. active) of them participate (and passive stakeholders are de facto free riders)\nIn my opinion this concept above is only the complement of currently missing mechanism: achieving equilibrium regarding costs of security between two parties with opposing interests.\nIt's easy to understand and - most important - it has no hardcoded value of tail emission - what is the clear proof it is based on a free market.\nAnd last but not least, if someone is 100% sure that income from transactions will takeover security support from block subsidy - accepting such proposal is like putting the money where the mouth is: this safety measure will never be triggered, then (no risk of fork)\n\n\nBest Regards\nJaroslaw\n\n\n\nW dniu 2022-12-23 20:29:20 u\u017cytkownik Jaroslaw via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> napisa\u0142:\n>\nNecessary or not - it doesn't hurt to plan the robust model, just in case. The proposal is:\n\nLet every 210,000 the code calculate the average difficulty of 100 last retargets (100 fit well in 210,000 / 2016 = 104.166)\nand compare with the maximum of all such values calculated before, every 210,000 blocks:\n\n\nif average_diff_of_last_100_retargets > maximum_of_all_previous_average_diffs\n\u00a0 \u00a0 \u00a0 \u00a0 do halving\nelse\n\u00a0 \u00a0 \u00a0 \u00a0 do nothing\n\n\nThis way:\n\n1. system cannot be played\n2. only in case of destructive halving: system waits for the recovery of network security\n\n\nBest Regards\nJaroslaw\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n\n\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Billy Tetrud",
                "date": "2023-01-04T16:03:10",
                "message_text_only": "> In Bitcoin \"the show must go on\" and someone must pay for it. Active\n[and/or] passive users\n\nI certainly agree.\n\n> or more precisely: tiny inflation\n\n\ud83d\udc4d\n\n> Right now security comes from almost fully from ~1.8% inflation.\n\nBest I could find, fees make up about 13% of miner revenue\n<https://decrypt.co/57740/bitcoin-miners-now-earn-1-btc-in-fees-per-block>.\nSo yes, the vast majority of security comes from coinbase rewards. I assume\nyou're implying that ~13% of today's security is not enough? I would love\nto see any quantitative thoughts you have on how one might determine that.\n\nHave there been any thoughts put out in the community as to what size of\nthreat is unlikely enough to arise that we don't need to worry about it?\nMaybe 1% of the yearly government budgets\n<https://en.wikipedia.org/wiki/List_of_countries_by_government_budget> of\nthe world would be an upper bound on how much anyone would expect could\nrealistically be brought to bear? Today that would be maybe around $350\nbillion.\n\nOr perhaps a better way to estimate would be calculating the size of the\nmotivation of an attacker. For example, this paper\n<https://files.stlouisfed.org/files/htdocs/publications/review/92/03/Seigniorage_Mar_Apr1992.pdf>\nseems\nto conclude that the US government was extracting a maximum of ~$20\nbillion/year in 1982 dollars (so maybe $60 billion/year in 2022 dollars if\nyou go by CPI). If we scale this up to the entire world of governments,\nthis seems like it would place an upper bound of $180 billion/year of\nseigniorage extraction that would be at risk if bitcoin might put the\ncurrencies they gain seigniorage from out of business. Over 10 years (about\nas far as we can expect any government to think), that's almost $2\ntrillion.\n\nWhereas it would currently cost probably less than $7 billion to purchase a\n50% share of bitcoin miners. To eventually reach a level of $350 billion,\nbitcoin's price would need to reach about $800,000 / bitcoin. That seems\nwithin the realm of possibility. To reach a level of $2 trillion, you'd\nneed a price of $4.3 million/bitcoin. That's still probably within the\nrealm of possibility, but certainly not as likely.  If you then assume we\nwon't have significant coinbase rewards by that point, and only 13% of the\nequivalent revenue (from fees) would be earned, then a price of ~$6 million\nwould be needed to support a $350 billion and $34 million to support a $2\ntrillion security. I think that second one is getting up towards the realm\nof impossibility, so if we think that much security is necessary, we might\nhave to rethink things. Its also quite possible, as the network of people\nwho accept and use bitcoin as payment grows, that the fee market will grow\nsuperlinearly in comparison to market cap, which would make these kind of\nhigh levels of security more realistic.\n\nAnyways if it turns out that fees alone don't look like they're supporting\nenough security, we have a good amount of time to come to that conclusion\nand do something about it.\n\n> Deflation in Bitcoin is not 1:1 matter like in gold, for example...\nDeflation in Bitcoin is more complex issue\n\nIt's helpful to keep our language precise here. Price inflation and\ndeflation act identically in bitcoin and gold and anything else. What you\nseem to be talking about at this point is monetary inflation (specifically,\na reduction in it) which of course operates differently on the machinery of\nbitcoin than it does in the machinery of gold or other things. Whereas my\ncomment about you mentioning Gresham's law was specifically talking about\nprice inflation, not the effects of the coin emission machinery in bitcoin.\n\nOn Mon, Jan 2, 2023 at 5:02 PM <jk_14 at op.pl> wrote:\n\n>\n>\n> Right now security comes from almost fully from ~1.8% inflation.\n> In November mempool was inflated to ~150MB and people were rather waiting\n> for cheap transactions back.\n> Instead of being happy that system is closer for a while to default\n> working area.\n>\n> Deflation in Bitcoin is not 1:1 matter like in gold, for example.\n> If all plain gold available to mine would be finished - gold mines as\n> unprofitable enterprices are immediately closed.\n> And it doesn't affect security of gold already in circulation.\n> In Bitcoin \"the show must go on\" and someone must pay for it.\n> Active and passive users together (balanced by market play) or: only\n> active users (in current scenario, long-term).\n>\n> Deflation (or more precisely: tiny inflation) in Bitcoin is more complex\n> issue with more repercussions than in gold.\n> In case of drop of network security - the tax will be paid anyway, in\n> Bitcoin price.\n> So, there is an self-regulating mechanism here. The harsh one, but still.\n>\n>\n>\n> W dniu 2023-01-02 05:53:57 u\u017cytkownik Billy Tetrud <billy.tetrud at gmail.com>\n> napisa\u0142:\n> > is surely better than not delaying it.\n>\n> I might agree, but I don't think it really solves the problem well enough\n> to be worth it. Any solution that would solve the problem better would make\n> delaying halvings unnecessary.\n>\n> > there is non-zero risk that people will hoard it more and more,\n> according to old Gresham's law\n>\n>\n> Gresham's law doesn't apply here. Gresham's law is about the interaction\n> between two currencies with a fixed, usually government-enforced exchange\n> rate. You seem to be saying that Bitcoin will be hoarded because Bitcoin\n> inflation reduces every halving. But even with 0 inflation, it certainly\n> won't cause all Bitcoin to be hoarded. Also, \"hoarding\" is also known as\n> \"saving\", and there's nothing wrong with saving. The spectre of deflation\n> comes from a misunderstanding of deflation and why it happens during bad\n> economic times. It is an effect, not a cause.\n>\n>\n> On Sun, Jan 1, 2023, 15:23 <jk_14 at op.pl> wrote:\n>\n> Yes, the idea is:\n> if mining activity is growing - let's execute consecutive halvings\n> but if miner exodus has happened - let's delay next halving until mining\n> activity is recovered to previous levels\n>\n> If it gets to the point where a sudden drop in mining difficulty happens -\n> delaying the next halving may be not sufficient to correct, but is surely\n> better than not delaying it.\n>\n> While Bitcoin is better and better money with every halving in comparision\n> to other types of money - there is non-zero risk that people will hoard it\n> more and more, according to old Gresham's law (\"HODL\"). And this way\n> decreasing liquidity / transactions volume. The positive feedback loop - is\n> my real concern here.\n>\n> Regarding the relationship between difficulty and security - I fully agree.\n> But ASIC technology is already matured. And also any technology\n> breakthrough is a short event within 4 years period.\n> So growth of difficulty could be gained by technology breakthrough, but\n> any sudden drop of difficulty would be always an issue, while there is no\n> such thing as: ASIC technology regression.\n>\n> Obviously, not complicated solution would be better than complicated one.\n>\n>\n>\n>\n> W dniu 2022-12-30 19:21:10 u\u017cytkownik Billy Tetrud <billy.tetrud at gmail.com>\n> napisa\u0142:\n> If the idea is to ensure that a catastrophic miner exodus doesn't happen,\n> the \"difference\" you're calculating should only care about downward\n> differences. Upward differences indicate more mining activity and so\n> shouldn't cause a halving skip.\n>\n>\n> But I don't think any scheme like this that only acts on the basis of\n> difficulty will be sufficient. If it gets to the point where a sudden drop\n> in mining difficulty happens, it is very likely that simply delaying the\n> next halving or even ending halving all together will not be sufficient to\n> correct for whatever is causing hashrate to tank. There is also the danger\n> of simple difficulty stagnation, which this mechanism wouldn't detect.\n>\n>\n> The relationship between difficulty and security becomes less and less\n> predictable the longer you want to look ahead. There's no long term\n> relation between difficulty and any reasonable security target. A security\n> target might be something like \"no colluding group with less than $1\n> trillion dollars at their disposal could successfully 51% attack the\n> network (with a probability of blah blah)\". There is no way to today\n> program in any code that detects based on difficult alone when that\n> criteria is violated. You would have to program in assumptions about the\n> cost of hashrate projected into the future.\n>\n>\n> I can't think of any robust automatic way to do this. I think to a certain\n> degree, it will have to be a change that happens in a fork of some kind\n> (soft or hard) periodically (every 10 years? 30 years?). The basic\n> relations needed is really the cost in Bitcoin of the security target (ie\n> the minimum number of Bitcoin it should take to 51% attack the system) and\n> the cost in Bitcoin of acquiring a unit of hashrate. This could be simply\n> input into the code, or could use some complicated oracle system. But with\n> that relation, the system could be programmed to calculate the difficulty\n> necessary to keep the system secure.\n>\n>\n> Once that is in place, the system could automatically adjust the subsidy\n> up or down to attract more or less miners, or it could adjust the block\n> size up or down to change the fee market such that more or less total fees\n> are collected each block to attract more or less miners.\n>\n>\n> On Tue, Dec 27, 2022, 09:41 Jaroslaw via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> It seems like the more elegant solution could be by using a chainwork\n> parameter instead.\n> i.e. comparison just before halving - if the last 210,000 block interval\n> has a higher chainwork difference between the begining and the end of\n> interval\n> than any other such inter-halving interval before.\n>\n> LIttle digression yet:\n> A system in which all users participate in ensuring its security looks\n> better than one in which only some (i.e. active) of them participate (and\n> passive stakeholders are de facto free riders)\n> In my opinion this concept above is only the complement of currently\n> missing mechanism: achieving equilibrium regarding costs of security\n> between two parties with opposing interests.\n> It's easy to understand and - most important - it has no hardcoded value\n> of tail emission - what is the clear proof it is based on a free market.\n> And last but not least, if someone is 100% sure that income from\n> transactions will takeover security support from block subsidy - accepting\n> such proposal is like putting the money where the mouth is: this safety\n> measure will never be triggered, then (no risk of fork)\n>\n>\n> Best Regards\n> Jaroslaw\n>\n>\n>\n> W dniu 2022-12-23 20:29:20 u\u017cytkownik Jaroslaw via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> napisa\u0142:\n> >\n> Necessary or not - it doesn't hurt to plan the robust model, just in case.\n> The proposal is:\n>\n> Let every 210,000 the code calculate the average difficulty of 100 last\n> retargets (100 fit well in 210,000 / 2016 = 104.166)\n> and compare with the maximum of all such values calculated before, every\n> 210,000 blocks:\n>\n>\n> if average_diff_of_last_100_retargets >\n> maximum_of_all_previous_average_diffs\n>         do halving\n> else\n>         do nothing\n>\n>\n> This way:\n>\n> 1. system cannot be played\n> 2. only in case of destructive halving: system waits for the recovery of\n> network security\n>\n>\n> Best Regards\n> Jaroslaw\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230104/dfb173d9/attachment-0001.html>"
            },
            {
                "author": "jk_14 at op.pl",
                "date": "2023-01-07T18:52:47",
                "message_text_only": "> Anyways if it turns out that fees alone don't look like they're supporting enough security, we have a good amount of time to come to that conclusion and do something about it.\u00a0\n\nThe worst-case scenario is that the first global hashrate regression may take place in 2028.\nInstead of the average price increase at least x2 every halving - the global hashrate may gradually decrease from that point. Again, it would be the worst-case scenario.\n\nIn my proposal you don't need to think about any calculations - just simple logic which we have right now. No hardcoded values and the free market in its finest - self-regulating the level of taxation of parties involved, but with opposite interests. And the mechanism would try to fix a global hashrate regression if appear.\nIn other words: let's be optimistic regarding fees, but with emergency mechanism built-in just in case.\nThe only drawback here is that the system is already running.\n\nIn my personal opinion avoiding long-term global hashrate regression is more important for store of value feature than the 21M schelling point (or trap...)\n\n\n\n\nW dniu 2023-01-04 17:03:33 u\u017cytkownik Billy Tetrud <billy.tetrud at gmail.com> napisa\u0142:\n> In Bitcoin \"the show must go on\" and someone must pay for it. Active [and/or] passive users\u00a0\n\n\nI certainly\u00a0agree.\u00a0\n\n\n> or more precisely: tiny inflation\n\n\n\ud83d\udc4d\n\n\n> Right now security comes from almost fully from ~1.8% inflation.\n\n\nBest I could find, fees make up about 13% of miner revenue. So yes, the vast majority of security comes from coinbase rewards. I assume you're implying that ~13% of today's security is not enough? I would love to see any quantitative\u00a0thoughts you have on how one might determine that.\u00a0\n\n\nHave there been any thoughts put out in the community as to what size of threat is unlikely enough to arise\u00a0that we don't need to worry about it? Maybe 1% of the yearly\u00a0government budgets\u00a0of the world\u00a0would be an upper bound on how much anyone would expect could realistically be brought to bear? Today that would be maybe around $350 billion.\u00a0\n\n\nOr perhaps a better way to estimate would be calculating the size of the motivation of an attacker. For example, this paper\u00a0seems to conclude that the US government was extracting a maximum of ~$20 billion/year in 1982 dollars (so maybe $60 billion/year in 2022 dollars if you go by CPI). If we scale this up to the entire world of governments, this seems like it would place an upper bound of $180 billion/year of seigniorage extraction that would be at risk if bitcoin might put the currencies they gain seigniorage from out of business. Over 10 years (about as far as we can expect any government to think), that's almost $2 trillion.\u00a0\n\n\nWhereas it would currently cost probably less than $7 billion\u00a0to purchase a 50% share of bitcoin miners. To eventually reach a level of $350 billion, bitcoin's price would need to reach about $800,000 / bitcoin. That seems within the realm of possibility. To reach a level of $2 trillion, you'd need a price of $4.3 million/bitcoin. That's still probably within the realm of possibility, but certainly not as likely.\u00a0 If you then assume we won't have significant coinbase rewards by that point, and only 13% of the equivalent revenue (from fees) would be earned, then a price of ~$6 million would be needed to support a $350 billion and $34 million to support a $2 trillion security. I think that second one is getting up towards the realm of impossibility, so if we think that much security is necessary, we might have to rethink things. Its also quite possible, as the network of people who accept and use bitcoin as payment grows, that the fee market will grow superlinearly in comparison to market cap, which would make these kind of high levels of security more realistic.\u00a0\n\n\nAnyways if it turns out that fees alone don't look like they're supporting enough security, we have a good amount of time to come to that conclusion and do something about it.\u00a0\n\n\n> Deflation in Bitcoin is not 1:1 matter like in gold, for example...\u00a0 Deflation in Bitcoin is more complex issue\n\n\nIt's helpful to keep our language precise here. Price inflation and deflation act identically in bitcoin and gold and anything else. What you seem to be talking about at this point is monetary inflation (specifically, a reduction in it) which of course operates differently on the machinery of bitcoin than it does in the machinery of gold or other things. Whereas my comment about you mentioning Gresham's law was specifically talking about price inflation, not the effects of the coin emission machinery in bitcoin.\u00a0\n\n\n\n\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Eric",
                "date": "2023-01-07T23:22:02",
                "message_text_only": "if by security you mean the security of the currency, i don't think people\nhave much to worry about\n\ncoinbase as far as i know is starting to behave more bank-like.  i think\nthere is a nostr bot that does block updates and doesn't factor in coinbase\nat all\n\nOn Sat, Jan 7, 2023 at 2:13 PM Jaroslaw via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n> > Anyways if it turns out that fees alone don't look like they're\n> supporting enough security, we have a good amount of time to come to that\n> conclusion and do something about it.\n>\n> The worst-case scenario is that the first global hashrate regression may\n> take place in 2028.\n> Instead of the average price increase at least x2 every halving - the\n> global hashrate may gradually decrease from that point. Again, it would be\n> the worst-case scenario.\n>\n> In my proposal you don't need to think about any calculations - just\n> simple logic which we have right now. No hardcoded values and the free\n> market in its finest - self-regulating the level of taxation of parties\n> involved, but with opposite interests. And the mechanism would try to fix a\n> global hashrate regression if appear.\n> In other words: let's be optimistic regarding fees, but with emergency\n> mechanism built-in just in case.\n> The only drawback here is that the system is already running.\n>\n> In my personal opinion avoiding long-term global hashrate regression is\n> more important for store of value feature than the 21M schelling point (or\n> trap...)\n>\n>\n>\n>\n> W dniu 2023-01-04 17:03:33 u\u017cytkownik Billy Tetrud <billy.tetrud at gmail.com>\n> napisa\u0142:\n> > In Bitcoin \"the show must go on\" and someone must pay for it. Active\n> [and/or] passive users\n>\n>\n> I certainly agree.\n>\n>\n> > or more precisely: tiny inflation\n>\n>\n> \ud83d\udc4d\n>\n>\n> > Right now security comes from almost fully from ~1.8% inflation.\n>\n>\n> Best I could find, fees make up about 13% of miner revenue. So yes, the\n> vast majority of security comes from coinbase rewards. I assume you're\n> implying that ~13% of today's security is not enough? I would love to see\n> any quantitative thoughts you have on how one might determine that.\n>\n>\n> Have there been any thoughts put out in the community as to what size of\n> threat is unlikely enough to arise that we don't need to worry about it?\n> Maybe 1% of the yearly government budgets of the world would be an upper\n> bound on how much anyone would expect could realistically be brought to\n> bear? Today that would be maybe around $350 billion.\n>\n>\n> Or perhaps a better way to estimate would be calculating the size of the\n> motivation of an attacker. For example, this paper seems to conclude that\n> the US government was extracting a maximum of ~$20 billion/year in 1982\n> dollars (so maybe $60 billion/year in 2022 dollars if you go by CPI). If we\n> scale this up to the entire world of governments, this seems like it would\n> place an upper bound of $180 billion/year of seigniorage extraction that\n> would be at risk if bitcoin might put the currencies they gain seigniorage\n> from out of business. Over 10 years (about as far as we can expect any\n> government to think), that's almost $2 trillion.\n>\n>\n> Whereas it would currently cost probably less than $7 billion to purchase\n> a 50% share of bitcoin miners. To eventually reach a level of $350 billion,\n> bitcoin's price would need to reach about $800,000 / bitcoin. That seems\n> within the realm of possibility. To reach a level of $2 trillion, you'd\n> need a price of $4.3 million/bitcoin. That's still probably within the\n> realm of possibility, but certainly not as likely.  If you then assume we\n> won't have significant coinbase rewards by that point, and only 13% of the\n> equivalent revenue (from fees) would be earned, then a price of ~$6 million\n> would be needed to support a $350 billion and $34 million to support a $2\n> trillion security. I think that second one is getting up towards the realm\n> of impossibility, so if we think that much security is necessary, we might\n> have to rethink things. Its also quite possible, as the network of people\n> who accept and use bitcoin as payment grows, that the fee market will grow\n> superlinearly in comparison to market cap, which would make these kind of\n> high levels of security more realistic.\n>\n>\n> Anyways if it turns out that fees alone don't look like they're supporting\n> enough security, we have a good amount of time to come to that conclusion\n> and do something about it.\n>\n>\n> > Deflation in Bitcoin is not 1:1 matter like in gold, for example...\n> Deflation in Bitcoin is more complex issue\n>\n>\n> It's helpful to keep our language precise here. Price inflation and\n> deflation act identically in bitcoin and gold and anything else. What you\n> seem to be talking about at this point is monetary inflation (specifically,\n> a reduction in it) which of course operates differently on the machinery of\n> bitcoin than it does in the machinery of gold or other things. Whereas my\n> comment about you mentioning Gresham's law was specifically talking about\n> price inflation, not the effects of the coin emission machinery in bitcoin.\n>\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230107/2f281099/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2023-01-18T20:58:07",
                "message_text_only": "On Sun, Jan 01, 2023 at 11:42:50PM +1100, Alfie John wrote:\n> On 31 Dec 2022, at 10:28 am, Peter Todd via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > \n> >> This way:\n> >> \n> >> 1. system cannot be played\n> >> 2. only in case of destructive halving: system waits for the recovery of network security\n> > \n> > The immediate danger we have with halvings is that in a competitive market,\n> > profit margins tend towards marginal costs - the cost to produce an additional\n> > unit of production - rather than total costs - the cost necessary to recover\n> > prior and future expenses. Since the halving is a sudden shock to the system,\n> > under the right conditions we could have a significant amount of hashing power\n> > just barely able to afford to hash prior to the halving, resulting in all that\n> > hashing power immediately having to shut down and fees increasing dramatically,\n> > and likely, chaotically.  Your proposal does not address that problem as it can\n> > only measure difficulty prior to the halving point.\n> \n> \n> > ... Since the halving is a sudden shock to the system\n> \n> Is it though? Since everyone knows of the possible outcomes, wouldn't a possible halving be priced in? \n\nRe-read that I said. That explains why despite the halving being a forseeable\nevent, there's no mechanism to \"price it in\" when it comes to hashing power.\n\n> > resulting in all that hashing power immediately having to shut down and fees increasing dramatically\n> \n> Which should cause that hashing power to come back because of this fee increases.\n\nRight now the total reward per transaction is $63, three orders of magnitude\nhigher than typical fees. Sufficient fee increases to bring back hashing power\nin a scenario like that would cause enormous disruption to many things,\nincluding Lightning channels.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230118/8746a7e7/attachment-0001.sig>"
            },
            {
                "author": "jk_14 at op.pl",
                "date": "2023-01-21T10:20:02",
                "message_text_only": "This is the phrase that should be recalled very often:\n\n\"the total reward per transaction is Three Orders of Magnitude\nhigher than typical fees. Sufficient fee increases to bring back hashing power\nin a scenario like that would cause Enormous Disruption to many things,\nincluding Lightning channels\"\n\n> Your proposal does not address that problem as it can only measure difficulty prior to the halving point\n\nYes, my proposal of fixing the inevitable (but only spreaded over the long time) failure - is quite conservative, surprisingly.\n\nSimplifying it to the edge case:\nIf in a four-year perspective there is no the average price +100% increase - to properly compensate last halving,\nbut instead there is a hashrate -50% drop - another possible and \"proper\" (!) compensation\n\n- absolutely don't worse the situation by executing next halving,\naccept such drop because there is nothing you can do about it\nand wait with halvings for the hashrate to recover. As long as it takes.\nMaybe even 20 years if necessary (fortunately we are at mature phase of ASIC technology right now),\nAnd iterate.\n\nThis way we land at lowest possible annual inflation and set by a free market.\n\nAs I said this is quite conservative approach. It would suit bitcoin,.\nToo bad it wasn't foreseen at the beginning...\n\n\n\n\nW dniu 2023-01-18 21:58:15 u\u017cytkownik Peter Todd <pete at petertodd.org> napisa\u0142:\n> On Sun, Jan 01, 2023 at 11:42:50PM +1100, Alfie John wrote:\n> On 31 Dec 2022, at 10:28 am, Peter Todd via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > \n> >> This way:\n> >> \n> >> 1. system cannot be played\n> >> 2. only in case of destructive halving: system waits for the recovery of network security\n> > \n> > The immediate danger we have with halvings is that in a competitive market,\n> > profit margins tend towards marginal costs - the cost to produce an additional\n> > unit of production - rather than total costs - the cost necessary to recover\n> > prior and future expenses. Since the halving is a sudden shock to the system,\n> > under the right conditions we could have a significant amount of hashing power\n> > just barely able to afford to hash prior to the halving, resulting in all that\n> > hashing power immediately having to shut down and fees increasing dramatically,\n> > and likely, chaotically.  Your proposal does not address that problem as it can\n> > only measure difficulty prior to the halving point.\n> \n> \n> > ... Since the halving is a sudden shock to the system\n> \n> Is it though? Since everyone knows of the possible outcomes, wouldn't a possible halving be priced in? \n\nRe-read that I said. That explains why despite the halving being a forseeable\nevent, there's no mechanism to \"price it in\" when it comes to hashing power.\n\n> > resulting in all that hashing power immediately having to shut down and fees increasing dramatically\n> \n> Which should cause that hashing power to come back because of this fee increases.\n\nRight now the total reward per transaction is $63, three orders of magnitude\nhigher than typical fees. Sufficient fee increases to bring back hashing power\nin a scenario like that would cause enormous disruption to many things,\nincluding Lightning channels.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org"
            },
            {
                "author": "John Tromp",
                "date": "2023-01-22T14:13:27",
                "message_text_only": "> Right now the total reward per transaction is $63, three orders of magnitude\n> higher than typical fees.\n\nNo need to exaggerate; this is only two orders of magnitude higher\nthan current fees, which are typically over $0.50"
            }
        ],
        "thread_summary": {
            "title": "Pseudocode for robust tail emission",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "jk_14 at op.pl",
                "Eric",
                "Peter Todd",
                "John Tromp",
                "Billy Tetrud",
                "Alfie John"
            ],
            "messages_count": 11,
            "total_messages_chars_count": 53004
        }
    },
    {
        "title": "[bitcoin-dev] A Bitcoin NFT System",
        "thread_messages": [
            {
                "author": "Vincenzo",
                "date": "2023-01-04T20:59:49",
                "message_text_only": "On Thu Dec 29, 2022 at 5:49 PM CET, Aymeric Vitte via bitcoin-dev wrote:\n> I am not a fan of NFTs as currently designed and used, centralized,\n> insecure, duplicable, virtual, stealable, not signed, expensive\n>\n> But if you consider that a NFT is anything that you can buy or store, as\n> something real, or electronic, or whatever, in the real world, web or\n> metaverse, then it becomes interesting\n>\n> NFTs are mostly used on Ethereum, and a \"mess\" to describe this stuff is\n> a weak word\n>\n> Then I wrote: \"A Bitcoin NFT system\": \n> https://gist.github.com/Ayms/01dbfebf219965054b4a3beed1bfeba7 ,\n> decentralized, secure, not expensive\n>\nThe link is gone or just private.\n\nCheers!\n\nVincent."
            },
            {
                "author": "Aymeric Vitte",
                "date": "2023-01-05T10:46:59",
                "message_text_only": "Hi Vincent,\n\nIndeed the gist was recorded (by mistake) as secret but the link is\nsupposed to work anyway, I have made it public, thanks for confirming\nthat it's working now, just in case I am attaching the text file\n\nRegards\n\nAymeric\n\n\nLe 04/01/2023 \u00e0 21:59, Vincenzo a \u00e9crit :\n> On Thu Dec 29, 2022 at 5:49 PM CET, Aymeric Vitte via bitcoin-dev wrote:\n>> I am not a fan of NFTs as currently designed and used, centralized,\n>> insecure, duplicable, virtual, stealable, not signed, expensive\n>>\n>> But if you consider that a NFT is anything that you can buy or store, as\n>> something real, or electronic, or whatever, in the real world, web or\n>> metaverse, then it becomes interesting\n>>\n>> NFTs are mostly used on Ethereum, and a \"mess\" to describe this stuff is\n>> a weak word\n>>\n>> Then I wrote: \"A Bitcoin NFT system\": \n>> https://gist.github.com/Ayms/01dbfebf219965054b4a3beed1bfeba7 ,\n>> decentralized, secure, not expensive\n>>\n> The link is gone or just private.\n>\n> Cheers!\n>\n> Vincent.\n\n-- \nSophia-Antipolis, France\nLinkedIn: https://fr.linkedin.com/in/aymeric-vitte-05855b26\nGitHub : https://www.github.com/Ayms\nMove your coins by yourself (browser version): https://peersm.com/wallet\nBitcoin transactions made simple: https://github.com/Ayms/bitcoin-transactions\ntorrent-live: https://github.com/Ayms/torrent-live\nnode-Tor : https://www.github.com/Ayms/node-Tor\nZcash wallets made simple: https://github.com/Ayms/zcash-wallets\nBitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets\nAnti-spies and private torrents, dynamic blocklist: http://torrent-live.peersm.com\nPeersm : http://www.peersm.com\n\n-------------- next part --------------\n# A Bitcoin NFT system\n\n## Introduction\n\nNFT is another barbarous name invented by the crypto folks meaning quasi nothing, by NFT in this proposal we will refer to whatever can be sold or referenced from the real world, web and metaverse using a blockchain system\n\nThe easy way solution is usually to use Ethereum, create some NFT tokens via ERC-721/1155 contracts and/or use some marketplaces such as Opensea which always lead to a centralized and insecure system that can collapse at any moment, and which is still complicate since nobody understand what is behind, fortunately (or not) MetaMask will do the job for you without you understanding what you are doing, like wrapped ETH\n\nThis is explained here: [Bitcoin, Ethereum, smart contracts, lightning, sidechains, Defi, DApps, NFTs, DEX, DAO explained - Centralization vs Decentralization](https://gist.github.com/Ayms/04b3084a14ee202e707b3faec57ed26e)\n\nEspecially: [tokens and sidechains](https://gist.github.com/Ayms/04b3084a14ee202e707b3faec57ed26e#example-2-tokens-and-sidechains)\n\nAnd: [Opensea](https://gist.github.com/Ayms/04b3084a14ee202e707b3faec57ed26e#example-6-opensea)\n\n## Purpose\n\nThe purpose of this proposal is to propose <b>a simple NFT system based on the Bitcoin blockchain</b>, assuming that the main purpose of a NFT is to be sold/bought, but not only, it can be something that you keep for yourself proving your ownership on the blockchain, the advantages compared to using Ethereum or any blockchain/sidechain on both networks (or others) will be explained below\n\n## Referencing a NFT\n\nA NFT can be a physical good, an electronic item (such as image), something virtual or not, in fact anything\n\nIf we take a document or an image, a simple reference to the NFT is its hash (SHA-256), but it is not really relevant since it's easy for a thief to slightly modify the document or image which will lead to a different hash\n\nNow in what follows we will allocate a hash to reference any NFT\n\n## The final hash reference to a NFT\n\nDepending on the conditions the hash can be known (public marketplace for example) or unknown (only the buyer and the seller know it or the owner)\n\nThererfore the reference to the NFT will be the SHA-256 of its hash, which then hides the original hash, this is of course useless if the hash is known but let's keep the same convention, called |reference| in what follows\n\n## Minting a NFT on Bitcoin\n\n\"Minting a NFT\" just means to record your ownership of the NFT in the blockchain\n\nTo mint a NFT that you own on Bitcoin, you can do a transaction whose output will be:\n\n    From address A (the seller):\n     OP_RETURN |reference|\n     DUST (A or B) or refund to an address of yourself (A or B)\n\nSince the transaction is signed by A, we know that A is the owner of the reference, but note that from this point the public key of A is known, therefore if the purpose is to store a long term NFT then use another address B of yourself (because in some time it will be easier to crack a bitcoin address for which we know the public key rather than one that holds some bitcoins but has not been spent, so knowing only the hash of this address)\n\nAn OP_RETURN output is just something where you can store some data, limited to 512B\n\nA lot of variations are possible in this article, for example you can decide to allocate one A address per NFT, it just depends on the use cases\n\nFor all the transactions mentioned in this article, opt-in for Replace By Fee must not be used [Replace By Fee](https://bitcoin.fr/replace-by-fee-rbf/)\n\nIt would be possible not to use OP_RETURN and for example store the |reference| in scriptSig, then construct an address (scriptPubKey) that will drop it while executing, but this would be a non standard transaction, so you shoud rely on some miners to mine it, without stealing your NFT, since it's easy for a miner to change the transaction and mint the NFT for himself\n\n## Minting several NFTs in one transaction\n\nYou can do the very same using the Chainpoint protocol, which is to build a Merkle tree from |reference1| |reference2|....|reference N| (for which we recall will be the SHA-256 of the original NFTs hash), and replace the root of this tree in |reference| above\n\nHow Chainpoint work is well described here: [Chainpoint Proof](https://github.com/chainpoint/whitepaper/issues/5#issuecomment-304452107), basically while making a Chainpoint tree you get a proof to rebuild it from any |reference| (that you must know), which is just the other hashes that you need to know to reach the root \n\nIn that case only the owner of the multiple NFTs knows the proofs and/or the |references|, unless he decides to release them, which he should do as described in the next paragraph (because the hashes and proofs must be known to check the validity of NFT transfers)\n\nIn what follows we will make reference to a Chainpoint |reference|\n\n## Avoiding double minting\n\nDouble minting of NFTs is quite trivial, as far as we know there are zero protections against this in existing systems (Ethereum and others), the principle is simple: you mint several time the same NFT and possibly sell it several time or get robbed (the thief does transactions using the |reference| that he saw), because it's easy for a malicious person to do the very same transaction that you did and pretend the ownership of the NFT, or it's easy for you to mint it to different marketplaces or mint it several time, and then sell it several time\n\nTo illustrate this, read [Solving the double minting problem](https://medium.com/geekculture/solving-the-nft-double-minting-problem-with-computer-vision-c57bbbb4652d)\n\nUnfortunately the above article just refers to images and the solution does not work at all at the end, because associating the picture to the camera or whatever that took it does not work and is easy to fake\n\nComing back to our bitcoin case, it seems simple to elect you as the real owner of the NFT since you made the transaction first, then based on the timestamp you are definitely the owner, but no\n\nThe thief could intercept your transaction, decide not to broadcast it further, replace it by a transaction minting your NFT for himself, with higher network fees for example and more direct access to miners, then this transaction will be the first one and the thief would have stolen your NFT\n\nYou can't do anything against this without using a third party to record that you had the knowledge of the |reference| first\n\nThe same problematic exists for example for js code loading, see [Bitcoin-transactions wallet/tools](https://peersm.com/wallet), here our github account is used to check the code\n\nThen the very same is proposed here, preferably using the [Wayback Machine](https://archive.org/web/) to store |reference| (that you are the only one to know at this stage) signed by A before minting the NFT, then for sure you are the owner of the NFT because you are the first one to have referenced |reference| and signed it with your address (note again that as soon as you sign something the public key of A is known)\n\nYou can use other systems, github, even Twitter\n\nWhat if all of those systems disappear and I can't prove anything any longer? Well, then probably internet has disappeared too\n\nAnyway like the WebRTC peer introduction mechanism you can do this by any means you like, your own website, even a letter, you just need to prove that A had the knowledge of the double hash first\n\nNow the double minting case is supposed to be marginal, except if you are a thief yourself\n\n## Transferring a NFT for free\n\nFor a gift or whatever other reason, A can decide to transfer a NFT for free to C, the transaction will be:\n\n    From address A (the seller):\n     OP_RETURN |some code||reference|\n     DUST (C) and refund to an address of yourself (A or B)\n\t\t\n|some code| is just used to make the distinction between a minting operation\n\nA sends the original hash to C\n\t\t\n## C Buying a NFT from A\n\nThe transaction will be:\n\n    From address C (the buyer):\n     OP_RETURN |price||some code||reference|- signed by A\n     |price| to A and refund to C or other C address\n\t\t\nOnce A and C have agreed on a price then A should send to C the |price||some code||reference| signed by him, which proves that the deal is accepted\n\nIf C tries to cheat and does broadcast a transaction that does not match the agreed price (since C cannot fake the signature of A), then C is still not the owner of the NFT and has lost its bitcoin, if a thief intercepts the transaction and does another one for himself, that's the same, since he is not willing to pay for the price\n\nIf A receives the correct amount, then he will send to C the original hash of |reference|, the same principle applies for all following transactions\n\nWe can not really enforce this, A could not send the hash, but for what purpose? This represents the same trust model than when you are buying some goods (the hash is the ticket), now in normal conditions the original hash is not really mandatory\n\nNote that we know from minting/signing the public key of A, therefore anybody can verify the signature in the OP_RETURN\n\nNote again that the signature of A is enough to know its public key and check the signature, <b>therefore minting is not mandatory</b> (depending on the use cases again)\n\nNote also that in Ethereum style, that's the other way around, where A would initiate the final transaction to the NFT contract that will transfer to C\n\n## C selling a NFT to D\n\nThat's the same:\n\n    From address D (the buyer):\n     OP_RETURN |price||some code||reference|- signed by C\n     |price| to C and refund to D or other D address\n\t\t\nC sends the original hash to D\n\n## C Buying a NFT from A from a Chainpoint |reference|\n\nA sends to C the |price||some code||reference||Chainpoint proof| signed by him, C then do the following transaction:\n\n    From address C (the buyer):\n     OP_RETURN |price||some code||reference||Chainpoint proof|- signed by A\n     |price| to A and refund to C or other C address\n\t\t\n## Group/organization C of buyers\n\nC is then a multisig address funded by the group, and as before:\n\n\tFrom address C (the buyers):\n\t\tOP_RETURN |price||some code||reference|- signed by A\n\t\t|price| to A and refund to C or other C address\n\nSame principle would apply for Chainpoint |reference|\n\n## But, in all of this, what is the purpose of the double hash for |reference|?\n\nJust to hide (if applicable) that the hash is only known by the buyer and the seller if the transactions are correct, then if anything unexpected happens they can prove that they are the only one to know the original hash of the NFT\n\nWe can take here the example of Kering (luxury group): [Kering NFT solution](https://www.ulysse-nardin.com/zh_en/blockchain-technology ), this is a variation of what is proposed here, when you buy a luxury product you get a certificate, then you go to their website, use the certificate information + your name & co, then you get a second certificate and the associated hash, the hash is then stored in the bitcoin blockchain in an OP_RETURN using the Chainpoint protocol via [Woleet](https://www.woleet.io/fr/accueil/) service\n\nSo here you are the only one to know the hash (with Kering), the problem of this solution is that it is very centralized and you can't correlate the hash to the product. That's where the double hash method could be used, you print on the luxury product the double hash (QR code or whatever) and give the original hash to the buyer, then if the product is counterfeit (with the same double hash) or stolen you can detect it since the malicious seller does not know the original hash\n\n## Costs\n\nCurrently the network fees are 6-12 satoshis per byte, and the dust limit can be set at 546 satoshis\n\nIt's not possible to give a precise number here since the transactions depend on the number of inputs but an [estimation](https://www.bitstamp.net/learn/blockchain/how-are-btc-transaction-fees-determined/) would be less than 1 USD per transaction (probably 0.5 USD) against 70 to 100 USD on gas fees in Ethereum\n\nThis is far less than Ethereum NFT tokens see [Moxie experience](https://gist.github.com/Ayms/04b3084a14ee202e707b3faec57ed26e#example-7-moxies-experience)\n\n## Is this system centralized?\n\nNo unlike whatever exists about sidechains (except Lightning), Ethereum NFTs or Dstuff, it is not, because you do not rely on any centralized platform to use it (the third party is the one you choose and it can't interact with the transactions), or Oracles that would be involved in your transactions creation/confirmation/transfer\n\n## Is this system anonymous?\n\nNot totally as currently designed, like Lightning again, since buyer/seller must \"know\" each others, but they can do it via anonymous means like Signal, Protonmail, Telegram or other stuff, or Tor, the advise is to create a new good old gmail address for this purpose and use PGP keys\n\nIn any case others around do not know what is going on\n\n## Why not a super sidechain instead?\n\nYou can read this conclusion from a previous article: [Conclusion](https://gist.github.com/Ayms/04b3084a14ee202e707b3faec57ed26e#conclusion)\n\nWe have different examples, but let's take a few, Binance Smart Chain, supposedly compatible with Ethereum is a mess (or FTT/FTX token, see [Collapse of FTX and others: the Fxcking Token Theory](\nhttps://www.linkedin.com/posts/aymeric-vitte-05855b26_the-fxcking-token-theory-as-a-coincidence-activity-6996493598653317120-vCpA)), and [Stacks](https://protos.com/what-is-stacks-and-does-it-really-serve-as-defi-for-bitcoin/) for Bitcoin supposedly allowing \"smart contracts\" and NFT on bitcoin is the same, both centralized, insecure, making it difficult to swap the tokens between one chain to another\n\nLet's hope this article shows how powerfull (and less costly) is bitcoin, if we elaborate on Stacks it's another blockchain on top of Bitcoin, they register the blocks inside OP_RETURN in Bitcoin via a \"proof of transfer\". Why do they do this? No idea but surely because their blockchain is neither secured neither decentralized, this is just flooding the Bitcoin network with useless transactions duplicating proofs on two blockchains\n\nThere are many \"proof-of-whatever\" that are just fake stuff (like Filecoin proof of replication https://github.com/protocol/research/issues/4), useless, quite dubious, as stated in our conclusion above the intermediate states shoud be burned and only the final/relevant result stored in Bitcoin, this is very exactly what is doing Lightning\n\nIn any case, whatever sidechain/blockchain is proposed can only lead to something funny, because even the worldwide networks bitcoin and ethereum are still not decentralized today, despite of their age, because something like a few thousands nodes is not what we can call a decentralized system, then proposing something decentralized on top of it or not does not look serious, the only real decentralized network remains Bittorrent\n\nThis proposal seems to contradict one of our article [How stablecoins really work (Tether example), how to crack them and why you should take care if you use them](https://www.linkedin.com/pulse/how-stablecoins-really-work-tether-example-crack-them-aymeric-vitte/), but no in fact since everything is signed here\n\n## Local Bitcoin\n\nWe will not talk here about the craziness of the Bitcoin forks period, which we have described here: [\"Bitcoin Tartuffe - User guide: How to create your bitcoin fork in 5mn, fool everybody and become rich](https://www.linkedin.com/pulse/user-guide-how-create-your-bitcoin-fork-5mn-fool-everybody-vitte)\n\nBut we can envision a local Bitcoin, which would be limited to an area and is just a fork of the Bitcoin code (and not the Bitcoin network), restarting from the beginning\n\nThen it's possible to adapt the rules according to the local area, for example reducing the costs by eliminating the dust limit, network fees by mining at lowest difficulty, unlimited supply and no halving\n\nThis seems to contradict the good practices of Bitcoin but not really, since the area is limited to people that are supposed to cooperate together, then the risk of attack is limited also\n\nYou might say that we are criticizing in [Bitcoin, Ethereum, smart contracts, lightning, sidechains, Defi, DApps, NFTs, DEX, DAO explained - Centralization vs Decentralization](https://gist.github.com/Ayms/04b3084a14ee202e707b3faec57ed26e) the fact that most of ERC-20/NFT tokens are of zero value, because coming from nowhere, but we are proposing the same\n\nNot exactly, the local Bitcoin could be seen as a barter money, with a stable value, mined by everybody in the area, then unlike ERC-20 tokens you cannot issue 1 Billion of them all of a sudden\n\nIt remains decentralized since everybody is participating and get rewarded according to the probability distribution\n\nTo a certain extent, since the local area management might decide to shut down the system\n\nAn example is a cruise boat (who might have limited connectivity to interact with an outside blockchain and is not willing to burn its batteries with mining) where each element of the boat (cabins, kitchen, etc) would mine the local Bitcoin, then you can buy/exchange NFTs on the boat, services, use the token for the casino, restaurants, shops, the boat metaverse, etc\n\nThe local Bitcoin can be crawled from the outside and transactions can be sent from the outside, so, for example, luxury vendors and others can propose NFTs on board (and deliver them on board or when the boat is at some harbour), and NFTs can be proposed inside the boat shops/metaverse\n\nIndeed since the local Bitcoin is not a fork of Bitcoin network, you don't have to implement replay protection (SIGHASH), then usual Bitcoin transactions can be built and sent to the local Bitcoin, of course you must not make mistakes between both chains\n\nHow can we bridge this with the Bitcoin network ? The first answer is: why would you need to do this? If so, then you need to use a bridge which will be of course something centralized, right now we are not aware of other possible methods (like atomic swap or other bulls)\n\nThis can apply to planes and many other local areas\n\n## License and funding\n\nWhile this work is public, it is not in the public area and under any open source license, then to use it, please contact: aymeric peersm com, PGP Key fingerprint : 65EF AE3D 973F D36A AB25 1225 2A1D 7B34 2627 AC39\n\nThe goal here is to reference who did fund this proposal\n\nhttps://web.archive.org/web/20221231185645/https://gist.github.com/Ayms/01dbfebf219965054b4a3beed1bfeba7"
            }
        ],
        "thread_summary": {
            "title": "A Bitcoin NFT System",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Vincenzo",
                "Aymeric Vitte"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 20877
        }
    },
    {
        "title": "[bitcoin-dev] Refreshed BIP324",
        "thread_messages": [
            {
                "author": "Pieter Wuille",
                "date": "2023-01-05T22:06:29",
                "message_text_only": "------- Original Message -------\nOn Friday, November 18th, 2022 at 3:24 AM, Anthony Towns <aj at erisian.com.au> wrote:\n\n> > * etc\n> > So this gives a uniform space which commands can be assigned from, and there is no strict need for thinking of the short-binary and long-alphabetic commands as distinct. In v2, some short ones would be treated as aliases for old long-alphabetic ones. But new commands could also just be introduced as short ones only (even in v1).\n> \n> Isn't that optimising for the wrong thing? Aren't the goals we want:\n> \n> 1) it should be easy to come up with a message identifier without\n> accidently conflicting with someone else's proposal\n> \n> 2) commonly used messages on the wire should have a short encoding\n> in order to save bandwidth\n> \n> Depending on how much the p2p protocol ossifies, which messages are\n> \"commonly used on the wire\" might be expected to change; and picking an\n> otherwise meaningless value from a set of 102 elements seems likely to\n> produce conflicts...\n\nOh, yes. I meant this as an encoding scheme, not as a (replacement for) the negotiation/coordination mechanism. There could still be an initial assignment for 1-byte encodings, and/or an explicit mechanism to negotiate other assignment, and/or nothing at all for now.\n\nI just thought it would be interesting to have a uniform encoding without explicit distinction between \"short commands\" and \"long commands\" at that layer.\n\nBut maybe none of this is worth it, as it's perhaps more complexity than the alternative, and the alternative already has a working implementation and written-up specification.\n\nCheers,\n\n-- \nPieter"
            },
            {
                "author": "Anthony Towns",
                "date": "2023-01-05T23:12:50",
                "message_text_only": "On Thu, Jan 05, 2023 at 10:06:29PM +0000, Pieter Wuille via bitcoin-dev wrote:\n> > > So this gives a uniform space which commands can be assigned from, and there is no strict need for thinking of the short-binary and long-alphabetic commands as distinct. In v2, some short ones would be treated as aliases for old long-alphabetic ones. But new commands could also just be introduced as short ones only (even in v1).\n> Oh, yes. I meant this as an encoding scheme, not as a (replacement for) the negotiation/coordination mechanism. There could still be an initial assignment for 1-byte encodings, and/or an explicit mechanism to negotiate other assignment, and/or nothing at all for now.\n> \n> I just thought it would be interesting to have a uniform encoding without explicit distinction between \"short commands\" and \"long commands\" at that layer.\n> But maybe none of this is worth it, as it's perhaps more complexity than the alternative, and the alternative already has a working implementation and written-up specification.\n\nHeh, I was just looking at this yesterday, but failing to quite reach\na conclusion.\n\nOne thing I hadn't realised about this was that it's not actually\na restriction compared to what we currently allow with p2p v1:\nCMessageHeader::IsCommandValid() already rejects commands that use\ncharacters outside of 0x20 to 0x7E, so the high bit is already available\nfor signalling when we reach the last byte.\n\nThe current implementation for 324 does the aliasing\nas part of V2TransportDeserializer::GetMessage and\nV2TransportSerializer::prepareForTransport. That makes a lot of sense,\nbut particularly if we were to negotiate short commands sometime around\nVERSION or VERACK, it might make more sense for the aliasing to move up\nto the protocol layer rather than have it close to the  wire layer. In\nthat case having a uniform encoding means we could just keep using\nCSerializedNetMsg whether we're sending a short command or a multibyte\nascii command -- without a uniform encoding, if we wanted to move short\ncommands up a layer, I think we'd need to change CSerializedNetMsg to\nhave m_type be a `std::variant<uint8_t,std::string>` instead of just a\nstring, or something similar.\n\nI think I'm leaning towards \"it doesn't matter either way\" though:\n\n * if we can negotiate short commands on a per-peer basis, then once\n   negotiation's finished we'll only be using short commands so saving a\n   byte on long commands doesn't matter much\n\n * if we've only got around 30 or 40 commands we understand anyway\n   (even counting one-time-only negotiation stuff), then it doesn't\n   matter if we can do 102, 126 or 242 short commands since those are\n   all more than we need\n\n * whether we'd have to tweak an internal struct if we want to change\n   the way our code is structured shouldn't really be much of an influence\n   on protocol design...\n\nCheers,\naj"
            },
            {
                "author": "Anthony Towns",
                "date": "2023-01-09T08:11:05",
                "message_text_only": "On Fri, Jan 06, 2023 at 09:12:50AM +1000, Anthony Towns via bitcoin-dev wrote:\n> On Thu, Jan 05, 2023 at 10:06:29PM +0000, Pieter Wuille via bitcoin-dev wrote:\n> > Oh, yes. I meant this as an encoding scheme, not as a (replacement for) the negotiation/coordination mechanism. There could still be an initial assignment for 1-byte encodings, and/or an explicit mechanism to negotiate other assignment, and/or nothing at all for now.\n\n> The current implementation for 324 does the aliasing\n> as part of V2TransportDeserializer::GetMessage and\n> V2TransportSerializer::prepareForTransport. That makes a lot of sense,\n> [...]\n\nSo I think you can make this setup work with a negotiated assignment of\nshortids, perhaps starting off something like:\n\nhttps://github.com/ajtowns/bitcoin/commit/6b8edd754bdcb582e293e4f5d0b41297711bdbb7\n\nThat has a 242 element array per peer giving the mappings (which\nis just ~250 bytes per peer) for deserialization, which seems\nworkable. [0]\n\nIt also has a single global map for serialization, so we'll always shorten\nCFILTER to shortid 39 for every peer that supports shortids, even, eg, for\na peer who's told us they'll send CFILTER as shortid 99 and that we should\ninterpret shortid 39 from them as NEWFEATUREX. That has three advantages:\n\n * each peer can choose a mapping that minimises their own outbound\n   traffic, even potentially for asymmetric connections, and don't need\n   to coordinate with the other peer to decide a common optimal mapping\n   that they both use across their connection\n\n * you don't have to have different serialization tables per-peer,\n   reducing memory usage / implementation complexity\n\n * you can leave V2TransportSerializer as a `const` object, and not have\n   to introduce additional locking logic to be able to update its\n   state...\n\nI'm not seeing a good way to introduce shortids for future one-shot\nnegotiation messages though (like VERSION, VERACK, SENDADDRV2,\nWTXIDRELAY, SENDTXRCNCL):\n\n * if you explicitly announce the mapping first, you're just wasting\n   bytes (\"99=FOOBAR; 99 baz quux\" vs just \"FOOBAR baz quux\")\n * if you negotiate the tables you support between VERSION/VERACK and\n   then choose a mutually supported table after VERACK, that's too late\n   for pre-VERACK negotation messages\n * announcing the tables you support as part of the VERSION message\n   would work, but seems a bit klunky\n\nAlso, if you did want to shift to a new table, you'd probably want to\nalways support sending/receiving {37, 44, 46, 47, 36} messages?\n\nI guess I still kind-of think it'd make more sense to just reserve\nshortids for post-VERACK messages that are going to be sent more\nthan once per connection... At that point, even if you don't have any\ntable in common with your peer, just following VERACK with an immediate\nannouncement of each shortid you want to use and its meaning would still\nmake reasonable sense.\n\nIf we included the ability to define your own shortids concurrently\nwith bip324 rollout, then I think nodes could always have a static set\nof shortids they use for all their peers for outbound messages, which,\nas above, seems like it would make for simpler implementations.\n\nie, you might send:\n\n   VERSION\n   SHORTIDTBLS [\"\",\"awesomeshortids\"]\n   WTXIDRELAY\n   SENDADDRV2\n   SENDPACKAGES 1\n   VERACK\n   SHORTID \"\" [(52,\"getpkgtxns\"), (53, \"pkgtxns\"), (54, \"ancpkginfo\")] \n\n...but you'd do all that long form, and only switch to shortids for\nmessages after you've declared exactly what your shortids are going to\nbe.\n\n(where \"\" is the table name for bip324's table, and \"awesomeshortids\"\nis an updated table that includes the package relay commands already,\nperhaps)\n\nCheers,\naj\n\n[0] m_deserializer is used from the SocketHandler thread in\n    CNode::ReceiveMsgBytes(), but the p2p protocol is managed from the\n    MessageHandler thread; with multiple messages potentially deserialized\n    into vRecvMsg() at once -- but that means that if the first message\n    redefines shortid decoding, and the second message uses one of the\n    redefined shortids, it will have already been decoded incorrectly.\n    So that would need some futzing about still."
            }
        ],
        "thread_summary": {
            "title": "Refreshed BIP324",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Anthony Towns",
                "Pieter Wuille"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 8628
        }
    },
    {
        "title": "[bitcoin-dev] Bitcoin Contracting Primitives WG 3rd Meeting, Tuesday 17 Jan. 18:00 UTC",
        "thread_messages": [
            {
                "author": "Antoine Riard",
                "date": "2023-01-06T00:26:03",
                "message_text_only": "Hi list,\n\nI'm proposing Tuesday 17th January at 18:00 UTC, i.e week from now for the\n3rd Bitcoin contracting primitives WG meeting (the third Tuesday of January\nmonth, as done previously).\n\nAs a soft proposal for an agenda, it would be to start with the leftover of\nthe last meeting agenda. Namely, roaming over all the contracting protocol\nand use-case, to ensure there is exhaustivity of the R&D effort w.r.t known\nideas issued by the community during the past years. If you have been\nworking on a use-case, and it's missing in the current listing, feel free\nto open a PR or bump me to do so (still same with primitives themselves\nofc).\n\nThe second part could be to take time to listen to everyone blockers in\ntheir contracting primitives/covenant research.\n\nAbout the R&D effort, one of my personal goal for the coming year would be\nto nurture some websites, with the archive material progressively gathered\nin the repository. The website would present the contracting protocol\nresearch according to the best engineering/scientific\nstandards and ensure ideas neutrality. Ideally, it would enable collection\nof feedback on dimensions like privacy or economic scaling from the Bitcoin\ncommunity of stakeholders at large, with Pretty Graphics (tm).\n\nBeyond, pursuing a \"decentralized\" spirit, looking forward to starting\nrotating meetings host during the year, as we're doing with BOLTs where\nit's rotating between Lightning implementations contributors. If you're\ninterested in hosting one of the monthly meetings, feel free to open an\nissue against the repository or bump me.\n\nCommunication venue is #bitcoin-contracting-primitives-wg on Libera Chat.\nLogs of the previous session are available here [0].\n\nLet it know if you have more questions or feedback.\n\nCheers,\nAntoine\n\n[0]\nhttps://github.com/ariard/bitcoin-contracting-primitives-wg/blob/main/meetings/meetings-20-12.md\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230105/f93e5e0c/attachment.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2023-01-17T01:50:05",
                "message_text_only": "Reminder: this is happening this _upcoming_ Tuesday.\n\nLooking forward to the third session to roam over all the contracting\nprotocol use-cases and then listen to everyone doing research in the\ncontracting primitives/covenant spaces, where they would like more brain\npower!\n\nBest,\nAntoine\n\nLe ven. 6 janv. 2023 \u00e0 00:26, Antoine Riard <antoine.riard at gmail.com> a\n\u00e9crit :\n\n> Hi list,\n>\n> I'm proposing Tuesday 17th January at 18:00 UTC, i.e week from now for the\n> 3rd Bitcoin contracting primitives WG meeting (the third Tuesday of January\n> month, as done previously).\n>\n> As a soft proposal for an agenda, it would be to start with the leftover\n> of the last meeting agenda. Namely, roaming over all the contracting\n> protocol and use-case, to ensure there is exhaustivity of the R&D effort\n> w.r.t known ideas issued by the community during the past years. If you\n> have been working on a use-case, and it's missing in the current listing,\n> feel free to open a PR or bump me to do so (still same with primitives\n> themselves ofc).\n>\n> The second part could be to take time to listen to everyone blockers in\n> their contracting primitives/covenant research.\n>\n> About the R&D effort, one of my personal goal for the coming year would be\n> to nurture some websites, with the archive material progressively gathered\n> in the repository. The website would present the contracting protocol\n> research according to the best engineering/scientific\n> standards and ensure ideas neutrality. Ideally, it would enable collection\n> of feedback on dimensions like privacy or economic scaling from the Bitcoin\n> community of stakeholders at large, with Pretty Graphics (tm).\n>\n> Beyond, pursuing a \"decentralized\" spirit, looking forward to starting\n> rotating meetings host during the year, as we're doing with BOLTs where\n> it's rotating between Lightning implementations contributors. If you're\n> interested in hosting one of the monthly meetings, feel free to open an\n> issue against the repository or bump me.\n>\n> Communication venue is #bitcoin-contracting-primitives-wg on Libera Chat.\n> Logs of the previous session are available here [0].\n>\n> Let it know if you have more questions or feedback.\n>\n> Cheers,\n> Antoine\n>\n> [0]\n> https://github.com/ariard/bitcoin-contracting-primitives-wg/blob/main/meetings/meetings-20-12.md\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230117/1feabd0b/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Bitcoin Contracting Primitives WG 3rd Meeting, Tuesday 17 Jan. 18:00 UTC",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Antoine Riard"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 4568
        }
    },
    {
        "title": "[bitcoin-dev] Roles and procedures around adding a bitcoin core maintainer",
        "thread_messages": [
            {
                "author": "alicexbt",
                "date": "2023-01-07T05:11:00",
                "message_text_only": "Hi Michael,\n\n> I don't think ranting and raving or throwing toys out the pram on the mailing list is the productive way to go though.\n\nIt was the best possible way I found to summarize everything, look for opinions to improve the process, feedback about PR #25871 open since 140 days and includes no raving.\n\n> I'll chat to some people offline and see what the confusion is and hopefully this can be resolved without unnecessary drama. \n\nI like all my Bitcoin and Bitcoin Core communication to be public for transparency and documentation purposes. Except reporting vulnerabilities although some bitcoin core developers even post vulns in public as GitHub issue when it involves other implementations.\n\n\n/dev/fd0\nfloppy disc guy\n\nSent with Proton Mail secure email.\n\n\n------- Original Message -------\nOn Wednesday, December 21st, 2022 at 12:14 AM, Michael Folkson michaelfolkson at protonmail.com wrote:\n\n\n\n> Hi alicexbt\n> \n> There does seem to be some confusion on this which I'm going to look into. I don't think ranting and raving or throwing toys out the pram on the mailing list is the productive way to go though. I'll chat to some people offline and see what the confusion is and hopefully this can be resolved without unnecessary drama. I'll respond in the new year. I don't know if you celebrate but if you do Happy Holidays.\n> \n> Thanks\n> Michael\n> \n> --\n> Michael Folkson\n> Email: michaelfolkson at protonmail.com\n> Keybase: michaelfolkson\n> PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n> \n> ------- Original Message -------\n> On Monday, December 19th, 2022 at 23:58, alicexbt via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:\n> \n> > Hi Bitcoin Developers,\n> > \n> > List of present bitcoin core maintainers:\n> > \n> > Username\n> > \n> > Focus Area\n> > \n> > MarcoFalke\n> > \n> > General, QA\n> > \n> > fanquake\n> > \n> > General, Build\n> > \n> > hebasto\n> > \n> > General, UI/UX\n> > \n> > achow101\n> > \n> > General, Wallet\n> > \n> > glozow\n> > \n> > General, Mempool\n> > \n> > Last 2 developers that stepped down as bitcoin core maintainer:\n> > \n> > Username\n> > \n> > -------------\n> > \n> > sipa\n> > \n> > laanwj\n> > \n> > Process followed in adding last maintainer:\n> > \n> > 1) fanquake nominated glowzow as rbf/mempool/validation maintainer.\n> > \n> > 2) It was discussed in an IRC meeting and most of the developers agreed to add her as new maintainer except mild NACK from Jeremy Rubin. Some contributors did not like different opinions being shared in the meeting.\n> > \n> > 3) A pull request was opened by glowzow to add keys. There were several ACKs, 2 NACKs and 1 meta concept NACK.\n> > \n> > My NACK: https://github.com/bitcoin/bitcoin/pull/25524#issuecomment-1172518409\n> > \n> > NACK by jamesob: https://github.com/bitcoin/bitcoin/pull/25524#issuecomment-1172570635\n> > \n> > Meta concept NACK by luke-jr: https://github.com/bitcoin/bitcoin/pull/25524#issuecomment-1175625779\n> > \n> > Eventually everyone agreed to add glowzow as maintainer and improve the process of adding maintainers. Pull request was merged by MarcoFalke.\n> > \n> > Initiatives to improve the process and documentation:\n> > \n> > 1) Jeremy opened a pull request and there were lot of disagreements with the documentation. It was closed since a related PR with less changes could be easy to agree upon.\n> > \n> > 2) Related pull request with minimal documentation was also closed by Jeremy with a comment that desire to improve docs seems to be missing based on reviews.\n> > \n> > 3) Jeremy opened an issue with title 'Call for Maintainer: P2P & Networking + Privacy' which was changed later and 'Privacy' was removed. He nominated jonatack and vasild was already self nominated so mentioned in the pull request. Nobody appreciated this effort to nominate self or others for a new maintainer. Later this was closed.\n> > \n> > 4) I had opened an issue with title Call for Maintainer: Privacy'. This even involved privacy of contributors and not just bitcoin core. It received some comments that made no sense and I eventually closed the issue.\n> > \n> > Process being followed for adding vasild as maintainer:\n> > \n> > 1) vasild volunteered to be a new maintainer on IRC\n> > \n> > 2) It was discussed in IRC meeting, some developers ACKed it and there were no issues.\n> > \n> > 3) A pull request was opened by vasild to add keys which is still open and its been 4 months. There were already some ACKs from the IRC meeting and pull request also received some ACKs (16 until now). fanquake, dergoegge and JeremyRubin had some disagreements. Jeremy had recently withdrawn all ACK/NACK from bitcoin core repository for some reasons, fanquake has not replied yet and dergoegge had some new disagreements although don't mind if the pull request is merged.\n> > \n> > 4) Earlier disagreements were related to scoping and it was changed by vasild\n> > \n> > 4) There was even a comment that disrespected vasild's contributions in bitcoin core and we had to literally share pull requests in which vasild has improved bitcoin core.\n> > \n> > 5) I tried adding the topic for a bitcoin core dev weekly meeting but did not achieve anything.\n> > \n> > Since Bitcoin Core is the reference implementation for Bitcoin and used by 90% nodes, what should be the ideal process or changes you would expect in roles, procedures etc.?\n> > \n> > - 'Call for maintainers' issue should be opened if contributors or maintainers need a new maintainer.\n> > \n> > - Discussion about nominated contributors in an IRC meeting where everyone is allowed to share their opinion.\n> > \n> > - One of the nominated contributor that gets most ACKs could open pull request to add keys. Everyone can ACK/NACK this PR with reasons.\n> > \n> > - Maintainers should be unbiased in merging these pull requests.\n> > \n> > - New maintainer should not be funded by the organization that already does it for most of the maintainers.\n> > \n> > - Long term contributors that are not living in a first world country should be encouraged.\n> > \n> > - Either we should agree every maintainer is a general maintainer that can merge pull request from different modules or define scope for present and new maintainers. We can't do both.\n> > \n> > - Self merging pull requests should be avoided.\n> > \n> > Let me know if you have any thoughts that could improve this process and involve less politics.\n> > \n> > /dev/fd0\n> > \n> > 'floppy disc guy'\n> > \n> > Sent with Proton Mail secure email.\n> > \n> > _______________________________________________\n> > \n> > bitcoin-dev mailing list\n> > \n> > bitcoin-dev at lists.linuxfoundation.org\n> > \n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "Roles and procedures around adding a bitcoin core maintainer",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "alicexbt"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 6617
        }
    },
    {
        "title": "[bitcoin-dev] OP_VAULT: a new vault proposal",
        "thread_messages": [
            {
                "author": "James O'Beirne",
                "date": "2023-01-09T16:07:54",
                "message_text_only": "For the last few years, I've been interested in vaults as a way to\nsubstantially derisk custodying Bitcoin, both at personal and commercial\nscales. Instead of abating with familiarity, as enthusiasm sometimes\ndoes, my conviction that vaults are an almost necessary part of bitcoin's\nviability has only grown over the years.\n\nSince people first started discussing vaults, it's been pretty clear that\nsome kind of covenant-enabling consensus functionality is necessary to\nprovide the feature set necessary to make vault use practical.\n\nEarlier last year I experimented with using OP_CTV[1], a limited covenant\nmechanism, to implement a \"minimum-viable\" vault design. I found that the\ninherent limitations of a precomputed covenant scheme left the resulting\nvault implementation wanting, even though it was an improvement over\nexisting strategies that rely on presigned transactions and (hopefully)\nephemeral keys.\n\nBut I also found proposed \"general\" covenant schemes to be\nunsuitable for this use. The bloated scriptPubKeys, both in size and\ncomplexity, that would result when implementing something like a vault\nweren't encouraging. Also importantly, the social-consensus quagmire\nregarding which covenant proposal to actually deploy feels at times\nintractable.\n\nAs a result, I wanted to explore a middle way: a design solely concerned\nwith making the best vault use possible, with covenant functionality as a\nsecondary consideration. In other words, a proposal that would deliver\nthe safety benefits of vaults to users without getting hung up on\ntrying to solve the general problem of covenants.\n\nAt first this design, OP_VAULT, was just sort of a pipe dream. But as I\ndid more thinking (and eventually implementing) I became more convinced\nthat, even if it isn't considered for soft-fork, it is a worthwhile\ndevice to serve as a standard benchmark against which other proposals\nmight be judged.\n\nI wrote a paper that summarizes my findings and the resulting proposal:\nhttps://jameso.be/vaults.pdf\n\nalong with an accompanying draft implementation:\nhttps://github.com/bitcoin/bitcoin/pull/26857\n\nI might work on a BIP if there's interest.\n\nJames\n\n[1]: https://github.com/jamesob/simple-ctv-vault\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230109/21bf137d/attachment.html>"
            },
            {
                "author": "rot13maxi",
                "date": "2023-01-09T19:02:26",
                "message_text_only": "Hey James,\n\nReally cool proposal. I\u2019ve been thinking a lot lately about script paths for inheritance. In a lot of the \u201chave a relative time lock that allows a different key to spend coins, or allows a smaller threshold of a multisig to spend\u201d schemes, you have the problem of needing to \u201crefresh\u201d all of your coins when the timelock is close to maturation. In a lot of the \u201cuse multisig with ephemeral keys to emulate covenants\u201d schemes, you have to pre-commit to the terminal destination well in advance of the spend-path being used, which leads to all kinds of thorny questions about security and availability of *those* keys. In other words, you either have to have unbound destinations but a timer that needs resetting, or you have unbound time but fixed destinations. This design gets you the best of both because the destination SPKs aren\u2019t committed to until the unvaulting process starts. This (or something like this with destination binding at unvault-time) would be an incredibly useful tool for inheritance designs in wallets.\n\nI need to think a bit more about the recovery path not having any real encumbrances on it. Maybe in practice if you\u2019re worried about DoS, you have UTXOs that commit to multiple vault paths that have tweaked recovery destinations or something, or maybe it really is the right move to say that if recovery is triggered, you probably do want it for all of your inflight unvaultings.\n\nLooking forward to reading this a few more times and talking more about it.\n\nThanks!\nrijndael\n\nOn Mon, Jan 9, 2023 at 11:07 AM, James O'Beirne via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> For the last few years, I've been interested in vaults as a way to\n> substantially derisk custodying Bitcoin, both at personal and commercial\n> scales. Instead of abating with familiarity, as enthusiasm sometimes\n> does, my conviction that vaults are an almost necessary part of bitcoin's\n> viability has only grown over the years.\n>\n> Since people first started discussing vaults, it's been pretty clear that\n> some kind of covenant-enabling consensus functionality is necessary to\n> provide the feature set necessary to make vault use practical.\n>\n> Earlier last year I experimented with using OP_CTV[1], a limited covenant\n> mechanism, to implement a \"minimum-viable\" vault design. I found that the\n> inherent limitations of a precomputed covenant scheme left the resulting\n> vault implementation wanting, even though it was an improvement over\n> existing strategies that rely on presigned transactions and (hopefully)\n> ephemeral keys.\n>\n> But I also found proposed \"general\" covenant schemes to be\n> unsuitable for this use. The bloated scriptPubKeys, both in size and\n> complexity, that would result when implementing something like a vault\n> weren't encouraging. Also importantly, the social-consensus quagmire\n> regarding which covenant proposal to actually deploy feels at times\n> intractable.\n>\n> As a result, I wanted to explore a middle way: a design solely concerned\n> with making the best vault use possible, with covenant functionality as a\n> secondary consideration. In other words, a proposal that would deliver\n> the safety benefits of vaults to users without getting hung up on\n> trying to solve the general problem of covenants.\n>\n> At first this design, OP_VAULT, was just sort of a pipe dream. But as I\n> did more thinking (and eventually implementing) I became more convinced\n> that, even if it isn't considered for soft-fork, it is a worthwhile\n> device to serve as a standard benchmark against which other proposals\n> might be judged.\n>\n> I wrote a paper that summarizes my findings and the resulting proposal:\n> https://jameso.be/vaults.pdf\n>\n> along with an accompanying draft implementation:\n> https://github.com/bitcoin/bitcoin/pull/26857\n>\n> I might work on a BIP if there's interest.\n>\n> James\n> [1]: https://github.com/jamesob/simple-ctv-vault\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230109/80a1b3e0/attachment.html>"
            },
            {
                "author": "Greg Sanders",
                "date": "2023-01-09T19:31:51",
                "message_text_only": "Hi James and co,\n\nCurrently there is no way to make this compatible with scripthashes of any\nkind, since the script interpreter has no insight into the OP_UNVAULT\noutputs' \"execution script\", and one of the arguments of OP_UNVAULT is\nfreeform, resulting in an unpredictable output scriptpubkey.\n\nI think the fix is just requiring a single additional witness data item\nduring OP_VAULT spend(for unvault path), mandating the\n<target-outputs-hash> to be included in the witness stack as an input to\nOP_VAULT opcode, and transaction introspection then checks to make sure the\nwitness item and the corresponding output script template matches the\nexpected.\n\nThis would only be necessary for the unvaulting path, and not for the\nrecovery path.\n\nCheers,\nGreg\n\nOn Mon, Jan 9, 2023 at 2:10 PM rot13maxi via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hey James,\n>\n> Really cool proposal. I\u2019ve been thinking a lot lately about script paths\n> for inheritance. In a lot of the \u201chave a relative time lock that allows a\n> different key to spend coins, or allows a smaller threshold of a multisig\n> to spend\u201d schemes, you have the problem of needing to \u201crefresh\u201d all of your\n> coins when the timelock is close to maturation. In a lot of the \u201cuse\n> multisig with ephemeral keys to emulate covenants\u201d schemes, you have to\n> pre-commit to the terminal destination well in advance of the spend-path\n> being used, which leads to all kinds of thorny questions about security and\n> availability of *those* keys. In other words, you either have to have\n> unbound destinations but a timer that needs resetting, or you have unbound\n> time but fixed destinations. This design gets you the best of both because\n> the destination SPKs aren\u2019t committed to until the unvaulting process\n> starts. This (or something like this with destination binding at\n> unvault-time) would be an incredibly useful tool for inheritance designs in\n> wallets.\n>\n> I need to think a bit more about the recovery path not having any real\n> encumbrances on it. Maybe in practice if you\u2019re worried about DoS, you have\n> UTXOs that commit to multiple vault paths that have tweaked recovery\n> destinations or something, or maybe it really is the right move to say that\n> if recovery is triggered, you probably do want it for all of your inflight\n> unvaultings.\n>\n> Looking forward to reading this a few more times and talking more about\n> it.\n>\n> Thanks!\n> rijndael\n>\n>\n> On Mon, Jan 9, 2023 at 11:07 AM, James O'Beirne via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> For the last few years, I've been interested in vaults as a way to\n> substantially derisk custodying Bitcoin, both at personal and commercial\n> scales. Instead of abating with familiarity, as enthusiasm sometimes\n> does, my conviction that vaults are an almost necessary part of bitcoin's\n> viability has only grown over the years.\n>\n> Since people first started discussing vaults, it's been pretty clear that\n> some kind of covenant-enabling consensus functionality is necessary to\n> provide the feature set necessary to make vault use practical.\n>\n> Earlier last year I experimented with using OP_CTV[1], a limited covenant\n> mechanism, to implement a \"minimum-viable\" vault design. I found that the\n> inherent limitations of a precomputed covenant scheme left the resulting\n> vault implementation wanting, even though it was an improvement over\n> existing strategies that rely on presigned transactions and (hopefully)\n> ephemeral keys.\n>\n> But I also found proposed \"general\" covenant schemes to be\n> unsuitable for this use. The bloated scriptPubKeys, both in size and\n> complexity, that would result when implementing something like a vault\n> weren't encouraging. Also importantly, the social-consensus quagmire\n> regarding which covenant proposal to actually deploy feels at times\n> intractable.\n>\n> As a result, I wanted to explore a middle way: a design solely concerned\n> with making the best vault use possible, with covenant functionality as a\n> secondary consideration. In other words, a proposal that would deliver\n> the safety benefits of vaults to users without getting hung up on\n> trying to solve the general problem of covenants.\n>\n> At first this design, OP_VAULT, was just sort of a pipe dream. But as I\n> did more thinking (and eventually implementing) I became more convinced\n> that, even if it isn't considered for soft-fork, it is a worthwhile\n> device to serve as a standard benchmark against which other proposals\n> might be judged.\n>\n> I wrote a paper that summarizes my findings and the resulting proposal:\n> https://jameso.be/vaults.pdf\n>\n> along with an accompanying draft implementation:\n> https://github.com/bitcoin/bitcoin/pull/26857\n>\n> I might work on a BIP if there's interest.\n>\n> James\n>\n> [1]: https://github.com/jamesob/simple-ctv-vault\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230109/7ca41bc5/attachment-0001.html>"
            },
            {
                "author": "James O'Beirne",
                "date": "2023-01-09T20:32:34",
                "message_text_only": "Hey Greg,\n\nI think what you're trying to get at here is that the OP_UNVAULT\nscriptPubKey *must* be a bare script so that the OP_VAULT spend logic can\nverify that we're spending an OP_VAULT output into a compatible OP_UNVAULT\noutput, and that's true. The OP_UNVAULT scriptPubKey also must contain the\ntarget hash because that has is used when validating that spend to ensure\nthat the final unvault target matches what was advertised when the\nOP_UNVAULT output was created.\n\nSo I'm not sure what problem you're trying to solve by putting the target\nhash  on the OP_VAULT spend witness stack. If it were placed there, it\nwouldn't be accessible during OP_UNVAULT spend AFAICT. I agree it would be\nnice to figure out a way to allow the OP_UNVAULT scriptPubKey to not be\nbare, which may require moving the target hash out of it, but we'd have to\nfigure out a mechanism to properly forward the target hash for validation.\n\nBest,\nJames\n\nOn Mon, Jan 9, 2023 at 2:32 PM Greg Sanders <gsanders87 at gmail.com> wrote:\n\n> Hi James and co,\n>\n> Currently there is no way to make this compatible with scripthashes of any\n> kind, since the script interpreter has no insight into the OP_UNVAULT\n> outputs' \"execution script\", and one of the arguments of OP_UNVAULT is\n> freeform, resulting in an unpredictable output scriptpubkey.\n>\n> I think the fix is just requiring a single additional witness data item\n> during OP_VAULT spend(for unvault path), mandating the\n> <target-outputs-hash> to be included in the witness stack as an input to\n> OP_VAULT opcode, and transaction introspection then checks to make sure the\n> witness item and the corresponding output script template matches the\n> expected.\n>\n> This would only be necessary for the unvaulting path, and not for the\n> recovery path.\n>\n> Cheers,\n> Greg\n>\n> On Mon, Jan 9, 2023 at 2:10 PM rot13maxi via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Hey James,\n>>\n>> Really cool proposal. I\u2019ve been thinking a lot lately about script paths\n>> for inheritance. In a lot of the \u201chave a relative time lock that allows a\n>> different key to spend coins, or allows a smaller threshold of a multisig\n>> to spend\u201d schemes, you have the problem of needing to \u201crefresh\u201d all of your\n>> coins when the timelock is close to maturation. In a lot of the \u201cuse\n>> multisig with ephemeral keys to emulate covenants\u201d schemes, you have to\n>> pre-commit to the terminal destination well in advance of the spend-path\n>> being used, which leads to all kinds of thorny questions about security and\n>> availability of *those* keys. In other words, you either have to have\n>> unbound destinations but a timer that needs resetting, or you have unbound\n>> time but fixed destinations. This design gets you the best of both because\n>> the destination SPKs aren\u2019t committed to until the unvaulting process\n>> starts. This (or something like this with destination binding at\n>> unvault-time) would be an incredibly useful tool for inheritance designs in\n>> wallets.\n>>\n>> I need to think a bit more about the recovery path not having any real\n>> encumbrances on it. Maybe in practice if you\u2019re worried about DoS, you have\n>> UTXOs that commit to multiple vault paths that have tweaked recovery\n>> destinations or something, or maybe it really is the right move to say that\n>> if recovery is triggered, you probably do want it for all of your inflight\n>> unvaultings.\n>>\n>> Looking forward to reading this a few more times and talking more about\n>> it.\n>>\n>> Thanks!\n>> rijndael\n>>\n>>\n>> On Mon, Jan 9, 2023 at 11:07 AM, James O'Beirne via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>> For the last few years, I've been interested in vaults as a way to\n>> substantially derisk custodying Bitcoin, both at personal and commercial\n>> scales. Instead of abating with familiarity, as enthusiasm sometimes\n>> does, my conviction that vaults are an almost necessary part of bitcoin's\n>> viability has only grown over the years.\n>>\n>> Since people first started discussing vaults, it's been pretty clear that\n>> some kind of covenant-enabling consensus functionality is necessary to\n>> provide the feature set necessary to make vault use practical.\n>>\n>> Earlier last year I experimented with using OP_CTV[1], a limited covenant\n>> mechanism, to implement a \"minimum-viable\" vault design. I found that the\n>> inherent limitations of a precomputed covenant scheme left the resulting\n>> vault implementation wanting, even though it was an improvement over\n>> existing strategies that rely on presigned transactions and (hopefully)\n>> ephemeral keys.\n>>\n>> But I also found proposed \"general\" covenant schemes to be\n>> unsuitable for this use. The bloated scriptPubKeys, both in size and\n>> complexity, that would result when implementing something like a vault\n>> weren't encouraging. Also importantly, the social-consensus quagmire\n>> regarding which covenant proposal to actually deploy feels at times\n>> intractable.\n>>\n>> As a result, I wanted to explore a middle way: a design solely concerned\n>> with making the best vault use possible, with covenant functionality as a\n>> secondary consideration. In other words, a proposal that would deliver\n>> the safety benefits of vaults to users without getting hung up on\n>> trying to solve the general problem of covenants.\n>>\n>> At first this design, OP_VAULT, was just sort of a pipe dream. But as I\n>> did more thinking (and eventually implementing) I became more convinced\n>> that, even if it isn't considered for soft-fork, it is a worthwhile\n>> device to serve as a standard benchmark against which other proposals\n>> might be judged.\n>>\n>> I wrote a paper that summarizes my findings and the resulting proposal:\n>> https://jameso.be/vaults.pdf\n>>\n>> along with an accompanying draft implementation:\n>> https://github.com/bitcoin/bitcoin/pull/26857\n>>\n>> I might work on a BIP if there's interest.\n>>\n>> James\n>>\n>> [1]: https://github.com/jamesob/simple-ctv-vault\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230109/b0fc1b4c/attachment.html>"
            },
            {
                "author": "James O'Beirne",
                "date": "2023-01-10T13:35:04",
                "message_text_only": "Greg explained his suggestion to me off-list, and I think it's a good one.\nTo summarize, consider the normal \"output flow\" of an expected vault use:\n\n(i) output to be vaulted\n  => (ii) OP_VAULT output\n    => (iii) OP_UNVAULT \"trigger\" output\n      => (iv) final output\n\nIn my existing draft implementation, all outputs aside from (iii), the\nOP_UNVAULT trigger, can be P2TR or P2WSH. In other words, those outputs can\nhide their true script until spend. In my draft, the OP_UNVAULT trigger had\nto be bare so that the script interpreter could inspect part of it for\nvalidity: \"does this OP_UNVAULT have the same <recovery-spk-hash> and\n<spend-delay> as the OP_VAULT?\"\n\nIf that output wasn't bare, because the <target-hash> is variable at the\ntime of OP_UNVAULT output creation, the script interpreter would have no\nway of constructing the expected scriptPubKey.\n\nGreg's suggestion would allow that output to be any kind of script. He\nsuggests to put the <target-hash> onto the witness stack when spending the\nOP_VAULT output (and creating the OP_UNVAULT output). If we did that, the\nscript interpreter could e.g. use a NUMS point (i.e. a publicly known point\nwith no usable private key) to construct a Taproot configuration that looks\nlike\n\n  tr(NUMS, {<OP_UNVAULT <recovery-key> <spend-delay> <target-hash>})\n\nand check if the scriptPubKey of the proposed OP_UNVAULT output matches\nthat. This would allow all outputs in vault lifecycles to be P2TR, for\nexample, which would conceal the operation of the vault - a very nice\nfeature!\n\nThis would also allow the OP_VAULT/OP_UNVAULT opcodes to be implemented as\nTaproot-only OP_SUCCESSx opcodes, if that was decided to be preferable.\n\nThe problem is how to (and whether to) enable something similar for witness\nv0 outputs. For example, if we want the (ii) and (iii) output scripts to\nlive behind P2WSH. One (kind of hacky) option to enable this is to have the\nscript interpreter construct the expected OP_UNVAULT scriptPubKey on the\nbasis of what witness version it sees. For example, if it sees \"OP_0 <32\nbytes data>\", it would use <target-hash> on the witness stack to construct\na fitting P2WSH scriptPubKey that is compatible with the OP_VAULT being\nspent, and then match against that. But if it detects \"OP_1 <32 bytes\ndata>\", it would do the same process for an expected Taproot-with-NUMS\noutput.\n\n---\n\nAnyway, sorry if that was more verbose than necessary, but I think it's a\nreally great suggestion from Greg. I'll look at modifying the\nimplementation accordingly. I'd be curious to hear what others think as\nwell.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230110/c1b0d445/attachment.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2023-01-10T12:29:52",
                "message_text_only": "On Mon, Jan 09, 2023 at 11:07:54AM -0500, James O'Beirne via bitcoin-dev wrote:\n> But I also found proposed \"general\" covenant schemes to be\n> unsuitable for this use. The bloated scriptPubKeys,\n\nI don't think that makes sense? With a general scheme, you'd only be\nbloating the witness data (perhaps including the witness script) not\nthe scriptPubKey?\n\n\n\nTerminology suggestion: instead of calling it the \"recovery\" path,\ncall it \"freezing your funds\". Then you're using your \"hot wallet\" (aka\nunvault-spk-hash) for the unvault path for all your normal transactions,\nbut if there's a problem, you freeze your funds, and they're now only\naccessible via your \"cold wallet\" (aka recovery-spk-hash).\n\n\n\nAs I understand it, your scheme is:\n\n  scriptPubKey: <vault tag> <#recovery> (<delay> <#unvault>)\n\nwhere #recovery is the sha256 hashes of an arbitrary scriptPubKey,\n#unvault is the sha256 hash of a witness script, and delay is a relative\nblock count.\n\nThis scriptPubKey allows for two spend paths:\n\n  recovery: spends directly to <recovery>; verified by checking it\n    the hash of the sPK matches <#recovery> and the amount it preserved\n\n  unvaulting:\n    spends to a scriptPubKey of <unvault tag> <#recovery> (<delay> <#target>)\n    verified by checking that the witness script hashes to #unvault and\n    is satisfied, and #target is a CTV-ish commitment of the eventual\n    withdrawal (any CHECKSIG operations in the unvault witness script will\n    commit to the sPK output, preventing this from being modified by\n    third parties). #recovery and delay must match the values from the\n    vault scriptPubKey.\n\nThe unvault scriptPubKey likewise likewise allows for two spend paths:\n\n  recovery: same as above\n\n  withdrawal:\n    verifies that all the outputs hash to #target, and that nSequence\n    is set to a relative timelock of at least delay.\n\n\n\nThis means that as soon as your recovery address (the preimage to\n#recovery) is revealed, anyone can move all your funds into cold storage\n(presuming they're willing to pay the going feerate to do so). I think\nthis is a feature, not a bug, though: if your hot wallet is compromised,\nmoving all your funds to cold storage is desirable; and if you want to\nhave different hot wallets with a single cold wallet, then you can use\na HD cold-wallet so that revealing one address corresponding to one hot\nwallet, doesn't reveal the addresses corresponding to other hot wallets.\n(This is addressed in the \"Denial-of-service protection\" section)\n\nIt does however mean that the public key for your cold wallet needs to\nbe handled secretly though -- if you take the cold wallet xpub and send\nit to a random electrum server to check your cold wallet balance, that\nwould allow a malicious party to lock up all your funds.\n\nI think it might be better to use a pay-to-contract construction for\nthe recovery path, rather than an empty witness. That is, take your\nrecovery address R, and calculate #recovery=sha256(R, sha256(secret))\n(where \"secret\" is something derived from the R's private key, so that\nit can be easily recovered if you only have your cold wallet and lose\nall your metadata). When you want to recover all your funds to address R,\nyou reveal sha256(secret) in the witness data and R in the scriptPubKey,\nOP_VAULT hashes these together and checks the result matches #recovery,\nand only then allows it. That would allow you to treat R as public\nknowledge, without risking your funds getting randomly frozen.\n\n\n\nThis construct allows delayed withdrawals (ie \"the cold wallet can\nwithdraw instantly, the hot wallet can withdraw only after delay blocks\"),\nbut I don't think it provides any way to cap withdrawals (\"the cold\nwallet can withdraw all funds, the hot wallet can only withdraw up to\nX funds per day/week\").  Having a fixed limit probably isn't compatible\nwith having a multi-utxo vault (\"you can withdraw $10k per day\" doesn't\nhelp if your $5M is split across 500 $10k utxos, and the limit is only\nenforced per-utxo), but I think a percentage limit would be.\n\n\n\nI think a generic OP_UNVAULT can be used to simulate OP_CTV: replace\n\"<h> OP_CTV\" with \"<000..0> 0 <h> OP_UNVAULT\". The paper seems to put\n\"OP_UNVAULT\" first, but the code seems to expect it to come last, not\nsure what's up with that inconsistency.\n\nI'm not sure why you'd want a generic opcode though; if you want the\ndata to be visible in the scriptPubKey, you need to use a new segwit\nversion with structured data, anyway; so why not just do that?\n\n\n\nI think there's maybe a cleverer way of batching / generalising checking\nthat input/output amounts match. That is, rather than just checking that\n\"the input's a vault; so the corresponding output must be one of these\npossibilities, and the input/output values must exactly match\", that\nit's generalised to be:\n\n  * set A = the sum of each input that's taking the unvaulting path \n    from a vault scriptPubKey with #recovery=X\n  * set B = the sum of each output that has an unvault tag with\n    #recovery=X\n  * set C = the sum of each output that has a vault tag with\n    #recovery=X\n  * check that A=B+C\n\n(That allows consolidation of your vault via your hot wallet, just by not\nhaving any unvault outputs, so B=0. I suspect that if you allowed for\nkeyless consolidation of your vault, that that would be a griefing/DoS\nvector)\n\nThis differs from the actual proposal, AIUI, which instead requires that\nthere are just two outputs - an ephemeral anchor for attaching fees, and\nthe primary vault or unvault output, and that all the inputs are\nvaulting/unvaulting txs.\n\nI think one meaningful difference between these two approaches is that\nthe current proposal means unvaulting locks up the entire utxo for the\ndelay period, rather than just the amount you're trying to unvault. eg,\nif you have a single vault utxo X with 1000 BTC, you have delay set to\n1008 blocks (1 week), and you decide on Tuesday that you wish to withdraw\n50 BTC, creating an unvault tx spending 50 BTC somewhere and 950 BTC back\nto your vault, you can't spend any of the 950 BTC for another two weeks:\none week for the unvault to confirm and it to go back into your vault,\nand another week for the next unvault to confirm.\n\nChanging the unvault construction to have an optional OP_VAULT output\nwould remedy that, I think.\n\n\n\nIt would be fairly dangerous to combine a construction like this (which\nencourages the vault sPK to be reused) with APO signatures on the hot\nwallet -- in that case the signature could just be replayed against a\ndifferent vault utxo, and you'd be paying for things twice. But provided\nthat vault spends are only (1) signed by the hot wallet, or (2) being\nfrozen and moved to the recovery sPK, then you should have complete\ncontrol over your utxos/coins, and using APO probably isn't interesting\nanyway.\n\n\n\nWhat would it look like to just hide all this under taproot?\n\nFirst you'd just leave the internal pubkey as your cold wallet key\n(or a NUMS point if your cold wallet is complicated).\n\nWorking backwards, your unvault output needs two script paths:\n\n  1) move_funds_to(recovery-spk)\n  2) <D> OP_CSV; move_funds_to(X)\n\nYour vault output also needs two paths:\n\n  1) move_funds_to(recovery-spk)\n  2) hot-wallet-script; move_funds_to(unvault[X])\n\nThat obviously requires a \"move_funds_to\" operator, which, using\nliquid's operators (roughly), could be something like:\n\n  PUSHCURRENTINPUTINDEX\n  DUP2 INSPECTINPUTVALUE SWAP INSPECTOUTVALUE EQUALVERIFY\n  INSPECTOUTPUTSCRIPTPUBKEY \"x\" EQUAL\n\nwhich is just ~8 bytes overhead, or could perhaps be something fancier\nthat supports the batching/consolidation abilities discussed above.\n\nIt also needs some way of constructing \"unvault[X]\", which could be a\nTLUV-like construction.\n\nThat all seems possible to me; though certainly needs more work/thought\nthan just having dedicated opcodes and stuffing the data directly in\nthe sPK.\n\nCheers,\naj"
            },
            {
                "author": "James O'Beirne",
                "date": "2023-01-10T20:22:54",
                "message_text_only": "Thanks for the thoughtful reply AJ.\n\n\n> I don't think that makes sense? With a general scheme, you'd only be\n> bloating the witness data (perhaps including the witness script) not\n> the scriptPubKey?\n\nSorry, sloppy language on my part. To be charitable, I'm talking about\nthe \"figurative sPK,\" which of course these days lives in the witness\nfor script-path-ish spends. Maybe the witness discount means that\n\"complicated\" scripts aren't as big a deal, depending on the actual\ndifference in raw script size.\n\n\n> I think it might be better to use a pay-to-contract construction for\n> the recovery path, rather than an empty witness.\n\nSo I guess the one advantage that what you're proposing has over just\nusing a recovery-path key signature is that it's all derivable from your\ncold privkey; you don't have to worry about accidentally losing the\nrecovery-path key.\n\nOf course you're still vulnerable to spurious sweeps if the\nsha256(secret) value gets found out, which presumably you'd want in an\naccessible cache to avoid touching the cold secret every time you want\nto sweep.\n\nWhat do you think about the idea of making the recovery-path\nauthorization behavior variable on a single byte flag preceding the 32\nbyte data push, as I mentioned in another post? I think it may make\nsense to leave this option open to end-users (and also allow for some\nupgradeability).\n\n\n> I think a generic OP_UNVAULT can be used to simulate OP_CTV: replace\n> \"<h> OP_CTV\" with \"<000..0> 0 <h> OP_UNVAULT\".\n\nYup, that's an inefficient way of emulating CTV. If people want CTV, we\nshould just look at activating CTV. Greg Sanders has a thing about\n\"jetting\" CTV into this proposal (I think) so that the code-paths are\nshared, but I haven't figured out how that would work. They really\ndon't share that much code AFAICT.\n\n\n> The paper seems to put \"OP_UNVAULT\" first, but the code seems to\n> expect it to come last, not sure what's up with that inconsistency.\n\nAgain some sloppy notation on my part. What I sort of meant in the paper\nwas a kind of functional notation `OP_VAULT(param1, param2, ...)`. Let's\nchalk that up to my inexperience actually working on script stuff.\n\n\n> I think there's maybe a cleverer way of batching / generalising\n> checking that input/output amounts match.\n>\n> [...]\n>\n>  * set C = the sum of each output that has a vault tag with\n>    #recovery=X\n\nThis would also need to take into account that the <spend-delay>s are\ncompatible, but your point is well taken.\n\n\n> I think one meaningful difference between these two approaches is that\n> the current proposal means unvaulting locks up the entire utxo for the\n> delay period, rather than just the amount you're trying to unvault.\n\nThis is a really good point and I think is one that's important to\nincorporate with a change to the existing proposal.\n\nA simple fix for facilitating the use of a \"partial revault\" while the\nOP_UNVAULT UTXO is outstanding would be to allow for an optional\nthird output that is a redeposit back to the identical OP_VAULT sPK that\nis being spent by the OP_UNVAULT transaction, then the script\ninterpreter would just ensure that the nValue of those two outputs sums\nto the sum of the input nValues.\n\nI can see what you're saying about having more generic \"group amounts by\ncompatible vault params, and then compare to similarly grouped outputs,\"\nbut I'm just wondering if there are other uses that enables besides the\npartial-revault thing I mentioned above. If not, I'd probably rather just\nstick\nwith something simple like having the third optional re-vault output.\n\n\n> Changing the unvault construction to have an optional OP_VAULT output\n> would remedy that, I think.\n\nOh - okay, this is what you're saying. Right!\n\nIs that a sufficient change, or are there other benefits that the more\ncomplicated clever-I/O-vault-grouping would enable that you have in\nmind?\n\n\n> What would it look like to just hide all this under taproot?\n>\n> [...]\n>\n> It also needs some way of constructing \"unvault[X]\", which could be a\n> TLUV-like construction.\n>\n> That all seems possible to me; though certainly needs more work/thought\n> than just having dedicated opcodes and stuffing the data directly in\n> the sPK.\n\nI think this is a very important comparison to do, but I'm eager to see\ncode for things like this. There have been a lot of handwavey proposals\nlately without tangible code artifacts. I'm eager to see what these\nalternatives look like in practice - i.e. in functional tests.\n\n\nThanks again for the great mail.\nJames\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230110/a5579378/attachment.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2023-01-11T06:52:28",
                "message_text_only": "On Tue, Jan 10, 2023 at 03:22:54PM -0500, James O'Beirne wrote:\n> > I don't think that makes sense? With a general scheme, you'd only be\n> > bloating the witness data (perhaps including the witness script) not\n> > the scriptPubKey?\n> Sorry, sloppy language on my part. To be charitable, I'm talking about\n> the \"figurative sPK,\" which of course these days lives in the witness\n> for script-path-ish spends. Maybe the witness discount means that\n> \"complicated\" scripts aren't as big a deal, depending on the actual\n> difference in raw script size.\n\nSure. I think there's three aspects that matter for the witness script:\n\n 1) it shouldn't be long/costly to do things that are common and easy:\n    if you can express \"OP_VAULT\" in ~70 bytes, you shouldn't have to\n    spend 1000 bytes to do so; if it can be cheap to validate, you\n    shouldn't have to pay 100x markup in fees to use it. With the\n    exception of things that build up from basics (like\n    CAT/CHECKSIGFROMSTACK approaches), I think this is mostly fine\n    though.\n\n 2) once someone figures out a design, it should be easy to reuse;\n    but I think that's not a big deal: you just write up a spec for\n    your script, and people use that in their different wallet software,\n    much like the specialised scripts for lightning HTLCs\n\n 3) primitives should be designed to be easy to safely build on and\n    scripts should be as easy as possible to analyse once written;\n    ie, we want things more like miniscript than \"The story of Mel,\n    a Real Programmer\"\n\nWith some caveats (like that using the cold wallet xpub to scan the\nblockchain before you've frozen all your funds is dangerous), OP_VAULT\nseems really good on all those fronts, of course.\n\n> > I think it might be better to use a pay-to-contract construction for\n> > the recovery path, rather than an empty witness.\n> So I guess the one advantage that what you're proposing has over just\n> using a recovery-path key signature is that it's all derivable from your\n> cold privkey; you don't have to worry about accidentally losing the\n> recovery-path key.\n> Of course you're still vulnerable to spurious sweeps if the\n> sha256(secret) value gets found out, which presumably you'd want in an\n> accessible cache to avoid touching the cold secret every time you want\n> to sweep.\n\nSure, \"sha256(secret)\" itself needs to be semi-secret -- it allows anyone\nwho knows it to freeze your funds, even if it doesn't allow anyone to\nsteal them. You could presumably do all the usual things to protect that\nsecret: split it up with secret sharing; put it in a hardware wallet;\nkeep it offline; etc.\n\n> What do you think about the idea of making the recovery-path\n> authorization behavior variable on a single byte flag preceding the 32\n> byte data push, as I mentioned in another post?\n\n] \"if\n] <recovery-path-hash> is 32 bytes, treat it as it's currently used. If it's\n] 33 bytes, use the first byte as a parameter for how to interpret it.\" To\n] start with, an extra prefix byte of 0x00 could mean \"require a witness\n] satisfying the scriptPubKey that hashes to the remaining 32 bytes\" in the\n] same way we do the unvault signing.\n\nI don't think 33 bytes would be enough? There isn't really a way to\ncommit to the recovery destination within the script? So I think you'd\nneed \"<32 byte recovery-path-hash><n byte scriptPubKey>\"\n\nAside from that, my opinion's one/all of:\n\na) sounds fine\n\nb) maybe you could just always have it include a scriptPubKey? for the\ntimes when you just want \"reveal the cold wallet preimage\" just have\nthe scriptPubKey be the single byte \"OP_TRUE\"; for the times when you\nit to be \"reveal random preimage\" you'd have it be the 22 byte \"HASH160\n<hash160(sha256(secret))> EQUAL\"?\n\nc) delegation to a script is a great idea, that's come up multiple times\n(OP_EVAL, BIP117, graftroot) -- it's probably better to have it\navailable as a generic feature, than bolted on to particular features\n\n> > I think a generic OP_UNVAULT can be used to simulate OP_CTV: replace\n> > \"<h> OP_CTV\" with \"<000..0> 0 <h> OP_UNVAULT\".\n> Yup, that's an inefficient way of emulating CTV.\n\nSure; I think it's only interesting in establishing how powerful the\nconstruct is in the abstract. It's not an exact match for CTV since it\nhashes some things differently.\n\nI don't really think it's necessarily that inefficient fwiw; \"0 SHA256\n0 <h> UNVAULT\" is only 3 more bytes than \"<h> CTV\", could give you an\nunspendable recovery path, provided UNVAULT wants either a BIP341 tagged\nhash (which is what the implementation does, by the looks), or a HASH256\nfor the recovery path.\n\n(Again, this assumes UNVAULT is available in script, and isn't just a\nspecial scriptPubKey type)\n\n> > I think there's maybe a cleverer way of batching / generalising\n> > checking that input/output amounts match.\n> > [...]\n> >  * set C = the sum of each output that has a vault tag with\n> >    #recovery=X\n> This would also need to take into account that the <spend-delay>s are\n> compatible, but your point is well taken.\n\nSure, I guess output/UNVAULT delay >= input/VAULT delay would be\nsufficient for that.\n\nI guess having all that stuff exposed in the scriptPubKey would be\nslightly annoying for griefing -- you set your wallet delay to be 1008\nblocks, and someone sends to your vault with a 1007 block delay, or a\n6 block delay: does your wallet notice the tx? if it does, what do you\ndo with those funds?\n\nMaybe that would be a(nother) good reason to hide the OP_VAULT side of\nthings in taproot (or p2wsh): then you just have a (shorter) taproot\nsPK that encodes both things, and people normally don't even have enough\ninformation to correctly tweak your sPK to have a different delay.\n\n> I can see what you're saying about having more generic \"group amounts by\n> compatible vault params, and then compare to similarly grouped outputs,\"\n\nI mostly wrote that up because that's what I imagined your batching\ndoing before I'd finished reading...\n\nA big disadvantage of that approach compared to yours is that you have\nto analyse a potentially arbitrarily large transaction as a whole while\nvalidating each input -- this input uses recovery key XXX which output's\nmatch; this input uses recovery key YYY; oh, this one uses XXX again;\netc. With your approach, you only have to look at particular outputs.\n\nAn advantage of that extra complexity is that you could combine operations\nfrom multiple different vaults into a single transaction, potentially\nalong with fees or a coinjoin or whatever else. Maybe that would be\ninteresting if this were something you could code in script via some\ngeneric opcodes; but here it would be extra complexity in consensus code,\nand it doesn't seem like a good match for OP_VAULT's design parameters.\n\nCheers,\naj"
            },
            {
                "author": "James O'Beirne",
                "date": "2023-01-10T14:17:49",
                "message_text_only": "Forwarding in some conceptual feedback from the pull request.\n\n>From ariard:\n\n> I've few open questions, like if the recovery path should be committed\nwith a signature rather than protected by a simple scriptpubkey preimage.\n\nThat's something I've wondered about too. I have to ruminate on AJ's good\npost about this, but a pretty straightforward way of enabling this (at the\nexpense of some complexity) is to do something like \"if\n<recovery-path-hash> is 32 bytes, treat it as it's currently used. If it's\n33 bytes, use the first byte as a parameter for how to interpret it.\" To\nstart with, an extra prefix byte of 0x00 could mean \"require a witness\nsatisfying the scriptPubKey that hashes to the remaining 32 bytes\" in the\nsame way we do the unvault signing. This would enable a \"sign-to-recover\"\nflow at the option of the user, specified during vault creation.\n\n> The current OP_VAULT implementation is using OP_NOP repurposing but this\ndoesn't seem compatible with Taproot-only extensions (e.g ANYPREVOUT) and\nmaybe a OP_SUCCESS could be used.\n\nYes, with Greg's suggestion of putting <target-hash> on the witness stack\nduring OP_VAULT (-> OP_UNVAULT) spend, we could conceivably move\nOP_VAULT/OP_UNVAULT into Taproot-only OP_SUCCESSx opcodes. I haven't\nthought hard about how worthwhile it is to preserve the ability to use\nOP_VAULT in pre-Taproot contexts.\n\n> There is a conceptual wonder, if a CTV and template malleability approach\nwouldn't better suit the vault use-case and allow other ones, as such\nbetter re-usability of primitives.\n\nI dedicated a whole section of the paper (\"Precomputed vaults with\ncovenants\") to explaining why precomputed covenant mechanisms have big\nshortcomings for vaults.\n\nThat said, a number of people have commented about OP_VAULT's ability to\n(inefficiently) emulate CTV. I'm still very supportive of CTV, I just don't\nreally have any uses I personally understand inside and out aside from\nvaults... so if others do, they should really post about it on this list\nand we should resume working on an activation for CTV!\n\n---\n\n>From naumenkogs:\n\n> I'm personally not sure batching withdrawals is that compelling... It's a\nnice-to-have, but I'd think about the benefits dropping this feature would\nprovide.\n\nHaving familiarity with a few large-scale custodial operations, I think\nbatching is a really big deal. And if you're going to support multiple\ndeposits to the same vault, no support for batching is going to result in a\nlot of unnecessary output creation even as a small user if you're, e.g.,\ndoing weekly automated deposits from an exchange to a vault you've\nconfigured.\n\nDarosior comments:\n\n> On the contrary i think the batching feature is very compelling. The\nimpossibility to batch Unvaults in Revault is a major drawback: it\nsignificantly increases the cost of any realistic operation (you need one\nwhole additional transaction per input, and each have likely more than one\noutput). It also potentially increases the cost on the network (you'd\nlikely want some sort of anchor output on each Unvault tx, that you might\nnot spend, so that's 2*n outputs created with n the number of coins spent):\nwe definitely don't want to prevent batching. The ability to batch the\nrecovery transactions (what we called Emergency tx in Revault) is also very\ncompelling but i think your comment was only about batched withdrawals.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230110/de2020d1/attachment-0001.html>"
            },
            {
                "author": "Andrew Chow",
                "date": "2023-01-16T23:47:09",
                "message_text_only": "Hi James,\n\nThis seems like a promising proposal, but I noticed have a few issues\nregarding batching and privacy.\n\nIt seems like this proposal will encourage address reuse for vaults, at\nleast in some parts. It seems like it would not be difficult to ensure\nthat each vault address was unique through the use of key derivation.\nThe recovery and unvault scripts could be produced from ranged\ndescriptors and so there would each vault address would be unique as\neach recovery and unvault script is unique. It would not be hard to have\ndescriptors for vaults, which would then allow for usage of other\ndescriptors and miniscript into the recovery and unvault scripts.\n\nHowever the current construction makes it impossible to spend these\nvaults together. Since OP_VAULT requires the recovery script of the\nunvault output to match what's provided in the input, if there are\nmultiple inputs with different recovery scripts, then the transaction\nwill fail. I'm not sure how this could be solved though.\n\nBut from my reading of the code, it looks like the unvault scripts can\nbe unique, so at least address reuse can be avoided here. It just means\nthat the recovery scripts must be the same, and this would leave an\nidentifying mark on chain for every unvault. An observer would be able\nto correlate unvault transactions by the hashes of the recovery scripts,\nand I think this would be rather detrimental to user privacy, not to\nmention that sweeping to recovery would also reveal all of your coins too.\n\nOn the topic of address reuse, the implemented optional re-vault output\nexplicitly requires address reuse, as well as breaking the batched\nunvaulting of scripts that have different unvault scripts. It's\ncurrently implemented as requiring the unvault script to exactly match\nthe prevout script of the inputs being spent. This means that all inputs\nmust have the same script. I think it would be sufficient to do the same\ncheck as the OP_UNVAULT script and just require that the recovery script\nand the delay are the same, with the hash of the trigger script being\nprovided in the input in the same way the target hash is provided for\nOP_UNVAULT. This would break the address reuse requirement.\n\nI'm also not convinced that OP_VAULT and OP_UNVAULT should be allowed\nfor bare and P2WSH outputs. It seems like it would make sense to just\nlimit their usage to tapscripts as this would simply their implementation.\n\n\nAndrew\n\nOn 01/09/2023 11:07 AM, James O'Beirne via bitcoin-dev wrote:\n> For the last few years, I've been interested in vaults as a way to\n> substantially derisk custodying Bitcoin, both at personal and commercial\n> scales. Instead of abating with familiarity, as enthusiasm sometimes\n> does, my conviction that vaults are an almost necessary part of bitcoin's\n> viability has only grown over the years.\n>\n> Since people first started discussing vaults, it's been pretty clear that\n> some kind of covenant-enabling consensus functionality is necessary to\n> provide the feature set necessary to make vault use practical.\n>\n> Earlier last year I experimented with using OP_CTV[1], a limited covenant\n> mechanism, to implement a \"minimum-viable\" vault design. I found that the\n> inherent limitations of a precomputed covenant scheme left the resulting\n> vault implementation wanting, even though it was an improvement over\n> existing strategies that rely on presigned transactions and (hopefully)\n> ephemeral keys.\n>\n> But I also found proposed \"general\" covenant schemes to be\n> unsuitable for this use. The bloated scriptPubKeys, both in size and\n> complexity, that would result when implementing something like a vault\n> weren't encouraging. Also importantly, the social-consensus quagmire\n> regarding which covenant proposal to actually deploy feels at times\n> intractable.\n>\n> As a result, I wanted to explore a middle way: a design solely concerned\n> with making the best vault use possible, with covenant functionality as a\n> secondary consideration. In other words, a proposal that would deliver\n> the safety benefits of vaults to users without getting hung up on\n> trying to solve the general problem of covenants.\n>\n> At first this design, OP_VAULT, was just sort of a pipe dream. But as I\n> did more thinking (and eventually implementing) I became more convinced\n> that, even if it isn't considered for soft-fork, it is a worthwhile\n> device to serve as a standard benchmark against which other proposals\n> might be judged.\n>\n> I wrote a paper that summarizes my findings and the resulting proposal:\n> https://jameso.be/vaults.pdf\n>\n> along with an accompanying draft implementation:\n> https://github.com/bitcoin/bitcoin/pull/26857\n>\n> I might work on a BIP if there's interest.\n>\n> James\n>\n> [1]: https://github.com/jamesob/simple-ctv-vault"
            },
            {
                "author": "Anthony Towns",
                "date": "2023-01-17T07:46:38",
                "message_text_only": "On Mon, Jan 16, 2023 at 11:47:09PM +0000, Andrew Chow via bitcoin-dev wrote:\n> It seems like this proposal will encourage address reuse for vaults,\n\n(That is listed as an explicit goal: \"A single vault scriptPubKey should\nbe able to \"receive\" multiple deposits\")\n\n> However the current construction makes it impossible to spend these\n> vaults together. Since OP_VAULT requires the recovery script of the\n> unvault output to match what's provided in the input,\n\nI don't think this part is a big problem -- the recovery path requires\nrevealing a secret, but if you separate that secret from the recovery\npath sPK, you could vary the secret. ie:\n\n  unvault1 delay recovery1 VAULT\n  unvault2 delay recovery2 VAULT\n\nwhere recovery1 = SHA256(SHA256(secret1), rSPK) and recovery2 =\nSHA256(SHA256(secret2), rSPK), and both are spendable when the top stack\nelement is secretN and the first output pays at least the sum of all\nthe OP_VAULT using inputs to rSPK. So batched recovery could work fine,\nI think.\n\n(If you're using the same \"recovery\" parameter to each VAULT, then\nyou're revealing which txs are in your vault at spend time, rather than\nat receive time, which doesn't seem all that much better to me)\n\nBut the problem with this is it prevents you from combining vaults when\nspending normally: so if you've got a bunch of vaults with 1 BTC each,\nand want to spend 10 BTC on a house, you'll need to make 11 separate\ntransactions:\n\n  * 10 txs each spending a single vault utxo, satisfying\n      <uN> <delay> <rN> OP_VAULT\n    via the uN path, creating an output of\n      <outhash> <delay> <rN> OP_UNVAULT\n\n  * 1 tx spending all the OP_UNVAULT outputs to a common set of outputs\n    <uN>, with nSequence set to a relative timelock of at least <delay>\n\nWhereas if you use an identical OP_VAULT script for all the utxos in\nyour vault, that can look like:\n\n  * 1 tx, spending all the vault utxos, to a single OP_UNVAULT output,\n    with the same <delay> <rN> that all the inputs share.\n\n  * 1 tx spending the OP_UNVAULT output after a delay\n\nBut maybe you can get the best of both worlds just by having the unvault\npath for OP_VAULT require you to put the vout number for its corresponding\nOP_UNVAULT output on the stack? Then if you're doing address reuse, you\nuse a single vout for multiple inputs; and if you're avoiding address\nreuse, you use multiple outputs, and provide the mapping between inputs\nand outputs explicitly.\n\nCheers,\naj"
            },
            {
                "author": "Billy Tetrud",
                "date": "2023-01-18T19:00:29",
                "message_text_only": "I like the proposal of a targeted wallet vault opcode. It keeps things\nconstrained, limiting objections to those of the form \"but if we had X it\nwould do all this and more so why add this complexity when it will be\nobsoleted in the future?\"\n\n> An idealized vault\n> no existing vault design meets this set of features.\n\nMy proposal for efficient wallet vaults\n<https://github.com/fresheneesz/bip-efficient-bitcoin-vaults> was designed\nto meet all of those criteria, and allows batching as well. It also allows\nsending in a single transaction vs the two it would take with OP_VAULT and\nseveral other benefits. However, it uses a general covenant opcode to do\nit, along with several other new opcodes specified in that write up.\n\n>  it must first be spent into an OP_UNVAULT output\n\nI don't see in the write up how a node verifies that the destination of a\nspend using an OP_VAULT output uses an appropriate OP_UNVAULT script. I see\nyou mentioned above (but not in the write up) that the script pub key needs\nto be bare. But it would be very helpful if you detailed exactly how this\nis intended to be done in that document.\n\nIt seems that Greg Sanders noticed the same thing. I like his suggestion as\nyou reworded it above, makes a lot of sense.\n\n> I think the fix is just requiring a single additional witness data item\nduring OP_VAULT spend (for unvault path), mandating the\n<target-outputs-hash> to be included in the witness stack as an input to\nOP_VAULT opcode, and transaction introspection then checks to make sure the\nwitness item and the corresponding output script template matches the\nexpected.\n\n>  If it becomes necessary to make use of the recovery path, the recovery\nscriptPubKey will be revealed, which means that any other vaults with that\nrecovery path may be swept there by an unauthenticated party.\n\nAnother issue is that the recovery path becomes the easiest mechanism of\nattack. It would usually be prudent to store this recovery address with\nevery key to the vault, and potentially in other places as well, so as to\nminimize the possibility that the funds are lost or stolen. In such a\nsituation, this means that an attacker that finds any key can grief the\nvault by spending it to its recovery address. My \"efficient wallet vault\"\ndesign I mentioned above doesn't have this issue, nor the associated\nbatching DoS attack.\n\n> if the recovery path should be committed with a signature\n> This would enable a \"sign-to-recover\" flow at the option of the user,\nspecified during vault creation.\n\nThis is rather limiting isn't it? Losing the key required to sign loses\nyour recovery option. Seems brittle. It seems better to me to include a\n<recovery-initiation-spk-hash> that operates similarly to\n<recovery-spk-hash> - where some arbitrary script must be fulfilled to\nallow the recovery path to be spent. For a recovery path, you'd probably\noften want one of the keys required to spend from the recovery address,\nsince without access to one of those keys, you can't spend from the\nrecovery address anyway (and the spend path is an effective burn). Having\nsome ability to allow an n-of-m situation for triggering the recovery path\nseems desirable.\n\n>  What do you think about the idea of making the recovery-path\nauthorization behavior variable on a single byte flag preceding the 32 byte\ndata push, as I mentioned in another post?\n\nA more arbitrary construct here that allows you to use any kind of recovery\nscript would be much more flexible and preclude the need for any kind of\nswitching like this. It seems like it would also solve the issue Andrew\nChow mentioned where recovery transactions can only be batched if they all\nshare the same recovery output, since each output can simply include the\nappropriate witness matching its recovery scriptPubkey.\n\nTho I don't think I quite understand why you mention the constraint\nrequiring recovery batching to only be done with vault outputs that have\nmatching recovery destinations. Wouldn't it be reasonably possible to allow\nrecovery outputs with any recovery address to be batched, and the amount\nsums sent to each to be added up and verified?\n\n>  1. script validation rules could require some allowable \u201crange\u201d of\namount discounts\n>  seems like a bad design\n\n> 2. script validation rules could require that the unvault/recovery\noutputs preserve the full value\n> seems like the preferable approach\n\nBoth have tradeoffs. I would not call #1 an inherently bad design. I would\npoint out that for 2, disallowing the spending of vault funds without\naccess to already-unvaulted bitcoin seems like a very inconvenient design,\nsince it would require you in the best of cases to create more complex\ntransactions (or cpfp transaction chains) involving a 2nd wallet that you\ninvolve in the unvaulting process, and in the worst case (if you have no\nother bitcoin or other money on hand) you have to go asking a 3rd party for\ntheir bitcoin to use in the unvaulting process. If someday wallet vaults\nare the standard wallet construct, people might not even want to have a\nnon-vault wallet just for use in unvaulting.\n\n#1 is the approach I used to design OP_LIMITFEECONTRIBUTION\n<https://github.com/fresheneesz/bip-efficient-bitcoin-vaults/blob/main/lfc/bip-limit-fee-contribution.md>,\nwhich allows for a fee-range specification that depends on recent median\nfees included in blocks. This allows rather flexibly limiting fees to a\nparticular range of \"priorities\" regardless of fee environment.\n\nFor #2, it seems like Jeremy Rubin's Sponsor transactions would be ideal\nfor facilitation of adding a fee to an unvaulting transaction.\n\n>  In the case of a withdrawal, unvaulted funds can skip the \u201cwarm\u201d wallet\nstep that precomputed vault funds must pass through on their way to\ndestinations only known at unvault time.\n\nHmm, it seems inaccurate to say that step is \"skipped\". While there isn't a\nwarm wallet step, its replaced with an OP_UNVAULT script step. So its not\nskipped as much as modified I think, right?\n\nIt looks like the way the OP_UNVAULT is specified prevents any use where\nyou don't know the full set of outputs, which might happen in cases where\ncertain sighash flags might be useful (signing some outputs you know, but\nallowing outputs that you don't know to be added later). This is another\nthing my \"efficient wallet vault\" design should allow.\n\n\n\n\n\nOn Tue, Jan 17, 2023 at 1:47 AM Anthony Towns via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Mon, Jan 16, 2023 at 11:47:09PM +0000, Andrew Chow via bitcoin-dev\n> wrote:\n> > It seems like this proposal will encourage address reuse for vaults,\n>\n> (That is listed as an explicit goal: \"A single vault scriptPubKey should\n> be able to \"receive\" multiple deposits\")\n>\n> > However the current construction makes it impossible to spend these\n> > vaults together. Since OP_VAULT requires the recovery script of the\n> > unvault output to match what's provided in the input,\n>\n> I don't think this part is a big problem -- the recovery path requires\n> revealing a secret, but if you separate that secret from the recovery\n> path sPK, you could vary the secret. ie:\n>\n>   unvault1 delay recovery1 VAULT\n>   unvault2 delay recovery2 VAULT\n>\n> where recovery1 = SHA256(SHA256(secret1), rSPK) and recovery2 =\n> SHA256(SHA256(secret2), rSPK), and both are spendable when the top stack\n> element is secretN and the first output pays at least the sum of all\n> the OP_VAULT using inputs to rSPK. So batched recovery could work fine,\n> I think.\n>\n> (If you're using the same \"recovery\" parameter to each VAULT, then\n> you're revealing which txs are in your vault at spend time, rather than\n> at receive time, which doesn't seem all that much better to me)\n>\n> But the problem with this is it prevents you from combining vaults when\n> spending normally: so if you've got a bunch of vaults with 1 BTC each,\n> and want to spend 10 BTC on a house, you'll need to make 11 separate\n> transactions:\n>\n>   * 10 txs each spending a single vault utxo, satisfying\n>       <uN> <delay> <rN> OP_VAULT\n>     via the uN path, creating an output of\n>       <outhash> <delay> <rN> OP_UNVAULT\n>\n>   * 1 tx spending all the OP_UNVAULT outputs to a common set of outputs\n>     <uN>, with nSequence set to a relative timelock of at least <delay>\n>\n> Whereas if you use an identical OP_VAULT script for all the utxos in\n> your vault, that can look like:\n>\n>   * 1 tx, spending all the vault utxos, to a single OP_UNVAULT output,\n>     with the same <delay> <rN> that all the inputs share.\n>\n>   * 1 tx spending the OP_UNVAULT output after a delay\n>\n> But maybe you can get the best of both worlds just by having the unvault\n> path for OP_VAULT require you to put the vout number for its corresponding\n> OP_UNVAULT output on the stack? Then if you're doing address reuse, you\n> use a single vout for multiple inputs; and if you're avoiding address\n> reuse, you use multiple outputs, and provide the mapping between inputs\n> and outputs explicitly.\n>\n> Cheers,\n> aj\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230118/17d1061b/attachment.html>"
            },
            {
                "author": "James O'Beirne",
                "date": "2023-01-18T23:37:51",
                "message_text_only": "> I don't see in the write up how a node verifies that the destination\n> of a spend using an OP_VAULT output uses an appropriate OP_UNVAULT\n> script.\n\nIt's probably quicker for you to just read through the\nimplementation that I reference in the last section of the paper.\n\nhttps://github.com/bitcoin/bitcoin/blob/fdfd5e93f96856fbb41243441177a40ebbac6085/src/script/interpreter.cpp#L1419-L1456\n\n> It would usually be prudent to store this recovery address with every\n> key to the vault, ...\n\nI'm not sure I really follow here. Worth noting now that in OP_VAULT the\nrecovery path can be optionally gated by an arbitrary scriptPubKey.\n\n> This is rather limiting isn't it? Losing the key required to sign\n> loses your recovery option.\n\nThis functionality is optional in OP_VAULT as of today. You can specify\nOP_TRUE (or maybe I should allow empty?) in the <recovery-params> to\ndisable any signing necessary for recovery.\n\n> Wouldn't it be reasonably possible to allow recovery outputs with any\n> recovery address to be batched, and the amount sums sent to each to be\n> added up and verified?\n\nI think the space savings from this is pretty negligible, since you're\njust saving on the transaction overhead, and it makes the implementation\ndecently more complicated. One benefit might be sharing a common\nfee-management output (e.g. ephemeral anchor) across the separate vaults\nbeing recovered.\n\n> If someday wallet vaults are the standard wallet construct, people\n> might not even want to have a non-vault wallet just for use in\n> unvaulting.\n\nIf you truly lacked any non-vaulted UTXOs and couldn't get any at a\nreasonable price (?), I can imagine there might be a mechanism where you\ninclude a payout output to some third party in a drafted unvault trigger\ntransaction, and they provide a spend of the ephemeral output.\n\nThough you do raise a good point that this construction as written may\nnot be compatible with SIGHASH_GROUP... I'd have to think about that\none.\n\n> Hmm, it seems inaccurate to say that step is \"skipped\". While there\n> isn't a warm wallet step, its replaced with an OP_UNVAULT script step.\n\nIt is \"skipped\" in the sense that your bitcoin can't be stolen by having\nto pass through some intermediate wallet during an authorized withdrawal\nto a given target, in the way that they could if you had to prespecify\nan unvault target when creating the vault.\n\n\n---\n\n\n> My proposal for efficient wallet vaults was designed to meet all of\n> those criteria, and allows batching as well.\n\nProbably a discussion of your proposal merits a different thread, but\nsome thoughts that occur:\n\n\n> [from the README]\n>\n> OP_BEFOREBLOCKVERIFY - Verifies that the block the transaction is\n> within has a block height below a particular number. This allows a\n> spend-path to expire.\n\nI think this breaks fundamental reorgability of transactions. I think\nsome of the other opcodes, e.g the one that limits fee contribution on\nthe basis of historical feerate, are going to be similarly\ncontroversial.\n\n> This is done by using a static intermediate address that has no values\n> that are unique to the particular wallet vault address.\n\nDoes mean either that (i) this proposal doesn't have dynamic unvaulting\ntargets or, (ii) if you do, in order to be batch unvaulted, vaulted\ncoins need to first be spent into this intermediate output?\n\nIt sounds like (ii) is the case, given that your unvault target\nspecification lives in (I think?) the witness for the spend creating the\nintermediate output.\n\nIf the intermediate address doesn't have any values which are unique to\na particular vault, how do you authorize recoveries from it?\n\n---\n\nPersonally I think if you'd like to pursue your proposal, it'd be\nvaluable to see a full implementation. Might also make it easier to\nassess the viability of the proposal.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230118/f32b1cef/attachment.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2023-01-19T22:49:30",
                "message_text_only": ">> It would usually be prudent to store this recovery address with every key\nto the vault, ...\n> Worth noting now that in OP_VAULT the recovery path can be optionally\ngated by an arbitrary scriptPubKey.\n\nGating by a scriptPubKey solves the problem I was talking about there.\nHowever, thinking about it more, I realized doing this basically turns\nOP_VAULT into something able to do general covenants. By making\n`unvault-target-hash`\nunsatisfiable (set to some random number that isn't derived from a hash)\nthe delay wouldn't matter, but arbitrary conditions can be set on spending\nthe utxo to the \"recovery address\" which could be another OP_UNVAULT\ndestination. It seems like that could be used as a general CTV-like\ncovenant.\n\n>> Wouldn't it be reasonably possible to allow recovery outputs with any\n>> recovery address to be batched, and the amount sums sent to each to be\n>> added up and verified?\n> I think the space savings from this is pretty negligible\n\nBesides space savings, there's the consideration of the usability of the\nvault and downstream code complexity. One of the criteria I designed the\n\"efficient wallet vaults\" opcodes for is that the vault spend should be\nspendable in situations where a normal output is spendable as much as\npossible. Having a constraint that prevents one type of otherwise spendable\noutput from being combined with another type would add complexity to all\ndownstream code which now has to have special cases - either a simple error\nand recovery if they just want to disallow that type of opcode being\ncombined with other types (which may degrade the user experience by asking\nthem to provide a different utxo or do an unvaulting first), or some kind\nof special case handling to make it work. Any kind of hybrid wallet\nincorporating a vault (eg a wallet that combines a vault and a hot address\nor lightning channel, kind of like Phoenix combines a lightning channel and\na normal onchain wallet) would need to deal with this kind of extra\ncomplexity during utxo selection.\n\nAre there currently any situations where one otherwise-spendable utxo can't\nbe mixed with another? If not, this added edge case deserves some extra\nconsideration I think.\n\n> I can imagine there might be a mechanism where you include a payout\noutput to some third party in a drafted unvault trigger transaction, and\nthey provide a spend of the ephemeral output.\n\nI agree that's doable. I just think it merits some consideration as to\nwhether that complexity (both for downstream code and for users) is a\nfavorable trade off vs having a solution to reasonably bound fees spendable\nfrom the vault.\n\nConsider the case where a self-custodying user would have a small set of\nkeys (2? 3?) and use all those keys to secure their vault, and just 1 of\nthem to secure their hot wallet. It doesn't seem an implausible case and I\ncould imagine that kind of set up becoming quite common. In such a case, if\nthe hot wallet key is stolen, it means one vault key is also stolen and the\nhot wallets funds could be stolen at the same time as an unvaulting is\ntriggered. The need to figure out how to coordinate a 3rd party's help to\nrecover is at best an added difficulty and delay.\n\nAn alternative would be to keep a completely separate hot wallet key that\nisn't use as part of the vault. But because key storage is by far the most\ndifficult and costly part of self-custody, every additional key that needs\nto be stored is a significant additional burden (that's one of the benefits\nof wallet vaults - fewer seeds needed for a given amount of\nsecurity/redundancy).\n\nAnother alternative would be to have a hot wallet that for its primary\nspend-path uses a memory-only passphrase on one of the vault seeds (so\ncompromise of a vault seed won't compromise the hot wallet) and has a\nrecovery spend path that uses multiple (or all) vault seeds to recover if\nyou forget the passphrase. It certainly seems like something can be worked\nout here to make the end user experience reasonable, but the additional\noperational complexity this would entail still deserves consideration.\n\n>> OP_BEFOREBLOCKVERIFY\n> I think this breaks fundamental reorgability of transactions.\n\nI discuss this in the Reorg Safety section here\n<https://github.com/fresheneesz/bip-efficient-bitcoin-vaults/blob/main/bbv/bip-beforeblockverify.md#reorg-safety>\n.\n\n>> This is done by using a static intermediate address that has no values\n>> that are unique to the particular wallet vault address.\n> Does mean .. that (ii) .. in order to be batch unvaulted [with dynamic\nunvaulting\ntargets], vaulted\n> coins need to first be spent into this intermediate output?\n\nIt does support dynamic unvaulting using OP_PUSHOUTPUTSTACK\n<https://github.com/fresheneesz/bip-efficient-bitcoin-vaults/blob/main/pos/bip-pushoutputstack.md>,\nwhich adds data to an output that carries over to the execution when\nspending the output created by a utxo that uses OP_PUSHOUTPUTSTACK. So the\ndesign is done such that once the intermediate output has been confirmed\nand the unvaulting delay has passed, it is then fully owned by the\nrecipient without a second transaction (because of the use of\nOP_BEFOREBLOCKVERIFY). If OP_BEFOREBLOCKVERIFY is deemed to be\nunacceptable, then the intermediate output is fully intermediate and a 2nd\ntransaction would be required to get it to its committed recipient.\n\n> it'd be valuable to see a full implementation\n\nWhile OP_BEFOREBLOCKVERIFY can be dropped with only somewhat minor degraded\nusability, OP_PUSHOUTPUTSTACK is necessary for the proposal to work as\nintended. I would want to see some support for the high-level concepts it\nintroduces before spending significant time on an implementation. It does\nsomething fundamentally new that other opcodes haven't done: add \"hidden\"\ndata onto the output that allows for committing to destination addresses.\nMaybe something along the line of Greg Sanders' suggestion for your\nproposal could replace the need for this, but I'm not sure its possible\nwith how OP_CD is designed.\n\nOn Wed, Jan 18, 2023 at 5:38 PM James O'Beirne <james.obeirne at gmail.com>\nwrote:\n\n> > I don't see in the write up how a node verifies that the destination\n> > of a spend using an OP_VAULT output uses an appropriate OP_UNVAULT\n> > script.\n>\n> It's probably quicker for you to just read through the\n> implementation that I reference in the last section of the paper.\n>\n>\n> https://github.com/bitcoin/bitcoin/blob/fdfd5e93f96856fbb41243441177a40ebbac6085/src/script/interpreter.cpp#L1419-L1456\n>\n> > It would usually be prudent to store this recovery address with every\n> > key to the vault, ...\n>\n> I'm not sure I really follow here. Worth noting now that in OP_VAULT the\n> recovery path can be optionally gated by an arbitrary scriptPubKey.\n>\n> > This is rather limiting isn't it? Losing the key required to sign\n> > loses your recovery option.\n>\n> This functionality is optional in OP_VAULT as of today. You can specify\n> OP_TRUE (or maybe I should allow empty?) in the <recovery-params> to\n> disable any signing necessary for recovery.\n>\n> > Wouldn't it be reasonably possible to allow recovery outputs with any\n> > recovery address to be batched, and the amount sums sent to each to be\n> > added up and verified?\n>\n> I think the space savings from this is pretty negligible, since you're\n> just saving on the transaction overhead, and it makes the implementation\n> decently more complicated. One benefit might be sharing a common\n> fee-management output (e.g. ephemeral anchor) across the separate vaults\n> being recovered.\n>\n> > If someday wallet vaults are the standard wallet construct, people\n> > might not even want to have a non-vault wallet just for use in\n> > unvaulting.\n>\n> If you truly lacked any non-vaulted UTXOs and couldn't get any at a\n> reasonable price (?), I can imagine there might be a mechanism where you\n> include a payout output to some third party in a drafted unvault trigger\n> transaction, and they provide a spend of the ephemeral output.\n>\n> Though you do raise a good point that this construction as written may\n> not be compatible with SIGHASH_GROUP... I'd have to think about that\n> one.\n>\n> > Hmm, it seems inaccurate to say that step is \"skipped\". While there\n> > isn't a warm wallet step, its replaced with an OP_UNVAULT script step.\n>\n> It is \"skipped\" in the sense that your bitcoin can't be stolen by having\n> to pass through some intermediate wallet during an authorized withdrawal\n> to a given target, in the way that they could if you had to prespecify\n> an unvault target when creating the vault.\n>\n>\n> ---\n>\n>\n> > My proposal for efficient wallet vaults was designed to meet all of\n> > those criteria, and allows batching as well.\n>\n> Probably a discussion of your proposal merits a different thread, but\n> some thoughts that occur:\n>\n>\n> > [from the README]\n> >\n> > OP_BEFOREBLOCKVERIFY - Verifies that the block the transaction is\n> > within has a block height below a particular number. This allows a\n> > spend-path to expire.\n>\n> I think this breaks fundamental reorgability of transactions. I think\n> some of the other opcodes, e.g the one that limits fee contribution on\n> the basis of historical feerate, are going to be similarly\n> controversial.\n>\n> > This is done by using a static intermediate address that has no values\n> > that are unique to the particular wallet vault address.\n>\n> Does mean either that (i) this proposal doesn't have dynamic unvaulting\n> targets or, (ii) if you do, in order to be batch unvaulted, vaulted\n> coins need to first be spent into this intermediate output?\n>\n> It sounds like (ii) is the case, given that your unvault target\n> specification lives in (I think?) the witness for the spend creating the\n> intermediate output.\n>\n> If the intermediate address doesn't have any values which are unique to\n> a particular vault, how do you authorize recoveries from it?\n>\n> ---\n>\n> Personally I think if you'd like to pursue your proposal, it'd be\n> valuable to see a full implementation. Might also make it easier to\n> assess the viability of the proposal.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230119/acfb7455/attachment-0001.html>"
            },
            {
                "author": "James O'Beirne",
                "date": "2023-01-18T22:45:01",
                "message_text_only": "I've implemented three changes based on suggestions from Greg Sanders\nand AJ Towns.\n\nI've segmented the changes into commits that should be\nreasonable to follow, even though I'll probably rearrange the commit\nstructure later on.\n\n1. Greg's suggestion: OP_UNVAULT outputs can now live behind\nscripthashes. This means that the lifecycle of a vault can live entirely\nwithin, say, Taproot. In this case the only thing that would reveal\nthe operation of the vault would be the content of the witness stack\nwhen triggering an unvault or recovering. So I think no real privacy\nbenefits over the previous scheme, but certainly some efficiency ones\nsince we're moving more content from the scriptPubKey into the witness.\n\n Commit here:\nhttps://github.com/bitcoin/bitcoin/pull/26857/commits/cd33d120c67cda7eb5c6bbfe3a1ea9fb6c5b93d1\n\n2. AJ's suggestion: unvault trigger transactions can now have an extra\n\"revault\" output that redeposits some balance to the same vault\nscriptPubKey from which it came. This is nice because if the delay\nperiod is long, you may want to manage a remaining vault balance\nseparately while the spent balance is pending an unvault.\n\n Commit here:\nhttps://github.com/bitcoin/bitcoin/pull/26857/commits/cf3764fb503bc17c4438d1322ecf780b9dc3ef30\n\n3. AJ's suggestion: instead of specifying <recovery-spk-hash>, introduce\na replacement parameter: <recovery-params>. This contains the same\ntarget recovery sPK hash as before, but the remaining bytes contain a\nscriptPubKey that functions as authorization for the recovery process.\nThis allows users to optionally avoid the risk of \"recovery replays\" at\nthe expense of having to maintain a recovery key. Users can opt out of\nthis by passing OP_TRUE for the recovery sPK. I guess maybe I could even\nsupport just omitting an sPK altogether for the legacy behavior.\n\nCommit here:\nhttps://github.com/bitcoin/bitcoin/pull/26857/commits/fdfd5e93f96856fbb41243441177a40ebbac6085\n\n\nThe suggestions were good ones, and I think they've improved the\nproposal.\n\nMy next steps are to do minor updates to the paper and start writing a\nBIP draft.\n\nThanks to achow for the valuable feedback, which I'm still mulling on.\n\nJames\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230118/a1c7719a/attachment.html>"
            },
            {
                "author": "James O'Beirne",
                "date": "2023-01-20T17:43:14",
                "message_text_only": "Andrew, thanks for taking the time.\n\n> It seems like this proposal will encourage address reuse for vaults,\n> at least in some parts. It seems like it would not be difficult to\n> ensure that each vault address was unique through the use of key\n> derivation.\n\nI think it's worth stepping back and noting that this proposal defers\nthe level of privacy-vs.-efficiency to be decided by the end user.\n\nFor users who are very privacy conscious and are doing fairly low volume\n(a few vaulted coins a month?), trading the ability to do batched\noperations for no privacy loss seems reasonable. They can use a ranged\ndescriptor to generate recovery paths and unvault sPKs and reveal no\nmarginal information during recoveries or unvaults.\n\nThough of course there still may be an obvious temporal association\nacross transactions in the case of recovery - everything with the same\nunvault key has to be recovered at once.\n\nFor users who expect to have large numbers of vaulted coins and maybe\ndon't care as much about privacy (e.g. many commercial users), revealing\nthe related nature of coins that are being unvaulted or recovered seems\nlike an okay cost to pay. Such users might decide to create \"tranches\"\nof vaults with different parameters in whatever manner makes sense for\ntheir use.\n\nImportantly: in either case, you can always keep the nature of\nstill-vaulted coins hidden by burying the OP_VAULT script in a taptree\nand varying the internal key you use for each coin with a ranged\ndescriptor. This way, only the revealed contents of unvaults and\nrecoveries can be associated. So I think that's your worst case, which\nreally doesn't seem bad to me.\n\nAs an aside, a goal in supporting address reuse wasn't for address reuse\n*per se* - it was to remove potential catastrophe resulting from the\ncase where you give someone a vault address to deposit to, and they wind\nup depositing to it multiple times - whether you warned them against it\nor not.\n\n\n> I'm not sure how [batching without privacy loss] could be solved\n> though.\n\nFor recovery, I think it might be intractable at the moment.\n\nSeemingly unrelated vaults which have the same recovery parameters will\npresumably be recovered together if the unvault key is compromised. The\nsimple fact that these outputs are being spent together and are OP_VAULT\nscripts fundamentally reveals their association during recovery, no way\naround that AFAICT.\n\nSimilarly for the unvaulting, you can't get around revealing that you're\nspending a group of outputs that contain an OP_VAULT script.\n\nAs mentioned above, unvaulting -- regardless of whether your vault\nconfiguration supports batching or not -- *never* has to correlate\nunvaulted coins to still-vaulted coins if you are either\n\n  (i) varying the internal key used for the OP_VAULT taptrees, or\n  (ii) using the optional \"authorized recovery\" feature and are varying\n       that sPK with a ranged descriptor.\n\nSo there's really never a case where unvaults have to reveal a\nrelationship to, or between, still-vaulted coins. Subsequent unvaults\nmay be able to be correlated though on the basis of the recovery sPK\ntarget hash.\n\n\n> It just means that the recovery scripts must be the same, and this\n> would leave an identifying mark on chain for every unvault.\n\nThis is only true if the user decides to create vaults which share\n\"literal\" recovery paths. At the risk of belaboring the point for\nclarity, you can avoid this by generating the different \"literal\"\nrecovery paths from a ranged descriptor which is controlled by a single\nprivate key -- of course at the cost of no batched recovery.\n\n\n> not to mention that sweeping to recovery would also reveal all of your\n> coins too.\n\nMaybe it's worth contextualizing that recovery sweeps really only happen\nas a final fallback in catastrophic cases that, in a world without\nvaults, would result in the coins simply being stolen. In this case I\nwould guess most users are willing to make the privacy trade to retain\nownership of their coins.\n\n\n> I think it would be sufficient to do the same check as the OP_UNVAULT\n> script [when validating the revault output in the unvault trigger\n> transaction] and just require that the recovery script and the delay\n> are the same\n\nConsider that this allows the holder of the unvault key (possibly an\nattacker) to immediately spend the vault into a new vault configuration\nwith a different unvault key.\n\nObviously the recovery path would still be the same, and so the owner of\nthe vault could still sweep if the unvault key switch was unauthorized,\nbut I'll need to think a little bit more about whether this change is\nmore fundamental.\n\nGenerally that would be easy to implement, though. I'll think on it. My\nsuspicion is that you're right and this would be a good change.\n\n\n> I'm also not convinced that OP_VAULT and OP_UNVAULT should be allowed\n> for bare and P2WSH outputs. It seems like it would make sense to just\n> limit their usage to tapscripts as this would simply their\n> implementation.\n\nI think you're right, and accordingly I'll shortly be reimplementing\nthis with OP_SUCCESSx overrides instead of OP_NOPs. I'd be curious to\nknow if anyone has any objections to this - i.e. if there's a solid case\nfor vaults in a pre-taproot context.\n\nJames\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230120/b645548b/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "OP_VAULT: a new vault proposal",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "rot13maxi",
                "Anthony Towns",
                "Andrew Chow",
                "Billy Tetrud",
                "James O'Beirne",
                "Greg Sanders"
            ],
            "messages_count": 16,
            "total_messages_chars_count": 82075
        }
    },
    {
        "title": "[bitcoin-dev] Why Full-RBF Makes DoS Attacks on Multiparty Protocols Significantly More Expensive",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2023-01-09T22:18:52",
                "message_text_only": "I was reminded recently that while Suhas Daftuar cited tx-pinning as a reason\nto remove full-rbf, he neglected to mention that tx-pinning greatly increases\nthe cost of attacks on multi-party protocols. Him (rhetorically?) asking(4):\n\n    \"Does fullrbf offer any benefits other than breaking zeroconf business\n     practices?\"\n\n...has caused a lot of confusion by implying that there were no benefits. So\nI'm writing this to set the record straight and provide an easily cited\nexplanation as to why full-rbf - even with tx-pinning - is a valuable\nimprovement for multi-party protocols like coinjoins that rely on transactions\ncontaining multiple inputs exclusively controlled(1) by different parties.\n\ntl;dr: without full-rbf people can intentionally and unintentionally DoS attack\nmulti-party protocols by double-spending their inputs with low-fee txs, holding\nup progress until that low-fee tx gets mined. This could take days, weeks, or\neven worse. Modulo intentional tx-pinning, full-RBF fixes this by ensuring that\na higher fee transaction gets mined in a reasonable amount of time so the\nprotocol makes forward progress. And as for tx-pinning, exploiting it is very\nexpensive, so full-rbf still makes the situation much better than the status\nquo.\n\n\n# The Double-Spend DoS Attack on Multi-Party, Multi-Input, Transactions\n\nIf a protocol constructs transactions containing multiple inputs exclusively\ncontrolled by different parties, those parties can intentionally and\nunintentionally double-spend those inputs in alternate transactions. For\nexample, in a Wasabi coinjoin any one of the hundreds of participants could\nsign and broadcast a transaction spending their input. If they do that at the\nright time, as much as ~100% of the hashing power may see the double-spend\nfirst, prior to the intended coinjoin transaction. This in fact does happen\nregularly in production to Wasabi coinjoins, probably due to people\naccidentally running different wallets at the same time using the same seed, as\nwell as people importing their seeds into alternative wallets.\n\nBy itself this isn't a significant problem: Wasabi coinjoins are a two phase\nprotocol, and, like any multi-step, multi-party protocol, they have to deal\nwith the fact that participants in the protocol may fail to complete all the\nsteps necessary for a transaction to be completed. It's very common for one or\nmore parties in a Wasabi coinjoin to fail to complete both steps of the\nprotocol, and a majority of Wasabi coinjoin rounds fail. Wasabi deals with this\neconomically by (temporarily or ~permanently) blacklisting UTXOs that failed to\ncomplete a round, making DoS attacks expensive by forcing the attacker to tie\nup funds/create new UTXOs.\n\nSimilarly, in use-cases such as multi-party-funded Lightning channels(5), an\nattacker can always DoS attack the protocol by participating in a channel open,\nand then failing to allow payments to be routed through it. The solution is\nagain to use economics to ensure the attack is sufficiently costly.\n\nHowever, under the right circumstances double-spends are an unusually powerful\nDoS attack on multi-party, multi-input, transaction. When mempool demand is\nhigh, low fee transactions can take arbitrarily long to get mined. Bitcoin has\nseen periods of mempool demand where low-fee transactions would take *months*\nto get mined. Transaction expiry does not solve this problem, as anyone can\nrebroadcast transactions at any time. In these circumstances without\ntransaction replacement a multi-party transaction such as a Wasabi coinjoin\ncould be held up indefinitely by a low-fee double-spend.\n\n\n## How Full-RBF Mitigates the Double-Spend DoS Attack\n\nModulo tx-pinning, full-rbf mitigates the double-spend DoS attack in a very\nstraightforward way: the low fee transaction is replaced by the higher fee\ntransaction, resulting in the latter getting mined in a reasonable amount of\ntime and the protocol making forward progress.\n\nNote that the converse is not a useful attack: if the attacker broadcasts a\nhigh-fee double spend, higher than the intended multi-party transaction, the\ntransaction will get mined in a reasonable amount of time, costing the attacker\nmoney and the defender nothing beyond wasted time. Multi-party protocols always\nhave the property that attackers can spend money to DoS attack by creating more\nUTXOs/identities/etc, so this isn't any worse than the status quo!\n\n\n## Transaction Pinning\n\nSo what about transaction pinning? The term actually refers to a few different\ntechniques that can make it difficult/expensive to fee-bump a transaction.\nWe're interested in the techniques relevant to replacements, namely\nexploitation of:\n\n1. BIP-125 RBF Rule #3: a replacement transaction is required to pay\nthe higher absolute fee (not just fee rate) than the sum of fees paid by all\ntransactions being replaced.\n\n2. BIP-125 RBF Rule #5: the number of transactions replaced at one time must\nnot exceed 100. Note that this rule only exists due to limitations of the\nexisting Bitcoin Core implementation; it has absolute no economic rational and\nshould be removed by either improving the implementation's scalability issues,\nor rejecting transactions that could make a transaction unreplaceable(2).\n\nExploiting either rule is expensive. To exploit rule #3 the attacker has to\nbroadcast fee-paying transactions paying a total amount of fees higher than the\ndefender is willing to pay. Since transactions don't expire, in almost all\ncircumstances those transactions will eventually be mined, costing the attacker\nmuch more money than they would have spent without full-rbf.\n\nTo exploit rule #5, the attacker has to broadcast 100x more fee-paying\ntransactions than they otherwise would have. As with rule #3, those\ntransactions will almost always eventually be mined, costing the attacker\nsignificantly more money than they would have spent without full-rbf. And, as\nmentioned above(2), rule #5 is merely an artifact of the existing\nimplementation which can and should be fixed.\n\nThe only avenue for an attacker to avoid transaction pinning costs is\namortisation: reusing the extra transactions required for pinning for other\nattacks/other purposes. But of course, amortisation is *already* a potent cost\nreduction strategy for attacks on multi-party protocols such as coinjoin, so\nthe existence of transaction pinning doesn't appreciably change the situation.\nAgain, there are mitigations such as requiring participants to post nLockTime'd\nfee-paying transactions(3), and limiting attacks to parties who are heavily\ninvested in Bitcoin for other reasons is a valuable improvement on the status\nquo.\n\n\n# Conclusion\n\nFar from not \"offering any benefits other than breaking zeroconf business\npractices\"(4), full-rbf clearly improves Bitcoin for multi-party protocols,\namong the many other reasons to adopt it.\n\n\n# Footnotes\n\n1. What do I mean by \"exclusively controlled\"? Let's compare coinjoin, to an\n   ordinary single-payer Lightning channel. In a coinjoin, the goal is to get a\n   transaction mined containing multiple inputs from different parties. Each of\n   these inputs is individually, exclusively controlled by a single party:\n   without the cooperation of any other party that input that be spend. This is\n   unlike an ordinary single-payer Lightning channel: while the commitment\n   transactions are multi-party transactions, the multisig transaction outputs\n   involved are *jointly* controlled by both parties, and thus neither party can\n   spend it without the cooperation of the other at some point.\n\n2. [bitcoin-dev] Removing BIP-125 Rule #5 Pinning with the Always-Replaceable\n   Invariant, https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-November/021175.html\n\n3. [bitcoin-dev] Using Full-RBF to fix BIP-125 Rule #3 Pinning with nLockTime,\n   https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-November/021176.html\n\n4. https://github.com/bitcoin/bitcoin/pull/26438\n\n5. There are even more exotic proposed Lightning-related protocols where a failure\n   of transaction replacement can cause the loss of funds. I'm not covering\n   those scenarios because they have such strong requirements - beyond what\n   full-rbf offers - that the technical community does not have consensus that\n   these proposed protocols are even viable.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230109/e2dd405c/attachment-0001.sig>"
            },
            {
                "author": "David A. Harding",
                "date": "2023-01-10T07:11:46",
                "message_text_only": "On 2023-01-09 12:18, Peter Todd via bitcoin-dev wrote:\n> [The quote:]\n> \n>     \"Does fullrbf offer any benefits other than breaking zeroconf \n> business\n>      practices?\"\n> \n> ...has caused a lot of confusion by implying that there were no \n> benefits. [...]\n> \n> tl;dr: without full-rbf people can intentionally and unintentionally \n> DoS attack\n> multi-party protocols by double-spending their inputs with low-fee txs, \n> holding\n> up progress until that low-fee tx gets mined.\n\nHi Peter,\n\nI'm confused.  Isn't this an easily solvable issue without full-RBF?\nLet's say Alice, Bob, Carol, and Mallory create a coinjoin transaction.\nMallory either intentionally or unintentionally creates a conflicting\ntransaction that does not opt-in to RBF.\n\nYou seem to be proposing that the other participants force the coinjoin\nto complete by having the coinjoin transaction replace Mallory's\nconflicting transaction, which requires a full-RBF world.\n\nBut isn't it also possible in a non-full-RBF world for Alice, Bob, and\nCarol to simply create a new coinjoin transaction which does not include\nany of Mallory's inputs so it doesn't conflict with Mallory's\ntransaction?  That way their second coinjoin transaction can confirm\nindependently of Mallory's transaction.\n\nLikewise, if Alice and Mallory attempt an LN dual funding and Mallory\ncreates a conflict, Alice can just create an alternative dual funding\nwith Bob rather than try to use full-RBF to force Mallory's earlier dual\nfunding to confirm.\n\n> ## Transaction Pinning\n> \n> Exploiting either rule is expensive.\n\nI think this transaction pinning attack against coinjoins and dual\nfundings is also solved in a non-full-RBF world by the honest\nparticipants just creating a non-conflicting transaction.\n\nThat said, if I'm missing something and these attacks do actually apply,\nthen it might be worth putting price figures on the attack in terms most\npeople will understand.  The conflicting inputs attack you described in\nthe beginning as being solved by full-RBF costs about $0.05 USD at\n$17,000/BTC.  The transaction pinning attack you imply is unsolved by\nfull-RBF costs about $17.00.  If both attacks apply, any protocol which\nis vulnerable to a $17.00 attack still seems highly vulnerable to me, so\nit doesn't feel like a stretch to say that full-RBF lacks significant\nbenefits for those protocols.\n\nThanks,\n\n-Dave"
            },
            {
                "author": "Peter Todd",
                "date": "2023-01-10T08:47:48",
                "message_text_only": "On Mon, Jan 09, 2023 at 09:11:46PM -1000, David A. Harding wrote:\n> On 2023-01-09 12:18, Peter Todd via bitcoin-dev wrote:\n> > [The quote:]\n> > \n> >     \"Does fullrbf offer any benefits other than breaking zeroconf\n> > business\n> >      practices?\"\n> > \n> > ...has caused a lot of confusion by implying that there were no\n> > benefits. [...]\n> > \n> > tl;dr: without full-rbf people can intentionally and unintentionally DoS\n> > attack\n> > multi-party protocols by double-spending their inputs with low-fee txs,\n> > holding\n> > up progress until that low-fee tx gets mined.\n> \n> Hi Peter,\n> \n> I'm confused.  Isn't this an easily solvable issue without full-RBF?\n> Let's say Alice, Bob, Carol, and Mallory create a coinjoin transaction.\n> Mallory either intentionally or unintentionally creates a conflicting\n> transaction that does not opt-in to RBF.\n> \n> You seem to be proposing that the other participants force the coinjoin\n> to complete by having the coinjoin transaction replace Mallory's\n> conflicting transaction, which requires a full-RBF world.\n> \n> But isn't it also possible in a non-full-RBF world for Alice, Bob, and\n> Carol to simply create a new coinjoin transaction which does not include\n> any of Mallory's inputs so it doesn't conflict with Mallory's\n> transaction?  That way their second coinjoin transaction can confirm\n> independently of Mallory's transaction.\n\nHow do you propose that the participants learn about the double-spend? Without\nknowing that it happened, they can't respond as you suggested.\n\n> Likewise, if Alice and Mallory attempt an LN dual funding and Mallory\n> creates a conflict, Alice can just create an alternative dual funding\n> with Bob rather than try to use full-RBF to force Mallory's earlier dual\n> funding to confirm.\n\nSame issue.\n\nAnd of course, in both cases full-rbf makes Mallory have to actually pay full\nprice for the attack. Either because the intended transaction goes through. Or\nbecause their double-spending DoS attack had to be much more expensive in the\nfirst place.\n\n> > ## Transaction Pinning\n> > \n> > Exploiting either rule is expensive.\n> \n> I think this transaction pinning attack against coinjoins and dual\n> fundings is also solved in a non-full-RBF world by the honest\n> participants just creating a non-conflicting transaction.\n> \n> That said, if I'm missing something and these attacks do actually apply,\n> then it might be worth putting price figures on the attack in terms most\n> people will understand.  The conflicting inputs attack you described in\n> the beginning as being solved by full-RBF costs about $0.05 USD at\n> $17,000/BTC.  The transaction pinning attack you imply is unsolved by\n> full-RBF costs about $17.00.  If both attacks apply, any protocol which\n> is vulnerable to a $17.00 attack still seems highly vulnerable to me, so\n> it doesn't feel like a stretch to say that full-RBF lacks significant\n> benefits for those protocols.\n\nCoinjoins are an automated process that happens constantly. As I described in\nmy email, it's totally normal for them to fail constantly - I was told by\nWasabi that only ~25% of coinjoin rounds succeed right now, a figure that\nfrankly was much higher than I expected. Being forced to spend $17/round rather\nthan $0.05/round is a huge improvement that adds up to serious money at the\nscale at which Wasabi and similar protocols operate at.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230110/9e0a6645/attachment.sig>"
            },
            {
                "author": "David A. Harding",
                "date": "2023-01-10T10:02:35",
                "message_text_only": "On 2023-01-09 22:47, Peter Todd wrote:\n> How do you propose that the participants learn about the double-spend? \n> Without\n> knowing that it happened, they can't respond as you suggested.\n\nI can think of various ways---many of them probably the same ideas that\nwould occur to you.  More concise than listing them is to just assume\nthey exist and realize that any protocol software which wants to defeat\nthe $17.00 pinning attack needs to implement some sort of conflict\nmonitoring system---but by using that monitoring system to defeat the\n$17.00 pinning attack, the software also defeats the $0.05 individual\nconflicting input attack without any need for full-RBF.\n\nFull-RBF provides no benefits here except those which are already\nprovided by other necessary tools.\n\nThanks,\n\n-Dave"
            },
            {
                "author": "Peter Todd",
                "date": "2023-01-10T10:06:22",
                "message_text_only": "On Tue, Jan 10, 2023 at 12:02:35AM -1000, David A. Harding wrote:\n> On 2023-01-09 22:47, Peter Todd wrote:\n> > How do you propose that the participants learn about the double-spend?\n> > Without\n> > knowing that it happened, they can't respond as you suggested.\n> \n> I can think of various ways---many of them probably the same ideas that\n> would occur to you.\n\nRather than playing games, how about you actually list those ways.\n\n> More concise than listing them is to just assume\n> they exist and realize that any protocol software which wants to defeat\n> the $17.00 pinning attack needs to implement some sort of conflict\n> monitoring system---but by using that monitoring system to defeat the\n> $17.00 pinning attack, the software also defeats the $0.05 individual\n> conflicting input attack without any need for full-RBF.\n\nRemember, we'd like decentralized coinjoin implementations like Joinmarket to\nwork. How does a decentralized coinjoin implement \"conflict monitoring\"?\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230110/7315ce24/attachment-0001.sig>"
            },
            {
                "author": "David A. Harding",
                "date": "2023-01-10T20:14:47",
                "message_text_only": "On 2023-01-10 00:06, Peter Todd wrote:\n> Remember, we'd like decentralized coinjoin implementations like \n> Joinmarket to\n> work. How does a decentralized coinjoin implement \"conflict \n> monitoring\"?\n\n1. Run a relay node with a conflict-detection patch.  Stock Bitcoin Core\n    with -debug=mempoolrej will tell you when it rejects a transaction\n    for conflicting with a transaction already in the mempool, e.g.:\n\n       2022-11-01T02:53:17Z \n867b85d68d7a7244c1d65c4797006b56973110ac243ab5ee15a8c4d220060c58 from \npeer=58 was not accepted: txn-mempool-conflict\n\n    I think it would be easy to extend this facility to list the inputs\n    which conflicted.  So if Alice sees a conflict created by Mallory,\n    she can create a new coinjoin transaction without Mallory.  This\n    method has the advantage of being fast and attributing fault,\n    although it does require Alice's node be online at the time Mallory's\n    conflict is propagated.\n\n2. Simply assume a conflict exists for otherwise unexplainable failures.\n    For example, if Alice sees several new blocks whose bottom feerates\n    are well below the feerates of an unconfirmed coinjoin transaction\n    that Alice helped create and broadcast, she can assume it's a\n    conflict that is preventing preventing confirmation of the coinjoin.\n    She can find an entirely different set of collaborators and create a\n    non-conflicting transaction without ever needing to know which inputs\n    from the original transaction conflicted.  This method has the\n    disadvantage of being slow (on the order of hours) and not \nattributing\n    fault, although it doesn't require Alice has any information beyond \ncopies\n    of recent blocks.\n\nI didn't list these methods or others before because the specific method \nused to\ndetect conflicts doesn't matter to the realization that software which\nuses conflict detection and evasion to defeat the $17.00 attack also\ndefeats the $0.05 attack without any need for full-RBF.\n\n-Dave"
            },
            {
                "author": "Peter Todd",
                "date": "2023-01-13T23:37:04",
                "message_text_only": "On Tue, Jan 10, 2023 at 10:14:47AM -1000, David A. Harding wrote:\n> On 2023-01-10 00:06, Peter Todd wrote:\n> > Remember, we'd like decentralized coinjoin implementations like\n> > Joinmarket to\n> > work. How does a decentralized coinjoin implement \"conflict monitoring\"?\n> \n> 1. Run a relay node with a conflict-detection patch.  Stock Bitcoin Core\n>    with -debug=mempoolrej will tell you when it rejects a transaction\n>    for conflicting with a transaction already in the mempool, e.g.:\n> \n>       2022-11-01T02:53:17Z\n> 867b85d68d7a7244c1d65c4797006b56973110ac243ab5ee15a8c4d220060c58 from\n> peer=58 was not accepted: txn-mempool-conflict\n> \n>    I think it would be easy to extend this facility to list the inputs\n>    which conflicted.  So if Alice sees a conflict created by Mallory,\n>    she can create a new coinjoin transaction without Mallory.  This\n>    method has the advantage of being fast and attributing fault,\n>    although it does require Alice's node be online at the time Mallory's\n>    conflict is propagated.\n\nSo for something as simple as reliable coinjoining - an important privacy\nfeature that we'd like all wallets to use - you expect people to run\nwell-connected 24/7 nodes running specialty software?\n\nEven if you run a node as you suggest, there's certainly no guarantee that\nyou'd learn about any double-spend without doing a severe sybil attack against\nthe network; the 8 outgoing nodes a typical node has samples a tiny fraction of\nthe network. And *even if* you sybil attack to try to detect conflicts there's\nstill no guarantee as attackers can use all kinds of special techniques to get\ntransactions into miner mempools and not others.\n\n> 2. Simply assume a conflict exists for otherwise unexplainable failures.\n>    For example, if Alice sees several new blocks whose bottom feerates\n>    are well below the feerates of an unconfirmed coinjoin transaction\n>    that Alice helped create and broadcast, she can assume it's a\n>    conflict that is preventing preventing confirmation of the coinjoin.\n>    She can find an entirely different set of collaborators and create a\n>    non-conflicting transaction without ever needing to know which inputs\n>    from the original transaction conflicted.  This method has the\n>    disadvantage of being slow (on the order of hours) and not attributing\n>    fault, although it doesn't require Alice has any information beyond\n> copies\n>    of recent blocks.\n\nYou're suggesting that to avoid enabling full-rbf, coinjoin's and other\ndecentralized multi-party protocols risk getting coins tied up for hours trying\nto do conflict resolution rather than just fixing the underlying problem with\nwhat's literally a one-line code change that 17% of the v24.x nodes have\ndecided to enable.\n\n> I didn't list these methods or others before because the specific method\n> used to\n> detect conflicts doesn't matter to the realization that software which\n> uses conflict detection and evasion to defeat the $17.00 attack also\n> defeats the $0.05 attack without any need for full-RBF.\n\nFact is, full-rbf defeats those attacks much better. And I'm amazed that you\ndon't consider raising the cost of attacks on coinjoins and similar\ndecentralized protocols by almost three orders of magnitude to be important:\nwhy are you prioritizing a few highly centralized, often AML/KYC'd, unconfirmed\ntx acceptance services over decentralized protocols which provide privacy and\nsecurity to a lot more users?\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230113/5516cb92/attachment.sig>"
            },
            {
                "author": "alicexbt",
                "date": "2023-01-10T09:19:39",
                "message_text_only": "Hi Peter,\n\n> ## How Full-RBF Mitigates the Double-Spend DoS Attack\n> \n> Modulo tx-pinning, full-rbf mitigates the double-spend DoS attack in a very\n> straightforward way: the low fee transaction is replaced by the higher fee\n> transaction, resulting in the latter getting mined in a reasonable amount of\n> time and the protocol making forward progress.\n\nAsking this question based on a [discussion on twitter][0]. How would you get extra sats to increase the fees? It seems this would be possible with Joinmarket, Wasabi and even joinstr although things would get worse for Whirlpool. Whirlpool coinjoin transactions do not signal BIP 125 RBF so they were not replaceable earlier, however attacker would be able to perform DoS attacks now by double spending their inputs used in coinjoin.\n\n[0]: https://twitter.com/dammkewl/status/1599692908860706818\n\n/dev/fd0\nfloppy disc guy\n\nSent with Proton Mail secure email.\n\n------- Original Message -------\nOn Tuesday, January 10th, 2023 at 3:48 AM, Peter Todd via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n\n> I was reminded recently that while Suhas Daftuar cited tx-pinning as a reason\n> to remove full-rbf, he neglected to mention that tx-pinning greatly increases\n> the cost of attacks on multi-party protocols. Him (rhetorically?) asking(4):\n> \n> \"Does fullrbf offer any benefits other than breaking zeroconf business\n> practices?\"\n> \n> ...has caused a lot of confusion by implying that there were no benefits. So\n> I'm writing this to set the record straight and provide an easily cited\n> explanation as to why full-rbf - even with tx-pinning - is a valuable\n> improvement for multi-party protocols like coinjoins that rely on transactions\n> containing multiple inputs exclusively controlled(1) by different parties.\n> \n> tl;dr: without full-rbf people can intentionally and unintentionally DoS attack\n> multi-party protocols by double-spending their inputs with low-fee txs, holding\n> up progress until that low-fee tx gets mined. This could take days, weeks, or\n> even worse. Modulo intentional tx-pinning, full-RBF fixes this by ensuring that\n> a higher fee transaction gets mined in a reasonable amount of time so the\n> protocol makes forward progress. And as for tx-pinning, exploiting it is very\n> expensive, so full-rbf still makes the situation much better than the status\n> quo.\n> \n> \n> # The Double-Spend DoS Attack on Multi-Party, Multi-Input, Transactions\n> \n> If a protocol constructs transactions containing multiple inputs exclusively\n> controlled by different parties, those parties can intentionally and\n> unintentionally double-spend those inputs in alternate transactions. For\n> example, in a Wasabi coinjoin any one of the hundreds of participants could\n> sign and broadcast a transaction spending their input. If they do that at the\n> right time, as much as ~100% of the hashing power may see the double-spend\n> first, prior to the intended coinjoin transaction. This in fact does happen\n> regularly in production to Wasabi coinjoins, probably due to people\n> accidentally running different wallets at the same time using the same seed, as\n> well as people importing their seeds into alternative wallets.\n> \n> By itself this isn't a significant problem: Wasabi coinjoins are a two phase\n> protocol, and, like any multi-step, multi-party protocol, they have to deal\n> with the fact that participants in the protocol may fail to complete all the\n> steps necessary for a transaction to be completed. It's very common for one or\n> more parties in a Wasabi coinjoin to fail to complete both steps of the\n> protocol, and a majority of Wasabi coinjoin rounds fail. Wasabi deals with this\n> economically by (temporarily or ~permanently) blacklisting UTXOs that failed to\n> complete a round, making DoS attacks expensive by forcing the attacker to tie\n> up funds/create new UTXOs.\n> \n> Similarly, in use-cases such as multi-party-funded Lightning channels(5), an\n> attacker can always DoS attack the protocol by participating in a channel open,\n> and then failing to allow payments to be routed through it. The solution is\n> again to use economics to ensure the attack is sufficiently costly.\n> \n> However, under the right circumstances double-spends are an unusually powerful\n> DoS attack on multi-party, multi-input, transaction. When mempool demand is\n> high, low fee transactions can take arbitrarily long to get mined. Bitcoin has\n> seen periods of mempool demand where low-fee transactions would take months\n> to get mined. Transaction expiry does not solve this problem, as anyone can\n> rebroadcast transactions at any time. In these circumstances without\n> transaction replacement a multi-party transaction such as a Wasabi coinjoin\n> could be held up indefinitely by a low-fee double-spend.\n> \n> \n> ## How Full-RBF Mitigates the Double-Spend DoS Attack\n> \n> Modulo tx-pinning, full-rbf mitigates the double-spend DoS attack in a very\n> straightforward way: the low fee transaction is replaced by the higher fee\n> transaction, resulting in the latter getting mined in a reasonable amount of\n> time and the protocol making forward progress.\n> \n> Note that the converse is not a useful attack: if the attacker broadcasts a\n> high-fee double spend, higher than the intended multi-party transaction, the\n> transaction will get mined in a reasonable amount of time, costing the attacker\n> money and the defender nothing beyond wasted time. Multi-party protocols always\n> have the property that attackers can spend money to DoS attack by creating more\n> UTXOs/identities/etc, so this isn't any worse than the status quo!\n> \n> \n> ## Transaction Pinning\n> \n> So what about transaction pinning? The term actually refers to a few different\n> techniques that can make it difficult/expensive to fee-bump a transaction.\n> We're interested in the techniques relevant to replacements, namely\n> exploitation of:\n> \n> 1. BIP-125 RBF Rule #3: a replacement transaction is required to pay\n> the higher absolute fee (not just fee rate) than the sum of fees paid by all\n> transactions being replaced.\n> \n> 2. BIP-125 RBF Rule #5: the number of transactions replaced at one time must\n> not exceed 100. Note that this rule only exists due to limitations of the\n> existing Bitcoin Core implementation; it has absolute no economic rational and\n> should be removed by either improving the implementation's scalability issues,\n> or rejecting transactions that could make a transaction unreplaceable(2).\n> \n> Exploiting either rule is expensive. To exploit rule #3 the attacker has to\n> broadcast fee-paying transactions paying a total amount of fees higher than the\n> defender is willing to pay. Since transactions don't expire, in almost all\n> circumstances those transactions will eventually be mined, costing the attacker\n> much more money than they would have spent without full-rbf.\n> \n> To exploit rule #5, the attacker has to broadcast 100x more fee-paying\n> transactions than they otherwise would have. As with rule #3, those\n> transactions will almost always eventually be mined, costing the attacker\n> significantly more money than they would have spent without full-rbf. And, as\n> mentioned above(2), rule #5 is merely an artifact of the existing\n> implementation which can and should be fixed.\n> \n> The only avenue for an attacker to avoid transaction pinning costs is\n> amortisation: reusing the extra transactions required for pinning for other\n> attacks/other purposes. But of course, amortisation is already a potent cost\n> reduction strategy for attacks on multi-party protocols such as coinjoin, so\n> the existence of transaction pinning doesn't appreciably change the situation.\n> Again, there are mitigations such as requiring participants to post nLockTime'd\n> fee-paying transactions(3), and limiting attacks to parties who are heavily\n> invested in Bitcoin for other reasons is a valuable improvement on the status\n> quo.\n> \n> \n> # Conclusion\n> \n> Far from not \"offering any benefits other than breaking zeroconf business\n> practices\"(4), full-rbf clearly improves Bitcoin for multi-party protocols,\n> among the many other reasons to adopt it.\n> \n> \n> # Footnotes\n> \n> 1. What do I mean by \"exclusively controlled\"? Let's compare coinjoin, to an\n> ordinary single-payer Lightning channel. In a coinjoin, the goal is to get a\n> transaction mined containing multiple inputs from different parties. Each of\n> these inputs is individually, exclusively controlled by a single party:\n> without the cooperation of any other party that input that be spend. This is\n> unlike an ordinary single-payer Lightning channel: while the commitment\n> transactions are multi-party transactions, the multisig transaction outputs\n> involved are jointly controlled by both parties, and thus neither party can\n> spend it without the cooperation of the other at some point.\n> \n> 2. [bitcoin-dev] Removing BIP-125 Rule #5 Pinning with the Always-Replaceable\n> Invariant, https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-November/021175.html\n> \n> 3. [bitcoin-dev] Using Full-RBF to fix BIP-125 Rule #3 Pinning with nLockTime,\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-November/021176.html\n> \n> 4. https://github.com/bitcoin/bitcoin/pull/26438\n> \n> 5. There are even more exotic proposed Lightning-related protocols where a failure\n> of transaction replacement can cause the loss of funds. I'm not covering\n> those scenarios because they have such strong requirements - beyond what\n> full-rbf offers - that the technical community does not have consensus that\n> these proposed protocols are even viable.\n> \n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Peter Todd",
                "date": "2023-01-10T10:03:16",
                "message_text_only": "On Tue, Jan 10, 2023 at 09:19:39AM +0000, alicexbt wrote:\n> Hi Peter,\n> \n> > ## How Full-RBF Mitigates the Double-Spend DoS Attack\n> > \n> > Modulo tx-pinning, full-rbf mitigates the double-spend DoS attack in a very\n> > straightforward way: the low fee transaction is replaced by the higher fee\n> > transaction, resulting in the latter getting mined in a reasonable amount of\n> > time and the protocol making forward progress.\n> \n> Asking this question based on a [discussion on twitter][0]. How would you get extra sats to increase the fees?\n\nYou're misunderstanding the issue. There is no need for extra sats to increase\nfees. Coinjoin transactions already have fees set at a level at which you'd\nexpect them to be mined in a reasonable amount of time. Full-RBF ensures that,\nmodulo tx pinning, either the coinjoin gets mined, or any double-spend has to\nhave a high enough feerate that it will be mined in a reasonable amount of time\nas well.\n\n> It seems this would be possible with Joinmarket, Wasabi and even joinstr although things would get worse for Whirlpool. Whirlpool coinjoin transactions do not signal BIP 125 RBF so they were not replaceable earlier\n\nBringing up Whirlpool here is silly. Everyone knows Samourai has made, at best,\nsome rather insane technical decisions. Quite likely downright malicious with\ntheir xpub collection. Their opinion isn't relevant. Cite reputable sources.\n\nAnyway, Wasabi would like to move to making coinjoins opt-in to RBF. Though\nfull-rbf may come sooner; for technical reasons opt-in RBF is ugly to implement\nnow as activation needs to be coordinated accross all clients:\n\nhttps://github.com/zkSNACKs/WalletWasabi/issues/9041#issuecomment-1376653020\n\n> however attacker would be able to perform DoS attacks now by double spending their inputs used in coinjoin.\n\nAs I explained, attackers can already do this with or without full-rbf simply\nby picking the right time to broadcast the double spend. It's not an effective\nattack anyway: with a UTXO you can already hold up a coinjoin round by simply\nfailing to complete stage #2 of the coinjoin. Actually doing a double-spend\nsimply guarantees that you're spending money on it. It's only effective with\nlow-fee double-spends in the absence of full-rbf.\n\n> [0]: https://twitter.com/dammkewl/status/1599692908860706818\n\nThis tweet is nuts. Eg \"Gives well connected mining pools an added advantage\"\nis simply false. Full-RBF does the exact opposite.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230110/60b6077c/attachment.sig>"
            },
            {
                "author": "alicexbt",
                "date": "2023-01-10T17:10:37",
                "message_text_only": "Hi Peter,\n\n> Bringing up Whirlpool here is silly. Everyone knows Samourai has made, at best,\n> some rather insane technical decisions. Quite likely downright malicious with\n> their xpub collection. Their opinion isn't relevant. Cite reputable sources.\n\nI didn't want this thread to become a wasabi vs samourai debate instead wanted to focus on full-rbf and how it affects different coinjoin implementations. Samourai wallet can be used with [dojo][0] that includes full node and Whirlpool can be used in [sparrow Wallet][1] as well. There are several reasons to not use wasabi and consider their opinion irrelevant. Wasabi has many privacy issues including address reuse and consolidation in a coinjoin tx. They completely lost their reputation after deciding to work with chain analysis firms that help governments for censorship of some UTXOs.\n\nEven _nothingmuch_ who has contributed to Wasabi's coinjoin implementation has [no major issues][2] with whirlpool if used properly. Some [tweets][3] in this thread even show their incompetence and major issues with wabisabi.\n\nAnyway thanks for responding to other things I mentioned in last email.\n\n\n[0]: https://code.samourai.io/dojo/samourai-dojo\n[1]: https://sparrowwallet.com/docs/mixing-whirlpool.html\n[2]: https://twitter.com/search?lang=en&q=whirlpool%20(from%3AmHaGqnOACyFm0h5)&src=typed_query\n[3]: https://twitter.com/mHaGqnOACyFm0h5/status/1538748210210013184\n\n\n/dev/fd0\nfloppy disc guy\n\nSent with Proton Mail secure email.\n\n------- Original Message -------\nOn Tuesday, January 10th, 2023 at 3:33 PM, Peter Todd <pete at petertodd.org> wrote:\n\n\n> On Tue, Jan 10, 2023 at 09:19:39AM +0000, alicexbt wrote:\n> \n> > Hi Peter,\n> > \n> > > ## How Full-RBF Mitigates the Double-Spend DoS Attack\n> > > \n> > > Modulo tx-pinning, full-rbf mitigates the double-spend DoS attack in a very\n> > > straightforward way: the low fee transaction is replaced by the higher fee\n> > > transaction, resulting in the latter getting mined in a reasonable amount of\n> > > time and the protocol making forward progress.\n> > \n> > Asking this question based on a discussion on twitter. How would you get extra sats to increase the fees?\n> \n> \n> You're misunderstanding the issue. There is no need for extra sats to increase\n> fees. Coinjoin transactions already have fees set at a level at which you'd\n> expect them to be mined in a reasonable amount of time. Full-RBF ensures that,\n> modulo tx pinning, either the coinjoin gets mined, or any double-spend has to\n> have a high enough feerate that it will be mined in a reasonable amount of time\n> as well.\n> \n> > It seems this would be possible with Joinmarket, Wasabi and even joinstr although things would get worse for Whirlpool. Whirlpool coinjoin transactions do not signal BIP 125 RBF so they were not replaceable earlier\n> \n> \n> Bringing up Whirlpool here is silly. Everyone knows Samourai has made, at best,\n> some rather insane technical decisions. Quite likely downright malicious with\n> their xpub collection. Their opinion isn't relevant. Cite reputable sources.\n> \n> Anyway, Wasabi would like to move to making coinjoins opt-in to RBF. Though\n> full-rbf may come sooner; for technical reasons opt-in RBF is ugly to implement\n> now as activation needs to be coordinated accross all clients:\n> \n> https://github.com/zkSNACKs/WalletWasabi/issues/9041#issuecomment-1376653020\n> \n> > however attacker would be able to perform DoS attacks now by double spending their inputs used in coinjoin.\n> \n> \n> As I explained, attackers can already do this with or without full-rbf simply\n> by picking the right time to broadcast the double spend. It's not an effective\n> attack anyway: with a UTXO you can already hold up a coinjoin round by simply\n> failing to complete stage #2 of the coinjoin. Actually doing a double-spend\n> simply guarantees that you're spending money on it. It's only effective with\n> low-fee double-spends in the absence of full-rbf.\n> \n> \n> This tweet is nuts. Eg \"Gives well connected mining pools an added advantage\"\n> is simply false. Full-RBF does the exact opposite.\n> \n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org"
            },
            {
                "author": "Peter Todd",
                "date": "2023-01-13T23:46:58",
                "message_text_only": "On Tue, Jan 10, 2023 at 05:10:37PM +0000, alicexbt via bitcoin-dev wrote:\n> Hi Peter,\n> \n> > Bringing up Whirlpool here is silly. Everyone knows Samourai has made, at best,\n> > some rather insane technical decisions. Quite likely downright malicious with\n> > their xpub collection. Their opinion isn't relevant. Cite reputable sources.\n> \n> I didn't want this thread to become a wasabi vs samourai debate instead wanted to focus on full-rbf and how it affects different coinjoin implementations. Samourai wallet can be used with [dojo][0] that includes full node and Whirlpool can be used in [sparrow Wallet][1] as well. There are several reasons to not use wasabi and consider their opinion irrelevant. Wasabi has many privacy issues including address reuse and consolidation in a coinjoin tx.\n\nLol, the \"address reuse\" thing I actually mentioned in my email. Avoiding\naddress reuse from user errors like loading the same seed into different\nwallets isn't realistic, and it has no real impact on other users.\n\nNo reasonable person things Samourai's default option of uploading xpubs is\nsane. The debate here is over and arguing otherwise is just wasting everyone's\ntime on this mailing list.\n\n> They completely lost their reputation after deciding to work with chain analysis firms that help governments for censorship of some UTXOs.\n\nCitation on them working with chain analysis firms? Or did they just roll their\nown blacklist?\n\nAnyway, the blacklisting is just a bit of cowardness. That's not a big deal.\nLots of Bitcoin entitites have done the cowardly thing and implemented\nblacklists on the suggestion of their lawyers.  We're probably better off if we\ndon't set the bar so high that while you're risking jail time implementing\ncoinjoin, we demand you to take even more risks by not implementing some mostly\nsymbolic blacklists that affect hardly anyone.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230113/b1f4363e/attachment-0001.sig>"
            }
        ],
        "thread_summary": {
            "title": "Why Full-RBF Makes DoS Attacks on Multiparty Protocols Significantly More Expensive",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "alicexbt",
                "David A. Harding",
                "Peter Todd"
            ],
            "messages_count": 11,
            "total_messages_chars_count": 41439
        }
    },
    {
        "title": "[bitcoin-dev] SIGHASH_GROUP vs Ephemeral anchors",
        "thread_messages": [
            {
                "author": "Anthony Towns",
                "date": "2023-01-11T08:00:14",
                "message_text_only": "Hello world,\n\nI think it's interesting to compare SIGHASH_GROUP [0] and Ephemeral\nanchors [1].\n\n[0] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-July/019243.html\n[1] https://github.com/bitcoin/bitcoin/pull/26403\n\nSIGHASH_GROUP is the idea that you provide a way for the inputs of a\ntransaction to divide the outputs of a transaction into non-overlapping\ngroups. So input 1 can specify \"I'm starting a group of 3 outputs\",\ninput 2 can specify \"I'm using the same group as the previous input\",\ninput 3 can specify \"I'm starting a group of 2 outputs\"; and each input\ncan use the SIGHASH_GROUP flag to specify their signature is signing\nfor the subgroup they've specified, rather than just a single output,\nor all of them.\n\nThe idea behind this is that then you can use a signature to link a\nset of inputs and outputs via a signature in a way that's more general\nthan SIGHASH_ANYONECANPAY (since you can have many inputs attesting to\nthe same subset of outputs), SIGHASH_SINGLE (since you can attest to\nmultiple outputs), and SIGHASH_ALL (since you don't have to attest to\nall outputs). This means that (eg) you can have a tx closing a lightning\nchannel commit to a dozen outputs that specify where the channel's funds\nend up, but are also able to add additional inputs to cover fees, and\nadditional outputs to collect the change from those fees.\n\nEphemeral anchors, by contrast, are just a realy policy level rule that a\ntransaction may create a single 0-value output with sPK of OP_TRUE (the\n\"ephemeral anchor\"), and that that tx won't be rejected as being dust,\nprovided that the tx that introduces the anchor pays 0 fees (so it is not\nprofitable to mine on its own) and that it's relayed as a package with\nanother tx that spends that anchor. (And there are additional proposed\nrules beyond those)\n\nTo some degree, this provides an alternative way of getting the same\nbenefits of SIGHASH_GROUP: if you were constructing a transaction\nconsisting of {i1,i2,i3,i4,f} -> {o1,o2,o3,c} with {i1,i2,i3} committing to\n{o1} and {i4} committing to {o2,o3} and f providing the fees with c\ncollecting the change, you could instead create three transactions:\n\n   {i1,i2,i3} -> {o1, eph1}\n   {i4} -> {o2,o3, eph2}\n   {eph1, eph2, f} -> {c}\n\n(where eph1/eph2 are ephemeral anchors) and instead of signing with\nSIGHASH_GROUP, you'd just sign with SIGHASH_ALL.\n\n(This is likewise similar to the \"sponsored transactions\" concept [2],\nwhere a transaction may \"sponsor\" another transaction, meaning it cannot\nbe included in a block unless the transaction it sponsors is also included\nin the block. Given the \"txs-may-only-have-one-sponsor\" rule, ephemeral\nanchors could be considered as \"you can design a tx that always has a\nsponsor, or never has a sponsor\")\n\n[2] https://bitcoinops.org/en/newsletters/2020/09/23/#transaction-fee-sponsorship\n\nEphemeral anchors aren't a complete replacement for SIGHASH_GROUP --\nif i1 had two signatures, one signing with SIGHASH_GROUP, but the other\nsigning with SIGHASH_ALL, then it's difficult to duplicate that behaviour\nexactly with ephemeral anchors. However, it's likely the only benefit\nto using SIGHASH_ALL there is to reduce malleability risk, and ephemeral\nanchors probably already achieve that.\n\nAdditionally, if the value of i1+i2+i3 was less than o1 or i4 was less\nthan o2+o3, then the introduction of f is too late to compensate for\nthat with ephemeral anchors, but would have been fine with SIGHASH_GROUP.\n\nThe detailed proposed rules for ephemeral anchors as they stand are,\nI think:\n\n> A transaction with one or more CScript([OP_2]) output MUST:\n>  eph.1) Be 0-fee\n>  eph.2) Have [the ephemeral anchor output] spent in the same memppol relay\n>         package\n>  eph.3) Be nversion==3\n>  eph.4) Must not have more than one [such output]\n\n - https://github.com/bitcoin/bitcoin/pull/26403/commits/431a5e3e0376d8bf55563a0168e79dd73b04a1f8\n\nAnd implied by \"nversion==3\":\n\n> v3.1) A V3 transaction can be replaced, [...]\n> v3.2) Any descendant of an unconfirmed V3 transaction must also be V3.\n> v3.3) A V3 transaction's unconfirmed ancestors must all be V3.\n> v3.4) A V3 transaction cannot have more than 1 descendant.\n> v3.5) A V3 transaction that has an unconfirmed V3 ancestor cannot be\n>    larger than 1000 virtual bytes.\n> v3.4b) A V3 transaction cannot have more than 1 ancestor\n\n - https://github.com/bitcoin/bitcoin/blob/0c089a327a70d16f824b1b4dfd029d260cc43f09/doc/policy/version3_transactions.md\n\nThe v3.4b rule unfortunately prevents ephemeral anchors from being used\nto provide fees for multiple input/output groups in the way I suggest\nabove. That's intended to prevent attaching large ancestors to a package,\nallowing the descendent to be high fee / low feerate, thus preventing\nthat descendant from both being replaced (due to requiring a higher\nabsolute fee) and mined (due to having a low fee rate). (I suspect the\nonly way to remove that restriction without reinstating the pinning\nvector is to allow replacements that have a higher fee rate, even though\nthey have a lower absolute fee)\n\nAnyway, in theory, I think ephemeral anchors get most of the potential\nbenefits of SIGHASH_GROUP, particularly if the (v3.4b) rule can be removed\nor loosened somehow. And it's obviously much less costly to implement:\nit's relay policy only, rather than a consensus change; and it only\noperates at the transaction level, so we don't have to start worrying\nabout pinning of inputs vs whole transactions.\n\nCheers,\naj"
            },
            {
                "author": "Antoine Riard",
                "date": "2023-01-12T02:06:21",
                "message_text_only": "Hi AJ,\n\n> The idea behind this is that then you can use a signature to link a\n> set of inputs and outputs via a signature in a way that's more general\n> than SIGHASH_ANYONECANPAY (since you can have many inputs attesting to\n> the same subset of outputs), SIGHASH_SINGLE (since you can attest to\n> multiple outputs), and SIGHASH_ALL (since you don't have to attest to\n> all outputs). This means that (eg) you can have a tx closing a lightning\n> channel commit to a dozen outputs that specify where the channel's funds\n> end up, but are also able to add additional inputs to cover fees, and\n> additional outputs to collect the change from those fees.\n\nTo precise more one of the use-case for SIGHASH_GROUP, you can have one LSP\nwith hundreds of Lightning channels opened with as much (mobile)\ncounterparties, of which the majority are probably offline most of their\ntimes to aggregate the LN commitment transaction in a single bundle with\none pair of input/output. Aggregation should be non-interactive,\nfee-savings from a L2 viewpoint would be all the saved anchor outputs,\nblockspace-savings from a full-node viewpoint would be those same anchor\noutputs.\n\n> To some degree, this provides an alternative way of getting the same\n> benefits of SIGHASH_GROUP: if you were constructing a transaction\n> consisting of {i1,i2,i3,i4,f} -> {o1,o2,o3,c} with {i1,i2,i3} committing\nto\n> {o1} and {i4} committing to {o2,o3} and f providing the fees with c\n> collecting the change, you could instead create three transactions:\n>\n>    {i1,i2,i3} -> {o1, eph1}\n>    {i4} -> {o2,o3, eph2}\n>    {eph1, eph2, f} -> {c}\n\nI think here a SIGHASH_GROUP-like would be: {i1, i2, i3, i4 i.f, o1, o2,\no3, o.f}. Where `i.f` is the input for feeding external value in the\nbundle, `o.f` the output for output fees.\n\nCompared to anchors, I believe you're saving 1-input/2-outputs of fees for\na construction expressing the same semantics ?\n\n> However, it's likely the only benefit to using SIGHASH_ALL there is to\nreduce\n> malleability risk, and ephemeral anchors probably already achieve that.\n\nIf my understanding is correct with ephemeral anchors, we allow third-party\nmalleability (i.e a entity not owning any key in the funding channel\noutput) of the chain of transactions. If nversion=3 is robust against\n\"classic\" pinnings at the commitment\ntransaction-level by a counterparty (scenario 2b in [0] iirc), it should\nhold against external parties. However, it might introduce issues, where a\ncommon CPFP is conflicted on one of its input, e.g in the example above\neph1 is replaced by  malicious better fee/feerate eph1', cancelling {i4,\no2, o3} fee-bumping.\n\n[0]\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2020-June/002758.html\n\n> The v3.4b rule unfortunately prevents ephemeral anchors from being used\n> to provide fees for multiple input/output groups in the way I suggest\n> above\n\nSee point above, as providing fees for multiple input/output groups *might*\nbe unsafe, so I think this is a limited downside anyway.\n\n> (I suspect the only way to remove that restriction without reinstating\nthe pinning\n> vector is to allow replacements that have a higher fee rate, even though\n> they have a lower absolute fee)\n\n>From my memory, I think this is correct -- Though now you're introducing\nthe issue where one might be able to downgrade the fee content of your\nminer mempool in a period of emptiness. However, I believe if we move to a\nhigher fee rate only, it might make\nthe cost of the replacement issue above.\n\n> Anyway, in theory, I think ephemeral anchors get most of the potential\n> benefits of SIGHASH_GROUP, particularly if the (v3.4b) rule can be removed\n> or loosened somehow. And it's obviously much less costly to implement:\n> it's relay policy only, rather than a consensus change; and it only\n> operates at the transaction level, so we don't have to start worrying\n> about pinning of inputs vs whole transactions.\n\nYes I think the only clear benefit of SIGHASH_GROUP over ephemeral anchors\nis the fee/blockspace savings for some types of LN deployments. Comes at\nfar higher engineering resources as it's a consensus change rather than\nrelay policy only. However, in the future, if it is combined with other\nchanges like malleability of the output amounts, it could unlock use-cases\nlike \"dynamic value reallocation\" from unknown initial sending.\n\nBest,\nAntoine\n\nLe mer. 11 janv. 2023 \u00e0 03:00, Anthony Towns via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Hello world,\n>\n> I think it's interesting to compare SIGHASH_GROUP [0] and Ephemeral\n> anchors [1].\n>\n> [0]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-July/019243.html\n> [1] https://github.com/bitcoin/bitcoin/pull/26403\n>\n> SIGHASH_GROUP is the idea that you provide a way for the inputs of a\n> transaction to divide the outputs of a transaction into non-overlapping\n> groups. So input 1 can specify \"I'm starting a group of 3 outputs\",\n> input 2 can specify \"I'm using the same group as the previous input\",\n> input 3 can specify \"I'm starting a group of 2 outputs\"; and each input\n> can use the SIGHASH_GROUP flag to specify their signature is signing\n> for the subgroup they've specified, rather than just a single output,\n> or all of them.\n>\n> The idea behind this is that then you can use a signature to link a\n> set of inputs and outputs via a signature in a way that's more general\n> than SIGHASH_ANYONECANPAY (since you can have many inputs attesting to\n> the same subset of outputs), SIGHASH_SINGLE (since you can attest to\n> multiple outputs), and SIGHASH_ALL (since you don't have to attest to\n> all outputs). This means that (eg) you can have a tx closing a lightning\n> channel commit to a dozen outputs that specify where the channel's funds\n> end up, but are also able to add additional inputs to cover fees, and\n> additional outputs to collect the change from those fees.\n>\n> Ephemeral anchors, by contrast, are just a realy policy level rule that a\n> transaction may create a single 0-value output with sPK of OP_TRUE (the\n> \"ephemeral anchor\"), and that that tx won't be rejected as being dust,\n> provided that the tx that introduces the anchor pays 0 fees (so it is not\n> profitable to mine on its own) and that it's relayed as a package with\n> another tx that spends that anchor. (And there are additional proposed\n> rules beyond those)\n>\n> To some degree, this provides an alternative way of getting the same\n> benefits of SIGHASH_GROUP: if you were constructing a transaction\n> consisting of {i1,i2,i3,i4,f} -> {o1,o2,o3,c} with {i1,i2,i3} committing to\n> {o1} and {i4} committing to {o2,o3} and f providing the fees with c\n> collecting the change, you could instead create three transactions:\n>\n>    {i1,i2,i3} -> {o1, eph1}\n>    {i4} -> {o2,o3, eph2}\n>    {eph1, eph2, f} -> {c}\n>\n> (where eph1/eph2 are ephemeral anchors) and instead of signing with\n> SIGHASH_GROUP, you'd just sign with SIGHASH_ALL.\n>\n> (This is likewise similar to the \"sponsored transactions\" concept [2],\n> where a transaction may \"sponsor\" another transaction, meaning it cannot\n> be included in a block unless the transaction it sponsors is also included\n> in the block. Given the \"txs-may-only-have-one-sponsor\" rule, ephemeral\n> anchors could be considered as \"you can design a tx that always has a\n> sponsor, or never has a sponsor\")\n>\n> [2]\n> https://bitcoinops.org/en/newsletters/2020/09/23/#transaction-fee-sponsorship\n>\n> Ephemeral anchors aren't a complete replacement for SIGHASH_GROUP --\n> if i1 had two signatures, one signing with SIGHASH_GROUP, but the other\n> signing with SIGHASH_ALL, then it's difficult to duplicate that behaviour\n> exactly with ephemeral anchors. However, it's likely the only benefit\n> to using SIGHASH_ALL there is to reduce malleability risk, and ephemeral\n> anchors probably already achieve that.\n>\n> Additionally, if the value of i1+i2+i3 was less than o1 or i4 was less\n> than o2+o3, then the introduction of f is too late to compensate for\n> that with ephemeral anchors, but would have been fine with SIGHASH_GROUP.\n>\n> The detailed proposed rules for ephemeral anchors as they stand are,\n> I think:\n>\n> > A transaction with one or more CScript([OP_2]) output MUST:\n> >  eph.1) Be 0-fee\n> >  eph.2) Have [the ephemeral anchor output] spent in the same memppol\n> relay\n> >         package\n> >  eph.3) Be nversion==3\n> >  eph.4) Must not have more than one [such output]\n>\n>  -\n> https://github.com/bitcoin/bitcoin/pull/26403/commits/431a5e3e0376d8bf55563a0168e79dd73b04a1f8\n>\n> And implied by \"nversion==3\":\n>\n> > v3.1) A V3 transaction can be replaced, [...]\n> > v3.2) Any descendant of an unconfirmed V3 transaction must also be V3.\n> > v3.3) A V3 transaction's unconfirmed ancestors must all be V3.\n> > v3.4) A V3 transaction cannot have more than 1 descendant.\n> > v3.5) A V3 transaction that has an unconfirmed V3 ancestor cannot be\n> >    larger than 1000 virtual bytes.\n> > v3.4b) A V3 transaction cannot have more than 1 ancestor\n>\n>  -\n> https://github.com/bitcoin/bitcoin/blob/0c089a327a70d16f824b1b4dfd029d260cc43f09/doc/policy/version3_transactions.md\n>\n> The v3.4b rule unfortunately prevents ephemeral anchors from being used\n> to provide fees for multiple input/output groups in the way I suggest\n> above. That's intended to prevent attaching large ancestors to a package,\n> allowing the descendent to be high fee / low feerate, thus preventing\n> that descendant from both being replaced (due to requiring a higher\n> absolute fee) and mined (due to having a low fee rate). (I suspect the\n> only way to remove that restriction without reinstating the pinning\n> vector is to allow replacements that have a higher fee rate, even though\n> they have a lower absolute fee)\n>\n> Anyway, in theory, I think ephemeral anchors get most of the potential\n> benefits of SIGHASH_GROUP, particularly if the (v3.4b) rule can be removed\n> or loosened somehow. And it's obviously much less costly to implement:\n> it's relay policy only, rather than a consensus change; and it only\n> operates at the transaction level, so we don't have to start worrying\n> about pinning of inputs vs whole transactions.\n>\n> Cheers,\n> aj\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230111/46655b50/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "SIGHASH_GROUP vs Ephemeral anchors",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Anthony Towns",
                "Antoine Riard"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 15985
        }
    },
    {
        "title": "[bitcoin-dev] Using OP_VAULT to improve DLCs",
        "thread_messages": [
            {
                "author": "Ben Carman",
                "date": "2023-01-12T12:32:06",
                "message_text_only": "Hi list,\n\nAfter reading through James's OP_VAULT proposal this week, I had a realization that this can be used for more than a deep cold storage wallet.\n\nInstead of vaulting and unvaulting, we can just send to a OP_UNVAULT output.\nWhen using OP_UNVAULT if we set the `recovery-spk-hash` to a burn address (ie OP_RETURN `<random value>`)\nand the `delay-period` to `0` we can use it as a not-so simple covenant with the `unvault-target-hash` being\nset to whatever output restrictions you want to create.\n\nGiven this we can recreate a lot of what CTV promises, one of my favorites being\n[Lloyd's improvement to DLCs](https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019808.html)\n(I recommend reading that first)\n\nA similiar construction could be done by creating a taproot tree similiar to LLoyd's construction with each leaf looking like:\n\n`<hash-of-burn-spk> 0 <CET-hash_i> OP_UNVAULT <CET_i> CHECKSIG`\n\nIn the same as Lloyd's proposal: when the oracle(s) reveals their attestations either party can combine them to get the secret key corresponding to `CET_i` and spend the coins to the CET (whose `unvault-target-hash`\nhash is `CET-hash`) which distributes the funds according to the contract.\n\n## Comparison\n\nCompared to the original CTV proposal, this should get all the same computational savings. However, it would use more blockchain space.\n\nThe main downside I see is our final spending script will be slightly larger.\nInstead of just having `<hash> OP_CTV` it will be replaced with `<hash> 0 <hash> OP_UNVAULT` (34 bytes extra, not including the witness discount).\nHowever, this may be negligible in the case of a DLC with many outcomes as a lot of the input size will be coming from the control block.\nThis also can always be skipped by doing a cooperative close of the DLC if the internal-key of the taproot tree can be spent using something like MuSig.\n\nI imagine a lot of the other applications for CTV can be recreated with OP_VAULT using this same trick.\n\n# Credits\n\n- Lloyd Fournier for the original proposal\n- James O'Beirne for the OP_VAULT proposal and giving me the idea to skip the intial OP_VAULT and just use OP_UNVAULT\n\n\n\nBest,\n\nbenthecarman\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230112/192b1f7f/attachment.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2023-01-19T22:42:43",
                "message_text_only": "That's an interesting mechanism. Since the goal of OP_VAULT was to avoid\nbeing another general covenant proposal, that avenue could be blocked by\nrequiring that for a transaction spending a utxo with a script using\nOP_UNVAULT, the script (or taproot tree) must *only* contain that one\nopcode call (perhaps with an escape hatch that OP_UNVAULT turns into a NOOP\nif that constraint isn't satisfied). If no other conditions can be placed\non the utxo, then the only relevant condition is the delay (and the\nprescribed output targets).\n\nEven with this restriction, it could be used for Jeremy Rubin's congestion\ncontrol transactions, which just commits to a list of future outputs, to be\nsent when the fee environment is cheaper.\n\nHowever, James mentioned adding <recovery-params> that includes a\nscriptPubKey for authorizing recovery. With that addition, OP_UNVAULT can\nbe used to do more general covenants by making `unvault-target-hash`\nunsatisfiable (set to some random number that isn't derived from a hash)\nthe delay wouldn't matter, but arbitrary conditions can be set on spending\nthe utxo to the \"recovery address\" which could be another OP_UNVAULT. It\nseems like that could be used as a general CTV-like covenant.\n\nOn Fri, Jan 13, 2023 at 2:04 PM Ben Carman via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi list,\n>\n> After reading through James's OP_VAULT proposal this week, I had a realization that this can be used for more than a deep cold storage wallet.\n>\n> Instead of vaulting and unvaulting, we can just send to a OP_UNVAULT output.\n> When using OP_UNVAULT if we set the `recovery-spk-hash` to a burn address (ie OP_RETURN `<random value>`)\n> and the `delay-period` to `0` we can use it as a not-so simple covenant with the `unvault-target-hash` being\n> set to whatever output restrictions you want to create.\n>\n> Given this we can recreate a lot of what CTV promises, one of my favorites being\n> [Lloyd's improvement to DLCs](https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019808.html)\n> (I recommend reading that first)\n>\n> A similiar construction could be done by creating a taproot tree similiar to LLoyd's construction with each leaf looking like:\n>\n> `<hash-of-burn-spk> 0 <CET-hash_i> OP_UNVAULT <CET_i> CHECKSIG`\n>\n> In the same as Lloyd's proposal: when the oracle(s) reveals their attestations either party can combine them to get the secret key corresponding to `CET_i` and spend the coins to the CET (whose `unvault-target-hash`\n> hash is `CET-hash`) which distributes the funds according to the contract.\n>\n> ## Comparison\n>\n> Compared to the original CTV proposal, this *should *get all the same computational savings. However, it would use more blockchain space.\n>\n> The main downside I see is our final spending script will be slightly larger.\n> Instead of just having `<hash> OP_CTV` it will be replaced with `<hash> 0 <hash> OP_UNVAULT` (34 bytes extra, not including the witness discount).\n> However, this may be negligible in the case of a DLC with many outcomes as a lot of the input size will be coming from the control block.\n> This also can always be skipped by doing a cooperative close of the DLC if the internal-key of the taproot tree can be spent using something like MuSig.\n>\n> I imagine a lot of the other applications for CTV can be recreated with OP_VAULT using this same trick.\n>\n> # Credits\n>\n> - Lloyd Fournier for the original proposal\n> - James O'Beirne for the OP_VAULT proposal and giving me the idea to skip the intial OP_VAULT and just use OP_UNVAULT\n>\n>\n>\n> Best,\n>\n> benthecarman\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230119/ca21c32f/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Using OP_VAULT to improve DLCs",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Billy Tetrud",
                "Ben Carman"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 6324
        }
    },
    {
        "title": "[bitcoin-dev] A proposal for Full RBF to not exclude Zero Conf use case",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2023-01-13T23:53:45",
                "message_text_only": "On Sun, Dec 18, 2022 at 10:06:15AM +0200, Daniel Lipshitz wrote:\n> GAP600 is not a trxs processor or liquidity provider we service merchants,\n> payment processors & non-custodial liquidity providers - our service is\n> purely the 0-conf enabling our clients to accept 0-conf. Clients access our\n> service via API - sending us the Trx hash & output address. Our service is\n> not based on AML/KYC it is purely an analysis of the Bitcoin network.\n\nI checked and to sign up for your service, you ask for the name, phone number,\nemail, and company name.\n\nThat is an example of AML/KYC. By learning the tx hash and output address, you\nlearn which addresses are associated with what real world entity is paying for\nyour service. You learning that information for what you claim is ~10% of all\ntransactions is a significant privacy concern. On that basiss alone, I would\nargue that full-rbf should be implemented specifically to destroy your business\nand stop the collection of that data.\n\n> I am not at liberty to share names of other services which have developed\n> their own 0-conf service - they include a payment processor on a gambling\n> platform which services multiple gambling operators, a standalone gaming\n> payment processor, and a payment processor recently I have come across. We\n> also do not have a significant presence in Asia - so I don't have\n> visibility there.\n\nNo, I asked you for information on what companies are actually using *your*\nservice. You claim to be involved with a huge % of all transactions. If that is\nin fact true, obviously it shouldn't be hard to provide some examples of\nmerchants using GAP600 to accept unconfirmed txs.\n\n> I don't see it being necessarily an either/or approach here. The risk\n> looking to be mitigated with FullRBF seems to be able to be mitigated with\n> FullRBF but with a swop limitation of at least the Inputs of Trx1 being in\n> Trx2 - no flagging required. Added to this all these trxs always have the\n> OptinRBF so if these platforms need to be able to recreate completely their\n> trxs they have that option as well. The option to Swop out or bump up trxs\n> seems to be well covered under those two options.\n\nYou are not correct. One of the most important use-cases for full-rbf is\nmulti-party transactions; adding that limitation to full-rbf negates that\nusecase. See my post on why full-rbf makes DoS attacks on multiparty protocols\nsignificantly more expensive:\n\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2023-January/021322.html\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230113/bcc6cae8/attachment.sig>"
            },
            {
                "author": "Daniel Lipshitz",
                "date": "2023-01-14T20:15:30",
                "message_text_only": "On Sat, Jan 14, 2023 at 1:53 AM Peter Todd <pete at petertodd.org> wrote:\n\n> On Sun, Dec 18, 2022 at 10:06:15AM +0200, Daniel Lipshitz wrote:\n> > GAP600 is not a trxs processor or liquidity provider we service\n> merchants,\n> > payment processors & non-custodial liquidity providers - our service is\n> > purely the 0-conf enabling our clients to accept 0-conf. Clients access\n> our\n> > service via API - sending us the Trx hash & output address. Our service\n> is\n> > not based on AML/KYC it is purely an analysis of the Bitcoin network.\n>\n> I checked and to sign up for your service, you ask for the name, phone\n> number,\n> email, and company name.\n>\n> That is an example of AML/KYC. By learning the tx hash and output address,\n> you\n> learn which addresses are associated with what real world entity is paying\n> for\n> your service. You learning that information for what you claim is ~10% of\n> all\n> transactions is a significant privacy concern. On that basiss alone, I\n> would\n> argue that full-rbf should be implemented specifically to destroy your\n> business\n> and stop the collection of that data.\n>\n> We have standard commercial information about the payment processors, non\ncustodial liquidity providers and merchants which become our clients - we\ndo not have any kyc/aml information or telephone number on who is sending\nour clients the bitcoin for deposit.  For us these are just bitcoin Trx\nwhich our clients choose to benefit from 0-conf deposit recognition. Our\nservice is provided via API with the only information our clients share\nwith us, regarding a specific bitcoin transaction, being public bitcoin\ninformation like trx hash and output address.\n\n> I am not at liberty to share names of other services which have developed\n> > their own 0-conf service - they include a payment processor on a gambling\n> > platform which services multiple gambling operators, a standalone gaming\n> > payment processor, and a payment processor recently I have come across.\n> We\n> > also do not have a significant presence in Asia - so I don't have\n> > visibility there.\n>\n> No, I asked you for information on what companies are actually using *your*\n> service. You claim to be involved with a huge % of all transactions. If\n> that is\n> in fact true, obviously it shouldn't be hard to provide some examples of\n> merchants using GAP600 to accept unconfirmed txs.\n>\n\nAs already posted here\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-December/021240.html\nMax CEO from Coinspaid who has provided Cpoinspaid address clusters, see\nlink, is available to discuss further and may choose to share further\ninformation on the merchants they support.\n\n>\n> > I don't see it being necessarily an either/or approach here. The risk\n> > looking to be mitigated with FullRBF seems to be able to be mitigated\n> with\n> > FullRBF but with a swop limitation of at least the Inputs of Trx1 being\n> in\n> > Trx2 - no flagging required. Added to this all these trxs always have the\n> > OptinRBF so if these platforms need to be able to recreate completely\n> their\n> > trxs they have that option as well. The option to Swop out or bump up\n> trxs\n> > seems to be well covered under those two options.\n>\n> You are not correct. One of the most important use-cases for full-rbf is\n> multi-party transactions; adding that limitation to full-rbf negates that\n> usecase. See my post on why full-rbf makes DoS attacks on multiparty\n> protocols\n> significantly more expensive:\n>\n>\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2023-January/021322.html\n\n\nI also note that there is ongoing debate as to the need for full RBF see\nhere\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2023-January/021331.html\n.\n\nThis seems to be an extreme edge case - with Opt-in RBF, FSS Full RBF and\ncommon sense - offering enough coverage to mitigate.\n\n0-conf although may not be liked by some actors in Bitcoin, is engaged with\nfree choice and understanding of the risks. 0-conf is a long standing and\nsignificant use case which should not be ignored. 0-conf demise should be\nviewed as being a major and unnecessary cost to FullRBF as currently\nimplemented.\n\n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230114/70440238/attachment-0001.html>"
            },
            {
                "author": "Daniel Lipshitz",
                "date": "2023-01-16T10:19:35",
                "message_text_only": "Some further clarity on our unique trx hashes queried to our platform, our\ninitial and followup numbers on unique trx hashes queried were not accurate\n- apologies. Bitcoin addresses queried and Usd value and unique were\naccurate. This is as a result of our platform viewing each queried bitcoin\naddress as a transaction from our point of view.\n\n November 2022\n  Total queried unique bitcoin address- circa 1.5m trxs\n  Unique Bitcoin trx hashes queried- circa 500k\n  USD value - circa 220m\n  December 2022\n   Total queried unique bitcoin address- circa 1.7m trxs\n   Unique Bitcoin trx hashes queried - circa 500k\n   USD value - circa 200m\n\nThere are further merchants and service providers who enable 0-conf on\nBitcoin who are not working via our platform - I do not know their numbers\nbut believe they are significant. 0-conf on Bitcoin with its understood\nrisks is a significant use case.\n\nFor third party clarification please see\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-December/021239.html\nand\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-December/021238.html\n________________________________\n\nDaniel Lipshitz\nGAP600| www.gap600.com\n\n\n\n\nOn Sat, Jan 14, 2023 at 10:15 PM Daniel Lipshitz <daniel at gap600.com> wrote:\n\n>\n>\n>\n> On Sat, Jan 14, 2023 at 1:53 AM Peter Todd <pete at petertodd.org> wrote:\n>\n>> On Sun, Dec 18, 2022 at 10:06:15AM +0200, Daniel Lipshitz wrote:\n>> > GAP600 is not a trxs processor or liquidity provider we service\n>> merchants,\n>> > payment processors & non-custodial liquidity providers - our service is\n>> > purely the 0-conf enabling our clients to accept 0-conf. Clients access\n>> our\n>> > service via API - sending us the Trx hash & output address. Our service\n>> is\n>> > not based on AML/KYC it is purely an analysis of the Bitcoin network.\n>>\n>> I checked and to sign up for your service, you ask for the name, phone\n>> number,\n>> email, and company name.\n>>\n>> That is an example of AML/KYC. By learning the tx hash and output\n>> address, you\n>> learn which addresses are associated with what real world entity is\n>> paying for\n>> your service. You learning that information for what you claim is ~10% of\n>> all\n>> transactions is a significant privacy concern. On that basiss alone, I\n>> would\n>> argue that full-rbf should be implemented specifically to destroy your\n>> business\n>> and stop the collection of that data.\n>>\n>> We have standard commercial information about the payment processors, non\n> custodial liquidity providers and merchants which become our clients - we\n> do not have any kyc/aml information or telephone number on who is sending\n> our clients the bitcoin for deposit.  For us these are just bitcoin Trx\n> which our clients choose to benefit from 0-conf deposit recognition. Our\n> service is provided via API with the only information our clients share\n> with us, regarding a specific bitcoin transaction, being public bitcoin\n> information like trx hash and output address.\n>\n> > I am not at liberty to share names of other services which have developed\n>> > their own 0-conf service - they include a payment processor on a\n>> gambling\n>> > platform which services multiple gambling operators, a standalone gaming\n>> > payment processor, and a payment processor recently I have come across.\n>> We\n>> > also do not have a significant presence in Asia - so I don't have\n>> > visibility there.\n>>\n>> No, I asked you for information on what companies are actually using\n>> *your*\n>> service. You claim to be involved with a huge % of all transactions. If\n>> that is\n>> in fact true, obviously it shouldn't be hard to provide some examples of\n>> merchants using GAP600 to accept unconfirmed txs.\n>>\n>\n> As already posted here\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-December/021240.html\n> Max CEO from Coinspaid who has provided Cpoinspaid address clusters, see\n> link, is available to discuss further and may choose to share further\n> information on the merchants they support.\n>\n>>\n>> > I don't see it being necessarily an either/or approach here. The risk\n>> > looking to be mitigated with FullRBF seems to be able to be mitigated\n>> with\n>> > FullRBF but with a swop limitation of at least the Inputs of Trx1 being\n>> in\n>> > Trx2 - no flagging required. Added to this all these trxs always have\n>> the\n>> > OptinRBF so if these platforms need to be able to recreate completely\n>> their\n>> > trxs they have that option as well. The option to Swop out or bump up\n>> trxs\n>> > seems to be well covered under those two options.\n>>\n>> You are not correct. One of the most important use-cases for full-rbf is\n>> multi-party transactions; adding that limitation to full-rbf negates that\n>> usecase. See my post on why full-rbf makes DoS attacks on multiparty\n>> protocols\n>> significantly more expensive:\n>>\n>>\n>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2023-January/021322.html\n>\n>\n> I also note that there is ongoing debate as to the need for full RBF see\n> here\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2023-January/021331.html\n> .\n>\n> This seems to be an extreme edge case - with Opt-in RBF, FSS Full RBF and\n> common sense - offering enough coverage to mitigate.\n>\n> 0-conf although may not be liked by some actors in Bitcoin, is engaged\n> with free choice and understanding of the risks. 0-conf is a long standing\n> and significant use case which should not be ignored. 0-conf demise should\n> be viewed as being a major and unnecessary cost to FullRBF as currently\n> implemented.\n>\n>> --\n>> https://petertodd.org 'peter'[:-1]@petertodd.org\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230116/f3948317/attachment.html>"
            },
            {
                "author": "Erik Aronesty",
                "date": "2023-01-17T17:07:54",
                "message_text_only": "> 0-conf on Bitcoin with its understood risks is a significant use case\n\nand that use case doesn't change, at all, with full rbf.   the risk profile\nwill, likely, remain the same.   observation of the fee paid, history of\ndoing business with the customer, transaction size are all needed.\n\nOn Mon, Jan 16, 2023 at 1:50 PM Daniel Lipshitz via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Some further clarity on our unique trx hashes queried to our platform, our\n> initial and followup numbers on unique trx hashes queried were not accurate\n> - apologies. Bitcoin addresses queried and Usd value and unique were\n> accurate. This is as a result of our platform viewing each queried bitcoin\n> address as a transaction from our point of view.\n>\n>  November 2022\n>   Total queried unique bitcoin address- circa 1.5m trxs\n>   Unique Bitcoin trx hashes queried- circa 500k\n>   USD value - circa 220m\n>   December 2022\n>    Total queried unique bitcoin address- circa 1.7m trxs\n>    Unique Bitcoin trx hashes queried - circa 500k\n>    USD value - circa 200m\n>\n> There are further merchants and service providers who enable 0-conf on\n> Bitcoin who are not working via our platform - I do not know their numbers\n> but believe they are significant. 0-conf on Bitcoin with its understood\n> risks is a significant use case.\n>\n> For third party clarification please see\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-December/021239.html\n> and\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-December/021238.html\n> ________________________________\n>\n> Daniel Lipshitz\n> GAP600| www.gap600.com\n>\n>\n>\n>\n> On Sat, Jan 14, 2023 at 10:15 PM Daniel Lipshitz <daniel at gap600.com>\n> wrote:\n>\n>>\n>>\n>>\n>> On Sat, Jan 14, 2023 at 1:53 AM Peter Todd <pete at petertodd.org> wrote:\n>>\n>>> On Sun, Dec 18, 2022 at 10:06:15AM +0200, Daniel Lipshitz wrote:\n>>> > GAP600 is not a trxs processor or liquidity provider we service\n>>> merchants,\n>>> > payment processors & non-custodial liquidity providers - our service is\n>>> > purely the 0-conf enabling our clients to accept 0-conf. Clients\n>>> access our\n>>> > service via API - sending us the Trx hash & output address. Our\n>>> service is\n>>> > not based on AML/KYC it is purely an analysis of the Bitcoin network.\n>>>\n>>> I checked and to sign up for your service, you ask for the name, phone\n>>> number,\n>>> email, and company name.\n>>>\n>>> That is an example of AML/KYC. By learning the tx hash and output\n>>> address, you\n>>> learn which addresses are associated with what real world entity is\n>>> paying for\n>>> your service. You learning that information for what you claim is ~10%\n>>> of all\n>>> transactions is a significant privacy concern. On that basiss alone, I\n>>> would\n>>> argue that full-rbf should be implemented specifically to destroy your\n>>> business\n>>> and stop the collection of that data.\n>>>\n>>> We have standard commercial information about the payment processors,\n>> non custodial liquidity providers and merchants which become our clients -\n>> we do not have any kyc/aml information or telephone number on who is\n>> sending our clients the bitcoin for deposit.  For us these are just bitcoin\n>> Trx which our clients choose to benefit from 0-conf deposit recognition.\n>> Our service is provided via API with the only information our clients share\n>> with us, regarding a specific bitcoin transaction, being public bitcoin\n>> information like trx hash and output address.\n>>\n>> > I am not at liberty to share names of other services which have\n>>> developed\n>>> > their own 0-conf service - they include a payment processor on a\n>>> gambling\n>>> > platform which services multiple gambling operators, a standalone\n>>> gaming\n>>> > payment processor, and a payment processor recently I have come\n>>> across. We\n>>> > also do not have a significant presence in Asia - so I don't have\n>>> > visibility there.\n>>>\n>>> No, I asked you for information on what companies are actually using\n>>> *your*\n>>> service. You claim to be involved with a huge % of all transactions. If\n>>> that is\n>>> in fact true, obviously it shouldn't be hard to provide some examples of\n>>> merchants using GAP600 to accept unconfirmed txs.\n>>>\n>>\n>> As already posted here\n>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-December/021240.html\n>> Max CEO from Coinspaid who has provided Cpoinspaid address clusters, see\n>> link, is available to discuss further and may choose to share further\n>> information on the merchants they support.\n>>\n>>>\n>>> > I don't see it being necessarily an either/or approach here. The risk\n>>> > looking to be mitigated with FullRBF seems to be able to be mitigated\n>>> with\n>>> > FullRBF but with a swop limitation of at least the Inputs of Trx1\n>>> being in\n>>> > Trx2 - no flagging required. Added to this all these trxs always have\n>>> the\n>>> > OptinRBF so if these platforms need to be able to recreate completely\n>>> their\n>>> > trxs they have that option as well. The option to Swop out or bump up\n>>> trxs\n>>> > seems to be well covered under those two options.\n>>>\n>>> You are not correct. One of the most important use-cases for full-rbf is\n>>> multi-party transactions; adding that limitation to full-rbf negates that\n>>> usecase. See my post on why full-rbf makes DoS attacks on multiparty\n>>> protocols\n>>> significantly more expensive:\n>>>\n>>>\n>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2023-January/021322.html\n>>\n>>\n>> I also note that there is ongoing debate as to the need for full RBF see\n>> here\n>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2023-January/021331.html\n>> .\n>>\n>> This seems to be an extreme edge case - with Opt-in RBF, FSS Full RBF and\n>> common sense - offering enough coverage to mitigate.\n>>\n>> 0-conf although may not be liked by some actors in Bitcoin, is engaged\n>> with free choice and understanding of the risks. 0-conf is a long standing\n>> and significant use case which should not be ignored. 0-conf demise should\n>> be viewed as being a major and unnecessary cost to FullRBF as currently\n>> implemented.\n>>\n>>> --\n>>> https://petertodd.org 'peter'[:-1]@petertodd.org\n>>>\n>> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230117/fd2d84ef/attachment-0001.html>"
            },
            {
                "author": "Daniel Lipshitz",
                "date": "2023-01-17T17:27:13",
                "message_text_only": "> > 0-conf on Bitcoin with its understood risks is a significant use case\n>\n> and that use case doesn't change, at all, with full rbf.   the risk\n> profile will, likely, remain the same.   observation of the fee paid,\n> history of doing business with the customer, transaction size are all\n> needed.\n>\n\nCurrently 0-conf recognition is done without any KYC on the payor, this\nincludes activities like, gaming, non-custodial trading and applications.\nIn general OptinRBF is not possible to offer 0-conf since as soon as it is\nrecognised it can be double spent. Full RBF would make all trxs just like\nOptinRBF. FullRBF but with FSS implemented will still enable 0-conf\nacceptance.\n\n>\n> On Mon, Jan 16, 2023 at 1:50 PM Daniel Lipshitz via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Some further clarity on our unique trx hashes queried to our platform,\n>> our initial and followup numbers on unique trx hashes queried were not\n>> accurate - apologies. Bitcoin addresses queried and Usd value and unique\n>> were accurate. This is as a result of our platform viewing each queried\n>> bitcoin address as a transaction from our point of view.\n>>\n>>  November 2022\n>>   Total queried unique bitcoin address- circa 1.5m trxs\n>>   Unique Bitcoin trx hashes queried- circa 500k\n>>   USD value - circa 220m\n>>   December 2022\n>>    Total queried unique bitcoin address- circa 1.7m trxs\n>>    Unique Bitcoin trx hashes queried - circa 500k\n>>    USD value - circa 200m\n>>\n>> There are further merchants and service providers who enable 0-conf on\n>> Bitcoin who are not working via our platform - I do not know their numbers\n>> but believe they are significant. 0-conf on Bitcoin with its understood\n>> risks is a significant use case.\n>>\n>> For third party clarification please see\n>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-December/021239.html\n>> and\n>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-December/021238.html\n>> ________________________________\n>>\n>> Daniel Lipshitz\n>> GAP600| www.gap600.com\n>>\n>>\n>>\n>>\n>> On Sat, Jan 14, 2023 at 10:15 PM Daniel Lipshitz <daniel at gap600.com>\n>> wrote:\n>>\n>>>\n>>>\n>>>\n>>> On Sat, Jan 14, 2023 at 1:53 AM Peter Todd <pete at petertodd.org> wrote:\n>>>\n>>>> On Sun, Dec 18, 2022 at 10:06:15AM +0200, Daniel Lipshitz wrote:\n>>>> > GAP600 is not a trxs processor or liquidity provider we service\n>>>> merchants,\n>>>> > payment processors & non-custodial liquidity providers - our service\n>>>> is\n>>>> > purely the 0-conf enabling our clients to accept 0-conf. Clients\n>>>> access our\n>>>> > service via API - sending us the Trx hash & output address. Our\n>>>> service is\n>>>> > not based on AML/KYC it is purely an analysis of the Bitcoin network.\n>>>>\n>>>> I checked and to sign up for your service, you ask for the name, phone\n>>>> number,\n>>>> email, and company name.\n>>>>\n>>>> That is an example of AML/KYC. By learning the tx hash and output\n>>>> address, you\n>>>> learn which addresses are associated with what real world entity is\n>>>> paying for\n>>>> your service. You learning that information for what you claim is ~10%\n>>>> of all\n>>>> transactions is a significant privacy concern. On that basiss alone, I\n>>>> would\n>>>> argue that full-rbf should be implemented specifically to destroy your\n>>>> business\n>>>> and stop the collection of that data.\n>>>>\n>>>> We have standard commercial information about the payment processors,\n>>> non custodial liquidity providers and merchants which become our clients -\n>>> we do not have any kyc/aml information or telephone number on who is\n>>> sending our clients the bitcoin for deposit.  For us these are just bitcoin\n>>> Trx which our clients choose to benefit from 0-conf deposit recognition.\n>>> Our service is provided via API with the only information our clients share\n>>> with us, regarding a specific bitcoin transaction, being public bitcoin\n>>> information like trx hash and output address.\n>>>\n>>> > I am not at liberty to share names of other services which have\n>>>> developed\n>>>> > their own 0-conf service - they include a payment processor on a\n>>>> gambling\n>>>> > platform which services multiple gambling operators, a standalone\n>>>> gaming\n>>>> > payment processor, and a payment processor recently I have come\n>>>> across. We\n>>>> > also do not have a significant presence in Asia - so I don't have\n>>>> > visibility there.\n>>>>\n>>>> No, I asked you for information on what companies are actually using\n>>>> *your*\n>>>> service. You claim to be involved with a huge % of all transactions. If\n>>>> that is\n>>>> in fact true, obviously it shouldn't be hard to provide some examples of\n>>>> merchants using GAP600 to accept unconfirmed txs.\n>>>>\n>>>\n>>> As already posted here\n>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-December/021240.html\n>>> Max CEO from Coinspaid who has provided Cpoinspaid address clusters, see\n>>> link, is available to discuss further and may choose to share further\n>>> information on the merchants they support.\n>>>\n>>>>\n>>>> > I don't see it being necessarily an either/or approach here. The risk\n>>>> > looking to be mitigated with FullRBF seems to be able to be mitigated\n>>>> with\n>>>> > FullRBF but with a swop limitation of at least the Inputs of Trx1\n>>>> being in\n>>>> > Trx2 - no flagging required. Added to this all these trxs always have\n>>>> the\n>>>> > OptinRBF so if these platforms need to be able to recreate completely\n>>>> their\n>>>> > trxs they have that option as well. The option to Swop out or bump up\n>>>> trxs\n>>>> > seems to be well covered under those two options.\n>>>>\n>>>> You are not correct. One of the most important use-cases for full-rbf is\n>>>> multi-party transactions; adding that limitation to full-rbf negates\n>>>> that\n>>>> usecase. See my post on why full-rbf makes DoS attacks on multiparty\n>>>> protocols\n>>>> significantly more expensive:\n>>>>\n>>>>\n>>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2023-January/021322.html\n>>>\n>>>\n>>> I also note that there is ongoing debate as to the need for full RBF see\n>>> here\n>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2023-January/021331.html\n>>> .\n>>>\n>>> This seems to be an extreme edge case - with Opt-in RBF, FSS Full RBF\n>>> and common sense - offering enough coverage to mitigate.\n>>>\n>>> 0-conf although may not be liked by some actors in Bitcoin, is engaged\n>>> with free choice and understanding of the risks. 0-conf is a long standing\n>>> and significant use case which should not be ignored. 0-conf demise should\n>>> be viewed as being a major and unnecessary cost to FullRBF as currently\n>>> implemented.\n>>>\n>>>> --\n>>>> https://petertodd.org 'peter'[:-1]@petertodd.org\n>>>>\n>>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230117/d31b7aba/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "A proposal for Full RBF to not exclude Zero Conf use case",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Erik Aronesty",
                "Daniel Lipshitz",
                "Peter Todd"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 26646
        }
    },
    {
        "title": "[bitcoin-dev] A new Bitcoin implementation integrated with Core Lightning",
        "thread_messages": [
            {
                "author": "Michael Folkson",
                "date": "2023-01-14T20:26:07",
                "message_text_only": "I tweeted this [0] back in November 2022.\n\n\"With the btcd bugs and the analysis paralysis on a RBF policy option in Core increasingly thinking @BitcoinKnots and consensus compatible forks of Core are the future. Gonna chalk that one up to another thing @LukeDashjr was right about all along.\"\n\nA new bare bones Knots style Bitcoin implementation (in C++/C) integrated with Core Lightning was a long term idea I had (and presumably many others have had) but the dysfunction on the Bitcoin Core project this week (if anything it has been getting worse over time, not better) has made me start to take the idea more seriously. It is clear to me that the current way the Bitcoin Core project is being managed is not how I would like an open source project to be managed. Very little discussion is public anymore and decisions seem to be increasingly made behind closed doors or in private IRC channels (to the extent that decisions are made at all). Core Lightning seems to have the opposite problem. It is managed effectively in the open (admittedly with fewer contributors) but doesn't have the eyeballs or the usage that Bitcoin Core does. Regardless, selfishly I at some point would like a bare bones Bitcoin and Lightning implementation integrated in one codebase. The Bitcoin Core codebase has collected a lot of cruft over time and the ultra conservatism that is needed when treating (potential) consensus code seems to permeate into parts of the codebase that no one is using, definitely isn't consensus code and should probably just be removed.\n\nThe libbitcoinkernel project was (is?) an attempt to extract the consensus engine out of Core but it seems like it won't achieve that as consensus is just too slippery a concept and Knots style consensus compatible codebase forks of Bitcoin Core seem to still the model. To what extent you can safely chop off this cruft and effectively maintain this less crufty fork of Bitcoin Core also isn't clear to me yet.\n\nThen there is the question of whether it makes sense to mix C and C++ code that people have different views on. C++ is obviously a superset of C but assuming this merging of Bitcoin Core and Core Lightning is/was the optimal final destination it surely would have been better if Core Lightning was written in the same language (i.e. with classes) as Bitcoin Core.\n\nI'm just floating the idea to (hopefully) hear from people who are much more familiar with the entirety of the Bitcoin Core and Core Lightning codebases. It would be an ambitious long term project but it would be nice to focus on some ambitious project(s) (even if just conceptually) for a while given (thankfully) there seems to be a lull in soft fork activation chaos.\n\nThanks\nMichael\n\n[0]: https://twitter.com/michaelfolkson/status/1589220155006910464?s=20&t=GbPm7w5BqS7rS3kiVFTNcw\n\n--\nMichael Folkson\nEmail: michaelfolkson at [protonmail.com](http://protonmail.com/)\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230114/306cd5ac/attachment-0001.html>"
            },
            {
                "author": "alicexbt",
                "date": "2023-01-15T13:04:05",
                "message_text_only": "Hi Michael,\n\nIf I were to fork bitcoin core and maintain an implementation, I wouldn't integrate any lightning implementation with it. Instead remove some things from bitcoin core and keep it simple. There is also scope for improving privacy. Example: https://github.com/bitcoinknots/bitcoin/issues/50\n\nYou might find the commits in this branch interesting if you are looking to remove things from bitcoin core and maintain an implementation with no gui, wallet, less RPCs etc.\n\nhttps://github.com/jeremyRubin/bitcoin/commits/delete-it-all\n\n\n/dev/fd0\nfloppy disc guy\n\n\nSent with Proton Mail secure email.\n\n------- Original Message -------\nOn Sunday, January 15th, 2023 at 1:56 AM, Michael Folkson via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n\n> I tweeted this [0] back in November 2022.\n> \n> \"With the btcd bugs and the analysis paralysis on a RBF policy option in Core increasingly thinking @BitcoinKnots and consensus compatible forks of Core are the future. Gonna chalk that one up to another thing @LukeDashjr was right about all along.\"\n> \n> A new bare bones Knots style Bitcoin implementation (in C++/C) integrated with Core Lightning was a long term idea I had (and presumably many others have had) but the dysfunction on the Bitcoin Core project this week (if anything it has been getting worse over time, not better) has made me start to take the idea more seriously. It is clear to me that the current way the Bitcoin Core project is being managed is not how I would like an open source project to be managed. Very little discussion is public anymore and decisions seem to be increasingly made behind closed doors or in private IRC channels (to the extent that decisions are made at all). Core Lightning seems to have the opposite problem. It is managed effectively in the open (admittedly with fewer contributors) but doesn't have the eyeballs or the usage that Bitcoin Core does. Regardless, selfishly I at some point would like a bare bones Bitcoin and Lightning implementation integrated in one codebase. The Bitcoin Core codebase has collected a lot of cruft over time and the ultra conservatism that is needed when treating (potential) consensus code seems to permeate into parts of the codebase that no one is using, definitely isn't consensus code and should probably just be removed.\n> \n> The libbitcoinkernel project was (is?) an attempt to extract the consensus engine out of Core but it seems like it won't achieve that as consensus is just too slippery a concept and Knots style consensus compatible codebase forks of Bitcoin Core seem to still the model. To what extent you can safely chop off this cruft and effectively maintain this less crufty fork of Bitcoin Core also isn't clear to me yet.\n> \n> Then there is the question of whether it makes sense to mix C and C++ code that people have different views on. C++ is obviously a superset of C but assuming this merging of Bitcoin Core and Core Lightning is/was the optimal final destination it surely would have been better if Core Lightning was written in the same language (i.e. with classes) as Bitcoin Core.\n> \n> I'm just floating the idea to (hopefully) hear from people who are much more familiar with the entirety of the Bitcoin Core and Core Lightning codebases. It would be an ambitious long term project but it would be nice to focus on some ambitious project(s) (even if just conceptually) for a while given (thankfully) there seems to be a lull in soft fork activation chaos.\n> \n> Thanks\n> Michael\n> \n> [0]:\u00a0https://twitter.com/michaelfolkson/status/1589220155006910464?s=20&t=GbPm7w5BqS7rS3kiVFTNcw\n> \n> --\n> Michael Folkson\n> Email: michaelfolkson at protonmail.com\n> Keybase: michaelfolkson\n> PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3"
            }
        ],
        "thread_summary": {
            "title": "A new Bitcoin implementation integrated with Core Lightning",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "alicexbt",
                "Michael Folkson"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 6927
        }
    },
    {
        "title": "[bitcoin-dev] [Lightning-dev] A new Bitcoin implementation integrated with Core Lightning",
        "thread_messages": [
            {
                "author": "Fabian",
                "date": "2023-01-14T20:34:38",
                "message_text_only": "Hi Michael,\n\nhave you seen Mako? It might at least be a good start for what you would like to achieve: https://github.com/chjj/mako\n\nBest,\nFabian\n------- Original Message -------\nOn Saturday, January 14th, 2023 at 9:26 PM, Michael Folkson via Lightning-dev <lightning-dev at lists.linuxfoundation.org> wrote:\n\n> I tweeted this [0] back in November 2022.\n>\n> \"With the btcd bugs and the analysis paralysis on a RBF policy option in Core increasingly thinking @BitcoinKnots and consensus compatible forks of Core are the future. Gonna chalk that one up to another thing @LukeDashjr was right about all along.\"\n>\n> A new bare bones Knots style Bitcoin implementation (in C++/C) integrated with Core Lightning was a long term idea I had (and presumably many others have had) but the dysfunction on the Bitcoin Core project this week (if anything it has been getting worse over time, not better) has made me start to take the idea more seriously. It is clear to me that the current way the Bitcoin Core project is being managed is not how I would like an open source project to be managed. Very little discussion is public anymore and decisions seem to be increasingly made behind closed doors or in private IRC channels (to the extent that decisions are made at all). Core Lightning seems to have the opposite problem. It is managed effectively in the open (admittedly with fewer contributors) but doesn't have the eyeballs or the usage that Bitcoin Core does. Regardless, selfishly I at some point would like a bare bones Bitcoin and Lightning implementation integrated in one codebase. The Bitcoin Core codebase has collected a lot of cruft over time and the ultra conservatism that is needed when treating (potential) consensus code seems to permeate into parts of the codebase that no one is using, definitely isn't consensus code and should probably just be removed.\n>\n> The libbitcoinkernel project was (is?) an attempt to extract the consensus engine out of Core but it seems like it won't achieve that as consensus is just too slippery a concept and Knots style consensus compatible codebase forks of Bitcoin Core seem to still the model. To what extent you can safely chop off this cruft and effectively maintain this less crufty fork of Bitcoin Core also isn't clear to me yet.\n>\n> Then there is the question of whether it makes sense to mix C and C++ code that people have different views on. C++ is obviously a superset of C but assuming this merging of Bitcoin Core and Core Lightning is/was the optimal final destination it surely would have been better if Core Lightning was written in the same language (i.e. with classes) as Bitcoin Core.\n>\n> I'm just floating the idea to (hopefully) hear from people who are much more familiar with the entirety of the Bitcoin Core and Core Lightning codebases. It would be an ambitious long term project but it would be nice to focus on some ambitious project(s) (even if just conceptually) for a while given (thankfully) there seems to be a lull in soft fork activation chaos.\n>\n> Thanks\n> Michael\n>\n> [0]: https://twitter.com/michaelfolkson/status/1589220155006910464?s=20&t=GbPm7w5BqS7rS3kiVFTNcw\n>\n> --\n> Michael Folkson\n> Email: michaelfolkson at [protonmail.com](http://protonmail.com/)\n> Keybase: michaelfolkson\n> PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230114/acc6e73d/attachment.html>"
            },
            {
                "author": "Michael Folkson",
                "date": "2023-01-14T20:45:38",
                "message_text_only": "I saw it was announced, yes. The author is brilliant, he has now managed two alternative implementations of Core in two different languages :)\n\nThe problem though and why I and many others think the Knots style fork of Core is the better option is because you avoid reimplementing consensus code in a different language. If you're ultra conservative about consensus code you either want to run Core in parallel with your alternative implementation to check they don't go out of consensus or you want to run the same consensus code as Core in a Knots like fork. Hence a Knots like fork of Core in C++ integrated with Core Lightning in C seems like the better option to me for serious running in production like use cases.\n\n--\nMichael Folkson\nEmail: michaelfolkson at [protonmail.com](http://protonmail.com/)\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n\n------- Original Message -------\nOn Saturday, January 14th, 2023 at 20:34, Fabian <fjahr at protonmail.com> wrote:\n\n> Hi Michael,\n>\n> have you seen Mako? It might at least be a good start for what you would like to achieve: https://github.com/chjj/mako\n>\n> Best,\n> Fabian\n> ------- Original Message -------\n> On Saturday, January 14th, 2023 at 9:26 PM, Michael Folkson via Lightning-dev <lightning-dev at lists.linuxfoundation.org> wrote:\n>\n>> I tweeted this [0] back in November 2022.\n>>\n>> \"With the btcd bugs and the analysis paralysis on a RBF policy option in Core increasingly thinking @BitcoinKnots and consensus compatible forks of Core are the future. Gonna chalk that one up to another thing @LukeDashjr was right about all along.\"\n>>\n>> A new bare bones Knots style Bitcoin implementation (in C++/C) integrated with Core Lightning was a long term idea I had (and presumably many others have had) but the dysfunction on the Bitcoin Core project this week (if anything it has been getting worse over time, not better) has made me start to take the idea more seriously. It is clear to me that the current way the Bitcoin Core project is being managed is not how I would like an open source project to be managed. Very little discussion is public anymore and decisions seem to be increasingly made behind closed doors or in private IRC channels (to the extent that decisions are made at all). Core Lightning seems to have the opposite problem. It is managed effectively in the open (admittedly with fewer contributors) but doesn't have the eyeballs or the usage that Bitcoin Core does. Regardless, selfishly I at some point would like a bare bones Bitcoin and Lightning implementation integrated in one codebase. The Bitcoin Core codebase has collected a lot of cruft over time and the ultra conservatism that is needed when treating (potential) consensus code seems to permeate into parts of the codebase that no one is using, definitely isn't consensus code and should probably just be removed.\n>>\n>> The libbitcoinkernel project was (is?) an attempt to extract the consensus engine out of Core but it seems like it won't achieve that as consensus is just too slippery a concept and Knots style consensus compatible codebase forks of Bitcoin Core seem to still the model. To what extent you can safely chop off this cruft and effectively maintain this less crufty fork of Bitcoin Core also isn't clear to me yet.\n>>\n>> Then there is the question of whether it makes sense to mix C and C++ code that people have different views on. C++ is obviously a superset of C but assuming this merging of Bitcoin Core and Core Lightning is/was the optimal final destination it surely would have been better if Core Lightning was written in the same language (i.e. with classes) as Bitcoin Core.\n>>\n>> I'm just floating the idea to (hopefully) hear from people who are much more familiar with the entirety of the Bitcoin Core and Core Lightning codebases. It would be an ambitious long term project but it would be nice to focus on some ambitious project(s) (even if just conceptually) for a while given (thankfully) there seems to be a lull in soft fork activation chaos.\n>>\n>> Thanks\n>> Michael\n>>\n>> [0]: https://twitter.com/michaelfolkson/status/1589220155006910464?s=20&t=GbPm7w5BqS7rS3kiVFTNcw\n>>\n>> --\n>> Michael Folkson\n>> Email: michaelfolkson at [protonmail.com](http://protonmail.com/)\n>> Keybase: michaelfolkson\n>> PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230114/f701a6df/attachment-0001.html>"
            },
            {
                "author": "Michael Folkson",
                "date": "2023-01-16T15:45:36",
                "message_text_only": "Hi alicexbt\n\nThanks for the suggestion. I'll take a look at the branch.\n\nI'm personally pretty bullish on Lightning and Core Lightning is criminally underused. Plus it is more exciting (and hopefully will attract more contributors) to try something ambitious than just trim Core. I'll see if it is something the Core Lightning contributors might be interested in helping out on. I remember that Rusty said on a podcast that if he had another life he'd have liked to have worked on Core. This way he could potentially do both :)\n\nThanks\nMichael\n\n--\nMichael Folkson\nEmail: michaelfolkson at protonmail.com\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n\n\n------- Original Message -------\nOn Sunday, January 15th, 2023 at 12:58, alicexbt <alicexbt at protonmail.com> wrote:\n\n\n> Hi Michael,\n> \n> If I were to fork bitcoin core and maintain an implementation, I wouldn't integrate any lightning implementation with it. Instead remove some things from bitcoin core and keep it simple. There is also scope for improving privacy. Example: https://github.com/bitcoinknots/bitcoin/issues/50\n> \n> You might find the commits in this branch interesting if you are looking to remove things from bitcoin core and maintain an implementation with no gui, wallet, less RPCs etc.\n> \n> https://github.com/jeremyRubin/bitcoin/commits/delete-it-all\n> \n> \n> /dev/fd0\n> floppy disc guy\n> \n> Sent with Proton Mail secure email.\n> \n> ------- Original Message -------\n> On Sunday, January 15th, 2023 at 1:56 AM, Michael Folkson via Lightning-dev lightning-dev at lists.linuxfoundation.org wrote:\n> \n> \n> \n> > I tweeted this 0 back in November 2022.\n> > \n> > \"With the btcd bugs and the analysis paralysis on a RBF policy option in Core increasingly thinking @BitcoinKnots and consensus compatible forks of Core are the future. Gonna chalk that one up to another thing @LukeDashjr was right about all along.\"\n> > \n> > A new bare bones Knots style Bitcoin implementation (in C++/C) integrated with Core Lightning was a long term idea I had (and presumably many others have had) but the dysfunction on the Bitcoin Core project this week (if anything it has been getting worse over time, not better) has made me start to take the idea more seriously. It is clear to me that the current way the Bitcoin Core project is being managed is not how I would like an open source project to be managed. Very little discussion is public anymore and decisions seem to be increasingly made behind closed doors or in private IRC channels (to the extent that decisions are made at all). Core Lightning seems to have the opposite problem. It is managed effectively in the open (admittedly with fewer contributors) but doesn't have the eyeballs or the usage that Bitcoin Core does. Regardless, selfishly I at some point would like a bare bones Bitcoin and Lightning implementation integrated in one codebase. The Bitcoin Core codebase has collected a lot of cruft over time and the ultra conservatism that is needed when treating (potential) consensus code seems to permeate into parts of the codebase that no one is using, definitely isn't consensus code and should probably just be removed.\n> > \n> > The libbitcoinkernel project was (is?) an attempt to extract the consensus engine out of Core but it seems like it won't achieve that as consensus is just too slippery a concept and Knots style consensus compatible codebase forks of Bitcoin Core seem to still the model. To what extent you can safely chop off this cruft and effectively maintain this less crufty fork of Bitcoin Core also isn't clear to me yet.\n> > \n> > Then there is the question of whether it makes sense to mix C and C++ code that people have different views on. C++ is obviously a superset of C but assuming this merging of Bitcoin Core and Core Lightning is/was the optimal final destination it surely would have been better if Core Lightning was written in the same language (i.e. with classes) as Bitcoin Core.\n> > \n> > I'm just floating the idea to (hopefully) hear from people who are much more familiar with the entirety of the Bitcoin Core and Core Lightning codebases. It would be an ambitious long term project but it would be nice to focus on some ambitious project(s) (even if just conceptually) for a while given (thankfully) there seems to be a lull in soft fork activation chaos.\n> > \n> > Thanks\n> > Michael\n> > \n> > --\n> > Michael Folkson\n> > Email: michaelfolkson at protonmail.com\n> > Keybase: michaelfolkson\n> > PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3"
            }
        ],
        "thread_summary": {
            "title": "A new Bitcoin implementation integrated with Core Lightning",
            "categories": [
                "bitcoin-dev",
                "Lightning-dev"
            ],
            "authors": [
                "Fabian",
                "Michael Folkson"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 12601
        }
    },
    {
        "title": "[bitcoin-dev] Serverless Payjoin",
        "thread_messages": [
            {
                "author": "Dan Gould",
                "date": "2023-01-22T20:50:44",
                "message_text_only": "Hi all,\n\nI'm publishing a payjoin upgrade in response to a request from this list. The payjoin receiver no longer has to run a public server. They lean on a relay for the connection and share a symmetric-key for security rather than a TLS certificate or a Tor hidden service.\n\nI think this work raises a greater problem which is that payjoin assumes synchronous communication while it\u2019s an asynchronous world.\n\nI added the full write-up in plain text below, though I recommend reading the gist for improved formatting and in order to benefit from future edits:\nhttps://gist.github.com/DanGould/243e418752fff760c9f6b23bba8a32f9\n\nBest regards,\nDan\n\n\n\nServerless Payjoin\n\n\nReceive surveillance-busting bitcoin transfers without hosting a secure endpoint\n\n\n\nOVERVIEW\n\n\nPayjoin[1] solves the sole privacy problem left open in the bitcoin paper, that transactions with multiple inputs \"necessarily reveal that their inputs were owned by the same owner.\"[2] Breaking that common-input ownership assumption requires contributions from multiple owners via interaction, namely hosting a server endpoint secured by a certificate on the receiving side. This problem has been singled out on this list as a barrier to greater payjoin adoption.[3]\n\nInstead of a peer-hosted endpoint, this scheme weilds a TURN[4] relay for connectivity and symmetric cryptography for security. Without a replacement for secured networking, the relay could steal funds. Aside from a pre-shared secret and relayed networking, the protocol takes the same form as the existing BIP 78 payjoin spec.\n\n\n\nBASIC SCHEME\n\n\nThe recipient requests that the relay allocate them an endpoint at which they may be reached by UDP. Once allocated, they listen on it. They then generate a 256-bit key, psk. Out of band, they share a BIP 21[5] payjoin uri including their unique relay allocation endpoint in the pj query parameter and psk in a new psk query parameter.\n\nThe sender constructs their request containing an original PSBT as in BIP 78. Instead of sending it over TLS or Tor, they follow noise framework NNpsk0[6] pattern. They encrypt the request using psk alongside an ephemeral sender key and MAC. The resulting ciphertext ensures message secrecy and integrity when relayed to the recipient by the pj endpoint.\n\nThe pay-to-endpoint protocol proceeds to produce a payjoin as in BIP 78 except that messages are secured by the noise NNpsk0 pattern rather than TLS or Tor.\n\n\n\nIMPROVEMENTS\n\n\nHTTP/3\n\nTURN defaults to UDP. In order to adhere to the BIP 78 protocol HTTP messaging, HTTP/3 should be used on top of TURN/UDP.\nOffline Asynchronous Payjoins\n\nIt may be possible for a relay to hold a requeust for an offline payjoin peer until that peer comes online. However, the BIP 78 spec recommends broadcasting request PSBTs in the case of an offline counterparty. Doing so exposes a na\u00efve, surveillance-vulnerable transaction which payjoin intends to avoid. More research needs to be done before such a protocol can be recommended.\n\n\nNostr\n\nWhile a custom Nostr relay could in theory replace the TURN relay while sharing shnorr crypto with bitcoin, it would require another protocol to synchronize networking, since Nostr makes no assumptions about whether a peer is online or not, and a careful cryptography audit to secure. TURN and Noise are already well understood, tested, and have production library support across multiple popular languages and other bitcoin-related projects. Noise even has tooling for formal verification. Nostr relays may prove more likely to allow public access and more robust if we figure out async payjoin, however.\n\n\n\nNOTEWORTHY DETAILS\n\n\nAttack vectors\n\nSince TURN relays can be used for any kind of internet traffic they are vulnerable to the tragedy of the commons. Relay operators may impose authentication requirements for endpoint allocation provisions.\n\nSince psk is a symmetric key, the first message containing the sender's original PSBT does not have forward secrecy.\n\n\nNetwork Privacy\n\nPeers will only see the IP address of the TURN relay but not their peer's. TURN relays may be made available via Tor hidden service in addition to IP to allow either of the peers to protect their IP with Tor without forcing the other to use it too.\n\n\n\nIMPLEMENTATION\n\n\nI've published working proof of concept sender, receiver clients and relay code in rust[7]\n\n\n\nACKNOWLEDGEMENTS\n\n\nDeepest gratitude to Ethan Heilman for sitting down with me to help get to the bottom of the requirements of this problem, to Ruben Somsen for this slick format, and to all those engaged in defending the right to privacy.\n\n\n\nREFERENCES\n\n\n[1]  BIP 78 A Simple Payjoin Proposal, Nicolas Doier:\nhttps://github.com/bitcoin/bips/blob/master/bip-0078.mediawiki\n\n[2]  Bitcoin: A Peer-to-Peer Electronic Cash System, Satoshi Nakamoto:\nhttps://chaincase.app/bitcoin.pdf\n\n[3]  [bitcoin-dev] PayJoin adoption, Craig Raw:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-January/018358.html\n\n[4]  RFC 5766: Traversal Using Relays around NAT (TURN):\nhttps://www.rfc-editor.org/rfc/rfc5766\n\n[5]  BIP 21 URI Scheme, Nils Schneider, Matt Corallo:\n https://github.com/bitcoin/bips/blob/master/bip-0021.mediawiki\n\n[6]  Noise Explorer: NNpsk0:\nhttps://noiseexplorer.com/patterns/NNpsk0\n\n[7]  Serverless PayJoin PoC:\nhttps://github.com/chaincase-app/payjoin/pull/21"
            }
        ],
        "thread_summary": {
            "title": "Serverless Payjoin",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Dan Gould"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 5324
        }
    },
    {
        "title": "[bitcoin-dev] Wallet vaults with pre-signed transactions but no ephemeral keys",
        "thread_messages": [
            {
                "author": "Billy Tetrud",
                "date": "2023-01-23T17:39:41",
                "message_text_only": "In the discussion around James' OP_VAULT proposal, it was implied that\nprecomputed vaults must use ephemeral keys that must be deleted as part of\nthe vaulting protocol, like Bryan Bishop's proposal\n<https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-August/017229.html>.\nLooking around, I haven't been able to find any wallet vault proposal that\ndoesn't require ephemeral keys, so at the risk of posting something that's\nobvious to everyone, I wanted to share a simple way to do a wallet vault\nwithout requiring any key deletion.\n\nThe basic idea is to create an N-of-N multisig address, and pre-sign some\ntransactions from it with N-1 keys to an address with several timelocked\nspend paths. This has the fallback that funds can always be spent\nimmediately if you use all the keys, just like a normal N-of-N multisig\naddress (since that's what it is at its core), but the usage of any of the\npre-signed transactions leads to an address that guarantees a clawback\nwithin a time window. Here's a 3-of-3 example:\n\n*Vault Initialization*:\n1. Create 3 of 3 Vault Address\n2. Create an Interim Address that can send with:\n * 1 of 3 keys after a timelock of 1 month\n * 2 of 3 keys after a timelock of 1 week\n * 3 of 3 keys with no timelock\n\n*Vaulting*:\n1. Create a transaction sending an output to the Vault Address\n2. Create a transaction spending that Vault Address output to the Interim\nAddress\n3. Presign one copy of the step-2 transaction for each of the three\ncombinations of two keys.\n4. Store seeds separately, store the wallet config as well as the three\npresigned transactions with each seed.\n\n*Unvaulting*:\n1. Sign one of the pre-signed transactions with the missing signature.\n2. Broadcast\n3. Wait the appropriate timelock for the number of keys you want to sign\nwith.\n4. Create a transaction sending from the Interim Address.\n5. Broadcast\n\n*Recovering *(after unvaulting step 2 after the broadcasted transaction to\nthe Interim Address has been mined):\n1. Sign the utxo with all three keys to any destination. Alternatively sign\nwith two keys, wait 1 week.\n2. Broadcast it\n\nThis has the usual downsides of pre-signed vaults that you need to backup\nthese transactions for each vaulting, complications involving the\nflexibility (or lack thereof) of fees, and inflexibility in how much to\nunvault (must be the whole utxo, no change). This could of course be\naugmented with various techniques to make fee handling more flexible\n(anchor outputs, multiple versions of the presigned transactions with\ndifferent fees, etc). More complicated presigning schemes could allow for\nsome flexibility in unvaulting amount (eg by sending change back into the\nvault, and creating additional pre-signed transactions for that new output).\n\nIt also has the same downside that OP_CTV vaults have, where a stolen key\ncan steal funds from the interim address by racing the owner with their own\ntransaction once the necessary delay has passed. Note that James' OP_VAULT\nopcode wouldn't have this problem.\n\nBut not requiring any toxic waste keys seems like a pretty good benefit\nover Bryan Bishop's original proposal.\n\nAnyways sorry if this was already on people's radar and just too obvious to\npost about.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230123/eb519e3d/attachment.html>"
            },
            {
                "author": "darosior",
                "date": "2023-01-26T14:30:56",
                "message_text_only": "Hello Billy,\n\nYes it's basically a (simple) instantiation of Revault. You can find more at [https://github.com/revault](https://github.com/revault/) (you most likely want the `practical-revault` repo). There is a description of the concept, the specification of a communication protocol between the participants as well as the implementation of the whole.\n\nSuch a design presents some advantages, but it has two major issues:\n\n- You need to make sure all your watchtowers received the Cancel signature before you sign the Unvault transaction. But how can you get this guarantee in the usual (and reasonable) model of an untrusted laptop?\n- You can only have policies on the Unvault transaction (eg \"You can only Unvault up to X BTC during working hours\"), not on the Spend transaction (eg \"You can only send coins to a Script with pubkey Y required in all spending paths\"). Revault allows to use cosigning servers that act as anti-replay oracles to have policies on the spend, but this is obviously *very* suboptimal.\n\nBoth issues are solvable with covenants.\n\nAntoine Poinsot\n------- Original Message -------\nLe lundi 23 janvier 2023 \u00e0 6:39 PM, Billy Tetrud via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> In the discussion around James' OP_VAULT proposal, it was implied that precomputed vaults must use ephemeral keys that must be deleted as part of the vaulting protocol, like [Bryan Bishop's proposal](https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-August/017229.html). Looking around, I haven't been able to find any wallet vault proposal that doesn't require ephemeral keys, so at the risk of posting something that's obvious to everyone, I wanted to share a simple way to do a wallet vault without requiring any key deletion.\n>\n> The basic idea is to create an N-of-N multisig address, and pre-sign some transactions from it with N-1 keys to an address with several timelocked spend paths. This has the fallback that funds can always be spent immediately if you use all the keys, just like a normal N-of-N multisig address (since that's what it is at its core), but the usage of any of the pre-signed transactions leads to an address that guarantees a clawback within a time window. Here's a 3-of-3 example:\n>\n> Vault Initialization:\n> 1. Create 3 of 3 Vault Address\n> 2. Create an Interim Address that can send with:\n> * 1 of 3 keys after a timelock of 1 month\n> * 2 of 3 keys after a timelock of 1 week\n> * 3 of 3 keys with no timelock\n>\n> Vaulting:\n> 1. Create a transaction sending an output to the Vault Address\n> 2. Create a transaction spending that Vault Address output to the Interim Address\n> 3. Presign one copy of the step-2 transaction for each of the three combinations of two keys.\n> 4. Store seeds separately, store the wallet config as well as the three presigned transactions with each seed.\n>\n> Unvaulting:\n> 1. Sign one of the pre-signed transactions with the missing signature.\n> 2. Broadcast\n> 3. Wait the appropriate timelock for the number of keys you want to sign with.\n> 4. Create a transaction sending from the Interim Address.\n> 5. Broadcast\n> Recovering (after unvaulting step 2 after the broadcasted transaction to the Interim Address has been mined):\n> 1. Sign the utxo with all three keys to any destination. Alternatively sign with two keys, wait 1 week.\n> 2. Broadcast it\n>\n> This has the usual downsides of pre-signed vaults that you need to backup these transactions for each vaulting, complications involving the flexibility (or lack thereof) of fees, and inflexibility in how much to unvault (must be the whole utxo, no change). This could of course be augmented with various techniques to make fee handling more flexible (anchor outputs, multiple versions of the presigned transactions with different fees, etc). More complicated presigning schemes could allow for some flexibility in unvaulting amount (eg by sending change back into the vault, and creating additional pre-signed transactions for that new output).\n>\n> It also has the same downside that OP_CTV vaults have, where a stolen key can steal funds from the interim address by racing the owner with their own transaction once the necessary delay has passed. Note that James' OP_VAULT opcode wouldn't have this problem.\n>\n> But not requiring any toxic waste keys seems like a pretty good benefit over Bryan Bishop's original proposal.\n>\n> Anyways sorry if this was already on people's radar and just too obvious to post about.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230126/01be3f93/attachment.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2023-01-31T15:02:51",
                "message_text_only": "Ah good to know someone's put work into this kind of idea. Thanks for the\nreference!\n\nOn Thu, Jan 26, 2023 at 8:31 AM darosior <darosior at protonmail.com> wrote:\n\n> Hello Billy,\n>\n> Yes it's basically a (simple) instantiation of Revault. You can find more\n> at https://github.com/revault (you most likely want the\n> `practical-revault` repo). There is a description of the concept, the\n> specification of a communication protocol between the participants as well\n> as the implementation of the whole.\n>\n> Such a design presents some advantages, but it has two major issues:\n>\n>    - You need to make sure all your watchtowers received the Cancel\n>    signature before you sign the Unvault transaction. But how can you get this\n>    guarantee in the usual (and reasonable) model of an untrusted laptop?\n>    - You can only have policies on the Unvault transaction (eg \"You can\n>    only Unvault up to X BTC during working hours\"), not on the Spend\n>    transaction (eg \"You can only send coins to a Script with pubkey Y required\n>    in all spending paths\"). Revault allows to use cosigning servers that act\n>    as anti-replay oracles to have policies on the spend, but this is obviously\n>    *very* suboptimal.\n>\n>\n> Both issues are solvable with covenants.\n>\n> Antoine Poinsot\n> ------- Original Message -------\n> Le lundi 23 janvier 2023 \u00e0 6:39 PM, Billy Tetrud via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>\n> In the discussion around James' OP_VAULT proposal, it was implied that\n> precomputed vaults must use ephemeral keys that must be deleted as part of\n> the vaulting protocol, like Bryan Bishop's proposal\n> <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-August/017229.html>.\n> Looking around, I haven't been able to find any wallet vault proposal that\n> doesn't require ephemeral keys, so at the risk of posting something that's\n> obvious to everyone, I wanted to share a simple way to do a wallet vault\n> without requiring any key deletion.\n>\n> The basic idea is to create an N-of-N multisig address, and pre-sign some\n> transactions from it with N-1 keys to an address with several timelocked\n> spend paths. This has the fallback that funds can always be spent\n> immediately if you use all the keys, just like a normal N-of-N multisig\n> address (since that's what it is at its core), but the usage of any of the\n> pre-signed transactions leads to an address that guarantees a clawback\n> within a time window. Here's a 3-of-3 example:\n>\n> *Vault Initialization*:\n> 1. Create 3 of 3 Vault Address\n> 2. Create an Interim Address that can send with:\n> * 1 of 3 keys after a timelock of 1 month\n> * 2 of 3 keys after a timelock of 1 week\n> * 3 of 3 keys with no timelock\n>\n> *Vaulting*:\n> 1. Create a transaction sending an output to the Vault Address\n> 2. Create a transaction spending that Vault Address output to the Interim\n> Address\n> 3. Presign one copy of the step-2 transaction for each of the three\n> combinations of two keys.\n> 4. Store seeds separately, store the wallet config as well as the three\n> presigned transactions with each seed.\n>\n> *Unvaulting*:\n> 1. Sign one of the pre-signed transactions with the missing signature.\n> 2. Broadcast\n> 3. Wait the appropriate timelock for the number of keys you want to sign\n> with.\n> 4. Create a transaction sending from the Interim Address.\n> 5. Broadcast\n>\n> *Recovering *(after unvaulting step 2 after the broadcasted transaction\n> to the Interim Address has been mined):\n> 1. Sign the utxo with all three keys to any destination. Alternatively\n> sign with two keys, wait 1 week.\n> 2. Broadcast it\n>\n> This has the usual downsides of pre-signed vaults that you need to backup\n> these transactions for each vaulting, complications involving the\n> flexibility (or lack thereof) of fees, and inflexibility in how much to\n> unvault (must be the whole utxo, no change). This could of course be\n> augmented with various techniques to make fee handling more flexible\n> (anchor outputs, multiple versions of the presigned transactions with\n> different fees, etc). More complicated presigning schemes could allow for\n> some flexibility in unvaulting amount (eg by sending change back into the\n> vault, and creating additional pre-signed transactions for that new output).\n>\n> It also has the same downside that OP_CTV vaults have, where a stolen key\n> can steal funds from the interim address by racing the owner with their own\n> transaction once the necessary delay has passed. Note that James' OP_VAULT\n> opcode wouldn't have this problem.\n>\n> But not requiring any toxic waste keys seems like a pretty good benefit\n> over Bryan Bishop's original proposal.\n>\n> Anyways sorry if this was already on people's radar and just too obvious\n> to post about.\n>\n>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230131/3f850526/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Wallet vaults with pre-signed transactions but no ephemeral keys",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "darosior",
                "Billy Tetrud"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 12976
        }
    },
    {
        "title": "[bitcoin-dev] Wallet policies for descriptor wallets",
        "thread_messages": [
            {
                "author": "darosior",
                "date": "2023-01-23T19:53:18",
                "message_text_only": "Hello Salvatore,\n\nIt's not something about the specifications of wallet policies, but regarding the guidelines for implementers on signing devices. Quoting BIP-wallet-policies:\n\n> Moreover, other limitations like the limited size of the screen might affect what design choices are available in practice. Therefore, minimizing the size of the information shown on-screen is important for a good user experience; that is crucial since the ability for the user to completely validate on-screen the kind of script used (and each of the involved keys) is a prerequisite for secure usage, as the machine that is interacting with the hardware signer (and running the software wallet) is considered untrusted.\n\n> The device shows the wallet policy to the user using the secure screen.\n\n\u200b\n\n> - Template with miniscript for \"1 of 2 equally likely keys\":\n>   wsh(or_b(pk(@0/**),s:pk(@1/**)))\n\n\u200b\n\nActually you can save a few more characters, and gain some clarity, by showing the \"semantic policy\" instead of the actual Miniscript.\n\nIf the intent is for the user to verify the semantic of the Bitcoin Script they are importing, you can just drop all the type information.\n\nFor instance, for a Miniscript representing the Miniscript policy \"a 3-of-3 that becomes a 2-of-3 after 90 days\" instead of showing:\n\nthresh(3,pk(Alice),s:pk(Bob),s:pk(Carol),sln:older(12960))\n\nYou could show:\n\nthresh(3,pk(Alice),pk(Bob),pk(Carol),older(12960))\n\nFor this specific example you'd save 8 (confusing) characters to be verified on the signing device.\n\nI wonder if signing devices could even go further and display a plain english verification to the user, like \"This policy contains 4 spending paths. Be ready to verify the 4 spending paths. The first spending path is Alice, Bob and Carol signing together. The second spending path is Bob and Carol signing together after 90 days. The third spending path is Alice and Carol signing together after 90 days.\n\nThe third spending path is Alice and Bob signing together after 90 days\n\n.\"\n\nIt seems feasible to be doable in a general manner from a Miniscript \"semantic policy\".\n\nI guess it clashes with the user willing to check their backup against the policy registered on the device. You could always have the user-friendly policy check at first and have an option to show the raw descriptor for them to be able to cross-check it with their backup.\n\nPS: the numerous usage of the word \"policy\" is getting complex lol, is it a Miniscript concrete policy, a Miniscript semantic policy, a BIP-wallet-policies policy? :)\n\nAntoine Poinsot\n\n------- Original Message -------\nLe lundi 21 novembre 2022 \u00e0 12:27 PM, Salvatore Ingala via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Hi list,\n>\n> Following up on this topic, I now opened a pull request with the BIP proposal:\n>\n> https://github.com/bitcoin/bips/pull/1389\n>\n> I also attempted a proof-of-concept of how an integration of wallet policies to HWI might look like:\n>\n> https://github.com/bitcoin-core/HWI/pull/647\n>\n> which might help to provide context, and also serves as a demo of the possible UX flows with hardware signers (as currently implemented in the Ledger bitcoin app).\n>\n> There are no substantial changes to the initial version proposed to the list:\n> - some additional restrictions to the allowed descriptors were added as further simplifications;\n> - added test vectors and observations on backwards compatibility;\n> - general improvements to the text.\n>\n> I look forward to your comments and improvements.\n> Salvatore Ingala\n>\n> On Thu, 5 May 2022 at 16:32, Salvatore Ingala <salvatore.ingala at gmail.com> wrote:\n>\n>> In the implementation work to implement descriptors and miniscript support in hardware wallets [a][b], I encountered a number of challenges. Some of them are technical in nature (e.g. due to constraints of embedded development). Others are related to the attempts of shaping a good user experience; with bitcoin reaching more people who are not tech-savvy, self-custody is only as secure as what those newcomers can use easily enough.\n>>\n>> The main tool that I am using to address some of these challenges is a layer that sits _on top_ of descriptors/miniscript, while staying very close to it. Since there is nothing that is vendor-specific in the vast majority of the approach I'm currently using, I tried to distill it here for your comments, and will propose a BIP if this is deemed valuable.\n>>\n>> I called the language \"wallet policies\" (suggestions for a better name are welcome). I believe an approach based on wallet policies can benefit all hardware wallets (stateless or not) that want to securely support complex scripts; moreover, wallet policies are close enough to descriptors that their integration should be extremely easy for any software wallet that is currently using descriptors.\n>>\n>> [a]: https://blog.ledger.com/bitcoin-2 - early demo\n>> [b]: https://blog.ledger.com/miniscript-is-coming - miniscript example\n>>\n>> Salvatore Ingala\n>>\n>> ======================================================\n>>\n>> This document starts with a discussion on the motivation for wallet policies, followed by their formal definition, and some recommendations for implementations.\n>>\n>> == Rationale ==\n>> Output script descriptors [1] were introduced in bitcoin-core as a way to represent collections of output scripts. It is a very general and flexible language, designed to catch all the possible use-cases of bitcoin wallets (that is, if you know the script and you have the necessary keys, it will be possible to sign transactions with bitcoin-core's descriptor-based wallets).\n>>\n>> Unfortunately, descriptors are not a perfect match for the typical usage of hardware wallets. Most hardware wallets have the following limitations compared to a general-purpose machine running bitcoin-core:\n>>\n>> - they are embedded devices with limited RAM and computational power;\n>> - they might not be able to import additional private keys (all the keys are generated from a single seed via [BIP-32](https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki));\n>> - they might not have permanent storage (*stateless* hardware wallet design).\n>>\n>> Moreover, other limitations like the limited size of the screen might affect what design choices are available in practice. Therefore, minimizing the size of the information shown on-screen is important for a good user experience.\n>>\n>> A more native, compact representation of the wallet receive/change would also benefit the UX of software wallets using descriptors to represent software wallets using descriptors/miniscript for multisignature or other complex locking conditions.\n>>\n>> === Security and UX concerns of scripts in hardware wallets ===\n>> For a hardware wallet, allowing the usage of complex scripts presents challenges in terms of both security and user experience.\n>>\n>> ==== Security issues ====\n>>\n>> One of the security properties that hardware wallets strive to guarantee is the following: **as long as the user correctly verifies the information that is shown on the hardware wallet's screen before approving, no action can be performed without the user's consent**.\n>> This must hold even in scenarios where the attacker has full control of the machine that is connected to the hardware wallet, and can execute arbitrary requests or tamper with the legitimate user's requests.\n>>\n>> Therefore, it is not at all trivial to allow complex scripts, especially if they contain keys that belong to third parties.\n>> The hardware wallet must guarantee that the user knows precisely *what* \"policy\" is being used to spend the funds, and that the \"unspent\" funds (if any) will be protected by the same policy. This makes it impossible for an attacker to surreptitiously modify the policy, therefore stealing or burning user's funds.\n>>\n>> ==== UX issues ====\n>>\n>> With miniscript (and taproot trees) allowing substantially more complex spending policies to be used, it becomes more challenging to make sure that the user is able _in practice_ to verify the information on the screen. Therefore, there are two fundamental design goals to strive for:\n>> - Minimize the amount of information that is shown on screen - so that the user can actually validate it.\n>> - Minimize the number of times the user has to validate such information.\n>>\n>> Designing a secure protocol for the coordination of a descriptor wallet among distant parties is also a challenging problem that is out of scope in this document. See BIP-129 [2] for an approach designed for multisignature wallets.\n>>\n>> === Policy registration as a solution ===\n>>\n>> A solution to address the security concerns, and part of the UX concerns, is to have a *registration* flow for the wallet policy in the hardware wallet. The \"wallet policy\" must contain enough information to generate all the relevant addresses/scripts, and for the hardware wallet to identify the keys that it controls and that are needed to spend the funds sent to those addresses.\n>>\n>> Before a new policy is used for the first time, the user will register a `wallet policy` into the hardware wallet. While the details of the process are out of scope in this document, the flow should be something similar to the following:\n>>\n>> 1) The software wallet initiates a _wallet policy registration_ on the hardware wallet; the information should include the wallet policy, but also a unique *name* that identifies the policy.\n>> 2) The hardware wallet shows the wallet policy to the user using the secure screen.\n>> 3) After inspecting the policy and comparing it with a trusted source (for example a printed backup), the user approves the policy.\n>> 4) If stateful, the hardware wallet persists the policy in its permanent memory; if stateless, it returns a \"proof of registration\".\n>>\n>> The details of how to create a proof of registration are out of scope for this document; using a *message authentication codes* on a hash committing to the wallet policy, its name and any additional metadata is an effective solution if correctly executed.\n>>\n>> Once a policy is registered, the hardware wallet can perform the usual operations securely:\n>> - generating receive and change addresses;\n>> - showing addresses on the secure screen;\n>> - sign transactions spending from a wallet, while correctly identifying change addresses and computing the transaction fees.\n>>\n>> Before any of the actions mentioned above, the hardware wallet will retrieve the policy from its permanent storage if stateful; if stateless it will validate the _proof of registration_ before using the wallet policy provided by the client.\n>> Once the previously registered policy is correctly identified and approved by the user (for example by its name), and *as long as the policy registration was executed securely*, hardware wallets can provide a user experience similar to the usual one for single-signature transactions.\n>>\n>> === Avoiding blowup in descriptor size ===\n>>\n>> While reusing a pubkey in different branches of a miniscript is explicitly forbidden by miniscript (as it has certain negative security implications), it is still reasonable to reuse the same *xpub* in multiple places, albeit with different final steps of derivation (so that the actual pubkeys that are used in the script are indeed different).\n>>\n>> For example, using Taproot, a *3*-of-*5* multisignature wallet could use:\n>> - a key path with a 5-of-5 MuSig\n>> - a script tree with a tree of 10 different 3-of-3 MuSig2 scripts, that are generated, plus a leaf with a fallback *3*-of-*5* multisignature using plain multisignature (with `OP_CHECKSIGADD`).\n>>\n>> This could look similar to:\n>>\n>> ```\n>> tr(musig2(xpubA,xpubB,xpubC,xpubD,xpubE)/<0;1>/*), {\n>> {\n>> {\n>> pk(musig2(xpubA,xpubB,xpubC)/<2;3>/*),\n>> {\n>> pk(musig2(xpubA,xpubB,xpubD)/<4;5>/*)\n>> pk(musig2(xpubA,xpubB,xpubE)/<6;7>/*),\n>> }\n>> },\n>> {\n>> pk(musig2(xpubA,xpubC,xpubD)/<8;9>/*),\n>> {\n>> pk(musig2(xpubA,xpubC,xpubE)/<10;11>/*),\n>> pk(musig2(xpubA,xpubD,xpubE)/<12;13>/*)\n>> }\n>> }\n>> },\n>> {\n>> {\n>> pk(musig2(xpubB,xpubC,xpubD)/<14;15>/*),\n>> pk(musig2(xpubB,xpubC,xpubE)/<16;17>/*)\n>> },\n>> {\n>> pk(musig2(xpubB,xpubD,xpubE)/<18;19>/*),\n>> {\n>> pk(musig2(xpubC,xpubD,xpubE)/<20;21>/*),\n>> sortedmulti_a(3,\n>> xpubA/<22;23>/*,\n>> xpubB/<22;23>/*,\n>> xpubC/<22;23>/*,\n>> xpubD/<22;23>/*,\n>> xpubE/<22;23>/*)\n>> }\n>> }\n>> }\n>> })\n>> ```\n>>\n>> Note that each root xpub appears 8 times. With xpubs being up to 118 bytes long, the length of the full descriptor can get extremely long (the problem gets *exponentially* worse with larger multisignature schemes).\n>>\n>> Replacing the common part of the key with a short key placeholder and moving the key expression separately helps to keep the size of the wallet policy small, which is crucial to allow human inspection in the registration flow.\n>>\n>> === Restrictions on the supported descriptors ====\n>>\n>> The policy language proposed in this document purposely targets only a stricter subset of the output descriptors language, and it attempts to generalize in the most natural way the approach that is already used for single-signature *accounts* (as described in BIP-44 [3], BIP-49 [4], BIP-84 [5], or BIP-86 [6]), or in multisignature setups (see for example BIP-48 [7] and BIP-87 [8]).\n>>\n>> Unlike the BIPs mentioned above, it is not tied to any specific script template, as it applies to arbitrary scripts that can be represented with descriptors and miniscript.\n>>\n>> Supporting only a reduced feature set when compared to output descriptors helps in implementations (especially on hardware wallets), while attempting to capture all the common use cases. More features can be added in the future if motivated by real world necessity.\n>>\n>> By keeping the structure of the wallet policy language very close to that of descriptors, it should be straightforward to:\n>> - write wallet policy parsers;\n>> - extract the descriptors defined by a wallet policy;\n>> - convert a pair of descriptors describing a wallet \"account\" used in current implementations into the corresponding wallet policy.\n>>\n>> == Wallet policies ==\n>>\n>> This section formally defines wallet policies, and how they relate to output script descriptors.\n>>\n>> === Formal definition ===\n>>\n>> A wallet policy is composed by a wallet descriptor template, together with a vector of key information items.\n>>\n>> ==== Wallet descriptor template ====\n>>\n>> A wallet descriptor template is a `SCRIPT` expression.\n>>\n>> `SCRIPT` expressions:\n>> - `sh(SCRIPT)` (top level only): P2SH embed the argument.\n>> - `wsh(SCRIPT)` (top level or inside `sh` only): P2WSH embed the argument.\n>> - `pkh(KP)` (not inside `tr`): P2PKH output for the given public key (use `addr` if you only know the pubkey hash).\n>> - `wpkh(KP)` (top level or inside `sh` only): P2WPKH output for the given compressed pubkey.\n>> - `multi(k,KP_1,KP_2,...,KP_n)`: k-of-n multisig script.\n>> - `sortedmulti(k,KP_1,KP_2,...,KP_n)`: k-of-n multisig script with keys sorted lexicographically in the resulting script.\n>> - `tr(KP)` or `tr(KP,TREE)` (top level only): P2TR output with the specified key as internal key, and optionally a tree of script paths.\n>> - any valid miniscript template (inside `wsh` or `tr` only).\n>>\n>> `TREE` expressions:\n>> - any `SCRIPT` expression\n>> - An open brace `{`, a `TREE` expression, a comma `,`, a `TREE` expression, and a closing brace `}`\n>>\n>> Note: \"miniscript templates\" are not formally defined in this version of the document, but it is straightforward to adapt this approach.\n>> `KP` expressions (key placeholders) consist of\n>> - a single character `@`\n>> - followed by a non-negative decimal number, with no leading zeros (except for `@0`).\n>> - possibly followed by either:\n>> - the string `/**`, or\n>> - a string of the form `/<NUM;NUM>/*`, for two distinct decimal numbers `NUM` representing unhardened derivations\n>>\n>> The `/**` in the placeholder template represents commonly used paths for receive/change addresses, and is equivalent to `<0;1>`.\n>>\n>> The placeholder `@i` for some number *i* represents the *i*-th key in the vector of key origin information (which must be of size at least *i* + 1, or the wallet policy is invalid).\n>>\n>> ==== Key informations vector ====\n>>\n>> Each element of the key origin information vector is a `KEY` expression.\n>>\n>> - Optionally, key origin information, consisting of:\n>> - An open bracket `[`\n>> - Exactly 8 hex characters for the fingerprint of the master key from which this key is derived from (see [BIP32](https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki) for details)\n>> - Followed by zero or more `/NUM'` path elements to indicate hardened derivation steps between the fingerprint and the xpub that follows\n>> - A closing bracket `]`\n>> - Followed by the actual key, which is either\n>> - a hex-encoded pubkey, which is either\n>> - inside `wpkh` and `wsh`, only compressed public keys are permitted (exactly 66 hex characters starting with `02` or `03`.\n>> - inside `tr`, x-only pubkeys are also permitted (exactly 64 hex characters).\n>> - a serialized extended public key (`xpub`) (as defined in [BIP 32](https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki))\n>>\n>> The placeholder `@i` for some number *i* represents the *i*-th key in the vector of key orIgin information (which must be of size at least *i* + 1, or the wallet policy is invalid).\n>>\n>> The policy template is invalid if any placeholder `@i` has derivation steps while the corresponding `(i+1)`-th element of the keys vector is not an xpub.\n>>\n>> ==== Additional rules ====\n>>\n>> The wallet policy is invalid if any placeholder expression with additional derivation steps is used when the corresponding key information is not an xpub.\n>>\n>> The key information vector *should* be ordered so that placeholder `@i` never appear for the first time before an occurrence of `@j` for some `j < i`; for example, the first placeholder is always `@0`, the next one is `@1`, etc.\n>>\n>> === Descriptor derivation ===\n>>\n>> From a wallet descriptor template (and the associated vector of key informations), one can therefore obtain the 1-dimensional descriptor for receive and change addresses by:\n>>\n>> - replacing each key placeholder with the corresponding key origin information;\n>> - replacing every `/**` with `/0/*` for the receive descriptor, and `/1/*` for the change descriptor;\n>> - replacing every `/<M,N>` with `/M` for the receive descriptor, and `/N` for the change descriptor.\n>>\n>> For example, the wallet descriptor `pkh(@0/**)` with key information `[\"[d34db33f/44'/0'/0']xpub6ERApfZwUNrhLCkDtcHTcxd75RbzS1ed54G1LkBUHQVHQKqhMkhgbmJbZRkrgZw4koxb5JaHWkY4ALHY2grBGRjaDMzQLcgJvLJuZZvRcEL\"]` produces the following two descriptors:\n>>\n>> - Receive descriptor: `pkh([d34db33f/44'/0'/0']xpub6ERApfZwUNrhLCkDtcHTcxd75RbzS1ed54G1LkBUHQVHQKqhMkhgbmJbZRkrgZw4koxb5JaHWkY4ALHY2grBGRjaDMzQLcgJvLJuZZvRcEL/0/*)`\n>>\n>> - Change descriptor: `pkh([d34db33f/44'/0'/0']xpub6ERApfZwUNrhLCkDtcHTcxd75RbzS1ed54G1LkBUHQVHQKqhMkhgbmJbZRkrgZw4koxb5JaHWkY4ALHY2grBGRjaDMzQLcgJvLJuZZvRcEL/1/*)`\n>>\n>> === Implementation guidelines ===\n>>\n>> Implementations must not necessarily implement all of the possible wallet policies defined by this standard, but it is recommended to clearly document any limitation.\n>>\n>> Implementations can add additional metadata that is stored together with the wallet policy for the purpose of wallet policy registration and later usage. Metadata can be vendor-specific and is out of the scope of this document.\n>>\n>> Any implementation in a general-purpose software wallet allowing arbitrary scripts (or any scripts that involve external cosigners) should put great care into a process for backing up a wallet policy. In fact, unlike typical single-signature scenarios, the seed alone is no longer enough to discover wallet policies with existing funds, and the loss of the backup is likely to lead to permanent loss of funds.\n>>\n>> Avoiding key reuse among different wallet accounts is also extremely important, but out of scope for this document.\n>>\n>> == Examples ==\n>>\n>> Some examples of wallet descriptor templates (vectors of keys omitted for simplicity):\n>> - Template for a native segwit account:\n>> wpkh(@0/**)\n>>\n>> - Template for a taproot BIP86 account:\n>> tr(@0/**)\n>> - Template for a native segwit 2-of-3:\n>> wsh(sortedmulti(2, at 0/**, at 1/**, at 2/**))\n>> - Template with miniscript for \"1 of 2 equally likely keys\":\n>> wsh(or_b(pk(@0/**),s:pk(@1/**)))\n>>\n>> More examples (esp. targeting miniscript on taproot) will be added in the future.\n>>\n>> == References ==\n>>\n>> * [1] - Output Script Descriptors: https://github.com/bitcoin/bitcoin/blob/master/doc/descriptors.md\n>> * [2] - BIP-129 (Bitcoin Secure Multisig Setup): https://github.com/bitcoin/bips/blob/master/bip-0129.mediawiki\n>> * [3] - BIP-44: https://github.com/bitcoin/bips/blob/master/bip-0044.mediawiki\n>> * [4] - BIP-49: https://github.com/bitcoin/bips/blob/master/bip-0049.mediawiki\n>> * [5] - BIP-84: https://github.com/bitcoin/bips/blob/master/bip-0084.mediawiki\n>> * [6] - BIP-86: https://github.com/bitcoin/bips/blob/master/bip-0086.mediawiki\n>> * [7] - BIP-48: https://github.com/bitcoin/bips/blob/master/bip-0048.mediawiki\n>> * [8] - BIP-87: https://github.com/bitcoin/bips/blob/master/bip-0087.mediawiki\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230123/90a326ef/attachment-0001.html>"
            },
            {
                "author": "Salvatore Ingala",
                "date": "2023-01-24T08:38:29",
                "message_text_only": "Hi Antoine,\n\nThanks for your very interesting suggestions!\n\nOn Mon, 23 Jan 2023 at 20:53, darosior <darosior at protonmail.com> wrote:\n\n> Actually you can save a few more characters, and gain some clarity, by showing the \"semantic policy\" instead of the actual Miniscript. If the intent is for the user to verify the semantic of the Bitcoin Script they are importing, you can just drop all the type information.\n> For instance, for a Miniscript representing the Miniscript policy \"a 3-of-3 that becomes a 2-of-3 after 90 days\" instead of showing:\n>\n> thresh(3,pk(Alice),s:pk(Bob),s:pk(Carol),sln:older(12960))\n> You could show:\n>\n> thresh(3,pk(Alice),pk(Bob),pk(Carol),older(12960))\n> For this specific example you'd save 8 (confusing) characters to be verified on the signing device.\n>\n>\nI thought about that, and I still consider it a possible future improvement\nin UX. However, I wasn't comfortable deploying it in this way for the\nfollowing reason: if there is malware in your software wallet at policy\nregistration time, the malware could find a different miniscript with the\nsame semantic policy.\nThe result is now a mismatch between the wallet policy in the user's backup\nand the one where funds are actually received. The user might see funds\nmysteriously disappear, while the attacker would know the actual miniscript\npolicy, enabling ransom attacks.\n\nThe attack seems very unlikely today, and for many interesting semantic\npolicies, there are probably not many miniscript policies to sift through\nin case of recovery.\nHowever, I suspect it will become more realistic in a taproot world, where\nthe semantic policy of each tapleaf could have multiple options, resulting\nin combinatorial explosion.\nFor example, if there are 2 options for the miniscript of each leaf, and n\nleaves, you would have 2^n possible descriptors with the same semantic\npolicy.\n\nOne solution might be to explicitly enumerate (or at least upper-bound) the\nnumber of possible descriptors that are lifted to the same policy, and use\nthe simplified UX if this number is not too large.\nHaving a set of standard recovery tools for those situations might make\nthis approach more viable in my opinion.\n\nI wonder if signing devices could even go further and display a plain\nenglish verification to the user, like \"This policy contains 4\nspending paths. Be ready to verify the 4 spending paths. The first\nspending path is Alice, Bob and Carol signing together. The second\nspending path is Bob and Carol signing together after 90 days. The\nthird spending path is Alice and Carol signing together after 90 days.\nThe third spending path is Alice and Bob signing together after 90\ndays.\"\n> It seems feasible to be doable in a general manner from a Miniscript \"semantic policy\".\n>\n> A lower-hanging fruit might be to find ways of registering\nxpubs/identities on the device, so that one could replace xpubs with\n\"Alice\" and \"Bob\".\nOnce that's done, that might be one of the possible approaches to simplify\nthe UX flow.\nI suspect the design space to be quite large and I have not yet put enough\nthought into it.\n\nI guess it clashes with the user willing to check their backup against\nthe policy registered on the device. You could always have the\nuser-friendly policy check at first and have an option to show the raw\ndescriptor for them to be able to cross-check it with their backup.\n>\n> I'm assuming the user will do the minimum amount of work they are forced\nto do, therefore I only consider this safe iff we address the\nminiscript-combinatorial-explosion issues above.\n\nPS: the numerous usage of the word \"policy\" is getting complex lol, is\nit a Miniscript concrete policy, a Miniscript semantic policy, a\nBIP-wallet-policies policy? :)\n>\n> ...yeah, we should have a policy against that!\n\nSalvatore Ingala\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230124/4f44543a/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Wallet policies for descriptor wallets",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Salvatore Ingala",
                "darosior"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 25479
        }
    },
    {
        "title": "[bitcoin-dev] A Universal Coin Swap system based on bitcoin and a Bitcoin NFT system",
        "thread_messages": [
            {
                "author": "Aymeric Vitte",
                "date": "2023-01-26T20:15:38",
                "message_text_only": "Please see:\n\n\"A Bitcoin NFT system\"\nhttps://gist.github.com/Ayms/01dbfebf219965054b4a3beed1bfeba7 \"The\npurpose of this proposal is to propose a simple NFT system based on the\nBitcoin blockchain, assuming that the main purpose of a NFT is to be\nsold/bought, but not only, it can be something that you keep for\nyourself proving your ownership on the blockchain, or something that you\noffer to someone else, the advantages compared to using Ethereum or any\nblockchain/sidechain on both networks (or others) will be explained below\"\n\nAnd the continuation:\n\n\"A Universal Coin Swap system based on bitcoin\"\nhttps://gist.github.com/Ayms/029125db2583e1cf9c3209769eb2cdd7 \"The\npurpose here is to propose a simple Coin Swap decentralized system based\non Bitcoin but that works for all blockchains/tokens\"\n\nThe idea is to propose something simple, that people can understand,\nsecured, decentralized, easy to implement/use, not expensive for the\nusers, unlike Ethereum solutions, showing also that bitcoin can easily\ndo in a much more simple manner what ethereum is doing\n\nIt's a bit similar to Lightning but not as sophisticated, basically the\nproof of deals are stored in OP_RETURN (looks trivial, yes, but unlike\nother solutions the proposals store a real proof and does not flood the\nbitcoin network with funny stuff, it could be turned into bitcoin\ncontracts which most likely will not be recognized as standard), we\ncannot enforce everything like Lightning but the trust here is more\nbased on a reputation model, and same as Lightning the cheater just lose\nits bitcoin or get tagged as a cheater, will be tracked and might assume\nthe consequences later\n\nAs written, I am a fan of Lightning but see it more as a middle/long\nterm relationship between people since coins must be locked into a\nmultisig transaction while here we are more talking about a one time\ndeal, where you don't know if you will buy something else to the seller,\nwith which coin and where (metaverse for example)\n\nFor your review and comments, here or in private, no real inventions\nhere but some non usual ideas like the double hash, the third party and\nothers, solving also one of my personal problematic since years: \"how to\nsell a secret NFT?\"\n\nIn any case it remains decentralized (but of course some tools/wallets\nmust ease the process for the users), like Lightning, and unlike\neverything that is existing today in those areas to my knowledge, except\nLightning again\n\nRegards\n\nAymeric\n\n\n-- \nSophia-Antipolis, France\nCV: https://www.peersm.com/CVAV.pdf\nLinkedIn: https://fr.linkedin.com/in/aymeric-vitte-05855b26\nGitHub : https://www.github.com/Ayms\nA Universal Coin Swap system based on Bitcoin: https://gist.github.com/Ayms/029125db2583e1cf9c3209769eb2cdd7\nA bitcoin NFT system: https://gist.github.com/Ayms/01dbfebf219965054b4a3beed1bfeba7\nMove your coins by yourself (browser version): https://peersm.com/wallet\nBitcoin transactions made simple: https://github.com/Ayms/bitcoin-transactions\ntorrent-live: https://github.com/Ayms/torrent-live\nnode-Tor : https://www.github.com/Ayms/node-Tor\nAnti-spies and private torrents, dynamic blocklist: http://torrent-live.peersm.com\nPeersm : http://www.peersm.com"
            },
            {
                "author": "Aymeric Vitte",
                "date": "2023-01-31T18:36:28",
                "message_text_only": "I am not sure to understand the current discussion about ordinals\nrelayed by the press, it's from my standpoint a no for storing things in\nwitness, and a no to use ordinals as a NFT system\n\nPlease remember that NFTs are not only electronic things, it can be real\nthings, or whatever you like, just referenced in the blockchain by a\n\"double hash\" for my proposal\n\nBitcoin is currently completely out of the future: the so-called \"web3\",\nand lightning will not solve everything\n\nThen please see my two proposals and comment, it's simple and easy to\nread, and just complies with all comments related to/against the\nordinals proposal (storing things for my proposals in a good old\nOP_RETURN the hashes and signatures only, not flooding bitcoin with\nproofs of nothing, docs, images, etc), or just point out other better\nsystems based on bitcoin\n\nIt just shows again that bitcoin is better in all aspects compared to\nother systems\n\nI am not against ordinals but don't see it for \"web3\"\n\n\nLe 26/01/2023 \u00e0 21:15, Aymeric Vitte a \u00e9crit :\n> Please see:\n>\n> \"A Bitcoin NFT system\"\n> https://gist.github.com/Ayms/01dbfebf219965054b4a3beed1bfeba7 \"The\n> purpose of this proposal is to propose a simple NFT system based on the\n> Bitcoin blockchain, assuming that the main purpose of a NFT is to be\n> sold/bought, but not only, it can be something that you keep for\n> yourself proving your ownership on the blockchain, or something that you\n> offer to someone else, the advantages compared to using Ethereum or any\n> blockchain/sidechain on both networks (or others) will be explained below\"\n>\n> And the continuation:\n>\n> \"A Universal Coin Swap system based on bitcoin\"\n> https://gist.github.com/Ayms/029125db2583e1cf9c3209769eb2cdd7 \"The\n> purpose here is to propose a simple Coin Swap decentralized system based\n> on Bitcoin but that works for all blockchains/tokens\"\n>\n> The idea is to propose something simple, that people can understand,\n> secured, decentralized, easy to implement/use, not expensive for the\n> users, unlike Ethereum solutions, showing also that bitcoin can easily\n> do in a much more simple manner what ethereum is doing\n>\n> It's a bit similar to Lightning but not as sophisticated, basically the\n> proof of deals are stored in OP_RETURN (looks trivial, yes, but unlike\n> other solutions the proposals store a real proof and does not flood the\n> bitcoin network with funny stuff, it could be turned into bitcoin\n> contracts which most likely will not be recognized as standard), we\n> cannot enforce everything like Lightning but the trust here is more\n> based on a reputation model, and same as Lightning the cheater just lose\n> its bitcoin or get tagged as a cheater, will be tracked and might assume\n> the consequences later\n>\n> As written, I am a fan of Lightning but see it more as a middle/long\n> term relationship between people since coins must be locked into a\n> multisig transaction while here we are more talking about a one time\n> deal, where you don't know if you will buy something else to the seller,\n> with which coin and where (metaverse for example)\n>\n> For your review and comments, here or in private, no real inventions\n> here but some non usual ideas like the double hash, the third party and\n> others, solving also one of my personal problematic since years: \"how to\n> sell a secret NFT?\"\n>\n> In any case it remains decentralized (but of course some tools/wallets\n> must ease the process for the users), like Lightning, and unlike\n> everything that is existing today in those areas to my knowledge, except\n> Lightning again\n>\n> Regards\n>\n> Aymeric\n>\n>\n\n-- \nSophia-Antipolis, France\nCV: https://www.peersm.com/CVAV.pdf\nLinkedIn: https://fr.linkedin.com/in/aymeric-vitte-05855b26\nGitHub : https://www.github.com/Ayms\nA Universal Coin Swap system based on Bitcoin: https://gist.github.com/Ayms/029125db2583e1cf9c3209769eb2cdd7\nA bitcoin NFT system: https://gist.github.com/Ayms/01dbfebf219965054b4a3beed1bfeba7\nMove your coins by yourself (browser version): https://peersm.com/wallet\nBitcoin transactions made simple: https://github.com/Ayms/bitcoin-transactions\ntorrent-live: https://github.com/Ayms/torrent-live\nnode-Tor : https://www.github.com/Ayms/node-Tor\nAnti-spies and private torrents, dynamic blocklist: http://torrent-live.peersm.com\nPeersm : http://www.peersm.com"
            }
        ],
        "thread_summary": {
            "title": "A Universal Coin Swap system based on bitcoin and a Bitcoin NFT system",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Aymeric Vitte"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 7478
        }
    },
    {
        "title": "[bitcoin-dev] Ordinal Inscription Size Limits",
        "thread_messages": [
            {
                "author": "Robert Dickinson",
                "date": "2023-01-27T12:44:10",
                "message_text_only": "I'm curious what opinions exist and what actions might be taken by core\ndevelopers regarding storing unlimited amounts of NFT (or other?) content\nas witness data (https://docs.ordinals.com/inscriptions.html). The ordinal\nscheme is elegant and genius IMHO, but when I think about the future disk\nuse of all unpruned nodes, I question whether unlimited storage is wise to\nallow for such use cases. Wouldn't it be better to find a way to impose a\nsize limit similar to OP_RETURN for such inscriptions?\n\nI think it would be useful to link a sat to a deed or other legal construct\nfor proof of ownership in the real world, so that real property can be\ntransferred on the blockchain using ordinals, but storing the property\nitself on the blockchain seems nonsensical to me.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230127/a6a99a6c/attachment.html>"
            },
            {
                "author": "rot13maxi",
                "date": "2023-01-27T12:58:49",
                "message_text_only": "Hello,\n\n\u201cUnlimited storage\u201d isn\u2019t really accurate. It\u2019s witness data in a taproot transaction, so the block size limit still applies. Anyone who runs an unpruned bitcoin node should be capacity-planning their disk space assuming that in the future blocks will be more full - as demand for blockspace increases, people will make better use of the space that we already have and average block weight will trend upwards. If you\u2019re thinking about how much disk you will need when we have consistently full blocks, ordinal inscriptions don\u2019t change that number.\n\n- rijndael\n\nOn Fri, Jan 27, 2023 at 7:44 AM, Robert Dickinson via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> I'm curious what opinions exist and what actions might be taken by core developers regarding storing unlimited amounts of NFT (or other?) content as witness data (https://docs.ordinals.com/inscriptions.html). The ordinal scheme is elegant and genius IMHO, but when I think about the future disk use of all unpruned nodes, I question whether unlimited storage is wise to allow for such use cases. Wouldn't it be better to find a way to impose a size limit similar to OP_RETURN for such inscriptions?\n>\n> I think it would be useful to link a sat to a deed or other legal construct for proof of ownership in the real world, so that real property can be transferred on the blockchain using ordinals, but storing the property itself on the blockchain seems nonsensical to me.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230127/88503ad3/attachment.html>"
            },
            {
                "author": "alicexbt",
                "date": "2023-01-28T10:58:12",
                "message_text_only": "Hi Bitcoin Developers,\n\n> Anyone who runs an unpruned bitcoin node should be capacity-planning their disk space assuming that in the future blocks will be more full - as demand for blockspace increases, people will make better use of the space that we already have and average block weight will trend upwards. If you\u2019re thinking about how much disk you will need when we have consistently full blocks, ordinal inscriptions don\u2019t change that number.\u00a0\n\nI completely agree with this.\n\n> If we ban \"useless data\" then it would be easy for would-be data storers\nto instead embed their data inside \"useful\" data such as dummy\nsignatures or public keys.\n\n> There's a reasonable argument that this sort of data is toxic to the\nnetwork, since even though \"the market is willing to bear\" the price of\nscares blockspace, if people were storing NFTs and other crap on the\nchain, then the Bitcoin fee market would become entangled with random\npump&dump markets, undermining legitimate use cases and potentially\npreventing new technology like LN from gaining a strong foothold.\n\nInitially I considered ordinals and the use of witness for inscriptions useless and harmful. However I have changed my opinion after looking at different things and reading several comments. I do not consider such things 'useless' or 'crap' and it won't affect bitcoin fee market negatively. There is no threat to LN as well.\n\nI consider every bitcoin transaction a legit use case and would like to share an example and different perspective of how such inscriptions might be used at different places:\n\nDuring the festival of Diwali, it is a common tradition among many Indian families to buy gold coins with the image of the goddess Laxmi, the goddess of wealth and prosperity. The coins are often bought as a symbol of good luck and prosperity for the upcoming year. They may also be given as gifts to family and friends or used as a form of investment. The coins can be purchased from a variety of sources, including jewelry stores and online retailers.\n\nIf people start buying bitcoin during Diwali, and sellers use the witness to include the image of Laxmi in the inputs used, it would be an innovative way of combining traditional customs with modern technology. Since some users consider bitcoin as digital gold, I won't be surprised if this really happens in future and won't consider it bad as the transactions are paying for block space used.\n\n/dev/fd0\nfloppy disc guy\n\nSent with Proton Mail secure email.\n\n------- Original Message -------\nOn Friday, January 27th, 2023 at 6:28 PM, rot13maxi via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n\n> Hello,\n> \n> \u201cUnlimited storage\u201d isn\u2019t really accurate. It\u2019s witness data in a taproot transaction, so the block size limit still applies. Anyone who runs an unpruned bitcoin node should be capacity-planning their disk space assuming that in the future blocks will be more full - as demand for blockspace increases, people will make better use of the space that we already have and average block weight will trend upwards. If you\u2019re thinking about how much disk you will need when we have consistently full blocks, ordinal inscriptions don\u2019t change that number.\u00a0\n> \n> - rijndael\n> \n> On Fri, Jan 27, 2023 at 7:44 AM, Robert Dickinson via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> > I'm curious what opinions exist and what actions might be taken by core developers regarding storing unlimited amounts of NFT (or other?) content as witness data (https://docs.ordinals.com/inscriptions.html). The ordinal scheme is elegant and genius IMHO, but when I think about the future disk use of all unpruned nodes, I question whether unlimited storage is wise to allow for such use cases. Wouldn't it be better to find a way to impose a size limit similar to OP_RETURN for such inscriptions?\n> > \n> > I think it would be useful to link a sat to a deed or other legal construct for proof of ownership in the real world, so that real property can be transferred on the blockchain using ordinals, but storing the property itself on the blockchain seems nonsensical to me."
            },
            {
                "author": "Robert Dickinson",
                "date": "2023-01-29T10:34:54",
                "message_text_only": "On Sat, Jan 28, 2023 at 7:58 AM alicexbt <alicexbt at protonmail.com> wrote:\n>\n> Hi Bitcoin Developers,\n>\n> > Anyone who runs an unpruned bitcoin node should be capacity-planning their disk space assuming that in the future blocks will be more full - as demand for blockspace increases, people will make better use of the space that we already have and average block weight will trend upwards. If you\u2019re thinking about how much disk you will need when we have consistently full blocks, ordinal inscriptions don\u2019t change that number.\n>\n> I completely agree with this.\n\nI fully agree with this too. It was a sloppy remark on my part--\nthanks for claifying. Underlying my remark was a bit of disgust from\nknowing that in the future, a (perhaps large) X number of GB of what\nshould have been financial data will actually turn out to be something\nelse entirely. Time/space on the Bitcoin blockchain is a shared\nlimited resource and should be treated accordingly. We can say \"no\nworries...the price and demand will sort everything out,\" but\nhopefully we all want Bitcoin to be the best financial tool it can be.\n\n\u201cIf the ax is not sharp and he does not make it sharp, then he must\nuse more strength. Wisdom helps one to do well.\u201d"
            },
            {
                "author": "Erik Aronesty",
                "date": "2023-01-31T08:58:46",
                "message_text_only": "my only concern is that as block space gets limited the likelihood of soft\nfork opcode tech improvement proposals getting accepted by the community\ngoes down\n\nschnorr sigs are extremely useful to me (anon, cheap multisig)\n\nand some sort of vault tech would be very helpful as well\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230131/8dfd128b/attachment.html>"
            },
            {
                "author": "Andrew Poelstra",
                "date": "2023-01-27T13:21:00",
                "message_text_only": "On Fri, Jan 27, 2023 at 09:44:10AM -0300, Robert Dickinson via bitcoin-dev wrote:\n> I'm curious what opinions exist and what actions might be taken by core\n> developers regarding storing unlimited amounts of NFT (or other?) content\n> as witness data (https://docs.ordinals.com/inscriptions.html). The ordinal\n> scheme is elegant and genius IMHO, but when I think about the future disk\n> use of all unpruned nodes, I question whether unlimited storage is wise to\n> allow for such use cases. Wouldn't it be better to find a way to impose a\n> size limit similar to OP_RETURN for such inscriptions?\n> \n> I think it would be useful to link a sat to a deed or other legal construct\n> for proof of ownership in the real world, so that real property can be\n> transferred on the blockchain using ordinals, but storing the property\n> itself on the blockchain seems nonsensical to me.\n\nUnfortunately, as near as I can tell there is no sensible way to prevent\npeople from storing arbitrary data in witnesses without incentivizing\neven worse behavior and/or breaking legitimate use cases.\n\nIf we ban \"useless data\" then it would be easy for would-be data storers\nto instead embed their data inside \"useful\" data such as dummy\nsignatures or public keys. Doing so would incur a ~2x cost to them, but\nif 2x is enough to disincentivize storage, then there's no need to have\nthis discussion because they will will be forced to stop due to fee\nmarket competition anyway. (And if not, it means there is little demand\nfor Bitcoin blockspace, so what's the problem with paying miners to fill\nit with data that validators don't even need to perform real computation\non?).\n\nBut if we were to ban \"useful\" data, for example, saying that a witness\ncan't have more than 20 signatures in it, then we are into the same\nproblem we had pre-Taproot: that it is effectively impossible construct\nsigning policies in a general and composeable way, because any software\nthat does so will need to account for multiple independent limits. We\ndeliberately replaced such limits with \"you need to pay 50 weight for\neach signature\" to makes this sort of analysis tractable.\n\nThere's a reasonable argument that this sort of data is toxic to the\nnetwork, since even though \"the market is willing to bear\" the price of\nscares blockspace, if people were storing NFTs and other crap on the\nchain, then the Bitcoin fee market would become entangled with random\npump&dump markets, undermining legitimate use cases and potentially\npreventing new technology like LN from gaining a strong foothold. But\nfrom a technical point of view, I don't see any principled way to stop\nthis.\n\n\n\n-- \nAndrew Poelstra\nDirector of Research, Blockstream\nEmail: apoelstra at wpsoftware.net\nWeb:   https://www.wpsoftware.net/andrew\n\nThe sun is always shining in space\n    -Justin Lewis-Webster\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 488 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230127/1ca9b7cf/attachment.sig>"
            },
            {
                "author": "Aymeric Vitte",
                "date": "2023-01-27T15:43:46",
                "message_text_only": "Le 27/01/2023 \u00e0 14:21, Andrew Poelstra via bitcoin-dev a \u00e9crit :\n> if people were storing NFTs and other crap on the\n> chain, then the Bitcoin fee market would become entangled with random\n> pump&dump markets\nSo you mean that Bitcoin is out for NFTs, Metaverse and \"web3\"?\n\nLN is good but I don't think it can really adapt to everything, what I\nproposed yesterday looks complementary\n\nI clearly dislike the current NFTs existing systems, and to make it\nshort NFTs as a whole until recently, it depends on what people mean by\n\"NFT\", and I did dislike any solution based on OP_RETURN (shxtty stuff\nflooding bitcoin with stupid proofs of nothing)\n\nBUT I changed my mind, one can say that I am contradicting myself\neverywhere (links in the proposals), but no, explaining why in the proposals\n\nNote that in my proposals you don't need to \"mint\" the NFTs (using a\nthird party but not a stupid ethereum/bitcoin like super sidechain) and\nthat you can reference millions of them in one transaction (low value\nNFTs like loyalty programms, discount coupons) in that case of course\nthe low value NFTs are centralized\n\nThat's the future, Bitcoin being out of this does not look plausible,\ncurrently NOBODY envisions bitcoin or LN for a web3 system, so people\nhere might destroy my proposals, then please do, but I find them quite\ngood compared to whatever exist\n\n \n\n-- \nSophia-Antipolis, France\nCV: https://www.peersm.com/CVAV.pdf\nLinkedIn: https://fr.linkedin.com/in/aymeric-vitte-05855b26\nGitHub : https://www.github.com/Ayms\nA Universal Coin Swap system based on Bitcoin: https://gist.github.com/Ayms/029125db2583e1cf9c3209769eb2cdd7\nA bitcoin NFT system: https://gist.github.com/Ayms/01dbfebf219965054b4a3beed1bfeba7\nMove your coins by yourself (browser version): https://peersm.com/wallet\nBitcoin transactions made simple: https://github.com/Ayms/bitcoin-transactions\ntorrent-live: https://github.com/Ayms/torrent-live\nnode-Tor : https://www.github.com/Ayms/node-Tor\nAnti-spies and private torrents, dynamic blocklist: http://torrent-live.peersm.com\nPeersm : http://www.peersm.com"
            },
            {
                "author": "Aymeric Vitte",
                "date": "2023-01-28T16:47:46",
                "message_text_only": "I saw the other posts, then storage in witness, no, ordinals no for now,\nlet's keep things simple and understandable\n\nI forgot to mention that the proposals envision a \"local bitcoin\" (for a\nmetaverse for example) wich is a fork of Bitcoin code, but of course not\na fork of Bitcoin, and which of course does not store anything in the\nmain Bitcoin chain, but which remains compatible (tx format) and coins\nbetween chains can be swapped\n\nMaybe nobody care, but the idea is to put bitcoin into the party as an\nalternative to this ethereum mess\n\n\nLe 27/01/2023 \u00e0 16:43, Aymeric Vitte a \u00e9crit :\n>\n> Le 27/01/2023 \u00e0 14:21, Andrew Poelstra via bitcoin-dev a \u00e9crit :\n>> if people were storing NFTs and other crap on the\n>> chain, then the Bitcoin fee market would become entangled with random\n>> pump&dump markets\n> So you mean that Bitcoin is out for NFTs, Metaverse and \"web3\"?\n>\n> LN is good but I don't think it can really adapt to everything, what I\n> proposed yesterday looks complementary\n>\n> I clearly dislike the current NFTs existing systems, and to make it\n> short NFTs as a whole until recently, it depends on what people mean by\n> \"NFT\", and I did dislike any solution based on OP_RETURN (shxtty stuff\n> flooding bitcoin with stupid proofs of nothing)\n>\n> BUT I changed my mind, one can say that I am contradicting myself\n> everywhere (links in the proposals), but no, explaining why in the proposals\n>\n> Note that in my proposals you don't need to \"mint\" the NFTs (using a\n> third party but not a stupid ethereum/bitcoin like super sidechain) and\n> that you can reference millions of them in one transaction (low value\n> NFTs like loyalty programms, discount coupons) in that case of course\n> the low value NFTs are centralized\n>\n> That's the future, Bitcoin being out of this does not look plausible,\n> currently NOBODY envisions bitcoin or LN for a web3 system, so people\n> here might destroy my proposals, then please do, but I find them quite\n> good compared to whatever exist\n>\n>  \n>\n\n-- \nSophia-Antipolis, France\nCV: https://www.peersm.com/CVAV.pdf\nLinkedIn: https://fr.linkedin.com/in/aymeric-vitte-05855b26\nGitHub : https://www.github.com/Ayms\nA Universal Coin Swap system based on Bitcoin: https://gist.github.com/Ayms/029125db2583e1cf9c3209769eb2cdd7\nA bitcoin NFT system: https://gist.github.com/Ayms/01dbfebf219965054b4a3beed1bfeba7\nMove your coins by yourself (browser version): https://peersm.com/wallet\nBitcoin transactions made simple: https://github.com/Ayms/bitcoin-transactions\ntorrent-live: https://github.com/Ayms/torrent-live\nnode-Tor : https://www.github.com/Ayms/node-Tor\nAnti-spies and private torrents, dynamic blocklist: http://torrent-live.peersm.com\nPeersm : http://www.peersm.com"
            },
            {
                "author": "Robert Dickinson",
                "date": "2023-01-28T04:26:15",
                "message_text_only": "On Fri, Jan 27, 2023 at 10:21 AM Andrew Poelstra\n<apoelstra at wpsoftware.net> wrote:\n\n8<\n\n> Unfortunately, as near as I can tell there is no sensible way to prevent\n> people from storing arbitrary data in witnesses without incentivizing\n> even worse behavior and/or breaking legitimate use cases.\n\n8<\n\n> There's a reasonable argument that this sort of data is toxic to the\n> network, since even though \"the market is willing to bear\" the price of\n> scares blockspace, if people were storing NFTs and other crap on the\n> chain, then the Bitcoin fee market would become entangled with random\n> pump&dump markets, undermining legitimate use cases and potentially\n> preventing new technology like LN from gaining a strong foothold. But\n> from a technical point of view, I don't see any principled way to stop\n> this.\n>\n>\n>\n> --\n> Andrew Poelstra\n> Director of Research, Blockstream\n> Email: apoelstra at wpsoftware.net\n> Web:   https://www.wpsoftware.net/andrew\n>\n> The sun is always shining in space\n>     -Justin Lewis-Webster\n>\n\nThank you for your reply and explanations. If it be so, then I think\nthe principled route would be to make it a priority to continuously\neducate people on the morals of the matter. Rather than for fads and\nscams, the world would be a better place if ordinal inscriptions were\nused for enduring, practical, and universally beneficial purposes,\nsuch as for domain name inscription to solve the DNS centralization\nproblem."
            }
        ],
        "thread_summary": {
            "title": "Ordinal Inscription Size Limits",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "rot13maxi",
                "Aymeric Vitte",
                "Andrew Poelstra",
                "alicexbt",
                "Erik Aronesty",
                "Robert Dickinson"
            ],
            "messages_count": 9,
            "total_messages_chars_count": 17727
        }
    },
    {
        "title": "[bitcoin-dev] Ephemeral Anchors: Fixing V3 Package RBF againstpackage limit pinning",
        "thread_messages": [
            {
                "author": "Greg Sanders",
                "date": "2023-01-27T14:05:20",
                "message_text_only": "Hello again dev,\n\nDue to the interest in the proposal and the prodding of certain folks, I've\nwritten up a short draft BIP of the Ephemeral Anchors idea here:\nhttps://github.com/instagibbs/bips/blob/ephemeral_anchor/bip-ephemeralanchors.mediawiki\n\nThe pull request at https://github.com/bitcoin/bitcoin/pull/26403 has been\nrefreshed on top of the latest V3 proposal, but the BIP itself is\nunaffected.\n\nCheers,\nGreg\n\nOn Wed, Nov 30, 2022 at 10:32 AM Greg Sanders <gsanders87 at gmail.com> wrote:\n\n> Small update.\n>\n> A bit ago I went ahead and implemented ephemeral anchors on top of the V3\n> proposal to see what the complexity looks like:\n> https://github.com/bitcoin/bitcoin/pull/26403\n>\n> Roughly 130 loc excluding tests, using OP_2 instead of OP_TRUE to not camp\n> the value that is used elsewhere.\n>\n> Please let me know if you have any early feedback on this!\n>\n> Greg\n>\n> On Thu, Oct 20, 2022 at 9:42 AM Greg Sanders <gsanders87 at gmail.com> wrote:\n>\n>> So it doesn't look like I'm ignoring a good question:\n>>\n>> No solid noninteractive ideas, unless we get some very flexible sighash\n>> softfork. Interactively, I think you can get collaborative fee bumps under\n>> the current consensus regime and ephemeral anchors. The child will just be\n>> built with inputs from different people.\n>>\n>> On Wed, Oct 19, 2022 at 11:12 AM James O'Beirne <james.obeirne at gmail.com>\n>> wrote:\n>>\n>>> I'm also very happy to see this proposal, since it gets us closer to\n>>> having a mechanism that allows the contribution to feerate in an\n>>> \"unauthenticated\" way, which seems to be a very helpful feature for vaults\n>>> and other contracting protocols.\n>>>\n>>> One possible advantage of the sponsors interface -- and I'm curious for\n>>> your input here Greg -- is that with sponsors, assuming we relaxed the \"one\n>>> sponsor per sponsoree\" constraint, multiple uncoordinated parties can\n>>> collaboratively bump a tx's feerate. A simple example would be a batch\n>>> withdrawal from an exchange could be created with a low feerate, and then\n>>> multiple users with a vested interest of expedited confirmation could all\n>>> \"chip in\" to raise the feerate with multiple sponsor transactions.\n>>>\n>>> Having a single ephemeral output seems to create a situation where a\n>>> single UTXO has to shoulder the burden of CPFPing a package. Is there some\n>>> way we could (possibly later) amend the ephemeral anchor interface to allow\n>>> for this kind of collaborative sponsoring? Could you maybe see \"chained\"\n>>> ephemeral anchors that would allow this?\n>>>\n>>>\n>>> On Tue, Oct 18, 2022 at 12:52 PM Jeremy Rubin via bitcoin-dev <\n>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>>> Excellent proposal and I agree it does capture much of the spirit of\n>>>> sponsors w.r.t. how they might be used for V3 protocols.\n>>>>\n>>>> The only drawbacks I see is they don't work for lower tx version\n>>>> contracts, so there's still something to be desired there, and that the\n>>>> requirement to sweep the output must be incentive compatible for the miner,\n>>>> or else they won't enforce it (pass the buck onto the future bitcoiners).\n>>>> The Ephemeral UTXO concept can be a consensus rule (see\n>>>> https://rubin.io/public/pdfs/multi-txn-contracts.pdf \"Intermediate\n>>>> UTXO\") we add later on in lieu of managing them by incentive, so maybe it's\n>>>> a cleanup one can punt.\n>>>>\n>>>> One question I have is if V3 is designed for lightning, and this is\n>>>> designed for lightning, is there any sense in requiring these outputs for\n>>>> v3? That might help with e.g. anonymity set, as well as potentially keep\n>>>> the v3 surface smaller.\n>>>>\n>>>> On Tue, Oct 18, 2022 at 11:51 AM Greg Sanders via bitcoin-dev <\n>>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>>\n>>>>> > does that effectively mark output B as unspendable once the child\n>>>>> gets confirmed?\n>>>>>\n>>>>> Not at all. It's a normal spend like before, since the parent has been\n>>>>> confirmed. It's completely unrestricted, not being bound to any\n>>>>> V3/ephemeral anchor restrictions on size, version, etc.\n>>>>>\n>>>>> On Tue, Oct 18, 2022 at 11:47 AM Arik Sosman via bitcoin-dev <\n>>>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>>>\n>>>>>> Hi Greg,\n>>>>>>\n>>>>>> Thank you very much for sharing your proposal!\n>>>>>>\n>>>>>> I think there's one thing about the second part of your proposal that\n>>>>>> I'm missing. Specifically, assuming the scenario of a v3 transaction with\n>>>>>> three outputs, A, B, and the ephemeral anchor OP_TRUE. If a child\n>>>>>> transaction spends A and OP_TRUE, does that effectively mark output B as\n>>>>>> unspendable once the child gets confirmed? If so, isn't the implication\n>>>>>> therefore that to safely spend a transaction with an ephemeral anchor, all\n>>>>>> outputs must be spent? Thanks!\n>>>>>>\n>>>>>> Best,\n>>>>>> Arik\n>>>>>>\n>>>>>> On Tue, Oct 18, 2022, at 6:52 AM, Greg Sanders via bitcoin-dev wrote:\n>>>>>>\n>>>>>> Hello Everyone,\n>>>>>>\n>>>>>> Following up on the \"V3 Transaction\" discussion here\n>>>>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-September/020937.html\n>>>>>> , I would like to elaborate a bit further on some potential follow-on work\n>>>>>> that would make pinning severely constrained in many setups].\n>>>>>>\n>>>>>> V3 transactions may solve bip125 rule#3 and rule#5 pinning attacks\n>>>>>> under some constraints[0]. This means that when a replacement is to be made\n>>>>>> and propagated, it costs the expected amount of fees to do so. This is a\n>>>>>> great start. What's left in this subset of pinning is *package limit*\n>>>>>> pinning. In other words, a fee-paying transaction cannot enter the mempool\n>>>>>> due to the existing mempool package it is being added to already being too\n>>>>>> large in count or vsize.\n>>>>>>\n>>>>>> Zooming into the V3 simplified scenario for sake of discussion,\n>>>>>> though this problem exists in general today:\n>>>>>>\n>>>>>> V3 transactions restrict the package limit of a V3 package to one\n>>>>>> parent and one child. If the parent transaction includes two outputs which\n>>>>>> can be immediately spent by separate parties, this allows one party to\n>>>>>> disallow a spend from the other. In Gloria's proposal for ln-penalty, this\n>>>>>> is worked around by reducing the number of anchors per commitment\n>>>>>> transaction to 1, and each version of the commitment transaction has a\n>>>>>> unique party's key on it. The honest participant can spend their version\n>>>>>> with their anchor and package RBF the other commitment transaction safely.\n>>>>>>\n>>>>>> What if there's only one version of the commitment transaction, such\n>>>>>> as in other protocols like duplex payment channels, eltoo? What about multi\n>>>>>> party payments?\n>>>>>>\n>>>>>> In the package RBF proposal, if the parent transaction is identical\n>>>>>> to an existing transaction in the mempool, the parent will be detected and\n>>>>>> removed from the package proposal. You are then left with a single V3 child\n>>>>>> transaction, which is then proposed for entry into the mempool. In the case\n>>>>>> of another parent output already being spent, this is simply rejected,\n>>>>>> regardless of feerate of the new child.\n>>>>>>\n>>>>>> I have two proposed solutions, of which I strongly prefer the latter:\n>>>>>>\n>>>>>> 1) Expand a carveout for \"sibling eviction\", where if the new child\n>>>>>> is paying \"enough\" to bump spends from the same parent, it knocks its\n>>>>>> sibling out of the mempool and takes the one child slot. This would solve\n>>>>>> it, but is a new eviction paradigm that would need to be carefully worked\n>>>>>> through.\n>>>>>>\n>>>>>> 2) Ephemeral Anchors (my real policy-only proposal)\n>>>>>>\n>>>>>> Ephemeral Anchors is a term which means an output is watermarked as\n>>>>>> an output that MUST be spent in a V3 package. We mark this anchor by being\n>>>>>> the bare script `OP_TRUE` and of course make these outputs standard to\n>>>>>> relay and spend with empty witness data.\n>>>>>>\n>>>>>> Also as a simplifying assumption, we require the parent transaction\n>>>>>> with such an output to be 0-fee. This makes mempool reasoning simpler in\n>>>>>> case the child-spend is somehow evicted, guaranteeing the parent will be as\n>>>>>> well.\n>>>>>>\n>>>>>> Implications:\n>>>>>>\n>>>>>> a) If the ephemeral anchor MUST be spent, we can allow *any* value,\n>>>>>> even dust, even 0, without worrying about bloating the utxo set. We relax\n>>>>>> this policy for maximum smart contract flexibility and specification\n>>>>>> simplicity..\n>>>>>>\n>>>>>> b) Since this anchor MUST be spent, any spending of other outputs in\n>>>>>> the same parent transaction MUST directly double-spend prior spends of the\n>>>>>> ephemeral anchor. This causes the 1 block CSV timelock on outputs to be\n>>>>>> removed in these situations. This greatly magnifies composability of smart\n>>>>>> contracts, as now we can do things like safely splice directly into new\n>>>>>> channels, into statechains, your custodial wallet account, your cold\n>>>>>> wallet, wherever, without requiring other wallets to support arbitrary\n>>>>>> scripts. Also it hurts that 1 CSV time locked scripts may not be miniscript\n>>>>>> compatible to begin with...\n>>>>>>\n>>>>>> c) *Anyone* can bump the transaction, without any transaction key\n>>>>>> material. This is essentially achieving Jeremy's Transaction Sponsors (\n>>>>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-September/018168.html)\n>>>>>> proposal without consensus changes. As long as someone gets a fully signed\n>>>>>> parent, they can execute a bump with minimal wallet tooling. If a\n>>>>>> transaction author doesn\u2019t want a \u201csponsor\u201d, do not include the output.\n>>>>>>\n>>>>>> d) Lightning Carve-out(\n>>>>>> https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-October/002240.html)\n>>>>>> is superseded by this logic, as we are not restricted to two immediately\n>>>>>> spendable output scenarios. In its place, robust multi-party fee bumping is\n>>>>>> possible.\n>>>>>>\n>>>>>> e) This also benefits more traditional wallet scenarios, as change\n>>>>>> outputs can no longer be pinned, and RBF/CPFP becomes robust. Payees in\n>>>>>> simple spends cannot pin you. Batched payouts become a lot less painful.\n>>>>>> This was one of the motivating use cases that created the term \u201cpinning\u201d in\n>>>>>> the first place(\n>>>>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-February/015717.html),\n>>>>>> even if LN/L2 discussion has largely overtaken it due to HTLC theft risks.\n>>>>>>\n>>>>>> Open Question(s):\n>>>>>>\n>>>>>>\n>>>>>>    1.\n>>>>>>\n>>>>>>    If we allow non-zero value in ephemeral outputs, does this open\n>>>>>>    up a MEV we are worried about? Wallets should toss all the value directly\n>>>>>>    to fees, and add their own additional fees on top, otherwise miners have\n>>>>>>    incentive to make the smallest utxo burn transaction to claim those funds.\n>>>>>>    They just confirmed your parent transaction anyways, so do we care?\n>>>>>>    2.\n>>>>>>\n>>>>>>    SIGHASH_GROUP like constructs would allow uncommitted ephemeral\n>>>>>>    anchors to be added at spend time, depending on spending requirements.\n>>>>>>    SIGHASH_SINGLE already allows this.\n>>>>>>\n>>>>>>\n>>>>>>\n>>>>>>\n>>>>>> Hopefully this gives people something to consider as we move forward\n>>>>>> in thinking about mempool design within the constraints we have today.\n>>>>>>\n>>>>>>\n>>>>>> Greg\n>>>>>>\n>>>>>> 0: With V3 transactions where you have \"veto power\" over all the\n>>>>>> inputs in that transaction. Therefore something like ANYONECANPAY is still\n>>>>>> broken. We need a more complex solution, which I\u2019m punting for the sake of\n>>>>>> progress.\n>>>>>> _______________________________________________\n>>>>>> bitcoin-dev mailing list\n>>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>>\n>>>>>>\n>>>>>> _______________________________________________\n>>>>>> bitcoin-dev mailing list\n>>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>>\n>>>>> _______________________________________________\n>>>>> bitcoin-dev mailing list\n>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>\n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>\n>>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230127/63147572/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Ephemeral Anchors: Fixing V3 Package RBF againstpackage limit pinning",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Greg Sanders"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 12686
        }
    },
    {
        "title": "[bitcoin-dev] Reference example bech32m address for future segwit versions",
        "thread_messages": [
            {
                "author": "David A. Harding",
                "date": "2023-01-31T00:02:51",
                "message_text_only": "Hi y'all!,\n\nOne of the benefits proposed for bech32 (and, by extension, bech32m) is\nthat spender wallets shouldn't need to be upgraded to pay segwit outputs\ndefined in future soft forks.  For example, Bitcoin Core today can pay a\nbech32m address for a segwit v2 output, even though no meaning has been\nassigned to output scripts matching a segwit v2 template.\n\nHowever, testing this behavior in production[1] can create an annoyance\nfor developers of future soft forks.  They will need to deal with any\nexisting outputs paid to the templates used in that proposed soft fork.\nSee, for example, some discussion by developer 0xB10C about payments to\nsegwit v1 addresses before activation of the taproot soft fork:\nhttps://b10c.me/blog/007-spending-p2tr-pre-activation/\n\nI was wondering if it would be useful to have a canonical examples of\nfuture segwit addresses that are designed to be very unlikely to\ninterfere with future soft forks but which would still reasonably\nexercise wallets supporting bech32m.  I think of this as the rough\nequivalent of the RFC2606 domain \"example.com\" which has been reserved\nfor examples in documentation.\n\nSpecifically, I'm thinking of the following addresses, one each for\nmainnet and testnet:\n\n- HRP: bc for mainnet; tb for testent\n- Witness version: 16 (the last segwit version)\n- Witness program: 0x0000.  Two bytes is the minimum allowed\n   by BIP141, but it's far too small to make any sort of secure \ncommitment,\n   so I'm hoping it won't conflict with any future use\n\nI think we should try to start with just one reserved address per\nnetwork, but if that isn't enough, I think we could allow any two-byte\nwitness program with witness version 16.\n\nOutputs paid to reserved addresses will still be anyone-can-spend, so\nthere's no change required to Bitcoin Core or other software and those\noutputs can still be cleaned out of the UTXO set.  Additionally, if we\never *really* need that address space for a soft fork, it will be\navailable.\n\nAre there any objections to this idea, or suggestions for a better way\nto go about it?\n\nThanks!,\n\n-Dave\n\n[1] Testing in production should be avoided because it uses block space\nthat could otherwise be used by actual value transfers.  Also, it costs\nmoney and pollutes the UTXO set (at least temporarily).  However, when\ntesting whether proprietary third-party software, such as an exchange,\nsupports payments to future segwit versions, sometimes the only\nconvenient method is to actually pay the address for a future segwit\nversion.  Additionally, my specific use case is just to write \ndocumentation\nabout bech32m---but I worry that people will pay my example of a future \nsegwit\nversion address."
            },
            {
                "author": "Greg Sanders",
                "date": "2023-01-31T14:30:34",
                "message_text_only": "Hi David,\n\n>From practical experience, I think you'll find that most exchanges will not\nenable sends to future segwit versions,\nas from a risk perspective it's likely a mistake to send funds there. That\nsaid, as long as we don't change\nthe checksum again(!), updating to new versions should be fairly straight\nforward. Every update will be a matter\nof allowing a new version and a new length instead of requiring\nlibrary updates. Making sure the most popular\nopen source libraries support it is probably the best way to spend energy\nensuring that future upgrades go smoothly.\n\nBest,\nGreg\n\nOn Mon, Jan 30, 2023 at 8:25 PM David A. Harding via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi y'all!,\n>\n> One of the benefits proposed for bech32 (and, by extension, bech32m) is\n> that spender wallets shouldn't need to be upgraded to pay segwit outputs\n> defined in future soft forks.  For example, Bitcoin Core today can pay a\n> bech32m address for a segwit v2 output, even though no meaning has been\n> assigned to output scripts matching a segwit v2 template.\n>\n> However, testing this behavior in production[1] can create an annoyance\n> for developers of future soft forks.  They will need to deal with any\n> existing outputs paid to the templates used in that proposed soft fork.\n> See, for example, some discussion by developer 0xB10C about payments to\n> segwit v1 addresses before activation of the taproot soft fork:\n> https://b10c.me/blog/007-spending-p2tr-pre-activation/\n>\n> I was wondering if it would be useful to have a canonical examples of\n> future segwit addresses that are designed to be very unlikely to\n> interfere with future soft forks but which would still reasonably\n> exercise wallets supporting bech32m.  I think of this as the rough\n> equivalent of the RFC2606 domain \"example.com\" which has been reserved\n> for examples in documentation.\n>\n> Specifically, I'm thinking of the following addresses, one each for\n> mainnet and testnet:\n>\n> - HRP: bc for mainnet; tb for testent\n> - Witness version: 16 (the last segwit version)\n> - Witness program: 0x0000.  Two bytes is the minimum allowed\n>    by BIP141, but it's far too small to make any sort of secure\n> commitment,\n>    so I'm hoping it won't conflict with any future use\n>\n> I think we should try to start with just one reserved address per\n> network, but if that isn't enough, I think we could allow any two-byte\n> witness program with witness version 16.\n>\n> Outputs paid to reserved addresses will still be anyone-can-spend, so\n> there's no change required to Bitcoin Core or other software and those\n> outputs can still be cleaned out of the UTXO set.  Additionally, if we\n> ever *really* need that address space for a soft fork, it will be\n> available.\n>\n> Are there any objections to this idea, or suggestions for a better way\n> to go about it?\n>\n> Thanks!,\n>\n> -Dave\n>\n> [1] Testing in production should be avoided because it uses block space\n> that could otherwise be used by actual value transfers.  Also, it costs\n> money and pollutes the UTXO set (at least temporarily).  However, when\n> testing whether proprietary third-party software, such as an exchange,\n> supports payments to future segwit versions, sometimes the only\n> convenient method is to actually pay the address for a future segwit\n> version.  Additionally, my specific use case is just to write\n> documentation\n> about bech32m---but I worry that people will pay my example of a future\n> segwit\n> version address.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20230131/28cd677c/attachment.html>"
            },
            {
                "author": "David A. Harding",
                "date": "2023-01-31T23:33:13",
                "message_text_only": "On 2023-01-31 04:30, Greg Sanders wrote:\n> Hi David,\n> \n> From practical experience, I think you'll find that most exchanges\n> will not enable sends to future segwit versions,\n> as from a risk perspective it's likely a mistake to send funds there.\n\nHi Greg!,\n\nI thought the best practice[1] was that wallets would spend to the \noutput indicated by any valid bech32m address.  You seem to implying \nthat the best practice is the opposite: that wallets should only send to \noutputs they know can be secured (i.e., which are not currently \nanyone-can-spend).  The more restrictive approach seems kind of sad to \nme since any problem which can result in a user accidentally withdrawing \nto a future segwit version could even more easily result in them \nwithdrawing to a witness program for which there is no solution (i.e., \nno key or script is known to spend).\n\nIf it is a best practice, then I think there's a benefit to being able \nto test it even when other people's proprietary software is involved.  A \nwallet or service likely to follow that best practice may be more likely \nto follow other best practices which cannot be as easily tested for.  \nBut, if it's going to be tested, I want the testing to use the address \nleast likely to cause problems for protocol developers in the future.  \nDo you (and others on this list) have any reason to believe OP_16 \nOP_PUSH2 0000 would be a problematic script, or can you think of a \nbetter script?\n\nThanks!,\n\n-Dave\n\n[1] BIP350, emphasis in original: \"[...] we emphatically recommend [...] \nensuring that your implementation supports sending to v1 **and higher \nversions.**\""
            }
        ],
        "thread_summary": {
            "title": "Reference example bech32m address for future segwit versions",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "David A. Harding",
                "Greg Sanders"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 8149
        }
    }
]