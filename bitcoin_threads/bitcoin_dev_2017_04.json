[
    {
        "title": "[bitcoin-dev] Hard fork proposal from last week's meeting",
        "thread_messages": [
            {
                "author": "Rodney Morris",
                "date": "2017-04-01T01:41:58",
                "message_text_only": "I didn't say typical, I said every. Currently a raspberry pi on shitty adsl\ncan run a full node. What's wrong with needing a high end pc and good\nconnectivity to run a full node?\n\nPeople that want to, can. People that don't want to, won't, no matter how\nlow spec the machine you need.\n\nIf nobody uses bitcoin, all the security in the world provides no value.\nThe value of bitcoin is provided by people using bitcoin, and people will\nonly use bitcoin if it provides value to them.  Security is one aspect\nonly. And the failure to understand that is what has led to the block size\ndebate.\n\nRodney\n\nOn 1 Apr 2017 10:12, \"Eric Voskuil\" <eric at voskuil.org> wrote:\n\n-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\n\nOn 03/31/2017 02:23 PM, Rodney Morris via bitcoin-dev wrote:\n> If the obsession with every personal computer being able to run a\n> fill node continues then bitcoin will be consigned to the dustbin\n> of history,\n\nThe cause of the block size debate is the failure to understand the\nBitcoin security model. This failure is perfectly exemplified by the\nabove statement. If a typical personal computer cannot run a node\nthere is no security.\n\ne\n-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v2.0.22 (GNU/Linux)\n\niQEcBAEBCAAGBQJY3uJ8AAoJEDzYwH8LXOFOrBoH/1VdXQObKZ2JPHL387Sd8qT4\nzzWt8tKFD+6/uCS8re97h1lZcbwb3EzBOB1J15mJ3fqTOU/rPCitN+JZAMgpw/z9\nNGNp4KQDHo3vLiWWOq2GhJzyVAOcDKYLsY8/NrHK91OtABD2XIq9gERwRoZZE4rb\nOPSjSAGvDK8cki72O7HpyEKX5WEyHsHNK/JmBDdTjlzkMcNEbBlYMgO24RC6x+UA\n8Fh17rOcfGv6amIbmS7mK3EMkkGL83WmsgJKXNl4inI1R8z5hVKRqOFMPxmTDXVc\ndEHtw8poHOX1Ld85m0+Tk2S7IdH66PCnhsKL9l6vlH02uAvLNfKxb+291q2g3YU=\n=HPCK\n-----END PGP SIGNATURE-----\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170401/748c1f45/attachment.html>"
            },
            {
                "author": "Jared Lee Richardson",
                "date": "2017-04-01T06:15:09",
                "message_text_only": "> So your cluster isn't going to need to plan to handle 15k transactions per second, you're really looking at more like 200k or even 500k transactions per second to handle peak-volumes. And if it can't, you're still going to see full blocks.\n\nWhen I first began to enter the blocksize debate slime-trap that we\nhave all found ourselves in, I had the same line of reasoning that you\nhave now.  It is clearly untenable that blockchains are an incredibly\ninefficient and poorly designed system for massive scales of\ntransactions, as I'm sure you would agree.  Therefore, I felt it was\nan important point for people to accept this reality now and stop\ntrying to use Blockchains for things they weren't good for, as much\nfor their own good as anyone elses.  I backed this by calculating some\nminer fee requirements as well as the very issue you raised.  A few\npeople argued with me rationally, and gradually I was forced to look\nat a different question: Granted that we cannot fit all desired\ntransactions on a blockchain, how many CAN we effectively fit?\n\nIt took another month before I actually changed my mind.  What changed\nit was when I tried to make estimations, assuming all the reasonable\ntrends I could find held, about future transaction fees and future\nnode costs.  Did they need to go up exponentially?  How fast, what\nwould we be dealing with in the future?  After seeing the huge\ndivergence in node operational costs without size increases($3 vs\n$3000 after some number of years stands out in my memory), I tried to\nadjust various things, until I started comparing the costs in BTC\nterms.  I eventually realized that comparing node operational costs in\nBTC per unit time versus transaction costs in dollars revealed that\nnode operational costs per unit time could decrease without causing\ntransaction fees to rise.  The transaction fees still had to hit $1 or\n$2, sometimes $4, to remain a viable protection, but otherwise they\ncould become stable around those points and node operational costs per\nunit time still decreased.\n\nNone of that may mean anything to you, so you may ignore it all if you\nlike, but my point in all of that is that I once used similar logic,\nbut any disagreements we may have does not mean I magically think as\nyou implied above.  Some people think blockchains should fit any\ntransaction of any size, and I'm sure you and I would both agree\nthat's ridiculous.  Blocks will nearly always be full in the future.\nThere is no need to attempt to handle unusual volume increases - The\nfee markets will balance it and the use-cases that can barely afford\nto fit on-chain will simply have to wait for awhile.  The question is\nnot \"can we handle all traffic,\" it is \"how many use-cases can we\nenable without sacrificing our most essential features?\"  (And for\nthat matter, what is each essential feature, and what is it worth?)\n\nThere are many distinct cut-off points that we could consider.  On the\nextreme end, Raspberry Pi's and toasters are out.  Data-bound mobile\nphones are out for at least the next few years if ever.  Currently the\nconcern is around home user bandwidth limits.  The next limit after\nthat may either be the CPU, memory, or bandwidth of a single top-end\nPC.  The limit after that may be the highest dataspeeds that large,\nremote Bitcoin mining facilities are able to afford, but after fees\nrise and a few years, they may remove that limit for us.  Then the\nnext limit might be on the maximum amount of memory available within a\nsingle datacenter server.\n\nAt each limit we consider, we have a choice of killing off a number of\non-chain usecases versus the cost of losing the nodes who can't reach\nthe next limit effectively.  I have my inclinations about where the\nlimits would be best set, but the reality is I don't know the numbers\non the vulnerability and security risks associated with various node\ndistributions.  I'd really like to, because if I did I could begin\nevaluating the costs on each side.\n\n> How much RAM do you need to process blocks like that?\n\nThat's a good question, and one I don't have a good handle on.  How\ndoes Bitcoin's current memory usage scale?  It can't be based on the\nUTXO, which is 1.7 GB while my node is only using ~450mb of ram.  How\ndoes ram consumption increase with a large block versus small ones?\nAre there trade-offs that can be made to write to disk if ram usage\ngrew too large?\n\nIf that proved to be a prohibitively large growth number, that becomes\na worthwhile number to consider for scaling.  Of note, you can\ncurrently buy EC2 instances with 256gb of ram easily, and in 14 years\nthat will be even higher.\n\n> So you have to rework the code to operate on a computer cluster.\n\nI believe this is exactly the kind of discussion we should be having\n14 years before it might be needed.  Also, this wouldn't be unique -\nSome software I have used in the past (graphite metric collection)\ncame pre-packaged with the ability to scale out to multiple machines\nsplit loads and replicate the data, and so could future node software.\n\n> Further, are storage costs consistent when we're talking about setting up clusters? Are bandwidth costs consistent when we're talking about setting up clusters? Are RAM and CPU costs consistent when we're talking about setting up clusters? No, they aren't.\n\nBandwidth costs are, as intra-datacenter bandwidth is generally free.\nThe other ones warrant evaluation for the distant future.  I would\nexpect that CPU resources is the first thing we would have to change -\n13 thousand transactions per second is an awful lot to process.  I'm\nnot intimately familiar with the processing - Isn't it largely\nsignature verification of the transaction itself, plus a minority of\ntime spent checking and updating utxo values, and finally a small\nnumber of hashes to check block validity?  If signature verification\nwas controlling, a specialized asic chip(on a plug-in card) might be\nable to verify signatures hundreds of times faster, and it could even\nbe on a cheap 130nm chipset like the first asic miners rushed to\nmarket.  Point being, there are options and it may warrant looking\ninto after the risk to node reductions.\n\n> You'd need a handful of experts just to maintain such a thing.\n\nI don't think this is as big a deal as it first might seem.  The\nsoftware would already come written to be spanned onto multiple\nmachines - it just needs to be configured.  For the specific question\nat hand, the exchange would already have IT staff and datacenter\ncapacity/operations for their other operations.  In the more general\ncase, the numbers involved don't work out to extreme concerns at that\nlevel.  The highest cpu usage I've observed on my nodes is less than\n5%, less than 1% for the time I just checked, handling ~3 tx/s.  So\nbeing conservative, if it hits 100% on one core at 60-120 tx/s, that\nworks out to ~25-50 8-core machines.  But again, that's a 2-year old\nlaptop CPU and we're talking about 14 years into the future.  Even if\nit was 25 machines, that's the kind of operation a one or two man IT\nteam just runs on the side with their extra duties.  It isn't enough\nto hire a fulltime tech for.\n\n> Disks are going to be failing every day when you are storing multiple PB, so you can't just count a flat cost of $20/TB and expect that to work.\n\nI mean, that's literally what Amazon does for you with S3, which was\neven cheaper than the EBS datastore pricing I was looking at.  So....\nEven disregarding that, raid operation was a solved thing more than 10\nyears ago, and hard drives 14 years out would be roughly ~110 TB for a\n$240 hard drive at a 14%/year growth rate.  In 2034 the blockchain\nwould fit on 10 of those.  Not exactly a \"failing every day\" kind of\nproblem.  By 2040, you'd need *gasp* 22 $240 hard drives.  I mean, it\nis a lot, but not a lot like you're implying.\n\n> And you need a way to rebuild everything without taking the system offline.\n\nThat depends heavily upon the tradeoffs the businesses can make.  I\ndon't think node operation at an exchange is a five-nines uptime\noperation.  They could probably tolerate 3 nines.  The worst that\nhappens is occasionally people's withdrawals and deposit are delayed\nslightly.  It won't shut down trading.\n\n> I'm sure there are a dozen other significant issues that one of the Visa architects could tell you about when dealing with mission-critical data at this scale.\n\nVisa stores the only copy.  They can't afford to lose the data.\nBitcoin isn't like that, as others pointed out.  And for most\nbusinesses, if their node must be rebooted periodically, it isn't a\nhuge deal.\n\n> Once we grow the blocksize large enough that a single computer can't do all the processing all by itself we get into a world of much harder, much more expensive scaling problems.\n\nOk, when is that point, and what is the tradeoff in terms of nodes?\nJust because something is hard doesn't mean it isn't worth doing.\nThat's just a defeatist attitude.  How big can we get, for what\ntradeoffs, and what do we need to do to get there?\n\n> You have to check each transaction against each other transaction to make sure that they aren't double spending eachother.\n\nThis is really not that hard.  Have a central database, update/check\nthe utxo values in block-store increments.  If a utxo has already been\nused this increment, the block is invalid.  If the database somehow\ngot too big(not going to happen at these scales, but if it did), it\ncan be sharded trivially on the transaction information.  These are\nsolved problems, the free database software that's available is pretty\npowerful.\n\n> You have to be a lot more clever than that to get things working and consistent.\n\nNO, NOT CLEVER.  WE CAN'T DO THAT.\n\nSorry, I had to. :)\n\n> None of them have cost structures in the 6 digit range, and I'd bet (without actually knowing) that none of them have cost structures in the 7 digit range either.\n\nI know of and have experience working with systems that handled\nseveral orders of magnitude more data than this.  None of the issues\nbrought up above are problems that someone hasn't solved.  Transaction\ncommitments to databases?  Data consistency across multiple workers?\nData storage measured in exabytes?  Data storage and updates\napproaching hundreds of millions of datapoints per second?  These\nthings are done every single day at numerous companies.\n\nOn Fri, Mar 31, 2017 at 11:23 AM, David Vorick <david.vorick at gmail.com> wrote:\n> Sure, your math is pretty much entirely irrelevant because scaling systems\n> to massive sizes doesn't work that way.\n>\n> At 400B transactions per year we're looking at block sizes of 4.5 GB, and a\n> database size of petabytes. How much RAM do you need to process blocks like\n> that? Can you fit that much RAM into a single machine? Okay, you can't fit\n> that much RAM into a single machine. So you have to rework the code to\n> operate on a computer cluster.\n>\n> Already we've hit a significant problem. You aren't going to rewrite Bitcoin\n> to do block validation on a computer cluster overnight. Further, are storage\n> costs consistent when we're talking about setting up clusters? Are bandwidth\n> costs consistent when we're talking about setting up clusters? Are RAM and\n> CPU costs consistent when we're talking about setting up clusters? No, they\n> aren't. Clusters are a lot more expensive to set up per-resource because\n> they need to talk to eachother and synchronize with eachother and you have a\n> LOT more parts, so you have to build in redundancies that aren't necessary\n> in non-clusters.\n>\n> Also worth pointing out that peak transaction volumes are typically 20-50x\n> the size of typical transaction volumes. So your cluster isn't going to need\n> to plan to handle 15k transactions per second, you're really looking at more\n> like 200k or even 500k transactions per second to handle peak-volumes. And\n> if it can't, you're still going to see full blocks.\n>\n> You'd need a handful of experts just to maintain such a thing. Disks are\n> going to be failing every day when you are storing multiple PB, so you can't\n> just count a flat cost of $20/TB and expect that to work. You're going to\n> need redundancy and tolerance so that you don't lose the system when a few\n> of your hard drives all fail within minutes of eachother. And you need a way\n> to rebuild everything without taking the system offline.\n>\n> This isn't even my area of expertise. I'm sure there are a dozen other\n> significant issues that one of the Visa architects could tell you about when\n> dealing with mission-critical data at this scale.\n>\n> --------\n>\n> Massive systems operate very differently and are much more costly per-unit\n> than tiny systems. Once we grow the blocksize large enough that a single\n> computer can't do all the processing all by itself we get into a world of\n> much harder, much more expensive scaling problems. Especially because we're\n> talking about a distributed system where the nodes don't even trust each\n> other. And transaction processing is largely non-parallel. You have to check\n> each transaction against each other transaction to make sure that they\n> aren't double spending eachother. This takes synchronization and prevents\n> 500 CPUs from all crunching the data concurrently. You have to be a lot more\n> clever than that to get things working and consistent.\n>\n> When talking about scalability problems, you should ask yourself what other\n> systems in the world operate at the scales you are talking about. None of\n> them have cost structures in the 6 digit range, and I'd bet (without\n> actually knowing) that none of them have cost structures in the 7 digit\n> range either. In fact I know from working in a related industry that the\n> cost structures for the datacenters (plus the support engineers, plus the\n> software management, etc.) that do airline ticket processing are above $5\n> million per year for the larger airlines. Visa is probably even more\n> expensive than that (though I can only speculate)."
            },
            {
                "author": "Jared Lee Richardson",
                "date": "2017-04-01T06:18:29",
                "message_text_only": "> If a typical personal computer cannot run a node\n> there is no security.\n\nIf you can't describe an attack that is made possible when typical\npersonal computers can't run nodes, this kind of logic has no place in\nthis discussion.\n\nOn Fri, Mar 31, 2017 at 4:13 PM, Eric Voskuil via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> -----BEGIN PGP SIGNED MESSAGE-----\n> Hash: SHA256\n>\n> On 03/31/2017 02:23 PM, Rodney Morris via bitcoin-dev wrote:\n>> If the obsession with every personal computer being able to run a\n>> fill node continues then bitcoin will be consigned to the dustbin\n>> of history,\n>\n> The cause of the block size debate is the failure to understand the\n> Bitcoin security model. This failure is perfectly exemplified by the\n> above statement. If a typical personal computer cannot run a node\n> there is no security.\n>\n> e\n> -----BEGIN PGP SIGNATURE-----\n> Version: GnuPG v2.0.22 (GNU/Linux)\n>\n> iQEcBAEBCAAGBQJY3uJ8AAoJEDzYwH8LXOFOrBoH/1VdXQObKZ2JPHL387Sd8qT4\n> zzWt8tKFD+6/uCS8re97h1lZcbwb3EzBOB1J15mJ3fqTOU/rPCitN+JZAMgpw/z9\n> NGNp4KQDHo3vLiWWOq2GhJzyVAOcDKYLsY8/NrHK91OtABD2XIq9gERwRoZZE4rb\n> OPSjSAGvDK8cki72O7HpyEKX5WEyHsHNK/JmBDdTjlzkMcNEbBlYMgO24RC6x+UA\n> 8Fh17rOcfGv6amIbmS7mK3EMkkGL83WmsgJKXNl4inI1R8z5hVKRqOFMPxmTDXVc\n> dEHtw8poHOX1Ld85m0+Tk2S7IdH66PCnhsKL9l6vlH02uAvLNfKxb+291q2g3YU=\n> =HPCK\n> -----END PGP SIGNATURE-----\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Eric Voskuil",
                "date": "2017-04-01T07:41:46",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\n\nOn 03/31/2017 11:18 PM, Jared Lee Richardson wrote:\n>> If a typical personal computer cannot run a node there is no\n>> security.\n> \n> If you can't describe an attack that is made possible when typical \n> personal computers can't run nodes, this kind of logic has no place\n> in this discussion.\n\n\"Governments are good at cutting off the heads of a centrally\ncontrolled networks...\"\n\ne\n-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v2.0.22 (GNU/Linux)\n\niQEcBAEBCAAGBQJY31m0AAoJEDzYwH8LXOFOayIH/0DcWukHZUVTV8952mkWnqjS\nRCM8StQOuuTQ/2elvKoZa/nEv1PvpOQEO/AxJDEdIKOqjdXoc/QdZT/Qj834yyFi\nmmNLm3x8voO7rTFEVtBrXQ4VYO7Zj5gVy6nRyMrhSGtzg4XqYiyGVoijiumfXOvq\nejLwyWJEf8klBwegIPkX4XX6UYjNyBt+E32Je7NxUbi54EPDRszWpEGGKfJrWiCQ\nJO2jqB3O2RbMd0J1onBt2AGsjeQSE3HO0EBQSkdGQZ7PVSdE3I49uT2aAaScnPOt\nymbNz4QtlUWWpUgEI6VSjxHCGjX4+Vrn3HLRwjLe4nS2EX3mOVNY8MHMvbCeAuY=\n=tD9k\n-----END PGP SIGNATURE-----"
            },
            {
                "author": "Natanael",
                "date": "2017-04-01T13:26:35",
                "message_text_only": "Den 1 apr. 2017 01:13 skrev \"Eric Voskuil via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org>:\n\n-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\n\nOn 03/31/2017 02:23 PM, Rodney Morris via bitcoin-dev wrote:\n> If the obsession with every personal computer being able to run a\n> fill node continues then bitcoin will be consigned to the dustbin\n> of history,\n\nThe cause of the block size debate is the failure to understand the\nBitcoin security model. This failure is perfectly exemplified by the\nabove statement. If a typical personal computer cannot run a node\nthere is no security.\n\n\nIf you're capable of running and trusting your own node chances are you\nalready have something better than a typical personal computer!\n\nAnd those who don't have it themselves likely know where they can run or\naccess a node they can trust.\n\nIf you're expecting average joe to trust the likely not updated node on his\nold unpatched computer full of viruses, you're going to have a bad time.\n\nThe real solution is to find ways to reduce the required trust in a\npractical manner.\n\nUsing lightweight clients with multiple servers have already been\nmentioned, Zero-knowledge proofs (if the can be made practical and stay\nsecure...) is another obvious future tool, and hardware wallets helps\nagainst malware.\n\nIf you truly want everybody to run their own full nodes, the only plausible\nsolution is managed hardware in the style of Chromebooks, except that you\ncould pick your own distribution and software repository. Meaning you're\nstill trusting the exact same people whose nodes you would otherwise rely\non, except now you're mirroring their nodes on your own hardware instead.\nWhich at most improves auditability.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170401/d37e06c6/attachment-0001.html>"
            },
            {
                "author": "Natanael",
                "date": "2017-04-01T14:45:41",
                "message_text_only": "Den 1 apr. 2017 16:35 skrev \"Eric Voskuil via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org>:\n\n-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\n\nOn 03/31/2017 11:18 PM, Jared Lee Richardson wrote:\n>> If a typical personal computer cannot run a node there is no\n>> security.\n>\n> If you can't describe an attack that is made possible when typical\n> personal computers can't run nodes, this kind of logic has no place\n> in this discussion.\n\n\"Governments are good at cutting off the heads of a centrally\ncontrolled networks...\"\n\n\nThat's what's so great about Bitcoin. The blockchain is the same\neverywhere.\n\nSo if you can connect to private peers in several jurisdictions, chances\nare they won't all be lying to you in the exact same way. Which is what\nthey would need to do to fool you.\n\nIf you run your own and can't protect it, they'll just hack your node and\nmake it lie to you.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170401/7833b665/attachment.html>"
            },
            {
                "author": "Leandro Coutinho",
                "date": "2017-04-01T16:15:32",
                "message_text_only": "One interesting thing to do is to compare how much does it cost to maintain\na bank check account and how much does it cost to run a full node.\n\nIt seems that it is about 120USD/year in USA:\nhttp://m.huffpost.com/us/entry/6219730\n\nA 4TB hard drive ~=115USD\nhttps://www.amazon.com/gp/aw/d/B01LQQH86A/ref=mp_s_a_1_4\n\nAnd it has a warranty of 3 years.\n\nAs your calculation shows, it will take more than 19 years to reach 4TB\nwith a 4MB blocksize.\n\nEm 29/03/2017 12:35, \"Johnson Lau via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> escreveu:\n\n\nOn 29 Mar 2017, at 14:24, Emin G\u00fcn Sirer via bitcoin-dev <bitcoin-dev at lists.\nlinuxfoundation.org> wrote:\n\n>Even when several of the experts involved in the document you refer has my\nrespect and admiration, I do not agree with some of their conclusions\n\nI'm one of the co-authors of that study. I'd be the first to agree with\nyour conclusion\nand argue that the 4MB size suggested in that paper should not be used\nwithout\ncompensation for two important changes to the network.\n\n\nOur recent measurements of the Bitcoin P2P network show that network speeds\nhave improved tremendously. From February 2016 to February 2017, the average\nprovisioned bandwidth of a reachable Bitcoin node went up by approximately\n70%.\nAnd that's just in the last year.\n\n\n4 * 144 * 30 = 17.3GB per month, or 207GB per year. Full node\ninitialisation will become prohibitive for most users until a shortcut is\nmade (e.g. witness pruning and UTXO commitment but these are not trust-free)\n\n\nFurther, the emergence of high-speed block relay networks, like Falcon (\nhttp://www.falcon-net.org)\nand FIBRE, as well as block compression, e.g. BIP152 and xthin, change the\npicture dramatically.\n\n\nAlso as the co-author of the selfish mining paper, you should know all\nthese technology assume big miners being benevolent.\n\n\nSo, the 4MB limit mentioned in our paper should not be used as a protocol\nlimit today.\n\nBest,\n- egs\n\n\n\nOn Tue, Mar 28, 2017 at 3:36 PM, Juan Garavaglia via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Alphonse,\n>\n>\n>\n> Even when several of the experts involved in the document you refer has my\n> respect and admiration, I do not agree with some of their conclusions some\n> of their estimations are not accurate other changed like Bootstrap Time,\n> Cost per Confirmed Transaction they consider a network of 450,000,00 GH and\n> today is 3.594.236.966 GH, the energy consumption per GH is old, the cost\n> of electricity is wrong even when the document was made and is hard to find\n> any parameter used that is valid for an analysis today.\n>\n>\n>\n> Again with all respect to the experts involved in that analysis is not\n> valid today.\n>\n>\n>\n> I tend to believe more in Moore\u2019s law, Butters' Law of Photonics and\n> Kryder\u2019s Law all has been verified for many years and support that 32 MB in\n> 2020 are possible and equals or less than 1 MB in 2010.\n>\n>\n>\n> Again may be is not possible Johnson Lau and LukeJr invested a significant\n> amount of time investigating ways to do a safe HF, and may be not possible\n> to do a safe HF today but from processing power, bandwidth and storage is\n> totally valid and Wang Chung proposal has solid grounds.\n>\n>\n>\n> Regards\n>\n>\n>\n> Juan\n>\n>\n>\n>\n>\n> *From:* Alphonse Pace [mailto:alp.bitcoin at gmail.com]\n> *Sent:* Tuesday, March 28, 2017 2:53 PM\n> *To:* Juan Garavaglia <jg at 112bit.com>; Wang Chun <1240902 at gmail.com>\n> *Cc:* Bitcoin Protocol Discussion <bitcoin-dev at lists.linuxfoundation.org>\n>\n> *Subject:* Re: [bitcoin-dev] Hard fork proposal from last week's meeting\n>\n>\n>\n> Juan,\n>\n>\n>\n> I suggest you take a look at this paper: http://fc16.ifca.ai/bit\n> coin/papers/CDE+16.pdf  It may help you form opinions based in science\n> rather than what appears to be nothing more than a hunch.  It shows that\n> even 4MB is unsafe.  SegWit provides up to this limit.\n>\n>\n>\n> 8MB is most definitely not safe today.\n>\n>\n>\n> Whether it is unsafe or impossible is the topic, since Wang Chun proposed\n> making the block size limit 32MiB.\n>\n>\n>\n>\n>\n> Wang Chun,\n>\n>\n> Can you specify what meeting you are talking about?  You seem to have not\n> replied on that point.  Who were the participants and what was the purpose\n> of this meeting?\n>\n>\n>\n> -Alphonse\n>\n>\n>\n> On Tue, Mar 28, 2017 at 12:33 PM, Juan Garavaglia <jg at 112bit.com> wrote:\n>\n> Alphonse,\n>\n>\n>\n> In my opinion if 1MB limit was ok in 2010, 8MB limit is ok on 2016 and\n> 32MB limit valid in next halving, from network, storage and CPU perspective\n> or 1MB was too high in 2010 what is possible or 1MB is to low today.\n>\n>\n>\n> If is unsafe or impossible to raise the blocksize is a different topic.\n>\n>\n>\n> Regards\n>\n>\n>\n> Juan\n>\n>\n>\n>\n>\n> *From:* bitcoin-dev-bounces at lists.linuxfoundation.org [mailto:\n> bitcoin-dev-bounces at lists.linuxfoundation.org] *On Behalf Of *Alphonse\n> Pace via bitcoin-dev\n> *Sent:* Tuesday, March 28, 2017 2:24 PM\n> *To:* Wang Chun <1240902 at gmail.com>; Bitcoin Protocol Discussion <\n> bitcoin-dev at lists.linuxfoundation.org>\n> *Subject:* Re: [bitcoin-dev] Hard fork proposal from last week's meeting\n>\n>\n>\n> What meeting are you referring to?  Who were the participants?\n>\n>\n>\n> Removing the limit but relying on the p2p protocol is not really a true\n> 32MiB limit, but a limit of whatever transport methods provide.  This can\n> lead to differing consensus if alternative layers for relaying are used.\n> What you seem to be asking for is an unbound block size (or at least\n> determined by whatever miners produce).  This has the possibility (and even\n> likelihood) of removing many participants from the network, including many\n> small miners.\n>\n>\n>\n> 32MB in less than 3 years also appears to be far beyond limits of safety\n> which are known to exist far sooner, and we cannot expect hardware and\n> networking layers to improve by those amounts in that time.\n>\n>\n>\n> It also seems like it would be much better to wait until SegWit activates\n> in order to truly measure the effects on the network from this increased\n> capacity before committing to any additional increases.\n>\n>\n>\n> -Alphonse\n>\n>\n>\n>\n>\n>\n>\n> On Tue, Mar 28, 2017 at 11:59 AM, Wang Chun via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> I've proposed this hard fork approach last year in Hong Kong Consensus\n> but immediately rejected by coredevs at that meeting, after more than\n> one year it seems that lots of people haven't heard of it. So I would\n> post this here again for comment.\n>\n> The basic idea is, as many of us agree, hard fork is risky and should\n> be well prepared. We need a long time to deploy it.\n>\n> Despite spam tx on the network, the block capacity is approaching its\n> limit, and we must think ahead. Shall we code a patch right now, to\n> remove the block size limit of 1MB, but not activate it until far in\n> the future. I would propose to remove the 1MB limit at the next block\n> halving in spring 2020, only limit the block size to 32MiB which is\n> the maximum size the current p2p protocol allows. This patch must be\n> in the immediate next release of Bitcoin Core.\n>\n> With this patch in core's next release, Bitcoin works just as before,\n> no fork will ever occur, until spring 2020. But everyone knows there\n> will be a fork scheduled. Third party services, libraries, wallets and\n> exchanges will have enough time to prepare for it over the next three\n> years.\n>\n> We don't yet have an agreement on how to increase the block size\n> limit. There have been many proposals over the past years, like\n> BIP100, 101, 102, 103, 104, 105, 106, 107, 109, 148, 248, BU, and so\n> on. These hard fork proposals, with this patch already in Core's\n> release, they all become soft fork. We'll have enough time to discuss\n> all these proposals and decide which one to go. Take an example, if we\n> choose to fork to only 2MB, since 32MiB already scheduled, reduce it\n> from 32MiB to 2MB will be a soft fork.\n>\n> Anyway, we must code something right now, before it becomes too late.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n\n\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170401/f0a4a59e/attachment-0001.html>"
            },
            {
                "author": "Jared Lee Richardson",
                "date": "2017-04-01T18:42:50",
                "message_text_only": "That's a quoted general statement that is highly subjective, not a\ndescription of an attack.  If you can't articulate a specific attack vector\nthat we're defending against, such a defense has no value.\n\nOn Apr 1, 2017 12:41 AM, \"Eric Voskuil\" <eric at voskuil.org> wrote:\n\n-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\n\nOn 03/31/2017 11:18 PM, Jared Lee Richardson wrote:\n>> If a typical personal computer cannot run a node there is no\n>> security.\n>\n> If you can't describe an attack that is made possible when typical\n> personal computers can't run nodes, this kind of logic has no place\n> in this discussion.\n\n\"Governments are good at cutting off the heads of a centrally\ncontrolled networks...\"\n\ne\n-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v2.0.22 (GNU/Linux)\n\niQEcBAEBCAAGBQJY31m0AAoJEDzYwH8LXOFOayIH/0DcWukHZUVTV8952mkWnqjS\nRCM8StQOuuTQ/2elvKoZa/nEv1PvpOQEO/AxJDEdIKOqjdXoc/QdZT/Qj834yyFi\nmmNLm3x8voO7rTFEVtBrXQ4VYO7Zj5gVy6nRyMrhSGtzg4XqYiyGVoijiumfXOvq\nejLwyWJEf8klBwegIPkX4XX6UYjNyBt+E32Je7NxUbi54EPDRszWpEGGKfJrWiCQ\nJO2jqB3O2RbMd0J1onBt2AGsjeQSE3HO0EBQSkdGQZ7PVSdE3I49uT2aAaScnPOt\nymbNz4QtlUWWpUgEI6VSjxHCGjX4+Vrn3HLRwjLe4nS2EX3mOVNY8MHMvbCeAuY=\n=tD9k\n-----END PGP SIGNATURE-----\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170401/715ac93a/attachment.html>"
            },
            {
                "author": "Staf Verhaegen",
                "date": "2017-04-02T19:02:02",
                "message_text_only": "Jared Lee Richardson via bitcoin-dev schreef op wo 29-03-2017 om 12:07\n[-0700]:\n\n> \n> It is all very unhealthy for Bitcoin.  Both sides need to accept that\n> microtransactions from all humans cannot go on-chain, and that never\n> increasing the blocksize doesn't mean millions of home users will run\n> nodes.  The node argument breaks down economically and the\n> microtransaction argument is an impossible mountain for a blockchain\n> to climb.\n\nWhat annoys me are people that seem to think that in order to promote\nlayer two scaling on-chain scaling has to be severely limited. I am\nconvinced that in order for layer 2 to flourish enough on-chain\nbandwidth has to be available, not artificial scarceness.\nIn order to allow more on-chain bandwidth also sharding solutions should\nbe investigated so not every transactions has to pass through each node\nand without the need of channels but protocol between nodes.\n\ngreets,\nStaf.\n\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 230 bytes\nDesc: This is a digitally signed message part\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170402/3f524adf/attachment.sig>"
            },
            {
                "author": "Staf Verhaegen",
                "date": "2017-04-02T19:12:06",
                "message_text_only": "Jared Lee Richardson via bitcoin-dev schreef op wo 29-03-2017 om 12:10\n[-0700]:\n> The proportion of users believing in microtransactions for all is also\n> larger than 5%,\n\nIn order to evaluate this statement the definition of microtransaction\nhas to be defined. I guess there will also be no consensus on that...\n\ngreets,\nStaf.\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 230 bytes\nDesc: This is a digitally signed message part\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170402/06b55e38/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "Hard fork proposal from last week's meeting",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Eric Voskuil",
                "Natanael",
                "Leandro Coutinho",
                "Jared Lee Richardson",
                "Rodney Morris",
                "Staf Verhaegen"
            ],
            "messages_count": 10,
            "total_messages_chars_count": 33307
        }
    },
    {
        "title": "[bitcoin-dev] Segwit2Mb - combined soft/hard fork - Request For Comments",
        "thread_messages": [
            {
                "author": "Samson Mow",
                "date": "2017-04-01T03:03:03",
                "message_text_only": "A compromise for the sake of compromise doesn't merit technical\ndiscussions. There are no benefits to be gained from a 2MB hard-fork at\nthis time and it would impose an unnecessary cost to the ecosystem for\ntesting and implementation.\n\nOn Fri, Mar 31, 2017 at 3:13 PM, Sergio Demian Lerner via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n>\n> On Fri, Mar 31, 2017 at 6:22 PM, Matt Corallo <lf-lists at mattcorallo.com>\n> wrote:\n>\n>> Hey Sergio,\n>>\n>> You appear to have ignored the last two years of Bitcoin hardfork\n>> research and understanding, recycling instead BIP 102 from 2015. There\n>> are many proposals which have pushed the state of hard fork research\n>> much further since then, and you may wish to read some of the posts on\n>> this mailing list listed at https://bitcoinhardforkresearch.github.io/\n>> and make further edits based on what you learn.\n>\n>\n> I've read every proposal that was published in the last two years and the\n> choice for NOT implementing any of the super cool research you cite is\n> intentional.\n>\n> We're in a deadlock and it seems we can't go forward adding more\n> functionality to segwit without the community approval (which include\n> miners). This is obvious to me.Then we have to go back.\n>\n> If this last resort solution is merged, we could go back to discuss\n> improvements with the\n>\n> Your goal of \"avoid\n>> technical changes\" appears to not have any basis outside of perceived\n>> compromise for compromise sake, only making such a hardfork riskier\n>> instead.\n>>\n>> You're are totally correct. It's a compromise for the compromise sake. I\n> couldn't have expressed it more clearly. However the only \"riskier\" element\n> is the hard forking date. We can move the date forward.\n>\n>\n>> At a minimum, in terms of pure technical changes, you should probably\n>> consider (probably among others):\n>>\n> a) Utilizing the \"hard fork signaling bit\" in the nVersion of the block.\n>>\n>\n> This I could consider, as it requires probably a single line of code.\n> Which BIP specifies this?\n>\n>\n>> b) Either limiting non-SegWit transactions in some way to fix the n**2\n>> sighash and FindAndDelete runtime and memory usage issues or fix them by\n>> utilizing the new sighash type which many wallets and projects have\n>> already implemented for SegWit in the spending of non-SegWit outputs.\n>>\n>\n> The Seghash problem has already been addressed by limiting the maximum\n> size of a transaction to 1 Mb.\n> The FindAndDelete problem has already been solved by the Core Developers,\n> so we don't have to worry about it anymore.\n>\n>\n>> c) Your really should have replay protection in any HF.\n>\n>\n> We could add a simple protection, although if we reach community consensus\n> and 95% of hashing power, does we really need to? Can the old chain still\n> be alive?\n> If more people ask for replay protection, I will merge Spoonet scheme or\n> develop the minimum possible replay protection (a simple signaling bit in\n> transaction version)\n>\n>\n>> d) You may wish to consider the possibility of tweaking the witness\n>> discount and possibly discounting other parts of the input - SegWit went\n>> a long ways towards making removal of elements from the UTXO set cheaper\n>> than adding them, but didn't quite get there, you should probably finish\n>> that job. This also provides additional tuneable parameters to allow you\n>> to increase the block size while not having a blowup in the worst-case\n>> block size.\n>>\n>\n> That is an interesting economic change and would be out of the scope of\n> segwit2mb.\n>\n>\n>> e) Additional commitments at the top of the merkle root - both for\n>> SegWit transactions and as additional space for merged mining and other\n>> commitments which we may wish to add in the future, this should likely\n>> be implemented an \"additional header\" ala Johnson Lau's Spoonnet proposal.\n>>\n>> That is an interesting technical improvement that is out of the scope of\n> segwit2mb.\n> We can keep discussing spoonet while we merge segwit2mb, as spoonnet\n> includes most of technical innovations.\n>\n>\n>> Additionally, I think your parameters here pose very significant risk to\n>> the Bitcoin ecosystem broadly.\n>>\n>> a) Activating a hard fork with less than 18/24 months (and even then...)\n>> from a fully-audited and supported release of full node software to\n>> activation date poses significant risks to many large software projects\n>> and users. I've repeatedly received feedback from various folks that a\n>> year or more is likely required in any hard fork to limit this risk, and\n>> limited pushback on that given the large increase which SegWit provides\n>> itself buying a ton of time.\n>>\n>> The feedback I received is slightly different from your feedback. Many\n> company CTOs have expressed that one year for a Bitcoin hard-fork was\n> period they could schedule a secure upgrade.\n>\n>\n>\n>> b) Having a significant discontinuity in block size increase only serves\n>> to confuse and mislead users and businesses, forcing them to rapidly\n>> adapt to a Bitcoin which changed overnight both by hardforking, and by\n>> fees changing suddenly. Instead, having the hard fork activate technical\n>> changes, and then slowly increasing the block size over the following\n>> several years keeps things nice and continuous and also keeps us from\n>> having to revisit ye old blocksize debate again six months after\n>> activation.\n>>\n>> This is something worth considering. There is the old Pieter BIP103\n> proposal has good parameters (17.7% per year).\n>\n> c) You should likely consider the effect of the many technological\n>> innovations coming down the pipe in the coming months. Technologies like\n>> Lightning, TumbleBit, and even your own RootStock could significantly\n>> reduce fee pressure as transactions move to much faster and more\n>> featureful systems.\n>>\n>> RSK sidechain team would have to take very tough decisions if Bitcoin\n> splits, as RSK platform cannot be pegged to two different cryptocurrencies.\n> We could launch two platforms, but RSK value proposition is \"supporting the\n> advance of Bitcoin, the cryptocurrecy with highest network effect\". You\n> understand that if Bitcoin splits Bitcoin BTC/BTU separately may cease to\n> be the cryptocurrencies with higher volume/market cap/network effect.\n>\n> Therefore all RSK people that I talked too would prefer to avoid a split\n> at all cost, reather that to be the winners of the scaling war.\n>\n>\n>\n>> On March 31, 2017 5:09:18 PM EDT, Sergio Demian Lerner via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> >Hi everyone,\n>> >\n>> >Segwit2Mb is the project to merge into Bitcoin a minimal patch that\n>> >aims to\n>> >untangle the current conflict between different political positions\n>> >regarding segwit activation vs. an increase of the on-chain blockchain\n>> >space through a standard block size increase. It is not a new solution,\n>> >but\n>> >it should be seen more as a least common denominator.\n>> >\n>> >Segwit2Mb combines segwit as it is today in Bitcoin 0.14+ with a 2MB\n>> >block\n>> >size hard-fork activated ONLY if segwit activates (95% of miners\n>> >signaling), but at a fixed future date.\n>> >\n>> >The sole objective of this proposal is to re-unite the Bitcoin\n>> >community\n>> >and avoid a cryptocurrency split. Segwit2Mb does not aim to be best\n>> >possible technical solution to solve Bitcoin technical limitations.\n>> >However, this proposal does not imply a compromise to the future\n>> >scalability or decentralization of Bitcoin, as a small increase in\n>> >block\n>> >size has been proven by several core and non-core developers not to\n>> >affect\n>> >Bitcoin value propositions.\n>> >\n>> >In the worst case, a 2X block size increase has much lower economic\n>> >impact\n>> >than the last bitcoin halving (<10%), which succeeded without problem.\n>> >\n>> >On the other side, Segwit2Mb primary goal is to be minimalistic: in\n>> >this\n>> >patch some choices have been made to reduce the number of lines\n>> >modified in\n>> >the current Bitcoin Core state (master branch), instead of implementing\n>> >the\n>> >most elegant solution. This is because I want to reduce the time it\n>> >takes\n>> >for core programmers and reviewers to check the correctness of the\n>> >code,\n>> >and to report and correct bugs.\n>> >\n>> >The patch was built by forking the master branch of Bitcoin Core,\n>> >mixing a\n>> >few lines of code from Jeff Garzik's BIP102,  and defining a second\n>> >versionbits activation bit (bit 2) for the combined activation.\n>> >\n>> >The combined activation of segwit and 2Mb hard-fork nVersion bit is 2\n>> >(DEPLOYMENT_SEGWIT_AND_2MB_BLOCKS).\n>> >\n>> >This means that segwit can still be activated without the 2MB hard-fork\n>> >by\n>> >signaling bit 1 in nVersion  (DEPLOYMENT_SEGWIT).\n>> >\n>> >The tentative lock-in and hard-fork dates are the following:\n>> >\n>> >Bit 2 signaling StartTime = 1493424000; // April 29th, 2017\n>> >\n>> >Bit 2 signaling Timeout = 1503964800; // August 29th, 2017\n>> >\n>> >HardForkTime = 1513209600; // Thu, 14 Dec 2017 00:00:00 GMT\n>> >\n>> >\n>> >The hard-fork is conditional to 95% of the hashing power has approved\n>> >the\n>> >segwit2mb soft-fork and the segwit soft-fork has been activated (which\n>> >should occur 2016 blocks after its lock-in time)\n>> >\n>> >For more information on how soft-forks are signaled and activated, see\n>> >https://github.com/bitcoin/bips/blob/master/bip-0009.mediawiki\n>> >\n>> >This means that segwit would be activated before 2Mb: this is\n>> >inevitable,\n>> >as versionbits have been designed to have fixed activation periods and\n>> >thresholds for all bits. Making segwit and 2Mb fork activate together\n>> >at a\n>> >delayed date would have required a major re-write of this code, which\n>> >would\n>> >contradict the premise of creating a minimalistic patch. However, once\n>> >segwit is activated, the hard-fork is unavoidable.\n>> >\n>> >Although I have coded a first version of the segwit2mb patch (which\n>> >modifies 120 lines of code, and adds 220 lines of testing code), I\n>> >would\n>> >prefer to wait to publish the source code until more comments have been\n>> >received from the community.\n>> >\n>> >To prevent worsening block verification time because of the O(N^2)\n>> >hashing\n>> >problem, the simple restriction that transactions cannot be larger than\n>> >1Mb\n>> >has been kept. Therefore the worse-case of block verification time has\n>> >only\n>> >doubled.\n>> >\n>> >Regarding the hard-fork activation date, I want to give enough time to\n>> >all\n>> >active economic nodes to upgrade. As of Fri Mar 31 2017,\n>> >https://bitnodes.21.co/nodes/ reports that 6332 out of 6955 nodes (91%)\n>> >have upgraded to post 0.12 versions. Upgrade to post 0.12 versions can\n>> >be\n>> >used to identify economic active nodes, because in the 0.12 release\n>> >dynamic\n>> >fees were introduced, and currently no Bitcoin automatic payment system\n>> >can\n>> >operate without automatic discovery of the current fee rate. A pre-0.12\n>> >would require constant manual intervention.\n>> >Therefore I conclude that no more than 91% of the network nodes\n>> >reported by\n>> >bitnodes are active economic nodes.\n>> >\n>> >As Bitcoin Core 0.12 was released on February 2016, the time for this\n>> >91%\n>> >to upgrade has been around one year (under a moderate pressure of\n>> >operational problems with unconfirmed transactions).\n>> >Therefore we can expect a similar or lower time to upgrade for a\n>> >hard-fork,\n>> >after developers have discussed and approved the patch, and it has been\n>> >reviewed and merged and 95% of the hashing power has signaled for it\n>> >(the\n>> >pressure not to upgrade being a complete halt of the operations).\n>> >However I\n>> >suggest that we discuss the hard-fork date and delay it if there is a\n>> >real\n>> >need to.\n>> >\n>> >Currently time works against the Bitcoin community, and so is delaying\n>> >a\n>> >compromise solution. Most of the community agree that halting the\n>> >innovation for several years is a very bad option.\n>> >\n>> >After the comments collected by the community, a BIP will be written\n>> >describing the resulting proposal details.\n>> >\n>> >If segwit2mb locks-in, before hard-fork occurs all bitcoin nodes should\n>> >be\n>> >updated to a Segwit2Mb enabled node to prevent them to be forked-away\n>> >in a\n>> >chain with almost no hashing-power.\n>> >\n>> >The proof of concept patch was made for Bitcoin Core but should be\n>> >easily\n>> >ported to other Bitcoin protocol implementations that already support\n>> >versionbits. Lightweight (SPV) wallets should not be affected as they\n>> >generally do not check the block size.\n>> >\n>> >I personally want to see the Lightning Network in action this year, use\n>> >the\n>> >non-malleability features in segwit, see the community discussing other\n>> >exciting soft-forks in the scaling roadmap, Schnorr sigs, drivechains\n>> >and\n>> >MAST.\n>> >\n>> >I want to see miners, developers and industry side-by-side pushing\n>> >Bitcoin\n>> >forward, to increase the value of Bitcoin and prevent high transaction\n>> >fees\n>> >to put out of business use-cases that could have high positive social\n>> >impact.\n>> >\n>> >I believe in the strength of a unified Bitcoin community. If you're a\n>> >developer, please give your opinion, suggest changes, audit it, and\n>> >take a\n>> >stand with me to unlock the current Bitcoin deadlock.\n>> >\n>> >Contributions to the segwit2mb project are welcomed and awaited. The\n>> >only\n>> >limitation is to stick to the principle that the patch should be as\n>> >simple\n>> >to audit as possible. As an example, I wouldn't feel confident if the\n>> >patch\n>> >modified more than ~150 lines of code.\n>> >\n>> >Improvements unrelated to a 2 Mb increase or segwit, as beneficial as\n>> >it\n>> >may be to Bitcoin, should not be part of segwit2Mb.\n>> >\n>> >This proposal should not prevent other consensus proposals to be\n>> >simultaneously merged: segwit2mb is a last resort solution in case we\n>> >can\n>> >not reach consensus on anything better.\n>> >\n>> >Again, the proposal is only a starting point: community feedback is\n>> >expected and welcomed.\n>> >\n>> >Regards,\n>> >Sergio Demian Lerner\n>>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170331/b05637e8/attachment-0001.html>"
            },
            {
                "author": "Sergio Demian Lerner",
                "date": "2017-04-01T03:35:11",
                "message_text_only": "Even if the proposal involves a political compromise, any change to the\ncode must be technically evaluated.\nThe patch was made to require the least possible time for auditing. I'm\ntalking about reviewing 120 lines of code (not counting comments or\nspace) which 30 of them are changes to constants. A core programmer audited\nit in less than one hour.\n\nAlso you're risking the unique opportunity to see segwit activated for\nwhat?\nMaybe we can reach a similar agreement for segwit activation in two years.\nThat's will be too late. The remaining cryptocurrency ecosystem do move\nforward.\n\n\n\nOn Sat, Apr 1, 2017 at 12:03 AM, Samson Mow <samson.mow at gmail.com> wrote:\n\n> A compromise for the sake of compromise doesn't merit technical\n> discussions. There are no benefits to be gained from a 2MB hard-fork at\n> this time and it would impose an unnecessary cost to the ecosystem for\n> testing and implementation.\n>\n> On Fri, Mar 31, 2017 at 3:13 PM, Sergio Demian Lerner via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>>\n>>\n>> On Fri, Mar 31, 2017 at 6:22 PM, Matt Corallo <lf-lists at mattcorallo.com>\n>> wrote:\n>>\n>>> Hey Sergio,\n>>>\n>>> You appear to have ignored the last two years of Bitcoin hardfork\n>>> research and understanding, recycling instead BIP 102 from 2015. There\n>>> are many proposals which have pushed the state of hard fork research\n>>> much further since then, and you may wish to read some of the posts on\n>>> this mailing list listed at https://bitcoinhardforkresearch.github.io/\n>>> and make further edits based on what you learn.\n>>\n>>\n>> I've read every proposal that was published in the last two years and the\n>> choice for NOT implementing any of the super cool research you cite is\n>> intentional.\n>>\n>> We're in a deadlock and it seems we can't go forward adding more\n>> functionality to segwit without the community approval (which include\n>> miners). This is obvious to me.Then we have to go back.\n>>\n>> If this last resort solution is merged, we could go back to discuss\n>> improvements with the\n>>\n>> Your goal of \"avoid\n>>> technical changes\" appears to not have any basis outside of perceived\n>>> compromise for compromise sake, only making such a hardfork riskier\n>>> instead.\n>>>\n>>> You're are totally correct. It's a compromise for the compromise sake. I\n>> couldn't have expressed it more clearly. However the only \"riskier\" element\n>> is the hard forking date. We can move the date forward.\n>>\n>>\n>>> At a minimum, in terms of pure technical changes, you should probably\n>>> consider (probably among others):\n>>>\n>> a) Utilizing the \"hard fork signaling bit\" in the nVersion of the block.\n>>>\n>>\n>> This I could consider, as it requires probably a single line of code.\n>> Which BIP specifies this?\n>>\n>>\n>>> b) Either limiting non-SegWit transactions in some way to fix the n**2\n>>> sighash and FindAndDelete runtime and memory usage issues or fix them by\n>>> utilizing the new sighash type which many wallets and projects have\n>>> already implemented for SegWit in the spending of non-SegWit outputs.\n>>>\n>>\n>> The Seghash problem has already been addressed by limiting the maximum\n>> size of a transaction to 1 Mb.\n>> The FindAndDelete problem has already been solved by the Core Developers,\n>> so we don't have to worry about it anymore.\n>>\n>>\n>>> c) Your really should have replay protection in any HF.\n>>\n>>\n>> We could add a simple protection, although if we reach community\n>> consensus and 95% of hashing power, does we really need to? Can the old\n>> chain still be alive?\n>> If more people ask for replay protection, I will merge Spoonet scheme or\n>> develop the minimum possible replay protection (a simple signaling bit in\n>> transaction version)\n>>\n>>\n>>> d) You may wish to consider the possibility of tweaking the witness\n>>> discount and possibly discounting other parts of the input - SegWit went\n>>> a long ways towards making removal of elements from the UTXO set cheaper\n>>> than adding them, but didn't quite get there, you should probably finish\n>>> that job. This also provides additional tuneable parameters to allow you\n>>> to increase the block size while not having a blowup in the worst-case\n>>> block size.\n>>>\n>>\n>> That is an interesting economic change and would be out of the scope of\n>> segwit2mb.\n>>\n>>\n>>> e) Additional commitments at the top of the merkle root - both for\n>>> SegWit transactions and as additional space for merged mining and other\n>>> commitments which we may wish to add in the future, this should likely\n>>> be implemented an \"additional header\" ala Johnson Lau's Spoonnet\n>>> proposal.\n>>>\n>>> That is an interesting technical improvement that is out of the scope of\n>> segwit2mb.\n>> We can keep discussing spoonet while we merge segwit2mb, as spoonnet\n>> includes most of technical innovations.\n>>\n>>\n>>> Additionally, I think your parameters here pose very significant risk to\n>>> the Bitcoin ecosystem broadly.\n>>>\n>>> a) Activating a hard fork with less than 18/24 months (and even then...)\n>>> from a fully-audited and supported release of full node software to\n>>> activation date poses significant risks to many large software projects\n>>> and users. I've repeatedly received feedback from various folks that a\n>>> year or more is likely required in any hard fork to limit this risk, and\n>>> limited pushback on that given the large increase which SegWit provides\n>>> itself buying a ton of time.\n>>>\n>>> The feedback I received is slightly different from your feedback. Many\n>> company CTOs have expressed that one year for a Bitcoin hard-fork was\n>> period they could schedule a secure upgrade.\n>>\n>>\n>>\n>>> b) Having a significant discontinuity in block size increase only serves\n>>> to confuse and mislead users and businesses, forcing them to rapidly\n>>> adapt to a Bitcoin which changed overnight both by hardforking, and by\n>>> fees changing suddenly. Instead, having the hard fork activate technical\n>>> changes, and then slowly increasing the block size over the following\n>>> several years keeps things nice and continuous and also keeps us from\n>>> having to revisit ye old blocksize debate again six months after\n>>> activation.\n>>>\n>>> This is something worth considering. There is the old Pieter BIP103\n>> proposal has good parameters (17.7% per year).\n>>\n>> c) You should likely consider the effect of the many technological\n>>> innovations coming down the pipe in the coming months. Technologies like\n>>> Lightning, TumbleBit, and even your own RootStock could significantly\n>>> reduce fee pressure as transactions move to much faster and more\n>>> featureful systems.\n>>>\n>>> RSK sidechain team would have to take very tough decisions if Bitcoin\n>> splits, as RSK platform cannot be pegged to two different cryptocurrencies.\n>> We could launch two platforms, but RSK value proposition is \"supporting the\n>> advance of Bitcoin, the cryptocurrecy with highest network effect\". You\n>> understand that if Bitcoin splits Bitcoin BTC/BTU separately may cease to\n>> be the cryptocurrencies with higher volume/market cap/network effect.\n>>\n>> Therefore all RSK people that I talked too would prefer to avoid a split\n>> at all cost, reather that to be the winners of the scaling war.\n>>\n>>\n>>\n>>> On March 31, 2017 5:09:18 PM EDT, Sergio Demian Lerner via bitcoin-dev <\n>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>> >Hi everyone,\n>>> >\n>>> >Segwit2Mb is the project to merge into Bitcoin a minimal patch that\n>>> >aims to\n>>> >untangle the current conflict between different political positions\n>>> >regarding segwit activation vs. an increase of the on-chain blockchain\n>>> >space through a standard block size increase. It is not a new solution,\n>>> >but\n>>> >it should be seen more as a least common denominator.\n>>> >\n>>> >Segwit2Mb combines segwit as it is today in Bitcoin 0.14+ with a 2MB\n>>> >block\n>>> >size hard-fork activated ONLY if segwit activates (95% of miners\n>>> >signaling), but at a fixed future date.\n>>> >\n>>> >The sole objective of this proposal is to re-unite the Bitcoin\n>>> >community\n>>> >and avoid a cryptocurrency split. Segwit2Mb does not aim to be best\n>>> >possible technical solution to solve Bitcoin technical limitations.\n>>> >However, this proposal does not imply a compromise to the future\n>>> >scalability or decentralization of Bitcoin, as a small increase in\n>>> >block\n>>> >size has been proven by several core and non-core developers not to\n>>> >affect\n>>> >Bitcoin value propositions.\n>>> >\n>>> >In the worst case, a 2X block size increase has much lower economic\n>>> >impact\n>>> >than the last bitcoin halving (<10%), which succeeded without problem.\n>>> >\n>>> >On the other side, Segwit2Mb primary goal is to be minimalistic: in\n>>> >this\n>>> >patch some choices have been made to reduce the number of lines\n>>> >modified in\n>>> >the current Bitcoin Core state (master branch), instead of implementing\n>>> >the\n>>> >most elegant solution. This is because I want to reduce the time it\n>>> >takes\n>>> >for core programmers and reviewers to check the correctness of the\n>>> >code,\n>>> >and to report and correct bugs.\n>>> >\n>>> >The patch was built by forking the master branch of Bitcoin Core,\n>>> >mixing a\n>>> >few lines of code from Jeff Garzik's BIP102,  and defining a second\n>>> >versionbits activation bit (bit 2) for the combined activation.\n>>> >\n>>> >The combined activation of segwit and 2Mb hard-fork nVersion bit is 2\n>>> >(DEPLOYMENT_SEGWIT_AND_2MB_BLOCKS).\n>>> >\n>>> >This means that segwit can still be activated without the 2MB hard-fork\n>>> >by\n>>> >signaling bit 1 in nVersion  (DEPLOYMENT_SEGWIT).\n>>> >\n>>> >The tentative lock-in and hard-fork dates are the following:\n>>> >\n>>> >Bit 2 signaling StartTime = 1493424000; // April 29th, 2017\n>>> >\n>>> >Bit 2 signaling Timeout = 1503964800; // August 29th, 2017\n>>> >\n>>> >HardForkTime = 1513209600; // Thu, 14 Dec 2017 00:00:00 GMT\n>>> >\n>>> >\n>>> >The hard-fork is conditional to 95% of the hashing power has approved\n>>> >the\n>>> >segwit2mb soft-fork and the segwit soft-fork has been activated (which\n>>> >should occur 2016 blocks after its lock-in time)\n>>> >\n>>> >For more information on how soft-forks are signaled and activated, see\n>>> >https://github.com/bitcoin/bips/blob/master/bip-0009.mediawiki\n>>> >\n>>> >This means that segwit would be activated before 2Mb: this is\n>>> >inevitable,\n>>> >as versionbits have been designed to have fixed activation periods and\n>>> >thresholds for all bits. Making segwit and 2Mb fork activate together\n>>> >at a\n>>> >delayed date would have required a major re-write of this code, which\n>>> >would\n>>> >contradict the premise of creating a minimalistic patch. However, once\n>>> >segwit is activated, the hard-fork is unavoidable.\n>>> >\n>>> >Although I have coded a first version of the segwit2mb patch (which\n>>> >modifies 120 lines of code, and adds 220 lines of testing code), I\n>>> >would\n>>> >prefer to wait to publish the source code until more comments have been\n>>> >received from the community.\n>>> >\n>>> >To prevent worsening block verification time because of the O(N^2)\n>>> >hashing\n>>> >problem, the simple restriction that transactions cannot be larger than\n>>> >1Mb\n>>> >has been kept. Therefore the worse-case of block verification time has\n>>> >only\n>>> >doubled.\n>>> >\n>>> >Regarding the hard-fork activation date, I want to give enough time to\n>>> >all\n>>> >active economic nodes to upgrade. As of Fri Mar 31 2017,\n>>> >https://bitnodes.21.co/nodes/ reports that 6332 out of 6955 nodes (91%)\n>>> >have upgraded to post 0.12 versions. Upgrade to post 0.12 versions can\n>>> >be\n>>> >used to identify economic active nodes, because in the 0.12 release\n>>> >dynamic\n>>> >fees were introduced, and currently no Bitcoin automatic payment system\n>>> >can\n>>> >operate without automatic discovery of the current fee rate. A pre-0.12\n>>> >would require constant manual intervention.\n>>> >Therefore I conclude that no more than 91% of the network nodes\n>>> >reported by\n>>> >bitnodes are active economic nodes.\n>>> >\n>>> >As Bitcoin Core 0.12 was released on February 2016, the time for this\n>>> >91%\n>>> >to upgrade has been around one year (under a moderate pressure of\n>>> >operational problems with unconfirmed transactions).\n>>> >Therefore we can expect a similar or lower time to upgrade for a\n>>> >hard-fork,\n>>> >after developers have discussed and approved the patch, and it has been\n>>> >reviewed and merged and 95% of the hashing power has signaled for it\n>>> >(the\n>>> >pressure not to upgrade being a complete halt of the operations).\n>>> >However I\n>>> >suggest that we discuss the hard-fork date and delay it if there is a\n>>> >real\n>>> >need to.\n>>> >\n>>> >Currently time works against the Bitcoin community, and so is delaying\n>>> >a\n>>> >compromise solution. Most of the community agree that halting the\n>>> >innovation for several years is a very bad option.\n>>> >\n>>> >After the comments collected by the community, a BIP will be written\n>>> >describing the resulting proposal details.\n>>> >\n>>> >If segwit2mb locks-in, before hard-fork occurs all bitcoin nodes should\n>>> >be\n>>> >updated to a Segwit2Mb enabled node to prevent them to be forked-away\n>>> >in a\n>>> >chain with almost no hashing-power.\n>>> >\n>>> >The proof of concept patch was made for Bitcoin Core but should be\n>>> >easily\n>>> >ported to other Bitcoin protocol implementations that already support\n>>> >versionbits. Lightweight (SPV) wallets should not be affected as they\n>>> >generally do not check the block size.\n>>> >\n>>> >I personally want to see the Lightning Network in action this year, use\n>>> >the\n>>> >non-malleability features in segwit, see the community discussing other\n>>> >exciting soft-forks in the scaling roadmap, Schnorr sigs, drivechains\n>>> >and\n>>> >MAST.\n>>> >\n>>> >I want to see miners, developers and industry side-by-side pushing\n>>> >Bitcoin\n>>> >forward, to increase the value of Bitcoin and prevent high transaction\n>>> >fees\n>>> >to put out of business use-cases that could have high positive social\n>>> >impact.\n>>> >\n>>> >I believe in the strength of a unified Bitcoin community. If you're a\n>>> >developer, please give your opinion, suggest changes, audit it, and\n>>> >take a\n>>> >stand with me to unlock the current Bitcoin deadlock.\n>>> >\n>>> >Contributions to the segwit2mb project are welcomed and awaited. The\n>>> >only\n>>> >limitation is to stick to the principle that the patch should be as\n>>> >simple\n>>> >to audit as possible. As an example, I wouldn't feel confident if the\n>>> >patch\n>>> >modified more than ~150 lines of code.\n>>> >\n>>> >Improvements unrelated to a 2 Mb increase or segwit, as beneficial as\n>>> >it\n>>> >may be to Bitcoin, should not be part of segwit2Mb.\n>>> >\n>>> >This proposal should not prevent other consensus proposals to be\n>>> >simultaneously merged: segwit2mb is a last resort solution in case we\n>>> >can\n>>> >not reach consensus on anything better.\n>>> >\n>>> >Again, the proposal is only a starting point: community feedback is\n>>> >expected and welcomed.\n>>> >\n>>> >Regards,\n>>> >Sergio Demian Lerner\n>>>\n>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170401/68144b3e/attachment-0001.html>"
            },
            {
                "author": "Jared Lee Richardson",
                "date": "2017-04-01T06:55:42",
                "message_text_only": "> Remember that the \"hashpower required to secure bitcoin\" is determined\n> as a percentage of total Bitcoins transacted on-chain in each block\n\nCan you explain this statement a little better?  What do you mean by\nthat?  What does the total bitcoins transacted have to do with\nhashpower required?\n\nOn Fri, Mar 31, 2017 at 2:22 PM, Matt Corallo via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Hey Sergio,\n>\n> You appear to have ignored the last two years of Bitcoin hardfork\n> research and understanding, recycling instead BIP 102 from 2015. There\n> are many proposals which have pushed the state of hard fork research\n> much further since then, and you may wish to read some of the posts on\n> this mailing list listed at https://bitcoinhardforkresearch.github.io/\n> and make further edits based on what you learn. Your goal of \"avoid\n> technical changes\" appears to not have any basis outside of perceived\n> compromise for compromise sake, only making such a hardfork riskier\n> instead.\n>\n> At a minimum, in terms of pure technical changes, you should probably\n> consider (probably among others):\n>\n> a) Utilizing the \"hard fork signaling bit\" in the nVersion of the block.\n> b) Either limiting non-SegWit transactions in some way to fix the n**2\n> sighash and FindAndDelete runtime and memory usage issues or fix them by\n> utilizing the new sighash type which many wallets and projects have\n> already implemented for SegWit in the spending of non-SegWit outputs.\n> c) Your really should have replay protection in any HF. The clever fix from\n> Spoonnet for poor scaling of optionally allowing non-SegWit outputs to\n> be spent with SegWit's sighash provides this all in one go.\n> d) You may wish to consider the possibility of tweaking the witness\n> discount and possibly discounting other parts of the input - SegWit went\n> a long ways towards making removal of elements from the UTXO set cheaper\n> than adding them, but didn't quite get there, you should probably finish\n> that job. This also provides additional tuneable parameters to allow you\n> to increase the block size while not having a blowup in the worst-case\n> block size.\n> e) Additional commitments at the top of the merkle root - both for\n> SegWit transactions and as additional space for merged mining and other\n> commitments which we may wish to add in the future, this should likely\n> be implemented an \"additional header\" ala Johnson Lau's Spoonnet proposal.\n>\n> Additionally, I think your parameters here pose very significant risk to\n> the Bitcoin ecosystem broadly.\n>\n> a) Activating a hard fork with less than 18/24 months (and even then...)\n> from a fully-audited and supported release of full node software to\n> activation date poses significant risks to many large software projects\n> and users. I've repeatedly received feedback from various folks that a\n> year or more is likely required in any hard fork to limit this risk, and\n> limited pushback on that given the large increase which SegWit provides\n> itself buying a ton of time.\n>\n> b) Having a significant discontinuity in block size increase only serves\n> to confuse and mislead users and businesses, forcing them to rapidly\n> adapt to a Bitcoin which changed overnight both by hardforking, and by\n> fees changing suddenly. Instead, having the hard fork activate technical\n> changes, and then slowly increasing the block size over the following\n> several years keeps things nice and continuous and also keeps us from\n> having to revisit ye old blocksize debate again six months after activation.\n>\n> c) You should likely consider the effect of the many technological\n> innovations coming down the pipe in the coming months. Technologies like\n> Lightning, TumbleBit, and even your own RootStock could significantly\n> reduce fee pressure as transactions move to much faster and more\n> featureful systems.\n>\n> Commitments to aggressive hard fork parameters now may leave miners\n> without much revenue as far out as the next halving (which current\n> transaction growth trends are indicating we'd just only barely reach 2MB\n> of transaction volume, let alone if you consider the effects of users\n> moving to systems which provide more features for Bitcoin transactions).\n> This could lead to a precipitous drop in hashrate as miners are no\n> longer sufficiently compensated.\n>\n> Remember that the \"hashpower required to secure bitcoin\" is determined\n> as a percentage of total Bitcoins transacted on-chain in each block, so\n> as subsidy goes down, miners need to be paid with fees, not just price\n> increases. Even if we were OK with hashpower going down compared to the\n> value it is securing, betting the security of Bitcoin on its price\n> rising exponentially to match decreasing subsidy does not strike me as a\n> particularly inspiring tradeoff.\n>\n> There aren't many great technical solutions to some of these issues, as\n> far as I'm aware, but it's something that needs to be incredibly\n> carefully considered before betting the continued security of Bitcoin on\n> exponential on-chain growth, something which we have historically never\n> seen.\n>\n> Matt\n>\n>\n> On March 31, 2017 5:09:18 PM EDT, Sergio Demian Lerner via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>Hi everyone,\n>>\n>>Segwit2Mb is the project to merge into Bitcoin a minimal patch that\n>>aims to\n>>untangle the current conflict between different political positions\n>>regarding segwit activation vs. an increase of the on-chain blockchain\n>>space through a standard block size increase. It is not a new solution,\n>>but\n>>it should be seen more as a least common denominator.\n>>\n>>Segwit2Mb combines segwit as it is today in Bitcoin 0.14+ with a 2MB\n>>block\n>>size hard-fork activated ONLY if segwit activates (95% of miners\n>>signaling), but at a fixed future date.\n>>\n>>The sole objective of this proposal is to re-unite the Bitcoin\n>>community\n>>and avoid a cryptocurrency split. Segwit2Mb does not aim to be best\n>>possible technical solution to solve Bitcoin technical limitations.\n>>However, this proposal does not imply a compromise to the future\n>>scalability or decentralization of Bitcoin, as a small increase in\n>>block\n>>size has been proven by several core and non-core developers not to\n>>affect\n>>Bitcoin value propositions.\n>>\n>>In the worst case, a 2X block size increase has much lower economic\n>>impact\n>>than the last bitcoin halving (<10%), which succeeded without problem.\n>>\n>>On the other side, Segwit2Mb primary goal is to be minimalistic: in\n>>this\n>>patch some choices have been made to reduce the number of lines\n>>modified in\n>>the current Bitcoin Core state (master branch), instead of implementing\n>>the\n>>most elegant solution. This is because I want to reduce the time it\n>>takes\n>>for core programmers and reviewers to check the correctness of the\n>>code,\n>>and to report and correct bugs.\n>>\n>>The patch was built by forking the master branch of Bitcoin Core,\n>>mixing a\n>>few lines of code from Jeff Garzik's BIP102,  and defining a second\n>>versionbits activation bit (bit 2) for the combined activation.\n>>\n>>The combined activation of segwit and 2Mb hard-fork nVersion bit is 2\n>>(DEPLOYMENT_SEGWIT_AND_2MB_BLOCKS).\n>>\n>>This means that segwit can still be activated without the 2MB hard-fork\n>>by\n>>signaling bit 1 in nVersion  (DEPLOYMENT_SEGWIT).\n>>\n>>The tentative lock-in and hard-fork dates are the following:\n>>\n>>Bit 2 signaling StartTime = 1493424000; // April 29th, 2017\n>>\n>>Bit 2 signaling Timeout = 1503964800; // August 29th, 2017\n>>\n>>HardForkTime = 1513209600; // Thu, 14 Dec 2017 00:00:00 GMT\n>>\n>>\n>>The hard-fork is conditional to 95% of the hashing power has approved\n>>the\n>>segwit2mb soft-fork and the segwit soft-fork has been activated (which\n>>should occur 2016 blocks after its lock-in time)\n>>\n>>For more information on how soft-forks are signaled and activated, see\n>>https://github.com/bitcoin/bips/blob/master/bip-0009.mediawiki\n>>\n>>This means that segwit would be activated before 2Mb: this is\n>>inevitable,\n>>as versionbits have been designed to have fixed activation periods and\n>>thresholds for all bits. Making segwit and 2Mb fork activate together\n>>at a\n>>delayed date would have required a major re-write of this code, which\n>>would\n>>contradict the premise of creating a minimalistic patch. However, once\n>>segwit is activated, the hard-fork is unavoidable.\n>>\n>>Although I have coded a first version of the segwit2mb patch (which\n>>modifies 120 lines of code, and adds 220 lines of testing code), I\n>>would\n>>prefer to wait to publish the source code until more comments have been\n>>received from the community.\n>>\n>>To prevent worsening block verification time because of the O(N^2)\n>>hashing\n>>problem, the simple restriction that transactions cannot be larger than\n>>1Mb\n>>has been kept. Therefore the worse-case of block verification time has\n>>only\n>>doubled.\n>>\n>>Regarding the hard-fork activation date, I want to give enough time to\n>>all\n>>active economic nodes to upgrade. As of Fri Mar 31 2017,\n>>https://bitnodes.21.co/nodes/ reports that 6332 out of 6955 nodes (91%)\n>>have upgraded to post 0.12 versions. Upgrade to post 0.12 versions can\n>>be\n>>used to identify economic active nodes, because in the 0.12 release\n>>dynamic\n>>fees were introduced, and currently no Bitcoin automatic payment system\n>>can\n>>operate without automatic discovery of the current fee rate. A pre-0.12\n>>would require constant manual intervention.\n>>Therefore I conclude that no more than 91% of the network nodes\n>>reported by\n>>bitnodes are active economic nodes.\n>>\n>>As Bitcoin Core 0.12 was released on February 2016, the time for this\n>>91%\n>>to upgrade has been around one year (under a moderate pressure of\n>>operational problems with unconfirmed transactions).\n>>Therefore we can expect a similar or lower time to upgrade for a\n>>hard-fork,\n>>after developers have discussed and approved the patch, and it has been\n>>reviewed and merged and 95% of the hashing power has signaled for it\n>>(the\n>>pressure not to upgrade being a complete halt of the operations).\n>>However I\n>>suggest that we discuss the hard-fork date and delay it if there is a\n>>real\n>>need to.\n>>\n>>Currently time works against the Bitcoin community, and so is delaying\n>>a\n>>compromise solution. Most of the community agree that halting the\n>>innovation for several years is a very bad option.\n>>\n>>After the comments collected by the community, a BIP will be written\n>>describing the resulting proposal details.\n>>\n>>If segwit2mb locks-in, before hard-fork occurs all bitcoin nodes should\n>>be\n>>updated to a Segwit2Mb enabled node to prevent them to be forked-away\n>>in a\n>>chain with almost no hashing-power.\n>>\n>>The proof of concept patch was made for Bitcoin Core but should be\n>>easily\n>>ported to other Bitcoin protocol implementations that already support\n>>versionbits. Lightweight (SPV) wallets should not be affected as they\n>>generally do not check the block size.\n>>\n>>I personally want to see the Lightning Network in action this year, use\n>>the\n>>non-malleability features in segwit, see the community discussing other\n>>exciting soft-forks in the scaling roadmap, Schnorr sigs, drivechains\n>>and\n>>MAST.\n>>\n>>I want to see miners, developers and industry side-by-side pushing\n>>Bitcoin\n>>forward, to increase the value of Bitcoin and prevent high transaction\n>>fees\n>>to put out of business use-cases that could have high positive social\n>>impact.\n>>\n>>I believe in the strength of a unified Bitcoin community. If you're a\n>>developer, please give your opinion, suggest changes, audit it, and\n>>take a\n>>stand with me to unlock the current Bitcoin deadlock.\n>>\n>>Contributions to the segwit2mb project are welcomed and awaited. The\n>>only\n>>limitation is to stick to the principle that the patch should be as\n>>simple\n>>to audit as possible. As an example, I wouldn't feel confident if the\n>>patch\n>>modified more than ~150 lines of code.\n>>\n>>Improvements unrelated to a 2 Mb increase or segwit, as beneficial as\n>>it\n>>may be to Bitcoin, should not be part of segwit2Mb.\n>>\n>>This proposal should not prevent other consensus proposals to be\n>>simultaneously merged: segwit2mb is a last resort solution in case we\n>>can\n>>not reach consensus on anything better.\n>>\n>>Again, the proposal is only a starting point: community feedback is\n>>expected and welcomed.\n>>\n>>Regards,\n>>Sergio Demian Lerner\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Sergio Demian Lerner",
                "date": "2017-04-01T11:44:11",
                "message_text_only": "Some people have asked me for the current implementation of this patch to\nreview. I remind you that the current patch does not implement the\nhard-fork signaling, as requested by Matt.\n\nThe Segwit2Mb patch can be found here:\nhttps://github.com/SergioDemianLerner/bitcoin/commits/master\n\nFor now, the segwit2mb repo has a single test file using the old internal\nblockchain building method (test/block_size_tests.cpp). This must be\nreplaced soon with a better external test using the bitcoin/qa/rpc-tests\ntests, which I will begin to work on now after I collect all comments from\nthe community.\n\n\nregards\n\n\nOn Sat, Apr 1, 2017 at 3:55 AM, Jared Lee Richardson <jaredr26 at gmail.com>\nwrote:\n\n> > Remember that the \"hashpower required to secure bitcoin\" is determined\n> > as a percentage of total Bitcoins transacted on-chain in each block\n>\n> Can you explain this statement a little better?  What do you mean by\n> that?  What does the total bitcoins transacted have to do with\n> hashpower required?\n>\n> On Fri, Mar 31, 2017 at 2:22 PM, Matt Corallo via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > Hey Sergio,\n> >\n> > You appear to have ignored the last two years of Bitcoin hardfork\n> > research and understanding, recycling instead BIP 102 from 2015. There\n> > are many proposals which have pushed the state of hard fork research\n> > much further since then, and you may wish to read some of the posts on\n> > this mailing list listed at https://bitcoinhardforkresearch.github.io/\n> > and make further edits based on what you learn. Your goal of \"avoid\n> > technical changes\" appears to not have any basis outside of perceived\n> > compromise for compromise sake, only making such a hardfork riskier\n> > instead.\n> >\n> > At a minimum, in terms of pure technical changes, you should probably\n> > consider (probably among others):\n> >\n> > a) Utilizing the \"hard fork signaling bit\" in the nVersion of the block.\n> > b) Either limiting non-SegWit transactions in some way to fix the n**2\n> > sighash and FindAndDelete runtime and memory usage issues or fix them by\n> > utilizing the new sighash type which many wallets and projects have\n> > already implemented for SegWit in the spending of non-SegWit outputs.\n> > c) Your really should have replay protection in any HF. The clever fix\n> from\n> > Spoonnet for poor scaling of optionally allowing non-SegWit outputs to\n> > be spent with SegWit's sighash provides this all in one go.\n> > d) You may wish to consider the possibility of tweaking the witness\n> > discount and possibly discounting other parts of the input - SegWit went\n> > a long ways towards making removal of elements from the UTXO set cheaper\n> > than adding them, but didn't quite get there, you should probably finish\n> > that job. This also provides additional tuneable parameters to allow you\n> > to increase the block size while not having a blowup in the worst-case\n> > block size.\n> > e) Additional commitments at the top of the merkle root - both for\n> > SegWit transactions and as additional space for merged mining and other\n> > commitments which we may wish to add in the future, this should likely\n> > be implemented an \"additional header\" ala Johnson Lau's Spoonnet\n> proposal.\n> >\n> > Additionally, I think your parameters here pose very significant risk to\n> > the Bitcoin ecosystem broadly.\n> >\n> > a) Activating a hard fork with less than 18/24 months (and even then...)\n> > from a fully-audited and supported release of full node software to\n> > activation date poses significant risks to many large software projects\n> > and users. I've repeatedly received feedback from various folks that a\n> > year or more is likely required in any hard fork to limit this risk, and\n> > limited pushback on that given the large increase which SegWit provides\n> > itself buying a ton of time.\n> >\n> > b) Having a significant discontinuity in block size increase only serves\n> > to confuse and mislead users and businesses, forcing them to rapidly\n> > adapt to a Bitcoin which changed overnight both by hardforking, and by\n> > fees changing suddenly. Instead, having the hard fork activate technical\n> > changes, and then slowly increasing the block size over the following\n> > several years keeps things nice and continuous and also keeps us from\n> > having to revisit ye old blocksize debate again six months after\n> activation.\n> >\n> > c) You should likely consider the effect of the many technological\n> > innovations coming down the pipe in the coming months. Technologies like\n> > Lightning, TumbleBit, and even your own RootStock could significantly\n> > reduce fee pressure as transactions move to much faster and more\n> > featureful systems.\n> >\n> > Commitments to aggressive hard fork parameters now may leave miners\n> > without much revenue as far out as the next halving (which current\n> > transaction growth trends are indicating we'd just only barely reach 2MB\n> > of transaction volume, let alone if you consider the effects of users\n> > moving to systems which provide more features for Bitcoin transactions).\n> > This could lead to a precipitous drop in hashrate as miners are no\n> > longer sufficiently compensated.\n> >\n> > Remember that the \"hashpower required to secure bitcoin\" is determined\n> > as a percentage of total Bitcoins transacted on-chain in each block, so\n> > as subsidy goes down, miners need to be paid with fees, not just price\n> > increases. Even if we were OK with hashpower going down compared to the\n> > value it is securing, betting the security of Bitcoin on its price\n> > rising exponentially to match decreasing subsidy does not strike me as a\n> > particularly inspiring tradeoff.\n> >\n> > There aren't many great technical solutions to some of these issues, as\n> > far as I'm aware, but it's something that needs to be incredibly\n> > carefully considered before betting the continued security of Bitcoin on\n> > exponential on-chain growth, something which we have historically never\n> > seen.\n> >\n> > Matt\n> >\n> >\n> > On March 31, 2017 5:09:18 PM EDT, Sergio Demian Lerner via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >>Hi everyone,\n> >>\n> >>Segwit2Mb is the project to merge into Bitcoin a minimal patch that\n> >>aims to\n> >>untangle the current conflict between different political positions\n> >>regarding segwit activation vs. an increase of the on-chain blockchain\n> >>space through a standard block size increase. It is not a new solution,\n> >>but\n> >>it should be seen more as a least common denominator.\n> >>\n> >>Segwit2Mb combines segwit as it is today in Bitcoin 0.14+ with a 2MB\n> >>block\n> >>size hard-fork activated ONLY if segwit activates (95% of miners\n> >>signaling), but at a fixed future date.\n> >>\n> >>The sole objective of this proposal is to re-unite the Bitcoin\n> >>community\n> >>and avoid a cryptocurrency split. Segwit2Mb does not aim to be best\n> >>possible technical solution to solve Bitcoin technical limitations.\n> >>However, this proposal does not imply a compromise to the future\n> >>scalability or decentralization of Bitcoin, as a small increase in\n> >>block\n> >>size has been proven by several core and non-core developers not to\n> >>affect\n> >>Bitcoin value propositions.\n> >>\n> >>In the worst case, a 2X block size increase has much lower economic\n> >>impact\n> >>than the last bitcoin halving (<10%), which succeeded without problem.\n> >>\n> >>On the other side, Segwit2Mb primary goal is to be minimalistic: in\n> >>this\n> >>patch some choices have been made to reduce the number of lines\n> >>modified in\n> >>the current Bitcoin Core state (master branch), instead of implementing\n> >>the\n> >>most elegant solution. This is because I want to reduce the time it\n> >>takes\n> >>for core programmers and reviewers to check the correctness of the\n> >>code,\n> >>and to report and correct bugs.\n> >>\n> >>The patch was built by forking the master branch of Bitcoin Core,\n> >>mixing a\n> >>few lines of code from Jeff Garzik's BIP102,  and defining a second\n> >>versionbits activation bit (bit 2) for the combined activation.\n> >>\n> >>The combined activation of segwit and 2Mb hard-fork nVersion bit is 2\n> >>(DEPLOYMENT_SEGWIT_AND_2MB_BLOCKS).\n> >>\n> >>This means that segwit can still be activated without the 2MB hard-fork\n> >>by\n> >>signaling bit 1 in nVersion  (DEPLOYMENT_SEGWIT).\n> >>\n> >>The tentative lock-in and hard-fork dates are the following:\n> >>\n> >>Bit 2 signaling StartTime = 1493424000; // April 29th, 2017\n> >>\n> >>Bit 2 signaling Timeout = 1503964800; // August 29th, 2017\n> >>\n> >>HardForkTime = 1513209600; // Thu, 14 Dec 2017 00:00:00 GMT\n> >>\n> >>\n> >>The hard-fork is conditional to 95% of the hashing power has approved\n> >>the\n> >>segwit2mb soft-fork and the segwit soft-fork has been activated (which\n> >>should occur 2016 blocks after its lock-in time)\n> >>\n> >>For more information on how soft-forks are signaled and activated, see\n> >>https://github.com/bitcoin/bips/blob/master/bip-0009.mediawiki\n> >>\n> >>This means that segwit would be activated before 2Mb: this is\n> >>inevitable,\n> >>as versionbits have been designed to have fixed activation periods and\n> >>thresholds for all bits. Making segwit and 2Mb fork activate together\n> >>at a\n> >>delayed date would have required a major re-write of this code, which\n> >>would\n> >>contradict the premise of creating a minimalistic patch. However, once\n> >>segwit is activated, the hard-fork is unavoidable.\n> >>\n> >>Although I have coded a first version of the segwit2mb patch (which\n> >>modifies 120 lines of code, and adds 220 lines of testing code), I\n> >>would\n> >>prefer to wait to publish the source code until more comments have been\n> >>received from the community.\n> >>\n> >>To prevent worsening block verification time because of the O(N^2)\n> >>hashing\n> >>problem, the simple restriction that transactions cannot be larger than\n> >>1Mb\n> >>has been kept. Therefore the worse-case of block verification time has\n> >>only\n> >>doubled.\n> >>\n> >>Regarding the hard-fork activation date, I want to give enough time to\n> >>all\n> >>active economic nodes to upgrade. As of Fri Mar 31 2017,\n> >>https://bitnodes.21.co/nodes/ reports that 6332 out of 6955 nodes (91%)\n> >>have upgraded to post 0.12 versions. Upgrade to post 0.12 versions can\n> >>be\n> >>used to identify economic active nodes, because in the 0.12 release\n> >>dynamic\n> >>fees were introduced, and currently no Bitcoin automatic payment system\n> >>can\n> >>operate without automatic discovery of the current fee rate. A pre-0.12\n> >>would require constant manual intervention.\n> >>Therefore I conclude that no more than 91% of the network nodes\n> >>reported by\n> >>bitnodes are active economic nodes.\n> >>\n> >>As Bitcoin Core 0.12 was released on February 2016, the time for this\n> >>91%\n> >>to upgrade has been around one year (under a moderate pressure of\n> >>operational problems with unconfirmed transactions).\n> >>Therefore we can expect a similar or lower time to upgrade for a\n> >>hard-fork,\n> >>after developers have discussed and approved the patch, and it has been\n> >>reviewed and merged and 95% of the hashing power has signaled for it\n> >>(the\n> >>pressure not to upgrade being a complete halt of the operations).\n> >>However I\n> >>suggest that we discuss the hard-fork date and delay it if there is a\n> >>real\n> >>need to.\n> >>\n> >>Currently time works against the Bitcoin community, and so is delaying\n> >>a\n> >>compromise solution. Most of the community agree that halting the\n> >>innovation for several years is a very bad option.\n> >>\n> >>After the comments collected by the community, a BIP will be written\n> >>describing the resulting proposal details.\n> >>\n> >>If segwit2mb locks-in, before hard-fork occurs all bitcoin nodes should\n> >>be\n> >>updated to a Segwit2Mb enabled node to prevent them to be forked-away\n> >>in a\n> >>chain with almost no hashing-power.\n> >>\n> >>The proof of concept patch was made for Bitcoin Core but should be\n> >>easily\n> >>ported to other Bitcoin protocol implementations that already support\n> >>versionbits. Lightweight (SPV) wallets should not be affected as they\n> >>generally do not check the block size.\n> >>\n> >>I personally want to see the Lightning Network in action this year, use\n> >>the\n> >>non-malleability features in segwit, see the community discussing other\n> >>exciting soft-forks in the scaling roadmap, Schnorr sigs, drivechains\n> >>and\n> >>MAST.\n> >>\n> >>I want to see miners, developers and industry side-by-side pushing\n> >>Bitcoin\n> >>forward, to increase the value of Bitcoin and prevent high transaction\n> >>fees\n> >>to put out of business use-cases that could have high positive social\n> >>impact.\n> >>\n> >>I believe in the strength of a unified Bitcoin community. If you're a\n> >>developer, please give your opinion, suggest changes, audit it, and\n> >>take a\n> >>stand with me to unlock the current Bitcoin deadlock.\n> >>\n> >>Contributions to the segwit2mb project are welcomed and awaited. The\n> >>only\n> >>limitation is to stick to the principle that the patch should be as\n> >>simple\n> >>to audit as possible. As an example, I wouldn't feel confident if the\n> >>patch\n> >>modified more than ~150 lines of code.\n> >>\n> >>Improvements unrelated to a 2 Mb increase or segwit, as beneficial as\n> >>it\n> >>may be to Bitcoin, should not be part of segwit2Mb.\n> >>\n> >>This proposal should not prevent other consensus proposals to be\n> >>simultaneously merged: segwit2mb is a last resort solution in case we\n> >>can\n> >>not reach consensus on anything better.\n> >>\n> >>Again, the proposal is only a starting point: community feedback is\n> >>expected and welcomed.\n> >>\n> >>Regards,\n> >>Sergio Demian Lerner\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170401/8b281935/attachment-0001.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2017-04-01T12:33:18",
                "message_text_only": "Segwit replaces the 1 mb size limit with a weight limit of 4 mb. After\nsegwit there's no need for MAX_BLOCK_BASE_SIZE anymore, let alone\nMAX_BLOCK2_BASE_SIZE.\nThus, by \"hf to 2 mb\" it seems you just really mean hardforking from 4\nmb weight to 8 mb weight.\n\nI would also use the hardfork bit (sign bit in block.nNersion) as matt comments.\n\n> We're in a deadlock and it seems we can't go forward adding more functionality to segwit without the community approval (which include miners). This is obvious to me.Then we have to go back.\n\nIf segwit is controversial the way it is (I still don't understand why\ndespite having insistently asking to users and miners who claim to\noppose it), adding more consensus rule changes won't make it any less\ncontroversial. If anything, it would be removing consensus rule\nchanges, not adding them that could make it less controversial.\n\nBy no means I want to dissuade you from working on this bip proposal,\nbut I really don't see how it helps getting out of the deadlock at\nall.\n\n\nOn Sat, Apr 1, 2017 at 1:44 PM, Sergio Demian Lerner via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Some people have asked me for the current implementation of this patch to\n> review. I remind you that the current patch does not implement the hard-fork\n> signaling, as requested by Matt.\n>\n> The Segwit2Mb patch can be found here:\n> https://github.com/SergioDemianLerner/bitcoin/commits/master\n>\n> For now, the segwit2mb repo has a single test file using the old internal\n> blockchain building method (test/block_size_tests.cpp). This must be\n> replaced soon with a better external test using the bitcoin/qa/rpc-tests\n> tests, which I will begin to work on now after I collect all comments from\n> the community.\n>\n>\n> regards\n>\n>\n>\n> On Sat, Apr 1, 2017 at 3:55 AM, Jared Lee Richardson <jaredr26 at gmail.com>\n> wrote:\n>>\n>> > Remember that the \"hashpower required to secure bitcoin\" is determined\n>> > as a percentage of total Bitcoins transacted on-chain in each block\n>>\n>> Can you explain this statement a little better?  What do you mean by\n>> that?  What does the total bitcoins transacted have to do with\n>> hashpower required?\n>>\n>>\n>> On Fri, Mar 31, 2017 at 2:22 PM, Matt Corallo via bitcoin-dev\n>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> > Hey Sergio,\n>> >\n>> > You appear to have ignored the last two years of Bitcoin hardfork\n>> > research and understanding, recycling instead BIP 102 from 2015. There\n>> > are many proposals which have pushed the state of hard fork research\n>> > much further since then, and you may wish to read some of the posts on\n>> > this mailing list listed at https://bitcoinhardforkresearch.github.io/\n>> > and make further edits based on what you learn. Your goal of \"avoid\n>> > technical changes\" appears to not have any basis outside of perceived\n>> > compromise for compromise sake, only making such a hardfork riskier\n>> > instead.\n>> >\n>> > At a minimum, in terms of pure technical changes, you should probably\n>> > consider (probably among others):\n>> >\n>> > a) Utilizing the \"hard fork signaling bit\" in the nVersion of the block.\n>> > b) Either limiting non-SegWit transactions in some way to fix the n**2\n>> > sighash and FindAndDelete runtime and memory usage issues or fix them by\n>> > utilizing the new sighash type which many wallets and projects have\n>> > already implemented for SegWit in the spending of non-SegWit outputs.\n>> > c) Your really should have replay protection in any HF. The clever fix\n>> > from\n>> > Spoonnet for poor scaling of optionally allowing non-SegWit outputs to\n>> > be spent with SegWit's sighash provides this all in one go.\n>> > d) You may wish to consider the possibility of tweaking the witness\n>> > discount and possibly discounting other parts of the input - SegWit went\n>> > a long ways towards making removal of elements from the UTXO set cheaper\n>> > than adding them, but didn't quite get there, you should probably finish\n>> > that job. This also provides additional tuneable parameters to allow you\n>> > to increase the block size while not having a blowup in the worst-case\n>> > block size.\n>> > e) Additional commitments at the top of the merkle root - both for\n>> > SegWit transactions and as additional space for merged mining and other\n>> > commitments which we may wish to add in the future, this should likely\n>> > be implemented an \"additional header\" ala Johnson Lau's Spoonnet\n>> > proposal.\n>> >\n>> > Additionally, I think your parameters here pose very significant risk to\n>> > the Bitcoin ecosystem broadly.\n>> >\n>> > a) Activating a hard fork with less than 18/24 months (and even then...)\n>> > from a fully-audited and supported release of full node software to\n>> > activation date poses significant risks to many large software projects\n>> > and users. I've repeatedly received feedback from various folks that a\n>> > year or more is likely required in any hard fork to limit this risk, and\n>> > limited pushback on that given the large increase which SegWit provides\n>> > itself buying a ton of time.\n>> >\n>> > b) Having a significant discontinuity in block size increase only serves\n>> > to confuse and mislead users and businesses, forcing them to rapidly\n>> > adapt to a Bitcoin which changed overnight both by hardforking, and by\n>> > fees changing suddenly. Instead, having the hard fork activate technical\n>> > changes, and then slowly increasing the block size over the following\n>> > several years keeps things nice and continuous and also keeps us from\n>> > having to revisit ye old blocksize debate again six months after\n>> > activation.\n>> >\n>> > c) You should likely consider the effect of the many technological\n>> > innovations coming down the pipe in the coming months. Technologies like\n>> > Lightning, TumbleBit, and even your own RootStock could significantly\n>> > reduce fee pressure as transactions move to much faster and more\n>> > featureful systems.\n>> >\n>> > Commitments to aggressive hard fork parameters now may leave miners\n>> > without much revenue as far out as the next halving (which current\n>> > transaction growth trends are indicating we'd just only barely reach 2MB\n>> > of transaction volume, let alone if you consider the effects of users\n>> > moving to systems which provide more features for Bitcoin transactions).\n>> > This could lead to a precipitous drop in hashrate as miners are no\n>> > longer sufficiently compensated.\n>> >\n>> > Remember that the \"hashpower required to secure bitcoin\" is determined\n>> > as a percentage of total Bitcoins transacted on-chain in each block, so\n>> > as subsidy goes down, miners need to be paid with fees, not just price\n>> > increases. Even if we were OK with hashpower going down compared to the\n>> > value it is securing, betting the security of Bitcoin on its price\n>> > rising exponentially to match decreasing subsidy does not strike me as a\n>> > particularly inspiring tradeoff.\n>> >\n>> > There aren't many great technical solutions to some of these issues, as\n>> > far as I'm aware, but it's something that needs to be incredibly\n>> > carefully considered before betting the continued security of Bitcoin on\n>> > exponential on-chain growth, something which we have historically never\n>> > seen.\n>> >\n>> > Matt\n>> >\n>> >\n>> > On March 31, 2017 5:09:18 PM EDT, Sergio Demian Lerner via bitcoin-dev\n>> > <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> >>Hi everyone,\n>> >>\n>> >>Segwit2Mb is the project to merge into Bitcoin a minimal patch that\n>> >>aims to\n>> >>untangle the current conflict between different political positions\n>> >>regarding segwit activation vs. an increase of the on-chain blockchain\n>> >>space through a standard block size increase. It is not a new solution,\n>> >>but\n>> >>it should be seen more as a least common denominator.\n>> >>\n>> >>Segwit2Mb combines segwit as it is today in Bitcoin 0.14+ with a 2MB\n>> >>block\n>> >>size hard-fork activated ONLY if segwit activates (95% of miners\n>> >>signaling), but at a fixed future date.\n>> >>\n>> >>The sole objective of this proposal is to re-unite the Bitcoin\n>> >>community\n>> >>and avoid a cryptocurrency split. Segwit2Mb does not aim to be best\n>> >>possible technical solution to solve Bitcoin technical limitations.\n>> >>However, this proposal does not imply a compromise to the future\n>> >>scalability or decentralization of Bitcoin, as a small increase in\n>> >>block\n>> >>size has been proven by several core and non-core developers not to\n>> >>affect\n>> >>Bitcoin value propositions.\n>> >>\n>> >>In the worst case, a 2X block size increase has much lower economic\n>> >>impact\n>> >>than the last bitcoin halving (<10%), which succeeded without problem.\n>> >>\n>> >>On the other side, Segwit2Mb primary goal is to be minimalistic: in\n>> >>this\n>> >>patch some choices have been made to reduce the number of lines\n>> >>modified in\n>> >>the current Bitcoin Core state (master branch), instead of implementing\n>> >>the\n>> >>most elegant solution. This is because I want to reduce the time it\n>> >>takes\n>> >>for core programmers and reviewers to check the correctness of the\n>> >>code,\n>> >>and to report and correct bugs.\n>> >>\n>> >>The patch was built by forking the master branch of Bitcoin Core,\n>> >>mixing a\n>> >>few lines of code from Jeff Garzik's BIP102,  and defining a second\n>> >>versionbits activation bit (bit 2) for the combined activation.\n>> >>\n>> >>The combined activation of segwit and 2Mb hard-fork nVersion bit is 2\n>> >>(DEPLOYMENT_SEGWIT_AND_2MB_BLOCKS).\n>> >>\n>> >>This means that segwit can still be activated without the 2MB hard-fork\n>> >>by\n>> >>signaling bit 1 in nVersion  (DEPLOYMENT_SEGWIT).\n>> >>\n>> >>The tentative lock-in and hard-fork dates are the following:\n>> >>\n>> >>Bit 2 signaling StartTime = 1493424000; // April 29th, 2017\n>> >>\n>> >>Bit 2 signaling Timeout = 1503964800; // August 29th, 2017\n>> >>\n>> >>HardForkTime = 1513209600; // Thu, 14 Dec 2017 00:00:00 GMT\n>> >>\n>> >>\n>> >>The hard-fork is conditional to 95% of the hashing power has approved\n>> >>the\n>> >>segwit2mb soft-fork and the segwit soft-fork has been activated (which\n>> >>should occur 2016 blocks after its lock-in time)\n>> >>\n>> >>For more information on how soft-forks are signaled and activated, see\n>> >>https://github.com/bitcoin/bips/blob/master/bip-0009.mediawiki\n>> >>\n>> >>This means that segwit would be activated before 2Mb: this is\n>> >>inevitable,\n>> >>as versionbits have been designed to have fixed activation periods and\n>> >>thresholds for all bits. Making segwit and 2Mb fork activate together\n>> >>at a\n>> >>delayed date would have required a major re-write of this code, which\n>> >>would\n>> >>contradict the premise of creating a minimalistic patch. However, once\n>> >>segwit is activated, the hard-fork is unavoidable.\n>> >>\n>> >>Although I have coded a first version of the segwit2mb patch (which\n>> >>modifies 120 lines of code, and adds 220 lines of testing code), I\n>> >>would\n>> >>prefer to wait to publish the source code until more comments have been\n>> >>received from the community.\n>> >>\n>> >>To prevent worsening block verification time because of the O(N^2)\n>> >>hashing\n>> >>problem, the simple restriction that transactions cannot be larger than\n>> >>1Mb\n>> >>has been kept. Therefore the worse-case of block verification time has\n>> >>only\n>> >>doubled.\n>> >>\n>> >>Regarding the hard-fork activation date, I want to give enough time to\n>> >>all\n>> >>active economic nodes to upgrade. As of Fri Mar 31 2017,\n>> >>https://bitnodes.21.co/nodes/ reports that 6332 out of 6955 nodes (91%)\n>> >>have upgraded to post 0.12 versions. Upgrade to post 0.12 versions can\n>> >>be\n>> >>used to identify economic active nodes, because in the 0.12 release\n>> >>dynamic\n>> >>fees were introduced, and currently no Bitcoin automatic payment system\n>> >>can\n>> >>operate without automatic discovery of the current fee rate. A pre-0.12\n>> >>would require constant manual intervention.\n>> >>Therefore I conclude that no more than 91% of the network nodes\n>> >>reported by\n>> >>bitnodes are active economic nodes.\n>> >>\n>> >>As Bitcoin Core 0.12 was released on February 2016, the time for this\n>> >>91%\n>> >>to upgrade has been around one year (under a moderate pressure of\n>> >>operational problems with unconfirmed transactions).\n>> >>Therefore we can expect a similar or lower time to upgrade for a\n>> >>hard-fork,\n>> >>after developers have discussed and approved the patch, and it has been\n>> >>reviewed and merged and 95% of the hashing power has signaled for it\n>> >>(the\n>> >>pressure not to upgrade being a complete halt of the operations).\n>> >>However I\n>> >>suggest that we discuss the hard-fork date and delay it if there is a\n>> >>real\n>> >>need to.\n>> >>\n>> >>Currently time works against the Bitcoin community, and so is delaying\n>> >>a\n>> >>compromise solution. Most of the community agree that halting the\n>> >>innovation for several years is a very bad option.\n>> >>\n>> >>After the comments collected by the community, a BIP will be written\n>> >>describing the resulting proposal details.\n>> >>\n>> >>If segwit2mb locks-in, before hard-fork occurs all bitcoin nodes should\n>> >>be\n>> >>updated to a Segwit2Mb enabled node to prevent them to be forked-away\n>> >>in a\n>> >>chain with almost no hashing-power.\n>> >>\n>> >>The proof of concept patch was made for Bitcoin Core but should be\n>> >>easily\n>> >>ported to other Bitcoin protocol implementations that already support\n>> >>versionbits. Lightweight (SPV) wallets should not be affected as they\n>> >>generally do not check the block size.\n>> >>\n>> >>I personally want to see the Lightning Network in action this year, use\n>> >>the\n>> >>non-malleability features in segwit, see the community discussing other\n>> >>exciting soft-forks in the scaling roadmap, Schnorr sigs, drivechains\n>> >>and\n>> >>MAST.\n>> >>\n>> >>I want to see miners, developers and industry side-by-side pushing\n>> >>Bitcoin\n>> >>forward, to increase the value of Bitcoin and prevent high transaction\n>> >>fees\n>> >>to put out of business use-cases that could have high positive social\n>> >>impact.\n>> >>\n>> >>I believe in the strength of a unified Bitcoin community. If you're a\n>> >>developer, please give your opinion, suggest changes, audit it, and\n>> >>take a\n>> >>stand with me to unlock the current Bitcoin deadlock.\n>> >>\n>> >>Contributions to the segwit2mb project are welcomed and awaited. The\n>> >>only\n>> >>limitation is to stick to the principle that the patch should be as\n>> >>simple\n>> >>to audit as possible. As an example, I wouldn't feel confident if the\n>> >>patch\n>> >>modified more than ~150 lines of code.\n>> >>\n>> >>Improvements unrelated to a 2 Mb increase or segwit, as beneficial as\n>> >>it\n>> >>may be to Bitcoin, should not be part of segwit2Mb.\n>> >>\n>> >>This proposal should not prevent other consensus proposals to be\n>> >>simultaneously merged: segwit2mb is a last resort solution in case we\n>> >>can\n>> >>not reach consensus on anything better.\n>> >>\n>> >>Again, the proposal is only a starting point: community feedback is\n>> >>expected and welcomed.\n>> >>\n>> >>Regards,\n>> >>Sergio Demian Lerner\n>> > _______________________________________________\n>> > bitcoin-dev mailing list\n>> > bitcoin-dev at lists.linuxfoundation.org\n>> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Natanael",
                "date": "2017-04-01T13:15:15",
                "message_text_only": "Den 1 apr. 2017 14:33 skrev \"Jorge Tim\u00f3n via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org>:\n\nSegwit replaces the 1 mb size limit with a weight limit of 4 mb.\n\n\nThat would make it a hardfork, not a softfork, if done exactly as you say.\n\nSegwit only separates out signature data. The 1 MB limit remains, but would\nnow only cover the contents of the transaction scripts. With segwit that\nmeans we have two (2) size limits, not one. This is important to remember.\nEven with segwit + MAST for large complex scripts, there's still going to\nbe a very low limit to the total number of possible transactions per block.\nAnd not all transactions will get the same space savings.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170401/a30e2efd/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2017-04-01T14:07:32",
                "message_text_only": "On Sat, Apr 1, 2017 at 3:15 PM, Natanael <natanael.l at gmail.com> wrote:\n>\n>\n> Den 1 apr. 2017 14:33 skrev \"Jorge Tim\u00f3n via bitcoin-dev\"\n> <bitcoin-dev at lists.linuxfoundation.org>:\n>\n> Segwit replaces the 1 mb size limit with a weight limit of 4 mb.\n>\n>\n> That would make it a hardfork, not a softfork, if done exactly as you say.\n>\n> Segwit only separates out signature data. The 1 MB limit remains, but would\n> now only cover the contents of the transaction scripts. With segwit that\n> means we have two (2) size limits, not one. This is important to remember.\n> Even with segwit + MAST for large complex scripts, there's still going to be\n> a very low limit to the total number of possible transactions per block. And\n> not all transactions will get the same space savings.\n\nNo, because of the way the weight is calculated, it is impossible to\ncreate a block that old nodes would perceive as bigger than 1 mb\nwithout also violating the weight limit.\nAfter segwit activation, nodes supporting segwit don't need to\nvalidate the 1 mb size limit anymore as long as they validate the\nweight limit. The weight is also the only notion of cost miners need\nto consider when comparing txs by feerate (fee per cost, before segwit\ntx_fee/tx_size, post-segwit tx_fee/tx_weight).\nThis is important to remember, because having 2 separated limits or\ncosts would make block creation and relay policies much harder to\nimplement.\n\nTherefore a hardfork after segwit can just increase the weight limit\nand completely forget about the pre-segwit 1 mb size limit."
            },
            {
                "author": "Natanael",
                "date": "2017-04-01T15:34:24",
                "message_text_only": "Den 1 apr. 2017 16:07 skrev \"Jorge Tim\u00f3n\" <jtimon at jtimon.cc>:\n\nOn Sat, Apr 1, 2017 at 3:15 PM, Natanael <natanael.l at gmail.com> wrote:\n>\n>\n> Den 1 apr. 2017 14:33 skrev \"Jorge Tim\u00f3n via bitcoin-dev\"\n> <bitcoin-dev at lists.linuxfoundation.org>:\n>\n> Segwit replaces the 1 mb size limit with a weight limit of 4 mb.\n>\n>\n> That would make it a hardfork, not a softfork, if done exactly as you say.\n>\n> Segwit only separates out signature data. The 1 MB limit remains, but\nwould\n> now only cover the contents of the transaction scripts. With segwit that\n> means we have two (2) size limits, not one. This is important to remember.\n> Even with segwit + MAST for large complex scripts, there's still going to\nbe\n> a very low limit to the total number of possible transactions per block.\nAnd\n> not all transactions will get the same space savings.\n\nNo, because of the way the weight is calculated, it is impossible to\ncreate a block that old nodes would perceive as bigger than 1 mb\nwithout also violating the weight limit.\nAfter segwit activation, nodes supporting segwit don't need to\nvalidate the 1 mb size limit anymore as long as they validate the\nweight limit.\n\n\nhttps://github.com/bitcoin/bips/blob/master/bip-0141.mediawiki#Block_size\n\nHuh, that's odd. It really does still count raw blockchain data blocksize.\n\nIt just uses a ratio between how many units each byte is worth for block\ndata vs signature data, plus a cap to define the maximum. So the current\nmax is 4 MB, with 1 MB of non-witness blockchain data being weighted to 4x\n= 4 MB. That just means you replaced the two limits with one limit and a\nratio.\n\nA hardfork increasing the size would likely have the ratio modified too.\nWith exactly the same effect as if it was two limits...\n\nEither way, there's still going to be non-segwit nodes for ages.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170401/831f0f43/attachment-0001.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2017-04-02T04:57:48",
                "message_text_only": "On Sat, Apr 1, 2017 at 5:34 PM, Natanael <natanael.l at gmail.com> wrote:\n> Den 1 apr. 2017 16:07 skrev \"Jorge Tim\u00f3n\" <jtimon at jtimon.cc>:\n> On Sat, Apr 1, 2017 at 3:15 PM, Natanael <natanael.l at gmail.com> wrote:\n>> Den 1 apr. 2017 14:33 skrev \"Jorge Tim\u00f3n via bitcoin-dev\"\n>> <bitcoin-dev at lists.linuxfoundation.org>:\n>> Segwit replaces the 1 mb size limit with a weight limit of 4 mb.\n>> That would make it a hardfork, not a softfork, if done exactly as you say.\n> No, because of the way the weight is calculated, it is impossible to\n> create a block that old nodes would perceive as bigger than 1 mb\n> without also violating the weight limit.\n> After segwit activation, nodes supporting segwit don't need to\n> validate the 1 mb size limit anymore as long as they validate the\n> weight limit.\n>\n> https://github.com/bitcoin/bips/blob/master/bip-0141.mediawiki#Block_size\n>\n> Huh, that's odd. It really does still count raw blockchain data blocksize.\n\nIt's not odd, it's just counter-intuitive. How can \"< 4 mb weight\" be\na more restrictive rule than \"< 1 mb size\"? Well, it is, that's why\nsegwit's size increase is a softfork.\nIt is not that hard once you look at the actual weight formula:\nsegregated_sigs_sise + (other_size * 4) < 4 \"mb\"\n\nIt is impossible to produce to produce a block that violates the 1 mb\nsize limit but doesn't violate the 4 mb weight limit too.\nThere can be block that are < 1 mb size but 20 mb in weight, but those\nare invalid according to the new 4 mb weight rule.\nAt the same time, any block that violates the < 1 mb rule for old\nnodes will be invalid not only to old nodes but also to any node\nvalidating the new 4mb rule. This is not by chance but a design choice\nfor any block size increase within segwit to remain a softfork, which\nis what can be deployed faster.\n\nOne extreme example would be any 1 mb block today. 1 \"mb\" of a block\ntoday times 4 is 4 mb, so it complies with the new 4 mb weight rule.\nThe opposite extreme example would be 4 mb of signatures and 0 mb of\n\"other data\", but this example is not really possible in practice\nbecause signatures need some tx to be part of to be part of the block\nitself.\nThe most extreme examples I have seen on testnet are 3.7 mb blocks,\nbut those don't represent the average usage today (whenever you read\nthis).\n\nOne common misunderstanding is that users who aren't using payment\nchannels (that includes lightning but also other smart contracts) or\nusers that aren't using mutlisig can't enjoy the so called \"discount\":\nthere's no reasonable argument for rejecting the \"discount\" on your\nown transactions once/if segwit gets activated.\n\nI would prefer to call the absence of \"discount\" *penalization*.\nSignatures are unreasonable penalized pre-segwit, and there's more\nthings that remain unreasonably penalized with respect to their\ninfluence on the current utxo after segwit. But signatures are by far\nthe biggest in data space and validation time, and the most important\nunreasonable yet unintended penalization pre-segwit.\n\n> It just uses a ratio between how many units each byte is worth for block\n> data vs signature data, plus a cap to define the maximum. So the current max\n> is 4 MB, with 1 MB of non-witness blockchain data being weighted to 4x = 4\n> MB. That just means you replaced the two limits with one limit and a ratio.\n\nExactly, once one maximum limit is defined, no need for two limits.\nBut the current max is 1 mb size, not 4 mb weight until/unless segwit\nis activated.\nSome people complain about 4 mb weight not being as much as 4 mb size,\nand that is correct, but both are bigger than 1 mb size.\n\n> A hardfork increasing the size would likely have the ratio modified too.\n\nIf the single ratio needs to be modified, it can be modified now\nbefore any rule changes are activated, no need to change the consensus\nrules more than needed.\n\n> With exactly the same effect as if it was two limits...\n\nIf you don't see any disadvantage on having one single limit if/when\nsegwit gets activated, I don't see the point of maintaining two\nlimits, but if you're happy to maintain the branch with the redundant\none you may get my ack: I don't see any disadvantage on checking the\nsame thing twice besides performance,\n\n> Either way, there's still going to be non-segwit nodes for ages.\n\nThat's precisely why it's good segwit has been designed to be backward\ncompatible as a bip9 softfork."
            },
            {
                "author": "Natanael",
                "date": "2017-04-02T10:03:31",
                "message_text_only": "My point, if you missed it, is that there's a mathematical equivalence\nbetween using two limits (and calculating the ratio) vs using one limit and\na ratio. The output is fully identical. The only difference is the order of\noperations. Saying there's no blocksize limit with this is pretty\nmeaningless, because you're just saying you're using an abstraction that\ndoesn't make the limit visible.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170402/11f9c875/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2017-04-02T11:43:54",
                "message_text_only": "Just saying that we can talk in terms of weight alone after segwit. 8 mb\nweight is much more clear than 2 mb size to me. 2 mb size seems to\nobfuscate the actual new limit with the proposed hf, which simply 8 mb\nweight.\n\nOn 2 Apr 2017 12:03 pm, \"Natanael\" <natanael.l at gmail.com> wrote:\n\n> My point, if you missed it, is that there's a mathematical equivalence\n> between using two limits (and calculating the ratio) vs using one limit and\n> a ratio. The output is fully identical. The only difference is the order of\n> operations. Saying there's no blocksize limit with this is pretty\n> meaningless, because you're just saying you're using an abstraction that\n> doesn't make the limit visible.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170402/73a2096f/attachment.html>"
            },
            {
                "author": "Btc Drak",
                "date": "2017-04-03T14:40:04",
                "message_text_only": "On Fri, Mar 31, 2017 at 10:09 PM, Sergio Demian Lerner via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> The hard-fork is conditional to 95% of the hashing power has approved the\n> segwit2mb soft-fork and the segwit soft-fork has been activated (which\n> should occur 2016 blocks after its lock-in time)\n>\n\nMiners signalling they have upgraded by flipping a bit in the nVersion\nfield has little relevance in a hard fork. If 100% of the hash power\nindicates they are running this proposal, but the nodes don't upgrade, what\nwill happen?\n\nFor the record, I actually talk a lot about hard forks with various\ndevelopers and am very interested in the research that Johnson in\nparticular is pioneering. However, I have failed to understand your point\nabout 95% miner signalling in relation to a hard fork, so I am eagerly\nawaiting your explanation.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170403/daee0659/attachment.html>"
            },
            {
                "author": "Erik Aronesty",
                "date": "2017-04-06T02:27:34",
                "message_text_only": "I personally appreciate the minimal changes, and often encourage\ndevelopment to be done this way - when it needs to be released quickly.\nBut does this need to be released quickly?\n\n- maybe the proposal should be renamed segwit 8mb and be discussed solely\nin terms of block weights.\n\n- a high consensus hard fork is probably preferable to a low consensus soft\nfork, however there is nothing to indicate that segwit as it stands isnt\nalready very high consensus except for a handful of pool operators\nprotecting fee income.\n\n- miners who currently object to segwit while pretending to like larger\nblocks will find some excuse to object to this too.\n\n- Given the challenges miners seem to have in flipping bits, I expect any\nfork that requires 95pct hash power to be vaporware.\n\nOn Apr 3, 2017 11:02 AM, \"Btc Drak via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Fri, Mar 31, 2017 at 10:09 PM, Sergio Demian Lerner via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> The hard-fork is conditional to 95% of the hashing power has approved the\n>> segwit2mb soft-fork and the segwit soft-fork has been activated (which\n>> should occur 2016 blocks after its lock-in time)\n>>\n>\n> Miners signalling they have upgraded by flipping a bit in the nVersion\n> field has little relevance in a hard fork. If 100% of the hash power\n> indicates they are running this proposal, but the nodes don't upgrade, what\n> will happen?\n>\n> For the record, I actually talk a lot about hard forks with various\n> developers and am very interested in the research that Johnson in\n> particular is pioneering. However, I have failed to understand your point\n> about 95% miner signalling in relation to a hard fork, so I am eagerly\n> awaiting your explanation.\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170405/5dcedc23/attachment.html>"
            },
            {
                "author": "Sergio Demian Lerner",
                "date": "2017-04-06T20:58:56",
                "message_text_only": "Responding between lines...\n\nOn Wed, Apr 5, 2017 at 11:27 PM, Erik Aronesty <earonesty at gmail.com> wrote:\n\n> I personally appreciate the minimal changes, and often encourage\n> development to be done this way - when it needs to be released quickly.\n> But does this need to be released quickly?\n>\n>\nSegwit2mb is a last resort option. It does not need to be released quickly.\nNot at all. It just needs to be there in case no other option is chosen. I\nput tentative dates. We can move them.\n\n\n> - maybe the proposal should be renamed segwit 8mb and be discussed solely\n> in terms of block weights.\n>\n\nThe name does not matter much. The name means joining segwit with a 2Mb\nhard-fork. It's a grammatical name.\n\nI also disagree with the idea that segwit2mb is a 8mb increase. As stated\nby in the Bitcon.org website [1] and backed up by scientific research, \u201ca\nblock filled with standard single-signature P2PKH transactions would be\nabout 1.6MB and a block filled with 2-of-2 multisignature transactions\nwould be about 2.0MB.\u201d. As standard blocks are a combination between P2PKH,\nand 2-of-3 multisignatures, the actual average segwit block size will be\nclose to 2.0MB.\n\nBecause Segwit2Mb doubles the maximum size of a block, the average block\nsize for a block filed with average transactions is 4.0Mb.\n\n[1] https://bitcoin.org/en/bitcoin-core/capacity-increases-faq#segwit-size\n\nI can explain in a following e-mail why creating 8Mb blocks on purpose is\ngenerally is an irrational choice. And in the case where it could provide\nan economic benefit, adding parallel block validation to Core nullifies any\nadversary advantage.\n\n\n\n> a high consensus hard fork is probably preferable to a low consensus soft\nfork, however there is nothing to indicate that segwit as it stands isnt\nalready very high consensus except for a handful of pool operators\nprotecting fee income.\n\nYou and me may never know the reasons why these operators (or many many of\nother users) prefer to increase the block-size. I suppose it has to do high\nthe high current transaction fees as compared to less than a year ago.\nAnyway consensus can only be achieved if one understands the others may\nhave reasons that do not match ours.\n\nLast, if this proposal is rejected by any side, then we'll definitively\nlearn that side is not looking for any consensual resolution of the\nconflict.\n\n\n> - miners who currently object to segwit while pretending to like larger\n> blocks will find some excuse to object to this too.\n>\n>\nIf they do, we'll get a lot of public, verifiable information from that\nfact. The cost to include this patch is low compared with the benefit it\ncan bring and the information we can gather in case one of the sides\nrejects it.\n\nHowever, I've received positive feedback from them until now.\n\n\n> - Given the challenges miners seem to have in flipping bits, I expect any\n> fork that requires 95pct hash power to be vaporware.\n>\n\nThen we'll learn a lot about hard-forks, the limits of miner \"voting\" and\nthe quorums we can expect.\n\nRegards,\nSergio.\n\n\n>\n> On Apr 3, 2017 11:02 AM, \"Btc Drak via bitcoin-dev\" <bitcoin-dev at lists.\n> linuxfoundation.org> wrote:\n>\n>> On Fri, Mar 31, 2017 at 10:09 PM, Sergio Demian Lerner via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> The hard-fork is conditional to 95% of the hashing power has approved\n>>> the segwit2mb soft-fork and the segwit soft-fork has been activated (which\n>>> should occur 2016 blocks after its lock-in time)\n>>>\n>>\n>> Miners signalling they have upgraded by flipping a bit in the nVersion\n>> field has little relevance in a hard fork. If 100% of the hash power\n>> indicates they are running this proposal, but the nodes don't upgrade, what\n>> will happen?\n>>\n>> For the record, I actually talk a lot about hard forks with various\n>> developers and am very interested in the research that Johnson in\n>> particular is pioneering. However, I have failed to understand your point\n>> about 95% miner signalling in relation to a hard fork, so I am eagerly\n>> awaiting your explanation.\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170406/5c6c4314/attachment-0001.html>"
            },
            {
                "author": "Sergio Demian Lerner",
                "date": "2017-04-06T20:42:19",
                "message_text_only": "The 95% miner signaling is important to prevent two Bitcoin forks, such as\nwhat happened with Ethereum HF and Ethereum Classic.\n\nBitcoin has a very slow difficulty re-targeting algorithm. A fork that has\njust 95% miner support will initially (for 2016 blocks) be 5% slower (an\naverage block every 10 minutes and 30 seconds). The transaction capacity of\nthe new Bitcoin protocol is reduced only 5%.\nHowever the chain with 5% if the hashing power not only has a 20x capacity\nreduction, but confirms transactions in 20x more time. So the mempool will\ngrow 400 times. It must be noted that fees increased 10x from the moment\nblocks were half full, to the moment blocks became saturated. I'm sure no\nBitcoin (pre-fork) user will be willing to pay 100x times the transaction\nfees to use such a slow and insecure network.\n\nSo a 6-block confirmation will take 20 hours in the original chain and the\noriginal chain will be in this almost useless slow state for an average of\n2016 blocks, or 280 days.\nIf the original blockchain hard-forks to re-adjust the difficulty, then it\nwill just represent an alt-coin having 5% of Bitcoin community, and it\ncan't affect Bitcoin (the segwit2mb fork).\n\n\nOn Mon, Apr 3, 2017 at 11:40 AM, Btc Drak <btcdrak at gmail.com> wrote:\n\n> On Fri, Mar 31, 2017 at 10:09 PM, Sergio Demian Lerner via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> The hard-fork is conditional to 95% of the hashing power has approved the\n>> segwit2mb soft-fork and the segwit soft-fork has been activated (which\n>> should occur 2016 blocks after its lock-in time)\n>>\n>\n> Miners signalling they have upgraded by flipping a bit in the nVersion\n> field has little relevance in a hard fork. If 100% of the hash power\n> indicates they are running this proposal, but the nodes don't upgrade, what\n> will happen?\n>\n> For the record, I actually talk a lot about hard forks with various\n> developers and am very interested in the research that Johnson in\n> particular is pioneering. However, I have failed to understand your point\n> about 95% miner signalling in relation to a hard fork, so I am eagerly\n> awaiting your explanation.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170406/a91923df/attachment.html>"
            },
            {
                "author": "Sergio Demian Lerner",
                "date": "2017-04-06T21:03:12",
                "message_text_only": "Ups. My mistake:  the mempool will not grow 400 times, the is no square\nthere.\nI will initially grow 20 times. Multiplied by the number of times a\ntransaction may need to be replaced with one with higher fees. Maybe 50\ntimes, but not 400.\n\n\n\nOn Thu, Apr 6, 2017 at 5:42 PM, Sergio Demian Lerner <\nsergio.d.lerner at gmail.com> wrote:\n\n> The 95% miner signaling is important to prevent two Bitcoin forks, such as\n> what happened with Ethereum HF and Ethereum Classic.\n>\n> Bitcoin has a very slow difficulty re-targeting algorithm. A fork that has\n> just 95% miner support will initially (for 2016 blocks) be 5% slower (an\n> average block every 10 minutes and 30 seconds). The transaction capacity of\n> the new Bitcoin protocol is reduced only 5%.\n> However the chain with 5% if the hashing power not only has a 20x capacity\n> reduction, but confirms transactions in 20x more time. So the mempool will\n> grow 400 times. It must be noted that fees increased 10x from the moment\n> blocks were half full, to the moment blocks became saturated. I'm sure no\n> Bitcoin (pre-fork) user will be willing to pay 100x times the transaction\n> fees to use such a slow and insecure network.\n>\n> So a 6-block confirmation will take 20 hours in the original chain and the\n> original chain will be in this almost useless slow state for an average of\n> 2016 blocks, or 280 days.\n> If the original blockchain hard-forks to re-adjust the difficulty, then it\n> will just represent an alt-coin having 5% of Bitcoin community, and it\n> can't affect Bitcoin (the segwit2mb fork).\n>\n>\n> On Mon, Apr 3, 2017 at 11:40 AM, Btc Drak <btcdrak at gmail.com> wrote:\n>\n>> On Fri, Mar 31, 2017 at 10:09 PM, Sergio Demian Lerner via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> The hard-fork is conditional to 95% of the hashing power has approved\n>>> the segwit2mb soft-fork and the segwit soft-fork has been activated (which\n>>> should occur 2016 blocks after its lock-in time)\n>>>\n>>\n>> Miners signalling they have upgraded by flipping a bit in the nVersion\n>> field has little relevance in a hard fork. If 100% of the hash power\n>> indicates they are running this proposal, but the nodes don't upgrade, what\n>> will happen?\n>>\n>> For the record, I actually talk a lot about hard forks with various\n>> developers and am very interested in the research that Johnson in\n>> particular is pioneering. However, I have failed to understand your point\n>> about 95% miner signalling in relation to a hard fork, so I am eagerly\n>> awaiting your explanation.\n>>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170406/3b9044c6/attachment.html>"
            },
            {
                "author": "Aymeric Vitte",
                "date": "2017-04-06T22:29:51",
                "message_text_only": "Not sure to get how you are answering the question\n\n>  If the original blockchain hard-forks to re-adjust the difficulty,\nthen it will just represent an alt-coin having 5% of Bitcoin community,\nand it can't affect Bitcoin (the segwit2mb fork).\n\ndestroys the whole thing\n\nBecause if the nodes don't upgrade and just implement the patch to\nadjust the difficulty, what happens? You get a 95% mining power chain\nwith \"no\" nodes and a 5% one with \"all\" the nodes\n\nI really don't get in all those discussions why the nodes are always\ndisconsidered compared to the miners, ie why they seem to be of zero\nimportance and are supposed to obey whatever you ask them\n\nAnd apparently the trend is not going to revert if we look at the\npriority features sent in the asicboost thread where motivating and\nscaling full nodes is still something you need very powerful glasses to\nsee coming\n\n\nLe 06/04/2017 \u00e0 22:42, Sergio Demian Lerner via bitcoin-dev a \u00e9crit :\n> The 95% miner signaling is important to prevent two Bitcoin forks,\n> such as what happened with Ethereum HF and Ethereum Classic.\n>\n> Bitcoin has a very slow difficulty re-targeting algorithm. A fork that\n> has just 95% miner support will initially (for 2016 blocks) be 5%\n> slower (an average block every 10 minutes and 30 seconds). The\n> transaction capacity of the new Bitcoin protocol is reduced only 5%. \n> However the chain with 5% if the hashing power not only has a 20x\n> capacity reduction, but confirms transactions in 20x more time. So the\n> mempool will grow 400 times. It must be noted that fees increased 10x\n> from the moment blocks were half full, to the moment blocks became\n> saturated. I'm sure no Bitcoin (pre-fork) user will be willing to pay\n> 100x times the transaction fees to use such a slow and insecure network.\n>\n> So a 6-block confirmation will take 20 hours in the original chain and\n> the original chain will be in this almost useless slow state for an\n> average of 2016 blocks, or 280 days. \n> If the original blockchain hard-forks to re-adjust the difficulty,\n> then it will just represent an alt-coin having 5% of Bitcoin\n> community, and it can't affect Bitcoin (the segwit2mb fork).\n>\n>\n> On Mon, Apr 3, 2017 at 11:40 AM, Btc Drak <btcdrak at gmail.com\n> <mailto:btcdrak at gmail.com>> wrote:\n>\n>     On Fri, Mar 31, 2017 at 10:09 PM, Sergio Demian Lerner via\n>     bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org\n>     <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n>\n>         The hard-fork is conditional to 95% of the hashing power has\n>         approved the segwit2mb soft-fork and the segwit soft-fork has\n>         been activated (which should occur 2016 blocks after its\n>         lock-in time)\n>\n>\n>     Miners signalling they have upgraded by flipping a bit in the\n>     nVersion field has little relevance in a hard fork. If 100% of the\n>     hash power indicates they are running this proposal, but the nodes\n>     don't upgrade, what will happen?\n>\n>     For the record, I actually talk a lot about hard forks with\n>     various developers and am very interested in the research that\n>     Johnson in particular is pioneering. However, I have failed to\n>     understand your point about 95% miner signalling in relation to a\n>     hard fork, so I am eagerly awaiting your explanation.\n>\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n-- \nZcash wallets made simple: https://github.com/Ayms/zcash-wallets\nBitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets\nGet the torrent dynamic blocklist: http://peersm.com/getblocklist\nCheck the 10 M passwords list: http://peersm.com/findmyass\nAnti-spies and private torrents, dynamic blocklist: http://torrent-live.org\nPeersm : http://www.peersm.com\ntorrent-live: https://github.com/Ayms/torrent-live\nnode-Tor : https://www.github.com/Ayms/node-Tor\nGitHub : https://www.github.com/Ayms\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170407/b0414860/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Segwit2Mb - combined soft/hard fork - Request For Comments",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Natanael",
                "Samson Mow",
                "Aymeric Vitte",
                "Sergio Demian Lerner",
                "Jorge Tim\u00f3n",
                "Btc Drak",
                "Erik Aronesty",
                "Jared Lee Richardson"
            ],
            "messages_count": 17,
            "total_messages_chars_count": 99172
        }
    },
    {
        "title": "[bitcoin-dev] A Better MMR Definition",
        "thread_messages": [
            {
                "author": "praxeology_guy",
                "date": "2017-04-01T10:18:12",
                "message_text_only": "Peter Todd,\n\nThis MMR structure looks good to me. I really like how wallets keep their MMR proof and txo index instead of requiring the entire network to maintain an index on txids w/ plain old utxo snapshots.\n\nRe: \"only left or right child of inner node be a fully spent node\"... that sounds fine to me, but the software should virtually consider that the previous dissapearing leaf nodes still exist. It would instead say be a special case handled by the meta hashing function. Would save a good amount of time from unneccesary hashing. Might also do the rule: if a parent node has a single fully spent child node, its hash is equal to its other child's hash.\n\nBelow is questions about txo/utxo MMR commitments after reading: \"https://petertodd.org/2016/delayed-txo-commitments\".\n\nI'm mainly concerned about the performance of recalculating all of the node hashes on old spends. But probably with a long enough delay policy, it shouldn't be an issue.\n\nThen the issues with people keeping their MMR proofs up to date and discovering received txos before they get pruned. Sure would be nice if a wallet didn't have to keep on updating their MMR proof. Hopefully spends would refer to old txos by their MMR index.\n\nHow are you ordering MMR additions? Are you only adding old utxos to the MMR? Or every old txo? I think you are doing all old txos (mostly would be spent nodes), but why not just old utxos? Are you doing it to make MMR index = blockchain txo index, so such an index can be used in all TX inputs except for non-confirmed transactions? Potentially a tx could use a MMR.ix, allblock'stxo.ix (if we want to maintian that index), or tx.id & vout.ix depending on how old the tx is.\n\nWhat is the process for removing old utxos from the utxo set, and placing them into the MMR? Are you going to keep height in the utxo db, and then iterate through the whole thing?\n\nAre you still proposing a 1 year txo commitment delay? Do you have any data/preformance studies judging the cost of a larger utxo and longer delay vs smaller utxo and shorder delay? I would figure a longer delay would be better as long as the utxo doesn't get too big. Longer delay means less MMR maintainance.\n\nHave you considered not making a new commitment on every block, instead maybe every 6*24 blocks or so? This could reduce the number of times the same nodes are re-hashed, while not having much impact on security.\n\nWhat about re-orgs? You'd have to restore the previous leaf & inner nodes. You'd need both the txos and MMR proofs, right? It looks like you clear this info from the TXO journal when the delay duration threshold is met. I guess this info might also be stored with the block data, and could be recovered from there.\n\nWhat are your current thoughts on block size weighting for MMR proofs? Just count the extra byte length that full nodes need to relay as simliar to SegWit witness data?\n\nCheers,\nPraxeology Guy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170401/0059dc09/attachment.html>"
            },
            {
                "author": "praxeology_guy",
                "date": "2017-04-01T19:46:12",
                "message_text_only": "gmaxwell told me that most nodes would keep a full copy of the top of the MMR tree.\n\nHere I am exploring how this could be policy-ized to solve two problems:\n- MMR proofs change over time\n- How to coordinate nodes to get them to keep different portions of the MMR, so that everyone can prune most of the structure, but the entire network still retains multiple copies of the full MMR.\n\nDefine deltaLeafHeight as the number of tree layers between a node and the leaves in the MMR data structure. We make it a policy that nodes are expected to have all nodes above deltaLeafHeight = DLH_REQUIRED, but that nodes are free to prune any nodes with a deltaLeafHeight < DLH_REQUIRED. Of course a node could prune at DLH_REQUIRED or higher, but what I am proposing is that messages and proofs by default would only include nodes at deltaLeafHeight < DLH_REQUIRED.\n\nGiven the above, If a wallet didn't want to be continuously concerned about updating their MMR proof for its coins, then for each coin:\n- store the set of utxo digests that are children of the \"root nearest\" node that is at deltaLeafHeight = DLH_REQUIRED. Call such a set of utxo digests the \"pruned relatives\".\n- Pruned relative count = 2^DLH_REQUIRED -1\n- Guessing the spentness status of the pruned relatives would worst case take 2^(pruned relative count) guesses.\n- in the case where the MMR holds all txos (not just utxos at addition time)... the wallet should also keep record of which of the pruned relatives were utxos.\n- Any future information discovered about whether a pruned relative is spent would reduce the worst case guess count by a factor of 2.\n\nAs an example, in the case where DLH_REQUIRED = 3:\n- pruned relative count = 7\n- worst case spentness guess count = 128\n\nWallets storing the digests of pruned relatives could also help the entire network be able to discover otherwise lost portions of the MMR. If wallets stored not just the pruned relatives digests, but also their corresponding utxos, they could help other nodes find lost coins.\n\nCheers,\nPraxeology Guy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170401/084274f5/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "A Better MMR Definition",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "praxeology_guy"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 5315
        }
    },
    {
        "title": "[bitcoin-dev] hard-fork \"X+Y\" compromise discussion re-run",
        "thread_messages": [
            {
                "author": "Adam Back",
                "date": "2017-04-01T13:18:18",
                "message_text_only": "I agree with everything Matt said.  This \"X+Y\" \"compromise\" is not a\nnew proposal and has been hashed over multiple times in the past\ndating back to at least fall 2015, ignores basically all design\nconsiderations and research over the last  years, doesn't understand\nthe real-politic of the delays, and so doesn't even help in the\npolitical domain.\n\nI have taken the liberty of making a reddit thread with some of the\nprevious explainers about why this doesn't work in practice (even\nignoring all politics and hypothetically assuming it was a great\nall-new idea), let the discussion commence!\n\nhttps://www.reddit.com/r/Bitcoin/comments/62rrlv/how_about_a_new_compromise_activate_the_existing/\n\nUASF is a more logical step, than these \"X+Y\" politically motivated\nhard-forks, though UASF has risks vs SegWit BIPs in flight, the delay\nand risk is far lower than political hard-forks.\n\nI have set the reply-to to bitcoin-discuss.\n\nAdam"
            }
        ],
        "thread_summary": {
            "title": "hard-fork \"X+Y\" compromise discussion re-run",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Adam Back"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 931
        }
    },
    {
        "title": "[bitcoin-dev] Guessing the spentness status of the pruned relatives",
        "thread_messages": [
            {
                "author": "praxeology_guy",
                "date": "2017-04-01T20:04:33",
                "message_text_only": "Bitcoin nodes could also keep a spentness status list, where each bit in the spentness status list corresponds to whether a txo in the MMR is spent. This could make it so that disconnected wallets didn't have to guess the pruned relative spentness status when it reconnects to the network... and help prevent DoS attacks.\n\nKeeping such a bit list would consume considerably less space if stxos were never added to the MMR. Putting portions of such a list in the node at height DLH_REQUIRED would made R/W operations on the bit list more local to other data that is going to be R/W.\n\nCheers,\nPraxeology Guy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170401/c545f81f/attachment.html>"
            },
            {
                "author": "bfd at cock.lu",
                "date": "2017-04-01T23:38:45",
                "message_text_only": "If a wallet is unaware of spends of its own coins (ie, transactions\nwere made it can't have known about), there's probably bigger problems\ngoing on. You might enjoy the topic on this mailing list on committed\nbloom filters however, as this solves a similar issue without needing\nan ever-growing list of hundreds of millions of spent outputs.\n\n\nOn 2017-04-02 06:04, praxeology_guy via bitcoin-dev wrote:\n> Bitcoin nodes could also keep a spentness status list, where each bit\n> in the spentness status list corresponds to whether a txo in the MMR\n> is spent.  This could make it so that disconnected wallets didn't have\n> to guess the pruned relative spentness status when it reconnects to\n> the network... and help prevent DoS attacks."
            },
            {
                "author": "praxeology_guy",
                "date": "2017-04-02T01:10:53",
                "message_text_only": "Not sure if you are BFD or BF Trolling D, BFTD. But I will bite this time.\n\nSorry I mistakenly forgot to change the subject back to \"A Better MMR Definition\" when I decided to send the email to the dev list instead of directly to Peter. So then you made such a reply without knowing context.\n\nWith using the MMR data structure for txo commitments, its preferable that wallets only keep information pertinent to their own spendable coins. In previous communication we talked about how wallets could maintain the changing MMR proof for their old coins. Yes wallets know which of their own coins are spent. But with MMR proofs wallets also need to know the spentness status of close relatives in the MMR tree... in order to construct a valid MMR proof that their own coin is not spent.\n\nHope that... clears it up for you.\n\nCheers,\nP. Guy\n\n-------- Original Message --------\nSubject: Re: [bitcoin-dev] Guessing the spentness status of the pruned relatives\nLocal Time: April 1, 2017 6:38 PM\nUTC Time: April 1, 2017 11:38 PM\nFrom: bfd at cock.lu\nTo: praxeology_guy <praxeology_guy at protonmail.com>, Bitcoin Protocol Discussion <bitcoin-dev at lists.linuxfoundation.org>\n\nIf a wallet is unaware of spends of its own coins (ie, transactions\nwere made it can't have known about), there's probably bigger problems\ngoing on. You might enjoy the topic on this mailing list on committed\nbloom filters however, as this solves a similar issue without needing\nan ever-growing list of hundreds of millions of spent outputs.\n\nOn 2017-04-02 06:04, praxeology_guy via bitcoin-dev wrote:\n> Bitcoin nodes could also keep a spentness status list, where each bit\n> in the spentness status list corresponds to whether a txo in the MMR\n> is spent. This could make it so that disconnected wallets didn't have\n> to guess the pruned relative spentness status when it reconnects to\n> the network... and help prevent DoS attacks.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170401/050e506f/attachment-0001.html>"
            },
            {
                "author": "Bram Cohen",
                "date": "2017-04-02T01:27:17",
                "message_text_only": "On Sat, Apr 1, 2017 at 6:10 PM, praxeology_guy via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n> With using the MMR data structure for txo commitments, its preferable that\n> wallets only keep information pertinent to their own spendable coins.  In\n> previous communication we talked about how wallets could maintain the\n> changing MMR proof for their old coins.  Yes wallets know which of their\n> own coins are spent.  But with MMR proofs wallets also need to know the\n> spentness status of close relatives in the MMR tree... in order to\n> construct a valid MMR proof that their own coin is not spent.\n>\n\nDid you read the post that I made about the TXO bitfield yesterday? That\ngives what I believe is a much better way of handling this whole issue,\nallowing wallets to keep track of nothing other than the proof of position\nof their txo, which never changes.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170401/797fce51/attachment.html>"
            },
            {
                "author": "praxeology_guy",
                "date": "2017-04-02T01:58:31",
                "message_text_only": "Bram Cohen,\n\nIn R&D: First its appropriate to explore all interesting ideas, and help each other improve their ideas. Last, when there is a deadline that needs to be met, we compare various options and decide on which to go with.\n\nI'm on the First step still.\n\nIf you really want to push me to saying it, I'm not a fan of the Patricia Tree for bitcoin txos. I think its too much work for everyone to do when other options are available. But I'm not trying to say that your design is bad or wont work... I'm just personally not interested in it at this time.\n\nCheers,\nPraxeology Guy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170401/caec5662/attachment.html>"
            },
            {
                "author": "Bram Cohen",
                "date": "2017-04-02T02:18:25",
                "message_text_only": "On Sat, Apr 1, 2017 at 6:58 PM, praxeology_guy <\npraxeology_guy at protonmail.com> wrote:\n\n> Bram Cohen,\n>\n> In R&D: First its appropriate to explore all interesting ideas, and help\n> each other improve their ideas.  Last, when there is a deadline that needs\n> to be met, we compare various options and decide on which to go with.\n>\n> I'm on the First step still.\n>\n\nIn that case you should read my txo bitfield proposal, instead of taking my\npostings yesterday as a prompt to respond to something completely unrelated.\n\n\n> If you really want to push me to saying it, I'm not a fan of the Patricia\n> Tree for bitcoin txos.  I think its too much work for everyone to do when\n> other options are available.  But I'm not trying to say that your design is\n> bad or wont work... I'm just personally not interested in it at this time.\n>\n\nMy bitfield proposal is different from the patricia trie stuff. Also your\nobjection about patricia tries being 'too much work' is nonsensical because\nthey're quite a bit simpler than MMRs.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170401/d4aaf777/attachment.html>"
            },
            {
                "author": "praxeology_guy",
                "date": "2017-04-02T03:37:38",
                "message_text_only": "Bram Cohen,\n\nMy apologies, I guess I glossed over your \"The TXO bitfield\" because by subject I thought it just had something to do with changing the txo's data structure.\n\nYes what you are proposing with \"The TXO bitfield\" is pretty much exactly the same as the MMR data structure... EXCEPT yours has the wonderful benefit of the MMR proofs not changing. Excellent idea!\n\nBasically your idea is a change in how the MMR data is modified on spend... moving it from changing the leaf nodes to changing a node closer to the root... and particularly it seems you are making such a deltaLeaveHeight = block height... which might be a different height for each block, but not that big of a deal.\n\nWhich leads me to modifying the MMR structure so that the spentness bit array is actually part of the nodes at height DLH_REQUIRED's hash... and that the leaf nodes don't actually get changed to empty as Peter is proposing, instead the leaf nodes stay the same. This results in the same wonderful benefit of the MMR proofs not changing, just like in your \"TXO bitfield\" proposal.\n\nI still like the MMR structure better, in the case that only utxos are added after a long delay. The delay and adding only utxos allows much fewer additions to the spentness bitfield and it's merkle tree. But if we are going to make commitments on the entire txo set instead of some policy of N blocks delayed utxos... your \"TXO bitfield\" idea looks great.\n\nSay... one bad thing about only adding delayed utxos to the MMR, as I am proposing, is that the index changes/is created when the delayed addition happens. Verses with \"txo bitfield\" or adding all txos to the MMR, the index is created when the block is first made.\n\nThank you so much for your TXO bitfield idea... and harping on me about it. I'm really excited about these designs. :) As a funny side note,I had actually considered putting the spentness bitfield in the deltaLeafHeight = DLH_REQUIRED node's merkle hash... but quickly dismissed it since we were already were replacing the leafs w/ empties (which is a duplication of information). Your idea was the inspiration to switch from changing to empties to changing the spentness bits.\n\nHumbled, Thanks,\n\nPraxeology Guy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170401/ca3741fc/attachment.html>"
            },
            {
                "author": "praxeology_guy",
                "date": "2017-04-02T20:43:40",
                "message_text_only": "TL;DR: using spentness bits scales linearly... vs swapping digest leafs with empties can scale with logorithmically increasing storage requirements. So if you are using a 32 byte hash and spentness bits, you are pretty much limited to only being able to prune 8 to 12 layers. This corresponds to an MMR proof length of 512 to 768 bytes.\n\n===============\n\nAfter doing some calculations:\nGiven that the spentness bit fields are 1 bit per txo, and markle hash size is 32 bytes... When using spentness bits in the merkle tree hashes instead of setting leaf nodes to empty, increasing the DLH_REQUIRED beyond 8 quickly has diminishing returns.\n\nWith DLH_REQUIRED = 8, the spentness bits take up 30% of the data structure's space. MMR proof size = 512 bytes.\n\nWith DLH_REQUIRED = 12, the spentness bits take up 87% of the data structure's space. MMR proof size = 768 bytes.\n\nUsing stats from blockchain.info (I know not very reliable)... I figure there would be about 12E6 delayed utxo only txos added to an MMR per year w/ the current block size. 200E6 txo/year added to the MMR per year if spent txos are added too.\n\nUsing DLH_REQUIRED = 12 (or 8)\nWith 12E6 txo/year added to the MMR, the MMR grows by about 1.5MB (or 5MB) per year.\nWith 200E6 txo/year added to the MMR, the MMR grows by about 27.5MB (or 80MB) per year.\n\nSince the spentness bits are not in any way compressed by the MMR tree... this puts a hard limit on the potential gains by pruning more.\n\nA growth rate of 27MB to 80MB per year for adding all txos to the MMR doesn't sound too bad. But if the block size is increased, we may soon decide we'd rather switch from using spentness bits to changing digest nodes to empty nodes. Only adding utxos at a delayed time gives more breathing room.\n\nCheers,\nPraxeology Guy\n\nP.S. This analysis also applies to Bram Cohen's \"TXO bitfield\". Instead of DLH_REQUIRED, his node with spendess bits would be at a block with about 4000 txos, which just happens to equal a DLH_REQUIRED = 12.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170402/44931f22/attachment-0001.html>"
            },
            {
                "author": "Bram Cohen",
                "date": "2017-04-03T03:13:52",
                "message_text_only": "On Sun, Apr 2, 2017 at 1:43 PM, praxeology_guy via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> TL;DR: using spentness bits scales linearly... vs swapping digest leafs\n> with empties can scale with logorithmically increasing storage\n> requirements.  So if you are using a 32 byte hash and spentness bits, you\n> are pretty much limited to only being able to prune 8 to 12 layers.  This\n> corresponds to an MMR proof length of 512 to 768 bytes.\n>\n\nYes the point of the txo bitfield is that the constant factors are so good\nthat it's entirely under control. Making the memory commitments smaller\nrequires that the proofs be kept up to date and increases CPU requirements\nand proof size. It would be entirely reasonable to make an MMRs of the\nbitfield or the insertion index data structure but they aren't needed\nimmediately if ever. For the insertion ordering structure it's reasonable\nto require full nodes to cache the top bunch of layers to make the proofs\nsmaller, but a very expedient approximation of that is to make them simply\nremember a root per block for all the insertions contained therein, and\nhave full nodes remember all of those.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170402/7d4e165b/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Guessing the spentness status of the pruned relatives",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Bram Cohen",
                "bfd at cock.lu",
                "praxeology_guy"
            ],
            "messages_count": 9,
            "total_messages_chars_count": 12550
        }
    },
    {
        "title": "[bitcoin-dev] Committed bloom filters for improved wallet performance and SPV security",
        "thread_messages": [
            {
                "author": "bfd at cock.lu",
                "date": "2017-04-01T23:49:03",
                "message_text_only": "On 2017-02-17 11:28, Chris Belcher via bitcoin-dev wrote:\n> I think this committed bloom filter idea is very good and much better\n> than bip37, but for good privacy for when bitcoin is used often still\n> requires certain behavior namely downloading blocks\n> from many different peers with new tor circuits.\n> \n> Note that I've been dealing with counting transaction subgraphs but\n> actually finding them from blocks might also be computationally\n> infeasible. Although a Bayesian approach worked very\n> well for similar transaction subgraph linking\n> [https://arxiv.org/pdf/1612.06747v3.pdf]\n> \n> It would also be interesting to analyze what information a spy can get\n> if they are missing some blocks that the wallet downloaded.\n> \n> For the long term, private and high-volume bitcoin use will be best\n> served by off-chain transactions. They will probably be a huge win just\n> because the large and public blockchain is such a non-private\n> way of doing things.\n> \n\nThank you for the analysis, this generally matches my views about the\nproperties offered by the system.\n\nI've generally developed the opinion that BIP37 is effectively unused\nby all but a very small number of wallets and services now, setting up\nsinkhole nodes in the network to monitor `filterload` commands seems\nto back that up."
            }
        ],
        "thread_summary": {
            "title": "Committed bloom filters for improved wallet performance and SPV security",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "bfd at cock.lu"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1299
        }
    },
    {
        "title": "[bitcoin-dev] High fees / centralization",
        "thread_messages": [
            {
                "author": "Staf Verhaegen",
                "date": "2017-04-02T19:45:11",
                "message_text_only": "Jared Lee Richardson via bitcoin-dev schreef op do 30-03-2017 om 19:01\n[-0700]:\n> That would be blockchain sharding.\n> \n> Would be amazing if someone could figure out how to do it trustlessly.\n> So far I'm not convinced it is possible to resolve the conflicts\n> between the shards and commit transactions between shards.\n\nI'm thinking more of a system where different nodes can agree to do part\nof the transaction processing. In that way 20000 nodes could work like\n5000 full nodes.\n\ngreets,\nStaf.\n\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 230 bytes\nDesc: This is a digitally signed message part\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170402/a393a7a5/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "High fees / centralization",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Staf Verhaegen"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 798
        }
    },
    {
        "title": "[bitcoin-dev] BIP draft: Extended block header hardfork",
        "thread_messages": [
            {
                "author": "Johnson Lau",
                "date": "2017-04-02T20:13:23",
                "message_text_only": "This is the first of a series of BIPs describing my \u201cspoonnet\u201d experimental hardfork. Recently many bitcoin businesses expressed their requirements for supporting a hardfork proposal. While it is proven to be extremely difficult to obtain community-wide consensus, spoonnet fulfills all the commonly requested features, including deterministic activation logic, strong and simple 2-way replay protection, wipe-out protection, and predictable resources use. A few more BIPs are coming to describe these features.\n\nThe activation is purely based on flag day. Since it is very difficult to measure community consensus on-chain, this may only be done off-chain, and everyone switch to the new software when the vast majority agree. This is more a social issue than a technical one.\n\nReference implementation for consensus codes could be found at: https://github.com/jl2012/bitcoin/tree/spoonnet2 . This does not include mempool, wallet, and mining support. Mempool and wallet support are more tricky due to replay attack protection.\n\nBIP: ? \nLayer: Consensus (hard fork) \nTitle: Extended block header hardfork \nAuthor: Johnson Lau <jl2012 at xbt.hk> \nComments-Summary: No comments yet. \nComments-URI: \nStatus: Draft \nType: Standards Track \nCreated: 2017-03-31 \nLicense: BSD-2-Clause\n\n\nAbstract\n\nThis BIP proposes a flexible and upgradable extended block header format thorough a hardfork.\n\nMotivation\n\nIn the current Bitcoin protocol, the block header is fixed at 80 bytes with no space reserved for additional data. The coinbase transaction becomes the only practical location for new consensus-critical data, such as those proposed by BIP100 and BIP141. Although this preserves maximal backward compatibility for full nodes, it is not ideal for light clients because the size of coinbase transaction and depth of Merkle tree are indefinite.\n\nThis BIP proposes an extended block header format with the following objectives:\n\n\t\u2022 To provide a flexible header format which is easily upgradeable with softforks.\n\t\u2022 Old light nodes following the hardfork chain if it has most proof-of-work, but not seeing any transactions.\n\t\u2022 Being compatible with the Stratum mining protocol to avoid mining machine upgrade.\n\t\u2022 Having a deterministic hardfork activation.\n\t\u2022 Being a permanent hardfork, as supporting nodes will not accept blocks mined in old rules after hardfork is activated.\n\nSpecification\n\nThe following rules are activated when the median timestamp of the past 11 blocks is equal to or greater than a to-be-determined time and after activation of BIP65.\n\n\n\t\u2022 the nVersion of the block header MUST have the most significant bit (the sign bit) signalled.\n\t\u2022 for the purpose of counting softforks proposal signalling (BIP9), the sign bit is ignored.\n\t\u2022 segregated witness MUST be enabled, if it had not been already activated through the BIP9 mechanism.\n\t\u2022 the witness of the first input of the coinbase transaction MUST have exactly one stack item (the \"extended header\"), with the following data:\n\t\t\u2022 bytes 0 to 3: nHeight MUST be equal to the height of this block (signed little endian)\n\t\t\u2022 bytes 4 to 5: MUST be exactly 0x0000\n\t\t\u2022 bytes 6 to 53: extra data with no meaning in Bitcoin protocol\n\t\t\u2022 bytes 54 to 85: hashMerkleRoot the transaction Merkle root (calculated in the same way as the original Merkle root in the block header)\n\t\t\u2022 bytes 86 to 117: hashWitnessRoot the witness Merkle root (NOT calculated in the way described in BIP141)\n\t\t\u2022 bytes 118 to 121: nTx MUST be equal to the number of transactions in this block (unsigned little endian, minimum 1)\n\t\t\u2022 bytes 122 to 129: nFees MUST be equal to the total transaction fee paid by all transactions, except the coinbase transaction, in the block (unsigned little endian)\n\t\t\u2022 bytes 130 to 137: nWeight MUST be equal to or greater than the total weight of all transactions in the block (to be described in another BIP. NOT calculated in the way described in BIP141)\n\t\t\u2022 bytes 138+ : Reserved space for future upgrades\n\t\u2022 bytes 36 to 67 in the block header, the place originally for the hashMerkleRoot is replaced by the double SHA256 hash of the extended header.\n\t\u2022 size of the extended header MUST be at least 138 bytes.\n\t\u2022 wtxid of the coinbase transaction is calculated as if the witness of its first input is empty.\n\t\u2022 the hashWitnessRoot is calculated with all wtxid as leaves, in a way similar to the hashMerkleRoot.\n\t\u2022 the OP_RETURN witness commitment rules described in BIP141 is not enforced.\n\t\u2022 The witness reserved valued described in BIP141 is removed from the protocol.\nA special extheader softfork is defined, with the following BIP9 parameters:\n\t\u2022 bit: 15\n\t\u2022 starttime: 0\n\t\u2022 timeout: 0xffffffff\nUntil the extheader softfork is activated, the following extra rules are enforced:\n\t\u2022 nWeight MUST be exactly equal to the total weight of all transactions in the block\n\t\u2022 size of the extended header MUST NOT be larger than 152 bytes\nActivation of the special extheader softfork is independent to the activation time of the hardfork. If the special softfork is activated before the hardfork, the aforementioned extra rules will not be enforced when the hardfork is activated. Nodes that are not aware of the new rules should consider extheader softfork as an unknown upgrade and issue warnings to users accordingly.\n\nRationale\n\nThis hardfork employs a simple flag day deployment based on the median timestamp of previous blocks. Beyond this point, supporting nodes will not accept blocks with original rules. This ensures a deterministic and permanent departure with the original rules.\n\nThe witness field of the coinbase input is used as a convenient unused space to store the extended header. For any other purposes the extended header is not considered as part of the coinbase transaction (it is removed when the wtxid is calculated) This design minimizes the changes in the peer-to-peer protocol between full nodes, as no new message type is required to transmit the extended header. However, a new protocol message is still needed for serving light nodes.\n\nCommitting to the block height allows determining the value before all parental headers are obtained.\n\nBy fixing the bytes 4 to 5 as 0x0000, in the worst case an unupgraded light node may consider the block has only one transaction with no input and output, and will not see any real transactions.\n\nThe 48 byte extra data field is reserved for miners for any purposes and designed to be compatible with the Stratum mining protocol. Miners are expected to use 4 to 16 bytes as extra nonce, and 32 to 44 bytes for merge mining. This requires a hardfork for all AuxPOW blockchains, while significantly reduces the size of AuxPOW block headers.\n\nhashMerkleRoot is relocated to the extended header, followed by hashWitnessRoot. The new structure allows hashWitnessRoot committing to the wtxid of coinbase transaction with extended header removed.\n\nCommitting to the number of transactions allows light nodes to determine the Merkle tree structure easily.\n\nCommitting to the transaction fees and block weight provides information for fees estimation.\n\nThe reserved space (16 bytes until the extheader softfork is activated) MUST NOT be used without community consensus. It should be primarily used for consensus critical commitments and network status data for light nodes. Other arbitrary data should use the extra data field in extended header or the scriptSig of the coinbase transaction.\n\nThe special extheader softfork allows future protocol upgrades to increase the size of the extended header and redefine the calculation of block weight in a backward compatible way.\n\nOther proposed hardfork changes are described in other BIPs.\n\nCompatibility\n\nThis is a hard forking change, which breaking compatibility with old full node and light node. It should not be deployed without widespread consensus.\n\nOld full nodes will consider the block header nVersion as invalid and refuse the follow the hardfork chain.\n\nDepending on the design of light nodes, they may consider the hardfork chain as the best chain if it has the most total proof-of-work. However, they will not see any transactions in the chain and cease to properly function until either upgrading to the new rules, or rejecting the new rules with the negative block header nVersion.\n\nReference implementation\n\nhttps://github.com/jl2012/bitcoin/tree/spoonnet2\n\nCopyright\n\nThis BIP is licensed under the 2-clause BSD license."
            },
            {
                "author": "Russell O'Connor",
                "date": "2017-04-02T20:39:13",
                "message_text_only": "On Sun, Apr 2, 2017 at 4:13 PM, Johnson Lau via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>         \u2022 the witness of the first input of the coinbase transaction MUST\n> have exactly one stack item (the \"extended header\"), with the following\n> data:\n>                 \u2022 bytes 0 to 3: nHeight MUST be equal to the height of\n> this block (signed little endian)\n>\n\n Someone told me a while back that it would be more natural if we move the\nnHeight from the coinbase script to the coinbase locktime.  Have you\nconsidered doing this?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170402/98da6ac9/attachment.html>"
            },
            {
                "author": "Johnson Lau",
                "date": "2017-04-03T03:36:13",
                "message_text_only": "> On 3 Apr 2017, at 04:39, Russell O'Connor <roconnor at blockstream.io> wrote:\n> \n> On Sun, Apr 2, 2017 at 4:13 PM, Johnson Lau via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n>         \u2022 the witness of the first input of the coinbase transaction MUST have exactly one stack item (the \"extended header\"), with the following data:\n>                 \u2022 bytes 0 to 3: nHeight MUST be equal to the height of this block (signed little endian)\n> \n>  Someone told me a while back that it would be more natural if we move the nHeight from the coinbase script to the coinbase locktime.  Have you considered doing this?\n\n\nYes, it\u2019d look much better. But I\u2019m thinking of a different approach: instead of using a hash of 0000\u2026.0000, we use the hash of previous block for the coinbase input. With some new SIGHASH design, this allows people to pay to a child of a particular block. This is actually implemented in my spoonnet2 branch. I\u2019ll describe it with a BIP soon\n\nHowever, what I\u2019m trying to do in the extended block header is independent to the design of coinbase tx. Here I\u2019m trying to let people knowing the height just by a header and extended header (<300 bytes), without requiring all headers in the history.\n\nAlso I forgot to post the link of the BIP: https://github.com/jl2012/bips/blob/spoonnet/bip-extheader.mediawiki <https://github.com/jl2012/bips/blob/spoonnet/bip-extheader.mediawiki>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170403/7b6c9678/attachment.html>"
            },
            {
                "author": "Tom Zander",
                "date": "2017-04-04T11:47:57",
                "message_text_only": "On Sunday, 2 April 2017 22:39:13 CEST Russell O'Connor via bitcoin-dev \nwrote:\n>  Someone told me a while back that it would be more natural if we move the\n> nHeight from the coinbase script to the coinbase locktime.  Have you\n> considered doing this?\n\nThat change would not be a consensus change and thus free to make any day.\n\n-- \nTom Zander\nBlog: https://zander.github.io\nVlog: https://vimeo.com/channels/tomscryptochannel"
            },
            {
                "author": "James Hilliard",
                "date": "2017-04-04T14:59:12",
                "message_text_only": "It is a consensus rule\nhttps://github.com/bitcoin/bips/blob/master/bip-0034.mediawiki\n\nOn Tue, Apr 4, 2017 at 6:47 AM, Tom Zander via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> On Sunday, 2 April 2017 22:39:13 CEST Russell O'Connor via bitcoin-dev\n> wrote:\n>>  Someone told me a while back that it would be more natural if we move the\n>> nHeight from the coinbase script to the coinbase locktime.  Have you\n>> considered doing this?\n>\n> That change would not be a consensus change and thus free to make any day.\n>\n> --\n> Tom Zander\n> Blog: https://zander.github.io\n> Vlog: https://vimeo.com/channels/tomscryptochannel\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Tom Zander",
                "date": "2017-04-04T15:32:47",
                "message_text_only": "Can you tell me where it is enforced?\n\nThe only place I found was here;\nhttps://github.com/bitcoin/bitcoin/blob/master/src/validation.cpp#L1793\n\nwhich doesn\u2019t enforce it, all that code does is check that the txid is \nunknown or fully spent.\nAnd since the below idea from Russel would change the txid, it would seem no \nfull client would reject this.\n\nMaybe its in a BIP, but I can\u2019t find it in the code.\n\nOn Tuesday, 4 April 2017 16:59:12 CEST James Hilliard wrote:\n> It is a consensus rule\n> https://github.com/bitcoin/bips/blob/master/bip-0034.mediawiki\n> \n> On Tue, Apr 4, 2017 at 6:47 AM, Tom Zander via bitcoin-dev\n> \n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > On Sunday, 2 April 2017 22:39:13 CEST Russell O'Connor via bitcoin-dev\n> > \n> > wrote:\n> >>  Someone told me a while back that it would be more natural if we move\n> >>  the\n> >> \n> >> nHeight from the coinbase script to the coinbase locktime.  Have you\n> >> considered doing this?\n> > \n> > That change would not be a consensus change and thus free to make any\n> > day.\n\n\n-- \nTom Zander\nBlog: https://zander.github.io\nVlog: https://vimeo.com/channels/tomscryptochannel"
            },
            {
                "author": "Greg Sanders",
                "date": "2017-04-04T15:44:40",
                "message_text_only": "That's BIP30, he linked BIP34:\nhttps://github.com/bitcoin/bitcoin/blob/master/src/validation.cpp#L3004\n\nOn Tue, Apr 4, 2017 at 11:32 AM, Tom Zander via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Can you tell me where it is enforced?\n>\n> The only place I found was here;\n> https://github.com/bitcoin/bitcoin/blob/master/src/validation.cpp#L1793\n>\n> which doesn\u2019t enforce it, all that code does is check that the txid is\n> unknown or fully spent.\n> And since the below idea from Russel would change the txid, it would seem\n> no\n> full client would reject this.\n>\n> Maybe its in a BIP, but I can\u2019t find it in the code.\n>\n> On Tuesday, 4 April 2017 16:59:12 CEST James Hilliard wrote:\n> > It is a consensus rule\n> > https://github.com/bitcoin/bips/blob/master/bip-0034.mediawiki\n> >\n> > On Tue, Apr 4, 2017 at 6:47 AM, Tom Zander via bitcoin-dev\n> >\n> > <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > > On Sunday, 2 April 2017 22:39:13 CEST Russell O'Connor via bitcoin-dev\n> > >\n> > > wrote:\n> > >>  Someone told me a while back that it would be more natural if we move\n> > >>  the\n> > >>\n> > >> nHeight from the coinbase script to the coinbase locktime.  Have you\n> > >> considered doing this?\n> > >\n> > > That change would not be a consensus change and thus free to make any\n> > > day.\n>\n>\n> --\n> Tom Zander\n> Blog: https://zander.github.io\n> Vlog: https://vimeo.com/channels/tomscryptochannel\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170404/911d38f1/attachment.html>"
            },
            {
                "author": "Jean-Paul Kogelman",
                "date": "2017-04-04T16:03:51",
                "message_text_only": "Tom,\n\nIt's clear that you have some rather large gaps in your knowledge of Bitcoin, its rules, implementation and game theory. I highly encourage you spend some time learning more about these things before continuing posting here. \n\nhttps://www.reddit.com/r/BitcoinBeginners/ is a good place to start. It's a safe place where you can ask any question you want without fear of being laughed at.\n\nKind regards,\n\n\nJean-Paul\n\n> On Apr 4, 2017, at 8:44 AM, Greg Sanders via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> That's BIP30, he linked BIP34: https://github.com/bitcoin/bitcoin/blob/master/src/validation.cpp#L3004\n> \n>> On Tue, Apr 4, 2017 at 11:32 AM, Tom Zander via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> Can you tell me where it is enforced?\n>> \n>> The only place I found was here;\n>> https://github.com/bitcoin/bitcoin/blob/master/src/validation.cpp#L1793\n>> \n>> which doesn\u2019t enforce it, all that code does is check that the txid is\n>> unknown or fully spent.\n>> And since the below idea from Russel would change the txid, it would seem no\n>> full client would reject this.\n>> \n>> Maybe its in a BIP, but I can\u2019t find it in the code.\n>> \n>> On Tuesday, 4 April 2017 16:59:12 CEST James Hilliard wrote:\n>> > It is a consensus rule\n>> > https://github.com/bitcoin/bips/blob/master/bip-0034.mediawiki\n>> >\n>> > On Tue, Apr 4, 2017 at 6:47 AM, Tom Zander via bitcoin-dev\n>> >\n>> > <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> > > On Sunday, 2 April 2017 22:39:13 CEST Russell O'Connor via bitcoin-dev\n>> > >\n>> > > wrote:\n>> > >>  Someone told me a while back that it would be more natural if we move\n>> > >>  the\n>> > >>\n>> > >> nHeight from the coinbase script to the coinbase locktime.  Have you\n>> > >> considered doing this?\n>> > >\n>> > > That change would not be a consensus change and thus free to make any\n>> > > day.\n>> \n>> \n>> --\n>> Tom Zander\n>> Blog: https://zander.github.io\n>> Vlog: https://vimeo.com/channels/tomscryptochannel\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170404/d77e03fd/attachment.html>"
            },
            {
                "author": "Tom Zander",
                "date": "2017-04-04T16:17:02",
                "message_text_only": "I notice you didn\u2019t read the actual full line :)\nIf you click on it, you\u2019ll notice at the end of the line it says;\n\n\u201cchainparams.GetConsensus().BIP34Hash\u201d\n\nso, this is about BIP34.\n\n\nOn Tuesday, 4 April 2017 17:44:40 CEST Greg Sanders wrote:\n> That's BIP30, he linked BIP34:\n> https://github.com/bitcoin/bitcoin/blob/master/src/validation.cpp#L3004\n> \n> On Tue, Apr 4, 2017 at 11:32 AM, Tom Zander via bitcoin-dev <\n> \n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > Can you tell me where it is enforced?\n> > \n> > The only place I found was here;\n> > https://github.com/bitcoin/bitcoin/blob/master/src/validation.cpp#L1793\n> > \n> > which doesn\u2019t enforce it, all that code does is check that the txid is\n> > unknown or fully spent.\n> > And since the below idea from Russel would change the txid, it would\n> > seem\n> > no\n> > full client would reject this.\n> > \n> > Maybe its in a BIP, but I can\u2019t find it in the code.\n> > \n> > On Tuesday, 4 April 2017 16:59:12 CEST James Hilliard wrote:\n> > > It is a consensus rule\n> > > https://github.com/bitcoin/bips/blob/master/bip-0034.mediawiki\n> > > \n> > > On Tue, Apr 4, 2017 at 6:47 AM, Tom Zander via bitcoin-dev\n> > > \n> > > <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > > > On Sunday, 2 April 2017 22:39:13 CEST Russell O'Connor via\n> > > > bitcoin-dev\n> > > > \n> > > > wrote:\n> > > >>  Someone told me a while back that it would be more natural if we\n> > > >>  move\n> > > >>  the\n> > > >> \n> > > >> nHeight from the coinbase script to the coinbase locktime.  Have\n> > > >> you\n> > > >> considered doing this?\n> > > > \n> > > > That change would not be a consensus change and thus free to make\n> > > > any\n> > > > day.\n\n\n-- \nTom Zander\nBlog: https://zander.github.io\nVlog: https://vimeo.com/channels/tomscryptochannel"
            }
        ],
        "thread_summary": {
            "title": "BIP draft: Extended block header hardfork",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Tom Zander",
                "Johnson Lau",
                "James Hilliard",
                "Russell O'Connor",
                "Jean-Paul Kogelman",
                "Greg Sanders"
            ],
            "messages_count": 9,
            "total_messages_chars_count": 19304
        }
    },
    {
        "title": "[bitcoin-dev] BIP proposal: Generalized version bits voting (bip-genvbvoting)",
        "thread_messages": [
            {
                "author": "Sancho Panza",
                "date": "2017-04-03T09:06:02",
                "message_text_only": "\u00a1Hola!\n\nPlease find below a proposal [resubmission] for a new informational BIP\nprovisionally named 'bip-genvbvoting'.\n\nI present it here in rough draft for your esteemed consideration and as\na basis for discussion.\n\nBest regards,\nSancho\n\n--- begin draft of bip-genvbvoting ---\n\n==Preamble==\n\nBIP: ?\nTitle: Generalized version bits voting\nAuthor: Sancho Panza <sanch0panza at protonmail.com>\nStatus: Draft\nType: Informational\nCreated: 2017-03-29\nReplaces: 9\nLicense: CC0-1.0\nGNU-All-Permissive\n\n==Abstract==\n\nThis document describes a generalized version bits voting scheme based\non and intended to replace BIP9.\n\nThe generalization consists of allowing each version bit to be treated\nindividually using a configurable percentage threshold and window size,\ninstead of the fixed 95% (mainnet) and 2016 block window specified in\nBIP9.\n\nThe state machine and governing parameters (name, bit, starttime,\ntimeout) remain as is, but additional parameters called 'threshold' and\n'windowsize' are added to the per-bit set.\n\nAs before, a set of per-chain parameters will exist for the version bits\ngoverned by BIP9.\n\n==Motivation==\n\nThe Bitcoin protocol requires a flexible consensus-finding scheme\nto ensure that it can adapt to the needs of the market (its users) and\nremain competitive as an electronic payment system.\n\nWhile BIP9 has served the community reasonably well until now, the\nauthor remarks several shortcomings in its approach:\n\n- it limits itself to backward-compatible changes, precluding its\napplicability to hard forks\n\n- a fixed 95% threshold is not flexible enough to allow for a 'spectrum\nof contentiousness' to be represented\n\n- the 95% threshold allows small minorities to veto proposed changes,\nlead to stagnation (viz. current standoffs)\n\nA more generalized implementation of voting on changes using version bits\ncan address these issues in a way that can satisfy the needs of both soft\nand hard forks, as well as represent varying degrees of contentiousness.\n\n==Specification==\n\nTo be elaborated.\n\nIt is thought that only cosmetic changes are needed to generalize from\nonly soft forks to 'soft or hard forks', and to add the additional\nper-bit parameters 'threshold' and 'windowsize'\n\nReferences to fixed values will need to be eliminated and replaced\nby respective parameters.\n\nThe design of the state machine is envisioned to remain unchanged.\n\n==Implementation==\n\nA reference implementation can be constructed after elaboration of\nthe specification.\n\n==Copyright==\n\nThis BIP is dual-licensed under the Creative Commons CC0 1.0 Universal\nand GNU All-Permissive licenses.\n\n--- end draft of bip-genvbvoting ---\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170403/fe041087/attachment.html>"
            },
            {
                "author": "Tom Zander",
                "date": "2017-04-04T11:16:08",
                "message_text_only": "On Monday, 3 April 2017 11:06:02 CEST Sancho Panza wrote:\n> ==Specification==\n> \n> To be elaborated.\n\nPlease do elaborate :)\n\nThe meat of the proposal is missing.\n \n> It is thought that only cosmetic changes are needed to generalize from\n> only soft forks to 'soft or hard forks', and to add the additional\n> per-bit parameters 'threshold' and 'windowsize'\n\nI agree that the type of forks are rather irrelevant to the voting \nmechanism. As we remember that BIP109 used a voting bit too.\n\nThe per-bit (lets call that per-proposal) parameter threshold and windowsize \nare a different matter though, based on the next paragraph you wrote;\n\n> The design of the state machine is envisioned to remain unchanged.\n\nThe entire point of BIP9 is to allow nodes that do not know about an upgrade \nto still have a functional state machine. But I don\u2019t see how you can have a \nstate machine if the two basic variables that drive it are not specified.\n\nNow, to be clear, I am a big fan of making the window size and the threshold \nmore flexible.\nBut in my opinion we would not be able to have a state machine without those \nvariables in the actual BIP because old nodes would miss the data to \ntransition to certain states.\n\nMaybe an idea; we have 30 bits. 2 currently in use (although we could reuse \nthe CSV one). Maybe we can come up with 3 default sets of properties and \nwhen a proposal starts to use bit 11 it behaves differently than if it uses \n22.\n-- \nTom Zander\nBlog: https://zander.github.io\nVlog: https://vimeo.com/channels/tomscryptochannel"
            },
            {
                "author": "Sancho Panza",
                "date": "2017-04-04T16:41:31",
                "message_text_only": "Thanks for the feedback.\nI'll post a link to more refined proposal on github once that elaboration is more complete.\nFor now I think more discussion will be very helpful.\nI think the flexibility around the tallying window size will take the most careful consideration, so that a user of this proposal can retain full compatibility with BIP9 for a certain versionbit if (s)he wishes.\n\nThe entire point of BIP9 is to allow nodes that do not know about an upgrade\nto still have a functional state machine. But I don\u2019t see how you can have a\nstate machine if the two basic variables that drive it are not specified.\n\nWhat I mean by the state machine remaining essentially unchanged is that its basic design (states and transitions) would remain the same.\nBut the parameters that decide those transitions would be unique per bit.\nI think you misunderstood me if you think there will be strictly one singular state machine.\n\nInstead nodes would effectively be running a state machine instance for each signaling bit - with each state machine possibly (but not necessarily!) configured differently.\n\nAn initial implementation might provide this all in compiled code.\nA slightly more sophisticated implementation would push the signaling configuration mostly into an external configuration file which could adhere to a fixed format and could easily be adapted and shared between implementations.\n\nBut in my opinion we would not be able to have a state machine without those\nvariables in the actual BIP because old nodes would miss the data to\ntransition to certain states.\n\nAs I see it, this is the same situation we are in now with old nodes - they see that there is some action on unknown bits, but they can do nothing more than warn their operators about this.\nThis proposal does not fundamentally change that situation.\n\nMaybe an idea; we have 30 bits. 2 currently in use (although we could reuse\nthe CSV one). Maybe we can come up with 3 default sets of properties and\nwhen a proposal starts to use bit 11 it behaves differently than if it uses\n22.\n\nOne could place conventions on how certain bit ranges are used, but I don't much see the point of the BIP doing this, although it could suggest examples.\n\nI would prefer if the BIP's reference implementation provides strict BIP9 compatibility in that how it configures the bits (i.e. all with 2016 block windows evaluated in strict synchronicity with BIP9, and default 95% thresholds).\nOf course in reality most bits are unused today.\nSomeone wishing to use a bit for a feature deployment would announce so publicly (e.g. in a BIP) and release an implementation which is suitably configured.\nOthers wishing to provide compatibility with that feature would adjust their code and bip-genvbvoting configuration files accordingly.\n\nSancho\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170404/9f226b05/attachment-0001.html>"
            },
            {
                "author": "Sancho Panza",
                "date": "2017-04-04T16:49:58",
                "message_text_only": "[Apologies, reposting this in an attempt to improve on the botched formatting of previous reply. I am still getting used to the limitations of this mail service.]\n\nThanks for the feedback.\nI'll post a link to more refined proposal on github once that elaboration is more complete.\nFor now I think more discussion will be very helpful.\nI think the flexibility around the tallying window size will take the most careful consideration, so that a user of this proposal can retain full compatibility with BIP9 for a certain versionbit if (s)he wishes.\n\n> The entire point of BIP9 is to allow nodes that do not know about an upgrade\n> to still have a functional state machine. But I don\u2019t see how you can have a\n> state machine if the two basic variables that drive it are not specified.\n\nWhat I mean by the state machine remaining essentially unchanged is that its basic design (states and transitions) would remain the same.\nBut the parameters that decide those transitions would be unique per bit.\nI think you misunderstood me if you think there will be strictly one singular state machine.\n\nInstead nodes would effectively be running a state machine instance for each signaling bit - with each state machine possibly (but not necessarily!) configured differently.\n\nAn initial implementation might provide this all in compiled code.\nA slightly more sophisticated implementation would push the signaling configuration mostly into an external configuration file which could adhere to a fixed format and could easily be adapted and shared between implementations.\n\n> But in my opinion we would not be able to have a state machine without those\n> variables in the actual BIP because old nodes would miss the data to\n> transition to certain states.\n\nAs I see it, this is the same situation we are in now with old nodes - they see that there is some action on unknown bits, but they can do nothing more than warn their operators about this.\nThis proposal does not fundamentally change that situation.\n\n> Maybe an idea; we have 30 bits. 2 currently in use (although we could reuse\n> the CSV one). Maybe we can come up with 3 default sets of properties and\n> when a proposal starts to use bit 11 it behaves differently than if it uses\n> 22.\n\nOne could place conventions on how certain bit ranges are used, but I don't much see the point of the BIP doing this, although it could suggest examples.\n\nI would prefer if the BIP's reference implementation provides strict BIP9 compatibility in that how it configures the bits (i.e. all with 2016 block windows evaluated in strict synchronicity with BIP9, and default 95% thresholds).\nOf course in reality most bits are unused today.\nSomeone wishing to use a bit for a feature deployment would announce so publicly (e.g. in a BIP) and release an implementation which is suitably configured.\nOthers wishing to provide compatibility with that feature would adjust their code and bip-genvbvoting configuration files accordingly.\n\nSancho\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170404/09cb963b/attachment.html>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2017-04-04T18:01:51",
                "message_text_only": "On Monday, April 03, 2017 9:06:02 AM Sancho Panza via bitcoin-dev wrote:\n> While BIP9 has served the community reasonably well until now, the\n> author remarks several shortcomings in its approach:\n> \n> - it limits itself to backward-compatible changes, precluding its\n> applicability to hard forks\n\nBIP 9 doesn't limit itself, merely acknowledges the *inherent* nature of it \nnot being applicable to hardforks. BIP 9 provides a mechanism for having \nminers coordinate softforks because they can make the upgrade process smoother \nthis way. But the same is not true of hardforks: miners are essentially \nirrelevant to them, and cannot make the process any smoother. Therefore, BIP 9 \nand any miner signalling in general is not very useful for deploying these.\n\n> - a fixed 95% threshold is not flexible enough to allow for a 'spectrum\n> of contentiousness' to be represented\n> \n> - the 95% threshold allows small minorities to veto proposed changes,\n> lead to stagnation (viz. current standoffs)\n\nSoftforks are not required to use BIP 9, and even if they do, they are not \nrequired to use the recommended thresholds.\n\nBasically, the problems you're trying to solve don't exist...\n\nLuke"
            },
            {
                "author": "Sancho Panza",
                "date": "2017-04-04T19:28:38",
                "message_text_only": "> BIP 9 doesn't limit itself, merely acknowledges the *inherent* nature of it\n> not being applicable to hardforks. BIP 9 provides a mechanism for having\n> miners coordinate softforks because they can make the upgrade process smoother\n> this way. But the same is not true of hardforks: miners are essentially\n> irrelevant to them, and cannot make the process any smoother.\n\nI accept that BIP9 is inherently concerned only with softforks, as it is explicit about this in every instance.\nHowever, I see no fundamental distinction between the 'royal privilege' assigned to miners w.r.t. softfork activation and the role they would play in properly coordinated hardforks.\nIn either case, the majority of miners would hopefully want to wait for the right conditions to create the fork block, whether that block be the first one to contain SegWit transactions or the first one to be larger than 1MB (to give two current examples).\nThe advance coordination with the rest of the users in the system seems important in either case.\n\nThis is a big motivator for this BIP: the versionbits can be used as a coordination mechanism for hardforks just as much as softforks.\nWith the added flexibility offered by this BIP, miners could use these bits to make the process smoother for softforks as well as hardforks.\n\nFor example (this is an idea I did not write in the initial BIP draft), the period for which a fork on a particular bit remains LOCKED_IN could be made customizable too, instead of one single retargeting period.\nThis would allow fork implementors to specify a longer adaptation period suitable to the impacts of the feature they are planning to deploy.\n\n> Therefore, BIP 9 and any miner signalling in general is not very useful\n> for deploying these.\n\nI think BIP9 is a very useful tool that allows a decent determination of how much of the hashing power supports a particular fork proposal.\n\nMy view is that both soft and hard forks without support from the majority of miners place themselves at high risk.\nIn general every soft fork can result in a counter hardfork by those who are not aligned with its objectives, just like every hardfork can result in a counter softfork for the same reason by those opposed to it.\n\nIt seems to me that this somewhat balances out the (dis)advantages and effectively puts these fork types on a similar footing.\nThis is a rationale for generalizing the signaling mechanism introduced by BIP9.\n\nIn practice, developers will still need to choose whether their feature is best deployed by softfork or hardfork. This proposal affords them that choice, and does not propose any arbitrary conditions (e.g. a predefined split of the versionbits range into particular categories of forks or activation levels).\n\n> Softforks are not required to use BIP 9, and even if they do, they are not\n> required to use the recommended thresholds.\n\nThis is true, but introducing more flexibility into the signaling framework of BIP9 means it will be more useful for further developments - including a potential hardfork which was on the Core roadmap to accomodate certain wishlist items that cannot easily be addressed by softforks.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170404/07c94625/attachment-0001.html>"
            },
            {
                "author": "Tom Zander",
                "date": "2017-04-05T10:08:51",
                "message_text_only": "On Tuesday, 4 April 2017 20:01:51 CEST Luke Dashjr via bitcoin-dev wrote:\n> BIP 9 provides a mechanism for having\n> miners coordinate softforks because they can make the upgrade process\n> smoother this way. But the same is not true of hardforks: miners are\n> essentially irrelevant to them, and cannot make the process any smoother.\n\nCan you explain how miners are irrelevant if the upgrade is not a soft fork?\n\n-- \nTom Zander\nBlog: https://zander.github.io\nVlog: https://vimeo.com/channels/tomscryptochannel"
            },
            {
                "author": "Thomas Kerin",
                "date": "2017-04-05T14:09:59",
                "message_text_only": "A schism is just that: miners can't ameliorate a HF transition in the way they can censor transactions without permission. This is how miners became a convenient way to activate soft-forks. \n\nSo while BIP9 can indicate the later censorship (a soft fork) in a way that nodes can follow (or not) a hardfork always requires nodes to upgrade to the version increasing the degrees of freedom of the system. \n\nSignaling is less useful here: the change is not opt-in and will require coordination; and the continuation of the chain thereafter depends on people actually running the hard-fork code, not just being aware there is something happening.\n\n\nOn 04/05/2017 12:08 PM, Tom Zander via bitcoin-dev wrote:\n\nOn Tuesday, 4 April 2017 20:01:51 CEST Luke Dashjr via bitcoin-dev wrote: \n\nBIP 9 provides a mechanism for having miners coordinate softforks because they can make the upgrade process smoother this way. But the same is not true of hardforks: miners are essentially irrelevant to them, and cannot make the process any smoother. \n\nCan you explain how miners are irrelevant if the upgrade is not a soft fork? \n\n\n\n-- \nSent from my Android device with K-9 Mail. Please excuse my brevity.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170405/a0b4d424/attachment.html>"
            },
            {
                "author": "Sancho Panza",
                "date": "2017-04-08T21:58:43",
                "message_text_only": "Thomas,\n\n> the change is not opt-in and will require coordination; and the continuation of the chain thereafter depends on people actually running the hard-fork code, not just being aware there is something happening.\n\nThis situation applies to soft forks as well.\n\n- if you wish your software to validate correctly, it is not opt-in\n- it requires coordination to activate without much orphan risk to miners (hence BIP9). Witness the long preparation time ahead of SegWit deployment for wallet providers, miners etc. to coordinate to support it on their systems\n- after activation, it depends on people running it (most notably miners, otherwise the soft-fork is no longer enforced leading to a hard fork)\n- awareness alone does not ensure full validation capability is retained during a soft fork\n\nTherefore, these differences seem insignificant enough to merit treating soft and hard forks equal in terms of the coordination features afforded through the versionbits.\n\nSancho\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170408/973f66ea/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "BIP proposal: Generalized version bits voting (bip-genvbvoting)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Thomas Kerin",
                "Luke Dashjr",
                "Tom Zander",
                "Sancho Panza"
            ],
            "messages_count": 9,
            "total_messages_chars_count": 18022
        }
    },
    {
        "title": "[bitcoin-dev] Extension block proposal by Jeffrey et al",
        "thread_messages": [
            {
                "author": "Luke Dashjr",
                "date": "2017-04-04T18:03:56",
                "message_text_only": "Recently there has been some discussion of an apparent work-in-progress \nextension block proposal by Christopher Jeffrey, Joseph Poon, Fedor Indutny, \nand Steven Pair. Since this hasn't been formally posted on the ML yet, perhaps \nit is still in pre-draft stages and not quite ready for review, but in light \nof public interest, I think it is appropriate to open it to discussion, and \ntoward this end, I have reviewed the current revision.\n\nFor reference, the WIP proposal itself is here:\n    https://github.com/tothemoon-org/extension-blocks\n\n==Overall analysis & comparison==\n\nThis is a relatively complicated proposal, creating a lot of additional \ntechnical debt and complexity in comparison to both BIP 141 and hardforks. It \noffers no actual benefits beyond BIP 141 or hardforks, so seems irrational to \nconsider at face value. In fact, it fits much better the inaccurate criticisms \nmade by segwit detractors against BIP 141.\n\nThat being said, this proposal is very interesting in construction and is for \nthe most part technically sound. While ill-fit to merely making blocks larger, \nit may be an ideal fit for fundamentally different block designs such as \nRootstock and MimbleWimble in absence of decentralised non-integrated \nsidechains (extension blocks are fundamentally sidechains tied into Bitcoin \ndirectly).\n\n==Fundamental problem==\n\nExtension blocks are a risk of creating two classes of \"full nodes\": those \nwhich verify the full block (and are therefore truly full nodes), and those \nwhich only verify the \"base\" block. However, because the extension is \nconsensus-critical, the latter are in fact not full nodes at all, and are left \ninsecure like pseudo-SPV (not even real SPV) nodes. This technical nature is \nof course true of a softfork as well, but softforks are intentionally designed \nsuch that all nodes are capable of trivially upgrading, and there is no \nexpectation for anyone to run with pre-softfork rules.\n\nIn general, hardforks can provide the same benefits of an extension block, but \nwithout the false expectation and pointless complexity.\n\n==Other problems & questions==\n\n> These outpoints may not be spent inside the mempool (they must be redeemed \nfrom the next resolution txid in reality).\n\nThis breaks the ability to spend unconfirmed funds in the same block (as is \nrequired for CPFP).\n\nThe extension block's transaction count is not cryptographically committed-to \nanywhere. (This is an outstanding bug in Bitcoin today, but impractical to \nexploit in practice; however, exploiting it in an extension block may not be \nas impractical, and it should be fixed given the opportunity.)\n\n> The merkle root is to be calculated as a merkle tree with all extension \nblock txids and wtxids as the leaves.\n\nThis needs to elaborate how the merkle tree is constructed. Are all the txids \nfollowed by all the wtxids (tx hashes)? Are they alternated? Are txid and \nwtxid trees built independently and merged at the tip?\n\n> Output script code aside from witness programs, p2pkh or p2sh is considered \ninvalid in extension blocks.\n\nWhy? This prevents extblock users from sending to bare multisig or other \nvarious possible destinations. (While static address forms do not exist for \nother types, they can all be used by the payment protocol.)\n\nAdditionally, this forbids datacarrier (OP_RETURN), and forces spam to create \nunprovably-unspendable UTXOs. Is that intentional?\n\n> The maximum extension size should be intentionally high.\n\nThis has the same \"attacks can do more damage than ordinary benefit\" issue as \nBIP141, but even more extreme since it is planned to be used for future size \nincreases.\n\n> Witness key hash v0 shall be worth 1 point, multiplied by a factor of 8.\n\nWhat is a \"point\"? What does it mean multiplied by a factor of 8? Why not just \nsay \"8 points\"?\n\n> Witness script hash v0 shall be worth the number of accurately counted \nsigops in the redeem script, multiplied by a factor of 8.\n\nPlease define \"accurately counted\" here. Is this using BIP16 static counting, \nor accurately counting sigops during execution?\n\n> To reduce the chance of having redeem scripts which simply allow for garbage \ndata in the witness vector, every 73 bytes in the serialized witness vector is \nworth 1 additional point.\n\nIs the size rounded up or down? If down, 72-byte scripts will carry 0 \npoints...)\n\n==Trivial & process==\n\nBIPs must be in MediaWiki format, not Markdown. They should be submitted for \ndiscussion to the bitcoin-dev mailing list, not social media and news.\n\n> Layer: Consensus (soft-fork)\n\nExtension blocks are more of a hard-fork IMO.\n\n> License: Public Domain\n\nBIPs may not be \"public domain\" due to non-recognition in some jurisdictions. \nCan you agree on one or more of these? \nhttps://github.com/bitcoin/bips/blob/master/bip-0002.mediawiki#Recommended_licenses\n\n> ## Abstract\n> \n> This specification defines a method of increasing bitcoin transaction \nthroughput without altering any existing consensus rules.\n\nThis is inaccurate. Even softforks alter consensus rules.\n\n> ## Motivation\n> \n> Bitcoin retargetting ensures that the time in between mined blocks will be \nroughly 10 minutes. It is not possible to change this rule. There has been \ngreat debate regarding other ways of increasing transaction throughput, with \nno proposed consensus-layer solutions that have proven themselves to be\nparticularly safe.\n\nBlock time seems entirely unrelated to this spec. Motivation is unclear.\n\n> Extension blocks leverage several features of BIP141, BIP143, and BIP144 for \ntransaction opt-in, serialization, verification, and network services, and as \nsuch, extension block activation entails BIP141 activation.\n\nAs stated in the next paragraph, the rules in BIP 141 are fundamentally \nincompatible with this one, so saying BIP 141 is activated is confusingly \nincorrect.\n\n> This specification should be considered an extension and modification to \nthese BIPs. Extension blocks are _not_ compatible with BIP141 in its current \nform, and will require a few minor additional rules.\n\nExtension blocks should be compatible with BIP 141, there doesn\u2019t appear to be \nany justification for not making them compatible.\n\n> This specification prescribes a way of fooling non-upgraded nodes into \nbelieving the existing UTXO set is still behaving as they would expect.\n\nThe UTXO set behaves fundamentally different to old nodes with this proposal, \nalbeit in a mostly compatible manner.\n\n> Note that canonical blocks containing entering outputs MUST contain an \nextension block commitment (all zeroes if nothing is present in the extension \nblock).\n\nPlease explain why in Rationale.\n\n> Coinbase outputs MUST NOT contain witness programs, as they cannot be \nsweeped by the resolution transaction due to previously existing consensus \nrules.\n\nSeems like an annoying technical debt. I wonder if it can be avoided.\n\n> The genesis resolution transaction MAY also include a 1-100 byte pushdata in \nthe first input script, allowing the miner of the genesis resolution to add a \nspecial message. The pushdata MUST be castable to a true boolean.\n\nWhy? Unlike the coinbase, this seems to create additional technical debt with \nno apparent purpose. Better to just have a consensus rule every input must be \nnull.\n\n> The resolution transaction's version MUST be set to the uint32 max (`2^32 - \n1`).\n\nTransaction versions are signed, so I assume this is actually simply -1. \n(While signed transaction versions seemed silly to me, using it for special \ncases like this actually makes sense.)\n\n> ### Exiting the extension block\n\nShould specify that spending such an exit must use the resolution txid, not \nthe extblock's txid.\n\n> On the policy layer, transaction fees may be calculated by transaction cost \nas well as additional size/legacy-sigops added to the canonical block due to \nentering or exiting outputs.\n\nBIPs should not specify policy at all. Perhaps prefix \"For the avoidance of \ndoubt:\" to be clear that miners may perform any fee logic they like.\n\n> Transactions within the extended transaction vector MAY include a witness \nvector using BIP141 transaction serialization.\n\nSince extblock transactions are all required to be segwit, why wouldn't this \nbe mandatory?\n\n> - BIP141's nested P2SH feature is no longer available, and no longer a \nconsensus rule.\n\nNote this makes adoption slower: wallets cannot use the extblock until the \neconomy has updated to support segwit-native addresses.\n\n> To reduce the chance of having redeem scripts which simply allow for garbage \ndata in the witness vector, every 73 bytes in the serialized witness vector is \nworth 1 additional point.\n\nPlease explain why 73 bytes in Rationale.\n\n> This leaves room for 7 future soft-fork upgrades to relax DoS limits.\n\nHow so? Please explain.\n\n> A consensus dust threshold is now enforced within the extension block.\n\nWhy?\n\n> If the second highest transaction version bit (30th bit) is set to to `1` \nwithin an extension block transaction, an extra 700-bytes is reserved on the \ntransaction space used up in the block.\n\nWhy wouldn't users set this on all transactions?\n\n> `default_witness_commitment` has been renamed to \n`default_extension_commitment` and includes the extension block commitment \nscript.\n\n`default_witness_commitment` was never part of the GBT spec. At least describe \nwhat this new key is.\n\n> - Deployment name: `extblk` (appears as `!extblk` in GBT).\n\nShould be just `extblk` if backward compatibility is supported (and `!extblk` \nwhen not).\n\n> The \"deactivation\" deployment's start time...\n\nWhat about timeout? None? To continue the extension block, must it be \ndeactivated and reactivated in parallel?"
            },
            {
                "author": "Johnson Lau",
                "date": "2017-04-04T18:35:01",
                "message_text_only": "I feel particularly disappointed that while this BIP is 80% similar to my proposal made 2 months ago ( https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013490.html ), Matt Corallo was only the person replied me. Also, this BIP seems ignored the txid malleability of the resolution tx, as my major technical critique of xblock design.\n\nBut anyway, here I\u2019m only making comments on the design. As I said in my earlier post, I consider this more as an academic topic than something really ready for production use.\n\n> This specification defines a method of increasing bitcoin transaction throughput without altering any existing consensus rules.\n\nSoftforks by definition tighten consensus rules\n\n> There has been great debate regarding other ways of increasing transaction throughput, with no proposed consensus-layer solutions that have proven themselves to be particularly safe.\n\nso the authors don\u2019t consider segwit as a consensus-layer solution to increase transaction throughput, or not think segwit is safe? But logically speaking if segwit is not safe, this BIP could only be worse. OTOH, segwit also obviously increases tx throughput, although it may not be as much as some people wish to have.\n\n> This specification refines many of Lau's ideas, and offers a much simpler method of tackling the value transfer issue, which, in Lau's proposal, was solved with consensus-layer UTXO selection.\n\nThe 2013 one is outdated. As the authors are not quoting it, not sure if they read my January proposal\n\n>  extension block activation entails BIP141 activation.\n\nI think extension block in the proposed form actually breaks BIP141. It may say it activates segregated witness as a general idea, but not a specific proposal like BIP141\n\n> The merkle root is to be calculated as a merkle tree with all extension block txids and wtxids as the leaves.\n\nIt needs to be more specific here. How are they exactly arranged? I suggest it uses a root of all txids, and a root of all wtxids, and combine them as the commitment. The reason is to allow people to prune the witness data, yet still able to serve the pruned tx to light wallets. If it makes txid and wtxid as pairs, after witness pruning it still needs to store all the wtxids or it can\u2019t reconstruct the tree\n\n> Outputs signal to exit the extension block if the contained script is either a minimally encoded P2PKH or P2SH script.\n\nThis hits the biggest question I asked in my January post: do you want to allow direct exit payment to legacy addresses? As a block reorg will almost guarantee changing txid of the resolution tx, that will permanently invalidate all the child txs based on the resolution tx. This is a significant change to the current tx model. To fix this, you need to make exit outputs unspendable for up to 100 blocks. Doing this, however, will make legacy wallet users very confused as they do not anticipate funding being locked up for a long period of time. So you can\u2019t let the money sent back to a legacy address directly, but sent to a new format address that only recognized by new wallet, which understands the lock up requirement. This way, however, introduces friction and some fungibility issues, and I\u2019d expect people using cross chain atomic swap to exchange bitcoin and xbitcoin\n\nTo summarise, my questions are:\n1. Is it acceptable to have massive txid malleability and transaction chain invalidation for every natural happening reorg?  Yes: the current spec is ok; No: next question (I\u2019d say no)\n2. Is locking up exit outputs the best way to deal with the problem? (I tried really hard to find a better solution but failed)\n3. How long the lock-up period should be? Answer could be anywhere from 1 to 100\n4. With a lock-up period, should it allow direct exit to legacy address? (I think it\u2019s ok if the lock-up is short, like 1-2 block. But is that safe enough?)\n5. Due to the fungibility issues, it may need a new name for the tokens in the ext-block\n\n> Verification of transactions within the extension block shall enforce all currently deployed softforks, along with an extra BIP141-like ruleset.\n\nI suggest to only allow push-only and OP_RETURN scriptPubKey in xblock. Especially, you don\u2019t want to replicate the sighash bug to xblock. Also, requires scriptSig to be always empty\n\n> This leaves room for 7 future soft-fork upgrades to relax DoS limits.\n\nWhy 7? There are 16 unused witness program versions\n\n> Witness script hash v0 shall be worth the number of accurately counted sigops in the redeem script, multiplied by a factor of 8.\n\nThere is a flaw here: witness script with no sigop will be counted as 0 and have a lot free space\n\n> every 73 bytes in the serialized witness vector is worth 1 additional point.\n\nso 72 bytes is 1 point or 0 point? Maybe it should just scale everything up by 64 or 128, and make 1 witness byte = 1 point . So it won\u2019t provide any \u201cfree space\u201d in the block.\n\n> Currently defined witness programs (v0) are each worth 8 points. Unknown witness program outputs are worth 1 point. Any exiting output is always worth 8 points.\n\nI\u2019d suggest to have at least 16 points for each witness v0 output, so it will make it always more expensive to create than spend UTXO. It may even provide extra \u201cdiscount\u201d if a tx has more input than output. The overall objective is to limit the UTXO growth. The ext block should be mainly for making transactions, not store of value (I\u2019ll explain later)\n\n> Dust Threshold\n\nIn general I think it\u2019s ok, but I\u2019d suggest a higher threshold like 5000 satoshi. It may also combine the threshold with the output witness version, so unknown version may have a lower or no threshold. Alternatively, it may start with a high threshold and leave a backdoor softfork to reduce it.\n\n> Deactivation\n\nIt is a double-edged sword. While it is good for us to be able to discard an unused chain, it may create really bad user experience and people may even lose money. For example, people may have opened Lightning channels and they will find it not possible to close the channel. So you need to make sure people are not making time-locked tx for years, and require people to refresh their channel regularly. And have big red warning when the deactivation SF is locked in. Generally, xblock with deactivation should never be used as long-term storage of value.\n\n\u2014\u2014\u2014\u2014\nsome general comments:\n\n1. This BIP in current form is not compatible with BIP141. Since most nodes are already upgraded to BIP141, this BIP must not be activated unless BIP141 failed to activate. However, if the community really endorse the idea of ext block, I see no reason why we couldn\u2019t activate BIP141 first (which could be done in 2 weeks), then work together to make ext block possible. Ext block is more complicated than segwit. If it took dozens of developers a whole year to release segwit, I don\u2019t see how ext block could become ready for production with less time and efforts.\n\n2. Another reason to make this BIP compatible with BIP141 is we also need malleability fix in the main chain. As the xblock has a deactivation mechanism, it can\u2019t be used for longterm value storage.\n\n3. I think the size and cost limit of the xblock should be lower at the beginning, and increases as we find it works smoothly. It could be a predefined growth curve like BIP103, or a backdoor softfork. With the current design, it leaves a massive space for miners to fill up with non-tx garbage. Also, I\u2019d also like to see a complete SPV fraud-proof solution before the size grows bigger.\n\n\n> On 5 Apr 2017, at 02:03, Luke Dashjr via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> Recently there has been some discussion of an apparent work-in-progress \n> extension block proposal by Christopher Jeffrey, Joseph Poon, Fedor Indutny, \n> and Steven Pair. Since this hasn't been formally posted on the ML yet, perhaps \n> it is still in pre-draft stages and not quite ready for review, but in light \n> of public interest, I think it is appropriate to open it to discussion, and \n> toward this end, I have reviewed the current revision.\n> \n> For reference, the WIP proposal itself is here:\n>    https://github.com/tothemoon-org/extension-blocks\n> \n> ==Overall analysis & comparison==\n> \n> This is a relatively complicated proposal, creating a lot of additional \n> technical debt and complexity in comparison to both BIP 141 and hardforks. It \n> offers no actual benefits beyond BIP 141 or hardforks, so seems irrational to \n> consider at face value. In fact, it fits much better the inaccurate criticisms \n> made by segwit detractors against BIP 141.\n> \n> That being said, this proposal is very interesting in construction and is for \n> the most part technically sound. While ill-fit to merely making blocks larger, \n> it may be an ideal fit for fundamentally different block designs such as \n> Rootstock and MimbleWimble in absence of decentralised non-integrated \n> sidechains (extension blocks are fundamentally sidechains tied into Bitcoin \n> directly).\n> \n> ==Fundamental problem==\n> \n> Extension blocks are a risk of creating two classes of \"full nodes\": those \n> which verify the full block (and are therefore truly full nodes), and those \n> which only verify the \"base\" block. However, because the extension is \n> consensus-critical, the latter are in fact not full nodes at all, and are left \n> insecure like pseudo-SPV (not even real SPV) nodes. This technical nature is \n> of course true of a softfork as well, but softforks are intentionally designed \n> such that all nodes are capable of trivially upgrading, and there is no \n> expectation for anyone to run with pre-softfork rules.\n> \n> In general, hardforks can provide the same benefits of an extension block, but \n> without the false expectation and pointless complexity.\n> \n> ==Other problems & questions==\n> \n>> These outpoints may not be spent inside the mempool (they must be redeemed \n> from the next resolution txid in reality).\n> \n> This breaks the ability to spend unconfirmed funds in the same block (as is \n> required for CPFP).\n> \n> The extension block's transaction count is not cryptographically committed-to \n> anywhere. (This is an outstanding bug in Bitcoin today, but impractical to \n> exploit in practice; however, exploiting it in an extension block may not be \n> as impractical, and it should be fixed given the opportunity.)\n> \n>> The merkle root is to be calculated as a merkle tree with all extension \n> block txids and wtxids as the leaves.\n> \n> This needs to elaborate how the merkle tree is constructed. Are all the txids \n> followed by all the wtxids (tx hashes)? Are they alternated? Are txid and \n> wtxid trees built independently and merged at the tip?\n> \n>> Output script code aside from witness programs, p2pkh or p2sh is considered \n> invalid in extension blocks.\n> \n> Why? This prevents extblock users from sending to bare multisig or other \n> various possible destinations. (While static address forms do not exist for \n> other types, they can all be used by the payment protocol.)\n> \n> Additionally, this forbids datacarrier (OP_RETURN), and forces spam to create \n> unprovably-unspendable UTXOs. Is that intentional?\n> \n>> The maximum extension size should be intentionally high.\n> \n> This has the same \"attacks can do more damage than ordinary benefit\" issue as \n> BIP141, but even more extreme since it is planned to be used for future size \n> increases.\n> \n>> Witness key hash v0 shall be worth 1 point, multiplied by a factor of 8.\n> \n> What is a \"point\"? What does it mean multiplied by a factor of 8? Why not just \n> say \"8 points\"?\n> \n>> Witness script hash v0 shall be worth the number of accurately counted \n> sigops in the redeem script, multiplied by a factor of 8.\n> \n> Please define \"accurately counted\" here. Is this using BIP16 static counting, \n> or accurately counting sigops during execution?\n> \n>> To reduce the chance of having redeem scripts which simply allow for garbage \n> data in the witness vector, every 73 bytes in the serialized witness vector is \n> worth 1 additional point.\n> \n> Is the size rounded up or down? If down, 72-byte scripts will carry 0 \n> points...)\n> \n> ==Trivial & process==\n> \n> BIPs must be in MediaWiki format, not Markdown. They should be submitted for \n> discussion to the bitcoin-dev mailing list, not social media and news.\n> \n>> Layer: Consensus (soft-fork)\n> \n> Extension blocks are more of a hard-fork IMO.\n> \n>> License: Public Domain\n> \n> BIPs may not be \"public domain\" due to non-recognition in some jurisdictions. \n> Can you agree on one or more of these? \n> https://github.com/bitcoin/bips/blob/master/bip-0002.mediawiki#Recommended_licenses\n> \n>> ## Abstract\n>> \n>> This specification defines a method of increasing bitcoin transaction \n> throughput without altering any existing consensus rules.\n> \n> This is inaccurate. Even softforks alter consensus rules.\n> \n>> ## Motivation\n>> \n>> Bitcoin retargetting ensures that the time in between mined blocks will be \n> roughly 10 minutes. It is not possible to change this rule. There has been \n> great debate regarding other ways of increasing transaction throughput, with \n> no proposed consensus-layer solutions that have proven themselves to be\n> particularly safe.\n> \n> Block time seems entirely unrelated to this spec. Motivation is unclear.\n> \n>> Extension blocks leverage several features of BIP141, BIP143, and BIP144 for \n> transaction opt-in, serialization, verification, and network services, and as \n> such, extension block activation entails BIP141 activation.\n> \n> As stated in the next paragraph, the rules in BIP 141 are fundamentally \n> incompatible with this one, so saying BIP 141 is activated is confusingly \n> incorrect.\n> \n>> This specification should be considered an extension and modification to \n> these BIPs. Extension blocks are _not_ compatible with BIP141 in its current \n> form, and will require a few minor additional rules.\n> \n> Extension blocks should be compatible with BIP 141, there doesn\u2019t appear to be \n> any justification for not making them compatible.\n> \n>> This specification prescribes a way of fooling non-upgraded nodes into \n> believing the existing UTXO set is still behaving as they would expect.\n> \n> The UTXO set behaves fundamentally different to old nodes with this proposal, \n> albeit in a mostly compatible manner.\n> \n>> Note that canonical blocks containing entering outputs MUST contain an \n> extension block commitment (all zeroes if nothing is present in the extension \n> block).\n> \n> Please explain why in Rationale.\n> \n>> Coinbase outputs MUST NOT contain witness programs, as they cannot be \n> sweeped by the resolution transaction due to previously existing consensus \n> rules.\n> \n> Seems like an annoying technical debt. I wonder if it can be avoided.\n> \n>> The genesis resolution transaction MAY also include a 1-100 byte pushdata in \n> the first input script, allowing the miner of the genesis resolution to add a \n> special message. The pushdata MUST be castable to a true boolean.\n> \n> Why? Unlike the coinbase, this seems to create additional technical debt with \n> no apparent purpose. Better to just have a consensus rule every input must be \n> null.\n> \n>> The resolution transaction's version MUST be set to the uint32 max (`2^32 - \n> 1`).\n> \n> Transaction versions are signed, so I assume this is actually simply -1. \n> (While signed transaction versions seemed silly to me, using it for special \n> cases like this actually makes sense.)\n> \n>> ### Exiting the extension block\n> \n> Should specify that spending such an exit must use the resolution txid, not \n> the extblock's txid.\n> \n>> On the policy layer, transaction fees may be calculated by transaction cost \n> as well as additional size/legacy-sigops added to the canonical block due to \n> entering or exiting outputs.\n> \n> BIPs should not specify policy at all. Perhaps prefix \"For the avoidance of \n> doubt:\" to be clear that miners may perform any fee logic they like.\n> \n>> Transactions within the extended transaction vector MAY include a witness \n> vector using BIP141 transaction serialization.\n> \n> Since extblock transactions are all required to be segwit, why wouldn't this \n> be mandatory?\n> \n>> - BIP141's nested P2SH feature is no longer available, and no longer a \n> consensus rule.\n> \n> Note this makes adoption slower: wallets cannot use the extblock until the \n> economy has updated to support segwit-native addresses.\n> \n>> To reduce the chance of having redeem scripts which simply allow for garbage \n> data in the witness vector, every 73 bytes in the serialized witness vector is \n> worth 1 additional point.\n> \n> Please explain why 73 bytes in Rationale.\n> \n>> This leaves room for 7 future soft-fork upgrades to relax DoS limits.\n> \n> How so? Please explain.\n> \n>> A consensus dust threshold is now enforced within the extension block.\n> \n> Why?\n> \n>> If the second highest transaction version bit (30th bit) is set to to `1` \n> within an extension block transaction, an extra 700-bytes is reserved on the \n> transaction space used up in the block.\n> \n> Why wouldn't users set this on all transactions?\n> \n>> `default_witness_commitment` has been renamed to \n> `default_extension_commitment` and includes the extension block commitment \n> script.\n> \n> `default_witness_commitment` was never part of the GBT spec. At least describe \n> what this new key is.\n> \n>> - Deployment name: `extblk` (appears as `!extblk` in GBT).\n> \n> Should be just `extblk` if backward compatibility is supported (and `!extblk` \n> when not).\n> \n>> The \"deactivation\" deployment's start time...\n> \n> What about timeout? None? To continue the extension block, must it be \n> deactivated and reactivated in parallel?\n> \n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2017-04-05T14:05:37",
                "message_text_only": "Hi Y'all,\n\nThanks to luke-jr and jl2012 for publishing your analysis of the\nxblocks proposal. I'd like to also present some analysis but instead focus\non the professed LN safety enhancing scheme in the proposal. It's a bit\nunderspecified, so I've taken the liberty of extrapolating a bit to fill\nin the gaps to the point that I can analyze it.\n\nTLDR; The xblock proposal includes a sub-proposal for LN which is\nessentially a block-size decrease for each open channel within the network.\nThis decrease reserves space in blocks to allow honest parties guaranteed\nspace in the blocks to punish dishonest channel counter parties. As a result\nthe block size is permanently decreased for each channel open. Some may\nconsider this cost prohibitively high.\n\n>> If the second highest transaction version bit (30th bit) is set to to `1`\n>> within an extension block transaction, an extra 700-bytes is reserved on\n>> the transaction space used up in the block.\n\n> Why wouldn't users set this on all transactions?\n\nAs the proposal stands now, it seems that users _are_ able to unilaterally\nuse this for all their Bitcoin transactions, as there's no additional cost\nto using the smart-contract safety feature outlined in the proposal.\n\nThe new safety measures proposed near the end of this xblock proposal\ncould itself consume a dedicated document outlining the prior background,\ncontext, and implications of this new safety feature. Throughout the rest\nof this post, I'll be referring to the scheme as a Pre-Allocated\nSmart-contract Dispute arena (PASDA, chosen because it sounds kinda like\n\"pasta\", which brings me many keks). It's rather insufficiently described\nand\nunder specified as it stands in the proposal. As a result, if one doesn't\nhave the necessary prior context, it might've been skipped over entirely\nas it's difficult to extract the sub-proposal from the greater proposal. I\nthink I possess the necessary prior context required to required to\nproperly analyze the sub-proposal. As a result, I would like to illuminate\nthe readers of the ML so y'all may also be able to evaluate this\nsub-proposal independently.\n\n\n## Background\n\nFirst, some necessary background. Within LN as it exists today there is\none particularly nasty systematic risk related to blockchain availability\nin the case of a channel dispute. This risk is clearly outlined in the\noriginal white paper, and in my opinion a satisfactory solution to the\nrisks which safe guard the use of very high-value channels has yet to be\npresented.\n\n\n### Chain Spam/Censorship Attack Vector\n\nThe attack vector mentioned in the original paper is a reoccurring attack\nin systems of this nature: DoS attacks. As it stands today, if a channel\ncounterparty is able to (solely, or in collaboration with other attackers)\nprevent one from committing a transaction to the chain, they're able to\nsteal money from the honest participant in the channel. The attack\nproceeds something like this:\n\n   * Mallory opens a very large channel with me.\n   * We transfer money back and forth in the channel as normal. The nature\n     of these transfers isn't very important. The commitment balances may\n     be modified due to Mallory making multi-hop payments through my\n     channel, or possibly from Mallory directly purchasing some goods I\n     offer, paying via the channel.\n   * Let's call the current commitment number state S_i. In the lifetime\n     of the channel there may exist some state S_j (i < j) s.t Mallory's\n     balance in S_i, is larger than S_j.\n   * At this point, depending on the value of the channel's time-based\n     security parameter (T) it may be possible for Mallory to broadcast\n     state S_i (which has been revoked), and prevent me being able to\n     include by my punishment transaction (PTX) within the blockchain.\n   * If Mallory is able to incapacitate me for a period of time T, or\n     censor my transactions from the chain (either selectively or via a\n     spam attack), then at time K (K > T + B, where B is the time the\n     commitment transaction was stamped in the chain), then she'll be free\n     to walk away with her settled balance at state S_i. For the sake of\n     simplicity, we're ignoring HTLC's.\n   * Mallory's gain is the difference between the balance at state S_i and\n     S_j. Deepening on the gap between the states, my settled balance at\n     state S_i and the her balance delta, she may be able to fully recoup\n     the funds she initially place in the channel.\n\n\n### The Role of Channel Reserves as Partial Mitigation\n\nA minor mitigation to this attack that's purely commitment transaction\npolicy is to mandate that Mallory's balance in the channel never dips\nbelow some reserve value R. Otherwise, if at state S_j, Mallory has a\nsettled balance of 0 within he channel (all the money if on my side), then\nthe attack outline above can under certain conditions be _costless_ from\nher PoV. Replicate this simultaneously across the network in a synchronized\nmanner (possibly getting some help from your miner friends) and this\nbecomes a bit of a problem (to say the least).\n\nTaking this a step further another mitigation that's been proposed is to\nalso use the channel reserve to implement a _ceiling_ on the maximum size\nof _any_ in flight HTLC. Similar to the scheme above, this is meant to\neliminate the possibility of a \"costless\" attack, as if channel throughput\nis artificially constrained, then the value of pending HTLC's isn't\nenticing enough to launch a channel breach attack.\n\n\n### Analysis of Attack Feasibility/Difficulty\n\nThe difficulty of the attack is dependant on the time-denominated security\nparameter T, and the adversaries ability to collude with miners. Purely\nspamming the chain given a very larger T value may be prohibitively\nexpensive for the attacker and their profit from launching the attack\nwould need to outweigh the cost in transaction fees and idle bitcoin\nrequired to launch the attack. Considering the case of colluding with\nminers, if mining is highly centralized (as it is now), then that may be a\nmore attractive attack avenue. In a world of highly decentralized mining\n(let's say a lofty goal of no pool commanding > 5% of the hash power),\nthen the attack is much more difficult.\n\n(as an aside schemes that involve transactions committing to the inputs\nthey're spending and revealing them at a later date/block (committed\ntransactions) may address the miner censorship attack vector)\n\nDepending one's target use of channels, the individuals they open channels\nwith, the applications that run on top of the channels, the amount of\ncoins within the channel, and the choice of the time parameter T, the\nattack outline above may or may not be an issue from your PoV.  However,\nin order to realize LN's maximum potential of being able to enter a\nsmart-contract with a complete stranger on the internet trustlessly,\nwithout fearing conditions that may lead to monetary losses, the attack\nvector should be mitigated if possible.\n\nIn the words of The Architect of the Matrix (and referenced by Tadge at\nhis \"Level of LN\" talk at Scaling Bitcoin Hong Kong: \"There are levels of\nsurvival we are prepared to accept\". There exist levels of LN and usage of\nchannels, that may not consider this a dire issue.\n\nOK, with the necessary background and context laid out, I'll now analyze\nthe solution proposed within the greater xblock proposal, making a brief\ndetour to briefly described another proposed solution.\n\n### Timestop\n\nA prior proposed solution to the failure scenario described above is\nwhat's known as \"time stop\". This was proposed by gmaxwell and was briefly\ntouched upon in the original LN white paper. The mechanism of the\ntime-denominated security parameter T in today's channel construction is\nenforced using OpCheckSequenceVerify. After broadcasting a commitment\ntransaction, all outputs paying to the broadcaster of the commitment are\nencumbered with a relative time delay of T blocks, meaning they are unable\nto claim the funds until time T has elapsed. This time margin gives the\nhonest party an opportunity to broadcast their punishment transaction\niff, the broadcaster has broadcast a prior revoked state.\n\nThe idea of time stomp is to introduce a special sequence-locks block\nheight to the system. This block height would increase with each block\nalong with the regular block height _unless_ the block reaches a certain\nsustained \"high water mark\". As an example, let's assume that when 3\nblocks in row are above 75% capacity, then the sequence-lock clock stops\nticking.\n\nThe effect of this change is to morph the security risk into simply a\npostponement of the judgment within the contract. With this, DoS attacks\nsimply delay the (seemingly) inevitable punishment of the dishonest party\nwithin the contract.\n\nAside from some informal discussions and the brief section within the\noriginal white paper, many details of this proposal are left\nunderspecified. For example: how do miners signal to full nodes that the\nsequence-lock clock has stopped? What's the high water mark threshold? Can\nit go on indefinitely? Should this feature be opt-in?\n\nI think this proposal should be considered in tandem with the proposal\nwithin the xblock proposal as both have a few unanswered questions that\nneed to be further explored.\n\n## Pre-Allocated Smart-Contract Dispute Area (PASDA)\n\nAight, now to the LN enhancing proposal that's buried within the\ngreater xblock proposal. Introducing some new terminology, I've been\ncalling this a: Pre-Allocated Smart-contract Dispute Arena or (PASDA) for\nshort. In a nut shell, the key idea of the proposal is this: transactions\nthat mark the commencement of a smart contract who's security depends on\navailability of block space for disputes are able to _pre allocate_ a\nsection of the block that will _always_ be _reserved_ for dispute\ntransactions. With this, contracts is  _guaranteed_ space in blocks to\nhandle disputes in the case that the contract breaks down. As an analogy:\nwhen you enter in a contract with a contractor to build your dream\nkitchen, you _also_ go to a court and reserve a 1-hour block in their\nscheduled to handle a dispute _just in case_ one arises. In the event of a\npeaceful resolution to the contract, the space is freed up.\n\nThe description in the paper is a bit light on the details, so I'll say up\nfront that I'm extrapolating w.r.t to some mechanisms of the construction.\nHowever, I've been involved in some private conversations where the idea\nwas thrown around, so I think I have enough context to _maybe_ fill in\nsome of the gaps in the proposal.\n\nI'll now restate the proposal. Smart contract transactions set a certain\nbit in their version number. This bit indicates that they wish to\npre-allocate N bytes in _all_ further blocks _until_ the contract has been\nreserved. In the specific context of payment channels, this means that\nonce a channel is open, until it has been closed, it _decreases_ the\navailable block size for all other transactions. As this is a very\naggressive proposal I think the authors took advantage of the new design\nspace within xblocks to include something that may not be readily accepted\nas a modification to the rules of the main chain.\n\nThe concrete parameters chosen in the proposal are: each channel opening\ntransaction reserves 700-bytes within _each_ block in the chain until the\ntransaction has been closed. This pre-allocation has the following\nconstraint: a transaction can _only_ take advantage of this allocation iff\nit's spending the _first_ output of a smart-contract transaction (has a\nparticular bit in the version set). This means that only dispute\nresolution transactions can utilize this space.\n\nThe proposal references two allocations, which I've squinted very hard at\nfor half a day in an attempt to parse the rules governing them, but so far\nI've been unable to glean any further details. From my squinting, I\ninterpret that half of the allocation is reserved for spending the\nself-output of a transaction in the last 2016 blocks (two weeks) and the\nother half is dedicated to spending the first output of a commitment\ntransaction in the _same_ block.\n\nI'm unsure as to why these allocations are separate, and why they aren't\njust combined into a single allocation.\n\n### Modification to LN Today\n\nThis change would require a slight modification to LN as it's currently\ndefined today. ATM, we use BIP 69 in order the inputs and outputs of a\ntransaction. This is helpful as it lets us just send of signatures for new\nstates as both sides already know the order of the inputs and outputs.\nWith PASDA, we'd now need to omit the to-self-output (the output in my\ncommitment transaction paying to myself my settled balance) from this\nordering and _always_ make it the first output (txid:0).\n\nThe second change is that this proposal puts a ceiling on on the CSV value\nallowed by any channel. All CSV delays _must-weeks otherwise, they're\nunable to take advantage of the arena.\n\n### Modifications to Bitcoin\n\nIn order to implement this within Bitcoin, a third utxo set (regular\nblock, xblock) must be maintained by all full nodes. Alternatively, this\ncan just be a bit in the xblock utxo set. The implementation doesn't\nreally matter. Before attempting to pack transactions into a block, the\ntotal allocation within the PASDA utxo-set must be summed up, and\nsubtracted from the block size cap. Only transactions which validly spend\nfrom one of these UTXO's are able to take advantage of the new space in\nthe block.\n\n## Analysis of PASDA\n\nOK, now for some analysis. First, let's assume that transactions which\ncreate PASDA UTXO's aren't subject to any additional constraints. If so,\nthen this means that _any_ transaction can freely create PASDA UTXO's and\n_decrease_ the block size for _all_ transactions until the UTXO has been\nspent. If my interpretation is correct, then this introduces a new attack\nvector which allows _anyone_ to nearly permanently decrease the block size\nfor all-time with next to zero additional cost. If this is correct, then\nit seems that miners have _zero_ incentive to _ever_ include a transaction\nthat creates a PASDA output in their blocks as it robs them of future\nrevenue and decreases the available capacity in the system, possibly\npermanently proportionally to _each_ unspent PASDA output in the chain.\n\nAlternatively, let's say the transactions which create PASDA outputs\n_must_ pay a disproportionately high fee in order to pay up front for\ntheir consumption of the size within all future blocks. If so, then a\nquestion that arises is: How large a fee? If the fee is very large, then\nthe utilization of the smart-contract battling arena is only reserved to\nvery high valued channels who can afford very high fees. This may be\nacceptable as if you have a $5 channel, then are you really at risk at\nsuch a large scale attack on Bitcoin just to steal $5 from you? It's\nimportant to note that many attacks on LN's contract resolution\ncapabilities are also a direct attack on Bitcoin. However, in a world of\ndynamic fees, then it may be the case that the fee paid 6 months ago is\nnow a measly fee an no longer covers the costs to miners (and even the\nentire system...).\n\nFinally, here's something I thought of earlier today that possibly\nmitigates the downside from the PoV of the miners (everyone else must\nstill accept the costs of a permanent block size decrease). Let's say that\nin order to create a PASDA output fees are paid as normal. However, for\n_each_ subsequent block, the participants of the contract _must_ pay a\ntribute to miners to account for their loss in revenue due to the\nreduction in block size. Essentially, all PASDA outputs must pay _rent_\nfor their pre-allocated space. If rent isn't paid sufficiently and on-time,\nthen the pre-allocate arena space is revoked by miners. There're a few\nways to construct this payment, but I'll leave that to follow up work as I\njust want to shed some light on the PASDA and its implications.\n\n## Conclusion\n\nI've attempted to fill in some gaps for y'all w.r.t exactly what the\nsub-proposal within the greater xblock proposal consists of and some\npossible implications. I'd like to note that I've taken the liberty of\nfilling on some gaps within the sub-proposal as only a single section\nwithin the greater proposal has been allocated to it. PASDA itself could\nlikely fill up an entirely distinct propsal by itself spanning several\npages. To the authors of the proposal: if my interpretation is inaccurate\nplease correct me as I'd also like to better understand the proposal. It's\npossible that everything I've said in this (now rather long) email is\nincorrect.\n\nIf you've made it this far, thank you for taking the time out of your day\nto consider my thoughts. It's my hope that we can further analyze this\nsub-proposal in detail and discuss its construction as well as its\nimplications on smart-contracts like payment channels on top of Bitcoin.\n\nPASDA purports to address one half of the systematic risks in LN by\npossibly eliminating the DoS vector attack against LN. However, the costs\nof PASDA are very high, and possibly prohibitively so. In my opinion, the\nsecond attack vector lies in the ability of miners to arbitrarily censor\ntransactions spending a particular output. Fungibility enhancing\ntechniques such as Committed Transactions may be a viable path forward to\npatch this attack vector.\n\n-- roasbeef\n\n\nOn Tue, Apr 4, 2017 at 8:35 PM Johnson Lau via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> I feel particularly disappointed that while this BIP is 80% similar to my\n> proposal made 2 months ago (\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013490.html\n> ), Matt Corallo was only the person replied me. Also, this BIP seems\n> ignored the txid malleability of the resolution tx, as my major technical\n> critique of xblock design.\n>\n> But anyway, here I\u2019m only making comments on the design. As I said in my\n> earlier post, I consider this more as an academic topic than something\n> really ready for production use.\n>\n> > This specification defines a method of increasing bitcoin transaction\n> throughput without altering any existing consensus rules.\n>\n> Softforks by definition tighten consensus rules\n>\n> > There has been great debate regarding other ways of increasing\n> transaction throughput, with no proposed consensus-layer solutions that\n> have proven themselves to be particularly safe.\n>\n> so the authors don\u2019t consider segwit as a consensus-layer solution to\n> increase transaction throughput, or not think segwit is safe? But logically\n> speaking if segwit is not safe, this BIP could only be worse. OTOH, segwit\n> also obviously increases tx throughput, although it may not be as much as\n> some people wish to have.\n>\n> > This specification refines many of Lau's ideas, and offers a much\n> simpler method of tackling the value transfer issue, which, in Lau's\n> proposal, was solved with consensus-layer UTXO selection.\n>\n> The 2013 one is outdated. As the authors are not quoting it, not sure if\n> they read my January proposal\n>\n> >  extension block activation entails BIP141 activation.\n>\n> I think extension block in the proposed form actually breaks BIP141. It\n> may say it activates segregated witness as a general idea, but not a\n> specific proposal like BIP141\n>\n> > The merkle root is to be calculated as a merkle tree with all extension\n> block txids and wtxids as the leaves.\n>\n> It needs to be more specific here. How are they exactly arranged? I\n> suggest it uses a root of all txids, and a root of all wtxids, and combine\n> them as the commitment. The reason is to allow people to prune the witness\n> data, yet still able to serve the pruned tx to light wallets. If it makes\n> txid and wtxid as pairs, after witness pruning it still needs to store all\n> the wtxids or it can\u2019t reconstruct the tree\n>\n> > Outputs signal to exit the extension block if the contained script is\n> either a minimally encoded P2PKH or P2SH script.\n>\n> This hits the biggest question I asked in my January post: do you want to\n> allow direct exit payment to legacy addresses? As a block reorg will almost\n> guarantee changing txid of the resolution tx, that will permanently\n> invalidate all the child txs based on the resolution tx. This is a\n> significant change to the current tx model. To fix this, you need to make\n> exit outputs unspendable for up to 100 blocks. Doing this, however, will\n> make legacy wallet users very confused as they do not anticipate funding\n> being locked up for a long period of time. So you can\u2019t let the money sent\n> back to a legacy address directly, but sent to a new format address that\n> only recognized by new wallet, which understands the lock up requirement.\n> This way, however, introduces friction and some fungibility issues, and I\u2019d\n> expect people using cross chain atomic swap to exchange bitcoin and xbitcoin\n>\n> To summarise, my questions are:\n> 1. Is it acceptable to have massive txid malleability and transaction\n> chain invalidation for every natural happening reorg?  Yes: the current\n> spec is ok; No: next question (I\u2019d say no)\n> 2. Is locking up exit outputs the best way to deal with the problem? (I\n> tried really hard to find a better solution but failed)\n> 3. How long the lock-up period should be? Answer could be anywhere from 1\n> to 100\n> 4. With a lock-up period, should it allow direct exit to legacy address?\n> (I think it\u2019s ok if the lock-up is short, like 1-2 block. But is that safe\n> enough?)\n> 5. Due to the fungibility issues, it may need a new name for the tokens in\n> the ext-block\n>\n> > Verification of transactions within the extension block shall enforce\n> all currently deployed softforks, along with an extra BIP141-like ruleset.\n>\n> I suggest to only allow push-only and OP_RETURN scriptPubKey in xblock.\n> Especially, you don\u2019t want to replicate the sighash bug to xblock. Also,\n> requires scriptSig to be always empty\n>\n> > This leaves room for 7 future soft-fork upgrades to relax DoS limits.\n>\n> Why 7? There are 16 unused witness program versions\n>\n> > Witness script hash v0 shall be worth the number of accurately counted\n> sigops in the redeem script, multiplied by a factor of 8.\n>\n> There is a flaw here: witness script with no sigop will be counted as 0\n> and have a lot free space\n>\n> > every 73 bytes in the serialized witness vector is worth 1 additional\n> point.\n>\n> so 72 bytes is 1 point or 0 point? Maybe it should just scale everything\n> up by 64 or 128, and make 1 witness byte = 1 point . So it won\u2019t provide\n> any \u201cfree space\u201d in the block.\n>\n> > Currently defined witness programs (v0) are each worth 8 points. Unknown\n> witness program outputs are worth 1 point. Any exiting output is always\n> worth 8 points.\n>\n> I\u2019d suggest to have at least 16 points for each witness v0 output, so it\n> will make it always more expensive to create than spend UTXO. It may even\n> provide extra \u201cdiscount\u201d if a tx has more input than output. The overall\n> objective is to limit the UTXO growth. The ext block should be mainly for\n> making transactions, not store of value (I\u2019ll explain later)\n>\n> > Dust Threshold\n>\n> In general I think it\u2019s ok, but I\u2019d suggest a higher threshold like 5000\n> satoshi. It may also combine the threshold with the output witness version,\n> so unknown version may have a lower or no threshold. Alternatively, it may\n> start with a high threshold and leave a backdoor softfork to reduce it.\n>\n> > Deactivation\n>\n> It is a double-edged sword. While it is good for us to be able to discard\n> an unused chain, it may create really bad user experience and people may\n> even lose money. For example, people may have opened Lightning channels and\n> they will find it not possible to close the channel. So you need to make\n> sure people are not making time-locked tx for years, and require people to\n> refresh their channel regularly. And have big red warning when the\n> deactivation SF is locked in. Generally, xblock with deactivation should\n> never be used as long-term storage of value.\n>\n> \u2014\u2014\u2014\u2014\n> some general comments:\n>\n> 1. This BIP in current form is not compatible with BIP141. Since most\n> nodes are already upgraded to BIP141, this BIP must not be activated unless\n> BIP141 failed to activate. However, if the community really endorse the\n> idea of ext block, I see no reason why we couldn\u2019t activate BIP141 first\n> (which could be done in 2 weeks), then work together to make ext block\n> possible. Ext block is more complicated than segwit. If it took dozens of\n> developers a whole year to release segwit, I don\u2019t see how ext block could\n> become ready for production with less time and efforts.\n>\n> 2. Another reason to make this BIP compatible with BIP141 is we also need\n> malleability fix in the main chain. As the xblock has a deactivation\n> mechanism, it can\u2019t be used for longterm value storage.\n>\n> 3. I think the size and cost limit of the xblock should be lower at the\n> beginning, and increases as we find it works smoothly. It could be a\n> predefined growth curve like BIP103, or a backdoor softfork. With the\n> current design, it leaves a massive space for miners to fill up with non-tx\n> garbage. Also, I\u2019d also like to see a complete SPV fraud-proof solution\n> before the size grows bigger.\n>\n>\n> > On 5 Apr 2017, at 02:03, Luke Dashjr via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >\n> > Recently there has been some discussion of an apparent work-in-progress\n> > extension block proposal by Christopher Jeffrey, Joseph Poon, Fedor\n> Indutny,\n> > and Steven Pair. Since this hasn't been formally posted on the ML yet,\n> perhaps\n> > it is still in pre-draft stages and not quite ready for review, but in\n> light\n> > of public interest, I think it is appropriate to open it to discussion,\n> and\n> > toward this end, I have reviewed the current revision.\n> >\n> > For reference, the WIP proposal itself is here:\n> >    https://github.com/tothemoon-org/extension-blocks\n> >\n> > ==Overall analysis & comparison==\n> >\n> > This is a relatively complicated proposal, creating a lot of additional\n> > technical debt and complexity in comparison to both BIP 141 and\n> hardforks. It\n> > offers no actual benefits beyond BIP 141 or hardforks, so seems\n> irrational to\n> > consider at face value. In fact, it fits much better the inaccurate\n> criticisms\n> > made by segwit detractors against BIP 141.\n> >\n> > That being said, this proposal is very interesting in construction and\n> is for\n> > the most part technically sound. While ill-fit to merely making blocks\n> larger,\n> > it may be an ideal fit for fundamentally different block designs such as\n> > Rootstock and MimbleWimble in absence of decentralised non-integrated\n> > sidechains (extension blocks are fundamentally sidechains tied into\n> Bitcoin\n> > directly).\n> >\n> > ==Fundamental problem==\n> >\n> > Extension blocks are a risk of creating two classes of \"full nodes\":\n> those\n> > which verify the full block (and are therefore truly full nodes), and\n> those\n> > which only verify the \"base\" block. However, because the extension is\n> > consensus-critical, the latter are in fact not full nodes at all, and\n> are left\n> > insecure like pseudo-SPV (not even real SPV) nodes. This technical\n> nature is\n> > of course true of a softfork as well, but softforks are intentionally\n> designed\n> > such that all nodes are capable of trivially upgrading, and there is no\n> > expectation for anyone to run with pre-softfork rules.\n> >\n> > In general, hardforks can provide the same benefits of an extension\n> block, but\n> > without the false expectation and pointless complexity.\n> >\n> > ==Other problems & questions==\n> >\n> >> These outpoints may not be spent inside the mempool (they must be\n> redeemed\n> > from the next resolution txid in reality).\n> >\n> > This breaks the ability to spend unconfirmed funds in the same block (as\n> is\n> > required for CPFP).\n> >\n> > The extension block's transaction count is not cryptographically\n> committed-to\n> > anywhere. (This is an outstanding bug in Bitcoin today, but impractical\n> to\n> > exploit in practice; however, exploiting it in an extension block may\n> not be\n> > as impractical, and it should be fixed given the opportunity.)\n> >\n> >> The merkle root is to be calculated as a merkle tree with all extension\n> > block txids and wtxids as the leaves.\n> >\n> > This needs to elaborate how the merkle tree is constructed. Are all the\n> txids\n> > followed by all the wtxids (tx hashes)? Are they alternated? Are txid and\n> > wtxid trees built independently and merged at the tip?\n> >\n> >> Output script code aside from witness programs, p2pkh or p2sh is\n> considered\n> > invalid in extension blocks.\n> >\n> > Why? This prevents extblock users from sending to bare multisig or other\n> > various possible destinations. (While static address forms do not exist\n> for\n> > other types, they can all be used by the payment protocol.)\n> >\n> > Additionally, this forbids datacarrier (OP_RETURN), and forces spam to\n> create\n> > unprovably-unspendable UTXOs. Is that intentional?\n> >\n> >> The maximum extension size should be intentionally high.\n> >\n> > This has the same \"attacks can do more damage than ordinary benefit\"\n> issue as\n> > BIP141, but even more extreme since it is planned to be used for future\n> size\n> > increases.\n> >\n> >> Witness key hash v0 shall be worth 1 point, multiplied by a factor of 8.\n> >\n> > What is a \"point\"? What does it mean multiplied by a factor of 8? Why\n> not just\n> > say \"8 points\"?\n> >\n> >> Witness script hash v0 shall be worth the number of accurately counted\n> > sigops in the redeem script, multiplied by a factor of 8.\n> >\n> > Please define \"accurately counted\" here. Is this using BIP16 static\n> counting,\n> > or accurately counting sigops during execution?\n> >\n> >> To reduce the chance of having redeem scripts which simply allow for\n> garbage\n> > data in the witness vector, every 73 bytes in the serialized witness\n> vector is\n> > worth 1 additional point.\n> >\n> > Is the size rounded up or down? If down, 72-byte scripts will carry 0\n> > points...)\n> >\n> > ==Trivial & process==\n> >\n> > BIPs must be in MediaWiki format, not Markdown. They should be submitted\n> for\n> > discussion to the bitcoin-dev mailing list, not social media and news.\n> >\n> >> Layer: Consensus (soft-fork)\n> >\n> > Extension blocks are more of a hard-fork IMO.\n> >\n> >> License: Public Domain\n> >\n> > BIPs may not be \"public domain\" due to non-recognition in some\n> jurisdictions.\n> > Can you agree on one or more of these?\n> >\n> https://github.com/bitcoin/bips/blob/master/bip-0002.mediawiki#Recommended_licenses\n> >\n> >> ## Abstract\n> >>\n> >> This specification defines a method of increasing bitcoin transaction\n> > throughput without altering any existing consensus rules.\n> >\n> > This is inaccurate. Even softforks alter consensus rules.\n> >\n> >> ## Motivation\n> >>\n> >> Bitcoin retargetting ensures that the time in between mined blocks will\n> be\n> > roughly 10 minutes. It is not possible to change this rule. There has\n> been\n> > great debate regarding other ways of increasing transaction throughput,\n> with\n> > no proposed consensus-layer solutions that have proven themselves to be\n> > particularly safe.\n> >\n> > Block time seems entirely unrelated to this spec. Motivation is unclear.\n> >\n> >> Extension blocks leverage several features of BIP141, BIP143, and\n> BIP144 for\n> > transaction opt-in, serialization, verification, and network services,\n> and as\n> > such, extension block activation entails BIP141 activation.\n> >\n> > As stated in the next paragraph, the rules in BIP 141 are fundamentally\n> > incompatible with this one, so saying BIP 141 is activated is confusingly\n> > incorrect.\n> >\n> >> This specification should be considered an extension and modification to\n> > these BIPs. Extension blocks are _not_ compatible with BIP141 in its\n> current\n> > form, and will require a few minor additional rules.\n> >\n> > Extension blocks should be compatible with BIP 141, there doesn\u2019t appear\n> to be\n> > any justification for not making them compatible.\n> >\n> >> This specification prescribes a way of fooling non-upgraded nodes into\n> > believing the existing UTXO set is still behaving as they would expect.\n> >\n> > The UTXO set behaves fundamentally different to old nodes with this\n> proposal,\n> > albeit in a mostly compatible manner.\n> >\n> >> Note that canonical blocks containing entering outputs MUST contain an\n> > extension block commitment (all zeroes if nothing is present in the\n> extension\n> > block).\n> >\n> > Please explain why in Rationale.\n> >\n> >> Coinbase outputs MUST NOT contain witness programs, as they cannot be\n> > sweeped by the resolution transaction due to previously existing\n> consensus\n> > rules.\n> >\n> > Seems like an annoying technical debt. I wonder if it can be avoided.\n> >\n> >> The genesis resolution transaction MAY also include a 1-100 byte\n> pushdata in\n> > the first input script, allowing the miner of the genesis resolution to\n> add a\n> > special message. The pushdata MUST be castable to a true boolean.\n> >\n> > Why? Unlike the coinbase, this seems to create additional technical debt\n> with\n> > no apparent purpose. Better to just have a consensus rule every input\n> must be\n> > null.\n> >\n> >> The resolution transaction's version MUST be set to the uint32 max\n> (`2^32 -\n> > 1`).\n> >\n> > Transaction versions are signed, so I assume this is actually simply -1.\n> > (While signed transaction versions seemed silly to me, using it for\n> special\n> > cases like this actually makes sense.)\n> >\n> >> ### Exiting the extension block\n> >\n> > Should specify that spending such an exit must use the resolution txid,\n> not\n> > the extblock's txid.\n> >\n> >> On the policy layer, transaction fees may be calculated by transaction\n> cost\n> > as well as additional size/legacy-sigops added to the canonical block\n> due to\n> > entering or exiting outputs.\n> >\n> > BIPs should not specify policy at all. Perhaps prefix \"For the avoidance\n> of\n> > doubt:\" to be clear that miners may perform any fee logic they like.\n> >\n> >> Transactions within the extended transaction vector MAY include a\n> witness\n> > vector using BIP141 transaction serialization.\n> >\n> > Since extblock transactions are all required to be segwit, why wouldn't\n> this\n> > be mandatory?\n> >\n> >> - BIP141's nested P2SH feature is no longer available, and no longer a\n> > consensus rule.\n> >\n> > Note this makes adoption slower: wallets cannot use the extblock until\n> the\n> > economy has updated to support segwit-native addresses.\n> >\n> >> To reduce the chance of having redeem scripts which simply allow for\n> garbage\n> > data in the witness vector, every 73 bytes in the serialized witness\n> vector is\n> > worth 1 additional point.\n> >\n> > Please explain why 73 bytes in Rationale.\n> >\n> >> This leaves room for 7 future soft-fork upgrades to relax DoS limits.\n> >\n> > How so? Please explain.\n> >\n> >> A consensus dust threshold is now enforced within the extension block.\n> >\n> > Why?\n> >\n> >> If the second highest transaction version bit (30th bit) is set to to\n> `1`\n> > within an extension block transaction, an extra 700-bytes is reserved on\n> the\n> > transaction space used up in the block.\n> >\n> > Why wouldn't users set this on all transactions?\n> >\n> >> `default_witness_commitment` has been renamed to\n> > `default_extension_commitment` and includes the extension block\n> commitment\n> > script.\n> >\n> > `default_witness_commitment` was never part of the GBT spec. At least\n> describe\n> > what this new key is.\n> >\n> >> - Deployment name: `extblk` (appears as `!extblk` in GBT).\n> >\n> > Should be just `extblk` if backward compatibility is supported (and\n> `!extblk`\n> > when not).\n> >\n> >> The \"deactivation\" deployment's start time...\n> >\n> > What about timeout? None? To continue the extension block, must it be\n> > deactivated and reactivated in parallel?\n> >\n> >\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170405/8d1710e5/attachment-0001.html>"
            },
            {
                "author": "Greg Sanders",
                "date": "2017-04-05T15:37:22",
                "message_text_only": "I'd appreciate the authors chiming in, but I read the PASDA differently:\n\n1) If a transaction is mined with a certain bit set, it reserves 700 bytes\nfor that particular block.\n2) In that space, 2 transactions may happen:\na) First, a transaction penalizing the \"parent\" transaction for fraud by\nspending the funds immediately\nb) Second, a \"free rider\" transaction that penalizes fraud within a ~2 week\nwindow\n\nThis means during systematic flooding of closing transactions by\nGoldfinger, vigilant watchers of their channels can immediately punish the\nfraud in the same block using (a), and if they are unable to, need to find\nspace within two weeks in (b).\n\nThis is really in the LN weeds however, so I'll refrain from evaluating the\nefficacy of such a solution.\n\nOn Wed, Apr 5, 2017 at 10:05 AM, Olaoluwa Osuntokun via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi Y'all,\n>\n> Thanks to luke-jr and jl2012 for publishing your analysis of the\n> xblocks proposal. I'd like to also present some analysis but instead focus\n> on the professed LN safety enhancing scheme in the proposal. It's a bit\n> underspecified, so I've taken the liberty of extrapolating a bit to fill\n> in the gaps to the point that I can analyze it.\n>\n> TLDR; The xblock proposal includes a sub-proposal for LN which is\n> essentially a block-size decrease for each open channel within the network.\n> This decrease reserves space in blocks to allow honest parties guaranteed\n> space in the blocks to punish dishonest channel counter parties. As a\n> result\n> the block size is permanently decreased for each channel open. Some may\n> consider this cost prohibitively high.\n>\n> >> If the second highest transaction version bit (30th bit) is set to to\n> `1`\n> >> within an extension block transaction, an extra 700-bytes is reserved on\n> >> the transaction space used up in the block.\n>\n> > Why wouldn't users set this on all transactions?\n>\n> As the proposal stands now, it seems that users _are_ able to unilaterally\n> use this for all their Bitcoin transactions, as there's no additional cost\n> to using the smart-contract safety feature outlined in the proposal.\n>\n> The new safety measures proposed near the end of this xblock proposal\n> could itself consume a dedicated document outlining the prior background,\n> context, and implications of this new safety feature. Throughout the rest\n> of this post, I'll be referring to the scheme as a Pre-Allocated\n> Smart-contract Dispute arena (PASDA, chosen because it sounds kinda like\n> \"pasta\", which brings me many keks). It's rather insufficiently described\n> and\n> under specified as it stands in the proposal. As a result, if one doesn't\n> have the necessary prior context, it might've been skipped over entirely\n> as it's difficult to extract the sub-proposal from the greater proposal. I\n> think I possess the necessary prior context required to required to\n> properly analyze the sub-proposal. As a result, I would like to illuminate\n> the readers of the ML so y'all may also be able to evaluate this\n> sub-proposal independently.\n>\n>\n> ## Background\n>\n> First, some necessary background. Within LN as it exists today there is\n> one particularly nasty systematic risk related to blockchain availability\n> in the case of a channel dispute. This risk is clearly outlined in the\n> original white paper, and in my opinion a satisfactory solution to the\n> risks which safe guard the use of very high-value channels has yet to be\n> presented.\n>\n>\n> ### Chain Spam/Censorship Attack Vector\n>\n> The attack vector mentioned in the original paper is a reoccurring attack\n> in systems of this nature: DoS attacks. As it stands today, if a channel\n> counterparty is able to (solely, or in collaboration with other attackers)\n> prevent one from committing a transaction to the chain, they're able to\n> steal money from the honest participant in the channel. The attack\n> proceeds something like this:\n>\n>    * Mallory opens a very large channel with me.\n>    * We transfer money back and forth in the channel as normal. The nature\n>      of these transfers isn't very important. The commitment balances may\n>      be modified due to Mallory making multi-hop payments through my\n>      channel, or possibly from Mallory directly purchasing some goods I\n>      offer, paying via the channel.\n>    * Let's call the current commitment number state S_i. In the lifetime\n>      of the channel there may exist some state S_j (i < j) s.t Mallory's\n>      balance in S_i, is larger than S_j.\n>    * At this point, depending on the value of the channel's time-based\n>      security parameter (T) it may be possible for Mallory to broadcast\n>      state S_i (which has been revoked), and prevent me being able to\n>      include by my punishment transaction (PTX) within the blockchain.\n>    * If Mallory is able to incapacitate me for a period of time T, or\n>      censor my transactions from the chain (either selectively or via a\n>      spam attack), then at time K (K > T + B, where B is the time the\n>      commitment transaction was stamped in the chain), then she'll be free\n>      to walk away with her settled balance at state S_i. For the sake of\n>      simplicity, we're ignoring HTLC's.\n>    * Mallory's gain is the difference between the balance at state S_i and\n>      S_j. Deepening on the gap between the states, my settled balance at\n>      state S_i and the her balance delta, she may be able to fully recoup\n>      the funds she initially place in the channel.\n>\n>\n> ### The Role of Channel Reserves as Partial Mitigation\n>\n> A minor mitigation to this attack that's purely commitment transaction\n> policy is to mandate that Mallory's balance in the channel never dips\n> below some reserve value R. Otherwise, if at state S_j, Mallory has a\n> settled balance of 0 within he channel (all the money if on my side), then\n> the attack outline above can under certain conditions be _costless_ from\n> her PoV. Replicate this simultaneously across the network in a synchronized\n> manner (possibly getting some help from your miner friends) and this\n> becomes a bit of a problem (to say the least).\n>\n> Taking this a step further another mitigation that's been proposed is to\n> also use the channel reserve to implement a _ceiling_ on the maximum size\n> of _any_ in flight HTLC. Similar to the scheme above, this is meant to\n> eliminate the possibility of a \"costless\" attack, as if channel throughput\n> is artificially constrained, then the value of pending HTLC's isn't\n> enticing enough to launch a channel breach attack.\n>\n>\n> ### Analysis of Attack Feasibility/Difficulty\n>\n> The difficulty of the attack is dependant on the time-denominated security\n> parameter T, and the adversaries ability to collude with miners. Purely\n> spamming the chain given a very larger T value may be prohibitively\n> expensive for the attacker and their profit from launching the attack\n> would need to outweigh the cost in transaction fees and idle bitcoin\n> required to launch the attack. Considering the case of colluding with\n> miners, if mining is highly centralized (as it is now), then that may be a\n> more attractive attack avenue. In a world of highly decentralized mining\n> (let's say a lofty goal of no pool commanding > 5% of the hash power),\n> then the attack is much more difficult.\n>\n> (as an aside schemes that involve transactions committing to the inputs\n> they're spending and revealing them at a later date/block (committed\n> transactions) may address the miner censorship attack vector)\n>\n> Depending one's target use of channels, the individuals they open channels\n> with, the applications that run on top of the channels, the amount of\n> coins within the channel, and the choice of the time parameter T, the\n> attack outline above may or may not be an issue from your PoV.  However,\n> in order to realize LN's maximum potential of being able to enter a\n> smart-contract with a complete stranger on the internet trustlessly,\n> without fearing conditions that may lead to monetary losses, the attack\n> vector should be mitigated if possible.\n>\n> In the words of The Architect of the Matrix (and referenced by Tadge at\n> his \"Level of LN\" talk at Scaling Bitcoin Hong Kong: \"There are levels of\n> survival we are prepared to accept\". There exist levels of LN and usage of\n> channels, that may not consider this a dire issue.\n>\n> OK, with the necessary background and context laid out, I'll now analyze\n> the solution proposed within the greater xblock proposal, making a brief\n> detour to briefly described another proposed solution.\n>\n> ### Timestop\n>\n> A prior proposed solution to the failure scenario described above is\n> what's known as \"time stop\". This was proposed by gmaxwell and was briefly\n> touched upon in the original LN white paper. The mechanism of the\n> time-denominated security parameter T in today's channel construction is\n> enforced using OpCheckSequenceVerify. After broadcasting a commitment\n> transaction, all outputs paying to the broadcaster of the commitment are\n> encumbered with a relative time delay of T blocks, meaning they are unable\n> to claim the funds until time T has elapsed. This time margin gives the\n> honest party an opportunity to broadcast their punishment transaction\n> iff, the broadcaster has broadcast a prior revoked state.\n>\n> The idea of time stomp is to introduce a special sequence-locks block\n> height to the system. This block height would increase with each block\n> along with the regular block height _unless_ the block reaches a certain\n> sustained \"high water mark\". As an example, let's assume that when 3\n> blocks in row are above 75% capacity, then the sequence-lock clock stops\n> ticking.\n>\n> The effect of this change is to morph the security risk into simply a\n> postponement of the judgment within the contract. With this, DoS attacks\n> simply delay the (seemingly) inevitable punishment of the dishonest party\n> within the contract.\n>\n> Aside from some informal discussions and the brief section within the\n> original white paper, many details of this proposal are left\n> underspecified. For example: how do miners signal to full nodes that the\n> sequence-lock clock has stopped? What's the high water mark threshold? Can\n> it go on indefinitely? Should this feature be opt-in?\n>\n> I think this proposal should be considered in tandem with the proposal\n> within the xblock proposal as both have a few unanswered questions that\n> need to be further explored.\n>\n> ## Pre-Allocated Smart-Contract Dispute Area (PASDA)\n>\n> Aight, now to the LN enhancing proposal that's buried within the\n> greater xblock proposal. Introducing some new terminology, I've been\n> calling this a: Pre-Allocated Smart-contract Dispute Arena or (PASDA) for\n> short. In a nut shell, the key idea of the proposal is this: transactions\n> that mark the commencement of a smart contract who's security depends on\n> availability of block space for disputes are able to _pre allocate_ a\n> section of the block that will _always_ be _reserved_ for dispute\n> transactions. With this, contracts is  _guaranteed_ space in blocks to\n> handle disputes in the case that the contract breaks down. As an analogy:\n> when you enter in a contract with a contractor to build your dream\n> kitchen, you _also_ go to a court and reserve a 1-hour block in their\n> scheduled to handle a dispute _just in case_ one arises. In the event of a\n> peaceful resolution to the contract, the space is freed up.\n>\n> The description in the paper is a bit light on the details, so I'll say up\n> front that I'm extrapolating w.r.t to some mechanisms of the construction.\n> However, I've been involved in some private conversations where the idea\n> was thrown around, so I think I have enough context to _maybe_ fill in\n> some of the gaps in the proposal.\n>\n> I'll now restate the proposal. Smart contract transactions set a certain\n> bit in their version number. This bit indicates that they wish to\n> pre-allocate N bytes in _all_ further blocks _until_ the contract has been\n> reserved. In the specific context of payment channels, this means that\n> once a channel is open, until it has been closed, it _decreases_ the\n> available block size for all other transactions. As this is a very\n> aggressive proposal I think the authors took advantage of the new design\n> space within xblocks to include something that may not be readily accepted\n> as a modification to the rules of the main chain.\n>\n> The concrete parameters chosen in the proposal are: each channel opening\n> transaction reserves 700-bytes within _each_ block in the chain until the\n> transaction has been closed. This pre-allocation has the following\n> constraint: a transaction can _only_ take advantage of this allocation iff\n> it's spending the _first_ output of a smart-contract transaction (has a\n> particular bit in the version set). This means that only dispute\n> resolution transactions can utilize this space.\n>\n> The proposal references two allocations, which I've squinted very hard at\n> for half a day in an attempt to parse the rules governing them, but so far\n> I've been unable to glean any further details. From my squinting, I\n> interpret that half of the allocation is reserved for spending the\n> self-output of a transaction in the last 2016 blocks (two weeks) and the\n> other half is dedicated to spending the first output of a commitment\n> transaction in the _same_ block.\n>\n> I'm unsure as to why these allocations are separate, and why they aren't\n> just combined into a single allocation.\n>\n> ### Modification to LN Today\n>\n> This change would require a slight modification to LN as it's currently\n> defined today. ATM, we use BIP 69 in order the inputs and outputs of a\n> transaction. This is helpful as it lets us just send of signatures for new\n> states as both sides already know the order of the inputs and outputs.\n> With PASDA, we'd now need to omit the to-self-output (the output in my\n> commitment transaction paying to myself my settled balance) from this\n> ordering and _always_ make it the first output (txid:0).\n>\n> The second change is that this proposal puts a ceiling on on the CSV value\n> allowed by any channel. All CSV delays _must-weeks otherwise, they're\n> unable to take advantage of the arena.\n>\n> ### Modifications to Bitcoin\n>\n> In order to implement this within Bitcoin, a third utxo set (regular\n> block, xblock) must be maintained by all full nodes. Alternatively, this\n> can just be a bit in the xblock utxo set. The implementation doesn't\n> really matter. Before attempting to pack transactions into a block, the\n> total allocation within the PASDA utxo-set must be summed up, and\n> subtracted from the block size cap. Only transactions which validly spend\n> from one of these UTXO's are able to take advantage of the new space in\n> the block.\n>\n> ## Analysis of PASDA\n>\n> OK, now for some analysis. First, let's assume that transactions which\n> create PASDA UTXO's aren't subject to any additional constraints. If so,\n> then this means that _any_ transaction can freely create PASDA UTXO's and\n> _decrease_ the block size for _all_ transactions until the UTXO has been\n> spent. If my interpretation is correct, then this introduces a new attack\n> vector which allows _anyone_ to nearly permanently decrease the block size\n> for all-time with next to zero additional cost. If this is correct, then\n> it seems that miners have _zero_ incentive to _ever_ include a transaction\n> that creates a PASDA output in their blocks as it robs them of future\n> revenue and decreases the available capacity in the system, possibly\n> permanently proportionally to _each_ unspent PASDA output in the chain.\n>\n> Alternatively, let's say the transactions which create PASDA outputs\n> _must_ pay a disproportionately high fee in order to pay up front for\n> their consumption of the size within all future blocks. If so, then a\n> question that arises is: How large a fee? If the fee is very large, then\n> the utilization of the smart-contract battling arena is only reserved to\n> very high valued channels who can afford very high fees. This may be\n> acceptable as if you have a $5 channel, then are you really at risk at\n> such a large scale attack on Bitcoin just to steal $5 from you? It's\n> important to note that many attacks on LN's contract resolution\n> capabilities are also a direct attack on Bitcoin. However, in a world of\n> dynamic fees, then it may be the case that the fee paid 6 months ago is\n> now a measly fee an no longer covers the costs to miners (and even the\n> entire system...).\n>\n> Finally, here's something I thought of earlier today that possibly\n> mitigates the downside from the PoV of the miners (everyone else must\n> still accept the costs of a permanent block size decrease). Let's say that\n> in order to create a PASDA output fees are paid as normal. However, for\n> _each_ subsequent block, the participants of the contract _must_ pay a\n> tribute to miners to account for their loss in revenue due to the\n> reduction in block size. Essentially, all PASDA outputs must pay _rent_\n> for their pre-allocated space. If rent isn't paid sufficiently and on-time,\n> then the pre-allocate arena space is revoked by miners. There're a few\n> ways to construct this payment, but I'll leave that to follow up work as I\n> just want to shed some light on the PASDA and its implications.\n>\n> ## Conclusion\n>\n> I've attempted to fill in some gaps for y'all w.r.t exactly what the\n> sub-proposal within the greater xblock proposal consists of and some\n> possible implications. I'd like to note that I've taken the liberty of\n> filling on some gaps within the sub-proposal as only a single section\n> within the greater proposal has been allocated to it. PASDA itself could\n> likely fill up an entirely distinct propsal by itself spanning several\n> pages. To the authors of the proposal: if my interpretation is inaccurate\n> please correct me as I'd also like to better understand the proposal. It's\n> possible that everything I've said in this (now rather long) email is\n> incorrect.\n>\n> If you've made it this far, thank you for taking the time out of your day\n> to consider my thoughts. It's my hope that we can further analyze this\n> sub-proposal in detail and discuss its construction as well as its\n> implications on smart-contracts like payment channels on top of Bitcoin.\n>\n> PASDA purports to address one half of the systematic risks in LN by\n> possibly eliminating the DoS vector attack against LN. However, the costs\n> of PASDA are very high, and possibly prohibitively so. In my opinion, the\n> second attack vector lies in the ability of miners to arbitrarily censor\n> transactions spending a particular output. Fungibility enhancing\n> techniques such as Committed Transactions may be a viable path forward to\n> patch this attack vector.\n>\n> -- roasbeef\n>\n>\n> On Tue, Apr 4, 2017 at 8:35 PM Johnson Lau via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> I feel particularly disappointed that while this BIP is 80% similar to my\n>> proposal made 2 months ago ( https://lists.linuxfoundation.\n>> org/pipermail/bitcoin-dev/2017-January/013490.html ), Matt Corallo was\n>> only the person replied me. Also, this BIP seems ignored the txid\n>> malleability of the resolution tx, as my major technical critique of xblock\n>> design.\n>>\n>> But anyway, here I\u2019m only making comments on the design. As I said in my\n>> earlier post, I consider this more as an academic topic than something\n>> really ready for production use.\n>>\n>> > This specification defines a method of increasing bitcoin transaction\n>> throughput without altering any existing consensus rules.\n>>\n>> Softforks by definition tighten consensus rules\n>>\n>> > There has been great debate regarding other ways of increasing\n>> transaction throughput, with no proposed consensus-layer solutions that\n>> have proven themselves to be particularly safe.\n>>\n>> so the authors don\u2019t consider segwit as a consensus-layer solution to\n>> increase transaction throughput, or not think segwit is safe? But logically\n>> speaking if segwit is not safe, this BIP could only be worse. OTOH, segwit\n>> also obviously increases tx throughput, although it may not be as much as\n>> some people wish to have.\n>>\n>> > This specification refines many of Lau's ideas, and offers a much\n>> simpler method of tackling the value transfer issue, which, in Lau's\n>> proposal, was solved with consensus-layer UTXO selection.\n>>\n>> The 2013 one is outdated. As the authors are not quoting it, not sure if\n>> they read my January proposal\n>>\n>> >  extension block activation entails BIP141 activation.\n>>\n>> I think extension block in the proposed form actually breaks BIP141. It\n>> may say it activates segregated witness as a general idea, but not a\n>> specific proposal like BIP141\n>>\n>> > The merkle root is to be calculated as a merkle tree with all extension\n>> block txids and wtxids as the leaves.\n>>\n>> It needs to be more specific here. How are they exactly arranged? I\n>> suggest it uses a root of all txids, and a root of all wtxids, and combine\n>> them as the commitment. The reason is to allow people to prune the witness\n>> data, yet still able to serve the pruned tx to light wallets. If it makes\n>> txid and wtxid as pairs, after witness pruning it still needs to store all\n>> the wtxids or it can\u2019t reconstruct the tree\n>>\n>> > Outputs signal to exit the extension block if the contained script is\n>> either a minimally encoded P2PKH or P2SH script.\n>>\n>> This hits the biggest question I asked in my January post: do you want to\n>> allow direct exit payment to legacy addresses? As a block reorg will almost\n>> guarantee changing txid of the resolution tx, that will permanently\n>> invalidate all the child txs based on the resolution tx. This is a\n>> significant change to the current tx model. To fix this, you need to make\n>> exit outputs unspendable for up to 100 blocks. Doing this, however, will\n>> make legacy wallet users very confused as they do not anticipate funding\n>> being locked up for a long period of time. So you can\u2019t let the money sent\n>> back to a legacy address directly, but sent to a new format address that\n>> only recognized by new wallet, which understands the lock up requirement.\n>> This way, however, introduces friction and some fungibility issues, and I\u2019d\n>> expect people using cross chain atomic swap to exchange bitcoin and xbitcoin\n>>\n>> To summarise, my questions are:\n>> 1. Is it acceptable to have massive txid malleability and transaction\n>> chain invalidation for every natural happening reorg?  Yes: the current\n>> spec is ok; No: next question (I\u2019d say no)\n>> 2. Is locking up exit outputs the best way to deal with the problem? (I\n>> tried really hard to find a better solution but failed)\n>> 3. How long the lock-up period should be? Answer could be anywhere from 1\n>> to 100\n>> 4. With a lock-up period, should it allow direct exit to legacy address?\n>> (I think it\u2019s ok if the lock-up is short, like 1-2 block. But is that safe\n>> enough?)\n>> 5. Due to the fungibility issues, it may need a new name for the tokens\n>> in the ext-block\n>>\n>> > Verification of transactions within the extension block shall enforce\n>> all currently deployed softforks, along with an extra BIP141-like ruleset.\n>>\n>> I suggest to only allow push-only and OP_RETURN scriptPubKey in xblock.\n>> Especially, you don\u2019t want to replicate the sighash bug to xblock. Also,\n>> requires scriptSig to be always empty\n>>\n>> > This leaves room for 7 future soft-fork upgrades to relax DoS limits.\n>>\n>> Why 7? There are 16 unused witness program versions\n>>\n>> > Witness script hash v0 shall be worth the number of accurately counted\n>> sigops in the redeem script, multiplied by a factor of 8.\n>>\n>> There is a flaw here: witness script with no sigop will be counted as 0\n>> and have a lot free space\n>>\n>> > every 73 bytes in the serialized witness vector is worth 1 additional\n>> point.\n>>\n>> so 72 bytes is 1 point or 0 point? Maybe it should just scale everything\n>> up by 64 or 128, and make 1 witness byte = 1 point . So it won\u2019t provide\n>> any \u201cfree space\u201d in the block.\n>>\n>> > Currently defined witness programs (v0) are each worth 8 points.\n>> Unknown witness program outputs are worth 1 point. Any exiting output is\n>> always worth 8 points.\n>>\n>> I\u2019d suggest to have at least 16 points for each witness v0 output, so it\n>> will make it always more expensive to create than spend UTXO. It may even\n>> provide extra \u201cdiscount\u201d if a tx has more input than output. The overall\n>> objective is to limit the UTXO growth. The ext block should be mainly for\n>> making transactions, not store of value (I\u2019ll explain later)\n>>\n>> > Dust Threshold\n>>\n>> In general I think it\u2019s ok, but I\u2019d suggest a higher threshold like 5000\n>> satoshi. It may also combine the threshold with the output witness version,\n>> so unknown version may have a lower or no threshold. Alternatively, it may\n>> start with a high threshold and leave a backdoor softfork to reduce it.\n>>\n>> > Deactivation\n>>\n>> It is a double-edged sword. While it is good for us to be able to discard\n>> an unused chain, it may create really bad user experience and people may\n>> even lose money. For example, people may have opened Lightning channels and\n>> they will find it not possible to close the channel. So you need to make\n>> sure people are not making time-locked tx for years, and require people to\n>> refresh their channel regularly. And have big red warning when the\n>> deactivation SF is locked in. Generally, xblock with deactivation should\n>> never be used as long-term storage of value.\n>>\n>> \u2014\u2014\u2014\u2014\n>> some general comments:\n>>\n>> 1. This BIP in current form is not compatible with BIP141. Since most\n>> nodes are already upgraded to BIP141, this BIP must not be activated unless\n>> BIP141 failed to activate. However, if the community really endorse the\n>> idea of ext block, I see no reason why we couldn\u2019t activate BIP141 first\n>> (which could be done in 2 weeks), then work together to make ext block\n>> possible. Ext block is more complicated than segwit. If it took dozens of\n>> developers a whole year to release segwit, I don\u2019t see how ext block could\n>> become ready for production with less time and efforts.\n>>\n>> 2. Another reason to make this BIP compatible with BIP141 is we also need\n>> malleability fix in the main chain. As the xblock has a deactivation\n>> mechanism, it can\u2019t be used for longterm value storage.\n>>\n>> 3. I think the size and cost limit of the xblock should be lower at the\n>> beginning, and increases as we find it works smoothly. It could be a\n>> predefined growth curve like BIP103, or a backdoor softfork. With the\n>> current design, it leaves a massive space for miners to fill up with non-tx\n>> garbage. Also, I\u2019d also like to see a complete SPV fraud-proof solution\n>> before the size grows bigger.\n>>\n>>\n>> > On 5 Apr 2017, at 02:03, Luke Dashjr via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> >\n>> > Recently there has been some discussion of an apparent work-in-progress\n>> > extension block proposal by Christopher Jeffrey, Joseph Poon, Fedor\n>> Indutny,\n>> > and Steven Pair. Since this hasn't been formally posted on the ML yet,\n>> perhaps\n>> > it is still in pre-draft stages and not quite ready for review, but in\n>> light\n>> > of public interest, I think it is appropriate to open it to discussion,\n>> and\n>> > toward this end, I have reviewed the current revision.\n>> >\n>> > For reference, the WIP proposal itself is here:\n>> >    https://github.com/tothemoon-org/extension-blocks\n>> >\n>> > ==Overall analysis & comparison==\n>> >\n>> > This is a relatively complicated proposal, creating a lot of additional\n>> > technical debt and complexity in comparison to both BIP 141 and\n>> hardforks. It\n>> > offers no actual benefits beyond BIP 141 or hardforks, so seems\n>> irrational to\n>> > consider at face value. In fact, it fits much better the inaccurate\n>> criticisms\n>> > made by segwit detractors against BIP 141.\n>> >\n>> > That being said, this proposal is very interesting in construction and\n>> is for\n>> > the most part technically sound. While ill-fit to merely making blocks\n>> larger,\n>> > it may be an ideal fit for fundamentally different block designs such as\n>> > Rootstock and MimbleWimble in absence of decentralised non-integrated\n>> > sidechains (extension blocks are fundamentally sidechains tied into\n>> Bitcoin\n>> > directly).\n>> >\n>> > ==Fundamental problem==\n>> >\n>> > Extension blocks are a risk of creating two classes of \"full nodes\":\n>> those\n>> > which verify the full block (and are therefore truly full nodes), and\n>> those\n>> > which only verify the \"base\" block. However, because the extension is\n>> > consensus-critical, the latter are in fact not full nodes at all, and\n>> are left\n>> > insecure like pseudo-SPV (not even real SPV) nodes. This technical\n>> nature is\n>> > of course true of a softfork as well, but softforks are intentionally\n>> designed\n>> > such that all nodes are capable of trivially upgrading, and there is no\n>> > expectation for anyone to run with pre-softfork rules.\n>> >\n>> > In general, hardforks can provide the same benefits of an extension\n>> block, but\n>> > without the false expectation and pointless complexity.\n>> >\n>> > ==Other problems & questions==\n>> >\n>> >> These outpoints may not be spent inside the mempool (they must be\n>> redeemed\n>> > from the next resolution txid in reality).\n>> >\n>> > This breaks the ability to spend unconfirmed funds in the same block\n>> (as is\n>> > required for CPFP).\n>> >\n>> > The extension block's transaction count is not cryptographically\n>> committed-to\n>> > anywhere. (This is an outstanding bug in Bitcoin today, but impractical\n>> to\n>> > exploit in practice; however, exploiting it in an extension block may\n>> not be\n>> > as impractical, and it should be fixed given the opportunity.)\n>> >\n>> >> The merkle root is to be calculated as a merkle tree with all extension\n>> > block txids and wtxids as the leaves.\n>> >\n>> > This needs to elaborate how the merkle tree is constructed. Are all the\n>> txids\n>> > followed by all the wtxids (tx hashes)? Are they alternated? Are txid\n>> and\n>> > wtxid trees built independently and merged at the tip?\n>> >\n>> >> Output script code aside from witness programs, p2pkh or p2sh is\n>> considered\n>> > invalid in extension blocks.\n>> >\n>> > Why? This prevents extblock users from sending to bare multisig or other\n>> > various possible destinations. (While static address forms do not exist\n>> for\n>> > other types, they can all be used by the payment protocol.)\n>> >\n>> > Additionally, this forbids datacarrier (OP_RETURN), and forces spam to\n>> create\n>> > unprovably-unspendable UTXOs. Is that intentional?\n>> >\n>> >> The maximum extension size should be intentionally high.\n>> >\n>> > This has the same \"attacks can do more damage than ordinary benefit\"\n>> issue as\n>> > BIP141, but even more extreme since it is planned to be used for future\n>> size\n>> > increases.\n>> >\n>> >> Witness key hash v0 shall be worth 1 point, multiplied by a factor of\n>> 8.\n>> >\n>> > What is a \"point\"? What does it mean multiplied by a factor of 8? Why\n>> not just\n>> > say \"8 points\"?\n>> >\n>> >> Witness script hash v0 shall be worth the number of accurately counted\n>> > sigops in the redeem script, multiplied by a factor of 8.\n>> >\n>> > Please define \"accurately counted\" here. Is this using BIP16 static\n>> counting,\n>> > or accurately counting sigops during execution?\n>> >\n>> >> To reduce the chance of having redeem scripts which simply allow for\n>> garbage\n>> > data in the witness vector, every 73 bytes in the serialized witness\n>> vector is\n>> > worth 1 additional point.\n>> >\n>> > Is the size rounded up or down? If down, 72-byte scripts will carry 0\n>> > points...)\n>> >\n>> > ==Trivial & process==\n>> >\n>> > BIPs must be in MediaWiki format, not Markdown. They should be\n>> submitted for\n>> > discussion to the bitcoin-dev mailing list, not social media and news.\n>> >\n>> >> Layer: Consensus (soft-fork)\n>> >\n>> > Extension blocks are more of a hard-fork IMO.\n>> >\n>> >> License: Public Domain\n>> >\n>> > BIPs may not be \"public domain\" due to non-recognition in some\n>> jurisdictions.\n>> > Can you agree on one or more of these?\n>> > https://github.com/bitcoin/bips/blob/master/bip-0002.\n>> mediawiki#Recommended_licenses\n>> >\n>> >> ## Abstract\n>> >>\n>> >> This specification defines a method of increasing bitcoin transaction\n>> > throughput without altering any existing consensus rules.\n>> >\n>> > This is inaccurate. Even softforks alter consensus rules.\n>> >\n>> >> ## Motivation\n>> >>\n>> >> Bitcoin retargetting ensures that the time in between mined blocks\n>> will be\n>> > roughly 10 minutes. It is not possible to change this rule. There has\n>> been\n>> > great debate regarding other ways of increasing transaction throughput,\n>> with\n>> > no proposed consensus-layer solutions that have proven themselves to be\n>> > particularly safe.\n>> >\n>> > Block time seems entirely unrelated to this spec. Motivation is unclear.\n>> >\n>> >> Extension blocks leverage several features of BIP141, BIP143, and\n>> BIP144 for\n>> > transaction opt-in, serialization, verification, and network services,\n>> and as\n>> > such, extension block activation entails BIP141 activation.\n>> >\n>> > As stated in the next paragraph, the rules in BIP 141 are fundamentally\n>> > incompatible with this one, so saying BIP 141 is activated is\n>> confusingly\n>> > incorrect.\n>> >\n>> >> This specification should be considered an extension and modification\n>> to\n>> > these BIPs. Extension blocks are _not_ compatible with BIP141 in its\n>> current\n>> > form, and will require a few minor additional rules.\n>> >\n>> > Extension blocks should be compatible with BIP 141, there doesn\u2019t\n>> appear to be\n>> > any justification for not making them compatible.\n>> >\n>> >> This specification prescribes a way of fooling non-upgraded nodes into\n>> > believing the existing UTXO set is still behaving as they would expect.\n>> >\n>> > The UTXO set behaves fundamentally different to old nodes with this\n>> proposal,\n>> > albeit in a mostly compatible manner.\n>> >\n>> >> Note that canonical blocks containing entering outputs MUST contain an\n>> > extension block commitment (all zeroes if nothing is present in the\n>> extension\n>> > block).\n>> >\n>> > Please explain why in Rationale.\n>> >\n>> >> Coinbase outputs MUST NOT contain witness programs, as they cannot be\n>> > sweeped by the resolution transaction due to previously existing\n>> consensus\n>> > rules.\n>> >\n>> > Seems like an annoying technical debt. I wonder if it can be avoided.\n>> >\n>> >> The genesis resolution transaction MAY also include a 1-100 byte\n>> pushdata in\n>> > the first input script, allowing the miner of the genesis resolution to\n>> add a\n>> > special message. The pushdata MUST be castable to a true boolean.\n>> >\n>> > Why? Unlike the coinbase, this seems to create additional technical\n>> debt with\n>> > no apparent purpose. Better to just have a consensus rule every input\n>> must be\n>> > null.\n>> >\n>> >> The resolution transaction's version MUST be set to the uint32 max\n>> (`2^32 -\n>> > 1`).\n>> >\n>> > Transaction versions are signed, so I assume this is actually simply -1.\n>> > (While signed transaction versions seemed silly to me, using it for\n>> special\n>> > cases like this actually makes sense.)\n>> >\n>> >> ### Exiting the extension block\n>> >\n>> > Should specify that spending such an exit must use the resolution txid,\n>> not\n>> > the extblock's txid.\n>> >\n>> >> On the policy layer, transaction fees may be calculated by transaction\n>> cost\n>> > as well as additional size/legacy-sigops added to the canonical block\n>> due to\n>> > entering or exiting outputs.\n>> >\n>> > BIPs should not specify policy at all. Perhaps prefix \"For the\n>> avoidance of\n>> > doubt:\" to be clear that miners may perform any fee logic they like.\n>> >\n>> >> Transactions within the extended transaction vector MAY include a\n>> witness\n>> > vector using BIP141 transaction serialization.\n>> >\n>> > Since extblock transactions are all required to be segwit, why wouldn't\n>> this\n>> > be mandatory?\n>> >\n>> >> - BIP141's nested P2SH feature is no longer available, and no longer a\n>> > consensus rule.\n>> >\n>> > Note this makes adoption slower: wallets cannot use the extblock until\n>> the\n>> > economy has updated to support segwit-native addresses.\n>> >\n>> >> To reduce the chance of having redeem scripts which simply allow for\n>> garbage\n>> > data in the witness vector, every 73 bytes in the serialized witness\n>> vector is\n>> > worth 1 additional point.\n>> >\n>> > Please explain why 73 bytes in Rationale.\n>> >\n>> >> This leaves room for 7 future soft-fork upgrades to relax DoS limits.\n>> >\n>> > How so? Please explain.\n>> >\n>> >> A consensus dust threshold is now enforced within the extension block.\n>> >\n>> > Why?\n>> >\n>> >> If the second highest transaction version bit (30th bit) is set to to\n>> `1`\n>> > within an extension block transaction, an extra 700-bytes is reserved\n>> on the\n>> > transaction space used up in the block.\n>> >\n>> > Why wouldn't users set this on all transactions?\n>> >\n>> >> `default_witness_commitment` has been renamed to\n>> > `default_extension_commitment` and includes the extension block\n>> commitment\n>> > script.\n>> >\n>> > `default_witness_commitment` was never part of the GBT spec. At least\n>> describe\n>> > what this new key is.\n>> >\n>> >> - Deployment name: `extblk` (appears as `!extblk` in GBT).\n>> >\n>> > Should be just `extblk` if backward compatibility is supported (and\n>> `!extblk`\n>> > when not).\n>> >\n>> >> The \"deactivation\" deployment's start time...\n>> >\n>> > What about timeout? None? To continue the extension block, must it be\n>> > deactivated and reactivated in parallel?\n>> >\n>> >\n>> > _______________________________________________\n>> > bitcoin-dev mailing list\n>> > bitcoin-dev at lists.linuxfoundation.org\n>> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170405/7864e3c1/attachment-0001.html>"
            },
            {
                "author": "Joseph Poon",
                "date": "2017-04-05T16:25:31",
                "message_text_only": "On Wed, Apr 05, 2017 at 11:37:22AM -0400, Greg Sanders via bitcoin-dev wrote:\n> I'd appreciate the authors chiming in, but I read the PASDA differently:\n> \n> 1) If a transaction is mined with a certain bit set, it reserves 700 bytes\n> for that particular block.\n> 2) In that space, 2 transactions may happen:\n> a) First, a transaction penalizing the \"parent\" transaction for fraud by\n> spending the funds immediately\n> b) Second, a \"free rider\" transaction that penalizes fraud within a ~2 week\n> window\n> \n> This means during systematic flooding of closing transactions by\n> Goldfinger, vigilant watchers of their channels can immediately punish the\n> fraud in the same block using (a), and if they are unable to, need to find\n> space within two weeks in (b).\n> \n> This is really in the LN weeds however, so I'll refrain from evaluating the\n> efficacy of such a solution.\n\nYes, that is correct. I haven't had a chance to review Laolu's summary\nyet, haven't had a chance to talk to him today since I was away from the\nkeyboard for most of the day, would have been unable to review things.\n\nSection \"b\" above only allows for free riding on the first output of a\ntransaction with the bit set within the past 2016 blocks. It does not\nallow free riding on outputs without that bit set in the transaction.\n\nAdditionally, the presumption is that the attacker fills up the\nmempool with incorrect prior commitment transactions.\n\nThe attack scenario is Mallory asks everyone to open a channel with her.\nMallory only has 1 BTC. With sufficiently low tx fees, Mallory can use\nthat one bitcoin to open many ~1 BTC channels. All of those channels had\na prior state which Mallory had ~1 BTC, and a current state where she\nhas none. She broadcasts these thousands of prior states where she has\n~1 BTC.\n\nThe presumption is the penalty transaction in many cases has a very\nsmall fee, since it is already covered by the commitment.\n\nThis mitigates systemic goldfinger attacks since it is unlikely they can\nget enough transactions in. Additionally the transactions waiting on the\nmempool allows for many to be notified and fill up the first reserved\nspace. The attacker would likely be attempting to fill up the mempool\n(longer block times help here with security!!!). It is presumed that\nthere is some small amount in reserve so there is some fee reward\ncovered for enforcing the penalty. This construction allows for the\namount in reserve to be significantly smaller and much more resilient\nagainst even the largest of goldfinger attacks.\n\n(This isn't a full mitigation, as there are certain conditions related\nto miner-attacker coordination with high hashpower. Attacker-Miner\ncoordination is presumed to be out-of-scope, especially in relation to\n51% attacks, since it's sort of a moot point, if they have the funds to\nmount this attack so that it's profitable, it gets pretty close for them\nto have a very significant hashpower anyway.)\n\nI'll add a clarification to the specification on github soon. The intent\nof this is to reduce the cost of setting up LN channels with funds in\nreserve, with minimal code changes. Future changes which could be\ndesired if this is usable would be use additional tx flag bits to select\nhow many outputs in a transaction apply to enable a large payment of\nfunds pending in-flight.\n\n-- \nJoseph Poon"
            },
            {
                "author": "Johnson Lau",
                "date": "2017-04-05T17:04:10",
                "message_text_only": "> On 5 Apr 2017, at 22:05, Olaoluwa Osuntokun <laolu32 at gmail.com> wrote:\n> \n> \n> The concrete parameters chosen in the proposal are: each channel opening\n> transaction reserves 700-bytes within _each_ block in the chain until the\n> transaction has been closed. \n> \n> \n\nWhy so? It seems you are describing it as a softfork. With hardfork or extension block, a new rule could simply grant extra space when the tagged UTXO is spent. So if the usual block size limit is 1MB, when the special UTXO is made, the block size limit decreases to 1MB-700 byte, and the user has to pay for that 700 byte. When it is spent, the block size will become 1MB+700 byte.\n\nBut miners or even users may abuse this system: they may try to claim all the unused space when the blocks are not congested, or when they are mining empty block, and sell those tagged UTXO later. So I think we need to limit the reservable space in each block, and deduct more space than it is reserved. For example, if 700 bytes are reserved, the deduction has to be 1400 byte.\n\nWith BIP68, there are 8 unused bits in nSequence. We may use a few bits to let users to fine tune the space they want to reserve. Maybe 1 = 256 bytes\n\nI think this is an interesting idea to explorer and I\u2019d like to include this in my hardfork proposal.\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170406/1c3fc479/attachment.html>"
            },
            {
                "author": "Christopher Jeffrey",
                "date": "2017-04-05T17:43:43",
                "message_text_only": "Hi Johnson,\n\nReally appreciate the comments. I know this idea is your baby.\n\n> so the authors don\u2019t consider segwit as a consensus-layer solution to\n> increase transaction throughput, or not think segwit is safe? But\n> logically speaking if segwit is not safe, this BIP could only be\n> worse. OTOH, segwit also obviously increases tx throughput, although\n> it may not be as much as some people wish to have.\n\nSegwit wasn't considered to be a part of that statement. It was\nreferring to the numerous hardfork proposals we've seen over the past\nfew years. Segwit is safe, but wouldn't be a comparable block size\nincrease to what ext. blocks could potentially offer.\n\n> I think extension block in the proposed form actually breaks BIP141.\n> It may say it activates segregated witness as a general idea, but not\n> a specific proposal like BIP141\n\nAgreed. Needs to be reworded as it currently stands. Though, I suppose\nit would be possible to allow for compatibility with segwit in the\nmainchain if we utilize your idea of using a separate wit. program\nversions for the extension block. A slightly minor change to the spec,\njust a big change to the reference impl. code. It is doable.\n\n> This hits the biggest question I asked in my January post: do you want\n> to allow direct exit payment to legacy addresses? As a block reorg\n> will almost guarantee changing txid of the resolution tx, that will\n> permanently invalidate all the child txs based on the resolution tx.\n> This is a significant change to the current tx model. To fix this, you\n> need to make exit outputs unspendable for up to 100 blocks. Doing\n> this, however, will make legacy wallet users very confused as they do\n> not anticipate funding being locked up for a long period of time. So\n> you can\u2019t let the money sent back to a legacy address directly, but\n> sent to a new format address that only recognized by new wallet, which\n> understands the lock up requirement. This way, however, introduces\n> friction and some fungibility issues, and I\u2019d expect people using\n> cross chain atomic swap to exchange bitcoin and xbitcoin\n\nYes, this issue is probably the biggest edge case in the proposal.\n\nI think there's two possible solutions:\n\nFirst solution:\n\nLike you said, add a maturity requirement for exiting outputs. Likely\nlower than coinbase's 100 block requirement. To solve the issue of\nnon-upgraded wallets not being aware of this rule and spending early,\nhave upgraded mempool implementations accept/relay txs that contain\nearly spends of exits, but not mine them until they are mature. This way\nnon-upgraded wallets do not end up broadcasting transactions that are\nconsidered invalid to the rest of the network.\n\nDepending on how wallets handle reorgs, a non-upgraded wallet may put\nreorg'd spend chains from exits back into an unconfirmed state, when in\nreality they should probably delete them or mark them conflicted in some\nway. This may be an acceptable compromise as the wallet will still see\nthe funds as unconfirmed when they really don't exist anymore, but maybe\nunconfirmed is good enough. Users are pretty used to dropping\nnon-confirming txs from their wallet, and this is much better than\nlegacy wallets seeing there funds as confirmed when they could be\npermanently reorged out at any moment.\n\nSecond solution:\n\nMove all exiting outputs to the coinbase. This will enforce a 100 block\nmaturity requirement and non-upgraded wallets will be aware of this.\n\nThe first solution might require more implementation, but allows more\nflexibility with the maturity requirement. The second solution is\npossibly simpler, but sticks to a hard 100 block limit.\n\n> 1. Is it acceptable to have massive txid malleability and transaction\n> chain invalidation for every natural happening reorg?  Yes: the\n> current spec is ok; No: next question (I\u2019d say no)\n\nAnswered above.\n\n> 2. Is locking up exit outputs the best way to deal with the problem?\n> (I tried really hard to find a better solution but failed)\n\nYou've probably thought about this more than anyone, so I'd say yes, it\nmay be the only way. Painful, but necessary.\n\n> 3. How long the lock-up period should be? Answer could be anywhere\n> from 1 to 100\n\nI imagine having something lower than 100 would be preferable to users,\nmaybe somewhere in the 5 to 15 range. A 15 block reorg on mainnet is\nseriously unlikely unless something strange is happening. A 5 block\nreorg is still pretty unlikely, but possible. The coinbase solution only\nallows for 100 blocks though.\n\n> 4. With a lock-up period, should it allow direct exit to legacy\n> address? (I think it\u2019s ok if the lock-up is short, like 1-2 block. But\n> is that safe enough?)\n\nI think so. Adding a kind of special address probably creates more\nissues than it solves.\n\n> 5. Due to the fungibility issues, it may need a new name for the\n> tokens in the ext-block\n\nI suppose the market will decide whether that's the case.\n\nIt's worth noting, if segwit is not activated on the mainchain, it\ncreates a much bigger incentive to use the extension block, and\npotentially ensures that users will have less of a reason to exit.\n\n--\nChristopher Jeffrey (JJ) <chjjeffrey at gmail.com>\nCTO & Bitcoin Menace, purse.io\nhttps://github.com/chjj\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 488 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170405/6fd08871/attachment-0001.sig>"
            },
            {
                "author": "Johnson Lau",
                "date": "2017-04-10T10:14:36",
                "message_text_only": "> On 6 Apr 2017, at 01:43, Christopher Jeffrey <chjj at purse.io> wrote:\n> \n> \n>> This hits the biggest question I asked in my January post: do you want\n>> to allow direct exit payment to legacy addresses? As a block reorg\n>> will almost guarantee changing txid of the resolution tx, that will\n>> permanently invalidate all the child txs based on the resolution tx.\n>> This is a significant change to the current tx model. To fix this, you\n>> need to make exit outputs unspendable for up to 100 blocks. Doing\n>> this, however, will make legacy wallet users very confused as they do\n>> not anticipate funding being locked up for a long period of time. So\n>> you can\u2019t let the money sent back to a legacy address directly, but\n>> sent to a new format address that only recognized by new wallet, which\n>> understands the lock up requirement. This way, however, introduces\n>> friction and some fungibility issues, and I\u2019d expect people using\n>> cross chain atomic swap to exchange bitcoin and xbitcoin\n> \n> Yes, this issue is probably the biggest edge case in the proposal.\n> \n> I think there's two possible solutions:\n> \n> First solution:\n> \n> Like you said, add a maturity requirement for exiting outputs. Likely\n> lower than coinbase's 100 block requirement. To solve the issue of\n> non-upgraded wallets not being aware of this rule and spending early,\n> have upgraded mempool implementations accept/relay txs that contain\n> early spends of exits, but not mine them until they are mature. This way\n> non-upgraded wallets do not end up broadcasting transactions that are\n> considered invalid to the rest of the network.\n\nThis won\u2019t solve the problem. Think about the following conversation:\n\nAlice (not upgraded): Please pay 1 BTC to my address 1ALicExyz\nBob (upgraded): ok, paid, please check\n\n10 minutes later\n\nAlice: received and confirmed, thanks!\n\n5 minutes later:\n\nCarol (not upgraded): Please pay 0.5BTC to my address 3CaroLXXX\nAlice: paid, please check\n\n1 hour later:\n\nCarol: it\u2019s not confirmed. Have you paid enough fees?\nAlice: ok, I\u2019ll RBF/CPFP it\n\n2 hours later:\n\nCarol: it\u2019s still not confirmed.\nAlice: I have already paid double fees. Maybe the network is congested and I need to pay more\u2026..\n\nRepeat until the lock up period ends.\n\nSo this so-called \u201csoftfork\u201d actually made non-upgraded wallet totally unusable. If failed to meet the very important requirement of a softfork: backward compatibility\n\nMore discussion:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-April/013985.html <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-April/013985.html>\n\n\n> \n> Depending on how wallets handle reorgs, a non-upgraded wallet may put\n> reorg'd spend chains from exits back into an unconfirmed state, when in\n> reality they should probably delete them or mark them conflicted in some\n> way. This may be an acceptable compromise as the wallet will still see\n> the funds as unconfirmed when they really don't exist anymore, but maybe\n> unconfirmed is good enough. Users are pretty used to dropping\n> non-confirming txs from their wallet, and this is much better than\n> legacy wallets seeing there funds as confirmed when they could be\n> permanently reorged out at any moment.\n> \n> Second solution:\n> \n> Move all exiting outputs to the coinbase. This will enforce a 100 block\n> maturity requirement and non-upgraded wallets will be aware of this.\n\nThis is also unacceptable.\n\nWhen someone says \"Please pay 1 BTC to my address 1ALicExyz\u201d, no one anticipates being paid by a coinbase output. Some exchanges like btc-e explicitly reject coinbase payment.\n\nSuch deterioration in user experience is unacceptable. It basically forces everyone to upgrade, i.e. a hardfork with soft fork\u2019s skin\n\n\n\n> \n> The first solution might require more implementation, but allows more\n> flexibility with the maturity requirement. The second solution is\n> possibly simpler, but sticks to a hard 100 block limit.\n> \n>> 1. Is it acceptable to have massive txid malleability and transaction\n>> chain invalidation for every natural happening reorg?  Yes: the\n>> current spec is ok; No: next question (I\u2019d say no)\n> \n> Answered above.\n> \n>> 2. Is locking up exit outputs the best way to deal with the problem?\n>> (I tried really hard to find a better solution but failed)\n> \n> You've probably thought about this more than anyone, so I'd say yes, it\n> may be the only way. Painful, but necessary.\n> \n>> 3. How long the lock-up period should be? Answer could be anywhere\n>> from 1 to 100\n> \n> I imagine having something lower than 100 would be preferable to users,\n> maybe somewhere in the 5 to 15 range. A 15 block reorg on mainnet is\n> seriously unlikely unless something strange is happening. A 5 block\n> reorg is still pretty unlikely, but possible. The coinbase solution only\n> allows for 100 blocks though.\n> \n>> 4. With a lock-up period, should it allow direct exit to legacy\n>> address? (I think it\u2019s ok if the lock-up is short, like 1-2 block. But\n>> is that safe enough?)\n> \n> I think so. Adding a kind of special address probably creates more\n> issues than it solves.\n\n\nAs I explained above, no legacy wallet would anticipate a lock up. If you want to make a softfork, all burden of incompatibility must be taken by the upgraded system. Only allow exit to a new address guarantees that only upgraded wallet will see the locked-up tx:\n\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013490.html <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013490.html>\n> \n>> 5. Due to the fungibility issues, it may need a new name for the\n>> tokens in the ext-block\n> \n> I suppose the market will decide whether that's the case.\n> \n> It's worth noting, if segwit is not activated on the mainchain, it\n> creates a much bigger incentive to use the extension block, and\n> potentially ensures that users will have less of a reason to exit.\n> \n\nI think it\u2019s unacceptable if malleability is not fixed in main chain, for 3 reasons: \n\n1. a solution is *already* available and tested for > 1 year.\n\n2. the deactivation design (which I think is an interesting idea) makes the ext block unsuitable for long-term storage of value.\n\n3. LN over main chain allows instant exchange of main coin and xcoin without going through the ugly 2-way-peg process.\n\n\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170410/036f6206/attachment-0001.html>"
            },
            {
                "author": "Christopher Jeffrey",
                "date": "2017-04-05T16:54:05",
                "message_text_only": "Hi Luke,\n\nThank you for the review. Many of these points should definitely be\naddressed in the spec. Although extension blocks has working code, the\nspec is currently still in a draft state now and could really use all\nthe feedback it can get. A few rules are still up in the air before we\nsetup a public testnet for it.\n\nThere's understandable confusion about this, but this proposal is not\nmeant to be a BIP. If it were meant to be a BIP, we still might not have\neven submitted it yet as it needs a bit more revision still.\n\nI'm just going to go over a lot of these and explain the reasoning.\n\n> This breaks the ability to spend unconfirmed funds in the same block\n> (as is required for CPFP).\n\nYeah, child-pays-for-parent is practically impossible for exiting\noutputs. I don't see a good way around this. We tried to figure out a\ndecent solution while initially drafting this. It's possible with tons\nof trickery, but likely not worth it.\n\n> The extension block's transaction count is not cryptographically\n> committed-to anywhere. (This is an outstanding bug in Bitcoin today,\n> but impractical to exploit in practice; however, exploiting it in an\n> extension block may not be as impractical, and it should be fixed\n> given the opportunity.)\n\nYes. The merkle commitments are something we could definitely improve.\nOpen to suggestions. Personally, I don't consider myself a merkle\nexpert.\n\n> This needs to elaborate how the merkle tree is constructed. Are all\n> the txids followed by all the wtxids (tx hashes)? Are they alternated?\n> Are txid and wtxid trees built independently and merged at the tip?\n\nAs of right now, the reference implementation uses the former, but\nagain, the merkle commitments are something up in the air. It'd be nice\nto keep it as flexible as possible for SPV proofs on either the txids or\nwxtids.\n\n> Why? This prevents extblock users from sending to bare multisig or\n> other various possible destinations. (While static address forms do\n> not exist for other types, they can all be used by the payment\n> protocol.)\n\nRequiring only p2pkh and p2sh for exits reduces the possibility of more\nUTXO set size bloat (at least slightly). Non-standard scripts are a\nproblem since they cannot be compressed for storage. I don't see it as\nimportant to allow naked multisig outputs. Currently, if users wanted to\nuse a naked multisig (why?), they can always use the 1mb chain directly.\n\n> Additionally, this forbids datacarrier (OP_RETURN), and forces spam to\n> create unprovably-unspendable UTXOs. Is that intentional?\n\nAll outputs within the extension block are meant to be witness programs.\nThis was done for simplicity. The 1mb chain is still usable for any\nOP_RETURNs committed to the chain. More thought on this would be good\nthough.\n\n> > The maximum extension size should be intentionally high.\n>\n> This has the same \"attacks can do more damage than ordinary benefit\"\n> issue as BIP141, but even more extreme since it is planned to be used\n> for future size increases.\n\n> What is a \"point\"? What does it mean multiplied by a factor of 8? Why\n> not just say \"8 points\"?\n\nJust for consistency of wording.\n\nThe notion of cost creates a system of points which are multiplied by a\nfactor chosen by the witness program version. Unknown witness programs\nhave a factor of 1. If, in the future, we soft-fork in a new witness\nprogram version, its chosen factor could be 7 or 6. The idea being,\nfuture versions could add less \"cost\" to the block, allowing for\nrelaxing dos limits over time via soft-fork.\n\nI would much rather have people arguing over whether to soft-fork dos\nlimits than whether to hard-fork dos limits.\n\nSo the idea here is, we have a hard limit (say 6mb) for quick sanity\nchecking and DoS prevention, and a soft-forkable soft limit (e.g. 2mb).\n\nHaving unknown witness program versions be worth only 1 point does\nenable the possibility that a worst case block could be up to the \"hard\"\nmax extension size limit. This is also a possibility with segwit, but\nyes, less severe with segwit assuming the max ext. block size is above\n3mb.\n\nMore discussion and running of numbers is probably necessary before we\ncome up with optimal limits here.\n\n> Please define \"accurately counted\" here. Is this using BIP16 static\n> counting, or accurately counting sigops during execution?\n\nIt's meant to refer to BIP16 static counting. I believe the actual\nargument passed to the function in Bitcoin Core is called `fAccurate`.\nMany other implementations use the same terminology. The counting during\nexecution proposed by Gavin's hardfork BIP isn't widely implemented as\nfar as I know.\n\n> Is the size rounded up or down? If down, 72-byte scripts will carry 0\n> points...)\n\nRounded up. The code included this earlier, but I took the whole\nweighing against size out temporarily. Will be updated to match the\nspec.\n\n> BIPs must be in MediaWiki format, not Markdown. They should be\n> submitted for discussion to the bitcoin-dev mailing list, not social\n> media and news.\n\nYeah, that's sort of a bias of mine. I prefer markdown, and everyone\nelse helping out with the spec seemed to be okay with my preference. The\nmediawiki format is offensive to me. In any case, this isn't really\nmeant to be a BIP.\n\n> Extension blocks are more of a hard-fork IMO.\n\nCould you expand on why you consider this a hardfork?\n\n> Block time seems entirely unrelated to this spec. Motivation is\n> unclear.\n\nTransaction throughput is related to this spec. Block time and size are\nboth related to transaction throughput. It's meant to say something to\nthe effect of \"changing retargetting is likely infeasible with a\nsoft-fork, but changing block size may not be as much of a problem.\"\nCould be reworded.\n\n> As stated in the next paragraph, the rules in BIP 141 are\n> fundamentally incompatible with this one, so saying BIP 141 is\n> activated is confusingly incorrect.\n\nTrue. Should be reworded.\n\n> Extension blocks should be compatible with BIP 141, there doesn\u2019t\n> appear to be any justification for not making them compatible.\n\nThe implementation initially seemed a lot simpler when moving all segwit\nbehavior to the extension block. The initial conception was to have all\nwitness programs be entrances into and scripts within the extension\nblock, but I guess there's no reason we couldn't do something like\nJohnson proposed and have different witness program versions be the\next-block-only programs. It just involves me rewriting a bit of code in\nthe reference implementation, and backporting a lot of code to the\noriginal branch.\n\n> > Note that canonical blocks containing entering outputs MUST contain\n> > an extension block commitment (all zeroes if nothing is present in\n> > the extension block).\n>\n> Please explain why in Rationale.\n\nThis can be removed, and something I initially added to my own code\nduring initial implementation as a simple check ahead of time to check\nfor entering outputs.\n\n> > Coinbase outputs MUST NOT contain witness programs, as they cannot\n> > be sweeped by the resolution transaction due to previously existing\n> > consensus rules.\n>\n> Seems like an annoying technical debt. I wonder if it can be avoided.\n\nI think there is a way around it, just not a real viable way: requiring\nminers to resolve the witness program outputs in the coinbase 100 blocks\nago. But this will cause miners to attack each other, since they're now\npotentially adding size to another miners block. It also causes a load\nof other issues with wallets.\n\nI don't see the coinbase output rule as that much of an issue though.\nThe 1mb chain will remain the realm of miners and long-term hodlers for\nsure. If they want to switch to the ext. block, they can always just\nsweep their outputs.\n\n> Why? Unlike the coinbase, this seems to create additional technical\n> debt with no apparent purpose. Better to just have a consensus rule\n> every input must be null.\n\nIt's a pretty simple consensus check, and might be a fun extra to have.\nThe genesis block has a pretty unique mystique to it. Might be fun to\nreplicate that in the genesis resolution.\n\n> Transaction versions are signed, so I assume this is actually simply\n> -1.  (While signed transaction versions seemed silly to me, using it\n> for special cases like this actually makes sense.)\n\nYeah, transaction versions are just bits as far as I'm concerned. It\ndepends on how you want to interpret them. But yeah, it would be `-1` if\nyou were to consider it an int32. My own code just treats them as\nunsigned.\n\n> Should specify that spending such an exit must use the resolution\n> txid, not the extblock's txid.\n\nAgreed.\n\n> BIPs should not specify policy at all. Perhaps prefix \"For the\n> avoidance of doubt:\" to be clear that miners may perform any fee logic\n> they like.\n\nMentioning policy as an aside seemed useful here for now for a clearer\nunderstanding. A good deal of this spec may be separated out as some\nkind of commentary on implementation details eventually.\n\n> Since extblock transactions are all required to be segwit, why\n> wouldn't this be mandatory?\n\nThat was originally only referring to serialization (segwit allows empty\nwitness vectors in serialization). I will reword this to refer to\nverification only.\n\n> Note this makes adoption slower: wallets cannot use the extblock until\n> the economy has updated to support segwit-native addresses.\n\nNested P2SH would be hard to do for the ext. block, short of some added\ntrickery (miners only redeeming that output for entrance once the redeem\nscript is revealed).\n\n> Please explain why 73 bytes in Rationale.\n\nDER-formatted signature size. \"Inputs cost\" was originally designed to\nreflect sigops. To prevent tons of garbage data in the witness vector,\nthe vector's size is also considered a \"sigop/cost\" for every 73 bytes.\nIt should probably start weighing sigops points and size points\ndifferently though, or treat them as separate metrics.\n\n> > A consensus dust threshold is now enforced within the extension\n> > block.\n>\n> Why?\n\nAnother measure to potentially reduce UTXO spam. Will clarify.\n\n> Why wouldn't users set this on all transactions?\n\nIt looks like Laolu beat me to commenting on this. Both Joseph and Laolu\nwill have better commentary on this than me, so I'll let them handle\nthis.\n\n> > The \"deactivation\" deployment's start time...\n>\n> What about timeout? None? To continue the extension block, must it be\n> deactivated and reactivated in parallel?\n\nTimeout of 1 year. That may have gotten lost in the frequent revisions\nwe did.\n\nOnce voting has successfully activated the deactivation bit, the\nlocked-in time is 26 retarget intervals (approx. 1 year).\n\nSo, the simplest proposal for deactivation we came up with returns the\nOP_TRUE to being anyone-can-spend. By that time, a future softfork\n(activated in the same versionbit) can introduce code to handle the\nmigration of funds elsewhere. The anyone-can-spend part does sound\npretty odd at first glance, but it's the only way to get new behavior in\nhere without a hardfork.\n\nThe merkle proof proposal is tougher, because we would have to write\ncode _now_ to handle the migration. And since we don't know what future\nextension blocks might look like or how they might behave, this is\npretty difficult.\n\n---\n\nI will open a few issues on the repo for some of the points made here.\n\n--\nChristopher Jeffrey (JJ) <chjj at purse.io>\nCTO & Bitcoin Menace, purse.io\nhttps://github.com/chjj\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 488 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170405/3c3700c0/attachment.sig>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2017-04-06T17:18:35",
                "message_text_only": "On Wednesday, April 05, 2017 4:54:05 PM Christopher Jeffrey via bitcoin-dev \nwrote:\n> There's understandable confusion about this, but this proposal is not\n> meant to be a BIP.\n\nOh? If this was not meant to be a Bitcoin Improvement Proposal, perhaps you \nshould clarify somewhere what altcoin you are proposing it for. As it stands, \nit certainly did read much like it was meant to be a BIP, and apparently many \nothers thought so as well.\n\nAdmittedly, the bitcoin-dev ML isn't the place for altcoin discussions, and \nI'm not particularly interested in spending my time aiding altcoins, so I'll \njust end the conversation here until someone re-proposes something similar for \nBitcoin.\n\nSorry for confusing the nature of your work,\n\nLuke"
            }
        ],
        "thread_summary": {
            "title": "Extension block proposal by Jeffrey et al",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Greg Sanders",
                "Christopher Jeffrey",
                "Johnson Lau",
                "Olaoluwa Osuntokun",
                "Luke Dashjr",
                "Joseph Poon"
            ],
            "messages_count": 10,
            "total_messages_chars_count": 131610
        }
    },
    {
        "title": "[bitcoin-dev] Base Size Increase and Segregated Witness with Discount Governors (SW-DGov) Hardfork",
        "thread_messages": [
            {
                "author": "Oliver Petruzel",
                "date": "2017-04-05T05:18:22",
                "message_text_only": "Evening all,\n\nThe following BIP submission summarizes an idea that I've been tossing\naround for the last year.  I understand that there may be nuances to SegWit\nand current consensus layer mechanisms that I may not fully understand, so\nplease do not hesitate to shred the following text to pieces (I can handle\nit, I promise!).\n\nPlease note that this BIP assumes failure of the current softfork version\nof SegWit to activate in November -- something that I personally do not\nwish to see(!). However, given the real possibility of that happening, or\nperhaps just some newfound willingness (by \"the community\") to support a\nhardfork in lieu of a stalemate, I figure now is as good a time as any to\nshare the idea in black and white.\n\nI would really appreciate any/all feedback from the dev community on the\ntechnical merits (read: feasibility) of the idea. I would especially\nappreciate feedback from the SegWit developers who designed the current\nimplementation in 0.14, as they likely have the most intimate knowledge of\nSegWit's nuances, and the entire BIP below would likely rely on their\nwillingness to develop a hardfork version.\n\nNothing in this BIP is set in stone -- including all values and timelines\n-- but, I do hope the following text effectively captures the gist of the\nidea, and I do thank you ahead of time for your consideration of the\nproposal.\n\n\nRespectfully,\nOliver Petruzel\n\n-------------------------------------------------------------------------------\n\nBIP:  TBD\nLayer: Consensus (hard fork)\nTitle: Base Size Increase and Segregated Witness with Discount Governors\n(SW-DGov) Hardfork\nAuthor: Oliver Petruzel <opetruzel at gmail.com>\nComments-Summary: No comments yet.\nComments-URI:\nStatus: Draft\nType: Standards Track\nCreated: 2017-04-05\nLicense: PD\n\nAbstract\n\nThis BIP proposes a method of combining an immediate base size increase to\n2MB and a hardfork version of Segregated Witness (SegWit).  The SegWit\nportion of the hardfork will leverage Discount Governors to control (or\n\u201cgovern\u201d) the pace of the increase over a period of 145,152 blocks\n(approximately three (3) years).\n\n\nMotivation\n\nGiven the possibility of the current softfork version of SegWit failing to\nactivate in November 2017, this BIP aims to provide a hardfork alternative\nthat would provide every user in the ecosystem with the fixes and changes\nthey need to move forward with other great projects, while also tightly\ncontrolling the rate at which the total weight of blocks increases during\nthe next three years.  The predictable nature of the increases will provide\nminers, full node operators, and other users of the system with the ability\nto plan their development, resources, and operations accordingly.  The\nfixed nature of the increases will also allow all full nodes to maintain a\nfixed set of rules for block validity (consensus).\n\n\nSpecification\n\nThe following changes will be made to the client:\n\n* An immediate increase of base size to 2,000,000 bytes (perhaps leveraging\ncode changes similar to those described in BIP 109).\n\n\u00b7 A hardfork version of SegWit that maintains all of the fixes present in\nthe softfork version, including (but not limited to):\n- Fix for the Malleability issue\n- Linear scaling of sighash operations\n- Signing of input values\n- Increased security for multisig via pay-to-script-hash (P2SH)\n- Script versioning\n- Reducing UTXO growth\n- Moving towards a single combined block limit\n\n* In addition to those fixes listed above, the hardfork version of SegWit\nwill include the following:\n\n- Rather than using the fixed (75%) Discount found in the softfork version\nof SegWit, the hardfork version will leverage Discount Governing to control\nthe pace of total block weight increases over a three (3) year period of\ntime.  The use of Discount Governors will allow a steady increase over that\nperiod from an immediate 2MB to 8MB total.  There are several ways these\nincreases can be handled \u2013 either by hardcoding the scheduled increases in\nthe initial hardfork, or perhaps using subsequent softforks (additional\ninput/discussion needed on the best way to handle the increases.\n- Example increase schedule: +12.5% every 24,192 blocks (roughly every six\n(6) months).  The increases would cap at the same 75% Discount rate found\nin the current softfork version of SegWit.\n- Each time the Discount increases \u2013 every 24,192 blocks -- the Total Block\nWeight value would also increase to appropriately compensate for the added\nDiscount.\n\n\nRationale\n\nThis hardfork employs a simple flag day deployment based on the median\ntimestamp of previous blocks. Beyond this point, supporting nodes will not\naccept blocks with original rules.  This ensures a deterministic and\npermanent departure with the original rules.\n\nThe use of Discount Governors to control the pace of the increase will\nresult in a predictable and stable increase over the period of three (3)\nyears.\n\nIf, at any time, the increases present problems for the network -- such as\ncentralization concerns, negative impacts on the fee market(s), or other\nunforeseen problems -- a softfork could be leveraged to halt the increases.\n\nThe pace of the increases is described using the following table:\n\nTime -- Base Size (bytes) -- Total Discount -- Total Block Weight (bytes)\nFlag Day (FD) -- 2,000,000 -- 0.00% -- 2,000,000\nFD+24,192 Blocks -- 2,000,000 -- 12.5% -- 2,285,715\nFD+48,384 Blocks -- 2,000,000 -- 25.0% -- 2,666,667\nFD+72,576 Blocks -- 2,000,000 -- 37.5% -- 3,200,000\nFD+96,768 Blocks -- 2,000,000 -- 50.0% -- 4,000,000\nFD+120,960 Blocks -- 2,000,000 -- 62.5% -- 5,333,334\nFD+145,152 Blocks -- 2,000,000 -- 75.0% -- 8,000,000\n\nBased on the above, the \"effective blocksize increase,\" or the number of\ntransactions per block, will also scale with each Discount increase.\n\n\nCompatibility\n\nThis proposal requires a hardfork that does not maintain compatibility with\nprevious clients and rules for consensus.  It should not be deployed\nwithout widespread consensus.\n\nWallet software and other applications will also need to be upgraded to\nmaintain compatibility.\n\nThe hardfork Flag Day will need to be coordinated/determined during the\ndevelopment and testing stages for the hardfork \u2013 estimated at 9-12 months\nto ensure a safe rollout of the hardfork to all network participants.\n\n\nReference implementation\n\nTBD\n\n\nCopyright\n\nThis document is placed in the public domain.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170405/a8eeaf5b/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Base Size Increase and Segregated Witness with Discount Governors (SW-DGov) Hardfork",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Oliver Petruzel"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 6533
        }
    },
    {
        "title": "[bitcoin-dev] A different approach to define and understand softforks and hardforks",
        "thread_messages": [
            {
                "author": "Johnson Lau",
                "date": "2017-04-05T10:28:07",
                "message_text_only": "Softforks and hardforks are usually defined in terms of block validity (BIP99): making valid blocks invalid is a softfork, making invalid blocks valid is a hardfork, and SFs are usually considered as less disruptive as it is considered to be \u201copt-in\u201d. However, as shown below this technical definition could be very misleading. Here I\u2019m trying to redefine the terminology in terms of software upgrade necessity and difficulty.\n\nSoftforks are defined as consensus rule changes that non-upgraded software will be able to function exactly as usual, as if the rule changes have never happened\n\nHardforks are defined as consensus rule changes that non-upgraded software will cease to function or be severely handicapped\n\nSFs and HFs under this definitions is a continuum, which I call it \u201chardfork-ness\u201d. A pure softfork has no hardfork-ness.\n\n*Mining node\n\nUnder this definitions, for miners, any trivial consensus rule changes is somewhat a hardfork, as miners can\u2019t reliably use non-upgraded software to create blocks. However, there is still 3 levels of \u201chardfork-ness\u201d, for example:\n\n1. Those with lower hardfork-ness would be the SFs that miners do not need to upgrade their software at all. Instead, the minimum requirement is to setup a boarder node with latest rules to make sure they won\u2019t mine on top of an invalid block. Examples include CSV and Segwit\n\n2. Some SFs have higher hardfork-ness, for example BIP65 and BIP66. The minimum actions needed include setting up a boarder node and change the block version. BIP34 has even higher hardfork-ness as more actions are needed to follow the new consensus.\n\n3. Anything else, ranging from simple HFs like BIP102 to complete HFs like spoonnet, or soft-hardfork like forcenet, have the highest hardfork-ness. In these cases, boarder nodes are completely useless. Miners have to upgrade their servers in order to stay with the consensus.\n\n*Non-mining full node\n\nSimilarly, in terms of non-mining full node, as the main function is to fully-validate all applicable rules on the network, any consensus change is a hardfork for this particular function. However, a technical SF would have much lower hardfork-ness than a HF, as a border node is everything needed in a SF. Just consider a company has some difficult-to-upgrade software that depends on Bitcoin Core 0.8. Using a 0.13.1+ boarder node will make sure they will always follow the latest rules. In case of a HF, they have no choice but to upgrade the backend system.\n\nSo we may use the costs of running a boarder node to further define the hardfork-ness of SFs, and it comes to the additional resources needed:\n\n1. Things like BIP34, 65, 66, and CSV involves trivial resources use so they have lowest hardfork-ness.\n\n2. Segwit is higher because of increased block size.\n\n3. Extension block has very high hardfork-ness as people may not have enough resources to run a boarder node.\n\n* Fully validating wallets\n\nIn terms of the wallet function in full node, without considering the issues of validation, the hardfork-ness could be ranked as below:\n\n1. BIP34, 65, 66, CSV, segwit all have no hardfork-ness for wallets. Non-upgraded wallets will work exactly in the same way as before. Users won\u2019t notice any change at all. (In some cases they may not see a new tx until it has 1 confirmation, but this is a mild issue and 0-conf is unsafe anyway)\n\n2. Extension block, as presented in my January post ( https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013490.html <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013490.html> ), has higher hardfork-ness, as users of legacy wallets may find it difficult to receive payments from upgraded wallet. However, once they got paid, the user experience is same as before\n\n3. Another extension block proposal ( https://github.com/tothemoon-org/extension-blocks <https://github.com/tothemoon-org/extension-blocks> ) has very high hardfork-ness for wallets, as legacy wallets will frequently and suddenly find that incoming and outgoing txs becoming invalid, and need to sign the invalidated txs again, even no one is trying to double spend.\n\n4. Hardfork rule changes have highest hardfork-ness for full node wallets\n\nI\u2019ll explain the issues with extension block in a separate post in details\n\n* Real SPV wallet\n\nThe SPV wallets as proposed by Satoshi should have the ability to fully validate the rules when needed, so they could be somehow seen as fully validating wallets. So far, real SPV wallet is just vapourware.\n\n* Fake SPV wallet, aka light wallet\n\nAll the so-called SPV wallets we have today are fake SPV according to whitepaper definition. Since they validate nothing, the hardfork-ness profile is very different:\n\n1. BIP34, 65, 66, CSV, segwit has no hardfork-ness for light wallets. Block size HF proposals (BIP10x) and Bitcoin Unlimited also have no hardfork-ness (superficially, but not philosophically). Along the same line, even an inflation hardfork has no hardfork-ness for light wallets.\n\n2. Extension block has the same kind of hardfork-ness issue as I mentioned.\n\n3. HFs that deliberately breaks light wallets, such as spoonnet, is a complete hardfork.\n\nWhile some people try to leverage weakness of light wallets, the inability to validate any important rules like block size, double spending, and inflation is a serious vulnerability.\n\n===========\n\nBefore I finish, I\u2019d also like to analyse some other interesting cases.\n\n1. Soft-hardfork: which requires miners to mine empty blocks with 0 reward, and put the tx merkle tree in the legacy coinbase (e.g. https://github.com/luke-jr/bips/blob/bip-mmhf/bip-mmhf.mediawiki <https://github.com/luke-jr/bips/blob/bip-mmhf/bip-mmhf.mediawiki> ). This allows most hardfork-ing changes including block size and inflation. In terms of block validity this is a softfork. But with the definition I presented, soft-hardforks are clearly hardforks for every practical purposes.\n\n2. On-chain KYC, blacklist, account freezing: technically softforks, but all are very disruptive hardforks in terms of user experience.\n\n3. Lightning network and side chains are not consensus rule changes, and they could provide new features without any hardfork-ness.\n\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170405/9085157d/attachment-0001.html>"
            },
            {
                "author": "greg misiorek",
                "date": "2017-04-05T10:41:45",
                "message_text_only": "I'm not an expert to refute this classification, but would love to see those points addressed by those in the know, without resorting to ad hominem, even though I know it's really hard.\n\nthx, gm\n________________________________\nFrom: Johnson Lau via bitcoin-dev<mailto:bitcoin-dev at lists.linuxfoundation.org>\nSent: \u200e4/\u200e5/\u200e2017 6:28 AM\nTo: bitcoin-dev<mailto:bitcoin-dev at lists.linuxfoundation.org>\nSubject: [bitcoin-dev] A different approach to define and understand softforks and hardforks\n\nSoftforks and hardforks are usually defined in terms of block validity (BIP99): making valid blocks invalid is a softfork, making invalid blocks valid is a hardfork, and SFs are usually considered as less disruptive as it is considered to be \u201copt-in\u201d. However, as shown below this technical definition could be very misleading. Here I\u2019m trying to redefine the terminology in terms of software upgrade necessity and difficulty.\n\nSoftforks are defined as consensus rule changes that non-upgraded software will be able to function exactly as usual, as if the rule changes have never happened\n\nHardforks are defined as consensus rule changes that non-upgraded software will cease to function or be severely handicapped\n\nSFs and HFs under this definitions is a continuum, which I call it \u201chardfork-ness\u201d. A pure softfork has no hardfork-ness.\n\n*Mining node\n\nUnder this definitions, for miners, any trivial consensus rule changes is somewhat a hardfork, as miners can\u2019t reliably use non-upgraded software to create blocks. However, there is still 3 levels of \u201chardfork-ness\u201d, for example:\n\n1. Those with lower hardfork-ness would be the SFs that miners do not need to upgrade their software at all. Instead, the minimum requirement is to setup a boarder node with latest rules to make sure they won\u2019t mine on top of an invalid block. Examples include CSV and Segwit\n\n2. Some SFs have higher hardfork-ness, for example BIP65 and BIP66. The minimum actions needed include setting up a boarder node and change the block version. BIP34 has even higher hardfork-ness as more actions are needed to follow the new consensus.\n\n3. Anything else, ranging from simple HFs like BIP102 to complete HFs like spoonnet, or soft-hardfork like forcenet, have the highest hardfork-ness. In these cases, boarder nodes are completely useless. Miners have to upgrade their servers in order to stay with the consensus.\n\n*Non-mining full node\n\nSimilarly, in terms of non-mining full node, as the main function is to fully-validate all applicable rules on the network, any consensus change is a hardfork for this particular function. However, a technical SF would have much lower hardfork-ness than a HF, as a border node is everything needed in a SF. Just consider a company has some difficult-to-upgrade software that depends on Bitcoin Core 0.8. Using a 0.13.1+ boarder node will make sure they will always follow the latest rules. In case of a HF, they have no choice but to upgrade the backend system.\n\nSo we may use the costs of running a boarder node to further define the hardfork-ness of SFs, and it comes to the additional resources needed:\n\n1. Things like BIP34, 65, 66, and CSV involves trivial resources use so they have lowest hardfork-ness.\n\n2. Segwit is higher because of increased block size.\n\n3. Extension block has very high hardfork-ness as people may not have enough resources to run a boarder node.\n\n* Fully validating wallets\n\nIn terms of the wallet function in full node, without considering the issues of validation, the hardfork-ness could be ranked as below:\n\n1. BIP34, 65, 66, CSV, segwit all have no hardfork-ness for wallets. Non-upgraded wallets will work exactly in the same way as before. Users won\u2019t notice any change at all. (In some cases they may not see a new tx until it has 1 confirmation, but this is a mild issue and 0-conf is unsafe anyway)\n\n2. Extension block, as presented in my January post ( https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013490.html ), has higher hardfork-ness, as users of legacy wallets may find it difficult to receive payments from upgraded wallet. However, once they got paid, the user experience is same as before\n\n3. Another extension block proposal ( https://github.com/tothemoon-org/extension-blocks ) has very high hardfork-ness for wallets, as legacy wallets will frequently and suddenly find that incoming and outgoing txs becoming invalid, and need to sign the invalidated txs again, even no one is trying to double spend.\n\n4. Hardfork rule changes have highest hardfork-ness for full node wallets\n\nI\u2019ll explain the issues with extension block in a separate post in details\n\n* Real SPV wallet\n\nThe SPV wallets as proposed by Satoshi should have the ability to fully validate the rules when needed, so they could be somehow seen as fully validating wallets. So far, real SPV wallet is just vapourware.\n\n* Fake SPV wallet, aka light wallet\n\nAll the so-called SPV wallets we have today are fake SPV according to whitepaper definition. Since they validate nothing, the hardfork-ness profile is very different:\n\n1. BIP34, 65, 66, CSV, segwit has no hardfork-ness for light wallets. Block size HF proposals (BIP10x) and Bitcoin Unlimited also have no hardfork-ness (superficially, but not philosophically). Along the same line, even an inflation hardfork has no hardfork-ness for light wallets.\n\n2. Extension block has the same kind of hardfork-ness issue as I mentioned.\n\n3. HFs that deliberately breaks light wallets, such as spoonnet, is a complete hardfork.\n\nWhile some people try to leverage weakness of light wallets, the inability to validate any important rules like block size, double spending, and inflation is a serious vulnerability.\n\n===========\n\nBefore I finish, I\u2019d also like to analyse some other interesting cases.\n\n1. Soft-hardfork: which requires miners to mine empty blocks with 0 reward, and put the tx merkle tree in the legacy coinbase (e.g. https://github.com/luke-jr/bips/blob/bip-mmhf/bip-mmhf.mediawiki ). This allows most hardfork-ing changes including block size and inflation. In terms of block validity this is a softfork. But with the definition I presented, soft-hardforks are clearly hardforks for every practical purposes.\n\n2. On-chain KYC, blacklist, account freezing: technically softforks, but all are very disruptive hardforks in terms of user experience.\n\n3. Lightning network and side chains are not consensus rule changes, and they could provide new features without any hardfork-ness.\n\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170405/8a579a9b/attachment.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2017-04-07T10:14:07",
                "message_text_only": "Random misreadings of your post aside (maybe it's time to moderate this list a bit more again), I think this is a reasonable model, and certainly more terminology/understanding is useful, given I and many others have been making arguments based on these differences.\n\nOne thing you may wish to further include may be that many soft forks do not require any miner upgrade at all due to standardness rules. Eg OP_CSV and SegWit both only require miners upgrade if they wish to receive the additional fees from new transactions using these features.\n\nMatt\n\nOn April 5, 2017 12:28:07 PM GMT+02:00, Johnson Lau via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>Softforks and hardforks are usually defined in terms of block validity\n>(BIP99): making valid blocks invalid is a softfork, making invalid\n>blocks valid is a hardfork, and SFs are usually considered as less\n>disruptive as it is considered to be \u201copt-in\u201d. However, as shown below\n>this technical definition could be very misleading. Here I\u2019m trying to\n>redefine the terminology in terms of software upgrade necessity and\n>difficulty.\n>\n>Softforks are defined as consensus rule changes that non-upgraded\n>software will be able to function exactly as usual, as if the rule\n>changes have never happened\n>\n>Hardforks are defined as consensus rule changes that non-upgraded\n>software will cease to function or be severely handicapped\n>\n>SFs and HFs under this definitions is a continuum, which I call it\n>\u201chardfork-ness\u201d. A pure softfork has no hardfork-ness.\n>\n>*Mining node\n>\n>Under this definitions, for miners, any trivial consensus rule changes\n>is somewhat a hardfork, as miners can\u2019t reliably use non-upgraded\n>software to create blocks. However, there is still 3 levels of\n>\u201chardfork-ness\u201d, for example:\n>\n>1. Those with lower hardfork-ness would be the SFs that miners do not\n>need to upgrade their software at all. Instead, the minimum requirement\n>is to setup a boarder node with latest rules to make sure they won\u2019t\n>mine on top of an invalid block. Examples include CSV and Segwit\n>\n>2. Some SFs have higher hardfork-ness, for example BIP65 and BIP66. The\n>minimum actions needed include setting up a boarder node and change the\n>block version. BIP34 has even higher hardfork-ness as more actions are\n>needed to follow the new consensus.\n>\n>3. Anything else, ranging from simple HFs like BIP102 to complete HFs\n>like spoonnet, or soft-hardfork like forcenet, have the highest\n>hardfork-ness. In these cases, boarder nodes are completely useless.\n>Miners have to upgrade their servers in order to stay with the\n>consensus.\n>\n>*Non-mining full node\n>\n>Similarly, in terms of non-mining full node, as the main function is to\n>fully-validate all applicable rules on the network, any consensus\n>change is a hardfork for this particular function. However, a technical\n>SF would have much lower hardfork-ness than a HF, as a border node is\n>everything needed in a SF. Just consider a company has some\n>difficult-to-upgrade software that depends on Bitcoin Core 0.8. Using a\n>0.13.1+ boarder node will make sure they will always follow the latest\n>rules. In case of a HF, they have no choice but to upgrade the backend\n>system.\n>\n>So we may use the costs of running a boarder node to further define the\n>hardfork-ness of SFs, and it comes to the additional resources needed:\n>\n>1. Things like BIP34, 65, 66, and CSV involves trivial resources use so\n>they have lowest hardfork-ness.\n>\n>2. Segwit is higher because of increased block size.\n>\n>3. Extension block has very high hardfork-ness as people may not have\n>enough resources to run a boarder node.\n>\n>* Fully validating wallets\n>\n>In terms of the wallet function in full node, without considering the\n>issues of validation, the hardfork-ness could be ranked as below:\n>\n>1. BIP34, 65, 66, CSV, segwit all have no hardfork-ness for wallets.\n>Non-upgraded wallets will work exactly in the same way as before. Users\n>won\u2019t notice any change at all. (In some cases they may not see a new\n>tx until it has 1 confirmation, but this is a mild issue and 0-conf is\n>unsafe anyway)\n>\n>2. Extension block, as presented in my January post (\n>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013490.html\n><https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013490.html>\n>), has higher hardfork-ness, as users of legacy wallets may find it\n>difficult to receive payments from upgraded wallet. However, once they\n>got paid, the user experience is same as before\n>\n>3. Another extension block proposal (\n>https://github.com/tothemoon-org/extension-blocks\n><https://github.com/tothemoon-org/extension-blocks> ) has very high\n>hardfork-ness for wallets, as legacy wallets will frequently and\n>suddenly find that incoming and outgoing txs becoming invalid, and need\n>to sign the invalidated txs again, even no one is trying to double\n>spend.\n>\n>4. Hardfork rule changes have highest hardfork-ness for full node\n>wallets\n>\n>I\u2019ll explain the issues with extension block in a separate post in\n>details\n>\n>* Real SPV wallet\n>\n>The SPV wallets as proposed by Satoshi should have the ability to fully\n>validate the rules when needed, so they could be somehow seen as fully\n>validating wallets. So far, real SPV wallet is just vapourware.\n>\n>* Fake SPV wallet, aka light wallet\n>\n>All the so-called SPV wallets we have today are fake SPV according to\n>whitepaper definition. Since they validate nothing, the hardfork-ness\n>profile is very different:\n>\n>1. BIP34, 65, 66, CSV, segwit has no hardfork-ness for light wallets.\n>Block size HF proposals (BIP10x) and Bitcoin Unlimited also have no\n>hardfork-ness (superficially, but not philosophically). Along the same\n>line, even an inflation hardfork has no hardfork-ness for light\n>wallets.\n>\n>2. Extension block has the same kind of hardfork-ness issue as I\n>mentioned.\n>\n>3. HFs that deliberately breaks light wallets, such as spoonnet, is a\n>complete hardfork.\n>\n>While some people try to leverage weakness of light wallets, the\n>inability to validate any important rules like block size, double\n>spending, and inflation is a serious vulnerability.\n>\n>===========\n>\n>Before I finish, I\u2019d also like to analyse some other interesting cases.\n>\n>1. Soft-hardfork: which requires miners to mine empty blocks with 0\n>reward, and put the tx merkle tree in the legacy coinbase (e.g.\n>https://github.com/luke-jr/bips/blob/bip-mmhf/bip-mmhf.mediawiki\n><https://github.com/luke-jr/bips/blob/bip-mmhf/bip-mmhf.mediawiki> ).\n>This allows most hardfork-ing changes including block size and\n>inflation. In terms of block validity this is a softfork. But with the\n>definition I presented, soft-hardforks are clearly hardforks for every\n>practical purposes.\n>\n>2. On-chain KYC, blacklist, account freezing: technically softforks,\n>but all are very disruptive hardforks in terms of user experience.\n>\n>3. Lightning network and side chains are not consensus rule changes,\n>and they could provide new features without any hardfork-ness."
            }
        ],
        "thread_summary": {
            "title": "A different approach to define and understand softforks and hardforks",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Matt Corallo",
                "greg misiorek",
                "Johnson Lau"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 20059
        }
    },
    {
        "title": "[bitcoin-dev] Proof-of-Loss",
        "thread_messages": [
            {
                "author": "Mirelo",
                "date": "2017-04-05T19:12:20",
                "message_text_only": "With the feedback on Proof-of-Loss (always privately to my email), I realized the article was hard to understand for lacking:\n\n* A more explicit definition of transaction rights.\n* An overview of how the algorithm works.\n\nAs an abstract could not contain all that, I wrote an introduction with examples.\n\nI also adopted a suggestion of including the current block height in the proof-of-loss data once I realized:\n\n* Preventing the same proof-of-loss from chaining consecutive blocks was not the purpose of the proof-of-loss context, which did it statistically rather than logically.\n* The presence of that height in the block header made serial chaining easier to enforce, by removing the need to include additional block height information.\n\nWhile revising the algorithm, I made some corrections, mainly to:\n\n* Transaction prioritization (which now uses fees instead of rights).\n* Inactivity fees.\n\nFinally, the new version more aptly derives the design and often has better wording.\n\nThe new text is available at:\n\nhttps://proof-of-loss.money/\n\nMirelo\n\n-------- Original Message --------\nSubject: Proof-of-Loss\nLocal Time: February 4, 2017 10:39 AM\nUTC Time: February 4, 2017 12:39 PM\nFrom: mirelo at deugh-ausgam-valis.com\nTo: bitcoin-dev at lists.linuxfoundation.org <bitcoin-dev at lists.linuxfoundation.org>\n\nAn alternative consensus algorithm to both proof-of-work and proof-of-stake, proof-of-loss addresses all their deficiencies, including the lack of an organic block size limit, the risks of mining centralization, and the \"nothing at stake\" problem:\n\nhttps://proof-of-loss.money/\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170405/de66a17d/attachment.html>"
            },
            {
                "author": "Erik Aronesty",
                "date": "2017-04-06T02:43:18",
                "message_text_only": "Is this the same as proof of burn?\n\nOn Apr 5, 2017 5:28 PM, \"Mirelo via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> With the feedback on Proof-of-Loss (always privately to my email), I\n> realized the article was hard to understand for lacking:\n>\n> * A more explicit definition of transaction rights.\n> * An overview of how the algorithm works.\n>\n> As an abstract could not contain all that, I wrote an introduction with\n> examples.\n>\n> I also adopted a suggestion of including the current block height in the\n> proof-of-loss data once I realized:\n>\n> * Preventing the same proof-of-loss from chaining consecutive blocks was\n> not the purpose of the proof-of-loss context, which did it statistically\n> rather than logically.\n> * The presence of that height in the block header made serial chaining\n> easier to enforce, by removing the need to include additional block height\n> information.\n>\n> While revising the algorithm, I made some corrections, mainly to:\n>\n> * Transaction prioritization (which now uses fees instead of rights).\n> * Inactivity fees.\n>\n> Finally, the new version more aptly derives the design and often has\n> better wording.\n>\n> The new text is available at:\n>\n> https://proof-of-loss.money/\n>\n> Mirelo\n>\n>\n>\n> -------- Original Message --------\n> Subject: Proof-of-Loss\n> Local Time: February 4, 2017 10:39 AM\n> UTC Time: February 4, 2017 12:39 PM\n> From: mirelo at deugh-ausgam-valis.com\n> To: bitcoin-dev at lists.linuxfoundation.org <bitcoin-dev at lists.\n> linuxfoundation.org>\n>\n> An alternative consensus algorithm to both proof-of-work and\n> proof-of-stake, *proof-of-loss* addresses all their deficiencies,\n> including the lack of an organic block size limit, the risks of mining\n> centralization, and the \"nothing at stake\" problem:\n>\n> *https://proof-of-loss.money/ <https://proof-of-loss.money/>*\n>\n>\n>\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170405/d4948d98/attachment.html>"
            },
            {
                "author": "Mirelo",
                "date": "2017-04-06T05:47:17",
                "message_text_only": "Erik,\n\nNo, it is not, but I would like to ask anyone with any feedback on proof-of-loss to please direct it only to my email, or else follow the discussion links on the Proof-of-Loss home page.\n\nThanks,\n\nMirelo\n\n-------- Original Message --------\nSubject: Re: [bitcoin-dev] Proof-of-Loss\nLocal Time: April 5, 2017 11:43 PM\nUTC Time: April 6, 2017 2:43 AM\nFrom: earonesty at gmail.com\nTo: Mirelo <mirelo at deugh-ausgam-valis.com>, Bitcoin Protocol Discussion <bitcoin-dev at lists.linuxfoundation.org>\n\nIs this the same as proof of burn?\n\nOn Apr 5, 2017 5:28 PM, \"Mirelo via bitcoin-dev\" <bitcoin-dev at lists.linuxfoundation.org> wrote:\nWith the feedback on Proof-of-Loss (always privately to my email), I realized the article was hard to understand for lacking:\n\n* A more explicit definition of transaction rights.\n* An overview of how the algorithm works.\n\nAs an abstract could not contain all that, I wrote an introduction with examples.\n\nI also adopted a suggestion of including the current block height in the proof-of-loss data once I realized:\n\n* Preventing the same proof-of-loss from chaining consecutive blocks was not the purpose of the proof-of-loss context, which did it statistically rather than logically.\n* The presence of that height in the block header made serial chaining easier to enforce, by removing the need to include additional block height information.\n\nWhile revising the algorithm, I made some corrections, mainly to:\n\n* Transaction prioritization (which now uses fees instead of rights).\n* Inactivity fees.\n\nFinally, the new version more aptly derives the design and often has better wording.\n\nThe new text is available at:\n\nhttps://proof-of-loss.money/\n\nMirelo\n\n-------- Original Message --------\nSubject: Proof-of-Loss\nLocal Time: February 4, 2017 10:39 AM\nUTC Time: February 4, 2017 12:39 PM\nFrom: mirelo at deugh-ausgam-valis.com\nTo: bitcoin-dev at lists.linuxfoundation.org <bitcoin-dev at lists.linuxfoundation.org>\n\nAn alternative consensus algorithm to both proof-of-work and proof-of-stake, proof-of-loss addresses all their deficiencies, including the lack of an organic block size limit, the risks of mining centralization, and the \"nothing at stake\" problem:\n\nhttps://proof-of-loss.money/\n\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170406/5e53f185/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Proof-of-Loss",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Mirelo",
                "Erik Aronesty"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 6608
        }
    },
    {
        "title": "[bitcoin-dev] BIP proposal: Inhibiting a covert attack on the Bitcoin POW function",
        "thread_messages": [
            {
                "author": "Gregory Maxwell",
                "date": "2017-04-05T21:37:45",
                "message_text_only": "A month ago I was explaining the attack on Bitcoin's SHA2 hashcash which\nis exploited by ASICBOOST and the various steps which could be used to\nblock it in the network if it became a problem.\n\nWhile most discussion of ASICBOOST has focused on the overt method\nof implementing it, there also exists a covert method for using it.\n\nAs I explained one of the approaches to inhibit covert ASICBOOST I\nrealized that my words were pretty much also describing the SegWit\ncommitment structure.\n\nThe authors of the SegWit proposal made a specific effort to not be\nincompatible with any mining system and, in particular, changed the\ndesign at one point to accommodate mining chips with forced payout\naddresses.\n\nHad there been awareness of exploitation of this attack an effort\nwould have been made to avoid incompatibility-- simply to separate\nconcerns.  But the best methods of implementing the covert attack\nare significantly incompatible with virtually any method of\nextending Bitcoin's transaction capabilities; with the notable\nexception of extension blocks (which have their own problems).\n\nAn incompatibility would go a long way to explain some of the\nmore inexplicable behavior from some parties in the mining\necosystem so I began looking for supporting evidence.\n\nReverse engineering of a particular mining chip has demonstrated\nconclusively that ASICBOOST has been implemented\nin hardware.\n\nOn that basis, I offer the following BIP draft for discussion.\nThis proposal does not prevent the attack in general, but only\ninhibits covert forms of it which are incompatible with\nimprovements to the Bitcoin protocol.\n\nI hope that even those of us who would strongly prefer that\nASICBOOST be blocked completely can come together to support\na protective measure that separates concerns by inhibiting\nthe covert use of it that potentially blocks protocol improvements.\n\nThe specific activation height is something I currently don't have\na strong opinion, so I've left it unspecified for the moment.\n\n<pre>\n  BIP: TBD\n  Layer: Consensus\n  Title: Inhibiting a covert attack on the Bitcoin POW function\n  Author: Greg Maxwell <greg at xiph.org>\n  Status: Draft\n  Type: Standards Track\n  Created: 2016-04-05\n  License: PD\n</pre>\n\n==Abstract==\n\nThis proposal inhibits the covert exploitation of a known\nvulnerability in Bitcoin Proof of Work function.\n\nThe key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n\"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this\ndocument are to be interpreted as described in RFC 2119.\n\n==Motivation==\n\nDue to a design oversight the Bitcoin proof of work function has a potential\nattack which can allow an attacking miner to save up-to 30% of their energy\ncosts (though closer to 20% is more likely due to implementation overheads).\n\nTimo Hanke and Sergio Demian Lerner claim to hold a patent on this attack,\nwhich they have so far not licensed for free and open use by the public.\nThey have been marketing their patent licenses under the trade-name\nASICBOOST.  The document takes no position on the validity or enforceability\nof the patent.\n\nThere are two major ways of exploiting the underlying vulnerability: One\nobvious way which is highly detectable and is not in use on the network\ntoday and a covert way which has significant interaction and potential\ninterference with the Bitcoin protocol.  The covert mechanism is not\neasily detected except through its interference with the protocol.\n\nIn particular, the protocol interactions of the covert method can block the\nimplementation of virtuous improvements such as segregated witness.\n\nExploitation of this vulnerability could result in payoff of as much as\n$100 million USD per year at the time this was written (Assuming at\n50% hash-power miner was gaining a 30% power advantage and that mining\nwas otherwise at profit equilibrium).  This could have a phenomenal\ncentralizing effect by pushing mining out of profitability for all\nother participants, and the income from secretly using this\noptimization could be abused to significantly distort the Bitcoin\necosystem in order to preserve the advantage.\n\nReverse engineering of a mining ASIC from a major manufacture has\nrevealed that it contains an undocumented, undisclosed ability\nto make use of this attack. (The parties claiming to hold a\npatent on this technique were completely unaware of this use.)\n\nOn the above basis the potential for covert exploitation of this\nvulnerability and the resulting inequality in the mining process\nand interference with useful improvements presents a clear and\npresent danger to the Bitcoin system which requires a response.\n\n==Background==\n\nThe general idea of this attack is that SHA2-256 is a merkle damgard hash\nfunction which consumes 64 bytes of data at a time.\n\nThe Bitcoin mining process repeatedly hashes an 80-byte 'block header' while\nincriminating a 32-bit nonce which is at the end of this header data. This\nmeans that the processing of the header involves two runs of the compression\nfunction run-- one that consumes the first 64 bytes of the header and a\nsecond which processes the remaining 16 bytes and padding.\n\nThe initial 'message expansion' operations in each step of the SHA2-256\nfunction operate exclusively on that step's 64-bytes of input with no\ninfluence from prior data that entered the hash.\n\nBecause of this if a miner is able to prepare a block header with\nmultiple distinct first 64-byte chunks but identical 16-byte\nsecond chunks they can reuse the computation of the initial\nexpansion for multiple trials. This reduces power consumption.\n\nThere are two broad ways of making use of this attack. The obvious\nway is to try candidates with different version numbers.  Beyond\nupsetting the soft-fork detection logic in Bitcoin nodes this has\nlittle negative effect but it is highly conspicuous and easily\nblocked.\n\nThe other method is based on the fact that the merkle root\ncommitting to the transactions is contained in the first 64-bytes\nexcept for the last 4 bytes of it.  If the miner finds multiple\ncandidate root values which have the same final 32-bit then they\ncan use the attack.\n\nTo find multiple roots with the same trailing 32-bits the miner can\nuse efficient collision finding mechanism which will find a match\nwith as little as 2^16 candidate roots expected, 2^24 operations to\nfind a 4-way hit, though low memory approaches require more\ncomputation.\n\nAn obvious way to generate different candidates is to grind the\ncoinbase extra-nonce but for non-empty blocks each attempt will\nrequire 13 or so additional sha2 runs which is very inefficient.\n\nThis inefficiency can be avoided by computing a sqrt number of\ncandidates of the left side of the hash tree (e.g. using extra\nnonce grinding) then an additional sqrt number of candidates of\nthe right  side of the tree using transaction permutation or\nsubstitution of a small number of transactions.  All combinations\nof the left and right side are then combined with only a single\nhashing operation virtually eliminating all tree related\noverhead.\n\nWith this final optimization finding a 4-way collision with a\nmoderate amount of memory requires ~2^24 hashing operations\ninstead of the >2^28 operations that would be require for\nextra-nonce  grinding which would substantially erode the\nbenefit of the attack.\n\nIt is this final optimization which this proposal blocks.\n\n==New consensus rule==\n\nBeginning block X and until block Y the coinbase transaction of\neach block MUST either contain a BIP-141 segwit commitment or a\ncorrect WTXID commitment with ID 0xaa21a9ef.\n\n(See BIP-141 \"Commitment structure\" for details)\n\nExisting segwit using miners are automatically compatible with\nthis proposal. Non-segwit miners can become compatible by simply\nincluding an additional output matching a default commitment\nvalue returned as part of getblocktemplate.\n\nMiners SHOULD NOT automatically discontinue the commitment\nat the expiration height.\n\n==Discussion==\n\nThe commitment in the left side of the tree to all transactions\nin the right side completely prevents the final sqrt speedup.\n\nA stronger inhibition of the covert attack in the form of\nrequiring the least significant bits of the block timestamp\nto be equal to a hash of the first 64-bytes of the header. This\nwould increase the collision space from 32 to 40 or more bits.\nThe root value could be required to meet a specific hash prefix\nrequirement in order to increase the computational work required\nto try candidate roots. These change would be more disruptive and\nthere is no reason to believe that it is currently necessary.\n\nThe proposed rule automatically sunsets. If it is no longer needed\ndue to the introduction of stronger rules or the acceptance of the\nversion-grinding form then there would be no reason to continue\nwith this requirement.  If it is still useful at the expiration\ntime the rule can simply be extended with a new softfork that\nsets longer date ranges.\n\nThis sun-setting avoids the accumulation of technical debt due\nto retaining enforcement of this rule when it is no longer needed\nwithout requiring a hard fork to remove it.\n\n== Overt attack ==\n\nThe non-covert form can be trivially blocked by requiring that\nthe header version match the coinbase transaction version.\n\nThis proposal does not include this block because this method\nmay become generally available without restriction in the future,\ndoes not generally interfere with improvements in the protocol,\nand because it is so easily detected that it could be blocked if\nit becomes an issue in the future.\n\n==Backward compatibility==\n\n\n==Implementation==\n\n\n==Acknowledgments==\n\n\n==Copyright==\n\nThis document is placed in the public domain."
            },
            {
                "author": "theymos",
                "date": "2017-04-05T23:05:18",
                "message_text_only": "This seems to be a serious security problem.  Would it be possible to have\na flag-day softfork included in Bitcoin Core as soon as 0.14.1? I think that a trigger\n3-6 months from release should be sufficient for enough of the economy to upgrade,\ngiven the severity of the issue.\n\nBIP 141 says that the the commitment is optional if there are no SegWit transactions in\nthe block,  so will today's SegWit-ready miners always produce it even when optional\naccording to BIP 141, as required by this softfork?\n\nOn Wed, Apr 5, 2017, at 04:37 PM, Gregory Maxwell via bitcoin-dev wrote:\n> A month ago I was explaining the attack on Bitcoin's SHA2 hashcash which\n> is exploited by ASICBOOST and the various steps which could be used to\n> block it in the network if it became a problem.\n> \n> While most discussion of ASICBOOST has focused on the overt method\n> of implementing it, there also exists a covert method for using it.\n> \n> As I explained one of the approaches to inhibit covert ASICBOOST I\n> realized that my words were pretty much also describing the SegWit\n> commitment structure.\n> \n> The authors of the SegWit proposal made a specific effort to not be\n> incompatible with any mining system and, in particular, changed the\n> design at one point to accommodate mining chips with forced payout\n> addresses.\n> \n> Had there been awareness of exploitation of this attack an effort\n> would have been made to avoid incompatibility-- simply to separate\n> concerns.  But the best methods of implementing the covert attack\n> are significantly incompatible with virtually any method of\n> extending Bitcoin's transaction capabilities; with the notable\n> exception of extension blocks (which have their own problems).\n> \n> An incompatibility would go a long way to explain some of the\n> more inexplicable behavior from some parties in the mining\n> ecosystem so I began looking for supporting evidence.\n> \n> Reverse engineering of a particular mining chip has demonstrated\n> conclusively that ASICBOOST has been implemented\n> in hardware.\n> \n> On that basis, I offer the following BIP draft for discussion.\n> This proposal does not prevent the attack in general, but only\n> inhibits covert forms of it which are incompatible with\n> improvements to the Bitcoin protocol.\n> \n> I hope that even those of us who would strongly prefer that\n> ASICBOOST be blocked completely can come together to support\n> a protective measure that separates concerns by inhibiting\n> the covert use of it that potentially blocks protocol improvements.\n> \n> The specific activation height is something I currently don't have\n> a strong opinion, so I've left it unspecified for the moment.\n> \n> <pre>\n>   BIP: TBD\n>   Layer: Consensus\n>   Title: Inhibiting a covert attack on the Bitcoin POW function\n>   Author: Greg Maxwell <greg at xiph.org>\n>   Status: Draft\n>   Type: Standards Track\n>   Created: 2016-04-05\n>   License: PD\n> </pre>\n> \n> ==Abstract==\n> \n> This proposal inhibits the covert exploitation of a known\n> vulnerability in Bitcoin Proof of Work function.\n> \n> The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n> \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this\n> document are to be interpreted as described in RFC 2119.\n> \n> ==Motivation==\n> \n> Due to a design oversight the Bitcoin proof of work function has a potential\n> attack which can allow an attacking miner to save up-to 30% of their energy\n> costs (though closer to 20% is more likely due to implementation overheads).\n> \n> Timo Hanke and Sergio Demian Lerner claim to hold a patent on this attack,\n> which they have so far not licensed for free and open use by the public.\n> They have been marketing their patent licenses under the trade-name\n> ASICBOOST.  The document takes no position on the validity or enforceability\n> of the patent.\n> \n> There are two major ways of exploiting the underlying vulnerability: One\n> obvious way which is highly detectable and is not in use on the network\n> today and a covert way which has significant interaction and potential\n> interference with the Bitcoin protocol.  The covert mechanism is not\n> easily detected except through its interference with the protocol.\n> \n> In particular, the protocol interactions of the covert method can block the\n> implementation of virtuous improvements such as segregated witness.\n> \n> Exploitation of this vulnerability could result in payoff of as much as\n> $100 million USD per year at the time this was written (Assuming at\n> 50% hash-power miner was gaining a 30% power advantage and that mining\n> was otherwise at profit equilibrium).  This could have a phenomenal\n> centralizing effect by pushing mining out of profitability for all\n> other participants, and the income from secretly using this\n> optimization could be abused to significantly distort the Bitcoin\n> ecosystem in order to preserve the advantage.\n> \n> Reverse engineering of a mining ASIC from a major manufacture has\n> revealed that it contains an undocumented, undisclosed ability\n> to make use of this attack. (The parties claiming to hold a\n> patent on this technique were completely unaware of this use.)\n> \n> On the above basis the potential for covert exploitation of this\n> vulnerability and the resulting inequality in the mining process\n> and interference with useful improvements presents a clear and\n> present danger to the Bitcoin system which requires a response.\n> \n> ==Background==\n> \n> The general idea of this attack is that SHA2-256 is a merkle damgard hash\n> function which consumes 64 bytes of data at a time.\n> \n> The Bitcoin mining process repeatedly hashes an 80-byte 'block header' while\n> incriminating a 32-bit nonce which is at the end of this header data. This\n> means that the processing of the header involves two runs of the compression\n> function run-- one that consumes the first 64 bytes of the header and a\n> second which processes the remaining 16 bytes and padding.\n> \n> The initial 'message expansion' operations in each step of the SHA2-256\n> function operate exclusively on that step's 64-bytes of input with no\n> influence from prior data that entered the hash.\n> \n> Because of this if a miner is able to prepare a block header with\n> multiple distinct first 64-byte chunks but identical 16-byte\n> second chunks they can reuse the computation of the initial\n> expansion for multiple trials. This reduces power consumption.\n> \n> There are two broad ways of making use of this attack. The obvious\n> way is to try candidates with different version numbers.  Beyond\n> upsetting the soft-fork detection logic in Bitcoin nodes this has\n> little negative effect but it is highly conspicuous and easily\n> blocked.\n> \n> The other method is based on the fact that the merkle root\n> committing to the transactions is contained in the first 64-bytes\n> except for the last 4 bytes of it.  If the miner finds multiple\n> candidate root values which have the same final 32-bit then they\n> can use the attack.\n> \n> To find multiple roots with the same trailing 32-bits the miner can\n> use efficient collision finding mechanism which will find a match\n> with as little as 2^16 candidate roots expected, 2^24 operations to\n> find a 4-way hit, though low memory approaches require more\n> computation.\n> \n> An obvious way to generate different candidates is to grind the\n> coinbase extra-nonce but for non-empty blocks each attempt will\n> require 13 or so additional sha2 runs which is very inefficient.\n> \n> This inefficiency can be avoided by computing a sqrt number of\n> candidates of the left side of the hash tree (e.g. using extra\n> nonce grinding) then an additional sqrt number of candidates of\n> the right  side of the tree using transaction permutation or\n> substitution of a small number of transactions.  All combinations\n> of the left and right side are then combined with only a single\n> hashing operation virtually eliminating all tree related\n> overhead.\n> \n> With this final optimization finding a 4-way collision with a\n> moderate amount of memory requires ~2^24 hashing operations\n> instead of the >2^28 operations that would be require for\n> extra-nonce  grinding which would substantially erode the\n> benefit of the attack.\n> \n> It is this final optimization which this proposal blocks.\n> \n> ==New consensus rule==\n> \n> Beginning block X and until block Y the coinbase transaction of\n> each block MUST either contain a BIP-141 segwit commitment or a\n> correct WTXID commitment with ID 0xaa21a9ef.\n> \n> (See BIP-141 \"Commitment structure\" for details)\n> \n> Existing segwit using miners are automatically compatible with\n> this proposal. Non-segwit miners can become compatible by simply\n> including an additional output matching a default commitment\n> value returned as part of getblocktemplate.\n> \n> Miners SHOULD NOT automatically discontinue the commitment\n> at the expiration height.\n> \n> ==Discussion==\n> \n> The commitment in the left side of the tree to all transactions\n> in the right side completely prevents the final sqrt speedup.\n> \n> A stronger inhibition of the covert attack in the form of\n> requiring the least significant bits of the block timestamp\n> to be equal to a hash of the first 64-bytes of the header. This\n> would increase the collision space from 32 to 40 or more bits.\n> The root value could be required to meet a specific hash prefix\n> requirement in order to increase the computational work required\n> to try candidate roots. These change would be more disruptive and\n> there is no reason to believe that it is currently necessary.\n> \n> The proposed rule automatically sunsets. If it is no longer needed\n> due to the introduction of stronger rules or the acceptance of the\n> version-grinding form then there would be no reason to continue\n> with this requirement.  If it is still useful at the expiration\n> time the rule can simply be extended with a new softfork that\n> sets longer date ranges.\n> \n> This sun-setting avoids the accumulation of technical debt due\n> to retaining enforcement of this rule when it is no longer needed\n> without requiring a hard fork to remove it.\n> \n> == Overt attack ==\n> \n> The non-covert form can be trivially blocked by requiring that\n> the header version match the coinbase transaction version.\n> \n> This proposal does not include this block because this method\n> may become generally available without restriction in the future,\n> does not generally interfere with improvements in the protocol,\n> and because it is so easily detected that it could be blocked if\n> it becomes an issue in the future.\n> \n> ==Backward compatibility==\n> \n> \n> ==Implementation==\n> \n> \n> ==Acknowledgments==\n> \n> \n> ==Copyright==\n> \n> This document is placed in the public domain.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2017-04-06T00:17:17",
                "message_text_only": "On Wed, Apr 5, 2017 at 11:05 PM, theymos via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> This seems to be a serious security problem.  Would it be possible to have\n> a flag-day softfork included in Bitcoin Core as soon as 0.14.1? I think that a trigger\n> 3-6 months from release should be sufficient for enough of the economy to upgrade,\n> given the severity of the issue.\n\nNot 0.14.1 because that is in RC already and will hopefully be out in a week.\n\nI think the speed of adoption depends a lot of the level of support\nfrom the community. I don't believe there are any technical hurdles to\nimplementing this relatively quickly (and I specifically propose using\nthe users choice of the segwit commitment or a modified form in order\nto lower the technical complexity and risk).\n\n> BIP 141 says that the the commitment is optional if there are no SegWit transactions in\n> the block,  so will today's SegWit-ready miners always produce it even when optional\n> according to BIP 141, as required by this softfork?\n\nThis is the default behavior as of 0.13.2, but I haven't gone out to\nmeasure this which is why the backwards compatibility section of the\nBIP isn't written yet.\n\n\nWhile I'm posting, I've had a dozen off-list emails that presented me\nwith some FAQ:\n\nMany people asked what other protocol upgrades beyond segwit could run\ninto the same incompatibility.\n\nMany proposed improvements to Bitcoin require additional\ntransaction-dependent commitment data.\n\nExamples include:\n\n(1) Segwit.\n(2) UTXO commitments. (non-delayed, at least)\n(3) Committed Bloom filters\n(4) Committed address indexes\n(5) STXO commitments (non-delayed).\n(6) Weak blocks\n(7) Most kinds of fraud proofs\n-- to state a few.\n\nUnfortunately, putting *any* commitment to data dependent on the right\nhand side of the hash tree in the left hand side (e.g. coinbase) means\na massive increase in the computation required for covert boosting,\nbecause it means you can't use the left+right side combinations to\neliminate most of the hashing.\n\nIt's plausible, in fact, that this extra computation could completely\nnullify the ASICBOOST advantage-- though this depends a lot on the\nfine details of the implementation.\n\nThis proposal does not itself propose nullifying ASICBOOST entirely,\nit proposes severely handicapping the covert form of it, and\neliminating the differential advantage for boosting miners related to\nthe use of transaction-dependent commitments.\n\nBasically there are two completely separate concerns: that boosting\ncan produce a monopoly advantage which could be severely harmful to\nthe ecosystem, and that the efficient implementation of _covert_\nboosting can severely harm many useful protocol improvements.   My\nproposal only addresses the second concern, by (I believe) completely\nleveling the playing field so that opposing commitments will not break\nboosting any worse, and by making covert boosting less appealing in\ngeneral.\n\nUse of the segwit-style commitment even in non-segwit blocks is sufficient\nbecause the segwit commitment commits to all  transactions  (except\nthe coinbase) and not just segwit ones.\n(It was designed this way so that lite clients that needed witness\ndata could work with just one tree)."
            },
            {
                "author": "Joseph Poon",
                "date": "2017-04-06T00:39:00",
                "message_text_only": "#bitcoin at freenode:\n 00:04    gmaxwell| lol poon pretending that he isn't complicit in all this stuff.\n\nAre you *fucking* serious? Is this how you resolve all problems? I'm\ntaking you seriously and having second thoughts and want to make public\ncommitments to do the right thing without any evidence and you come out\nand say *this*?\n\nOn Thu, Apr 06, 2017 at 12:17:17AM +0000, Gregory Maxwell via bitcoin-dev wrote:\n> On Wed, Apr 5, 2017 at 11:05 PM, theymos via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > This seems to be a serious security problem.  Would it be possible to have\n> > a flag-day softfork included in Bitcoin Core as soon as 0.14.1? I think that a trigger\n> > 3-6 months from release should be sufficient for enough of the economy to upgrade,\n> > given the severity of the issue.\n> \n> Not 0.14.1 because that is in RC already and will hopefully be out in a week.\n> \n> I think the speed of adoption depends a lot of the level of support\n> from the community. I don't believe there are any technical hurdles to\n> implementing this relatively quickly (and I specifically propose using\n> the users choice of the segwit commitment or a modified form in order\n> to lower the technical complexity and risk).\n> \n> > BIP 141 says that the the commitment is optional if there are no SegWit transactions in\n> > the block,  so will today's SegWit-ready miners always produce it even when optional\n> > according to BIP 141, as required by this softfork?\n> \n> This is the default behavior as of 0.13.2, but I haven't gone out to\n> measure this which is why the backwards compatibility section of the\n> BIP isn't written yet.\n> \n> \n> While I'm posting, I've had a dozen off-list emails that presented me\n> with some FAQ:\n> \n> Many people asked what other protocol upgrades beyond segwit could run\n> into the same incompatibility.\n> \n> Many proposed improvements to Bitcoin require additional\n> transaction-dependent commitment data.\n> \n> Examples include:\n> \n> (1) Segwit.\n> (2) UTXO commitments. (non-delayed, at least)\n> (3) Committed Bloom filters\n> (4) Committed address indexes\n> (5) STXO commitments (non-delayed).\n> (6) Weak blocks\n> (7) Most kinds of fraud proofs\n> -- to state a few.\n> \n> Unfortunately, putting *any* commitment to data dependent on the right\n> hand side of the hash tree in the left hand side (e.g. coinbase) means\n> a massive increase in the computation required for covert boosting,\n> because it means you can't use the left+right side combinations to\n> eliminate most of the hashing.\n> \n> It's plausible, in fact, that this extra computation could completely\n> nullify the ASICBOOST advantage-- though this depends a lot on the\n> fine details of the implementation.\n> \n> This proposal does not itself propose nullifying ASICBOOST entirely,\n> it proposes severely handicapping the covert form of it, and\n> eliminating the differential advantage for boosting miners related to\n> the use of transaction-dependent commitments.\n> \n> Basically there are two completely separate concerns: that boosting\n> can produce a monopoly advantage which could be severely harmful to\n> the ecosystem, and that the efficient implementation of _covert_\n> boosting can severely harm many useful protocol improvements.   My\n> proposal only addresses the second concern, by (I believe) completely\n> leveling the playing field so that opposing commitments will not break\n> boosting any worse, and by making covert boosting less appealing in\n> general.\n> \n> Use of the segwit-style commitment even in non-segwit blocks is sufficient\n> because the segwit commitment commits to all  transactions  (except\n> the coinbase) and not just segwit ones.\n> (It was designed this way so that lite clients that needed witness\n> data could work with just one tree).\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n-- \nJoseph Poon"
            },
            {
                "author": "Joseph Poon",
                "date": "2017-04-06T00:40:26",
                "message_text_only": "Ahh, sorry all for this public message. :(\n\nOn Wed, Apr 05, 2017 at 05:39:00PM -0700, Joseph Poon wrote:\n> #bitcoin at freenode:\n>  00:04    gmaxwell| lol poon pretending that he isn't complicit in all this stuff.\n> \n> Are you *fucking* serious? Is this how you resolve all problems? I'm\n> taking you seriously and having second thoughts and want to make public\n> commitments to do the right thing without any evidence and you come out\n> and say *this*?\n\n-- \nJoseph Poon"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2017-04-06T01:32:03",
                "message_text_only": "On Thu, Apr 6, 2017 at 12:39 AM, Joseph Poon <joseph at lightning.network> wrote:\n> #bitcoin at freenode:\n>  00:04    gmaxwell| lol poon pretending that he isn't complicit in all this stuff.\n>\n> Are you *fucking* serious? Is this how you resolve all problems? I'm\n> taking you seriously and having second thoughts and want to make public\n> commitments to do the right thing without any evidence and you come out\n> and say *this*?\n\nI apologize for the glib talk on chat and I hope you understand that\nthe tone in such venues is significantly informal; and that my remark\nwas a causal one among friends which was not intended in a spirit as\nseriously as you've taken it.\n\nThat said, two days ago you participated in a highly unusual\nannouncement of a protocol change that-- rather than being sent for\ncommunity review in any plausible venue for that purpose-- was\nannounced as a done deal in embargoed media announcements.  This\nproposed protocol change seemed custom tailored to preserve covert\nboosting, and incorporated direct support for lightning -- and the\nleading competing theory was that a large miner opposed segwit\nspecifically because they wanted to block lightning. Moreover, I have\nheard reports I consider reliable that this work was funded by the\nminer in question.\n\nIn the time since, when people asked for revisions to the proposal to\nnot block segwit they received responses from the Bcoin account on\ntwitter that \"there would be no amendments\", and I was sent leaked\nchatlogs of you making considerably hostile statements, claiming that\nif your extension block proposal is \"a litmus test for corruption\",\nand claimed (before AFAIK anyone had had a chance to comment on it)\nthat the Bitcoin project contributors opposed it for \"nonsense\nreasons\".\n\nIt is with this in mind that when you tried to pull me into an off the\nrecord conversation that I responded stating:\n\n\"[...] I am disinclined to communicate with you except in email where I can\nget third party transferable proof of our communication.  I'm\nconcerned that you may now be involved in a conspiracy which I do not\nwant to be implicated in myself.\n\nIt is my estimation that, for that above reason, it would be in my\nbest interest to not communicate with you at all.  But in all your\nprior interactions you appeared to have integrity and sense, so out of\nrespect for that history I'm willing to communicate with you, but only\nin public or in email where my end is on gmail.\"\n\nThis was two days ago and you did not respond further.\n\nWith that in mind I hope you do not find some casual crap-talking on\nchat to be especially surprising.\n\nI understand that you didn't intend for the initial message to be\nposted in public, so I'm sorry for continuing the thread here-- but I\nthought it was useful for people to understand the context behind that\nglib remark: Including the point that I do not know for a fact that\nyou are complicit in anything, but I consider your recent actions to\nbe highly concerning."
            },
            {
                "author": "Joseph Poon",
                "date": "2017-04-06T02:09:49",
                "message_text_only": "On Thu, Apr 06, 2017 at 01:32:03AM +0000, Gregory Maxwell wrote:\n> On Thu, Apr 6, 2017 at 12:39 AM, Joseph Poon <joseph at lightning.network> wrote:\n> > #bitcoin at freenode:\n> >  00:04    gmaxwell| lol poon pretending that he isn't complicit in all this stuff.\n> >\n> > Are you *fucking* serious? Is this how you resolve all problems? I'm\n> > taking you seriously and having second thoughts and want to make public\n> > commitments to do the right thing without any evidence and you come out\n> > and say *this*?\n\nApologies to the list.\n\n> I apologize for the glib talk on chat and I hope you understand that\n> the tone in such venues is significantly informal; and that my remark\n> was a causal one among friends which was not intended in a spirit as\n> seriously as you've taken it.\n\nYou're still presuming ill-will. I'm seriously offended. I'm not upset\nwith the glib talk, I'm upset that you think I have ill will.\n\n> That said, two days ago you participated in a highly unusual\n> announcement of a protocol change that-- rather than being sent for\n> community review in any plausible venue for that purpose-- was\n> announced as a done deal in embargoed media announcements.  This\n> proposed protocol change seemed custom tailored to preserve covert\n> boosting, and incorporated direct support for lightning -- and the\n> leading competing theory was that a large miner opposed segwit\n> specifically because they wanted to block lightning. Moreover, I have\n> heard reports I consider reliable that this work was funded by the\n> miner in question.\n\nWe specifically told you guys privately and publicly when asked that it\nwas simply to be able to do it in 2 weeks. Check out the code, it was\nmuch faster to do it that way. The spec wasn't complete and I have\npersonal biases against doing it on the main-chain since it would\nbenefit things if there was smart contract proections on the main chain\nas well, which I figured would be more controversial. I never said\nanything about public commitments to transactions. In fact, I'm pretty\ngood at figuring things out and tend to cargo-cult things (since culture\nis the genetic memory is civlizations), if I saw BIP141/SegWit required\na commitment instead of it being optional, I would've probably thought\nabout it. Why wasn't this required as part of SegWit? BIP141 is still\nvulnerable. Why did you pull this out just now? I'm totally blindsided\nhere, hence my earlier reply of wanting to resolve it in the Extension\nBlock proposal.\n\n> In the time since, when people asked for revisions to the proposal to\n> not block segwit they received responses from the Bcoin account on\n> twitter that \"there would be no amendments\", and I was sent leaked\n> chatlogs of you making considerably hostile statements, claiming that\n> if your extension block proposal is \"a litmus test for corruption\",\n> and claimed (before AFAIK anyone had had a chance to comment on it)\n> that the Bitcoin project contributors opposed it for \"nonsense\n> reasons\".\n\nI never participated in that, and the specific announcement here\nindicates that changes will be happening. The intention was to get it\nout as a draft and *working* demo code.\n\nhttps://medium.com/purse-essays/ready-for-liftoff-a5533f4de0b6\n\nThat was specifically after Core developers accused me of publicly\nacting in poor form without any understanding of the situation. I was\nespecially annoyed because all of you are acting with similar secrecy,\neven worse, there is specific organization by Core which the public is\nnot aware of. Think about it from my perspective, you all blocked me out\nintentionally for months and then accuse me of going to journalists for\na couple hours before? I'm seriously hurt.\n\n> It is with this in mind that when you tried to pull me into an off the\n> record conversation that I responded stating:\n> \n> \"[...] I am disinclined to communicate with you except in email where I can\n> get third party transferable proof of our communication.  I'm\n> concerned that you may now be involved in a conspiracy which I do not\n> want to be implicated in myself.\n> \n> It is my estimation that, for that above reason, it would be in my\n> best interest to not communicate with you at all.  But in all your\n> prior interactions you appeared to have integrity and sense, so out of\n> respect for that history I'm willing to communicate with you, but only\n> in public or in email where my end is on gmail.\"\n\nNice you cut out the beginning which explains on *why* I didn't reply:\n\n\"with an embargoed press release in Forbes.\n\nThat's how you roll now, right? :-/\"\n\nWhy didn't you include your entire message?\n\nThat was in reply to my initial message reaching out to you and Adam\nBack:\n\"Hi, would you like a phone call tomorrow?\n\nI am in Thailand right now, I understand if what I did is upsetting, my\ngoal was not to upset you.\n\nI deeply respect you both technically, but I do believe what I am doing\nis right. If you could find a way, I would be extremely grateful if we\ncould chat sometime.\"\n\nReplying with a beginning like that with that kind of hostility means I\nsort of don't know how to reply! Further, you didn't express any real\nconcerns to me. I just figured you were mad and wanted to give you time\nto cool off. Calling someone up is a way to explain over a higher\nbandwidth medium gives material reiteration of a real honest heartfelt\napology in misunderstanding.\n\n> This was two days ago and you did not respond further.\n> \n> With that in mind I hope you do not find some casual crap-talking on\n> chat to be especially surprising.\n> \n> I understand that you didn't intend for the initial message to be\n> posted in public, so I'm sorry for continuing the thread here-- but I\n> thought it was useful for people to understand the context behind that\n> glib remark: Including the point that I do not know for a fact that\n> you are complicit in anything, but I consider your recent actions to\n> be highly concerning.\n\nI'm only including more details in the email because you had deceptive\nframing. I normally would *never* include contents in a private email\nmessage and believe this is already the gray area. I already feel\nuncomfortable publishing my message to you without permission, but I\nfeel it's necessary context, but I will not continue. Would you like to\nhave a public call instead? I really want to talk to you to express that\nI really mean what's best for bitcoin. I've had a sleepless night\nthinking about these things, this type of drama is *NOT* good for\nbitcoin.\n\nI came here with good intent, even with Core and Blockstream being\noutright hostile and controlling with many personal problems over the\nyears which I have never aired previously. I can tell when I'm not\nwelcome. I'm going to take a break from all of this.\n\n-- \nJoseph Poon"
            },
            {
                "author": "Anthony Towns",
                "date": "2017-04-05T23:25:41",
                "message_text_only": "On Wed, Apr 05, 2017 at 09:37:45PM +0000, Gregory Maxwell via bitcoin-dev wrote:\n> The Bitcoin mining process repeatedly hashes an 80-byte 'block header' while\n> incriminating a 32-bit nonce \n\nThat should probably be \"incrementing\"...\n\nCheers,\naj"
            },
            {
                "author": "Joseph Poon",
                "date": "2017-04-05T23:42:41",
                "message_text_only": "Hi Greg,\n\nOn Wed, Apr 05, 2017 at 09:37:45PM +0000, Gregory Maxwell via bitcoin-dev wrote:\n> Reverse engineering of a particular mining chip has demonstrated\n> conclusively that ASICBOOST has been implemented\n> in hardware.\n> \n> On that basis, I offer the following BIP draft for discussion.\n> This proposal does not prevent the attack in general, but only\n> inhibits covert forms of it which are incompatible with\n> improvements to the Bitcoin protocol.\n> \n> I hope that even those of us who would strongly prefer that\n> ASICBOOST be blocked completely can come together to support\n> a protective measure that separates concerns by inhibiting\n> the covert use of it that potentially blocks protocol improvements.\n> \n> [...]\n> \n> ==New consensus rule==\n> \n> Beginning block X and until block Y the coinbase transaction of\n> each block MUST either contain a BIP-141 segwit commitment or a\n> correct WTXID commitment with ID 0xaa21a9ef.\n> \n> (See BIP-141 \"Commitment structure\" for details)\n> \n> Existing segwit using miners are automatically compatible with\n> this proposal. Non-segwit miners can become compatible by simply\n> including an additional output matching a default commitment\n> value returned as part of getblocktemplate.\n> \n> Miners SHOULD NOT automatically discontinue the commitment\n> at the expiration height.\n\nDecentralized systems without patent encumbrance is an important topic\nfor me. We'd be very interested in adding this into extension blocks.\n\nClaims like these merit serious attention. If you can provide any kind\nof proof or documentation of this (doesn't need to be conclusive, just\nsomething), I will provide my word and promise publicly here and now\nthat I will personally see to it that a commitment which solves this\n(albeit possibly using a slightly different format to make it\ncompatible) is added into the Extension Blocks spec. If there is\nevidence, my support and authorship of the Extension Block specification\nis contingent upon resolving this issue.\n\nWe have added an issue here:\nhttps://github.com/tothemoon-org/extension-blocks/issues/6\n\nI'm interested in a more detailed explanation on how the Merle tree\nstructure works so we can add it to the spec, I didn't follow exactly\nthe new consensus rule and its mechanism in those several lines.\n\nWe will begin making a pull request adding it into our specification,\nbut more clarity on how to do it on its own would be helpful. We will\nalso consider the code exposure change to adding in SegWit on the\nCanonical/1MB chain if it is more elegant to implement.\n\nPackaging this into our proposal would not only be important, but\nhelpful to the end goals of this proposal as it becomes a standard\nsoft-fork consensus rule which has greater guarantees around\nenforcibility than user-actication.\n\nFurther, can you provide clarity and confirmation into why this\ncommitment wasn't required as part of SegWit? \n\n-- \nJoseph Poon"
            },
            {
                "author": "Jonathan Toomim",
                "date": "2017-04-06T02:10:27",
                "message_text_only": "Just checking to see if I understand this optimization correctly. In order to find merkle roots in which the rightmost 32 bits are identical (i.e. partial hash collisions), we want to compute as many merkle root hashes as quickly as possible. The fastest way to do this is to take the top level of the Merkle tree, and to collect a set of left branches and right branches which can be independently manipulated. While the left branch can easily be manipulated by changing the extranonce in the coinbase transaction, the right branch would need to be modified by changing one of the transactions in the right branch or by changing the number of transactions in the right branch. Correct so far?\n\nWith the stratum mining protocol, the server (the pool) includes enough information for the coinbase transaction to be modified by stratum client (the miner), but it does not include any information about the right side of the merkle tree except for the top-level hash. Stratum also does not allow the client to supply any modifications to the merkle tree (including the right side) back to the stratum server. This means that any implementation of this final optimization would need to be using a protocol other than stratum, like getblocktemplate, correct?\n\nI think it would be helpful for the discussion to know if this optimization were currently being used or not, and if so, how widely.\n\nAll of the consumer-grade hardware that I have seen defaults to stratum-only operation, and I have not seen or heard of any hardware available that can run more efficiently using getblocktemplate. As the current pool infrastructure uses stratum exclusively, this optimization would require significant retooling among pools, and probably a redesign of their core algorithms to help discover and share these partial collisions more frequently. It's possible that some large private farms have deployed a special system for solo mining that uses this optimization, of course, but it's also possible that there's a teapot in space somewhere between the orbit of Earth and Mars.\n\nDo you know of any ways to perform this optimization via stratum? If not, do you have any evidence that this optimization is actually being used by private solo mining farms? Or is this discussion purely about preventing this optimization from being used in the future?\n\n-jtoomim\n\n> On Apr 5, 2017, at 2:37 PM, Gregory Maxwell via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> An obvious way to generate different candidates is to grind the\n> coinbase extra-nonce but for non-empty blocks each attempt will\n> require 13 or so additional sha2 runs which is very inefficient.\n> \n> This inefficiency can be avoided by computing a sqrt number of\n> candidates of the left side of the hash tree (e.g. using extra\n> nonce grinding) then an additional sqrt number of candidates of\n> the right  side of the tree using transaction permutation or\n> substitution of a small number of transactions.  All combinations\n> of the left and right side are then combined with only a single\n> hashing operation virtually eliminating all tree related\n> overhead.\n> \n> With this final optimization finding a 4-way collision with a\n> moderate amount of memory requires ~2^24 hashing operations\n> instead of the >2^28 operations that would be require for\n> extra-nonce  grinding which would substantially erode the\n> benefit of the attack.\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170405/591656bf/attachment.sig>"
            },
            {
                "author": "Jared Lee Richardson",
                "date": "2017-04-06T20:21:12",
                "message_text_only": "> Just checking to see if I understand this optimization correctly. In order to find merkle roots in which the rightmost 32 bits are identical (i.e. partial hash collisions), we want to compute as many merkle root hashes as quickly as possible. The fastest way to do this is to take the top level of the Merkle tree, and to collect a set of left branches and right branches which can be independently manipulated. While the left branch can easily be manipulated by changing the extranonce in the coinbase transaction, the right branch would need to be modified by changing one of the transactions in the right branch or by changing the number of transactions in the right branch. Correct so far?\n\nEnvisioning it in my head and trying to read the white paper, it\nsounds like the process for a non-stratum mining farm would be this:\n\nOn primary server with sufficient memory, calculate ~4k-6k valid\nleft-side merkle tree roots and ~4k-6k right-side merkle tree roots.\nThen try hashing every left-side option with every right-side option.\nI'm not sure if modern asic chips are sufficiently generic that they\ncan also sha256-double-hash those combinations, but it seems logical\nto assume that the permutations of those hashes could be computed on\nan asic, perhaps via additional hardware installed on the server.\nHashing these is easier if there are fewer steps, i.e., fewer\ntransactions.\n\nOut of this will come N(2-16 at most, higher not needed) colliding\nmerkle roots where the last 4 bytes are identical.  Those N different\nmerkle combinations are what can be used on the actual mining devices,\nand those are all that needs to be sent for the optimization to work.\n\nOn the actual mining device, what is done is to take the identical\n(collision) right 4 bytes of the merkle root and hash it with one\nnonce value.  Since you have N(assume 8) inputs that all work with the\nsame value, calculating this single hash of once nonce is equivalent\nto calculating 8 nonce hashes during the normal process, and this step\nis 1/4th of the normal hashing process.  This hash(or mid-value?) is\nthen sent to 8 different cores which complete the remaining 3 hash\nsteps with each given collision value.  Then you increment the nonce\nonce and start over.\n\nThis works out to a savings of (assuming compressor and expander steps\nof SHA2 require computationally the same amount of time) 25% * (7 / 8)\nwhere N=8.\n\nGreg, or someone else, can you confirm that this is the right\nunderstanding of the approach?\n\n> I have not seen or heard of any hardware available that can run more efficiently using getblocktemplate.\n\nAs above, it doesn't require such a massive change.  They just need to\nretrieve N different sets of work from the central server instead of 1\nset of work.  The central server itself might need substantial\nbandwidth if it farmed out the merkle-root hashing computational space\nto miners.  Greg, is that what you're assuming they are doing?  Now\nthat I think about it, even that situation could be improved.  Suppose\nyou have N miners who can do either a merkle-tree combinatoric\ndouble-sha or a block-nonce double-sha.  The central server calculates\nthe left and right merkle treeset to be combined and also assigns each\nminer each a unique workspace within those combinatorics.  The miners\ncompute each hash in their workspace and shard the results within\nthemselves according to the last 16 bits.  Each miner then needs only\nthe memory for 1/Nth of the workspace, and can report back to the\ncentral server only the highest number of collisions it has found\nuntil the central server is satisfied and returns the miners to normal\n(collided) mining.\n\nSeems quite workable in a large mining farm to me, and would allow the\ncollisions to be found very, very quickly.\n\nThat said, it strikes me that there may be some statistical method by\nwhich we can isolate which pools seem to have used this approach\nagainst the background noise of other pools.  Hmm...\n\nJared\n\n\n\nOn Wed, Apr 5, 2017 at 7:10 PM, Jonathan Toomim via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Just checking to see if I understand this optimization correctly. In order to find merkle roots in which the rightmost 32 bits are identical (i.e. partial hash collisions), we want to compute as many merkle root hashes as quickly as possible. The fastest way to do this is to take the top level of the Merkle tree, and to collect a set of left branches and right branches which can be independently manipulated. While the left branch can easily be manipulated by changing the extranonce in the coinbase transaction, the right branch would need to be modified by changing one of the transactions in the right branch or by changing the number of transactions in the right branch. Correct so far?\n>\n> With the stratum mining protocol, the server (the pool) includes enough information for the coinbase transaction to be modified by stratum client (the miner), but it does not include any information about the right side of the merkle tree except for the top-level hash. Stratum also does not allow the client to supply any modifications to the merkle tree (including the right side) back to the stratum server. This means that any implementation of this final optimization would need to be using a protocol other than stratum, like getblocktemplate, correct?\n>\n> I think it would be helpful for the discussion to know if this optimization were currently being used or not, and if so, how widely.\n>\n> All of the consumer-grade hardware that I have seen defaults to stratum-only operation, and I have not seen or heard of any hardware available that can run more efficiently using getblocktemplate. As the current pool infrastructure uses stratum exclusively, this optimization would require significant retooling among pools, and probably a redesign of their core algorithms to help discover and share these partial collisions more frequently. It's possible that some large private farms have deployed a special system for solo mining that uses this optimization, of course, but it's also possible that there's a teapot in space somewhere between the orbit of Earth and Mars.\n>\n> Do you know of any ways to perform this optimization via stratum? If not, do you have any evidence that this optimization is actually being used by private solo mining farms? Or is this discussion purely about preventing this optimization from being used in the future?\n>\n> -jtoomim\n>\n>> On Apr 5, 2017, at 2:37 PM, Gregory Maxwell via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>> An obvious way to generate different candidates is to grind the\n>> coinbase extra-nonce but for non-empty blocks each attempt will\n>> require 13 or so additional sha2 runs which is very inefficient.\n>>\n>> This inefficiency can be avoided by computing a sqrt number of\n>> candidates of the left side of the hash tree (e.g. using extra\n>> nonce grinding) then an additional sqrt number of candidates of\n>> the right  side of the tree using transaction permutation or\n>> substitution of a small number of transactions.  All combinations\n>> of the left and right side are then combined with only a single\n>> hashing operation virtually eliminating all tree related\n>> overhead.\n>>\n>> With this final optimization finding a 4-way collision with a\n>> moderate amount of memory requires ~2^24 hashing operations\n>> instead of the >2^28 operations that would be require for\n>> extra-nonce  grinding which would substantially erode the\n>> benefit of the attack.\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Peter Todd",
                "date": "2017-04-06T02:31:23",
                "message_text_only": "On Wed, Apr 05, 2017 at 09:37:45PM +0000, Gregory Maxwell via bitcoin-dev wrote:\n> On that basis, I offer the following BIP draft for discussion.\n> This proposal does not prevent the attack in general, but only\n> inhibits covert forms of it which are incompatible with\n> improvements to the Bitcoin protocol.\n> \n> I hope that even those of us who would strongly prefer that\n> ASICBOOST be blocked completely can come together to support\n> a protective measure that separates concerns by inhibiting\n> the covert use of it that potentially blocks protocol improvements.\n\nWhile I'm in favour of blocking covert usage of ASICBOOST, there's every reason\nto block non-covert usage of it as well. In a low margin business like mining,\nthe advatange it gives is enormous - quite possibly 10x your profit margin -\nand given that barrier free access to being able to purchase ASICs is already\nan archilles heal for Bitcoin there is every reason to eliminate this legal\nvulnerability. Additionally, it's a technical vulnerability as well: we want\ngetting into the ASIC manufacturing and design business to have as low barriers\nto entry as is feasible, and the ASICBOOST exploit significantly increases the\nminimum capital requirements to do so.\n\nRemember that the whole purpose of PoW is to destroy value on a level playing\nfield. Anything that inhibits a level playing field is an exploit. While this\nisn't standard crypto - we can't fix every exploit completely - since we're\ngoing to do a technical change to partially mitigate the ASCIBOOST exploit\nthere is every reason to fully mitigate it.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170405/90b81cc9/attachment-0001.sig>"
            },
            {
                "author": "Bram Cohen",
                "date": "2017-04-06T02:39:08",
                "message_text_only": "On Wed, Apr 5, 2017 at 7:31 PM, Peter Todd via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n> While I'm in favour of blocking covert usage of ASICBOOST, there's every\n> reason\n> to block non-covert usage of it as well. In a low margin business like\n> mining,\n> the advatange it gives is enormous - quite possibly 10x your profit margin\n> -\n> and given that barrier free access to being able to purchase ASICs is\n> already\n> an archilles heal for Bitcoin there is every reason to eliminate this legal\n> vulnerability. Additionally, it's a technical vulnerability as well: we\n> want\n> getting into the ASIC manufacturing and design business to have as low\n> barriers\n> to entry as is feasible, and the ASICBOOST exploit significantly increases\n> the\n> minimum capital requirements to do so.\n>\n\nAsicboost also has the problem that it isn't treating the hashing as a\nblack box, and thus has impacts on what gets mined. In particular it\ncreates an incentive to make blocks smaller. That's a very unwanted effect,\nand anything like it should be engineered out on principle.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170405/bd9a7d92/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2017-04-06T02:49:10",
                "message_text_only": "On Wed, Apr 05, 2017 at 07:39:08PM -0700, Bram Cohen wrote:\n> On Wed, Apr 5, 2017 at 7:31 PM, Peter Todd via bitcoin-dev <\n> > While I'm in favour of blocking covert usage of ASICBOOST, there's every\n> > reason\n> > to block non-covert usage of it as well. In a low margin business like\n> > mining,\n> > the advatange it gives is enormous - quite possibly 10x your profit margin\n> > -\n> > and given that barrier free access to being able to purchase ASICs is\n> > already\n> > an archilles heal for Bitcoin there is every reason to eliminate this legal\n> > vulnerability. Additionally, it's a technical vulnerability as well: we\n> > want\n> > getting into the ASIC manufacturing and design business to have as low\n> > barriers\n> > to entry as is feasible, and the ASICBOOST exploit significantly increases\n> > the\n> > minimum capital requirements to do so.\n> >\n> \n> Asicboost also has the problem that it isn't treating the hashing as a\n> black box, and thus has impacts on what gets mined. In particular it\n> creates an incentive to make blocks smaller. That's a very unwanted effect,\n> and anything like it should be engineered out on principle.\n\nAgreed! There's no benefit to Bitcoin for having it - one way or the other\nminers are going to destroy ~12BTC/block worth of energy. Meanwhile it appears\nto have lead to something like a year of stupid political bullshit based on a\nsecret advantage - there's no reason to invite a repeat of this episode.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170405/887d8809/attachment.sig>"
            },
            {
                "author": "Erik Aronesty",
                "date": "2017-04-06T03:11:53",
                "message_text_only": "If the primary purpose of pow is to destroy value, then a masked proof of\nburn to an expanded address that assigns the private key holder the right\nto mine only in the next Nth block would be sufficient.  Expanding the\naddress space so that addresses can only be proven invalid only with the\nprivate key.  Miners can then not trivially game the system by excluding\ntx...without killing the entire system.  ( Like POW ... miners lose many\nburns since only one valid proof is deterministically selected. Difficult\nadjusted upward based on the number of valid proofs per block.)\n\nThe other part of \"real POW\" is that miners take *time* to mine.  Proof of\ndestroyed value us not sufficient.  Proof of time spent is critical....\nsomething even a masked burn cannot provide.\n\nOn Apr 5, 2017 10:49 PM, \"Peter Todd via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Wed, Apr 05, 2017 at 07:39:08PM -0700, Bram Cohen wrote:\n> > On Wed, Apr 5, 2017 at 7:31 PM, Peter Todd via bitcoin-dev <\n> > > While I'm in favour of blocking covert usage of ASICBOOST, there's\n> every\n> > > reason\n> > > to block non-covert usage of it as well. In a low margin business like\n> > > mining,\n> > > the advatange it gives is enormous - quite possibly 10x your profit\n> margin\n> > > -\n> > > and given that barrier free access to being able to purchase ASICs is\n> > > already\n> > > an archilles heal for Bitcoin there is every reason to eliminate this\n> legal\n> > > vulnerability. Additionally, it's a technical vulnerability as well: we\n> > > want\n> > > getting into the ASIC manufacturing and design business to have as low\n> > > barriers\n> > > to entry as is feasible, and the ASICBOOST exploit significantly\n> increases\n> > > the\n> > > minimum capital requirements to do so.\n> > >\n> >\n> > Asicboost also has the problem that it isn't treating the hashing as a\n> > black box, and thus has impacts on what gets mined. In particular it\n> > creates an incentive to make blocks smaller. That's a very unwanted\n> effect,\n> > and anything like it should be engineered out on principle.\n>\n> Agreed! There's no benefit to Bitcoin for having it - one way or the other\n> miners are going to destroy ~12BTC/block worth of energy. Meanwhile it\n> appears\n> to have lead to something like a year of stupid political bullshit based\n> on a\n> secret advantage - there's no reason to invite a repeat of this episode.\n>\n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170405/5b8d1ed4/attachment-0001.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2017-04-06T03:23:37",
                "message_text_only": "On Wed, Apr 05, 2017 at 11:11:53PM -0400, Erik Aronesty wrote:\n> If the primary purpose of pow is to destroy value, then a masked proof of\n> burn to an expanded address that assigns the private key holder the right\n\nYou're talking about proof-of-stake here.\n\nAt best it's very difficult for such a \"proof-of-burn\" to _actually_ be a\nproof, as the burn only happens if the consensus mechanism ultimately includes\nthat burn. Contrast that to proof-of-work's incredibly simple proof: you _know_\nenergy was destroyed to find a PoW solution, regardless of what consensus is\nultimately reached.\n\nIt's the difference between a computer secured from hackers with an anti-virus\nscanner, and a computer secured by the fact that it's not connected to the\ninternet at all.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170405/6a415f35/attachment.sig>"
            },
            {
                "author": "David Vorick",
                "date": "2017-04-06T03:23:22",
                "message_text_only": "I have a practical concern related to the amount of activation energy\nrequired to get something like this through. We are talking about\nimplementing something that would remove tens to hundreds of millions of\ndollars of mining revenue for miners who have already gambled that this\nincome would be available to them.\n\nThat's not something they are going to let go of without a fight, and we've\nalready seen this with the segwit resistance. Further, my understanding is\nthat this makes a UASF a lot more difficult. Mining hardware that has\nunique optimizations on one chain only can resist a UASF beyond a simple\neconomic majority, because they can do more hashes on the same amount of\nrevenue. Threshold for success is no longer 51%, especially if you are\nexpecting the miners to struggle (and this is a case where they have a very\ngood reason to struggle). Any resistance from the hashrate during the early\ndays of a UASF will inevitably cause large reorgs for older nodes, and is\nnot much better than a hardfork.\n\nI don't know what the right answer is. But I know that we are not going to\nget segwit without a fight. We are not going to invalidate covert asicboost\nwithout a fight. And we are working with a system that actively (and is\ndemonstrably very effective at doing it) resists changes which are\ncontentious. This is definitely a contentious change, because an important\npart of the community (the miners) is going to be actively resisting it.\n\nI urge everybody to realize how difficult something like this is going to\nbe to pull off. We are literally talking about invalidating hardware (or at\nleast the optimized bits). It's only going to succeed if everybody is\nconclusively on board. As you consider proposals, realize that anything\nwhich is not the simplest and least contentious is already dead.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170405/e22feba7/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2017-04-06T03:42:40",
                "message_text_only": "On Wed, Apr 05, 2017 at 11:23:22PM -0400, David Vorick wrote:\n> I urge everybody to realize how difficult something like this is going to\n> be to pull off. We are literally talking about invalidating hardware (or at\n> least the optimized bits). It's only going to succeed if everybody is\n> conclusively on board. As you consider proposals, realize that anything\n> which is not the simplest and least contentious is already dead.\n\nOne of the things going for us here is that Bitmain has been keeping ASICBOOST\nfrom their own customers - as far as we know they haven't been sharing it, and\nthus they're the only ones you can actually use it.\n\nSo while we're pissing off Bitmain in disabling it, we wouldn't be affecting\nanyone else.\n\nEqually, mining is a zero-sum game: if no-one can use ASICBOOST, miners are in\nthe same position as before. ASICBOOST is only relevant to miners like Bitmain\nwho have access to it while other miners don't.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170405/4c5534ae/attachment.sig>"
            },
            {
                "author": "Thomas Daede",
                "date": "2017-04-06T05:46:27",
                "message_text_only": "On 04/05/2017 08:23 PM, David Vorick via bitcoin-dev wrote:\n> I have a practical concern related to the amount of activation energy\n> required to get something like this through. We are talking about\n> implementing something that would remove tens to hundreds of millions of\n> dollars of mining revenue for miners who have already gambled that this\n> income would be available to them.\n\nThe proposed BIP only removes covert ASICBOOST. As long as the ASICs can\nalso do the non-covert ASICBOOST, it shouldn't have any impact on miner\nrevenue."
            },
            {
                "author": "Jonathan Toomim",
                "date": "2017-04-06T06:24:04",
                "message_text_only": "Ethically, this situation has some similarities to the DAO fork. We have an entity who closely examined the code, found an unintended characteristic of that code, and made use of that characteristic in order to gain tens of millions of dollars. Now that developers are aware of it, they want to modify the code in order to negate as much of the gains as possible.\n\nThere are differences, too, of course: the DAO attacker was explicitly malicious and stole Ether from others, whereas Bitmain is just optimizing their hardware better than anyone else and better than some of us think they should be allowed to.\n\nIn both cases, developers are proposing that the developers and a majority of users collude to reduce the wealth of a single entity by altering the blockchain rules.\n\nIn the case of the DAO fork, users were stealing back stolen funds, but that justification doesn't apply in this case. On the other hand, in this case we're talking about causing someone a loss by reducing the value of hardware investments rather than forcibly taking back their coins, which is less direct and maybe more justifiable.\n\nWhile I don't like patented mining algorithms, I also don't like the idea of playing Calvin Ball on the blockchain. Rule changes should not be employed as a means of disempowering and empoverishing particular entities without very good reason. Whether patenting a mining optimization qualifies as good reason is questionable.\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170405/fe7677aa/attachment-0001.sig>"
            },
            {
                "author": "David Vorick",
                "date": "2017-04-06T12:04:16",
                "message_text_only": "On Thu, Apr 6, 2017 at 2:24 AM, Jonathan Toomim via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Ethically, this situation has some similarities to the DAO fork. We have\n> an entity who closely examined the code, found an unintended characteristic\n> of that code, and made use of that characteristic in order to gain tens of\n> millions of dollars. Now that developers are aware of it, they want to\n> modify the code in order to negate as much of the gains as possible.\n>\n> There are differences, too, of course: the DAO attacker was explicitly\n> malicious and stole Ether from others, whereas Bitmain is just optimizing\n> their hardware better than anyone else and better than some of us think\n> they should be allowed to.\n>\n> In both cases, developers are proposing that the developers and a majority\n> of users collude to reduce the wealth of a single entity by altering the\n> blockchain rules.\n>\n> In the case of the DAO fork, users were stealing back stolen funds, but\n> that justification doesn't apply in this case. On the other hand, in this\n> case we're talking about causing someone a loss by reducing the value of\n> hardware investments rather than forcibly taking back their coins, which is\n> less direct and maybe more justifiable.\n>\n> While I don't like patented mining algorithms, I also don't like the idea\n> of playing Calvin Ball on the blockchain. Rule changes should not be\n> employed as a means of disempowering and empoverishing particular entities\n> without very good reason. Whether patenting a mining optimization qualifies\n> as good reason is questionable.\n>\n\nBitmain is blocking protocol upgrades to preserve their mining advantage.\nThis is quite distinct from someone taking advantage of a visibly broken\nand highly toxic smart contract to net themselves tens of millions of\ndollars. Further, Bitmain is performing a patented hardware optimization.\nThe patents mean that other miners are unable to capitalize on these\noptimizations. These optimizations are to the tune of 30%. If you give one\nplayer in the mining industry a permanent 30% cost advantage they will\neventually own everything. It's an industry where margins tend towards zero.\n\nThe asicboost patent is a direct threat to the health of the Bitcoin\necosystem, and now we have visible proof. The war against segwit and the\nstrife with Bitcoin Unlimited was very damaging to the ecosystem, damaging\nto the price, and holding back significant improvements and upgrades to the\nBitcoin protocol. I interpret this as a direct attack on the Bitcoin\necosystem.\n\nI don't know if changing the rules to nullify asicboost is the right move.\nI'm sure this won't be the last patent that causes damage to the ecosystem.\nBut you need to recognize that the issue is not that Bitmain ran a hardware\noptimization. It's that hardware optimizations exist which directly inhibit\nupgrading the protocol. And it's that hardware optimizations exist\nencumbered by patents enough to give one party a decisive advantage in\nmining, decisive enough for them to build a single, centralized monopoly.\n\nEach problem is separate, and each problem is significant, and each problem\nis fundamental. The DAO attack was a one-time bout of stupidity that\nthreatened a fixed amount of money. asicboost is an ongoing status that\ndirectly damages Bitcoin's ability to upgrade, and directly damage\nBitcoin's ability to retain any modicum of decentralization in the\nhashrate. The DAO issue did neither of these things for ethereum.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170406/5ed69d3a/attachment.html>"
            },
            {
                "author": "Marco",
                "date": "2017-04-06T16:49:13",
                "message_text_only": "On 04/06/2017 03:24 AM, Jonathan Toomim via bitcoin-dev wrote:\n> Ethically, this situation has some similarities to the DAO fork. We have an entity who closely examined the code, found an unintended characteristic of that code, and made use of that characteristic in order to gain tens of millions of dollars. Now that developers are aware of it, they want to modify the code in order to negate as much of the gains as possible.\n>\n> There are differences, too, of course: the DAO attacker was explicitly malicious and stole Ether from others, whereas Bitmain is just optimizing their hardware better than anyone else and better than some of us think they should be allowed to.\n>\n> In both cases, developers are proposing that the developers and a majority of users collude to reduce the wealth of a single entity by altering the blockchain rules.\n>\n> In the case of the DAO fork, users were stealing back stolen funds, but that justification doesn't apply in this case. On the other hand, in this case we're talking about causing someone a loss by reducing the value of hardware investments rather than forcibly taking back their coins, which is less direct and maybe more justifiable.\n>\n> While I don't like patented mining algorithms, I also don't like the idea of playing Calvin Ball on the blockchain. Rule changes should not be employed as a means of disempowering and empoverishing particular entities without very good reason. Whether patenting a mining optimization qualifies as good reason is questionable.\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\nQuite different in that the DAO fork was about an application level bug\nand this current proposal is about a possibly dangerous incentive at\nprotocol level.\nIn the first, a protocol change was called to recover funds lost for an\napplication level bug. In the latter, a protocol change is being called\nto address a perceived incentive problem in the protocol.\n\nA good comparison would be if a protocol level change was being proposed\nfor a case like mt gox. But it's not.\n\nPlus... This proposal only addresses one covert asicboost and not other\novert forms.\nEven though we may, as well, have good reasons to block other overt forms.\n\nMarco Agner\nhttps://www.agner.io\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170406/c08b52e0/attachment-0001.html>"
            },
            {
                "author": "Alex Mizrahi",
                "date": "2017-04-06T17:04:29",
                "message_text_only": "> Ethically, this situation has some similarities to the DAO fork.\n\n\nThere are no similarities.\n\nThe DAO fork was against the principles of cryptocurrencies: a change of\nthe ledger done in violation of pre-agreed rules. The whole point of\ncryptocurrency is to avoid shit like that. (E.g. a central banker changing\nledger as he wants.)\n\nGreg's proposal is in line with the principles of cryptocurrencies:\nPoW-based cryptocurrency can work only if there is a competition between\nminers, which requires all miners to have equal access to the technology.\n\nThe notion that Bitmain is entitled to future profits is completely\nridiculous. Every investment has a risk, and doing unusual stuff which\nboosts your profits is associated with increased risk. Developers just need\nto make sure all miners are on equal grounds, as that's the whole point of\nthe protocol. If Bitmain loses their profits because of that it's really\njust Bitmain's problem.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170406/622850e0/attachment.html>"
            },
            {
                "author": "Alex Mizrahi",
                "date": "2017-04-06T17:13:27",
                "message_text_only": "> Ethically, this situation has some similarities to the DAO fork.\n\n\nMuch better analogy:\n\n1. An ISV make software which makes use of an undocumented OS feature.\n2. That feature is no longer present in the next OS release.\n3. ISV suffers losses because its software cannot work under new OS, and\nthus people stop buying it.\n\nI think 99% of programmers would agree that this loss was inflicted by a\nbad decision of ISV, and not by OS vendor changing OS internals. Relying on\nundocumented features is something you do on your own risk.\n\nI think it is ethically unambiguous to everyone who isn't on Bitmain's\npayroll.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170406/5645efed/attachment.html>"
            },
            {
                "author": "Jannes Faber",
                "date": "2017-04-07T12:59:13",
                "message_text_only": "On 6 April 2017 at 19:13, Alex Mizrahi via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n> Ethically, this situation has some similarities to the DAO fork.\n>\n>\n> Much better analogy:\n>\n> 1. An ISV make software which makes use of an undocumented OS feature.\n> 2. That feature is no longer present in the next OS release.\n> 3. ISV suffers losses because its software cannot work under new OS, and\n> thus people stop buying it.\n>\n> I think 99% of programmers would agree that this loss was inflicted by a\n> bad decision of ISV, and not by OS vendor changing OS internals. Relying on\n> undocumented features is something you do on your own risk.\n>\n\nRight. And in this case, code still is law: if the code specifies a version\nnumber field and some miner finds an optimization that only works when the\nversion number == 1 then it's his own problem once the network upgrades to\nversion 2. In no way is there anything ethical about blocking the upgrade.\n\nHistory is not an indicator of the possible values any field can hold in\nthe future. Limiting your operation to some arbitrary subset is at your own\nrisk.\n\nRegarding the comparison: I haven't heard anyone even suggest rolling back\nthe last year of the blockchain to undo the damage already done, any\ncomparison can end there. If Jonathan wants to persist with this comparison\nit would be more like people deciding to stop further funding of the hacked\ncontract. Yeah, that evil.\n\n--\nJannes Faber\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170407/ea9891e9/attachment.html>"
            },
            {
                "author": "Erik Aronesty",
                "date": "2017-04-07T13:28:13",
                "message_text_only": "It is *not proof of stake.* when:\n\na) burn happens regardless of whether you successfully mine.\nb) miner cannot know which tx are burns\nc) the majority of burns cannot be used for mining and are simply lost\n(poisson discovery distribution)\nd) burn involves real risk: *every bit as much at stake *\n\n(It's the difference between a computer secured by not being connected to\nthe internet, and a computer secured by re-imaging from a computer that\nwas, in the past, not connected to the internet.)\n\nIt is possible to craft a burn-network such that the only way for a miner\nto prevent a burn is to prevent all transactions other than his own.\n\nThis is still a weakness, and I can't see a way around it though.\n\n\nOn Fri, Apr 7, 2017 at 8:59 AM, Jannes Faber via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n> On 6 April 2017 at 19:13, Alex Mizrahi via bitcoin-dev <bitcoin-dev at lists.\n> linuxfoundation.org> wrote:\n>\n>>\n>> Ethically, this situation has some similarities to the DAO fork.\n>>\n>>\n>> Much better analogy:\n>>\n>> 1. An ISV make software which makes use of an undocumented OS feature.\n>> 2. That feature is no longer present in the next OS release.\n>> 3. ISV suffers losses because its software cannot work under new OS, and\n>> thus people stop buying it.\n>>\n>> I think 99% of programmers would agree that this loss was inflicted by a\n>> bad decision of ISV, and not by OS vendor changing OS internals. Relying on\n>> undocumented features is something you do on your own risk.\n>>\n>\n> Right. And in this case, code still is law: if the code specifies a\n> version number field and some miner finds an optimization that only works\n> when the version number == 1 then it's his own problem once the network\n> upgrades to version 2. In no way is there anything ethical about blocking\n> the upgrade.\n>\n> History is not an indicator of the possible values any field can hold in\n> the future. Limiting your operation to some arbitrary subset is at your own\n> risk.\n>\n> Regarding the comparison: I haven't heard anyone even suggest rolling back\n> the last year of the blockchain to undo the damage already done, any\n> comparison can end there. If Jonathan wants to persist with this comparison\n> it would be more like people deciding to stop further funding of the hacked\n> contract. Yeah, that evil.\n>\n> --\n> Jannes Faber\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170407/0c0e32b0/attachment-0001.html>"
            },
            {
                "author": "Jared Lee Richardson",
                "date": "2017-04-06T17:31:13",
                "message_text_only": "To me, all of these miss the main objection.  If a miner found an\noptimization and kept it for themselves, that's their prerogative.\nBut if that optimization also happens to directly discourage the\ngrowth and improvement of the protocol in many unforseen ways, and it\nalso encourages the miner to include fewer transactions per block,\nthat directly hurts Bitcoin and its future.  Something should clearly\nbe done about it when the latter is at issue.  I agree with you that\nthe former is a relative nonissue.\n\nOn Wed, Apr 5, 2017 at 11:24 PM, Jonathan Toomim via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Ethically, this situation has some similarities to the DAO fork. We have an entity who closely examined the code, found an unintended characteristic of that code, and made use of that characteristic in order to gain tens of millions of dollars. Now that developers are aware of it, they want to modify the code in order to negate as much of the gains as possible.\n>\n> There are differences, too, of course: the DAO attacker was explicitly malicious and stole Ether from others, whereas Bitmain is just optimizing their hardware better than anyone else and better than some of us think they should be allowed to.\n>\n> In both cases, developers are proposing that the developers and a majority of users collude to reduce the wealth of a single entity by altering the blockchain rules.\n>\n> In the case of the DAO fork, users were stealing back stolen funds, but that justification doesn't apply in this case. On the other hand, in this case we're talking about causing someone a loss by reducing the value of hardware investments rather than forcibly taking back their coins, which is less direct and maybe more justifiable.\n>\n> While I don't like patented mining algorithms, I also don't like the idea of playing Calvin Ball on the blockchain. Rule changes should not be employed as a means of disempowering and empoverishing particular entities without very good reason. Whether patenting a mining optimization qualifies as good reason is questionable.\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Jared Lee Richardson",
                "date": "2017-04-06T17:26:52",
                "message_text_only": "> We are not going to invalidate covert asicboost without a fight. And we are working with a system that actively (and is demonstrably very effective at doing it) resists changes which are contentious. This is definitely a contentious change, because an important part of the community (the miners) is going to be actively resisting it.\n\nI'd just like to point out, invalidating asicboost has only a very\nlimited number of potential detractors.  Only a mining farm that\nself-mined and used custom software would be able to exploit this.\nEvery other mining farm on the planet, plus any users wishing for more\ntransactions to be included in blocks would be in favor of this,\nassuming the theory that it favors fewer transactions is correct.\nThat makes it less contentious than many other alternatives.  It might\neven force the mining operation(s) in question to flip and support SW\nin order to avoid losing face and/or appearing guilty.\n\nAs an additional plus, nearly all of the BU crowd and most BU\nsupporting miners would have little reason to object to Asicboost -\nBased on philosophy alone, but not based on any practical\nconsiderations.\n\nJared\n\nOn Wed, Apr 5, 2017 at 8:23 PM, David Vorick via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> I have a practical concern related to the amount of activation energy\n> required to get something like this through. We are talking about\n> implementing something that would remove tens to hundreds of millions of\n> dollars of mining revenue for miners who have already gambled that this\n> income would be available to them.\n>\n> That's not something they are going to let go of without a fight, and we've\n> already seen this with the segwit resistance. Further, my understanding is\n> that this makes a UASF a lot more difficult. Mining hardware that has unique\n> optimizations on one chain only can resist a UASF beyond a simple economic\n> majority, because they can do more hashes on the same amount of revenue.\n> Threshold for success is no longer 51%, especially if you are expecting the\n> miners to struggle (and this is a case where they have a very good reason to\n> struggle). Any resistance from the hashrate during the early days of a UASF\n> will inevitably cause large reorgs for older nodes, and is not much better\n> than a hardfork.\n>\n> I don't know what the right answer is. But I know that we are not going to\n> get segwit without a fight. We are not going to invalidate covert asicboost\n> without a fight. And we are working with a system that actively (and is\n> demonstrably very effective at doing it) resists changes which are\n> contentious. This is definitely a contentious change, because an important\n> part of the community (the miners) is going to be actively resisting it.\n>\n> I urge everybody to realize how difficult something like this is going to be\n> to pull off. We are literally talking about invalidating hardware (or at\n> least the optimized bits). It's only going to succeed if everybody is\n> conclusively on board. As you consider proposals, realize that anything\n> which is not the simplest and least contentious is already dead.\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Alex Mizrahi",
                "date": "2017-04-06T15:36:23",
                "message_text_only": "> Agreed! There's no benefit to Bitcoin for having it - one way or the other\n> miners are going to destroy ~12BTC/block worth of energy. Meanwhile it\n> appears\n> to have lead to something like a year of stupid political bullshit based\n> on a\n> secret advantage - there's no reason to invite a repeat of this episode.\n\n\nBut is it even possible to completely remove ASICBOOST optimization\npossibility?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170406/f18699ac/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2017-04-06T17:51:37",
                "message_text_only": "On Thu, Apr 6, 2017 at 4:39 AM, Bram Cohen via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> Asicboost also has the problem that it isn't treating the hashing as a black\n> box, and thus has impacts on what gets mined. In particular it creates an\n> incentive to make blocks smaller. That's a very unwanted effect, and\n> anything like it should be engineered out on principle.\n\nThis is an interesting point.\n\nIf you have a precise description why it makes an incentive to make\nblocks smaller I would love to read it.\nSomebody asked and I didn't have an answer.\nI imagine you try several reorderings sometimes excluding certain\nbranches of the merkle tree, permuting the branches you exclude or\nsomething similar, but I really don't know the algorithm in detail and\nI didn't want to say something inaccurate."
            },
            {
                "author": "bfd at cock.lu",
                "date": "2017-04-06T07:24:03",
                "message_text_only": "Miners blocking SegWit due to ASICBOOST requirements also means they \nwould block future deployment of committed bloom filters.\n\nOn 2017-04-06 00:37, Gregory Maxwell via bitcoin-dev wrote:\n> A month ago I was explaining the attack on Bitcoin's SHA2 hashcash \n> which\n> is exploited by ASICBOOST and the various steps which could be used to\n> block it in the network if it became a problem.\n> \n> While most discussion of ASICBOOST has focused on the overt method\n> of implementing it, there also exists a covert method for using it.\n> \n> As I explained one of the approaches to inhibit covert ASICBOOST I\n> realized that my words were pretty much also describing the SegWit\n> commitment structure.\n> \n> The authors of the SegWit proposal made a specific effort to not be\n> incompatible with any mining system and, in particular, changed the\n> design at one point to accommodate mining chips with forced payout\n> addresses.\n> \n> Had there been awareness of exploitation of this attack an effort\n> would have been made to avoid incompatibility-- simply to separate\n> concerns.  But the best methods of implementing the covert attack\n> are significantly incompatible with virtually any method of\n> extending Bitcoin's transaction capabilities; with the notable\n> exception of extension blocks (which have their own problems).\n> \n> An incompatibility would go a long way to explain some of the\n> more inexplicable behavior from some parties in the mining\n> ecosystem so I began looking for supporting evidence.\n> \n> Reverse engineering of a particular mining chip has demonstrated\n> conclusively that ASICBOOST has been implemented\n> in hardware.\n> \n> On that basis, I offer the following BIP draft for discussion.\n> This proposal does not prevent the attack in general, but only\n> inhibits covert forms of it which are incompatible with\n> improvements to the Bitcoin protocol.\n> \n> I hope that even those of us who would strongly prefer that\n> ASICBOOST be blocked completely can come together to support\n> a protective measure that separates concerns by inhibiting\n> the covert use of it that potentially blocks protocol improvements.\n> \n> The specific activation height is something I currently don't have\n> a strong opinion, so I've left it unspecified for the moment.\n> \n> <pre>\n>   BIP: TBD\n>   Layer: Consensus\n>   Title: Inhibiting a covert attack on the Bitcoin POW function\n>   Author: Greg Maxwell <greg at xiph.org>\n>   Status: Draft\n>   Type: Standards Track\n>   Created: 2016-04-05\n>   License: PD\n> </pre>\n> \n> ==Abstract==\n> \n> This proposal inhibits the covert exploitation of a known\n> vulnerability in Bitcoin Proof of Work function.\n> \n> The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n> \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this\n> document are to be interpreted as described in RFC 2119.\n> \n> ==Motivation==\n> \n> Due to a design oversight the Bitcoin proof of work function has a \n> potential\n> attack which can allow an attacking miner to save up-to 30% of their \n> energy\n> costs (though closer to 20% is more likely due to implementation \n> overheads).\n> \n> Timo Hanke and Sergio Demian Lerner claim to hold a patent on this \n> attack,\n> which they have so far not licensed for free and open use by the \n> public.\n> They have been marketing their patent licenses under the trade-name\n> ASICBOOST.  The document takes no position on the validity or \n> enforceability\n> of the patent.\n> \n> There are two major ways of exploiting the underlying vulnerability: \n> One\n> obvious way which is highly detectable and is not in use on the network\n> today and a covert way which has significant interaction and potential\n> interference with the Bitcoin protocol.  The covert mechanism is not\n> easily detected except through its interference with the protocol.\n> \n> In particular, the protocol interactions of the covert method can block \n> the\n> implementation of virtuous improvements such as segregated witness.\n> \n> Exploitation of this vulnerability could result in payoff of as much as\n> $100 million USD per year at the time this was written (Assuming at\n> 50% hash-power miner was gaining a 30% power advantage and that mining\n> was otherwise at profit equilibrium).  This could have a phenomenal\n> centralizing effect by pushing mining out of profitability for all\n> other participants, and the income from secretly using this\n> optimization could be abused to significantly distort the Bitcoin\n> ecosystem in order to preserve the advantage.\n> \n> Reverse engineering of a mining ASIC from a major manufacture has\n> revealed that it contains an undocumented, undisclosed ability\n> to make use of this attack. (The parties claiming to hold a\n> patent on this technique were completely unaware of this use.)\n> \n> On the above basis the potential for covert exploitation of this\n> vulnerability and the resulting inequality in the mining process\n> and interference with useful improvements presents a clear and\n> present danger to the Bitcoin system which requires a response.\n> \n> ==Background==\n> \n> The general idea of this attack is that SHA2-256 is a merkle damgard \n> hash\n> function which consumes 64 bytes of data at a time.\n> \n> The Bitcoin mining process repeatedly hashes an 80-byte 'block header' \n> while\n> incriminating a 32-bit nonce which is at the end of this header data. \n> This\n> means that the processing of the header involves two runs of the \n> compression\n> function run-- one that consumes the first 64 bytes of the header and a\n> second which processes the remaining 16 bytes and padding.\n> \n> The initial 'message expansion' operations in each step of the SHA2-256\n> function operate exclusively on that step's 64-bytes of input with no\n> influence from prior data that entered the hash.\n> \n> Because of this if a miner is able to prepare a block header with\n> multiple distinct first 64-byte chunks but identical 16-byte\n> second chunks they can reuse the computation of the initial\n> expansion for multiple trials. This reduces power consumption.\n> \n> There are two broad ways of making use of this attack. The obvious\n> way is to try candidates with different version numbers.  Beyond\n> upsetting the soft-fork detection logic in Bitcoin nodes this has\n> little negative effect but it is highly conspicuous and easily\n> blocked.\n> \n> The other method is based on the fact that the merkle root\n> committing to the transactions is contained in the first 64-bytes\n> except for the last 4 bytes of it.  If the miner finds multiple\n> candidate root values which have the same final 32-bit then they\n> can use the attack.\n> \n> To find multiple roots with the same trailing 32-bits the miner can\n> use efficient collision finding mechanism which will find a match\n> with as little as 2^16 candidate roots expected, 2^24 operations to\n> find a 4-way hit, though low memory approaches require more\n> computation.\n> \n> An obvious way to generate different candidates is to grind the\n> coinbase extra-nonce but for non-empty blocks each attempt will\n> require 13 or so additional sha2 runs which is very inefficient.\n> \n> This inefficiency can be avoided by computing a sqrt number of\n> candidates of the left side of the hash tree (e.g. using extra\n> nonce grinding) then an additional sqrt number of candidates of\n> the right  side of the tree using transaction permutation or\n> substitution of a small number of transactions.  All combinations\n> of the left and right side are then combined with only a single\n> hashing operation virtually eliminating all tree related\n> overhead.\n> \n> With this final optimization finding a 4-way collision with a\n> moderate amount of memory requires ~2^24 hashing operations\n> instead of the >2^28 operations that would be require for\n> extra-nonce  grinding which would substantially erode the\n> benefit of the attack.\n> \n> It is this final optimization which this proposal blocks.\n> \n> ==New consensus rule==\n> \n> Beginning block X and until block Y the coinbase transaction of\n> each block MUST either contain a BIP-141 segwit commitment or a\n> correct WTXID commitment with ID 0xaa21a9ef.\n> \n> (See BIP-141 \"Commitment structure\" for details)\n> \n> Existing segwit using miners are automatically compatible with\n> this proposal. Non-segwit miners can become compatible by simply\n> including an additional output matching a default commitment\n> value returned as part of getblocktemplate.\n> \n> Miners SHOULD NOT automatically discontinue the commitment\n> at the expiration height.\n> \n> ==Discussion==\n> \n> The commitment in the left side of the tree to all transactions\n> in the right side completely prevents the final sqrt speedup.\n> \n> A stronger inhibition of the covert attack in the form of\n> requiring the least significant bits of the block timestamp\n> to be equal to a hash of the first 64-bytes of the header. This\n> would increase the collision space from 32 to 40 or more bits.\n> The root value could be required to meet a specific hash prefix\n> requirement in order to increase the computational work required\n> to try candidate roots. These change would be more disruptive and\n> there is no reason to believe that it is currently necessary.\n> \n> The proposed rule automatically sunsets. If it is no longer needed\n> due to the introduction of stronger rules or the acceptance of the\n> version-grinding form then there would be no reason to continue\n> with this requirement.  If it is still useful at the expiration\n> time the rule can simply be extended with a new softfork that\n> sets longer date ranges.\n> \n> This sun-setting avoids the accumulation of technical debt due\n> to retaining enforcement of this rule when it is no longer needed\n> without requiring a hard fork to remove it.\n> \n> == Overt attack ==\n> \n> The non-covert form can be trivially blocked by requiring that\n> the header version match the coinbase transaction version.\n> \n> This proposal does not include this block because this method\n> may become generally available without restriction in the future,\n> does not generally interfere with improvements in the protocol,\n> and because it is so easily detected that it could be blocked if\n> it becomes an issue in the future.\n> \n> ==Backward compatibility==\n> \n> \n> ==Implementation==\n> \n> \n> ==Acknowledgments==\n> \n> \n> ==Copyright==\n> \n> This document is placed in the public domain.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Luke Dashjr",
                "date": "2017-04-06T09:17:48",
                "message_text_only": "On Wednesday, April 05, 2017 9:37:45 PM Gregory Maxwell via bitcoin-dev wrote:\n> Beginning block X and until block Y the coinbase transaction of\n> each block MUST either contain a BIP-141 segwit commitment or a\n> correct WTXID commitment with ID 0xaa21a9ef.\n\nWhy not simply require the BIP-141 commitment?\n\n> Existing segwit using miners are automatically compatible with\n> this proposal.\n\nNot entirely. The commitment is not required until segwit activates.\nBut this should be trivial to implement at least.\n\n> == Overt attack ==\n> \n> The non-covert form can be trivially blocked by requiring that\n> the header version match the coinbase transaction version.\n> \n> This proposal does not include this block because this method\n> may become generally available without restriction in the future,\n> does not generally interfere with improvements in the protocol,\n> and because it is so easily detected that it could be blocked if\n> it becomes an issue in the future.\n\nHow does it not interfere with BIP 9? I suppose the versionbits could be moved \nto the generation transaction version, but this would hide them from light \nclients.\n\n> This document is placed in the public domain.\n\nCould you please use one of these?\n\n    https://github.com/bitcoin/bips/blob/master/bip-0002.mediawiki#Recommended_licenses\n\nLuke"
            },
            {
                "author": "Luv Khemani",
                "date": "2017-04-06T12:02:32",
                "message_text_only": "Hi Greg\n\n\nGreat work in discovering this!\n\n\n> A month ago I was explaining the attack on Bitcoin's SHA2 hashcash which\nis exploited by ASICBOOST and the various steps which could be used to\nblock it in the network if it became a problem.\n\n\nCould you elaborate on why you consider ASICBOOST to be an attack? Attack here implies ill-intent by the practitioner towards the network as a primary motivating factor.\n\nPersonally, i see this as a miner acting in his self-interest and had i been a miner and knew about the covert method, i would use it too.\n\nSo while i'm no fan of Bitmain/Jihan, i do not condone the vilification he has received over the use of ASICBOOST to gain an edge.\n\nI know i'm griping over semantics, but in the current political climate, they can be amplified by some to cause more drama than is healthy.\n\n\nOther thoughts:\n\n\nSeveral people have commented that blocking the use of this covert technique is unethical or \"wrong\".\nTo quote Emin:\n\n>Taking action to block this is similar to the government taking measures to block Elon Musk's more efficient electric cars. Specifically prosecuting a chosen miner, in the current political climate, would send a terrible message of absolute centralization in the hands of one particular developer group, and it would severely damage Bitcoin mining and the coin's security.\n\nThis is a poor analogy and extremely misleading as the the basis for blocking has nothing to do with efficiency and more to do with the following:\n\n1) Blocking upgrades to the protocol that are deemed by the vast majority of the technical community/Bitcoin Businesses as being the best way forward\n\n2) An advantage by a miner/group, especially one with majority hashrate is a threat to decentralisation and security of the network and it is entirely justifiable for devs to nullify such an advantage.\nYou can see it as an arms race where miners are always finding ways to gain an edge and devs trying to discover such edges and nullify them to level the playing field.\nThis is how the game works and it should not be viewed in a political angle or taken personally by either party. Miners are acting in their self-interest and Devs are trying to secure the network and increase decentralisation.\nBoth are doing their job.\n\nJust by revealing the info, you have effectively ensured the nullification of any edge enjoyed by miners using the covert technique in the medium to long term.\nEither miners not using the technique will all start signalling for SegWit to nullify their competitors edge or they will procure hardware which has the edge.\n\nGiven the threat to decentralisation, i also believe UASF will gain more momentum as users seek to protect the network from further miner centralisation.\n\n\n________________________________\nFrom: bitcoin-dev-bounces at lists.linuxfoundation.org <bitcoin-dev-bounces at lists.linuxfoundation.org> on behalf of Gregory Maxwell via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>\nSent: Thursday, April 6, 2017 5:37 AM\nTo: Bitcoin Dev\nSubject: [bitcoin-dev] BIP proposal: Inhibiting a covert attack on the Bitcoin POW function\n\nA month ago I was explaining the attack on Bitcoin's SHA2 hashcash which\nis exploited by ASICBOOST and the various steps which could be used to\nblock it in the network if it became a problem.\n\nWhile most discussion of ASICBOOST has focused on the overt method\nof implementing it, there also exists a covert method for using it.\n\nAs I explained one of the approaches to inhibit covert ASICBOOST I\nrealized that my words were pretty much also describing the SegWit\ncommitment structure.\n\nThe authors of the SegWit proposal made a specific effort to not be\nincompatible with any mining system and, in particular, changed the\ndesign at one point to accommodate mining chips with forced payout\naddresses.\n\nHad there been awareness of exploitation of this attack an effort\nwould have been made to avoid incompatibility-- simply to separate\nconcerns.  But the best methods of implementing the covert attack\nare significantly incompatible with virtually any method of\nextending Bitcoin's transaction capabilities; with the notable\nexception of extension blocks (which have their own problems).\n\nAn incompatibility would go a long way to explain some of the\nmore inexplicable behavior from some parties in the mining\necosystem so I began looking for supporting evidence.\n\nReverse engineering of a particular mining chip has demonstrated\nconclusively that ASICBOOST has been implemented\nin hardware.\n\nOn that basis, I offer the following BIP draft for discussion.\nThis proposal does not prevent the attack in general, but only\ninhibits covert forms of it which are incompatible with\nimprovements to the Bitcoin protocol.\n\nI hope that even those of us who would strongly prefer that\nASICBOOST be blocked completely can come together to support\na protective measure that separates concerns by inhibiting\nthe covert use of it that potentially blocks protocol improvements.\n\nThe specific activation height is something I currently don't have\na strong opinion, so I've left it unspecified for the moment.\n\n<pre>\n  BIP: TBD\n  Layer: Consensus\n  Title: Inhibiting a covert attack on the Bitcoin POW function\n  Author: Greg Maxwell <greg at xiph.org>\n  Status: Draft\n  Type: Standards Track\n  Created: 2016-04-05\n  License: PD\n</pre>\n\n==Abstract==\n\nThis proposal inhibits the covert exploitation of a known\nvulnerability in Bitcoin Proof of Work function.\n\nThe key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n\"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this\ndocument are to be interpreted as described in RFC 2119.\n\n==Motivation==\n\nDue to a design oversight the Bitcoin proof of work function has a potential\nattack which can allow an attacking miner to save up-to 30% of their energy\ncosts (though closer to 20% is more likely due to implementation overheads).\n\nTimo Hanke and Sergio Demian Lerner claim to hold a patent on this attack,\nwhich they have so far not licensed for free and open use by the public.\nThey have been marketing their patent licenses under the trade-name\nASICBOOST.  The document takes no position on the validity or enforceability\nof the patent.\n\nThere are two major ways of exploiting the underlying vulnerability: One\nobvious way which is highly detectable and is not in use on the network\ntoday and a covert way which has significant interaction and potential\ninterference with the Bitcoin protocol.  The covert mechanism is not\neasily detected except through its interference with the protocol.\n\nIn particular, the protocol interactions of the covert method can block the\nimplementation of virtuous improvements such as segregated witness.\n\nExploitation of this vulnerability could result in payoff of as much as\n$100 million USD per year at the time this was written (Assuming at\n50% hash-power miner was gaining a 30% power advantage and that mining\nwas otherwise at profit equilibrium).  This could have a phenomenal\ncentralizing effect by pushing mining out of profitability for all\nother participants, and the income from secretly using this\noptimization could be abused to significantly distort the Bitcoin\necosystem in order to preserve the advantage.\n\nReverse engineering of a mining ASIC from a major manufacture has\nrevealed that it contains an undocumented, undisclosed ability\nto make use of this attack. (The parties claiming to hold a\npatent on this technique were completely unaware of this use.)\n\nOn the above basis the potential for covert exploitation of this\nvulnerability and the resulting inequality in the mining process\nand interference with useful improvements presents a clear and\npresent danger to the Bitcoin system which requires a response.\n\n==Background==\n\nThe general idea of this attack is that SHA2-256 is a merkle damgard hash\nfunction which consumes 64 bytes of data at a time.\n\nThe Bitcoin mining process repeatedly hashes an 80-byte 'block header' while\nincriminating a 32-bit nonce which is at the end of this header data. This\nmeans that the processing of the header involves two runs of the compression\nfunction run-- one that consumes the first 64 bytes of the header and a\nsecond which processes the remaining 16 bytes and padding.\n\nThe initial 'message expansion' operations in each step of the SHA2-256\nfunction operate exclusively on that step's 64-bytes of input with no\ninfluence from prior data that entered the hash.\n\nBecause of this if a miner is able to prepare a block header with\nmultiple distinct first 64-byte chunks but identical 16-byte\nsecond chunks they can reuse the computation of the initial\nexpansion for multiple trials. This reduces power consumption.\n\nThere are two broad ways of making use of this attack. The obvious\nway is to try candidates with different version numbers.  Beyond\nupsetting the soft-fork detection logic in Bitcoin nodes this has\nlittle negative effect but it is highly conspicuous and easily\nblocked.\n\nThe other method is based on the fact that the merkle root\ncommitting to the transactions is contained in the first 64-bytes\nexcept for the last 4 bytes of it.  If the miner finds multiple\ncandidate root values which have the same final 32-bit then they\ncan use the attack.\n\nTo find multiple roots with the same trailing 32-bits the miner can\nuse efficient collision finding mechanism which will find a match\nwith as little as 2^16 candidate roots expected, 2^24 operations to\nfind a 4-way hit, though low memory approaches require more\ncomputation.\n\nAn obvious way to generate different candidates is to grind the\ncoinbase extra-nonce but for non-empty blocks each attempt will\nrequire 13 or so additional sha2 runs which is very inefficient.\n\nThis inefficiency can be avoided by computing a sqrt number of\ncandidates of the left side of the hash tree (e.g. using extra\nnonce grinding) then an additional sqrt number of candidates of\nthe right  side of the tree using transaction permutation or\nsubstitution of a small number of transactions.  All combinations\nof the left and right side are then combined with only a single\nhashing operation virtually eliminating all tree related\noverhead.\n\nWith this final optimization finding a 4-way collision with a\nmoderate amount of memory requires ~2^24 hashing operations\ninstead of the >2^28 operations that would be require for\nextra-nonce  grinding which would substantially erode the\nbenefit of the attack.\n\nIt is this final optimization which this proposal blocks.\n\n==New consensus rule==\n\nBeginning block X and until block Y the coinbase transaction of\neach block MUST either contain a BIP-141 segwit commitment or a\ncorrect WTXID commitment with ID 0xaa21a9ef.\n\n(See BIP-141 \"Commitment structure\" for details)\n\nExisting segwit using miners are automatically compatible with\nthis proposal. Non-segwit miners can become compatible by simply\nincluding an additional output matching a default commitment\nvalue returned as part of getblocktemplate.\n\nMiners SHOULD NOT automatically discontinue the commitment\nat the expiration height.\n\n==Discussion==\n\nThe commitment in the left side of the tree to all transactions\nin the right side completely prevents the final sqrt speedup.\n\nA stronger inhibition of the covert attack in the form of\nrequiring the least significant bits of the block timestamp\nto be equal to a hash of the first 64-bytes of the header. This\nwould increase the collision space from 32 to 40 or more bits.\nThe root value could be required to meet a specific hash prefix\nrequirement in order to increase the computational work required\nto try candidate roots. These change would be more disruptive and\nthere is no reason to believe that it is currently necessary.\n\nThe proposed rule automatically sunsets. If it is no longer needed\ndue to the introduction of stronger rules or the acceptance of the\nversion-grinding form then there would be no reason to continue\nwith this requirement.  If it is still useful at the expiration\ntime the rule can simply be extended with a new softfork that\nsets longer date ranges.\n\nThis sun-setting avoids the accumulation of technical debt due\nto retaining enforcement of this rule when it is no longer needed\nwithout requiring a hard fork to remove it.\n\n== Overt attack ==\n\nThe non-covert form can be trivially blocked by requiring that\nthe header version match the coinbase transaction version.\n\nThis proposal does not include this block because this method\nmay become generally available without restriction in the future,\ndoes not generally interfere with improvements in the protocol,\nand because it is so easily detected that it could be blocked if\nit becomes an issue in the future.\n\n==Backward compatibility==\n\n\n==Implementation==\n\n\n==Acknowledgments==\n\n\n==Copyright==\n\nThis document is placed in the public domain.\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\nbitcoin-dev -- Bitcoin Protocol Discussion - Linux Foundation<https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>\nlists.linuxfoundation.org\nBitcoin development and protocol discussion. This list is lightly moderated. - No offensive posts, no personal attacks. - Posts must concern development of bitcoin ...\n\n\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170406/bd8d70c4/attachment-0001.html>"
            },
            {
                "author": "Bryan Bishop",
                "date": "2017-04-06T12:11:35",
                "message_text_only": "On Thu, Apr 6, 2017 at 7:02 AM, Luv Khemani via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Could you elaborate on why you consider ASICBOOST to be an attack? Attack\n> here implies ill-intent by the practitioner towards the network as a\n> primary motivating factor.\n>\n>\nSee\nhttps://www.reddit.com/r/Bitcoin/comments/63otrp/gregory_maxwell_major_asic_manufacturer_is/dfwcki3/\n\n\"\"\"\nI think that it is an attack is a completely unambiguous technical\ndescription of what it is. If a signature is supposed to resist forgery\nagainst 2^128 operations, but you find a way to do it with 2^80 instead,\nthis is an attack. It is, perhaps, not a very concerning attack and you may\nor may not change your signature scheme to avoid it or may just instead say\nthe scheme has 2^80 security. But there is no doubt that it would be called\nan attack, especially if it was not described in the original proposal.\n\nIn Bitcoin's Proof of Work, you are attempting to prove a certain amount of\nwork has been done. This shortcut significantly reduces the amount of work.\nIt's an attack. Normally it wouldn't be a serious attack-- it would just\nget appended to the defacto definition of what the Bitcoin Proof of work\nis-- similar to the signature system just getting restarted as having 2^80\nsecurity-- but in it's covert form it cannot just be adopted because it\nblocks many further improvements (not just segwit, but the vast majority of\nother proposals), and additional the licensing restrictions inhibit\nadoption.\n\nThe proposal I posted does not prevent the technique, only the covert form:\nThat is, it doesn't even attempt to solve the patented tech eventually will\ncentralize the system problem. It is narrowly targeted at the interference\nwith upgrades.\n\nTaking a step back-- even ignoring my geeking out about the technical\ndefinition of 'attack' in crypographic contexts, we have a set of issues\nhere that left addressed will seriously harm the system going forward for\nthe the significant monetary benefit of an exploiting party. I think that\nalso satisfies a lay definition of the term: Something someone does, that\nnone one expected, that makes them money at everyone elses expense.\n\"\"\"\n\n- Bryan\nhttp://heybryan.org/\n1 512 203 0507\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170406/7032bdb1/attachment.html>"
            },
            {
                "author": "Timo Hanke",
                "date": "2017-04-06T17:43:56",
                "message_text_only": "Bryan,\n\nInteresting argument, but I think it is not an accurate comparison. People\nusually mean that, for example, say 2^80 of the original operations are\nneeded rather than the intended 2^128 to find a collision. This could be\nthe case in a broken algorithms such as a toy SHA variant with too small\nstates and too few rounds. These kind of attacks usually refer to that\nsomething is learned from prior evaluations that be should't be possible to\nbe learned. For example, if someone could somehow construct a pre-image in\n256 evaluations, getting one additional bit right at a time. Similar to a\ncheap combination lock where you can figure out the correct 4 digits in a\nworst case of 4*10 attempts by \"feeling\" it, rather than having to do the\nintended 10,000 attempts. That's the kind of thing that would be called an\n\"attack\".\n\nHere, however, we are talking about making the individual operations\ncheaper by a constant of ~20%, not changing the number of operations. That\ndoesn't qualify as an attack in the sense that you mean.\n\nBest,\nTimo\n\n\n\n\nOn Thu, Apr 6, 2017 at 5:11 AM, Bryan Bishop via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Thu, Apr 6, 2017 at 7:02 AM, Luv Khemani via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Could you elaborate on why you consider ASICBOOST to be an attack? Attack\n>> here implies ill-intent by the practitioner towards the network as a\n>> primary motivating factor.\n>>\n>>\n> See https://www.reddit.com/r/Bitcoin/comments/63otrp/\n> gregory_maxwell_major_asic_manufacturer_is/dfwcki3/\n>\n> \"\"\"\n> I think that it is an attack is a completely unambiguous technical\n> description of what it is. If a signature is supposed to resist forgery\n> against 2^128 operations, but you find a way to do it with 2^80 instead,\n> this is an attack. It is, perhaps, not a very concerning attack and you may\n> or may not change your signature scheme to avoid it or may just instead say\n> the scheme has 2^80 security. But there is no doubt that it would be called\n> an attack, especially if it was not described in the original proposal.\n>\n> In Bitcoin's Proof of Work, you are attempting to prove a certain amount\n> of work has been done. This shortcut significantly reduces the amount of\n> work. It's an attack. Normally it wouldn't be a serious attack-- it would\n> just get appended to the defacto definition of what the Bitcoin Proof of\n> work is-- similar to the signature system just getting restarted as having\n> 2^80 security-- but in it's covert form it cannot just be adopted because\n> it blocks many further improvements (not just segwit, but the vast majority\n> of other proposals), and additional the licensing restrictions inhibit\n> adoption.\n>\n> The proposal I posted does not prevent the technique, only the covert\n> form: That is, it doesn't even attempt to solve the patented tech\n> eventually will centralize the system problem. It is narrowly targeted at\n> the interference with upgrades.\n>\n> Taking a step back-- even ignoring my geeking out about the technical\n> definition of 'attack' in crypographic contexts, we have a set of issues\n> here that left addressed will seriously harm the system going forward for\n> the the significant monetary benefit of an exploiting party. I think that\n> also satisfies a lay definition of the term: Something someone does, that\n> none one expected, that makes them money at everyone elses expense.\n> \"\"\"\n>\n> - Bryan\n> http://heybryan.org/\n> 1 512 203 0507 <(512)%20203-0507>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170406/fe03bb52/attachment.html>"
            },
            {
                "author": "Luv Khemani",
                "date": "2017-04-06T12:30:51",
                "message_text_only": "Just to add on to the ethical issue of blocking this.\n\n\nIf blocking the covert form of ASICBOOST is seen as unethical, then the same can be said about libsecp256k1, various client optimisations, Compactblocks.\n\nAll of which seek to reduce the efficacy of large miners and selfish mining.\n\n\nI also find it very ironic that the author of the Selfish Mining paper who rang alarm bells about miner centralisation in 2013 is now opposing attempts to reduce miner centralisation.\n\n\n________________________________\nFrom: bitcoin-dev-bounces at lists.linuxfoundation.org <bitcoin-dev-bounces at lists.linuxfoundation.org> on behalf of Luv Khemani via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>\nSent: Thursday, April 6, 2017 8:02 PM\nTo: Gregory Maxwell; Bitcoin Protocol Discussion\nSubject: Re: [bitcoin-dev] BIP proposal: Inhibiting a covert attack on the Bitcoin POW function\n\n\nHi Greg\n\n\nGreat work in discovering this!\n\n\n> A month ago I was explaining the attack on Bitcoin's SHA2 hashcash which\nis exploited by ASICBOOST and the various steps which could be used to\nblock it in the network if it became a problem.\n\n\nCould you elaborate on why you consider ASICBOOST to be an attack? Attack here implies ill-intent by the practitioner towards the network as a primary motivating factor.\n\nPersonally, i see this as a miner acting in his self-interest and had i been a miner and knew about the covert method, i would use it too.\n\nSo while i'm no fan of Bitmain/Jihan, i do not condone the vilification he has received over the use of ASICBOOST to gain an edge.\n\nI know i'm griping over semantics, but in the current political climate, they can be amplified by some to cause more drama than is healthy.\n\n\nOther thoughts:\n\n\nSeveral people have commented that blocking the use of this covert technique is unethical or \"wrong\".\nTo quote Emin:\n\n>Taking action to block this is similar to the government taking measures to block Elon Musk's more efficient electric cars. Specifically prosecuting a chosen miner, in the current political climate, would send a terrible message of absolute centralization in the hands of one particular developer group, and it would severely damage Bitcoin mining and the coin's security.\n\nThis is a poor analogy and extremely misleading as the the basis for blocking has nothing to do with efficiency and more to do with the following:\n\n1) Blocking upgrades to the protocol that are deemed by the vast majority of the technical community/Bitcoin Businesses as being the best way forward\n\n2) An advantage by a miner/group, especially one with majority hashrate is a threat to decentralisation and security of the network and it is entirely justifiable for devs to nullify such an advantage.\nYou can see it as an arms race where miners are always finding ways to gain an edge and devs trying to discover such edges and nullify them to level the playing field.\nThis is how the game works and it should not be viewed in a political angle or taken personally by either party. Miners are acting in their self-interest and Devs are trying to secure the network and increase decentralisation.\nBoth are doing their job.\n\nJust by revealing the info, you have effectively ensured the nullification of any edge enjoyed by miners using the covert technique in the medium to long term.\nEither miners not using the technique will all start signalling for SegWit to nullify their competitors edge or they will procure hardware which has the edge.\n\nGiven the threat to decentralisation, i also believe UASF will gain more momentum as users seek to protect the network from further miner centralisation.\n\n\n________________________________\nFrom: bitcoin-dev-bounces at lists.linuxfoundation.org <bitcoin-dev-bounces at lists.linuxfoundation.org> on behalf of Gregory Maxwell via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>\nSent: Thursday, April 6, 2017 5:37 AM\nTo: Bitcoin Dev\nSubject: [bitcoin-dev] BIP proposal: Inhibiting a covert attack on the Bitcoin POW function\n\nA month ago I was explaining the attack on Bitcoin's SHA2 hashcash which\nis exploited by ASICBOOST and the various steps which could be used to\nblock it in the network if it became a problem.\n\nWhile most discussion of ASICBOOST has focused on the overt method\nof implementing it, there also exists a covert method for using it.\n\nAs I explained one of the approaches to inhibit covert ASICBOOST I\nrealized that my words were pretty much also describing the SegWit\ncommitment structure.\n\nThe authors of the SegWit proposal made a specific effort to not be\nincompatible with any mining system and, in particular, changed the\ndesign at one point to accommodate mining chips with forced payout\naddresses.\n\nHad there been awareness of exploitation of this attack an effort\nwould have been made to avoid incompatibility-- simply to separate\nconcerns.  But the best methods of implementing the covert attack\nare significantly incompatible with virtually any method of\nextending Bitcoin's transaction capabilities; with the notable\nexception of extension blocks (which have their own problems).\n\nAn incompatibility would go a long way to explain some of the\nmore inexplicable behavior from some parties in the mining\necosystem so I began looking for supporting evidence.\n\nReverse engineering of a particular mining chip has demonstrated\nconclusively that ASICBOOST has been implemented\nin hardware.\n\nOn that basis, I offer the following BIP draft for discussion.\nThis proposal does not prevent the attack in general, but only\ninhibits covert forms of it which are incompatible with\nimprovements to the Bitcoin protocol.\n\nI hope that even those of us who would strongly prefer that\nASICBOOST be blocked completely can come together to support\na protective measure that separates concerns by inhibiting\nthe covert use of it that potentially blocks protocol improvements.\n\nThe specific activation height is something I currently don't have\na strong opinion, so I've left it unspecified for the moment.\n\n<pre>\n  BIP: TBD\n  Layer: Consensus\n  Title: Inhibiting a covert attack on the Bitcoin POW function\n  Author: Greg Maxwell <greg at xiph.org>\n  Status: Draft\n  Type: Standards Track\n  Created: 2016-04-05\n  License: PD\n</pre>\n\n==Abstract==\n\nThis proposal inhibits the covert exploitation of a known\nvulnerability in Bitcoin Proof of Work function.\n\nThe key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n\"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this\ndocument are to be interpreted as described in RFC 2119.\n\n==Motivation==\n\nDue to a design oversight the Bitcoin proof of work function has a potential\nattack which can allow an attacking miner to save up-to 30% of their energy\ncosts (though closer to 20% is more likely due to implementation overheads).\n\nTimo Hanke and Sergio Demian Lerner claim to hold a patent on this attack,\nwhich they have so far not licensed for free and open use by the public.\nThey have been marketing their patent licenses under the trade-name\nASICBOOST.  The document takes no position on the validity or enforceability\nof the patent.\n\nThere are two major ways of exploiting the underlying vulnerability: One\nobvious way which is highly detectable and is not in use on the network\ntoday and a covert way which has significant interaction and potential\ninterference with the Bitcoin protocol.  The covert mechanism is not\neasily detected except through its interference with the protocol.\n\nIn particular, the protocol interactions of the covert method can block the\nimplementation of virtuous improvements such as segregated witness.\n\nExploitation of this vulnerability could result in payoff of as much as\n$100 million USD per year at the time this was written (Assuming at\n50% hash-power miner was gaining a 30% power advantage and that mining\nwas otherwise at profit equilibrium).  This could have a phenomenal\ncentralizing effect by pushing mining out of profitability for all\nother participants, and the income from secretly using this\noptimization could be abused to significantly distort the Bitcoin\necosystem in order to preserve the advantage.\n\nReverse engineering of a mining ASIC from a major manufacture has\nrevealed that it contains an undocumented, undisclosed ability\nto make use of this attack. (The parties claiming to hold a\npatent on this technique were completely unaware of this use.)\n\nOn the above basis the potential for covert exploitation of this\nvulnerability and the resulting inequality in the mining process\nand interference with useful improvements presents a clear and\npresent danger to the Bitcoin system which requires a response.\n\n==Background==\n\nThe general idea of this attack is that SHA2-256 is a merkle damgard hash\nfunction which consumes 64 bytes of data at a time.\n\nThe Bitcoin mining process repeatedly hashes an 80-byte 'block header' while\nincriminating a 32-bit nonce which is at the end of this header data. This\nmeans that the processing of the header involves two runs of the compression\nfunction run-- one that consumes the first 64 bytes of the header and a\nsecond which processes the remaining 16 bytes and padding.\n\nThe initial 'message expansion' operations in each step of the SHA2-256\nfunction operate exclusively on that step's 64-bytes of input with no\ninfluence from prior data that entered the hash.\n\nBecause of this if a miner is able to prepare a block header with\nmultiple distinct first 64-byte chunks but identical 16-byte\nsecond chunks they can reuse the computation of the initial\nexpansion for multiple trials. This reduces power consumption.\n\nThere are two broad ways of making use of this attack. The obvious\nway is to try candidates with different version numbers.  Beyond\nupsetting the soft-fork detection logic in Bitcoin nodes this has\nlittle negative effect but it is highly conspicuous and easily\nblocked.\n\nThe other method is based on the fact that the merkle root\ncommitting to the transactions is contained in the first 64-bytes\nexcept for the last 4 bytes of it.  If the miner finds multiple\ncandidate root values which have the same final 32-bit then they\ncan use the attack.\n\nTo find multiple roots with the same trailing 32-bits the miner can\nuse efficient collision finding mechanism which will find a match\nwith as little as 2^16 candidate roots expected, 2^24 operations to\nfind a 4-way hit, though low memory approaches require more\ncomputation.\n\nAn obvious way to generate different candidates is to grind the\ncoinbase extra-nonce but for non-empty blocks each attempt will\nrequire 13 or so additional sha2 runs which is very inefficient.\n\nThis inefficiency can be avoided by computing a sqrt number of\ncandidates of the left side of the hash tree (e.g. using extra\nnonce grinding) then an additional sqrt number of candidates of\nthe right  side of the tree using transaction permutation or\nsubstitution of a small number of transactions.  All combinations\nof the left and right side are then combined with only a single\nhashing operation virtually eliminating all tree related\noverhead.\n\nWith this final optimization finding a 4-way collision with a\nmoderate amount of memory requires ~2^24 hashing operations\ninstead of the >2^28 operations that would be require for\nextra-nonce  grinding which would substantially erode the\nbenefit of the attack.\n\nIt is this final optimization which this proposal blocks.\n\n==New consensus rule==\n\nBeginning block X and until block Y the coinbase transaction of\neach block MUST either contain a BIP-141 segwit commitment or a\ncorrect WTXID commitment with ID 0xaa21a9ef.\n\n(See BIP-141 \"Commitment structure\" for details)\n\nExisting segwit using miners are automatically compatible with\nthis proposal. Non-segwit miners can become compatible by simply\nincluding an additional output matching a default commitment\nvalue returned as part of getblocktemplate.\n\nMiners SHOULD NOT automatically discontinue the commitment\nat the expiration height.\n\n==Discussion==\n\nThe commitment in the left side of the tree to all transactions\nin the right side completely prevents the final sqrt speedup.\n\nA stronger inhibition of the covert attack in the form of\nrequiring the least significant bits of the block timestamp\nto be equal to a hash of the first 64-bytes of the header. This\nwould increase the collision space from 32 to 40 or more bits.\nThe root value could be required to meet a specific hash prefix\nrequirement in order to increase the computational work required\nto try candidate roots. These change would be more disruptive and\nthere is no reason to believe that it is currently necessary.\n\nThe proposed rule automatically sunsets. If it is no longer needed\ndue to the introduction of stronger rules or the acceptance of the\nversion-grinding form then there would be no reason to continue\nwith this requirement.  If it is still useful at the expiration\ntime the rule can simply be extended with a new softfork that\nsets longer date ranges.\n\nThis sun-setting avoids the accumulation of technical debt due\nto retaining enforcement of this rule when it is no longer needed\nwithout requiring a hard fork to remove it.\n\n== Overt attack ==\n\nThe non-covert form can be trivially blocked by requiring that\nthe header version match the coinbase transaction version.\n\nThis proposal does not include this block because this method\nmay become generally available without restriction in the future,\ndoes not generally interfere with improvements in the protocol,\nand because it is so easily detected that it could be blocked if\nit becomes an issue in the future.\n\n==Backward compatibility==\n\n\n==Implementation==\n\n\n==Acknowledgments==\n\n\n==Copyright==\n\nThis document is placed in the public domain.\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\nbitcoin-dev -- Bitcoin Protocol Discussion - Linux Foundation<https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>\nlists.linuxfoundation.org\nBitcoin development and protocol discussion. This list is lightly moderated. - No offensive posts, no personal attacks. - Posts must concern development of bitcoin ...\n\n\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170406/55036fce/attachment-0001.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2017-04-06T15:15:10",
                "message_text_only": "On Thu, Apr 6, 2017 at 2:30 PM, Luv Khemani via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> Just to add on to the ethical issue of blocking this.\n>\n>\n> If blocking the covert form of ASICBOOST is seen as unethical, then the same can be said about libsecp256k1, various client optimisations, Compactblocks.\n\nThis is simply a non sequitur. These optimizations benefit users. On\nthe other hand, asicboost doesn't benefit users in any way, it only\nbenefits some miners if and only if not all miners use it. It\nobviously harms the miners that aren't using it by making them less\nprofitable (maybe to the point that they lose money).\nIf all miners use it or if no one of them uses it is equivalent from\nthe point of view of the user. In fact, the very fact of allowing it\nmakes the network less secure unless every single honest miner uses\nit, for an attacker could use it against the network.\n\nEven if asicboost was good for users in any way (which as explained\nisn't), this proposal doesn't disable it, only the covert form that\ncannot be proven to be used.\n\nTherefore there's no rational arguments to oppose this proposal unless\nyou are (or are invested in):\n\nA) A Miner currently using the covert form of asicboost.\n\nB) A Miner planning to use the covert form of asicboost soon.\n\nC) An attacker using or planning to use the covert form of asicboost.\n\n> All of which seek to reduce the efficacy of large miners and selfish mining.\n\nAsicboost doesn't seek this and doesn't help with this in any way."
            },
            {
                "author": "Daniel Robinson",
                "date": "2017-04-06T15:41:32",
                "message_text_only": "I think you're misreading Luv. He's defending the idea of blocking covert\nASICBOOST, not defending ASICBOOST.\n\nOn Thu, Apr 6, 2017 at 11:16 AM Jorge Tim\u00f3n via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Thu, Apr 6, 2017 at 2:30 PM, Luv Khemani via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >\n> > Just to add on to the ethical issue of blocking this.\n> >\n> >\n> > If blocking the covert form of ASICBOOST is seen as unethical, then the\n> same can be said about libsecp256k1, various client optimisations,\n> Compactblocks.\n>\n> This is simply a non sequitur. These optimizations benefit users. On\n> the other hand, asicboost doesn't benefit users in any way, it only\n> benefits some miners if and only if not all miners use it. It\n> obviously harms the miners that aren't using it by making them less\n> profitable (maybe to the point that they lose money).\n> If all miners use it or if no one of them uses it is equivalent from\n> the point of view of the user. In fact, the very fact of allowing it\n> makes the network less secure unless every single honest miner uses\n> it, for an attacker could use it against the network.\n>\n> Even if asicboost was good for users in any way (which as explained\n> isn't), this proposal doesn't disable it, only the covert form that\n> cannot be proven to be used.\n>\n> Therefore there's no rational arguments to oppose this proposal unless\n> you are (or are invested in):\n>\n> A) A Miner currently using the covert form of asicboost.\n>\n> B) A Miner planning to use the covert form of asicboost soon.\n>\n> C) An attacker using or planning to use the covert form of asicboost.\n>\n> > All of which seek to reduce the efficacy of large miners and selfish\n> mining.\n>\n> Asicboost doesn't seek this and doesn't help with this in any way.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170406/f688b993/attachment.html>"
            },
            {
                "author": "Andreas Schildbach",
                "date": "2017-04-06T16:13:55",
                "message_text_only": "On 04/05/2017 11:37 PM, Gregory Maxwell via bitcoin-dev wrote:\n\n> Reverse engineering of a particular mining chip has demonstrated\n> conclusively that ASICBOOST has been implemented in hardware.\n\nDo you plan to release details about this, or is it already documented\nsomewhere?"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2017-04-06T21:38:31",
                "message_text_only": "On Wed, Apr 5, 2017 at 9:37 PM, Gregory Maxwell <greg at xiph.org> wrote:\n> each block MUST either contain a BIP-141 segwit commitment or a\n> correct WTXID commitment with ID 0xaa21a9ef.\n\nIt was just pointed out to me that the proposed ID (which I just\nselected to be above the segwit one) collides with one chosen in\nanother non-BIP proposal.  This wasn't intentional, and I'll happily\nchange the value when I update the document."
            },
            {
                "author": "Oliver Petruzel",
                "date": "2017-04-06T04:47:10",
                "message_text_only": "> > One of the things going for us here is that Bitmain has been keeping\nASICBOOST\n> > from their own customers - as far as we know they haven't been sharing\nit, and\n> > thus they're the only ones you can actually use it.\n> >\n> > So while we're pissing off Bitmain in disabling it, we wouldn't be\naffecting\n> > anyone else.\n> >\n> > Equally, mining is a zero-sum game: if no-one can use ASICBOOST, miners\nare in\n> > the same position as before. ASICBOOST is only relevant to miners like\nBitmain\n> > who have access to it while other miners don't.\n\nPeter -\nDo we know that for a fact, though? What evidence or intelligence do we\nhave to indicate Bitmain itself is the only entity using the covert boost?\n\nA few possibilities:\n1. They could have already shared it with a limited number of strategic\npartners;\n2. They could have offered to share it with various parties in exchange for\nsomething (money, support for BU, etc); or\n3. They could provide the custom firmware/software to select parties as a\ndirect response to this disclosure -- which would enhance their defenses\nagainst a soft fork.\n4. They could share the firmware/software with EVERY owner of their\nequipment in a last-ditch defense against a soft fork. (after all, some\nadvantage over other equipment manufacturers is still better than no\nadvantage, right?)\n\nAssumptions could lead to failure, so these are just some things to keep in\nmind.\n\nRespectfully,\nOliver\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170406/eb744e90/attachment.html>"
            },
            {
                "author": "Raystonn .",
                "date": "2017-04-06T04:49:42",
                "message_text_only": "Great catch, and a good proposal for a fix.  Pushing the activation height out to allow existing hardware to enter obsolescence prior to activation may help reduce miner resistance.  It may also avoid legal threats from those currently abusing.  If miners still resist, the threat of an earlier activation height is a good negotiating tool.\n\nRaystonn\n\nOn 5 Apr 2017 2:39 p.m., Gregory Maxwell via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\nA month ago I was explaining the attack on Bitcoin's SHA2 hashcash which\nis exploited by ASICBOOST and the various steps which could be used to\nblock it in the network if it became a problem.\n\nWhile most discussion of ASICBOOST has focused on the overt method\nof implementing it, there also exists a covert method for using it.\n\nAs I explained one of the approaches to inhibit covert ASICBOOST I\nrealized that my words were pretty much also describing the SegWit\ncommitment structure.\n\nThe authors of the SegWit proposal made a specific effort to not be\nincompatible with any mining system and, in particular, changed the\ndesign at one point to accommodate mining chips with forced payout\naddresses.\n\nHad there been awareness of exploitation of this attack an effort\nwould have been made to avoid incompatibility-- simply to separate\nconcerns.  But the best methods of implementing the covert attack\nare significantly incompatible with virtually any method of\nextending Bitcoin's transaction capabilities; with the notable\nexception of extension blocks (which have their own problems).\n\nAn incompatibility would go a long way to explain some of the\nmore inexplicable behavior from some parties in the mining\necosystem so I began looking for supporting evidence.\n\nReverse engineering of a particular mining chip has demonstrated\nconclusively that ASICBOOST has been implemented\nin hardware.\n\nOn that basis, I offer the following BIP draft for discussion.\nThis proposal does not prevent the attack in general, but only\ninhibits covert forms of it which are incompatible with\nimprovements to the Bitcoin protocol.\n\nI hope that even those of us who would strongly prefer that\nASICBOOST be blocked completely can come together to support\na protective measure that separates concerns by inhibiting\nthe covert use of it that potentially blocks protocol improvements.\n\nThe specific activation height is something I currently don't have\na strong opinion, so I've left it unspecified for the moment.\n\n<pre>\n  BIP: TBD\n  Layer: Consensus\n  Title: Inhibiting a covert attack on the Bitcoin POW function\n  Author: Greg Maxwell <greg at xiph.org>\n  Status: Draft\n  Type: Standards Track\n  Created: 2016-04-05\n  License: PD\n</pre>\n\n==Abstract==\n\nThis proposal inhibits the covert exploitation of a known\nvulnerability in Bitcoin Proof of Work function.\n\nThe key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n\"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this\ndocument are to be interpreted as described in RFC 2119.\n\n==Motivation==\n\nDue to a design oversight the Bitcoin proof of work function has a potential\nattack which can allow an attacking miner to save up-to 30% of their energy\ncosts (though closer to 20% is more likely due to implementation overheads).\n\nTimo Hanke and Sergio Demian Lerner claim to hold a patent on this attack,\nwhich they have so far not licensed for free and open use by the public.\nThey have been marketing their patent licenses under the trade-name\nASICBOOST.  The document takes no position on the validity or enforceability\nof the patent.\n\nThere are two major ways of exploiting the underlying vulnerability: One\nobvious way which is highly detectable and is not in use on the network\ntoday and a covert way which has significant interaction and potential\ninterference with the Bitcoin protocol.  The covert mechanism is not\neasily detected except through its interference with the protocol.\n\nIn particular, the protocol interactions of the covert method can block the\nimplementation of virtuous improvements such as segregated witness.\n\nExploitation of this vulnerability could result in payoff of as much as\n$100 million USD per year at the time this was written (Assuming at\n50% hash-power miner was gaining a 30% power advantage and that mining\nwas otherwise at profit equilibrium).  This could have a phenomenal\ncentralizing effect by pushing mining out of profitability for all\nother participants, and the income from secretly using this\noptimization could be abused to significantly distort the Bitcoin\necosystem in order to preserve the advantage.\n\nReverse engineering of a mining ASIC from a major manufacture has\nrevealed that it contains an undocumented, undisclosed ability\nto make use of this attack. (The parties claiming to hold a\npatent on this technique were completely unaware of this use.)\n\nOn the above basis the potential for covert exploitation of this\nvulnerability and the resulting inequality in the mining process\nand interference with useful improvements presents a clear and\npresent danger to the Bitcoin system which requires a response.\n\n==Background==\n\nThe general idea of this attack is that SHA2-256 is a merkle damgard hash\nfunction which consumes 64 bytes of data at a time.\n\nThe Bitcoin mining process repeatedly hashes an 80-byte 'block header' while\nincriminating a 32-bit nonce which is at the end of this header data. This\nmeans that the processing of the header involves two runs of the compression\nfunction run-- one that consumes the first 64 bytes of the header and a\nsecond which processes the remaining 16 bytes and padding.\n\nThe initial 'message expansion' operations in each step of the SHA2-256\nfunction operate exclusively on that step's 64-bytes of input with no\ninfluence from prior data that entered the hash.\n\nBecause of this if a miner is able to prepare a block header with\nmultiple distinct first 64-byte chunks but identical 16-byte\nsecond chunks they can reuse the computation of the initial\nexpansion for multiple trials. This reduces power consumption.\n\nThere are two broad ways of making use of this attack. The obvious\nway is to try candidates with different version numbers.  Beyond\nupsetting the soft-fork detection logic in Bitcoin nodes this has\nlittle negative effect but it is highly conspicuous and easily\nblocked.\n\nThe other method is based on the fact that the merkle root\ncommitting to the transactions is contained in the first 64-bytes\nexcept for the last 4 bytes of it.  If the miner finds multiple\ncandidate root values which have the same final 32-bit then they\ncan use the attack.\n\nTo find multiple roots with the same trailing 32-bits the miner can\nuse efficient collision finding mechanism which will find a match\nwith as little as 2^16 candidate roots expected, 2^24 operations to\nfind a 4-way hit, though low memory approaches require more\ncomputation.\n\nAn obvious way to generate different candidates is to grind the\ncoinbase extra-nonce but for non-empty blocks each attempt will\nrequire 13 or so additional sha2 runs which is very inefficient.\n\nThis inefficiency can be avoided by computing a sqrt number of\ncandidates of the left side of the hash tree (e.g. using extra\nnonce grinding) then an additional sqrt number of candidates of\nthe right  side of the tree using transaction permutation or\nsubstitution of a small number of transactions.  All combinations\nof the left and right side are then combined with only a single\nhashing operation virtually eliminating all tree related\noverhead.\n\nWith this final optimization finding a 4-way collision with a\nmoderate amount of memory requires ~2^24 hashing operations\ninstead of the >2^28 operations that would be require for\nextra-nonce  grinding which would substantially erode the\nbenefit of the attack.\n\nIt is this final optimization which this proposal blocks.\n\n==New consensus rule==\n\nBeginning block X and until block Y the coinbase transaction of\neach block MUST either contain a BIP-141 segwit commitment or a\ncorrect WTXID commitment with ID 0xaa21a9ef.\n\n(See BIP-141 \"Commitment structure\" for details)\n\nExisting segwit using miners are automatically compatible with\nthis proposal. Non-segwit miners can become compatible by simply\nincluding an additional output matching a default commitment\nvalue returned as part of getblocktemplate.\n\nMiners SHOULD NOT automatically discontinue the commitment\nat the expiration height.\n\n==Discussion==\n\nThe commitment in the left side of the tree to all transactions\nin the right side completely prevents the final sqrt speedup.\n\nA stronger inhibition of the covert attack in the form of\nrequiring the least significant bits of the block timestamp\nto be equal to a hash of the first 64-bytes of the header. This\nwould increase the collision space from 32 to 40 or more bits.\nThe root value could be required to meet a specific hash prefix\nrequirement in order to increase the computational work required\nto try candidate roots. These change would be more disruptive and\nthere is no reason to believe that it is currently necessary.\n\nThe proposed rule automatically sunsets. If it is no longer needed\ndue to the introduction of stronger rules or the acceptance of the\nversion-grinding form then there would be no reason to continue\nwith this requirement.  If it is still useful at the expiration\ntime the rule can simply be extended with a new softfork that\nsets longer date ranges.\n\nThis sun-setting avoids the accumulation of technical debt due\nto retaining enforcement of this rule when it is no longer needed\nwithout requiring a hard fork to remove it.\n\n== Overt attack ==\n\nThe non-covert form can be trivially blocked by requiring that\nthe header version match the coinbase transaction version.\n\nThis proposal does not include this block because this method\nmay become generally available without restriction in the future,\ndoes not generally interfere with improvements in the protocol,\nand because it is so easily detected that it could be blocked if\nit becomes an issue in the future.\n\n==Backward compatibility==\n\n\n==Implementation==\n\n\n==Acknowledgments==\n\n\n==Copyright==\n\nThis document is placed in the public domain.\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170406/d0ae8901/attachment-0001.html>"
            },
            {
                "author": "praxeology_guy",
                "date": "2017-04-06T07:47:04",
                "message_text_only": "If this is the underlying reason why SegWit is being delayed... that is pretty deplorable.\n\nProbably too late now for bitcoin, but maybe it would be good to pre-mix the block header bits around before it even enters the SHA256 hash. Not sure if best to use a hardcoded map, or to make the map with the tx merkle root as a seed. Depends on how hard it is to find good nonce (etc) bit location collisions.\n\nMaybe gmaxwell's solution is good enough for this particular problem... but the above recommendation might help improve bitcoin's available remaining puzzle difficulty.\n\nAnother thing that could be done is increase the number of times SHA256 is performed... but now we are really talking about altering the PoW algorithm. Correct me if I'm wrong: The more number of times its performed, the less any patent-able pre or post calculation skipping/caching have an effect on efficiency.\n\nCheers,\nPraxeology Guy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170406/d91bb805/attachment.html>"
            },
            {
                "author": "David Vorick",
                "date": "2017-04-06T12:13:23",
                "message_text_only": ">\n> Another thing that could be done is increase the number of times SHA256 is\n> performed... but now we are really talking about altering the PoW\n> algorithm.  Correct me if I'm wrong: The more number of times its\n> performed, the less any patent-able pre or post calculation\n> skipping/caching have an effect on efficiency.\n>\n\nThe more complex that the PoW algorithm is, the more likely it is that\nsomeone finds a unique and special method for optimizing it that they are\nable to patent. And the more difficult it is to create specialized hardware\nto run that algorithm, meaning that there will be fewer players who are\nable to do so profitably (higher fixed costs).\n\nIf you want to talk about changing the PoW algorithm, you really want to be\nlooking to simplify it so that it's more obvious (not that you can ever be\ncompletely sure) that there are no hidden or unexpected optimizations that\nsomeone could patent.\n\nWe can even do a lot better than SHA. Cryptographic hash functions need to\nbe collision resistant, and collision resistance is the property that\nusually breaks. Preimage resistance and partial preimage resistance (and\nsecond preimage resistance) is generally easier to protect - to the best of\nour knowledge, md5 would actually still be a secure PoW function today.\n\nIt's bitterly ironic to me that so much research and effort has been put\ninto making asic-resistant PoW algorithms when in the long run\nasic-resistance only leads to problems like these - single parties who have\nfound significant optimizations and not shared them, completely destroying\nany chance of a level playing field and giving themselves a centralized\nmonopoly - a result that is supremely unhealthy for the rest of the\ncommunity.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170406/35d33b8c/attachment.html>"
            },
            {
                "author": "Russell O'Connor",
                "date": "2017-04-06T13:55:31",
                "message_text_only": "Hi Jonathan,\n\nThe proposal raised here does not deny miners the ability to use ASICBOOST.\nMiners can still use overt ASICBOOST by version bit fiddling and get the\nsame power savings.  In fact, overt ASICBOOST is much easier to implement\nthan covert ASICBOOST, so I don't really understand what the objection is.\n\n\nOn Apr 6, 2017 13:44, \"Jonathan Toomim via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\nEthically, this situation has some similarities to the DAO fork. We have an\nentity who closely examined the code, found an unintended characteristic of\nthat code, and made use of that characteristic in order to gain tens of\nmillions of dollars. Now that developers are aware of it, they want to\nmodify the code in order to negate as much of the gains as possible.\n\nThere are differences, too, of course: the DAO attacker was explicitly\nmalicious and stole Ether from others, whereas Bitmain is just optimizing\ntheir hardware better than anyone else and better than some of us think\nthey should be allowed to.\n\nIn both cases, developers are proposing that the developers and a majority\nof users collude to reduce the wealth of a single entity by altering the\nblockchain rules.\n\nIn the case of the DAO fork, users were stealing back stolen funds, but\nthat justification doesn't apply in this case. On the other hand, in this\ncase we're talking about causing someone a loss by reducing the value of\nhardware investments rather than forcibly taking back their coins, which is\nless direct and maybe more justifiable.\n\nWhile I don't like patented mining algorithms, I also don't like the idea\nof playing Calvin Ball on the blockchain. Rule changes should not be\nemployed as a means of disempowering and empoverishing particular entities\nwithout very good reason. Whether patenting a mining optimization qualifies\nas good reason is questionable.\n\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170406/2a016626/attachment.html>"
            },
            {
                "author": "Daniele Pinna",
                "date": "2017-04-07T01:34:17",
                "message_text_only": "Can you please not forget to supply us more details on the claims made\nregarding the reverse engineering of the Asic chip?\n\nIt is absolutely crucial that we get these independently verified ASAP.\n\nDaniele\n\nMessage: 2\n> Date: Thu, 6 Apr 2017 21:38:31 +0000\n> From: Gregory Maxwell <greg at xiph.org>\n> To: Bitcoin Dev <bitcoin-dev at lists.linuxfoundation.org>\n> Subject: Re: [bitcoin-dev] BIP proposal: Inhibiting a covert attack on\n>         the     Bitcoin POW function\n> Message-ID:\n>         <CAAS2fgSTrMjKZVpL4wRidnzTCC9O3OEF=oCnROf1pggz2cDgJA at mail.\n> gmail.com>\n> Content-Type: text/plain; charset=UTF-8\n> On Wed, Apr 5, 2017 at 9:37 PM, Gregory Maxwell <greg at xiph.org> wrote:\n> > each block MUST either contain a BIP-141 segwit commitment or a\n> > correct WTXID commitment with ID 0xaa21a9ef.\n> It was just pointed out to me that the proposed ID (which I just\n> selected to be above the segwit one) collides with one chosen in\n> another non-BIP proposal.  This wasn't intentional, and I'll happily\n> change the value when I update the document.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170407/6fcc62db/attachment.html>"
            },
            {
                "author": "Emilian Ursu",
                "date": "2017-04-07T06:46:47",
                "message_text_only": "The fact that this is possible should be enough for us to implement \nmeassures against it.\n\nOn Fri, 7 Apr 2017, Daniele Pinna via bitcoin-dev wrote:\n\n> \n> Can you please not forget to supply us more details on the claims made regarding the reverse engineering of the Asic chip?\n> \n> It is absolutely crucial that we get these independently verified ASAP.\n> \n> Daniele\n>\n>       Message: 2\n>       Date: Thu, 6 Apr 2017 21:38:31 +0000\n>       From: Gregory Maxwell <greg at xiph.org>\n>       To: Bitcoin Dev <bitcoin-dev at lists.linuxfoundation.org>\n>       Subject: Re: [bitcoin-dev] BIP proposal: Inhibiting a covert attack on\n>       \u00a0 \u00a0 \u00a0 \u00a0 the\u00a0 \u00a0 \u00a0Bitcoin POW function\n>       Message-ID:\n>       \u00a0 \u00a0 \u00a0 \u00a0 <CAAS2fgSTrMjKZVpL4wRidnzTCC9O3OEF=oCnROf1pggz2cDgJA at mail.gmail.com>\n>       Content-Type: text/plain; charset=UTF-8\n>       On Wed, Apr 5, 2017 at 9:37 PM, Gregory Maxwell <greg at xiph.org> wrote:\n>       > each block MUST either contain a BIP-141 segwit commitment or a\n>       > correct WTXID commitment with ID 0xaa21a9ef.\n>       It was just pointed out to me that the proposed ID (which I just\n>       selected to be above the segwit one) collides with one chosen in\n>       another non-BIP proposal.\u00a0 This wasn't intentional, and I'll happily\n>       change the value when I update the document.\n> \n> \u00a0\n> \n>"
            },
            {
                "author": "Alex Mizrahi",
                "date": "2017-04-07T07:44:59",
                "message_text_only": "> Can you please not forget to supply us more details on the claims made\n> regarding the reverse engineering of the Asic chip?\n>\n> It is absolutely crucial that we get these independently verified ASAP.\n>\n\nBitmain confirmed that their chips support ASICBOOST and it can be used for\nmining:\n\nhttps://blog.bitmain.com/en/regarding-recent-allegations-smear-campaigns/\n\nThey claim that they don't use it on mainnet, but that claim cannot be\nverified. it is possible to do covert ASICBOOST in a 100% covert manner.\n(It can be done without \"transaction reordering\" so it's not worth\nanalyzing blocks etc.)\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170407/b6eb9b2e/attachment-0001.html>"
            },
            {
                "author": "praxeology_guy",
                "date": "2017-04-07T08:08:10",
                "message_text_only": "Daniele Pinna,\n\nCan you please not forget to supply us more details on the claims made regarding the reverse engineering of the Asic chip?\n\ngmaxwell told me that back even in S7 chips its possible to set the SHA256 midstate/IV instead of just resetting it to the standard SHA256 IV. This essentially allows you to re-use midstates, which is one of the key necessary features for the ASICBOOST optimization to work. From the chip's perspective there is not much difference between the covert and overt optimization methods, particularly given that the whole IV/midstate vector can be set.\n\nThe covert method just requires more work than the overt method:. overt you just permutate the version bits, vs the covert one requires you find partial hash collisions of the tx merkle root. The extra work to find the partial tx merkle root hash collisions could be done at different stages in the mining system... some speculate that it could be done in the miner's FPGA.\n\nNot sure how exactly gmaxwell (or his friend) did it. I don't currently own any mining hardware nor the time to do it myself.\n\nCheers,\nPraxeology Guy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170407/1a863b5f/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "BIP proposal: Inhibiting a covert attack on the Bitcoin POW function",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Andreas Schildbach",
                "Anthony Towns",
                "praxeology_guy",
                "Jonathan Toomim",
                "David Vorick",
                "Timo Hanke",
                "Peter Todd",
                "Marco",
                "Jorge Tim\u00f3n",
                "Gregory Maxwell",
                "Erik Aronesty",
                "Bram Cohen",
                "Emilian Ursu",
                "Bryan Bishop",
                "Luv Khemani",
                "Daniel Robinson",
                "Daniele Pinna",
                "Alex Mizrahi",
                "Jannes Faber",
                "Luke Dashjr",
                "Jared Lee Richardson",
                "Oliver Petruzel",
                "Thomas Daede",
                "bfd at cock.lu",
                "theymos",
                "Russell O'Connor",
                "Raystonn .",
                "Joseph Poon"
            ],
            "messages_count": 49,
            "total_messages_chars_count": 158717
        }
    },
    {
        "title": "[bitcoin-dev] bip-genvbvoting : more complete specification up for review",
        "thread_messages": [
            {
                "author": "Sancho Panza",
                "date": "2017-04-06T17:12:46",
                "message_text_only": "Hi all,\n\nI have put up an initial draft of the full 'bip-genvbvoting' (generalized version bits voting) specification for review:\n\nhttps://github.com/sanch0panza/bips/blob/bip-genvbvoting/bip-genvbvoting.mediawiki\n\nComments are again most welcome - and my thanks to those reviewers who took a look at the initial rough draft [1].\n\nA prime goal is that this BIP proposal should end up allowing full backward compatibility with the existing BIP9 state machine, if wishing to do so for a deployment. In fact, this will be necessary to maintain full compatibility with any ongoing deployments.\n\nI will work on a reference implementation which might also turn up inadequacies of the proposed specification. A link to this will follow once it is mature enough for review.\n\nSancho\n\n[1]https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-April/013969.html\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170406/cfcbb5b6/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "bip-genvbvoting : more complete specification up for review",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Sancho Panza"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1039
        }
    },
    {
        "title": "[bitcoin-dev] Praxeological Analysis of PoW Policy Changes, Re: ASICBOOST",
        "thread_messages": [
            {
                "author": "praxeology_guy",
                "date": "2017-04-06T20:12:21",
                "message_text_only": "Praxeological Analysis of PoW Policy Changes, Re: ASICBOOST\n\n=========================\nOn the $100M profit claim\n=========================\n\nFirst I'd like to confirm Gregory Maxwell's assertion that covert use of ASICBOOST could result in $100 million USD per year profits.\n\nprofit = reward - costs.\nTotal reward is fixed at (12.5 block reward + 3 fees) * 6 per hour * 24 per day * 365.25 days per year * $1150 USD per bitcoin ~= 1,000,000,000 or 1Billion USD per year.\n\nMiners normally compete against each other until there is only a very small, practically zero profit. Lets say that 50% of the mining hashpower are operating at profit = 0, and the other 50% are operating with > 0 profit due to the 20% increased efficiency of the covert optimization.\n\nHow much profit is earned by the covert optimization operators?\n\nHalf of the operators would have a cost of ~$500,000,000.\nHalf would have a cost of ~0.8 * ~$500,000,000 = $400,000,000, leaving profit = $100,000,000.\n\nBut does this make sense? What if 95% of hashing power miners used the more efficient process, and 5% didn't. Would this still result in using a similar formula, with $950M * 0.2 = $190M profits? I believe it would. Essentially, the the 95% of the miners are colluding to not increase their capital & hashing power enough to erase their profits. Hence an entity or multiple entities may be colluding to decrease the security of ordering (double spend prevention) of Bitcoin transactions.\n\nHence a claim that as much as $100M per year could be gained by using the ASICBOOST Optimization is a valid claim.\n\n==================================================\nMiners and Money Owners have Different Motivations\n==================================================\n\nMoney owners and miners have different motivations. Miners are currently concerned about the 1-2 year ROI of their capital. In the long term, as ASIC technology for Bitcoin matures, miners will have a longer term ROI concern. For money owners: Short term money owners are looking to transfer their money in the most efficient manner. Long term money owners are looking for a money they expect will become more valuable in the future due to its ability to handle more users with a higher money transfer efficiency than other competing currencies.\n\n$100M per year is a pretty good reason for a miner to want to delay Bitcoin policy improvements that primarily benefit the money owner, yet have only marginal utilitarian benefits for the miner, but evaporate their ability to have such an income.\n\n================================\nMoney Owner Perspective Analysis\n================================\n\nMoney owners strive to have a have a PoW algorithm that does not give a subset of the world an advantage by government interference. Such interference threatens bitcoin's decentralized nature, and hence the users' ability to have a money who's policies are dictated by themselves rather than a centralized entity.\n\nChanging the PoW algorithm in a way that makes existing ASIC miner capital worthless... is undesirable because it creates new opportunities for first to market optimizations to centralize mining. It also makes bitcoin's security weaker because the uncertainty of the PoW algorithm de-incentivizes the effort to invest in mining capital, which creates a larger threat for a future malicious threat to perform the 51% attack. For the duration that a new PoW algorithm is not fully optimized with the current latest ASIC manufacturing techniques, and there remains undiscovered optimizations, the double spend security is weaker.\n\nGregory Maxwell's proposal does not make existing mining capital worthless... it only removes the advantage of using the patent encumbered optimization. Existing capital, particularly the S9, remains being the most efficient capital available for mining Bitcoin. Activating such a proposal will set a precedent for mining equipment manufacturers and operators to expect that certain classes of patented optimizations will only have a limited ROI timeframe before they are made unavailable due to users changing the PoW policy. Miners may still pursue optimizations that are not encumbered by patents without concern that their optimization advantage will be disabled just for the purpose of benefiting some other arbitrary set of miners.\n\nGiven that a money owner would not want Bitcoin's ability to transfer money efficently be encumbered in the long term for the sake of miner's profits... in the case where even a non-patent encumbered optimization conflicts with an upgrade to Bitcoin for the money owners... then its a question of how much the change increases bitcoin's money transfer efficiency, and how generous the money owners are towards allowing the optimization-capital-invested miner. I use the word \"generous\" because the policy users choose for the money supply is entirely voluntary. No contract was made to continue using the same exact PoW algorithm. The guiding reason to keep or change the PoW algorithm to increase the money transfer efficiency of the money. Double spends, and the properties of PoW that secure against double spends, are a large factor in determining such efficiency.\n\n========================================================\nImpact of Changing the PoW Policy vs Covert ASICBOOST\n========================================================\n\nMiners currently using this optimization will lose 20% profits. Old and less efficient mining equipment using the optimization might no longer be profitable in mining Bitcoins. Miners using this optimization but using more costly energy may also no longer be profitable in mining Bitcoins. Difficulty will decrease, and miners not using the optimization will have greater profits and grow in numbers. The difficulty decrease may make older equipment and higher cost energy locations become profitable once again.\n\nLong term impact on miners: As discussed in the money owner's perspective, mainly this will reduce the motivation to perform the R&D, manufacturing, and purchase of patent encumbered optimizations. Realizing that the users may also at a future date disable an optimization in order to in some way make an improvement to Bitcoin will also put a damper on advancing the development of more efficient mining hardware, which is once again desirable to users as it makes the transaction ordering more future proof.\n\nThis may also be a lesson to hardware manufacturers that they should not make their chips extremely special purpose... that having some flexibility in the algorithms the device can run may help make their hardware still have other uses in the case that users decide to change the PoW policy. For example, it may be wise for the manufacturer to support an operating mode where only the nonce bit are permutated and no SHA256 operations are skipped due other assumptions about the block header data.\n\n===============================\nPraxeology Guy's Recommendation\n===============================\n\nMake it a policy that patent encumbered PoW optimizations are countered/prevented if possible while minimizing the disruption on the utility and availability of optimized mining capital equipment. Owners of Bitcoin should support and activate the proposed PoW policy change by Gregory Maxwell as soon as possible to counter the ASICBOOST patent encumbrance... unless the creators of the ASICBOOST patent transfer their IP to the public domain. SegWit should not be delayed for the purpose of being generous to those who first implement ASICBOOST in their mining operations. Future ASICs and mining equipment should be made with the option to run without optimizations that make assumptions about policy that is subject to change in a future soft fork.\n\nCheers,\nPraxeology Guy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170406/6dd461aa/attachment-0001.html>"
            },
            {
                "author": "praxeology_guy",
                "date": "2017-04-07T00:11:00",
                "message_text_only": "> \"... put a damper on advancing the development of more efficient mining hardware, which is once again desirable to users as it makes the transaction ordering more future proof.\"\n\nRun on sentence sorry. I meant to say that development of more efficient/mature mining hardware sooner is desirable to money owners/traders. So anything that could dis-incentivize R&D to mature ASICs would be bad. PoW policy changes should be made carefully in order to minimize this hampering effect.\n\nI didn't mean to imply that Gregory Maxwell's current BIP countered/disabled both the evident and covert versions of asicboost. I think his BIP is a good idea, to quickly release a version that blocks the patented covert optimization... and then later we can consider taking steps to further disable the patented evident version of asicboost if it becomes a problem.\n\nCheers,\nPraxeology Guy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170406/9e69691e/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Praxeological Analysis of PoW Policy Changes, Re: ASICBOOST",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "praxeology_guy"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 8991
        }
    },
    {
        "title": "[bitcoin-dev] Draft BIP: Version bits extension with guaranteed lock-in",
        "thread_messages": [
            {
                "author": "shaolinfry",
                "date": "2017-04-06T21:25:02",
                "message_text_only": "After some thought I managed to simplify the original uaversionbits proposal introducing a simple boolean flag to guarantee lock-in of a BIP9 deployment by the timeout. This seems to be the simplest form combining optional flag day activation with BIP9. This brings the best of both worlds allowing user activated soft forks that can be activated early by the hash power.\n\nSpecification: https://github.com/shaolinfry/bips/blob/bip-uavb/bip-uaversionbits.mediawiki\nPrevious discussion: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-February/013643.html\n\n<pre>\nBIP: ?\nTitle: Version bits extension with guaranteed lock-in\nAuthor: Shaolin Fry <shaolinfry at protonmail.ch>\nComments-Summary: No comments yet.\nComments-URI: https://github.com/bitcoin/bips/wiki/Comments:BIP-????\nStatus: Draft\nType: Informational\nCreated: 2017-02-01\nLicense: BSD-3-Clause\nCC0-1.0\n</pre>\n\n==Abstract==\n\nThis document specifies an extension to BIP9 that introduces an additional activation parameter to guarantee activation of backward-compatible changes (further called \"soft forks\").\n\n==Motivation==\n\nBIP9 introduced a mechanism for doing parallel soft forking deployments based on repurposing the block nVersion field. Activation is dependent on near unanimous hashrate signalling which may be impractical and is also subject to veto by a small minority of non-signalling hashrate.\n\nThis specification provides a way to optionally guarantee lock-in at the end of the BIP9 timeout, and therefore activation.\n\n==Specification==\n\nThis specification adds a new per-chain deployment parameter to the existing BIP9 specification as follows:\n\n# The '''lockinontimeout''' boolean if set to true, will transition state to LOCKED_IN at timeout if not already ACTIVE.\n\n===State transitions===\n\n<img src=\"bip-uaversionbits/states.png\" align=\"middle\"></img>\n\nThe state transition workflow is exactly the same as in BIP9 with an additional rule: During the STARTED state if the '''lockinontimeout''' is set to true, the state will transition to LOCKED_IN when '''timeout''' is reached.\n\ncase STARTED:\n// BIP9 specification follows\nif (GetMedianTimePast(block.parent) >= timeout) {\nreturn (fLockInOnTimeout == true) ? THRESHOLD_LOCKED_IN : THRESHOLD_FAILED\n}\nint count = 0;\nwalk = block;\nfor (i = 0; i < 2016; i++) {\nwalk = walk.parent;\nif (walk.nVersion & 0xE0000000 == 0x20000000 && (walk.nVersion >> bit) & 1 == 1) {\ncount++;\n}\n}\nif (count >= threshold) {\nreturn LOCKED_IN;\n}\nreturn STARTED;\n\n=== Reference implementation ===\n\nhttps://github.com/bitcoin/bitcoin/compare/master...shaolinfry:bip-uaversionbits\n\n==Deployments==\n\nA living list of deployment proposals can be found [[bip-0009/assignments.mediawiki|here]].\n\n==Copyright==\n\nThis document is dual licensed as BSD 3-clause, and Creative Commons CC0 1.0 Universal.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170406/f7781921/attachment-0001.html>"
            },
            {
                "author": "praxeology_guy",
                "date": "2017-04-07T08:38:03",
                "message_text_only": "shaolinfry,\n\nNot sure if you noticed my comments on your earlier orphaning proposal... but if you did you should already know that I really like this proposal... particularly since orphaning valid old blocks is completely unnecessary.\n\nI really like how you pulled out the \"lockinontimeout\" variable so that this same method could be used in future softfork proposals... instead of hardcoding a special case hack for SegWit.\n\n- it would be nice if the user could set this variable in a configuration file.\n- it would be nice if the user could set the \"nTimeout\" in \"src/chainparams.cpp\" in a configuratoin file too. This could be used allow a user to expedite when a softfork would become active on his node when combined with .\"lockinontimeout\".\n\nDevelopers such as the Core team could put more conservative values in the program, and then community members such as miners and nodes who feel more strongly about SegWit could either compile their own settings or maybe copy a popular configuration file if such was made possible.\n\nCheers,\nPraxeology Guy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170407/5c37bfa4/attachment-0001.html>"
            },
            {
                "author": "Ryan Grant",
                "date": "2017-04-07T13:55:57",
                "message_text_only": "The primary failure mode of a user's misconfiguration of nTimeout will\nbe a stopped chain.\n\nIf less-sophisticated users are offered these configuration settings\nthen chaintip progress failures that result from them should be\nprominently displayed."
            },
            {
                "author": "praxeology_guy",
                "date": "2017-04-07T17:56:55",
                "message_text_only": "Ryan Grant,\n\nTLDR Unless I'm missing something, your claim that a misconfiguration would result in a stop chain is wrong because BIP9 only works on soft forks.\n\nDoes BIP9 work with hard forks? Pretty sure it is only for soft forks. If you want to make a hard fork, there is not much point in waiting for any particular miner hash power adoption rate.\n\nWith a softfork, here is the only condition for a \"stopped chain\":\n1. User adopts more stringent rules.\n2. Someone maliciously creates an invalid block as evaluated by the more stringent rules in #1, but that is valid to older nodes\n3. No one ever mines a different block at the height of the block in #2, instead all of the miners only build on top of the block built at #2.\n\nThe user would have to adopt a soft fork at a time where no miner has also done the same, and where someone creates a contradictory block (which normally wouldn't happen unless someone was being malicious).\n\nNever the less, I kind of like the idea of the user being notified when a newly activated more stringent soft fork rule caused a block to be rejected. The first time it happens, a message could come up, and then for some time after maybe it would be logged somewhere easily accessible. Such an event could be an excellent trigger to enable replay attack prevention, although maybe not automatically... unless everyone was pretty sure that a long-standing competing fork was likely to occur.\n\nCheers,\nPraxeology Guy\n\n-------- Original Message --------\nSubject: Re: [bitcoin-dev] Draft BIP: Version bits extension with guaranteed lock-in\nLocal Time: April 7, 2017 8:55 AM\nUTC Time: April 7, 2017 1:55 PM\nFrom: bitcoin-dev at lists.linuxfoundation.org\nTo: Bitcoin Protocol Discussion <bitcoin-dev at lists.linuxfoundation.org>\n\nThe primary failure mode of a user's misconfiguration of nTimeout will\nbe a stopped chain.\n\nIf less-sophisticated users are offered these configuration settings\nthen chaintip progress failures that result from them should be\nprominently displayed.\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170407/3fcd54e0/attachment.html>"
            },
            {
                "author": "Ryan Grant",
                "date": "2017-04-08T04:48:34",
                "message_text_only": "Praxeology Guy,\n\nOn Fri, Apr 7, 2017 at 12:56 PM, praxeology_guy\n<praxeology_guy at protonmail.com> wrote:\n> TLDR Unless I'm missing something, your claim that a\n> misconfiguration would result in a stop chain is wrong because BIP9\n> only works on soft forks.\n\nIf our rule change timing is different from changes on the chain with\nmost work, then (extending Johnson Lau's terminology a bit) we may\nexperience subjective hardfork-ness; due to miners creating blocks\nwhich the economic majority goes on to accept, though they have a less\nrestrictive ruleset than ours.\n\n> The user would have to adopt a soft fork at a time where no miner\n> has also done the same, and where someone creates a contradictory\n> block (which normally wouldn't happen unless someone was being\n> malicious).\n\nCorrect for the segwit soft fork, which is narrowing the definition\nof a nonstandard transaction.  It's safe to say that if a block with a\ntx violating cleanstack were to occur on a non-segwit chain, that it\nwas for malicious reasons.\n\nHowever, some future forks - that a full node experiences as\nlow subjective hardfork-ness (i.e. soft forks) - might restrict\nmore common things.\n\n> Never the less, I kind of like the idea of the user being notified\n> when a newly activated more stringent soft fork rule caused a block\n> to be rejected.  The first time it happens, a message could come up,\n> and then for some time after maybe it would be logged somewhere\n> easily accessible.\n\nSure, a nice-to-have would be a SetfLargeWorkInvalidChainFound() that\nwas aware as well, though clients can make these decisions themselves."
            },
            {
                "author": "Kekcoin",
                "date": "2017-04-18T12:37:29",
                "message_text_only": "> After some thought I managed to simplify the original uaversionbits proposal introducing a simple boolean flag to guarantee lock-in of a BIP9 deployment by the timeout. This seems to be the simplest form combining optional flag day activation with BIP9. This brings the best of both worlds allowing user activated soft forks that can be activated early by the hash power.\n\nAfter mulling over this proposal I think it is quite elegant; however there is one big \"regression\" in functionality in regards to BIP9 which it extends upon; a lack of back-out procedure. That is to say, if a protocol change is deployed using this BIP9-with-lock-in-on-timeout method, it is no longer possible to abstain from activating it if it is shown to contain a critical flaw.\n\nI suggest that a second version bit can be used as an abandonment vote; with sufficient hashpower (50% might be enough since it is no longer about safe coordination of protocol change deployment) the proposed protocol change is abandoned. This changes the dynamic from BIP9's \"opt-in\" to an \"opt-out\" system, still governed by hashpower, but far less susceptible to minority miner veto.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170418/7599c704/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Draft BIP: Version bits extension with guaranteed lock-in",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "shaolinfry",
                "Ryan Grant",
                "Kekcoin",
                "praxeology_guy"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 9782
        }
    },
    {
        "title": "[bitcoin-dev] Using a storage engine without UTXO-index",
        "thread_messages": [
            {
                "author": "Tomas",
                "date": "2017-04-06T22:12:27",
                "message_text_only": "I have been working on a bitcoin implementation that uses a different\napproach to indexing for verifying the order of transactions. Instead of\nusing an index of unspent outputs, double spends are verified by using a\nspend-tree where spends are scanned against spent outputs instead of\nunspent outputs.\n\nThis allows for much better concurrency, as not only blocks, but also\nindividual inputs can be verified fully in parallel.\n\nI explain the approach at https://bitcrust.org, source code is available\nat https://github.com/tomasvdw/bitcrust\n\nI am sharing this not only to ask for your feedback, but also to call\nfor a clear separation of protocol and implementations: As this\nsolution, reversing the costs of outputs and inputs, seems to have\nexcellent performance characteristics (as shown in the test results),\nupdates to the protocol addressing the UTXO growth, might not be worth\nconsidering *protocol improvements* and it might be best to address\nthese concerns as implementation details.\n\nKind regards,\nTomas van der Wansem\ntomas at bitcrust.org\nBitcrust"
            },
            {
                "author": "Eric Voskuil",
                "date": "2017-04-06T23:38:23",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\n\nOn 04/06/2017 03:12 PM, Tomas via bitcoin-dev wrote:\n\nHi Tomas,\n\n> I have been working on a bitcoin implementation that uses a\n> different approach to indexing for verifying the order of\n> transactions. Instead of using an index of unspent outputs, double\n> spends are verified by using a spend-tree where spends are scanned\n> against spent outputs instead of unspent outputs.\n\nThis is the approach that genjix used in libbitcoin version2. With the\nexception of de-linking (not deleted) in the case of reorgs, the\nentire store is append only, implemented in a small set of memory\nmapped files. The downsides to the approach are:\n\n(1) higher than necessary storage space requirement due to storing the\nindexing data required for correlate the spends, and\n\n(2) higher than necessary validation complexity and cost in terms of\ncomputing the spent-ness (including spender height) of an output.\n\nHis implementation used a hash table, so performance-wise it did quite\nwell and would theoretically outperform a tree, O(1) vs. O(log2(N)).\n\n> This allows for much better concurrency, as not only blocks, but\n> also individual inputs can be verified fully in parallel.\n\nI was successful in parallelizing input validation (across the inputs\nof an unconfirmed tx and across the set of all inputs in a block)\nusing the v2 store. However, it is not the case that the spends\napproach is necessary for concurrency.\n\nTo resolve the above two problems the version3 store does not use a\nspends table/index. Nor does it store any table of UTXOs. Yet\nvalidation is highly parallelized. Instead of additional indexes it\nuses the tx hash table, augmented with 32 bits per output for spender\nheight. So there is a O(1) cost of finding the tx and a O(N) cost of\nfinding the spender height where N is the number of outputs in the tx.\nBut because the number of outputs in a tx is bounded (by block size)\nthis is constant time in the number of transactions.\n\nThis works out much faster than the spends table, and without the\nstorage cost or complexity disadvantages. It also scales with\navailable hardware, as the memory mapped files become in-memory hash\ntables. For low memory machines we found it was important to implement\nan opaque UTXO cache to limit paging, but for higher end systems zero\ncache is optimal.\n\n> I am sharing this not only to ask for your feedback, but also to\n> call for a clear separation of protocol and implementations: As\n> this solution, reversing the costs of outputs and inputs, seems to\n> have excellent performance characteristics (as shown in the test\n> results), updates to the protocol addressing the UTXO growth, might\n> not be worth considering *protocol improvements* and it might be\n> best to address these concerns as implementation details.\n\nI don't follow this part, maybe you could clarify. A spends index\ngrows with the size of the spend set (forever) as it cannot be pruned,\nwhich certainly exceeds the size of the UTXO set (unless nothing is\nspent). The advantage is that you don't have to keep rewriting the\nstore when you use a spends set (because the store can be append only).\n\nFeel free to message me if you'd like to discuss in more detail, or to\ncontinue on the libbitcoin mailing list (copied).\n\ne\n-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v2.0.22 (GNU/Linux)\n\niQEcBAEBCAAGBQJY5tFpAAoJEDzYwH8LXOFOcMgH/2mw5iOvUYNwvZ2z0KKTSUOA\nPd8d5mKoWvd94QxhQ+RyTbkEkMhHl75+zcBgRsfUTtZlBIe/Z0+OgVIN6ibEw+WD\nw7k3HqgQi9gLgydEelxTAX+z3dJ24n4kCCdKAmZbBuK+Yr/7AViugbEqYemKepku\npRWZZS74MUvrYesc0xPn4Ao3DTzMjjY0K2mkuqV8jlwdfZjlAQX9pTx+iSCuMhkd\nHJ8w7s8QnjVnUeOlLe29mZwaFJPyOTLJMqgDE6s2sXacAy5QQbVCatygvDQ8A/wC\nktBnKPFb2lGX3bGKu/KwABegBy/hyec+NP0wFR+0MVivCwTK1+SjeHu5MNOSVlM=\n=tfVj\n-----END PGP SIGNATURE-----"
            },
            {
                "author": "Tomas",
                "date": "2017-04-07T00:17:47",
                "message_text_only": "Hi Eric,\n\nThanks, but I get the impression that the similarity is rather\nsuperficial.  \n\nTo address your points:\n\n> (1) higher than necessary storage space requirement due to storing the\n> indexing data required for correlate the spends, and\n\nHmm. No. Spends are simply scanned in the spend-tree (full tree,\nprunable, fully 5.6gb), or caught by the spend-index (bit index,\nnon-prunable, fully 180mb). Neither impose significant storage\nrequirements.\n\n> 2) higher than necessary validation complexity and cost in terms of\n> computing the spent-ness (including spender height) of an output.\n>\n> With the exception of de-linking (not deleted) in the case of reorgs, the\n> entire store is append only, implemented in a small set of memory\n> mapped file\n\nI guess this is the key difference. As the spend-tree stores the spend\ninformation in a tree structure, no reorgs are required, and the\nresulting code is actually much less complex.\n\nBitcrust simply scans the tree. Although earlier designs used a\nskip-list, it turns out that accompanied by a spent-index lagging a few\nblocks behind, raw scanning is faster then anything even though it needs\nto scan ~5 blocks times ~4000 inputs before reaching the first\nspent-index,  the actual scan is highly cache efficient and little more\nthen a \"REP SCASQ\", reaching sub-microsecond per input on each core\n*including* the lookup in the spend index.\n\n > I don't follow this part, maybe you could clarify. A spends index\n> grows with the size of the spend set (forever) as it cannot be pruned,\n> which certainly exceeds the size of the UTXO set (unless nothing is\n> spent). The advantage is that you don't have to keep rewriting the\n> store when you use a spends set (because the store can be append only).\n\nMy point is, that the spend tree grows per *input* of a transaction\ninstead of per *output* of a transaction, because this is what is\nscanned on order validation.\n\nThe spend tree can be pruned because the spend index (~200mb) catches\nearly spends.\n\nDisregarding the baseload script validation, the peak load order\nvalidation of bitcrust is more negatively effected by a transaction with\nmany inputs than by a transaction of many outputs.\n\nI encourage you to check out the results at https://bitcrust.org\n\nRegards,\nTomas\n\nOn Fri, Apr 7, 2017, at 01:38, Eric Voskuil wrote:\n> -----BEGIN PGP SIGNED MESSAGE-----\n> Hash: SHA256\n> \n> On 04/06/2017 03:12 PM, Tomas via bitcoin-dev wrote:\n> \n> Hi Tomas,\n> \n> > I have been working on a bitcoin implementation that uses a\n> > different approach to indexing for verifying the order of\n> > transactions. Instead of using an index of unspent outputs, double\n> > spends are verified by using a spend-tree where spends are scanned\n> > against spent outputs instead of unspent outputs.\n> \n> This is the approach that genjix used in libbitcoin version2. With the\n> exception of de-linking (not deleted) in the case of reorgs, the\n> entire store is append only, implemented in a small set of memory\n> mapped files. The downsides to the approach are:\n> \n> (1) higher than necessary storage space requirement due to storing the\n> indexing data required for correlate the spends, and\n> \n> (2) higher than necessary validation complexity and cost in terms of\n> computing the spent-ness (including spender height) of an output.\n> \n> His implementation used a hash table, so performance-wise it did quite\n> well and would theoretically outperform a tree, O(1) vs. O(log2(N)).\n> \n> > This allows for much better concurrency, as not only blocks, but\n> > also individual inputs can be verified fully in parallel.\n> \n> I was successful in parallelizing input validation (across the inputs\n> of an unconfirmed tx and across the set of all inputs in a block)\n> using the v2 store. However, it is not the case that the spends\n> approach is necessary for concurrency.\n> \n> To resolve the above two problems the version3 store does not use a\n> spends table/index. Nor does it store any table of UTXOs. Yet\n> validation is highly parallelized. Instead of additional indexes it\n> uses the tx hash table, augmented with 32 bits per output for spender\n> height. So there is a O(1) cost of finding the tx and a O(N) cost of\n> finding the spender height where N is the number of outputs in the tx.\n> But because the number of outputs in a tx is bounded (by block size)\n> this is constant time in the number of transactions.\n> \n> This works out much faster than the spends table, and without the\n> storage cost or complexity disadvantages. It also scales with\n> available hardware, as the memory mapped files become in-memory hash\n> tables. For low memory machines we found it was important to implement\n> an opaque UTXO cache to limit paging, but for higher end systems zero\n> cache is optimal.\n> \n> > I am sharing this not only to ask for your feedback, but also to\n> > call for a clear separation of protocol and implementations: As\n> > this solution, reversing the costs of outputs and inputs, seems to\n> > have excellent performance characteristics (as shown in the test\n> > results), updates to the protocol addressing the UTXO growth, might\n> > not be worth considering *protocol improvements* and it might be\n> > best to address these concerns as implementation details.\n> \n> I don't follow this part, maybe you could clarify. A spends index\n> grows with the size of the spend set (forever) as it cannot be pruned,\n> which certainly exceeds the size of the UTXO set (unless nothing is\n> spent). The advantage is that you don't have to keep rewriting the\n> store when you use a spends set (because the store can be append only).\n> \n> Feel free to message me if you'd like to discuss in more detail, or to\n> continue on the libbitcoin mailing list (copied).\n> \n> e\n> -----BEGIN PGP SIGNATURE-----\n> Version: GnuPG v2.0.22 (GNU/Linux)\n> \n> iQEcBAEBCAAGBQJY5tFpAAoJEDzYwH8LXOFOcMgH/2mw5iOvUYNwvZ2z0KKTSUOA\n> Pd8d5mKoWvd94QxhQ+RyTbkEkMhHl75+zcBgRsfUTtZlBIe/Z0+OgVIN6ibEw+WD\n> w7k3HqgQi9gLgydEelxTAX+z3dJ24n4kCCdKAmZbBuK+Yr/7AViugbEqYemKepku\n> pRWZZS74MUvrYesc0xPn4Ao3DTzMjjY0K2mkuqV8jlwdfZjlAQX9pTx+iSCuMhkd\n> HJ8w7s8QnjVnUeOlLe29mZwaFJPyOTLJMqgDE6s2sXacAy5QQbVCatygvDQ8A/wC\n> ktBnKPFb2lGX3bGKu/KwABegBy/hyec+NP0wFR+0MVivCwTK1+SjeHu5MNOSVlM=\n> =tfVj\n> -----END PGP SIGNATURE-----"
            },
            {
                "author": "Eric Voskuil",
                "date": "2017-04-08T22:37:50",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\n\nOn 04/06/2017 05:17 PM, Tomas wrote:\n> Thanks, but I get the impression that the similarity is rather \n> superficial.\n\nMy point was that \"Using a storage engine without UTXO-index\" has been\ndone, and may be a useful reference, not that implementation details\nare the same.\n\n> To address your points:\n\nBelow you addressed two points I made regarding the downside of the\noriginal libbitcoin implementation. These were initial learnings that\ninformed future implementations (also without a UTXO index). These\nwere not comparisons to your implementation.\n\n>> (1) higher than necessary storage space requirement due to\n>> storing the indexing data required for correlate the spends, and\n> \n> Hmm. No. Spends are simply scanned in the spend-tree (full tree, \n> prunable, fully 5.6gb), or caught by the spend-index (bit index, \n> non-prunable, fully 180mb). Neither impose significant storage \n> requirements.\n> \n>> 2) higher than necessary validation complexity and cost in terms\n>> of computing the spent-ness (including spender height) of an\n>> output.\n>> \n>> With the exception of de-linking (not deleted) in the case of\n>> reorgs, the entire store is append only, implemented in a small\n>> set of memory mapped file\n> \n> I guess this is the key difference. As the spend-tree stores the\n> spend information in a tree structure, no reorgs are required, and\n> the resulting code is actually much less complex.\n\nThe references to \"higher than necessary storage\" and \"higher than\nnecessary validation cost\" are explicitly relative statements,\ncomparing earlier and later libbitcoin implementations.\n\nIt is not clear to me how you are relating both the storage cost\n(\"Hmm. No. ... Neither impose significant storage requirements.\") and\ncode complexity (\"... resulting code is actually much less complex\")\nof your tx ordering software to my statements. Do you think I am wrong\nand libbitcoin v3 is not actually more space and code efficient than\nlibbitcoin v2?\n\nBut given that you have thrown some numbers and ideas out in a request\nfor feedback, I'm happy to give you some based on several years of\nexperience working closely with these issues.\n\nFirst, I remain confused on your comments pertaining to UTXO growth\nand network protocol. I followed your conversation with Greg and it\nremains unclear to me. From what I understand you have isolated order\n(double spend) from script validation. I think we all understand that\nscript validation requires inputs and outputs while double spend\ndetection requires correlation of inputs. What I do not understand is\nyour choice of optimization axis.\n\nDetection of double spend is not useful in isolation. One must also\nvalidate scripts, which requires outputs. I can see that there is an\nopportunity to reject blocks (within the same branch) faster by\nvalidating for double spends before validating script. But unconfirmed\ntransactions do not exist in a branch, and are therefore not truly\nconflicting, until they are mined. And even after they are mined\nconflicting txs remain potentially valid in other branches. So\nrejecting txs due to conflict comes down to a denial of service\npolicy, which ultimately must be based on fee increment (e.g. RBF).\nBut fees are based on the amount of the output value that remains\nunspent in the transaction. So this in turn requires the retrieval of\noutputs.\n\nAnd yet the remaining scenario of fast rejection of invalid blocks is\nnot a meaningful optimization. Optimizing for the case where a block\nhas valid and sufficient PoW and yet is invalid (for double spend) is\ncounterproductive. And even so, the txs within the invalid block may\nbe entirely valid independent of the block, so you are back to looking\nup their outputs to obtain fees in the case of a double spend or to\nvalidate script otherwise. In all cases you need to get the outputs.\n\n> Bitcrust simply scans the tree. Although earlier designs used a \n> skip-list, it turns out that accompanied by a spent-index lagging a\n> few blocks behind, raw scanning is faster then anything even though\n> it needs to scan ~5 blocks times ~4000 inputs before reaching the\n> first spent-index,  the actual scan is highly cache efficient and\n> little more then a \"REP SCASQ\", reaching sub-microsecond per input\n> on each core *including* the lookup in the spend index.\n\nI realize that you see the implementation of the ordering validation\nas interesting detail, but I find it hard to justify contemplating the\nimplementation in isolation from the output lookup requirement. And if\none must looking up both outputs and spends for each validation, it\nmakes more sense to co-locate that data.\n\nRecovering in one step all data necessary to validate a tx has real\nadvantages over either interleaving queries and validation or\nsplitting input vs. output validation queries into two steps. It is a\nsignificantly more test-friendly approach, has better performance\ncharacteristics, and simplifies code. I cannot see any reason to\nperform the data read for double spend validation in isolation of that\nfor script validation.\n\n>> I don't follow this part, maybe you could clarify. A spends\n>> index grows with the size of the spend set (forever) as it cannot\n>> be pruned, which certainly exceeds the size of the UTXO set\n>> (unless nothing is spent). The advantage is that you don't have\n>> to keep rewriting the store when you use a spends set (because\n>> the store can be append only).\n> \n> My point is, that the spend tree grows per *input* of a\n> transaction instead of per *output* of a transaction, because this\n> is what is scanned on order validation.\n\nI think the conversation with Greg resolved my questions in this area.\nWhat I find interesting is the reliance on Core's UTXO store to\nimplement script validation. This is not, \"a storage engine without a\nUTXO-index\" as it has a dependency on Core's UTXO index.\n\nOn the other hand the initial libbitcoin implementation that I\ndescribed to you is *actually* a bitcoin store with no UTXO index. The\ncurrent implementation is as well, however it is implemented\ndifferently and is much more efficient than the original. How it\ncompares to your design is not really the point and impossible to\nmeasure until you have production code.\n\nI can say however that your assumptions about the storage (and\nperformance) superiority of the design, or at least its\nimplementation, seem unfounded. If you are storing more index data\n(5.6gb) than 32 bits per output, you are using more space than\nproduction implementations. As for complexity, I don't think you'll\nget any simpler than a loop to populate spend heights from a hash\ntable and a loop to test their integer values.\n\n> The spend tree can be pruned because the spend index (~200mb)\n> catches early spends.\n> \n> Disregarding the baseload script validation, the peak load order \n> validation of bitcrust is more negatively effected by a transaction\n> with many inputs than by a transaction of many outputs.\n> \n> I encourage you to check out the results at https://bitcrust.org\n\nIf by results you are referring to performance numbers, it's very hard\nto draw any conclusions without a full benchmark. It's great that if\nyou are able to boost Core, but from my perspective the numbers aren't\nespecially compelling.\n\nAs for some of the site's comments, these again cause me to question\nthe optimization choices:\n\n\"Blocks can be verified in parallel...\"\n\nDespite the site's explanation I cannot think of any reason to ever\nvalidate two blocks at the same time. You would always prioritize the\nblock with the greatest PoW. Doing otherwise just slows down the net\nvalidation in all but the pathological case where a miner has produced\nan *invalid* block with *more* PoW than another valid block which\narrived at the node within the same second. Rejecting a *valid* block\nwith more PoW in favor of one with *less* \"processing\" is a hard fork,\nso you probably wouldn't want to do that either. But with compact\nblock validation times approaching 25ms it's hard to justify stopping\na block validation for any reason.\n\nThat's not to say parallel block validation difficult to do. If you\ncan validate one block's full set of inputs in parallel (which is not\nnovel) doing the same with additional blocks has trivial additional\ncomplexity.\n\n\"The storage engine is optimized from ground up for\nxthin/compact-block synchronization. This ensures that when the\nmajority of transactions are already synced, incoming blocks can be\nverified at minimal resources using order-validation only.\"\n\nThere are two distinct considerations here. One is pre-validation of\ntxs and the other is compact announcements. Just to be clear, the\nformer does not require the latter. Libbitcoin for example fully\nexploits the former, independent of compactness. With a low min fee\nsetting and a few peers it is typical for the node to have\npre-validated 100% of non-coinbase txs. Averages at 1 satoshi per byte\nare about 99.9%, effectively amortizing all script validation cost. So\nthis optimization is neither novel nor limited to compactness (which\nis about reducing latency).\n\nI am also interested in your previous comments about soft forks. These\nare material considerations that Greg touched on but it doesn't sound\nlike you fully appreciate just yet. When a tx is pre-validated the\nrules applied must be the same rules as those of some future block.\nYet a tx can be included in more than one block (different branches).\nAcross branches and even in one branch, validation rules change, and\ncan change back. The changes are based on accumulated branch history.\nPre-validation can later become invalidated, and differently in\ndifferent branches. And maintaining proper context requires either\nstoring state that you are apparently not storing, or invalidating\noptimizations. Based on your comments you do not seem to be accounting\nfor this in your storage assumptions or in your results. A recent post\nby Greg highlights the complexity and consensus criticality of these\nconsiderations.\n\nBy \"order-validation only\" I believe you are referring to a\ndetermination of whether the txs organized into a candidate block\ndouble spend internal to the block or in the ancestry. Assuming that\none recovers outputs at the same time (and presumably from the same\nlocation) as spender height (which is required both for validating\nspends of a coinbase and for determination of whether the spend is\nabove the fork point), this determination is straightforward. One\nsimply loops over the spender records and invalidates a tx that has a\nspender height not above the fork point (while also validating\ncoinbase maturity using the same height). A loop over the set of\nin-memory spend heights of each output a tx is certainly fast enough\nto not be worthy of any further optimization. And as previously\ndiscussed, the population of the spender heights is not even a\nmaterial additional cost over obtaining the (necessary) output scripts.\n\nThe hash table store that I described can fully navigate the block\ntree and transaction DAG, since the stored tx, parent and point hashes\nare also natural keys and each link is navigable in constant time. It\nis also lock-free, can concurrently write any number of blocks during\ninitial block download and supports read/write concurrency. It has\nsuccessfully indexed and stored the entire blockchain from the P2P\nnetwork in 16 minutes (locally). It also stores both confirmed and\nunconfirmed transactions in the same store, so there is nothing to\nwrite when a block is confirmed except for the block header/hashes and\nupdates to spender heights for any output spent by the new block's\ntxs. It is similarly capable of storage in the block table of weak\nchain blocks...\n\nBut one thing it does *not* do is maintain spender and fork state for\nmultiple branches. In other words it is optimized for one long chain,\nnot multiple long branches. Your approach has a limited (in terms of\ndouble spend identification) optimization for reorganization (i.e. a\nchange to the strong chain identity). However, applying that\noptimization to the full store and supportive of soft forks, as\nopposed to just input ordering, is a much larger task than it appears\nyou have attempted. I know, as I created a design for that approach\nand after some time scrapped it. The cost of performing the\nreorganization in the above store is low enough and very long reorgs\ninfrequent enough, for the optimization to be counterproductive. It's\nelegant in theory, but in practice it increases storage requirements,\nimpacts general performance and significantly increases complexity.\nBitcoin's data model pushes one away from a tree design in that it is\nalways pruning the tree. Having the tree is necessary, but it's not\nsomething to optimize for.\n\ne\n\n\n> Regards, Tomas\n> \n> On Fri, Apr 7, 2017, at 01:38, Eric Voskuil wrote: On 04/06/2017\n> 03:12 PM, Tomas via bitcoin-dev wrote:\n> \n> Hi Tomas,\n> \n>>>> I have been working on a bitcoin implementation that uses a \n>>>> different approach to indexing for verifying the order of \n>>>> transactions. Instead of using an index of unspent outputs,\n>>>> double spends are verified by using a spend-tree where spends\n>>>> are scanned against spent outputs instead of unspent\n>>>> outputs.\n> \n> This is the approach that genjix used in libbitcoin version2. With\n> the exception of de-linking (not deleted) in the case of reorgs,\n> the entire store is append only, implemented in a small set of\n> memory mapped files. The downsides to the approach are:\n> \n> (1) higher than necessary storage space requirement due to storing\n> the indexing data required for correlate the spends, and\n> \n> (2) higher than necessary validation complexity and cost in terms\n> of computing the spent-ness (including spender height) of an\n> output.\n> \n> His implementation used a hash table, so performance-wise it did\n> quite well and would theoretically outperform a tree, O(1) vs.\n> O(log2(N)).\n> \n>>>> This allows for much better concurrency, as not only blocks,\n>>>> but also individual inputs can be verified fully in\n>>>> parallel.\n> \n> I was successful in parallelizing input validation (across the\n> inputs of an unconfirmed tx and across the set of all inputs in a\n> block) using the v2 store. However, it is not the case that the\n> spends approach is necessary for concurrency.\n> \n> To resolve the above two problems the version3 store does not use\n> a spends table/index. Nor does it store any table of UTXOs. Yet \n> validation is highly parallelized. Instead of additional indexes\n> it uses the tx hash table, augmented with 32 bits per output for\n> spender height. So there is a O(1) cost of finding the tx and a\n> O(N) cost of finding the spender height where N is the number of\n> outputs in the tx. But because the number of outputs in a tx is\n> bounded (by block size) this is constant time in the number of\n> transactions.\n> \n> This works out much faster than the spends table, and without the \n> storage cost or complexity disadvantages. It also scales with \n> available hardware, as the memory mapped files become in-memory\n> hash tables. For low memory machines we found it was important to\n> implement an opaque UTXO cache to limit paging, but for higher end\n> systems zero cache is optimal.\n> \n>>>> I am sharing this not only to ask for your feedback, but also\n>>>> to call for a clear separation of protocol and\n>>>> implementations: As this solution, reversing the costs of\n>>>> outputs and inputs, seems to have excellent performance\n>>>> characteristics (as shown in the test results), updates to\n>>>> the protocol addressing the UTXO growth, might not be worth\n>>>> considering *protocol improvements* and it might be best to\n>>>> address these concerns as implementation details.\n> \n> I don't follow this part, maybe you could clarify. A spends index \n> grows with the size of the spend set (forever) as it cannot be\n> pruned, which certainly exceeds the size of the UTXO set (unless\n> nothing is spent). The advantage is that you don't have to keep\n> rewriting the store when you use a spends set (because the store\n> can be append only).\n> \n> Feel free to message me if you'd like to discuss in more detail, or\n> to continue on the libbitcoin mailing list (copied).\n> \n> e\n-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v2.0.22 (GNU/Linux)\n\niQEcBAEBCAAGBQJY6WY4AAoJEDzYwH8LXOFO+wwH/1uE/+P1+KLJWTkcttVWsO//\nQAlikqg0HLFDtkd5jaYsBtx6op/Uz2o53ohZwVJt71ITCjQQI+yYK2RjBX92xIhd\nK0rE901Np4PfMFbDA60LB0c/65aPlkUCr3f2PYIlizJs4Qq5Kn2sIpC5v9T3B7H4\nMPq5UJwoPP+m3RZ9TSsVyee3ejHYXM7y2VNNnnWD3edIioA3cLh+y6sczpco2Hpa\nP+GSDnv2cwV6FA22Is1Z15tpfLyQnPrrGJ9QEJJ15vnhCTxZe0j1PQ4y+OOZh5Iq\nmqBkGRNPeUnPAPDM+/qvhr2kUyxFbaJNtwg5HDGHWFOq5B/YeKxVk8Qjnk+9epA=\n=XRKl\n-----END PGP SIGNATURE-----"
            },
            {
                "author": "Tomas",
                "date": "2017-04-08T23:58:02",
                "message_text_only": "Thank you for your elaborate response Eric,\n\nOn Sun, Apr 9, 2017, at 00:37, Eric Voskuil wrote:\n> My point was that \"Using a storage engine without UTXO-index\" has been\n> done, and may be a useful reference, not that implementation details\n> are the same.\n\nI haven't dived into libbitcoin V2/V3 enough to  fully grasp it and\nthough your comments help, I still not fully do.  I will answer below\nwhat is related to bitcrust itself.\n\nMy post wasn't posted to claim innovation; I merely try to explain how\nBitcrust works and why   it performs well. \n\n\n> First, I remain confused on your comments pertaining to UTXO growth\n> and network protocol. I followed your conversation with Greg and it\n> remains unclear to me. From what I understand you have isolated order\n> (double spend) from script validation. I think we all understand that\n> script validation requires inputs and outputs while double spend\n> detection requires correlation of inputs. What I do not understand is\n> your choice of optimization axis.\n> \n> Detection of double spend is not useful in isolation. One must also\n> validate scripts, which requires outputs. I can see that there is an\n> opportunity to reject blocks (within the same branch) faster by\n> validating for double spends before validating script. But unconfirmed\n> transactions do not exist in a branch, and are therefore not truly\n> conflicting, until they are mined. And even after they are mined\n> conflicting txs remain potentially valid in other branches. So\n> rejecting txs due to conflict comes down to a denial of service\n> policy, which ultimately must be based on fee increment (e.g. RBF).\n> But fees are based on the amount of the output value that remains\n> unspent in the transaction. So this in turn requires the retrieval of\n> outputs.\n> \n> And yet the remaining scenario of fast rejection of invalid blocks is\n> not a meaningful optimization. Optimizing for the case where a block\n> has valid and sufficient PoW and yet is invalid (for double spend) is\n> counterproductive. And even so, the txs within the invalid block may\n> be entirely valid independent of the block, so you are back to looking\n> up their outputs to obtain fees in the case of a double spend or to\n> validate script otherwise. In all cases you need to get the outputs.\n> \n> > Bitcrust simply scans the tree. Although earlier designs used a \n> > skip-list, it turns out that accompanied by a spent-index lagging a\n> > few blocks behind, raw scanning is faster then anything even though\n> > it needs to scan ~5 blocks times ~4000 inputs before reaching the\n> > first spent-index,  the actual scan is highly cache efficient and\n> > little more then a \"REP SCASQ\", reaching sub-microsecond per input\n> > on each core *including* the lookup in the spend index.\n> \n> I realize that you see the implementation of the ordering validation\n> as interesting detail, but I find it hard to justify contemplating the\n> implementation in isolation from the output lookup requirement. And if\n> one must looking up both outputs and spends for each validation, it\n> makes more sense to co-locate that data.\n> \n> Recovering in one step all data necessary to validate a tx has real\n> advantages over either interleaving queries and validation or\n> splitting input vs. output validation queries into two steps. It is a\n> significantly more test-friendly approach, has better performance\n> characteristics, and simplifies code. I cannot see any reason to\n> perform the data read for double spend validation in isolation of that\n> for script validation.\n\n\nYou seem to ignore here the difference between base load and peak load.\nIf Compact blocks/XThin with further optimizations can presync nearly\n100% of the transactions, and nodes can do as much as possible when a\ntransaction comes in, the time spent when a block comes in can be\nminimized and a lot more transactions can be handled with the same\nresources.\n\nThe reason for \"splitting\" is that for an incoming transaction the\nspent-state of the outputs being spent isn't particularly relevant as\nyou seem to acknowledge. When the block comes in, the actual output data\nisn't relevant.\n\nThe *only* thing that needs to be checked when a block comes in is the\norder, and the spend-tree approach absolves the need to access outputs\nhere.\n\nAs it also absolves the need for reorgs this greatly simplifies the\ndesign. I am not sure why you say that a one-step approach is more\n\"test-friendly\" as this seems to be unrelated.\n\n> \n> If by results you are referring to performance numbers, it's very hard\n> to draw any conclusions without a full benchmark. It's great that if\n> you are able to boost Core, but from my perspective the numbers aren't\n> especially compelling.\n>\n\nI fully agree and hopefully do not pretend to hide that my numbers are\npremature without a full implementation. I just think they are promising\nenough to  convince at least myself to move on with this model.\n \n> Despite the site's explanation I cannot think of any reason to ever\n> validate two blocks at the same time. You would always prioritize the\n> block with the greatest PoW. Doing otherwise just slows down the net\n> validation in all but the pathological case where a miner has produced\n> an *invalid* block with *more* PoW than another valid block which\n> arrived at the node within the same second. Rejecting a *valid* block\n> with more PoW in favor of one with *less* \"processing\" is a hard fork,\n> so you probably wouldn't want to do that either. But with compact\n> block validation times approaching 25ms it's hard to justify stopping\n> a block validation for any reason.\n\nI don't get what you are saying. Why pick the greatest PoW of two\ncompeting blocks? If two blocks come in, an implementation is free to\nchoose whichever block to build on. Choosing so is not a \"hardfork\".\nParallel validation simply makes it easier to make an optimal choice,\nfor if two blocks come in, the one that is validated fastest can be\nbuild upon without the risk of validationless mining.\n\n> \n> That's not to say parallel block validation difficult to do. If you\n> can validate one block's full set of inputs in parallel (which is not\n> novel) doing the same with additional blocks has trivial additional\n> complexity.\n\nI am not trying to claim novelty here.\n\n> I am also interested in your previous comments about soft forks. These\n> are material considerations that Greg touched on but it doesn't sound\n> like you fully appreciate just yet. When a tx is pre-validated the\n> rules applied must be the same rules as those of some future block.\n> Yet a tx can be included in more than one block (different branches).\n> Across branches and even in one branch, validation rules change, and\n> can change back. The changes are based on accumulated branch history.\n> Pre-validation can later become invalidated, and differently in\n> different branches. And maintaining proper context requires either\n> storing state that you are apparently not storing, or invalidating\n> optimizations. Based on your comments you do not seem to be accounting\n> for this in your storage assumptions or in your results. A recent post\n> by Greg highlights the complexity and consensus criticality of these\n> considerations.\n\nFrankly, I think this is a bit of an exaggeration. Soft forks are\ncounted on a hand, and I don't think there are many - if any -\ntransactions in the current chain that have changed compliance based on\nheight. This makes this a compliance issue and not a performance issue\nand the solution I have explained, to add height-based compliance as\nmeta data of validation seems to \nbe adequate and safe.\n\n\n> The hash table store that I described can fully navigate the block\n> tree and transaction DAG, since the stored tx, parent and point hashes\n> are also natural keys and each link is navigable in constant time. It\n> is also lock-free, can concurrently write any number of blocks during\n> initial block download and supports read/write concurrency. It has\n> successfully indexed and stored the entire blockchain from the P2P\n> network in 16 minutes (locally). It also stores both confirmed and\n> unconfirmed transactions in the same store, so there is nothing to\n> write when a block is confirmed except for the block header/hashes and\n> updates to spender heights for any output spent by the new block's\n> txs. It is similarly capable of storage in the block table of weak\n> chain blocks...\n> \n\nI think I get the gist of your approach and it sounds very interesting\nand I will definitely dive in deeper.\n\nIt also seems sufficiently different from Bitcrust to merit competing on\n(eventual) results instead of the complicated theory alone.\n\nBest,\nTomas"
            },
            {
                "author": "Eric Voskuil",
                "date": "2017-04-11T01:44:57",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\n\nOn 04/08/2017 04:58 PM, Tomas wrote:\n> You seem to ignore here the difference between base load and peak \n> load. If Compact blocks/XThin with further optimizations can \n> presync nearly 100% of the transactions, and nodes can do as much \n> as possible when a transaction comes in, the time spent when a \n> block comes in can be minimized and a lot more transactions can be \n> handled with the same resources.\n\nMaybe it's an issue of terminology. I have never used the terms\nbase/peak load. However I've been trying to get across, poorly I\nsuppose, that this is actually implemented in libbitcoin. I generally\nrefer to it as tx pre-validation. I've also tried to relate that you\nare unnecessarily relating pre-validation to compactness. These are\nunrelated ideas and better considered independently. One can get\nnearly all of the benefit of pre-validation while still receiving\nblocks (vs. compact blocks). The advantage of compactness is reduced\nlatency of the block announcement. The reason for pre-validation is\namortization of the validation and/or storage cost of a block.\n\n> The reason for \"splitting\" is that for an incoming transaction the\n>  spent-state of the outputs being spent isn't particularly\n> relevant as you seem to acknowledge. When the block comes in, the\n> actual output data isn't relevant.\n\nAs I understand it you would split tx inputs and outputs and send them\nindependently, and that you intend this to be a P2P network\noptimization - not a consensus rule change. So my comments are based\non those inferences. If we are talking about consensus changes this\nconversation will end up in an entirely different place.\n\nI don't agree with the input/output relevance statements above. When a\ntx is announced the entire tx is relevant. It cannot be validated as\noutputs only. If it cannot be validated it cannot be stored by the\nnode. Validating the outputs only would require the node store invalid\ntransactions.\n\nI do accept that a double-spend detection is not an optimal criteria\nby which to discard a tx. One also needs fee information. But without\ndouble-spend knowledge the node has no rational way to defend itself\nagainst an infinity of transactions that spend the minimal fee but\nalso have conflicting inputs (i.e. risking the fee only once). So tx\n(pool) validation requires double-spend knowledge and at least a\nsummary from outputs.\n\n> The *only* thing that needs to be checked when a block comes in is \n> the order, and the spend-tree approach absolves the need to access \n> outputs here.\n\nInputs that are already valid against prevouts remain valid assuming\nconsensus rules have not changed. But any input that spends a coinbase\nmust be validated for prevout height once there is a block context for\nvalidation. Additionally the set of txs must be validated for total\nsize, sigops, and fee claim. So it's not true that conflict detection\nalone is sufficient. Yet one can cache a tx's size, sigops, fee and\nminimum height in a graph so that when a block appears that contains\nthat tx the input validation can be skipped.\n\nIgnoring the (actual) requirement for the full tx on the pool\nvalidation, the required \"order\" validation at (compact or other)\nblock arrival basically consists of traversing each tx, ensuring none\nare confirmed in a block below the fork point; traversing each each of\nits confirmed inputs, ensuring that none are spent in a block below\nthe fork point; and ensuring the block's set of transactions do not\ncontain missing inputs and do not double spend internal to the block.\n\nThis and the above-mentioned other required per-transaction block\nvalidation data can be cached to an in-memory structure as a potential\noptimization over navigating the store, and as you say, does not\ntherefore require the actual outputs (script/value). But the original\nissue of needing full transactions for independent transaction\nvalidation remains.\n\n> As it also absolves the need for reorgs this greatly simplifies the\n> design.\n\nA reorg is conceptual and cannot be engineered out. What you are\nreferring to is a restructuring of stored information as a consequence\nof a reorg. I don't see this as related to the above. The ability to\nperform reorganization via a branch pointer swap is based not on the\norder or factoring of validation but instead on the amount of\ninformation stored. It requires more information to maintain multiple\nbranches.\n\nTransactions have confirmation states, validation contexts and spender\nheights for potentially each branch of an unbounded number of\nbranches. It is this requirement to maintain that state for each\nbranch that makes this design goal a very costly trade-off of space\nand complexity for reorg speed. As I mentioned earlier, it's the\noptimization for this scenario that I find questionable.\n\n> I am not sure why you say that a one-step approach is more \n> \"test-friendly\" as this seems to be unrelated.\n\nFull separation of concerns allows all validation to be performed in\nisolation from the store. As such validation state can be faked and\nprovided to a tx, block or chain, for the purpose of test. Validation\nthat interacts with a complex store during validation is harder to\nfake and tests can be hard to verify.\n\nIt's not really the \"one-step\" approach that make this possible. In\nfact that's not an accurate description. Validation and storage of txs\nand blocks consists of four steps:\n\n(1) context free\n(2) contextual (chain-based)\n(3) expensive (script eval)\n(4) storage and notification\n\nSo we have:\n\ntx.check()\ntx.accept(state)\ntx.connect(state)\nchain.organize(tx)\n\nblock.check()\nblock.accept(state)\nblock.connect(state)\nchain.organize(block)\n\n...where \"chain\" is the store, from which \"state\" is derived. The\nstate for an unconfirmed tx is based on the presumption that the tx\nwould be mined in the next block. If that is not the case then its\npre-validation can become invalidated. So from my perspective, this\ndiscussion is all about populating state. Anything that cannot be\nplaced into that pattern would complicate both the conceptual model\nand testing. We've also seen that this isolation also has performance\nadvantages, as it facilitates optimizations that are otherwise\nchallenging.\n\n>> Despite the site's explanation I cannot think of any reason to \n>> ever validate two blocks at the same time. You would always \n>> prioritize the block with the greatest PoW. Doing otherwise just \n>> slows down the net validation in all but the pathological case \n>> where a miner has produced an *invalid* block with *more* PoW \n>> than another valid block which arrived at the node within the \n>> same second. Rejecting a *valid* block with more PoW in favor of \n>> one with *less* \"processing\" is a hard fork, so you probably \n>> wouldn't want to do that either. But with compact block \n>> validation times approaching 25ms it's hard to justify stopping\n>> a block validation for any reason.\n> \n> I don't get what you are saying. Why pick the greatest PoW of two \n> competing blocks?\n\nBecause choosing the lesser amount of work is non-consensus behavior.\nUnder the same circumstances (i.e. having seen the same set of blocks)\ntwo nodes will disagree on whether there is one confirmation or no\nconfirmations for a given tx. This disagreement will persist (i.e. why\ntake the weaker block only to turn around and replace it with the\nstronger block that arrives a few seconds or minutes later). It stands\nto reason that if one rejects a stronger block under a race condition,\none would reorg out a stronger block when a weaker block arrives a\nlittle after the stronger block. Does this \"optimization\" then apply\nto chains of blocks too?\n\n> If two blocks come in, an implementation is free to choose \n> whichever block to build on.\n\nImplementations are free to choose no blocks. That's not really the issu\ne.\n\n> Choosing so is not a \"hardfork\".\n\nAccepting a block that all previous implementations would have\nrejected under the same circumstance could be considered a hard fork,\nbut you may be right.\n\nYet the classification is not essential to my point. Nor is any\nmaterial change required to validate blocks in parallel. We can do it\nusing current design, but it doesn't make sense to do so.\n\n> Parallel validation simply makes it easier to make an optimal \n> choice, for if two blocks come in, the one that is validated \n> fastest can be build upon without the risk of validationless \n> mining.\n\nThis is not an optimization, since it should always be optimal to\nvalidate blocks independently. Performing multiple together inherently\nslows both of them. And the advantage to not validating *either* would\nremain.\n\n>> I am also interested in your previous comments about soft forks. \n>> These are material considerations that Greg touched on but it \n>> doesn't sound like you fully appreciate just yet. When a tx is \n>> pre-validated the rules applied must be the same rules as those \n>> of some future block. Yet a tx can be included in more than one \n>> block (different branches). Across branches and even in one \n>> branch, validation rules change, and can change back. The\n>> changes are based on accumulated branch history. Pre-validation\n>> can later become invalidated, and differently in different\n>> branches. And maintaining proper context requires either storing\n>> state that you are apparently not storing, or invalidating\n>> optimizations. Based on your comments you do not seem to be\n>> accounting for this in your storage assumptions or in your\n>> results. A recent post by Greg highlights the complexity and\n>> consensus criticality of these considerations.\n> \n> Frankly, I think this is a bit of an exaggeration. Soft forks are \n> counted on a hand, and I don't think there are many - if any - \n> transactions in the current chain that have changed compliance \n> based on height.\n\nHope is a bug.\n\n> This makes this a compliance issue and not a performance issue\n\nYou cannot have a useful performance measure without full compliance.\n\n> and the solution I have explained, to add height-based compliance \n> as meta data of validation seems to be adequate and safe.\n\nIf you intend this to be useful it has to help build the chain, not\njust rely on hardwiring checkpoints once rule changes are presumed to\nbe buried deeply enough to do so (as the result of other implementations\n).\n\nI understand this approach, it was ours at one time. There is a\nsignificant difference, and your design is to some degree based on a\nfailure to fully consider this. I encourage you to not assume any\nconsensus-related detail is too small.\n\n>> The hash table store that I described can fully navigate the \n>> block tree and transaction DAG, since the stored tx, parent and \n>> point hashes are also natural keys and each link is navigable in \n>> constant time. It is also lock-free, can concurrently write any \n>> number of blocks during initial block download and supports \n>> read/write concurrency. It has successfully indexed and stored \n>> the entire blockchain from the P2P network in 16 minutes \n>> (locally). It also stores both confirmed and unconfirmed \n>> transactions in the same store, so there is nothing to write\n>> when a block is confirmed except for the block header/hashes and\n>>  updates to spender heights for any output spent by the new \n>> block's txs. It is similarly capable of storage in the block \n>> table of weak chain blocks...\n> \n> I think I get the gist of your approach and it sounds very \n> interesting and I will definitely dive in deeper.\n\nIt's worth noting that many of your stated objectives, including\nmodularity, developer platform, store isolation, consensus rule\nisolation (including optional use of libbitcoinconsensus) are implemente\nd.\n\nIt seems like you are doing some good work and it's not my intent to\ndiscourage that. Libbitcoin is open source, I don't get paid and I'm\nnot selling anything. But if you are going down this path you should\nbe aware of it and may benefit from our successes as well as some of\nthe other stuff :). And hopefully we can get the benefit of your\ninsights as well.\n\ne\n-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v2.0.22 (GNU/Linux)\n\niQEcBAEBCAAGBQJY7DUUAAoJEDzYwH8LXOFOTB0H/jDtfnC6B9CtGrCTPtET+dDx\nr0uQ0SXo40AUTplyKQ228rVkjmZyczTOtIP5uNvKpvlr9wW8TyYzFzNW4RNCNtdP\nxZ9OjrfC24J2n+m1b9z9+CA85qAQxzLztBybDYzXCJG/dQ+y++7BR+rILGiRWUhs\nlROeaEMqlDl0fy5J3dlpe0RGZJPSRqlxW7EBNHYc3IEDNL+j5m80/tWb6H5a3Mv8\n7GTr6ulZef/04u/hRTXQ0ONy0MAIoi63HNHQuR0wF70ewGVmtFY4RHXEnNi+ucIG\nw3QZuNTPtjqIS+ZbpFuqBop+L3CtId9+jxaBAao2tEieoIUl/faLjdTPP+r0n6A=\n=5mz8\n-----END PGP SIGNATURE-----"
            },
            {
                "author": "Tomas",
                "date": "2017-04-11T08:43:30",
                "message_text_only": "On Tue, Apr 11, 2017, at 03:44, Eric Voskuil wrote:\n\n> As I understand it you would split tx inputs and outputs and send them\n> independently, and that you intend this to be a P2P network\n> optimization - not a consensus rule change. So my comments are based\n> on those inferences. If we are talking about consensus changes this\n> conversation will end up in an entirely different place.\n\n> I don't agree with the input/output relevance statements above. When a\n> tx is announced the entire tx is relevant. It cannot be validated as\n> outputs only. If it cannot be validated it cannot be stored by the\n> node. Validating the outputs only would require the node store invalid\n> transactions.\n\nSplitting transactions only happens *on storage* and is just a minor\noptimization compared to storing them in full. (actually a very recent\nchange with only marginally better results). This is simply because the\noutput scripts are read on script validation, and storing the outputs of\nthe transaction separately ensures better spatial locality of reference\n(the inputs are just \"in the way\"). This is not relevant when using a\nUTXO-index, because the outputs are then directly stored in the index,\nwhere bitcrust has to read them from the transaction data.\n\nIt is not my intention to send them independently.\n \n> I do accept that a double-spend detection is not an optimal criteria\n> by which to discard a tx. One also needs fee information. But without\n> double-spend knowledge the node has no rational way to defend itself\n> against an infinity of transactions that spend the minimal fee but\n> also have conflicting inputs (i.e. risking the fee only once). So tx\n> (pool) validation requires double-spend knowledge and at least a\n> summary from outputs.\n\nDouble spent information is still available to the network node and\ncould still be used for DoS protection, although I do believe\nalternatives may exist.\n \n> \n> A reorg is conceptual and cannot be engineered out. What you are\n> referring to is a restructuring of stored information as a consequence\n> of a reorg. I don't see this as related to the above. The ability to\n> perform reorganization via a branch pointer swap is based not on the\n> order or factoring of validation but instead on the amount of\n> information stored. It requires more information to maintain multiple\n> branches.\n> \n> Transactions have confirmation states, validation contexts and spender\n> heights for potentially each branch of an unbounded number of\n> branches. It is this requirement to maintain that state for each\n> branch that makes this design goal a very costly trade-off of space\n> and complexity for reorg speed. As I mentioned earlier, it's the\n> optimization for this scenario that I find questionable.\n\nSure, we can still call switching tips a \"reorg\". And it is indeed a\ntrade off as orphan blocks are stored, but a block in the spend tree\ntakes only ~12kb and contains  the required state information. \n\nI believe this trade off  reduced complexity. For the earlier tree this\ncould be pruned.\n\n> Because choosing the lesser amount of work is non-consensus behavior.\n> Under the same circumstances (i.e. having seen the same set of blocks)\n> two nodes will disagree on whether there is one confirmation or no\n> confirmations for a given tx. This disagreement will persist (i.e. why\n> take the weaker block only to turn around and replace it with the\n> stronger block that arrives a few seconds or minutes later). It stands\n> to reason that if one rejects a stronger block under a race condition,\n> one would reorg out a stronger block when a weaker block arrives a\n> little after the stronger block. Does this \"optimization\" then apply\n> to chains of blocks too?\n\nThe blockchain is - by design - only eventually consistent across nodes.\nEven if nodes would use the same \"tip-selection\" rules, you cannot rely\non all blocks being propagated and hence each transaction having the\nsame number of confirmations across all nodes.\n\nAs a simpler example, if two miners both mine a block at approximately\nthe same time and send it to each other, then surely they would want to\ncontinue mining on their own block. Otherwise they would be throwing\naway their own reward.  \n\nAnd yes, this can also happen over multiple blocks, but the chances of\nconsistency are vastly increased with each confirmation.\n\n> Accepting a block that all previous implementations would have\n> rejected under the same circumstance could be considered a hard fork,\n> but you may be right.\n\nI am not talking about rejecting blocks, I am only talking choosing on\nwhich tip to mine.\n\n> > Frankly, I think this is a bit of an exaggeration. Soft forks are \n> > counted on a hand, and I don't think there are many - if any - \n> > transactions in the current chain that have changed compliance \n> > based on height.\n> \n> Hope is a bug.\n> \n> If you intend this to be useful it has to help build the chain, not\n> just rely on hardwiring checkpoints once rule changes are presumed to\n> be buried deeply enough to do so (as the result of other implementations\n> ).\n> \n> I understand this approach, it was ours at one time. There is a\n> significant difference, and your design is to some degree based on a\n> failure to fully consider this. I encourage you to not assume any\n> consensus-related detail is too small.\n\nI am not failing to consider this, and I don't consider this too small .\nBut ensuring contextual transaction validity by \"validate =>  valid with\nrules X,Y,Z\" and then checking the active rules (softfork activation) on\norder validation, will give logically the same results as \"validate with\nX,Y,Z => valid\". This is not \"hardwiring checkpoints\" at all.\n\n> You cannot have a useful performance measure without full compliance.\n\nI agree that the results are preliminary and I will post more if the\nproduct reaches later stages.\n\n> It's worth noting that many of your stated objectives, including\n> modularity, developer platform, store isolation, consensus rule\n> isolation (including optional use of libbitcoinconsensus) are implemente\n> d.\n> \n> It seems like you are doing some good work and it's not my intent to\n> discourage that. Libbitcoin is open source, I don't get paid and I'm\n> not selling anything. But if you are going down this path you should\n> be aware of it and may benefit from our successes as well as some of\n> the other stuff :). And hopefully we can get the benefit of your\n> insights as well.\n \n\nThank you, I will definitely further dive into libbitcoin, and see what\ninsights I can use for Bitcrust.\n\nTomas"
            },
            {
                "author": "Eric Voskuil",
                "date": "2017-04-11T09:41:34",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\n\nOn 04/11/2017 01:43 AM, Tomas wrote:\n> Splitting transactions only happens *on storage* and is just a\n> minor optimization compared to storing them in full.\n\nOk\n\n> Sure, we can still call switching tips a \"reorg\". And it is indeed\n> a trade off as orphan blocks are stored, but a block in the spend\n> tree takes only ~12kb and contains the required state information.\n> \nIt's not the headers/tx-hashes of the blocks that I'm referring to, it\nis the confirmation and spend information relative to all txs and all\noutputs for each branch. This reverse navigation (i.e. utxo\ninformation) is essential, must be persistent and is branch-relative.\n\n> The blockchain is - by design - only eventually consistent across\n> nodes. Even if nodes would use the same \"tip-selection\" rules, you\n> cannot rely on all blocks being propagated and hence each\n> transaction having the same number of confirmations across all\n> nodes.\n> \n> As a simpler example, if two miners both mine a block at\n> approximately the same time and send it to each other, then surely\n> they would want to continue mining on their own block. Otherwise\n> they would be throwing away their own reward.\n\nThat's not your concurrent validation scenario. In the scenario you\ndescribed, the person chooses the weaker block of two that require\nvalidation because it's better somehow, not because it's his own\n(which does not require validation).\n\n> And yes, this can also happen over multiple blocks, but the chances\n> of consistency are vastly increased with each confirmation.\n\nConsistency is reached, despite seeing things at different times,\nbecause people use the same rules. If the economy ran on arbitrary\nblock preference consistency would be elusive.\n\n> I am not talking about rejecting blocks, I am only talking choosing\n> on which tip to mine.\n\nThis line of reasoning has me a bit baffled. Yet as I said, it's not\nimportant to the question at hand. It is not likely to be optimal to\nvalidate concurrently even if you consider selection of a weaker block\nadvantageous.\n\n>> If you intend this to be useful it has to help build the chain,\n>> not just rely on hardwiring checkpoints once rule changes are\n>> presumed to be buried deeply enough to do so (as the result of\n>> other implementations ).\n>> \n>> I understand this approach, it was ours at one time. There is a \n>> significant difference, and your design is to some degree based\n>> on a failure to fully consider this. I encourage you to not\n>> assume any consensus-related detail is too small.\n> \n> I am not failing to consider this, and I don't consider this too\n> small . But ensuring contextual transaction validity by \"validate\n> =>  valid with rules X,Y,Z\" and then checking the active rules\n> (softfork activation) on order validation, will give logically the\n> same results as \"validate with X,Y,Z => valid\". This is not\n> \"hardwiring checkpoints\" at all.\n\nStoring the validation flags with each tx is exactly what libbitcoin\ndoes (otherwise pre-validation would be infeasible). But that was not\nthe full point. You said on this in response previously:\n\n>>> ...height-based compliance as meta data of validation seems to\n>>> be\nadequate and safe.\n\nI read this as encoding the height at which a fork historically\nactivated. If you intend to track activation for each branch that will\nnot be \"height-based\" it will be history based.\n\ne\n-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v2.0.22 (GNU/Linux)\n\niQEcBAEBCAAGBQJY7KTHAAoJEDzYwH8LXOFOI+QH/RzX++1TNLC9DEMWioE7SmMj\nyKOrP8WEkOnnrZdFKxVmwV9oZBekEvDABMnJmFiW5TMjsmPz7XwKAYzV0Y5L5oGU\nfZYo3IOPyr0dA9TcpP15gNziR6pFUBq/QTYB6BcbUvvlkJv6xjgIdedgDMEyREWU\nHm/JU5g7gQUQd6MIDWbQ9FbYjtPuNSRQi851YfIn5mDivT4HuidaqQYMd9t5yS2Z\nFuoQBI6L5GTJIqml1bTwJ0wsA7+ZseBEgMn1TT1ehy2v1FFJTojTpzIwG+m3eiXg\nTxN3U/+fNAj+sKBb8Hq+nb7DvgjvKHyHuyRryBju7yq5d5rsb6meXcoiOtAznP8=\n=fRXf\n-----END PGP SIGNATURE-----"
            },
            {
                "author": "Tomas",
                "date": "2017-04-11T10:04:01",
                "message_text_only": "On Tue, Apr 11, 2017, at 11:41, Eric Voskuil wrote:\n> It's not the headers/tx-hashes of the blocks that I'm referring to, it\n> is the confirmation and spend information relative to all txs and all\n> outputs for each branch. This reverse navigation (i.e. utxo\n> information) is essential, must be persistent and is branch-relative.\n\nThat is exactly what is stored in the spend-tree. \n\n>> As a simpler example, if two miners both mine a block at\n>> approximately the same time and send it to each other, then surely\n>> they would want to continue mining on their own block. Otherwise\n>> they would be throwing away their own reward.\n\n> That's not your concurrent validation scenario. In the scenario you\n> described, the person chooses the weaker block of two that require\n> validation because it's better somehow, not because it's his own\n> (which does not require validation).\n\n> Consistency is reached, despite seeing things at different times,\n> because people use the same rules. If the economy ran on arbitrary\n> block preference consistency would be elusive.\n\nNo but my example shows  that it is up to the miner to choose which tip\nto work on. This is not using different rules, it is just optimizing its\nincome. This means that the economy *does* run on arbitrary \"block\npreference\", even if it is not running on arbitrary rules.\n\nIf two blocks are competing, a miner could optimize its decision which\nto mine on, not just on whether one of the blocks is his own, but also\non fees, or on excessive validation costs.\n\n> I read this as encoding the height at which a fork historically\n> activated. If you intend to track activation for each branch that will\n> not be \"height-based\" it will be history based.\n\nI understand \"height-based\" was not the right wording, as it is of\ncourse branch-specific. Per tip ruleset metadata, must be matched with\nper-transaction ruleset metadata.\n\nTomas"
            },
            {
                "author": "Marcos mayorga",
                "date": "2017-04-07T07:55:48",
                "message_text_only": "Hi Tomas,\n\nI've read it and think it is an excellent work, I'd like to see it\nintegrated into bitcoin-core as a 'kernel module'.\n\nI see there are a lot of proof of concepts out there, IMO every one\ndeserve a room in the bitcoin client as a selectable feature, to make the\nsoftware more flexible and less dictatorial, an user could easily select\nwhich features she wants to run.\n\nBest regards,\nMarcos\n\n> I have been working on a bitcoin implementation that uses a different\n> approach to indexing for verifying the order of transactions. Instead of\n> using an index of unspent outputs, double spends are verified by using a\n> spend-tree where spends are scanned against spent outputs instead of\n> unspent outputs.\n>\n> This allows for much better concurrency, as not only blocks, but also\n> individual inputs can be verified fully in parallel.\n>\n> I explain the approach at https://bitcrust.org, source code is available\n> at https://github.com/tomasvdw/bitcrust\n>\n> I am sharing this not only to ask for your feedback, but also to call\n> for a clear separation of protocol and implementations: As this\n> solution, reversing the costs of outputs and inputs, seems to have\n> excellent performance characteristics (as shown in the test results),\n> updates to the protocol addressing the UTXO growth, might not be worth\n> considering *protocol improvements* and it might be best to address\n> these concerns as implementation details.\n>\n> Kind regards,\n> Tomas van der Wansem\n> tomas at bitcrust.org\n> Bitcrust\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Tomas",
                "date": "2017-04-07T08:47:56",
                "message_text_only": "Thank you Marcos,\n\nThough written in Rust, bitcrust-db is definitely usable as pluggable\nmodule as its interface will be roughly some queries, add_tx and\nadd_block with blobs and flags. (Bitcrust internally uses a\ndeserialize-only model, keeping references to the blobs with the parsed\ndata).  \n\nHowever, from Core's side I believe network and storage are currently\nrather tightly coupled, which will make this far from trivial.\n\nRegardless, I am also hoping (with funding & a team) to build a Bitcrust\nnetworking component as well to bring a strong competitor to the market.\n\nbest,\nTomas\n\n\n\nOn Fri, Apr 7, 2017, at 09:55, Marcos mayorga wrote:\n> Hi Tomas,\n> \n> I've read it and think it is an excellent work, I'd like to see it\n> integrated into bitcoin-core as a 'kernel module'.\n> \n> I see there are a lot of proof of concepts out there, IMO every one\n> deserve a room in the bitcoin client as a selectable feature, to make the\n> software more flexible and less dictatorial, an user could easily select\n> which features she wants to run.\n> \n> Best regards,\n> Marcos\n> \n> > I have been working on a bitcoin implementation that uses a different\n> > approach to indexing for verifying the order of transactions. Instead of\n> > using an index of unspent outputs, double spends are verified by using a\n> > spend-tree where spends are scanned against spent outputs instead of\n> > unspent outputs.\n> >\n> > This allows for much better concurrency, as not only blocks, but also\n> > individual inputs can be verified fully in parallel.\n> >\n> > I explain the approach at https://bitcrust.org, source code is available\n> > at https://github.com/tomasvdw/bitcrust\n> >\n> > I am sharing this not only to ask for your feedback, but also to call\n> > for a clear separation of protocol and implementations: As this\n> > solution, reversing the costs of outputs and inputs, seems to have\n> > excellent performance characteristics (as shown in the test results),\n> > updates to the protocol addressing the UTXO growth, might not be worth\n> > considering *protocol improvements* and it might be best to address\n> > these concerns as implementation details.\n> >\n> > Kind regards,\n> > Tomas van der Wansem\n> > tomas at bitcrust.org\n> > Bitcrust\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> >\n> \n>"
            },
            {
                "author": "Greg Sanders",
                "date": "2017-04-07T14:14:31",
                "message_text_only": "Interesting work.\n\nI was wondering if you could tell us what specs for the machine being used\nas preliminary benchmark is here: https://bitcrust.org/results ?\n\nI'd be interested to also see comparisons with 0.14 which has some\nimprovements for script validation with more cores.\n\nOn Fri, Apr 7, 2017 at 4:47 AM, Tomas via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Thank you Marcos,\n>\n> Though written in Rust, bitcrust-db is definitely usable as pluggable\n> module as its interface will be roughly some queries, add_tx and\n> add_block with blobs and flags. (Bitcrust internally uses a\n> deserialize-only model, keeping references to the blobs with the parsed\n> data).\n>\n> However, from Core's side I believe network and storage are currently\n> rather tightly coupled, which will make this far from trivial.\n>\n> Regardless, I am also hoping (with funding & a team) to build a Bitcrust\n> networking component as well to bring a strong competitor to the market.\n>\n> best,\n> Tomas\n>\n>\n>\n> On Fri, Apr 7, 2017, at 09:55, Marcos mayorga wrote:\n> > Hi Tomas,\n> >\n> > I've read it and think it is an excellent work, I'd like to see it\n> > integrated into bitcoin-core as a 'kernel module'.\n> >\n> > I see there are a lot of proof of concepts out there, IMO every one\n> > deserve a room in the bitcoin client as a selectable feature, to make the\n> > software more flexible and less dictatorial, an user could easily select\n> > which features she wants to run.\n> >\n> > Best regards,\n> > Marcos\n> >\n> > > I have been working on a bitcoin implementation that uses a different\n> > > approach to indexing for verifying the order of transactions. Instead\n> of\n> > > using an index of unspent outputs, double spends are verified by using\n> a\n> > > spend-tree where spends are scanned against spent outputs instead of\n> > > unspent outputs.\n> > >\n> > > This allows for much better concurrency, as not only blocks, but also\n> > > individual inputs can be verified fully in parallel.\n> > >\n> > > I explain the approach at https://bitcrust.org, source code is\n> available\n> > > at https://github.com/tomasvdw/bitcrust\n> > >\n> > > I am sharing this not only to ask for your feedback, but also to call\n> > > for a clear separation of protocol and implementations: As this\n> > > solution, reversing the costs of outputs and inputs, seems to have\n> > > excellent performance characteristics (as shown in the test results),\n> > > updates to the protocol addressing the UTXO growth, might not be worth\n> > > considering *protocol improvements* and it might be best to address\n> > > these concerns as implementation details.\n> > >\n> > > Kind regards,\n> > > Tomas van der Wansem\n> > > tomas at bitcrust.org\n> > > Bitcrust\n> > > _______________________________________________\n> > > bitcoin-dev mailing list\n> > > bitcoin-dev at lists.linuxfoundation.org\n> > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> > >\n> >\n> >\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170407/7be2b4cf/attachment.html>"
            },
            {
                "author": "Tomas",
                "date": "2017-04-07T16:02:35",
                "message_text_only": "Thank you,\n\n\n\nThe benches are running in Google Cloud Engine; currently on 8 vCPU\n32gb, but I tend to switch hardware regularly.\n\n\nRoughly, the results are better for Bitcrust with high end hardware and\nthe difference for total block validations is mostly diminished at 2\nvCPU, 7,5 gb.\n\n\nNote that the spend-tree optimization primarily aims to improve peak\nload order validation; when a block with pre-synced transactions comes\nin, but this is tricky to accurately bench with Core using this simple\nmethod of comparison by logs.\n\n\nI will upgrade to, and show the results against 0.14 in the next weeks.\n\n\nBest,\n\nTomas\n\n\n\n\n\nOn Fri, Apr 7, 2017, at 16:14, Greg Sanders wrote:\n\n> Interesting work.\n\n> \n\n> I was wondering if you could tellank  us what specs for the machine\n> being used as preliminary benchmark is here:\n> https://bitcrust.org/results ?\n> \n\n> I'd be interested to also see comparisons with 0.14 which has some\n> improvements for script validation with more cores.\n> \n\n> On Fri, Apr 7, 2017 at 4:47 AM, Tomas via bitcoin-dev <bitcoin-\n> dev at lists.linuxfoundation.org> wrote:\n>> Thank you Marcos,\n\n>> \n\n>>  Though written in Rust, bitcrust-db is definitely usable as\n>>  pluggable\n>>  module as its interface will be roughly some queries, add_tx and\n\n>>  add_block with blobs and flags. (Bitcrust internally uses a\n\n>>  deserialize-only model, keeping references to the blobs with the\n>>  parsed\n>>  data).\n\n>> \n\n>>  However, from Core's side I believe network and storage are\n>>  currently\n>>  rather tightly coupled, which will make this far from trivial.\n\n>> \n\n>>  Regardless, I am also hoping (with funding & a team) to build a\n>>  Bitcrust\n>>  networking component as well to bring a strong competitor to the\n>>  market.\n>> \n\n>>  best,\n\n>>  Tomas\n\n>> \n\n>> \n\n>> \n\n>> \n\n>> On Fri, Apr 7, 2017, at 09:55, Marcos mayorga wrote:\n\n>>  > Hi Tomas,\n\n>>  >\n\n>>  > I've read it and think it is an excellent work, I'd like to see it\n>>  > integrated into bitcoin-core as a 'kernel module'.\n\n>>  >\n\n>>  > I see there are a lot of proof of concepts out there, IMO\n>>  > every one\n>>  > deserve a room in the bitcoin client as a selectable feature, to\n>>  > make the\n>>  > software more flexible and less dictatorial, an user could easily\n>>  > select\n>>  > which features she wants to run.\n\n>>  >\n\n>>  > Best regards,\n\n>>  > Marcos\n\n>>  >\n\n>>  > > I have been working on a bitcoin implementation that uses a\n>>  > > different\n>>  > > approach to indexing for verifying the order of transactions.\n>>  > > Instead of\n>>  > > using an index of unspent outputs, double spends are verified by\n>>  > > using a\n>>  > > spend-tree where spends are scanned against spent outputs\n>>  > > instead of\n>>  > > unspent outputs.\n\n>>  > >\n\n>>  > > This allows for much better concurrency, as not only blocks, but\n>>  > > also\n>>  > > individual inputs can be verified fully in parallel.\n\n>>  > >\n\n>>  > > I explain the approach at https://bitcrust.org, source code is\n>>  > > available\n>>  > > at https://github.com/tomasvdw/bitcrust\n\n>>  > >\n\n>>  > > I am sharing this not only to ask for your feedback, but also to\n>>  > > call\n>>  > > for a clear separation of protocol and implementations: As this\n>>  > > solution, reversing the costs of outputs and inputs, seems to\n>>  > > have\n>>  > > excellent performance characteristics (as shown in the test\n>>  > > results),\n>>  > > updates to the protocol addressing the UTXO growth, might not be\n>>  > > worth\n>>  > > considering *protocol improvements* and it might be best to\n>>  > > address\n>>  > > these concerns as implementation details.\n\n>>  > >\n\n>>  > > Kind regards,\n\n>>  > > Tomas van der Wansem\n\n>>  > > tomas at bitcrust.org\n\n>>  > > Bitcrust\n\n>>  > > _______________________________________________\n\n>>  > > bitcoin-dev mailing list\n\n>>  > > bitcoin-dev at lists.linuxfoundation.org\n\n>>  > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n>>  > >\n\n>>  >\n\n>>  >\n\n>>  _______________________________________________\n\n>>  bitcoin-dev mailing list\n\n>> bitcoin-dev at lists.linuxfoundation.org\n\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170407/68d9cba6/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2017-04-07T18:18:32",
                "message_text_only": "On Thu, Apr 6, 2017 at 10:12 PM, Tomas via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>As this\n> solution, reversing the costs of outputs and inputs, seems to have\n> excellent performance characteristics (as shown in the test results),\n> updates to the protocol addressing the UTXO growth, might not be worth\n> considering *protocol improvements*\n\nI'm still lost on this-- AFAICT your proposals long term resource\nrequirements are directly proportional to the amount of unspent output\ndata, which grows over time at some fraction of the total transaction\nvolume (plus the rate of spending which is more or less a constant).\n\nCan you help out my understanding here?"
            },
            {
                "author": "Bram Cohen",
                "date": "2017-04-07T18:39:18",
                "message_text_only": "Expanding on this question a bit, it's optimized for parallel access, but\nhard drive access isn't parallel and memory accesses are very fast, so\nshouldn't the target of optimization be about cramming as much as possible\nin memory and minimizing disk accesses?\n\nOn Fri, Apr 7, 2017 at 11:18 AM, Gregory Maxwell via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Thu, Apr 6, 2017 at 10:12 PM, Tomas via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >As this\n> > solution, reversing the costs of outputs and inputs, seems to have\n> > excellent performance characteristics (as shown in the test results),\n> > updates to the protocol addressing the UTXO growth, might not be worth\n> > considering *protocol improvements*\n>\n> I'm still lost on this-- AFAICT your proposals long term resource\n> requirements are directly proportional to the amount of unspent output\n> data, which grows over time at some fraction of the total transaction\n> volume (plus the rate of spending which is more or less a constant).\n>\n> Can you help out my understanding here?\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170407/6a7fb499/attachment-0001.html>"
            },
            {
                "author": "Eric Voskuil",
                "date": "2017-04-07T19:55:58",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\n\nOn 04/07/2017 11:39 AM, Bram Cohen via bitcoin-dev wrote:\n> Expanding on this question a bit, it's optimized for parallel\n> access, but hard drive access isn't parallel and memory accesses\n> are very fast, so shouldn't the target of optimization be about\n> cramming as much as possible in memory and minimizing disk\n> accesses?\n\nWhile this may seem to be the case it is not generally optimal. The\nquestion is overly broad as one may or may not be optimizing for any\ncombination of:\n\nstartup time (first usability)\nwarm-up time (priming)\nshutdown time (flush)\nfault tolerance (hard shutdown survivability)\ntop block validation (read speed)\nfull chain validation (read/write speed)\nRAM consumption\nDisk consumption\nQuery response\nServers (big RAM)\nDesktops (small RAM)\nMining (fast validation)\nWallets (background performance)\nSSD vs. HDD\n\nBut even limiting the question to input validation, all of these\nconsiderations (at least) are present.\n\nIdeally one wants the simplest implementation that is optimal under\nall considerations. While this may be a unicorn, it is possible to\nachieve a simple implementation (relative to alternatives) that allows\nfor the trade-offs necessary to be managed through configuration (by\nthe user and/or implementation).\n\nShoving the entire data set into RAM has the obvious problem of\nlimited RAM. Eventually the OS will be paging more of the data back to\ndisk (as virtual RAM). In other words this does not scale, as a change\nin hardware disproportionately impacts performance. Ideally one wants\nthe trade between \"disk\" and \"memory\" to be made by the underlying\nplatform, as that is its purpose. Creating one data structure for disk\nand another for memory not only increases complexity, but denies the\nplatform visibility into this trade-off. As such the platform\neventually ends up working directly against the optimization.\n\nAn on-disk structure that is not mapped into memory by the application\nallows the operating system to maintain as much or as little state in\nmemory as it considers optimal, given the other tasks that the user\nhas given it. In the case of memory mapped files (which are optimized\nby all operating systems as central to their virtual memory systems)\nit is possible for everything from zero to the full store to be memory\nresident.\n\nOptimization for lower memory platforms then becomes a process of\nreducing the need for paging. This is the purpose of a cache. The seam\nbetween disk and memory can be filled quite nicely by a small amount\nof cache. On high RAM systems any cache is actually a de-optimization\nbut on low RAM systems it can prevent excessive paging. This is\ndirectly analogous to a CPU cache. There are clear optimal points in\nterms of cache size, and the implementation and management of such a\ncache can and should be internal to a store. Of course a cache cannot\nprovide perfect scale all the way to zero RAM, but it scales quite\nwell for actual systems.\n\nWhile a particular drive may not support parallel operations one\nshould not assume that a disk-based store does not benefit from\nparallelism. Simply refer to the model described above and you will\nsee that with enough memory the entire blockchain can be\nmemory-resident, and for high performance operations a fraction of\nthat is sufficient for a high degree of parallelism.\n\nIn practice a cache of about 10k transactions worth of outputs is\noptimal for 8GB RAM. This requires just a few blocks for warm-up,\nwhich can be primed in inconsequential time at startup. Fault\ntolerance can be managed by flushing after all writes, which also\nreduces shutdown time to zero. For higher performance systems,\nflushing can be disabled entirely, increasing shutdown time but also\ndramatically increasing write performance. Given that the blockchain\nis a cache, this is a very reasonable trade-off in some scenarios. The\nmodel works just as well with HDD as SSD, although certainly SSD\nperforms better overall.\n\ne\n-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v2.0.22 (GNU/Linux)\n\niQEcBAEBCAAGBQJY5+7GAAoJEDzYwH8LXOFOsAsH/3QK55aWH6sAi6OsTwV1FLZV\nY/2SSjwn1vUh55MDkPpCxDwV99JqVwpk0vGM8mGg5s4ZS8sxOPqwGiBz/SZWbF9v\noStJS0DjUPnbYtI/mrC30GuAYVcKnc5DFDHvjX6f0xrLIzViFR7eiW0npUH6Xipt\nRI9Mockaf1CqqGExtbIqWal0YDEQGH0ekXRp7uEjh8nPUoKqTVvxDCgqVooQfvfx\nEeKX9ruSv/r91EM1JQuH8HBBF7+R24tmMtwbpGx0zrDg5ytpIyrRzVH/ze1Mj2a3\nZxThvofGzhKcDiTPWiJI11DBYUvhSH4Kx0uWLzFUA0gxPfWkZQKJWNDl2CEwljk=\n=C7rD\n-----END PGP SIGNATURE-----"
            },
            {
                "author": "Tomas",
                "date": "2017-04-07T21:44:43",
                "message_text_only": "Hi Eric,\n\nOn Fri, Apr 7, 2017, at 21:55, Eric Voskuil via bitcoin-dev wrote:\n> Optimization for lower memory platforms then becomes a process of\n> reducing the need for paging. This is the purpose of a cache. The seam\n> between disk and memory can be filled quite nicely by a small amount\n> of cache. On high RAM systems any cache is actually a de-optimization\n> but on low RAM systems it can prevent excessive paging. This is\n> directly analogous to a CPU cache. \n\n\nI am not entirely sure I agree with that, or understand it correctly.\n\nIf -for example - the data of some application is a set  of records\nwhich can be sorted from least frequently used to most frequently used\nthen doing just that sort will beat any application-layer cache.\nRegardless of size of data and size of RAM, you simply allow the OS to\nuse disk caching or memory map caching to work its  magic .\n\nIn fact, I would argue that an application-layer cache *only* makes\nsense if the data model shows a *hard* distinction between often and not\noften used data. If usage-frequency is a continuous line, caching is\nbest left to the OS by focussing on proper spatial and temporal locality\nof reference of your data, because the OS has much more information to\nmake the right decision."
            },
            {
                "author": "Eric Voskuil",
                "date": "2017-04-07T23:51:08",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\n\nOn 04/07/2017 02:44 PM, Tomas via bitcoin-dev wrote:\n> Hi Eric,\n> \n> On Fri, Apr 7, 2017, at 21:55, Eric Voskuil via bitcoin-dev wrote:\n>> Optimization for lower memory platforms then becomes a process\n>> of reducing the need for paging. This is the purpose of a cache.\n>> The seam between disk and memory can be filled quite nicely by a\n>> small amount of cache. On high RAM systems any cache is actually\n>> a de-optimization but on low RAM systems it can prevent excessive\n>> paging. This is directly analogous to a CPU cache.\n> \n> \n> I am not entirely sure I agree with that, or understand it\n> correctly.\n> \n> If -for example - the data of some application is a set  of\n> records which can be sorted from least frequently used to most\n> frequently used then doing just that sort will beat any\n> application-layer cache. Regardless of size of data and size of\n> RAM, you simply allow the OS to use disk caching or memory map\n> caching to work its  magic .\n\nIt's a reasonable assumption, and given that the no-explicit-cache\nimplementation is a subset of the optionally-cached implementation,\nwas of course the initial implementation.\n\n> In fact, I would argue that an application-layer cache *only*\n> makes sense if the data model shows a *hard* distinction between\n> often and not often used data. If usage-frequency is a continuous\n> line, caching is best left to the OS by focussing on proper spatial\n> and temporal locality of reference of your data, because the OS has\n> much more information to make the right decision.\n\nIn practice this is not the case. The Bitcoin data model is neither\ncontinuous nor strictly segregated by usage.\n\nIt is true that with sufficient RAM a cache is totally\ncounterproductive. It is also my experience that an independent UTXO\nstore is not a reasonable/necessary trade of disk space, memory\nscalability, and/or code complexity in exchange for speed.\n\nBut on lower memory systems a explicit cache is beneficial. The\ndifference is clearly measurable in production code by simply changing\nthe cache limit and testing on various configurations.\n\ne\n-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v2.0.22 (GNU/Linux)\n\niQEcBAEBCAAGBQJY6CXnAAoJEDzYwH8LXOFOf0YH/2qk3hYC6iEDW/DWM2ffkdb9\nQM7A29Pvbfw9Wjr5Xx+ugIQvlAr4T+nByOCT6AnrqNU5K3UUmbC0KIB1rEL94hsK\nQYVlLs0cOrjg8qKJpck+wcgiWw3VbEa/Y44hK7NLUxoy2HsLYaxPhqFH3GGgowqR\nsyga626jf2YUyudZxj1gFuqn7grkwghnzdrEUJMcqQo8IvCqjftGXlKxBGyB/AIs\nDx+5EWO9Q9IxrNpg/fsKKB6xkMxkmSx2hbD7dmEBvi/afbVF66rDTinjInG/LCju\npV7kT/GAWqGQGku6sQyAOexsxVhWA8EA/QEjvbyyGb+3YnR0s6nPk+CxO+RkOgo=\n=e+Pr\n-----END PGP SIGNATURE-----"
            },
            {
                "author": "Tomas",
                "date": "2017-04-07T21:14:51",
                "message_text_only": "Answering both,\n\n\n\nOn Fri, Apr 7, 2017 at 11:18 AM, Gregory Maxwell via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> \n\n>> I'm still lost on this-- AFAICT your proposals long term resource\n\n>> requirements are directly proportional to the amount of\n>> unspent output\n>> data, which grows over time at some fraction of the total transaction\n>> volume (plus the rate of spending which is more or less a constant).\n>> \n\n>> Can you help out my understanding here?\n\n>> \n\n\n\nOn Fri, Apr 7, 2017, at 20:39, Bram Cohen wrote:\n\n> Expanding on this question a bit, it's optimized for parallel access,\n> but hard drive access isn't parallel and memory accesses are very\n> fast, so shouldn't the target of optimization be about cramming as\n> much as possible in memory and minimizing disk accesses?\n\n\nThe long term *minimal disk storage* requirement, can obviously not be\nless then all the unspent outputs. Minimal disk requirements is not\nsomething bitcrust attempts to address.\n\n\n The storage that is accessed during peak load (block validation with\n pre-synced transactions), is minimized as this only needs the\n transaction index (to lookup ptrs from hashes), the tip of the spend-\n tree and the tip of the spend-index (together to check double\n spents/spending non-existing outputs). These not only easily fit in\n RAM, but are accessed in a cache efficient way. *These* only grow with\n inputs as the spend tree contains one record per input referencing the\n output being spent.\n\n\nScript validation is also not something bitcrust *directly* addresses;\nit uses libbitcoinconsensus for the actual validation and lookups to\noutputs are mostly similar. They are kept fast by trusting the OS on MRU\ncaching of transaction-outputs; I don't think that for this part the\nUTXO index has much drawbacks,. Bitcrust seems to have a small advantage\ndue to the awesomeness of Rayon's parallelization and the lock-free data\nstructures, but a disadvantage in that keeping all spent outputs\ndecreases spatial locality of reference. Script validation is not the\ninnovative part.\n\n\nTomas\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170407/f7db76c7/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2017-04-08T00:44:50",
                "message_text_only": "On Fri, Apr 7, 2017 at 9:14 PM, Tomas <tomas at tomasvdw.nl> wrote:\n> The long term *minimal disk storage* requirement, can obviously not be less\n> then all the unspent outputs.\n\nThen I think you may want to retract the claim that \"As this solution,\nreversing the costs of outputs and inputs, [...] updates to the\nprotocol addressing the UTXO growth, might not be worth considering\n*protocol improvements* \"\n\nAs you note that the output costs still bound the resource\nrequirements. Short of radical protocol changes like TXO-proofs the\nUTXO data remains a driving unavoidable long term resource cost, not\nan implementation detail.  Implementation optimizations like improving\nlocality further or keeping spentness in memory do not change this\nfact.\n\n> The storage that is accessed during peak load (block validation with\n> pre-synced transactions), is minimized as this only needs the transaction\n> index (to lookup ptrs from hashes), the tip of the spend-tree and the tip of\n\nLatency related costs in Bitcoin Core also do not depend on the number\nof outputs in transactions in a block. When a transaction is handled\nit goes into an in-memory buffer and only gets flushed later if isn't\nspent before the buffer fills.  A block will take more time to\nvalidate with more inputs, same as you observer, but the aggregate\nresource usage for users depends significantly on outputs (so, in fact\nthere is even further misaligned incentives than just the fact that\nsmall outputs have a outsized long term cost)."
            },
            {
                "author": "Tomas",
                "date": "2017-04-08T07:28:48",
                "message_text_only": "On Sat, Apr 8, 2017, at 02:44, Gregory Maxwell wrote:\n> As you note that the output costs still bound the resource\n> requirements. \n\nResource cost is not just a measure of storage requirement; data that\nneeds to be accessed during peak load induce more cost then data only\nused during base load or only rarely used.\n\n> Latency related costs in Bitcoin Core also do not depend on the number\n> of outputs in transactions in a block. When a transaction is handled\n> it goes into an in-memory buffer and only gets flushed later if isn't\n> spent before the buffer fills.  A block will take more time to\n> validate with more inputs, same as you observer, but the aggregate\n> resource usage for users depends significantly on outputs (so, in fact\n> there is even further misaligned incentives than just the fact that\n> small outputs have a outsized long term cost).\n\nIn Core, when a block comes the inputs are checked against the UTXO set\n(which grows with outputs)  even if pre-synced, to verify order. Am I\nwrong there? This is not in the case in bitcrust; it is instead checked\nagainst the spend-tree (which grows with inputs).\n\nHow \"significant\" this is, I neither know nor claim,  but it is an\ninteresting difference. \n\n> Then I think you may want to retract the claim that \"As this solution,\n> reversing the costs of outputs and inputs, [...] updates to the\n> protocol addressing the UTXO growth, might not be worth considering\n> *protocol improvements* \"\n\nI think you are being a bit harsh here . I am also clearly explaining\nthe difference only applies to peak load, and just making a suggestion.\nI simply want to stress the importance of protocol / implementation\nseparation as even though you are correct UTXO data is always a resource\ncost for script validation (as I also state), the ratio of different\ncosts are  not necessarily *identical* across implementation. \n\nNote that the converse also holds: In bitcrust, if the last few blocks\ncontain many inputs, the peak load verification for this block is\nslower. This is not the case in Core.\n\nTomas"
            },
            {
                "author": "Johnson Lau",
                "date": "2017-04-08T19:23:29",
                "message_text_only": "> On 8 Apr 2017, at 15:28, Tomas via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> \n> \n> I think you are being a bit harsh here . I am also clearly explaining\n> the difference only applies to peak load, and just making a suggestion.\n> I simply want to stress the importance of protocol / implementation\n> separation as even though you are correct UTXO data is always a resource\n> cost for script validation (as I also state), the ratio of different\n> costs are  not necessarily *identical* across implementation. \n> \n> Note that the converse also holds: In bitcrust, if the last few blocks\n> contain many inputs, the peak load verification for this block is\n> slower. This is not the case in Core.\n> \n> Tomas\n> \n\nI don\u2019t fully understand your storage engine. So the following deduction is just based on common sense.\n\na) It is possible to make unlimited number of 1-in-100-out txs\n\nb) The maximum number of 100-in-1-out txs is limited by the number of previous 1-in-100-out txs\n\nc) Since bitcrust performs not good with 100-in-1-out txs, for anti-DoS purpose you should limit the number of previous 1-in-100-out txs. \n\nd) Limit 1-in-100-out txs == Limit UTXO growth\n\nI\u2019m not surprised that you find an model more efficient than Core. But I don\u2019t believe one could find a model that doesn\u2019t become more efficient with UTXO growth limitation.\n\nMaybe you could try an experiment with regtest? Make a lot 1-in-100-out txs with many blocks, then spend all the UTXOs with 100-in-1-out txs. Compare the performance of bitcrust with core. Then repeat with 1-in-1-out chained txs (so the UTXO set is always almost empty)\n\nOne more question: what is the absolute minimum disk and memory usage in bitcrust, compared with the pruning mode in Core?"
            },
            {
                "author": "Tomas",
                "date": "2017-04-08T19:56:18",
                "message_text_only": "> I don\u2019t fully understand your storage engine. So the following deduction\n> is just based on common sense.\n> \n> a) It is possible to make unlimited number of 1-in-100-out txs\n> \n> b) The maximum number of 100-in-1-out txs is limited by the number of\n> previous 1-in-100-out txs\n> \n> c) Since bitcrust performs not good with 100-in-1-out txs, for anti-DoS\n> purpose you should limit the number of previous 1-in-100-out txs. \n> \n> d) Limit 1-in-100-out txs == Limit UTXO growth\n> \n> I\u2019m not surprised that you find an model more efficient than Core. But I\n> don\u2019t believe one could find a model that doesn\u2019t become more efficient\n> with UTXO growth limitation.\n\nMy efficiency claims are *only* with regards to order validation. If we\nassume all transactions are already pre-synced and verified, bitcrust's\norder validation is very fast, and (only slightly) negatively effected\nby input-counts.\n\nMost total time is spend during base load script validation, and UTXO\ngrowth is the definitely the limiting factor there, as the model here\nisn't all that different from Core's.\n\n\n> Maybe you could try an experiment with regtest? Make a lot 1-in-100-out\n> txs with many blocks, then spend all the UTXOs with 100-in-1-out txs.\n> Compare the performance of bitcrust with core. Then repeat with\n> 1-in-1-out chained txs (so the UTXO set is always almost empty)\n> \n\nAgain, this really depends on whether we focus on full block validation,\nin which case the 100-1, 1-100 distinction will be the similar to Core,\nor only regard order validation, in which case Bitcrust will have this\nodd reversal. \n\n\n> One more question: what is the absolute minimum disk and memory usage in\n> bitcrust, compared with the pruning mode in Core?\n\nAs bitcrust doesn't support this yet, I cannot give accurate numbers,\nbut I've provided some numbers estimates earlier in the thread.\n\n\nRereading my post and these comments, I may have stepped on some toes\nwith regards to SegWit's model. I like SegWit (though I may have a\nslight preference for BIP140), and I understand the reasons for the\n\"discount\", so this was not my intention. I just think that the reversal\nof costs during peak load order validation is a rather interesting\nfeature of using spend-tree  based validation. \n\nTomas"
            },
            {
                "author": "Johnson Lau",
                "date": "2017-04-08T20:21:04",
                "message_text_only": "> On 9 Apr 2017, at 03:56, Tomas <tomas at tomasvdw.nl> wrote:\n> \n> \n>> I don\u2019t fully understand your storage engine. So the following deduction\n>> is just based on common sense.\n>> \n>> a) It is possible to make unlimited number of 1-in-100-out txs\n>> \n>> b) The maximum number of 100-in-1-out txs is limited by the number of\n>> previous 1-in-100-out txs\n>> \n>> c) Since bitcrust performs not good with 100-in-1-out txs, for anti-DoS\n>> purpose you should limit the number of previous 1-in-100-out txs. \n>> \n>> d) Limit 1-in-100-out txs == Limit UTXO growth\n>> \n>> I\u2019m not surprised that you find an model more efficient than Core. But I\n>> don\u2019t believe one could find a model that doesn\u2019t become more efficient\n>> with UTXO growth limitation.\n> \n> My efficiency claims are *only* with regards to order validation. If we\n> assume all transactions are already pre-synced and verified, bitcrust's\n> order validation is very fast, and (only slightly) negatively effected\n> by input-counts.\n\npre-synced means already in mempool and verified? Then it sounds like we just need some mempool optimisation? The tx order in a block is not important, unless they are dependent\n\n> \n>> One more question: what is the absolute minimum disk and memory usage in\n>> bitcrust, compared with the pruning mode in Core?\n> \n> As bitcrust doesn't support this yet, I cannot give accurate numbers,\n> but I've provided some numbers estimates earlier in the thread.\n> \n> \n> Rereading my post and these comments, I may have stepped on some toes\n> with regards to SegWit's model. I like SegWit (though I may have a\n> slight preference for BIP140), and I understand the reasons for the\n> \"discount\", so this was not my intention. I just think that the reversal\n> of costs during peak load order validation is a rather interesting\n> feature of using spend-tree  based validation. \n> \n> Tomas\n\nPlease no conspiracy theory like stepping on someone\u2019s toes. I believe it\u2019s always nice to challenge the established model. However, as I\u2019m trying to make some hardfork design, I intend to have a stricter UTXO growth limit. As you said \"protocol addressing the UTXO growth, might not be worth considering protocol improvements*, it sounds like UTXO growth limit wouldn\u2019t be very helpful for your model, which I doubt."
            },
            {
                "author": "Tomas",
                "date": "2017-04-08T20:42:57",
                "message_text_only": "> Please no conspiracy theory like stepping on someone\u2019s toes. I believe\n> it\u2019s always nice to challenge the established model. However, as I\u2019m\n> trying to make some hardfork design, I intend to have a stricter UTXO\n> growth limit. As you said \"protocol addressing the UTXO growth, might not\n> be worth considering protocol improvements*, it sounds like UTXO growth\n> limit wouldn\u2019t be very helpful for your model, which I doubt. \n\nThank you. I realize that  this particular phrase implies that in my\ndesign, outputs are less costly then inputs, *in total resource costs*,\nwhich I can not defend without completely ignoring base load script\nverification. I rephrased it.\n\nTomas"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2017-04-08T22:12:09",
                "message_text_only": "On Sat, Apr 8, 2017 at 8:21 PM, Johnson Lau <jl2012 at xbt.hk> wrote:\n> pre-synced means already in mempool and verified? Then it sounds like we just need some mempool optimisation? The tx order in a block is not important, unless they are dependent\n\nIn Bitcoin Core the software _explicitly_ and intentionally does not\nexploit mempool pre-validation because doing that very easily leads to\nhard to detect consensus faults and makes all mempool code consensus\ncritical when it otherwise is not. There have been bugs in the past\nwhich would have split the network if this optimization had been used.\n\n(in particular, I believe I recall one related to correctly removing\ncoinbase spends from the mempool during reorganization that made them\nimmature; and with the optimization and without the CNB post-test\nwould have resulted in nodes that saw the reorg creating and accepting\nan invalid block, while nodes that didn't rejecting it; but because of\nprudent design it was largely harmless).\n\nBecause signature validation is cached, and takes the majority of the\nblock validation time the speed up from the risky optimization isn't\nthat considerable, and there are other lower hanging fruity with\nbigger payouts like Pieter's change to the per-txout management model\nand the new non-atomic flushing logic.... and these things don't make\nmore of the system consensus critical."
            },
            {
                "author": "Tomas",
                "date": "2017-04-08T22:34:11",
                "message_text_only": "On Sun, Apr 9, 2017, at 00:12, Gregory Maxwell wrote:\n> In Bitcoin Core the software _explicitly_ and intentionally does not\n> exploit mempool pre-validation because doing that very easily leads to\n> hard to detect consensus faults and makes all mempool code consensus\n> critical when it otherwise is not. There have been bugs in the past\n> which would have split the network if this optimization had been used.\n> \n> (in particular, I believe I recall one related to correctly removing\n> coinbase spends from the mempool during reorganization that made them\n> immature; and with the optimization and without the CNB post-test\n> would have resulted in nodes that saw the reorg creating and accepting\n> an invalid block, while nodes that didn't rejecting it; but because of\n> prudent design it was largely harmless).\n\nAlthough I don't quite follow the details (CNB post-test? Connect block\nI assume?), the risks you are describing seem to be rather specific to\nCore's implementation. For one, bitcrust does not or use need reorgs at\nall.\n\nDo you argue (or can you further explain) that the idea of splitting\nscript validation (or what you call mempool pre-validation), and order\nvalidation is introducing risks  inherent to the protocol? \n\nThanks,\nTomas"
            },
            {
                "author": "Troy Benjegerdes",
                "date": "2017-04-08T21:22:11",
                "message_text_only": "I would advise anyone worried about 'hard drive access' to order a\n512GB NVME (pci-express interface) flash drive (or a laptop), and\nI expect the performance will make you wonder why you ever bothered\nwith cloud.\n\nMy (very brief) analysis of the performance of a full chain download\non a new laptop was that there was more overhead in lock contention and\ndatabase writes and it barely loaded the machine. Now maybe this is\nbecause the flash **write** speed is slow (but still several orders\nof magnitude above spinning disk), but random reads are sure blazing\nfast.\n\nFlash storage sizes also appear to be growing at similiar rates as the\ntotal blockchain size.\n\nWhich begs another question: In a distributed byzantine fault-tolerant\nsystem, why do we even need to bother with persistant storage, except\nfor long-term archival and chain of custody issues, which we could \nserialize the in-memory structures out as a stream to things like tape\ndrives or write-once optical media.\n\n\nOn Fri, Apr 07, 2017 at 11:39:18AM -0700, Bram Cohen via bitcoin-dev wrote:\n> Expanding on this question a bit, it's optimized for parallel access, but\n> hard drive access isn't parallel and memory accesses are very fast, so\n> shouldn't the target of optimization be about cramming as much as possible\n> in memory and minimizing disk accesses?\n> \n> On Fri, Apr 7, 2017 at 11:18 AM, Gregory Maxwell via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> > On Thu, Apr 6, 2017 at 10:12 PM, Tomas via bitcoin-dev\n> > <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > >As this\n> > > solution, reversing the costs of outputs and inputs, seems to have\n> > > excellent performance characteristics (as shown in the test results),\n> > > updates to the protocol addressing the UTXO growth, might not be worth\n> > > considering *protocol improvements*\n> >\n> > I'm still lost on this-- AFAICT your proposals long term resource\n> > requirements are directly proportional to the amount of unspent output\n> > data, which grows over time at some fraction of the total transaction\n> > volume (plus the rate of spending which is more or less a constant).\n> >\n> > Can you help out my understanding here?\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> >\n\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Tomas",
                "date": "2017-04-07T00:48:52",
                "message_text_only": "On Fri, Apr 7, 2017, at 02:32, Gregory Maxwell wrote:\n\n> Perhaps a simple question would help:\n> \n> What is the minimal amount of space your system requires to take a new\n> block received from the P2P network and verifying that all its spends\n> were valid spends of existing coins unspent coins today?\n> \n> For Bitcoin Core the answer is ~2GB (plus the configuration handling\n> currently forces you to keep another 550MB of blocks for reorgs).\n\nBitcrust separates script validation (base load, when transaction come\nin) from order validation (peak load, when blocks come in).\n\nFor script validation it would obviously need the ~2GB (or I think\n~1.5GB) of outputs needed to validate these.  For order validation it\nneeds ~200mb or the spent-index (for bit-lookups) and I would guess\nroughly ~500mb of the spent-tree (for scanning), though I don't think\nthe 5.7GB full spend tree isn't worth pruning anytime soon.\n\nThen it is currently using a  ~1.5GB   index for transaction hash to\nfileptr lookups, though this could be made more space efficient."
            },
            {
                "author": "Gregory Maxwell",
                "date": "2017-04-07T01:09:26",
                "message_text_only": "On Fri, Apr 7, 2017 at 12:48 AM, Tomas <tomas at tomasvdw.nl> wrote:\n> Bitcrust separates script validation (base load, when transaction come\n> in) from order validation (peak load, when blocks come in).\n\nHow do you deal with validity rules changing based on block height?\n\n> For script validation it would obviously need the ~2GB (or I think\n> ~1.5GB) of outputs needed to validate these.\n\nSo it sounds like to work the software still needs an analog of a\n(U)TXO database? I am confused by the earlier comments about thinking\nthe the resource consumption of the (U)TXO database is not a\nconsideration in your design.\n\n> For order validation it\n> needs ~200mb or the spent-index (for bit-lookups) and I would guess\n> roughly ~500mb of the spent-tree (for scanning), though I don't think\n> the 5.7GB full spend tree isn't worth pruning anytime soon.\n\nIf you get a transaction claiming to spend 0xDEADBEEFDEADBEEF, an\noutput that never existed how does your spent index reject this spend?"
            },
            {
                "author": "Tomas",
                "date": "2017-04-07T01:29:07",
                "message_text_only": "On Fri, Apr 7, 2017, at 03:09, Gregory Maxwell wrote:\n> \n> How do you deal with validity rules changing based on block height?\n\nI expected that one :). Just like the 100 blocks coinbase rule, changes\nby softforks need to be added as metadata to the transaction-index, but\nthis is not yet in place.\n\nAs for the script validation itself using libbitcoinconsensus, this is a\nbit hairy as this expects the rules to be known. Luckily, simply\ngradually retrying using \"lower\" rules won't hurt performance, as\ntransaction that mismatch newer rules are rare.\n\nGenerally, bitcrust would appreciate a \"is valid with X rules\" instead \nof a \"validate with X rules\" approach.\n\n\n> So it sounds like to work the software still needs an analog of a\n> (U)TXO database? I am confused by the earlier comments about thinking\n> the the resource consumption of the (U)TXO database is not a\n> consideration in your design.\n\nNo, but transactional access is. Bitcrust just uses a\n*transaction-index*, where outputs can be looked up regardless of being\nspent. As the concept of being \"spent\" depends on the branch, script\nvalidation ignores this and simply looks up the outputs.\n\nTransactions are split in two parts for better locality of reference\nwhen accessing outputs.\n\nThe transaction index only looks similar to an \"UTXO-index\" after full\npruning.\n\n> If you get a transaction claiming to spend 0xDEADBEEFDEADBEEF, an\n> output that never existed how does your spent index reject this spend\n\nThe spend-tree is scanned until either DEADBEAF is found (=ERR double\nspent),  the transaction of DEADBEEF is found (=all ok!), or the start\nof the chain is reached (=ERR spending unknown output!)\n\nTo prevent actually having to scan to genesis, the spent-index \"catches\"\nthe search after a few blocks and performs the same lookup (positive for\ntx, negative for output) on a simple bit index."
            },
            {
                "author": "Tom Harding",
                "date": "2017-04-07T18:52:20",
                "message_text_only": "On Apr 6, 2017 6:31 PM, \"Tomas via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n\nBitcrust just uses a *transaction-index*, where outputs can be looked up\nregardless of being spent.\n\n\n\nA network in which many nodes maintain a transaction index also enables a\nclass of light node applications that ask peers to prove existence and\nspentness of TXO's.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170407/fab42b2a/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2017-04-07T19:42:22",
                "message_text_only": "On Fri, Apr 7, 2017 at 6:52 PM, Tom Harding via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> A network in which many nodes maintain a transaction index also enables a\n> class of light node applications that ask peers to prove existence and\n> spentness of TXO's.\n\nOnly with the additional commitment structure such as those proposed\nby Peter Todd in his stxo/txo commitment designs, e.g.\nhttps://petertodd.org/2016/delayed-txo-commitments"
            },
            {
                "author": "Tom Harding",
                "date": "2017-04-08T18:27:19",
                "message_text_only": "On Apr 7, 2017 12:42, \"Gregory Maxwell\" <greg at xiph.org> wrote:\n\nOn Fri, Apr 7, 2017 at 6:52 PM, Tom Harding via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> A network in which many nodes maintain a transaction index also enables a\n> class of light node applications that ask peers to prove existence and\n> spentness of TXO's.\n\nOnly with the additional commitment structure such as those proposed\nby Peter Todd in his stxo/txo commitment designs, e.g.\nhttps://petertodd.org/2016/delayed-txo-commitments\n\n\nLight nodes are improved by detecting invalid transactions, even before\nthey are mined.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170408/18ee4220/attachment.html>"
            },
            {
                "author": "Tomas",
                "date": "2017-04-08T19:23:40",
                "message_text_only": "On Sat, Apr 8, 2017, at 20:27, Tom Harding via bitcoin-dev wrote:\n\n> \n\n> \n\n> On Apr 7, 2017 12:42, \"Gregory Maxwell\" <greg at xiph.org> wrote:\n\n>> On Fri, Apr 7, 2017 at 6:52 PM, Tom Harding via bitcoin-dev\n\n>>  <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>>  > A network in which many nodes maintain a transaction index also\n>>  > enables a\n>>  > class of light node applications that ask peers to prove\n>>  > existence and\n>>  > spentness of TXO's.\n\n>> \n\n>> Only with the additional commitment structure such as those proposed\n>>  by Peter Todd in his stxo/txo commitment designs, e.g.\n\n>> https://petertodd.org/2016/delayed-txo-commitments\n\n> Light nodes are improved by detecting invalid transactions, even\n> before they are mined.\n> _________________________________________________\n\n\n\nI am not quite sure why you think this approach would help in this\nregard. I may be missing part of how Core works here, but Bitcrust's\ntxindex is merely used to lookup transactions from hashes and currently,\nand seems to  fulfil the same role  as Core's -txindex  mode.\n\n\nThis can be pruned, and in the future auto-pruned as the \"flat files\"\nused as base for all data allow for concurrent pruning. But unlike Core,\nit is always needed as without UTXO index, it is needed to find outputs\nduring base load validation.\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170408/8cb653d5/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Using a storage engine without UTXO-index",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Tomas",
                "Eric Voskuil",
                "Troy Benjegerdes",
                "Marcos mayorga",
                "Johnson Lau",
                "Gregory Maxwell",
                "Bram Cohen",
                "Tom Harding",
                "Greg Sanders"
            ],
            "messages_count": 35,
            "total_messages_chars_count": 108705
        }
    },
    {
        "title": "[bitcoin-dev] A Small Modification to Segwit",
        "thread_messages": [
            {
                "author": "Jimmy Song",
                "date": "2017-04-07T20:06:39",
                "message_text_only": "Hey everyone, This is an idea that I had about Segwit and Gregory's\nproposal from yesterday that I wanted to run by everyone on this list. I'm\nnot at all sure what this would mean for non-upgraded nodes on the network\nand would like feedback on that. This is not a formal BIP as it's a\nmodification to a previously submitted one, but I'm happy to formalize it\nif it would help.\n----------------------------------------\nMotivationOne of the interesting aspects of Gregory Maxwell\u2019s proposal is\nthat it only precludes the covert version of ASICBoost. He specifically\nleft the overt version alone.\n\nOvert ASICBoost requires grinding on the version bits of the Block header\ninstead of the Merkle Root. This is likely more efficient than the Merkle\nRoot grinding (aka covert ASICBoost) and requires way less resources (much\nless RAM, SHA256 calculations, no tx shuffling, etc).\n\nIf we combine Gregory Maxwell\u2019s proposal with BIP-141 (Segwit) and add a\nslight modification, this should, in theory, make ASICBoost a lot more\nuseful to miners and appeal to their financial interests.\nThe Modification\n\nCurrently, the version bits (currently 4 bytes, or 32 bits) in the header\nare used for BIP9 signaling. We change the version bits to a nonce-space so\nthe miners can use it for overt ASICBoost. The 32-bits are now moved over\nto the Coinbase transaction as part of the witness commitment. The witness\ncommitment goes from 38 bytes to 42 bytes, with the last 4 bytes being used\nas the version bits in the block header previously. The witness commitment\nbecomes required as per Gregory Maxwell\u2019s proposal.\nReasoning\n\nFirst, this brings ASICBoost out into the open. Covert ASICBoost becomes\nmuch more costly and overt ASICBoost is now encouraged.\n\nSecond, we can make this change relatively quickly. Most of the Segwit\ntesting stays valid and this change can be deployed relatively quickly.\n\nNote on SPV clients\n\nCurrently Segwit stores the witness commitment in the Coinbase tx, so\nlightweight clients will need to get the Coinbase tx + Merkle proof to\nvalidate segwit transactions anyway. Putting block version information in\nthe Coinbase tx will not impose an extra burden on upgraded light clients.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170407/93c88127/attachment.html>"
            },
            {
                "author": "Jimmy Song",
                "date": "2017-04-08T00:05:16",
                "message_text_only": "I've gotten feedback from Adam Back that you actually don't need all 32\nbits in the header for overt ASICBoost, so I'm modifying my proposal. Of\nthe 32-bit version field, bits 16 to 23 are reserved for miners, the\nwitness commitment stays as defined in BIP-141 except that it's now\nrequired. BIP9 then is modified so that bits 16 to 23 are now no longer\nusable.\n\nOn Fri, Apr 7, 2017 at 3:06 PM, Jimmy Song <jaejoon at gmail.com> wrote:\n\n> Hey everyone, This is an idea that I had about Segwit and Gregory's\n> proposal from yesterday that I wanted to run by everyone on this list. I'm\n> not at all sure what this would mean for non-upgraded nodes on the network\n> and would like feedback on that. This is not a formal BIP as it's a\n> modification to a previously submitted one, but I'm happy to formalize it\n> if it would help.\n> ----------------------------------------\n> MotivationOne of the interesting aspects of Gregory Maxwell\u2019s proposal is\n> that it only precludes the covert version of ASICBoost. He specifically\n> left the overt version alone.\n>\n> Overt ASICBoost requires grinding on the version bits of the Block header\n> instead of the Merkle Root. This is likely more efficient than the Merkle\n> Root grinding (aka covert ASICBoost) and requires way less resources\n> (much less RAM, SHA256 calculations, no tx shuffling, etc).\n>\n> If we combine Gregory Maxwell\u2019s proposal with BIP-141 (Segwit) and add a\n> slight modification, this should, in theory, make ASICBoost a lot more\n> useful to miners and appeal to their financial interests.\n> The Modification\n>\n> Currently, the version bits (currently 4 bytes, or 32 bits) in the header\n> are used for BIP9 signaling. We change the version bits to a nonce-space so\n> the miners can use it for overt ASICBoost. The 32-bits are now moved over\n> to the Coinbase transaction as part of the witness commitment. The witness\n> commitment goes from 38 bytes to 42 bytes, with the last 4 bytes being used\n> as the version bits in the block header previously. The witness commitment\n> becomes required as per Gregory Maxwell\u2019s proposal.\n> Reasoning\n>\n> First, this brings ASICBoost out into the open. Covert ASICBoost becomes\n> much more costly and overt ASICBoost is now encouraged.\n>\n> Second, we can make this change relatively quickly. Most of the Segwit\n> testing stays valid and this change can be deployed relatively quickly.\n>\n> Note on SPV clients\n>\n> Currently Segwit stores the witness commitment in the Coinbase tx, so\n> lightweight clients will need to get the Coinbase tx + Merkle proof to\n> validate segwit transactions anyway. Putting block version information in\n> the Coinbase tx will not impose an extra burden on upgraded light clients.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170407/8f71b5b3/attachment.html>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2017-04-08T14:59:12",
                "message_text_only": "I think it might be important that the mandatory commitment expire as in \nGreg's proposal - when we do eventually hardfork, it will be simpler to do in \na safe manner if such a commitment in the fake \"old block\" is not required.\n\nI don't like your proposal because it allows ASICBoost. ASICBoost effectively \nmakes SHA2 semi-ASIC-resistant. ASIC-resistance raises the barrier of entry to \nnew mining chip manufacturers, and gives a larger advantage to the miners able \nto make use of it. Instead, IMO we should fix the vulnerability exploited by \nASICBoost entirely to keep SHA2 as ASIC-friendly as possible - or change the \nPoW to an algorithm that is more ASIC-friendly.\n\nThat being said, I don't think I would oppose the proposal if it gained \nnotably better support than Segwit currently has (as yet another compromise), \nand the above concerns were addressed (eg, Bitfury and Canaan state they can \ncompete using ASICBoost and the patents are licensed freely to everyone).\n\nLuke\n\n\nOn Saturday, April 08, 2017 12:05:16 AM Jimmy Song via bitcoin-dev wrote:\n> I've gotten feedback from Adam Back that you actually don't need all 32\n> bits in the header for overt ASICBoost, so I'm modifying my proposal. Of\n> the 32-bit version field, bits 16 to 23 are reserved for miners, the\n> witness commitment stays as defined in BIP-141 except that it's now\n> required. BIP9 then is modified so that bits 16 to 23 are now no longer\n> usable.\n> \n> On Fri, Apr 7, 2017 at 3:06 PM, Jimmy Song <jaejoon at gmail.com> wrote:\n> > Hey everyone, This is an idea that I had about Segwit and Gregory's\n> > proposal from yesterday that I wanted to run by everyone on this list.\n> > I'm not at all sure what this would mean for non-upgraded nodes on the\n> > network and would like feedback on that. This is not a formal BIP as\n> > it's a modification to a previously submitted one, but I'm happy to\n> > formalize it if it would help.\n> > ----------------------------------------\n> > MotivationOne of the interesting aspects of Gregory Maxwell\u2019s proposal is\n> > that it only precludes the covert version of ASICBoost. He specifically\n> > left the overt version alone.\n> > \n> > Overt ASICBoost requires grinding on the version bits of the Block header\n> > instead of the Merkle Root. This is likely more efficient than the Merkle\n> > Root grinding (aka covert ASICBoost) and requires way less resources\n> > (much less RAM, SHA256 calculations, no tx shuffling, etc).\n> > \n> > If we combine Gregory Maxwell\u2019s proposal with BIP-141 (Segwit) and add a\n> > slight modification, this should, in theory, make ASICBoost a lot more\n> > useful to miners and appeal to their financial interests.\n> > The Modification\n> > \n> > Currently, the version bits (currently 4 bytes, or 32 bits) in the header\n> > are used for BIP9 signaling. We change the version bits to a nonce-space\n> > so the miners can use it for overt ASICBoost. The 32-bits are now moved\n> > over to the Coinbase transaction as part of the witness commitment. The\n> > witness commitment goes from 38 bytes to 42 bytes, with the last 4 bytes\n> > being used as the version bits in the block header previously. The\n> > witness commitment becomes required as per Gregory Maxwell\u2019s proposal.\n> > Reasoning\n> > \n> > First, this brings ASICBoost out into the open. Covert ASICBoost becomes\n> > much more costly and overt ASICBoost is now encouraged.\n> > \n> > Second, we can make this change relatively quickly. Most of the Segwit\n> > testing stays valid and this change can be deployed relatively quickly.\n> > \n> > Note on SPV clients\n> > \n> > Currently Segwit stores the witness commitment in the Coinbase tx, so\n> > lightweight clients will need to get the Coinbase tx + Merkle proof to\n> > validate segwit transactions anyway. Putting block version information in\n> > the Coinbase tx will not impose an extra burden on upgraded light\n> > clients."
            },
            {
                "author": "Jimmy Song",
                "date": "2017-04-08T15:17:47",
                "message_text_only": ">\n> I think it might be important that the mandatory commitment expire as in\n> Greg's proposal - when we do eventually hardfork, it will be simpler to do\n> in\n> a safe manner if such a commitment in the fake \"old block\" is not required.\n>\n\nOK, that makes sense. I'll modify my proposal this way:\n\nBeginning block X and until block Y the coinbase transaction of\neach block MUST contain a BIP-141 segwit commitment\n\n\n> I don't like your proposal because it allows ASICBoost. ASICBoost\n> effectively\n> makes SHA2 semi-ASIC-resistant. ASIC-resistance raises the barrier of\n> entry to\n> new mining chip manufacturers, and gives a larger advantage to the miners\n> able\n> to make use of it. Instead, IMO we should fix the vulnerability exploited\n> by\n> ASICBoost entirely to keep SHA2 as ASIC-friendly as possible - or change\n> the\n> PoW to an algorithm that is more ASIC-friendly.\n>\n\nOvert ASICBoost is allowed on the network already. Until a proposal\nexplicitly blocking overt ASICBoost as a soft fork is activated, this seems\nto be better than the current state which is that overt ASICBoost is\nallowed, but at a cost to BIP9 signals.\n\nJimmy\n\n\n> That being said, I don't think I would oppose the proposal if it gained\n> notably better support than Segwit currently has (as yet another\n> compromise),\n> and the above concerns were addressed (eg, Bitfury and Canaan state they\n> can\n> compete using ASICBoost and the patents are licensed freely to everyone).\n>\n> Luke\n>\n>\n> On Saturday, April 08, 2017 12:05:16 AM Jimmy Song via bitcoin-dev wrote:\n> > I've gotten feedback from Adam Back that you actually don't need all 32\n> > bits in the header for overt ASICBoost, so I'm modifying my proposal. Of\n> > the 32-bit version field, bits 16 to 23 are reserved for miners, the\n> > witness commitment stays as defined in BIP-141 except that it's now\n> > required. BIP9 then is modified so that bits 16 to 23 are now no longer\n> > usable.\n> >\n> > On Fri, Apr 7, 2017 at 3:06 PM, Jimmy Song <jaejoon at gmail.com> wrote:\n> > > Hey everyone, This is an idea that I had about Segwit and Gregory's\n> > > proposal from yesterday that I wanted to run by everyone on this list.\n> > > I'm not at all sure what this would mean for non-upgraded nodes on the\n> > > network and would like feedback on that. This is not a formal BIP as\n> > > it's a modification to a previously submitted one, but I'm happy to\n> > > formalize it if it would help.\n> > > ----------------------------------------\n> > > MotivationOne of the interesting aspects of Gregory Maxwell\u2019s proposal\n> is\n> > > that it only precludes the covert version of ASICBoost. He specifically\n> > > left the overt version alone.\n> > >\n> > > Overt ASICBoost requires grinding on the version bits of the Block\n> header\n> > > instead of the Merkle Root. This is likely more efficient than the\n> Merkle\n> > > Root grinding (aka covert ASICBoost) and requires way less resources\n> > > (much less RAM, SHA256 calculations, no tx shuffling, etc).\n> > >\n> > > If we combine Gregory Maxwell\u2019s proposal with BIP-141 (Segwit) and add\n> a\n> > > slight modification, this should, in theory, make ASICBoost a lot more\n> > > useful to miners and appeal to their financial interests.\n> > > The Modification\n> > >\n> > > Currently, the version bits (currently 4 bytes, or 32 bits) in the\n> header\n> > > are used for BIP9 signaling. We change the version bits to a\n> nonce-space\n> > > so the miners can use it for overt ASICBoost. The 32-bits are now moved\n> > > over to the Coinbase transaction as part of the witness commitment. The\n> > > witness commitment goes from 38 bytes to 42 bytes, with the last 4\n> bytes\n> > > being used as the version bits in the block header previously. The\n> > > witness commitment becomes required as per Gregory Maxwell\u2019s proposal.\n> > > Reasoning\n> > >\n> > > First, this brings ASICBoost out into the open. Covert ASICBoost\n> becomes\n> > > much more costly and overt ASICBoost is now encouraged.\n> > >\n> > > Second, we can make this change relatively quickly. Most of the Segwit\n> > > testing stays valid and this change can be deployed relatively quickly.\n> > >\n> > > Note on SPV clients\n> > >\n> > > Currently Segwit stores the witness commitment in the Coinbase tx, so\n> > > lightweight clients will need to get the Coinbase tx + Merkle proof to\n> > > validate segwit transactions anyway. Putting block version information\n> in\n> > > the Coinbase tx will not impose an extra burden on upgraded light\n> > > clients.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170408/0b4e08ad/attachment.html>"
            },
            {
                "author": "Timo Hanke",
                "date": "2017-04-08T16:19:01",
                "message_text_only": "Yes, you only need a few bits in the version number, probably less than 8.\n\nIf you encourage the overt method of using AsicBoost I would argue that you\nno longer need to dis-encourage the couvert method anymore as in Greg's\nproposal. Nobody would use the couvert method anyway because the overt\nmethod is so much simpler. So maybe the proposals can be completely\ndisentangled?\n\n\nOn Fri, Apr 7, 2017 at 5:05 PM, Jimmy Song via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> I've gotten feedback from Adam Back that you actually don't need all 32\n> bits in the header for overt ASICBoost, so I'm modifying my proposal. Of\n> the 32-bit version field, bits 16 to 23 are reserved for miners, the\n> witness commitment stays as defined in BIP-141 except that it's now\n> required. BIP9 then is modified so that bits 16 to 23 are now no longer\n> usable.\n>\n> On Fri, Apr 7, 2017 at 3:06 PM, Jimmy Song <jaejoon at gmail.com> wrote:\n>\n>> Hey everyone, This is an idea that I had about Segwit and Gregory's\n>> proposal from yesterday that I wanted to run by everyone on this list. I'm\n>> not at all sure what this would mean for non-upgraded nodes on the network\n>> and would like feedback on that. This is not a formal BIP as it's a\n>> modification to a previously submitted one, but I'm happy to formalize it\n>> if it would help.\n>> ----------------------------------------\n>> MotivationOne of the interesting aspects of Gregory Maxwell\u2019s proposal\n>> is that it only precludes the covert version of ASICBoost. He\n>> specifically left the overt version alone.\n>>\n>> Overt ASICBoost requires grinding on the version bits of the Block\n>> header instead of the Merkle Root. This is likely more efficient than the\n>> Merkle Root grinding (aka covert ASICBoost) and requires way less\n>> resources (much less RAM, SHA256 calculations, no tx shuffling, etc).\n>>\n>> If we combine Gregory Maxwell\u2019s proposal with BIP-141 (Segwit) and add a\n>> slight modification, this should, in theory, make ASICBoost a lot more\n>> useful to miners and appeal to their financial interests.\n>> The Modification\n>>\n>> Currently, the version bits (currently 4 bytes, or 32 bits) in the header\n>> are used for BIP9 signaling. We change the version bits to a nonce-space so\n>> the miners can use it for overt ASICBoost. The 32-bits are now moved\n>> over to the Coinbase transaction as part of the witness commitment. The\n>> witness commitment goes from 38 bytes to 42 bytes, with the last 4 bytes\n>> being used as the version bits in the block header previously. The witness\n>> commitment becomes required as per Gregory Maxwell\u2019s proposal.\n>> Reasoning\n>>\n>> First, this brings ASICBoost out into the open. Covert ASICBoost becomes\n>> much more costly and overt ASICBoost is now encouraged.\n>>\n>> Second, we can make this change relatively quickly. Most of the Segwit\n>> testing stays valid and this change can be deployed relatively quickly.\n>>\n>> Note on SPV clients\n>>\n>> Currently Segwit stores the witness commitment in the Coinbase tx, so\n>> lightweight clients will need to get the Coinbase tx + Merkle proof to\n>> validate segwit transactions anyway. Putting block version information in\n>> the Coinbase tx will not impose an extra burden on upgraded light clients.\n>>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170408/47873dc2/attachment-0001.html>"
            },
            {
                "author": "praxeology_guy",
                "date": "2017-04-08T01:48:01",
                "message_text_only": "Jimmy Song,\n\nWhy would the actual end users of Bitcoin (the long term and short term owners of bitcoins) who run fully verifying nodes want to change Bitcoin policy in order to make their money more vulnerable to 51% attack?\n\nIf anything, we would be making policy changes to prevent the use of patented PoW algorithms instead of making changes to enable them.\n\nThanks,\nPraxeology Guy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170407/94f12622/attachment.html>"
            },
            {
                "author": "Jimmy Song",
                "date": "2017-04-08T02:46:29",
                "message_text_only": "Praxeology Guy,\n\nWhy would the actual end users of Bitcoin (the long term and short term\n> owners of bitcoins) who run fully verifying nodes want to change Bitcoin\n> policy in order to make their money more vulnerable to 51% attack?\n>\n\nCertainly, if only one company made use of the extra nonce space, they\nwould have an advantage. But think of it this way, if some newer ASIC\noptimization comes up, would you rather have a non-ASICBoosted hash rate to\ndefend with or an ASICBoosted hash rate? Certainly, the latter, being\nhigher will secure the Bitcoin network better against newer optimizations.\n\n\n> If anything, we would be making policy changes to prevent the use of\n> patented PoW algorithms instead of making changes to enable them.\n>\n\nIs that patented in any jurisdiction, all jurisdictions or only certain\njurisdictions? Would a patent granted for SHA256 in Swaziland be sufficient\nfor Bitcoin to change the Proof of Work algorithm? This is a very\nsubjective judgment based on quasi-legality and I don't think that's a road\nthat Bitcoin should go down.\n\nCertainly, it would be better if the patent for ASICBoost were\nopen-sourced, but the legality of such-and-such thing in such-and-such\njurisdiction should not affect Bitcoin policy as that in itself introduces\nsignificant risk to the network. A sufficiently authoritarian government\ncan then grant a monopoly for various algorithms in their country and\nnegatively impact Bitcoin.\n\nIndeed, there are already many individuals that disobey the laws of their\ncountry to help the Bitcoin network run. I would expect the same with\npatents. Should there come a time when a patent or some other legal\nmaneuvering gives one network actor a large advantage to the detriment of\nthe network, I believe that Bitcoin will handle that in the specific case.\n\nIn the meantime, I believe such changes increase the odds of Segwit\nactually being accepted and activated as per BIP-141.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170407/fd84d18b/attachment-0001.html>"
            },
            {
                "author": "Pavel Moravec",
                "date": "2017-04-08T08:33:01",
                "message_text_only": "> Second, we can make this change relatively quickly. Most of the Segwit testing stays valid and this change can be deployed relatively quickly.\n\nIt is true only for nodes software. Most of the world's mining\ninfrastructure (at least for pool mining) is not ready for such\nchange. Current version of Stratum protocol doesn't support block\nversion changing. A broad adoption would require:\n\n- A new standard extension to the mining protocol (generally, we want\nthe hash rate to be free to change the used pool without efficiency\nloss)\n- Pool operators must change their software.\n- All miners must update their firmware IF they have compatible\nhardware (we know there is compatible hardware out there but\ndefinitely not all of the currently used). The firmware can be changed\nafter the mining protocol extension is settled.\n\nUntil all miners update (firmware or hardware), the change encourages\nlarge difference in mining efficiency. And IMO it gives another\nadvantage to large mining operations in general.\n\n> But think of it this way, if some newer ASIC optimization comes up, would you rather have a non-ASICBoosted hash rate to defend with or an ASICBoosted hash rate? Certainly, the latter, being higher will secure the Bitcoin network better against newer optimizations.\n\nYou make a strong assumption that the new optimization is not\ncompatible with overt ASICBoost. If it is compatible, ASICBoost\ndoesn't help you with \"defending against\" the new optimization at all.\nAnd it can be the case that the new optimization is based on ASICBoost\nso you can make the situation \"worse\" by allowing it.\n\n> Certainly, if only one company made use of the extra nonce space, they would have an advantage.\n\nCan you explain why the reality should be significantly different? In\nsufficiently near future.\n\n> Is that patented in any jurisdiction, all jurisdictions or only certain jurisdictions? Would a patent granted for SHA256 in Swaziland be sufficient for Bitcoin to change the Proof of Work algorithm?\n\nWe don't have to deal with any such theoretical situation now. You\nproposal goes in opposite direction, by adding support for patented\nalgorithm. I don't know myself what the possible legal implications\nare (maybe only for a subset of miners) so I consider it as an\nunnecessary risk. At least before some conclusive legal analysis says\ndifferently."
            },
            {
                "author": "Jimmy Song",
                "date": "2017-04-08T14:35:30",
                "message_text_only": "Pavel,\n\nUntil all miners update (firmware or hardware), the change encourages\n> large difference in mining efficiency. And IMO it gives another\n> advantage to large mining operations in general.\n>\n\nCertainly, there would have to be changes for stratum, pool software, etc.\nBut the monetary incentives align to all the changes needed.\n\nRemember, overt ASICBoost can get something like a 12.5% efficiency boost\nfrom toggling a single bit in the version (equivalent to 2 colliding work\nitems), 18.5% from 2 bits (equivalent to 4 colliding work items), 23.4%\nfrom 4 bits (see https://arxiv.org/ftp/arxiv/papers/1604/1604.00575.pdf).\nIn lieu of an explicit allowance of overt ASICBoost, the monetary\nincentives lead to odd BIP9 signaling, especially if 4 or more proposals\nsignal at once. There really isn't a practical way to block overt ASICBoost\nwithout forcing the version bits to be some value.\n\nIn other words, the question isn't about allowing/disallowing ASICBoost at\nthis point. The question is whether we want ASICBoost open or hidden.\n\n\n> You make a strong assumption that the new optimization is not\n> compatible with overt ASICBoost. If it is compatible, ASICBoost\n> doesn't help you with \"defending against\" the new optimization at all.\n> And it can be the case that the new optimization is based on ASICBoost\n> so you can make the situation \"worse\" by allowing it.\n>\n\nThis would only be the case if overt ASICBoost were not possible at all. It\nis currently possible to use overt ASICBoost, so optimizations based on\novert ASICBoost would also be possible unless something were done to\nactively block it.\n\n> Certainly, if only one company made use of the extra nonce space, they\n> would have an advantage.\n>\n> Can you explain why the reality should be significantly different? In\n> sufficiently near future.\n\n\nMarket incentives, I would imagine. How quickly that would be is not\nsomething I'm qualified to answer.\n\n\n> We don't have to deal with any such theoretical situation now. You\n> proposal goes in opposite direction, by adding support for patented\n> algorithm. I don't know myself what the possible legal implications\n> are (maybe only for a subset of miners) so I consider it as an\n> unnecessary risk. At least before some conclusive legal analysis says\n> differently.\n>\n\nI'm not adding support as much as explicitly allowing what's implicitly\nallowed. Whatever risks you imagine for this proposal exist on the network\ncurrently, with unmodified BIP-141 and with modified BIP-141. The\ndifference in adding the modification is that overt ASICBoost is explicitly\nallowed in the modified BIP-141 as to not hide it.\n\nJimmy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170408/14268b35/attachment.html>"
            },
            {
                "author": "Pavel Moravec",
                "date": "2017-04-08T16:38:03",
                "message_text_only": "Jimmy,\n\n>> Until all miners update (firmware or hardware), the change encourages\n>> large difference in mining efficiency. And IMO it gives another\n>> advantage to large mining operations in general.\n>\n> Certainly, there would have to be changes for stratum, pool software, etc.\n> But the monetary incentives align to all the changes needed.\n\nI agree. I only wanted to make clear, that the impact would be\nsignificant. Lot of parties would be involved with nonequivalent\nstarting positions.\n\n> Remember, overt ASICBoost can get something like a 12.5% efficiency boost\n> from toggling a single bit in the version (equivalent to 2 colliding work\n> items), 18.5% from 2 bits (equivalent to 4 colliding work items), 23.4% from\n> 4 bits (see https://arxiv.org/ftp/arxiv/papers/1604/1604.00575.pdf). In lieu\n> of an explicit allowance of overt ASICBoost, the monetary incentives lead to\n> odd BIP9 signaling, especially if 4 or more proposals signal at once. There\n> really isn't a practical way to block overt ASICBoost without forcing the\n> version bits to be some value.\n\nYou can e.g. place the version number into a coinbase, similarly to\nblock height. Then, it is the same (number of operations) as modifying\nthe coinbase directly.\n\nA cost of version in coinbase is 4B per block, sure, but it allows to\nsave all bits for \"more useful\" purposes. Either for BIP9 signalling\nor other future purposes I cannot see now. And it removes an incentive\nto mess with version bits.\n\nMining empty blocks and finding collisions by toggling bits there can\nbe prevented as well.\n\n> In other words, the question isn't about allowing/disallowing ASICBoost at\n> this point. The question is whether we want ASICBoost open or hidden.\n\nI think the ASICBoost can and should be prevented completely.\n\n\nPavel"
            },
            {
                "author": "Jimmy Song",
                "date": "2017-04-08T22:19:11",
                "message_text_only": "Pavel,\n\n\n> I agree. I only wanted to make clear, that the impact would be\n> significant. Lot of parties would be involved with nonequivalent\n> starting positions.\n>\n>\nI agree with you. I believe nonequivalent starting positions are the norm\nin mining, not the exception and hence don't believe this to be a problem.\n\n\n>\n> I think the ASICBoost can and should be prevented completely.\n>\n\nIt certainly can be and from the responses I'm getting, I believe there\nwould be at least a few people that would enthusiastically support a BIP to\ndo that. That is, however, a separate issue than my proposal. My proposal\naims to bring ASICBoost out into the open *while it is still possible*. A\nBIP to prevent ASICBoost completely is in that sense compatible.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170408/06fbf04c/attachment.html>"
            },
            {
                "author": "praxeology_guy",
                "date": "2017-04-08T18:15:43",
                "message_text_only": "ASICBOOST causes Bitcoin's PoW to become more memory/latency throttled instead of raw computation throttled.\n\nThere is the equation:\nPower Cost + Captial Rent + Labor ~= block reward + fees\n\nCapital Rent is a barrier to entry, and hence in desiring a more distributed system, we would like to minimize the Capital Rent portion of the equation.\n\nResolving memory/latency throttle requires a greater Captial Rent than raw computation throttle.\n\nHence (agreeing with Luke), ASICBOOST is not desirable, even if it wasn't a government enforced monopoly on mining.\n\nPlease let me know if I made a mistake.\n\nThanks,\nPraxeology Guy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170408/57669e84/attachment.html>"
            },
            {
                "author": "Eric Voskuil",
                "date": "2017-04-08T18:51:32",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\n\nOn 04/08/2017 11:15 AM, praxeology_guy via bitcoin-dev wrote:\n> ASICBOOST causes Bitcoin's PoW to become more memory/latency\n> throttled instead of raw computation throttled.\n> \n> There is the equation: Power Cost + Captial Rent + Labor ~= block\n> reward + fees\n> \n> Capital Rent is a barrier to entry, and hence in desiring a more \n> distributed system, we would like to minimize the Capital Rent\n> portion of the equation.\n> \n> Resolving memory/latency throttle requires a greater Captial Rent\n> than raw computation throttle.\n> \n> Hence (agreeing with Luke), ASICBOOST is not desirable, even if it \n> wasn't a government enforced monopoly on mining.\n> \n> Please let me know if I made a mistake.\n\nElectric power is not an abstraction, it's the output of machines.\nWhat you are referring to as Power Cost typically consists of a higher\nrent component than computing hardware, where rent is the sharing of a\nresource by multiple people. So by your reasoning you appear to have\ndrawn the wrong conclusion.\n\ne\n-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v2.0.22 (GNU/Linux)\n\niQEcBAEBCAAGBQJY6TEUAAoJEDzYwH8LXOFOiQIH/RN8YhLCokZtGoFZ+dOgwCxc\n/ej3m9CXVGyWvcCJMQd2ZJFgpjL5mgJdcCdaWoTeZfh0Nmvc3hDex46wWpUZc/mR\nNbRj56hyqe+cWAwQJJpAOWiJXjEuS3npXFvZIBpslECXCL6U+LSxdW9WSg0w+HBD\njihIlG2TeGSrMR/atKfSnVRAnz9ahPvgUwcR8l7oLsjP2JvBGl+fQHL5MwpvRg4a\nsXK3eMIeH7wGJyiKOwXyMeRMfCRlwpkBCw0R+FYt2Q5l/uwkRAKuJiYUlJixFAZA\nggQth02pFn/tASB49oBKZU3QviVRGgoIQ5DFyI8OPa10FeVsxaeNeBQylwWJA3c=\n=Ryo5\n-----END PGP SIGNATURE-----"
            },
            {
                "author": "praxeology_guy",
                "date": "2017-04-08T20:38:43",
                "message_text_only": "Eric Voskuil,\n\nTL;DR: Electrical power is a general purpose consumer good vs PoW mining equipment is a single purpose consumer good. Hence the mining equipment rent is the barrier to entry, given if you invest in power generation capital you could use the power for a different purpose.\n\nEach unit of electrical power (1 V* A = 1 Watt) is a finite unit of a highly non-durable consumable good.\n\nIt is true that electrical power is created by utilizing capital equipment, and the capital rent + labor of generating such power is the basis for the \"Power Cost\" component of the ideal miner competition profit equation.\n\nBut... electrical power is a general consumer good that can be used for many things, so investing in the capital to create it is not a very risky endeavor.\n\nOn the other hand, Bitcoin mining equipment capital is an EXTEREMELY specific kind of capital that only has exactly one use: efficiently/competitively mining a coin that has a particular PoW algorithm. Hence investing in bitcoin mining equipment is a more risky endeavor than power generation capital. Such a risk is a barrier to entry, and it is the barrier that is most considered when an entity considers mining Bitcoins.\n\nMature Arithmetic Logic Unit (ALU) bound PoW algorithms lacking new attacks (cryptographic definition) can only be out-dated by more efficient, more general purpose (less specific case proprietary) transistor fabrication technology.\n\nMemory Latency bound PoW algorithms lacking new attacks (cryptographic definition) have the risk of being encumbered by all sorts of physical hardware patent inventions. This is because latency has significantly more room for such specific-to-PoW non-general purpose inventions... beyond additional patents relating to memory technology on top of ALU patents. Patents, I should point out, either cause the price of capital equipment to increase or enforce a monopoly on the capital... neither of which are desirable.\n\nThe capital maturity outlook of memory latency bound algorithms is also significantly worse than ALU bound... due to all of the expected future patent-able optimizations that could improve memory latency. Hence investing in memory latency bound mining equipment is even riskier because of the likeliness of a new patented optimization making your capital non-competitive, and given its specific nature, worthless.\n\nThis discussion brings me to a new insight. We have said that some places have \"cheaper\" power than others, due to the non-durable nature of electrical power. With the existence of Bitcoin, given other cost factors being less significant, Bitcoin causes all sources of power everywhere to be more equal in price at a particular time.\n\nNow you might argue that memory latency bound PoW algorithms result in the mining capital component being the larger component than the electricity component being a good thing because: then mining would be less local to otherwise untapped (cheap) power sources. The problem with this is that as the mining capital matures (as all the optimizations are found, and the patents run out), we go strait back to the power cost being the largest component... and we had to suffer all the years of various entities unpredictably attaining a monopoly on mining in order to get there.\n\nPlease let me know if I made a mistake.\n\nThanks,\nPraxeology Guy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170408/f2ddc5bf/attachment-0001.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2017-04-09T11:46:22",
                "message_text_only": "On 8 Apr 2017 8:31 pm, \"praxeology_guy via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\nThere is the equation:\nPower Cost + Captial Rent + Labor ~= block reward + fees\n\n\nI don't know why many people insist on calling the subsidy the blick\nreward. Thw block reward is both the block subsidy plus the block fees.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170409/72862f71/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2017-04-08T16:27:48",
                "message_text_only": "On 8 Apr 2017 5:06 am, \"Jimmy Song via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\nPraxeology Guy,\n\nWhy would the actual end users of Bitcoin (the long term and short term\n> owners of bitcoins) who run fully verifying nodes want to change Bitcoin\n> policy in order to make their money more vulnerable to 51% attack?\n>\n\nCertainly, if only one company made use of the extra nonce space, they\nwould have an advantage. But think of it this way, if some newer ASIC\noptimization comes up, would you rather have a non-ASICBoosted hash rate to\ndefend with or an ASICBoosted hash rate? Certainly, the latter, being\nhigher will secure the Bitcoin network better against newer optimizations.\n\n\nWhy?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170408/9bce75cb/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2017-04-08T17:22:22",
                "message_text_only": "To be more specific, why \"being higher will secure the Bitcoin network\nbetter against newer optimizations\"?\nOr, to be more clear, let's forget about future \"optimizations\", let's\njust think of an attacker. Does asicboost being used by all miners\nmake the system more secure against an attacker? No, for the attacker\ncan use asicboost too.\nWhat about the case when not all the miners are using asicboost? Then\nthe attacker can actually get an advantage by suing asicboost.\n\nSometimes people compare asicboost with the use of asics in general as\nboth providing more security for the network and users. But I don't\nthink this is accurate. The existence of sha256d asics makes an attack\nwith general purpose computing hardware (or even more specialized\narchitectures like gpgpu) much more expensive and unlikely. As an\nalternative the attacker can spend additional resources investing in\nasics himself (again, making many attacks more expensive and\nunlikely).\n\nBut as far as I know, asicboost can be implemented with software\nrunning on general purpose hardware that integrates with regular\nsha256d asics. There is probably an advantage on having the asicboost\nimplementation \"in the same box\" as the sha256d, yet again the\nattacker can invest in hardware with the competitive advantage from\nhaving asicboost more intergrated with the sha256d asics too.\n\nTo reiterate, whether all miners use asicboost or only a subset of\nthem, I remain unconvinced that provides any additional security to\nthe network (to be more precise whether that makes \"tx history harder\nto rewrite\"), even if it results on the hashrate charts looking \"more\nsecure\".\n\n\nOn Sat, Apr 8, 2017 at 6:27 PM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n>\n>\n> On 8 Apr 2017 5:06 am, \"Jimmy Song via bitcoin-dev\"\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> Praxeology Guy,\n>\n>> Why would the actual end users of Bitcoin (the long term and short term\n>> owners of bitcoins) who run fully verifying nodes want to change Bitcoin\n>> policy in order to make their money more vulnerable to 51% attack?\n>\n>\n> Certainly, if only one company made use of the extra nonce space, they would\n> have an advantage. But think of it this way, if some newer ASIC optimization\n> comes up, would you rather have a non-ASICBoosted hash rate to defend with\n> or an ASICBoosted hash rate? Certainly, the latter, being higher will secure\n> the Bitcoin network better against newer optimizations.\n>\n>\n> Why?"
            },
            {
                "author": "Jimmy Song",
                "date": "2017-04-08T22:26:25",
                "message_text_only": "Jorge,\n\nSuppose someone figures out an ASIC optimization that's completely\nunrelated that gives X% speed boost over your non-ASICBoosted\nimplementation. If you ban ASICBoost, someone with this optimization can\nget 51% of the network by adding N machines with their new optimization. If\nyou allow ASICBoost and assuming this gets a 20% speed boost over\nnon-ASICBoosted hardware, someone with this optimization would need 1.2N\nmachines to get 51%. The network in that sense is 20% stronger against this\nattack in terms of cost.\n\nJimmy\n\nOn Sat, Apr 8, 2017 at 12:22 PM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n\n> To be more specific, why \"being higher will secure the Bitcoin network\n> better against newer optimizations\"?\n> Or, to be more clear, let's forget about future \"optimizations\", let's\n> just think of an attacker. Does asicboost being used by all miners\n> make the system more secure against an attacker? No, for the attacker\n> can use asicboost too.\n> What about the case when not all the miners are using asicboost? Then\n> the attacker can actually get an advantage by suing asicboost.\n>\n> Sometimes people compare asicboost with the use of asics in general as\n> both providing more security for the network and users. But I don't\n> think this is accurate. The existence of sha256d asics makes an attack\n> with general purpose computing hardware (or even more specialized\n> architectures like gpgpu) much more expensive and unlikely. As an\n> alternative the attacker can spend additional resources investing in\n> asics himself (again, making many attacks more expensive and\n> unlikely).\n>\n> But as far as I know, asicboost can be implemented with software\n> running on general purpose hardware that integrates with regular\n> sha256d asics. There is probably an advantage on having the asicboost\n> implementation \"in the same box\" as the sha256d, yet again the\n> attacker can invest in hardware with the competitive advantage from\n> having asicboost more intergrated with the sha256d asics too.\n>\n> To reiterate, whether all miners use asicboost or only a subset of\n> them, I remain unconvinced that provides any additional security to\n> the network (to be more precise whether that makes \"tx history harder\n> to rewrite\"), even if it results on the hashrate charts looking \"more\n> secure\".\n>\n>\n> On Sat, Apr 8, 2017 at 6:27 PM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n> >\n> >\n> > On 8 Apr 2017 5:06 am, \"Jimmy Song via bitcoin-dev\"\n> > <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >\n> > Praxeology Guy,\n> >\n> >> Why would the actual end users of Bitcoin (the long term and short term\n> >> owners of bitcoins) who run fully verifying nodes want to change Bitcoin\n> >> policy in order to make their money more vulnerable to 51% attack?\n> >\n> >\n> > Certainly, if only one company made use of the extra nonce space, they\n> would\n> > have an advantage. But think of it this way, if some newer ASIC\n> optimization\n> > comes up, would you rather have a non-ASICBoosted hash rate to defend\n> with\n> > or an ASICBoosted hash rate? Certainly, the latter, being higher will\n> secure\n> > the Bitcoin network better against newer optimizations.\n> >\n> >\n> > Why?\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170408/9e9e1ff3/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2017-04-09T11:48:27",
                "message_text_only": "Why won't the attacker use asicboost too? (Please don't say because of\npatents)\n\nOn 9 Apr 2017 12:26 am, \"Jimmy Song\" <jaejoon at gmail.com> wrote:\n\n> Jorge,\n>\n> Suppose someone figures out an ASIC optimization that's completely\n> unrelated that gives X% speed boost over your non-ASICBoosted\n> implementation. If you ban ASICBoost, someone with this optimization can\n> get 51% of the network by adding N machines with their new optimization. If\n> you allow ASICBoost and assuming this gets a 20% speed boost over\n> non-ASICBoosted hardware, someone with this optimization would need 1.2N\n> machines to get 51%. The network in that sense is 20% stronger against this\n> attack in terms of cost.\n>\n> Jimmy\n>\n> On Sat, Apr 8, 2017 at 12:22 PM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n>\n>> To be more specific, why \"being higher will secure the Bitcoin network\n>> better against newer optimizations\"?\n>> Or, to be more clear, let's forget about future \"optimizations\", let's\n>> just think of an attacker. Does asicboost being used by all miners\n>> make the system more secure against an attacker? No, for the attacker\n>> can use asicboost too.\n>> What about the case when not all the miners are using asicboost? Then\n>> the attacker can actually get an advantage by suing asicboost.\n>>\n>> Sometimes people compare asicboost with the use of asics in general as\n>> both providing more security for the network and users. But I don't\n>> think this is accurate. The existence of sha256d asics makes an attack\n>> with general purpose computing hardware (or even more specialized\n>> architectures like gpgpu) much more expensive and unlikely. As an\n>> alternative the attacker can spend additional resources investing in\n>> asics himself (again, making many attacks more expensive and\n>> unlikely).\n>>\n>> But as far as I know, asicboost can be implemented with software\n>> running on general purpose hardware that integrates with regular\n>> sha256d asics. There is probably an advantage on having the asicboost\n>> implementation \"in the same box\" as the sha256d, yet again the\n>> attacker can invest in hardware with the competitive advantage from\n>> having asicboost more intergrated with the sha256d asics too.\n>>\n>> To reiterate, whether all miners use asicboost or only a subset of\n>> them, I remain unconvinced that provides any additional security to\n>> the network (to be more precise whether that makes \"tx history harder\n>> to rewrite\"), even if it results on the hashrate charts looking \"more\n>> secure\".\n>>\n>>\n>> On Sat, Apr 8, 2017 at 6:27 PM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n>> >\n>> >\n>> > On 8 Apr 2017 5:06 am, \"Jimmy Song via bitcoin-dev\"\n>> > <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> >\n>> > Praxeology Guy,\n>> >\n>> >> Why would the actual end users of Bitcoin (the long term and short term\n>> >> owners of bitcoins) who run fully verifying nodes want to change\n>> Bitcoin\n>> >> policy in order to make their money more vulnerable to 51% attack?\n>> >\n>> >\n>> > Certainly, if only one company made use of the extra nonce space, they\n>> would\n>> > have an advantage. But think of it this way, if some newer ASIC\n>> optimization\n>> > comes up, would you rather have a non-ASICBoosted hash rate to defend\n>> with\n>> > or an ASICBoosted hash rate? Certainly, the latter, being higher will\n>> secure\n>> > the Bitcoin network better against newer optimizations.\n>> >\n>> >\n>> > Why?\n>>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170409/96cbca19/attachment.html>"
            },
            {
                "author": "Jimmy Song",
                "date": "2017-04-09T14:01:01",
                "message_text_only": "Jorge,\n\nWhy won't the attacker use asicboost too? (Please don't say because of\n> patents)\n>\n>\nWe're assuming the ASIC optimization in my example is incompatible with\nASICBoost. But if the new optimization were compatible with ASICBoost,\nyou're right, the network would be in an equivalent situation whether\nASICBoost was banned or not.\n\nI want to point out again that overt ASICBoost can be used on the network\ntoday. My proposal is to bring ASICBoost usage out into the open vs hiding\nit. Banning ASICBoost via protocol changes is another issue completely.\n\nJimmy\n\n\n> On 9 Apr 2017 12:26 am, \"Jimmy Song\" <jaejoon at gmail.com> wrote:\n>\n>> Jorge,\n>>\n>> Suppose someone figures out an ASIC optimization that's completely\n>> unrelated that gives X% speed boost over your non-ASICBoosted\n>> implementation. If you ban ASICBoost, someone with this optimization can\n>> get 51% of the network by adding N machines with their new optimization. If\n>> you allow ASICBoost and assuming this gets a 20% speed boost over\n>> non-ASICBoosted hardware, someone with this optimization would need 1.2N\n>> machines to get 51%. The network in that sense is 20% stronger against this\n>> attack in terms of cost.\n>>\n>> Jimmy\n>>\n>> On Sat, Apr 8, 2017 at 12:22 PM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n>>\n>>> To be more specific, why \"being higher will secure the Bitcoin network\n>>> better against newer optimizations\"?\n>>> Or, to be more clear, let's forget about future \"optimizations\", let's\n>>> just think of an attacker. Does asicboost being used by all miners\n>>> make the system more secure against an attacker? No, for the attacker\n>>> can use asicboost too.\n>>> What about the case when not all the miners are using asicboost? Then\n>>> the attacker can actually get an advantage by suing asicboost.\n>>>\n>>> Sometimes people compare asicboost with the use of asics in general as\n>>> both providing more security for the network and users. But I don't\n>>> think this is accurate. The existence of sha256d asics makes an attack\n>>> with general purpose computing hardware (or even more specialized\n>>> architectures like gpgpu) much more expensive and unlikely. As an\n>>> alternative the attacker can spend additional resources investing in\n>>> asics himself (again, making many attacks more expensive and\n>>> unlikely).\n>>>\n>>> But as far as I know, asicboost can be implemented with software\n>>> running on general purpose hardware that integrates with regular\n>>> sha256d asics. There is probably an advantage on having the asicboost\n>>> implementation \"in the same box\" as the sha256d, yet again the\n>>> attacker can invest in hardware with the competitive advantage from\n>>> having asicboost more intergrated with the sha256d asics too.\n>>>\n>>> To reiterate, whether all miners use asicboost or only a subset of\n>>> them, I remain unconvinced that provides any additional security to\n>>> the network (to be more precise whether that makes \"tx history harder\n>>> to rewrite\"), even if it results on the hashrate charts looking \"more\n>>> secure\".\n>>>\n>>>\n>>> On Sat, Apr 8, 2017 at 6:27 PM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n>>> >\n>>> >\n>>> > On 8 Apr 2017 5:06 am, \"Jimmy Song via bitcoin-dev\"\n>>> > <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>> >\n>>> > Praxeology Guy,\n>>> >\n>>> >> Why would the actual end users of Bitcoin (the long term and short\n>>> term\n>>> >> owners of bitcoins) who run fully verifying nodes want to change\n>>> Bitcoin\n>>> >> policy in order to make their money more vulnerable to 51% attack?\n>>> >\n>>> >\n>>> > Certainly, if only one company made use of the extra nonce space, they\n>>> would\n>>> > have an advantage. But think of it this way, if some newer ASIC\n>>> optimization\n>>> > comes up, would you rather have a non-ASICBoosted hash rate to defend\n>>> with\n>>> > or an ASICBoosted hash rate? Certainly, the latter, being higher will\n>>> secure\n>>> > the Bitcoin network better against newer optimizations.\n>>> >\n>>> >\n>>> > Why?\n>>>\n>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170409/f13e9088/attachment.html>"
            },
            {
                "author": "Erik Aronesty",
                "date": "2017-04-09T18:44:47",
                "message_text_only": "Curious: I'm not sure why a serious discussion of POW change is not on the\ntable as a part of a longer-term roadmap.\n\nDone right, a ramp down of reliance on SHA-256 and a ramp-up on some of the\nproven, np-complete graph-theoretic or polygon manipulation POW would keep\nBitcoin in commodity hardware and out of the hands of centralized\nmanufacturing for many years.\n\nClearly a level-playing field is critical to keeping centralization from\nbeing a \"defining feature\" of Bitcoin over the long term.   I've heard the\nterm \"level playing field\" bandied about quite a bit.   And it seems to me\nthat the risk of state actor control and botnet attacks is less than\nstate-actor manipulation of specialized manufacturing of \"SHA-256 forever\"\nhardware.   Indeed, the reliance on a fairly simple hash seems less and\nless likely a \"feature\" and more of a baggage.\n\nPerhaps regular, high-consensus POW changes might even be *necessary* as a\npart of good maintenance of cryptocurrency in general.   Killing the\nexisting POW, and using an as-yet undefined, but deployment-bit ready POW\nfield to flip-flop between the current and the \"next one\" every 8 years or\nor so, with a ramp down beginning in the 7th year....  A stub function that\nis guaranteed to fail unless a new consensus POW is selected within 7\nyears.\n\nSomething like that?\n\nHaven't thought about it *that* much, but I think the network would respond\nwell to a well known cutover date.   This would enable rapid-response to\nquantum tech, or some other needed POW switch as well... because the\nmechanisms would be in-place and ready to switch as needed.\n\nLots of people seem to panic over POW changes as \"irresponsible\", but it's\nonly irresponsible if done irresponsibly.\n\n\nOn Fri, Apr 7, 2017 at 9:48 PM, praxeology_guy via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Jimmy Song,\n>\n> Why would the actual end users of Bitcoin (the long term and short term\n> owners of bitcoins) who run fully verifying nodes want to change Bitcoin\n> policy in order to make their money more vulnerable to 51% attack?\n>\n> If anything, we would be making policy changes to prevent the use of\n> patented PoW algorithms instead of making changes to enable them.\n>\n> Thanks,\n> Praxeology Guy\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170409/d885a90e/attachment.html>"
            },
            {
                "author": "Jared Lee Richardson",
                "date": "2017-04-09T21:16:26",
                "message_text_only": "I can speak from personal experience regarding another very prominent\naltcoin that attempted to utilize an asic-resistant proof of work\nalgorithm, it is only a matter of time before the \"asic resistant\"\nalgorithm gets its own Asics.  The more complicated the algorithm, the more\nsecretive the asic technology is developed.  Even without it,\nmulti-megawatt gpu farms have already formed in the areas of the world with\nlow energy costs.  I'd support the goal if I thought it possible, but I\nreally don't think centralization of mining can be prevented.\n\nOn Apr 9, 2017 1:16 PM, \"Erik Aronesty via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Curious: I'm not sure why a serious discussion of POW change is not on the\n> table as a part of a longer-term roadmap.\n>\n> Done right, a ramp down of reliance on SHA-256 and a ramp-up on some of\n> the proven, np-complete graph-theoretic or polygon manipulation POW would\n> keep Bitcoin in commodity hardware and out of the hands of centralized\n> manufacturing for many years.\n>\n> Clearly a level-playing field is critical to keeping centralization from\n> being a \"defining feature\" of Bitcoin over the long term.   I've heard the\n> term \"level playing field\" bandied about quite a bit.   And it seems to me\n> that the risk of state actor control and botnet attacks is less than\n> state-actor manipulation of specialized manufacturing of \"SHA-256 forever\"\n> hardware.   Indeed, the reliance on a fairly simple hash seems less and\n> less likely a \"feature\" and more of a baggage.\n>\n> Perhaps regular, high-consensus POW changes might even be *necessary* as a\n> part of good maintenance of cryptocurrency in general.   Killing the\n> existing POW, and using an as-yet undefined, but deployment-bit ready POW\n> field to flip-flop between the current and the \"next one\" every 8 years or\n> or so, with a ramp down beginning in the 7th year....  A stub function that\n> is guaranteed to fail unless a new consensus POW is selected within 7\n> years.\n>\n> Something like that?\n>\n> Haven't thought about it *that* much, but I think the network would\n> respond well to a well known cutover date.   This would enable\n> rapid-response to quantum tech, or some other needed POW switch as well...\n> because the mechanisms would be in-place and ready to switch as needed.\n>\n> Lots of people seem to panic over POW changes as \"irresponsible\", but it's\n> only irresponsible if done irresponsibly.\n>\n>\n> On Fri, Apr 7, 2017 at 9:48 PM, praxeology_guy via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Jimmy Song,\n>>\n>> Why would the actual end users of Bitcoin (the long term and short term\n>> owners of bitcoins) who run fully verifying nodes want to change Bitcoin\n>> policy in order to make their money more vulnerable to 51% attack?\n>>\n>> If anything, we would be making policy changes to prevent the use of\n>> patented PoW algorithms instead of making changes to enable them.\n>>\n>> Thanks,\n>> Praxeology Guy\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170409/8d1ffe05/attachment-0001.html>"
            },
            {
                "author": "David Vorick",
                "date": "2017-04-09T23:51:29",
                "message_text_only": "On Apr 9, 2017 7:00 PM, \"Jared Lee Richardson via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\nI can speak from personal experience regarding another very prominent\naltcoin that attempted to utilize an asic-resistant proof of work\nalgorithm, it is only a matter of time before the \"asic resistant\"\nalgorithm gets its own Asics.  The more complicated the algorithm, the more\nsecretive the asic technology is developed.  Even without it,\nmulti-megawatt gpu farms have already formed in the areas of the world with\nlow energy costs.  I'd support the goal if I thought it possible, but I\nreally don't think centralization of mining can be prevented.\n\nOn Apr 9, 2017 1:16 PM, \"Erik Aronesty via bitcoin-dev\" <bitcoin-dev at lists.\nlinuxfoundation.org> wrote:\n\n> Curious: I'm not sure why a serious discussion of POW change is not on the\n> table as a part of a longer-term roadmap.\n>\n> Done right, a ramp down of reliance on SHA-256 and a ramp-up on some of\n> the proven, np-complete graph-theoretic or polygon manipulation POW would\n> keep Bitcoin in commodity hardware and out of the hands of centralized\n> manufacturing for many years.\n>\n> Clearly a level-playing field is critical to keeping centralization from\n> being a \"defining feature\" of Bitcoin over the long term.   I've heard the\n> term \"level playing field\" bandied about quite a bit.   And it seems to me\n> that the risk of state actor control and botnet attacks is less than\n> state-actor manipulation of specialized manufacturing of \"SHA-256 forever\"\n> hardware.   Indeed, the reliance on a fairly simple hash seems less and\n> less likely a \"feature\" and more of a baggage.\n>\n> Perhaps regular, high-consensus POW changes might even be *necessary* as a\n> part of good maintenance of cryptocurrency in general.   Killing the\n> existing POW, and using an as-yet undefined, but deployment-bit ready POW\n> field to flip-flop between the current and the \"next one\" every 8 years or\n> or so, with a ramp down beginning in the 7th year....  A stub function that\n> is guaranteed to fail unless a new consensus POW is selected within 7\n> years.\n>\n> Something like that?\n>\n> Haven't thought about it *that* much, but I think the network would\n> respond well to a well known cutover date.   This would enable\n> rapid-response to quantum tech, or some other needed POW switch as well...\n> because the mechanisms would be in-place and ready to switch as needed.\n>\n> Lots of people seem to panic over POW changes as \"irresponsible\", but it's\n> only irresponsible if done irresponsibly.\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n\nThe real bottleneck today is the amount of capex required to achieve\noptimal mining. I am strongly in favor of PoW research that investigates\nbetter PoW, but I do not think that any obvious strategies are known yet to\nimprove substantially on computation heavy hashcash.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170409/76d8a516/attachment.html>"
            },
            {
                "author": "Erik Aronesty",
                "date": "2017-04-10T00:20:49",
                "message_text_only": "Have you read the cuckoo cycle paper?  Finding cycles in massive graphs is\njust about the worst thing to use an ASIC for.\n\nIt might be a hitherto before unknown emergent property of cryptocurrencies\nin general that POW *must* change every 7-9 years.  Could bake that into\nthe protocol too...\n\nOn Apr 9, 2017 7:51 PM, \"David Vorick\" <david.vorick at gmail.com> wrote:\n\n>\n>\n> On Apr 9, 2017 7:00 PM, \"Jared Lee Richardson via bitcoin-dev\" <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> I can speak from personal experience regarding another very prominent\n> altcoin that attempted to utilize an asic-resistant proof of work\n> algorithm, it is only a matter of time before the \"asic resistant\"\n> algorithm gets its own Asics.  The more complicated the algorithm, the more\n> secretive the asic technology is developed.  Even without it,\n> multi-megawatt gpu farms have already formed in the areas of the world with\n> low energy costs.  I'd support the goal if I thought it possible, but I\n> really don't think centralization of mining can be prevented.\n>\n> On Apr 9, 2017 1:16 PM, \"Erik Aronesty via bitcoin-dev\" <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Curious: I'm not sure why a serious discussion of POW change is not on\n>> the table as a part of a longer-term roadmap.\n>>\n>> Done right, a ramp down of reliance on SHA-256 and a ramp-up on some of\n>> the proven, np-complete graph-theoretic or polygon manipulation POW would\n>> keep Bitcoin in commodity hardware and out of the hands of centralized\n>> manufacturing for many years.\n>>\n>> Clearly a level-playing field is critical to keeping centralization from\n>> being a \"defining feature\" of Bitcoin over the long term.   I've heard the\n>> term \"level playing field\" bandied about quite a bit.   And it seems to me\n>> that the risk of state actor control and botnet attacks is less than\n>> state-actor manipulation of specialized manufacturing of \"SHA-256 forever\"\n>> hardware.   Indeed, the reliance on a fairly simple hash seems less and\n>> less likely a \"feature\" and more of a baggage.\n>>\n>> Perhaps regular, high-consensus POW changes might even be *necessary* as\n>> a part of good maintenance of cryptocurrency in general.   Killing the\n>> existing POW, and using an as-yet undefined, but deployment-bit ready POW\n>> field to flip-flop between the current and the \"next one\" every 8 years or\n>> or so, with a ramp down beginning in the 7th year....  A stub function that\n>> is guaranteed to fail unless a new consensus POW is selected within 7\n>> years.\n>>\n>> Something like that?\n>>\n>> Haven't thought about it *that* much, but I think the network would\n>> respond well to a well known cutover date.   This would enable\n>> rapid-response to quantum tech, or some other needed POW switch as well...\n>> because the mechanisms would be in-place and ready to switch as needed.\n>>\n>> Lots of people seem to panic over POW changes as \"irresponsible\", but\n>> it's only irresponsible if done irresponsibly.\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n> The real bottleneck today is the amount of capex required to achieve\n> optimal mining. I am strongly in favor of PoW research that investigates\n> better PoW, but I do not think that any obvious strategies are known yet to\n> improve substantially on computation heavy hashcash.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170409/7a8d0a1e/attachment.html>"
            },
            {
                "author": "Thomas Daede",
                "date": "2017-04-10T01:45:24",
                "message_text_only": "On 04/09/2017 05:20 PM, Erik Aronesty via bitcoin-dev wrote:\n> Have you read the cuckoo cycle paper?  Finding cycles in massive graphs\n> is just about the worst thing to use an ASIC for.\n\nIt's actually the best thing to use an ASIC tightly coupled with DRAM\nfor - for example, HBM and HBM2 which reduce latency and increase\nthroughput by placing the DRAM on an interposer with the ASIC die, or\neven putting the logic on the DRAM die itself.\n\nIt would need at least proof that existing chips using HBM are ideal for\nCuckoo Cycle (unlikely) and that no DRAM manufacturer could ever be\ncoaxed into making an ASIC (even harder to guarantee).\n\nI think any long term PoW change is irrelevant to the review or adoption\nof the covert ASICBOOST BIPs, given the many unresolved problems of such\na change."
            },
            {
                "author": "Bram Cohen",
                "date": "2017-04-10T14:34:47",
                "message_text_only": "On Sun, Apr 9, 2017 at 11:44 AM, Erik Aronesty via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n> Clearly a level-playing field is critical to keeping centralization from\n> being a \"defining feature\" of Bitcoin over the long term.   I've heard the\n> term \"level playing field\" bandied about quite a bit.   And it seems to me\n> that the risk of state actor control and botnet attacks is less than\n> state-actor manipulation of specialized manufacturing of \"SHA-256 forever\"\n> hardware.   Indeed, the reliance on a fairly simple hash seems less and\n> less likely a \"feature\" and more of a baggage.\n>\n>\nWhatever your hashing function the bottleneck for mining will be power.\nEquihash and Cuckoo are serious attempts at making custom hardware have no\nbenefit over commodity hardware, but that's more about getting rid of\ncustom hardware manufacturers than it is about mining decentralization,\nalthough arguably if successful it might let botnets back in, which would\nimprove decentralization. While those have been surprisingly successful at\nresisting hardware so far, they might eventually fall as well, and if they\ndo they'll have even worse properties of centralizing around a mining\nhardware manufacturer than sha256 does.\n\nIt would be much safer to go the other way, to a PoW function whose best\nhardware implementation is particularly straightforward and well\nunderstood. In that case it would be best to go with sha3. Sha3 also has\nthe benefit of using the sponge construction, which makes it particularly\nresistant to asciboost-type attacks. It was picked out specifically because\nits design from a security standpoint was particularly\nconfidence-inspiring, and in this case it actually makes a difference.\nArguably you could also go with blake2b, whose 1024 bit block size\ncompletely obviates the asicboost concern entirely by cramming everything\ninto a single block. That also might have an even simpler design in\nhardware than sha3, but a real expert would need to opine on that one.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170410/3ff44513/attachment.html>"
            },
            {
                "author": "Bram Cohen",
                "date": "2017-04-10T14:46:35",
                "message_text_only": "On Sun, Apr 9, 2017 at 11:44 AM, Erik Aronesty via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n> Perhaps regular, high-consensus POW changes might even be *necessary* as a\n> part of good maintenance of cryptocurrency in general.   Killing the\n> existing POW, and using an as-yet undefined, but deployment-bit ready POW\n> field to flip-flop between the current and the \"next one\" every 8 years or\n> or so, with a ramp down beginning in the 7th year....  A stub function that\n> is guaranteed to fail unless a new consensus POW is selected within 7\n> years.\n>\n\nThat would force hard forks, cause huge governance problems on selecting\nthe new PoW algorithm, and probably cause even worse mining chip\nmanufacturer centralization because it would force miners to buy new chips\ninstead of sticking with the ones they've already got. They'll likely have\nto keep buying new ones anyway as technology improves but it doesn't help\nto force that process to go even faster.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170410/021b108b/attachment.html>"
            },
            {
                "author": "g",
                "date": "2017-04-10T15:25:05",
                "message_text_only": "Erik,\n\nI completely agree that it will be in the long term interest of bitcoin to migrate, gradually, toward a commoditized POW away from the current mass centralization. There is a big problem here though: Hundreds of millions of dollars have been spent on the current algorithm, and will be a huge loss if this is not done slowly enough, and the miners who control the chain currently would likely never allow this change to happen.\n\nDo you have any ideas regarding how to mitigate the damage of such a change for the invested parties? Or even how we can make the change agreeable for them?\n\nWarm regards,\nGarrett\n\n--\nGarrett MacDonald\n+1 720 515 2248\ng at cognitive.ch\nGPG Key\n\nOn Apr 9, 2017, 2:16 PM -0600, Erik Aronesty via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>, wrote:\n> Curious: I'm not sure why a serious discussion of POW change is not on the table as a part of a longer-term roadmap.\n>\n> Done right, a ramp down of reliance on SHA-256 and a ramp-up on some of the proven, np-complete graph-theoretic or polygon manipulation POW would keep Bitcoin in commodity hardware and out of the hands of centralized manufacturing for many years.\n>\n> Clearly a level-playing field is critical to keeping centralization from being a \"defining feature\" of Bitcoin over the long term. \u00a0 I've heard the term \"level playing field\" bandied about quite a bit. \u00a0 And it seems to me that the risk of state actor control and botnet attacks is less than state-actor manipulation of specialized manufacturing of \"SHA-256 forever\" hardware. \u00a0 Indeed, the reliance on a fairly simple hash seems less and less likely a \"feature\" and more of a baggage.\n>\n> Perhaps regular, high-consensus POW changes might even be *necessary* as a part of good maintenance of cryptocurrency in general. \u00a0 Killing the existing POW, and using an as-yet undefined, but deployment-bit ready POW field to flip-flop between the current and the \"next one\" every 8 years or or so, with a ramp down beginning in the 7th year....\u00a0 A stub function that is guaranteed to fail unless a new consensus POW is selected within 7 years.\n>\n> Something like that?\n>\n> Haven't thought about it *that* much, but I think the network would respond well to a well known cutover date. \u00a0 This would enable rapid-response to quantum tech, or some other needed POW switch as well... because the mechanisms would be in-place and ready to switch as needed.\n>\n> Lots of people seem to panic over POW changes as \"irresponsible\", but it's only irresponsible if done irresponsibly.\n>\n>\n> > On Fri, Apr 7, 2017 at 9:48 PM, praxeology_guy via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > > Jimmy Song,\n> > >\n> > > Why would the actual end users of Bitcoin (the long term and short term owners of bitcoins) who run fully verifying nodes want to change Bitcoin policy in order to make their money more vulnerable to 51% attack?\n> > >\n> > > If anything, we would be making policy changes to prevent the use of patented PoW algorithms instead of making changes to enable them.\n> > >\n> > > Thanks,\n> > > Praxeology Guy\n> > >\n> > > _______________________________________________\n> > > bitcoin-dev mailing list\n> > > bitcoin-dev at lists.linuxfoundation.org\n> > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> > >\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170410/f356547e/attachment.html>"
            },
            {
                "author": "Sancho Panza",
                "date": "2017-04-11T09:31:43",
                "message_text_only": "> I completely agree that it will be in the long term interest of bitcoin to migrate, gradually, toward a commoditized POW away from the current mass centralization. There is a big problem here though: Hundreds of millions of dollars have been spent on the current algorithm, and will be a huge loss if this is not done slowly enough, and the miners who control the chain currently would likely never allow this change to happen.\n\n> Do you have any ideas regarding how to mitigate the damage of such a change for the invested parties? Or even how we can make the change agreeable for them?\n\nApologies for interjecting a thought on this topic.\nMy belief is that Bitcoin could grow freely, and become worth enough so that mining becomes profitable even for those of us in countries without free / subsidized electricity.\n\nBy that time, buying commodity mining equipment (ASIC-based) from major manufacturers should be no problem, esp. not for existing Bitcoin holders.\n\nI see no sign that current major miners are principally opposed to such a natural process of decentralization of Bitcoin mining.\n\nSancho\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170411/e3decc02/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2017-04-11T13:00:29",
                "message_text_only": "The discussion is going offtopic. Can we please take vague discussions\nabout changing pow, so called \"asic resistance\", the environment etc\nto bitcoin-disscuss or some other forum?"
            },
            {
                "author": "Tom Zander",
                "date": "2017-04-11T07:59:33",
                "message_text_only": "The version field is still needed to actually allow future block version \nupgrades. We would cut off our road forward if that were to be blocked.\n\n\nOn Friday, 7 April 2017 22:06:39 CEST Jimmy Song via bitcoin-dev wrote:\n> Currently, the version bits (currently 4 bytes, or 32 bits) in the header\n> are used for BIP9 signaling. We change the version bits to a nonce-space\n> so the miners can use it for overt ASICBoost. The 32-bits are now moved\n> over to the Coinbase transaction as part of the witness commitment. The\n> witness commitment goes from 38 bytes to 42 bytes, with the last 4 bytes\n> being used as the version bits in the block header previously. The\n> witness commitment becomes required as per Gregory Maxwell\u2019s proposal.\n> Reasoning\n\n\n-- \nTom Zander\nBlog: https://zander.github.io\nVlog: https://vimeo.com/channels/tomscryptochannel"
            },
            {
                "author": "Sancho Panza",
                "date": "2017-04-11T13:25:21",
                "message_text_only": "Tom Zander wrote:\n\n> The version field is still needed to actually allow future block version upgrades. We would cut off our road forward if that were to be blocked.\n\nI tend to agree, if all 32 bits were given up to grinding.\n\nBut it's worth pointing out that BIP9 is purely informational, and the top 3 bits are still reserved for other purposes. One of them could perhaps be used to signal for an extended version field somewhere else, leaving the bottom 29 as entropy?\n\nNot a direction I prefer, but just a technical possibility perhaps.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170411/25ea97e0/attachment-0001.html>"
            },
            {
                "author": "Jimmy Song",
                "date": "2017-04-11T14:40:28",
                "message_text_only": "I've changed the proposal so only 8 bits are given to grinding so something\nlike 20 bits are available for signaling.\n\nI have to say I'm at a loss here as to what's next? Should I make a new BIP\nor try to convince the authors of BIP141 to modify their BIP? Could someone\ninform me on the next part of the process?\n\nOn Tue, Apr 11, 2017 at 8:25 AM, Sancho Panza via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Tom Zander wrote:\n>\n> > The version field is still needed to actually allow future block version\n> upgrades. We would cut off our road forward if that were to be blocked.\n>\n> I tend to agree, if all 32 bits were given up to grinding.\n>\n> But it's worth pointing out that BIP9 is purely informational, and the top\n> 3 bits are still reserved for other purposes. One of them could perhaps be\n> used to signal for an extended version field somewhere else, leaving the\n> bottom 29 as entropy?\n>\n> Not a direction I prefer, but just a technical possibility perhaps.\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170411/4b967431/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2017-04-11T21:25:45",
                "message_text_only": "On Tue, Apr 11, 2017 at 4:40 PM, Jimmy Song via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> I've changed the proposal so only 8 bits are given to grinding so something\n> like 20 bits are available for signaling.\n>\n> I have to say I'm at a loss here as to what's next? Should I make a new BIP\n> or try to convince the authors of BIP141 to modify their BIP? Could someone\n> inform me on the next part of the process?\n\nSee bip2, specifically\nhttps://github.com/bitcoin/bips/blob/master/bip-0002.mediawiki#bip-workflow\n\n\"Following a discussion, the proposal should be submitted to the BIPs\ngit repository as a pull request. This draft must be written in BIP\nstyle as described below, and named with an alias such as\n\"bip-johndoe-infinitebitcoins\" until the editor has assigned it a BIP\nnumber (authors MUST NOT self-assign BIP numbers).\"\n\nBut I think it's kind of late to modify bip141, given that there's\ncode out there with the current specification.\nI guess you can propose extensions or alternatives to replace it. I'm\nreally not sure what's the next step, but I don't think you have\nprovided enough motivation as to why we would want to maintain\nasicboost. You said it makes the network more secure, but that's not\nthe case, as explained, not even if all honest miners use it.\n\n> On Tue, Apr 11, 2017 at 8:25 AM, Sancho Panza via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>> Tom Zander wrote:\n>>\n>> > The version field is still needed to actually allow future block version\n>> > upgrades. We would cut off our road forward if that were to be blocked.\n>>\n>> I tend to agree, if all 32 bits were given up to grinding.\n>>\n>> But it's worth pointing out that BIP9 is purely informational, and the top\n>> 3 bits are still reserved for other purposes. One of them could perhaps be\n>> used to signal for an extended version field somewhere else, leaving the\n>> bottom 29 as entropy?\n>>\n>> Not a direction I prefer, but just a technical possibility perhaps.\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Jimmy Song",
                "date": "2017-04-11T23:42:41",
                "message_text_only": "Jorge, I'll be happy to discuss with you more about whether allowing\nASICBoost would actually secure the network more or not, but that's not my\nmain motivation. My main motivation is to get more miners to accept segwit.\n\nThe version bit usage part, I don't believe requires any code changes as\nthose bits aren't being used by BIP9 anyway, though some cleanup to\nrestrict them later is probably a good idea.\nThe requiring witness commitment part would require some changes, but\naccording to Timo Hanke, that's actually not necessary as overt is so much\nmore efficient.\n\nIn any case, I'm happy to close this discussion until there's some\nindication that more miners would accept segwit as a result of this change.\n\nJimmy\n\nOn Tue, Apr 11, 2017 at 4:25 PM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n\n> On Tue, Apr 11, 2017 at 4:40 PM, Jimmy Song via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > I've changed the proposal so only 8 bits are given to grinding so\n> something\n> > like 20 bits are available for signaling.\n> >\n> > I have to say I'm at a loss here as to what's next? Should I make a new\n> BIP\n> > or try to convince the authors of BIP141 to modify their BIP? Could\n> someone\n> > inform me on the next part of the process?\n>\n> See bip2, specifically\n> https://github.com/bitcoin/bips/blob/master/bip-0002.\n> mediawiki#bip-workflow\n>\n> \"Following a discussion, the proposal should be submitted to the BIPs\n> git repository as a pull request. This draft must be written in BIP\n> style as described below, and named with an alias such as\n> \"bip-johndoe-infinitebitcoins\" until the editor has assigned it a BIP\n> number (authors MUST NOT self-assign BIP numbers).\"\n>\n> But I think it's kind of late to modify bip141, given that there's\n> code out there with the current specification.\n> I guess you can propose extensions or alternatives to replace it. I'm\n> really not sure what's the next step, but I don't think you have\n> provided enough motivation as to why we would want to maintain\n> asicboost. You said it makes the network more secure, but that's not\n> the case, as explained, not even if all honest miners use it.\n>\n> > On Tue, Apr 11, 2017 at 8:25 AM, Sancho Panza via bitcoin-dev\n> > <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >>\n> >> Tom Zander wrote:\n> >>\n> >> > The version field is still needed to actually allow future block\n> version\n> >> > upgrades. We would cut off our road forward if that were to be\n> blocked.\n> >>\n> >> I tend to agree, if all 32 bits were given up to grinding.\n> >>\n> >> But it's worth pointing out that BIP9 is purely informational, and the\n> top\n> >> 3 bits are still reserved for other purposes. One of them could perhaps\n> be\n> >> used to signal for an extended version field somewhere else, leaving the\n> >> bottom 29 as entropy?\n> >>\n> >> Not a direction I prefer, but just a technical possibility perhaps.\n> >>\n> >> _______________________________________________\n> >> bitcoin-dev mailing list\n> >> bitcoin-dev at lists.linuxfoundation.org\n> >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> >>\n> >\n> >\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> >\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170411/95a10ad4/attachment.html>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2017-04-08T16:05:09",
                "message_text_only": "On Saturday, April 08, 2017 3:17:47 PM Jimmy Song wrote:\n> Overt ASICBoost is allowed on the network already. Until a proposal\n> explicitly blocking overt ASICBoost as a soft fork is activated, this seems\n> to be better than the current state which is that overt ASICBoost is\n> allowed, but at a cost to BIP9 signals.\n\nNo, it isn't allowed right now. Doing it wouldn't invalidate blocks, but it \nwould clearly be an attack on the network and cause harm. The same as if \nminers were to maliciously mine only empty blocks.\n\nLuke"
            },
            {
                "author": "Jimmy Song",
                "date": "2017-04-08T16:16:05",
                "message_text_only": ">\n>\n> No, it isn't allowed right now. Doing it wouldn't invalidate blocks, but it\n> would clearly be an attack on the network and cause harm. The same as if\n> miners were to maliciously mine only empty blocks.\n>\n>\nWhat's your definition of \"allowed\" then? Because a miner definitely can\nmine only empty blocks and a miner definitely can do overt ASICBoost (using\nas little as 1 bit of the version field) right now. I thought you meant\nallowed in the sense that if a block is allowed, it is a valid block on the\nnetwork. It sounds like you mean something else, perhaps, \"a block is\nallowed if it doesn't cause harm to the network.\" I'm not sure how you\nquantify that as that seems pretty subjective.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170408/06b7adc0/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2017-04-10T09:16:50",
                "message_text_only": "On 9 Apr 2017 4:01 pm, \"Jimmy Song\" <jaejoon at gmail.com> wrote:\n\nJorge,\n\nWhy won't the attacker use asicboost too? (Please don't say because of\n> patents)\n>\n>\nWe're assuming the ASIC optimization in my example is incompatible with\nASICBoost. But if the new optimization were compatible with ASICBoost,\nyou're right, the network would be in an equivalent situation whether\nASICBoost was banned or not.\n\n\nOnly if all honest miners use asicboost, otherwise the situation for an\nattack is not equivalent but worse with asicboost.\n\nI want to point out again that overt ASICBoost can be used on the network\ntoday. My proposal is to bring ASICBoost usage out into the open vs hiding\nit. Banning ASICBoost via protocol changes is another issue completely.\n\n\nDoesn't greg's proposal of disabling covert asicboost \"bring asicboost\nusage into the open vs hiding it\" too? It also does it without making any\nassumptions on whether we want to completely disable it later (I want)\nwhile your proposal assumes we do not.\n\nJimmy\n\n\n> On 9 Apr 2017 12:26 am, \"Jimmy Song\" <jaejoon at gmail.com> wrote:\n>\n>> Jorge,\n>>\n>> Suppose someone figures out an ASIC optimization that's completely\n>> unrelated that gives X% speed boost over your non-ASICBoosted\n>> implementation. If you ban ASICBoost, someone with this optimization can\n>> get 51% of the network by adding N machines with their new optimization. If\n>> you allow ASICBoost and assuming this gets a 20% speed boost over\n>> non-ASICBoosted hardware, someone with this optimization would need 1.2N\n>> machines to get 51%. The network in that sense is 20% stronger against this\n>> attack in terms of cost.\n>>\n>> Jimmy\n>>\n>> On Sat, Apr 8, 2017 at 12:22 PM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n>>\n>>> To be more specific, why \"being higher will secure the Bitcoin network\n>>> better against newer optimizations\"?\n>>> Or, to be more clear, let's forget about future \"optimizations\", let's\n>>> just think of an attacker. Does asicboost being used by all miners\n>>> make the system more secure against an attacker? No, for the attacker\n>>> can use asicboost too.\n>>> What about the case when not all the miners are using asicboost? Then\n>>> the attacker can actually get an advantage by suing asicboost.\n>>>\n>>> Sometimes people compare asicboost with the use of asics in general as\n>>> both providing more security for the network and users. But I don't\n>>> think this is accurate. The existence of sha256d asics makes an attack\n>>> with general purpose computing hardware (or even more specialized\n>>> architectures like gpgpu) much more expensive and unlikely. As an\n>>> alternative the attacker can spend additional resources investing in\n>>> asics himself (again, making many attacks more expensive and\n>>> unlikely).\n>>>\n>>> But as far as I know, asicboost can be implemented with software\n>>> running on general purpose hardware that integrates with regular\n>>> sha256d asics. There is probably an advantage on having the asicboost\n>>> implementation \"in the same box\" as the sha256d, yet again the\n>>> attacker can invest in hardware with the competitive advantage from\n>>> having asicboost more intergrated with the sha256d asics too.\n>>>\n>>> To reiterate, whether all miners use asicboost or only a subset of\n>>> them, I remain unconvinced that provides any additional security to\n>>> the network (to be more precise whether that makes \"tx history harder\n>>> to rewrite\"), even if it results on the hashrate charts looking \"more\n>>> secure\".\n>>>\n>>>\n>>> On Sat, Apr 8, 2017 at 6:27 PM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n>>> >\n>>> >\n>>> > On 8 Apr 2017 5:06 am, \"Jimmy Song via bitcoin-dev\"\n>>> > <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>> >\n>>> > Praxeology Guy,\n>>> >\n>>> >> Why would the actual end users of Bitcoin (the long term and short\n>>> term\n>>> >> owners of bitcoins) who run fully verifying nodes want to change\n>>> Bitcoin\n>>> >> policy in order to make their money more vulnerable to 51% attack?\n>>> >\n>>> >\n>>> > Certainly, if only one company made use of the extra nonce space, they\n>>> would\n>>> > have an advantage. But think of it this way, if some newer ASIC\n>>> optimization\n>>> > comes up, would you rather have a non-ASICBoosted hash rate to defend\n>>> with\n>>> > or an ASICBoosted hash rate? Certainly, the latter, being higher will\n>>> secure\n>>> > the Bitcoin network better against newer optimizations.\n>>> >\n>>> >\n>>> > Why?\n>>>\n>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170410/82352979/attachment.html>"
            },
            {
                "author": "Erik Aronesty",
                "date": "2017-04-10T18:17:03",
                "message_text_only": "I own some miners, but realistically their end of life is what, 6 months\nfrom now if I'm lucky?    If we used difficulty ramps on two selected\nPOW's, then the migration could be made smooth.   I don't think changing\nthe POW would be very challenging.  Personally, I would absolutely love to\nbe back in the business of buying GPU's instead of ASICs which are\nuniformly sketchy.   Does anyone *not* mine their own equipment before\n\"shipping late\" these days?\n\nMaybe sample a video game's GPU operations and try to develop a secure hash\nwhose optimal implementation uses them in a similar ratio?   Ultimately, I\nthink it would very challenging to find a POW that doesn't make a bad\nproblem worse.  I understand that's why you suggested SHA3.\n\nHopefully, the \"nanometer race\" we have will work more smoothly once the\nasicboost issue is resolved and competition can return to normal.   But\n\"waiting things out\" rarely seems to work in Bitcoin land.\n\n\n\n\n\n\nOn Mon, Apr 10, 2017 at 11:25 AM, g <g at cognitive.ch> wrote:\n\n> Erik,\n>\n> I completely agree that it will be in the long term interest of bitcoin to\n> migrate, gradually, toward a commoditized POW away from the current mass\n> centralization. There is a big problem here though: Hundreds of millions of\n> dollars have been spent on the current algorithm, and will be a huge loss\n> if this is not done slowly enough, and the miners who control the chain\n> currently would likely never allow this change to happen.\n>\n> Do you have any ideas regarding how to mitigate the damage of such a\n> change for the invested parties? Or even how we can make the change\n> agreeable for them?\n>\n> Warm regards,\n> Garrett\n>\n> --\n> Garrett MacDonald\n> +1 720 515 2248 <(720)%20515-2248>\n> g at cognitive.ch\n> GPG Key <https://pgp.mit.edu/pks/lookup?op=get&search=0x0A06E7F9E51DE2D6>\n>\n> On Apr 9, 2017, 2:16 PM -0600, Erik Aronesty via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org>, wrote:\n>\n> Curious: I'm not sure why a serious discussion of POW change is not on the\n> table as a part of a longer-term roadmap.\n>\n> Done right, a ramp down of reliance on SHA-256 and a ramp-up on some of\n> the proven, np-complete graph-theoretic or polygon manipulation POW would\n> keep Bitcoin in commodity hardware and out of the hands of centralized\n> manufacturing for many years.\n>\n> Clearly a level-playing field is critical to keeping centralization from\n> being a \"defining feature\" of Bitcoin over the long term.   I've heard the\n> term \"level playing field\" bandied about quite a bit.   And it seems to me\n> that the risk of state actor control and botnet attacks is less than\n> state-actor manipulation of specialized manufacturing of \"SHA-256 forever\"\n> hardware.   Indeed, the reliance on a fairly simple hash seems less and\n> less likely a \"feature\" and more of a baggage.\n>\n> Perhaps regular, high-consensus POW changes might even be *necessary* as a\n> part of good maintenance of cryptocurrency in general.   Killing the\n> existing POW, and using an as-yet undefined, but deployment-bit ready POW\n> field to flip-flop between the current and the \"next one\" every 8 years or\n> or so, with a ramp down beginning in the 7th year....  A stub function that\n> is guaranteed to fail unless a new consensus POW is selected within 7\n> years.\n>\n> Something like that?\n>\n> Haven't thought about it *that* much, but I think the network would\n> respond well to a well known cutover date.   This would enable\n> rapid-response to quantum tech, or some other needed POW switch as well...\n> because the mechanisms would be in-place and ready to switch as needed.\n>\n> Lots of people seem to panic over POW changes as \"irresponsible\", but it's\n> only irresponsible if done irresponsibly.\n>\n>\n> On Fri, Apr 7, 2017 at 9:48 PM, praxeology_guy via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Jimmy Song,\n>>\n>> Why would the actual end users of Bitcoin (the long term and short term\n>> owners of bitcoins) who run fully verifying nodes want to change Bitcoin\n>> policy in order to make their money more vulnerable to 51% attack?\n>>\n>> If anything, we would be making policy changes to prevent the use of\n>> patented PoW algorithms instead of making changes to enable them.\n>>\n>> Thanks,\n>> Praxeology Guy\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170410/1ba7de4f/attachment-0001.html>"
            },
            {
                "author": "g",
                "date": "2017-04-11T02:39:32",
                "message_text_only": "Makes sense. I would love if GPUs were back as the main hashing tool.\n\nHowever, we need to consider the environmental impact of mining, which currently consumes a quite exorbitant amount of energy. Any ideas on this front?\n\n--\nGarrett MacDonald\n+1 720 515 2248\ng at cognitive.ch\nGPG Key\n\nOn Apr 10, 2017, 12:17 PM -0600, Erik Aronesty <erik at q32.com>, wrote:\n> I own some miners, but realistically their end of life is what, 6 months from now if I'm lucky?\u00a0\u00a0\u00a0 If we used difficulty ramps on two selected POW's, then the migration could be made smooth.\u00a0\u00a0 I don't think changing the POW would be very challenging.\u00a0 Personally, I would absolutely love to be back in the business of buying GPU's instead of ASICs which are uniformly sketchy.\u00a0\u00a0 Does anyone *not* mine their own equipment before \"shipping late\" these days?\n>\n> Maybe sample a video game's GPU operations and try to develop a secure hash whose optimal implementation uses them in a similar ratio?\u00a0\u00a0 Ultimately, I think it would very challenging to find a POW that doesn't make a bad problem worse.\u00a0 I understand that's why you suggested SHA3.\n>\n> Hopefully, the \"nanometer race\" we have will work more smoothly once the asicboost issue is resolved and competition can return to normal.\u00a0\u00a0 But \"waiting things out\" rarely seems to work in Bitcoin land.\n>\n>\n>\n>\n>\n>\n> > On Mon, Apr 10, 2017 at 11:25 AM, g <g at cognitive.ch> wrote:\n> > > Erik,\n> > >\n> > > I completely agree that it will be in the long term interest of bitcoin to migrate, gradually, toward a commoditized POW away from the current mass centralization. There is a big problem here though: Hundreds of millions of dollars have been spent on the current algorithm, and will be a huge loss if this is not done slowly enough, and the miners who control the chain currently would likely never allow this change to happen.\n> > >\n> > > Do you have any ideas regarding how to mitigate the damage of such a change for the invested parties? Or even how we can make the change agreeable for them?\n> > >\n> > > Warm regards,\n> > > Garrett\n> > >\n> > > --\n> > > Garrett MacDonald\n> > > +1 720 515 2248\n> > > g at cognitive.ch\n> > > GPG Key\n> > >\n> > > On Apr 9, 2017, 2:16 PM -0600, Erik Aronesty via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>, wrote:\n> > > > Curious: I'm not sure why a serious discussion of POW change is not on the table as a part of a longer-term roadmap.\n> > > >\n> > > > Done right, a ramp down of reliance on SHA-256 and a ramp-up on some of the proven, np-complete graph-theoretic or polygon manipulation POW would keep Bitcoin in commodity hardware and out of the hands of centralized manufacturing for many years.\n> > > >\n> > > > Clearly a level-playing field is critical to keeping centralization from being a \"defining feature\" of Bitcoin over the long term. \u00a0 I've heard the term \"level playing field\" bandied about quite a bit. \u00a0 And it seems to me that the risk of state actor control and botnet attacks is less than state-actor manipulation of specialized manufacturing of \"SHA-256 forever\" hardware. \u00a0 Indeed, the reliance on a fairly simple hash seems less and less likely a \"feature\" and more of a baggage.\n> > > >\n> > > > Perhaps regular, high-consensus POW changes might even be *necessary* as a part of good maintenance of cryptocurrency in general. \u00a0 Killing the existing POW, and using an as-yet undefined, but deployment-bit ready POW field to flip-flop between the current and the \"next one\" every 8 years or or so, with a ramp down beginning in the 7th year....\u00a0 A stub function that is guaranteed to fail unless a new consensus POW is selected within 7 years.\n> > > >\n> > > > Something like that?\n> > > >\n> > > > Haven't thought about it *that* much, but I think the network would respond well to a well known cutover date. \u00a0 This would enable rapid-response to quantum tech, or some other needed POW switch as well... because the mechanisms would be in-place and ready to switch as needed.\n> > > >\n> > > > Lots of people seem to panic over POW changes as \"irresponsible\", but it's only irresponsible if done irresponsibly.\n> > > >\n> > > >\n> > > > > On Fri, Apr 7, 2017 at 9:48 PM, praxeology_guy via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > > > > > Jimmy Song,\n> > > > > >\n> > > > > > Why would the actual end users of Bitcoin (the long term and short term owners of bitcoins) who run fully verifying nodes want to change Bitcoin policy in order to make their money more vulnerable to 51% attack?\n> > > > > >\n> > > > > > If anything, we would be making policy changes to prevent the use of patented PoW algorithms instead of making changes to enable them.\n> > > > > >\n> > > > > > Thanks,\n> > > > > > Praxeology Guy\n> > > > > >\n> > > > > > _______________________________________________\n> > > > > > bitcoin-dev mailing list\n> > > > > > bitcoin-dev at lists.linuxfoundation.org\n> > > > > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> > > > > >\n> > > >\n> > > > _______________________________________________\n> > > > bitcoin-dev mailing list\n> > > > bitcoin-dev at lists.linuxfoundation.org\n> > > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170410/20eb5287/attachment-0001.html>"
            },
            {
                "author": "Staf Verhaegen",
                "date": "2017-04-11T18:39:11",
                "message_text_only": "g via bitcoin-dev schreef op ma 10-04-2017 om 20:39 [-0600]:\n\n> \n> However, we need to consider the environmental impact of mining, which\n> currently consumes a quite exorbitant amount of energy. Any ideas on\n> this front?\n\nEverything is relative. Some months ago I did some investigation and\nBitcoin mining used lees energy than the diesel used by the gold ore\nmining industry...\n\ngreets,\nStaf.\n\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 230 bytes\nDesc: This is a digitally signed message part\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170411/49d9f776/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "A Small Modification to Segwit",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Eric Voskuil",
                "Thomas Daede",
                "Staf Verhaegen",
                "David Vorick",
                "Timo Hanke",
                "Tom Zander",
                "praxeology_guy",
                "Jared Lee Richardson",
                "Jorge Tim\u00f3n",
                "Luke Dashjr",
                "Jimmy Song",
                "Pavel Moravec",
                "Erik Aronesty",
                "Bram Cohen",
                "Sancho Panza",
                "g"
            ],
            "messages_count": 41,
            "total_messages_chars_count": 97197
        }
    },
    {
        "title": "[bitcoin-dev] BIP Proposal: Inhibiting a covert optimization on the Bitcoin POW function",
        "thread_messages": [
            {
                "author": "Sergio Demian Lerner",
                "date": "2017-04-07T20:52:17",
                "message_text_only": "<pre>\n  BIP: TBD\n  Layer: Consensus\n  Title: Inhibiting a covert optimization on the Bitcoin POW function\n  Author: Sergio Demian Lerner <sergio.d.lerner at gmail.com>\n  Status: Draft\n  Type: Standards Track\n  Created: 2016-04-07\n  License: PD\n</pre>\n\n==Abstract==\n\nThis proposal inhibits the covert use of a known optimization in Bitcoin\nProof of Work function.\n\nThe key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n\"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this\ndocument are to be interpreted as described in RFC 2119.\n\n==Motivation==\n\nDue to a design oversight the Bitcoin proof of work function has a potential\noptimization which can allow a rational miner to save up-to 30% of their\nenergy\ncosts (though closer to 20% is more likely due to implementation overheads).\n\nTimo Hanke and Sergio Demian Lerner applied for a patent on this\noptimization. The company \"Sunrise Tech Group, Llc\" has offered to license\nit to any interested party in the past. Sunrise Tech Group has been\nmarketing their patent licenses under the trade-name ASICBOOST.  The\ndocument takes no position on the validity or enforceability of the patent.\n\nThere are two major ways of taking advantage of this optimization, as\ndescribed\nby the patent:\nOne way which is highly detectable and is not in use on the network\ntoday and a covert way which has significant interaction and potential\ninterference with the Bitcoin protocol.  The covert mechanism is not\neasily detected except through its interference with the protocol.\n\nIn particular, the protocol interactions of the covert method can block the\nimplementation of virtuous improvements such as segregated witness.\n\nThe use of this optimization could result in a big payoff, but the actual\nsum depends on the degree of research, investment and effort put into\ndesigning\nthe improved cores.\n\nOn the above basis the potential for covert use of this optimization\nin the covert form and interference with useful improvements presents a\ndanger to the Bitcoin system.\n\n==Background==\n\nThe general idea of this optimization is that SHA2-256 is a merkle damgard\nhash\nfunction which consumes 64 bytes of data at a time.\n\nThe Bitcoin mining process repeatedly hashes an 80-byte 'block header' while\nincriminating a 32-bit nonce which is at the end of this header data. This\nmeans that the processing of the header involves two runs of the compression\nfunction run-- one that consumes the first 64 bytes of the header and a\nsecond which processes the remaining 16 bytes and padding.\n\nThe initial 'message expansion' operations in each step of the SHA2-256\nfunction operate exclusively on that step's 64-bytes of input with no\ninfluence from prior data that entered the hash.\n\nBecause of this if a miner is able to prepare a block header with\nmultiple distinct first 64-byte chunks but identical 16-byte\nsecond chunks they can reuse the computation of the initial\nexpansion for multiple trials. This reduces power consumption.\n\nThere are two broad ways of making use of this optimization. The obvious\nway is to try candidates with different version numbers.  Beyond\nupsetting the soft-fork detection logic in Bitcoin nodes this has\nlittle negative effect but it is highly conspicuous and easily\nblocked.\n\nThe other method is based on the fact that the merkle root\ncommitting to the transactions is contained in the first 64-bytes\nexcept for the last 4 bytes of it.  If the miner finds multiple\ncandidate root values which have the same final 32-bit then they\ncan use the optimization.\n\nTo find multiple roots with the same trailing 32-bits the miner can\nuse efficient collision finding mechanism which will find a match\nwith as little as 2^16 candidate roots expected, 2^24 operations to\nfind a 4-way hit, though low memory approaches require more\ncomputation.\n\nAn obvious way to generate different candidates is to grind the\ncoinbase extra-nonce but for non-empty blocks each attempt will\nrequire 13 or so additional sha2 runs which is very inefficient.\n\nThis inefficiency can be avoided by computing a sqrt number of\ncandidates of the left side of the hash tree (e.g. using extra\nnonce grinding) then an additional sqrt number of candidates of\nthe right  side of the tree using transaction permutation or\nsubstitution of a small number of transactions.  All combinations\nof the left and right side are then combined with only a single\nhashing operation virtually eliminating all tree related\noverhead.\n\nWith this final optimization finding a 4-way collision with a\nmoderate amount of memory requires ~2^24 hashing operations\ninstead of the >2^28 operations that would be require for\nextra-nonce  grinding which would substantially erode the\nbenefit of the optimization.\n\nIt is this final optimization which this proposal blocks.\n\n==New consensus rule==\n\nBeginning block X and until block Y the coinbase transaction of\neach block MUST either contain a BIP-141 segwit commitment or a\ncorrect WTXID commitment with ID 0xaa21a9ef.\n\n(See BIP-141 \"Commitment structure\" for details)\n\nExisting segwit using miners are automatically compatible with\nthis proposal. Non-segwit miners can become compatible by simply\nincluding an additional output matching a default commitment\nvalue returned as part of getblocktemplate.\n\nMiners SHOULD NOT automatically discontinue the commitment\nat the expiration height.\n\n==Discussion==\n\nThe commitment in the left side of the tree to all transactions\nin the right side completely prevents the final sqrt speedup.\n\nA stronger inhibition of the covert optimization in the form of\nrequiring the least significant bits of the block timestamp\nto be equal to a hash of the first 64-bytes of the header. This\nwould increase the collision space from 32 to 40 or more bits.\nThe root value could be required to meet a specific hash prefix\nrequirement in order to increase the computational work required\nto try candidate roots. These change would be more disruptive and\nthere is no reason to believe that it is currently necessary.\n\nThe proposed rule automatically sunsets. If it is no longer needed\ndue to the introduction of stronger rules or the acceptance of the\nversion-grinding form then there would be no reason to continue\nwith this requirement.  If it is still useful at the expiration\ntime the rule can simply be extended with a new softfork that\nsets longer date ranges.\n\nThis sun-setting avoids the accumulation of technical debt due\nto retaining enforcement of this rule when it is no longer needed\nwithout requiring a hard fork to remove it.\n\n== Overt optimization ==\n\nA BIP for avoiding erroneous warning messages when miners use the overt\nversion\nof the optimization was proposed several years ago, in order to deter the\ncovert\nuse of the optimization. But that BIP was rejected.\nHowever, in light of the current discoveries, that BIP could be\nreconsidered.\n\nThe over optimization does not generally interfere with improvements in the\nprotocol.\n\n==Backward compatibility==\n\n\n==Implementation==\n\n\n==Acknowledgments==\n\nGreg Maxwell <greg at xiph.org> for the original report, which contained\nseveral errors that were corrected in the present proposal.\n\n==Copyright==\n\nThis document is placed in the public domain.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170407/46d4d476/attachment-0001.html>"
            },
            {
                "author": "Jan \u010capek",
                "date": "2017-04-07T22:48:11",
                "message_text_only": "Hi,\n\n1 comment below\nOn Fri, 7 Apr 2017 17:52:17 -0300\nSergio Demian Lerner via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> <pre>\n>   BIP: TBD\n>   Layer: Consensus\n>   Title: Inhibiting a covert optimization on the Bitcoin POW function\n>   Author: Sergio Demian Lerner <sergio.d.lerner at gmail.com>\n>   Status: Draft\n>   Type: Standards Track\n>   Created: 2016-04-07\n>   License: PD\n> </pre>\n> \n> ==Abstract==\n> \n> This proposal inhibits the covert use of a known optimization in\n> Bitcoin Proof of Work function.\n> \n> The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n> \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this\n> document are to be interpreted as described in RFC 2119.\n> \n> ==Motivation==\n> \n> Due to a design oversight the Bitcoin proof of work function has a\n> potential optimization which can allow a rational miner to save up-to\n> 30% of their energy\n> costs (though closer to 20% is more likely due to implementation\n> overheads).\n> \n> Timo Hanke and Sergio Demian Lerner applied for a patent on this\n> optimization. The company \"Sunrise Tech Group, Llc\" has offered to\n> license it to any interested party in the past. Sunrise Tech Group\n> has been marketing their patent licenses under the trade-name\n> ASICBOOST.  The document takes no position on the validity or\n> enforceability of the patent.\n> \n> There are two major ways of taking advantage of this optimization, as\n> described\n> by the patent:\n> One way which is highly detectable and is not in use on the network\n> today and a covert way which has significant interaction and potential\n> interference with the Bitcoin protocol.  The covert mechanism is not\n> easily detected except through its interference with the protocol.\n> \n> In particular, the protocol interactions of the covert method can\n> block the implementation of virtuous improvements such as segregated\n> witness.\n> \n> The use of this optimization could result in a big payoff, but the\n> actual sum depends on the degree of research, investment and effort\n> put into designing\n> the improved cores.\n> \n> On the above basis the potential for covert use of this optimization\n> in the covert form and interference with useful improvements presents\n> a danger to the Bitcoin system.\n> \n> ==Background==\n> \n> The general idea of this optimization is that SHA2-256 is a merkle\n> damgard hash\n> function which consumes 64 bytes of data at a time.\n> \n> The Bitcoin mining process repeatedly hashes an 80-byte 'block\n> header' while incriminating a 32-bit nonce which is at the end of\n> this header data. This means that the processing of the header\n> involves two runs of the compression function run-- one that consumes\n> the first 64 bytes of the header and a second which processes the\n> remaining 16 bytes and padding.\n> \n> The initial 'message expansion' operations in each step of the\n> SHA2-256 function operate exclusively on that step's 64-bytes of\n> input with no influence from prior data that entered the hash.\n> \n> Because of this if a miner is able to prepare a block header with\n> multiple distinct first 64-byte chunks but identical 16-byte\n> second chunks they can reuse the computation of the initial\n> expansion for multiple trials. This reduces power consumption.\n> \n> There are two broad ways of making use of this optimization. The\n> obvious way is to try candidates with different version numbers.\n> Beyond upsetting the soft-fork detection logic in Bitcoin nodes this\n> has little negative effect but it is highly conspicuous and easily\n> blocked.\n> \n> The other method is based on the fact that the merkle root\n> committing to the transactions is contained in the first 64-bytes\n> except for the last 4 bytes of it.  If the miner finds multiple\n> candidate root values which have the same final 32-bit then they\n> can use the optimization.\n> \n> To find multiple roots with the same trailing 32-bits the miner can\n> use efficient collision finding mechanism which will find a match\n> with as little as 2^16 candidate roots expected, 2^24 operations to\n> find a 4-way hit, though low memory approaches require more\n> computation.\n> \n> An obvious way to generate different candidates is to grind the\n> coinbase extra-nonce but for non-empty blocks each attempt will\n> require 13 or so additional sha2 runs which is very inefficient.\n> \n> This inefficiency can be avoided by computing a sqrt number of\n> candidates of the left side of the hash tree (e.g. using extra\n> nonce grinding) then an additional sqrt number of candidates of\n> the right  side of the tree using transaction permutation or\n> substitution of a small number of transactions.  All combinations\n> of the left and right side are then combined with only a single\n> hashing operation virtually eliminating all tree related\n> overhead.\n> \n> With this final optimization finding a 4-way collision with a\n> moderate amount of memory requires ~2^24 hashing operations\n> instead of the >2^28 operations that would be require for\n> extra-nonce  grinding which would substantially erode the\n> benefit of the optimization.\n> \n> It is this final optimization which this proposal blocks.\n> \n> ==New consensus rule==\n> \n> Beginning block X and until block Y the coinbase transaction of\n> each block MUST either contain a BIP-141 segwit commitment or a\n> correct WTXID commitment with ID 0xaa21a9ef.\n> \n> (See BIP-141 \"Commitment structure\" for details)\n> \n> Existing segwit using miners are automatically compatible with\n> this proposal. Non-segwit miners can become compatible by simply\n> including an additional output matching a default commitment\n> value returned as part of getblocktemplate.\n> \n> Miners SHOULD NOT automatically discontinue the commitment\n> at the expiration height.\n> \n> ==Discussion==\n> \n> The commitment in the left side of the tree to all transactions\n> in the right side completely prevents the final sqrt speedup.\n> \n> A stronger inhibition of the covert optimization in the form of\n> requiring the least significant bits of the block timestamp\n> to be equal to a hash of the first 64-bytes of the header. This\n> would increase the collision space from 32 to 40 or more bits.\n> The root value could be required to meet a specific hash prefix\n> requirement in order to increase the computational work required\n> to try candidate roots.\nRoot value pow - Does this mean that every miner would be penalized in\nthis way regardless of the actual number of transactions in the block?\n> These change would be more disruptive and\n> there is no reason to believe that it is currently necessary.\n> \n> The proposed rule automatically sunsets. If it is no longer needed\n> due to the introduction of stronger rules or the acceptance of the\n> version-grinding form then there would be no reason to continue\n> with this requirement.  If it is still useful at the expiration\n> time the rule can simply be extended with a new softfork that\n> sets longer date ranges.\n> \n> This sun-setting avoids the accumulation of technical debt due\n> to retaining enforcement of this rule when it is no longer needed\n> without requiring a hard fork to remove it.\n> \n> == Overt optimization ==\n> \n> A BIP for avoiding erroneous warning messages when miners use the\n> overt version\n> of the optimization was proposed several years ago, in order to deter\n> the covert\n> use of the optimization. But that BIP was rejected.\n> However, in light of the current discoveries, that BIP could be\n> reconsidered.\n> \n> The over optimization does not generally interfere with improvements\n> in the protocol.\n> \n> ==Backward compatibility==\n> \n> \n> ==Implementation==\n> \n> \n> ==Acknowledgments==\n> \n> Greg Maxwell <greg at xiph.org> for the original report, which contained\n> several errors that were corrected in the present proposal.\n> \n> ==Copyright==\n> \n> This document is placed in the public domain.\n\n\n\n-- \nCEO Braiins Systems | Slushpool.com\ntel: +420 604 566 382\nemail: jan.capek at braiins.cz\nhttp://braiins.cz\nhttp://slushpool.com"
            }
        ],
        "thread_summary": {
            "title": "BIP Proposal: Inhibiting a covert optimization on the Bitcoin POW function",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Sergio Demian Lerner",
                "Jan \u010capek"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 15375
        }
    },
    {
        "title": "[bitcoin-dev] Proposed CSV configuration file format for bip-genvbvoting",
        "thread_messages": [
            {
                "author": "Sancho Panza",
                "date": "2017-04-11T13:11:19",
                "message_text_only": "Hi,\n\nThe link below includes documentation about a proposed CSV-based file format for fork deployment data (tentative config filename: forks.csv). This is planned to be used by my reference implementation of bip-genvbvoting (which is still in development - TBA later).\nOther BIP9 improvement proposals are of course encouraged to use this format, and I'm happy to discuss extensions of it for things like supporting flag days or direct-to-activation transitions.\n\nRegards,\nSancho\n\nhttps://raw.githubusercontent.com/sanch0panza/bitcoin/genvbvoting-bu-dev/doc/genvbvoting.md\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170411/b925eb64/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Proposed CSV configuration file format for bip-genvbvoting",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Sancho Panza"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 755
        }
    },
    {
        "title": "[bitcoin-dev] BIP proposal draft: BIP43 \"purpose\" allocation for Ethereum",
        "thread_messages": [
            {
                "author": "Nick Johnson",
                "date": "2017-04-12T10:02:37",
                "message_text_only": "<pre>\n  BIP: bip-nickjohnson-ethereum-purpose\n  Layer: Applications\n  Title: Ethereum purpose allocation for Deterministic Wallets\n  Author: Nick Johnson <nick at ethereum.org>\n  Status: Proposed\n  Type: Standards Track\n  Created: 2017-04-12\n</pre>\n\n==Abstract==\n\nThis BIP defines a logical hierarchy for deterministic wallets on the Ethereum\nblockchain based on an algorithm described in BIP-0032 (BIP32 from now on) and\npurpose scheme described in BIP-0043 (BIP43 from now on).\n\nThis BIP is a particular application of BIP43.\n\n==Motivation==\n\nBecause Ethereum is based on account balances rather than UTXO, the hierarchy\ndefined by BIP44 is poorly suited. As a result, several competing\nderivation path strategies have sprung up for deterministic wallets, resulting\nin inter-client incompatibility. This BIP seeks to provide a path to standardise\nthis in a fashion better suited to Ethereum's unique requirements.\n\n==Path levels==\n\nWe define the following 2 levels in BIP32 path:\n\n<pre>\nm / purpose' / subpurpose' / *\n</pre>\n\nApostrophe in the path indicates that BIP32 hardened derivation is used.\n\nEach level has a special meaning, described in the chapters below.\n\n===Purpose===\n\nPurpose is a constant set to the hardened value of the BIP number assigned to\nthis BIP (equivalently, the BIP number, bitwise ORed with 0x80000000) following\nthe BIP43 recommendation.\nIt indicates that the subtree of this node is used according to this\nspecification.\n\nHardened derivation is used at this level.\n\n===Subpurpose===\nSubpurpose is set to the EIP number specifying the remainder of the BIP32\nderivation path. This permits new Ethereum-focused applications of\ndeterministic wallets without needing to interface with the BIP process.\n\nHardened derivation is used at this level.\n\n==Reference== * [[bip-0032.mediawiki|BIP32 - Hierarchical Deterministic\nWallets]] * [[bip-0043.mediawiki|BIP43 - Purpose Field for Deterministic\nWallets]]\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170412/f13f375a/attachment.html>"
            },
            {
                "author": "Tom Zander",
                "date": "2017-04-14T19:26:14",
                "message_text_only": "Thinking about this a bit, I support this proposal for a BIP.\nThis is not Bitcoin, but address types are bound to meet in meat-space and \nit would be good to have a central place where this is defined.\n\nI would very much appreciate someone that worked on BIP32/BIP43 itself to \ncomment on the details.\n\nQuoting bip 43;\n\n\n\"We encourage different schemes to apply for assigning a separate BIP\nnumber and use the same number for purpose field, so addresses won't be\ngenerated from overlapping BIP32 spaces.\"\n\n\n\nOn Wednesday, 12 April 2017 12:02:37 CEST Nick Johnson via bitcoin-dev \nwrote:\n> <pre>\n>   BIP: bip-nickjohnson-ethereum-purpose\n>   Layer: Applications\n>   Title: Ethereum purpose allocation for Deterministic Wallets\n>   Author: Nick Johnson <nick at ethereum.org>\n>   Status: Proposed\n>   Type: Standards Track\n>   Created: 2017-04-12\n> </pre>\n> \n> ==Abstract==\n> \n> This BIP defines a logical hierarchy for deterministic wallets on the\n> Ethereum blockchain based on an algorithm described in BIP-0032 (BIP32\n> from now on) and purpose scheme described in BIP-0043 (BIP43 from now\n> on).\n> \n> This BIP is a particular application of BIP43.\n> \n> ==Motivation==\n> \n> Because Ethereum is based on account balances rather than UTXO, the\n> hierarchy defined by BIP44 is poorly suited. As a result, several\n> competing derivation path strategies have sprung up for deterministic\n> wallets, resulting in inter-client incompatibility. This BIP seeks to\n> provide a path to standardise this in a fashion better suited to\n> Ethereum's unique requirements.\n> \n> ==Path levels==\n> \n> We define the following 2 levels in BIP32 path:\n> \n> <pre>\n> m / purpose' / subpurpose' / *\n> </pre>\n> \n> Apostrophe in the path indicates that BIP32 hardened derivation is used.\n> \n> Each level has a special meaning, described in the chapters below.\n> \n> ===Purpose===\n> \n> Purpose is a constant set to the hardened value of the BIP number assigned\n> to this BIP (equivalently, the BIP number, bitwise ORed with 0x80000000)\n> following the BIP43 recommendation.\n> It indicates that the subtree of this node is used according to this\n> specification.\n> \n> Hardened derivation is used at this level.\n> \n> ===Subpurpose===\n> Subpurpose is set to the EIP number specifying the remainder of the BIP32\n> derivation path. This permits new Ethereum-focused applications of\n> deterministic wallets without needing to interface with the BIP process.\n> \n> Hardened derivation is used at this level.\n> \n> ==Reference== * [[bip-0032.mediawiki|BIP32 - Hierarchical Deterministic\n> Wallets]] * [[bip-0043.mediawiki|BIP43 - Purpose Field for Deterministic\n> Wallets]]\n\n\n-- \nTom Zander\nBlog: https://zander.github.io\nVlog: https://vimeo.com/channels/tomscryptochannel"
            }
        ],
        "thread_summary": {
            "title": "BIP proposal draft: BIP43 \"purpose\" allocation for Ethereum",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Nick Johnson",
                "Tom Zander"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 4845
        }
    },
    {
        "title": "[bitcoin-dev] Deploying CT in Bitcoin without extension blocks?",
        "thread_messages": [
            {
                "author": "Oleg Andreev",
                "date": "2017-04-13T00:43:50",
                "message_text_only": "(This is a sketch, not a fully-formed proposal, just to kick off the discussion.)\n\nConfidential Transactions (by GMaxwell & Poelstra) require a new accounting model, \nnew representation of numbers (EC points as Pedersen commitments) and range proofs \nper number. Setting aside performance and bandwidth concerns (3-4Kb per output, \n50x more signature checks), how would we deploy that feature on Bitcoin network \nin the most compatible manner?\n\nI'll try to present a sketch of the proposal. I apologize if this discussion already\nhappened somewhere, although I couldn't find anything on this subject, apart from Elements \nsidechain proposal, of course.\n\nAt first glance we could create a new extblock and transaction format, add a protocol to\n\"convert\" money into and from such extblock, and commit to that extblock from the \nouter block's coinbase transaction. Unfortunately, this opens gates to a flood of\ndebates such as what should be the block size limit in such block, should we \ntake opportunity to fix over 9000 of pet-peeve issues with existing transactions\nand blocks, should we adjust inflation schedule, insert additional PoW, what would\nSatoshi say etc. Federated sidechain suffers from the same issues, plus adds \nconcerns regarding governance, although it would be more decoupled, which is useful.\n\nI tried to look at a possibility to make the change as compatible as possible,\nsticking confidential values right into the existing transaction structure and\nsee how that would look like. As a nice bonus, confidential transactions would have \nto fit into the hard-coded 1 Mb limit, preserving the drama around it :-P\n\nWe start with a segwit-enabled script versioning and introduce 2 new script versions:\nversion A has an actual program concatenated with the commitment, while version B \nhas only the commitment and allows mimblewimble usage (no signatures, non-interactive \ncut-through etc). Legacy cleartext amount can nicely act as \"min value\" to minimize\nthe range proof size, and range proofs themselves are provided separately in the\nsegregated witness payload.\n\nThen, we soft fork additional rules:\n\n1. In non-coinbase tx, sum of commitments on inputs must balance with sum of commitments\n   on the outputs plus the cleartext mining fee in the witness.\n2. Range proof can be confidential, based on borromean ring signature.\n3. Range proof can be non-confidential, consisting of an amount and raw blinding factor.\n4. Tx witness can have an excess value (cf. MW) and cleartext amount for a miner's fee.\n5. In coinbase tx, total plaintext reward + commitments must balance with subsidy, \n   legacy fees and new fees in the witness.\n6. Extra fees in the witness must be signed with the excess value's key.\n\nThe confidential transactions use the same UTXO set, can be co-authored with plaintext inputs/outputs\nusing legacy software and maybe even improve scalability by compressing on-chain transactions\nusing mimblewimble cut-through.\n\nThe rules above could have been made more complicated with export/import logic to allow users\nconverting their coins to and from confidential ones, but that would require\nmore complex support from miners to respect and merge outputs representing \"plaintext value bank\",\nmutate export transactions, which in turn requires introduction of a non-malleable TxID\nthat excludes miner-adjustable export/import outputs.\n\nThe rules above have a nice side effect that miners, being the minters of confidential coins, \ncan sell them at a premium, which creates an incentive for them to actually support\nthat feature and work on improving performance of rangeproof validation (e.g. in GPUs).\n\nWould love to hear comments and criticism of that approach.\n\nThanks!\nOleg."
            },
            {
                "author": "Adam Back",
                "date": "2017-04-13T01:43:46",
                "message_text_only": "See this soft-fork proposal from Felix Weiss\n\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-January/012194.html\n\nAdam\n\nOn Apr 12, 2017 5:43 PM, \"Oleg Andreev via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> (This is a sketch, not a fully-formed proposal, just to kick off the\n> discussion.)\n>\n> Confidential Transactions (by GMaxwell & Poelstra) require a new\n> accounting model,\n> new representation of numbers (EC points as Pedersen commitments) and\n> range proofs\n> per number. Setting aside performance and bandwidth concerns (3-4Kb per\n> output,\n> 50x more signature checks), how would we deploy that feature on Bitcoin\n> network\n> in the most compatible manner?\n>\n> I'll try to present a sketch of the proposal. I apologize if this\n> discussion already\n> happened somewhere, although I couldn't find anything on this subject,\n> apart from Elements\n> sidechain proposal, of course.\n>\n> At first glance we could create a new extblock and transaction format, add\n> a protocol to\n> \"convert\" money into and from such extblock, and commit to that extblock\n> from the\n> outer block's coinbase transaction. Unfortunately, this opens gates to a\n> flood of\n> debates such as what should be the block size limit in such block, should\n> we\n> take opportunity to fix over 9000 of pet-peeve issues with existing\n> transactions\n> and blocks, should we adjust inflation schedule, insert additional PoW,\n> what would\n> Satoshi say etc. Federated sidechain suffers from the same issues, plus\n> adds\n> concerns regarding governance, although it would be more decoupled, which\n> is useful.\n>\n> I tried to look at a possibility to make the change as compatible as\n> possible,\n> sticking confidential values right into the existing transaction structure\n> and\n> see how that would look like. As a nice bonus, confidential transactions\n> would have\n> to fit into the hard-coded 1 Mb limit, preserving the drama around it :-P\n>\n> We start with a segwit-enabled script versioning and introduce 2 new\n> script versions:\n> version A has an actual program concatenated with the commitment, while\n> version B\n> has only the commitment and allows mimblewimble usage (no signatures,\n> non-interactive\n> cut-through etc). Legacy cleartext amount can nicely act as \"min value\" to\n> minimize\n> the range proof size, and range proofs themselves are provided separately\n> in the\n> segregated witness payload.\n>\n> Then, we soft fork additional rules:\n>\n> 1. In non-coinbase tx, sum of commitments on inputs must balance with sum\n> of commitments\n>    on the outputs plus the cleartext mining fee in the witness.\n> 2. Range proof can be confidential, based on borromean ring signature.\n> 3. Range proof can be non-confidential, consisting of an amount and raw\n> blinding factor.\n> 4. Tx witness can have an excess value (cf. MW) and cleartext amount for a\n> miner's fee.\n> 5. In coinbase tx, total plaintext reward + commitments must balance with\n> subsidy,\n>    legacy fees and new fees in the witness.\n> 6. Extra fees in the witness must be signed with the excess value's key.\n>\n> The confidential transactions use the same UTXO set, can be co-authored\n> with plaintext inputs/outputs\n> using legacy software and maybe even improve scalability by compressing\n> on-chain transactions\n> using mimblewimble cut-through.\n>\n> The rules above could have been made more complicated with export/import\n> logic to allow users\n> converting their coins to and from confidential ones, but that would\n> require\n> more complex support from miners to respect and merge outputs representing\n> \"plaintext value bank\",\n> mutate export transactions, which in turn requires introduction of a\n> non-malleable TxID\n> that excludes miner-adjustable export/import outputs.\n>\n> The rules above have a nice side effect that miners, being the minters of\n> confidential coins,\n> can sell them at a premium, which creates an incentive for them to\n> actually support\n> that feature and work on improving performance of rangeproof validation\n> (e.g. in GPUs).\n>\n> Would love to hear comments and criticism of that approach.\n>\n> Thanks!\n> Oleg.\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170412/780a5a4a/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Deploying CT in Bitcoin without extension blocks?",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Adam Back",
                "Oleg Andreev"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 8203
        }
    },
    {
        "title": "[bitcoin-dev] Proposal: Soft Fork Threshold Signaling",
        "thread_messages": [
            {
                "author": "Thomas Voegtlin",
                "date": "2017-04-13T11:36:36",
                "message_text_only": "Disclaimer: I am fully supportive of Segregated Witness and its\nactivation by users through BIP148. However, I also believe that a\nsoft fork would be less risky if it was initially activated by miners,\nbefore the date set in BIP148. This proposal is not intended to\nreplace UASF, but to mitigate the risks.\n\nThe following idea might already have been proposed and discussed\nelsewhere. If that is the case, I am sorry for the noise.\n\n\nBackground\n==========\n\nBIP9 requires 95% of miner hashrate support in order to activate a\nsoft-fork. So far, the lack of miner consensus about Segwit has been\nfrustrating both users and developers. This had led some users to\npropose a soft fork activation regardless of the expressed level of\nminer support (UASF, BIP148).\n\nThere are many risks associated with UASF. If the fork is activated\nwith less than 50% of the hashing power, the blockchain will have two\ncompeting branches. In addition, if the hashrate on the forking branch\nis very low, that branch will be exposed to attacks, where non-empty\nblocks are systematically orphaned by adverse miners. This threat may\nbe a strong deterrent for miners willing to support the fork.\n\nThe main argument in favor of UASF is that users, not miners, give its\nvalue to Bitcoin. Therefore, users and markets should have the power\nto decide which branch of the fork has the most value, and\nprofit-driven miners should follow. If the soft-forking branch is\nvalued more than the non-forking branch, it will end up attracting a\nmajority of the hashing power, and the non-forking branch will\neventually be orphaned.\n\nFeedback through markets, however, will only work if the forking\nbranch can effectively be used. If the forking branch is rendered\nunusable by adverse miners, there is little chance the new coins will\never reach markets. To make things worse, profit-driven miners might\nadopt a passive attitude and decide to mine on the forking branch only\nonce a proper price has been set by markets, or only once they see\nthat it has enough hashing power to be usable. Thus, the lack of\nhashrate information prior to the soft fork increases the risk.\n\nOn the other hand, if a soft fork was initiated with more than 33% of\nthe hashing power, then it would probably be viable, because the\nremaining two thirds of the hashing power cannot successfully be\nallocated to mine blocks on the non-forking branch and to orphan\nblocks on the forking branch. Therefore, users will be able to move\ncoins on the forking branch, and markets will be able to set a price\non these coins, thus creating the feedback needed by profit-driven\nminers.\n\nToday about 30% of the hashing power are signaling their intention to\nactivate Segwit using BIP9. This hashrate is very close to the 33%\nthreshold, and it would probably be enough to initiate a viable soft\nfork; indeed we can expect additional hashing power to be gained from\nminers mining on both branches of the fork.\n\nHowever, nothing suggests that a soft fork triggered with 30% of the\nhashrate would be followed by the miners who are currently signaling\nSegwit using BIP9. BIP9 signaling means that these miners are willing\nto soft fork if support reaches 95%; it does not say anything about\ntheir intentions if support is as low as 30%. In other words, BIP9\nsignaling does allow miners to properly signal their intentions.\n\n\nBIP9 signaling\n==============\n\nThe activation threshold is part of the semantics of BIP9. Miners who\nuse BIP9 do not only signal their support for a soft fork; they also\nsignal to other miners that they will activate the soft fork if and\nonly if support reaches 95%.\n\nSome of these miners might actually be willing to activate a soft fork\nwith a lower support, even at the cost of creating two chains. Other\nminers might not be supportive of that idea, because they consider\nthat the danger of their blocks being orphaned is too high.\n\nThe problem is that this information, at which level of support miners\nare willing to initiate a soft fork, is not available. Thus, miners\nwho are willing to initiate a soft fork at a lower hashrate cannot\ncoordinate their action.\n\n\nProposal: Soft Fork Threshold Signaling\n=======================================\n\nMiners signal the threshold at which they are willing to activate a\nsoft fork. The value of the threshold is published in the coinbase\ntransaction of the block, with the corresponding version bit.\n\nMiners activate a soft fork if their threshold has been reached over\nthe last retargeting period. For example, if 504 of 2016 blocks signal\na soft fork with a threshold equal or lower to 25%, then the soft fork\nis activated by these miners.\n\nIf no activation threshold is reached, the current BIP9 signaling rate\nindicator is replaced by a distribution of signaling rates per\nthreshold. The public availability of threshold information allows\nminers to adjust their own threshold, and to optimize their chances of\nactivating the soft fork.\n\n\nUASF\n====\n\nEven if the soft fork is not activated by miners, this proposal will\nreduce the risks associated to a user activated soft fork (UASF). The\npublic availability of hashrate threshold information prior to the\nsoft fork will help miners decide whether they should join the fork\nright after it has been activated, before price information is\navailable.\n\n\nVulnerabilities\n===============\n\nThis proposal has similar vulnerabilities as BIP9: it is susceptible\nto fake signaling by miners, and to miners withholding hashing power\nbefore the fork."
            },
            {
                "author": "Sancho Panza",
                "date": "2017-04-13T14:17:59",
                "message_text_only": "Thomas,\n\nI wonder if you've seen my proposal on how to make BIP9 more configurable:\nhttps://github.com/sanch0panza/bips/blob/bip-genvbvoting/bip-genvbvoting.mediawiki\n\nThis could be extended with a coinbase signaling feature as you suggest.\nThis could include parameter information for forks which a miner is signaling, for coordination.\n\nCurrently I've not included something like this, but it might make a nice addition.\nOne problem is the limited space in coinbase for embedding data on the large number of possible independent deployments.\n\nRegards,\nSancho\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170413/99093746/attachment.html>"
            },
            {
                "author": "Thomas Voegtlin",
                "date": "2017-04-13T14:55:29",
                "message_text_only": "Hi Sancho,\n\nI saw your proposal. However, my point is that the threshold should be\npart of the signaling, and not fixed in the soft-fork proposal.\n\nI agree that coinbase space might be a limitation.\n\nTo avoid this, I realize that the threshold could be encoded in the\nversion bits. We have 32 version bits, and the top 3 bits must be set to\n001 in BIP9. In order to extend BIP9 in a backward compatible way, we\ncould set these 3 top bits to 010, and use the 29 remaining bits for\nsoft fork signaling.\n\nIf we use 7 bits per soft-fork proposal, we have enough space to encode\nfour simultaneous soft-fork proposals, and sub-percent granularity for\nthe threshold (2^7=128).\n\n\n\nLe 13/04/2017 \u00e0 16:17, Sancho Panza a \u00e9crit :\n> Thomas,\n> \n> I wonder if you've seen my proposal on how to make BIP9 more configurable:\n> https://github.com/sanch0panza/bips/blob/bip-genvbvoting/bip-genvbvoting.mediawiki\n> \n> This could be extended with a coinbase signaling feature as you suggest.\n> This could include parameter information for forks which a miner is signaling, for coordination.\n> \n> Currently I've not included something like this, but it might make a nice addition.\n> One problem is the limited space in coinbase for embedding data on the large number of possible independent deployments.\n> \n> Regards,\n> Sancho\n>"
            },
            {
                "author": "Sancho Panza",
                "date": "2017-04-13T16:35:41",
                "message_text_only": "> However, my point is that the threshold should be [...] not fixed in the soft-fork proposal\n\nMy proposal makes it configurable (as well as window size, grace period etc.)\n\n> I agree that coinbase space might be a limitation.\n\nI still like the coinbase idea though - more than using up the BIP9 versionbits range for verbose signaling.\n\nBIP9 (and other proposals which use those 29 versionbits) currently assume that the participants on the network will coordinate in some form or other, to agree on what the bits mean (in terms of change deployments).\n\nIt would be very easy to also agree on a set of \"standard\" threshold levels and map those onto e.g. 1 byte.\n\nThen, in the coinbase, one could have pairs of bit numbers and bytes, e.g. \"/1A/2B/3C/\" where the bytes values corresponding to 'A', 'B', 'C', ... are standardized deployment schedules that people find useful.\nSo a BIP9 conformant schedule could be A = 95% / 2016 window,\nwhile B = 75%/2016, etc.\n\nThis would be quite a compact yet still readable signaling. The space of values is large enough that I doubt we'd see much contention.\n\nRegards\nSancho\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170413/fd2d178a/attachment.html>"
            },
            {
                "author": "Thomas Voegtlin",
                "date": "2017-04-13T17:30:08",
                "message_text_only": "I think it is better not to use the coinbase, because it might collide\nwith other proposals, and because coinbase is not part of the block header.\n\nI agree that a small set of standard threshold may be sufficient; a one\npercent resolution is probably not needed. If we use 4 bits we can\nencode 15 different thresholds + zero (meaning no support). For example\nwe can have all thresholds between 25% and 95% separated by 5%.\n\nUsing 4 bits per soft-fork proposal leaves enough room to fit 7\nsimultaneous proposals in version bits. That should be plenty.\n\n> \n> I still like the coinbase idea though - more than using up the BIP9 versionbits range for verbose signaling.\n> \n> BIP9 (and other proposals which use those 29 versionbits) currently assume that the participants on the network will coordinate in some form or other, to agree on what the bits mean (in terms of change deployments).\n> \n> It would be very easy to also agree on a set of \"standard\" threshold levels and map those onto e.g. 1 byte.\n> \n> Then, in the coinbase, one could have pairs of bit numbers and bytes, e.g. \"/1A/2B/3C/\" where the bytes values corresponding to 'A', 'B', 'C', ... are standardized deployment schedules that people find useful.\n> So a BIP9 conformant schedule could be A = 95% / 2016 window,\n> while B = 75%/2016, etc.\n> \n> This would be quite a compact yet still readable signaling. The space of values is large enough that I doubt we'd see much contention.\n> \n> Regards\n> Sancho\n>"
            }
        ],
        "thread_summary": {
            "title": "Proposal: Soft Fork Threshold Signaling",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Thomas Voegtlin",
                "Sancho Panza"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 10290
        }
    },
    {
        "title": "[bitcoin-dev] Defending against empty or near empty blocks from malicious miner takeover?",
        "thread_messages": [
            {
                "author": "CANNON",
                "date": "2017-04-14T02:22:18",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA512\n\nOn 03/24/2017 04:27 PM, Emin G\u00fcn Sirer wrote:\n> Because there's no consensus on the contents of the mempool, this approach\n> is unsafe and will lead to forks. It also opens a new attack vector where\n> people can time the flood of new transactions with the discovery of a block\n> by a competitor, to orphan the block and to fork the chain.\n> \n\nI know this is a delayed reply.\n\nWithout intending to revive an older thread, my intentions are to clarify\nwhat I have meant in my original post just in case anyone misinterprets \nwhere I said\n\n\"For example would be something like this:\nIf block = (empty OR  <%75 of mempool) THEN discard\nThis threshold just an example.\"\n\nI should have clarified that with this idea blocks would not be rejected if \ndoes not match what that nodes have in their mempool, because as you have said,\nthere is no consensus on the contents of mempool and the mempool will vary from\nnode to node.\n\nInstead what I have meant is that with this idea, nodes would only reject blocks if\nthey are empty or less than a determined percentage when compared to what is in mempool.\n\nWhile this specific defense proposal I posted may or may not be a good idea, was only \nthrowing this idea out there to create discussion on possible defenses against an empty\nor near empty block attack.\n\n\n\n- --\nCannon\nPGP Fingerprint: 2BB5 15CD 66E7 4E28 45DC 6494 A5A2 2879 3F06 E832 \nEmail: cannon at cannon-ciota.info\n\nNOTICE: ALL EMAIL CORRESPONDENCE NOT SIGNED/ENCRYPTED WITH PGP SHOULD \nBE CONSIDERED POTENTIALLY FORGED, AND NOT PRIVATE. \nIf this matters to you, use PGP.\n-----BEGIN PGP SIGNATURE-----\n\niQIcBAEBCgAGBQJY8DIZAAoJEAYDai9lH2mwZKYP/jNJjyTeE09+IlsGolPV3Vp+\nsuJmUK26y8IbEzGxa8eVoX3w7407VNzqeT0jF8vK7oy97EPgszoiutbzYanKYH27\nRpck+FdW/Q5o6jqw59swX+KEvVao52ETPX3kV8ae5uA2txOBnn6C0qZbM5OxPVLN\nIHr7E0+bn9BQVuTzhep1wNWi4cDzyeIjYfRGArBTkuSBKxFtbPmTMLa67qsBGKVu\nJcGYm6rdDO0iVAR9od/Is9b+3gTW49x/3jBEdg7iCHc8KuGOilZaHfyU6xjt3fPo\nL2lxXxUuobFD68/f4ervFVMpAPpmPaS/MEkHMIhJex3szdlSe/WZsQm+2/j799Rg\nBa62pMOYvSR43WwlwX8eySUlVsPtJNtObKnRvDBOmOICgsZ3T9tHKjI+9IPVi9Ib\ns7yBBA1LFw4+c8wirzu1aaeDroJ3icqfU+tRe+nadQN1PMepk6sBUMu13bm8B3E3\nR8oo+jFZRRvJmx7HDDlJX9GHri8hktCNm/gtt0ksWwEgAQHixukmKoDVssAmsiZ4\nBbiWIA3ULciSKM782zDH7/GvDBbOurtV8TeubnV7DDARIA86COwuGjjk30Ltf3ia\n5gnFIicLkmdRMh4AU0jvvEpxrHWFFJmreoR+jnAXHMBGoA6ExVaqR2VQzcpb5SIb\nsqe/5499BqvJqS4ZFn7f\n=q+nx\n-----END PGP SIGNATURE-----"
            }
        ],
        "thread_summary": {
            "title": "Defending against empty or near empty blocks from malicious miner takeover?",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "CANNON"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2419
        }
    },
    {
        "title": "[bitcoin-dev] I do not support the BIP 148 UASF",
        "thread_messages": [
            {
                "author": "Gregory Maxwell",
                "date": "2017-04-14T07:56:31",
                "message_text_only": "I do not support the BIP148 UASF for some of the same reasons that I\ndo support segwit:  Bitcoin is valuable in part because it has high\nsecurity and stability, segwit was carefully designed to support and\namplify that engineering integrity that people can count on now and\ninto the future.\n\nI do not feel the the approach proposed in BIP148 really measures up\nto the standard set by segwit itself, or the existing best practices\nin protocol development in this community.\n\nThe primary flaw in BIP148 is that by forcing the activation of the\nexisting (non-UASF segwit) nodes it almost guarantees at a minor level\nof disruption.\n\nSegwit was carefully engineered so that older unmodified miners could\ncontinue operating _completely_ without interruption after segwit\nactivates.\n\nOlder nodes will not include segwit spends, and so their blocks will\nnot be invalid even if they do not have segwit support. They can\nupgrade to it on their own schedule. The only risk non-participating\nminers take after segwit activation is that if someone else mines an\ninvalid block they would extend it, a risk many miners already\nfrequently take with spy-mining.\n\nI do not think it is a horrible proposal: it is better engineered than\nmany things that many altcoins do, but just not up to our normal\nstandards. I respect the motivations of the authors of BIP 148.  If\nyour goal is the fastest possible segwit activation then it is very\nuseful to exploit the >80% of existing nodes that already support the\noriginal version of segwit.\n\nBut the fastest support should not be our goal, as a community-- there\nis always some reckless altcoin or centralized system that can support\nsomething faster than we can-- trying to match that would only erode\nour distinguishing value in being well engineered and stable.\n\n\"First do no harm.\" We should use the least disruptive mechanisms\navailable, and the BIP148 proposal does not meet that test.  To hear\nsome people-- non-developers on reddit and such-- a few even see the\nforced orphaning of 148 as a virtue, that it's punitive for\nmisbehaving miners. I could not not disagree with that perspective any\nmore strongly.\n\nOf course, I do not oppose the general concept of a UASF but\n_generally_ a soft-fork (of any kind) does not need to risk disruption\nof mining, just as segwit's activation does not.  UASF are the\noriginal kind of soft-fork and were the only kind of fork practiced by\nSatoshi. P2SH was activated based on a date, and all prior ones were\nbased on times or heights.  We introduced miner based activation as\npart of a process of making Bitcoin more stable in the common case\nwhere the ecosystem is all in harmony.  It's kind of weird to see UASF\nportrayed as something new.\n\nIt's important the users not be at the mercy of any one part of the\necosystem to the extent that we can avoid it-- be it developers,\nexchanges, chat forums, or mining hardware makers.  Ultimately the\nrules of Bitcoin work because they're enforced by the users\ncollectively-- that is what makes Bitcoin Bitcoin, it's what makes it\nsomething people can count on: the rules aren't easy to just change.\n\nThere have been some other UASF proposals that avoid the forced\ndisruption-- by just defining a new witness bit and allowing\nnon-upgraded-to-uasf miners and nodes to continue as non-upgraded, I\nthink they are vastly superior. They would be slower to deploy, but I\ndo not think that is a flaw.\n\nWe should have patience. Bitcoin is a system that should last for all\nages and power mankind for a long time-- ten years from now a couple\nyears of dispute will seem like nothing. But the reputation we earn\nfor stability and integrity, for being a system of money people can\ncount on will mean everything.\n\nIf these discussions come up, they'll come up in the form of reminding\npeople that Bitcoin isn't easily changed at a whim, even when the\nwhims are obviously good, and how that protects it from being managed\nlike all the competing systems of money that the world used to use\nwere managed. :)\n\nSo have patience, don't take short cuts.  Segwit is a good improvement\nand we should respect it by knowing that it's good enough to wait for,\nand for however its activated to be done the best way we know how."
            },
            {
                "author": "praxeology_guy",
                "date": "2017-04-14T16:50:47",
                "message_text_only": "Gregory Maxwell,\n\nCriticizing 148 without suggesting a specific alternative leaves the community in disarray.\n\nI know you are emphasizing patience. But at the same time, with your patience we are allowing ourselves to get dicked for longer than necessary.\n\nI think that core could easily develop code that could create a solid/reliable date/height based activation to allow miners to create SegWit block candidates and having nodes fully verify them. Shaolinfry is the only person Ive seen actually make such a proposal: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-April/014049.html. His makes it so that SegWit default gets activated at the end of the BIP9 signalling timeframe instead of default leaving it non-activated.\n\nI agree that 148 is is not ideal. Non-SegWit signaling blocks are not a Denial of Service, given that other activation methods are available. Someone just needs to code something up that is better that we can all use in a satisfying time frame. So far 148 is the most practical and reliable method I'm aware of.\n\nIf 148 causes orphaning and a fork, I don't think such really matters in the long term. The non-SegWit miners will probably just quickly give up their orphans once they realize that money users like being able to have non-mutable TX IDs. If they do create a long lasting branch... well that is good too, I'd be happy to no longer have them in our community. Good luck to them in creating a competitive money, so that we can all enjoy lower transaction fees.\n\nSegWit has already undergone enough testing. It is time to activate it.\n\nCheers,\nPraxeology Guy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170414/a6c9a6ec/attachment.html>"
            },
            {
                "author": "Chris Stewart",
                "date": "2017-04-14T17:36:34",
                "message_text_only": ">Criticizing 148 without suggesting a specific alternative leaves the\ncommunity in disarray.\n\nI really disagree with this sentiment, you don't need to provide\nalternatives to criticize a technical proposal. I don't like this \"active\nsegwit at all costs\" theme that has been going around the community. I am a\nfan of segwit, but we shouldn't push things through in an unsafe manner.\n\n>If 148 causes orphaning and a fork, I don't think such really matters in\nthe long term.  The non-SegWit miners will probably just quickly give up\ntheir orphans once they realize that money users like being able to have\nnon-mutable TX IDs.  If they do create a long lasting branch... well that\nis good too, I'd be happy to no longer have them in our community.  Good\nluck to them in creating a competitive money, so that we can all enjoy\nlower transaction fees.\n\nThis seems like a lot of reckless hand waving to me.\n\nFood for thought, why are we rejecting *all* blocks that do not signal\nsegwit? Can't we just reject blocks that *do not* signal segwit, but *do*\ncontain segwit transactions? It seems silly to me that if a miner mines a\nblock with all pre segwit txs to reject that block. Am I missing something\nhere?\n\n-Chris\n\nOn Fri, Apr 14, 2017 at 11:50 AM, praxeology_guy via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Gregory Maxwell,\n>\n> Criticizing 148 without suggesting a specific alternative leaves the\n> community in disarray.\n>\n> I know you are emphasizing patience.  But at the same time, with your\n> patience we are allowing ourselves to get dicked for longer than necessary.\n>\n> I think that core could easily develop code that could create a\n> solid/reliable date/height based activation to allow miners to create\n> SegWit block candidates and having nodes fully verify them.  Shaolinfry is\n> the only person Ive seen actually make such a proposal:\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/\n> 2017-April/014049.html.  His makes it so that SegWit default gets\n> activated at the end of the BIP9 signalling timeframe instead of default\n> leaving it non-activated.\n>\n> I agree that 148 is is not ideal.  Non-SegWit signaling blocks are not a\n> Denial of Service, given that other activation methods are available.\n> Someone just needs to code something up that is better that we can all use\n> in a satisfying time frame.  So far 148 is the most practical and reliable\n> method I'm aware of.\n>\n> If 148 causes orphaning and a fork, I don't think such really matters in\n> the long term.  The non-SegWit miners will probably just quickly give up\n> their orphans once they realize that money users like being able to have\n> non-mutable TX IDs.  If they do create a long lasting branch... well that\n> is good too, I'd be happy to no longer have them in our community.  Good\n> luck to them in creating a competitive money, so that we can all enjoy\n> lower transaction fees.\n>\n> SegWit has already undergone enough testing.  It is time to activate it.\n>\n> Cheers,\n> Praxeology Guy\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170414/065e4fee/attachment.html>"
            },
            {
                "author": "praxeology_guy",
                "date": "2017-04-14T18:33:39",
                "message_text_only": "Chris,\n\n>Food for thought, why are we rejecting *all* blocks that do not signal segwit? Can't we just reject blocks that *do not* signal segwit, but *do* contain segwit transactions? It seems silly to me that if a miner mines a block with all pre segwit txs to reject that block. Am I missing something here?\n\nIf you read my email, you will see that I am requesting that gmaxwell or someone code up an alternative that doesn't unnecessarily orphan blocks, just as you are requesting.\n\n> Re: old blocks containing SegWit transactions\nFrom my understanding, old blocks can contain txos w/ the new SegWit format. But if transaction tries to spend a new SegWit format txo in an old block, such would already break protocol rules, particularly for SegWit activated nodes. And old nodes don't have code that even knows how to spend SegWit format txos. Worst case, such may lead to a fork if <= 50% of the miners are verifying SegWit blocks.\n\n> Re: Reckless hand waving:\nMaybe first you need to prove that forks are necessarily bad for our long term success. How much do we need to be getting delayed in rolling out new good policy before we come to consensus on forking from the delayers?\n\nThe operating assumption of 148 is that no matter what we are going to fork. So might as well do it then in a controlled manner instead of later when someone creates an invalid SegWit block. Then my only recommendation would be to also implement a boilerplate replay attack prevention just in case the SegWit delayers aren't bluffing.\n\nCheers,\nPraxeology Guy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170414/9c02fbf0/attachment.html>"
            },
            {
                "author": "Tom Zander",
                "date": "2017-04-14T19:12:19",
                "message_text_only": "On Friday, 14 April 2017 18:50:47 CEST praxeology_guy via bitcoin-dev wrote:\n> Criticizing 148 without suggesting a specific alternative leaves the\n> community in disarray.\n\nHere is a list of clear alternatives;\n\nhttps://github.com/bitcoin/bips/\n\nSee the BIPs with number 010[1-8].\n\n-- \nTom Zander\nBlog: https://zander.github.io\nVlog: https://vimeo.com/channels/tomscryptochannel"
            },
            {
                "author": "Tom Zander",
                "date": "2017-04-14T19:20:39",
                "message_text_only": "On Friday, 14 April 2017 09:56:31 CEST Gregory Maxwell via bitcoin-dev \nwrote:\n> Segwit was carefully engineered so that older unmodified miners could\n> continue operating _completely_ without interruption after segwit\n> activates.\n\n\n> They [Older nodes] can\n> upgrade to it [segwit] on their own schedule. The only risk \n> non-participating\n> miners take after segwit activation is that if someone else mines an\n> invalid block they would extend it,\n\nThis is false,\n\na segwit transaction to the miner you describe is an \"everyone can spend\" \ntransaction, and as such a miner that does not validate the segregated area \nin a post-segwit world will be able to create blocks that will not validate \nfor segwit miners by including a transaction that spends a SW tx.\n\nThis would then lead to a chain-fork as the SW miners reject it and the non-\nSW miners continue to mine on it.\n\n\n-- \nTom Zander\nBlog: https://zander.github.io\nVlog: https://vimeo.com/channels/tomscryptochannel"
            },
            {
                "author": "James Hilliard",
                "date": "2017-04-14T19:33:49",
                "message_text_only": "On Fri, Apr 14, 2017 at 2:20 PM, Tom Zander via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> On Friday, 14 April 2017 09:56:31 CEST Gregory Maxwell via bitcoin-dev\n> wrote:\n>> Segwit was carefully engineered so that older unmodified miners could\n>> continue operating _completely_ without interruption after segwit\n>> activates.\n>\n>\n>> They [Older nodes] can\n>> upgrade to it [segwit] on their own schedule. The only risk\n>> non-participating\n>> miners take after segwit activation is that if someone else mines an\n>> invalid block they would extend it,\n>\n> This is false,\n>\n> a segwit transaction to the miner you describe is an \"everyone can spend\"\n> transaction, and as such a miner that does not validate the segregated area\n> in a post-segwit world will be able to create blocks that will not validate\n> for segwit miners by including a transaction that spends a SW tx.\n>\n> This would then lead to a chain-fork as the SW miners reject it and the non-\n> SW miners continue to mine on it.\n\nThis is false,\n\nThose \"everyone can spend\" transactions are prohibited from being\nmined due to policy rules. The risk is only in regards to mining on\ntop of an invalid block that intentionally mined an invalid SW\ntransaction.\n>\n>\n> --\n> Tom Zander\n> Blog: https://zander.github.io\n> Vlog: https://vimeo.com/channels/tomscryptochannel\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Tom Zander",
                "date": "2017-04-14T20:34:26",
                "message_text_only": "On Friday, 14 April 2017 21:33:49 CEST James Hilliard wrote:\n> This is false,\n> \n> Those \"everyone can spend\" transactions are prohibited from being\n> mined due to policy rules.\n\nI expected you to know this, but ok, I'll explain.\n\nA policy rule is not a protocol rule, a mining node is certainly not \nguarenteet to have it, and those that do typically make it configurable.\n\nIf you depend on one implementation and user configuration for the avoidance \nof chain forks, you are going to have a hard time.\n\nThanks for your thoughtful reply, though.\n-- \nTom Zander\nBlog: https://zander.github.io\nVlog: https://vimeo.com/channels/tomscryptochannel"
            },
            {
                "author": "James Hilliard",
                "date": "2017-04-14T20:51:04",
                "message_text_only": "On Fri, Apr 14, 2017 at 3:34 PM, Tom Zander <tomz at freedommail.ch> wrote:\n> On Friday, 14 April 2017 21:33:49 CEST James Hilliard wrote:\n>> This is false,\n>>\n>> Those \"everyone can spend\" transactions are prohibited from being\n>> mined due to policy rules.\n>\n> I expected you to know this, but ok, I'll explain.\n>\n> A policy rule is not a protocol rule, a mining node is certainly not\n> guarenteet to have it, and those that do typically make it configurable.\nYes one can override policy rules and mine invalid SW transactions,\nbut that's not something that's likely to happen accidentally.\n>\n> If you depend on one implementation and user configuration for the avoidance\n> of chain forks, you are going to have a hard time.\nWe don't depend on policy to avoid chain forks, policy however is\nquite useful for making forks smoother since it can prevent miners\nfrom accidentally mining invalid blocks and prevents users from\naccepting invalid transactions accidentally.\nThis doesn't remove the need for consensus rule enforcement of course.\n>\n> Thanks for your thoughtful reply, though.\n> --\n> Tom Zander\n> Blog: https://zander.github.io\n> Vlog: https://vimeo.com/channels/tomscryptochannel"
            },
            {
                "author": "Tom Zander",
                "date": "2017-04-14T20:58:15",
                "message_text_only": "On Friday, 14 April 2017 22:51:04 CEST James Hilliard wrote:\n> This doesn't remove the need for consensus rule enforcement of course.\n\nThanks for confirming my point.\n\nThis means that Gregory was incorrect saying that there is no risk to a non-\nupgraded node on a SegWit network mining a new invalid block. That risk is \nmost definitely there for any miners \"left behind\" operating on a different \nset of consensus rules than the majority.\n\nKind of obvious, when you think about it.\n\n-- \nTom Zander\nBlog: https://zander.github.io\nVlog: https://vimeo.com/channels/tomscryptochannel"
            },
            {
                "author": "James Hilliard",
                "date": "2017-04-14T21:10:46",
                "message_text_only": "On Fri, Apr 14, 2017 at 3:58 PM, Tom Zander via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> On Friday, 14 April 2017 22:51:04 CEST James Hilliard wrote:\n>> This doesn't remove the need for consensus rule enforcement of course.\n>\n> Thanks for confirming my point.\n>\n> This means that Gregory was incorrect saying that there is no risk to a non-\n> upgraded node on a SegWit network mining a new invalid block. That risk is\n> most definitely there for any miners \"left behind\" operating on a different\n> set of consensus rules than the majority.\nGreg is correct. There is effectively no risk to a non-upgrade\naccidentally mining a new invalid block itself, the only risk is that\na non-upgraded miner could itself mine on top of an invalid block. You\nwould have to intentionally modify the code to mine an invalid block\nwhich is not something that would be likely to happen accidentally.\n>\n> Kind of obvious, when you think about it.\n>\n> --\n> Tom Zander\n> Blog: https://zander.github.io\n> Vlog: https://vimeo.com/channels/tomscryptochannel\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2017-04-14T21:12:47",
                "message_text_only": "On Fri, Apr 14, 2017 at 9:10 PM, James Hilliard via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> would have to intentionally modify the code to mine an invalid block\n> which is not something that would be likely to happen accidentally.\n\nIIRC-- If you do it accidentally you'll fail the tests, though there\nhave been a couple reckless alternative implementations that have just\nripped out most of the tests...\n\nIn any case there is no need to speculate or guess-- invalid segwit\nspends are not being mined today..."
            },
            {
                "author": "Gregory Maxwell",
                "date": "2017-04-14T20:59:55",
                "message_text_only": "On Fri, Apr 14, 2017 at 8:34 PM, Tom Zander via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> I expected you to know this, but ok, I'll explain.\n\nPlease stop abusing participants on this list. Your activity is\nactively driving people off this list.\n\nJames Hilliard should be commended for correcting your misinformation.\n\n> If you depend on one implementation and user configuration for the avoidance\n> of chain forks, you are going to have a hard time.\n\nAnyone can modify their software to produce invalid blocks at any\ntime. If they want to be stupid, they can be stupid.\n\nThe fact remains that miners who haven't gone and wreaked their\nsoftware internals will not mine segwit incompatible blocks. Right now\n_no_ observable has broken node in this way."
            },
            {
                "author": "Steven Pine",
                "date": "2017-04-15T02:01:17",
                "message_text_only": "> Segwit is a good improvement\nand we should respect it by knowing that it's good enough to wait for,\nand for however its activated to be done the best way we know how.\n\nRegarding this last point I was under the impression that if Segwit did not\nactivate by November then core was going to move on, is that no longer the\ncase, does the core team plan on trying to activate Segwit in some other\nway?\n\nI am also curious, but has there been a softfork, hardfork, or other major\ncensus change that was not rolled out and done by the core team? I only\nmention this because BIP148, if it goes ahead (and is successful), would be\nthe first time a consensus change occurs outside of the core developers --\nbut again I am not an expert on the history of changes and could be wrong,\nI only bring this up because core developers have in the past stressed they\nare a part of the bitcoin ecosystem and not the drivers of it (at least\nthat is the ideal it seems).\n\nMy impression is that the community is ready for this and wants it, and if\nthat impression is correct it will go ahead. No one knows the future, and\nsimply assuming it's better to be slow and methodical isn't especially\nconvincing. Technology is in someways the history of failure, we like to\ncelebrate the seemingly sudden breakthroughs and successes but it's rare\nthat the original innovator retains a monopoly on their invention, more\noften it becomes quickly refined and iterated upon as market forces take\nhold to bring costs down and other external political issues\ntake precedence, all this is say that in ten years everyone could be\nchuckling over the 3 year bitcoin scaling debate, or they could be using\nlitecoin, or ethereum or some other crypto coin, or something entirely\ndifferent, no one knows.\n\nOn Fri, Apr 14, 2017 at 3:56 AM, Gregory Maxwell via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> I do not support the BIP148 UASF for some of the same reasons that I\n> do support segwit:  Bitcoin is valuable in part because it has high\n> security and stability, segwit was carefully designed to support and\n> amplify that engineering integrity that people can count on now and\n> into the future.\n>\n> I do not feel the the approach proposed in BIP148 really measures up\n> to the standard set by segwit itself, or the existing best practices\n> in protocol development in this community.\n>\n> The primary flaw in BIP148 is that by forcing the activation of the\n> existing (non-UASF segwit) nodes it almost guarantees at a minor level\n> of disruption.\n>\n> Segwit was carefully engineered so that older unmodified miners could\n> continue operating _completely_ without interruption after segwit\n> activates.\n>\n> Older nodes will not include segwit spends, and so their blocks will\n> not be invalid even if they do not have segwit support. They can\n> upgrade to it on their own schedule. The only risk non-participating\n> miners take after segwit activation is that if someone else mines an\n> invalid block they would extend it, a risk many miners already\n> frequently take with spy-mining.\n>\n> I do not think it is a horrible proposal: it is better engineered than\n> many things that many altcoins do, but just not up to our normal\n> standards. I respect the motivations of the authors of BIP 148.  If\n> your goal is the fastest possible segwit activation then it is very\n> useful to exploit the >80% of existing nodes that already support the\n> original version of segwit.\n>\n> But the fastest support should not be our goal, as a community-- there\n> is always some reckless altcoin or centralized system that can support\n> something faster than we can-- trying to match that would only erode\n> our distinguishing value in being well engineered and stable.\n>\n> \"First do no harm.\" We should use the least disruptive mechanisms\n> available, and the BIP148 proposal does not meet that test.  To hear\n> some people-- non-developers on reddit and such-- a few even see the\n> forced orphaning of 148 as a virtue, that it's punitive for\n> misbehaving miners. I could not not disagree with that perspective any\n> more strongly.\n>\n> Of course, I do not oppose the general concept of a UASF but\n> _generally_ a soft-fork (of any kind) does not need to risk disruption\n> of mining, just as segwit's activation does not.  UASF are the\n> original kind of soft-fork and were the only kind of fork practiced by\n> Satoshi. P2SH was activated based on a date, and all prior ones were\n> based on times or heights.  We introduced miner based activation as\n> part of a process of making Bitcoin more stable in the common case\n> where the ecosystem is all in harmony.  It's kind of weird to see UASF\n> portrayed as something new.\n>\n> It's important the users not be at the mercy of any one part of the\n> ecosystem to the extent that we can avoid it-- be it developers,\n> exchanges, chat forums, or mining hardware makers.  Ultimately the\n> rules of Bitcoin work because they're enforced by the users\n> collectively-- that is what makes Bitcoin Bitcoin, it's what makes it\n> something people can count on: the rules aren't easy to just change.\n>\n> There have been some other UASF proposals that avoid the forced\n> disruption-- by just defining a new witness bit and allowing\n> non-upgraded-to-uasf miners and nodes to continue as non-upgraded, I\n> think they are vastly superior. They would be slower to deploy, but I\n> do not think that is a flaw.\n>\n> We should have patience. Bitcoin is a system that should last for all\n> ages and power mankind for a long time-- ten years from now a couple\n> years of dispute will seem like nothing. But the reputation we earn\n> for stability and integrity, for being a system of money people can\n> count on will mean everything.\n>\n> If these discussions come up, they'll come up in the form of reminding\n> people that Bitcoin isn't easily changed at a whim, even when the\n> whims are obviously good, and how that protects it from being managed\n> like all the competing systems of money that the world used to use\n> were managed. :)\n>\n> So have patience, don't take short cuts.  Segwit is a good improvement\n> and we should respect it by knowing that it's good enough to wait for,\n> and for however its activated to be done the best way we know how.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n\n\n\n-- \nSteven Pine\n(510) 517-7075\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170414/1bf09a08/attachment.html>"
            },
            {
                "author": "Chris Stewart",
                "date": "2017-04-15T03:05:25",
                "message_text_only": ">Regarding this last point I was under the impression that if Segwit did\nnot activate by November then core was going to move on, is that no longer\nthe case, does the core team plan on trying to activate Segwit in some\nother way?\n\nSince block size seems to be the controversial issue, AFAIK we *could*\nremove the block size increase (by removing the discount on signature\ndata). This discount was put in place for two reasons\n\n1.) It allows for a block size increase\n2.) It makes it more expensive to create UTXOs. UTXO bloat is a problem on\nthe bitcoin network and segwit was an elegant way to make the network\nappreciate their real cost in terms of hardware/RAM.\n\nWe would still get the benefits of:\n1.) Tx malleability elimination\n2.) Script versioning\n\n-Chris\n\nOn Fri, Apr 14, 2017 at 9:01 PM, Steven Pine via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> > Segwit is a good improvement\n> and we should respect it by knowing that it's good enough to wait for,\n> and for however its activated to be done the best way we know how.\n>\n> Regarding this last point I was under the impression that if Segwit did\n> not activate by November then core was going to move on, is that no longer\n> the case, does the core team plan on trying to activate Segwit in some\n> other way?\n>\n> I am also curious, but has there been a softfork, hardfork, or other major\n> census change that was not rolled out and done by the core team? I only\n> mention this because BIP148, if it goes ahead (and is successful), would be\n> the first time a consensus change occurs outside of the core developers --\n> but again I am not an expert on the history of changes and could be wrong,\n> I only bring this up because core developers have in the past stressed they\n> are a part of the bitcoin ecosystem and not the drivers of it (at least\n> that is the ideal it seems).\n>\n> My impression is that the community is ready for this and wants it, and if\n> that impression is correct it will go ahead. No one knows the future, and\n> simply assuming it's better to be slow and methodical isn't especially\n> convincing. Technology is in someways the history of failure, we like to\n> celebrate the seemingly sudden breakthroughs and successes but it's rare\n> that the original innovator retains a monopoly on their invention, more\n> often it becomes quickly refined and iterated upon as market forces take\n> hold to bring costs down and other external political issues\n> take precedence, all this is say that in ten years everyone could be\n> chuckling over the 3 year bitcoin scaling debate, or they could be using\n> litecoin, or ethereum or some other crypto coin, or something entirely\n> different, no one knows.\n>\n> On Fri, Apr 14, 2017 at 3:56 AM, Gregory Maxwell via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> I do not support the BIP148 UASF for some of the same reasons that I\n>> do support segwit:  Bitcoin is valuable in part because it has high\n>> security and stability, segwit was carefully designed to support and\n>> amplify that engineering integrity that people can count on now and\n>> into the future.\n>>\n>> I do not feel the the approach proposed in BIP148 really measures up\n>> to the standard set by segwit itself, or the existing best practices\n>> in protocol development in this community.\n>>\n>> The primary flaw in BIP148 is that by forcing the activation of the\n>> existing (non-UASF segwit) nodes it almost guarantees at a minor level\n>> of disruption.\n>>\n>> Segwit was carefully engineered so that older unmodified miners could\n>> continue operating _completely_ without interruption after segwit\n>> activates.\n>>\n>> Older nodes will not include segwit spends, and so their blocks will\n>> not be invalid even if they do not have segwit support. They can\n>> upgrade to it on their own schedule. The only risk non-participating\n>> miners take after segwit activation is that if someone else mines an\n>> invalid block they would extend it, a risk many miners already\n>> frequently take with spy-mining.\n>>\n>> I do not think it is a horrible proposal: it is better engineered than\n>> many things that many altcoins do, but just not up to our normal\n>> standards. I respect the motivations of the authors of BIP 148.  If\n>> your goal is the fastest possible segwit activation then it is very\n>> useful to exploit the >80% of existing nodes that already support the\n>> original version of segwit.\n>>\n>> But the fastest support should not be our goal, as a community-- there\n>> is always some reckless altcoin or centralized system that can support\n>> something faster than we can-- trying to match that would only erode\n>> our distinguishing value in being well engineered and stable.\n>>\n>> \"First do no harm.\" We should use the least disruptive mechanisms\n>> available, and the BIP148 proposal does not meet that test.  To hear\n>> some people-- non-developers on reddit and such-- a few even see the\n>> forced orphaning of 148 as a virtue, that it's punitive for\n>> misbehaving miners. I could not not disagree with that perspective any\n>> more strongly.\n>>\n>> Of course, I do not oppose the general concept of a UASF but\n>> _generally_ a soft-fork (of any kind) does not need to risk disruption\n>> of mining, just as segwit's activation does not.  UASF are the\n>> original kind of soft-fork and were the only kind of fork practiced by\n>> Satoshi. P2SH was activated based on a date, and all prior ones were\n>> based on times or heights.  We introduced miner based activation as\n>> part of a process of making Bitcoin more stable in the common case\n>> where the ecosystem is all in harmony.  It's kind of weird to see UASF\n>> portrayed as something new.\n>>\n>> It's important the users not be at the mercy of any one part of the\n>> ecosystem to the extent that we can avoid it-- be it developers,\n>> exchanges, chat forums, or mining hardware makers.  Ultimately the\n>> rules of Bitcoin work because they're enforced by the users\n>> collectively-- that is what makes Bitcoin Bitcoin, it's what makes it\n>> something people can count on: the rules aren't easy to just change.\n>>\n>> There have been some other UASF proposals that avoid the forced\n>> disruption-- by just defining a new witness bit and allowing\n>> non-upgraded-to-uasf miners and nodes to continue as non-upgraded, I\n>> think they are vastly superior. They would be slower to deploy, but I\n>> do not think that is a flaw.\n>>\n>> We should have patience. Bitcoin is a system that should last for all\n>> ages and power mankind for a long time-- ten years from now a couple\n>> years of dispute will seem like nothing. But the reputation we earn\n>> for stability and integrity, for being a system of money people can\n>> count on will mean everything.\n>>\n>> If these discussions come up, they'll come up in the form of reminding\n>> people that Bitcoin isn't easily changed at a whim, even when the\n>> whims are obviously good, and how that protects it from being managed\n>> like all the competing systems of money that the world used to use\n>> were managed. :)\n>>\n>> So have patience, don't take short cuts.  Segwit is a good improvement\n>> and we should respect it by knowing that it's good enough to wait for,\n>> and for however its activated to be done the best way we know how.\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n>\n>\n> --\n> Steven Pine\n> (510) 517-7075\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170414/e2994657/attachment-0001.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2017-04-15T03:29:10",
                "message_text_only": "On Sat, Apr 15, 2017 at 2:01 AM, Steven Pine via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Regarding this last point I was under the impression that if Segwit did not\n> activate by November then core was going to move on, is that no longer the\n\nWow. Where did you get that idea? That is _absurd_ and untrue, and I\nstruggle a bit to even comprehend how someone could believe it.  It\nwould continue until something clearly better came along or people\nlost interest in it, why would it be anything else?\n\n> census change that was not rolled out and done by the core team? I only\n> mention this because BIP148, if it goes ahead (and is successful), would be\n> the first time a consensus change occurs outside of the core developers --\n> but again I am not an expert on the history of changes and could be wrong, I\n\nThere is a definitional issue there. There isn't much of \"the core\nteam\" there is a lot of amorphous public collaboration; everything\nends up being retroactively defined as the core team.  With open\nparticipation and hundreds of contributors and software running\neverywhere in the network, its unlikely that someone would advance to\nthe point of being able to make a credible proposal without at some\npoint making some improvement to the project or without the help of\nsomeone who has.\n\nIn some sense you are coming very close to asking for a list of people\nwho have contributed to Bitcoin without contributing to Bitcoin.\n\nCLTV was a proposal by Peter Todd whom has done a number of other\nthings in core but AFAIR had no involvement in any prior soft-fork\n(though perhaps I'm forgetting one?), though he subsequently\ncontributed to BIP66 (which activated before CLTV), and he contributed\nmostly after-the fact review of segwit. CSV was mostly the work of\nMark Friedenbach whom I believe was not involved in any prior or\nsubsequent soft-fork (and whos total contributions to Bitcoin core\nweigh in at 14 commits over 5 years).\n\n> My impression is that the community is ready for this and wants it, and if\n> that impression is correct it will go ahead. No one knows the future, and\n> simply assuming it's better to be slow and methodical isn't especially\n\nI am not suggesting slow. I am suggesting that we not be outright\nreckless. Some people are expecting changes which are effectively\norders of magnitude faster than changes in centralized systems\nelsewhere which are far easier and safer to take quickly.\n\n(Some more comparatives here:\nhttps://www.reddit.com/r/Bitcoin/comments/65bch8/gregory_maxwell_i_do_not_support_the_bip_148_uasf/dg9xfam/\n)\n\n> Technology is in someways the history of failure,\n\nBy all means, take risks-- but you don't get to choose to make other\npeoples things fail; you certainly don't get to demand their support,\nthough you could try to earn it if you care, by figuring out how to\nmeet their concerns."
            },
            {
                "author": "Steven Pine",
                "date": "2017-04-15T04:10:26",
                "message_text_only": "I don't want to be rude and I will refer to your expertise, but segwit does\nhave a 'time out' as defined in BIP 9 with the date of November 15th? Does\ncore plan on just releasing another BIP with a new timeout hoping it will\neventually get 95% census?\n\nAs for the other point, we can play semantics but that's boring, I guess my\nmeaning was every census change has gone through a core defined process\n(not counting the changes that occurred before there were BIPs and such),\nisn't that the case? If the currently discussed UASF goes through it would\nseem like the first time census occurred outside core's mailing list of\npull requests, acks, and merge to master, I only note it as a thing of\ninterest.\n\nTo be clear, the fast and reckless part for you is the mechanism by which\nsegwit could possibly be made active? Do you envision a means of segwit\nbeing made consensus that does not have 95% mining support?\n\nI appreciate your time and expertise, and to not take up anymore, back to\nlurking i go.\n\n\nOn Fri, Apr 14, 2017 at 11:29 PM, Gregory Maxwell <greg at xiph.org> wrote:\n\n> On Sat, Apr 15, 2017 at 2:01 AM, Steven Pine via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > Regarding this last point I was under the impression that if Segwit did\n> not\n> > activate by November then core was going to move on, is that no longer\n> the\n>\n> Wow. Where did you get that idea? That is _absurd_ and untrue, and I\n> struggle a bit to even comprehend how someone could believe it.  It\n> would continue until something clearly better came along or people\n> lost interest in it, why would it be anything else?\n>\n> > census change that was not rolled out and done by the core team? I only\n> > mention this because BIP148, if it goes ahead (and is successful), would\n> be\n> > the first time a consensus change occurs outside of the core developers\n> --\n> > but again I am not an expert on the history of changes and could be\n> wrong, I\n>\n> There is a definitional issue there. There isn't much of \"the core\n> team\" there is a lot of amorphous public collaboration; everything\n> ends up being retroactively defined as the core team.  With open\n> participation and hundreds of contributors and software running\n> everywhere in the network, its unlikely that someone would advance to\n> the point of being able to make a credible proposal without at some\n> point making some improvement to the project or without the help of\n> someone who has.\n>\n> In some sense you are coming very close to asking for a list of people\n> who have contributed to Bitcoin without contributing to Bitcoin.\n>\n> CLTV was a proposal by Peter Todd whom has done a number of other\n> things in core but AFAIR had no involvement in any prior soft-fork\n> (though perhaps I'm forgetting one?), though he subsequently\n> contributed to BIP66 (which activated before CLTV), and he contributed\n> mostly after-the fact review of segwit. CSV was mostly the work of\n> Mark Friedenbach whom I believe was not involved in any prior or\n> subsequent soft-fork (and whos total contributions to Bitcoin core\n> weigh in at 14 commits over 5 years).\n>\n> > My impression is that the community is ready for this and wants it, and\n> if\n> > that impression is correct it will go ahead. No one knows the future, and\n> > simply assuming it's better to be slow and methodical isn't especially\n>\n> I am not suggesting slow. I am suggesting that we not be outright\n> reckless. Some people are expecting changes which are effectively\n> orders of magnitude faster than changes in centralized systems\n> elsewhere which are far easier and safer to take quickly.\n>\n> (Some more comparatives here:\n> https://www.reddit.com/r/Bitcoin/comments/65bch8/gregory_maxwell_i_do_not_\n> support_the_bip_148_uasf/dg9xfam/\n> )\n>\n> > Technology is in someways the history of failure,\n>\n> By all means, take risks-- but you don't get to choose to make other\n> peoples things fail; you certainly don't get to demand their support,\n> though you could try to earn it if you care, by figuring out how to\n> meet their concerns.\n>\n\n\n\n-- \nSteven Pine\n(510) 517-7075\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170415/e046d7dd/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2017-04-15T04:47:43",
                "message_text_only": "On Sat, Apr 15, 2017 at 4:10 AM, Steven Pine <steven.pine at gmail.com> wrote:\n> but segwit does\n> have a 'time out' as defined in BIP 9 with the date of November 15th?\n\nThere is a technical requirement that BIP 9 bit allocations must have\na timeout so that a bit is not forever burned if a proposal is ever\nabandoned (e.g. because something better came along before it\nactivated).  This isn't a timeout for the proposal, but for the bit\nassignment.  If a proposal hasn't activated but there is still\ninterest it will just get a new bit (and can alternate back and forth\nbetween a pair). This is a timeout of the bit, not the proposal.\n\nIt has to be setup this way because there is no real way to\ncommunicate abandonment to old software, so a timeout must be set in\nadvance.\n\n> Does core plan\n\n\"Core\" doesn't plan on much of anything beyond the immediate pipeline\nof activities, similar to other large open source collaboration, or\nopen standards development organizations. It isn't a company.\nIndividuals have plans about their own work which they may collaborate\nin one place or another.\n\nBut allocating a new bit is how BIP9 works.\n\n> meaning was every census change has gone through a core defined process (not\n> counting the changes that occurred before there were BIPs and such), isn't\n\nWhat is a \"core defined process\"?  BIP _itself_ was created by someone\nwho, AFAICT, has never made a commit to Bitcoin Core.  Numbers are\ncurrently assigned, a nearly judgement-less administrative task, by\nsomeone that authors competing fork of the software (Knots).\n\n> it would seem like the first time census occurred outside core\n\nYet it was proposed on this list, had a BIP defined... if it got\neventually used it would presumably end up in the Bitcoin Core project\neventually... so what exactly is your definition of outside? Above you\nseemed to be saying a BIP was not outside, but here you are saying\nsomething documented as a BIP is outside?\n\nIf your preference is to not insult then it may be advisable to not\ndisregard distinctions which you do not understand as semantics. :) I\nam not prone to arguing over semantics-- the continually binning in\nalmost all public collaboration as the work of some centralized entity\nis really harmful to our community. The distinction is real, and not\nsemantics.\n\n> To be clear, the fast and reckless part for you is the mechanism by which\n> segwit could possibly be made active? Do you envision a means of segwit\n> being made consensus that does not have 95% mining support?\n\nSure, and I said so directly in my message.  I believe I was\nadequately clear that my complaint about BIP148 is specifically that\nit has forced orphaning of passive participants which can be easily\navoided but at the expense of actually needed users to adopt the\nchange.\n\nFor clarity, it could be summarized as: I would not classify BIP148 as\na user activated soft-fork but instead as \"user enforced miner\nsoft-fork activation\". The consequence of this is that it likely\ncannot achieve low disruptiveness-- this limitation would be excusable\nif we weren't aware of any alternative, but in this case we are and\nthe only relative downside of it is that users will need to upgrade\nfor it-- which should not be a problem in principle if we believe a\nUASF is really user activated."
            },
            {
                "author": "Cameron Garnham",
                "date": "2017-04-15T06:28:41",
                "message_text_only": "Hello,\n\nIt is hard for me to come out disagreeing with Maxwell, however in this case I feel I must.\n\nAs many may remember, there was quite some controversy about the BIP16 vs BIP 17 split; the main argument for BIP16 was the urgency of P2SH, and how this was the already \u201ctested and proven to work\u201d solution.\n\nI was one of the man hold-out supporters of BIP17, not for any clear reason (I now have a much better technical understanding of the Bitcoin technical details, as we all do); But because it was the \u2018more elegant\u2019 solution.  I knew from other fields of engineering that elegant solutions very often better deal with the \u2018unknown, unknowns\u2019.  I also didn\u2019t agree with Gavin\u2019s \u2018the sky is falling\u2019 sense of urgency.\n\nHowever, of-course the community got behind BIP16, it was activated, fortunately, without any signifiant incident.\n\nI did learn that in Bitcoin there is something more valuable than technical elegance: that is community buy-in. On the technical side, the engineers need to make sure the solutions are viable: however on the community side we need to make sure that the good solutions are adopted in a reasonable timeframe.\n\nIt is both my empirical view and heart-felt belief that the wider Bitcoin Community wants SegWit quickly. In this case the sacrifice of some technical elegance and correctness for expediency is prudent!\n\nIt is my unfortunate view that Maxwell is missing the political forest for the technical trees.  Not only is SegWit a technical solution to technical problems; it has come to represent, by the larger Bitcoin Community, a political solution to the conflict that we are waist-deep in every, single, day.\n\nBIP 148 is out terms of peace.  The Bitcoin Community is tired-to-death of this war and wants a resolution swiftly. BIP 148 proves a outlet, and in Maxwell words: \u201c...almost guarantees at a minor level of disruption.\u201d.\n\nI am willing to go through this minor level of disruption, as the daily disruption from the \u201cscaling debate war\u201d; in my personal online life, is far greater.\n\nSegWit is a exceptional feat of engineering, it solves and mitigates so many small and highly subtle issues within Bitcoin; yet still managed to be simple enough successfully reviewed: now the community is clearly calling for a quick activation of the \u2018viable\u2019 technical choice.\n\nIf you/we are going to provide any engineering solution to activating SegWit, then Swiftness should be the 1st priority after viability.\n\nBIP 148 is both Swift and Viable.\n\nCameron.\n\n\n\n> On 14 Apr 2017, at 10:56 AM, Gregory Maxwell via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> I do not support the BIP148 UASF for some of the same reasons that I\n> do support segwit:  Bitcoin is valuable in part because it has high\n> security and stability, segwit was carefully designed to support and\n> amplify that engineering integrity that people can count on now and\n> into the future.\n> \n> I do not feel the the approach proposed in BIP148 really measures up\n> to the standard set by segwit itself, or the existing best practices\n> in protocol development in this community.\n> \n> The primary flaw in BIP148 is that by forcing the activation of the\n> existing (non-UASF segwit) nodes it almost guarantees at a minor level\n> of disruption.\n> \n> Segwit was carefully engineered so that older unmodified miners could\n> continue operating _completely_ without interruption after segwit\n> activates.\n> \n> Older nodes will not include segwit spends, and so their blocks will\n> not be invalid even if they do not have segwit support. They can\n> upgrade to it on their own schedule. The only risk non-participating\n> miners take after segwit activation is that if someone else mines an\n> invalid block they would extend it, a risk many miners already\n> frequently take with spy-mining.\n> \n> I do not think it is a horrible proposal: it is better engineered than\n> many things that many altcoins do, but just not up to our normal\n> standards. I respect the motivations of the authors of BIP 148.  If\n> your goal is the fastest possible segwit activation then it is very\n> useful to exploit the >80% of existing nodes that already support the\n> original version of segwit.\n> \n> But the fastest support should not be our goal, as a community-- there\n> is always some reckless altcoin or centralized system that can support\n> something faster than we can-- trying to match that would only erode\n> our distinguishing value in being well engineered and stable.\n> \n> \"First do no harm.\" We should use the least disruptive mechanisms\n> available, and the BIP148 proposal does not meet that test.  To hear\n> some people-- non-developers on reddit and such-- a few even see the\n> forced orphaning of 148 as a virtue, that it's punitive for\n> misbehaving miners. I could not not disagree with that perspective any\n> more strongly.\n> \n> Of course, I do not oppose the general concept of a UASF but\n> _generally_ a soft-fork (of any kind) does not need to risk disruption\n> of mining, just as segwit's activation does not.  UASF are the\n> original kind of soft-fork and were the only kind of fork practiced by\n> Satoshi. P2SH was activated based on a date, and all prior ones were\n> based on times or heights.  We introduced miner based activation as\n> part of a process of making Bitcoin more stable in the common case\n> where the ecosystem is all in harmony.  It's kind of weird to see UASF\n> portrayed as something new.\n> \n> It's important the users not be at the mercy of any one part of the\n> ecosystem to the extent that we can avoid it-- be it developers,\n> exchanges, chat forums, or mining hardware makers.  Ultimately the\n> rules of Bitcoin work because they're enforced by the users\n> collectively-- that is what makes Bitcoin Bitcoin, it's what makes it\n> something people can count on: the rules aren't easy to just change.\n> \n> There have been some other UASF proposals that avoid the forced\n> disruption-- by just defining a new witness bit and allowing\n> non-upgraded-to-uasf miners and nodes to continue as non-upgraded, I\n> think they are vastly superior. They would be slower to deploy, but I\n> do not think that is a flaw.\n> \n> We should have patience. Bitcoin is a system that should last for all\n> ages and power mankind for a long time-- ten years from now a couple\n> years of dispute will seem like nothing. But the reputation we earn\n> for stability and integrity, for being a system of money people can\n> count on will mean everything.\n> \n> If these discussions come up, they'll come up in the form of reminding\n> people that Bitcoin isn't easily changed at a whim, even when the\n> whims are obviously good, and how that protects it from being managed\n> like all the competing systems of money that the world used to use\n> were managed. :)\n> \n> So have patience, don't take short cuts.  Segwit is a good improvement\n> and we should respect it by knowing that it's good enough to wait for,\n> and for however its activated to be done the best way we know how.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2017-04-15T07:04:45",
                "message_text_only": "On Sat, Apr 15, 2017 at 6:28 AM, Cameron Garnham <da2ce7 at gmail.com> wrote:\n> As many may remember, there was quite some controversy about the BIP16 vs BIP 17 split; the main argument for BIP16 was the urgency of P2SH, and how this was the already \u201ctested and proven to work\u201d solution.\n\nAnd as a result we ultimately got a clearly inferior solution (520\nbyte script limit; 80-bit security; months of orphaned blocks-- and\ntwo of those were not issues in BIP17).  I went along for the cram\nfest on 16 after 12 caught fire, and I was mistaken to do so.\n\nDoubly so because it took years for P2SH to achieve any kind of mass\ndeployment due to issues far away from consensus.  An extra two months\nspent on some ground-work (including communications and documentation)\ncould have pulled forward practical deployment by a year and given\ntime to find and fix some of the flaws in the design of P2SH.\n\n> BIP 148 is out (our?) terms of peace.  The Bitcoin Community is tired-to-death of this war and wants a resolution swiftly. BIP 148 proves a outlet, and in Maxwell words: \u201c...almost guarantees at a minor level of disruption.\u201d.\n\nIt seems I lost a word in my comment: that should have been \"almost\nguarantees at _least_ a minor level of disruption\". A minor level of\ndisruption is the _minimum_ amount of disruption, and for no good\nreason except an unprecedented and unjustified level of haste.\n\nConsidering that you did not spare a single word about the specific\nproperty that I am concerned about-- that the proposal will reject the\nblocks of passive participants, due to avoidable design limitations--\nI can't help but feel that you don't even care to understand the\nconcern I was bringing up. :(\n\nHow many people barely reviewed the specifics of the proposal simply\nbecause they want something fast and this proposal does something\nfast?\n\n> tired-to-death of this war and wants a resolution swiftly\n\nBy now competitors and opponents to Bitcoin have surely realized that\nthey can attack Bitcoin by stirring up drama.\n\nAs a result, the only way that we will ever be free from \"war\" is if\nwe choose to not let it impact us as much as possible. We must be\nimperturbable and continue working at the same level of excellence as\nif virtual shells weren't flying overhead-- or otherwise there is an\nincentive to keep them flying 24/7. Internet drama is remarkably cheap\nto generate. \"The only thing we have to fear is fear itself\".\n\nThe alternative is that we hand opponents a ready made formula for\ndisruption: astroturf enough drama up that Bitcoiners \"sacrifice\ncorrectness\" themselves right off a cliff in a futile attempt to make\nit go away. :)"
            },
            {
                "author": "Chris Acheson",
                "date": "2017-04-15T07:46:47",
                "message_text_only": "On 04/15/2017 03:04 AM, Gregory Maxwell via bitcoin-dev wrote:\n> Considering that you did not spare a single word about the specific \n> property that I am concerned about-- that the proposal will reject\n> the blocks of passive participants, due to avoidable design\n> limitations-- I can't help but feel that you don't even care to\n> understand the concern I was bringing up. :(\n\nNot sure if you missed my previous reply to you, but I'm curious about\nyour thoughts on this particular point. I contend that for any UASF,\norphaning non-signalling blocks on the flag date is safer than just\nconsidering the fork active on the flag date.\n\nUnless we have majority miner support for the fork, we have to assume\nthat a chain split will occur at some point. With the orphaning\napproach, we know exactly when that will be, and can plan around it.\nMiners know that they need to upgrade by the flag date in order to get\npaid. We even have an opportunity to back out if it looks like we don't\nhave enough economic support.\n\nWith the non-orphaning approach, the split won't occur until someone\nchooses to craft a malicious block (short bitcoin; rent hash power;\nprofit). We don't know when that will be, so we can't plan around it.\nSome nodes and miners will assume it won't happen at all. When it\nhappens, our responses to it will be clumsy, uncoordinated, and likely\npanicked.\n\nWhile the orphaning approach is potentially disruptive to miners, it is\nnecessarily so in order to minimize disruption to users. In general,\nusers should be prioritized over miners. The point of Bitcoin is to have\nsecure, digital money that we can *use*, not to enable people to earn\nmoney from running busy-work computations.\n\n> How many people barely reviewed the specifics of the proposal simply \n> because they want something fast and this proposal does something \n> fast?\n\nI have scrutinized the strategy of BIP148 a fair bit. I was initially\nopposed to it, but after Bitfury showed their support, and especially\nafter the Asicboost revelation, I think it has solid potential to\nsucceed. It would be a waste not to at least attempt to organize around\nit. If it turns out that we can't get the necessary support in time, we\ncan abandon the effort and reassess our options."
            },
            {
                "author": "Natanael",
                "date": "2017-04-15T13:23:35",
                "message_text_only": "Den 15 apr. 2017 13:51 skrev \"Chris Acheson via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org>:\n\n\nNot sure if you missed my previous reply to you, but I'm curious about\nyour thoughts on this particular point. I contend that for any UASF,\norphaning non-signalling blocks on the flag date is [maybe] safer [for\nthose in on the UASF fork] than just\nconsidering the fork active on the flag date.\n\n\nNote my additions.\n\nEnforcement by orphaning non-compliance makes it harder to reverse a buggy\nsoftfork, since you necessarily increase the effort needed to return enough\nmining power to the safe chain since you now have mostly unmonitored mining\nhardware fighting you actively, whose operators you might not be able to\ncontact. You'd practically have to hardfork out of the situation.\n\nThere's also the risk of the activation itself triggering concensus bugs\n(multiple incompatible UASF forks), if there's multiple implementations of\nit in the network (or one buggy one). We have already seen something like\nit happen. This can both happen on the miner side, client side or both\n(miner side only would lead to a ton of orphaned blocks, client side means\nnetsplit).\n\nIt is also not economically favorable for any individual miner to be the\none to mine empty blocks on top of any surviving softfork-incompatible\nchain. As a miner you would only volunteer to do it if you believe the\nsoftfork is necessary or itself will enable greater future profit.\n\nBesides that, I also just don't believe that UASF itself as a method to\nactivate softforks is a good choice. The only two reliable signals we have\nfor this purpose in Bitcoin are block height (flag day) and standard miner\nsignaling, as every other metric can be falsified or gamed.\n\nBut there's also more problems - a big one is that we have no way right now\nfor a node to tell another \"the transaction you just relayed to me is\ninvalid according to an active softfork\" (or \"will become invalid\". This\nmatters for several reasons.\n\nThe first one that came to my mind is that we have widespread usage of\nzero-confirmation payments in the network.\n\nThis was already dangerous for other reasons, but this UASF could make it\nguaranteed cost-free to exploit - because as many also know, we ALSO\nalready have a lot of nodes that do not enforce the non-default rejection\npolicies (otherwise we'd never see such transactions on blocks), including\nmany alternative Bitcoin clients.\n\nThe combination of these factors means that you can present an UASF invalid\ntransaction to a non-updated client that is supposedly protected by the\ndeliberate orphaning effort, and have it accept this as a payment. To never\nsee it get confirmed, or to eventually see it doublespent by an UASF-valid\ntransaction.\n\nI would not at all be surprised if it turned out that many\nzero-confirmation accepting services do not reject non-default\ntransactions, or if they aren't all UASF-segwit aware.\n\nThis is why a flag day or similar is more effective - it can't be ignored\nunlike \"just another one of those UASF proposals\" that you might not have\nevaluated or not expect to activate.\n\nThis is by the way also a reason that I believe that all nodes and services\nshould publish all concensus critical policies that they enforce. This\nwould make it far easier to alert somebody that they NEED TO prepare for\nwhatever proposal that might conflict with their active policies.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170415/31ad5f51/attachment.html>"
            },
            {
                "author": "Greg Sanders",
                "date": "2017-04-15T13:54:57",
                "message_text_only": "> Besides that, I also just don't believe that UASF itself as a method to\nactivate softforks is a good choice. The only two reliable signals we have\nfor this purpose in Bitcoin are block height (flag day) and standard miner\nsignaling, as every other metric can be falsified or gamed.\n\nUASF can be just a flag day.\n\nOn Sat, Apr 15, 2017 at 9:23 AM, Natanael via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n>\n> Den 15 apr. 2017 13:51 skrev \"Chris Acheson via bitcoin-dev\" <\n> bitcoin-dev at lists.linuxfoundation.org>:\n>\n>\n> Not sure if you missed my previous reply to you, but I'm curious about\n> your thoughts on this particular point. I contend that for any UASF,\n> orphaning non-signalling blocks on the flag date is [maybe] safer [for\n> those in on the UASF fork] than just\n> considering the fork active on the flag date.\n>\n>\n> Note my additions.\n>\n> Enforcement by orphaning non-compliance makes it harder to reverse a buggy\n> softfork, since you necessarily increase the effort needed to return enough\n> mining power to the safe chain since you now have mostly unmonitored mining\n> hardware fighting you actively, whose operators you might not be able to\n> contact. You'd practically have to hardfork out of the situation.\n>\n> There's also the risk of the activation itself triggering concensus bugs\n> (multiple incompatible UASF forks), if there's multiple implementations of\n> it in the network (or one buggy one). We have already seen something like\n> it happen. This can both happen on the miner side, client side or both\n> (miner side only would lead to a ton of orphaned blocks, client side means\n> netsplit).\n>\n> It is also not economically favorable for any individual miner to be the\n> one to mine empty blocks on top of any surviving softfork-incompatible\n> chain. As a miner you would only volunteer to do it if you believe the\n> softfork is necessary or itself will enable greater future profit.\n>\n> Besides that, I also just don't believe that UASF itself as a method to\n> activate softforks is a good choice. The only two reliable signals we have\n> for this purpose in Bitcoin are block height (flag day) and standard miner\n> signaling, as every other metric can be falsified or gamed.\n>\n> But there's also more problems - a big one is that we have no way right\n> now for a node to tell another \"the transaction you just relayed to me is\n> invalid according to an active softfork\" (or \"will become invalid\". This\n> matters for several reasons.\n>\n> The first one that came to my mind is that we have widespread usage of\n> zero-confirmation payments in the network.\n>\n> This was already dangerous for other reasons, but this UASF could make it\n> guaranteed cost-free to exploit - because as many also know, we ALSO\n> already have a lot of nodes that do not enforce the non-default rejection\n> policies (otherwise we'd never see such transactions on blocks), including\n> many alternative Bitcoin clients.\n>\n> The combination of these factors means that you can present an UASF\n> invalid transaction to a non-updated client that is supposedly protected by\n> the deliberate orphaning effort, and have it accept this as a payment. To\n> never see it get confirmed, or to eventually see it doublespent by an\n> UASF-valid transaction.\n>\n> I would not at all be surprised if it turned out that many\n> zero-confirmation accepting services do not reject non-default\n> transactions, or if they aren't all UASF-segwit aware.\n>\n> This is why a flag day or similar is more effective - it can't be ignored\n> unlike \"just another one of those UASF proposals\" that you might not have\n> evaluated or not expect to activate.\n>\n> This is by the way also a reason that I believe that all nodes and\n> services should publish all concensus critical policies that they enforce.\n> This would make it far easier to alert somebody that they NEED TO prepare\n> for whatever proposal that might conflict with their active policies.\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170415/3a6b3e64/attachment.html>"
            },
            {
                "author": "Cameron Garnham",
                "date": "2017-04-15T08:05:10",
                "message_text_only": "Thank-you for your prompt response,\n\nI believe I must have a different prospective of Bitcoin to you.  Ideologically I don\u2019t agree that miners can be passive participants in the Bitcoin Network; and I certainly don\u2019t see them acting as passive participants in the Bitcoin Community now.\n\nThe miners are very much political actors.  Hence why I fail to take-to-heart your concern \"that the proposal will reject the blocks of passive participants\u201d.\n\nWith AsicBoost, there are three miner groups: Those who use it (and have legal sanction to do so); Those who use it (without legal sanction); and those who don\u2019t use it.  If SegWit didn\u2019t directly affect miners, then one could possibly claim that there could be an ideal passive participant miner in reality. However since your important revelations on AsicBoost: SegWit cannot be a \u2018passive\u2019 option for miners.\n\nHence, I don\u2019t care about orphaning the blocks of of the theoretical \"passive participant\u201d miner. As I have no logical reasoning to suggest one could exists; and a large amount of evidence to suggesting one dose not exit.\n\n\nOn BIP 16 vs. BIP 17;  I cannot see how BIP 148 similar engineering tradeoffs.  Is there any long-term \u2018technical debt\u2019 from BIP 148 that I\u2019m unaware of that could be similar to BIP 16?\n\n\nOn the Drama:  Well 100M USD p/a can pay for lots of Drama; Hence going back to the first point: The miners are not passive participants when it comes to *any* form of activation of SegWit.\n\nCameron.\n\n\n\n> On 15 Apr 2017, at 10:04 AM, Gregory Maxwell <greg at xiph.org> wrote:\n> \n> On Sat, Apr 15, 2017 at 6:28 AM, Cameron Garnham <da2ce7 at gmail.com> wrote:\n>> As many may remember, there was quite some controversy about the BIP16 vs BIP 17 split; the main argument for BIP16 was the urgency of P2SH, and how this was the already \u201ctested and proven to work\u201d solution.\n> \n> And as a result we ultimately got a clearly inferior solution (520\n> byte script limit; 80-bit security; months of orphaned blocks-- and\n> two of those were not issues in BIP17).  I went along for the cram\n> fest on 16 after 12 caught fire, and I was mistaken to do so.\n> \n> Doubly so because it took years for P2SH to achieve any kind of mass\n> deployment due to issues far away from consensus.  An extra two months\n> spent on some ground-work (including communications and documentation)\n> could have pulled forward practical deployment by a year and given\n> time to find and fix some of the flaws in the design of P2SH.\n> \n>> BIP 148 is out (our?) terms of peace.  The Bitcoin Community is tired-to-death of this war and wants a resolution swiftly. BIP 148 proves a outlet, and in Maxwell words: \u201c...almost guarantees at a minor level of disruption.\u201d.\n> \n> It seems I lost a word in my comment: that should have been \"almost\n> guarantees at _least_ a minor level of disruption\". A minor level of\n> disruption is the _minimum_ amount of disruption, and for no good\n> reason except an unprecedented and unjustified level of haste.\n> \n> Considering that you did not spare a single word about the specific\n> property that I am concerned about-- that the proposal will reject the\n> blocks of passive participants, due to avoidable design limitations--\n> I can't help but feel that you don't even care to understand the\n> concern I was bringing up. :(\n> \n> How many people barely reviewed the specifics of the proposal simply\n> because they want something fast and this proposal does something\n> fast?\n> \n>> tired-to-death of this war and wants a resolution swiftly\n> \n> By now competitors and opponents to Bitcoin have surely realized that\n> they can attack Bitcoin by stirring up drama.\n> \n> As a result, the only way that we will ever be free from \"war\" is if\n> we choose to not let it impact us as much as possible. We must be\n> imperturbable and continue working at the same level of excellence as\n> if virtual shells weren't flying overhead-- or otherwise there is an\n> incentive to keep them flying 24/7. Internet drama is remarkably cheap\n> to generate. \"The only thing we have to fear is fear itself\".\n> \n> The alternative is that we hand opponents a ready made formula for\n> disruption: astroturf enough drama up that Bitcoiners \"sacrifice\n> correctness\" themselves right off a cliff in a futile attempt to make\n> it go away. :)"
            },
            {
                "author": "shaolinfry",
                "date": "2017-04-20T18:39:36",
                "message_text_only": "Dear Greg,\n\nThank you for taking the time to review the BIP148 proposal.\n\nI agree with much of your thoughts. I originally started working on a generalized way to deploy user activated soft forks, in a way that leveraged BIP9 to allow for optional faster MASF activation. BIP148 came about as a way to satify many people's frustrations about the current segwit activation. I have said several times in various places that the proposal requires a very high amount of consensus that needs to be present to make actual deployment feasible. BIP148 is certainly not what a normal UASF would or should look like.\n\nI remain convinced the community very much wants segwit activated and that the UASF movement in general has gained a lot of traction. While support for BIP148 is surprisingly high, there are definitely important players who support UASF in general but do not like BIP148 approach (which you rightly point out is a UASF to force a MASF).\n\nIn any case, I have been working on various iterations for generalized deployment of soft forks. My latest iteration adds a simple flag to a BIP9 deployment so the deployment will transition to LOCKED_IN at timeout if the deployment hasnt already activated or locked in by then. This is nice because it allows for a long deployment of a soft fork, giving the ecosystem plenty time to upgrade with an effective flagday at the end of the timeout. The hash power can still optionally activate earlier under MASF.\n\nBIP8 (was uaversionbits) can be seen here https://github.com/bitcoin/bips/blob/master/bip-0008.mediawiki\n\nWith BIP8 we could perform a UASF segwit deployment. Due to some complexities in the peering logic, I recommend a new deployment with a fresh bit that starts right after November 15th (when BIP9 segwit timesout) with a BIP8 timeout for April 2018. The code can deployed much earlier. For example if code was deployed today, it would give the economy a year to upgrade. Activation could still occur safely by MASF any time from now until April 2018 (SEGWIT until Nov, then UASEGWIT from Nov until April 2018).\n\nI am still working on the finer implementation details, but you can see a rough draft from this diff (which includes BIP8 in the first commit, and the proposed bip-segwit-uasf in the second commit).\n\nhttps://github.com/bitcoin/bitcoin/compare/master...shaolinfry:uasegwit-flagday\n\nI believe this approach would satisfy the more measured approach expected for Bitcoin and does not have the issues you brought up about BIP148.\n\nI do not support the BIP148 UASF for some of the same reasons that I\ndo support segwit: Bitcoin is valuable in part because it has high\nsecurity and stability, segwit was carefully designed to support and\namplify that engineering integrity that people can count on now and\ninto the future.\n\nI do not feel the the approach proposed in BIP148 really measures up\nto the standard set by segwit itself, or the existing best practices\nin protocol development in this community.\n\nThe primary flaw in BIP148 is that by forcing the activation of the\nexisting (non-UASF segwit) nodes it almost guarantees at a minor level\nof disruption.\n\nSegwit was carefully engineered so that older unmodified miners could\ncontinue operating _completely_ without interruption after segwit\nactivates.\n\nOlder nodes will not include segwit spends, and so their blocks will\nnot be invalid even if they do not have segwit support. They can\nupgrade to it on their own schedule. The only risk non-participating\nminers take after segwit activation is that if someone else mines an\ninvalid block they would extend it, a risk many miners already\nfrequently take with spy-mining.\n\nI do not think it is a horrible proposal: it is better engineered than\nmany things that many altcoins do, but just not up to our normal\nstandards. I respect the motivations of the authors of BIP 148. If\nyour goal is the fastest possible segwit activation then it is very\nuseful to exploit the >80% of existing nodes that already support the\noriginal version of segwit.\n\nBut the fastest support should not be our goal, as a community-- there\nis always some reckless altcoin or centralized system that can support\nsomething faster than we can-- trying to match that would only erode\nour distinguishing value in being well engineered and stable.\n\n\"First do no harm.\" We should use the least disruptive mechanisms\navailable, and the BIP148 proposal does not meet that test. To hear\nsome people-- non-developers on reddit and such-- a few even see the\nforced orphaning of 148 as a virtue, that it's punitive for\nmisbehaving miners. I could not not disagree with that perspective any\nmore strongly.\n\nOf course, I do not oppose the general concept of a UASF but\n_generally_ a soft-fork (of any kind) does not need to risk disruption\nof mining, just as segwit's activation does not. UASF are the\noriginal kind of soft-fork and were the only kind of fork practiced by\nSatoshi. P2SH was activated based on a date, and all prior ones were\nbased on times or heights. We introduced miner based activation as\npart of a process of making Bitcoin more stable in the common case\nwhere the ecosystem is all in harmony. It's kind of weird to see UASF\nportrayed as something new.\n\nIt's important the users not be at the mercy of any one part of the\necosystem to the extent that we can avoid it-- be it developers,\nexchanges, chat forums, or mining hardware makers. Ultimately the\nrules of Bitcoin work because they're enforced by the users\ncollectively-- that is what makes Bitcoin Bitcoin, it's what makes it\nsomething people can count on: the rules aren't easy to just change.\n\nThere have been some other UASF proposals that avoid the forced\ndisruption-- by just defining a new witness bit and allowing\nnon-upgraded-to-uasf miners and nodes to continue as non-upgraded, I\nthink they are vastly superior. They would be slower to deploy, but I\ndo not think that is a flaw.\n\nWe should have patience. Bitcoin is a system that should last for all\nages and power mankind for a long time-- ten years from now a couple\nyears of dispute will seem like nothing. But the reputation we earn\nfor stability and integrity, for being a system of money people can\ncount on will mean everything.\n\nIf these discussions come up, they'll come up in the form of reminding\npeople that Bitcoin isn't easily changed at a whim, even when the\nwhims are obviously good, and how that protects it from being managed\nlike all the competing systems of money that the world used to use\nwere managed. :)\n\nSo have patience, don't take short cuts. Segwit is a good improvement\nand we should respect it by knowing that it's good enough to wait for,\nand for however its activated to be done the best way we know how.\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170420/e1b512a5/attachment-0001.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2017-04-25T18:28:14",
                "message_text_only": "On Thu, Apr 20, 2017 at 6:39 PM, shaolinfry <shaolinfry at protonmail.ch> wrote:\n> I agree with much of your thoughts. I originally started working on a\n> generalized way to deploy user activated soft forks, in a way that leveraged\n> BIP9 to allow for optional faster MASF activation. BIP148 came about as a\n> way to satify many people's frustrations about the current segwit\n> activation. I have said several times in various places that the proposal\n> requires a very high amount of consensus that needs to be present to make\n> actual deployment feasible. BIP148 is certainly not what a normal UASF would\n> or should look like.\n>\n> I remain convinced the community very much wants segwit activated and that\n> the UASF movement in general has gained a lot of traction. While support for\n> BIP148 is surprisingly high, there are definitely important players who\n> support UASF in general but do not like BIP148 approach (which you rightly\n> point out is a UASF to force a MASF).\n[...]\n> With BIP8 we could perform a UASF segwit deployment. Due to some\n> complexities in the peering logic, I recommend a new deployment with a fresh\n> bit that starts right after November 15th (when BIP9 segwit timesout) with a\n> BIP8 timeout for April 2018. The code can deployed much earlier. For example\n> if code was deployed today, it would give the economy a year to upgrade.\n> Activation could still occur safely by MASF any time from now until April\n> 2018 (SEGWIT until Nov, then UASEGWIT from Nov until April 2018).\n>\n> I am still working on the finer implementation details, but you can see a\n> rough draft from this diff (which includes BIP8 in the first commit, and the\n> proposed bip-segwit-uasf in the second commit).\n>\n> https://github.com/bitcoin/bitcoin/compare/master...shaolinfry:uasegwit-flagday\n>\n> I believe this approach would satisfy the more measured approach expected\n> for Bitcoin and does not have the issues you brought up about BIP148.\n\nI have not reviewed it carefully yet, but I agree that it addresses my\nmain concern!  I think this is a much better approach. Thanks."
            },
            {
                "author": "Luke Dashjr",
                "date": "2017-04-25T18:46:09",
                "message_text_only": "On Tuesday 25 April 2017 6:28:14 PM Gregory Maxwell via bitcoin-dev wrote:\n> > https://github.com/bitcoin/bitcoin/compare/master...shaolinfry:uasegwit-f\n> > lagday\n> > \n> > I believe this approach would satisfy the more measured approach expected\n> > for Bitcoin and does not have the issues you brought up about BIP148.\n> \n> I have not reviewed it carefully yet, but I agree that it addresses my\n> main concern!  I think this is a much better approach. Thanks.\n\nFWIW, I disagree in this case. I think given the circumstances, if we are \ngoing to do a UASF for segwit at all, we need a clearly decisive outcome, \nwhich is given by BIP 148. Using the approach in BIP 8 makes sense in many \ncases, but in this case, it is liable to simply create a prolonged uncertainty \nwhere nobody knows the outcome when segwit's rules are challenged by a \nmalicious miner.\n\nIf BIP 148 fails to achieve widespread support, we could do a BIP 8-based UASF \nwith Segwit v2 (along with some other changes I suggested in the other \nthread), but I think the tradeoffs right now favour BIP 148 as the best UASF \ndeployment.\n\nLuke"
            },
            {
                "author": "Chris Acheson",
                "date": "2017-04-14T10:52:46",
                "message_text_only": "Speaking as one of the BIP148 agitators:\n\n> There have been some other UASF proposals that avoid the forced\n> disruption-- by just defining a new witness bit and allowing\n> non-upgraded-to-uasf miners and nodes to continue as non-upgraded, I\n> think they are vastly superior. They would be slower to deploy, but I\n> do not think that is a flaw.\n\nI'm assuming that you're referring to the flag date \"segwit is on now\"\napproach. This is more dangerous than the orphaning approach that BIP148\nuses.\n\nIf we orphan non-signalling blocks on the flag date and don't have\nmajority hash power supporting us, there will be a chain split on the\nflag day. We expect this to happen, we plan for it, and we employ\nstrategies to mitigate any damage. The bulk of the economy has\ncoordinated around this event happening. We even had the opportunity to\npull the plug before the flag date if things were looking too grim.\n\nAfter the dust settles, 100% of the miners are guaranteed to have\nupgraded, assuming they didn't choose to forgo 2+ weeks of income. Any\nfurther chain splits would have to be the result of deliberate action by\n51%+ of the mining power.\n\nIf we just have segwit activate on the flag date without orphaning the\nblocks of non-segwit miners, we set ourselves up for a chain split at\nsome unknown time in the future. Without majority hash power on our\nside, as soon as someone mines a segwit-invalid transaction, the chain\nwill split, with upgraded nodes and miners on one side, and non-upgraded\nnodes and miners on the other side. The segwit-invalid transaction\ndoesn't even need to come from someone with their own mining equipment.\nOpen a short on BTC, rent some hash power, profit.\n\nSince we don't know when this attack will occur, we won't be organized\nand ready for it. It's also easy for both miners and users to get\ncomplacent about it and fail to upgrade. The damage will be far worse\nthan if we had used the orphaning approach.\n\n> \"First do no harm.\" We should use the least disruptive mechanisms\n> available, and the BIP148 proposal does not meet that test.  To hear\n> some people-- non-developers on reddit and such-- a few even see the\n> forced orphaning of 148 as a virtue, that it's punitive for\n> misbehaving miners. I could not not disagree with that perspective any\n> more strongly.\n\nPunitive action against miners is not the point of BIP148, it's an\nunavoidable side-effect of making the UASF less disruptive for the users\nof Bitcoin. Minimizing disruption for users must take priority over\nminimizing disruption for miners. Given the intensity of this dispute\nand the bad faith of certain actors, some schadenfreude is bound to\noccur. Don't let that distract you from the actual reasons that BIP148\nis designed the way it is.\n\n> We should have patience. Bitcoin is a system that should last for all\n> ages and power mankind for a long time-- ten years from now a couple\n> years of dispute will seem like nothing. But the reputation we earn\n> for stability and integrity, for being a system of money people can\n> count on will mean everything.\n\nI respect this perspective, and I agree with it to a certain extent.\nHowever, continuing to wait has costs. I do not believe we have the\nluxury of continuing to wait for a couple more years. In doing so it's\nentirely possible that we may damage our reputation for stability and\nintegrity rather than build on it.\n\nWe have a window of opportunity with BIP148, and it would be a waste not\nto act on it. In the event that we still lack sufficient support by\nJuly, we can abandon the project, and make plans for how best to proceed\nfrom there."
            },
            {
                "author": "Mark Friedenbach",
                "date": "2017-04-15T13:42:25",
                "message_text_only": "Greg,\n\nIf I understand correctly, the crux of your argument against BIP148 is that\nit requires the segwit BIP9 activation flag to be set in every block after\nAug 1st, until segwit activates. This will cause miners which have not\nupgrade and indicated support for BIP141 (the segwit BIP) to find their\nblocks ignored by UASF nodes, at least for the month or two it takes to\nactivate segwit.\n\nIsn't this however the entire point of BIP148? I understand if you object\nto this, but let's be clear that this is a design requirement of the\nproposal, not a technical oversight. The alternative you present (new BIP\nbit) has the clear downside of not triggering BIP141 activation, and\ntherefore not enabling the new consensus rules on already deployed full\nnodes. BIP148 is making an explicit choice to favor dragging along those\nusers which have upgraded to BIP141 support over those miners who have\nfailed to upgrade.\n\nOn an aside, I'm somewhat disappointed that you have decided to make a\npublic statement against the UASF proposal. Not because we disagree -- that\nis fine -- but because any UASF must be a grassroots effort and\nendorsements (or denouncements) detract from that.\n\nMark Friedenbach\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170415/af2462dc/attachment-0001.html>"
            },
            {
                "author": "Ryan Grant",
                "date": "2017-04-15T14:54:00",
                "message_text_only": "On Sat, Apr 15, 2017 at 8:42 AM, Mark Friedenbach via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> The alternative [Greg presents] (new BIP bit) has the clear downside\n> of not triggering BIP141 activation, and therefore not enabling the\n> new consensus rules on already deployed full nodes. BIP148 is making\n> an explicit choice to favor dragging along those users which have\n> upgraded to BIP141 support over those miners who have failed to\n> upgrade.\n\nA proposal from yesterday would separate this concern; though not\nretroactively.  One way to name this proposal would be \"Catch-All\nSegwit Activation\".\n\n  \"extended BIP9 activation of segwit, for legacy nodes\"\n  https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-April/014160.html\n\nIf this release valve exists, then discussions (such as this thread)\ncan get back to focusing on finding the safest incentive-compatible\ntransitions, with time improving the situation instead of making it worse."
            },
            {
                "author": "Gregory Maxwell",
                "date": "2017-04-15T18:50:17",
                "message_text_only": "On Sat, Apr 15, 2017 at 1:42 PM, Mark Friedenbach via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> triggering BIP141 activation, and therefore not enabling the new consensus\n> rules on already deployed full nodes. BIP148 is making an explicit choice\n> to favor dragging along those users which have upgraded to BIP141 support\n> over those miners who have failed to upgrade.\n>\n\nI do not follow the argument that a critical design feature of a particular\n\"user activated soft fork\" could be that it is users don't need to be\ninvolved.  If the goal is user activation I would think that the\nexpectation would be that the overwhelming majority of users would be\nupgrading to do it, if that isn't the case, then it isn't really a user\nactivated softfork-- it's something else.\n\n\n> On an aside, I'm somewhat disappointed that you have decided to make a\n> public statement against the UASF proposal. Not because we disagree -- that\n> is fine -- but because any UASF must be a grassroots effort and\n> endorsements (or denouncements) detract from that.\n>\n\nSo it has to be supported by the public but I can't say why I don't support\nit? This seems extremely suspect to me.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170415/1328569c/attachment.html>"
            },
            {
                "author": "Erik Aronesty",
                "date": "2017-04-19T16:17:39",
                "message_text_only": "The \"UASF movement\" seems a bit premature to me - I doubt UASF will be\nnecessary if a WTXID commitment is tried first.   I think that should be\nfirst-efforts focus.\n\nOn Sat, Apr 15, 2017 at 2:50 PM, Gregory Maxwell via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Sat, Apr 15, 2017 at 1:42 PM, Mark Friedenbach via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> triggering BIP141 activation, and therefore not enabling the new\n>> consensus rules on already deployed full nodes. BIP148 is making an\n>> explicit choice to favor dragging along those users which have upgraded to\n>> BIP141 support over those miners who have failed to upgrade.\n>>\n>\n> I do not follow the argument that a critical design feature of a\n> particular \"user activated soft fork\" could be that it is users don't need\n> to be involved.  If the goal is user activation I would think that the\n> expectation would be that the overwhelming majority of users would be\n> upgrading to do it, if that isn't the case, then it isn't really a user\n> activated softfork-- it's something else.\n>\n>\n>> On an aside, I'm somewhat disappointed that you have decided to make a\n>> public statement against the UASF proposal. Not because we disagree -- that\n>> is fine -- but because any UASF must be a grassroots effort and\n>> endorsements (or denouncements) detract from that.\n>>\n>\n> So it has to be supported by the public but I can't say why I don't\n> support it? This seems extremely suspect to me.\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170419/045f3e4a/attachment.html>"
            },
            {
                "author": "Alphonse Pace",
                "date": "2017-04-20T14:23:40",
                "message_text_only": "A WTXID commitment would (likely) need to be a UASF.\n\n\nOn Wed, Apr 19, 2017 at 11:17 AM, Erik Aronesty via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> The \"UASF movement\" seems a bit premature to me - I doubt UASF will be\n> necessary if a WTXID commitment is tried first.   I think that should be\n> first-efforts focus.\n>\n> On Sat, Apr 15, 2017 at 2:50 PM, Gregory Maxwell via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> On Sat, Apr 15, 2017 at 1:42 PM, Mark Friedenbach via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> triggering BIP141 activation, and therefore not enabling the new\n>>> consensus rules on already deployed full nodes. BIP148 is making an\n>>> explicit choice to favor dragging along those users which have upgraded to\n>>> BIP141 support over those miners who have failed to upgrade.\n>>>\n>>\n>> I do not follow the argument that a critical design feature of a\n>> particular \"user activated soft fork\" could be that it is users don't need\n>> to be involved.  If the goal is user activation I would think that the\n>> expectation would be that the overwhelming majority of users would be\n>> upgrading to do it, if that isn't the case, then it isn't really a user\n>> activated softfork-- it's something else.\n>>\n>>\n>>> On an aside, I'm somewhat disappointed that you have decided to make a\n>>> public statement against the UASF proposal. Not because we disagree -- that\n>>> is fine -- but because any UASF must be a grassroots effort and\n>>> endorsements (or denouncements) detract from that.\n>>>\n>>\n>> So it has to be supported by the public but I can't say why I don't\n>> support it? This seems extremely suspect to me.\n>>\n>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170420/311c5641/attachment.html>"
            },
            {
                "author": "Erik Aronesty",
                "date": "2017-04-20T15:48:21",
                "message_text_only": "Bitcoin must level the playing field for mining or it is fundamentally\nbroken.   And there are two obvious solutions:\n\n1. WTXID commitment has as a flag day upgrade. It's a fix to a fairly\nserious security issue - made even worse by the existence of patents on the\ncode.\n\n2. Embed the code for performing a covert ASICBOOST into Bitcoin core's\nreference implementation.   But, since this would violate patents held in\nChina and the U.S., it could be a problem.\n\nOf these, I think the first should be far less controversial.\n\nOne or the other must be done - if we can't fix security and licensing\nproblems in Bitcoin, what can we fix?\n\n\nOn Thu, Apr 20, 2017 at 10:23 AM, Alphonse Pace <alp.bitcoin at gmail.com>\nwrote:\n\n> A WTXID commitment would (likely) need to be a UASF.\n>\n>\n> On Wed, Apr 19, 2017 at 11:17 AM, Erik Aronesty via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> The \"UASF movement\" seems a bit premature to me - I doubt UASF will be\n>> necessary if a WTXID commitment is tried first.   I think that should be\n>> first-efforts focus.\n>>\n>> On Sat, Apr 15, 2017 at 2:50 PM, Gregory Maxwell via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> On Sat, Apr 15, 2017 at 1:42 PM, Mark Friedenbach via bitcoin-dev <\n>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>>> triggering BIP141 activation, and therefore not enabling the new\n>>>> consensus rules on already deployed full nodes. BIP148 is making an\n>>>> explicit choice to favor dragging along those users which have upgraded to\n>>>> BIP141 support over those miners who have failed to upgrade.\n>>>>\n>>>\n>>> I do not follow the argument that a critical design feature of a\n>>> particular \"user activated soft fork\" could be that it is users don't need\n>>> to be involved.  If the goal is user activation I would think that the\n>>> expectation would be that the overwhelming majority of users would be\n>>> upgrading to do it, if that isn't the case, then it isn't really a user\n>>> activated softfork-- it's something else.\n>>>\n>>>\n>>>> On an aside, I'm somewhat disappointed that you have decided to make a\n>>>> public statement against the UASF proposal. Not because we disagree -- that\n>>>> is fine -- but because any UASF must be a grassroots effort and\n>>>> endorsements (or denouncements) detract from that.\n>>>>\n>>>\n>>> So it has to be supported by the public but I can't say why I don't\n>>> support it? This seems extremely suspect to me.\n>>>\n>>>\n>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170420/719b28e2/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "I do not support the BIP 148 UASF",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Ryan Grant",
                "Chris Stewart",
                "Steven Pine",
                "Natanael",
                "shaolinfry",
                "Tom Zander",
                "James Hilliard",
                "praxeology_guy",
                "Chris Acheson",
                "Alphonse Pace",
                "Gregory Maxwell",
                "Luke Dashjr",
                "Erik Aronesty",
                "Mark Friedenbach",
                "Cameron Garnham",
                "Greg Sanders"
            ],
            "messages_count": 34,
            "total_messages_chars_count": 92950
        }
    },
    {
        "title": "[bitcoin-dev] extended BIP9 activation of segwit, for legacy nodes",
        "thread_messages": [
            {
                "author": "Ryan Grant",
                "date": "2017-04-14T20:12:34",
                "message_text_only": "Segwit has proven more contentious to activate than anticipated\n(although my read has long been that the technical consensus is clear,\ndespite noisy objections).  No matter which method is used to\neventually activate segwit, or on what timeline, it would be\nbeneficial if validating nodes already capable of supporting segwit\ncould, without further upgrades, eventually participate to their\nfullest capacity.\n\nBIP9 assignments should reserve a backward compatibility bit which all\nyet-unknown segwit-compatible proposals may utilize.  These future\nproposals must be consensus compatible with BIPs 141, 143, & 147,\nexcept that they may use different deployment logic.\n\nThe motivation is so that any validating node software released after\nthis BIP9 assignment can eventually understand if segwit is activated\nby alternate means, even when the node is itself a legacy version.\nThis is important because the realities of system administration on\nthe Bitcoin network are that upgrades occur slowly (which is inherent\nin the security choice of not presenting an auto-upgrade feature).\nEven though segwit in particular is backwards compatible with old\nvalidating nodes, there are still distinct advantages to validating\nand generating segregated witness transactions.\n\nFor example, future BIP9-compatible deployment attempts might\nadditionally include a date-dependent UASF fallback.  If, either\nduring or after activation, deployment rules also require signaling\nfor segwit using the backwards-compatible bit here proposed, then\n(after 95% of recent blocks signal for the alternate segwit\ndeployment) more legacy nodes would understand and validate\ntransactions using segregated witnesses.\n\nAn expiration time of five years seems conservative:\n\n  // Alternate Deployment 1 of SegWit (BIP141, BIP143, and BIP147)\n  consensus.vDeployments[Consensus::DEPLOYMENT_SEGWIT_ALT1].bit = 2;\n  consensus.vDeployments[Consensus::DEPLOYMENT_SEGWIT_ALT1].nStartTime\n= 1510704000; // November 15th, 2017.\n  consensus.vDeployments[Consensus::DEPLOYMENT_SEGWIT_ALT1].nTimeout =\n1668470400; // November 15th, 2022.\n\nSegwit deployment logic would then look like:\n\n  bool IsWitnessEnabled(const CBlockIndex* pindexPrev,\n                        const Consensus::Params& params)\n  {\n      LOCK(cs_main);\n      return    (VersionBitsState(pindexPrev,\n                                  params,\n                                  Consensus::DEPLOYMENT_SEGWIT,\n                                  versionbitscache)\n                 == THRESHOLD_ACTIVE)\n             || (VersionBitsState(pindexPrev,\n                                  params,\n                                  Consensus::DEPLOYMENT_SEGWIT_ALT1,\n                                  versionbitscache)\n                 == THRESHOLD_ACTIVE);\n  }"
            },
            {
                "author": "shaolinfry",
                "date": "2017-04-14T20:33:40",
                "message_text_only": "You might be interested in my bip-uaversionbits proposal https://github.com/shaolinfry/bips/blob/bip-uavb/bip-uaversionbits.mediawiki\n\nSegwit has proven more contentious to activate than anticipated\n(although my read has long been that the technical consensus is clear,\ndespite noisy objections). No matter which method is used to\neventually activate segwit, or on what timeline, it would be\nbeneficial if validating nodes already capable of supporting segwit\ncould, without further upgrades, eventually participate to their\nfullest capacity.\n\nBIP9 assignments should reserve a backward compatibility bit which all\nyet-unknown segwit-compatible proposals may utilize. These future\nproposals must be consensus compatible with BIPs 141, 143, & 147,\nexcept that they may use different deployment logic.\n\nThe motivation is so that any validating node software released after\nthis BIP9 assignment can eventually understand if segwit is activated\nby alternate means, even when the node is itself a legacy version.\nThis is important because the realities of system administration on\nthe Bitcoin network are that upgrades occur slowly (which is inherent\nin the security choice of not presenting an auto-upgrade feature).\nEven though segwit in particular is backwards compatible with old\nvalidating nodes, there are still distinct advantages to validating\nand generating segregated witness transactions.\n\nFor example, future BIP9-compatible deployment attempts might\nadditionally include a date-dependent UASF fallback. If, either\nduring or after activation, deployment rules also require signaling\nfor segwit using the backwards-compatible bit here proposed, then\n(after 95% of recent blocks signal for the alternate segwit\ndeployment) more legacy nodes would understand and validate\ntransactions using segregated witnesses.\n\nAn expiration time of five years seems conservative:\n\n// Alternate Deployment 1 of SegWit (BIP141, BIP143, and BIP147)\nconsensus.vDeployments[Consensus::DEPLOYMENT_SEGWIT_ALT1].bit = 2;\nconsensus.vDeployments[Consensus::DEPLOYMENT_SEGWIT_ALT1].nStartTime\n= 1510704000; // November 15th, 2017.\nconsensus.vDeployments[Consensus::DEPLOYMENT_SEGWIT_ALT1].nTimeout =\n1668470400; // November 15th, 2022.\n\nSegwit deployment logic would then look like:\n\nbool IsWitnessEnabled(const CBlockIndex* pindexPrev,\nconst Consensus::Params& params)\n{\nLOCK(cs_main);\nreturn (VersionBitsState(pindexPrev,\nparams,\nConsensus::DEPLOYMENT_SEGWIT,\nversionbitscache)\n== THRESHOLD_ACTIVE)\n|| (VersionBitsState(pindexPrev,\nparams,\nConsensus::DEPLOYMENT_SEGWIT_ALT1,\nversionbitscache)\n== THRESHOLD_ACTIVE);\n}\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170414/685151e3/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "extended BIP9 activation of segwit, for legacy nodes",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Ryan Grant",
                "shaolinfry"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 5733
        }
    },
    {
        "title": "[bitcoin-dev] Diminishing Signaling Returns",
        "thread_messages": [
            {
                "author": "Crypto.Press",
                "date": "2017-04-15T13:51:58",
                "message_text_only": "Hi everyone, I have never posted to the list and do not commit the project\nproper so I do apologize if this is not even possible in advance.\n\nExamining bitcoin's past and contrasting it with other social contracts and\neven physical phenomena it would seem that as bitcoin continues to age\nentropy will continue to grow.\n\nThe scaling debate would seem to be a fore-bearer of this.\n\nOne main contributor to this in bitcoin is as people/businesses become\ninvolved and vested into their perspective it becomes minted in their\nidentity. The idea that this will somehow decrease as the user base grows\nwith more perspectives and use-cases seems counter-intuitive so I propose a\npotential way to combat entropy in a divided community.\n\nDiminishing Signaling Returns\n\nSince entropy is the increase of disorder in a system (the various scaling\nsolutions for this idea) and measuring via resource is not trustless, we\nneed a mechanism to essentially counteract entropy while not relying on\ngame-able metrics.\n\nWhat if we applied a rate of diminishing returns on signaling in BIP9?\nSomething along the lines of:\n\nEvery (X) block signaled The actual signal value diminishes from a 1 signal\n1 block to a fraction of a signal per block found after a burn in period of\nsome reasonable time to ensure a majority upgrade(this could even be\neffected after the timeout currently apart of BIP9 for ease of\nimplementation?).\n\nSome thoughts\n\n   - The pools that find more blocks would lose the ability to block the\n   network without taking an economical hit splitting up their hash power as\n   signaling was never intended to be a voting mechanism (to my knowledge).\n   - The longer that the signaling took place would eventually run a larger\n   pool's signaling influence to 0 first. This creates a balancing effect\n   between hash rate & #of miners actually signaling ready.\n   - Gamesmanship of this system would be visible to the community at\n   large. e.g. pools hash rate/blocks found jumps or declines significantly in\n   a short time frame, or specific time frame (when pools influence begins to\n   decline).\n   - creates multiple economic incentives for the mining community to be on\n   a similar page\n   - this as a feature of a soft forks greatly diminishes politics becoming\n   a factor in the future.\n\n\nunfortunately, this itself would require a soft fork if I am correct?\n\n\nAcceptance then becomes the question.\nWhile bitcoin has proven to be highly resilient, stagnation has destroyed\nmany systems/businesses and if the current state of affairs is any measure\nit would stand to reason that in the future this will only worsen. Taking\nthis action could be a solution to that stagnation.  So, it would be in\neveryone's best long-term interest to support a continually evolving\nbitcoin and would allow parties with ideas that differ the time and\nresources to fork in a more responsible manner without devoting their\nresources to politics. However, everyone would still have the time to voice\ntheir opinions during the burn-in/timeout period and of course before any\ncode was actually included through technical consensus.\n\nThoughts?\n\nRegards,\nBenjamin George\nCrypto.Press http://crypto.press\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170415/fd15b6fb/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Diminishing Signaling Returns",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Crypto.Press"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 3370
        }
    },
    {
        "title": "[bitcoin-dev] Malice Reactive Proof of Work Additions (MR POWA): Protecting Bitcoin from malicious miners",
        "thread_messages": [
            {
                "author": "Erik Aronesty",
                "date": "2017-04-16T20:04:56",
                "message_text_only": "This is a great solution.\n\n8 or more secure hashes, each of which can be implemented on GPU/CPU, but\nrotate through them - per block round robin.\n\nHardware, infrastructue investment is protected.  ASIC is not.\n\nEach pow has different tracking metrics and difficulty adjustments.  This\nmeans the difficulty adjust will be less accurate (1/8th the samples),  but\nthat's never been an issue.\n\nASIC will never beat this - because it will be 8x more expensive to\nmaintain the cold circuits.\n\nMiners with gpu/generalized hardware will always be in business.\n\nShould be done gradually and pre-emptively.    Change one at a time on a\nslow schedule, allowing a graceful transition.\n\n\n\n\n\nOn Mar 20, 2017 8:59 AM, \"Marcos mayorga via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi,\n> Just a thought,\n> Bitcoin developers shouldn't care about miners business model, they can\n> always sell their hw and close the bz as soon as bitcoin hardforks to\n> better ways of doing.\n> Just focus on making a better cryptocurrency, the more decentralized the\n> best.\n>\n> M\n>\n> > By doing this you're significantly changing the economic incentives\n> behind\n> > bitcoin mining. How can you reliably invest in hardware if you have no\n> > idea\n> > when or if your profitability is going to be cut by 50-75% based on a\n> > whim?\n> >\n> > You may also inadvertently create an entirely new attack vector if 50-75%\n> > of the SHA256 hardware is taken offline and purchased by an entity who\n> > intends to do harm to the network.\n> >\n> > Bitcoin only works if most miners are honest, this has been known since\n> > the\n> > beginning.\n> >\n> > On Mon, Mar 20, 2017 at 9:50 AM John Hardy via bitcoin-dev <\n> > bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >\n> >> I\u00e2\u20ac\u2122m very worried about the state of miner centralisation in Bitcoin.\n> >>\n> >> I always felt the centralising effects of ASIC manufacturing would\n> >> resolve\n> >> themselves once the first mover advantage had been exhausted and the\n> >> industry had the opportunity to mature.\n> >>\n> >> I had always assumed initial centralisation would be harmless since\n> >> miners\n> >> have no incentive to harm the network. This does not consider the risk\n> >> of a\n> >> single entity with sufficient power and either poor, malicious or\n> >> coerced\n> >> decision making. I now believe that such centralisation poses a huge\n> >> risk\n> >> to the security of Bitcoin and preemptive action needs to be taken to\n> >> protect the network from malicious actions by any party able to exert\n> >> influence over a substantial portion of SHA256 hardware.\n> >>\n> >> Inspired by UASF, I believe we should implement a Malicious miner\n> >> Reactive\n> >> Proof of Work Additions (MR POWA).\n> >>\n> >> This would be a hard fork activated in response to a malicious attempt\n> >> by\n> >> a hashpower majority to introduce a contentious hard fork.\n> >>\n> >> The activation would occur once a fork was detected violating protocol\n> >> (likely oversize blocks) with a majority of hashpower. The threshold and\n> >> duration for activation would need to be carefully considered.\n> >>\n> >> I don\u00e2\u20ac\u2122t think we should eliminate SHA256 as a hashing method and\n> >> change\n> >> POW entirely. That would be throwing the baby out with the bathwater and\n> >> hurt the non-malicious miners who have invested in hardware, making it\n> >> harder to gain their support.\n> >>\n> >> Instead I believe we should introduce multiple new proofs of work that\n> >> are\n> >> already established and proven within existing altcoin implementations.\n> >> As\n> >> an example we could add Scrypt, Ethash and Equihash. Much of the code\n> >> and\n> >> mining infrastructure already exists. Diversification of hardware (a mix\n> >> of\n> >> CPU and memory intensive methods) would also be positive for\n> >> decentralisation. Initial difficulty could simply be an estimated\n> >> portion\n> >> of existing infrastructure.\n> >>\n> >> This example would mean 4 proofs of work with 40 minute block target\n> >> difficulty for each. There could also be a rule that two different\n> >> proofs\n> >> of work must find a block before a method can start hashing again. This\n> >> means there would only be 50% of hardware hashing at a time, and a\n> >> sudden\n> >> gain or drop in hashpower from a particular method does not dramatically\n> >> impact the functioning of the network between difficulty adjustments.\n> >> This\n> >> also adds protection from attacks by the malicious SHA256 hashpower\n> >> which\n> >> could even be required to wait until all other methods have found a\n> >> block\n> >> before being allowed to hash again.\n> >>\n> >> 50% hashing time would mean that the cost of electricity in relation to\n> >> hardware would fall by 50%, reducing some of the centralising impact of\n> >> subsidised or inexpensive electricity in some regions over others.\n> >>\n> >> Such a hard fork could also, counter-intuitively, introduce a block size\n> >> increase since while we\u00e2\u20ac\u2122re hard forking it makes sense to minimise the\n> >> number of future hard forks where possible. It could also activate\n> >> SegWit\n> >> if it hasn\u00e2\u20ac\u2122t already.\n> >>\n> >> The beauty of this method is that it creates a huge risk to any\n> >> malicious\n> >> actor trying to abuse their position. Ideally, MR POWA would just serve\n> >> as\n> >> a deterrent and never activate.\n> >>\n> >> If consensus were to form around a hard fork in the future nodes would\n> >> be\n> >> able to upgrade and MR POWA, while automatically activating on\n> >> non-upgraded\n> >> nodes, would be of no economic significance: a vestigial chain\n> >> immediately\n> >> abandoned with no miner incentive.\n> >>\n> >> I think this would be a great way to help prevent malicious use of\n> >> hashpower to harm the network. This is the beauty of Bitcoin: for any\n> >> road\n> >> block that emerges the economic majority can always find a way around.\n> >>\n> >> _______________________________________________\n> >> bitcoin-dev mailing list\n> >> bitcoin-dev at lists.linuxfoundation.org\n> >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> >>\n> > --\n> > Andrew Johnson\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> >\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170416/d37bb2b1/attachment.html>"
            },
            {
                "author": "bfd at cock.lu",
                "date": "2017-04-17T01:28:56",
                "message_text_only": "On 2017-04-16 17:04, Erik Aronesty via bitcoin-dev wrote:\n> This is a great solution.\n> \n> 8 or more secure hashes, each of which can be implemented on GPU/CPU,\n> but rotate through them - per block round robin.\n> \n> Hardware, infrastructue investment is protected.  ASIC is not.\n> \n\nThe write time for configuring a FPGA with a fresh bitstream is measured \nin tens of milliseconds.\n\n\n> ASIC will never beat this - because it will be 8x more expensive to\n> maintain the cold circuits.\n> \n\nUnused circuits don't consume power, which is the main cost in running a \nminer."
            },
            {
                "author": "Erik Aronesty",
                "date": "2017-04-17T07:47:48",
                "message_text_only": "On Apr 16, 2017 6:28 PM, <bfd at cock.lu> wrote:\n\n\n\nOn 2017-04-16 17:04, Erik Aronesty via bitcoin-dev wrote:\n\n> This is a great solution.\n>\n> 8 or more secure hashes, each of which can be implemented on GPU/CPU,\n> but rotate through them - per block round robin.\n>\n> Hardware, infrastructue investment is protected.  ASIC is not.\n>\n>\nThe write time for configuring a FPGA with a fresh bitstream is measured in\ntens of milliseconds.\n\n\nI have no objections to the use of FPGA or any other commercially available\nhardware.\n\n\n\n\nASIC will never beat this - because it will be 8x more expensive to\n> maintain the cold circuits.\n>\n>\nUnused circuits don't consume power, which is the main cost in running a\nminer\n\n\nThey make GPUs or FPGAs (as u mentioned) far more affordable.  The problem\nis centralized manufacturing, which, in turn, is a side effect of a covert\nhardware mining optimization leading to a monopoly.\n\nA rotating POW seems to make ASIC manufacture impractical compared to\ngeneralized, commercially available hardware.\n\nIt's too bad we can't make the POW somehow dynamic so that any specialized\nhardware is impossible, and only GPU / FPGA is possible.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170417/f1e21176/attachment.html>"
            },
            {
                "author": "Erik Aronesty",
                "date": "2017-04-17T11:17:17",
                "message_text_only": "It's too bad we can't make the POW somehow dynamic so that any specialized\nhardware is impossible, and only GPU / FPGA is possible.\n\n\n\nMaybe a variant of Keccak where the size of the sponge is increased along\nwith additional zero bits required.  Seems like this would either have to\nresist specialized hardware or imply sha3 is compromised such that the size\nof the sponge does not incerase the number of possible output bits as\nexpected.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170417/f2cb0e77/attachment.html>"
            },
            {
                "author": "Natanael",
                "date": "2017-04-17T22:34:55",
                "message_text_only": "Den 17 apr. 2017 16:14 skrev \"Erik Aronesty via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org>:\n\n\nIt's too bad we can't make the POW somehow dynamic so that any specialized\nhardware is impossible, and only GPU / FPGA is possible.\n\n\n\nMaybe a variant of Keccak where the size of the sponge is increased along\nwith additional zero bits required.  Seems like this would either have to\nresist specialized hardware or imply sha3 is compromised such that the size\nof the sponge does not incerase the number of possible output bits as\nexpected.\n\n\nTechnically SHA3 (keccak) already has the SHAKE mode, an extensible output\nfunction (XOF). It's basically a hash with arbitary output length (with\nfixed state size, 256 bits is the common choice). A little bit like hooking\na hash straight into a stream cipher.\n\nThe other modes should *probably* not allow the same behavior, though. I\ncan't guarantee that however.\n\nYou may be interested in looking at parameterizable ciphers and if any of\nthem might be applicable to PoW.\n\nIMHO the best option if we change PoW is an algorithm that's moderately\nprocessing heavy (we still need reasonably fast verification) and which\nresists partial state reuse (not fast or fully \"linear\" in processing like\nSHA256) just for the sake of invalidating asicboost style attacks, and it\nshould also have an existing reference implementation for hardware that's\nprovably close in performance to the theoretical ideal implementation of\nthe algorithm (in other words, one where we know there's no hidden\noptimizations).\n\nAnything relying on memory or other such expensive components is likely to\nfall flat eventually as fast memory is made more compact, cheaper and moves\ncloser to the cores.\n\nThat should be approximately what it takes to level out the playing field\nin ASIC manufacturing, because then we would know there's no fancy tricks\nto deploy that would give one player unfair advantage. The competition\nwould mostly be about packing similar gate designs closely and energy\nefficiency. (Now that I think about it, the proof MAY have to consider\nenergy use too, as a larger and slower but more efficient chip still is\ncompetitive in mining...)\n\nWe should also put a larger nonce in the header if possible, to reduce the\nincentive to mess with the entropy elsewhere in blocks.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170418/d2274892/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Malice Reactive Proof of Work Additions (MR POWA): Protecting Bitcoin from malicious miners",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "bfd at cock.lu",
                "Natanael",
                "Erik Aronesty"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 11686
        }
    },
    {
        "title": "[bitcoin-dev] Small Nodes: A Better Alternative to Pruned Nodes",
        "thread_messages": [
            {
                "author": "David Vorick",
                "date": "2017-04-17T06:54:49",
                "message_text_only": "*Rationale:*\n\nA node that stores the full blockchain (I will use the term archival node)\nrequires over 100GB of disk space, which I believe is one of the most\nsignificant barriers to more people running full nodes. And I believe the\necosystem would benefit substantially if more users were running full nodes.\n\nThe best alternative today to storing the full blockchain is to run a\npruned node, which keeps only the UTXO set and throws away already verified\nblocks. The operator of the pruned node is able to enjoy the full security\nbenefits of a full node, but is essentially leeching the network, as they\nperformed a large download likely without contributing anything back.\n\nThis puts more pressure on the archival nodes, as the archival nodes need\nto pick up the slack and help new nodes bootstrap to the network. As the\npressure on archival nodes grows, fewer people will be able to actually run\narchival nodes, and the situation will degrade. The situation would likely\nbecome problematic quickly if bitcoin-core were to ship with the defaults\nset to a pruned node.\n\nEven further, the people most likely to care about saving 100GB of disk\nspace are also the people least likely to care about some extra bandwidth\nusage. For datacenter nodes, and for nodes doing lots of bandwidth, the\nbandwidth is usually the biggest cost of running the node. For home users\nhowever, as long as they stay under their bandwidth cap, the bandwidth is\nactually free. Ideally, new nodes would be able to bootstrap from nodes\nthat do not have to pay for their bandwidth, instead of needing to rely on\na decreasing percentage of heavy-duty archival nodes.\n\nI have (perhaps incorrectly) identified disk space consumption as the most\nsignificant factor in your average user choosing to run a pruned node or a\nlite client instead of a full node. The average user is not typically too\nworried about bandwidth, and is also not typically too worried about\ninitial blockchain download time. But the 100GB hit to your disk space can\nbe a huge psychological factor, especially if your hard drive only has\n500GB available in the first place, and 250+ GB is already consumed by\nother files you have.\n\nI believe that improving the disk usage situation would greatly benefit\ndecentralization, especially if it could be done without putting pressure\non archival nodes.\n\n*Small Nodes Proposal:*\n\nI propose an alternative to the pruned node that does not put undue\npressure on archival nodes, and would be acceptable and non-risky to ship\nas a default in bitcoin-core. For lack of a better name, I'll call this new\ntype of node a 'small node'. The intention is that bitcoin-core would\neventually ship 'small nodes' by default, such that the expected amount of\ndisk consumption drops from today's 100+ GB to less than 30 GB.\n\nMy alternative proposal has the following properties:\n\n+ Full nodes only need to store ~20% of the blockchain\n+ With very high probability, a new node will be able to recover the entire\nblockchain by connecting to 6 random small node peers.\n+ An attacker that can eliminate a chosen+ 95% of the full nodes running\ntoday will be unable to prevent new nodes from downloading the full\nblockchain, even if the attacker is also able to eliminate all archival\nnodes. (assuming all nodes today were small nodes instead of archival nodes)\n\nMethod:\n\nA small node will pick an index [5, 256). This index is that node's\npermanent index. When storing a block, instead of storing the full block,\nthe node will use Reed-Solomon coding to erasure code the block using a\n5-of-256 scheme. The result will be 256 pieces that are 20% of the size of\nthe block each. The node picks the piece that corresponds to its index, and\nstores that instead. (Indexes 0-4 are reserved for archival nodes -\nexplained later)\n\nThe node is now storing a fragment of every block. Alone, this fragment\ncannot be used to recover any piece of the blockchain. However, when paired\nwith any 5 unique fragments (fragments of the same index will not be\nunique), the full block can be recovered.\n\nNodes can optionally store more than 1 fragment each. At 5 fragments, the\nnode becomes a full archival node, and the chosen indexes should be 0-4.\nThis is advantageous for the archival node as the encoded data for the\nfirst 5 indexes will actually be identical to the block itself - there is\nno computational overhead for selecting the first indexes. There is also no\nneed to choose random indexes, because the full block can be recovered no\nmatter which indexes are chosen.\n\nWhen connecting to new peers, the indexes of each peer needs to be known.\nOnce peers totaling 5 unique indexes are discovered, blockchain download\ncan begin. Connecting to just 5 small node peers provides a >95% chance of\ngetting 5 uniques, with exponentially improving odds of success as you\nconnect to more peers. Connecting to a single archive node guarantees that\nany gaps can be filled.\n\nA good encoder should be able to turn a block into a 5-of-256 piece set in\nunder 10 milliseconds using a single core on a standard consumer desktop.\nThis should not slow down initial blockchain download substantially, though\nthe overhead is more than a rounding error.\n\n*DoS Prevention:*\n\nA malicious node may provide garbage data instead of the actual piece.\nGiven just the garbage data and 4 other correct pieces, it is impossible\n(best I know anyway) to tell which piece is the garbage piece.\n\nOne option in this case would be to seek out an archival node that could\nverify the correctness of the pieces, and identify the malicious node.\n\nAnother option would be to have the small nodes store a cryptographic\nchecksum of each piece. Obtaining the cryptographic checksum for all 256\npieces would incur a nontrivial amount of hashing (post segwit, as much as\n100MB of extra hashing per block), and would require an additional ~4kb of\nstorage per block. The hashing overhead here may be prohibitive.\n\nAnother solution would be to find additional pieces and brute-force\ncombinations of 5 until a working combination was discovered. Though this\nsounds nasty, it should take less than five seconds of computation to find\nthe working combination given 5 correct pieces and 2 incorrect pieces. This\ncomputation only needs to be performed once to identify the malicious peers.\n\nI also believe that alternative erasure coding schemes exist which actually\nare able to identify the bad pieces given sufficient good pieces, however I\ndon't know if they have the same computational performance as the best\nReed-Solomon coding implementations.\n\n*Deployment:*\n\nSmall nodes are completely useless unless the critical mass of 5 pieces can\nbe obtained. The first version that supports small node block downloads\nshould default everyone to an archival node (meaning indexes 0-4 are used)\n\nOnce there are enough small-node-enabled archive nodes, the default can be\nswitched so that nodes only have a single index by default. In the first\nfew days, when there are only a few small nodes, the previously-deployed\narchival nodes can help fill in the gaps, and the small nodes can be useful\nfor blockchain download right away.\n\n----------------------------------\n\nThis represents a non-trivial amount of code, but I believe that the result\nwould be a non-trivial increase in the percentage of users running full\nnodes, and a healthier overall network.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170417/513fc363/attachment.html>"
            },
            {
                "author": "Danny Thorpe",
                "date": "2017-04-17T07:11:07",
                "message_text_only": "1TB HDD is now available for under $40 USD.  How is the 100GB storage\nrequirement preventing anyone from setting up full nodes?\n\nOn Apr 16, 2017 11:55 PM, \"David Vorick via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> *Rationale:*\n>\n> A node that stores the full blockchain (I will use the term archival node)\n> requires over 100GB of disk space, which I believe is one of the most\n> significant barriers to more people running full nodes. And I believe the\n> ecosystem would benefit substantially if more users were running full nodes.\n>\n> The best alternative today to storing the full blockchain is to run a\n> pruned node, which keeps only the UTXO set and throws away already verified\n> blocks. The operator of the pruned node is able to enjoy the full security\n> benefits of a full node, but is essentially leeching the network, as they\n> performed a large download likely without contributing anything back.\n>\n> This puts more pressure on the archival nodes, as the archival nodes need\n> to pick up the slack and help new nodes bootstrap to the network. As the\n> pressure on archival nodes grows, fewer people will be able to actually run\n> archival nodes, and the situation will degrade. The situation would likely\n> become problematic quickly if bitcoin-core were to ship with the defaults\n> set to a pruned node.\n>\n> Even further, the people most likely to care about saving 100GB of disk\n> space are also the people least likely to care about some extra bandwidth\n> usage. For datacenter nodes, and for nodes doing lots of bandwidth, the\n> bandwidth is usually the biggest cost of running the node. For home users\n> however, as long as they stay under their bandwidth cap, the bandwidth is\n> actually free. Ideally, new nodes would be able to bootstrap from nodes\n> that do not have to pay for their bandwidth, instead of needing to rely on\n> a decreasing percentage of heavy-duty archival nodes.\n>\n> I have (perhaps incorrectly) identified disk space consumption as the most\n> significant factor in your average user choosing to run a pruned node or a\n> lite client instead of a full node. The average user is not typically too\n> worried about bandwidth, and is also not typically too worried about\n> initial blockchain download time. But the 100GB hit to your disk space can\n> be a huge psychological factor, especially if your hard drive only has\n> 500GB available in the first place, and 250+ GB is already consumed by\n> other files you have.\n>\n> I believe that improving the disk usage situation would greatly benefit\n> decentralization, especially if it could be done without putting pressure\n> on archival nodes.\n>\n> *Small Nodes Proposal:*\n>\n> I propose an alternative to the pruned node that does not put undue\n> pressure on archival nodes, and would be acceptable and non-risky to ship\n> as a default in bitcoin-core. For lack of a better name, I'll call this new\n> type of node a 'small node'. The intention is that bitcoin-core would\n> eventually ship 'small nodes' by default, such that the expected amount of\n> disk consumption drops from today's 100+ GB to less than 30 GB.\n>\n> My alternative proposal has the following properties:\n>\n> + Full nodes only need to store ~20% of the blockchain\n> + With very high probability, a new node will be able to recover the\n> entire blockchain by connecting to 6 random small node peers.\n> + An attacker that can eliminate a chosen+ 95% of the full nodes running\n> today will be unable to prevent new nodes from downloading the full\n> blockchain, even if the attacker is also able to eliminate all archival\n> nodes. (assuming all nodes today were small nodes instead of archival nodes)\n>\n> Method:\n>\n> A small node will pick an index [5, 256). This index is that node's\n> permanent index. When storing a block, instead of storing the full block,\n> the node will use Reed-Solomon coding to erasure code the block using a\n> 5-of-256 scheme. The result will be 256 pieces that are 20% of the size of\n> the block each. The node picks the piece that corresponds to its index, and\n> stores that instead. (Indexes 0-4 are reserved for archival nodes -\n> explained later)\n>\n> The node is now storing a fragment of every block. Alone, this fragment\n> cannot be used to recover any piece of the blockchain. However, when paired\n> with any 5 unique fragments (fragments of the same index will not be\n> unique), the full block can be recovered.\n>\n> Nodes can optionally store more than 1 fragment each. At 5 fragments, the\n> node becomes a full archival node, and the chosen indexes should be 0-4.\n> This is advantageous for the archival node as the encoded data for the\n> first 5 indexes will actually be identical to the block itself - there is\n> no computational overhead for selecting the first indexes. There is also no\n> need to choose random indexes, because the full block can be recovered no\n> matter which indexes are chosen.\n>\n> When connecting to new peers, the indexes of each peer needs to be known.\n> Once peers totaling 5 unique indexes are discovered, blockchain download\n> can begin. Connecting to just 5 small node peers provides a >95% chance of\n> getting 5 uniques, with exponentially improving odds of success as you\n> connect to more peers. Connecting to a single archive node guarantees that\n> any gaps can be filled.\n>\n> A good encoder should be able to turn a block into a 5-of-256 piece set in\n> under 10 milliseconds using a single core on a standard consumer desktop.\n> This should not slow down initial blockchain download substantially, though\n> the overhead is more than a rounding error.\n>\n> *DoS Prevention:*\n>\n> A malicious node may provide garbage data instead of the actual piece.\n> Given just the garbage data and 4 other correct pieces, it is impossible\n> (best I know anyway) to tell which piece is the garbage piece.\n>\n> One option in this case would be to seek out an archival node that could\n> verify the correctness of the pieces, and identify the malicious node.\n>\n> Another option would be to have the small nodes store a cryptographic\n> checksum of each piece. Obtaining the cryptographic checksum for all 256\n> pieces would incur a nontrivial amount of hashing (post segwit, as much as\n> 100MB of extra hashing per block), and would require an additional ~4kb of\n> storage per block. The hashing overhead here may be prohibitive.\n>\n> Another solution would be to find additional pieces and brute-force\n> combinations of 5 until a working combination was discovered. Though this\n> sounds nasty, it should take less than five seconds of computation to find\n> the working combination given 5 correct pieces and 2 incorrect pieces. This\n> computation only needs to be performed once to identify the malicious peers.\n>\n> I also believe that alternative erasure coding schemes exist which\n> actually are able to identify the bad pieces given sufficient good pieces,\n> however I don't know if they have the same computational performance as the\n> best Reed-Solomon coding implementations.\n>\n> *Deployment:*\n>\n> Small nodes are completely useless unless the critical mass of 5 pieces\n> can be obtained. The first version that supports small node block downloads\n> should default everyone to an archival node (meaning indexes 0-4 are used)\n>\n> Once there are enough small-node-enabled archive nodes, the default can be\n> switched so that nodes only have a single index by default. In the first\n> few days, when there are only a few small nodes, the previously-deployed\n> archival nodes can help fill in the gaps, and the small nodes can be useful\n> for blockchain download right away.\n>\n> ----------------------------------\n>\n> This represents a non-trivial amount of code, but I believe that the\n> result would be a non-trivial increase in the percentage of users running\n> full nodes, and a healthier overall network.\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170417/02c3dd58/attachment-0001.html>"
            },
            {
                "author": "David Vorick",
                "date": "2017-04-17T07:27:35",
                "message_text_only": "Most people do not want to go out and buy new hardware to run a Bitcoin\nnode. The want to use the hardware that they already own, and usually that\nhardware is going to have a non-generous amount of disk space. 500GB SSD\nwith no HDD is common in computers today.\n\nBut really, the best test is to go out and talk to people. Ask them if they\nrun a full node, and if they say no, ask them why not. In my experience,\nthe most common answer by a significant margin is that they don't want to\nlose the disk space. That psychology is far more important than any example\nof cheap hard drives. People don't want to go out and buy a hard drive so\nthat they can run Bitcoin. It's a non-starter.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170417/11863db8/attachment.html>"
            },
            {
                "author": "Erik Aronesty",
                "date": "2017-04-20T15:50:24",
                "message_text_only": "Try to find 1TB dedicated server hosting ...\n\nIf you want to set up an ecommerce site somewhere besides your living room,\nstorage costs are still a concern.\n\nOn Mon, Apr 17, 2017 at 3:11 AM, Danny Thorpe via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> 1TB HDD is now available for under $40 USD.  How is the 100GB storage\n> requirement preventing anyone from setting up full nodes?\n>\n> On Apr 16, 2017 11:55 PM, \"David Vorick via bitcoin-dev\" <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> *Rationale:*\n>>\n>> A node that stores the full blockchain (I will use the term archival\n>> node) requires over 100GB of disk space, which I believe is one of the most\n>> significant barriers to more people running full nodes. And I believe the\n>> ecosystem would benefit substantially if more users were running full nodes.\n>>\n>> The best alternative today to storing the full blockchain is to run a\n>> pruned node, which keeps only the UTXO set and throws away already verified\n>> blocks. The operator of the pruned node is able to enjoy the full security\n>> benefits of a full node, but is essentially leeching the network, as they\n>> performed a large download likely without contributing anything back.\n>>\n>> This puts more pressure on the archival nodes, as the archival nodes need\n>> to pick up the slack and help new nodes bootstrap to the network. As the\n>> pressure on archival nodes grows, fewer people will be able to actually run\n>> archival nodes, and the situation will degrade. The situation would likely\n>> become problematic quickly if bitcoin-core were to ship with the defaults\n>> set to a pruned node.\n>>\n>> Even further, the people most likely to care about saving 100GB of disk\n>> space are also the people least likely to care about some extra bandwidth\n>> usage. For datacenter nodes, and for nodes doing lots of bandwidth, the\n>> bandwidth is usually the biggest cost of running the node. For home users\n>> however, as long as they stay under their bandwidth cap, the bandwidth is\n>> actually free. Ideally, new nodes would be able to bootstrap from nodes\n>> that do not have to pay for their bandwidth, instead of needing to rely on\n>> a decreasing percentage of heavy-duty archival nodes.\n>>\n>> I have (perhaps incorrectly) identified disk space consumption as the\n>> most significant factor in your average user choosing to run a pruned node\n>> or a lite client instead of a full node. The average user is not typically\n>> too worried about bandwidth, and is also not typically too worried about\n>> initial blockchain download time. But the 100GB hit to your disk space can\n>> be a huge psychological factor, especially if your hard drive only has\n>> 500GB available in the first place, and 250+ GB is already consumed by\n>> other files you have.\n>>\n>> I believe that improving the disk usage situation would greatly benefit\n>> decentralization, especially if it could be done without putting pressure\n>> on archival nodes.\n>>\n>> *Small Nodes Proposal:*\n>>\n>> I propose an alternative to the pruned node that does not put undue\n>> pressure on archival nodes, and would be acceptable and non-risky to ship\n>> as a default in bitcoin-core. For lack of a better name, I'll call this new\n>> type of node a 'small node'. The intention is that bitcoin-core would\n>> eventually ship 'small nodes' by default, such that the expected amount of\n>> disk consumption drops from today's 100+ GB to less than 30 GB.\n>>\n>> My alternative proposal has the following properties:\n>>\n>> + Full nodes only need to store ~20% of the blockchain\n>> + With very high probability, a new node will be able to recover the\n>> entire blockchain by connecting to 6 random small node peers.\n>> + An attacker that can eliminate a chosen+ 95% of the full nodes running\n>> today will be unable to prevent new nodes from downloading the full\n>> blockchain, even if the attacker is also able to eliminate all archival\n>> nodes. (assuming all nodes today were small nodes instead of archival nodes)\n>>\n>> Method:\n>>\n>> A small node will pick an index [5, 256). This index is that node's\n>> permanent index. When storing a block, instead of storing the full block,\n>> the node will use Reed-Solomon coding to erasure code the block using a\n>> 5-of-256 scheme. The result will be 256 pieces that are 20% of the size of\n>> the block each. The node picks the piece that corresponds to its index, and\n>> stores that instead. (Indexes 0-4 are reserved for archival nodes -\n>> explained later)\n>>\n>> The node is now storing a fragment of every block. Alone, this fragment\n>> cannot be used to recover any piece of the blockchain. However, when paired\n>> with any 5 unique fragments (fragments of the same index will not be\n>> unique), the full block can be recovered.\n>>\n>> Nodes can optionally store more than 1 fragment each. At 5 fragments, the\n>> node becomes a full archival node, and the chosen indexes should be 0-4.\n>> This is advantageous for the archival node as the encoded data for the\n>> first 5 indexes will actually be identical to the block itself - there is\n>> no computational overhead for selecting the first indexes. There is also no\n>> need to choose random indexes, because the full block can be recovered no\n>> matter which indexes are chosen.\n>>\n>> When connecting to new peers, the indexes of each peer needs to be known.\n>> Once peers totaling 5 unique indexes are discovered, blockchain download\n>> can begin. Connecting to just 5 small node peers provides a >95% chance of\n>> getting 5 uniques, with exponentially improving odds of success as you\n>> connect to more peers. Connecting to a single archive node guarantees that\n>> any gaps can be filled.\n>>\n>> A good encoder should be able to turn a block into a 5-of-256 piece set\n>> in under 10 milliseconds using a single core on a standard consumer\n>> desktop. This should not slow down initial blockchain download\n>> substantially, though the overhead is more than a rounding error.\n>>\n>> *DoS Prevention:*\n>>\n>> A malicious node may provide garbage data instead of the actual piece.\n>> Given just the garbage data and 4 other correct pieces, it is impossible\n>> (best I know anyway) to tell which piece is the garbage piece.\n>>\n>> One option in this case would be to seek out an archival node that could\n>> verify the correctness of the pieces, and identify the malicious node.\n>>\n>> Another option would be to have the small nodes store a cryptographic\n>> checksum of each piece. Obtaining the cryptographic checksum for all 256\n>> pieces would incur a nontrivial amount of hashing (post segwit, as much as\n>> 100MB of extra hashing per block), and would require an additional ~4kb of\n>> storage per block. The hashing overhead here may be prohibitive.\n>>\n>> Another solution would be to find additional pieces and brute-force\n>> combinations of 5 until a working combination was discovered. Though this\n>> sounds nasty, it should take less than five seconds of computation to find\n>> the working combination given 5 correct pieces and 2 incorrect pieces. This\n>> computation only needs to be performed once to identify the malicious peers.\n>>\n>> I also believe that alternative erasure coding schemes exist which\n>> actually are able to identify the bad pieces given sufficient good pieces,\n>> however I don't know if they have the same computational performance as the\n>> best Reed-Solomon coding implementations.\n>>\n>> *Deployment:*\n>>\n>> Small nodes are completely useless unless the critical mass of 5 pieces\n>> can be obtained. The first version that supports small node block downloads\n>> should default everyone to an archival node (meaning indexes 0-4 are used)\n>>\n>> Once there are enough small-node-enabled archive nodes, the default can\n>> be switched so that nodes only have a single index by default. In the first\n>> few days, when there are only a few small nodes, the previously-deployed\n>> archival nodes can help fill in the gaps, and the small nodes can be useful\n>> for blockchain download right away.\n>>\n>> ----------------------------------\n>>\n>> This represents a non-trivial amount of code, but I believe that the\n>> result would be a non-trivial increase in the percentage of users running\n>> full nodes, and a healthier overall network.\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170420/9e3837a3/attachment-0001.html>"
            },
            {
                "author": "Aymeric Vitte",
                "date": "2017-04-20T23:42:03",
                "message_text_only": "??? what do you mean? (https://www.soyoustart.com/fr/serveurs-essential/)\n\n\nLe 20/04/2017 \u00e0 17:50, Erik Aronesty via bitcoin-dev a \u00e9crit :\n> Try to find 1TB dedicated server hosting ...\n>\n> If you want to set up an ecommerce site somewhere besides your living\n> room, storage costs are still a concern.\n>\n> On Mon, Apr 17, 2017 at 3:11 AM, Danny Thorpe via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org\n> <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n>\n>     1TB HDD is now available for under $40 USD.  How is the 100GB\n>     storage requirement preventing anyone from setting up full nodes?\n>\n>     On Apr 16, 2017 11:55 PM, \"David Vorick via bitcoin-dev\"\n>     <bitcoin-dev at lists.linuxfoundation.org\n>     <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n>\n>         *Rationale:*\n>\n>         A node that stores the full blockchain (I will use the term\n>         archival node) requires over 100GB of disk space, which I\n>         believe is one of the most significant barriers to more people\n>         running full nodes. And I believe the ecosystem would benefit\n>         substantially if more users were running full nodes.\n>\n>         The best alternative today to storing the full blockchain is\n>         to run a pruned node, which keeps only the UTXO set and throws\n>         away already verified blocks. The operator of the pruned node\n>         is able to enjoy the full security benefits of a full node,\n>         but is essentially leeching the network, as they performed a\n>         large download likely without contributing anything back.\n>\n>         This puts more pressure on the archival nodes, as the archival\n>         nodes need to pick up the slack and help new nodes bootstrap\n>         to the network. As the pressure on archival nodes grows, fewer\n>         people will be able to actually run archival nodes, and the\n>         situation will degrade. The situation would likely become\n>         problematic quickly if bitcoin-core were to ship with the\n>         defaults set to a pruned node.\n>\n>         Even further, the people most likely to care about saving\n>         100GB of disk space are also the people least likely to care\n>         about some extra bandwidth usage. For datacenter nodes, and\n>         for nodes doing lots of bandwidth, the bandwidth is usually\n>         the biggest cost of running the node. For home users however,\n>         as long as they stay under their bandwidth cap, the bandwidth\n>         is actually free. Ideally, new nodes would be able to\n>         bootstrap from nodes that do not have to pay for their\n>         bandwidth, instead of needing to rely on a decreasing\n>         percentage of heavy-duty archival nodes.\n>\n>         I have (perhaps incorrectly) identified disk space consumption\n>         as the most significant factor in your average user choosing\n>         to run a pruned node or a lite client instead of a full node.\n>         The average user is not typically too worried about bandwidth,\n>         and is also not typically too worried about initial blockchain\n>         download time. But the 100GB hit to your disk space can be a\n>         huge psychological factor, especially if your hard drive only\n>         has 500GB available in the first place, and 250+ GB is already\n>         consumed by other files you have.\n>\n>         I believe that improving the disk usage situation would\n>         greatly benefit decentralization, especially if it could be\n>         done without putting pressure on archival nodes.\n>\n>         *Small Nodes Proposal:*\n>\n>         I propose an alternative to the pruned node that does not put\n>         undue pressure on archival nodes, and would be acceptable and\n>         non-risky to ship as a default in bitcoin-core. For lack of a\n>         better name, I'll call this new type of node a 'small node'.\n>         The intention is that bitcoin-core would eventually ship\n>         'small nodes' by default, such that the expected amount of\n>         disk consumption drops from today's 100+ GB to less than 30 GB.\n>\n>         My alternative proposal has the following properties:\n>\n>         + Full nodes only need to store ~20% of the blockchain\n>         + With very high probability, a new node will be able to\n>         recover the entire blockchain by connecting to 6 random small\n>         node peers.\n>         + An attacker that can eliminate a chosen+ 95% of the full\n>         nodes running today will be unable to prevent new nodes from\n>         downloading the full blockchain, even if the attacker is also\n>         able to eliminate all archival nodes. (assuming all nodes\n>         today were small nodes instead of archival nodes)\n>\n>         Method:\n>\n>         A small node will pick an index [5, 256). This index is that\n>         node's permanent index. When storing a block, instead of\n>         storing the full block, the node will use Reed-Solomon coding\n>         to erasure code the block using a 5-of-256 scheme. The result\n>         will be 256 pieces that are 20% of the size of the block each.\n>         The node picks the piece that corresponds to its index, and\n>         stores that instead. (Indexes 0-4 are reserved for archival\n>         nodes - explained later)\n>\n>         The node is now storing a fragment of every block. Alone, this\n>         fragment cannot be used to recover any piece of the\n>         blockchain. However, when paired with any 5 unique fragments\n>         (fragments of the same index will not be unique), the full\n>         block can be recovered.\n>\n>         Nodes can optionally store more than 1 fragment each. At 5\n>         fragments, the node becomes a full archival node, and the\n>         chosen indexes should be 0-4. This is advantageous for the\n>         archival node as the encoded data for the first 5 indexes will\n>         actually be identical to the block itself - there is no\n>         computational overhead for selecting the first indexes. There\n>         is also no need to choose random indexes, because the full\n>         block can be recovered no matter which indexes are chosen.\n>\n>         When connecting to new peers, the indexes of each peer needs\n>         to be known. Once peers totaling 5 unique indexes are\n>         discovered, blockchain download can begin. Connecting to just\n>         5 small node peers provides a >95% chance of getting 5\n>         uniques, with exponentially improving odds of success as you\n>         connect to more peers. Connecting to a single archive node\n>         guarantees that any gaps can be filled.\n>\n>         A good encoder should be able to turn a block into a 5-of-256\n>         piece set in under 10 milliseconds using a single core on a\n>         standard consumer desktop. This should not slow down initial\n>         blockchain download substantially, though the overhead is more\n>         than a rounding error.\n>\n>         *DoS Prevention:*\n>\n>         A malicious node may provide garbage data instead of the\n>         actual piece. Given just the garbage data and 4 other correct\n>         pieces, it is impossible (best I know anyway) to tell which\n>         piece is the garbage piece.\n>\n>         One option in this case would be to seek out an archival node\n>         that could verify the correctness of the pieces, and identify\n>         the malicious node.\n>\n>         Another option would be to have the small nodes store a\n>         cryptographic checksum of each piece. Obtaining the\n>         cryptographic checksum for all 256 pieces would incur a\n>         nontrivial amount of hashing (post segwit, as much as 100MB of\n>         extra hashing per block), and would require an additional ~4kb\n>         of storage per block. The hashing overhead here may be\n>         prohibitive.\n>\n>         Another solution would be to find additional pieces and\n>         brute-force combinations of 5 until a working combination was\n>         discovered. Though this sounds nasty, it should take less than\n>         five seconds of computation to find the working combination\n>         given 5 correct pieces and 2 incorrect pieces. This\n>         computation only needs to be performed once to identify the\n>         malicious peers.\n>\n>         I also believe that alternative erasure coding schemes exist\n>         which actually are able to identify the bad pieces given\n>         sufficient good pieces, however I don't know if they have the\n>         same computational performance as the best Reed-Solomon coding\n>         implementations.\n>\n>         *Deployment:*\n>\n>         Small nodes are completely useless unless the critical mass of\n>         5 pieces can be obtained. The first version that supports\n>         small node block downloads should default everyone to an\n>         archival node (meaning indexes 0-4 are used)\n>\n>         Once there are enough small-node-enabled archive nodes, the\n>         default can be switched so that nodes only have a single index\n>         by default. In the first few days, when there are only a few\n>         small nodes, the previously-deployed archival nodes can help\n>         fill in the gaps, and the small nodes can be useful for\n>         blockchain download right away.\n>\n>         ----------------------------------\n>\n>         This represents a non-trivial amount of code, but I believe\n>         that the result would be a non-trivial increase in the\n>         percentage of users running full nodes, and a healthier\n>         overall network.\n>\n>         _______________________________________________\n>         bitcoin-dev mailing list\n>         bitcoin-dev at lists.linuxfoundation.org\n>         <mailto:bitcoin-dev at lists.linuxfoundation.org>\n>         https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>         <https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>\n>\n>\n>     _______________________________________________\n>     bitcoin-dev mailing list\n>     bitcoin-dev at lists.linuxfoundation.org\n>     <mailto:bitcoin-dev at lists.linuxfoundation.org>\n>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>     <https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>\n>\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n-- \nZcash wallets made simple: https://github.com/Ayms/zcash-wallets\nBitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets\nGet the torrent dynamic blocklist: http://peersm.com/getblocklist\nCheck the 10 M passwords list: http://peersm.com/findmyass\nAnti-spies and private torrents, dynamic blocklist: http://torrent-live.org\nPeersm : http://www.peersm.com\ntorrent-live: https://github.com/Ayms/torrent-live\nnode-Tor : https://www.github.com/Ayms/node-Tor\nGitHub : https://www.github.com/Ayms\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170421/4b1d9383/attachment-0001.html>"
            },
            {
                "author": "David Kaufman",
                "date": "2017-04-21T13:35:51",
                "message_text_only": "Hi Danny,\n\nOn Mon, Apr 17, 2017 at 3:11 AM, Danny Thorpe wrote:\n>\n> 1TB HDD is now available for under $40 USD.  How is the 100GB storage\n> requirement preventing anyone from setting up full nodes?\n\nYeah, but that's because most people (well, using myself as the\n\"target market\" anyway) are upgrading to SSD's for the faster boot and\nresponse times.  Modern consumer OS's run incredibly slow on\nnon-ssd drives!  And since the vast majority of consumer laptops sold\ntoday fall into the $400 to $700 range, a 200 - 500gb SSD is about the\nmost storage upgrade people can afford.\n\nAnd so I think David's premise, that having to devote only 30GB to\nrunning a full node instead of 100, would remove a major obstacle that\nprevents many more people running full bitcoin nodes.\n\nMy only suggestion is, does it scale?  I mean, if the bitcoin network\nvolume grows exponentially and in 2 years the blockchain is 500GB, can\nthe \"small node\" be adjusted down from one fifth of the blockchain to\njust one-tenth, or one twentieth?  Can different smalInesses\ninteroperate? Can I choose to store a small node with 20 - 30% of the\nblockchain, while others chose to share just 5% or 10% of it? Can I run\n\"less small\" node today that's 50GB?\n\nCan the default install be a \"small node\" that requires about 30GB of\nstorage (if that is indeed the sweet spot for enticing many more users to\nbringing nodes online), but allow the user at install time, to choose *how*\nsmall? To, say, drag a slider anywhere up and down the range from\n10GB to 100GB?\n\nIf not, then it will have to be revisited constantly as the blockchain\ngrows, and disk storage prices drop.  I suspect the blockchain will\ngrow in size, at some point in the not too distant future, much faster\nthan storage prices drop, so making small, smaller and smallest nodes\nthat can be configured to store more or less of it will be necessary\nto motivate most users to run nodes at all.  But when that happens,\nthere is likely to be exponentially *more* people using bitcoin, too!\nSo an exponentially growing number of users running (smaller and\nsmaller) nodes would take up the slack.\n\nThen, the blockchain would begin to look a lot more like a bittorrent,\nright? ;-) but -- happily -- one that you never need to download fully.\n\n-dave"
            },
            {
                "author": "Leandro Coutinho",
                "date": "2017-04-21T15:58:43",
                "message_text_only": "Maybe it already exists ...\n\n#9484 <https://github.com/bitcoin/bitcoin/pull/9484> 812714f\n<https://github.com/bitcoin/bitcoin/commit/812714f> Introduce assumevalid\nsetting to skip validation presumed valid scripts (gmaxwell)\nhttps://github.com/bitcoin/bitcoin/pull/9484\n\n..., but ...\nIt would be very interesting if a new node could decide to be a pruned node:\n  - it would need to trust one or more peers for the initial blockchain\ndownload, because the blocks downloaded would not be validated\n  - it would decide a time from when to get the blocks, like a week before\n  - once a day a routine would run that would prune blocks older than the\nchosen time\n\n\"\n\n*The unspent transaction outputs (which is the only essential piece ofdata\nnecessary for validation) are already kept in a separate database,so\ntechnically removing old blocks is perfectly possible.*\" Pieter Wuille\nhttps://bitcoin.stackexchange.com/questions/11170/why-is-pruning-not-considered-already-at-the-moment\n\n\nOn Fri, Apr 21, 2017 at 10:35 AM, David Kaufman via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi Danny,\n>\n> On Mon, Apr 17, 2017 at 3:11 AM, Danny Thorpe wrote:\n> >\n> > 1TB HDD is now available for under $40 USD.  How is the 100GB storage\n> > requirement preventing anyone from setting up full nodes?\n>\n> Yeah, but that's because most people (well, using myself as the\n> \"target market\" anyway) are upgrading to SSD's for the faster boot and\n> response times.  Modern consumer OS's run incredibly slow on\n> non-ssd drives!  And since the vast majority of consumer laptops sold\n> today fall into the $400 to $700 range, a 200 - 500gb SSD is about the\n> most storage upgrade people can afford.\n>\n> And so I think David's premise, that having to devote only 30GB to\n> running a full node instead of 100, would remove a major obstacle that\n> prevents many more people running full bitcoin nodes.\n>\n> My only suggestion is, does it scale?  I mean, if the bitcoin network\n> volume grows exponentially and in 2 years the blockchain is 500GB, can\n> the \"small node\" be adjusted down from one fifth of the blockchain to\n> just one-tenth, or one twentieth?  Can different smalInesses\n> interoperate? Can I choose to store a small node with 20 - 30% of the\n> blockchain, while others chose to share just 5% or 10% of it? Can I run\n> \"less small\" node today that's 50GB?\n>\n> Can the default install be a \"small node\" that requires about 30GB of\n> storage (if that is indeed the sweet spot for enticing many more users to\n> bringing nodes online), but allow the user at install time, to choose *how*\n> small? To, say, drag a slider anywhere up and down the range from\n> 10GB to 100GB?\n>\n> If not, then it will have to be revisited constantly as the blockchain\n> grows, and disk storage prices drop.  I suspect the blockchain will\n> grow in size, at some point in the not too distant future, much faster\n> than storage prices drop, so making small, smaller and smallest nodes\n> that can be configured to store more or less of it will be necessary\n> to motivate most users to run nodes at all.  But when that happens,\n> there is likely to be exponentially *more* people using bitcoin, too!\n> So an exponentially growing number of users running (smaller and\n> smaller) nodes would take up the slack.\n>\n> Then, the blockchain would begin to look a lot more like a bittorrent,\n> right? ;-) but -- happily -- one that you never need to download fully.\n>\n> -dave\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170421/a959f172/attachment.html>"
            },
            {
                "author": "Aymeric Vitte",
                "date": "2017-04-17T10:14:25",
                "message_text_only": "While I fully agree with the intent (increasing full nodes so a big\nminer waking up in a bad mood can't threaten the world any longer every\nday as it is now) I am not sure to get the interest of this proposal,\nbecause:\n\n- it's probably not a good idea to encourage the home users to run full\nnodes, there are many people running servers far from their capacity\nthat could easily run efficient full nodes\n\n- if someone can't allocate 100 GB today to run a full node, then we\ncan't expect him to allocate more in the future\n\n- the download time is a real concern\n\n- this proposal is a kind of reinventing torrents, while limiting the\nnumber of connections to something not efficient at all, I don't see why\nsomething that is proven to be super efficient (torrents) would be\nneeded to be reinvented, I am not saying that it should be used as the\nbittorrent network is doing but the concepts can be reused\n\n- I don't get at all the concept of \"archival\" nodes since it's another\nuseless step toward centralization\n\nI think the only way to increase full nodes it to design an incentive\nfor people to run them\n\n\nLe 17/04/2017 \u00e0 08:54, David Vorick via bitcoin-dev a \u00e9crit :\n> *Rationale:*\n>\n> A node that stores the full blockchain (I will use the term archival\n> node) requires over 100GB of disk space, which I believe is one of the\n> most significant barriers to more people running full nodes. And I\n> believe the ecosystem would benefit substantially if more users were\n> running full nodes.\n>\n> The best alternative today to storing the full blockchain is to run a\n> pruned node, which keeps only the UTXO set and throws away already\n> verified blocks. The operator of the pruned node is able to enjoy the\n> full security benefits of a full node, but is essentially leeching the\n> network, as they performed a large download likely without\n> contributing anything back.\n>\n> This puts more pressure on the archival nodes, as the archival nodes\n> need to pick up the slack and help new nodes bootstrap to the network.\n> As the pressure on archival nodes grows, fewer people will be able to\n> actually run archival nodes, and the situation will degrade. The\n> situation would likely become problematic quickly if bitcoin-core were\n> to ship with the defaults set to a pruned node.\n>\n> Even further, the people most likely to care about saving 100GB of\n> disk space are also the people least likely to care about some extra\n> bandwidth usage. For datacenter nodes, and for nodes doing lots of\n> bandwidth, the bandwidth is usually the biggest cost of running the\n> node. For home users however, as long as they stay under their\n> bandwidth cap, the bandwidth is actually free. Ideally, new nodes\n> would be able to bootstrap from nodes that do not have to pay for\n> their bandwidth, instead of needing to rely on a decreasing percentage\n> of heavy-duty archival nodes.\n>\n> I have (perhaps incorrectly) identified disk space consumption as the\n> most significant factor in your average user choosing to run a pruned\n> node or a lite client instead of a full node. The average user is not\n> typically too worried about bandwidth, and is also not typically too\n> worried about initial blockchain download time. But the 100GB hit to\n> your disk space can be a huge psychological factor, especially if your\n> hard drive only has 500GB available in the first place, and 250+ GB is\n> already consumed by other files you have.\n>\n> I believe that improving the disk usage situation would greatly\n> benefit decentralization, especially if it could be done without\n> putting pressure on archival nodes.\n>\n> *Small Nodes Proposal:*\n>\n> I propose an alternative to the pruned node that does not put undue\n> pressure on archival nodes, and would be acceptable and non-risky to\n> ship as a default in bitcoin-core. For lack of a better name, I'll\n> call this new type of node a 'small node'. The intention is that\n> bitcoin-core would eventually ship 'small nodes' by default, such that\n> the expected amount of disk consumption drops from today's 100+ GB to\n> less than 30 GB.\n>\n> My alternative proposal has the following properties:\n>\n> + Full nodes only need to store ~20% of the blockchain\n> + With very high probability, a new node will be able to recover the\n> entire blockchain by connecting to 6 random small node peers.\n> + An attacker that can eliminate a chosen+ 95% of the full nodes\n> running today will be unable to prevent new nodes from downloading the\n> full blockchain, even if the attacker is also able to eliminate all\n> archival nodes. (assuming all nodes today were small nodes instead of\n> archival nodes)\n>\n> Method:\n>\n> A small node will pick an index [5, 256). This index is that node's\n> permanent index. When storing a block, instead of storing the full\n> block, the node will use Reed-Solomon coding to erasure code the block\n> using a 5-of-256 scheme. The result will be 256 pieces that are 20% of\n> the size of the block each. The node picks the piece that corresponds\n> to its index, and stores that instead. (Indexes 0-4 are reserved for\n> archival nodes - explained later)\n>\n> The node is now storing a fragment of every block. Alone, this\n> fragment cannot be used to recover any piece of the blockchain.\n> However, when paired with any 5 unique fragments (fragments of the\n> same index will not be unique), the full block can be recovered.\n>\n> Nodes can optionally store more than 1 fragment each. At 5 fragments,\n> the node becomes a full archival node, and the chosen indexes should\n> be 0-4. This is advantageous for the archival node as the encoded data\n> for the first 5 indexes will actually be identical to the block itself\n> - there is no computational overhead for selecting the first indexes.\n> There is also no need to choose random indexes, because the full block\n> can be recovered no matter which indexes are chosen.\n>\n> When connecting to new peers, the indexes of each peer needs to be\n> known. Once peers totaling 5 unique indexes are discovered, blockchain\n> download can begin. Connecting to just 5 small node peers provides a\n> >95% chance of getting 5 uniques, with exponentially improving odds of\n> success as you connect to more peers. Connecting to a single archive\n> node guarantees that any gaps can be filled.\n>\n> A good encoder should be able to turn a block into a 5-of-256 piece\n> set in under 10 milliseconds using a single core on a standard\n> consumer desktop. This should not slow down initial blockchain\n> download substantially, though the overhead is more than a rounding error.\n>\n> *DoS Prevention:*\n>\n> A malicious node may provide garbage data instead of the actual piece.\n> Given just the garbage data and 4 other correct pieces, it is\n> impossible (best I know anyway) to tell which piece is the garbage piece.\n>\n> One option in this case would be to seek out an archival node that\n> could verify the correctness of the pieces, and identify the malicious\n> node.\n>\n> Another option would be to have the small nodes store a cryptographic\n> checksum of each piece. Obtaining the cryptographic checksum for all\n> 256 pieces would incur a nontrivial amount of hashing (post segwit, as\n> much as 100MB of extra hashing per block), and would require an\n> additional ~4kb of storage per block. The hashing overhead here may be\n> prohibitive.\n>\n> Another solution would be to find additional pieces and brute-force\n> combinations of 5 until a working combination was discovered. Though\n> this sounds nasty, it should take less than five seconds of\n> computation to find the working combination given 5 correct pieces and\n> 2 incorrect pieces. This computation only needs to be performed once\n> to identify the malicious peers.\n>\n> I also believe that alternative erasure coding schemes exist which\n> actually are able to identify the bad pieces given sufficient good\n> pieces, however I don't know if they have the same computational\n> performance as the best Reed-Solomon coding implementations.\n>\n> *Deployment:*\n>\n> Small nodes are completely useless unless the critical mass of 5\n> pieces can be obtained. The first version that supports small node\n> block downloads should default everyone to an archival node (meaning\n> indexes 0-4 are used)\n>\n> Once there are enough small-node-enabled archive nodes, the default\n> can be switched so that nodes only have a single index by default. In\n> the first few days, when there are only a few small nodes, the\n> previously-deployed archival nodes can help fill in the gaps, and the\n> small nodes can be useful for blockchain download right away.\n>\n> ----------------------------------\n>\n> This represents a non-trivial amount of code, but I believe that the\n> result would be a non-trivial increase in the percentage of users\n> running full nodes, and a healthier overall network.\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n-- \nZcash wallets made simple: https://github.com/Ayms/zcash-wallets\nBitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets\nGet the torrent dynamic blocklist: http://peersm.com/getblocklist\nCheck the 10 M passwords list: http://peersm.com/findmyass\nAnti-spies and private torrents, dynamic blocklist: http://torrent-live.org\nPeersm : http://www.peersm.com\ntorrent-live: https://github.com/Ayms/torrent-live\nnode-Tor : https://www.github.com/Ayms/node-Tor\nGitHub : https://www.github.com/Ayms\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170417/90387873/attachment-0001.html>"
            },
            {
                "author": "David Vorick",
                "date": "2017-04-19T17:30:30",
                "message_text_only": "On Tue, Apr 18, 2017 at 3:43 AM, Jonas Schnelli <dev at jonasschnelli.ch>\nwrote:\n\n>\n> Hi Dave\n>\n> *1. I agree that we need to have a way for pruned nodes to partially serve\n> historical blocks.*\n> My personal measurements told me that around ~80% of historical block\n> serving are between tip and -1\u2019000 blocks.\n> Currently, Core nodes have only two modes of operations, \u201eserver all\n> historical blocks\u201c or \u201enone\u201c.\n> This makes little sense especially if you prune to a target size of, lets\n> say, 80GB (~80% of the chain).\n> Ideally, there would be a mode where your full node can signal a third\n> mode \u201eI keep the last 1000 blocks\u201c (or make this more dynamic).\n>\n\nThat probably makes sense with small nodes too. The past 1000 blocks are\nsuch a small footprint compared to the rest of the chain.\n\n\n>\n> *2. Bootstrapping new peers*\n> I\u2019m not sure if full nodes must be the single point of historical data\n> storage. Full nodes provide a valuable service (verification, relay,\n> filtering, etc.). I\u2019m not sure if serving historical blocks is one of them.\n> Historical blocks could be made available on CDN\u2019s or other file storage\n> networks. You are going to verify them anyways,... the serving part is pure\n> data storage.\n> I\u2019m also pretty sure that some users have stopping running full nodes\n> because their upstream bandwidth consumption (because of serving historical\n> blocks) was getting intolerable.\n> Especially \u201econsumer\u201c peers must have been hit by this (little experience\n> in how to reduce traffic, upstream in general is bad for\n> consumers-connections, little resources in general).\n>\n\nPerhaps it is not, but I would think that it would be pretty\nstraightforward to configure a global bandwidth limit within Bitcoin. I\nknow many torrent clients, and clients for protocols like Tor and i2p\ninclude the ability to set both speed limits and monthly bandwidth limits.\nShipping core with sane default limits is probably sufficient to solve\nbandwidth issues for most users. I don't know if default limits may result\nin today's archive nodes pulling less weight though - changing the defaults\nto have limits may slow the network down as a whole.\n\nIn my experience (living in a city where most people have uncapped\nconnections), disk usage is usually the bigger issue, but certainly\nbandwidth is a known problem (especially for rural users) as well.\n\n\n>\n> Having a second option built into full nodes (or as an external bootstrap\n> service/app) how to download historical blocks during bootstrapping could\n> probably be a relieve for \"small nodes\u201c.\n> It could be a little daemon that downloads historical blocks from CDN\u2019s,\n> etc. and feeds them into your full node over p2p/8333 and kickstarts your\n> bootstrapping without bothering valuable peers.\n> Or, the alternative download, could be built into the full nodes main\n> logic.\n> And, if it wasn\u2019t obvious, this must not bypass the verification!\n>\n\nI worry about any type of CDN being a central point of failure. CDNs cost\nmoney, which means someone is footing the bill. Torrenting typically relies\non a DHT, which is much easier to attack than Bitcoin's peer network. It's\npossible that a decentralized CDN could be used, but I don't think any yet\nexist (though I am building one for unrelated reasons) which are both\nsufficiently secure and incentive-compatible to be considered as an\nalternative to using full nodes to bootstrap.\n\nI just don't want to end up in a situation where 90% of users are getting\ntheir blockchain from the same 3 websites or centralized services. And I\nalso don't want to rely on any p2p solution which would not stand up to a\nserious adversary. Right now, I think the bitcoin p2p network is by\nsignificant margin the best we've got. The landscape for decentralized data\ndistribution is evolving rapidly though, perhaps in a few years there will\nbe a superior alternative.\n\n\n> *To your proposal:*\n> - Isn\u2019t there a tiny finger-printing element if peers have to pick an\n> segmentation index?\n> - SPV bloom filter clients can\u2019t use fragmented blocks to filter txns?\n> Right? How could they avoid connecting to those peers?\n>\n> </jonas>\n>\n\nYes, there is finger-print that happens if you have nodes pick an index.\nAnd the fingerprint gets a lot worse if you have a node pick multiple\nindexes. Though, isn't it already required that nodes have some sort of IP\naddress or hidden service domain? I want to say that the fingerprint\ncreated by picking an index is not a big deal, because it can be separated\nfrom activity like transaction relaying and mining. Though, I am not\ncertain and perhaps it is a problem.\n\nTo be honest, I hadn't really considered SPV nodes at the time of writing.\nSmall nodes would still be seeing all of the new blocks, and per your\nsuggestion may also be storing the 1000 or so most recent blocks, but SPV\nnodes would still need some way to find all of their historical\ntransactions. The problem is not fetching blocks, it's figuring out which\nblocks are worth fetching. It may be sufficient to have nodes store a bloom\nfilter for each block indicating which addresses had activity. The bloom\nfilter would probably only need to be about 1% of the size of the full\nblock. That's not too much overhead (now you are storing 21% of the block\ninstead of just 20%), and would retain SPV compatibility.\n\nOn Mon, Apr 17, 2017 at 12:17 PM, praxeology_guy <\npraxeology_guy at protonmail.com> wrote:\n\n>\n> FYI With unspent coin snapshots, needing archive data becomes less\n> important.  People can synchronize from a later snapshot instead of the\n> genesis block.  See https://petertodd.org/2016/delayed-txo-commitments\n> for my current favorite idea.\n>\n>\nThis is something that I think could help a lot too. If the build processes\nincluded verifying the chain and then creating a utxo snapshot at say,\nblock 400,000, then nodes would no longer need to download, store, upload,\nor share blocks prior to that height. It means that a reorg deeper than\nthat point would hardfork the network. But a reorg 60k blocks deep is going\nto cause problems worse than a network split. Then, the only people who\nwould ever need to care about the early blocks are developers, and it's\nmore reasonable to expect developers to go through a longer process and\nhave more resources than everyday users.\n\nOn Mon, Apr 17, 2017 at 6:14 AM, Aymeric Vitte via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> While I fully agree with the intent (increasing full nodes so a big miner\n> waking up in a bad mood can't threaten the world any longer every day as it\n> is now) I am not sure to get the interest of this proposal, because:\n>\n> - it's probably not a good idea to encourage the home users to run full\n> nodes, there are many people running servers far from their capacity that\n> could easily run efficient full nodes\n>\nRunning a full node is the only way to avoid needing to trust others. It's\nalso how you make your opinion worthwhile for events like hard forks and\nUASFs. If decentralization is the primary motivation, it absolutely makes\nsense to encourage people to run their own full nodes. Without a full node,\nyou are at the mercy of the governance decisions by those who do have full\nnodes. But if you have a full node, you can chose to opt-out of any upgrade\n(example: ethereum classic nodes).\n\n\n> - if someone can't allocate 100 GB today to run a full node, then we can't\n> expect him to allocate more in the future\n>\nThat's why I'm proposing something to decrease the storage requirements.\n\n\n> - this proposal is a kind of reinventing torrents, while limiting the\n> number of connections to something not efficient at all, I don't see why\n> something that is proven to be super efficient (torrents) would be needed\n> to be reinvented, I am not saying that it should be used as the bittorrent\n> network is doing but the concepts can be reused\n>\nIt's different from torrents in that it uses specialized erasure coding to\nmake sure that every block is always available, even if an adversary is\nrunning around targeting all the nodes with a particular piece.\n\n\n> - I don't get at all the concept of \"archival\" nodes since it's another\n> useless step toward centralization\n>\n\"archival\" nodes are simply nodes with the full blockchain. Nobody can\nbootstrap on the network without them. Today, every bitcoin-core node is an\narchival node by default.\n\n> I think the only way to increase full nodes it to design an incentive for\n> people to run them\n>\nThe primary incentive is the sovereignty that it gives you. Running a\nBitcoin full node gives you security and protection against political\ngarbage that you can't get any other way. The network does currently depend\non altruism to allow people to download the blockchain, but as long as we\ncan keep the resource requirements of this altruism low, I think we can\nexpect it to continue. This proposal attempts to keep those requirements\nlow.\n\n\n\nOn Tue, Apr 18, 2017 at 6:50 AM, Tom Zander <tomz at freedommail.ch> wrote:\n\n> On Monday, 17 April 2017 08:54:49 CEST David Vorick via bitcoin-dev wrote:\n> > The best alternative today to storing the full blockchain is to run a\n> > pruned node\n>\n> The idea looks a little overly complex to me.\n>\n> I suggested something similar which is a much simpler version;\n> https://zander.github.io/scaling/Pruning/\n>\n> > # Random pruning mode\n> >\n> > There is a large gap between the two current modes of everything\n> > (currently 75GB) and only what we need (2GB or so).\n> >\n> > This mode would have two areas, it would keep a days worth of blocks to\n> > make sure that any reorgs etc would not cause a re-download, but it would\n> > have additionally have an area that can be used to store historical data\n> > to be shared on the network. Maybe 20 or 50GB.\n> >\n> > One main feature of Bitcoin is that we have massive replication. Each\n> node\n> > currently holds all the same data that every other node holds. But this\n> > doesn't have to be the case with pruned nodes. A node itself has no need\n> > for historic data at all.\n> >\n> > The suggestion is that a node stores a random set of blocks. Dropping\n> > random blocks as the node runs out of disk-space. Additionally, we would\n> > introduce a new way to download blocks from other nodes which allows the\n> > node to say it doesn't actually have the block requested.\n> >\n> > The effect of this setup is that many different nodes together end up\n> > having the total amount of blocks, even though each node only has a\n> > fraction of the total amount.\n>\n> --\n> Tom Zander\n> Blog: https://zander.github.io\n> Vlog: https://vimeo.com/channels/tomscryptochannel\n>\n\nYour proposal has a significant disadvantage: If every peer is dropping 75%\nof all blocks randomly, then you need to connect to a large number of peers\nto download the whole blockchain. Any given peer has a 25% chance of\nholding a block, or rather, a 75% chance of not holding a block. If you\nhave n peers, your probability of not being able to download a given block\nis 0.75^n. If you are downloading 450,000 blocks, you will need to connect\nto an expected 46 peers to download the whole blockchain.\n\nYour proposal is also a lot less able to handle active adversaries: if\nnodes are randomly dropping blocks, the probability that one block in\nparticular is dropped by everyone goes up significantly. And the problem\ngets a lot worse in the presence of an active adversary. If there are 8000\nnodes each dropping 75% of the blocks, then each block on average will only\nbe held by 2000 nodes. Probabilistically, some unlucky blocks will be held\nby fewer than 2000 nodes. An active adversary needs only to eliminate about\n2000 nodes (a chosen 2000 nodes) to knock a block off of the network. But\nmissing even a single block is a significant problem.\n\nYour proposal essentially requires that archive nodes still exist and be a\npart of a typical blockchain download. Given that, I don't think it would\nbe a sound idea to ship as a default in bitcoin core.\n\n\n\nOn Tue, Apr 18, 2017 at 9:07 AM, Tier Nolan via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> This has been discussed before.\n>\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/\n> 2015-May/008101.html\n>\n> including a list of nice to have features by Maxwell\n>\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/\n> 2015-May/008110.html\n>\n> You meet most of these rules, though you do have to download blocks from\n> multiple peers.\n>\n> The suggestion in that thread were for a way to compactly indicate which\n> blocks a node has.  Each node would then store a sub-set of all the\n> blocks.  You just download the blocks you want from the node that has them.\n>\n> Each node would be recommended to store the last few days worth anyway.\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\nI believe that my proposal does meet all of the requirements listed by\nMaxwell. Having a set of 8 random peers gives you a very high probability\nof being able to recover every single block. You would need to connect to\nat least 5 peers (and this is already >90% likely to be sufficient to\nrecover every block), but if you cannot connect to 5 random peers your node\nis probably in trouble anyway. Highly parallel, high speed downloads are\njust as possible with small nodes as with archive nodes. It only takes a\nfew bytes to indicate which part of the blockchain you have, and any 2\npeers have a less than 1% chance of overlapping.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170419/88f15eb2/attachment-0001.html>"
            },
            {
                "author": "Tom Zander",
                "date": "2017-04-20T09:46:33",
                "message_text_only": "On Wednesday, 19 April 2017 19:30:30 CEST David Vorick via bitcoin-dev \nwrote:\n> > I suggested something similar which is a much simpler version;\n> > https://zander.github.io/scaling/Pruning/\n\n> Your proposal has a significant disadvantage: If every peer is dropping\n> 75% of all blocks randomly, then you need to connect to a large number of\n> peers to download the whole blockchain.\n...\n> If you are downloading 450,000 blocks, you will need to\n> connect to an expected 46 peers to download the whole blockchain.\n\nI don\u2019t really see the problem here, even if your math is a off. (Statistics \nis difficult, I know). Connecting to many nodes to download faster is really \nnot an issue and already happens.\n\n> Your proposal is also a lot less able to handle active adversaries: if\n> nodes are randomly dropping blocks, the probability that one block in\n> particular is dropped by everyone goes up significantly. \n\nYou make the assumption that this new mode of pruning will be used by 100% \nof the network, this is not how distributed systems work.\n\n-- \nTom Zander\nBlog: https://zander.github.io\nVlog: https://vimeo.com/channels/tomscryptochannel"
            },
            {
                "author": "Andrew Poelstra",
                "date": "2017-04-20T20:32:12",
                "message_text_only": "On Thu, Apr 20, 2017 at 11:46:33AM +0200, Tom Zander via bitcoin-dev wrote:\n> On Wednesday, 19 April 2017 19:30:30 CEST David Vorick via bitcoin-dev \n> wrote:\n> > > I suggested something similar which is a much simpler version;\n> > > https://zander.github.io/scaling/Pruning/\n> \n> > Your proposal has a significant disadvantage: If every peer is dropping\n> > 75% of all blocks randomly, then you need to connect to a large number of\n> > peers to download the whole blockchain.\n> ...\n> > If you are downloading 450,000 blocks, you will need to\n> > connect to an expected 46 peers to download the whole blockchain.\n> \n> I don\u2019t really see the problem here, even if your math is a off. (Statistics \n> is difficult, I know). Connecting to many nodes to download faster is really \n> not an issue and already happens.\n>\n\nI think the expected number of peers is actually ~47.75, which is pretty\nclose to David's estimate, which was wrong in a way that was actually\nmore favorable to the \"everyone stores random blocks\" scheme than the\ntruth.\n\nEven assuming no archival nodes, and all nodes storing only one random\nindex between 5 and 255 inclusive, the chance of five arbitrary nodes\ngiving unique indices by chance is about 98.4%. To get the same probability\nfrom a scheme where each peer has only 25% of the blocks, you need to\nconnect to 59.59 nodes.\n\nThis is over a ten-times increase in the number of nodes required to\ndownload the entire chain, and requires participating nodes to use 25%\nmore space than David's proposal.\n\n> > Your proposal is also a lot less able to handle active adversaries: if\n> > nodes are randomly dropping blocks, the probability that one block in\n> > particular is dropped by everyone goes up significantly. \n> \n> You make the assumption that this new mode of pruning will be used by 100% \n> of the network, this is not how distributed systems work.\n>\n\nStoring random but complete blocks requires the assumption this is _not_ the\ncase; David's does not make any assumptions. So on top of the performance\nconsiderations there is this potential DoS vector.\n \n\n-- \nAndrew Poelstra\nMathematics Department, Blockstream\nEmail: apoelstra at wpsoftware.net\nWeb:   https://www.wpsoftware.net/andrew\n\n\"A goose alone, I suppose, can know the loneliness of geese\n who can never find their peace,\n whether north or south or west or east\"\n       --Joanna Newsom\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170420/2fc51438/attachment.sig>"
            },
            {
                "author": "Tom Zander",
                "date": "2017-04-21T08:27:36",
                "message_text_only": "On Thursday, 20 April 2017 22:32:12 CEST Andrew Poelstra wrote:\n> > > If you are downloading 450,000 blocks, you will need to\n> > > connect to an expected 46 peers to download the whole blockchain.\n> > \n> > I don\u2019t really see the problem here, even if your math is a off.\n> > (Statistics is difficult, I know). Connecting to many nodes to download\n> > faster is really not an issue and already happens.\n> \n> I think the expected number of peers is actually ~47.75\n\nNice to join bitcoin-dev, Andrew. Haven\u2019t seen you post here before.\n\nI\u2019m not sure how you reached that strange number, but I have to point out \nyour number is quite useless.\n\nThe actual amount of nodes you need to be 100% sure you find all the blocks \nwhen you know each node will have a completely random 25% of the blocks is \nnot a maths problem that leads to a single answer because of the randomness \ninvolved.\nThe actual answer is a series of probabilities.\n\nSame as the answer is to the age old question; how many coin flips does it \ntake to be 100% certain I have at least one \u201cHeads\u201d.\n\nIn our blocks retrieval scenario; with num-nodes < 4, probability is zero.\nThere is a really really small chance you will get 100% of the blocks with 4 \nnodes (actual number depends on the amount of total blocks you are looking \nfor).\nAnd this goes up as you add more nodes, but never reaches 100%\n\nAt the other end of this question you can ask what the chance is of at least \none block being lost when there are N nodes, a block nobody has. That chance \nis small with current > 6000 nodes, but not zero (a second reason why the \nprevious parag never reaches 100%).\n\nBottom line, it is silly to assume 100% of the nodes would be partial-\npruning, and if you continue on that path you will only have probabilities \nto predict how many nodes it takes to have 100% coverage, exact numbers are \nworse than useless, they are misleading.\n\nAs I said in my initial email, statistics is hard. Crypto is much easier in \nthat it is absolute. Either correct or false. Never in between.\n\nTo repeat, the goal of this pruning method is not to replace a full \n\u201carchival\u201d node, the goal of this pruning node is to provide an improvement \nover the current pruning node which stops any and all serving of historical \nblocks.\nAnyone that feels the need to talk about pruning modes like 100% of the full \nnodes will run it are in actual fact not talking about the real world. \nDistributed systems will never (and should never) end up being a mono-\nculture. Diversity is the essential thing you aim for.\n\nI would suggest we focus on the real world and not on irreleavant math \nexperiments that only lead to confusion.\n-- \nTom Zander\nBlog: https://zander.github.io\nVlog: https://vimeo.com/channels/tomscryptochannel"
            },
            {
                "author": "Aymeric Vitte",
                "date": "2017-04-20T11:27:32",
                "message_text_only": "Thanks but you did not answer all the points and some of your statements\nlook wrong, like the global ideas behind this proposal from my\nstandpoint, which basically is inventing strange things not reusing what\nis already proven to be working well and could provide the same result,\nwhich at the end is not the expected one, ie increasing full nodes, it\nsounds like a strange workaround to prevent the centralization of the\nblockchain when pruning will become the default\n\nTo answer some other comments in this thread, giving an incentive to run\nfull nodes does not mean that someone setting up tomorrow 10K nodes will\nbecome rich and/or will be able to control the network, the later being\nnot unlikely at all to happen in the current situation, the idea is more\nto motivate people that already have the resources to run full nodes,\nthen we mix the concepts of optimizing the resources at no additional\ncosts (and even decreasing costs since you get rewarded for the part\nthat you have already paid but don't use) with the one of running nodes\nto protect its business\n\nFor example\nhttps://gist.github.com/Ayms/aab6f8e08fef0792ab3448f542a826bf#proposal\nis showing some concepts where nodes can't position themselves where\nthey like and are registered in the system by the others (but forget the\nproof of something as written in this gist since I think the rewards\nshould not depend on usual miners) , so it becomes quite difficult that\nthey position themselves where they like to possibly get the rewards,\nfake the system, freeride, cheat, collude in pools or setup plenty of nodes\n\nComments below\n\n\nLe 19/04/2017 \u00e0 19:30, David Vorick via bitcoin-dev a \u00e9crit :\n> On Tue, Apr 18, 2017 at 3:43 AM, Jonas Schnelli <dev at jonasschnelli.ch\n> <mailto:dev at jonasschnelli.ch>> wrote:\n>\n>     I know many torrent clients, and clients for protocols like Tor\n>     and i2p include the ability to set both speed limits and monthly\n>     bandwidth limits.\n>\n\nYes, that's the easy part, the issue is more for the network to check\nthat users have sufficient bandwidth and don't cheat\n\n> I worry about any type of CDN being a central point of failure.\n\nOf course\n\n>  Torrenting typically relies on a DHT, which is much easier to attack\n> than Bitcoin's peer network.\n\nThen please explain how you would attack the bittorrent DHT and why it's\n\"much easier\" than attacking the btc network today, bittorrent is not\ndesigned for security/privacy, including its DHT, which btw is great,\nit's a common sign of misinformation to conclude that all DHTs are\nnecessarily insecure\n\n> I think the bitcoin p2p network is by significant margin the best\n> we've got.\n\nThe btc network can't be considered as a p2p network in its current form\nthen can't be the best one for now (and if it was then we would not be\nin today's situation)\n\n>\n> Yes, there is finger-print that happens if you have nodes pick an\n> index. And the fingerprint gets a lot worse if you have a node pick\n> multiple indexes.\n\nThis is another problem of your proposal, as well as\nfingerprinting/tracking peers based on what they have\n\n> Though, isn't it already required that nodes have some sort of IP\n> address or hidden service domain? I want to say that the fingerprint\n> created by picking an index is not a big deal, because it can be\n> separated from activity like transaction relaying and mining. Though,\n> I am not certain and perhaps it is a problem.\n\nAre you suggesting that the btc \"p2p\" network should be using the Tor\nnetwork, and especially the nodes hosting the/(a part of) the\nblockchain? This is of course a very bad idea and you would not\neliminate the tracking issue, a simple example is that despite ot the\nsize of the network it's not difficult to track the peers on the\nbittorrent network, you might not know who is the peer but you can\nfollow whatever he is doing, and hidding behind Tor or a VPN does not\nprevent this\n\n>\n>\n>\n> On Mon, Apr 17, 2017 at 6:14 AM, Aymeric Vitte via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org\n> <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n>\n>     While I fully agree with the intent (increasing full nodes so a\n>     big miner waking up in a bad mood can't threaten the world any\n>     longer every day as it is now) I am not sure to get the interest\n>     of this proposal, because:\n>\n>     - it's probably not a good idea to encourage the home users to run\n>     full nodes, there are many people running servers far from their\n>     capacity that could easily run efficient full nodes\n>\n> Running a full node is the only way to avoid needing to trust others.\n> It's also how you make your opinion worthwhile for events like hard\n> forks and UASFs. If decentralization is the primary motivation, it\n> absolutely makes sense to encourage people to run their own full\n> nodes. Without a full node, you are at the mercy of the governance\n> decisions by those who do have full nodes. But if you have a full\n> node, you can chose to opt-out of any upgrade (example: ethereum\n> classic nodes).\n\nIf you really know the Tor network, then you know why encouraging home\nusers to run full nodes is probably not a good idea\n\n\"Probably\" because the situation is not the same for btc and indeed UASF\nfor example is referring to \"users\" who today are not really \"users\" but\nintermediate nodes, so the decision finally is not made by the users\n\n>  \n>\n>     - if someone can't allocate 100 GB today to run a full node, then\n>     we can't expect him to allocate more in the future\n>\n> That's why I'm proposing something to decrease the storage requirements.\n\nThis is just delaying the problem, you are just proposing to store some\nparts of the blockchain not explaining how the peers will first setup\nthe nodes, some parts that will of course increase... then falling back\nin the issue that you are trying to address\n\n>  \n>\n>     - this proposal is a kind of reinventing torrents, while limiting\n>     the number of connections to something not efficient at all, I\n>     don't see why something that is proven to be super efficient\n>     (torrents) would be needed to be reinvented, I am not saying that\n>     it should be used as the bittorrent network is doing but the\n>     concepts can be reused\n>\n> It's different from torrents in that it uses specialized erasure\n> coding to make sure that every block is always available, even if an\n> adversary is running around targeting all the nodes with a particular\n> piece.\n\nYou are reinventing something that would be achieved easily using the\nconcepts of torrents or incremental ones (ie someone would seed the\nwhole thing, some others some parts of it, etc)\n\n>  \n>\n>     - I don't get at all the concept of \"archival\" nodes since it's\n>     another useless step toward centralization\n>\n> \"archival\" nodes are simply nodes with the full blockchain. Nobody can\n> bootstrap on the network without them. Today, every bitcoin-core node\n> is an archival node by default.\n\nWhat I meant is that you can't build a hierarchy of btc nodes: big\nnodes, medium nodes, small nodes, each node is free to decide how/if it\nparticipates to the network, so the wording of \"archival\" nodes for me\nis not adapted since it makes immediately think to centralized entities,\nbig organizations hosting the blockchain, etc\n\n>     I think the only way to increase full nodes it to design an\n>     incentive for people to run them\n>\n> The primary incentive is the sovereignty that it gives you. Running a\n> Bitcoin full node gives you security and protection against political\n> garbage that you can't get any other way. The network does currently\n> depend on altruism to allow people to download the blockchain, but as\n> long as we can keep the resource requirements of this altruism low, I\n> think we can expect it to continue. This proposal attempts to keep\n> those requirements low.\n\nThis is the usual answer but I don't believe it, people will rely on\nothers to run full nodes and secure them, and so on...\n\n>\n>\n>\n> I believe that my proposal does meet all of the requirements listed by\n> Maxwell.\n\nI did noy read them again, some others are listed in the link above\n\n> Having a set of 8 random peers gives you a very high probability of\n> being able to recover every single block. You would need to connect to\n> at least 5 peers (and this is already >90% likely to be sufficient to\n> recover every block), but if you cannot connect to 5 random peers your\n> node is probably in trouble anyway. Highly parallel, high speed\n> downloads are just as possible with small nodes as with archive nodes.\n> It only takes a few bytes to indicate which part of the blockchain you\n> have, and any 2 peers have a less than 1% chance of overlapping.\n\nAgain, you don't explain how you bootstrap the full nodes first, which\nis the main issue, and if the idea is that the pruning nodes will never\ndesync then you should try downloading x GB to resync connecting to 5/8\npeers possibly operating from home in different countries\n\n-- \nZcash wallets made simple: https://github.com/Ayms/zcash-wallets\nBitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets\nGet the torrent dynamic blocklist: http://peersm.com/getblocklist\nCheck the 10 M passwords list: http://peersm.com/findmyass\nAnti-spies and private torrents, dynamic blocklist: http://torrent-live.org\nPeersm : http://www.peersm.com\ntorrent-live: https://github.com/Ayms/torrent-live\nnode-Tor : https://www.github.com/Ayms/node-Tor\nGitHub : https://www.github.com/Ayms\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170420/6fff421d/attachment-0001.html>"
            },
            {
                "author": "Jonas Schnelli",
                "date": "2017-04-18T07:43:52",
                "message_text_only": "Hi Dave\n\n> A node that stores the full blockchain (I will use the term archival node) requires over 100GB of disk space, which I believe is one of the most significant barriers to more people running full nodes. And I believe the ecosystem would benefit substantially if more users were running full nodes.\n\n\nThanks for your proposal.\n\nI agree that 100GB of data may be cumbersome for some systems, especially if you target end user systems (Laptops/Desktops). Though, in my opinion, for those systems, CPU consumption is the biggest UX blocker.\nBootstrapping a full node on a decent consumer system with default parameters takes days, and, during this period, you probably run at full CPU capacity and you will be disturbed by constant fan noise. Standard tasks may be impossible because your system will be slowed down to a point where even word processing may get difficult.\nThis is because Core (with its default settings) is made to sync as fast as possible.\n\nOnce you have verified the chain and you reach the chain tip, indeed, it will be much better (until you shutdown for a couple of days/hours and have to re-sync/catch-up).\n\n1. I agree that we need to have a way for pruned nodes to partially serve historical blocks.\nMy personal measurements told me that around ~80% of historical block serving are between tip and -1\u2019000 blocks.\nCurrently, Core nodes have only two modes of operations, \u201eserver all historical blocks\u201c or \u201enone\u201c.\nThis makes little sense especially if you prune to a target size of, lets say, 80GB (~80% of the chain).\nIdeally, there would be a mode where your full node can signal a third mode \u201eI keep the last 1000 blocks\u201c (or make this more dynamic).\n\n2. Bootstrapping new peers\nI\u2019m not sure if full nodes must be the single point of historical data storage. Full nodes provide a valuable service (verification, relay, filtering, etc.). I\u2019m not sure if serving historical blocks is one of them. Historical blocks could be made available on CDN\u2019s or other file storage networks. You are going to verify them anyways,... the serving part is pure data storage.\nI\u2019m also pretty sure that some users have stopping running full nodes because their upstream bandwidth consumption (because of serving historical blocks) was getting intolerable.\nEspecially \u201econsumer\u201c peers must have been hit by this (little experience in how to reduce traffic, upstream in general is bad for consumers-connections, little resources in general).\n\nHaving a second option built into full nodes (or as an external bootstrap service/app) how to download historical blocks during bootstrapping could probably be a relieve for \"small nodes\u201c.\nIt could be a little daemon that downloads historical blocks from CDN\u2019s, etc. and feeds them into your full node over p2p/8333 and kickstarts your bootstrapping without bothering valuable peers.\nOr, the alternative download, could be built into the full nodes main logic.\nAnd, if it wasn\u2019t obvious, this must not bypass the verification!\n\nI\u2019m also aware of the downsides of this. This can eventually reduce decentralisation of the storage of historical bitcoin blockchain data and \u2013 eventually \u2013 increase the upstream bandwidth of peers willing to serve historical blocks (especially in a transition phase to a second \u201edownload\u201c-option).\nMaybe it\u2019s a tradeoff between reducing decentralisation by killing low resource nodes because serving historical blocks is getting too resource-intense _or_ reducing decentralisation by moving some percentage of the historical data storage away from the bitcoin p2p network.\nThe later seems more promising to me.\n\n\nTo your proposal:\n- Isn\u2019t there a tiny finger-printing element if peers have to pick an segmentation index?\n- SPV bloom filter clients can\u2019t use fragmented blocks to filter txns? Right? How could they avoid connecting to those peers?\n\n</jonas>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170418/9cf44da5/attachment.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: Message signed with OpenPGP\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170418/9cf44da5/attachment.sig>"
            },
            {
                "author": "Tom Zander",
                "date": "2017-04-18T10:50:31",
                "message_text_only": "On Monday, 17 April 2017 08:54:49 CEST David Vorick via bitcoin-dev wrote:\n> The best alternative today to storing the full blockchain is to run a\n> pruned node\n\nThe idea looks a little overly complex to me.\n\nI suggested something similar which is a much simpler version;\nhttps://zander.github.io/scaling/Pruning/\n\n> # Random pruning mode\n> \n> There is a large gap between the two current modes of everything\n> (currently 75GB) and only what we need (2GB or so).\n> \n> This mode would have two areas, it would keep a days worth of blocks to\n> make sure that any reorgs etc would not cause a re-download, but it would\n> have additionally have an area that can be used to store historical data\n> to be shared on the network. Maybe 20 or 50GB.\n> \n> One main feature of Bitcoin is that we have massive replication. Each node\n> currently holds all the same data that every other node holds. But this\n> doesn't have to be the case with pruned nodes. A node itself has no need\n> for historic data at all.\n> \n> The suggestion is that a node stores a random set of blocks. Dropping\n> random blocks as the node runs out of disk-space. Additionally, we would\n> introduce a new way to download blocks from other nodes which allows the\n> node to say it doesn't actually have the block requested.\n> \n> The effect of this setup is that many different nodes together end up\n> having the total amount of blocks, even though each node only has a\n> fraction of the total amount.\n\n-- \nTom Zander\nBlog: https://zander.github.io\nVlog: https://vimeo.com/channels/tomscryptochannel"
            },
            {
                "author": "Tier Nolan",
                "date": "2017-04-18T13:07:09",
                "message_text_only": "This has been discussed before.\n\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-May/008101.html\n\nincluding a list of nice to have features by Maxwell\n\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-May/008110.html\n\nYou meet most of these rules, though you do have to download blocks from\nmultiple peers.\n\nThe suggestion in that thread were for a way to compactly indicate which\nblocks a node has.  Each node would then store a sub-set of all the\nblocks.  You just download the blocks you want from the node that has them.\n\nEach node would be recommended to store the last few days worth anyway.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170418/324a9cfa/attachment.html>"
            },
            {
                "author": "Aymeric Vitte",
                "date": "2017-04-18T23:19:04",
                "message_text_only": ">From the initial post \" The situation would likely become problematic\nquickly if bitcoin-core were to ship with the defaults set to a pruned\nnode.\"\n\nSorry to be straight, I read the (painful) thread below, and most of\nwhat is in there is inept, wrong, obsolete... or biased, cf the first\nsentence above, if the idea is to invent a workaround to the fact that\npruning might/will become the default or might/will be set by the users\nas the default so full nodes might/will disappear then just say it\nclearly instead of proposing this kind of non-solution as a solution to\nsecure the blockchain\n\nI can't believe this is serious, people now are supposed to prune but\nwill be forced to host a part of the blockchain, how do you expect this\nto work, why people would do this? Knowing that to start pruning they\nneed a full node, then since we are there, why not continuing with a\nfull node... but indeed, why should they continue with a full node, and\ntherefore why should they accept to host a part of the blockchain if\nthey decline the first proposal?\n\nThis is absurd, you are not addressing the first priority given the\ncontext which is to quickly increase the full nodes and which obviously\nincludes an incentive for people to run them\n\nIt gives also the feeling that bitcoin wants to reinvent everything not\ncapitalizing on/knowing what exists, sorry again but the concepts of the\nproposal and others like archival nodes are just funny\n\n\nLe 18/04/2017 \u00e0 15:07, Tier Nolan via bitcoin-dev a \u00e9crit :\n> This has been discussed before.\n>\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-May/008101.html\n>\n> including a list of nice to have features by Maxwell\n>\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-May/008110.html\n>\n> You meet most of these rules, though you do have to download blocks\n> from multiple peers.\n>\n> The suggestion in that thread were for a way to compactly indicate\n> which blocks a node has.  Each node would then store a sub-set of all\n> the blocks.  You just download the blocks you want from the node that\n> has them.\n>\n> Each node would be recommended to store the last few days worth anyway.\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n-- \nZcash wallets made simple: https://github.com/Ayms/zcash-wallets\nBitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets\nGet the torrent dynamic blocklist: http://peersm.com/getblocklist\nCheck the 10 M passwords list: http://peersm.com/findmyass\nAnti-spies and private torrents, dynamic blocklist: http://torrent-live.org\nPeersm : http://www.peersm.com\ntorrent-live: https://github.com/Ayms/torrent-live\nnode-Tor : https://www.github.com/Ayms/node-Tor\nGitHub : https://www.github.com/Ayms\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170419/828e6b5a/attachment-0001.html>"
            },
            {
                "author": "udevNull",
                "date": "2017-04-19T04:28:48",
                "message_text_only": "I'd like to add to this. There is definitely a barrier of entry with regards to setting up a full node. Unless you're living in a first world country, the bandwidth requirements alone, will outright prevent you from even setting up a full node (sync since genesis).\n\nTo maintain that also becomes a sunk cost, as there is no financial incentive to run a node, only an idealogical one. Most of the people who benefit and will benefit from Bitcoin, are the un-banked. Which you will find in 3rd world countries, that don't have ISPs that provide the data packages, to cater for the requirements of running a full node. I'm sure many would like to, but simply cannot afford it.\n\nA user may not want to run a node at home, but rather on a digital ocean or AWS server, which they cannot afford to do either considering the bandwidth and storage costs associated with it. However, I don't think they should be excluded from participating in the network (supporting proposals, voicing their opinions, running their own wallets, writing their own applications on top of Bitcoin [which I think is extremely important]).\n\nSo I would definitely be in favour of a small node of sorts. It will present us with some interesting technical challenges along the way but it's definitely worth while looking into.\n\nFinancially incentivising nodes is a really weird area because it would allow someone to essentially automate the deployment of nodes. i.e. if a node can pay for itself 100% (even at a lesser value, it just becomes cheaper overall), you could write an application that uses an AWS API or a digital ocean API to automatically deploy 100's of nodes. Which sounds great but not if that person is malicious and wants to prevent the community adopting proposals.\n\nJust my 2 cents worth.\n\nSent with [ProtonMail](https://protonmail.com) Secure Email.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170419/fc380587/attachment.html>"
            },
            {
                "author": "Angel Leon",
                "date": "2017-04-19T13:47:40",
                "message_text_only": ">Financially incentivising nodes is a really weird area because it would\nallow someone to essentially automate the deployment of nodes. i.e. if a\nnode can pay for itself 100% (even at a lesser value, it just becomes\ncheaper overall), you could write an application that uses an AWS API or a\ndigital ocean API to automatically deploy 100's of nodes. Which sounds\ngreat but not if that person is malicious and wants to prevent the\ncommunity adopting proposals.\n\nwhat other projects have done to avoid such attacks (while incentivizing\neconomically running full nodes) is to only distribute part of the block\nrewards back such nodes if that node has committed/frozen a predetermined\namount of coins that can't be spent. This also leaves less liquidity for\nmarket speculation and a incentives for long term commitments.\n\nOn Wed, Apr 19, 2017 at 5:14 AM udevNull via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> I'd like to add to this. There is definitely a barrier of entry with\n> regards to setting up a full node. Unless you're living in a first world\n> country, the bandwidth requirements alone, will outright prevent you from\n> even setting up a full node (sync since genesis).\n>\n> To maintain that also becomes a sunk cost, as there is no financial\n> incentive to run a node, only an idealogical one. Most of the people who\n> benefit and will benefit from Bitcoin, are the un-banked. Which you will\n> find in 3rd world countries, that don't have ISPs that provide the data\n> packages, to cater for the requirements of running a full node. I'm sure\n> many would like to, but simply cannot afford it.\n>\n> A user may not want to run a node at home, but rather on a digital ocean\n> or AWS server, which they cannot afford to do either considering the\n> bandwidth and storage costs associated with it. However, I don't think they\n> should be excluded from participating in the network (supporting proposals,\n> voicing their opinions, running their own wallets, writing their own\n> applications on top of Bitcoin [which I think is extremely important]).\n>\n> So I would definitely be in favour of a small node of sorts. It will\n> present us with some interesting technical challenges along the way but\n> it's definitely worth while looking into.\n>\n> Financially incentivising nodes is a really weird area because it would\n> allow someone to essentially automate the deployment of nodes. i.e. if a\n> node can pay for itself 100% (even at a lesser value, it just becomes\n> cheaper overall), you could write an application that uses an AWS API or a\n> digital ocean API to automatically deploy 100's of nodes. Which sounds\n> great but not if that person is malicious and wants to prevent the\n> community adopting proposals.\n> Just my 2 cents worth.\n>\n>\n> Sent with ProtonMail <https://protonmail.com> Secure Email.\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170419/1c1dc634/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2017-04-21T20:38:36",
                "message_text_only": "On Mon, Apr 17, 2017 at 6:54 AM, David Vorick via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Rationale:\n>\n> A node that stores the full blockchain (I will use the term archival node)\n> requires over 100GB of disk space, which I believe is one of the most\n> significant barriers to more people running full nodes. And I believe the\n> ecosystem would benefit substantially if more users were running full nodes.\n\nHi David,\n\nI've been thinking about this subject for a long time (Tier Nolan\nlinked some of the threads; see also the coloration part of\nhttps://en.bitcoin.it/wiki/User:Gmaxwell/block_network_coding) and\nabout using FEC with the history for the last year or so. (we use FEC\nin practice in fibre for relay at the tip now, and that has a design\nhistory going back to 2013).\n\nAs Jonas points out, we're all set to also offer the 'recent blocks'\nseparately, so that is obviously going to happen and will help. The\nfree design parameter is the definition of recent, but we have good\nmeasurements for setting that now. But about history...\n\nI think I might have designed myself into a corner and perhaps you've\nshown a way out-- I think there are some serious limits in your\nproposal but perhaps we can fix them.  Let me give you what I had been\nthinking about, then hand you at least a couple improvements to your\nthinking:\n\nAs you hopefully now know (per Tier Nolan's) post: I'd previously been\nthinking about nodes keeping a deterministic random, independently\nselected subset which is self-leveling based on a small seed.   The\nconnection overhead can can be mitigated by working with chunks of\nblocks rather than single blocks.\n\nBut as you've observed, the failure probabilities are rather high,\nespecially if an active attacker targets nodes carrying less commonly\navailable blocks.  So I thought, okay we can use FEC to help improve\nthe recovery odds.\n\nSo I considered using a sparse random code (E.g. an LDPC erasure code\nlike in RFC 5170) the advantage of these codes is that they are very\nfast to decode, and they support having enormous codewords, so you can\na high probability of every peer having a unique ID, so there would\neffectively never need to be any duplication.\n\nBut then I ran into the problem that I had no reasonable way to\nrecover from bad data, short of pulling a single group from an archive\nthen banning whatever peers gave you bad chunks.\n\nSo that is kind of where I got stuck.\n\nI didn't even consider the advantage of only being able to use a few\npeers total, as I still assumed that it would be doing the random\ngroups thing as well. That's a great point.\n\nSo on your design:\n\nBeing able to have a lower bound of 20% seems like a serious\nlimitation to me: it would be fine today, but the chain will quite\npossibly be twice the current size in a year.... and in four years\nwe're back to where we are now. What I'd been thinking would be able\nto scale arbitrarily low.\n\n20% is small, but is it small enough? -- today that would get us back\ninto common SSDs being able to hold the whole chain, but that property\nwill probably be lost again in a couple years. I think with any fixed\nfraction we'll constantly be fighting against the opportunity cost of\nupgrading storage, if not the cost of the storage itself. (and I agree\nthis is the vast majority of the response from actual users,  sync\ntime and ongoing bandwidth usage seeming to tie for second; the latter\nof which can be mitigated in other ways but for the former see my\nremarks at the end)\n\nThe fact that having only a few required blocks needed lets you brute\nforce out the decode is a great point.  I hadn't considered that, and\nthe schemed I'd been considering are not communications efficient with\nonly a few blocks required e.g. they sometimes require a couple extra\nblocks to successfully decode, which is a lot of overhead if you're\nonly splitting 5 ways.\n\n> I also believe that alternative erasure coding schemes exist which actually\n> are able to identify the bad pieces given sufficient good pieces, however I\n> don't know if they have the same computational performance as the best\n> Reed-Solomon coding implementations.\n\nActually RS codes are _the_ codes you want to use for with decoding\nwith errors but the 'computationally efficient' error correcting\ndecoding (which is itself heinously slow) requires 2*errors + data\nnumber of blocks to decode. Not so useful if you're only looking at 5\nparts.\n\nRS decoding is pretty slow generally, esp compared to binary sparse\ncodes.  We were unable to make RS coding make sense for use in fast\nblock relay for this reason-- decoding time bottlenecked\nreconstruction. The most highly optimized RS codes make a special\noptimization which is not useful for your proposal: They're much\nfaster to decode if you're decoding from the first couple correction\nblocks.  Without these optimizations speeds from from 1GB/s to more\nlike 100MB/s or worse.  Though I suppose with assumevalid initial sync\nnow is pretty cpu-idle on most hardware.  (If 100MB/s sounds fast,\nkeep in mind that time spent decoding is time that can't be spent\nhashing/verifying/etc.. and on a lot hardware with fast broadband with\nsignature validation enabled we're cpu bound already)\n\n> Another option would be to have the small nodes store a cryptographic\n> checksum of each piece. Obtaining the cryptographic checksum for all 256\n> pieces would incur a nontrivial amount of hashing (post segwit, as much as\n> 100MB of extra hashing per block), and would require an additional ~4kb of\n> storage per block. The hashing overhead here may be prohibitive.\n\nThis was a complete non-starter when thinking about using these sparse\ncodes where the number of possible blocks is effectively unlimited.\n\nYour 4kb assumption isn't correct though: How you do it is that after\ndownloading a block you compute the 255 fragments (as an aside, you're\nsaying 256 but the most you can get is 255 for an 8-bit RS code).\nthen you compute a hashtree over them. Every node remembers the root,\nand the membership proofs for their chunk(s)... this is 256 bytes of\nextra storage.\n\nWhen you decode you use a majority to decide what root you are trying\nto decode. If it fails to result in valid blocks, you blacklist that\nroot, ban all those peers, and try the next.  Worst case cost is the\nnumber of invalid roots rather than peers choose 5.\n\nI'm not sure if combinitoral decode or the above would be easier to implement.\n\n> This represents a non-trivial amount of code, but I believe that the result\n> would be a non-trivial increase in the percentage of users running full\n> nodes, and a healthier overall network.\n\nThe RS coder stuff is small, even doing the fancy w/ error decodes it\nisn't huge. I expect complexity mostly will show up in the garbage\ninput handling.\n\nA couple other thoughts:\n\nThe coded blocks are not useful for things like bloom scanning or\nother lookup.  With the committed filter proposals you could still\nkeep the committed filters (even 5 way shared themselves...) so\nperhaps not that much of a concern.\n\nCoding the blocks will make their encoding normative. The current P2P\none we use is by no means more efficient,  Pieter and I have an\nencoding that works on a transaction by transaction basis and\ndecreases the size of the whole chain by ~28%, block-wide entropy\nencoding could reduce the size even further.  We'd hoped to at least\nstart using this per transaction coding locally, converting on the fly\nto the original serialization where needed (the idea would be to\neventually support the compacted serialization on the wire too).\nMaybe the answer there is to just get in whatever improvements we\nthink are reasonable before doing the coded block.\n\nI think the 20% floor is still too high, and too many nodes will be\nforced to just do the recent blocks things. perhaps not today, but\neventually.   I suppose we could just assume that the block is split\n10 ways, and the default is two indexes, but there is flexibility to\ngo down to one in the future, at the cost of needing more peers...\ncould run numbers on the decode failure odds for the 10 of 255 case...\nBut that would only improve it to 10%.   I suppose the proposal could\nbe combined with sparse chain storage like I was thinking years ago,\nbut much less sparsity would be needed so the downsides would be less\nsevere.\n\nE.g. if you want to store <10% of the chain you'd keep some 10% blocks\nfor random groups.  such a feature could be introduced later when it\nwas more important to keep less than 10 or 20 percent.\n\nRecovery odds could be improved with a second level of coding. E.g. if\nyour ID was not a constant but a seed into PRNG(height)%250+5  then\nyou also have a fraction of nodes store the xor between adjacent pairs\nthen you can fill in unrecoverable groups.  the limit of this thinking\nis exactly the sparse random code ECC schemes) Probably not worth the\ncomplexity, but just a thing to keep in mind...\n\nProbably the next step is to do some more focused benchmarks. I have\nsome code I can recommend offline.\n\nI can't help but feel that this might be a little bit of a waste of\ntime. I think the long term survival of the system is likely going to\nultimately depend on doing an assumevalid like gated sync of a UTXO\nset.  Ethereum already does something far more reckless (they let\nminers pick the state and blindly trust it with almost no depth\nrequired) and no one cares.  If that is done then all these concerns\nare greatly reduced, along with the 100(+++?)GB/history-year transfer\ncosts.  You'd still want to have archives, but do you care about 20%\narchives?\n\nReplying to some other comments in the thread:\n\n> A user may not want to run a node at home, but rather on a digital ocean or AWS server\n\nThis is almost if not quite entirely pointless. There are already many\nnodes on AWS and DO, adding more does not improve your security (you\nmust trust DO/AWS), does not improve your reliability (DO/AWS), does\nnot improve the network's capacity, etc.  About the only arguable\nbenefit I can see there beyond making more money for these hosts is\nfeel good points for (incorrectly) thinking you're helping the\nnetwork, and negligibly reducing the density of spy nodes (but wait:\nAWS/DO may well just be spying on your connections too..). and it it\nisn't like fast storage on these services is cheap.\n\n> Perhaps it is not, but I would think that it would be pretty straightforward to configure a global bandwidth limit within Bitcoin.\n\nWe have outbound transmission limits though not by default. Setting\nthem sensibly is something of a black art. Obviously these will\nimprove over time... and are more reasons that I agree with your\nrelative cost assumptions except for the sync-time part.\n\n> Fingerprinting?\n\nUnavoidable, but should be made inconsequential by making transaction\nannouncement more private independent of any of this. There are\nalready _MANY_ ways to fingerprint nodes, please don't think that\nexisting software has any real immunity here. We avoid adding new\nfingerprinting where we can... but they're very fingerprintable\nalready. Transaction announcement privacy MUST be fixed.  I assume if\nany of this is worth doing, it will also be worth the increase in\nfingerprinting. And at least this proposal would 'only' create 255\nnode classes.\n\n> SPV?\n\nAs I pointed out above, Vorick's idea is totally compatible with\ncommitted filters, and the filters can even be shared like the blocks\nare.\n\n> [People who fail at math]\n\nThe SNR on this list really sucks.  If you aren't spending a couple\nhours thinking about your responses or at least citing research that\ntook days of effort or more then you are probably wasting people's\ntime. Please respect the other users of the list.\n\n> Could there be a slider?\n\nAbsolutely, I assume if Vorick's proposal were implemented that nodes\nwould have the follow options: Pruned [UTXO + recent two weeks of\nblocks], 20%, 40%, 60%, 80%, 100% (archive)."
            },
            {
                "author": "Aymeric Vitte",
                "date": "2017-04-23T16:27:15",
                "message_text_only": "\"Absolutely, I assume if Vorick's proposal were implemented that nodes\nwould have the follow options: Pruned [UTXO + recent two weeks of\nblocks], 20%, 40%, 60%, 80%, 100% (archive).\"\n\nYes, and I think that they can have this in \"disorder\" (only 20% of\nblocks in the middle of the blockchain for example)\n\nThere are many reasons why I dislike David's proposal as it is, you\nmention some below, why 20%? too much? too small? what will happen\ntomorrow? etc, I gave others, and this still does not explain why people\nshould go for more than two weeks of blocks\n\nMaybe what I suggested here again\nhttps://gist.github.com/Ayms/aab6f8e08fef0792ab3448f542a826bf#proposal\ncould be considered\n\nThis is just a suggestion based on incremental \"torrents\", fortunately\nit comes from much more than days of work as you require below and is\nthe concatenation of thoughts from different projects/studies\n\nIt does follow your 8 rules (although I am not sure what you mean in\nrule 2 \"The decision to contact a node should need O(1) communications\",\n1 M limit works?) and proposes others, and it solves the issues\nmentioned below, or please someone tell me why it does not (I know\npeople here dislike DHTs, not sure why)\n\nExcept fingerprinting (and I don't know what is the SPV issue exactly)\nbut still is better, if the nodes behave like the groups with most\nnumerous peers (ie the group seeding, 20%, or the one seeding 40%, or\nthe one seeding about nothing (sic... subliminal message here...), etc)\nthen they just can't be fingerprinted (at least based on this feature)\n\nI think the main concepts are detailed enough but maybe that's not the\ncase, it's a really draft, if you look well the pruning case is\nconsidered, but not explained, like some other points, because\ncontinuing this work with no incentive solution for the seeders looks to\nme useless\n\n\nLe 21/04/2017 \u00e0 22:38, Gregory Maxwell via bitcoin-dev a \u00e9crit :\n> On Mon, Apr 17, 2017 at 6:54 AM, David Vorick via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> Rationale:\n>>\n>> A node that stores the full blockchain (I will use the term archival node)\n>> requires over 100GB of disk space, which I believe is one of the most\n>> significant barriers to more people running full nodes. And I believe the\n>> ecosystem would benefit substantially if more users were running full nodes.\n> Hi David,\n>\n> I've been thinking about this subject for a long time (Tier Nolan\n> linked some of the threads; see also the coloration part of\n> https://en.bitcoin.it/wiki/User:Gmaxwell/block_network_coding) and\n> about using FEC with the history for the last year or so. (we use FEC\n> in practice in fibre for relay at the tip now, and that has a design\n> history going back to 2013).\n>\n> As Jonas points out, we're all set to also offer the 'recent blocks'\n> separately, so that is obviously going to happen and will help. The\n> free design parameter is the definition of recent, but we have good\n> measurements for setting that now. But about history...\n>\n> I think I might have designed myself into a corner and perhaps you've\n> shown a way out-- I think there are some serious limits in your\n> proposal but perhaps we can fix them.  Let me give you what I had been\n> thinking about, then hand you at least a couple improvements to your\n> thinking:\n>\n> As you hopefully now know (per Tier Nolan's) post: I'd previously been\n> thinking about nodes keeping a deterministic random, independently\n> selected subset which is self-leveling based on a small seed.   The\n> connection overhead can can be mitigated by working with chunks of\n> blocks rather than single blocks.\n>\n> But as you've observed, the failure probabilities are rather high,\n> especially if an active attacker targets nodes carrying less commonly\n> available blocks.  So I thought, okay we can use FEC to help improve\n> the recovery odds.\n>\n> So I considered using a sparse random code (E.g. an LDPC erasure code\n> like in RFC 5170) the advantage of these codes is that they are very\n> fast to decode, and they support having enormous codewords, so you can\n> a high probability of every peer having a unique ID, so there would\n> effectively never need to be any duplication.\n>\n> But then I ran into the problem that I had no reasonable way to\n> recover from bad data, short of pulling a single group from an archive\n> then banning whatever peers gave you bad chunks.\n>\n> So that is kind of where I got stuck.\n>\n> I didn't even consider the advantage of only being able to use a few\n> peers total, as I still assumed that it would be doing the random\n> groups thing as well. That's a great point.\n>\n> So on your design:\n>\n> Being able to have a lower bound of 20% seems like a serious\n> limitation to me: it would be fine today, but the chain will quite\n> possibly be twice the current size in a year.... and in four years\n> we're back to where we are now. What I'd been thinking would be able\n> to scale arbitrarily low.\n>\n> 20% is small, but is it small enough? -- today that would get us back\n> into common SSDs being able to hold the whole chain, but that property\n> will probably be lost again in a couple years. I think with any fixed\n> fraction we'll constantly be fighting against the opportunity cost of\n> upgrading storage, if not the cost of the storage itself. (and I agree\n> this is the vast majority of the response from actual users,  sync\n> time and ongoing bandwidth usage seeming to tie for second; the latter\n> of which can be mitigated in other ways but for the former see my\n> remarks at the end)\n>\n> The fact that having only a few required blocks needed lets you brute\n> force out the decode is a great point.  I hadn't considered that, and\n> the schemed I'd been considering are not communications efficient with\n> only a few blocks required e.g. they sometimes require a couple extra\n> blocks to successfully decode, which is a lot of overhead if you're\n> only splitting 5 ways.\n>\n>> I also believe that alternative erasure coding schemes exist which actually\n>> are able to identify the bad pieces given sufficient good pieces, however I\n>> don't know if they have the same computational performance as the best\n>> Reed-Solomon coding implementations.\n> Actually RS codes are _the_ codes you want to use for with decoding\n> with errors but the 'computationally efficient' error correcting\n> decoding (which is itself heinously slow) requires 2*errors + data\n> number of blocks to decode. Not so useful if you're only looking at 5\n> parts.\n>\n> RS decoding is pretty slow generally, esp compared to binary sparse\n> codes.  We were unable to make RS coding make sense for use in fast\n> block relay for this reason-- decoding time bottlenecked\n> reconstruction. The most highly optimized RS codes make a special\n> optimization which is not useful for your proposal: They're much\n> faster to decode if you're decoding from the first couple correction\n> blocks.  Without these optimizations speeds from from 1GB/s to more\n> like 100MB/s or worse.  Though I suppose with assumevalid initial sync\n> now is pretty cpu-idle on most hardware.  (If 100MB/s sounds fast,\n> keep in mind that time spent decoding is time that can't be spent\n> hashing/verifying/etc.. and on a lot hardware with fast broadband with\n> signature validation enabled we're cpu bound already)\n>\n>> Another option would be to have the small nodes store a cryptographic\n>> checksum of each piece. Obtaining the cryptographic checksum for all 256\n>> pieces would incur a nontrivial amount of hashing (post segwit, as much as\n>> 100MB of extra hashing per block), and would require an additional ~4kb of\n>> storage per block. The hashing overhead here may be prohibitive.\n> This was a complete non-starter when thinking about using these sparse\n> codes where the number of possible blocks is effectively unlimited.\n>\n> Your 4kb assumption isn't correct though: How you do it is that after\n> downloading a block you compute the 255 fragments (as an aside, you're\n> saying 256 but the most you can get is 255 for an 8-bit RS code).\n> then you compute a hashtree over them. Every node remembers the root,\n> and the membership proofs for their chunk(s)... this is 256 bytes of\n> extra storage.\n>\n> When you decode you use a majority to decide what root you are trying\n> to decode. If it fails to result in valid blocks, you blacklist that\n> root, ban all those peers, and try the next.  Worst case cost is the\n> number of invalid roots rather than peers choose 5.\n>\n> I'm not sure if combinitoral decode or the above would be easier to implement.\n>\n>> This represents a non-trivial amount of code, but I believe that the result\n>> would be a non-trivial increase in the percentage of users running full\n>> nodes, and a healthier overall network.\n> The RS coder stuff is small, even doing the fancy w/ error decodes it\n> isn't huge. I expect complexity mostly will show up in the garbage\n> input handling.\n>\n> A couple other thoughts:\n>\n> The coded blocks are not useful for things like bloom scanning or\n> other lookup.  With the committed filter proposals you could still\n> keep the committed filters (even 5 way shared themselves...) so\n> perhaps not that much of a concern.\n>\n> Coding the blocks will make their encoding normative. The current P2P\n> one we use is by no means more efficient,  Pieter and I have an\n> encoding that works on a transaction by transaction basis and\n> decreases the size of the whole chain by ~28%, block-wide entropy\n> encoding could reduce the size even further.  We'd hoped to at least\n> start using this per transaction coding locally, converting on the fly\n> to the original serialization where needed (the idea would be to\n> eventually support the compacted serialization on the wire too).\n> Maybe the answer there is to just get in whatever improvements we\n> think are reasonable before doing the coded block.\n>\n> I think the 20% floor is still too high, and too many nodes will be\n> forced to just do the recent blocks things. perhaps not today, but\n> eventually.   I suppose we could just assume that the block is split\n> 10 ways, and the default is two indexes, but there is flexibility to\n> go down to one in the future, at the cost of needing more peers...\n> could run numbers on the decode failure odds for the 10 of 255 case...\n> But that would only improve it to 10%.   I suppose the proposal could\n> be combined with sparse chain storage like I was thinking years ago,\n> but much less sparsity would be needed so the downsides would be less\n> severe.\n>\n> E.g. if you want to store <10% of the chain you'd keep some 10% blocks\n> for random groups.  such a feature could be introduced later when it\n> was more important to keep less than 10 or 20 percent.\n>\n> Recovery odds could be improved with a second level of coding. E.g. if\n> your ID was not a constant but a seed into PRNG(height)%250+5  then\n> you also have a fraction of nodes store the xor between adjacent pairs\n> then you can fill in unrecoverable groups.  the limit of this thinking\n> is exactly the sparse random code ECC schemes) Probably not worth the\n> complexity, but just a thing to keep in mind...\n>\n> Probably the next step is to do some more focused benchmarks. I have\n> some code I can recommend offline.\n>\n> I can't help but feel that this might be a little bit of a waste of\n> time. I think the long term survival of the system is likely going to\n> ultimately depend on doing an assumevalid like gated sync of a UTXO\n> set.  Ethereum already does something far more reckless (they let\n> miners pick the state and blindly trust it with almost no depth\n> required) and no one cares.  If that is done then all these concerns\n> are greatly reduced, along with the 100(+++?)GB/history-year transfer\n> costs.  You'd still want to have archives, but do you care about 20%\n> archives?\n>\n> Replying to some other comments in the thread:\n>\n>> A user may not want to run a node at home, but rather on a digital ocean or AWS server\n> This is almost if not quite entirely pointless. There are already many\n> nodes on AWS and DO, adding more does not improve your security (you\n> must trust DO/AWS), does not improve your reliability (DO/AWS), does\n> not improve the network's capacity, etc.  About the only arguable\n> benefit I can see there beyond making more money for these hosts is\n> feel good points for (incorrectly) thinking you're helping the\n> network, and negligibly reducing the density of spy nodes (but wait:\n> AWS/DO may well just be spying on your connections too..). and it it\n> isn't like fast storage on these services is cheap.\n>\n>> Perhaps it is not, but I would think that it would be pretty straightforward to configure a global bandwidth limit within Bitcoin.\n> We have outbound transmission limits though not by default. Setting\n> them sensibly is something of a black art. Obviously these will\n> improve over time... and are more reasons that I agree with your\n> relative cost assumptions except for the sync-time part.\n>\n>> Fingerprinting?\n> Unavoidable, but should be made inconsequential by making transaction\n> announcement more private independent of any of this. There are\n> already _MANY_ ways to fingerprint nodes, please don't think that\n> existing software has any real immunity here. We avoid adding new\n> fingerprinting where we can... but they're very fingerprintable\n> already. Transaction announcement privacy MUST be fixed.  I assume if\n> any of this is worth doing, it will also be worth the increase in\n> fingerprinting. And at least this proposal would 'only' create 255\n> node classes.\n>\n>> SPV?\n> As I pointed out above, Vorick's idea is totally compatible with\n> committed filters, and the filters can even be shared like the blocks\n> are.\n>\n>> [People who fail at math]\n> The SNR on this list really sucks.  If you aren't spending a couple\n> hours thinking about your responses or at least citing research that\n> took days of effort or more then you are probably wasting people's\n> time. Please respect the other users of the list.\n>\n>> Could there be a slider?\n> Absolutely, I assume if Vorick's proposal were implemented that nodes\n> would have the follow options: Pruned [UTXO + recent two weeks of\n> blocks], 20%, 40%, 60%, 80%, 100% (archive).\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n-- \nZcash wallets made simple: https://github.com/Ayms/zcash-wallets\nBitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets\nGet the torrent dynamic blocklist: http://peersm.com/getblocklist\nCheck the 10 M passwords list: http://peersm.com/findmyass\nAnti-spies and private torrents, dynamic blocklist: http://torrent-live.org\nPeersm : http://www.peersm.com\ntorrent-live: https://github.com/Ayms/torrent-live\nnode-Tor : https://www.github.com/Ayms/node-Tor\nGitHub : https://www.github.com/Ayms"
            }
        ],
        "thread_summary": {
            "title": "Small Nodes: A Better Alternative to Pruned Nodes",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "David Vorick",
                "Tom Zander",
                "Aymeric Vitte",
                "Leandro Coutinho",
                "Andrew Poelstra",
                "Tier Nolan",
                "Angel Leon",
                "David Kaufman",
                "Erik Aronesty",
                "Gregory Maxwell",
                "Danny Thorpe",
                "udevNull",
                "Jonas Schnelli"
            ],
            "messages_count": 21,
            "total_messages_chars_count": 123881
        }
    },
    {
        "title": "[bitcoin-dev] Suggestions to improve opcodes with O(N) complexity",
        "thread_messages": [
            {
                "author": "Sergio Demian Lerner",
                "date": "2017-04-17T13:25:14",
                "message_text_only": "I came across O(N) behavior of two scripting opcodes, OP_IF and OP_ROLL. By\nexploiting edge cases for each of these two sub-optimal algorithms, I\nmanage to simulate a Segwit block that takes up to 5.6 seconds to verify on\na Ubuntu VM running on a single Core i5 processor. The simulation is based\non a single thread executing EvalScript(), the Bitcoin script execution\nfunction. The tests were not performed processing actual blocks. These\nresults should not make anyone worry, because there are worse problems in\nBitcoin block verification, and because Bitcoin employs several worker\nthreads for verifying scripts in parallel. For example, a Segwit block can\nrequest 80000 signature verifications when all transactions are P2WSH. It\nis said that Bitcoin Core (in a modern multi-core machine, using its\nmulti-threading verification capabilities) can verify 8000 ECDSA signatures\nper second. Therefore a malicious miner can create a Segwit block that\nrequires approximately 10 seconds to be verified. Since the examples\npresented in this post consume less than 10 seconds, I don\u2019t consider my\nfindings as vulnerabilities. However, if the block size is to be increased\nin the future, these problems should be considered prior increasing the\nblock size. The scripts presented here as examples do not leave the value\nstack empty, but the Bitcoin protocol does not require it. Bitcoin only\nrequires the top value to be true to accept the script.\n\nOP_IF abuse\n\nEvery time a Bitcoin script executes the OP_IF opcode, a boolean value\nindicating if the condition was true, false or the conditional was skipped\n(also represented as false) is pushed into the vfExec stack.  Every time an\nopcode is executed, the number of  false values in the vfExec stack is\ncounted using the following line:\n\nbool fExec = !count(vfExec.begin(), vfExec.end(), false);\n\nIf the count is non-zero, all subsequent instructions except OP_ELSE and\nOP_ENDIF are skipped. It is clear that the longer the conditional stack is,\nthe more it takes to count the false elements.\n\nThe following scriptPub or ScriptSig exploits this problem:\n\n0\nOP_IF { 100 times }\n\n0 { 9798 times }\n\nOP_ENDIF { 100 times }\n1\n\nThe vfExec vector is filled with 100 elements, and then each element is\nscanned 9799 times, totaling more than 979K items scanned. This took 2.5\nseconds in my test VM (for a block filled with these scriptSigs).\n\nTo re-write this logic with a O(1) algorithm, one simply has to count the\nnumber of true conditions in one variable (trueCount), and the number of\nfalse or skipped conditions following all true conditions in another\n(ignoreCount). Detecting if code needs to be executed or not requires just\ntesting if ignoreCount is zero.\n\nThe handling of OP_IF / OP_NOTIF / OP_ELSE should be like the following\npseudo-code:\n\nfExec = (ignoreCount==0);\n...\ncase OP_IF:\ncase OP_NOTIF:\n {\n   if (fExec)\n    {\n     ....compute fValue...\n     if (fValue) trueCount++; else ignoreCount++;\n    } else\n    ignoreCount++;\n } break;\n case OP_ELSE:\n {\n if ((trueCount==0) && (ignoreCount==0))\n     return set_error(serror, SCRIPT_ERR_UNBALANCED_CONDITIONAL);\n if (ignoreCount==0) { trueCount--; ignoreCount++; } else\n if (ignoreCount==1) { trueCount++; ignoreCount--; }\n } break;\ncase OP_ENDIF:\n {\n    if ((trueCount==0) && (ignoreCount==0))\n        return set_error(serror, SCRIPT_ERR_UNBALANCED_CONDITIONAL);\n    if (ignoreCount>0) ignoreCount--; else trueCount--;\n }\n break;\n\nYou may have noticed the strange behavior of Bitcoin\u2019s ELSE statement.\nBitcoin allows one to switch between true and false conditions several\ntimes. For example, the following script is valid and leaves the value 2 on\nthe stack:\n\n1 OP_IF OP_ELSE OP_ELSE 2 OP_ENDIF\n\nOP_ROLL\n\nThe second problem lies in the OP_ROLL opcode. This opcode removes a value\nat a given index from the value stack, and pushes it on top. As the Bitcoin\nCore stack stores a list of char std::vector by value (not by reference),\nand because the stack is itself a std::vector (not a linked list), then\nremoving the first elements requires moving all elements one position in\nmemory. The value stack can store a maximum of 1000 elements. The following\nscript fills the stack and then moves each stack element 200 times, so the\nnumber of moved elements is 200K. This took almost 5.6 seconds in my test\nVM (for a block filled with these scriptSigs).\n\n1  {999 times}\n998 OP_ROLL { 200 times }\n\nI tried other scripts, such as filling the stack with values of size 520\nusing DUP3, and then performing rolls, but all of them led to a block that\ntook less time, if the block is to be filled with the scripts.\n\nOne solution to this problem is use a linked list data structure instead of\na std::vector, to allow O(1) removal of items, but it still requires O(N)\nfor element lookup. A balanced tree where each internal node is augmented\nwith the number of children underneath can be used to provide efficient\nindexed access and efficient element removal. However, the overhead of such\ndata structure may kill its benefits.\n\nSo it may be the case that optimizing OP_ROLL will never really be\nrequired.\n\nBut these minor issues have to be taken into account if the scripting\nsystem is modified in any way. There are many subtle interactions. For\ninstance, it may seem that Segwit allows a transaction having a stack\ncontaining 2 million items to verify correctly, by having the witness stack\nfilled with 2M zero values, and by executing an empty witness script.\nHowever this is prevented by the cleanstack check.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170417/64f59d8e/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Suggestions to improve opcodes with O(N) complexity",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Sergio Demian Lerner"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 5682
        }
    },
    {
        "title": "[bitcoin-dev] Transaction signalling",
        "thread_messages": [
            {
                "author": "Erik Aronesty",
                "date": "2017-04-17T15:50:51",
                "message_text_only": "If users added a signal to OP_RETURN, might it be possible to tag all\nvalidated input addresses with that signal.\n\nThen a node can activate a new feature after the percentage of tagged input\naddresses reaches a certain level within a certain period of time?\n\nThis could be used in addition to a flag day to trigger activation of a\nfeature with some reassurance of user uptake.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170417/db0a38aa/attachment.html>"
            },
            {
                "author": "Marcel Jamin",
                "date": "2017-04-18T14:52:04",
                "message_text_only": "Probably a bad idea for various reasons, but tagging (fee paying)\ntransactions with info about the capabilities of the node that created\nit might be interesting? Might be useful to gauge economic support for\ncertain upgrades, especially if excluding long transaction chains,\netc. In the very least it would be a far better indicator than simply\ncounting reachable nodes.\n\nOn 17 April 2017 at 17:50, Erik Aronesty via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> If users added a signal to OP_RETURN, might it be possible to tag all\n> validated input addresses with that signal.\n>\n> Then a node can activate a new feature after the percentage of tagged input\n> addresses reaches a certain level within a certain period of time?\n>\n> This could be used in addition to a flag day to trigger activation of a\n> feature with some reassurance of user uptake.\n>\n>\n>\n>\n>\n>\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Erik Aronesty",
                "date": "2017-04-18T18:01:52",
                "message_text_only": "Just to be clear, the tagging would occur on the addresses, and the\nweighting would be by value, so it's a measure of economic significance.\nMajor exchanges will regularly tag massive amounts of Bitcoins with their\ncapabilities.\n\nJust adding a nice bit-field and a tagging standard, and then charting it\nmight be enough to \"think about how to use it later\".   The only problem\nwould be that this would interfere with \"other uses of op_return\" ...\ncolored coins, etc.\n\nPersonally, I think that's OK, since the purpose is to tag economically\nmeaningful nodes to the Bitcoin ecosystem and colored coins, by definition,\nonly have value to \"other ecosystems\".\n\n(Counterargument: Suppose in some future where this is used as an\nalternative to BIP9 for a user-coordinated code release - especially in\nsituations where miners have rejected activation of a widely-regarded\nproposal.  Suppose also, in that future, colored coin ICO's that use\nop-return are regularly used to float the shares of major corporation.  It\nmight be irresponsible to exclude them from coordinating protocol changes.)\n\n\n\n\n\nOn Tue, Apr 18, 2017 at 10:52 AM, Marcel Jamin <marcel at jamin.net> wrote:\n\n> Probably a bad idea for various reasons, but tagging (fee paying)\n> transactions with info about the capabilities of the node that created\n> it might be interesting? Might be useful to gauge economic support for\n> certain upgrades, especially if excluding long transaction chains,\n> etc. In the very least it would be a far better indicator than simply\n> counting reachable nodes.\n>\n> On 17 April 2017 at 17:50, Erik Aronesty via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > If users added a signal to OP_RETURN, might it be possible to tag all\n> > validated input addresses with that signal.\n> >\n> > Then a node can activate a new feature after the percentage of tagged\n> input\n> > addresses reaches a certain level within a certain period of time?\n> >\n> > This could be used in addition to a flag day to trigger activation of a\n> > feature with some reassurance of user uptake.\n> >\n> >\n> >\n> >\n> >\n> >\n> >\n> >\n> >\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> >\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170418/37e41773/attachment-0001.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2017-04-18T18:07:25",
                "message_text_only": "I really like the idea of extending signalling capabilities to the\nend-users. It gives stakeholders a voice in the decisions we take in\nthe network, and are a clear signal to all other involved parties. It\nreminds me of a student thesis I supervised some time ago [1], in\nwhich we explored various signalling ideas.\n\nI think we have a number of fields that may be used for such a\nsignalling, e.g., OP_RETURN, locktime, and output scripts. I think\nOP_RETURN is probably not the field you'd want to use though since it\nadds data that needs to be transferred, stored for bootstrap, and\noutputs in the UTXO would need to be tagged with additional\ninformation. Locktime has the advantage of being mostly a freeform\nfield for values in the past, but it clashes with other uses that may\nrely on it. Furthermore, it is the transaction creator that specifies\nthe locktime, hence the signal trails one hop behind the current\nowner, i.e., the actual stakeholder.\n\nI think probably the best field to signal would be the output\nscript. It is specified by the recipient of the funds, i.e., the\ncurrent owner, and is already stored in the UTXO, so a single pass can\ntally up the votes. We could for example use the last 4 bits of the\npubkey/pubkeyhash to opt in (3 leading 0 bits) and the vote (0/1\ndepending on the stakeholders desired signal). We'd need to define\nsimilar semantics for other script types, but getting the standard\nscripts to be recognized should be simple.\n\nIn the spirit of full disclosure I'd like to also mention some of the\ndownsides of voting this way. Unlike the OP_RETURN proposal, users\nthat do not intend to signal will also be included in the tally. I'd\nexpect the signals of these users to be random with a 50% chance of\neither outcome, so they should not influence the final result, but may\nmuddy the water depending on what part of the population is\nsignalling. The opt-in should make sure that the majority of votes are\nactually voluntary votes, and not just users that randomly select a\npubkey/pubkeyhash, and can be adjusted as desired, though higher\nvalues require more grinding on behalf of the users.\n\nThe grinding may also exacerbate some problems we already have with\nthe HD Wallet lookahead, since we now skip a number of addresses, so\nwe should not require too many opt-in bits.\n\nSo there are some problems we'd need to tackle, but I'm really excited\nabout this, as it could provide data to make informed decisions, and\nshould put an end to the endless speculation about the will of the\neconomic majority.\n\nCheers,\nChristian\n\n[1] http://pub.tik.ee.ethz.ch/students/2015-HS/SA-2015-30.pdf\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170418/cdbdf19f/attachment.html>"
            },
            {
                "author": "Tim Ruffing",
                "date": "2017-04-18T22:29:17",
                "message_text_only": "I don't have an opinion on whether signaling is a good idea in general.\n\nHowever I don't think that using addresses is a good idea, because this\nhas privacy implications. For example, it makes it much easier to link\nthe addresses, e.g., inputs with change address. (The change address\nvotes for the same proposal as the input address.)\n\nTim\n\nOn Tue, 2017-04-18 at 18:07 +0000, Christian Decker via bitcoin-dev\nwrote:\n> I really like the idea of extending signalling capabilities to the\n> end-users. It gives stakeholders a voice in the decisions we take in\n> the network, and are a clear signal to all other involved parties. It\n> reminds me of a student thesis I supervised some time ago [1], in\n> which we explored various signalling ideas.\n> \n> I think we have a number of fields that may be used for such a\n> signalling, e.g., OP_RETURN, locktime, and output scripts. I think\n> OP_RETURN is probably not the field you'd want to use though since it\n> adds data that needs to be transferred, stored for bootstrap, and\n> outputs in the UTXO would need to be tagged with additional\n> information. Locktime has the advantage of being mostly a freeform\n> field for values in the past, but it clashes with other uses that may\n> rely on it. Furthermore, it is the transaction creator that specifies\n> the locktime, hence the signal trails one hop behind the current\n> owner, i.e., the actual stakeholder.\n> \n> I think probably the best field to signal would be the output\n> script. It is specified by the recipient of the funds, i.e., the\n> current owner, and is already stored in the UTXO, so a single pass\n> can\n> tally up the votes. We could for example use the last 4 bits of the\n> pubkey/pubkeyhash to opt in (3 leading 0 bits) and the vote (0/1\n> depending on the stakeholders desired signal). We'd need to define\n> similar semantics for other script types, but getting the standard\n> scripts to be recognized should be simple.\n> \n> In the spirit of full disclosure I'd like to also mention some of the\n> downsides of voting this way. Unlike the OP_RETURN proposal, users\n> that do not intend to signal will also be included in the tally. I'd\n> expect the signals of these users to be random with a 50% chance of\n> either outcome, so they should not influence the final result, but\n> may\n> muddy the water depending on what part of the population is\n> signalling. The opt-in should make sure that the majority of votes\n> are\n> actually voluntary votes, and not just users that randomly select a\n> pubkey/pubkeyhash, and can be adjusted as desired, though higher\n> values require more grinding on behalf of the users.\n> \n> The grinding may also exacerbate some problems we already have with\n> the HD Wallet lookahead, since we now skip a number of addresses, so\n> we should not require too many opt-in bits.\n> \n> So there are some problems we'd need to tackle, but I'm really\n> excited\n> about this, as it could provide data to make informed decisions, and\n> should put an end to the endless speculation about the will of the\n> economic majority.\n> \n> Cheers,\n> Christian\n> \n> [1] http://pub.tik.ee.ethz.ch/students/2015-HS/SA-2015-30.pdf\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Erik Aronesty",
                "date": "2017-04-20T16:14:18",
                "message_text_only": "I agree, addresses create vulnerability, an OP_RETURN signal seems the\nsafest way to go for UA signalling.   I can model a BIP after BIP9, with\nsome discussion of how to properly collect statistics, and the ability for\nnodes to activate features based on an \"economic majority\" defined in this\nway.\n\nOn Tue, Apr 18, 2017 at 6:29 PM, Tim Ruffing via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> I don't have an opinion on whether signaling is a good idea in general.\n>\n> However I don't think that using addresses is a good idea, because this\n> has privacy implications. For example, it makes it much easier to link\n> the addresses, e.g., inputs with change address. (The change address\n> votes for the same proposal as the input address.)\n>\n> Tim\n>\n> On Tue, 2017-04-18 at 18:07 +0000, Christian Decker via bitcoin-dev\n> wrote:\n> > I really like the idea of extending signalling capabilities to the\n> > end-users. It gives stakeholders a voice in the decisions we take in\n> > the network, and are a clear signal to all other involved parties. It\n> > reminds me of a student thesis I supervised some time ago [1], in\n> > which we explored various signalling ideas.\n> >\n> > I think we have a number of fields that may be used for such a\n> > signalling, e.g., OP_RETURN, locktime, and output scripts. I think\n> > OP_RETURN is probably not the field you'd want to use though since it\n> > adds data that needs to be transferred, stored for bootstrap, and\n> > outputs in the UTXO would need to be tagged with additional\n> > information. Locktime has the advantage of being mostly a freeform\n> > field for values in the past, but it clashes with other uses that may\n> > rely on it. Furthermore, it is the transaction creator that specifies\n> > the locktime, hence the signal trails one hop behind the current\n> > owner, i.e., the actual stakeholder.\n> >\n> > I think probably the best field to signal would be the output\n> > script. It is specified by the recipient of the funds, i.e., the\n> > current owner, and is already stored in the UTXO, so a single pass\n> > can\n> > tally up the votes. We could for example use the last 4 bits of the\n> > pubkey/pubkeyhash to opt in (3 leading 0 bits) and the vote (0/1\n> > depending on the stakeholders desired signal). We'd need to define\n> > similar semantics for other script types, but getting the standard\n> > scripts to be recognized should be simple.\n> >\n> > In the spirit of full disclosure I'd like to also mention some of the\n> > downsides of voting this way. Unlike the OP_RETURN proposal, users\n> > that do not intend to signal will also be included in the tally. I'd\n> > expect the signals of these users to be random with a 50% chance of\n> > either outcome, so they should not influence the final result, but\n> > may\n> > muddy the water depending on what part of the population is\n> > signalling. The opt-in should make sure that the majority of votes\n> > are\n> > actually voluntary votes, and not just users that randomly select a\n> > pubkey/pubkeyhash, and can be adjusted as desired, though higher\n> > values require more grinding on behalf of the users.\n> >\n> > The grinding may also exacerbate some problems we already have with\n> > the HD Wallet lookahead, since we now skip a number of addresses, so\n> > we should not require too many opt-in bits.\n> >\n> > So there are some problems we'd need to tackle, but I'm really\n> > excited\n> > about this, as it could provide data to make informed decisions, and\n> > should put an end to the endless speculation about the will of the\n> > economic majority.\n> >\n> > Cheers,\n> > Christian\n> >\n> > [1] http://pub.tik.ee.ethz.ch/students/2015-HS/SA-2015-30.pdf\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170420/bfb42e56/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Transaction signalling",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Marcel Jamin",
                "Tim Ruffing",
                "Erik Aronesty",
                "Christian Decker"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 14482
        }
    },
    {
        "title": "[bitcoin-dev] Properties of an ideal PoW algorithm & implementation",
        "thread_messages": [
            {
                "author": "Natanael",
                "date": "2017-04-18T10:34:04",
                "message_text_only": "To expand on this below;\n\nDen 18 apr. 2017 00:34 skrev \"Natanael\" <natanael.l at gmail.com>:\n\nIMHO the best option if we change PoW is an algorithm that's moderately\nprocessing heavy (we still need reasonably fast verification) and which\nresists partial state reuse (not fast or fully \"linear\" in processing like\nSHA256) just for the sake of invalidating asicboost style attacks, and it\nshould also have an existing reference implementation for hardware that's\nprovably close in performance to the theoretical ideal implementation of\nthe algorithm (in other words, one where we know there's no hidden\noptimizations).\n\n[...] The competition would mostly be about packing similar gate designs\nclosely and energy efficiency. (Now that I think about it, the proof MAY\nhave to consider energy use too, as a larger and slower but more efficient\nchip still is competitive in mining...)\n\n\nWhat matters for miners in terms of cost is primarily (correctly computed)\nhashes per joule (watt-seconds). The most direct proxy for this in terms of\nalgorithm execution is the number of transistor (gate) activations per\ncomputed hash (PoW unit).\n\nTo prove that an implementation is near optimal, you would show there's a\nminimum number of necessary transistor activations per computed hash, and\nthat your implementation is within a reasonable range of that number.\n\nWe also need to show that for a practical implementation you can't reuse\nmuch internal state (easiest way is \"whitening\" the block header,\npre-hashing or having a slow hash with an initial whitening step of its\nown). This is to kill any ASICBOOST type optimization. Performance should\nbe constant, not linear relative to input size.\n\nThe PoW step should always be the most expensive part of creating a\ncomplete block candidate! Otherwise it loses part of its meaning. It should\nhowever still also be reasonably easy to verify.\n\nGiven that there's already PoW ASIC optimizations since years back that use\ndeliberately lossy hash computations just because those circuits can run\nfaster (X% of hashes are computed wrong, but you get Y% more computed\nhashes in return which exceeds the error rate), any proof of an\nimplementation being near optimal (for mining) must also consider the\npossibility of implementations of a design that deliberately allows errors\njust to reduce the total count of transistor activations per N amount of\ncomputed hashes. Yes, that means the reference implementation is allowed to\nbe lossy.\n\nSo for a reasonably large N (number of computed hashes, to take batch\nprocessing into consideration), the proof would show that there's a\nspecific ratio for a minimum number of average gate activations per\ncorrectly computed hash, a smallest ratio = X number of gate activations /\n(N * success rate) across all possible implementations of the algorithm.\nAnd you'd show your implementation is close to that ratio.\n\nIt would also have to consider a reasonable range of time-memory tradeoffs\nincluding the potential of precomputation. Hopefully we could implement an\nalgorithm that effectively makes such precomputation meaningless by making\nthe potential gain insignificant for any reasonable ASIC chip size and\namount of precomputation resources.\n\nA summary of important mining PoW algorithm properties;\n\n* Constant verification speed, reasonably fast even on slow hardware\n\n* As explained above, still slow / expensive enough to dominate the costs\nof block candidate creation\n\n* Difficulty must be easy to adjust (no problem for simple hash-style\nalgorithms like today)\n\n* Cryptographic strength, something like preimage resistance (the algorithm\ncan't allow forcing a particular output, the chance must not be better than\nrandom within any achievable computational bounds)\n\n* As explained above, no hidden shortcuts. Everybody has equal knowledge.\n\n* Predictable and close to constant PoW computation performance, and not\nlinear in performance relative to input size the way SHA256 is (lossy\nimplementations will always make it not-quite-constant)\n\n* As explained above, no significant reusable state or other reusable work\n(killing ASICBOOST)\n\n* As explained above, no meaningful precomputation possible. No unfair\nheadstarts.\n\n* Should only rely on just transistors for implementation, shouldn't rely\non memory or other components due to unknowable future engineering results\nand changes in cost\n\n* Reasonably compact implementation, measured in memory use, CPU load and\nsimilar metrics\n\n* Reasonably small inputs and outputs (in line with regular hashes)\n\n* All mining PoW should be \"embarrassingly parallel\" (highly\nparallellizable) with minimal or no gain from batch computation,\nperformance scaling should be linear with increased chip size & cycle\nspeed.\n\nWhat else is there? Did I miss anything important?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170418/64d7218b/attachment-0001.html>"
            },
            {
                "author": "praxeology_guy",
                "date": "2017-04-18T19:14:05",
                "message_text_only": "Natanael,\n\n=== Metal Layers ===\n\nOne factor in chip cost other than transistor count is the number of layers required to route all the interconnects in the desired die area constraint. The need for fewer layers can result in less patent-able costs of layering technology. Fewer layers are quicker and easier to manufacture.\n\nI'm not an expert in the field, and I can't vouch for the validity of the entirety of the paper, but this paper discusses various factors that impact chip cost design.\nhttp://www.cse.psu.edu/~juz138/files/3d-cost-tcad10.pdf\n\n=== Early nonce mixing, Variable Length Input with Near Constant Work ===\n\nTo minimize asicboost like optimizations... the entirety of the input should be mixed with the nonce data ASAP. For example with Bitcoin as it is now, the 80 byte block header doesn't fully fit in one 64 byte SHA256 input block. This results in a 2nd SHA256 block input that only has 4 bytes of nonce and the rest constant that are mixed much later than the rest of the input... which allows for unexpected optimizations.\n\nSolution: A hash algorithm that could have more linear computation time vs input size would be a 2 stage algorithm:\n1. 1st stage Merkle tree hash to pre-lossy-mix-compress the variable length input stream to the size of the 2nd stage state vector. Each bit of input should have about equal influence on each of the output bits. (Minimize information loss, maximize mixed-ness).\n2. Multi-round mixing of the 2nd stage, where this stage is significantly more work than the 1st stage.\n\nThis is somewhat done already in Bitcoin by the PoW doing SHA256 twice in serial. The first time is pretty much the merkle tree hash (a node with two children), and then the second time is the mult-round mixing. If the Bitcoin PoW did SHA256 three or four times or more, then asicboost like optimizations would have less of an effect.\n\nIn actual hardware, assuming a particular input length for the design can result in a significantly more optimized design than creating hardware that can handle a variable length input. So your design goal of \"not linear in performance relative to input size\" to me seems to be a hard one to attain... in practical, to support very large input sizes in a constant work fashion requires a trade off between memory/parallelization and die space. I think it would be better to make an assumption about the block header size, such as that it is exactly 80 bytes, or, at least something reasonable like the hardware should be able to support a block header size <= 128 bytes.\n\nCheers,\nPraxeology Guy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170418/3a1bd7d0/attachment-0001.html>"
            },
            {
                "author": "Tim Ruffing",
                "date": "2017-04-19T11:08:15",
                "message_text_only": "On Tue, 2017-04-18 at 12:34 +0200, Natanael via bitcoin-dev wrote:\n> To prove that an implementation is near optimal, you would show\n> there's a minimum number of necessary transistor activations per\n> computed hash, and that your implementation is within a reasonable\n> range of that number.\u00a0\n\nI'm not an expert on lower bounds of algorithms but I think proving\nsuch properties is basically out of reach for mankind currently.\n\n> \n> We also need to show that for a practical implementation you can't\n> reuse much internal state (easiest way is \"whitening\" the block\n> header, pre-hashing or having a slow hash with an initial whitening\n> step of its own). This is to kill any ASICBOOST type optimization.\n> Performance should be constant, not linear relative to input size.\u00a0\n\nYes, a reasonable thing in practice seems to use a slower hash function\n(or just iterating the hash function many times), see also this thread:\n https://twitter.com/Ethan_Heilman/status/850015029189644288 .\n\nPoW verification will still be fast enough. That's not the bottleneck\nof block verification anyway.\n\nAlso, I don't agree that a PoW function should not rely on memory.\nMemory-hard functions are the best we have currently.\n\n\nTim"
            },
            {
                "author": "Bram Cohen",
                "date": "2017-04-19T17:43:03",
                "message_text_only": "Repeatedly hashing to make it so that lossy implementations just fail\nsounds like a great idea. Also relying on a single crypto primitive which\nis as simple as possible is also a great idea, and specifically using\nblake2b is conservative because not only is it simple but its block size is\nlarger than the amount of data being hashed so asicboost-style attacks\ndon't apply at all and the logic of multiple blocks doesn't have to be\nbuilt.\n\nMemory hard functions are a valiant effort and are holding up better than\nexpected but the problem is that when they fail they fail catastrophically,\nimmediately going from running on completely commodity hardware to only\nrunning on hardware from the one vendor who's pulled off the feat of making\nit work. My guess is it's only a matter of time until that happens.\n\nSo the best PoW function we know of today, assuming that you're trying to\nmake mining hardware as commodity as possible, is to repeatedly hash using\nblake2b ten or maybe a hundred times.\n\nMind you, I still think hard forking the PoW function is a very bad idea,\nbut if you were to do it, that would be the way to go.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170419/6dddcc73/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Properties of an ideal PoW algorithm & implementation",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Tim Ruffing",
                "Natanael",
                "Bram Cohen",
                "praxeology_guy"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 10234
        }
    },
    {
        "title": "[bitcoin-dev] Exploring Network Rule Changes",
        "thread_messages": [
            {
                "author": "Cameron Garnham",
                "date": "2017-04-20T17:02:43",
                "message_text_only": "I have taken some time to think about consensus systems in-general; and write up a guide that explores the problems space of changing the rules of such systems.\n\nHopefully, this guide will clarify the different options available to the Bitcoin Community.\n\nI am posting this to the Bitcoin Development mailing list for review. Possibly a more comprehensive form of this document could be useful as an informative BIP.\n\n\n** Type of Change **\n\nThere are three categories of changes:\n\nS:\tAddition of a new Rule. (Soft-Fork)\nH:\tRemoval of an old Rule. (Hard-Fork)\nE:\tSubverting an old Rule. (\u201cEvil\u201d, Non-Traditional Soft-Fork)\n\n* Addition of a new Rule:\nAll previous rules in the system remain enforced as originally intended.\n\nThere are two sub-categories for the addition of a new rule:\n\n1:\tNew Functionally is added to the system, without effecting old use cases. (Opt-In New Functionality)\n2:\tFunctional users of the system must change their behaviour to suit the new rule. (Mandatory New Functionality)\n\n* Removal of an old Rule\nEquivalent replacing the entire system with any-new system.  All full-users of the system must migrate to the new system.\n\n* Subverting an old Rule\nNew Functionally is added that explicitly Replaces Old Functionality.\n\nUsers must upgrade and migrate to the new Functionally to continue using the system.\n\n\n** Type of Activation **\n\nThere are two types of activations:\n\nU.\tExternal Activations. (User Activated)\nM.\tInternal Activations. (Miner Activated, PoS Activated, Internal Governance Model, etc)\n\nIt is possible to have more than one Activation Strategy used concurrently.\n\n* External Activations\nThese Activations are dictated by facts that are not quantifiable from within the System.\n\nGenerally, this will be a set-of-users, external to the system, that come to their own agreement to change the system.\n\n* Internal Activations\nThese activations use some metric from within the system to determine if a proposed change is activated.\n\nGenerally, some sort of internal signalling or vetoing process will happen and based upon its results, will dictate the if the change is activated.\n\n\n** Type of Signalling **\n\nUsers within the system with more important roles may wish to (or be forced to) signal or (not) veto about a particular topic. This could be part of the activation strategy (internal activations), or just simply to quantify the support of the upcoming change.\n\nThere are two core types of Signalling:\n\nO:\tOptional\nF.\tForced\n\nThere are two styles of Signalling:\n\nN.\tNormal Signalling (Opt-In)\nV.\tVeto (Opt-Out)\n\n* Optional Signalling\nOrthogonal to the system rules; however, the signalling still may affect other system rules.\n\n* Enforced Signalling\nThis is a meta-rule change. Normally only temporally enforced upon the system. This rule change doesn\u2019t directly affect the core behaviour of the system; it is just used for meta-purposes in the scope of another rule change.\n\n* Normal Signalling\nPassive Behaviour signals no support.\n\n* Veto Signalling\nPassive Behaviour signals support.\n\n\nIf I have missed anything or if anything is not clear, please contact me.\n\nPS.\n\nFor example, you could call a BIP9 (SegWit) activation as a:  \u201cS1MON\".  And BIP 148 (SegWit) as:  \u201cS1UFN\u201d.  However this letter code is just for fun. :)\n\nCameron"
            }
        ],
        "thread_summary": {
            "title": "Exploring Network Rule Changes",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Cameron Garnham"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 3277
        }
    },
    {
        "title": "[bitcoin-dev] Segwit v2",
        "thread_messages": [
            {
                "author": "Luke Dashjr",
                "date": "2017-04-20T20:28:52",
                "message_text_only": "Since BIP 141's version bit assignment will timeout soon, and needing renewal, \nI was thinking it might make sense to make some minor tweaks to the spec for \nthe next deployment. These aren't critical, so it's perfectly fine if BIP 141 \nactivates as-is (potentially with BIP 148), but IMO would be an improvement if \na new deployment (non-BIP148 UASF and/or new versionbit) is needed.\n\n1. Change the dummy marker to 0xFF instead of 0. Using 0 creates ambiguity \nwith incomplete zero-input transactions, which has been a source of confusion \nfor raw transaction APIs. 0xFF would normally indicate a >32-bit input count, \nwhich is impossible right now (it'd require a >=158 GB transaction) and \nunlikely to ever be useful.\n\n2. Relax the consensus rules on when witness data is allowed for an input. \nCurrently, it is only allowed when the scriptSig is null, and the scriptPubKey \nbeing spent matches a very specific pattern. It is ignored by \"upgrade-safe\" \npolicy when the scriptPubKey doesn't match an even-more-specific pattern. \nInstead, I suggest we allow it (in the consensus layer only) in combination \nwith scriptSig and with any scriptPubKey, and consider these cases to be \n\"upgrade-safe\" policy ignoring.\n\nThe purpose of the second change is to be more flexible to any future \nsoftforks. I consider it minor because we don't know of any possibilities \nwhere it would actually be useful.\n\nThoughts?\n\nLuke"
            },
            {
                "author": "praxeology_guy",
                "date": "2017-04-26T08:51:51",
                "message_text_only": "Luke,\n\nI can't really advise on your proposed changes... but I have a couple new suggestions:\n\n=== Future Proof Commitment Extension Methodology ===\n1. I'm not a fan of how ambiguous the direction is on handling future commitment extensions. See https://github.com/bitcoin/bips/blob/master/bip-0141.mediawiki#Extensible_commitment_structure.\n\n1-byte - OP_RETURN (0x6a)\n1-byte - Push the following 36 bytes (0x24)\n4-byte - Commitment header (0xaa21a9ed)\n32-byte - Commitment hash: Double-SHA256(witness root hash|witness reserved value*)\n39th byte onwards: Optional data with no consensus meaning\n* \"witness reserved value\" _must_ also go in the input's scriptSig/witness field... blah blah blah warning warning.\n\nWhere is the new \"witness reserved value\" going to go for the next extension? Why waste 32 bytes in some arbitrary location, and then later when there is an extension, eventually be wasting 32 more bytes at some other arbitrary location?\n\nHere's a more well defined/future proof proposal that uses fewer bytes:\n\n1-byte - OP_RETURN (0x6a)\n1-byte - Push the following 36 bytes (0x24)\n4-byte - Commitment header (0xaa21a9ed)\n32-byte - Commitment hash: Double-SHA256(extension root A|extension root B|extension root C...)\n\nvariable bytes - Extension identifiers: array of extension identifiers\nvariable bytes - Extension roots: array of {extension identifier, extension root length, extension root}\nnext byte onwards - Optional data with no consensus meaning\n\n- The extension identifiers are ordered the same as the order of the extension roots in the merkle root.\n- The extension identifiers can be highly compacted together using a custom compression algorithm.\n- The array length for the extension roots can be a utf8-like format, where up to 0-127 can be represented with one byte, and 128-16383 can be represented with two bytes.\n- The extension roots themselves only need to be provided for the sake of clients that are unable or not desiring to compute a particular extension root, but do want to verify some of the extension roots.\n\nThis design is much more future proof, and uses less space. With SegWit only, this would take up maybe 1 byte for the extension identifiers (compressed length 1 and id 0 for wtxroot), and 1 byte for the extension roots (array length = 0).\n\n=== Replay Attack Prevention ===\n2. Implement the Policy ID 'replay attack\" prevention that I have suggested (but is in dev list purgatory), which increases each wtx length by 1 byte. This can be reduced in a block by clustering Policy ID ranges in the coinbase... or by guessing the Policy ID. Witness data would sign on the Policy ID... preventing replay if at least one branch adopted a new Policy ID.\n\nCheers,\nPraxeology Guy\n\n-------- Original Message --------\nSubject: [bitcoin-dev] Segwit v2\nLocal Time: April 20, 2017 3:28 PM\nUTC Time: April 20, 2017 8:28 PM\nFrom: bitcoin-dev at lists.linuxfoundation.org\nTo: bitcoin-dev at lists.linuxfoundation.org\n\nSince BIP 141's version bit assignment will timeout soon, and needing renewal,\nI was thinking it might make sense to make some minor tweaks to the spec for\nthe next deployment. These aren't critical, so it's perfectly fine if BIP 141\nactivates as-is (potentially with BIP 148), but IMO would be an improvement if\na new deployment (non-BIP148 UASF and/or new versionbit) is needed.\n\n1. Change the dummy marker to 0xFF instead of 0. Using 0 creates ambiguity\nwith incomplete zero-input transactions, which has been a source of confusion\nfor raw transaction APIs. 0xFF would normally indicate a >32-bit input count,\nwhich is impossible right now (it'd require a >=158 GB transaction) and\nunlikely to ever be useful.\n\n2. Relax the consensus rules on when witness data is allowed for an input.\nCurrently, it is only allowed when the scriptSig is null, and the scriptPubKey\nbeing spent matches a very specific pattern. It is ignored by \"upgrade-safe\"\npolicy when the scriptPubKey doesn't match an even-more-specific pattern.\nInstead, I suggest we allow it (in the consensus layer only) in combination\nwith scriptSig and with any scriptPubKey, and consider these cases to be\n\"upgrade-safe\" policy ignoring.\n\nThe purpose of the second change is to be more flexible to any future\nsoftforks. I consider it minor because we don't know of any possibilities\nwhere it would actually be useful.\n\nThoughts?\n\nLuke\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170426/6a50a078/attachment.html>"
            },
            {
                "author": "Johnson Lau",
                "date": "2017-04-26T19:31:38",
                "message_text_only": "I prefer not to do anything that requires pools software upgrade or wallet upgrade. So I prefer to keep the dummy marker, and not change the commitment structure as suggested by another post.\n\nFor your second suggestion, I think we should keep scriptSig empty as that should be obsoleted. If you want to put something in scriptSig, you should put it in witness instead.\n\nMaybe we could restrict witness to IsPushOnly() scriptPubKey, so miners can\u2019t put garbage to legacy txs. But I think relaxing the witness program size to 73 bytes is enough for any purpose.\n\n> On 21 Apr 2017, at 04:28, Luke Dashjr via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> Since BIP 141's version bit assignment will timeout soon, and needing renewal, \n> I was thinking it might make sense to make some minor tweaks to the spec for \n> the next deployment. These aren't critical, so it's perfectly fine if BIP 141 \n> activates as-is (potentially with BIP 148), but IMO would be an improvement if \n> a new deployment (non-BIP148 UASF and/or new versionbit) is needed.\n> \n> 1. Change the dummy marker to 0xFF instead of 0. Using 0 creates ambiguity \n> with incomplete zero-input transactions, which has been a source of confusion \n> for raw transaction APIs. 0xFF would normally indicate a >32-bit input count, \n> which is impossible right now (it'd require a >=158 GB transaction) and \n> unlikely to ever be useful.\n> \n> 2. Relax the consensus rules on when witness data is allowed for an input. \n> Currently, it is only allowed when the scriptSig is null, and the scriptPubKey \n> being spent matches a very specific pattern. It is ignored by \"upgrade-safe\" \n> policy when the scriptPubKey doesn't match an even-more-specific pattern. \n> Instead, I suggest we allow it (in the consensus layer only) in combination \n> with scriptSig and with any scriptPubKey, and consider these cases to be \n> \"upgrade-safe\" policy ignoring.\n> \n> The purpose of the second change is to be more flexible to any future \n> softforks. I consider it minor because we don't know of any possibilities \n> where it would actually be useful.\n> \n> Thoughts?\n> \n> Luke\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Luke Dashjr",
                "date": "2017-04-26T20:01:10",
                "message_text_only": "On Wednesday 26 April 2017 7:31:38 PM Johnson Lau wrote:\n> I prefer not to do anything that requires pools software upgrade or wallet\n> upgrade. So I prefer to keep the dummy marker, and not change the\n> commitment structure as suggested by another post.\n\nFair enough, I guess. Although I think the dummy marker could actually be non-\nconsensus critical so long as the hashing replaces it with a 0.\n\n> For your second suggestion, I think we should keep scriptSig empty as that\n> should be obsoleted. If you want to put something in scriptSig, you should\n> put it in witness instead.\n\nThere are things scriptSig can do that witness cannot today - specifically add \nadditional conditions under the signature. We can always obsolete scriptSig \nlater, after segwit has provided an alternative way to do this.\n\n> Maybe we could restrict witness to IsPushOnly() scriptPubKey, so miners\n> can\u2019t put garbage to legacy txs.\n\nThey already can malleate transactions and add garbage to the blocks. I don't \nsee the benefit here.\n\nLuke"
            },
            {
                "author": "Johnson Lau",
                "date": "2017-04-26T20:09:34",
                "message_text_only": "> On 27 Apr 2017, at 04:01, Luke Dashjr <luke at dashjr.org> wrote:\n> \n> On Wednesday 26 April 2017 7:31:38 PM Johnson Lau wrote:\n>> I prefer not to do anything that requires pools software upgrade or wallet\n>> upgrade. So I prefer to keep the dummy marker, and not change the\n>> commitment structure as suggested by another post.\n> \n> Fair enough, I guess. Although I think the dummy marker could actually be non-\n> consensus critical so long as the hashing replaces it with a 0.\n> \n>> For your second suggestion, I think we should keep scriptSig empty as that\n>> should be obsoleted. If you want to put something in scriptSig, you should\n>> put it in witness instead.\n> \n> There are things scriptSig can do that witness cannot today - specifically add \n> additional conditions under the signature. We can always obsolete scriptSig \n> later, after segwit has provided an alternative way to do this.\n\nYou can do this with witness too, which is also cheaper. Just need to make sure the signature covers a special part of the witness. I will make a proposal to Litecoin soon, which allows signing and executing extra scripts in witness. Useful for things like OP_PUSHBLOCKHASH\n\n> \n>> Maybe we could restrict witness to IsPushOnly() scriptPubKey, so miners\n>> can\u2019t put garbage to legacy txs.\n> \n> They already can malleate transactions and add garbage to the blocks. I don't \n> see the benefit here.\n\nWitness is cheaper and bigger\n\n> \n> Luke"
            },
            {
                "author": "Russell O'Connor",
                "date": "2017-04-26T21:34:29",
                "message_text_only": "On Wed, Apr 26, 2017 at 4:01 PM, Luke Dashjr via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> There are things scriptSig can do that witness cannot today - specifically\n> add\n> additional conditions under the signature. We can always obsolete scriptSig\n> later, after segwit has provided an alternative way to do this.\n>\n\nI'm not sure what you are referring to here.  The data in the scriptSigs\nare never signed.  The scriptSigs are always stripped from the transaction\nbefore the sigHash is made.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170426/f3cfa587/attachment.html>"
            },
            {
                "author": "praxeology_guy",
                "date": "2017-04-27T02:18:57",
                "message_text_only": "Johnson Lau,\n\n> not change the commitment structure as suggested by another post\n\nNot sure if you realize my proposal is backwards compatible. We could also merge the two arrays, which would be harder to compress, but a more simple format. Below I gave an example of how this would be backwards compatible.\n\n1-byte - OP_RETURN (0x6a)\n1-byte - Push the following 36 bytes (0x24)\n4-byte - Commitment header (0xaa21a9ed)\n32-byte - Commitment hash: Double-SHA256(witness root hash|witness reserved value*)\nvariable bytes - Extension roots: array of {extension identifier, extension root length, extension root}\nbytes onwards: Optional data with no consensus meaning\n\n* \"witness reserved value\" _must_ also go in the input's scriptSig/witness field\n\nHere is an example of the \"Extension roots\" with this format:\nExtension roots: 2, {0, 0, []}, {1, 0, []}\n\nsize = 2 // two elements in Commitment hash\n{ext.id = 0, length = 0, empty} // First element is the wtxid merkle root hash, must be calculated, not specified here\n{ext.id = 1, length = 0, empty} // Second element is the \"witness reserved value\", which is found in the scriptSig\n\nLater after all the miners upgrade, we could stop using the ext.id = 1 and also stop putting the unneccesary value in scriptSig.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170426/b8487e38/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Segwit v2",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Russell O'Connor",
                "Luke Dashjr",
                "Johnson Lau",
                "praxeology_guy"
            ],
            "messages_count": 7,
            "total_messages_chars_count": 13047
        }
    },
    {
        "title": "[bitcoin-dev] Reference implementation (wip) of bip-genvbvoting (generalized version bits)",
        "thread_messages": [
            {
                "author": "Sancho Panza",
                "date": "2017-04-20T20:56:45",
                "message_text_only": "Dear all,\n\nAn initial reference implementation of bip-genvbvoting (spec: [1]) is now available at\n\nhttps://github.com/sanch0panza/bitcoin/commits/genvbvoting-bu-dev-clean1\n\nstarting at commit fdd83383436ee43b072c258d4a6feb713902c77e .\n\nThis development is based against the Bitcoin Unlimited 'dev' branch, and has been submitted as PR458 to BU [2].\nThe naming of the new 'bipgenvb_forks' output section in the 'getblockchain' RPC interface is to be considered temporary while the BIP has no formal number.\n\nI would be happy to get any feedback while I implement a corresponding pull request for a reference implementation on Bitcoin Core. Due to other commitments this may come at a later stage - if someone else is eager to port it over, please feel free.\n\nRegards,\nSancho\n\n[1] https://github.com/sanch0panza/bips/blob/bip-genvbvoting/bip-genvbvoting.mediawiki\n[2] https://github.com/BitcoinUnlimited/BitcoinUnlimited/pull/458\n\nP.S. The revised \"unknown version check\" code is considered an implementation specific and not part of core functionality, and is consequently not fully covered by regression tests.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170420/3307cbdf/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Reference implementation (wip) of bip-genvbvoting (generalized version bits)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Sancho Panza"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1293
        }
    },
    {
        "title": "[bitcoin-dev] Bitcoin Core 0.14.1 released",
        "thread_messages": [
            {
                "author": "Wladimir J. van der Laan",
                "date": "2017-04-22T13:10:47",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA512\n\nBitcoin Core version 0.14.1 is now available from:\n\n  <https://bitcoin.org/bin/bitcoin-core-0.14.1/>\n\nOr, by torrent:\n\n  magnet:?xt=urn:btih:0482be8fc8e1c0b02162871e3591efc3d1d34585&dn=bitcoin-core-0.14.1&tr=udp%3A%2F%2Fpublic.popcorn-tracker.org%3A6969%2Fannounce&tr=http%3A%2F%2Fatrack.pow7.com%2Fannounce&tr=http%3A%2F%2Fbt.henbt.com%3A2710%2Fannounce&tr=http%3A%2F%2Fmgtracker.org%3A6969%2Fannounce&tr=http%3A%2F%2Fopen.touki.ru%2Fannounce.php&tr=http%3A%2F%2Fp4p.arenabg.ch%3A1337%2Fannounce&tr=http%3A%2F%2Fpow7.com%3A80%2Fannounce&tr=http%3A%2F%2Ftracker.dutchtracking.nl%3A80%2Fannounce\n\nThis is a new minor version release, including various bugfixes and\nperformance improvements, as well as updated translations.\n\nPlease report bugs using the issue tracker at github:\n\n  <https://github.com/bitcoin/bitcoin/issues>\n\nTo receive security and update notifications, please subscribe to:\n\n  <https://bitcoincore.org/en/list/announcements/join/>\n\nCompatibility\n==============\n\nBitcoin Core is extensively tested on multiple operating systems using\nthe Linux kernel, macOS 10.8+, and Windows Vista and later.\n\nMicrosoft ended support for Windows XP on [April 8th, 2014](https://www.microsoft.com/en-us/WindowsForBusiness/end-of-xp-support),\nNo attempt is made to prevent installing or running the software on Windows XP, you\ncan still do so at your own risk but be aware that there are known instabilities and issues.\nPlease do not report issues about Windows XP to the issue tracker.\n\nBitcoin Core should also work on most other Unix-like systems but is not\nfrequently tested on them.\n\nNotable changes\n===============\n\nRPC changes\n- -----------\n\n- - The first positional argument of `createrawtransaction` was renamed from\n  `transactions` to `inputs`.\n\n- - The argument of `disconnectnode` was renamed from `node` to `address`.\n\nThese interface changes break compatibility with 0.14.0, when the named\narguments functionality, introduced in 0.14.0, is used. Client software\nusing these calls with named arguments needs to be updated.\n\nMining\n- ------\n\nIn previous versions, getblocktemplate required segwit support from downstream\nclients/miners once the feature activated on the network. In this version, it\nnow supports non-segwit clients even after activation, by removing all segwit\ntransactions from the returned block template. This allows non-segwit miners to\ncontinue functioning correctly even after segwit has activated.\n\nDue to the limitations in previous versions, getblocktemplate also recommended\nnon-segwit clients to not signal for the segwit version-bit. Since this is no\nlonger an issue, getblocktemplate now always recommends signalling segwit for\nall miners. This is safe because ability to enforce the rule is the only\nrequired criteria for safe activation, not actually producing segwit-enabled\nblocks.\n\nUTXO memory accounting\n- ----------------------\n\nMemory usage for the UTXO cache is being calculated more accurately, so that\nthe configured limit (`-dbcache`) will be respected when memory usage peaks\nduring cache flushes.  The memory accounting in prior releases is estimated to\nonly account for half the actual peak utilization.\n\nThe default `-dbcache` has also been changed in this release to 450MiB.  Users\nwho currently set `-dbcache` to a high value (e.g. to keep the UTXO more fully\ncached in memory) should consider increasing this setting in order to achieve\nthe same cache performance as prior releases.  Users on low-memory systems\n(such as systems with 1GB or less) should consider specifying a lower value for\nthis parameter.\n\nAdditional information relating to running on low-memory systems can be found\nhere:\n[reducing-bitcoind-memory-usage.md](https://gist.github.com/laanwj/efe29c7661ce9b6620a7).\n\n0.14.1 Change log\n=================\n\nDetailed release notes follow. This overview includes changes that affect\nbehavior, not code moves, refactors and string updates. For convenience in locating\nthe code changes and accompanying discussion, both the pull request and\ngit merge commit are mentioned.\n\n### RPC and other APIs\n- - #10084 `142fbb2` Rename first named arg of createrawtransaction (MarcoFalke)\n- - #10139 `f15268d` Remove auth cookie on shutdown (practicalswift)\n- - #10146 `2fea10a` Better error handling for submitblock (rawodb, gmaxwell)\n- - #10144 `d947afc` Prioritisetransaction wasn't always updating ancestor fee (sdaftuar)\n- - #10204 `3c79602` Rename disconnectnode argument (jnewbery)\n\n### Block and transaction handling\n- - #10126 `0b5e162` Compensate for memory peak at flush time (sipa)\n- - #9912 `fc3d7db` Optimize GetWitnessHash() for non-segwit transactions (sdaftuar)\n- - #10133 `ab864d3` Clean up calculations of pcoinsTip memory usage (morcos)\n\n### P2P protocol and network code\n- - #9953/#10013 `d2548a4` Fix shutdown hang with >= 8 -addnodes set (TheBlueMatt)\n- - #10176 `30fa231` net: gracefully handle NodeId wrapping (theuni)\n\n### Build system\n- - #9973 `e9611d1` depends: fix zlib build on osx (theuni)\n\n### GUI\n- - #10060 `ddc2dd1` Ensure an item exists on the rpcconsole stack before adding (achow101)\n\n### Mining\n- - #9955/#10006 `569596c` Don't require segwit in getblocktemplate for segwit signalling or mining (sdaftuar)\n- - #9959/#10127 `b5c3440` Prevent slowdown in CreateNewBlock on large mempools (sdaftuar)\n\n### Tests and QA\n- - #10157 `55f641c` Fix the `mempool_packages.py` test (sdaftuar)\n\n### Miscellaneous\n- - #10037 `4d8e660` Trivial: Fix typo in help getrawtransaction RPC (keystrike)\n- - #10120 `e4c9a90` util: Work around (virtual) memory exhaustion on 32-bit w/ glibc (laanwj)\n- - #10130 `ecc5232` bitcoin-tx input verification (awemany, jnewbery)\n\nCredits\n=======\n\nThanks to everyone who directly contributed to this release:\n\n- - Alex Morcos\n- - Andrew Chow\n- - Awemany\n- - Cory Fields\n- - Gregory Maxwell\n- - James Evans\n- - John Newbery\n- - MarcoFalke\n- - Matt Corallo\n- - Pieter Wuille\n- - practicalswift\n- - rawodb\n- - Suhas Daftuar\n- - Wladimir J. van der Laan\n\nAs well as everyone that helped translating on [Transifex](https://www.transifex.com/projects/p/bitcoin/).\n\n\n-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v1\n\niQEcBAEBCgAGBQJY+1YNAAoJEHSBCwEjRsmme+YIAIkJLCjimADYBJoM8bnHK2Dc\n4KznlAjpXqFWb6taoSyWi+/6DtZxJF1MZm+iaDhqmTEy+ms313N2mBEd2xrSAPPL\nnYf84e3tgnwq07sQmUxVZXyhZe2R+m/kgy75TTZw+bonyXwwA3384F0L8gvV5Iu+\nkNNu6WggWUTvOADEFVKecgzWeT1FCYXklbNk+Z5T/YWWrKA8ATXgv45IIEKT8uI1\npqhKQxoqLM3ga7Df3VbzwXAYAOCaFzf+0nmTRoqDM5pX+FQ2hq0UM6joLnUNk2ee\nG4/nsNWAKg/6eycrA7Wvawwcozr2iYAov/YDj6pEI8UoeGcOdlSh69Seb1cngHg=\n=EHlY\n-----END PGP SIGNATURE-----"
            }
        ],
        "thread_summary": {
            "title": "Bitcoin Core 0.14.1 released",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Wladimir J. van der Laan"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 6600
        }
    },
    {
        "title": "[bitcoin-dev] Wallet: A User Friendly Data Structure",
        "thread_messages": [
            {
                "author": "praxeology_guy",
                "date": "2017-04-24T04:29:19",
                "message_text_only": "Below is a proposal for a wallet data structure that can enable a wallet to be user friendly.\n\nThis is a proposed partial solution to \"Pruned wallet support #9409\": https://github.com/bitcoin/bitcoin/issues/9409\nIt is envisioned to work with \"Complete hybrid full block SPV mode #9483\": https://github.com/bitcoin/bitcoin/pull/9483\n\nThe data structure implies that the wallet would/could run in a different process than a Bitcoin node. Furthermore, the wallet would have a set of whitelisted/trusted nodes, which may only be the user's nodes.\n\nCheers,\nPraxeology Guy\n\nThe primary purpose of this data structure is to enable the creation of a responsive and informative open-and-go wallet. Some of my design goals include:\n- Instant on. Can be put in cold storage, and years later be immediately operational.\n- Scans optional/happen in background. Supports partial scans of the block height range.\n- Functional even with only utxo set data\n- Fast loading of external private keys and other watched identifiers.\n- Supports wallet merging\n- Supports connection with different kinds of nodes/providers of transaction evidence\n- Communicates the reliability of the evidence with the user\n- Supports transaction deprecation and double spending\n- Can work in a different process than a Bitcoin relay node.\n- Works immediately/quickly even with a node that has only prefetched headers and only begun validating blocks.\n- Potentially allows a wallet to display unconfirmed transactions to the user even if it only has an SPV evidence source.\n\n=== Wallet ===\n- List: Spend Term\n- List: Wallet Coin\n- List: Spend Attempt\n- List: Transaction Evidence\n- List: Coin Scan\n- List: Evidence Source\n\n// The wallet would also need other data that existing wallets have such as information about deterministic keys and pre-allocated keys etc. For the wallet to work with SPV security, it would also need block headers.\n\n=== Spend Term ===\n- GUID\n- Name (Human readable)\n- Type: [private key/public key/address/out script]\n- Term Value\n- Creation date\n- List: Wallet Coin\n\n// A spend term is anything that can be used to identify a coin that should be tracked by this wallet. GUID should probably be the address or a hash of the out script.\n\n=== Evidence Source ===\n- GUID\n- Operator Name\n- Device Name\n- Software Name\n- Software Version\n- Public Key\n- Operator Trust: Self, Reliable Friend, Stranger\n- Device Security: Air Gap, Single Purpose Networked, Multipurpose Networked, Dedicated Remote Hosted, VPS Remote Hosted\n\n// An Evidence Source is a {operator, device, software} that fully validates blocks. It provides evidence of coin confirmations and spends. Maybe GUID should be a hash of the public key.\n\n=== Wallet Coin ===\nRequired:\n- Spend Term GUID\n- TXID\n- vout.ix\n- amount\n- output script\nOptional:\n- Wallet first discovery date\n- full TX data\n- List: Transaction Evidence\n- List: Spend Attempt\n- TXID Spent\n\n// A wallet coin is a bitcoin txo with some extra data relevant to the wallet. A wallet coin's ID is {TXID, vout.ix}.\n\n=== Spend Attempt ===\n- Wallet Coins List: {TXID, vout.ix}\n- TXID\n- Creation date\n- Relay date(s)\n- Is Deprecated?\n- Deprecated date\n- Replace By Fee (RBF) enabled?\n- List: Transaction Evidence\n\n// deprecating a spend attempt will clear the \"TXID Spent\" in Wallet Coin, and set \"Is Deprecated?\" and \"Deprecated date\".\n\n=== Evidence ===\nRequired:\n- tip hash\n- tip height\n- date\n- Evidence Source GUID\n- Evidence Source Signature\n\n=== Transaction Evidence : Evidence ===\nRequired:\n- TXID\nOptional:\n- discover date\n\n// Evidence of the creation or spend of a coin. There are different kinds of evidence from different sources. The kind and source impact the user's confidence in the validity and finality of a transaction.\n\n=== Confirmed UTXO Set Presence : Transaction Evidence ===\nRequired:\n- Is in UTXO set?\nOptional:\n- block height\n- block hash\n- block timestamp\n- Future: utxo set snaphot merkle proof\n- confirm date\n\n// Evidence (from a trustworthy source needed) that a coin is or is not in the confirmed utxo set. With the future possibility of utxo set snapshot merkle proof, the trustworthiness of the source is not as important. This evidence type is only used for Wallet Coins. It is not used for Spend Attempts.\n\n=== Confirmation : Transaction Evidence ===\nRequired:\n- block height\n- block hash\n- block timestamp\n- tx merkle proof\n- wtx merkle proof\n- is in greatest PoW chain?\nOptional:\n- confirm date\n- 51% attack cost to double spend\n- Future: Are/which reliable miners still creating blocks in the greatest PoW chain (network split risk)\n\n// Evidence that a transaction was confirmed.\n\n=== Discovery (Unconfirmed TX) : Transaction Evidence ===\nOptional:\n- Confirmable Evidence: tree of source input transactions that lead back to utxo outputs. Unconfirmed parents: discover date, size, fee.\n\n// Evidence merely that a transaction was discovered. There is no guarantee that the transaction will be confirmed. Preferably the Source that is providing the wallet with the discovery evidence will also provide evidence that the transaction could be confirmed... especially if the Source is not trusted. The wallet could ask other Sources whether the confirmed inputs are present in the Confirmed UTXO Set.\n\n=== Spend Term Scan : Evidence ===\n- Term List: Spend Term GUID\n- Target: [blocks, confirmed utxo set]\n- Range Start\n- Range End\n\n// The wallet would request a scan from the Source. It would provide the spend terms, target, and range. The Source would respond with the range actually scanned, in addition to any Transaction Evidence discovered.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170424/2cfdf37c/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Wallet: A User Friendly Data Structure",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "praxeology_guy"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 5766
        }
    },
    {
        "title": "[bitcoin-dev] Draft BIP: Segwit deployment with versionbits and guaranteed lock-in",
        "thread_messages": [
            {
                "author": "shaolinfry",
                "date": "2017-04-26T18:15:26",
                "message_text_only": "This is a draft BIP proposal to redeploy segwit using BIP-8, from the day after the current BIP9 segwit times out.\n\nThis BIP could be deployed long before Nov 15th 2016, for example in July allowing wide deployment to begin soon. The timeout (and this useractivation) could be set to roughly a year from then. However, considering around 70% of nodes upgraded to witness capability within 5-6 months, I personally think we could reduce the time, especially considering how much people want segwit - but I understand the need for more caution in Bitcoin.\n\nPreliminary dates are deploy within a couple months, startdate Nov 16th 2017, BIP8 timeout July 4th 2018.\n\n<pre>\nBIP: ?\nLayer: Consensus (soft fork)\nTitle: Segwit deployment with versionbits and guaranteed lock-in\nAuthor: Shaolin Fry <shaolinfry at protonmail.ch>\nComments-Summary: No comments yet.\nComments-URI: https://github.com/bitcoin/bips/wiki/Comments:BIP-????\nStatus: Draft\nType: Standards Track\nCreated: 2017-04-14\nLicense: BSD-3-Clause\nCC0-1.0\n</pre>\n\n==Abstract==\n\nThis document specifies a user activated soft fork for BIP141, BIP143 and BIP147 using versionbits with guaranteed lock-in.\n\n==Motivation==\n\nMiners have been reluctant to signal the BIP9 segwit deployment despite a large portion of the Bitcoin ecosystem who want the soft fork activated. This BIP specifies a user activated soft fork (UASF) that deploys segwit again using versionbits with guaranteed lock-in on timeout if the BIP is not already locked-in or activated by the timeout. This ensures users have sufficient time to prepare and no longer require a miner supermajority, while still allowing for an earlier miner activated soft fork (MASF).\n\n==Reference implementation==\n\nhttps://github.com/bitcoin/bitcoin/compare/master...shaolinfry:uasegwit-flagday\n\n==Specification==\n\nThis deployment will set service bit (1<<5) as NODE_UAWITNESS.\n\n==Deployment==\n\nThis BIP will be deployed by BIP8 with the name \"uasegwit\" and using bit 2.\n\nFor Bitcoin mainnet, the BIP8 starttime will be midnight 16 November 2017 UTC (Epoch timestamp 1510790400) and BIP8 timeout will be 4 July 2018 UTC (Epoch timestamp 1530662400).\n\nFor Bitcoin testnet, segwit is already activated so no deployment is specified.\n\n==Rationale==\n\nThis BIP can be deployed well in advance of the BIP8 '''starttime''' so that the '''timeout''' will be sufficiently far in the future to allow Bitcoin users to uprgade in preparation.\n\nThe '''starttime''' of this BIP is after the BIP9 \"segwit\" timeout to remove compatibility issues with old nodes.\n\n==References==\n\nhttps://github.com/bitcoin/bips/blob/master/bip-0008.mediawiki\n\nhttps://github.com/bitcoin/bips/blob/master/bip-0009.mediawiki\n\nhttps://github.com/bitcoin/bips/blob/master/bip-0141.mediawiki\n\nhttps://github.com/bitcoin/bips/blob/master/bip-0143.mediawiki\n\nhttps://github.com/bitcoin/bips/blob/master/bip-0147.mediawiki\n\n==Copyright==\n\nThis document is dual licensed as BSD 3-clause, and Creative Commons CC0 1.0 Universal.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170426/8b2a5dcf/attachment-0001.html>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2017-04-26T18:33:56",
                "message_text_only": "See Segwit v2 thread. Maybe we can collaborate on combining these.\n\nOn Wednesday 26 April 2017 6:15:26 PM shaolinfry via bitcoin-dev wrote:\n> This is a draft BIP proposal to redeploy segwit using BIP-8, from the day\n> after the current BIP9 segwit times out.\n> \n> This BIP could be deployed long before Nov 15th 2016, for example in July\n> allowing wide deployment to begin soon. The timeout (and this\n> useractivation) could be set to roughly a year from then. However,\n> considering around 70% of nodes upgraded to witness capability within 5-6\n> months, I personally think we could reduce the time, especially\n> considering how much people want segwit - but I understand the need for\n> more caution in Bitcoin.\n> \n> Preliminary dates are deploy within a couple months, startdate Nov 16th\n> 2017, BIP8 timeout July 4th 2018.\n> \n> <pre>\n> BIP: ?\n> Layer: Consensus (soft fork)\n> Title: Segwit deployment with versionbits and guaranteed lock-in\n> Author: Shaolin Fry <shaolinfry at protonmail.ch>\n> Comments-Summary: No comments yet.\n> Comments-URI: https://github.com/bitcoin/bips/wiki/Comments:BIP-????\n> Status: Draft\n> Type: Standards Track\n> Created: 2017-04-14\n> License: BSD-3-Clause\n> CC0-1.0\n> </pre>\n> \n> ==Abstract==\n> \n> This document specifies a user activated soft fork for BIP141, BIP143 and\n> BIP147 using versionbits with guaranteed lock-in.\n> \n> ==Motivation==\n> \n> Miners have been reluctant to signal the BIP9 segwit deployment despite a\n> large portion of the Bitcoin ecosystem who want the soft fork activated.\n> This BIP specifies a user activated soft fork (UASF) that deploys segwit\n> again using versionbits with guaranteed lock-in on timeout if the BIP is\n> not already locked-in or activated by the timeout. This ensures users have\n> sufficient time to prepare and no longer require a miner supermajority,\n> while still allowing for an earlier miner activated soft fork (MASF).\n> \n> ==Reference implementation==\n> \n> https://github.com/bitcoin/bitcoin/compare/master...shaolinfry:uasegwit-fla\n> gday\n> \n> ==Specification==\n> \n> This deployment will set service bit (1<<5) as NODE_UAWITNESS.\n> \n> ==Deployment==\n> \n> This BIP will be deployed by BIP8 with the name \"uasegwit\" and using bit 2.\n> \n> For Bitcoin mainnet, the BIP8 starttime will be midnight 16 November 2017\n> UTC (Epoch timestamp 1510790400) and BIP8 timeout will be 4 July 2018 UTC\n> (Epoch timestamp 1530662400).\n> \n> For Bitcoin testnet, segwit is already activated so no deployment is\n> specified.\n> \n> ==Rationale==\n> \n> This BIP can be deployed well in advance of the BIP8 '''starttime''' so\n> that the '''timeout''' will be sufficiently far in the future to allow\n> Bitcoin users to uprgade in preparation.\n> \n> The '''starttime''' of this BIP is after the BIP9 \"segwit\" timeout to\n> remove compatibility issues with old nodes.\n> \n> ==References==\n> \n> https://github.com/bitcoin/bips/blob/master/bip-0008.mediawiki\n> \n> https://github.com/bitcoin/bips/blob/master/bip-0009.mediawiki\n> \n> https://github.com/bitcoin/bips/blob/master/bip-0141.mediawiki\n> \n> https://github.com/bitcoin/bips/blob/master/bip-0143.mediawiki\n> \n> https://github.com/bitcoin/bips/blob/master/bip-0147.mediawiki\n> \n> ==Copyright==\n> \n> This document is dual licensed as BSD 3-clause, and Creative Commons CC0\n> 1.0 Universal."
            }
        ],
        "thread_summary": {
            "title": "Draft BIP: Segwit deployment with versionbits and guaranteed lock-in",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "shaolinfry",
                "Luke Dashjr"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 6471
        }
    },
    {
        "title": "[bitcoin-dev] Trustless Segwit activation bounty protocol (aka. bribing the miners)",
        "thread_messages": [
            {
                "author": "Matt Bell",
                "date": "2017-04-27T17:48:16",
                "message_text_only": "Hello everyone,\n\nI've been thinking of an alternative to possibly get Segwit activated\nsooner: bribing the miners. This proposal may not be necessary if everyone\nis already set on doing a UASF, but  miners seem to optimize for short-term\nprofits and this may make it easier for BitMain to accept its fate in\nlosing the ASICBoost advantage.\n\nHere is a possible trustless contract protocol where contributors could\npledge to a Segwit bounty which would be paid out to miners iff Segwit is\nactivated, else the funds are returned to the contributor:\n\n# Setup\n\n- The contributor picks a possible future height where Segwit may be\nactivated and when the funds should be released, H.\n- The contributor chooses a bounty contribution amount X.\n- The contributor generates 3 private keys (k1, k2, and k3) and\ncorresponding pubkeys (K1, K2, and K3).\n- The contributor creates and broadcasts the Funding Transaction, which has\n2 outputs:\n  * Output 0, X BTC, P2SH redeem script:\n    <H> CHECKLOCKTIMEVERIFY DROP\n    <K1> CHECKSIGVERIFY\n  * Output 1, 0.00001 BTC, P2SH redeem script:\n    <H> CHECKLOCKTIMEVERIFY DROP\n    <K2> CHECKSIGVERIFY\n- The contributor builds the Segwit Assertion Transaction:\n  * nTimeLock set to H\n  * Input 0, spends from Funding Transaction output 1, signed with k2,\nSIGHASH_ALL\n  * Output 0, 0.00001 BTC, P2WPKH using K3\n- The contributor builds the Bounty Payout Transaction:\n  * nTimeLock set to H\n  * Input 0, spends from Funding Transaction output 0, signed with k1,\nSIGHASH_ALL\n  * Input 1, spends from Segwit Assertion Transaction output 0, signed with\nk3, SIGHASH_ALL\n  * No outputs, all funds are paid to the miner\n- The contributor publishes the Segwit Assertion Transaction and Bounty\nPayout Transaction (with signatures) somewhere where miners can find them\n\n# Process\n\n1. After the setup, miners can find Funding Transactions confirmed on the\nchain, and verify the other 2 transactions are correct and have valid\nsignatures. They can sum all the valid bounty contracts they find to factor\ninto their expected mining profit.\n2A. Once the chain reaches height H-1, if Segwit has activated, miners can\nclaim the bounty payout by including the Segwit Assertion and Bounty Payout\ntransactions in their block H. Since Segwit has activated, the output from\nthe Segwit Assertion tx can be spent by the Bounty Payout, so both\ntransactions are valid and the miner receives the funds.\n2B. If Segwit has not activated at height H, Input 1 of the Bounty Payout\nis not valid since it spends a P2WPKH output, preventing the miner from\nincluding the Bounty Payout transaction in the block. (However, the output\nof the Segwit Assertion tx can be claimed since it is treated as\nanyone-can-spend, although this is not an issue since it is a very small\namount). The contributor can reclaim the funds from Output 0 of the Funding\ntx by creating a new transaction, signed with k1.\n\n# Notes\n\n- This is likely a win-win scenario for the contributors, since Segwit\nactivating will likely increase the price of Bitcoin, which gives a\npositive return if the price increases enough. If it does not activate, the\nfunds will be returned so nothing is at risk.\n- Contributors could choose H heights on or slightly after an upcoming\npossible activation height. If contributors pay out to many heights, then\nthe bounty can be split among many miners, it doesn't have to be\nwinner-take-all.\n- If Segwit does not activate at H, the contributor has until the next\npossible activation height to claim their refund without risking it being\ntaken by another miner. This could be outsourced by signing a refund\ntransaction which pays a fee to some third-party who will be online at H\nand can broadcast the transaction. If the contributor wants to pay a bounty\nfor a later height, they should create a new contract otherwise a miner\ncould invalidate the payout by spending the output of the Segwit Assertion.\n\nThanks, I'd like to hear everyone's thoughts. Let me know if you find any\nglaring flaws or have any other comments.\nMatt\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170427/a76ab38d/attachment.html>"
            },
            {
                "author": "Alex Mizrahi",
                "date": "2017-04-27T18:25:15",
                "message_text_only": ">\n> 2B. If Segwit has not activated at height H, Input 1 of the Bounty Payout\n> is not valid since it spends a P2WPKH output\n>\n\nIf SegWit has not activated at height H, P2WPKH is an \"anyone can spend\"\noutput.\nSegWit is a soft fork, all SegWit transactions must be interpreted as valid\nby old nodes.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170427/c0789035/attachment.html>"
            },
            {
                "author": "Alex Mizrahi",
                "date": "2017-04-27T18:41:17",
                "message_text_only": ">\n> 2B. If Segwit has not activated at height H, Input 1 of the Bounty Payout\n> is not valid since it spends a P2WPKH output, preventing the miner from\n> including the Bounty Payout transaction in the block. (However, the output\n> of the Segwit Assertion tx can be claimed since it is treated as\n> anyone-can-spend, although this is not an issue since it is a very small\n> amount).\n>\n\nIt's a small amount by itself, but miners who are aware of Bounty Payout\nTransaction will try to include both these transactions (and both are valid\nboth on SW and non-SW chains by definition of SW being a soft fork).\n\nIf you set timelock of BPT to (H+1) then you sort of discourage this\nbehavior because a miner of block H might be not the same as miner of block\n(H+1), thus he cannot grab this bounty for sure.\n\nStill, there is a chance that same miner will mine both blocks, so\ngame-theoretically it makes sense to insert SAT into your block since your\nexpected payoff is positive.\n\nSo I'm afraid miners will just grab these bounties regardless of segwit\nactivation.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170427/7874160d/attachment.html>"
            },
            {
                "author": "Antoine Le Calvez",
                "date": "2017-04-27T19:45:54",
                "message_text_only": "On 27/04/17 18:48, Matt Bell via bitcoin-dev wrote:\n>   * No outputs, all funds are paid to the miner\n\nA transaction _must_ have at least one output [1]. You can get the same \neffect by adding a 0 satoshis OP_RETURN output.\n\n[1]: \nhttps://github.com/bitcoin/bitcoin/blob/a6548a47a5548b4b43510c548a9418673ab751de/src/validation.cpp#L465"
            },
            {
                "author": "Johnson Lau",
                "date": "2017-04-27T20:10:03",
                "message_text_only": "As other explained, your scheme is broken.\n\nUnless we have a softfork first (OP_CHECKBIP9VERIFY: payment is valid only if a BIP9 proposal is active), it is not possible to create a softfork bounty in a decentralised way\n\nOn the other hand, hardfork bounty is very simple. You just need to make sure your tx violates existing rules\n\n\n> On 28 Apr 2017, at 01:48, Matt Bell via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> Hello everyone,\n> \n> I've been thinking of an alternative to possibly get Segwit activated sooner: bribing the miners. This proposal may not be necessary if everyone is already set on doing a UASF, but  miners seem to optimize for short-term profits and this may make it easier for BitMain to accept its fate in losing the ASICBoost advantage.\n> \n> Here is a possible trustless contract protocol where contributors could pledge to a Segwit bounty which would be paid out to miners iff Segwit is activated, else the funds are returned to the contributor:\n> \n> # Setup\n> \n> - The contributor picks a possible future height where Segwit may be activated and when the funds should be released, H.\n> - The contributor chooses a bounty contribution amount X.\n> - The contributor generates 3 private keys (k1, k2, and k3) and corresponding pubkeys (K1, K2, and K3).\n> - The contributor creates and broadcasts the Funding Transaction, which has 2 outputs:\n>   * Output 0, X BTC, P2SH redeem script:\n>     <H> CHECKLOCKTIMEVERIFY DROP\n>     <K1> CHECKSIGVERIFY\n>   * Output 1, 0.00001 BTC, P2SH redeem script:\n>     <H> CHECKLOCKTIMEVERIFY DROP\n>     <K2> CHECKSIGVERIFY\n> - The contributor builds the Segwit Assertion Transaction:\n>   * nTimeLock set to H\n>   * Input 0, spends from Funding Transaction output 1, signed with k2, SIGHASH_ALL\n>   * Output 0, 0.00001 BTC, P2WPKH using K3\n> - The contributor builds the Bounty Payout Transaction:\n>   * nTimeLock set to H\n>   * Input 0, spends from Funding Transaction output 0, signed with k1, SIGHASH_ALL\n>   * Input 1, spends from Segwit Assertion Transaction output 0, signed with k3, SIGHASH_ALL\n>   * No outputs, all funds are paid to the miner\n> - The contributor publishes the Segwit Assertion Transaction and Bounty Payout Transaction (with signatures) somewhere where miners can find them\n> \n> # Process\n> \n> 1. After the setup, miners can find Funding Transactions confirmed on the chain, and verify the other 2 transactions are correct and have valid signatures. They can sum all the valid bounty contracts they find to factor into their expected mining profit.\n> 2A. Once the chain reaches height H-1, if Segwit has activated, miners can claim the bounty payout by including the Segwit Assertion and Bounty Payout transactions in their block H. Since Segwit has activated, the output from the Segwit Assertion tx can be spent by the Bounty Payout, so both transactions are valid and the miner receives the funds.\n> 2B. If Segwit has not activated at height H, Input 1 of the Bounty Payout is not valid since it spends a P2WPKH output, preventing the miner from including the Bounty Payout transaction in the block. (However, the output of the Segwit Assertion tx can be claimed since it is treated as anyone-can-spend, although this is not an issue since it is a very small amount). The contributor can reclaim the funds from Output 0 of the Funding tx by creating a new transaction, signed with k1.\n> \n> # Notes\n> \n> - This is likely a win-win scenario for the contributors, since Segwit activating will likely increase the price of Bitcoin, which gives a positive return if the price increases enough. If it does not activate, the funds will be returned so nothing is at risk.\n> - Contributors could choose H heights on or slightly after an upcoming possible activation height. If contributors pay out to many heights, then the bounty can be split among many miners, it doesn't have to be winner-take-all.\n> - If Segwit does not activate at H, the contributor has until the next possible activation height to claim their refund without risking it being taken by another miner. This could be outsourced by signing a refund transaction which pays a fee to some third-party who will be online at H and can broadcast the transaction. If the contributor wants to pay a bounty for a later height, they should create a new contract otherwise a miner could invalidate the payout by spending the output of the Segwit Assertion.\n> \n> Thanks, I'd like to hear everyone's thoughts. Let me know if you find any glaring flaws or have any other comments.\n> Matt\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2017-04-27T21:05:47",
                "message_text_only": "Good morning all,\n\nAs other explained, your scheme is broken.\n\nUnless we have a softfork first (OP_CHECKBIP9VERIFY: payment is valid only if a BIP9 proposal is active), it is not possible to create a softfork bounty in a decentralised way\n\nOn the other hand, hardfork bounty is very simple. You just need to make sure your tx violates existing rules\n\nPerhaps, it's possible to invert the logic.\n\nWhen considering a softfork success/fail, the difference is this:\n\nThere exists some tx, where if the softfork fails, the tx is valid, and if the softfork succeeds, the tx is invalid.\n\nSo, an economic agent who wishes to push for a softork, can instead do:\n\n1. Select block heights H1 and H2, where H1 < H2.\n\n2. Create a valid Funding tx (valid in both softfork-pass and softfork-fail) with a single output, encumbered by the contract (CLTV H1 AND k1) OR (CLTV H2 AND k2), and transmit and put in block.\n\n3. Create a Softfork Failure Refund tx. This tx has to be invalid if the softfork succeeds, but valid if the softfork fails. It provides k1, and is spendable on height H1. It outputs back to the economic agent.\n\n4. Create a Softfork Success Payout tx. This tx has to be valid if the softfork fails. It outputs 0 to the economic agent, allowing any miner who includes it to get the payout as tx fee.\n\nIf at block H1, softfork has passed, then the Softfork Failure Refund tx is invalid and cannot be used by the economic agent to spend the output. The miners can then wait for block H2 to include Softfork Success Payout tx in a block and claim the tx fee. The risk here for the miners is that since k2 is generated by the economic agent, the economic agent can create a different tx spending that output before H2.\n\nIf at block H1, softfork has failed, then the Softfork Failure Refund is valid and the economic agent can get the Funding tx's output back to a normal output. The risk here for the economic agent is that miners can form a cartel to informally ignore the Softfork Failure Refund until block H2.\n\nRegards,\nZmnSCPxj\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170427/1a8d67c0/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Trustless Segwit activation bounty protocol (aka. bribing the miners)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Antoine Le Calvez",
                "Johnson Lau",
                "Matt Bell",
                "ZmnSCPxj",
                "Alex Mizrahi"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 13193
        }
    },
    {
        "title": "[bitcoin-dev] Quadratic Hashing in BIP 134",
        "thread_messages": [
            {
                "author": "Russell O'Connor",
                "date": "2017-04-28T20:53:05",
                "message_text_only": "I noticed that the the latest BIP 134\n<https://github.com/bitcoin/bips/blob/959fecc15bdad070afa63455468b1dba54655fa6/bip-0134.mediawiki>\nnow supports SIGHASH_SINGLE and friends.  However, this support seems to\nreintroduce some quadratic hashing behavior because it calls\n<https://github.com/zander/bitcoinclassic/blob/9c688c6d3866890f16a36aaea453e8bdd43c1266/src/script/interpreter.cpp#L1186>SerializePartialTransactionv4\nper non-SIGHASH_ALL input\n<https://github.com/bitcoinclassic/bitcoinclassic/blob/9c688c6d3866890f16a36aaea453e8bdd43c1266/src/script/interpreter.cpp#L1186>.\nIn particular, if each input in a transaction has one SIGHASH_SINGLE\nCHECKSIG operation then the total amount of hashing done for the\ntransaction will be quadratic in the number of inputs.  While amount of\nhashing is not as severe as with the SIGHASH_ALL case, the amount of\nhashing done is still non-linear.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170428/e70f4f4f/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Quadratic Hashing in BIP 134",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Russell O'Connor"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1070
        }
    }
]