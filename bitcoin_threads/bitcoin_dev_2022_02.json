[
    {
        "title": "[bitcoin-dev] Improving RBF policy",
        "thread_messages": [
            {
                "author": "Eric Voskuil",
                "date": "2022-02-01T00:08:30",
                "message_text_only": "> On Jan 31, 2022, at 15:15, Bram Cohen via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n\n\u2026\n\n> Is it still verboten to acknowledge that RBF is normal behavior and disallowing it is the feature, and that feature is mostly there to appease some people's delusions that zeroconf is a thing? It seems a bit overdue to disrespect the RBF flag in the direction of always assuming it's on.\n\nWhat flag?\n\n>> - **Incentive Compatibility**: Ensure that our RBF policy would not\n>>   accept replacement transactions which would decrease fee profits\n>>   of a miner. In general, if our mempool policy deviates from what is\n>> economically rational, it's likely that the transactions in our\n>> mempool will not match the ones in miners' mempools, making our\n>> fee estimation, compact block relay, and other mempool-dependent\n>> functions unreliable. Incentive-incompatible policy may also\n>> encourage transaction submission through routes other than the p2p\n>> network, harming censorship-resistance and privacy of Bitcoin payments.\n> \n> There are two different common regimes which result in different incentivized behavior. One of them is that there's more than a block's backlog in the mempool in which case between two conflicting transactions the one with the higher fee rate should win. In the other case where there isn't a whole block's worth of transactions the one with higher total value should win.\n\nThese are not distinct scenarios. The rational choice is the highest fee block-valid subgraph of the set of unconfirmed transactions, in both cases (within the limits of what is computationally feasible of course).\n\nWhen collecting pooled txs the only issue is DoS protection, which is simply a question of what any given miner is willing to pay, in terms of disk space, to archive conflicts for the opportunity to optimize block reward.\n\n> It would be nice to have consolidated logic which handles both, it seems the issue has to do with the slope of the supply/demand curve which in the first case is gentle enough to keep the one transaction from hitting the rate but in the second one is basically infinite.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220131/1aafa200/attachment.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2022-02-01T00:42:24",
                "message_text_only": "> Is it still verboten to acknowledge that RBF is normal behavior and\ndisallowing it is the feature, and that feature is mostly there to appease\nsome people's delusions that zeroconf is a thing? It seems a bit overdue to\ndisrespect the RBF flag in the direction of always assuming it's on.\n\nIf you're thinking about the opt-in flag, not the RBF rules, please see\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-June/019074.html\nThe latest state of the discussion is here :\nhttps://gnusha.org/bitcoin-core-dev/2021-10-21.log\nA gradual, multi-year deprecation sounds to be preferred to ease adaptation\nof the affected Bitcoin applications.\n\nUltimately, I think it might not be the last time we have to change\nhigh-impact tx-relay/mempool rules and a more formalized Core policy\ndeprecation process would be good.\n\n\n\nLe lun. 31 janv. 2022 \u00e0 18:15, Bram Cohen via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Gloria Zhao wrote:\n>\n>>\n>> This post discusses limitations of current Bitcoin Core RBF policy and\n>> attempts to start a conversation about how we can improve it,\n>> summarizing some ideas that have been discussed. Please reply if you\n>> have any new input on issues to be solved and ideas for improvement!\n>>\n>\n> Is it still verboten to acknowledge that RBF is normal behavior and\n> disallowing it is the feature, and that feature is mostly there to appease\n> some people's delusions that zeroconf is a thing? It seems a bit overdue to\n> disrespect the RBF flag in the direction of always assuming it's on.\n>\n>\n>> - **Incentive Compatibility**: Ensure that our RBF policy would not\n>>   accept replacement transactions which would decrease fee profits\n>>   of a miner. In general, if our mempool policy deviates from what is\n>> economically rational, it's likely that the transactions in our\n>> mempool will not match the ones in miners' mempools, making our\n>> fee estimation, compact block relay, and other mempool-dependent\n>> functions unreliable. Incentive-incompatible policy may also\n>> encourage transaction submission through routes other than the p2p\n>> network, harming censorship-resistance and privacy of Bitcoin payments.\n>>\n>\n> There are two different common regimes which result in different\n> incentivized behavior. One of them is that there's more than a block's\n> backlog in the mempool in which case between two conflicting transactions\n> the one with the higher fee rate should win. In the other case where there\n> isn't a whole block's worth of transactions the one with higher total value\n> should win. It would be nice to have consolidated logic which handles both,\n> it seems the issue has to do with the slope of the supply/demand curve\n> which in the first case is gentle enough to keep the one transaction from\n> hitting the rate but in the second one is basically infinite.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220131/06519b8c/attachment-0001.html>"
            },
            {
                "author": "Bram Cohen",
                "date": "2022-02-01T08:32:09",
                "message_text_only": "On Mon, Jan 31, 2022 at 4:08 PM Eric Voskuil <eric at voskuil.org> wrote:\n\n>\n>\n> On Jan 31, 2022, at 15:15, Bram Cohen via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> Is it still verboten to acknowledge that RBF is normal behavior and\n> disallowing it is the feature, and that feature is mostly there to appease\n> some people's delusions that zeroconf is a thing? It seems a bit overdue to\n> disrespect the RBF flag in the direction of always assuming it's on.\n>\n> What flag?\n>\n\nThe opt-in RBF flag in transactions.\n\n\n> There are two different common regimes which result in different\n> incentivized behavior. One of them is that there's more than a block's\n> backlog in the mempool in which case between two conflicting transactions\n> the one with the higher fee rate should win. In the other case where there\n> isn't a whole block's worth of transactions the one with higher total value\n> should win.\n>\n> These are not distinct scenarios. The rational choice is the highest fee\n> block-valid subgraph of the set of unconfirmed transactions, in both cases\n> (within the limits of what is computationally feasible of course).\n>\n\nIt's weird because which of two or more conflicting transactions should win\ncan oscillate back and forth depending on other stuff going on in the\nmempool. There's already a bit of that with child pays but this is stranger\nand has more oddball edge cases about which transactions to route.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220201/3f148e34/attachment-0001.html>"
            },
            {
                "author": "Eric Voskuil",
                "date": "2022-02-01T19:44:37",
                "message_text_only": "> On Feb 1, 2022, at 00:32, Bram Cohen <bram at chia.net> wrote:\n> \n>> On Mon, Jan 31, 2022 at 4:08 PM Eric Voskuil <eric at voskuil.org> wrote:\n>> \n>> \n>>>> On Jan 31, 2022, at 15:15, Bram Cohen via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>> Is it still verboten to acknowledge that RBF is normal behavior and disallowing it is the feature, and that feature is mostly there to appease some people's delusions that zeroconf is a thing? It seems a bit overdue to disrespect the RBF flag in the direction of always assuming it's on.\n>> What flag?\n> \n> The opt-in RBF flag in transactions.\n\nWas being facetious. The \u201cdisrespect\u201d referred to above assumes respect that some implementations have never given.\n\n>>> There are two different common regimes which result in different incentivized behavior. One of them is that there's more than a block's backlog in the mempool in which case between two conflicting transactions the one with the higher fee rate should win. In the other case where there isn't a whole block's worth of transactions the one with higher total value should win.\n>> These are not distinct scenarios. The rational choice is the highest fee block-valid subgraph of the set of unconfirmed transactions, in both cases (within the limits of what is computationally feasible of course).\n> \n> It's weird because which of two or more conflicting transactions should win can oscillate back and forth depending on other stuff going on in the mempool.\n\nThe assumption of RAM storage is an error and unrelated to network protocol. There is nothing \u201cgoing on\u201d in a set of unconfirmed valid transactions. They are logically unchanging.\n\n> There's already a bit of that with child pays but this is stranger and has more oddball edge cases about which transactions to route.\n\nThere\u2019s really no such thing. The p2p network is necessarily permissionless. A person can route whatever he wants. Presumably people will not generally waste their own bandwidth by routing what they believe to be unconfirmable. And whatever they would retain themselves is their presumption of confirmable.\n\nThis decision of what to retain one\u2019s self is just a graph traversal to determine the most valuable subset - an optimizing CSP (though generally suboptimal due to the time constraint).\n\nShort of DoS, the most profitable model is to retain *all* valid transactions. [Note that a spend conflict is not an invalidity. Two valid transactions can be confirmed in sibling branch blocks - both valid in some context.]\n\nSo the only consideration is low cost storage fill. The fee is a proof of spend, which like proof of work (for headers/blocks), is the basis of DoS protection (for unconfirmed transactions). The issue with two conflicting subgraphs is that one or the other is ultimately unspendable. As such the fee on each is non-cumulative and therefore only one (the highest) is providing DoS protection. Any subsequent conflicting subgraph must pay not only for itself, but for all preceding conflicting subgraphs.\n\nThis pays for the storage, which is a trade accepted by the owner of the node in order to have a preview of confirmable transactions. This supports both mining generation of candidate blocks and rapid validation/confirmation of blocks.\n\nIt\u2019s a rather straightforward system when considered in terms of how it actually works (ie from a consensus standpoint). The only p2p issue is the need to package transactions for consideration as a set, as otherwise parents may be discarded before children can pay for them. Any set up to a full block is entirely reasonable for consideration.\n\ne\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220201/db23f327/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Improving RBF policy",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Bram Cohen",
                "Eric Voskuil",
                "Antoine Riard"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 10955
        }
    },
    {
        "title": "[bitcoin-dev] Improving RBF Policy",
        "thread_messages": [
            {
                "author": "Anthony Towns",
                "date": "2022-02-01T01:56:37",
                "message_text_only": "On Mon, Jan 31, 2022 at 04:57:52PM +0100, Bastien TEINTURIER via bitcoin-dev wrote:\n> I'd like to propose a different way of looking at descendants that makes\n> it easier to design the new rules. The way I understand it, limiting the\n> impact on descendant transactions is only important for DoS protection,\n> not for incentive compatibility. I would argue that after evictions,\n> descendant transactions will be submitted again (because they represent\n> transactions that people actually want to make),\n\nI think that's backwards: we're trying to discourage people from wasting\nthe network's bandwidth, which they would do by publishing transactions\nthat will never get confirmed -- if they were to eventually get confirmed\nit wouldn't be a waste of bandwith, after all. But if the original\ndescendent txs were that sort of spam, then they may well not be\nsubmitted again if the ancestor tx reaches a fee rate that's actually\nlikely to confirm.\n\nI wonder sometimes if it could be sufficient to just have a relay rate\nlimit and prioritise by ancestor feerate though. Maybe something like:\n\n - instead of adding txs to each peers setInventoryTxToSend immediately,\n   set a mempool flag \"relayed=false\"\n\n - on a time delay, add the top N (by fee rate) \"relayed=false\" txs to\n   each peer's setInventoryTxToSend and mark them as \"relayed=true\";\n   calculate how much kB those txs were, and do this again after\n   SIZE/RATELIMIT seconds\n\n - don't include \"relayed=false\" txs when building blocks?\n\n - keep high-feerate evicted txs around for a while in case they get\n   mined by someone else to improve compact block relay, a la the\n   orphan pool?\n\nThat way if the network is busy, any attempt to do low fee rate tx spam\nwill just cause those txs to sit as relayed=false until they're replaced\nor the network becomes less busy and they're worth relaying. And your\nactual mempool accept policy can just be \"is this tx a higher fee rate\nthan the txs it replaces\"...\n\n> Even if bitcoin core releases a new version with updated RBF rules, as a\n> wallet you'll need to keep using the old rules for a long time if you\n> want to be safe.\n\nAll you need is for there to be *a* path that follows the new relay rules\nand gets from your node/wallet to perhaps 10% of hashpower, which seems\nlike something wallet providers could construct relatively quickly?\n\nCheers,\naj"
            },
            {
                "author": "Prayank",
                "date": "2022-02-01T02:47:18",
                "message_text_only": "Hi Bastein,\n\n> This work will highly improve the security of any multi-party contract trying to build on top of bitcoin\nDo you think such multi party contracts are vulnerable by design considering they rely on policy that cannot be enforced?\n\n> For starters, let me quickly explain why the current rules are hard to work with in the context of lightning\nUsing the term 'rules' can be confusing sometimes because it's just a policy and different from consensus rules. I wish we could change this in the BIP with something else.\n\n> I'm actually paying a high fee twice instead of once (and needlessly using on-chain space, our scarcest asset, because we could have avoided that additional transaction\nNot sure I understand this part because if a transaction is on-chain it can't be replaced.\u00a0\n\n> The second biggest pain point is rule 3. It prevents me from efficiently using my capital while it's unconfirmed\n> I'm curious to hear other people's thoughts on that. If it makes sense, I would propose the following very simple rules\nLooks interesting however not sure about X and Y.\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220201/48931d6d/attachment-0001.html>"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2022-02-01T09:30:12",
                "message_text_only": "Hi AJ, Prayank,\n\n> I think that's backwards: we're trying to discourage people from wasting\n> the network's bandwidth, which they would do by publishing transactions\n> that will never get confirmed -- if they were to eventually get confirmed\n> it wouldn't be a waste of bandwith, after all. But if the original\n> descendent txs were that sort of spam, then they may well not be\n> submitted again if the ancestor tx reaches a fee rate that's actually\n> likely to confirm.\n\nBut do you agree that descendants only matter for DoS resistance then,\nnot for miner incentives?\n\nI'm asking this because I think a new set of policies should separate\npolicies that address the miner incentives from policies that address\nthe DoS issues.\n\nThe two policies I proposed address miner incentives. I think they're\ninsufficient to address DoS issues. But adding a 3rd policy to address\nDoS issues may be a good solution?\n\nI think that rate-limiting p2p as you suggest (and Gloria also mentioned\nit) is likely a better way of fixing the DoS concerns than a descendant\nrule like BIP 125 rule 5 (which as I mentioned earlier, is problematic\nbecause the descendent set varies from one mempool to another).\n\nI would like to add a small update to my policy suggestions. The X and Y\npercentage increase should be met for both the ancestor scores AND the\ntransaction in isolation. Otherwise I could replace txA with txA' that\nuses a new ancestor txB that has a high fee and high feerate, while txA'\nhas a low fee and low feerate. It's then possible for txB to confirm\nwithout txA', and what would remain then in the mempool would be worse\nthan before the replacement.\n\n> All you need is for there to be *a* path that follows the new relay rules\n> and gets from your node/wallet to perhaps 10% of hashpower, which seems\n> like something wallet providers could construct relatively quickly?\n\nThat's true, maybe we can be more optimistic about the timeline for\nusing an updated set of policies ;)\n\n> Do you think such multi party contracts are vulnerable by design\n> considering they rely on policy that cannot be enforced?\n\nIt's a good question. Even though these policies cannot be enforced, if\nthey are rational to apply by nodes, I think it's ok to rely on them.\nOthers may disagree with that, but I guess it's worth a separate thread.\n\n> Not sure I understand this part because if a transaction is on-chain\n> it can't be replaced.\n\nSorry, that was a bit unclear.\n\nSuppose I have txA that I want to RBF, but I only have unconfirmed utxos\nand I can't simply lower its existing outputs to reach my desired\nfeerate.\n\nI must make one of my unconfirmed utxos confirm asap just to be able to\nuse it to RBF txA. That means I'll need to pay fees a first time just to\nconvert one of my unconfirmed utxos to a confirmed one. Then I'll pay\nthe fees to bump txA. I had to overpay fees compared to just using my\nunconfirmed utxo in the first place (and manage more complexity to track\nthe confirmation of my unconfirmed utxo).\n\nThanks for your feedback!\nBastien\n\nLe mar. 1 f\u00e9vr. 2022 \u00e0 03:47, Prayank <prayank at tutanota.de> a \u00e9crit :\n\n> Hi Bastein,\n>\n> > This work will highly improve the security of any multi-party contract\n> trying to build on top of bitcoin\n>\n> Do you think such multi party contracts are vulnerable by design\n> considering they rely on policy that cannot be enforced?\n>\n> > For starters, let me quickly explain why the current rules are hard to\n> work with in the context of lightning\n>\n> Using the term 'rules' can be confusing sometimes because it's just a\n> policy and different from consensus rules. I wish we could change this in\n> the BIP with something else.\n>\n> > I'm actually paying a high fee twice instead of once (and needlessly\n> using on-chain space, our scarcest asset, because we could have avoided\n> that additional transaction\n>\n> Not sure I understand this part because if a transaction is on-chain it\n> can't be replaced.\n>\n> > The second biggest pain point is rule 3. It prevents me from efficiently\n> using my capital while it's unconfirmed\n>\n> > I'm curious to hear other people's thoughts on that. If it makes sense,\n> I would propose the following very simple rules\n>\n> Looks interesting however not sure about X and Y.\n>\n>\n> --\n> Prayank\n>\n> A3B1 E430 2298 178F\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220201/16f7e8c3/attachment.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-02-02T10:21:16",
                "message_text_only": "On Tue, Feb 01, 2022 at 10:30:12AM +0100, Bastien TEINTURIER via bitcoin-dev wrote:\n> But do you agree that descendants only matter for DoS resistance then,\n> not for miner incentives?\n\nThere's an edge case where you're replacing tx A with tx X, and X's fee\nrate is higher than A's, but you'd be obsoleting descendent txs (B, C,\nD...) and thus replacing them with unrelated txs (L, M, N...), and the\ntotal feerate/fees of A+B+C+D... is nevertheless higher than X+L+M+N...\n\nBut I think that's probably unusual (transactions D and L are adjacent\nin the mempool, that's why L is chosen for the block; but somehow\nthere's a big drop off in value somewhere between B/C/D and L/M/N),\nand at least today, I don't think miners consider it essential to eke\nout every possible sat in fee income.\n\n(If, as per your example, you're actually replacing {A,B,C,D} with\n{X,Y,Z,W} where X pays higher fees than A and the package in total pays\neither the same or higher fees, that's certainly incentive compatible.\nThe tricky question is what happens when X arrives on its own and it\nmight be that no one ever sends a replacement for B,C,D)\n\n> The two policies I proposed address miner incentives. I think they're\n> insufficient to address DoS issues. But adding a 3rd policy to address\n> DoS issues may be a good solution?\n\n>>> 1. The transaction's ancestor absolute fees must be X% higher than the\n>>> previous transaction's ancestor fees\n>>> 2. The transaction's ancestor feerate must be Y% higher than the\n>>> previous transaction's ancestor feerate\n\nAbsolute fees only matter if your backlog's feerate drops off. If you've\ngot 100MB of txs offering 5sat/vb, then exchanging 50kB at 5sat/vb for\n1kB at 6sat/vb is still a win: your block gains 1000 sats in fees even\nthough your mempool loses 245,000 sats in fees.\n\nBut if your backlog's feerate does drop off, *and* that matters, then\nI don't think you can ignore the impact of the descendent transactions\nthat you might not get a replacement for.\n\nI think \"Y% higher\" rather than just \"higher\" is only useful for\nrate-limiting, not incentive compatibility. (Though maybe it helps\nstabilise a greedy algorithm in some cases?)\n\nCheers,\naj"
            },
            {
                "author": "Michael Folkson",
                "date": "2022-02-05T13:21:57",
                "message_text_only": "Thanks for this Bastien (and Gloria for initially posting about this).\n\nI sympathetically skimmed the eclair PR (https://github.com/ACINQ/eclair/pull/2113) dealing with replaceable transactions fee bumping.\n\nThere will continue to be a (hopefully) friendly tug of war on this probably for the rest of Bitcoin's existence. I am sure people like Luke, Prayank etc will (rightfully) continue to raise that Lightning and other second layer protocols shouldn't demand that policy rules be changed if there is a reason (e.g. DoS vector) for those rules on the base network. But if there are rules that have no upside, introduce unnecessary complexity for no reason and make Lightning implementers like Bastien's life miserable attempting to deal with them I really hope we can make progress on removing or simplifying them.\n\nThis is why I think it is important to understand the rationales for introducing the rules in the first place (and why it is safe to remove them if indeed it is) and being as rigorous as possible on the rationales for introducing additional rules. It sounds like from Gloria's initial post we are still at a brainstorming phase (which is fine) but knowing what we know today I really hope we can learn from the mistakes of the original BIP 125, namely the Core implementation not matching the BIP and the sparse rationales for the rules. As Bastien says this is not criticizing the original BIP 125 authors, 7 years is a long time especially in Bitcoin world and they probably weren't thinking about Bastien sitting down to write an eclair PR in late 2021 (and reviewers of that PR) when they wrote the BIP in 2015.\n\n--\nMichael Folkson\nEmail: michaelfolkson at [protonmail.com](http://protonmail.com/)\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n\n------- Original Message -------\nOn Monday, January 31st, 2022 at 3:57 PM, Bastien TEINTURIER via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi Gloria,\n>\n> Many thanks for raising awareness on these issues and constantly pushing\n> towards finding a better model. This work will highly improve the\n> security of any multi-party contract trying to build on top of bitcoin\n> (because most multi-party contracts will need to have timeout conditions\n> and participants will need to make some transactions confirm before a\n> timeout happens - otherwise they may lose funds).\n>\n> For starters, let me quickly explain why the current rules are hard to\n> work with in the context of lightning (but I believe most L2 protocols\n> will have the same issues). Feel free to skip this part if you are\n> already convinced.\n>\n> ## Motivation\n>\n> The biggest pain point is BIP 125 rule 2.\n> If I need to increase the fees of a time-sensitive transaction because\n> the feerate has been rising since I broadcast it, I may need to also pay\n> high fees just to produce a confirmed utxo that I can use. I'm actually\n> paying a high fee twice instead of once (and needlessly using on-chain\n> space, our scarcest asset, because we could have avoided that additional\n> transaction!).\n>\n> It also has some annoying \"non-determinism\".\n> Imagine that my transaction has been evicted from my mempool because its\n> feerate was too low. I could think \"Great, that means I don't have to\n> apply BIP 125 restrictions, I can just fund this transaction as if it\n> were a new one!\". But actually I do, because my transaction could still\n> be in miner's mempools and I have no way of knowing it...this means that\n> whenever I have broadcast a transaction, I must assume that I will\n> always need to abide by whatever replacement rules the network applies.\n>\n> Fortunately, as far as I understand it, this rule only exists because of\n> a previous implementation detail of bitcoin core, so there's simply no\n> good reason to keep it.\n>\n> The second biggest pain point is rule 3. It prevents me from efficiently\n> using my capital while it's unconfirmed. Whenever I'm using a big utxo\n> to fund a transaction, I will get a big change output, and it would\n> really be a waste to be unable to use that change output to fund other\n> transactions. In order to be capital-efficient, I will end up creating\n> descendant trees for my time-sensitive transactions. But as Gloria\n> explained, replacing all my children will cost me an absurdly large\n> amount of fees. So what I'm actually planning to do instead is to RBF\n> one of the descendants high enough to get the whole tree confirmed.\n> But if those descendants' timeouts were far in the future, that's a\n> waste, I paid a lot more fees for them than I should have. I'd like to\n> just replace my transaction and republish the invalidated children\n> independently.\n>\n> Rule 4 doesn't hurt as much as the two previous ones, I don't have too\n> much to say about it.\n>\n> To be fair to the BIP 125 authors, all of these scenarios were very hard\n> to forecast at the time this BIP was created. We needed years to build\n> on those rules to get a better understanding of their limitations and if\n> the rationale behind them made sense in the long term.\n>\n> ## Proposals\n>\n> I believe that now is a good time to re-think those, and I really like\n> Gloria's categorization of the design constraints.\n>\n> I'd like to propose a different way of looking at descendants that makes\n> it easier to design the new rules. The way I understand it, limiting the\n> impact on descendant transactions is only important for DoS protection,\n> not for incentive compatibility. I would argue that after evictions,\n> descendant transactions will be submitted again (because they represent\n> transactions that people actually want to make), so evicting them does\n> not have a negative impact on mining incentives (in a world where blocks\n> are full most of the time).\n>\n> I'm curious to hear other people's thoughts on that. If it makes sense,\n> I would propose the following very simple rules:\n>\n> 1. The transaction's ancestor absolute fees must be X% higher than the\n> previous transaction's ancestor fees\n> 2. The transaction's ancestor feerate must be Y% higher than the\n> previous transaction's ancestor feerate\n>\n> I believe it's completely ok to require increasing both the fees and\n> feerate if we don't take descendants into account, because you control\n> your ancestor set - whereas the descendant set may be completely out of\n> your control.\n>\n> This is very easy to use by wallets, because the ancestor set is easy to\n> obtain. And an important point is that the ancestor set is the same in\n> every mempool, whereas the descendant set is not (your mempool may have\n> rejected the last descendants, while other people's mempools may still\n> contain them).\n>\n> Because of that reason, I'd like to avoid having a rule that relies on\n> some size of the replaced descendant set: it may be valid in your\n> mempool but invalid in someone else's, which makes it exploitable for\n> pinning attacks.\n>\n> I believe these rules are incentive compatible (again, if you accept\n> the fact that the descendants will be re-submitted and mined as well,\n> so their fees aren't lost).\n>\n> Can we choose X and Y so that these two rules are also DoS-resistant?\n> Unfortunately I'm not sure, so maybe we'll need to add a third rule to\n> address that. But before we do, can someone detail what it costs for a\n> node to evict a descendant tree? Given that bitcoin core doesn't allow\n> chains of more than 25 transactions, the maximum number of transactions\n> being replaced will be bounded by 25 * N (where N is the number of\n> outputs of the transaction being replaced). If it's just O(n) pruning of\n> a graph, maybe that's ok? Or maybe we make X or Y depend on the number\n> of outputs of the transaction being replaced (this would need very\n> careful thoughts)?\n>\n> If you made it this far, thanks for reading!\n> A couple of comments on the previous messages:\n>\n>> Currently, if we see a transaction\n>> that has the same txid as one in the mempool, we reject it as a\n>> duplicate, even if the feerate is much higher. It's unclear to me if\n>> we have a very strong reason to change this, but noting it as a\n>> limitation of our current replacement policy.\n>\n> I don't see a strong reason from an L2 protocol's point of view yet, but\n> there are many unkown unknowns. But from a miner incentive's point of\n> view, we should keep the transaction with the higher feerate, shouldn't\n> we? In that case it's also a more efficient use of on-chain space, which\n> is a win, right?\n>\n>> We might have a more-or-less long transition period during which we support both...\n>\n> Yes, this is a long term thing.\n> Even if bitcoin core releases a new version with updated RBF rules, as a\n> wallet you'll need to keep using the old rules for a long time if you\n> want to be safe.\n>\n> But it's all the more reason to try to ship this as soon as possible,\n> this way maybe our grand-children will be able to benefit from it ;)\n> (just kidding on the timespan obviously).\n>\n> Cheers,\n> Bastien\n>\n> Le lun. 31 janv. 2022 \u00e0 00:11, Antoine Riard via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>\n>> Hi Gloria,\n>>\n>> Thanks for this RBF sum up. Few thoughts and more context comments if it can help other readers.\n>>\n>>> For starters, the absolute fee pinning attack is especially\n>>> problematic if we apply the same rules (i.e. Rule #3 and #4) in\n>>> Package RBF. Imagine that Alice (honest) and Bob (adversary) share a\n>>> LN channel. The mempool is rather full, so their pre-negotiated\n>>> commitment transactions' feerates would not be considered high\n>>> priority by miners. Bob broadcasts his commitment transaction and\n>>> attaches a very large child (100KvB with 100,000sat in fees) to his\n>>> anchor output. Alice broadcasts her commitment transaction with a\n>>> fee-bumping child (200vB with 50,000sat fees which is a generous\n>>> 250sat/vB), but this does not meet the absolute fee requirement. She\n>>> would need to add another 50,000sat to replace Bob's commitment\n>>> transaction.\n>>\n>> Solving LN pinning attacks, what we're aiming for is enabling a fair feerate bid between the counterparties, thus either forcing the adversary to overbid or to disengage from the confirmation competition. If the replace-by-feerate rule is adopted, there shouldn't be an incentive for Bob to\n>> pick up the first option. Though if he does, that's a winning outcome for Alice, as one of the commitment transactions confirms and her time-sensitive second-stage HTLC can be subsequently confirmed.\n>>\n>>> It's unclear to me if\n>>> we have a very strong reason to change this, but noting it as a\n>>> limitation of our current replacement policy. See [#24007][12].\n>>\n>> Deployment of Taproot opens interesting possibilities in the vaults/payment channels design space, where the tapscripts can commit to different set of timelocks/quorum of keys. Even if the pre-signed states stay symmetric, whoever is the publisher, the feerate cost to spend can fluctuate.\n>>\n>>> While this isn't completely broken, and the user interface is\n>>> secondary to the safety of the mempool policy\n>>\n>> I think with L2s transaction broadcast backend, the stability and clarity of the RBF user interface is primary. What we could be worried about is a too-much complex interface easing the way for an attacker to trigger your L2 node to issue policy-invalid chain of transactions. Especially, when we consider that an attacker might have leverage on chain of transactions composition (\"force broadcast of commitment A then commitment B, knowing they will share a CPFP\") or even transactions size (\"overload commitment A with HTLCs\").\n>>\n>>> * If the original transaction is in the top {0.75MvB, 1MvB} of the\n>>> mempool, apply the current rules (absolute fees must increase and\n>>> pay for the replacement transaction's new bandwidth). Otherwise, use a\n>>> feerate-only rule.\n>>\n>> How this new replacement rule would behave if you have a parent in the \"replace-by-feerate\" half but the child is in the \"replace-by-fee\" one ?\n>>\n>> If we allow the replacement of the parent based on the feerate, we might decrease the top block absolute fees.\n>>\n>> If we block the replacement of the parent based on the feerate because the replacement absolute fees aren't above the replaced package, we still preclude a pinning vector. The child might be low-feerate junk and even attached to a low ancestor-score branch.\n>>\n>> If I'm correct on this limitation, maybe we could turn off the \"replace-by-fee\" behavior as soon as the mempool is fulfilled with a few blocks ?\n>>\n>>> * Rate-limit how many replacements we allow per prevout.\n>>\n>> Depending on how it is implemented, though I would be concerned it introduces a new pinning vector in the context of shared-utxo. If it's a hardcoded constant, it could be exhausted by an adversary starting at the lowest acceptable feerate then slowly increasing while still not reaching\n>> the top of the mempool. Same if it's time-based or block-based, no guarantee the replacement slot is honestly used by your counterparty.\n>>\n>> Further, an above-the-average replacement frequency might just be the reflection of your confirmation strategy reacting to block schedule or mempools historical data. As long as the feerate penalty is paid, I lean to allow replacement.\n>> (One solution could be to associate per-user \"tag\" to the LN transactions, where each \"tag\" would have its own replacement slots, but privacy?)\n>>\n>>> * Rate-limit transaction validation in general, per peer.\n>>\n>> I think we could improve on the Core's new transaction requester logic. Maybe we could bind the peer announced flow based on the feerate score (modulo validation time) of the previously validated transactions from that peer ? That said, while related to RBF, it sounds to me that enhancing Core's rate-limiting transaction strategy is a whole discussion in itself [0]. Especially ensuring it's tolerant to the specific requirements of LN & consorts.\n>>\n>>> What should they be? We can do some arithmetic to see what happens if\n>>> you start with the biggest/lowest feerate transaction and do a bunch\n>>> of replacements. Maybe we end up with values that are high enough to\n>>> prevent abuse and make sense for applications/users that do RBF.\n>>\n>> That's a good question.\n>>\n>> One observation is that the attacker can always renew the set of DoSy utxos to pursue the attack. So maybe we could pick up constants scaled on the block size ? That way an attacker would have to burn fees, thus deterring them from launching an attack. Even if the attackers are miners, they have to renounce their income to acquire new DoSy utxos. If a low-fee period, we could scale up the constants ?\n>>\n>> Overall, I think there is the deployment issue to warn of. Moving to a new set of RBF rules implies for a lot of Bitcoin applications to rewrite their RBF logics. We might have a more-or-less long transition period during which we support both...\n>>\n>> Cheers,\n>> Antoine\n>>\n>> [0] https://github.com/bitcoin/bitcoin/pull/21224\n>>\n>> Le jeu. 27 janv. 2022 \u00e0 09:10, Gloria Zhao via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>\n>>> Hi everyone,\n>>>\n>>> This post discusses limitations of current Bitcoin Core RBF policy and\n>>> attempts to start a conversation about how we can improve it,\n>>> summarizing some ideas that have been discussed. Please reply if you\n>>> have any new input on issues to be solved and ideas for improvement!\n>>>\n>>> Just in case I've screwed up the text wrapping again, another copy can be\n>>> found here: https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff\n>>>\n>>> ## Background\n>>>\n>>> Please feel free to skip this section if you are already familiar\n>>> with RBF.\n>>>\n>>> Nodes may receive *conflicting* unconfirmed transactions, aka\n>>> \"double spends\" of the same inputs. Instead of always keeping the\n>>> first transaction, since v0.12, Bitcoin Core mempool policy has\n>>> included a set of Replace-by-Fee (RBF) criteria that allows the second\n>>> transaction to replace the first one and any descendants it may have.\n>>>\n>>> Bitcoin Core RBF policy was previously documented as BIP 125.\n>>> The current RBF policy is documented [here][1]. In summary:\n>>>\n>>> 1. The directly conflicting transactions all signal replaceability\n>>> explicitly.\n>>>\n>>> 2. The replacement transaction only includes an unconfirmed input if\n>>> that input was included in one of the directly conflicting\n>>> transactions.\n>>>\n>>> 3. The replacement transaction pays an absolute fee of at least the\n>>> sum paid by the original transactions.\n>>>\n>>> 4. The additional fees pays for the replacement transaction's\n>>> bandwidth at or above the rate set by the node's *incremental relay\n>>> feerate*.\n>>>\n>>> 5. The sum of all directly conflicting transactions' descendant counts\n>>> (number of transactions inclusive of itself and its descendants)\n>>> does not exceed 100.\n>>>\n>>> We can split these rules into 3 categories/goals:\n>>>\n>>> - **Allow Opting Out**: Some applications/businesses are unable to\n>>> handle transactions that are replaceable (e.g. merchants that use\n>>> zero-confirmation transactions). We (try to) help these businesses by\n>>> honoring BIP125 signaling; we won't replace transactions that have not\n>>> opted in.\n>>>\n>>> - **Incentive Compatibility**: Ensure that our RBF policy would not\n>>> accept replacement transactions which would decrease fee profits\n>>> of a miner. In general, if our mempool policy deviates from what is\n>>> economically rational, it's likely that the transactions in our\n>>> mempool will not match the ones in miners' mempools, making our\n>>> fee estimation, compact block relay, and other mempool-dependent\n>>> functions unreliable. Incentive-incompatible policy may also\n>>> encourage transaction submission through routes other than the p2p\n>>> network, harming censorship-resistance and privacy of Bitcoin payments.\n>>>\n>>> - **DoS Protection**: Limit two types of DoS attacks on the node's\n>>> mempool: (1) the number of times a transaction can be replaced and\n>>> (2) the volume of transactions that can be evicted during a\n>>> replacement.\n>>>\n>>> Even more abstract: our goal is to make a replacement policy that\n>>> results in a useful interface for users and safe policy for\n>>> node operators.\n>>>\n>>> ## Motivation\n>>>\n>>> There are a number of known problems with the current RBF policy.\n>>> Many of these shortcomings exist due to mempool limitations at the\n>>> time RBF was implemented or result from new types of Bitcoin usage;\n>>> they are not criticisms of the original design.\n>>>\n>>> ### Pinning Attacks\n>>>\n>>> The most pressing concern is that attackers may take advantage of\n>>> limitations in RBF policy to prevent other users' transactions from\n>>> being mined or getting accepted as a replacement.\n>>>\n>>> #### SIGHASH_ANYONECANPAY Pinning\n>>>\n>>> BIP125#2 can be bypassed by creating intermediary transactions to be\n>>> replaced together. Anyone can simply split a 1-input 1-output\n>>> transaction off from the replacement transaction, then broadcast the\n>>> transaction as is. This can always be done, and quite cheaply. More\n>>> details in [this comment][2].\n>>>\n>>> In general, if a transaction is signed with SIGHASH\\_ANYONECANPAY,\n>>> anybody can just attach a low feerate parent to this transaction and\n>>> lower its ancestor feerate. Even if you require SIGHASH\\_ALL which\n>>> prevents an attacker from changing any outputs, the input can be a\n>>> very low amount (e.g. just above the dust limit) from a low-fee\n>>> ancestor and still bring down the ancestor feerate of the transaction.\n>>>\n>>> TLDR: if your transaction is signed with SIGHASH\\_ANYONECANPAY and\n>>> signals replaceability, regardless of the feerate you broadcast at, an\n>>> attacker can lower its mining priority by adding an ancestor.\n>>>\n>>> #### Absolute Fee\n>>>\n>>> The restriction of requiring replacement transactions to increase the\n>>> absolute fee of the mempool has been described as \"bonkers.\" If the\n>>> original transaction has a very large descendant that pays a large\n>>> amount of fees, even if it has a low feerate, the replacement\n>>> transaction must now pay those fees in order to meet Rule #3.\n>>>\n>>> #### Package RBF\n>>>\n>>> There are a number of reasons why, in order to enable Package RBF, we\n>>> cannot use the same criteria.\n>>>\n>>> For starters, the absolute fee pinning attack is especially\n>>> problematic if we apply the same rules (i.e. Rule #3 and #4) in\n>>> Package RBF. Imagine that Alice (honest) and Bob (adversary) share a\n>>> LN channel. The mempool is rather full, so their pre-negotiated\n>>> commitment transactions' feerates would not be considered high\n>>> priority by miners. Bob broadcasts his commitment transaction and\n>>> attaches a very large child (100KvB with 100,000sat in fees) to his\n>>> anchor output. Alice broadcasts her commitment transaction with a\n>>> fee-bumping child (200vB with 50,000sat fees which is a generous\n>>> 250sat/vB), but this does not meet the absolute fee requirement. She\n>>> would need to add another 50,000sat to replace Bob's commitment\n>>> transaction.\n>>>\n>>> Disallowing new unconfirmed inputs (Rule #2) in Package RBF would be\n>>> broken for packages containing transactions already in the mempool,\n>>> explained [here][7].\n>>>\n>>> Note: I originally [proposed][6] Package RBF using the same Rule #3\n>>> and #4 before I realized how significant this pinning attack is. I'm\n>>> retracting that proposal, and a new set of Package RBF rules would\n>>> follow from whatever the new individual RBF rules end up being.\n>>>\n>>> #### Same Txid Different Witness\n>>>\n>>> Two transactions with the same non-witness data but different\n>>> witnesses have the same txid but different wtxid, and the same fee but\n>>> not necessarily the same feerate. Currently, if we see a transaction\n>>> that has the same txid as one in the mempool, we reject it as a\n>>> duplicate, even if the feerate is much higher. It's unclear to me if\n>>> we have a very strong reason to change this, but noting it as a\n>>> limitation of our current replacement policy. See [#24007][12].\n>>>\n>>> ### User Interface\n>>>\n>>> #### Using Unconfirmed UTXOs to Fund Replacements\n>>>\n>>> The restriction of only allowing confirmed UTXOs for funding a\n>>> fee-bump (Rule #2) can hurt users trying to fee-bump their\n>>> transactions and complicate wallet implementations. If the original\n>>> transaction's output value isn't sufficient to fund a fee-bump and/or\n>>> all of the user's other UTXOs are unconfirmed, they might not be able\n>>> to fund a replacement transaction. Wallet developers also need to\n>>> treat self-owned unconfirmed UTXOs as unusable for fee-bumping, which\n>>> adds complexity to wallet logic. For example, see BDK issues [#144][4]\n>>> and [#414][5].\n>>>\n>>> #### Interface Not Suitable for Coin Selection\n>>>\n>>> Currently, a user cannot simply create a replacement transaction\n>>> targeting a specific feerate or meeting a minimum fee amount and\n>>> expect to meet the RBF criteria. The fee amount depends on the size of\n>>> the replacement transaction, and feerate is almost irrelevant.\n>>>\n>>> Bitcoin Core's `bumpfee` doesn't use the RBF rules when funding the\n>>> replacement. It [estimates][13] a feerate which is \"wallet incremental\n>>> relay fee\" (a conservative overestimation of the node's incremental\n>>> relay fee) higher than the original transaction, selects coins for\n>>> that feerate, and hopes that it meets the RBF rules. It never fails\n>>> Rule #3 and #4 because it uses all original inputs and refuses to\n>>> bump a transaction with mempool descendants.\n>>>\n>>> This is suboptimal, but is designed to work with the coin selection\n>>> engine: select a feerate first, and then add fees to cover it.\n>>> Following the exact RBF rules would require working the other way\n>>> around: based on how much fees we've added to the transaction and its\n>>> current size, calculate the feerate to see if we meet Rule #4.\n>>>\n>>> While this isn't completely broken, and the user interface is\n>>> secondary to the safety of the mempool policy, we can do much better.\n>>> A much more user-friendly interface would depend *only* on the\n>>> fee and size of the original transactions.\n>>>\n>>> ### Updates to Mempool and Mining\n>>>\n>>> Since RBF was first implemented, a number of improvements have been\n>>> made to mempool and mining logic. For example, we now use ancestor\n>>> feerates in mining (allowing CPFP), and keep track of ancestor\n>>> packages in the mempool.\n>>>\n>>> ## Ideas for Improvements\n>>>\n>>> ### Goals\n>>>\n>>> To summarize, these seem to be desired changes, in order of priority:\n>>>\n>>> 1. Remove Rule #3. The replacement should not be *required* to pay\n>>> higher absolute fees.\n>>>\n>>> 2. Make it impossible for a replacement transaction to have a lower\n>>> mining score than the original transaction(s). This would eliminate\n>>> the `SIGHASH\\_ANYONECANPAY` pinning attack.\n>>>\n>>> 3. Remove Rule #2. Adding new unconfirmed inputs should be allowed.\n>>>\n>>> 4. Create a more helpful interface that helps wallet fund replacement\n>>> transactions that aim for a feerate and fee.\n>>>\n>>> ### A Different Model for Fees\n>>>\n>>> For incentive compatibility, I believe there are different\n>>> formulations we should consider. Most importantly, if we want to get\n>>> rid of the absolute fee rule, we can no longer think of it as \"the\n>>> transaction needs to pay for its own bandwidth,\" since we won't always\n>>> be getting additional fees. That means we need a new method of\n>>> rate-limiting replacements that doesn't require additional fees every\n>>> time.\n>>>\n>>> While it makes sense to think about monetary costs when launching a\n>>> specific type of attack, given that the fees are paid to the miner and\n>>> not to the mempool operators, maybe it doesn't make much sense to\n>>> think about \"paying for bandwidth\". Maybe we should implement\n>>> transaction validation rate-limiting differently, e.g. building it\n>>> into the P2P layer instead of the mempool policy layer.\n>>>\n>>> Recently, Suhas gave a [formulation][8] for incentive compatibility\n>>> that made sense to me: \"are the fees expected to be paid in the next\n>>> (N?) blocks higher or lower if we process this transaction?\"\n>>>\n>>> I started by thinking about this where N=1 or `1 + p`.\n>>> Here, a rational miner is looking at what fees they would\n>>> collect in the next block, and then some proportion `p` of the rest of\n>>> the blocks based on their hashrate. We're assuming `p` isn't *so high*\n>>> that they would be okay with lower absolute fees in the next 1 block.\n>>> We're also assuming `p` isn't *so low* that the miner doesn't care\n>>> about what's left of the mempool after this block.\n>>>\n>>> A tweak to this formulation is \"if we process this transaction, would\n>>> the fees in the next 1 block higher or lower, and is the feerate\n>>> density of the rest of the mempool higher or lower?\" This is pretty\n>>> similar, where N=1, but we consider the rest of the mempool by feerate\n>>> rather than fees.\n>>>\n>>> ### Mining Score of a Mempool Transaction\n>>>\n>>> We are often interested in finding out what\n>>> the \"mining score\" of a transaction in the mempool is. That is, when\n>>> the transaction is considered in block template building, what is the\n>>> feerate it is considered at?\n>>>\n>>> Obviously, it's not the transaction's individual feerate. Bitcoin Core\n>>> [mining code sorts][14] transactions by their ancestor feerate and\n>>> includes them packages at a time, keeping track of how this affects the\n>>> package feerates of remaining transactions in the mempool.\n>>>\n>>> *ancestor feerate*: Ancestor feerate is easily accessible information,\n>>> but it's not accurate either, because it doesn't take into account the\n>>> fact that subsets of a transaction's ancestor set can be included\n>>> without it. For example, ancestors may have high feerates on their own\n>>> or we may have [high feerate siblings][8].\n>>>\n>>> TLDR: *Looking at the current ancestor feerate of a transaction is\n>>> insufficient to tell us what feerate it will be considered at when\n>>> building a block template in the future.*\n>>>\n>>> *min(individual feerate, ancestor feerate)*: Another\n>>> heuristic that is simple to calculate based on current mempool tooling\n>>> is to use the [minimum of a transaction's individual score and its\n>>> ancestor score][10] as a conservative measure. But this can\n>>> overestimate as well (see the example below).\n>>>\n>>> *min ancestor feerate(tx + possible ancestor subsets)* We can also\n>>> take the minimum of every possible ancestor subset, but this can be\n>>> computationally expensive since there can be lots and lots of ancestor\n>>> subsets.\n>>>\n>>> *max ancestor feerate(tx + possible descendant subsets)*: Another idea\n>>> is to use the [maximum ancestor score of the transaction + each of its\n>>> descendants][9]. This doesn't work either; it has the same blindspot\n>>> of ancestor subsets being mined on their own.\n>>>\n>>> #### Mining Score Example\n>>>\n>>> Here's an example illustrating why mining score is tricky to\n>>> efficiently calculate for mempool transactions:\n>>>\n>>> Let's say you have same-size transactions A (21sat/vB), B (1sat/vB),\n>>> C(9sat/vB), D(5sat/vB).\n>>> The layout is: grandparent A, parent B, and two children C and D.\n>>>\n>>> ```\n>>> A\n>>> ^\n>>> B\n>>> ^ ^\n>>> C D\n>>> ```\n>>>\n>>> A miner using ancestor packages to build block templates will first\n>>> include A with a mining score of 21. Next, the miner will include B and\n>>> C with a mining score of 6. This leaves D, with a mining score of 5.\n>>>\n>>> Note: in this case, mining by ancestor feerate results in the most\n>>> rational decisions, but [a candidate set-based approach][10] which\n>>> makes ancestor feerate much less relevant could\n>>> be more advantageous in other situations.\n>>>\n>>> Here is a chart showing the \"true\" mining score alongside the values\n>>> calculating using imperfect heuristics described above. All of them\n>>> can overestimate or underestimate.\n>>>\n>>> ```\n>>> A B C D\n>>> mining score | 21 | 6 | 6 | 5 |\n>>> ancestor feerate | 21 | 11 | 10.3 | 9 |\n>>> min(individual, ancestor) | 21 | 1 | 9 | 5 |\n>>> min(tx + ancestor subsets) | 21 | 1 | 5 | 3 |\n>>> max(tx + descendants subsets) | 21 | 9 | 9 | 5 |\n>>>\n>>> ```\n>>>\n>>> Possibly the best solution for finding the \"mining score\" of a\n>>> transaction is to build a block template, see what feerate each\n>>> package is included at. Perhaps at some cutoff, remaining mempool\n>>> transactions can be estimated using some heuristic that leans\n>>> {overestimating, underestimating} depending on the situation.\n>>>\n>>> Mining score seems to be relevant in multiple places: Murch and I\n>>> recently [found][3] that it would be very important in\n>>> \"ancestor-aware\" funding of transactions (the wallet doesn't\n>>> incorporate ancestor fees when using unconfirmed transactions in coin\n>>> selection, which is a bug we want to fix).\n>>>\n>>> In general, it would be nice to know the exact mining priority of\n>>> one's unconfirmed transaction is. I can think of a few block/mempool\n>>> explorers who might want to display this information for users.\n>>>\n>>> ### RBF Improvement Proposals\n>>>\n>>> After speaking to quite a few people, here are some suggestions\n>>> for improvements that I have heard:\n>>>\n>>> * The ancestor score of the replacement must be {5, 10, N}% higher\n>>> than that of every original transaction.\n>>>\n>>> * The ancestor score of the replacement must be 1sat/vB higher than\n>>> that of every original transaction.\n>>>\n>>> * If the original transaction is in the top {0.75MvB, 1MvB} of the\n>>> mempool, apply the current rules (absolute fees must increase and\n>>> pay for the replacement transaction's new bandwidth). Otherwise, use a\n>>> feerate-only rule.\n>>>\n>>> * If fees don't increase, the size of the replacement transaction must\n>>> decrease by at least N%.\n>>>\n>>> * Rate-limit how many replacements we allow per prevout.\n>>>\n>>> * Rate-limit transaction validation in general, per peer.\n>>>\n>>> Perhaps some others on the mailing list can chime in to throw other\n>>> ideas into the ring and/or combine some of these rules into a sensible\n>>> policy.\n>>>\n>>> #### Replace by Feerate Only\n>>>\n>>> I don't think there's going to be a single-line feerate-based\n>>> rule that can incorporate everything we need.\n>>> On one hand, a feerate-only approach helps eliminate the issues\n>>> associated with Rule #3. On the other hand, I believe the main concern\n>>> with a feerate-only approach is how to rate limit replacements. We\n>>> don't want to enable an attack such as:\n>>>\n>>> 1. Attacker broadcasts large, low-feerate transaction, and attaches a\n>>> chain of descendants.\n>>>\n>>> 2. The attacker replaces the transaction with a smaller but higher\n>>> feerate transaction, attaching a new chain of descendants.\n>>>\n>>> 3. Repeat 1000 times.\n>>>\n>>> #### Fees in Next Block and Feerate for the Rest of the Mempool\n>>>\n>>> Perhaps we can look at replacements like this:\n>>>\n>>> 1. Calculate the directly conflicting transactions and, with their\n>>> descendants, the original transactions. Check signaling. Limit the\n>>> total volume (e.g. can't be more than 100 total or 1MvB or something).\n>>>\n>>> 2. Find which original transactions would be in the next ~1 block. The\n>>> replacement must pay at least this amount + X% in absolute fees. This\n>>> guarantees that the fees of the next block doesn't decrease.\n>>>\n>>> 3. Find which transactions would be left in the mempool after that ~1\n>>> block. The replacement's feerate must be Y% higher than the maximum\n>>> mining score of these transactions. This guarantees that you now have\n>>> only *better* candidates in your after-this-block mempool than you did\n>>> before, even if the size and fees the transactions decrease.\n>>>\n>>> 4. Now you have two numbers: a minimum absolute fee amount and a\n>>> minimum feerate. Check to see if the replacement(s) meet these\n>>> minimums. Also, a wallet would be able to ask the node \"What fee and\n>>> feerate would I need to put on a transaction replacing this?\" and use\n>>> this information to fund a replacement transaction, without needing to\n>>> guess or overshoot.\n>>>\n>>> Obviously, there are some magic numbers missing here. X and Y are\n>>> TBD constants to ensure we have some kind of rate limiting for the\n>>> number of replacements allowed using some set of fees.\n>>>\n>>> What should they be? We can do some arithmetic to see what happens if\n>>> you start with the biggest/lowest feerate transaction and do a bunch\n>>> of replacements. Maybe we end up with values that are high enough to\n>>> prevent abuse and make sense for applications/users that do RBF.\n>>>\n>>> ### Mempool Changes Need for Implementation\n>>>\n>>> As described in the mining score section above,\n>>> we may want additional tooling to more accurately assess\n>>> the economic gain of replacing transactions in our mempool.\n>>>\n>>> A few options have been discussed:\n>>>\n>>> * Calculate block templates on the fly when we need to consider a\n>>> replacement. However, since replacements are [quite common][11]\n>>> and the information might be useful for other things as well,\n>>> it may be worth it to cache a block template.\n>>>\n>>> * Keep a persistent block template so that we know what transactions\n>>> we would put in the next block. We need to remember the feerate\n>>> at which each transaction was included in the template, because an\n>>> ancestor package may be included in the same block template in\n>>> multiple subsets. Transactions included earlier alter the ancestor\n>>> feerate of the remaining transactions in the package. We also need\n>>> to keep track of the new feerates of transactions left over.\n>>>\n>>> * Divide the mempool into two layers, \"high feerate\" and \"low\n>>> feerate.\" The high feerate layer contains ~1 block of packages with\n>>> the highest ancestor feerates, and the low feerate layer contains\n>>> everything else. At the edge of a block, we have a Knapsacky problem\n>>> where the next highest ancestor feerate package might not fit, so we\n>>> would probably want the high feerate layer ~2MvB or something to avoid\n>>> underestimating the fees.\n>>>\n>>> ## Acknowledgements\n>>>\n>>> Thank you to everyone whose RBF-related suggestions, grievances,\n>>> criticisms and ideas were incorporated in this document:\n>>> Andrew Chow, Matt Corallo, Suhas Daftuar, Christian Decker,\n>>> Mark Erhardt, Lloyd Fournier, Lisa Neigut, John Newbery,\n>>> Antoine Poinsot, Antoine Riard, Larry Ruane,\n>>>\n>>> S3RK and Bastien Teinturier.\n>>>\n>>> Thanks for reading!\n>>>\n>>> Best,\n>>> Gloria\n>>> [1]: https://github.com/bitcoin/bitcoin/blob/master/doc/policy/mempool-replacements.md\n>>> [2]: https://github.com/bitcoin/bitcoin/pull/23121#issuecomment-929475999\n>>> [3]: https://github.com/Xekyo/bitcoin/commit/d754b0242ec69d42c570418aebf9c1335af0b8ea\n>>> [4]: https://github.com/bitcoindevkit/bdk/issues/144\n>>> [5]: https://github.com/bitcoindevkit/bdk/issues/414\n>>> [6]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019464.html\n>>> [7]: https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a#new-unconfirmed-inputs-rule-2\n>>> [8]: https://github.com/bitcoin/bitcoin/pull/23121#discussion_r777131366\n>>> [9]: https://github.com/bitcoin/bitcoin/pull/22290#issuecomment-865887922\n>>> [10]: https://gist.github.com/Xekyo/5cb413fe9f26dbce57abfd344ebbfaf2#file-candidate-set-based-block-building-md\n>>> [11]: https://github.com/bitcoin/bitcoin/pull/22539#issuecomment-885763670\n>>> [12]: https://github.com/bitcoin/bitcoin/pull/24007\n>>> [13]: https://github.com/bitcoin/bitcoin/blob/1a369f006fd0bec373b95001ed84b480e852f191/src/wallet/feebumper.cpp#L114\n>>>\n>>> [14]: https://github.com/bitcoin/bitcoin/blob/cf5bb048e80d4cde8828787b266b7f5f2e3b6d7b/src/node/miner.cpp#L310-L320\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220205/3641f38d/attachment-0001.html>"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2022-02-07T10:22:01",
                "message_text_only": "Good morning,\n\n> The tricky question is what happens when X arrives on its own and it\n> might be that no one ever sends a replacement for B,C,D)\n\nIt feels ok to me, but this is definitely arguable.\n\nIt covers the fact that B,C,D could have been fake transactions whose\nsole purpose was to do a pinning attack: in that case the attacker would\nhave found a way to ensure these transactions don't confirm anyway (or\npay minimal/negligible fees).\n\nIf these transactions were legitimate, I believe that their owners would\nremake them at some point (because these transactions reflect a business\nrelationship that needed to happen, so it should very likely still\nhappen). It's probably hard to verify because the new corresponding\ntransactions may have nothing in common with the first, but I think the\nsimplifications it offers for wallets is worth it (which is just my\nopinion and needs more scrutiny/feedback).\n\n> But if your backlog's feerate does drop off, *and* that matters, then\n> I don't think you can ignore the impact of the descendent transactions\n> that you might not get a replacement for.\n\nThat is because you're only taking into account the current backlog, and\nnot taking into account the fact that new items will be added to it soon\nto replace the evicted descendants. But I agree that this is a bet: we\ncan't predict the future and guarantee these replacements will come.\n\nIt is really a trade-off, ignoring descendents provides a much simpler\ncontract that doesn't vary from one mempool to another, but when your\nbacklog isn't full enough, you may lose some future profits if\ntransactions don't come in later.\n\n> I think \"Y% higher\" rather than just \"higher\" is only useful for\n> rate-limiting, not incentive compatibility. (Though maybe it helps\n> stabilise a greedy algorithm in some cases?)\n\nThat's true. I claimed these policies only address incentives, but using\na percentage increase addresses rate-limiting a bit as well (I couldn't\nresist trying to do at least something for it!). I find it a very easy\nmechanism to implement, while choosing an absolute value is hard (it's\nalways easier to think in relatives than absolutes).\n\n> This is why I think it is important to understand the rationales for\nintroducing the rules in the first place\n\nI completely agree. As you mentioned, we are still in brainstorming\nphase, once (if?) we start to converge on what could be better policies,\nwe do need to clearly explain each policy's expected goal. That will let\nfuture Bastien writing code in 2030 clearly highlight why the 2022 rules\ndon't make sense anymore!\n\nCheers,\nBastien\n\nLe sam. 5 f\u00e9vr. 2022 \u00e0 14:22, Michael Folkson <michaelfolkson at protonmail.com>\na \u00e9crit :\n\n> Thanks for this Bastien (and Gloria for initially posting about this).\n>\n> I sympathetically skimmed the eclair PR (\n> https://github.com/ACINQ/eclair/pull/2113) dealing with replaceable\n> transactions fee bumping.\n>\n> There will continue to be a (hopefully) friendly tug of war on this\n> probably for the rest of Bitcoin's existence. I am sure people like Luke,\n> Prayank etc will (rightfully) continue to raise that Lightning and other\n> second layer protocols shouldn't demand that policy rules be changed if\n> there is a reason (e.g. DoS vector) for those rules on the base network.\n> But if there are rules that have no upside, introduce unnecessary\n> complexity for no reason and make Lightning implementers like Bastien's\n> life miserable attempting to deal with them I really hope we can make\n> progress on removing or simplifying them.\n>\n> This is why I think it is important to understand the rationales for\n> introducing the rules in the first place (and why it is safe to remove them\n> if indeed it is) and being as rigorous as possible on the rationales for\n> introducing additional rules. It sounds like from Gloria's initial post we\n> are still at a brainstorming phase (which is fine) but knowing what we know\n> today I really hope we can learn from the mistakes of the original BIP 125,\n> namely the Core implementation not matching the BIP and the sparse\n> rationales for the rules. As Bastien says this is not criticizing the\n> original BIP 125 authors, 7 years is a long time especially in Bitcoin\n> world and they probably weren't thinking about Bastien sitting down to\n> write an eclair PR in late 2021 (and reviewers of that PR) when they wrote\n> the BIP in 2015.\n>\n> --\n> Michael Folkson\n> Email: michaelfolkson at protonmail.com\n> Keybase: michaelfolkson\n> PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n>\n>\n>\n> ------- Original Message -------\n> On Monday, January 31st, 2022 at 3:57 PM, Bastien TEINTURIER via\n> bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> Hi Gloria,\n>\n> Many thanks for raising awareness on these issues and constantly pushing\n> towards finding a better model. This work will highly improve the\n> security of any multi-party contract trying to build on top of bitcoin\n> (because most multi-party contracts will need to have timeout conditions\n> and participants will need to make some transactions confirm before a\n> timeout happens - otherwise they may lose funds).\n>\n> For starters, let me quickly explain why the current rules are hard to\n> work with in the context of lightning (but I believe most L2 protocols\n> will have the same issues). Feel free to skip this part if you are\n> already convinced.\n>\n> ## Motivation\n>\n> The biggest pain point is BIP 125 rule 2.\n> If I need to increase the fees of a time-sensitive transaction because\n> the feerate has been rising since I broadcast it, I may need to also pay\n> high fees just to produce a confirmed utxo that I can use. I'm actually\n> paying a high fee twice instead of once (and needlessly using on-chain\n> space, our scarcest asset, because we could have avoided that additional\n> transaction!).\n>\n> It also has some annoying \"non-determinism\".\n> Imagine that my transaction has been evicted from my mempool because its\n> feerate was too low. I could think \"Great, that means I don't have to\n> apply BIP 125 restrictions, I can just fund this transaction as if it\n> were a new one!\". But actually I do, because my transaction could still\n> be in miner's mempools and I have no way of knowing it...this means that\n> whenever I have broadcast a transaction, I must assume that I will\n> always need to abide by whatever replacement rules the network applies.\n>\n> Fortunately, as far as I understand it, this rule only exists because of\n> a previous implementation detail of bitcoin core, so there's simply no\n> good reason to keep it.\n>\n> The second biggest pain point is rule 3. It prevents me from efficiently\n> using my capital while it's unconfirmed. Whenever I'm using a big utxo\n> to fund a transaction, I will get a big change output, and it would\n> really be a waste to be unable to use that change output to fund other\n> transactions. In order to be capital-efficient, I will end up creating\n> descendant trees for my time-sensitive transactions. But as Gloria\n> explained, replacing all my children will cost me an absurdly large\n> amount of fees. So what I'm actually planning to do instead is to RBF\n> one of the descendants high enough to get the whole tree confirmed.\n> But if those descendants' timeouts were far in the future, that's a\n> waste, I paid a lot more fees for them than I should have. I'd like to\n> just replace my transaction and republish the invalidated children\n> independently.\n>\n> Rule 4 doesn't hurt as much as the two previous ones, I don't have too\n> much to say about it.\n>\n> To be fair to the BIP 125 authors, all of these scenarios were very hard\n> to forecast at the time this BIP was created. We needed years to build\n> on those rules to get a better understanding of their limitations and if\n> the rationale behind them made sense in the long term.\n>\n> ## Proposals\n>\n> I believe that now is a good time to re-think those, and I really like\n> Gloria's categorization of the design constraints.\n>\n> I'd like to propose a different way of looking at descendants that makes\n> it easier to design the new rules. The way I understand it, limiting the\n> impact on descendant transactions is only important for DoS protection,\n> not for incentive compatibility. I would argue that after evictions,\n> descendant transactions will be submitted again (because they represent\n> transactions that people actually want to make), so evicting them does\n> not have a negative impact on mining incentives (in a world where blocks\n> are full most of the time).\n>\n> I'm curious to hear other people's thoughts on that. If it makes sense,\n> I would propose the following very simple rules:\n>\n> 1. The transaction's ancestor absolute fees must be X% higher than the\n> previous transaction's ancestor fees\n> 2. The transaction's ancestor feerate must be Y% higher than the\n> previous transaction's ancestor feerate\n>\n> I believe it's completely ok to require increasing both the fees and\n> feerate if we don't take descendants into account, because you control\n> your ancestor set - whereas the descendant set may be completely out of\n> your control.\n>\n> This is very easy to use by wallets, because the ancestor set is easy to\n> obtain. And an important point is that the ancestor set is the same in\n> every mempool, whereas the descendant set is not (your mempool may have\n> rejected the last descendants, while other people's mempools may still\n> contain them).\n>\n> Because of that reason, I'd like to avoid having a rule that relies on\n> some size of the replaced descendant set: it may be valid in your\n> mempool but invalid in someone else's, which makes it exploitable for\n> pinning attacks.\n>\n> I believe these rules are incentive compatible (again, if you accept\n> the fact that the descendants will be re-submitted and mined as well,\n> so their fees aren't lost).\n>\n> Can we choose X and Y so that these two rules are also DoS-resistant?\n> Unfortunately I'm not sure, so maybe we'll need to add a third rule to\n> address that. But before we do, can someone detail what it costs for a\n> node to evict a descendant tree? Given that bitcoin core doesn't allow\n> chains of more than 25 transactions, the maximum number of transactions\n> being replaced will be bounded by 25 * N (where N is the number of\n> outputs of the transaction being replaced). If it's just O(n) pruning of\n> a graph, maybe that's ok? Or maybe we make X or Y depend on the number\n> of outputs of the transaction being replaced (this would need very\n> careful thoughts)?\n>\n> If you made it this far, thanks for reading!\n> A couple of comments on the previous messages:\n>\n> > Currently, if we see a transaction\n> > that has the same txid as one in the mempool, we reject it as a\n> > duplicate, even if the feerate is much higher. It's unclear to me if\n> > we have a very strong reason to change this, but noting it as a\n> > limitation of our current replacement policy.\n>\n> I don't see a strong reason from an L2 protocol's point of view yet, but\n> there are many unkown unknowns. But from a miner incentive's point of\n> view, we should keep the transaction with the higher feerate, shouldn't\n> we? In that case it's also a more efficient use of on-chain space, which\n> is a win, right?\n>\n> > We might have a more-or-less long transition period during which we\n> support both...\n>\n> Yes, this is a long term thing.\n> Even if bitcoin core releases a new version with updated RBF rules, as a\n> wallet you'll need to keep using the old rules for a long time if you\n> want to be safe.\n>\n> But it's all the more reason to try to ship this as soon as possible,\n> this way maybe our grand-children will be able to benefit from it ;)\n> (just kidding on the timespan obviously).\n>\n> Cheers,\n> Bastien\n>\n> Le lun. 31 janv. 2022 \u00e0 00:11, Antoine Riard via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>\n>> Hi Gloria,\n>>\n>> Thanks for this RBF sum up. Few thoughts and more context comments if it\n>> can help other readers.\n>>\n>> > For starters, the absolute fee pinning attack is especially\n>> > problematic if we apply the same rules (i.e. Rule #3 and #4) in\n>> > Package RBF. Imagine that Alice (honest) and Bob (adversary) share a\n>> > LN channel. The mempool is rather full, so their pre-negotiated\n>> > commitment transactions' feerates would not be considered high\n>> > priority by miners. Bob broadcasts his commitment transaction and\n>> > attaches a very large child (100KvB with 100,000sat in fees) to his\n>> > anchor output. Alice broadcasts her commitment transaction with a\n>> > fee-bumping child (200vB with 50,000sat fees which is a generous\n>> > 250sat/vB), but this does not meet the absolute fee requirement. She\n>> > would need to add another 50,000sat to replace Bob's commitment\n>> > transaction.\n>>\n>> Solving LN pinning attacks, what we're aiming for is enabling a fair\n>> feerate bid between the counterparties, thus either forcing the adversary\n>> to overbid or to disengage from the confirmation competition. If the\n>> replace-by-feerate rule is adopted, there shouldn't be an incentive for Bob\n>> to\n>> pick up the first option. Though if he does, that's a winning outcome for\n>> Alice, as one of the commitment transactions confirms and her\n>> time-sensitive second-stage HTLC can be subsequently confirmed.\n>>\n>> > It's unclear to me if\n>> > we have a very strong reason to change this, but noting it as a\n>> > limitation of our current replacement policy. See [#24007][12].\n>>\n>> Deployment of Taproot opens interesting possibilities in the\n>> vaults/payment channels design space, where the tapscripts can commit to\n>> different set of timelocks/quorum of keys. Even if the pre-signed states\n>> stay symmetric, whoever is the publisher, the feerate cost to spend can\n>> fluctuate.\n>>\n>> > While this isn't completely broken, and the user interface is\n>> > secondary to the safety of the mempool policy\n>>\n>> I think with L2s transaction broadcast backend, the stability and clarity\n>> of the RBF user interface is primary. What we could be worried about is a\n>> too-much complex interface easing the way for an attacker to trigger your\n>> L2 node to issue policy-invalid chain of transactions. Especially, when we\n>> consider that an attacker might have leverage on chain of transactions\n>> composition (\"force broadcast of commitment A then commitment B, knowing\n>> they will share a CPFP\") or even transactions size (\"overload commitment A\n>> with HTLCs\").\n>>\n>> > * If the original transaction is in the top {0.75MvB, 1MvB} of the\n>> > mempool, apply the current rules (absolute fees must increase and\n>> > pay for the replacement transaction's new bandwidth). Otherwise, use a\n>> > feerate-only rule.\n>>\n>> How this new replacement rule would behave if you have a parent in the\n>> \"replace-by-feerate\" half but the child is in the \"replace-by-fee\" one ?\n>>\n>> If we allow the replacement of the parent based on the feerate, we might\n>> decrease the top block absolute fees.\n>>\n>> If we block the replacement of the parent based on the feerate because\n>> the replacement absolute fees aren't above the replaced package, we still\n>> preclude a pinning vector. The child might be low-feerate junk and even\n>> attached to a low ancestor-score branch.\n>>\n>> If I'm correct on this limitation, maybe we could turn off the\n>> \"replace-by-fee\" behavior as soon as the mempool is fulfilled with a few\n>> blocks ?\n>>\n>> > * Rate-limit how many replacements we allow per prevout.\n>>\n>> Depending on how it is implemented, though I would be concerned it\n>> introduces a new pinning vector in the context of shared-utxo. If it's a\n>> hardcoded constant, it could be exhausted by an adversary starting at the\n>> lowest acceptable feerate then slowly increasing while still not reaching\n>> the top of the mempool. Same if it's time-based or block-based, no\n>> guarantee the replacement slot is honestly used by your counterparty.\n>>\n>> Further, an above-the-average replacement frequency might just be the\n>> reflection of your confirmation strategy reacting to block schedule or\n>> mempools historical data. As long as the feerate penalty is paid, I lean to\n>> allow replacement.\n>>\n>> (One solution could be to associate per-user \"tag\" to the LN\n>> transactions, where each \"tag\" would have its own replacement slots, but\n>> privacy?)\n>>\n>> > * Rate-limit transaction validation in general, per peer.\n>>\n>> I think we could improve on the Core's new transaction requester logic.\n>> Maybe we could bind the peer announced flow based on the feerate score\n>> (modulo validation time) of the previously validated transactions from that\n>> peer ? That said, while related to RBF, it sounds to me that enhancing\n>> Core's rate-limiting transaction strategy is a whole discussion in itself\n>> [0]. Especially ensuring it's tolerant to the specific requirements of LN &\n>> consorts.\n>>\n>> > What should they be? We can do some arithmetic to see what happens if\n>> > you start with the biggest/lowest feerate transaction and do a bunch\n>> > of replacements. Maybe we end up with values that are high enough to\n>> > prevent abuse and make sense for applications/users that do RBF.\n>>\n>> That's a good question.\n>>\n>> One observation is that the attacker can always renew the set of DoSy\n>> utxos to pursue the attack. So maybe we could pick up constants scaled on\n>> the block size ? That way an attacker would have to burn fees, thus\n>> deterring them from launching an attack. Even if the attackers are miners,\n>> they have to renounce their income to acquire new DoSy utxos. If a low-fee\n>> period, we could scale up the constants ?\n>>\n>>\n>> Overall, I think there is the deployment issue to warn of. Moving to a\n>> new set of RBF rules implies for a lot of Bitcoin applications to rewrite\n>> their RBF logics. We might have a more-or-less long transition period\n>> during which we support both...\n>>\n>> Cheers,\n>> Antoine\n>>\n>> [0] https://github.com/bitcoin/bitcoin/pull/21224\n>>\n>> Le jeu. 27 janv. 2022 \u00e0 09:10, Gloria Zhao via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>\n>>> Hi everyone,\n>>>\n>>> This post discusses limitations of current Bitcoin Core RBF policy and\n>>> attempts to start a conversation about how we can improve it,\n>>> summarizing some ideas that have been discussed. Please reply if you\n>>> have any new input on issues to be solved and ideas for improvement!\n>>>\n>>> Just in case I've screwed up the text wrapping again, another copy can be\n>>> found here:\n>>> https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff\n>>>\n>>> ## Background\n>>>\n>>> Please feel free to skip this section if you are already familiar\n>>> with RBF.\n>>>\n>>> Nodes may receive *conflicting* unconfirmed transactions, aka\n>>> \"double spends\" of the same inputs. Instead of always keeping the\n>>> first transaction, since v0.12, Bitcoin Core mempool policy has\n>>> included a set of Replace-by-Fee (RBF) criteria that allows the second\n>>> transaction to replace the first one and any descendants it may have.\n>>>\n>>> Bitcoin Core RBF policy was previously documented as BIP 125.\n>>> The current RBF policy is documented [here][1]. In summary:\n>>>\n>>> 1. The directly conflicting transactions all signal replaceability\n>>> explicitly.\n>>>\n>>> 2. The replacement transaction only includes an unconfirmed input if\n>>> that input was included in one of the directly conflicting\n>>> transactions.\n>>>\n>>> 3. The replacement transaction pays an absolute fee of at least the\n>>> sum paid by the original transactions.\n>>>\n>>> 4. The additional fees pays for the replacement transaction's\n>>> bandwidth at or above the rate set by the node's *incremental relay\n>>> feerate*.\n>>>\n>>> 5. The sum of all directly conflicting transactions' descendant counts\n>>> (number of transactions inclusive of itself and its descendants)\n>>> does not exceed 100.\n>>>\n>>> We can split these rules into 3 categories/goals:\n>>>\n>>> - **Allow Opting Out**: Some applications/businesses are unable to\n>>> handle transactions that are replaceable (e.g. merchants that use\n>>> zero-confirmation transactions). We (try to) help these businesses by\n>>> honoring BIP125 signaling; we won't replace transactions that have not\n>>> opted in.\n>>>\n>>> - **Incentive Compatibility**: Ensure that our RBF policy would not\n>>> accept replacement transactions which would decrease fee profits\n>>> of a miner. In general, if our mempool policy deviates from what is\n>>> economically rational, it's likely that the transactions in our\n>>> mempool will not match the ones in miners' mempools, making our\n>>> fee estimation, compact block relay, and other mempool-dependent\n>>> functions unreliable. Incentive-incompatible policy may also\n>>> encourage transaction submission through routes other than the p2p\n>>> network, harming censorship-resistance and privacy of Bitcoin payments.\n>>>\n>>> - **DoS Protection**: Limit two types of DoS attacks on the node's\n>>> mempool: (1) the number of times a transaction can be replaced and\n>>> (2) the volume of transactions that can be evicted during a\n>>> replacement.\n>>>\n>>> Even more abstract: our goal is to make a replacement policy that\n>>> results in a useful interface for users and safe policy for\n>>> node operators.\n>>>\n>>> ## Motivation\n>>>\n>>> There are a number of known problems with the current RBF policy.\n>>> Many of these shortcomings exist due to mempool limitations at the\n>>> time RBF was implemented or result from new types of Bitcoin usage;\n>>> they are not criticisms of the original design.\n>>>\n>>> ### Pinning Attacks\n>>>\n>>> The most pressing concern is that attackers may take advantage of\n>>> limitations in RBF policy to prevent other users' transactions from\n>>> being mined or getting accepted as a replacement.\n>>>\n>>> #### SIGHASH_ANYONECANPAY Pinning\n>>>\n>>> BIP125#2 can be bypassed by creating intermediary transactions to be\n>>> replaced together. Anyone can simply split a 1-input 1-output\n>>> transaction off from the replacement transaction, then broadcast the\n>>> transaction as is. This can always be done, and quite cheaply. More\n>>> details in [this comment][2].\n>>>\n>>> In general, if a transaction is signed with SIGHASH\\_ANYONECANPAY,\n>>> anybody can just attach a low feerate parent to this transaction and\n>>> lower its ancestor feerate. Even if you require SIGHASH\\_ALL which\n>>> prevents an attacker from changing any outputs, the input can be a\n>>> very low amount (e.g. just above the dust limit) from a low-fee\n>>> ancestor and still bring down the ancestor feerate of the transaction.\n>>>\n>>> TLDR: if your transaction is signed with SIGHASH\\_ANYONECANPAY and\n>>> signals replaceability, regardless of the feerate you broadcast at, an\n>>> attacker can lower its mining priority by adding an ancestor.\n>>>\n>>> #### Absolute Fee\n>>>\n>>> The restriction of requiring replacement transactions to increase the\n>>> absolute fee of the mempool has been described as \"bonkers.\" If the\n>>> original transaction has a very large descendant that pays a large\n>>> amount of fees, even if it has a low feerate, the replacement\n>>> transaction must now pay those fees in order to meet Rule #3.\n>>>\n>>> #### Package RBF\n>>>\n>>> There are a number of reasons why, in order to enable Package RBF, we\n>>> cannot use the same criteria.\n>>>\n>>> For starters, the absolute fee pinning attack is especially\n>>> problematic if we apply the same rules (i.e. Rule #3 and #4) in\n>>> Package RBF. Imagine that Alice (honest) and Bob (adversary) share a\n>>> LN channel. The mempool is rather full, so their pre-negotiated\n>>> commitment transactions' feerates would not be considered high\n>>> priority by miners. Bob broadcasts his commitment transaction and\n>>> attaches a very large child (100KvB with 100,000sat in fees) to his\n>>> anchor output. Alice broadcasts her commitment transaction with a\n>>> fee-bumping child (200vB with 50,000sat fees which is a generous\n>>> 250sat/vB), but this does not meet the absolute fee requirement. She\n>>> would need to add another 50,000sat to replace Bob's commitment\n>>> transaction.\n>>>\n>>> Disallowing new unconfirmed inputs (Rule #2) in Package RBF would be\n>>> broken for packages containing transactions already in the mempool,\n>>> explained [here][7].\n>>>\n>>> Note: I originally [proposed][6] Package RBF using the same Rule #3\n>>> and #4 before I realized how significant this pinning attack is. I'm\n>>> retracting that proposal, and a new set of Package RBF rules would\n>>> follow from whatever the new individual RBF rules end up being.\n>>>\n>>> #### Same Txid Different Witness\n>>>\n>>> Two transactions with the same non-witness data but different\n>>> witnesses have the same txid but different wtxid, and the same fee but\n>>> not necessarily the same feerate. Currently, if we see a transaction\n>>> that has the same txid as one in the mempool, we reject it as a\n>>> duplicate, even if the feerate is much higher. It's unclear to me if\n>>> we have a very strong reason to change this, but noting it as a\n>>> limitation of our current replacement policy. See [#24007][12].\n>>>\n>>> ### User Interface\n>>>\n>>> #### Using Unconfirmed UTXOs to Fund Replacements\n>>>\n>>> The restriction of only allowing confirmed UTXOs for funding a\n>>> fee-bump (Rule #2) can hurt users trying to fee-bump their\n>>> transactions and complicate wallet implementations. If the original\n>>> transaction's output value isn't sufficient to fund a fee-bump and/or\n>>> all of the user's other UTXOs are unconfirmed, they might not be able\n>>> to fund a replacement transaction. Wallet developers also need to\n>>> treat self-owned unconfirmed UTXOs as unusable for fee-bumping, which\n>>> adds complexity to wallet logic. For example, see BDK issues [#144][4]\n>>> and [#414][5].\n>>>\n>>> #### Interface Not Suitable for Coin Selection\n>>>\n>>> Currently, a user cannot simply create a replacement transaction\n>>> targeting a specific feerate or meeting a minimum fee amount and\n>>> expect to meet the RBF criteria. The fee amount depends on the size of\n>>> the replacement transaction, and feerate is almost irrelevant.\n>>>\n>>> Bitcoin Core's `bumpfee` doesn't use the RBF rules when funding the\n>>> replacement. It [estimates][13] a feerate which is \"wallet incremental\n>>> relay fee\" (a conservative overestimation of the node's incremental\n>>> relay fee) higher than the original transaction, selects coins for\n>>> that feerate, and hopes that it meets the RBF rules. It never fails\n>>> Rule #3 and #4 because it uses all original inputs and refuses to\n>>> bump a transaction with mempool descendants.\n>>>\n>>> This is suboptimal, but is designed to work with the coin selection\n>>> engine: select a feerate first, and then add fees to cover it.\n>>> Following the exact RBF rules would require working the other way\n>>> around: based on how much fees we've added to the transaction and its\n>>> current size, calculate the feerate to see if we meet Rule #4.\n>>>\n>>> While this isn't completely broken, and the user interface is\n>>> secondary to the safety of the mempool policy, we can do much better.\n>>> A much more user-friendly interface would depend *only* on the\n>>> fee and size of the original transactions.\n>>>\n>>> ### Updates to Mempool and Mining\n>>>\n>>> Since RBF was first implemented, a number of improvements have been\n>>> made to mempool and mining logic. For example, we now use ancestor\n>>> feerates in mining (allowing CPFP), and keep track of ancestor\n>>> packages in the mempool.\n>>>\n>>> ## Ideas for Improvements\n>>>\n>>> ### Goals\n>>>\n>>> To summarize, these seem to be desired changes, in order of priority:\n>>>\n>>> 1. Remove Rule #3. The replacement should not be *required* to pay\n>>> higher absolute fees.\n>>>\n>>> 2. Make it impossible for a replacement transaction to have a lower\n>>> mining score than the original transaction(s). This would eliminate\n>>> the `SIGHASH\\_ANYONECANPAY` pinning attack.\n>>>\n>>> 3. Remove Rule #2. Adding new unconfirmed inputs should be allowed.\n>>>\n>>> 4. Create a more helpful interface that helps wallet fund replacement\n>>> transactions that aim for a feerate and fee.\n>>>\n>>> ### A Different Model for Fees\n>>>\n>>> For incentive compatibility, I believe there are different\n>>> formulations we should consider. Most importantly, if we want to get\n>>> rid of the absolute fee rule, we can no longer think of it as \"the\n>>> transaction needs to pay for its own bandwidth,\" since we won't always\n>>> be getting additional fees. That means we need a new method of\n>>> rate-limiting replacements that doesn't require additional fees every\n>>> time.\n>>>\n>>> While it makes sense to think about monetary costs when launching a\n>>> specific type of attack, given that the fees are paid to the miner and\n>>> not to the mempool operators, maybe it doesn't make much sense to\n>>> think about \"paying for bandwidth\". Maybe we should implement\n>>> transaction validation rate-limiting differently, e.g. building it\n>>> into the P2P layer instead of the mempool policy layer.\n>>>\n>>> Recently, Suhas gave a [formulation][8] for incentive compatibility\n>>> that made sense to me: \"are the fees expected to be paid in the next\n>>> (N?) blocks higher or lower if we process this transaction?\"\n>>>\n>>> I started by thinking about this where N=1 or `1 + p`.\n>>> Here, a rational miner is looking at what fees they would\n>>> collect in the next block, and then some proportion `p` of the rest of\n>>> the blocks based on their hashrate. We're assuming `p` isn't *so high*\n>>> that they would be okay with lower absolute fees in the next 1 block.\n>>> We're also assuming `p` isn't *so low* that the miner doesn't care\n>>> about what's left of the mempool after this block.\n>>>\n>>> A tweak to this formulation is \"if we process this transaction, would\n>>> the fees in the next 1 block higher or lower, and is the feerate\n>>> density of the rest of the mempool higher or lower?\" This is pretty\n>>> similar, where N=1, but we consider the rest of the mempool by feerate\n>>> rather than fees.\n>>>\n>>> ### Mining Score of a Mempool Transaction\n>>>\n>>> We are often interested in finding out what\n>>> the \"mining score\" of a transaction in the mempool is. That is, when\n>>> the transaction is considered in block template building, what is the\n>>> feerate it is considered at?\n>>>\n>>> Obviously, it's not the transaction's individual feerate. Bitcoin Core\n>>> [mining code sorts][14] transactions by their ancestor feerate and\n>>> includes them packages at a time, keeping track of how this affects the\n>>> package feerates of remaining transactions in the mempool.\n>>>\n>>> *ancestor feerate*: Ancestor feerate is easily accessible information,\n>>> but it's not accurate either, because it doesn't take into account the\n>>> fact that subsets of a transaction's ancestor set can be included\n>>> without it. For example, ancestors may have high feerates on their own\n>>> or we may have [high feerate siblings][8].\n>>>\n>>> TLDR: *Looking at the current ancestor feerate of a transaction is\n>>> insufficient to tell us what feerate it will be considered at when\n>>> building a block template in the future.*\n>>>\n>>> *min(individual feerate, ancestor feerate)*: Another\n>>> heuristic that is simple to calculate based on current mempool tooling\n>>> is to use the [minimum of a transaction's individual score and its\n>>> ancestor score][10] as a conservative measure. But this can\n>>> overestimate as well (see the example below).\n>>>\n>>> *min ancestor feerate(tx + possible ancestor subsets)* We can also\n>>> take the minimum of every possible ancestor subset, but this can be\n>>> computationally expensive since there can be lots and lots of ancestor\n>>> subsets.\n>>>\n>>> *max ancestor feerate(tx + possible descendant subsets)*: Another idea\n>>> is to use the [maximum ancestor score of the transaction + each of its\n>>> descendants][9]. This doesn't work either; it has the same blindspot\n>>> of ancestor subsets being mined on their own.\n>>>\n>>> #### Mining Score Example\n>>>\n>>> Here's an example illustrating why mining score is tricky to\n>>> efficiently calculate for mempool transactions:\n>>>\n>>> Let's say you have same-size transactions A (21sat/vB), B (1sat/vB),\n>>> C(9sat/vB), D(5sat/vB).\n>>> The layout is: grandparent A, parent B, and two children C and D.\n>>>\n>>> ```\n>>> A\n>>> ^\n>>> B\n>>> ^ ^\n>>> C D\n>>> ```\n>>>\n>>> A miner using ancestor packages to build block templates will first\n>>> include A with a mining score of 21. Next, the miner will include B and\n>>> C with a mining score of 6. This leaves D, with a mining score of 5.\n>>>\n>>> Note: in this case, mining by ancestor feerate results in the most\n>>> rational decisions, but [a candidate set-based approach][10] which\n>>> makes ancestor feerate much less relevant could\n>>> be more advantageous in other situations.\n>>>\n>>> Here is a chart showing the \"true\" mining score alongside the values\n>>> calculating using imperfect heuristics described above. All of them\n>>> can overestimate or underestimate.\n>>>\n>>> ```\n>>> A B C D\n>>> mining score | 21 | 6 | 6 | 5 |\n>>> ancestor feerate | 21 | 11 | 10.3 | 9 |\n>>> min(individual, ancestor) | 21 | 1 | 9 | 5 |\n>>> min(tx + ancestor subsets) | 21 | 1 | 5 | 3 |\n>>> max(tx + descendants subsets) | 21 | 9 | 9 | 5 |\n>>>\n>>> ```\n>>>\n>>> Possibly the best solution for finding the \"mining score\" of a\n>>> transaction is to build a block template, see what feerate each\n>>> package is included at. Perhaps at some cutoff, remaining mempool\n>>> transactions can be estimated using some heuristic that leans\n>>> {overestimating, underestimating} depending on the situation.\n>>>\n>>> Mining score seems to be relevant in multiple places: Murch and I\n>>> recently [found][3] that it would be very important in\n>>> \"ancestor-aware\" funding of transactions (the wallet doesn't\n>>> incorporate ancestor fees when using unconfirmed transactions in coin\n>>> selection, which is a bug we want to fix).\n>>>\n>>> In general, it would be nice to know the exact mining priority of\n>>> one's unconfirmed transaction is. I can think of a few block/mempool\n>>> explorers who might want to display this information for users.\n>>>\n>>> ### RBF Improvement Proposals\n>>>\n>>> After speaking to quite a few people, here are some suggestions\n>>> for improvements that I have heard:\n>>>\n>>> * The ancestor score of the replacement must be {5, 10, N}% higher\n>>> than that of every original transaction.\n>>>\n>>> * The ancestor score of the replacement must be 1sat/vB higher than\n>>> that of every original transaction.\n>>>\n>>> * If the original transaction is in the top {0.75MvB, 1MvB} of the\n>>> mempool, apply the current rules (absolute fees must increase and\n>>> pay for the replacement transaction's new bandwidth). Otherwise, use a\n>>> feerate-only rule.\n>>>\n>>> * If fees don't increase, the size of the replacement transaction must\n>>> decrease by at least N%.\n>>>\n>>> * Rate-limit how many replacements we allow per prevout.\n>>>\n>>> * Rate-limit transaction validation in general, per peer.\n>>>\n>>> Perhaps some others on the mailing list can chime in to throw other\n>>> ideas into the ring and/or combine some of these rules into a sensible\n>>> policy.\n>>>\n>>> #### Replace by Feerate Only\n>>>\n>>> I don't think there's going to be a single-line feerate-based\n>>> rule that can incorporate everything we need.\n>>> On one hand, a feerate-only approach helps eliminate the issues\n>>> associated with Rule #3. On the other hand, I believe the main concern\n>>> with a feerate-only approach is how to rate limit replacements. We\n>>> don't want to enable an attack such as:\n>>>\n>>> 1. Attacker broadcasts large, low-feerate transaction, and attaches a\n>>> chain of descendants.\n>>>\n>>> 2. The attacker replaces the transaction with a smaller but higher\n>>> feerate transaction, attaching a new chain of descendants.\n>>>\n>>> 3. Repeat 1000 times.\n>>>\n>>> #### Fees in Next Block and Feerate for the Rest of the Mempool\n>>>\n>>> Perhaps we can look at replacements like this:\n>>>\n>>> 1. Calculate the directly conflicting transactions and, with their\n>>> descendants, the original transactions. Check signaling. Limit the\n>>> total volume (e.g. can't be more than 100 total or 1MvB or something).\n>>>\n>>> 2. Find which original transactions would be in the next ~1 block. The\n>>> replacement must pay at least this amount + X% in absolute fees. This\n>>> guarantees that the fees of the next block doesn't decrease.\n>>>\n>>> 3. Find which transactions would be left in the mempool after that ~1\n>>> block. The replacement's feerate must be Y% higher than the maximum\n>>> mining score of these transactions. This guarantees that you now have\n>>> only *better* candidates in your after-this-block mempool than you did\n>>> before, even if the size and fees the transactions decrease.\n>>>\n>>> 4. Now you have two numbers: a minimum absolute fee amount and a\n>>> minimum feerate. Check to see if the replacement(s) meet these\n>>> minimums. Also, a wallet would be able to ask the node \"What fee and\n>>> feerate would I need to put on a transaction replacing this?\" and use\n>>> this information to fund a replacement transaction, without needing to\n>>> guess or overshoot.\n>>>\n>>> Obviously, there are some magic numbers missing here. X and Y are\n>>> TBD constants to ensure we have some kind of rate limiting for the\n>>> number of replacements allowed using some set of fees.\n>>>\n>>> What should they be? We can do some arithmetic to see what happens if\n>>> you start with the biggest/lowest feerate transaction and do a bunch\n>>> of replacements. Maybe we end up with values that are high enough to\n>>> prevent abuse and make sense for applications/users that do RBF.\n>>>\n>>> ### Mempool Changes Need for Implementation\n>>>\n>>> As described in the mining score section above,\n>>> we may want additional tooling to more accurately assess\n>>> the economic gain of replacing transactions in our mempool.\n>>>\n>>> A few options have been discussed:\n>>>\n>>> * Calculate block templates on the fly when we need to consider a\n>>> replacement. However, since replacements are [quite common][11]\n>>> and the information might be useful for other things as well,\n>>> it may be worth it to cache a block template.\n>>>\n>>> * Keep a persistent block template so that we know what transactions\n>>> we would put in the next block. We need to remember the feerate\n>>> at which each transaction was included in the template, because an\n>>> ancestor package may be included in the same block template in\n>>> multiple subsets. Transactions included earlier alter the ancestor\n>>> feerate of the remaining transactions in the package. We also need\n>>> to keep track of the new feerates of transactions left over.\n>>>\n>>> * Divide the mempool into two layers, \"high feerate\" and \"low\n>>> feerate.\" The high feerate layer contains ~1 block of packages with\n>>> the highest ancestor feerates, and the low feerate layer contains\n>>> everything else. At the edge of a block, we have a Knapsacky problem\n>>> where the next highest ancestor feerate package might not fit, so we\n>>> would probably want the high feerate layer ~2MvB or something to avoid\n>>> underestimating the fees.\n>>>\n>>> ## Acknowledgements\n>>>\n>>> Thank you to everyone whose RBF-related suggestions, grievances,\n>>> criticisms and ideas were incorporated in this document:\n>>> Andrew Chow, Matt Corallo, Suhas Daftuar, Christian Decker,\n>>> Mark Erhardt, Lloyd Fournier, Lisa Neigut, John Newbery,\n>>> Antoine Poinsot, Antoine Riard, Larry Ruane,\n>>> S3RK and Bastien Teinturier.\n>>>\n>>> Thanks for reading!\n>>>\n>>> Best,\n>>> Gloria\n>>>\n>>> [1]:\n>>> https://github.com/bitcoin/bitcoin/blob/master/doc/policy/mempool-replacements.md\n>>> [2]:\n>>> https://github.com/bitcoin/bitcoin/pull/23121#issuecomment-929475999\n>>> [3]:\n>>> https://github.com/Xekyo/bitcoin/commit/d754b0242ec69d42c570418aebf9c1335af0b8ea\n>>> [4]: https://github.com/bitcoindevkit/bdk/issues/144\n>>> [5]: https://github.com/bitcoindevkit/bdk/issues/414\n>>> [6]:\n>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019464.html\n>>> [7]:\n>>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a#new-unconfirmed-inputs-rule-2\n>>> [8]: https://github.com/bitcoin/bitcoin/pull/23121#discussion_r777131366\n>>> [9]:\n>>> https://github.com/bitcoin/bitcoin/pull/22290#issuecomment-865887922\n>>> [10]:\n>>> https://gist.github.com/Xekyo/5cb413fe9f26dbce57abfd344ebbfaf2#file-candidate-set-based-block-building-md\n>>> [11]:\n>>> https://github.com/bitcoin/bitcoin/pull/22539#issuecomment-885763670\n>>> [12]: https://github.com/bitcoin/bitcoin/pull/24007\n>>> [13]:\n>>> https://github.com/bitcoin/bitcoin/blob/1a369f006fd0bec373b95001ed84b480e852f191/src/wallet/feebumper.cpp#L114\n>>> [14]:\n>>> https://github.com/bitcoin/bitcoin/blob/cf5bb048e80d4cde8828787b266b7f5f2e3b6d7b/src/node/miner.cpp#L310-L320\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220207/66d76ec4/attachment-0001.html>"
            },
            {
                "author": "Gloria Zhao",
                "date": "2022-02-07T11:16:26",
                "message_text_only": "Hi everyone,\n\nThanks for giving your attention to the post! I haven't had time to write\nresponses to everything, but sending my thoughts about what has been most\nnoteworthy to me:\n\n@jeremy:\n> A final point is that a verifiable delay function could be used over,\ne.g., each of the N COutpoints individually to rate-limit transaction\nreplacement. The VDF period can be made shorter / eliminated depending on\nthe feerate increase.\n\nThanks for the suggestion! In general, I don't think rate limiting by\noutpoint/prevout is a safe option, as it is particularly dangerous for L2\napplications with shared prevouts. For example, the prevout that LN channel\ncounterparties conflict on is the output from their shared funding tx. Any\nkind of limit on spending this prevout can be monopolized by a spammy\nattacker. For example, if you only allow 1 per minute, the attacker will\njust race to take up that slot every minute to prevent the honest party's\ntransaction from being accepted.\nThis is similar to the pinning attack based on monopolizing the\ntransaction's descendant limit, except we can't carve out an exemption\nbecause we wouldn't know whose replacement we're looking at.\n\n@tbast:\n> The way I understand it, limiting the impact on descendant transactions\nis only important for DoS protection, not for incentive compatibility.\n\n> I believe it's completely ok to require increasing both the fees and\nfeerate if we don't take descendants into account, because you control your\nancestor set - whereas the descendant set may be completely out of your\ncontrol.\n\nIgnoring descendants of direct conflicts would certainly make our lives\nmuch easier! Unfortunately, I don't think we can do this since they can be\nfee bumps, i.e., in AJ's example. Considering descendants is important for\nboth incentive compatibility and DoS.\nIf the replacement transaction has a higher feerate than its direct\nconflict, but the direct conflict also has high feerate descendants, we\nmight end up with lower fees and/or feerates by accepting the replacement.\n\n@aj:\n> I wonder sometimes if it could be sufficient to just have a relay rate\nlimit and prioritise by ancestor feerate though. Maybe something like:\n>\n> - instead of adding txs to each peers setInventoryTxToSend immediately,\n>   set a mempool flag \"relayed=false\"\n>\n> - on a time delay, add the top N (by fee rate) \"relayed=false\" txs to\n>   each peer's setInventoryTxToSend and mark them as \"relayed=true\";\n>   calculate how much kB those txs were, and do this again after\n>   SIZE/RATELIMIT seconds\n>\n> - don't include \"relayed=false\" txs when building blocks?\n\nWow cool! I think outbound tx relay size-based rate-limiting and\nprioritizing tx relay by feerate are great ideas for preventing spammers\nfrom wasting bandwidth network-wide. I agree, this would slow the low\nfeerate spam down, preventing a huge network-wide bandwidth spike. And it\nwould allow high feerate transactions to propagate as they should,\nregardless of how busy traffic is. Combined with inbound tx request\nrate-limiting, might this be sufficient to prevent DoS regardless of the\nfee-based replacement policies?\n\nOne point that I'm not 100% clear on: is it ok to prioritize the\ntransactions by ancestor feerate in this scheme? As I described in the\noriginal post, this can be quite different from the actual feerate we would\nconsider a transaction in a block for. The transaction could have a high\nfeerate sibling bumping its ancestor.\nFor example, A (1sat/vB) has 2 children: B (49sat/vB) and C (5sat/vB). If\nwe just received C, it would be incorrect to give it a priority equal to\nits ancestor feerate (3sat/vB) because if we constructed a block template\nnow, B would bump A, and C's new ancestor feerate is 5sat/vB.\nThen, if we imagine that top N is >5sat/vB, we're not relaying C. If we\nalso exclude C when building blocks, we're missing out on good fees.\n\n> - keep high-feerate evicted txs around for a while in case they get\n>   mined by someone else to improve compact block relay, a la the\n>   orphan pool?\n\nReplaced transactions are already added to vExtraTxnForCompact :D\n\n@ariard\n> Deployment of Taproot opens interesting possibilities in the\nvaults/payment channels design space, where the tapscripts can commit to\ndifferent set of timelocks/quorum of keys. Even if the pre-signed states\nstay symmetric, whoever is the publisher, the feerate cost to spend can\nfluctuate.\n\nIndeed, perhaps with taproot we may legitimately have\nsame-txid-different-witness transactions as a normal thing rather than rare\nedge case. But as with everything enabled by taproot, I wouldn't count our\ntapscript eggs until a concrete use case hatches and/or an application\nactually implements it.\n\n> How this new replacement rule would behave if you have a parent in the\n\"replace-by-feerate\" half but the child is in the \"replace-by-fee\" one ?\n\nThanks for considering my suggestion! This particular scenario is not\npossible, since a child cannot be considered for the next block without its\nparent. But if the original transactions are found both in and outside the\nnext block, I think it would be fine to just require both are met.\n\n> Overall, I think there is the deployment issue to warn of. Moving to a\nnew set of RBF rules implies for a lot of Bitcoin applications to rewrite\ntheir RBF logics.\n\nI agree that transitioning as painlessly as possible would be a huge\npriority in any kind of upgrade to mempool policy. I'm very interested in\nhearing wallet devs' feedback on this.\nI'm also not actually clear on what backwards compatibility in this\nscenario would look like. I imagine it to mean we run both sets of RBF\nrules and accept the replacement if it passes either one. Or do we only\naccept the replacement if it passes both?\nFor wallets, AJ's \"All you need is for there to be *a* path that follows\nthe new relay rules and gets from your node/wallet to perhaps 10% of\nhashpower\" makes sense to me (which would be the former). For merchants who\ncare more about making sure the original transaction isn't replaceable,\nwould they prefer that either policy is sufficient to prevent a replacement\n(more in line with the latter)? Or is that covered by signaling / am I\noverthinking this?\n\nThanks,\nGloria\n\nOn Mon, Feb 7, 2022 at 10:24 AM Bastien TEINTURIER via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Good morning,\n>\n> > The tricky question is what happens when X arrives on its own and it\n> > might be that no one ever sends a replacement for B,C,D)\n>\n> It feels ok to me, but this is definitely arguable.\n>\n> It covers the fact that B,C,D could have been fake transactions whose\n> sole purpose was to do a pinning attack: in that case the attacker would\n> have found a way to ensure these transactions don't confirm anyway (or\n> pay minimal/negligible fees).\n>\n> If these transactions were legitimate, I believe that their owners would\n> remake them at some point (because these transactions reflect a business\n> relationship that needed to happen, so it should very likely still\n> happen). It's probably hard to verify because the new corresponding\n> transactions may have nothing in common with the first, but I think the\n> simplifications it offers for wallets is worth it (which is just my\n> opinion and needs more scrutiny/feedback).\n>\n> > But if your backlog's feerate does drop off, *and* that matters, then\n> > I don't think you can ignore the impact of the descendent transactions\n> > that you might not get a replacement for.\n>\n> That is because you're only taking into account the current backlog, and\n> not taking into account the fact that new items will be added to it soon\n> to replace the evicted descendants. But I agree that this is a bet: we\n> can't predict the future and guarantee these replacements will come.\n>\n> It is really a trade-off, ignoring descendents provides a much simpler\n> contract that doesn't vary from one mempool to another, but when your\n> backlog isn't full enough, you may lose some future profits if\n> transactions don't come in later.\n>\n> > I think \"Y% higher\" rather than just \"higher\" is only useful for\n> > rate-limiting, not incentive compatibility. (Though maybe it helps\n> > stabilise a greedy algorithm in some cases?)\n>\n> That's true. I claimed these policies only address incentives, but using\n> a percentage increase addresses rate-limiting a bit as well (I couldn't\n> resist trying to do at least something for it!). I find it a very easy\n> mechanism to implement, while choosing an absolute value is hard (it's\n> always easier to think in relatives than absolutes).\n>\n> > This is why I think it is important to understand the rationales for\n> introducing the rules in the first place\n>\n> I completely agree. As you mentioned, we are still in brainstorming\n> phase, once (if?) we start to converge on what could be better policies,\n> we do need to clearly explain each policy's expected goal. That will let\n> future Bastien writing code in 2030 clearly highlight why the 2022 rules\n> don't make sense anymore!\n>\n> Cheers,\n> Bastien\n>\n> Le sam. 5 f\u00e9vr. 2022 \u00e0 14:22, Michael Folkson <\n> michaelfolkson at protonmail.com> a \u00e9crit :\n>\n>> Thanks for this Bastien (and Gloria for initially posting about this).\n>>\n>> I sympathetically skimmed the eclair PR (\n>> https://github.com/ACINQ/eclair/pull/2113) dealing with replaceable\n>> transactions fee bumping.\n>>\n>> There will continue to be a (hopefully) friendly tug of war on this\n>> probably for the rest of Bitcoin's existence. I am sure people like Luke,\n>> Prayank etc will (rightfully) continue to raise that Lightning and other\n>> second layer protocols shouldn't demand that policy rules be changed if\n>> there is a reason (e.g. DoS vector) for those rules on the base network.\n>> But if there are rules that have no upside, introduce unnecessary\n>> complexity for no reason and make Lightning implementers like Bastien's\n>> life miserable attempting to deal with them I really hope we can make\n>> progress on removing or simplifying them.\n>>\n>> This is why I think it is important to understand the rationales for\n>> introducing the rules in the first place (and why it is safe to remove them\n>> if indeed it is) and being as rigorous as possible on the rationales for\n>> introducing additional rules. It sounds like from Gloria's initial post we\n>> are still at a brainstorming phase (which is fine) but knowing what we know\n>> today I really hope we can learn from the mistakes of the original BIP 125,\n>> namely the Core implementation not matching the BIP and the sparse\n>> rationales for the rules. As Bastien says this is not criticizing the\n>> original BIP 125 authors, 7 years is a long time especially in Bitcoin\n>> world and they probably weren't thinking about Bastien sitting down to\n>> write an eclair PR in late 2021 (and reviewers of that PR) when they wrote\n>> the BIP in 2015.\n>>\n>> --\n>> Michael Folkson\n>> Email: michaelfolkson at protonmail.com\n>> Keybase: michaelfolkson\n>> PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n>>\n>>\n>>\n>> ------- Original Message -------\n>> On Monday, January 31st, 2022 at 3:57 PM, Bastien TEINTURIER via\n>> bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>> Hi Gloria,\n>>\n>> Many thanks for raising awareness on these issues and constantly pushing\n>> towards finding a better model. This work will highly improve the\n>> security of any multi-party contract trying to build on top of bitcoin\n>> (because most multi-party contracts will need to have timeout conditions\n>> and participants will need to make some transactions confirm before a\n>> timeout happens - otherwise they may lose funds).\n>>\n>> For starters, let me quickly explain why the current rules are hard to\n>> work with in the context of lightning (but I believe most L2 protocols\n>> will have the same issues). Feel free to skip this part if you are\n>> already convinced.\n>>\n>> ## Motivation\n>>\n>> The biggest pain point is BIP 125 rule 2.\n>> If I need to increase the fees of a time-sensitive transaction because\n>> the feerate has been rising since I broadcast it, I may need to also pay\n>> high fees just to produce a confirmed utxo that I can use. I'm actually\n>> paying a high fee twice instead of once (and needlessly using on-chain\n>> space, our scarcest asset, because we could have avoided that additional\n>> transaction!).\n>>\n>> It also has some annoying \"non-determinism\".\n>> Imagine that my transaction has been evicted from my mempool because its\n>> feerate was too low. I could think \"Great, that means I don't have to\n>> apply BIP 125 restrictions, I can just fund this transaction as if it\n>> were a new one!\". But actually I do, because my transaction could still\n>> be in miner's mempools and I have no way of knowing it...this means that\n>> whenever I have broadcast a transaction, I must assume that I will\n>> always need to abide by whatever replacement rules the network applies.\n>>\n>> Fortunately, as far as I understand it, this rule only exists because of\n>> a previous implementation detail of bitcoin core, so there's simply no\n>> good reason to keep it.\n>>\n>> The second biggest pain point is rule 3. It prevents me from efficiently\n>> using my capital while it's unconfirmed. Whenever I'm using a big utxo\n>> to fund a transaction, I will get a big change output, and it would\n>> really be a waste to be unable to use that change output to fund other\n>> transactions. In order to be capital-efficient, I will end up creating\n>> descendant trees for my time-sensitive transactions. But as Gloria\n>> explained, replacing all my children will cost me an absurdly large\n>> amount of fees. So what I'm actually planning to do instead is to RBF\n>> one of the descendants high enough to get the whole tree confirmed.\n>> But if those descendants' timeouts were far in the future, that's a\n>> waste, I paid a lot more fees for them than I should have. I'd like to\n>> just replace my transaction and republish the invalidated children\n>> independently.\n>>\n>> Rule 4 doesn't hurt as much as the two previous ones, I don't have too\n>> much to say about it.\n>>\n>> To be fair to the BIP 125 authors, all of these scenarios were very hard\n>> to forecast at the time this BIP was created. We needed years to build\n>> on those rules to get a better understanding of their limitations and if\n>> the rationale behind them made sense in the long term.\n>>\n>> ## Proposals\n>>\n>> I believe that now is a good time to re-think those, and I really like\n>> Gloria's categorization of the design constraints.\n>>\n>> I'd like to propose a different way of looking at descendants that makes\n>> it easier to design the new rules. The way I understand it, limiting the\n>> impact on descendant transactions is only important for DoS protection,\n>> not for incentive compatibility. I would argue that after evictions,\n>> descendant transactions will be submitted again (because they represent\n>> transactions that people actually want to make), so evicting them does\n>> not have a negative impact on mining incentives (in a world where blocks\n>> are full most of the time).\n>>\n>> I'm curious to hear other people's thoughts on that. If it makes sense,\n>> I would propose the following very simple rules:\n>>\n>> 1. The transaction's ancestor absolute fees must be X% higher than the\n>> previous transaction's ancestor fees\n>> 2. The transaction's ancestor feerate must be Y% higher than the\n>> previous transaction's ancestor feerate\n>>\n>> I believe it's completely ok to require increasing both the fees and\n>> feerate if we don't take descendants into account, because you control\n>> your ancestor set - whereas the descendant set may be completely out of\n>> your control.\n>>\n>> This is very easy to use by wallets, because the ancestor set is easy to\n>> obtain. And an important point is that the ancestor set is the same in\n>> every mempool, whereas the descendant set is not (your mempool may have\n>> rejected the last descendants, while other people's mempools may still\n>> contain them).\n>>\n>> Because of that reason, I'd like to avoid having a rule that relies on\n>> some size of the replaced descendant set: it may be valid in your\n>> mempool but invalid in someone else's, which makes it exploitable for\n>> pinning attacks.\n>>\n>> I believe these rules are incentive compatible (again, if you accept\n>> the fact that the descendants will be re-submitted and mined as well,\n>> so their fees aren't lost).\n>>\n>> Can we choose X and Y so that these two rules are also DoS-resistant?\n>> Unfortunately I'm not sure, so maybe we'll need to add a third rule to\n>> address that. But before we do, can someone detail what it costs for a\n>> node to evict a descendant tree? Given that bitcoin core doesn't allow\n>> chains of more than 25 transactions, the maximum number of transactions\n>> being replaced will be bounded by 25 * N (where N is the number of\n>> outputs of the transaction being replaced). If it's just O(n) pruning of\n>> a graph, maybe that's ok? Or maybe we make X or Y depend on the number\n>> of outputs of the transaction being replaced (this would need very\n>> careful thoughts)?\n>>\n>> If you made it this far, thanks for reading!\n>> A couple of comments on the previous messages:\n>>\n>> > Currently, if we see a transaction\n>> > that has the same txid as one in the mempool, we reject it as a\n>> > duplicate, even if the feerate is much higher. It's unclear to me if\n>> > we have a very strong reason to change this, but noting it as a\n>> > limitation of our current replacement policy.\n>>\n>> I don't see a strong reason from an L2 protocol's point of view yet, but\n>> there are many unkown unknowns. But from a miner incentive's point of\n>> view, we should keep the transaction with the higher feerate, shouldn't\n>> we? In that case it's also a more efficient use of on-chain space, which\n>> is a win, right?\n>>\n>> > We might have a more-or-less long transition period during which we\n>> support both...\n>>\n>> Yes, this is a long term thing.\n>> Even if bitcoin core releases a new version with updated RBF rules, as a\n>> wallet you'll need to keep using the old rules for a long time if you\n>> want to be safe.\n>>\n>> But it's all the more reason to try to ship this as soon as possible,\n>> this way maybe our grand-children will be able to benefit from it ;)\n>> (just kidding on the timespan obviously).\n>>\n>> Cheers,\n>> Bastien\n>>\n>> Le lun. 31 janv. 2022 \u00e0 00:11, Antoine Riard via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>\n>>> Hi Gloria,\n>>>\n>>> Thanks for this RBF sum up. Few thoughts and more context comments if it\n>>> can help other readers.\n>>>\n>>> > For starters, the absolute fee pinning attack is especially\n>>> > problematic if we apply the same rules (i.e. Rule #3 and #4) in\n>>> > Package RBF. Imagine that Alice (honest) and Bob (adversary) share a\n>>> > LN channel. The mempool is rather full, so their pre-negotiated\n>>> > commitment transactions' feerates would not be considered high\n>>> > priority by miners. Bob broadcasts his commitment transaction and\n>>> > attaches a very large child (100KvB with 100,000sat in fees) to his\n>>> > anchor output. Alice broadcasts her commitment transaction with a\n>>> > fee-bumping child (200vB with 50,000sat fees which is a generous\n>>> > 250sat/vB), but this does not meet the absolute fee requirement. She\n>>> > would need to add another 50,000sat to replace Bob's commitment\n>>> > transaction.\n>>>\n>>> Solving LN pinning attacks, what we're aiming for is enabling a fair\n>>> feerate bid between the counterparties, thus either forcing the adversary\n>>> to overbid or to disengage from the confirmation competition. If the\n>>> replace-by-feerate rule is adopted, there shouldn't be an incentive for Bob\n>>> to\n>>> pick up the first option. Though if he does, that's a winning outcome\n>>> for Alice, as one of the commitment transactions confirms and her\n>>> time-sensitive second-stage HTLC can be subsequently confirmed.\n>>>\n>>> > It's unclear to me if\n>>> > we have a very strong reason to change this, but noting it as a\n>>> > limitation of our current replacement policy. See [#24007][12].\n>>>\n>>> Deployment of Taproot opens interesting possibilities in the\n>>> vaults/payment channels design space, where the tapscripts can commit to\n>>> different set of timelocks/quorum of keys. Even if the pre-signed states\n>>> stay symmetric, whoever is the publisher, the feerate cost to spend can\n>>> fluctuate.\n>>>\n>>> > While this isn't completely broken, and the user interface is\n>>> > secondary to the safety of the mempool policy\n>>>\n>>> I think with L2s transaction broadcast backend, the stability and\n>>> clarity of the RBF user interface is primary. What we could be worried\n>>> about is a too-much complex interface easing the way for an attacker to\n>>> trigger your L2 node to issue policy-invalid chain of transactions.\n>>> Especially, when we consider that an attacker might have leverage on chain\n>>> of transactions composition (\"force broadcast of commitment A then\n>>> commitment B, knowing they will share a CPFP\") or even transactions size\n>>> (\"overload commitment A with HTLCs\").\n>>>\n>>> > * If the original transaction is in the top {0.75MvB, 1MvB} of the\n>>> > mempool, apply the current rules (absolute fees must increase and\n>>> > pay for the replacement transaction's new bandwidth). Otherwise, use a\n>>> > feerate-only rule.\n>>>\n>>> How this new replacement rule would behave if you have a parent in the\n>>> \"replace-by-feerate\" half but the child is in the \"replace-by-fee\" one ?\n>>>\n>>> If we allow the replacement of the parent based on the feerate, we might\n>>> decrease the top block absolute fees.\n>>>\n>>> If we block the replacement of the parent based on the feerate because\n>>> the replacement absolute fees aren't above the replaced package, we still\n>>> preclude a pinning vector. The child might be low-feerate junk and even\n>>> attached to a low ancestor-score branch.\n>>>\n>>> If I'm correct on this limitation, maybe we could turn off the\n>>> \"replace-by-fee\" behavior as soon as the mempool is fulfilled with a few\n>>> blocks ?\n>>>\n>>> > * Rate-limit how many replacements we allow per prevout.\n>>>\n>>> Depending on how it is implemented, though I would be concerned it\n>>> introduces a new pinning vector in the context of shared-utxo. If it's a\n>>> hardcoded constant, it could be exhausted by an adversary starting at the\n>>> lowest acceptable feerate then slowly increasing while still not reaching\n>>> the top of the mempool. Same if it's time-based or block-based, no\n>>> guarantee the replacement slot is honestly used by your counterparty.\n>>>\n>>> Further, an above-the-average replacement frequency might just be the\n>>> reflection of your confirmation strategy reacting to block schedule or\n>>> mempools historical data. As long as the feerate penalty is paid, I lean to\n>>> allow replacement.\n>>>\n>>> (One solution could be to associate per-user \"tag\" to the LN\n>>> transactions, where each \"tag\" would have its own replacement slots, but\n>>> privacy?)\n>>>\n>>> > * Rate-limit transaction validation in general, per peer.\n>>>\n>>> I think we could improve on the Core's new transaction requester logic.\n>>> Maybe we could bind the peer announced flow based on the feerate score\n>>> (modulo validation time) of the previously validated transactions from that\n>>> peer ? That said, while related to RBF, it sounds to me that enhancing\n>>> Core's rate-limiting transaction strategy is a whole discussion in itself\n>>> [0]. Especially ensuring it's tolerant to the specific requirements of LN &\n>>> consorts.\n>>>\n>>> > What should they be? We can do some arithmetic to see what happens if\n>>> > you start with the biggest/lowest feerate transaction and do a bunch\n>>> > of replacements. Maybe we end up with values that are high enough to\n>>> > prevent abuse and make sense for applications/users that do RBF.\n>>>\n>>> That's a good question.\n>>>\n>>> One observation is that the attacker can always renew the set of DoSy\n>>> utxos to pursue the attack. So maybe we could pick up constants scaled on\n>>> the block size ? That way an attacker would have to burn fees, thus\n>>> deterring them from launching an attack. Even if the attackers are miners,\n>>> they have to renounce their income to acquire new DoSy utxos. If a low-fee\n>>> period, we could scale up the constants ?\n>>>\n>>>\n>>> Overall, I think there is the deployment issue to warn of. Moving to a\n>>> new set of RBF rules implies for a lot of Bitcoin applications to rewrite\n>>> their RBF logics. We might have a more-or-less long transition period\n>>> during which we support both...\n>>>\n>>> Cheers,\n>>> Antoine\n>>>\n>>> [0] https://github.com/bitcoin/bitcoin/pull/21224\n>>>\n>>> Le jeu. 27 janv. 2022 \u00e0 09:10, Gloria Zhao via bitcoin-dev <\n>>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>>\n>>>> Hi everyone,\n>>>>\n>>>> This post discusses limitations of current Bitcoin Core RBF policy and\n>>>> attempts to start a conversation about how we can improve it,\n>>>> summarizing some ideas that have been discussed. Please reply if you\n>>>> have any new input on issues to be solved and ideas for improvement!\n>>>>\n>>>> Just in case I've screwed up the text wrapping again, another copy can\n>>>> be\n>>>> found here:\n>>>> https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff\n>>>>\n>>>> ## Background\n>>>>\n>>>> Please feel free to skip this section if you are already familiar\n>>>> with RBF.\n>>>>\n>>>> Nodes may receive *conflicting* unconfirmed transactions, aka\n>>>> \"double spends\" of the same inputs. Instead of always keeping the\n>>>> first transaction, since v0.12, Bitcoin Core mempool policy has\n>>>> included a set of Replace-by-Fee (RBF) criteria that allows the second\n>>>> transaction to replace the first one and any descendants it may have.\n>>>>\n>>>> Bitcoin Core RBF policy was previously documented as BIP 125.\n>>>> The current RBF policy is documented [here][1]. In summary:\n>>>>\n>>>> 1. The directly conflicting transactions all signal replaceability\n>>>> explicitly.\n>>>>\n>>>> 2. The replacement transaction only includes an unconfirmed input if\n>>>> that input was included in one of the directly conflicting\n>>>> transactions.\n>>>>\n>>>> 3. The replacement transaction pays an absolute fee of at least the\n>>>> sum paid by the original transactions.\n>>>>\n>>>> 4. The additional fees pays for the replacement transaction's\n>>>> bandwidth at or above the rate set by the node's *incremental relay\n>>>> feerate*.\n>>>>\n>>>> 5. The sum of all directly conflicting transactions' descendant counts\n>>>> (number of transactions inclusive of itself and its descendants)\n>>>> does not exceed 100.\n>>>>\n>>>> We can split these rules into 3 categories/goals:\n>>>>\n>>>> - **Allow Opting Out**: Some applications/businesses are unable to\n>>>> handle transactions that are replaceable (e.g. merchants that use\n>>>> zero-confirmation transactions). We (try to) help these businesses by\n>>>> honoring BIP125 signaling; we won't replace transactions that have not\n>>>> opted in.\n>>>>\n>>>> - **Incentive Compatibility**: Ensure that our RBF policy would not\n>>>> accept replacement transactions which would decrease fee profits\n>>>> of a miner. In general, if our mempool policy deviates from what is\n>>>> economically rational, it's likely that the transactions in our\n>>>> mempool will not match the ones in miners' mempools, making our\n>>>> fee estimation, compact block relay, and other mempool-dependent\n>>>> functions unreliable. Incentive-incompatible policy may also\n>>>> encourage transaction submission through routes other than the p2p\n>>>> network, harming censorship-resistance and privacy of Bitcoin payments.\n>>>>\n>>>> - **DoS Protection**: Limit two types of DoS attacks on the node's\n>>>> mempool: (1) the number of times a transaction can be replaced and\n>>>> (2) the volume of transactions that can be evicted during a\n>>>> replacement.\n>>>>\n>>>> Even more abstract: our goal is to make a replacement policy that\n>>>> results in a useful interface for users and safe policy for\n>>>> node operators.\n>>>>\n>>>> ## Motivation\n>>>>\n>>>> There are a number of known problems with the current RBF policy.\n>>>> Many of these shortcomings exist due to mempool limitations at the\n>>>> time RBF was implemented or result from new types of Bitcoin usage;\n>>>> they are not criticisms of the original design.\n>>>>\n>>>> ### Pinning Attacks\n>>>>\n>>>> The most pressing concern is that attackers may take advantage of\n>>>> limitations in RBF policy to prevent other users' transactions from\n>>>> being mined or getting accepted as a replacement.\n>>>>\n>>>> #### SIGHASH_ANYONECANPAY Pinning\n>>>>\n>>>> BIP125#2 can be bypassed by creating intermediary transactions to be\n>>>> replaced together. Anyone can simply split a 1-input 1-output\n>>>> transaction off from the replacement transaction, then broadcast the\n>>>> transaction as is. This can always be done, and quite cheaply. More\n>>>> details in [this comment][2].\n>>>>\n>>>> In general, if a transaction is signed with SIGHASH\\_ANYONECANPAY,\n>>>> anybody can just attach a low feerate parent to this transaction and\n>>>> lower its ancestor feerate. Even if you require SIGHASH\\_ALL which\n>>>> prevents an attacker from changing any outputs, the input can be a\n>>>> very low amount (e.g. just above the dust limit) from a low-fee\n>>>> ancestor and still bring down the ancestor feerate of the transaction.\n>>>>\n>>>> TLDR: if your transaction is signed with SIGHASH\\_ANYONECANPAY and\n>>>> signals replaceability, regardless of the feerate you broadcast at, an\n>>>> attacker can lower its mining priority by adding an ancestor.\n>>>>\n>>>> #### Absolute Fee\n>>>>\n>>>> The restriction of requiring replacement transactions to increase the\n>>>> absolute fee of the mempool has been described as \"bonkers.\" If the\n>>>> original transaction has a very large descendant that pays a large\n>>>> amount of fees, even if it has a low feerate, the replacement\n>>>> transaction must now pay those fees in order to meet Rule #3.\n>>>>\n>>>> #### Package RBF\n>>>>\n>>>> There are a number of reasons why, in order to enable Package RBF, we\n>>>> cannot use the same criteria.\n>>>>\n>>>> For starters, the absolute fee pinning attack is especially\n>>>> problematic if we apply the same rules (i.e. Rule #3 and #4) in\n>>>> Package RBF. Imagine that Alice (honest) and Bob (adversary) share a\n>>>> LN channel. The mempool is rather full, so their pre-negotiated\n>>>> commitment transactions' feerates would not be considered high\n>>>> priority by miners. Bob broadcasts his commitment transaction and\n>>>> attaches a very large child (100KvB with 100,000sat in fees) to his\n>>>> anchor output. Alice broadcasts her commitment transaction with a\n>>>> fee-bumping child (200vB with 50,000sat fees which is a generous\n>>>> 250sat/vB), but this does not meet the absolute fee requirement. She\n>>>> would need to add another 50,000sat to replace Bob's commitment\n>>>> transaction.\n>>>>\n>>>> Disallowing new unconfirmed inputs (Rule #2) in Package RBF would be\n>>>> broken for packages containing transactions already in the mempool,\n>>>> explained [here][7].\n>>>>\n>>>> Note: I originally [proposed][6] Package RBF using the same Rule #3\n>>>> and #4 before I realized how significant this pinning attack is. I'm\n>>>> retracting that proposal, and a new set of Package RBF rules would\n>>>> follow from whatever the new individual RBF rules end up being.\n>>>>\n>>>> #### Same Txid Different Witness\n>>>>\n>>>> Two transactions with the same non-witness data but different\n>>>> witnesses have the same txid but different wtxid, and the same fee but\n>>>> not necessarily the same feerate. Currently, if we see a transaction\n>>>> that has the same txid as one in the mempool, we reject it as a\n>>>> duplicate, even if the feerate is much higher. It's unclear to me if\n>>>> we have a very strong reason to change this, but noting it as a\n>>>> limitation of our current replacement policy. See [#24007][12].\n>>>>\n>>>> ### User Interface\n>>>>\n>>>> #### Using Unconfirmed UTXOs to Fund Replacements\n>>>>\n>>>> The restriction of only allowing confirmed UTXOs for funding a\n>>>> fee-bump (Rule #2) can hurt users trying to fee-bump their\n>>>> transactions and complicate wallet implementations. If the original\n>>>> transaction's output value isn't sufficient to fund a fee-bump and/or\n>>>> all of the user's other UTXOs are unconfirmed, they might not be able\n>>>> to fund a replacement transaction. Wallet developers also need to\n>>>> treat self-owned unconfirmed UTXOs as unusable for fee-bumping, which\n>>>> adds complexity to wallet logic. For example, see BDK issues [#144][4]\n>>>> and [#414][5].\n>>>>\n>>>> #### Interface Not Suitable for Coin Selection\n>>>>\n>>>> Currently, a user cannot simply create a replacement transaction\n>>>> targeting a specific feerate or meeting a minimum fee amount and\n>>>> expect to meet the RBF criteria. The fee amount depends on the size of\n>>>> the replacement transaction, and feerate is almost irrelevant.\n>>>>\n>>>> Bitcoin Core's `bumpfee` doesn't use the RBF rules when funding the\n>>>> replacement. It [estimates][13] a feerate which is \"wallet incremental\n>>>> relay fee\" (a conservative overestimation of the node's incremental\n>>>> relay fee) higher than the original transaction, selects coins for\n>>>> that feerate, and hopes that it meets the RBF rules. It never fails\n>>>> Rule #3 and #4 because it uses all original inputs and refuses to\n>>>> bump a transaction with mempool descendants.\n>>>>\n>>>> This is suboptimal, but is designed to work with the coin selection\n>>>> engine: select a feerate first, and then add fees to cover it.\n>>>> Following the exact RBF rules would require working the other way\n>>>> around: based on how much fees we've added to the transaction and its\n>>>> current size, calculate the feerate to see if we meet Rule #4.\n>>>>\n>>>> While this isn't completely broken, and the user interface is\n>>>> secondary to the safety of the mempool policy, we can do much better.\n>>>> A much more user-friendly interface would depend *only* on the\n>>>> fee and size of the original transactions.\n>>>>\n>>>> ### Updates to Mempool and Mining\n>>>>\n>>>> Since RBF was first implemented, a number of improvements have been\n>>>> made to mempool and mining logic. For example, we now use ancestor\n>>>> feerates in mining (allowing CPFP), and keep track of ancestor\n>>>> packages in the mempool.\n>>>>\n>>>> ## Ideas for Improvements\n>>>>\n>>>> ### Goals\n>>>>\n>>>> To summarize, these seem to be desired changes, in order of priority:\n>>>>\n>>>> 1. Remove Rule #3. The replacement should not be *required* to pay\n>>>> higher absolute fees.\n>>>>\n>>>> 2. Make it impossible for a replacement transaction to have a lower\n>>>> mining score than the original transaction(s). This would eliminate\n>>>> the `SIGHASH\\_ANYONECANPAY` pinning attack.\n>>>>\n>>>> 3. Remove Rule #2. Adding new unconfirmed inputs should be allowed.\n>>>>\n>>>> 4. Create a more helpful interface that helps wallet fund replacement\n>>>> transactions that aim for a feerate and fee.\n>>>>\n>>>> ### A Different Model for Fees\n>>>>\n>>>> For incentive compatibility, I believe there are different\n>>>> formulations we should consider. Most importantly, if we want to get\n>>>> rid of the absolute fee rule, we can no longer think of it as \"the\n>>>> transaction needs to pay for its own bandwidth,\" since we won't always\n>>>> be getting additional fees. That means we need a new method of\n>>>> rate-limiting replacements that doesn't require additional fees every\n>>>> time.\n>>>>\n>>>> While it makes sense to think about monetary costs when launching a\n>>>> specific type of attack, given that the fees are paid to the miner and\n>>>> not to the mempool operators, maybe it doesn't make much sense to\n>>>> think about \"paying for bandwidth\". Maybe we should implement\n>>>> transaction validation rate-limiting differently, e.g. building it\n>>>> into the P2P layer instead of the mempool policy layer.\n>>>>\n>>>> Recently, Suhas gave a [formulation][8] for incentive compatibility\n>>>> that made sense to me: \"are the fees expected to be paid in the next\n>>>> (N?) blocks higher or lower if we process this transaction?\"\n>>>>\n>>>> I started by thinking about this where N=1 or `1 + p`.\n>>>> Here, a rational miner is looking at what fees they would\n>>>> collect in the next block, and then some proportion `p` of the rest of\n>>>> the blocks based on their hashrate. We're assuming `p` isn't *so high*\n>>>> that they would be okay with lower absolute fees in the next 1 block.\n>>>> We're also assuming `p` isn't *so low* that the miner doesn't care\n>>>> about what's left of the mempool after this block.\n>>>>\n>>>> A tweak to this formulation is \"if we process this transaction, would\n>>>> the fees in the next 1 block higher or lower, and is the feerate\n>>>> density of the rest of the mempool higher or lower?\" This is pretty\n>>>> similar, where N=1, but we consider the rest of the mempool by feerate\n>>>> rather than fees.\n>>>>\n>>>> ### Mining Score of a Mempool Transaction\n>>>>\n>>>> We are often interested in finding out what\n>>>> the \"mining score\" of a transaction in the mempool is. That is, when\n>>>> the transaction is considered in block template building, what is the\n>>>> feerate it is considered at?\n>>>>\n>>>> Obviously, it's not the transaction's individual feerate. Bitcoin Core\n>>>> [mining code sorts][14] transactions by their ancestor feerate and\n>>>> includes them packages at a time, keeping track of how this affects the\n>>>> package feerates of remaining transactions in the mempool.\n>>>>\n>>>> *ancestor feerate*: Ancestor feerate is easily accessible information,\n>>>> but it's not accurate either, because it doesn't take into account the\n>>>> fact that subsets of a transaction's ancestor set can be included\n>>>> without it. For example, ancestors may have high feerates on their own\n>>>> or we may have [high feerate siblings][8].\n>>>>\n>>>> TLDR: *Looking at the current ancestor feerate of a transaction is\n>>>> insufficient to tell us what feerate it will be considered at when\n>>>> building a block template in the future.*\n>>>>\n>>>> *min(individual feerate, ancestor feerate)*: Another\n>>>> heuristic that is simple to calculate based on current mempool tooling\n>>>> is to use the [minimum of a transaction's individual score and its\n>>>> ancestor score][10] as a conservative measure. But this can\n>>>> overestimate as well (see the example below).\n>>>>\n>>>> *min ancestor feerate(tx + possible ancestor subsets)* We can also\n>>>> take the minimum of every possible ancestor subset, but this can be\n>>>> computationally expensive since there can be lots and lots of ancestor\n>>>> subsets.\n>>>>\n>>>> *max ancestor feerate(tx + possible descendant subsets)*: Another idea\n>>>> is to use the [maximum ancestor score of the transaction + each of its\n>>>> descendants][9]. This doesn't work either; it has the same blindspot\n>>>> of ancestor subsets being mined on their own.\n>>>>\n>>>> #### Mining Score Example\n>>>>\n>>>> Here's an example illustrating why mining score is tricky to\n>>>> efficiently calculate for mempool transactions:\n>>>>\n>>>> Let's say you have same-size transactions A (21sat/vB), B (1sat/vB),\n>>>> C(9sat/vB), D(5sat/vB).\n>>>> The layout is: grandparent A, parent B, and two children C and D.\n>>>>\n>>>> ```\n>>>> A\n>>>> ^\n>>>> B\n>>>> ^ ^\n>>>> C D\n>>>> ```\n>>>>\n>>>> A miner using ancestor packages to build block templates will first\n>>>> include A with a mining score of 21. Next, the miner will include B and\n>>>> C with a mining score of 6. This leaves D, with a mining score of 5.\n>>>>\n>>>> Note: in this case, mining by ancestor feerate results in the most\n>>>> rational decisions, but [a candidate set-based approach][10] which\n>>>> makes ancestor feerate much less relevant could\n>>>> be more advantageous in other situations.\n>>>>\n>>>> Here is a chart showing the \"true\" mining score alongside the values\n>>>> calculating using imperfect heuristics described above. All of them\n>>>> can overestimate or underestimate.\n>>>>\n>>>> ```\n>>>> A B C D\n>>>> mining score | 21 | 6 | 6 | 5 |\n>>>> ancestor feerate | 21 | 11 | 10.3 | 9 |\n>>>> min(individual, ancestor) | 21 | 1 | 9 | 5 |\n>>>> min(tx + ancestor subsets) | 21 | 1 | 5 | 3 |\n>>>> max(tx + descendants subsets) | 21 | 9 | 9 | 5 |\n>>>>\n>>>> ```\n>>>>\n>>>> Possibly the best solution for finding the \"mining score\" of a\n>>>> transaction is to build a block template, see what feerate each\n>>>> package is included at. Perhaps at some cutoff, remaining mempool\n>>>> transactions can be estimated using some heuristic that leans\n>>>> {overestimating, underestimating} depending on the situation.\n>>>>\n>>>> Mining score seems to be relevant in multiple places: Murch and I\n>>>> recently [found][3] that it would be very important in\n>>>> \"ancestor-aware\" funding of transactions (the wallet doesn't\n>>>> incorporate ancestor fees when using unconfirmed transactions in coin\n>>>> selection, which is a bug we want to fix).\n>>>>\n>>>> In general, it would be nice to know the exact mining priority of\n>>>> one's unconfirmed transaction is. I can think of a few block/mempool\n>>>> explorers who might want to display this information for users.\n>>>>\n>>>> ### RBF Improvement Proposals\n>>>>\n>>>> After speaking to quite a few people, here are some suggestions\n>>>> for improvements that I have heard:\n>>>>\n>>>> * The ancestor score of the replacement must be {5, 10, N}% higher\n>>>> than that of every original transaction.\n>>>>\n>>>> * The ancestor score of the replacement must be 1sat/vB higher than\n>>>> that of every original transaction.\n>>>>\n>>>> * If the original transaction is in the top {0.75MvB, 1MvB} of the\n>>>> mempool, apply the current rules (absolute fees must increase and\n>>>> pay for the replacement transaction's new bandwidth). Otherwise, use a\n>>>> feerate-only rule.\n>>>>\n>>>> * If fees don't increase, the size of the replacement transaction must\n>>>> decrease by at least N%.\n>>>>\n>>>> * Rate-limit how many replacements we allow per prevout.\n>>>>\n>>>> * Rate-limit transaction validation in general, per peer.\n>>>>\n>>>> Perhaps some others on the mailing list can chime in to throw other\n>>>> ideas into the ring and/or combine some of these rules into a sensible\n>>>> policy.\n>>>>\n>>>> #### Replace by Feerate Only\n>>>>\n>>>> I don't think there's going to be a single-line feerate-based\n>>>> rule that can incorporate everything we need.\n>>>> On one hand, a feerate-only approach helps eliminate the issues\n>>>> associated with Rule #3. On the other hand, I believe the main concern\n>>>> with a feerate-only approach is how to rate limit replacements. We\n>>>> don't want to enable an attack such as:\n>>>>\n>>>> 1. Attacker broadcasts large, low-feerate transaction, and attaches a\n>>>> chain of descendants.\n>>>>\n>>>> 2. The attacker replaces the transaction with a smaller but higher\n>>>> feerate transaction, attaching a new chain of descendants.\n>>>>\n>>>> 3. Repeat 1000 times.\n>>>>\n>>>> #### Fees in Next Block and Feerate for the Rest of the Mempool\n>>>>\n>>>> Perhaps we can look at replacements like this:\n>>>>\n>>>> 1. Calculate the directly conflicting transactions and, with their\n>>>> descendants, the original transactions. Check signaling. Limit the\n>>>> total volume (e.g. can't be more than 100 total or 1MvB or something).\n>>>>\n>>>> 2. Find which original transactions would be in the next ~1 block. The\n>>>> replacement must pay at least this amount + X% in absolute fees. This\n>>>> guarantees that the fees of the next block doesn't decrease.\n>>>>\n>>>> 3. Find which transactions would be left in the mempool after that ~1\n>>>> block. The replacement's feerate must be Y% higher than the maximum\n>>>> mining score of these transactions. This guarantees that you now have\n>>>> only *better* candidates in your after-this-block mempool than you did\n>>>> before, even if the size and fees the transactions decrease.\n>>>>\n>>>> 4. Now you have two numbers: a minimum absolute fee amount and a\n>>>> minimum feerate. Check to see if the replacement(s) meet these\n>>>> minimums. Also, a wallet would be able to ask the node \"What fee and\n>>>> feerate would I need to put on a transaction replacing this?\" and use\n>>>> this information to fund a replacement transaction, without needing to\n>>>> guess or overshoot.\n>>>>\n>>>> Obviously, there are some magic numbers missing here. X and Y are\n>>>> TBD constants to ensure we have some kind of rate limiting for the\n>>>> number of replacements allowed using some set of fees.\n>>>>\n>>>> What should they be? We can do some arithmetic to see what happens if\n>>>> you start with the biggest/lowest feerate transaction and do a bunch\n>>>> of replacements. Maybe we end up with values that are high enough to\n>>>> prevent abuse and make sense for applications/users that do RBF.\n>>>>\n>>>> ### Mempool Changes Need for Implementation\n>>>>\n>>>> As described in the mining score section above,\n>>>> we may want additional tooling to more accurately assess\n>>>> the economic gain of replacing transactions in our mempool.\n>>>>\n>>>> A few options have been discussed:\n>>>>\n>>>> * Calculate block templates on the fly when we need to consider a\n>>>> replacement. However, since replacements are [quite common][11]\n>>>> and the information might be useful for other things as well,\n>>>> it may be worth it to cache a block template.\n>>>>\n>>>> * Keep a persistent block template so that we know what transactions\n>>>> we would put in the next block. We need to remember the feerate\n>>>> at which each transaction was included in the template, because an\n>>>> ancestor package may be included in the same block template in\n>>>> multiple subsets. Transactions included earlier alter the ancestor\n>>>> feerate of the remaining transactions in the package. We also need\n>>>> to keep track of the new feerates of transactions left over.\n>>>>\n>>>> * Divide the mempool into two layers, \"high feerate\" and \"low\n>>>> feerate.\" The high feerate layer contains ~1 block of packages with\n>>>> the highest ancestor feerates, and the low feerate layer contains\n>>>> everything else. At the edge of a block, we have a Knapsacky problem\n>>>> where the next highest ancestor feerate package might not fit, so we\n>>>> would probably want the high feerate layer ~2MvB or something to avoid\n>>>> underestimating the fees.\n>>>>\n>>>> ## Acknowledgements\n>>>>\n>>>> Thank you to everyone whose RBF-related suggestions, grievances,\n>>>> criticisms and ideas were incorporated in this document:\n>>>> Andrew Chow, Matt Corallo, Suhas Daftuar, Christian Decker,\n>>>> Mark Erhardt, Lloyd Fournier, Lisa Neigut, John Newbery,\n>>>> Antoine Poinsot, Antoine Riard, Larry Ruane,\n>>>> S3RK and Bastien Teinturier.\n>>>>\n>>>> Thanks for reading!\n>>>>\n>>>> Best,\n>>>> Gloria\n>>>>\n>>>> [1]:\n>>>> https://github.com/bitcoin/bitcoin/blob/master/doc/policy/mempool-replacements.md\n>>>> [2]:\n>>>> https://github.com/bitcoin/bitcoin/pull/23121#issuecomment-929475999\n>>>> [3]:\n>>>> https://github.com/Xekyo/bitcoin/commit/d754b0242ec69d42c570418aebf9c1335af0b8ea\n>>>> [4]: https://github.com/bitcoindevkit/bdk/issues/144\n>>>> [5]: https://github.com/bitcoindevkit/bdk/issues/414\n>>>> [6]:\n>>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019464.html\n>>>> [7]:\n>>>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a#new-unconfirmed-inputs-rule-2\n>>>> [8]:\n>>>> https://github.com/bitcoin/bitcoin/pull/23121#discussion_r777131366\n>>>> [9]:\n>>>> https://github.com/bitcoin/bitcoin/pull/22290#issuecomment-865887922\n>>>> [10]:\n>>>> https://gist.github.com/Xekyo/5cb413fe9f26dbce57abfd344ebbfaf2#file-candidate-set-based-block-building-md\n>>>> [11]:\n>>>> https://github.com/bitcoin/bitcoin/pull/22539#issuecomment-885763670\n>>>> [12]: https://github.com/bitcoin/bitcoin/pull/24007\n>>>> [13]:\n>>>> https://github.com/bitcoin/bitcoin/blob/1a369f006fd0bec373b95001ed84b480e852f191/src/wallet/feebumper.cpp#L114\n>>>> [14]:\n>>>> https://github.com/bitcoin/bitcoin/blob/cf5bb048e80d4cde8828787b266b7f5f2e3b6d7b/src/node/miner.cpp#L310-L320\n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>\n>> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220207/34141dc7/attachment-0001.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-02-08T04:58:50",
                "message_text_only": "On Mon, Feb 07, 2022 at 11:16:26AM +0000, Gloria Zhao wrote:\n> @aj:\n> > I wonder sometimes if it could be sufficient to just have a relay rate\n> > limit and prioritise by ancestor feerate though. Maybe something like:\n> > - instead of adding txs to each peers setInventoryTxToSend immediately,\n> >   set a mempool flag \"relayed=false\"\n> > - on a time delay, add the top N (by fee rate) \"relayed=false\" txs to\n> >   each peer's setInventoryTxToSend and mark them as \"relayed=true\";\n> >   calculate how much kB those txs were, and do this again after\n> >   SIZE/RATELIMIT seconds\n\n> > - don't include \"relayed=false\" txs when building blocks?\n\nThe \"?\" was me not being sure that point is a good suggestion...\n\nMiners might reasonably decide to have no rate limit, and always relay,\nand never exclude txs -- but the question then becomes is whether they\nhear about the tx at all, so rate limiting behaviour could still be a\npotential problem for whoever made the tx.\n\n> Wow cool! I think outbound tx relay size-based rate-limiting and\n> prioritizing tx relay by feerate are great ideas for preventing spammers\n> from wasting bandwidth network-wide. I agree, this would slow the low\n> feerate spam down, preventing a huge network-wide bandwidth spike. And it\n> would allow high feerate transactions to propagate as they should,\n> regardless of how busy traffic is. Combined with inbound tx request\n> rate-limiting, might this be sufficient to prevent DoS regardless of the\n> fee-based replacement policies?\n\nI think you only want to do outbound rate limits, ie, how often you send\nINV, GETDATA and TX messages? Once you receive any of those, I think\nyou have to immediately process / ignore it, you can't really sensibly\ndefer it (beyond the existing queues we have that just build up while\nwe're busy processing other things first)?\n\n> One point that I'm not 100% clear on: is it ok to prioritize the\n> transactions by ancestor feerate in this scheme? As I described in the\n> original post, this can be quite different from the actual feerate we would\n> consider a transaction in a block for. The transaction could have a high\n> feerate sibling bumping its ancestor.\n> For example, A (1sat/vB) has 2 children: B (49sat/vB) and C (5sat/vB). If\n> we just received C, it would be incorrect to give it a priority equal to\n> its ancestor feerate (3sat/vB) because if we constructed a block template\n> now, B would bump A, and C's new ancestor feerate is 5sat/vB.\n> Then, if we imagine that top N is >5sat/vB, we're not relaying C. If we\n> also exclude C when building blocks, we're missing out on good fees.\n\nI think you're right that this would be ugly. It's something of a\nspecial case:\n\n a) you really care about C getting into the next block; but\n b) you're trusting B not being replaced by a higher fee tx that\n    doesn't have A as a parent; and\n c) there's a lot of txs bidding the floor of the next block up to a\n    level in-between the ancestor fee rate of 3sat/vB and the tx fee\n    rate of 5sat/vB\n\nWithout (a), maybe you don't care about it getting to a miner quickly.\nIf your trust in (b) was misplaced, then your tx's effective fee rate\nwill drop and (because of (c)), you'll lose anyway. And if the spam ends\nup outside of (c)'s range, either the rate limiting won't take effect\n(spam's too cheap) and you'll be fine, or you'll miss out on the block\nanyway (spam's paying more than your tx rate) and you never had any hope\nof making it in.\n\nNote that we already rate limit via INVENTORY_BROADCAST_MAX /\n*_INVENTORY_BROADCAST_INTERVAL; which gets to something like 10,500 txs\nper 10 minutes for outbound connections. This would be a weight based\nrate limit instead-of/in-addition-to that, I guess.\n\nAs far as a non-ugly approach goes, I think you'd have to be smarter about\ntracking the \"effective fee rate\" than the ancestor fee rate manages;\nmaybe that's something that could fall out of Murch and Clara's candidate\nset blockbuilding ideas [0] ?\n\nPerhaps that same work would also make it possible to come up with\na better answer to \"do I care that this replacement would invalidate\nthese descendents?\"\n\n[0] https://github.com/Xekyo/blockbuilding\n\n> > - keep high-feerate evicted txs around for a while in case they get\n> >   mined by someone else to improve compact block relay, a la the\n> >   orphan pool?\n> Replaced transactions are already added to vExtraTxnForCompact :D\n\nI guess I was thinking that it's just a 100 tx LRU cache, which might\nnot be good enough?\n\nMaybe it would be more on point to have a rate limit apply only to\nreplacement transactions?\n\n> For wallets, AJ's \"All you need is for there to be *a* path that follows\n> the new relay rules and gets from your node/wallet to perhaps 10% of\n> hashpower\" makes sense to me (which would be the former).\n\nPerhaps a corollarly of that is that it's *better* to have the mempool\nacceptance rule only consider economic incentives, and have the spam\nprevention only be about \"shall I tell my peers about this?\"\n\nIf you don't have that split; then the anti-spam rules can prevent you\nfrom getting the tx in the mempool at all; whereas if you do have the\nsplit, then even if the bitcoind anti-spam rules are blocking you at\nevery turn, you can still send your tx to miners by some other route,\nand then they can add it to their mempool directly without any hassle.\n\nCheers,\naj"
            },
            {
                "author": "lisa neigut",
                "date": "2022-02-09T17:57:59",
                "message_text_only": "Changing the way that RBF works is an excellent idea. Bitcoin is overdue\nfor revisiting these rules. Thanks to @glowzo et al for kicking off the\ndiscussion.\n\nI've been thinking about RBF for a very long time[1], and it's been fun to\nsee other people's thoughts on the topics. Here's my current thinking about\nthis, and proposal for how we should update the rules.\n\n### Changing the Rules? How to Change Them\nRegarding how to change them, Bram and aj are right -- we should move to a\nmodel where transaction relay candidates are evaluated on their net\nincrease in fees per byte paid, and remove the requirement that the gross\nfee of the preceding transaction is met or exceeded.\n\nOur current ruleset is over complicated because it attempts to solve two\nproblems at once: the miner's best interests (highest possible fee take)\nand relay policy.\n\nI believe this is a mistake and the mempool should change its goals.\nInstead, the mempool relay design for RBFs should be built around 1)\nincreasing the the per-byte fees paid of a transaction and 2) providing a\nsimple policy for applications building on top of bitcoin, such that\nknowledge of the mempool is not required for successfully issuing\nrelay-able RBF transactions.\n\n(A simple \"must increase the per-byte feerate\" heuristic for RBF relay\ncandidates has the nice benefit of being easy to program to on the\napplication side, and only requires knowledge of the previous candidate\ntransaction, not the entire mempool or any previous tx's relative position\nwithin it.)\n\nFinally, when blockspace is competitive , this simple policy ensures that\nthe per-byte value of every subsequent relayed transaction increases the\nper-byte value of pending bytes for the next block. This provides a measure\nof DoS protection and ensures that every relayed byte is more valuable (to\nthe miner/network) than the last.\n\n*The only time that RBF is critical for relay is during full block periods\n-- if there's not enough transactions to fill a block, using RBF to ensure\nthat a transaction is mined in a timely manner is moot. As such, RBF rules\nshould not be concerned with low-block environments.\n\n### Mempools and Relay\nThe mempool currently serves two masters: the profit motive of the miner\nand the relay motive of a utxo holder. It is in the interest of a user to\nsend the miner a high per-byte tx, such that it might end up in the next\nblock. It is in the miner's best interest to include the highest per-byte\nset of transactions in their block template.\n\nThere is some conflation here in the current RBF policies between what is\nin the mempool and what is considered a candidate for the block template.\nIf a miner has already included a more profitable package of txs into their\nblock template than a more valuable per-byte tx that the network has\nrelayed to them, it should be the responsibility of the block template\nconstructor to reject the new proposed tx, not the nodes relaying the\ntransaction to said miner.\n\nThis is a policy that the miner can (and should) implement at the level of\nthe template construction, however.\n\nIs it the responsibility of the mempool to provide the best \"historical\"\nblock opportunity for a miner (e.g. the highest paying block given all txs\nit's ever seen)? I would say no, that the ability of a utxo owner to\nre-state the spend condition of a pending transaction is more important,\nfrom a use-case perspective, and that the mempool should concern itself\nsolely with relaying increasingly more profitable bytes to miners. Let the\nminers concern themselves with deciding what the best policy for their own\nblock construction is, and the mempool with relaying the highest value\nbytes for the network. Net-net, this will benefit everyone as it becomes\neasier for users to re-submit txs with increasingly greater feerates,\ncreating more active competition for available blockspace as more\napplications are able to include it as a feature (and it works, reliable,\nas a relay mechanism).\n\n### Packages and RBF\nPackages make the increasing per-byte rule less of a guarantee that\nincreasing the per-byte value of a single transaction will net a given\nminer more fees than including the entire package.\n\nLet's decompose this a bit. It's helpful to think of tx packages as\n'composable txs'. Basically when you consider a 'package' it is actually a\nlarge tx with sub-components, the individual txs. As a 'composed tx', you\ncan calculate the per-byte feerate of the entire set. This is the number\nthat you, as someone issuing an RBF, would need to beat in order to move\nyour tx up in the pending block queue.\n\nRBF, however, is a transaction level policy: it allows you to replace any\n*one* component of a package, or tree, with the side effect of possibly\ninvalidating other candidate txs. If the 'composed tx' (aka package) had a\nnet per-byte value that was higher than the new replacement transaction\nbecause of a leaf tx that had an outsized per-byte feerate, then it would\nbe more profitable for the miner to have mined the entire package rather\nthan the replacement.\n\nThis edge case complicates the picture for the miner. Considered from the\nviewpoint of the user issuing the RBF, however, it is far simpler. In the\nideal case, a person is issuing an RBF because the previous tx tree, even\nwith its high fee sub-component, was not a candidate for the next block.\nAnd, in some cases, increasing the sub-component's per-byte feerate will\nnot achieve the goal of moving the tx any closer to being mined. It's only\nby increasing the feerate above the present feerate of the candidate plus\ndesendents (tx package) that the transaction will advance in the queue.\n\nWhile not uncomplicated, this is a simple enough metric for a wallet to\ntrack, and does not require any knowledge of the current mempool to\neffectively surpass. It's the wallet's responsibility to track this though;\nfailure to take descendants into account when deciding on the next per-byte\nfeerate for an RBF *will* mean that your RBF will be ineffective at\nachieving the goal of getting your UTXO spent. Any wallet is incentivized\nto always provide a higher per-byte feerate than the 'composed tx' (tx and\nits descendants), so as to ensure an actual improvement in the unconfirmed\ntransaction's position in the block queue, so to speak.\n\nNote that the v2 protocol for channel opens in lightning uses an RBF\nnegotiation that adheres basically to these rules (ea transaction must have\na strictly greater per-byte feerate).\n\nWe enforce a rate of 65/64th as the required increase in feerate for each\nsubsequent channel open transaction.\n\nhttps://github.com/lightning/bolts/pull/851/files#diff-ed04ca2c673fd6aabde69389511fa9ee60cb44d6b2ef6c88b549ffaa753d6afeR1154\n\n### RBF and DoS\n\nAdmittedly, changing these rules will increase the number of times that any\nUTXO is eligible to be retransmitted (relayed) via the bitcoin node\nnetwork. Strictly increasing the per-byte feerate however ensures that this\nre-relay is increasingly more expensive to the UTXO owner, however.\n\n\n### in exitus\nThese are the things I've been thinking about with regards to RBF. I hope\nthey can help to highlight the challenges in the RBF design space a bit\nmore clearly, as well as spelling out the case for using a simple heuristic\nsuch as \"solely increasing per-byte feerate\" as a good candidate for the\nrevised RBF policy.\n\n~niftynei\n\n[1]\nhttps://basicbitch.software/posts/2018-12-27-Explaining-Replace-By-Fee.html\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220209/c2920d26/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Improving RBF Policy",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Michael Folkson",
                "Anthony Towns",
                "lisa neigut",
                "Bastien TEINTURIER",
                "Gloria Zhao",
                "Prayank"
            ],
            "messages_count": 9,
            "total_messages_chars_count": 151767
        }
    },
    {
        "title": "[bitcoin-dev] TXHASH + CHECKSIGFROMSTACKVERIFY in lieu of CTV and ANYPREVOUT",
        "thread_messages": [
            {
                "author": "Anthony Towns",
                "date": "2022-02-01T01:16:39",
                "message_text_only": "On Fri, Jan 28, 2022 at 08:56:25AM -0500, Russell O'Connor via bitcoin-dev wrote:\n> > https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-July/019243.html\n> For more complex interactions, I was imagining combining this TXHASH\n> proposal with CAT and/or rolling SHA256 opcodes.  If TXHASH ended up\n> supporting relative or absolute input/output indexes then users could\n> assemble the hashes of the particular inputs and outputs they care about\n> into a single signed message.\n\nThat's certainly possible, but it sure seems overly complicated and\nerror prone...\n\n> > While I see the appeal of this from a language design perspective;\n> > I'm not sure it's really the goal we want. When I look at bitcoin's\n> > existing script, I see a lot of basic opcodes to do simple arithmetic and\n> > manipulate the stack in various ways, but the opcodes that are actually\n> > useful are more \"do everything at once\" things like check(multi)sig or\n> > sha256. It seems like what's most useful on the blockchain is a higher\n> > level language, rather than more of blockchain assembly language made\n> > up of small generic pieces. I guess \"program their own use cases from\n> > components\" seems to be coming pretty close to \"write your own crypto\n> > algorithms\" here...\n> Which operations in Script are actually composable today?\n\n> There is one aspect of Bitcoin Script that is composable, which is\n> (monotone) boolean combinations of the few primitive transaction conditions\n> that do exist.  The miniscript language captures nearly the entirety of\n> what is composable in Bitcoin Script today: which amounts to conjunctions,\n> disjunctions (and thresholds) of signatures, locktimes, and revealing hash\n> preimages.\n\nYeah; I think miniscript captures everything bitcion script is actually\nuseful for today, and if we were designing bitcoin from scratch and\nhad known that was the feature set we were going to end up with, we'd\nhave come up with something simpler and a fair bit more high level than\nbitcoin script for the interpreter.\n\n> I don't think there is much in the way of lessons to be drawn from how we\n> see Bitcoin Script used today with regards to programs built out of\n> reusable components.\n\nI guess I think one conclusion we should draw is some modesty in how\ngood we are at creating general reusable components. That is, bitcoin\nscript looks a lot like a relatively general expression language,\nthat should allow you to write interesting things; but in practice a\nlot of it was buggy (OP_VER hardforks and resource exhaustion issues),\nor not powerful enough to actually be interesting, or too complicated\nto actually get enough use out of [0].\n\n> TXHASH + CSFSV won't be enough by itself to allow for very interesting\n> programs Bitcoin Script yet, we still need CAT and friends for that,\n\n\"CAT\" and \"CHECKSIGFROMSTACK\" are both things that have been available in\nelements for a while; has anyone managed to build anything interesting\nwith them in practice, or are they only useful for thought experiments\nand blog posts? To me, that suggests that while they're useful for\ntheoretical discussion, they don't turn out to be a good design in\npractice.\n\n> but\n> CSFSV is at least a step in that direction.  CSFSV can take arbitrary\n> messages and these messages can be fixed strings, or they can be hashes of\n> strings (that need to be revealed), or they can be hashes returned from\n> TXHASH, or they can be locktime values, or they can be values that are\n> added or subtracted from locktime values, or they can be values used for\n> thresholds, or they can be other pubkeys for delegation purposes, or they\n> can be other signatures ... for who knows what purpose.\n\nI mean, if you can't even think of a couple of uses, that doesn't seem\nvery interesting to pursue in the near term? CTV has something like half\na dozen fairly near-term use cases, but obviously those can all be done\njust with TXHASH without a need for CSFS, and likewise all the ANYPREVOUT\nthings can obviously be done via CHECKSIG without either TXHASH or CSFS...\n\nTo me, the point of having CSFS (as opposed to CHECKSIG) seems to be\nverifying that an oracle asserted something; but for really simply boolean\ndecisions, doing that via a DLC seems better in general since that moves\nmore of the work off-chain; and for the case where the signature is being\nused to authenticate input into the script rather than just gating a path,\nthat feels a bit like a weaker version of graftroot?\n\nI guess I'd still be interested in the answer to:\n\n> > If we had CTV, POP_SIGDATA, and SIGHASH_NO_TX_DATA_AT_ALL but no OP_CAT,\n> > are there any practical use cases that wouldn't be covered that having\n> > TXHASH/CAT/CHECKSIGFROMSTACK instead would allow? Or where those would\n> > be significantly more convenient/efficient?\n> > \n> > (Assume \"y x POP_SIGDATA POP_SIGDATA p CHECKSIGVERIFY q CHECKSIG\"\n> > commits to a vector [x,y] via p but does not commit to either via q so\n> > that there's some \"CAT\"-like behaviour available)\n\nTXHASH seems to me to be clearly the more flexible opcode compared to\nCTV; but maybe all that flexibility is wasted, and all the real use\ncases actually just want CHECKSIG or CTV? I'd feel much better having\nsome idea of what the advantage of being flexible there is...\n\n\nBut all that aside, probably the real question is can we simplify CTV's\ntransaction message algorithm, if we assume APO is enabled simultaneously?\nIf it doesn't get simplified and needs its own hashing algorithm anyway,\nthat would be probably be a good reason to keep the separate.\n\nFirst, since ANYPREVOUT commits to the scriptPubKey, you'd need to use\nANYPREVOUTANYSCRIPT for CTV-like behaviour.\n\nANYPRVOUTANYSCRIPT is specced as commiting to:\n  nVersion\n  nLockTime\n  nSequence\n  spend_type and annex present\n  sha_annex (if present)\n  sha_outputs (ALL) or sha_single_output (SINGLE)\n  key_version\n  codesep_pos\n\nCTV commits to:\n  nVersion\n  nLockTime\n  scriptSig hash \"(maybe!)\"\n  input count\n  sequences hash\n  output count\n  outputs hash\n  input index\n\n(CTV thus allows annex malleability, since it neither commits to the\nannex nor forbids inclusion of an annex)\n\n\"output count\" and \"outputs index\" would both be covered by sha_outputs\nwith ANYPREVOUTANYSCRIPT|ALL.\n\nI think \"scriptSig hash\" is only covered to avoid txid malleability; but\njust adjusting your protocol to use APO signatures instead of relying on\nthe txid of future transactions also solves that problem.\n\nI believe \"sequences hash\", \"input count\" and \"input index\" are all an\nimportant part of ensuring that if you have two UTXOs distributing 0.42\nBTC to the same set of addresses via CTV, that you can't combine them in a\nsingle transaction and end up sending losing one of the UTXOs to fees. I\ndon't believe there's a way to resolve that with bip 118 alone, however\nthat does seem to be a similar problem to the one that SIGHASH_GROUP\ntries to solve.\n\nSIGHASH_GROUP [1] would be an alternative to ALL/SINGLE/NONE, with the exact\ngroup of outputs being committed to determined via the annex.\nANYPREVOUTANYSCRIPT|GROUP would commit to:\n\n  nVersion\n  nLockTime\n  nSequence\n  spend_type and annex present\n  sha_annex (if present)\n  sha_group_outputs (GROUP)\n  key_version\n  codesep_pos\n\nSo in that case if you have your two inputs:\n\n  0.42 [pays 0.21 to A, 0.10 to B, 0.10 to C]\n  0.42 [pays 0.21 to A, 0.10 to B, 0.10 to C]\n\nthen, either:\n\n  a) if they're both committed with GROUP and sig_group_count = 3, then\n     the outputs must be [0.21 A, 0.10 B, 0.10 C, 0.21 A, 0.10 B, 0.10\n     C], and you don't lose funds\n\n  b) if they're both committed with GROUP and the first is\n     sig_group_count=3 and the second is sig_group_count=0, then the\n     outputs can be [0.21 A, 0.10 B, 0.10 C, *anything] -- but in that\n     case the second input is already signalling that it's meant to be\n     paired with another input to fund the same three outputs, so any\n     funds loss is at least intentional\n\nNote that this means txids are very unstable: if a tx is only protected\nby SIGHASH_GROUP commitments then miners/relayers can add outputs, or\nreorganise the groups without making the tx invalid. Beyond requiring\nthe signatures to be APO/APOAS-based to deal with that, we'd also need\nto avoid txs getting rbf-pinned by some malicious third party who pulls\napart the groups and assembles a new tx that's hard to rbf but also\nunlikely to confirm due to having a low feerate.\n\nNote also that not reusing addresses solves this case -- it's only a\nproblem when you're paying the same amounts to the same addresses.\n\nBeing able to combine additional inputs and outputs at a later date\n(which necessarily changes the txid) is an advantage though: it lets\nyou add additional funds and claim change, which allows you to adjust\nto different fee rates.\n\nI don't think the SIGHASH_GROUP approach would work very well without\naccess to the annex, ie if you're trying to do CTV encoded either in a\nplain scriptPubKey or via segwit/p2sh.\n\nI think that would give 16 different sighashes, choosing one of four\noptions for outputs,\n\n ALL/NONE/SINGLE/GROUP\n   -- which outputs are committed to\n\nand one of four options for inputs,\n\n -/ANYONECANPAY/ANYPREVOUT/ANYPREVOUTANYSCRIPT\n   -- all inputs committed to, specific input committed to,\n      scriptpubkey/tapscript committed to, or just the\n      nseq/annex/codesep_pos\n\nvs the ~155,000 sighashes in the TXHASH proposal.\n\nI don't think there's an efficient way of doing SIGHASH_GROUP via tx\nintrospection opcodes that doesn't also introduce a quadratic hashing\nrisk -- you need to prevent different inputs from re-hashing distinct but\noverlapping sets of outputs, and if your opcodes only allow grabbing one\noutput at a time to add to the message being signed you have to do a lot\nof coding if you want to let the signer choose how many outputs to commit\nto; if you provide an opcode that grabs man outputs to hash, it seems\nhard to do that generically in a way that avoids quadratic behaviour.\n\nSo I think that suggests two alternative approaches, beyond the\nVERIFY-vs-PUSH semantic:\n\n - have a dedicated sighash type for CTV (either an explicit one for it,\n   per bip119, or support thousands of options like the proposal in this\n   thread, one of which happens to be about the same as the bip119 idea)\n\n - use ANYPREVOUTANYSCRIPT|GROUP for CTV, which means also implementing\n   annex parsing and better RBF behaviour to avoid those txs being\n   excessively vulnerable to pinning; with the advantage being that\n   txs using \"GROUP\" sigs can be combined either for batching purposes\n   or for adapting to the fee market after the signature has been made,\n   and the disadvantage that you can't rely on stable txids when looking\n   for CTV spends and have to continue using APO/APOAS when chaining\n   signatures on top of unconfirmed CTV outputs\n\nCheers,\naj\n\n[0] Here's bitmatrix trying to multiply two numbers together:\n     https://medium.com/bit-matrix/technical-how-does-bitmatrix-v1-multiply-two-integers-in-the-absence-of-op-mul-a58b7a3794a3\n\n    Likewise, doing a point preimage reveal via clever scripting\n    pre-taproot never saw an implementation, despite seeming\n    theoretically plausible.\n     https://lists.linuxfoundation.org/pipermail/lightning-dev/2015-November/000344.html\n\n[1] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-July/019243.html"
            },
            {
                "author": "Russell O'Connor",
                "date": "2022-02-08T02:16:10",
                "message_text_only": "On Mon, Jan 31, 2022 at 8:16 PM Anthony Towns <aj at erisian.com.au> wrote:\n\n> On Fri, Jan 28, 2022 at 08:56:25AM -0500, Russell O'Connor via bitcoin-dev\n> wrote:\n> > >\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-July/019243.html\n> > For more complex interactions, I was imagining combining this TXHASH\n> > proposal with CAT and/or rolling SHA256 opcodes.  If TXHASH ended up\n> > supporting relative or absolute input/output indexes then users could\n> > assemble the hashes of the particular inputs and outputs they care about\n> > into a single signed message.\n>\n> That's certainly possible, but it sure seems overly complicated and\n> error prone...\n>\n\nIndeed, and we really want something that can be programmed at redemption\ntime.\nThat probably involves something like how the historic MULTISIG worked by\nhaving list of input / output indexes be passed in along with length\narguments.\n\nI don't think there will be problems with quadratic hashing here because as\nmore inputs are list, the witness in turns grows larger itself.  The amount\nof stack elements that can be copied is limited by a constant (3DUP).\nCertainly care is needed here, but also keep in mind that an OP_HASH256\ndoes a double hash and costs one weight unit.\n\nThat said, your SIGHASH_GROUP proposal suggests that some sort of\nintra-input communication is really needed, and that is something I would\nneed to think about.\n\nWhile normally I'd be hesitant about this sort of feature creep, when we\nare talking about doing soft-forks, I really think it makes sense to think\nthrough these sorts of issues (as we are doing here).\n\n\n> > I don't think there is much in the way of lessons to be drawn from how we\n> > see Bitcoin Script used today with regards to programs built out of\n> > reusable components.\n>\n> I guess I think one conclusion we should draw is some modesty in how\n> good we are at creating general reusable components. That is, bitcoin\n> script looks a lot like a relatively general expression language,\n> that should allow you to write interesting things; but in practice a\n> lot of it was buggy (OP_VER hardforks and resource exhaustion issues),\n> or not powerful enough to actually be interesting, or too complicated\n> to actually get enough use out of [0].\n>\n\n> TXHASH + CSFSV won't be enough by itself to allow for very interesting\n> > programs Bitcoin Script yet, we still need CAT and friends for that,\n>\n> \"CAT\" and \"CHECKSIGFROMSTACK\" are both things that have been available in\n> elements for a while; has anyone managed to build anything interesting\n> with them in practice, or are they only useful for thought experiments\n> and blog posts? To me, that suggests that while they're useful for\n> theoretical discussion, they don't turn out to be a good design in\n> practice.\n>\n\nPerhaps the lesson to be drawn is that languages should support multiplying\ntwo numbers together.\n\nHaving 2/3rd of the language you need to write interesting programs doesn't\nmean that you get 2/3rd of the interesting programs written.\n\nBut beyond that, there is a lot more to a smart contract than just the\nScript.  Dmitry Petukhov has a fleshed out design for Asset based lending\non liquid at https://ruggedbytes.com/articles/ll/, despite the limitations\nof (pre-taproot) Elements Script.  But to make it a real thing you need\ninfrastructure for working with partial transactions, key management, etc.\n\n> but\n> > CSFSV is at least a step in that direction.  CSFSV can take arbitrary\n> > messages and these messages can be fixed strings, or they can be hashes\n> of\n> > strings (that need to be revealed), or they can be hashes returned from\n> > TXHASH, or they can be locktime values, or they can be values that are\n> > added or subtracted from locktime values, or they can be values used for\n> > thresholds, or they can be other pubkeys for delegation purposes, or they\n> > can be other signatures ... for who knows what purpose.\n>\n> I mean, if you can't even think of a couple of uses, that doesn't seem\n> very interesting to pursue in the near term? CTV has something like half\n> a dozen fairly near-term use cases, but obviously those can all be done\n> just with TXHASH without a need for CSFS, and likewise all the ANYPREVOUT\n> things can obviously be done via CHECKSIG without either TXHASH or CSFS...\n>\n> To me, the point of having CSFS (as opposed to CHECKSIG) seems to be\n> verifying that an oracle asserted something; but for really simply boolean\n> decisions, doing that via a DLC seems better in general since that moves\n> more of the work off-chain; and for the case where the signature is being\n> used to authenticate input into the script rather than just gating a path,\n> that feels a bit like a weaker version of graftroot?\n>\n\nI didn't really mean this as a list of applications; it was a list of\nvalues that CSFSV composes with. Applications include delegation of pubkeys\nand oracles, and, in the presence of CAT and transaction reflection\nprimitives, presumably many more things.\n\n\n> I guess I'd still be interested in the answer to:\n>\n> > > If we had CTV, POP_SIGDATA, and SIGHASH_NO_TX_DATA_AT_ALL but no\n> OP_CAT,\n> > > are there any practical use cases that wouldn't be covered that having\n> > > TXHASH/CAT/CHECKSIGFROMSTACK instead would allow? Or where those would\n> > > be significantly more convenient/efficient?\n> > >\n> > > (Assume \"y x POP_SIGDATA POP_SIGDATA p CHECKSIGVERIFY q CHECKSIG\"\n> > > commits to a vector [x,y] via p but does not commit to either via q so\n> > > that there's some \"CAT\"-like behaviour available)\n>\n\nI don't know if this is the answer you are looking for, but technically\nTXHASH + CAT + SHA256 awkwardly gives you limited transaction reflection.\nIn fact, you might not even need TXHASH, though it certainly helps.\n\n\n> TXHASH seems to me to be clearly the more flexible opcode compared to\n> CTV; but maybe all that flexibility is wasted, and all the real use\n> cases actually just want CHECKSIG or CTV? I'd feel much better having\n> some idea of what the advantage of being flexible there is...\n>\n\nThe flexibility of TXHASH is intended to head off the need for future soft\nforks.  If we had specific applications in mind, we could simply set up the\ntransaction hash flags to cover all the applications we know about.  But it\nis the applications that we don't know about that worry me.  If we don't\nput options in place with this soft-fork proposal, then they will need\ntheir own soft-fork down the line; and the next application after that, and\nso on.\n\nIf our attitude is to craft our soft-forks as narrowly as possible to limit\nthem to what only allows for given tasks, then we are going to end up\nneeding a lot more soft-forks, and that is not a good outcome.\n\nBut all that aside, probably the real question is can we simplify CTV's\n> transaction message algorithm, if we assume APO is enabled simultaneously?\n> If it doesn't get simplified and needs its own hashing algorithm anyway,\n> that would be probably be a good reason to keep the separate.\n>\n> First, since ANYPREVOUT commits to the scriptPubKey, you'd need to use\n> ANYPREVOUTANYSCRIPT for CTV-like behaviour.\n>\n> ANYPRVOUTANYSCRIPT is specced as commiting to:\n>   nVersion\n>   nLockTime\n>   nSequence\n>   spend_type and annex present\n>   sha_annex (if present)\n>   sha_outputs (ALL) or sha_single_output (SINGLE)\n>   key_version\n>   codesep_pos\n>\n> CTV commits to:\n>   nVersion\n>   nLockTime\n>   scriptSig hash \"(maybe!)\"\n>   input count\n>   sequences hash\n>   output count\n>   outputs hash\n>   input index\n>\n> (CTV thus allows annex malleability, since it neither commits to the\n> annex nor forbids inclusion of an annex)\n>\n> \"output count\" and \"outputs index\" would both be covered by sha_outputs\n> with ANYPREVOUTANYSCRIPT|ALL.\n>\n> I think \"scriptSig hash\" is only covered to avoid txid malleability; but\n> just adjusting your protocol to use APO signatures instead of relying on\n> the txid of future transactions also solves that problem.\n>\n> I believe \"sequences hash\", \"input count\" and \"input index\" are all an\n> important part of ensuring that if you have two UTXOs distributing 0.42\n> BTC to the same set of addresses via CTV, that you can't combine them in a\n> single transaction and end up sending losing one of the UTXOs to fees. I\n> don't believe there's a way to resolve that with bip 118 alone, however\n> that does seem to be a similar problem to the one that SIGHASH_GROUP\n> tries to solve.\n>\n\nIt was my understanding that it is only \"input count = 1\" that prevents\nthis issue.\n\nSIGHASH_GROUP [1] would be an alternative to ALL/SINGLE/NONE, with the exact\n> group of outputs being committed to determined via the annex.\n> ANYPREVOUTANYSCRIPT|GROUP would commit to:\n>\n>   nVersion\n>   nLockTime\n>   nSequence\n>   spend_type and annex present\n>   sha_annex (if present)\n>   sha_group_outputs (GROUP)\n>   key_version\n>   codesep_pos\n>\n> So in that case if you have your two inputs:\n>\n>   0.42 [pays 0.21 to A, 0.10 to B, 0.10 to C]\n>   0.42 [pays 0.21 to A, 0.10 to B, 0.10 to C]\n>\n> then, either:\n>\n>   a) if they're both committed with GROUP and sig_group_count = 3, then\n>      the outputs must be [0.21 A, 0.10 B, 0.10 C, 0.21 A, 0.10 B, 0.10\n>      C], and you don't lose funds\n>\n>   b) if they're both committed with GROUP and the first is\n>      sig_group_count=3 and the second is sig_group_count=0, then the\n>      outputs can be [0.21 A, 0.10 B, 0.10 C, *anything] -- but in that\n>      case the second input is already signalling that it's meant to be\n>      paired with another input to fund the same three outputs, so any\n>      funds loss is at least intentional\n>\n> Note that this means txids are very unstable: if a tx is only protected\n> by SIGHASH_GROUP commitments then miners/relayers can add outputs, or\n> reorganise the groups without making the tx invalid. Beyond requiring\n> the signatures to be APO/APOAS-based to deal with that, we'd also need\n> to avoid txs getting rbf-pinned by some malicious third party who pulls\n> apart the groups and assembles a new tx that's hard to rbf but also\n> unlikely to confirm due to having a low feerate.\n>\n> Note also that not reusing addresses solves this case -- it's only a\n> problem when you're paying the same amounts to the same addresses.\n>\n> Being able to combine additional inputs and outputs at a later date\n> (which necessarily changes the txid) is an advantage though: it lets\n> you add additional funds and claim change, which allows you to adjust\n> to different fee rates.\n>\n> I don't think the SIGHASH_GROUP approach would work very well without\n> access to the annex, ie if you're trying to do CTV encoded either in a\n> plain scriptPubKey or via segwit/p2sh.\n>\n> I think that would give 16 different sighashes, choosing one of four\n> options for outputs,\n>\n>  ALL/NONE/SINGLE/GROUP\n>    -- which outputs are committed to\n>\n> and one of four options for inputs,\n>\n>  -/ANYONECANPAY/ANYPREVOUT/ANYPREVOUTANYSCRIPT\n>    -- all inputs committed to, specific input committed to,\n>       scriptpubkey/tapscript committed to, or just the\n>       nseq/annex/codesep_pos\n>\n> vs the ~155,000 sighashes in the TXHASH proposal.\n>\n> I don't think there's an efficient way of doing SIGHASH_GROUP via tx\n> introspection opcodes that doesn't also introduce a quadratic hashing\n> risk -- you need to prevent different inputs from re-hashing distinct but\n> overlapping sets of outputs, and if your opcodes only allow grabbing one\n> output at a time to add to the message being signed you have to do a lot\n> of coding if you want to let the signer choose how many outputs to commit\n> to; if you provide an opcode that grabs man outputs to hash, it seems\n> hard to do that generically in a way that avoids quadratic behaviour.\n>\n> So I think that suggests two alternative approaches, beyond the\n> VERIFY-vs-PUSH semantic:\n>\n>  - have a dedicated sighash type for CTV (either an explicit one for it,\n>    per bip119, or support thousands of options like the proposal in this\n>    thread, one of which happens to be about the same as the bip119 idea)\n>\n>  - use ANYPREVOUTANYSCRIPT|GROUP for CTV, which means also implementing\n>    annex parsing and better RBF behaviour to avoid those txs being\n>    excessively vulnerable to pinning; with the advantage being that\n>    txs using \"GROUP\" sigs can be combined either for batching purposes\n>    or for adapting to the fee market after the signature has been made,\n>    and the disadvantage that you can't rely on stable txids when looking\n>    for CTV spends and have to continue using APO/APOAS when chaining\n>    signatures on top of unconfirmed CTV outputs\n>\n> Cheers,\n> aj\n>\n> [0] Here's bitmatrix trying to multiply two numbers together:\n>\n> https://medium.com/bit-matrix/technical-how-does-bitmatrix-v1-multiply-two-integers-in-the-absence-of-op-mul-a58b7a3794a3\n>\n>     Likewise, doing a point preimage reveal via clever scripting\n>     pre-taproot never saw an implementation, despite seeming\n>     theoretically plausible.\n>\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2015-November/000344.html\n>\n> [1]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-July/019243.html\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220207/848ec59e/attachment-0001.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-02-17T14:27:27",
                "message_text_only": "On Mon, Feb 07, 2022 at 09:16:10PM -0500, Russell O'Connor via bitcoin-dev wrote:\n> > > For more complex interactions, I was imagining combining this TXHASH\n> > > proposal with CAT and/or rolling SHA256 opcodes.\n> Indeed, and we really want something that can be programmed at redemption\n> time.\n\nI mean, ideally we'd want something that can be flexibly programmed at\nredemption time, in a way that requires very few bytes to express the\ncommon use cases, is very efficient to execute even if used maliciously,\nis hard to misuse accidently, and can be cleanly upgraded via soft fork\nin the future if needed?\n\nThat feels like it's probably got a \"fast, cheap, good\" paradox buried\nin there, but even if it doesn't, it doesn't seem like something you\ncan really achieve by tweaking around the edges?\n\n> That probably involves something like how the historic MULTISIG worked by\n> having list of input / output indexes be passed in along with length\n> arguments.\n> \n> I don't think there will be problems with quadratic hashing here because as\n> more inputs are list, the witness in turns grows larger itself.\n\nIf you cache the hash of each input/output, it would mean each byte of\nthe witness would be hashing at most an extra 32 bytes of data pulled\nfrom that cache, so I think you're right. Three bytes of \"script\" can\nalready cause you to rehash an additional ~500 bytes (DUP SHA256 DROP),\nso that should be within the existing computation-vs-weight relationship.\n\nIf you add the ability to hash a chosen output (as Rusty suggests, and\nwhich would allow you to simulate SIGHASH_GROUP), your probably have to\nincrease your cache to cover each outputs' scriptPubKey simultaneously,\nwhich might be annoying, but doesn't seem fatal.\n\n> That said, your SIGHASH_GROUP proposal suggests that some sort of\n> intra-input communication is really needed, and that is something I would\n> need to think about.\n\nI think the way to look at it is that it trades off spending an extra\nwitness byte or three per output (your way, give or take) vs only being\nable to combine transactions in limited ways (sighash_group), but being\nable to be more optimised than the more manual approach.\n\nThat's a fine tradeoff to make for something that's common -- you\nsave onchain data, make something easier to use, and can optimise the\nimplementation so that it handles the common case more efficiently.\n\n(That's a bit of a \"premature optimisation\" thing though -- we can't\ncurrently do SIGHASH_GROUP style things, so how can you sensibly justify\noptimising it because it's common, when it's not only currently not\ncommon, but also not possible? That seems to me a convincing reason to\nmake script more expressive)\n\n> While normally I'd be hesitant about this sort of feature creep, when we\n> are talking about doing soft-forks, I really think it makes sense to think\n> through these sorts of issues (as we are doing here).\n\n+1\n\nI guess I especially appreciate your goodwill here, because this has\nsure turned out to be a pretty long message as I think some of these\nthings through out loud :)\n\n> > \"CAT\" and \"CHECKSIGFROMSTACK\" are both things that have been available in\n> > elements for a while; has anyone managed to build anything interesting\n> > with them in practice, or are they only useful for thought experiments\n> > and blog posts? To me, that suggests that while they're useful for\n> > theoretical discussion, they don't turn out to be a good design in\n> > practice.\n> Perhaps the lesson to be drawn is that languages should support multiplying\n> two numbers together.\n\nWell, then you get to the question of whether that's enough, or if\nyou need to be able to multiply bignums together, etc? \n\nI was looking at uniswap-like things on liquid, and wanted to do constant\nproduct for multiple assets -- but you already get the problem that \"x*y\n< k\" might overflow if the output values x and y are ~50 bits each, and\nthat gets worse with three assets and wanting to calculate \"x*y*z < k\",\netc. And really you'd rather calculate \"a*log(x) + b*log(y) + c*log(z)\n< k\" instead, which then means implementing fixed point log in script...\n\n> Having 2/3rd of the language you need to write interesting programs doesn't\n> mean that you get 2/3rd of the interesting programs written.\n\nI guess to abuse that analogy: I think you're saying something like\nwe've currently got 67% of an ideal programming language, and CTV\nwould give us 68%, but that would only take us from 10% to 11% of the\ninteresting programs. I agree txhash might bump that up to, say, 69%\n(nice) but I'm not super convinced that even moves us from 11% to 12%\nof interesting programs, let alone a qualitative leap to 50% or 70%\nof interesting programs.\n\nIt's *possible* that the ideal combination of opcodes will turn out to\nbe CAT, TXHASH, CHECKSIGFROMSTACK, MUL64LE, etc, but it feels like it'd\nbe better working something out that fits together well, rather than\nadding things piecemeal and hoping we don't spend all that effort to\nend up in a local optimum that's a long way short of a global optimum?\n\n[rearranged:]\n\n> The flexibility of TXHASH is intended to head off the need for future soft\n> forks.  If we had specific applications in mind, we could simply set up the\n> transaction hash flags to cover all the applications we know about.  But it\n> is the applications that we don't know about that worry me.  If we don't\n> put options in place with this soft-fork proposal, then they will need\n> their own soft-fork down the line; and the next application after that, and\n> so on.\n> \n> If our attitude is to craft our soft-forks as narrowly as possible to limit\n> them to what only allows for given tasks, then we are going to end up\n> needing a lot more soft-forks, and that is not a good outcome.\n\nI guess I'm not super convinced that we're anywhere near the right level\nof generality that this would help in avoiding future soft forks? That's\nwhat I meant by it not covering SIGHASH_GROUP.\n\nI guess the model I have in my head, is that what we should ideally\nhave a general/flexible/expressive but expensive way of doing whatever\nscripting you like (so a \"SIMPLICITY_EXEC\" opcode, perhaps), but then,\nas new ideas get discovered and widely deployed, we should then make them\neasy and cheap to use (whether that's deploying a \"jet\" for the simplicity\ncode, or a dedicated opcode, or something else), but \"cheap to use\"\nmeans defining a new cost function (or defining new execution conditions\nfor something that was already cheaper than the cheapest existing way\nof encoding those execution conditions), which is itself a soft fork\nsince to make it \"cheaper\" means being able to fit more transactions\nusing that feature into a block than was previously possible..\n\nBut even then, based on [0], pure simplicity code to verify a signature\napparently takes 11 minutes, so that code probably should cost 66M vbytes\n(based on a max time to verify a block of 10 seconds), which would\nmake it obviously unusable as a bitcoin tx with their normal 100k vbyte\nlimit... Presumably an initial simplicity deployment would come with a\nbunch of jets baked in so that's less of an issue in practice... \n\nBut I think that means that even with simplicity you couldn't experiment\nwith alternative ECC curves or zero knowledge stuff without a soft fork\nto make the specific setup fast and cheap, first.\n\n[0] https://medium.com/blockstream/simplicity-jets-release-803db10fd589\n\n(I think this approach would already be an improvement in how we do soft\nforks, though: (1) for many things, you would already have on-chain\nevidence that this is something that's worthwhile, because people are\npaying high fees to do it via hand-coded simplicity, so there's no\nquestion of whether it will be used; (2) you can prove the jet and the\nsimplicity code do the exact same thing (and have unit/fuzz tests to\nverify it), so can be more confident that the implementation is correct;\n(3) maybe it's easier to describe in a bip that way too, since you can\njust reference the simplicity code it's replacing rather than having\nC++ code?)\n\nThat still probably doesn't cover every experiment you might want to do;\neg if you wanted to have your tx commit to a prior block hash, you'd\npresumably need a soft fork to expose that data; and if you wanted to\nextend the information about the utxo being spent (eg a parity bit for\nthe internal public key to make recursive TLUV work better) you'd need a\nsoft fork for that too.\n\n\nI guess a purist approach to generalising sighashes might look something\nlike:\n\n   [s] [shimplicity] DUP EXEC p CHECKSIGFROMSTACK\n\nwhere both s and shimplicity (== sighash + simplicity or shim + simplicity\n:) are provided by the signer, with s being a signature, and shimplicity\nbeing a simplicity script that builds a 32 byte message based on whatever\nbits of the transaction it chooses as well as the shimplicity script\nitself to prevent malleability.\n\nBut writing a shimplicity script all the time is annoying, so adding an\nextra opcode to avoid that makes sense, reducing it to:\n\n   [s] [sh] TXHASH p CHECKIGFROMSTACK\n\nwhich is then equivalent to the exisitng\n\n   [s|sh] p CHECKSIG\n\nThough in that case, wouldn't you just have \"txhash(sh)\" be your\nshimplicity script (in which case txhash is a jet rather than an opcode),\nand keep the program as \"DUP EXEC p CHECKSIGFROMSTACK\", which then gives\nthe signer maximum flexibility to either use a standard sighash, or\nwrite special code to do something new and magic?\n\nSo I think I'm 100% convinced that a (simplified) TXHASH makes sense in\na world where we have simplicity-equivalent scripting (and where there's\n*also* some more direct introspection functionality like Rusty's OP_TX\nor elements' tapscript opcodes or whatever).\n\n(I don't think there's much advantage of a TaggedHash opcode that\ntakes the tag as a parameter over just writing \"SHA256 DUP CAT SWAP CAT\nSHA256\", and if you were going to have a \"HASH_TapSighash\" opcode it\nprobably should be limited to hashing the same things from the bip that\ndefines it anyway. So having two simplicity functions, one for bip340\n(checksigfromstack) and one for bip342 (generating a signature message\nfor the current transaction) seems about ideal)\n\nBut, I guess that brings me back to more or less what Jeremy asked\nearlier in this thread:\n\n] Does it make \"more sense\" to invest the research and development effort\n] that would go into proving TXHASH safe, for example, into Simplicity\n] instead?\n\nShould we be trying to gradually turn script into a more flexible\nlanguage, one opcode at a time -- going from 11% to 12% to 13.4% to\n14.1% etc of coverage of interesting programs -- or should we invest\nthat time/effort into working on simplicity (or something like chialisp\nor similar) instead? That is, something where we could actually evaluate\nhow all the improved pieces fit together rather than guessing how it might\nwork if we maybe in future add CAT or 64 bit maths or something else...\n\nIf we put all our \"language design\" efforts into simplicity/whatever,\nwe could leave script as more of a \"macro\" language than a programming\none; that is, focus on it being an easy, cheap, safe way of doing the\nmost common things. I think that would still be worthwhile, both before\nand after simplicity/* is available?\n\nI think my opinions are:\n\n * recursive covenants are not a problem; avoiding them isn't and\n   shouldn't be a design goal; and trying to prevent other people using\n   them is wasted effort\n\n * having a language redesign is worthwhile -- there are plenty of ways\n   to improve script, and there's enough other blockchain languages out\n   there by now that we ought be able to avoid a \"second system effect\"\n   disaster\n\n * CTV via legacy script saves ~17 vbytes compared to going via\n   tapscript (since the CTV hash is already in the scriptPubKey and the\n   internal pubkey isn't needed, so neither need to be revealed to spend)\n   and avoids the taproot ECC equation check, at the cost of using up\n   an OP_NOP opcode. That seems worthwhile to me. Comparatively, TXHASH\n   saves ~8 vbytes compared to emulating it with CTV (because you don't\n   have to supply an unacceptable hash on demand). So having both may be\n   worthwhile, but if we only have one, CTV seems the bigger saving? And\n   we're not wasting an opcode if we do CTV now and add TXHASH later,\n   since we TXHASH isn't NOP-compatible and can't be included in legacy\n   script anyway.\n\n * TXHASH's \"PUSH\" behaviour vs CTV's \"examine the stack but don't\n   change it, and VERIFY\" behaviour is independent of the question of \n   if we want to supply flags to CTV/TXHASH so they're more flexible\n\nAnd perhaps less strongly:\n\n * I don't like the ~18 TXHASH flags; for signing/CTV behaviour, they're\n   both overkill (they have lots of seemingly useless combinations)\n   and insufficient (don't cover SIGHASH_GROUP), and they add additional\n   bytes of witness data, compared to CTV's zero-byte default or CHECKSIG's\n   zero/one-byte sighash which only do things we know are useful (well,\n   some of those combinations might not be useful either...).\n\n * If we're deliberately trying to add transaction introspection, then\n   all the flags do make sense, but Rusty's unhashed \"TX\" approach seems\n   better than TXHASH for that (assuming we want one opcode versus the\n   many opcodes elements use). But if we want that, we probably should\n   also add maths opcodes that can cope with output amounts, at least;\n   and perhaps also have some way for signatures to some witness data\n   that's used as script input. Also, convenient introspection isn't\n   really compatible with convenient signing without some way of\n   conveniently converting data into a tagged hash. \n\n * I'm not really convinced CTV is ready to start trying to deploy\n   on mainnet even in the next six months; I'd much rather see some real\n   third-party experimentation *somewhere* public first, and Jeremy's CTV\n   signet being completely empty seems like a bad sign to me. Maybe that\n   means we should tentatively merge the feature and deploy it on the\n   default global signet though?  Not really sure how best to get more\n   real world testing; but \"deploy first, test later\" doesn't sit right.\n\nI'm not at all sure about bundling CTV with ANYPREVOUT and SIGHASH_GROUP:\n\nPros:\n\n - with APO available, you don't have to worry as much if spending\n   a CTV output doesn't result in a guaranteed txid, and thus don't need\n   to commit to scriptSigs and they like\n\n - APOAS and CTV are pretty similar in what they hash\n\n - SIGHASH_GROUP lets you add extra extra change outputs to a CTV spend\n   which you can't otherwise do\n\n - reusing APOAS's tx hash scheme for CTV would avoid some of the weird\n   ugly bits in CTV (that the input index is committed to and that the\n   scriptSig is only \"maybe!\" included)\n\n - defining SIGHASH_GROUP and CTV simultaneously might let you define\n   the groups in a way that is compatible between tapscript (annex-based)\n   and legacy CTV. On the other hand, this probably still works provided\n   you deploy SIGHASH_GROUP /after/ CTV is specced in (by defining CTV\n   behaviour for a different length arg)\n\nCons:\n\n - just APOAS|ALL doesn't quite commit to the same things as bip 119 CTV\n   and that matters if you reuse CTV addresses\n\n - SIGHASH_GROUP assumes use of the annex, which would need to be\n   specced out; SIGHASH_GROUP itself doesn't really have a spec yet either\n\n - txs signed by APOAS|GROUP are more malleable than txs with a bip119\n   CTV hash which might be annoying to handle even non-adversarially\n\n - that malleability with current RBF rules might lead to pinning\n   problems\n\nI guess for me that adds up to:\n\n * For now, I think I prefer OP_CTV over either OP_TXHASH alone or both\n   OP_CTV and OP_TXHASH\n\n * I'd like to see CTV get more real-world testing before considering\n   deployment\n\n * If APO/SIGHASH_GROUP get specced, implemented *and* tested by the\n   time CTV is tested enough to think about deploying it, bundle them\n\n * Unless CTV testing takes ages, it's pretty unlikely it'll be worth\n   simplifying CTV to more closely match APO's tx hashing\n\n * CAT, CHECKSIGFROMSTACK, tx introspection, better maths *are* worth\n   prioritising, but would be better as part of a more thorough language\n   overhaul (since you can analyse how they interact with each other\n   in combination, and you get a huge jump from ~10% to ~80% benefit,\n   instead of tiny incremental ones)?\n\nI guess that's all partly dependent on thinking that, TXHASH isn't\ngreat for tx introspection (especially without CAT) and, (without tx\nintrospection and decent math opcodes), DLCs already provide all the\ninteresting oracle behaviour you're really going to get...\n\n> I don't know if this is the answer you are looking for, but technically\n> TXHASH + CAT + SHA256 awkwardly gives you limited transaction reflection.\n> In fact, you might not even need TXHASH, though it certainly helps.\n\nYeah, it wasn't really what I was looking for but it does demolish that\nspecific thought experiment anyway.\n\n> > I believe \"sequences hash\", \"input count\" and \"input index\" are all an\n> > important part of ensuring that if you have two UTXOs distributing 0.42\n> > BTC to the same set of addresses via CTV, that you can't combine them in a\n> > single transaction and end up sending losing one of the UTXOs to fees. I\n> > don't believe there's a way to resolve that with bip 118 alone, however\n> > that does seem to be a similar problem to the one that SIGHASH_GROUP\n> > tries to solve.\n> It was my understanding that it is only \"input count = 1\" that prevents\n> this issue.\n\nIf you have input count = 1, that solves the issue, but you could also\nhave input count > 1, and simply commit to different input indexes to\nallow/require you to combine two CTV utxos into a common set of new\noutputs, or you could have input count > 1 but input index = 1 for both\nutxos to prevent combining them with each other, but allow adding a fee\nfunding input (but not a change output; and at a cost of an unpredictable\ntxid).\n\n(I only listed \"sequences hash\" there because it implicitly commits to\n\"input count\")\n\nCheers,\naj"
            },
            {
                "author": "Russell O'Connor",
                "date": "2022-02-17T14:50:53",
                "message_text_only": "On Thu, Feb 17, 2022 at 9:27 AM Anthony Towns <aj at erisian.com.au> wrote:\n\n>\n> I guess that's all partly dependent on thinking that, TXHASH isn't\n> great for tx introspection (especially without CAT) and, (without tx\n> introspection and decent math opcodes), DLCs already provide all the\n> interesting oracle behaviour you're really going to get...\n>\n\nYou left out CSFSV's ability to do pubkey delegation.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220217/6b08b8c5/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2022-02-08T03:40:15",
                "message_text_only": "Russell O'Connor via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> writes:\n> Given the overlap in functionality between CTV and ANYPREVOUT, I think it\n> makes sense to decompose their operations into their constituent pieces and\n> reassemble their behaviour programmatically.  To this end, I'd like to\n> instead propose OP_TXHASH and OP_CHECKSIGFROMSTACKVERIFY.\n>\n> OP_TXHASH would pop a txhash flag from the stack and compute a (tagged)\n> txhash in accordance with that flag, and push the resulting hash onto the\n> stack.\n\nIt may be worth noting that OP_TXHASH can be further decomposed into\nOP_TX (and OP_TAGGEDHASH, or just reuse OP_SHA256).\n\nOP_TX would place the concatenated selected fields onto the stack\n(rather than hashing them) This is more compact for some tests\n(e.g. testing tx version for 2 is \"OP_TX(version) 1 OP_EQUALS\" vs\n\"OP_TXHASH(version) 012345678...aabbccddeeff OP_EQUALS\"), and also range\ntesting (e.g amount less than X or greater than X, or less than 3 inputs).\n\n> I believe the difficulties with upgrading TXHASH can be mitigated by\n> designing a robust set of TXHASH flags from the start.  For example having\n> bits to control whether (1) the version is covered; (2) the locktime is\n> covered; (3) txids are covered; (4) sequence numbers are covered; (5) input\n> amounts are covered; (6) input scriptpubkeys are covered; (7) number of\n> inputs is covered; (8) output amounts are covered; (9) output scriptpubkeys\n> are covered; (10) number of outputs is covered; (11) the tapbranch is\n> covered; (12) the tapleaf is covered; (13) the opseparator value is\n> covered; (14) whether all, one, or no inputs are covered; (15) whether all,\n> one or no outputs are covered; (16) whether the one input position is\n> covered; (17) whether the one output position is covered; (18) whether the\n> sighash flags are covered or not (note: whether or not the sighash flags\n> are or are not covered must itself be covered).  Possibly specifying which\n> input or output position is covered in the single case and whether the\n> position is relative to the input's position or is an absolute position.\n\nThese easily map onto OP_TX, \"(1) the version is pushed as u32, (2) the\nlocktime is pushed as u32, ...\".\n\nWe might want to push SHA256() of scripts instead of scripts themselves,\nto reduce possibility of DoS.\n\nI suggest, also, that 14 (and similarly 15) be defined two bits:\n00 - no inputs\n01 - all inputs\n10 - current input\n11 - pop number from stack, fail if >= number of inputs or no stack elems.\n\nCheers,\nRusty."
            },
            {
                "author": "Jeremy Rubin",
                "date": "2022-02-08T04:34:30",
                "message_text_only": "Rusty,\n\nNote that this sort of design introduces recursive covenants similarly to\nhow I described above.\n\nWhether that is an issue or not precluding this sort of design or not, I\ndefer to others.\n\nBest,\n\nJeremy\n\n\nOn Mon, Feb 7, 2022 at 7:57 PM Rusty Russell via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Russell O'Connor via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>\n> writes:\n> > Given the overlap in functionality between CTV and ANYPREVOUT, I think it\n> > makes sense to decompose their operations into their constituent pieces\n> and\n> > reassemble their behaviour programmatically.  To this end, I'd like to\n> > instead propose OP_TXHASH and OP_CHECKSIGFROMSTACKVERIFY.\n> >\n> > OP_TXHASH would pop a txhash flag from the stack and compute a (tagged)\n> > txhash in accordance with that flag, and push the resulting hash onto the\n> > stack.\n>\n> It may be worth noting that OP_TXHASH can be further decomposed into\n> OP_TX (and OP_TAGGEDHASH, or just reuse OP_SHA256).\n>\n> OP_TX would place the concatenated selected fields onto the stack\n> (rather than hashing them) This is more compact for some tests\n> (e.g. testing tx version for 2 is \"OP_TX(version) 1 OP_EQUALS\" vs\n> \"OP_TXHASH(version) 012345678...aabbccddeeff OP_EQUALS\"), and also range\n> testing (e.g amount less than X or greater than X, or less than 3 inputs).\n>\n> > I believe the difficulties with upgrading TXHASH can be mitigated by\n> > designing a robust set of TXHASH flags from the start.  For example\n> having\n> > bits to control whether (1) the version is covered; (2) the locktime is\n> > covered; (3) txids are covered; (4) sequence numbers are covered; (5)\n> input\n> > amounts are covered; (6) input scriptpubkeys are covered; (7) number of\n> > inputs is covered; (8) output amounts are covered; (9) output\n> scriptpubkeys\n> > are covered; (10) number of outputs is covered; (11) the tapbranch is\n> > covered; (12) the tapleaf is covered; (13) the opseparator value is\n> > covered; (14) whether all, one, or no inputs are covered; (15) whether\n> all,\n> > one or no outputs are covered; (16) whether the one input position is\n> > covered; (17) whether the one output position is covered; (18) whether\n> the\n> > sighash flags are covered or not (note: whether or not the sighash flags\n> > are or are not covered must itself be covered).  Possibly specifying\n> which\n> > input or output position is covered in the single case and whether the\n> > position is relative to the input's position or is an absolute position.\n>\n> These easily map onto OP_TX, \"(1) the version is pushed as u32, (2) the\n> locktime is pushed as u32, ...\".\n>\n> We might want to push SHA256() of scripts instead of scripts themselves,\n> to reduce possibility of DoS.\n>\n> I suggest, also, that 14 (and similarly 15) be defined two bits:\n> 00 - no inputs\n> 01 - all inputs\n> 10 - current input\n> 11 - pop number from stack, fail if >= number of inputs or no stack elems.\n>\n> Cheers,\n> Rusty.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220207/2a0a1a47/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2022-02-15T08:45:10",
                "message_text_only": "Jeremy Rubin <jeremy.l.rubin at gmail.com> writes:\n> Rusty,\n>\n> Note that this sort of design introduces recursive covenants similarly to\n> how I described above.\n>\n> Whether that is an issue or not precluding this sort of design or not, I\n> defer to others.\n\nGood point!\n\nBut I think it's a distinction without meaning: AFAICT iterative\ncovenants are possible with OP_CTV and just as powerful, though\ntechnically finite.  I can constrain the next 100M spends, for\nexample: if I insist on those each having incrementing nLocktime,\nthat's effectively forever.\n\nThanks!\nRusty."
            },
            {
                "author": "Jeremy Rubin",
                "date": "2022-02-15T18:57:35",
                "message_text_only": "Hi Rusty,\n\nPlease see my post in the other email thread\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019886.html\n\nThe differences in this regard are several, and worth understanding beyond\n\"you can iterate CTV\". I'd note a few clear examples for showing that \"CTV\nis just as powerful\" is not a valid claim:\n\n1) CTV requires the contract to be fully enumerated and is non-recursive.\nFor example, a simple contract that allows n participants to take an action\nin any order requires factorially many pre-computations, not just linear or\nconstant. For reference, 24! is about 2**80. Whereas for a more\ninterpretive covenant -- which is often introduced with the features for\nrecursion -- you can compute the programs for these addresses in constant\ntime.\n2) CTV requires the contract to be fully enumerated: For example, a simple\ncontract one could write is \"Output 0 script matches Output 1\", and the set\nof outcomes is again unbounded a-priori. With CTV you need to know the set\nof pairs you'd like to be able to expand to a-priori\n3) Combining 1 and 2, you could imagine recursing on an open-ended thing\nlike creating many identical outputs over time but not constraining what\nthose outputs are. E.g., Output 0 matches Input 0, Output 1 matches Output\n2.\n\nI think for your point the inverse seems to hold: for the limited\nsituations we might want to set up, CTV often ends up being sufficient\nbecause usually we can enumerate all the possible outcomes we'd like (or at\nleast find a mapping onto such a construction). CTV is indeed very\npowerful, but as I demonstrated above, not powerful in the same way\n(\"Complexity Class\") that OP_TX or TXHASH might be.\n\nAt the very least we should clearly understand *what* and *why* we are\nadvocating for more sophisticated designs and have a thorough understanding\nof the protocol complexity we are motivated to introduce the expanded\nfunctionality. Further, if one advocates for TX/TXHASH on a featureful\nbasis, it's at least a technical ACK on the functionality CTV is\nintroducing (as it is a subset) and perhaps a disagreement on project\nmanagement, which I think is worth noting. There is a very wide gap between\n\"X is unsafe\" and \"I prefer Y which X is a subset of ''.\n\nI'll close by repeating : Whether that [the recursive/open ended\nproperties] is an issue or not precluding this sort of design or not, I\ndefer to others.\n\nBest,\n\nJeremy\n\n\n\n\nBest,\n\nJeremy\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n\n\nOn Tue, Feb 15, 2022 at 12:46 AM Rusty Russell <rusty at rustcorp.com.au>\nwrote:\n\n> Jeremy Rubin <jeremy.l.rubin at gmail.com> writes:\n> > Rusty,\n> >\n> > Note that this sort of design introduces recursive covenants similarly to\n> > how I described above.\n> >\n> > Whether that is an issue or not precluding this sort of design or not, I\n> > defer to others.\n>\n> Good point!\n>\n> But I think it's a distinction without meaning: AFAICT iterative\n> covenants are possible with OP_CTV and just as powerful, though\n> technically finite.  I can constrain the next 100M spends, for\n> example: if I insist on those each having incrementing nLocktime,\n> that's effectively forever.\n>\n> Thanks!\n> Rusty.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220215/3c2a5425/attachment.html>"
            },
            {
                "author": "Russell O'Connor",
                "date": "2022-02-15T19:12:30",
                "message_text_only": "On Tue, Feb 15, 2022 at 1:57 PM Jeremy Rubin <jeremy.l.rubin at gmail.com>\nwrote:\n\n> Hi Rusty,\n>\n> Please see my post in the other email thread\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019886.html\n>\n> The differences in this regard are several, and worth understanding beyond\n> \"you can iterate CTV\". I'd note a few clear examples for showing that \"CTV\n> is just as powerful\" is not a valid claim:\n>\n> 1) CTV requires the contract to be fully enumerated and is non-recursive.\n> For example, a simple contract that allows n participants to take an action\n> in any order requires factorially many pre-computations, not just linear or\n> constant. For reference, 24! is about 2**80. Whereas for a more\n> interpretive covenant -- which is often introduced with the features for\n> recursion -- you can compute the programs for these addresses in constant\n> time.\n> 2) CTV requires the contract to be fully enumerated: For example, a simple\n> contract one could write is \"Output 0 script matches Output 1\", and the set\n> of outcomes is again unbounded a-priori. With CTV you need to know the set\n> of pairs you'd like to be able to expand to a-priori\n> 3) Combining 1 and 2, you could imagine recursing on an open-ended thing\n> like creating many identical outputs over time but not constraining what\n> those outputs are. E.g., Output 0 matches Input 0, Output 1 matches Output\n> 2.\n>\n> I think for your point the inverse seems to hold: for the limited\n> situations we might want to set up, CTV often ends up being sufficient\n> because usually we can enumerate all the possible outcomes we'd like (or at\n> least find a mapping onto such a construction). CTV is indeed very\n> powerful, but as I demonstrated above, not powerful in the same way\n> (\"Complexity Class\") that OP_TX or TXHASH might be.\n>\n\nJust to be clear, if OP_TXHASH is restricted to including the flags for the\nvalues to be hashed (at least for OP_TXHASH0), we don't appear to enter\nrecursive covenant territory, as long as we remain without OP_CAT.\n\n\n> At the very least we should clearly understand *what* and *why* we are\n> advocating for more sophisticated designs and have a thorough understanding\n> of the protocol complexity we are motivated to introduce the expanded\n> functionality. Further, if one advocates for TX/TXHASH on a featureful\n> basis, it's at least a technical ACK on the functionality CTV is\n> introducing (as it is a subset) and perhaps a disagreement on project\n> management, which I think is worth noting. There is a very wide gap between\n> \"X is unsafe\" and \"I prefer Y which X is a subset of ''.\n>\n\nI'm certainly of the opinion we should have some feature to enable the\ncommitment of outputs.  It seems quite useful in various protocols.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220215/13792914/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2022-02-16T02:26:14",
                "message_text_only": "Jeremy Rubin <jeremy.l.rubin at gmail.com> writes:\n> Hi Rusty,\n>\n> Please see my post in the other email thread\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019886.html\n>\n> The differences in this regard are several, and worth understanding beyond\n> \"you can iterate CTV\". I'd note a few clear examples for showing that \"CTV\n> is just as powerful\" is not a valid claim:\n>\n> 1) CTV requires the contract to be fully enumerated and is non-recursive.\n> For example, a simple contract that allows n participants to take an action\n> in any order requires factorially many pre-computations, not just linear or\n> constant. For reference, 24! is about 2**80. Whereas for a more\n> interpretive covenant -- which is often introduced with the features for\n> recursion -- you can compute the programs for these addresses in constant\n> time.\n> 2) CTV requires the contract to be fully enumerated: For example, a simple\n> contract one could write is \"Output 0 script matches Output 1\", and the set\n> of outcomes is again unbounded a-priori. With CTV you need to know the set\n> of pairs you'd like to be able to expand to a-priori\n> 3) Combining 1 and 2, you could imagine recursing on an open-ended thing\n> like creating many identical outputs over time but not constraining what\n> those outputs are. E.g., Output 0 matches Input 0, Output 1 matches Output\n> 2.\n\nOh agreed.  It was distinction of \"recursive\" vs \"not recursive\" which\nwas less useful in this context.\n\n\"limited to complete enumeration\" is the more useful distinction: it's a\nbright line between CTV and TXHASH IMHO.\n\n> I'll close by repeating : Whether that [the recursive/open ended\n> properties] is an issue or not precluding this sort of design or not, I\n> defer to others.\n\nYeah.  There's been some feeling that complex scripting is bad, because\npeople can lose money (see the various attempts to defang\nSIGHASH_NOINPUT).  I reject that; since script exists, we've crossed the\nRubicon, so let's make the tools as clean and clear as we can.\n\nCheers!\nRusty."
            },
            {
                "author": "Russell O'Connor",
                "date": "2022-02-16T04:10:19",
                "message_text_only": "On Tue, Feb 15, 2022 at 10:45 PM Rusty Russell <rusty at rustcorp.com.au>\nwrote:\n\n> Jeremy Rubin <jeremy.l.rubin at gmail.com> writes:\n> > Hi Rusty,\n> >\n> > Please see my post in the other email thread\n> >\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019886.html\n> >\n> > The differences in this regard are several, and worth understanding\n> beyond\n> > \"you can iterate CTV\". I'd note a few clear examples for showing that\n> \"CTV\n> > is just as powerful\" is not a valid claim:\n> >\n> > 1) CTV requires the contract to be fully enumerated and is non-recursive.\n> > For example, a simple contract that allows n participants to take an\n> action\n> > in any order requires factorially many pre-computations, not just linear\n> or\n> > constant. For reference, 24! is about 2**80. Whereas for a more\n> > interpretive covenant -- which is often introduced with the features for\n> > recursion -- you can compute the programs for these addresses in constant\n> > time.\n> > 2) CTV requires the contract to be fully enumerated: For example, a\n> simple\n> > contract one could write is \"Output 0 script matches Output 1\", and the\n> set\n> > of outcomes is again unbounded a-priori. With CTV you need to know the\n> set\n> > of pairs you'd like to be able to expand to a-priori\n> > 3) Combining 1 and 2, you could imagine recursing on an open-ended thing\n> > like creating many identical outputs over time but not constraining what\n> > those outputs are. E.g., Output 0 matches Input 0, Output 1 matches\n> Output\n> > 2.\n>\n> Oh agreed.  It was distinction of \"recursive\" vs \"not recursive\" which\n> was less useful in this context.\n>\n> \"limited to complete enumeration\" is the more useful distinction: it's a\n> bright line between CTV and TXHASH IMHO.\n>\n\nIf TXHASH is limited to requiring the flags be included in the hash (as is\ndone with sighash) I believe TXHASH has the same \"up front\" nature that CTV\nhas.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220215/cf036cf5/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "TXHASH + CHECKSIGFROMSTACKVERIFY in lieu of CTV and ANYPREVOUT",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Russell O'Connor",
                "Anthony Towns",
                "Rusty Russell",
                "Jeremy Rubin"
            ],
            "messages_count": 11,
            "total_messages_chars_count": 60221
        }
    },
    {
        "title": "[bitcoin-dev] Recursive covenant opposition, or the absence thereof, was Re: TXHASH + CHECKSIGFROMSTACKVERIFY in lieu of CTV and ANYPREVOUT",
        "thread_messages": [
            {
                "author": "David A. Harding",
                "date": "2022-02-11T00:55:31",
                "message_text_only": "On Mon, Feb 07, 2022 at 08:34:30PM -0800, Jeremy Rubin via bitcoin-dev wrote:\n> Whether [recursive covenants] is an issue or not precluding this sort\n> of design or not, I defer to others.\n\nFor reference, I believe the last time the merits of allowing recursive\ncovenants was discussed at length on this list[1], not a single person\nreplied to say that they were opposed to the idea.\n\nI would like to suggest that anyone opposed to recursive covenants speak\nfor themselves (if any intelligent such people exist).  Citing the risk\nof recursive covenants without presenting a credible argument for the\nsource of that risk feels to me like (at best) stop energy[2] and (at\nworst) FUD.\n\n-Dave\n\n[1] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-July/019203.html\n[2] http://radio-weblogs.com/0107584/stories/2002/05/05/stopEnergyByDaveWiner.html\n    (thanks to AJ who told me about stop energy one time when I was\n    producing it)\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220211/c471f94d/attachment.sig>"
            },
            {
                "author": "James O'Beirne",
                "date": "2022-02-11T17:42:11",
                "message_text_only": "I don't oppose recursive covenants per se, but in prior posts I have\nexpressed uncertainty about proposals that enable more \"featureful\"\ncovenants by adding more kinds of computation into bitcoin script.\n\nNot that anyone here is necessarily saying otherwise, but I am very\ninterested in limiting operations in bitcoin script to \"verification\" (vs.\n\"computation\") to the extent practical, and instead encouraging general\ncomputation be done off-chain. This of course isn't a new observation and I\nthink the last few years have been very successful to that effect, e.g. the\npopularity of the \"scriptless scripts\" idea and Taproot's emphasis on\nembedding computational artifacts in key tweaks.\n\nMy (maybe unfounded?) worry about opcodes like OP_CAT and OP_TX is that\nmore logic will live in script than is necessary, and so the burden to\nverify the chain may grow and the extra \"degrees of freedom\" in script may\nmake it harder to reason about. But I guess at this point there aren't\nalternative means to construct new kinds of sighashes that are necessary\nfor some interesting covenants.\n\nOne thing I like about CTV is that it buys a lot of functionality without\nincreasing the \"surface area\" of script's design. In general I think there\nis a lot to be said for this \"jets\"-style approach[0] of codifying the\nscript operations that you'd actually want to do into single opcodes. This\nadds functionality while introducing minimal surface area to script, giving\nscript implementers more flexibility for, say, optimization. But of course\nthis comes at the cost of precluding experimentation, and probably\nrequiring more soft-forking. Though whether the place for script\nexperimentation using more general-purpose opcodes on the main chain is\nanother interesting debate...\n\nSorry for going a little off-topic there.\n\n[0]: https://medium.com/blockstream/simplicity-jets-release-803db10fd589\n\n\nOn Thu, Feb 10, 2022 at 7:55 PM David A. Harding via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Mon, Feb 07, 2022 at 08:34:30PM -0800, Jeremy Rubin via bitcoin-dev\n> wrote:\n> > Whether [recursive covenants] is an issue or not precluding this sort\n> > of design or not, I defer to others.\n>\n> For reference, I believe the last time the merits of allowing recursive\n> covenants was discussed at length on this list[1], not a single person\n> replied to say that they were opposed to the idea.\n>\n> I would like to suggest that anyone opposed to recursive covenants speak\n> for themselves (if any intelligent such people exist).  Citing the risk\n> of recursive covenants without presenting a credible argument for the\n> source of that risk feels to me like (at best) stop energy[2] and (at\n> worst) FUD.\n>\n> -Dave\n>\n> [1]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-July/019203.html\n> [2]\n> http://radio-weblogs.com/0107584/stories/2002/05/05/stopEnergyByDaveWiner.html\n>     (thanks to AJ who told me about stop energy one time when I was\n>     producing it)\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220211/bf500325/attachment.html>"
            },
            {
                "author": "digital vagabond",
                "date": "2022-02-11T18:12:28",
                "message_text_only": "This is Shinobi (can verify out of band at @brian_trollz on Twitter, I only\nsigned up to the list with this email to read initially, but feel like I\nshould reply to this as I think I am one of the only people in this space\nwho has voiced concerns with recursive covenants).\n\nMy concerns don't really center specifically around recursion itself\nnecessarily, but unbounded recursion in combination with too much\ngenerality/flexibility in what types of conditions future UTXOs can be\nencumbered with based on the restriction of such covenants. Forgive the\nhand waiving arguments without getting into specific opcodes, but I would\nsummarize my concerns with a hypothetical construct that I believe would be\nincredibly damaging to fungibility. Imagine a covenant design that was\nflexible enough to create an encumbrance like this: a script specifies a\nspecific key in a multisig controlled by some authority figure (or a branch\nin the script that would allow unilateral control by such an authority),\nand the conditions of the covenant would perpetually require than any spend\nfrom the covenant can only be sent to a script involving that key from said\nauthority, preventing by consensus any removal of that central authorities\ninvolvement in control over that UTXO. Such a construct would present\ndangerous implications to the fungibility of individual UTXOs by\nintroducing a totally different risk model in being paid with such a coin\ncompared to any other coin not encumbered by such a condition, and also\npotentially introduce a shift in the scope of what a 51% attack could\naccomplish in terms of permanent consequences attempting to coerce coins\ninto such covenants, as opposed to right now only being able to accomplish\ncensorship or temporary network disruption.\n\nI know that such a walled garden could easily be constructed now with\nmultisig and restrictions on where coins can be withdrawn to from exchanges\nor whatever place they initially purchased from, as is demonstrated by the\nimplementation of the Asset Management Platform by Blockstream for use on\nLiquid with regulated equity tokens, but I think the important distinction\nbetween such non-consensus system designed to enforce such restrictions and\na recursive covenant to accomplish the same is that in the case of a\nmultisig/non-consensus based system, exit from that restriction is still\npossible under the consensus rules of the protocol. If such a construct was\npossible to build with a recursive covenant enforced by consensus, coins\nencumbered by such a covenant would literally be incapable of escaping\nthose restrictions without hardforking the protocol, leaving any such UTXOs\npermanently non-fungible with ones not encumbered by such conditions.\n\nI'm not that deeply familiar with all the working pieces involved in the\nrecent TXHASH + CSFS proposal, and whether such a type of overly (IMO)\ngeneralized recursion would be possible to construct, but one of the\nreasons CTV does not bother me in terms of such concerns is the inability\nto infinitely recurse in such a generalized way given the requirements to\nexactly specify the destination of future spends in constructing a chain of\nCTV encumbrances. I'd very much appreciate any feedback on my concerns, and\nif this side tracks the discussion I apologize, but I felt given the issue\nhas been mentioned a few times in this thread it was appropriate for me to\nvoice the concerns here so they could be addressed directly.\n\nOn Fri, Feb 11, 2022 at 11:42 AM James O'Beirne via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> I don't oppose recursive covenants per se, but in prior posts I have\n> expressed uncertainty about proposals that enable more \"featureful\"\n> covenants by adding more kinds of computation into bitcoin script.\n>\n> Not that anyone here is necessarily saying otherwise, but I am very\n> interested in limiting operations in bitcoin script to \"verification\" (vs.\n> \"computation\") to the extent practical, and instead encouraging general\n> computation be done off-chain. This of course isn't a new observation and I\n> think the last few years have been very successful to that effect, e.g. the\n> popularity of the \"scriptless scripts\" idea and Taproot's emphasis on\n> embedding computational artifacts in key tweaks.\n>\n> My (maybe unfounded?) worry about opcodes like OP_CAT and OP_TX is that\n> more logic will live in script than is necessary, and so the burden to\n> verify the chain may grow and the extra \"degrees of freedom\" in script may\n> make it harder to reason about. But I guess at this point there aren't\n> alternative means to construct new kinds of sighashes that are necessary\n> for some interesting covenants.\n>\n> One thing I like about CTV is that it buys a lot of functionality without\n> increasing the \"surface area\" of script's design. In general I think there\n> is a lot to be said for this \"jets\"-style approach[0] of codifying the\n> script operations that you'd actually want to do into single opcodes. This\n> adds functionality while introducing minimal surface area to script, giving\n> script implementers more flexibility for, say, optimization. But of course\n> this comes at the cost of precluding experimentation, and probably\n> requiring more soft-forking. Though whether the place for script\n> experimentation using more general-purpose opcodes on the main chain is\n> another interesting debate...\n>\n> Sorry for going a little off-topic there.\n>\n> [0]: https://medium.com/blockstream/simplicity-jets-release-803db10fd589\n>\n>\n> On Thu, Feb 10, 2022 at 7:55 PM David A. Harding via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> On Mon, Feb 07, 2022 at 08:34:30PM -0800, Jeremy Rubin via bitcoin-dev\n>> wrote:\n>> > Whether [recursive covenants] is an issue or not precluding this sort\n>> > of design or not, I defer to others.\n>>\n>> For reference, I believe the last time the merits of allowing recursive\n>> covenants was discussed at length on this list[1], not a single person\n>> replied to say that they were opposed to the idea.\n>>\n>> I would like to suggest that anyone opposed to recursive covenants speak\n>> for themselves (if any intelligent such people exist).  Citing the risk\n>> of recursive covenants without presenting a credible argument for the\n>> source of that risk feels to me like (at best) stop energy[2] and (at\n>> worst) FUD.\n>>\n>> -Dave\n>>\n>> [1]\n>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-July/019203.html\n>> [2]\n>> http://radio-weblogs.com/0107584/stories/2002/05/05/stopEnergyByDaveWiner.html\n>>     (thanks to AJ who told me about stop energy one time when I was\n>>     producing it)\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220211/d4080387/attachment-0001.html>"
            },
            {
                "author": "darosior",
                "date": "2022-02-12T10:54:51",
                "message_text_only": "> Such a construct would present dangerous implications to the fungibility of individual UTXOs by introducing a totally different risk model in being paid with such a coin compared to any other coin not encumbered by such a condition\n\nHow is that different from being paid in an altcoin?\nIt seems to me that being able to say \"sorry, your money isn't good here\" is at the heart of Bitcoin's security (similarly to enforcing the network rules with your node). If someone can coerce you into using another currency, you've already lost.\n\nNow there is left the influence on the system of an user being coerced into using gov coin (on another chain) or an encumbered bit coin. Sure the latter would decrease the supply available, but that's already possible to do today.\n\n------- Original Message -------\nLe vendredi 11 f\u00e9vrier 2022 \u00e0 7:12 PM, digital vagabond via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> This is Shinobi (can verify out of band at @brian_trollz on Twitter, I only signed up to the list with this email to read initially, but feel like I should reply to this as I think I am one of the only people in this space who has voiced concerns with recursive covenants).\n>\n> My concerns don't really center specifically around recursion itself necessarily, but unbounded recursion in combination with too much generality/flexibility in what types of conditions future UTXOs can be encumbered with based on the restriction of such covenants. Forgive the hand waiving arguments without getting into specific opcodes, but I would summarize my concerns with a hypothetical construct that I believe would be incredibly damaging to fungibility. Imagine a covenant design that was flexible enough to create an encumbrance like this: a script specifies a specific key in a multisig controlled by some authority figure (or a branch in the script that would allow unilateral control by such an authority), and the conditions of the covenant would perpetually require than any spend from the covenant can only be sent to a script involving that key from said authority, preventing by consensus any removal of that central authorities involvement in control over that UTXO. Such a construct would present dangerous implications to the fungibility of individual UTXOs by introducing a totally different risk model in being paid with such a coin compared to any other coin not encumbered by such a condition, and also potentially introduce a shift in the scope of what a 51% attack could accomplish in terms of permanent consequences attempting to coerce coins into such covenants, as opposed to right now only being able to accomplish censorship or temporary network disruption.\n>\n> I know that such a walled garden could easily be constructed now with multisig and restrictions on where coins can be withdrawn to from exchanges or whatever place they initially purchased from, as is demonstrated by the implementation of the Asset Management Platform by Blockstream for use on Liquid with regulated equity tokens, but I think the important distinction between such non-consensus system designed to enforce such restrictions and a recursive covenant to accomplish the same is that in the case of a multisig/non-consensus based system, exit from that restriction is still possible under the consensus rules of the protocol. If such a construct was possible to build with a recursive covenant enforced by consensus, coins encumbered by such a covenant would literally be incapable of escaping those restrictions without hardforking the protocol, leaving any such UTXOs permanently non-fungible with ones not encumbered by such conditions.\n>\n> I'm not that deeply familiar with all the working pieces involved in the recent TXHASH + CSFS proposal, and whether such a type of overly (IMO) generalized recursion would be possible to construct, but one of the reasons CTV does not bother me in terms of such concerns is the inability to infinitely recurse in such a generalized way given the requirements to exactly specify the destination of future spends in constructing a chain of CTV encumbrances. I'd very much appreciate any feedback on my concerns, and if this side tracks the discussion I apologize, but I felt given the issue has been mentioned a few times in this thread it was appropriate for me to voice the concerns here so they could be addressed directly.\n>\n> On Fri, Feb 11, 2022 at 11:42 AM James O'Beirne via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> I don't oppose recursive covenants per se, but in prior posts I have expressed uncertainty about proposals that enable more \"featureful\" covenants by adding more kinds of computation into bitcoin script.\n>>\n>> Not that anyone here is necessarily saying otherwise, but I am very interested in limiting operations in bitcoin script to \"verification\" (vs. \"computation\") to the extent practical, and instead encouraging general computation be done off-chain. This of course isn't a new observation and I think the last few years have been very successful to that effect, e.g. the popularity of the \"scriptless scripts\" idea and Taproot's emphasis on embedding computational artifacts in key tweaks.\n>>\n>> My (maybe unfounded?) worry about opcodes like OP_CAT and OP_TX is that more logic will live in script than is necessary, and so the burden to verify the chain may grow and the extra \"degrees of freedom\" in script may make it harder to reason about. But I guess at this point there aren't alternative means to construct new kinds of sighashes that are necessary for some interesting covenants.\n>>\n>> One thing I like about CTV is that it buys a lot of functionality without increasing the \"surface area\" of script's design. In general I think there is a lot to be said for this \"jets\"-style approach[0] of codifying the script operations that you'd actually want to do into single opcodes. This adds functionality while introducing minimal surface area to script, giving script implementers more flexibility for, say, optimization. But of course this comes at the cost of precluding experimentation, and probably requiring more soft-forking. Though whether the place for script experimentation using more general-purpose opcodes on the main chain is another interesting debate...\n>>\n>> Sorry for going a little off-topic there.\n>>\n>> [0]: https://medium.com/blockstream/simplicity-jets-release-803db10fd589\n>>\n>> On Thu, Feb 10, 2022 at 7:55 PM David A. Harding via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> On Mon, Feb 07, 2022 at 08:34:30PM -0800, Jeremy Rubin via bitcoin-dev wrote:\n>>>> Whether [recursive covenants] is an issue or not precluding this sort\n>>>> of design or not, I defer to others.\n>>>\n>>> For reference, I believe the last time the merits of allowing recursive\n>>> covenants was discussed at length on this list[1], not a single person\n>>> replied to say that they were opposed to the idea.\n>>>\n>>> I would like to suggest that anyone opposed to recursive covenants speak\n>>> for themselves (if any intelligent such people exist). Citing the risk\n>>> of recursive covenants without presenting a credible argument for the\n>>> source of that risk feels to me like (at best) stop energy[2] and (at\n>>> worst) FUD.\n>>>\n>>> -Dave\n>>>\n>>> [1] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-July/019203.html\n>>> [2] http://radio-weblogs.com/0107584/stories/2002/05/05/stopEnergyByDaveWiner.html\n>>> (thanks to AJ who told me about stop energy one time when I was\n>>> producing it)\n>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220212/99a82200/attachment.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-02-12T15:59:03",
                "message_text_only": "> in the case of a multisig/non-consensus based system, exit from that\nrestriction is still possible\n\nBut why do we care if someone reduces the value of coins they own by\npermanently encumbering them in some way? Burning coins permanently\nencumbers them so much they can't be spent at all. If the worry is\ndepleting the supply of sats, don't worry, the amount of value lost by\nthose encumbered is gained but the rest of the coins. Just like burning,\nencumbering your coins in a way that devalues them is a donation to the\nrest of us.\n\nCould you clarify what harm there is to those who choose not to accept such\nencumbered coins? Or are you just saying that those who do accept such\nencumbered coins may be harmed by doing so?\n\nOn Sat, Feb 12, 2022, 06:11 darosior via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Such a construct would present dangerous implications to the fungibility\n> of individual UTXOs by introducing a totally different risk model in being\n> paid with such a coin compared to any other coin not encumbered by such a\n> condition\n>\n>\n> How is that different from being paid in an altcoin?\n> It seems to me that being able to say \"sorry, your money isn't good here\"\n> is at the heart of Bitcoin's security (similarly to enforcing the network\n> rules with your node). If someone can coerce you into using another\n> currency, you've already lost.\n>\n> Now there is left the influence on the system of an user being coerced\n> into using gov coin (on another chain) or an encumbered bit coin. Sure the\n> latter would decrease the supply available, but that's already possible to\n> do today.\n>\n> ------- Original Message -------\n> Le vendredi 11 f\u00e9vrier 2022 \u00e0 7:12 PM, digital vagabond via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>\n> This is Shinobi (can verify out of band at @brian_trollz on Twitter, I\n> only signed up to the list with this email to read initially, but feel like\n> I should reply to this as I think I am one of the only people in this space\n> who has voiced concerns with recursive covenants).\n>\n> My concerns don't really center specifically around recursion itself\n> necessarily, but unbounded recursion in combination with too much\n> generality/flexibility in what types of conditions future UTXOs can be\n> encumbered with based on the restriction of such covenants. Forgive the\n> hand waiving arguments without getting into specific opcodes, but I would\n> summarize my concerns with a hypothetical construct that I believe would be\n> incredibly damaging to fungibility. Imagine a covenant design that was\n> flexible enough to create an encumbrance like this: a script specifies a\n> specific key in a multisig controlled by some authority figure (or a branch\n> in the script that would allow unilateral control by such an authority),\n> and the conditions of the covenant would perpetually require than any spend\n> from the covenant can only be sent to a script involving that key from said\n> authority, preventing by consensus any removal of that central authorities\n> involvement in control over that UTXO. Such a construct would present\n> dangerous implications to the fungibility of individual UTXOs by\n> introducing a totally different risk model in being paid with such a coin\n> compared to any other coin not encumbered by such a condition, and also\n> potentially introduce a shift in the scope of what a 51% attack could\n> accomplish in terms of permanent consequences attempting to coerce coins\n> into such covenants, as opposed to right now only being able to accomplish\n> censorship or temporary network disruption.\n>\n> I know that such a walled garden could easily be constructed now with\n> multisig and restrictions on where coins can be withdrawn to from exchanges\n> or whatever place they initially purchased from, as is demonstrated by the\n> implementation of the Asset Management Platform by Blockstream for use on\n> Liquid with regulated equity tokens, but I think the important distinction\n> between such non-consensus system designed to enforce such restrictions and\n> a recursive covenant to accomplish the same is that in the case of a\n> multisig/non-consensus based system, exit from that restriction is still\n> possible under the consensus rules of the protocol. If such a construct was\n> possible to build with a recursive covenant enforced by consensus, coins\n> encumbered by such a covenant would literally be incapable of escaping\n> those restrictions without hardforking the protocol, leaving any such UTXOs\n> permanently non-fungible with ones not encumbered by such conditions.\n>\n> I'm not that deeply familiar with all the working pieces involved in the\n> recent TXHASH + CSFS proposal, and whether such a type of overly (IMO)\n> generalized recursion would be possible to construct, but one of the\n> reasons CTV does not bother me in terms of such concerns is the inability\n> to infinitely recurse in such a generalized way given the requirements to\n> exactly specify the destination of future spends in constructing a chain of\n> CTV encumbrances. I'd very much appreciate any feedback on my concerns, and\n> if this side tracks the discussion I apologize, but I felt given the issue\n> has been mentioned a few times in this thread it was appropriate for me to\n> voice the concerns here so they could be addressed directly.\n>\n> On Fri, Feb 11, 2022 at 11:42 AM James O'Beirne via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> I don't oppose recursive covenants per se, but in prior posts I have\n>> expressed uncertainty about proposals that enable more \"featureful\"\n>> covenants by adding more kinds of computation into bitcoin script.\n>>\n>> Not that anyone here is necessarily saying otherwise, but I am very\n>> interested in limiting operations in bitcoin script to \"verification\" (vs.\n>> \"computation\") to the extent practical, and instead encouraging general\n>> computation be done off-chain. This of course isn't a new observation and I\n>> think the last few years have been very successful to that effect, e.g. the\n>> popularity of the \"scriptless scripts\" idea and Taproot's emphasis on\n>> embedding computational artifacts in key tweaks.\n>>\n>> My (maybe unfounded?) worry about opcodes like OP_CAT and OP_TX is that\n>> more logic will live in script than is necessary, and so the burden to\n>> verify the chain may grow and the extra \"degrees of freedom\" in script may\n>> make it harder to reason about. But I guess at this point there aren't\n>> alternative means to construct new kinds of sighashes that are necessary\n>> for some interesting covenants.\n>>\n>> One thing I like about CTV is that it buys a lot of functionality without\n>> increasing the \"surface area\" of script's design. In general I think there\n>> is a lot to be said for this \"jets\"-style approach[0] of codifying the\n>> script operations that you'd actually want to do into single opcodes. This\n>> adds functionality while introducing minimal surface area to script, giving\n>> script implementers more flexibility for, say, optimization. But of course\n>> this comes at the cost of precluding experimentation, and probably\n>> requiring more soft-forking. Though whether the place for script\n>> experimentation using more general-purpose opcodes on the main chain is\n>> another interesting debate...\n>>\n>> Sorry for going a little off-topic there.\n>>\n>> [0]: https://medium.com/blockstream/simplicity-jets-release-803db10fd589\n>>\n>>\n>> On Thu, Feb 10, 2022 at 7:55 PM David A. Harding via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> On Mon, Feb 07, 2022 at 08:34:30PM -0800, Jeremy Rubin via bitcoin-dev\n>>> wrote:\n>>> > Whether [recursive covenants] is an issue or not precluding this sort\n>>> > of design or not, I defer to others.\n>>>\n>>> For reference, I believe the last time the merits of allowing recursive\n>>> covenants was discussed at length on this list[1], not a single person\n>>> replied to say that they were opposed to the idea.\n>>>\n>>> I would like to suggest that anyone opposed to recursive covenants speak\n>>> for themselves (if any intelligent such people exist). Citing the risk\n>>> of recursive covenants without presenting a credible argument for the\n>>> source of that risk feels to me like (at best) stop energy[2] and (at\n>>> worst) FUD.\n>>>\n>>> -Dave\n>>>\n>>> [1]\n>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-July/019203.html\n>>> [2]\n>>> http://radio-weblogs.com/0107584/stories/2002/05/05/stopEnergyByDaveWiner.html\n>>> (thanks to AJ who told me about stop energy one time when I was\n>>> producing it)\n>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220212/582bc544/attachment-0001.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-02-17T15:15:28",
                "message_text_only": "On Fri, Feb 11, 2022 at 12:12:28PM -0600, digital vagabond via bitcoin-dev wrote:\n> Imagine a covenant design that was\n> flexible enough to create an encumbrance like this: a script specifies a\n> specific key in a multisig controlled by some authority figure (or a branch\n> in the script that would allow unilateral control by such an authority),\n> and the conditions of the covenant would perpetually require than any spend\n> from the covenant can only be sent to a script involving that key from said\n> authority, preventing by consensus any removal of that central authorities\n> involvement in control over that UTXO.\n\n> I know that such a walled garden could easily be constructed now with\n> multisig and restrictions on where coins can be withdrawn to from exchanges\n> or whatever [...], but I think the important distinction\n> between such non-consensus system designed to enforce such restrictions and\n> a recursive covenant to accomplish the same is that in the case of a\n> multisig/non-consensus based system, exit from that restriction is still\n> possible under the consensus rules of the protocol.\n\nI think that sort of encumberance is already possible: you send bitcoin\nto an OP_RETURN address and that is registered on some other system as a\nway of \"minting\" coins there (ie, \"proof of burn\") at which point rules\nother than bitcoin's apply. Bitcoin consensus guarantees the value can't\nbe extracted back out of the OP_RETURN value.\n\nI think spacechains effectively takes up this concept for their one-way\npeg:\n\n  https://bitcoin.stackexchange.com/questions/100537/what-is-spacechain\n\n  https://medium.com/@RubenSomsen/21-million-bitcoins-to-rule-all-sidechains-the-perpetual-one-way-peg-96cb2f8ac302\n\n(I think spacechains requires a covenant construct to track the\nsingle-tx-per-bitcoin-block that commits to the spacechain, but that's\nnot directly used for the BTC value that was pegged into the spacechain)\n\nIf we didn't have OP_RETURN, you could instead pay to a pubkey that's\nconstructed from a NUMS point / or a pedersen commitment, that's (roughly)\nguaranteed unspendable, at least until secp256k1 is broken via bitcoin's\nconsensus rules (with the obvious disadvantage that nodes then can't\nremove these outputs from the utxo set).\n\nThat was also used for XCP/Counterparty's ICO in 2014, at about 823 uBTC\nper XCP on average (depending on when you got in it was between 666\nuBTC/XCP and 1000 uBTC/XCP apparently), falling to a current price of\nabout 208 uBTC per XCP. It was about 1000 uBTC/XCP until mid 2018 though.\n\n  https://counterparty.io/news/why-proof-of-burn/\n  https://github.com/CounterpartyXCP/Documentation/blob/master/Basics/FAQ-XCP.md\n\nThese seem like they might be bad things for people to actually do\n(why would you want to be paid to mine a spacechain in coins that can\nonly fall in value relative to bitcoin?), and certainly I don't think\nwe should do things just to make this easier; but it seems more like a\n\"here's why you're hurting yourself if you do this\" thing, rather than a\n\"we can prevent you from doing it and we will\" thing.\n\nCheers,\naj"
            },
            {
                "author": "Lucky Star",
                "date": "2022-02-14T02:40:52",
                "message_text_only": "Hello,\n\nI'm opposed to recursive covenants because they allow the government to _gradually_ restrict all bitcoins.\n\nWithout covenants, other miners can fork to a free blockchain, if the government tells miners each transaction to be added in the block. Thus the government cannot impose desires on the Bitcoin community. With covenants, the government gradually forces all companies to use the permissible covenants. There is no free blockchain, and the government controls more bitcoins each day.\n\nBitcoin experts Greg Maxwell and Peter Todd explained this reason and many others on the forum.[1] More experts also agreed, and it's common knowledge. I strongly recommend to support the OP_CHECKTEMPLATEVERIFY. It is well reviewed, and it protects the Bitcoin community from the bad effects of covenants. With OP_CHECKTEMPLATEVERIFY, we achieve the best of both worlds.\n\nWith best regards,\nLucky Star\n\n[1] Maxwell, Greg. \"CoinCovenants using SCIP signatures, an amusingly bad idea.\" https://bitcointalk.org/index.php?topic=278122.0;all\n\n> On Mon, Feb 07, 2022 at 08:34:30PM -0800, Jeremy Rubin via bitcoin-dev wrote:\n> > Whether [recursive covenants] is an issue or not precluding this sort\n> > of design or not, I defer to others.\n>\n>\n> For reference, I believe the last time the merits of allowing recursive\n> covenants was discussed at length on this list[1], not a single person\n> replied to say that they were opposed to the idea.\n>\n>\n> I would like to suggest that anyone opposed to recursive covenants speak\n> for themselves (if any intelligent such people exist). Citing the risk\n> of recursive covenants without presenting a credible argument for the\n> source of that risk feels to me like (at best) stop energy[2] and (at\n> worst) FUD.\n>\n>\n> -Dave\n>\n>\n> [1] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-July/019203.html\n> [2] http://radio-weblogs.com/0107584/stories/2002/05/05/stopEnergyByDaveWiner.html\n> (thanks to AJ who told me about stop energy one time when I was\n> producing it)\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220214/6ec4b8cd/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-18T07:34:52",
                "message_text_only": "Good morning Dave,\n\n> On Mon, Feb 07, 2022 at 08:34:30PM -0800, Jeremy Rubin via bitcoin-dev wrote:\n>\n> > Whether [recursive covenants] is an issue or not precluding this sort\n> > of design or not, I defer to others.\n>\n> For reference, I believe the last time the merits of allowing recursive\n> covenants was discussed at length on this list[1], not a single person\n> replied to say that they were opposed to the idea.\n>\n> I would like to suggest that anyone opposed to recursive covenants speak\n> for themselves (if any intelligent such people exist). Citing the risk\n> of recursive covenants without presenting a credible argument for the\n> source of that risk feels to me like (at best) stop energy[2] and (at\n> worst) FUD.\n\nLet me try to give that a shot.\n\n(Just to be clear, I am not an artificial intelligence, thus, I am not an \"intelligent such people\".)\n\nThe objection here is that recursion can admit partial (i.e. Turing-complete) computation.\nTuring-completeness implies that the halting problem cannot be solved for arbitrary programs in the language.\n\nNow, a counter-argument to that is that rather than using arbitrary programs, we should just construct programs from provably-terminating components.\nThus, even though the language may admit arbitrary programs that cannot provably terminate, \"wise\" people will just focus on using that subset of the language, and programming styles within the language, which have proofs of termination.\nOr in other words: people can just avoid accepting coin that is encumbered with a SCRIPT that is not trivially shown to be non-recursive.\n\nThe counter-counter-argument is that it leaves such validation to the user, and we should really create automation (i.e. lower-level non-sentient programs) to perform that validation on behalf of the user.\n***OR*** we could just design our language so that such things are outright rejected by the language as a semantic error, of the same type as `for (int x = 0; x = y; x++);` is a semantic error that most modern C compilers will reject if given `-Wall -Werror`.\n\n\nYes, we want users to have freedom to shoot themselves in the feet, but we also want, when it is our turn to be the users, to keep walking with two feet as long as we can.\n\nAnd yes, you could instead build a *separate* tool that checks if your SCRIPT can be proven to be non-recursive, and let the recursive construct remain in the interpreter and just require users who don't want their feet shot to use the separate tool.\nThat is certainly a valid alternate approach.\nIt is certainly valid to argue as well, that if a possibly-recursive construct is used, and you cannot find a proof-of-non-recursion, you should avoid coins encumbered with that SCRIPT (which is just a heuristic that approximate a tool for proof-of-non-recursion).\n\nOn the other hand, if we have the ability to identify SCRIPTs that have some proof-of-non-recursion, why is such a tool not built into the interpreter itself (in the form of operations that are provably non-recursive), why have a separate tool that people might be too lazy to actually use?\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-23T11:28:36",
                "message_text_only": "Subject: Turing-Completeness, And Its Enablement Of Drivechains\n\nIntroduction\n============\n\nRecently, David Harding challenged those opposed to recursive covenants\nfor *actual*, *concrete* reasons why recursive covenants are a Bad Thing\n(TM).\n\nGenerally, it is accepted that recursive covenants, together with the\nability to update loop variables, is sufficiently powerful to be\nconsidered Turing-complete.\nSo, the question is: why is Turing-completness bad, if it requires\n*multiple* transactions in order to implement Turing-completeness?\nSurely the practical matter that fees must be paid for each transaction\nserves as a backstop against Turing-completeness?\ni.e. Fees end up being the \"maximum number of steps\", which prevents a\nlanguage from becoming truly Turing-complete.\n\nI point out here that Drivechains is implementable on a Turing-complete\nlanguage.\nAnd we have already rejected Drivechains, for the following reason:\n\n1.  Sidechain validators and mainchain miners have a strong incentive to\n    merge their businesses.\n2.  Mainchain miners end up validating and commiting to sidechain blocks.\n3.  Ergo, sidechains on Drivechains become a block size increase.\n\nAlso:\n\n1.  The sidechain-to-mainchain peg degrades the security of sidechain\n    users from consensus \"everyone must agree to the rules\" to democracy\n    \"if enough enfranchised voters say so, they can beat you up and steal\n    your money\".\n\nIn this write-up, I will demonstrate how recursive covenants, with\nloop variable update, is sufficient to implement a form Drivechains.\nLogically, if the construct is general enough to form Drivechains, and\nwe rejected Drivechains, we should also reject the general construct.\n\nDigression: `OP_TLUV` And `OP_CAT` Implement Recursive Covenants\n================================================================\n\nLet me now do some delaying tactics and demonstrate how `OP_TLUV` and\n`OP_CAT` allow building recursive covenants by quining.\n\n`OP_TLUV` has a mode where the current Tapleaf is replaced, and the\nnew address is synthesized.\nThen, an output of the transaction is validated to check that it has\nthe newly-synthesized address.\n\nLet me sketch how a simple recursive covenant can be built.\nFirst, we split the covenant into three parts:\n\n1.  A hash.\n2.  A piece of script which validates that the first witness item\n    hashes to the above given hash in part #1, and then pushes that\n    item into the alt stack.\n3.  A piece of script which takes the item from the alt stack,\n    hashes it, then concatenates a `OP_PUSH` of the hash to that\n    item, then does a replace-mode `OP_TLUV`.\n\nParts 1 and 2 must directly follow each other, but other SCRIPT\nlogic can be put in between parts 2 and 3.\nPart 3 can even occur multiple times, in various `OP_IF` branches.\n\nIn order to actually recurse, the top item in the witness stack must\nbe the covenant script, *minus* the hash.\nThis is supposed to be the quining argument.\n\nThe convenant script part #2 then checks that the quining argument\nmatches the hash that is hardcoded into the SCRIPT.\nThis hash is the hash of the *rest* of the SCRIPT.\nIf the quining argument matches, then it *is* the SCRIPT minus its\nhash, and we know that we can use that to recreate the original SCRIPT.\nIt then pushes them out of the way into the alt stack.\n\nPart #3 then recovers the original SCRIPT from the alt stack, and\nresynthesizes the original SCRIPT.\nThe `OP_TLUV` is then able to resynthesize the original address.\n\nUpdating Loop Variables\n-----------------------\n\nBut repeating the same SCRIPT over and over is boring.\n\nWhat is much more interesting is to be able to *change* the SCRIPT\non each iteration, such that certain values on the SCRIPT can be\nchanged.\n\nSuppose our SCRIPT has a loop variable `i` that we want to change\neach time we execute our SCRIPT.\n\nWe can simply put this loop variable after part 1 and before part 2.\nThen part 2 is modified to first push this loop variable onto the\nalt stack.\n\nThe SCRIPT that gets checked is always starts from part 2.\nThus, the SCRIPT, minus the loop variable, is always constant.\nThe SCRIPT can then access the loop variable from the alt stack.\nPart 2 can be extended so that the loop variable is on top of the\nquined SCRIPT on the alt stack.\nThis lets the SCRIPT easily access the loop variable.\nThe SCRIPT can also update the loop variable by replacing the top\nof the alt stack with a different item.\n\nThen part 3 first pops the alt stack top (the loop variable),\nconcatenates it with an appropriate push, then performs the\nhash-then-concatenate dance.\nThis results in a SCRIPT that is the same as the original SCRIPT,\nbut with the loop variable possibly changed.\n\nThe SCRIPT can use multiple loop variables; it is simply a question\nof how hard it would be to access from the alt stack.\n\nDrivechains Over Recursive Covenants\n====================================\n\nDrivechains can be split into four parts:\n\n1.  A way to commit to the sidechain blocks.\n2.  A way to move funds from mainchain to sidechain.\n3.  A way to store sidechain funds.\n4.  A way to move funds from sidechain to mainchain.\n\nThe first three can be easily implemented by a recursive covenant\nwithout a loop variable, together with an opcode to impose some\nrestriction on amounts, such as `OP_IN_OUT_AMOUNT`.\n\nThe technique we would use would be to put the entire sidechain\nfunds into a single UTXO, protected by a recursive covenant.\nThe recursive covenant ensures that it can store the sidechain\nfunds.\nThis covers part 3.\n\nThe recursive covenant could, with the help of `OP_CAT` and\n`OP_CTV`, check that every transaction spending the UTXO has a\nsecond output that is an `OP_RETURN` with a commitment to the\nsidechain block.\nWe can ensure that only one such transaction exists in each\nmainchain block by adding a `<1> OP_CSV`, ensuring that only one\nsidechain-commitment transaction can occur on each mainchain\nblock.\nThis covers part 1.\n\nMainchain-to-sidechain pegs require the cooperation of a\nsidechain validator.\nThe sidechain validator creates a block that instantiates the\npeg-in on the sidechain, then creates a transaction that commits\nto that sidechain block including the peg-in, and spending the\ncurrent sidechain UTXO *and* the mainchain funds being transferred\nin.\nThen the entity requesting the peg-in checks the sidechain block\nand the commitment on the transaction, then signs the transaction.\nThe value restriction on the recursive covenant should then be to\nallow the output to be equal, or larger, than the input.\nThis covers part 2.\n\nThe recursive sidechain covenant by itself has a constant SCRIPT,\nand thus has a constant address.\n\nThe last part of Drivechains -- sidechain-to-mainchain peg ---\nis significantly more interesting.\n\nDigression: Hashes As Peano Naturals\n------------------------------------\n\nIt is possible to represent natural numbers using the following\nHaskell data type:\n\n```Haskell\ndata Nat = Z\n         | S Nat\n-- Z :: Nat\n-- S :: Nat -> Nat\n```\n\nWe can represent naturals as:\n\n* `0` == `Z`\n* `1` == `S Z`\n* `2` == `S (S Z)`\n* `3` == `S (S (S Z))`\n* etc.\n\nHow do we translate this into Bitcoin SCRIPT?\n\n* `Z` == Any arbitrary 160-bit number.\n* `S` == `OP_HASH160`.\n\nThus:\n\n* `0` == `Z`\n* `1` == `hash160(Z)`\n* `2` == `hash160(hash160(Z))`\n* `3` == `hash160(hash160(hash160(Z)))`\n* etc.\n\nIn particular:\n\n* We can increment a number by simply doing `OP_HASH160`.\n* We can decrement a number by having the supposed\n  decrementation be supplied on the witness stack, then\n  validating that it is indeed the next lower number by\n  hashing the witness item and comparing it to the number\n  we have.\n\nNote also that ***we do not need `OP_ADD` or `OP_SUB` for\nthis***, though that would actually make it simpler.\n(But yeah, the whole point is that *BITCOIN IS A LOT MORE\nPOWERFUL THAN YOU EXPECT*.)\n\nThis is relevant to us due to how sidechain-to-mainchain\npegs are implemented.\n\nDrivechain Peg-Out\n------------------\n\nIn Drivechains, first somebody proposes to withdraw some\namount of funds from the sidechain to a mainchain address.\nThen mainchain miners enter a voting period, during\nwhich they either agree to the withdrawal, or disagree.\n\nWe can use the above schema to keep track of a running\ntotal number of votes.\n\nWe define some numbers:\n\n* `Z` == `0`\n* `P` == some maximum time period.\n\nWe then encode `Z`, `P / 2`, and `P` using the hashed-Peano\nencoding in the previous subsection.\n\nIn order to allow withdrawals, we have an alternate branch,\nsuch as a different Tapleaf, for a withdrawal SCRIPT.\nThis only requires that the first output has the same address\nas itself (i.e. the sidechain covenant), and the second output\nhas a new recursive covenant, the peg-out covenant.\n\nThe peg-out covenant has three loop variables:\n\n* `v`, initialized to `Z`.\n  * This is the \"validity level\" of the peg-out.\n  * Voters who want to vote \"for validity\" would *increment*\n    this count.\n  * Voters who want to vote \"against validity\" would\n    *do nothing*.\n* `t`, initialized to `Z`.\n  * This is the voting time period.\n  * Each time the peg-out covenant is used, this loop\n    variable is incremented.\n  * Once it reaches `P`, voting ends and the voting\n    branches of the peg-out covenant are disabled,\n* `a`, initialized to the peg-out address.\n  * This is not actually changed in the covenant, but\n    it is useful to keep it in the loop variable storage\n    area.\n  * With `OP_CTV` this can be an address that commits to\n    any number of promised outputs.\n\nThe peg-out covenant has these branches:\n\n* If `v` equals `P / 2`, then the UTXO can be spent to the\n  address `a`.\n  This is synthesized with an `OP_CTV` and `OP_CAT`.\n* If `t` equals `P`, then the UTXO can only be spent\n  by being pegged into the sidechain covenant.\n  If this branch is not entered, we increment `t`.\n  * This implies an inter-recursion between the sidechain\n    covenant and the peg-out covenant.\n* Check if the witness stack top is true or not:\n  * If true, increment `v` and recurse (\"vote-for\" branch).\n  * Else just recurse (\"vote-against\" branch).\n\n### Fixing Inter-recursion\n\nWe can observe that the inter-recursion between the sidechain\ncovenant and the peg-out covenant is problematic:\n\n* `OP_CTV` requires that the hash of the output covenant is\n  known.\n* `OP_TLUV` will only replace the same output index as the\n  input index it is on.\n\nThis prevents the inter-recursion between the sidechain\ncovenant and the peg-out covenant.\n\nTo fix this, we can observe that we can translate any set\nof inter-recursive functions, such as this:\n\n```Haskell\nfoo :: FooArg -> Result\nfoo fa = bar (fooCode fa)\nbar :: BarArg -> Result\nbar ba = foo (barCode ba)\n```\n\n...into a single self-recursive function:\n\n```Haskell\nfooBar :: Either FooArg BarArg -> Result\nfooBar a = case a of\n             Left  fa -> fooBar (Right (fooCode fa))\n             Right ba -> fooBar (Left (barCode ba))\n```\n\nSimilarly, we can instead convert the inter-recursive\nsidechain and peg-out covenants into a single\nself-recursive covenant.\n\nThis single covenant would have the same set of loop\nvariables `v`, `t`, and `a` as the peg-out covenant\ndescribed above.\nThis time, `a` is not an address, but an entire output\n(i.e. `scriptPubKey` and `amount`).\n\nBy default, `v`, `t`, and `a` are a number `0`.\nIf so, then there is no pending peg-out being voted on.\n\nIf there is no pending peg-out, then either we just\ncommit to a sidechain block, or we commit to a sidechain\nblock *and* start a new peg-out by filling in `a`, and\ninitializing `v` and `t` to `Z`.\n\nIf there is a pending peg-out, then either we just commit\nto a sidechain block (and implicitly downvote the pending\npeg-out) or commit to a sidechain block *and* indicate an\nupvote of the pending peg-out.\n\nIf `v` has reached the limit then we require, using\n`OP_CTV`, that `a` appear on the second output, and that\nthe same SCRIPT (with `v`, `t`, and `a` reseet to `0`)\nis on the first output, and do not impose any minimum\nvalue for the first output, and the sidechain commitment\nis now an `OP_RETURN` on the third output, and no other\noutputs.\n\nIf `t` has reached the limit, then we require simply that\nthe `v`, `t`, and `a` are reset to 0 and the sidechain\ncommitment.\n\nWith the above, all components of Drivechain are implementable\nwith:\n\n* `OP_TLUV`\n* `OP_CAT`\n* `OP_CTV`\n* `OP_IN_OUT_AMOUNT` of some kind, including the ability to\n  check the output amount is larger than the input amount\n  (e.g. by `OP_EQUAL` or `OP_GREATER`).\n* Existing Bitcoin SCRIPT (`OP_ADD` **not** needed!).\n\nConclusion\n==========\n\nPH34R THE RECURSIVE COVENANT!\nPH34R!!!!!!!"
            },
            {
                "author": "Paul Sztorc",
                "date": "2022-02-23T18:14:53",
                "message_text_only": "On 2/23/2022 6:28 AM, ZmnSCPxj via bitcoin-dev wrote:\n\n> ... Drivechains is implementable on a Turing-complete\n> language.\n> And we have already rejected Drivechains, for the following reason:\n>\n> 1.  Sidechain validators and mainchain miners have a strong incentive to\n>      merge their businesses.\n> 2.  Mainchain miners end up validating and commiting to sidechain blocks.\n> 3.  Ergo, sidechains on Drivechains become a block size increase.\n\nIs this indeed the reason? Because it is not a good one.\n\nFirst, (as always) we must ignore BIP 301*. (Since it was invented to cancel point 1. Which it does -- by giving an incentive for side-validators and main-miners to UN-merge their businesses.)\n\nWith that out of the way, let's swap \"blocksize increase\" for \"mining via natural gas flaring\" :\n\n1. Oil drillers and mainchain miners have a strong incentive** to merge their businesses.\n2. Mainchain miners end up drilling for oil.\n3. Ergo, sidechains on Drivechains become a requirement, that full nodes mine for oil.\n\nThe above logic is flawed, because full nodes can ignore the mining process. Nodes outrank miners.\n\nMerged mining is, in principle, no different from any other source of mining profitability. I believe there is an irrational prejudice against merged mining, because MM takes the form of software. It would be like an NFL referee who refuses to allow their child to play an NFL videogame, on the grounds that the reffing in the game is different from how the parent would ref. But that makes no difference to anything. The only relevant issue is if the child has fun playing the videogame.\n\n(And of course, merged mining long predates drivechain, and miners are MMing now, and have been for years. It was Satoshi who co-invented merged mining, so the modern prejudice against it is all the more mysterious.)\n\n> Also:\n>\n> 1.  The sidechain-to-mainchain peg degrades the security of sidechain\n>      users from consensus \"everyone must agree to the rules\" to democracy\n>      \"if enough enfranchised voters say so, they can beat you up and steal\n>      your money\".\n>\n> In this write-up, I will...\n\nThis is also a mischaracterization.\n\nDrivechain will not work if 51% hashrate is attacking the network. But that is the case for everything, including the Lightning Network***.\n\nSo there is no sense in which the security is \"degraded\". To establish that, one would need arguments about what will probably happen and why. Which is exactly what my original Nov 2015 article contains: truthcoin.info/blog/drivechain/#drivechains-security , as does my Peer Review section :https://www.drivechain.info/peer-review/peer-review-new/  \n\n(And, today Largeblocker-types do not have any \"everyone must agree to the rules\" consensus, at all. Anyone who wants to use a sidechain-feature today, must obtain it via Altcoin or via real-world trust. So the current security is \"nothing\" and so it is hard to see how that could be \"degraded\".)\n\n--\n\nI am not sure it is a good use of my time to talk to this list about Drivechain. My Nov 2015 article anticipated all of the relevant misunderstandings. Almost nothing has changed since then.\n\nAs far as I am concerned, Drivechain was simply ahead of its time. Eventually, one or more of the following --the problem of Altcoins, the human desire for freedom and creativity, the meta-consensus/upgrade/ossification problem, the problem of persistently low security budget, and/or the expressiveness of Bitcoin smart contracts-- will force Bitcoiners to relearn drivechain-lore and eventually adopt something drivechain-like. At which point I will write to historians to demand credit. That is my plan so far, at least.\n\n--\n\nAs to the actual content of your post, it seems pro-Drivechain.\n\nAfter all, you are saying that Recursive Covenants --> Turing Completeness --> Drivechain. So, which would you rather have? The hacky, bizzaro, covenant-Drivechain, or my pure optimized transparent Bip300-Drivechain? Seems that this is exactly what I predicted: people eventually reinventing Drivechain.\n\nOn this topic, in 2015-2016 I wrote a few papers and gave a few recorded talks****, in which I compared the uncontrollable destructive chaos of Turing Completeness, to a \"categorical\" Turing Completeness where contracts are sorted by category (ie, all of the BitName contracts in the Namecoin-sidechain, all of the oracle contracts in the oracle sidechain, etc). The categorical strategy allows, paradoxically (and perhaps counterintuitively), for more expressive contracts, since you can prevent smart contracts from attacking each other. (They must have a category, so if they aren't Name-contracts they cannot live in the Namecoin-sidechain -- they ultimately must live in an \"Evil Sidechain\", which the miners have motive and opportunity to simply disable.) If people are now talking about how Turing Completeness can lead to smart contracts attacking each other, then I suppose I was years ahead-of-my-time with that, as well. Incidentally, my conclusion was that this problem is BEST solved by allowing miners to censor contract-categories (aka censor sidechain-categories, aka 'beat people up' as you put it), which is how I invented drivechain in the first place.\n\n*Shrug*,\nPaul\n\n\n\n*A small table which explains how this works:https://github.com/bitcoin/bips/blob/master/bip-0301.mediawiki#notation-and-example\n\n**Doubtless many of you have heard of this new trend: oil drillers encounter unwanted natural gas, in areas where there are no natural gas customers. Instead of waste this gas, they have begun selling it to miners.https://economictimes.indiatimes.com/news/international/business/oil-drillers-and-bitcoin-miners-bond-over-natural-gas/articleshow/82828878.cms  .\n\n***As is well known, it is easy for 51% hashrate to double-spend in the LN, by censoring 'justice transactions'. Moreover, miners seem likely to evade retribution if they do this, as they can restrain the scale, timing, victims, circumstances etc of the attack.\n\n****https://www.youtube.com/watch?v=xGu0o8HH10U&list=PLw8-6ARlyVciMH79ZyLOpImsMug3LgNc4&index=1\nhttps://www.truthcoin.info/blog/contracts-oracles-sidechains/\nhttps://www.truthcoin.info/blog/drivechain-op-code/\nhttps://www.truthcoin.info/blog/wise-contracts/\n\n\n\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220223/c5e2b726/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-24T02:20:30",
                "message_text_only": "Good morning Paul, welcome back, and the list,\n\n\nFor the most part I am reluctant to add Turing-completeness due to the Principle of Least Power.\n\nWe saw this play out on the web browser technology.\nA full Turing-complete language was included fairly early in a popular HTML implementation, which everyone else then copied.\nIn the beginning, it had very loose boundaries, and protections against things like cross-site scripting did not exist.\nEventually, W3C cracked down and modern JavaScript is now a lot more sandboxed than at the beginning --- restricting its power.\nIn addition, for things like \"change the color of this bit when the mouse hovers it\", which used to be implemented in JavaScript, were moved to CSS, a non-Turing-complete language.\n\nThe Principle of Least Power is that we should strive to use the language with *only what we need*, and naught else.\n\nSo I think for the most part that Turing-completeness is dangerous.\nThere may be things, other than Drivechain, that you might object to enabling in Bitcoin, and if those things can be implemented in a Turing-complete language, then they are likely implementable in recursive covenants.\n\nThat the web *started* with a powerful language that was later restricted is fine for the web.\nAfter all, the main use of the web is showing videos of attractive female humans, and cute cats.\n(WARNING: WHEN I TAKE OVER THE WORLD, I WILL TILE IT WITH CUTE CAT PICTURES.)\n(Note: I am not an AI that seeks to take over the world.)\nBut Bitcoin protects money, which I think is more important, as it can be traded not only for videos of attractive female humans, and cute cats, but other, lesser things as well.\nSo I believe some reticence towards recursive covenants, and other things it may enable, is warranted,\n\nPrinciple of Least Power exists, though admittedly, this principle was developed for the web.\nThe web is a server-client protocol, but Bitcoin is peer-to-peer, so it seems certainly possible that Principle of Least Power does not apply to Bitcoin.\nAs I understand it, however, the Principle of Least Power exists *precisely* because increased power often lets third parties do more than what was expected, including things that might damage the interests of the people who allowed the increased power to exist, or things that might damage the interests of *everyone*.\n\nOne can point out as well, that despite the problems that JavaScript introduced, it also introduced GMail and the now-rich Web ecosystem.\n\nPerhaps one might liken recursive covenants to the box that was opened by Pandora.\nOnce opened, what is released cannot be put back.\nYet perhaps at the bottom of this box, is Hope?\n\n\n\nAlso: Go not to the elves for counsel, for they will say both no and yes.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-02-24T06:53:05",
                "message_text_only": "On Wed, Feb 23, 2022 at 11:28:36AM +0000, ZmnSCPxj via bitcoin-dev wrote:\n> Subject: Turing-Completeness, And Its Enablement Of Drivechains\n\n> And we have already rejected Drivechains,\n\nThat seems overly strong to me.\n\n> for the following reason:\n> 1.  Sidechain validators and mainchain miners have a strong incentive to\n>     merge their businesses.\n> 2.  Mainchain miners end up validating and commiting to sidechain blocks.\n> 3.  Ergo, sidechains on Drivechains become a block size increase.\n\nI think there are two possible claims about drivechains that would make\nthem unattractive, if true:\n\n 1) that adding a drivechain is a \"block size increase\" in the sense\n    that every full node and every miner need to do more work when\n    validating a block, in order to be sure whether the majority of hash\n    rate will consider it valid, or will reject it and refuse to build\n    on it because it's invalid because of some external drivechain rule\n\n 2) that funds deposited in drivechains will be stolen because\n    the majority of hashrate is not enforcing drivechain rules (or that\n    deposited funds cannot be withdrawn, but will instead be stuck in\n    the drivechain, rather than having a legitimate two-way peg)\n\nAnd you could combine those claims, saying that one or the other will\nhappen (depending on whether more or less than 50% of hashpower is\nenforcing drivechain rules), and either is bad, even though you don't\nknow which will happen.\n\nI believe drivechain advocates argue a third outcome is possible where\nneither of those claims hold true, where only a minority of hashrates\nneeds to validate the drivechain rules, but that is still sufficient\nto prevent drivechain funds from being stolen.\n\nOne way to \"reject\" drivechains is simply to embrace the second claim --\nthat putting money into drivechains isn't safe, and that miners *should*\nclaim coins that have been drivehcain encumbered (or that miners\nshould not assist with withdrawing funds, leaving them trapped in the\ndrivechain). In some sense this is already the case: bip300 rules aren't\nenforced, so funds committed today via bip300 can likely expect to be\nstolen, and likely won't receive the correct acks, so won't progress\neven if they aren't stolen.\n\n\n\nI think a key difference between tx-covenant based drivechains and bip300\ndrivechains is hashpower endorsement: if 50% of hashpower acks enforcement\nof a new drivechain (as required in bip300 for a new drivechain to exist\nat all), there's an implicit threat that any block proposing an incorrect\nwithdrawal from that blockchain will have their block considered invalid\nand get reorged out -- either directly by that hashpower majority, or\nindirectly by users conducting a UASF forcing the hashpower majority to\nreject those blocks.\n\nI think removing that implicit threat changes the game theory\nsubstantially: rather than deposited funds being withdrawn due to the\ndrivechain rules, you'd instead expect them to be withdrawn according to\nwhoever's willing to offer the miners the most upfront fees to withdraw\nthe funds.\n\nThat seems to me to mean you'd frequently expect to end up in a scorched\nearth scenario, where someone attempts to steal, then they and the\nlegitimate owner gets into a bidding war, with the result that most\nof the funds end up going to miners in fees. Because of the upfront\npayment vs delayed collection of withdrawn funds, maybe it could end up\nas a dollar auction, with the two parties competing to lose the least,\nbut still both losing substantial amounts?\n\nSo I think covenant-based drivechains would be roughly the same as bip300\ndrivechains, where a majority of hashpower used software implementing\nthe following rules:\n\n - always endorse any proposed drivechain\n - always accept any payment into a drivechain\n - accept bids to ack/nack withdrawals, then ack/nack depending on\n   whoever pays the most\n\nYou could probably make covenant-based drivechains a closer match to\nbip300 drivechains if a script could determine if an input was from a\n(100-block prior) coinbase or not.\n\n> Logically, if the construct is general enough to form Drivechains, and\n> we rejected Drivechains, we should also reject the general construct.\n\nNot providing X because it can only be used for E, may generalise to not\nproviding Y which can also only be used for E, but it doesn't necessarily\ngeneralise to not providing Z which can be used for both G and E.\n\nI think it's pretty reasonable to say:\n\n a) adding dedicated consensus features for drivechains is a bad idea\n    in the absence of widespread consensus that drivechains are likely\n    to work as designed and be a benefit to bitcoin overall\n\n b) if you want to risk your own funds by leaving your coins on an\n    exchange or using lightning or eltoo or tumbling/coinjoin or payment\n    pools or drivechains or being #reckless in some other way, and aren't\n    asking for consensus changes, that's your business\n\nCheers,\naj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-24T12:03:32",
                "message_text_only": "Good morning aj,\n\n> > Logically, if the construct is general enough to form Drivechains, and\n> > we rejected Drivechains, we should also reject the general construct.\n>\n> Not providing X because it can only be used for E, may generalise to not\n> providing Y which can also only be used for E, but it doesn't necessarily\n> generalise to not providing Z which can be used for both G and E.\n\nDoes this not work only if the original objection to merging in BIP-300 was of the form:\n\n* X implements E.\n* Z implements G and E.\n* Therefore, we should not merge in X and instead should merge in the more general construct Z.\n\n?\n\nWhere:\n\n* E = Drivechains\n* X = BIP-300\n* Z = some general computation facility\n* G = some feature.\n\nBut my understanding is that most of the NACKs on the BIP-300 were of the form:\n\n* X implements E.\n* E is bad.\n* Therefore, we should not merge in X.\n\nIf the above statement \"E is bad\" holds, then:\n\n* Z implements G and E.\n* Therefore, we should not merge in Z.\n\nWhere Z = something that implements recursive covenants.\n\nI think we really need someone who NACKed BIP-300 to speak up.\nIf my understanding is correct and that the original objection was \"Drivechains are bad for reasons R[0], R[1]...\", then:\n\n* You can have either of these two positions:\n  * R[0], R[1] ... are specious arguments and Drivechains are not bad, therefore we can merge in a feature that enables Recursive Covenants -> Turing-Completeness -> Drivechains.\n    * Even if you NACKed before, you *are* allowed to change your mind and move to this position.\n  * R[0], R[1] ... are valid arguments are Drivechains are bad, therefore we should **NOT** merge in a feature that implements Recursive Covenants -> Turing-Completeness -> Drivechains.\n\nYou cannot have it both ways.\nAdmittedly, there may be some set of restrictions that prevent Turing-Completeness from implementing Drivechains, but you have to demonstrate a proof of that set of restrictions existing.\n\n> I think it's pretty reasonable to say:\n>\n> a) adding dedicated consensus features for drivechains is a bad idea\n> in the absence of widespread consensus that drivechains are likely\n> to work as designed and be a benefit to bitcoin overall\n>\n> b) if you want to risk your own funds by leaving your coins on an\n> exchange or using lightning or eltoo or tumbling/coinjoin or payment\n> pools or drivechains or being #reckless in some other way, and aren't\n> asking for consensus changes, that's your business\n\n*Shrug* I do not really see the distinction here --- in a world with Drivechains, you are free to not put your coins in a Drivechain-backed sidechain, too.\n\n(Admittedly, Drivechains does get into a Mutually Assured Destruction argument, so that may not hold.\nBut if Drivechains going into a MAD argument is an objection, then I do not see why covenant-based Drivechains would also not get into the same MAD argument --- and if you want to avoid the MADness, you cannot support recursive covenants, either.\nRemember, 51% attackers can always censor the blockchain, regardless of whether you put the Drivechain commitments into the coinbase, or in an ostensibly-paid-by-somebody-else transaction.)\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-02-26T05:38:03",
                "message_text_only": "@ZmnSCPxj\n> we have already rejected Drivechains\n\nI also think this is kind of dubious. I don't remember consensus being to\n\"reject\" drivechains, as much as consensus was that it wasn't a priority\nand there wasn't a lot of interest in doing on it from many people (I'm\nsure Paul could comment further on that).\n\n> sidechains on Drivechains become a block size increase.\n\nWhile this would be true for those who opt into a particular drivechain, I\nthink its important to note that it would *not* be identical to a\nmain-chain block size increase in a very important way: normal bitcoin\nminers and nodes nodes that don't care about drivechains would not see a\nblocksize increase.\n\nBut even in the hypothetical scenario where *all* mainchain miners expand\ntheir business to sidechains, it still does not negatively affect normal\nbitcoin nodes that don't care about drivechains. The important things\n<https://github.com/fresheneesz/bitcoinThroughputAnalysis> about a \"normal\"\nblocksize increase are:\n\nA. It increases the machine resources necessary for IBD, transaction relay,\nand validation\nB. It probably increases the growth rate of the UTXO set, increasing memory\nnecessary to store that.\nC. It increases the cost of storing the blockchain on non-pruned nodes\nD. It increases the average propagation time of new blocks, which increases\nminer centralization pressure.\n\nThe existence of drivechains with every miner opted into (some of) them\nwould only negatively impact item D. Normal bitcoin nodes wouldn't need to\nuse any extra resources if they don't care about drivechains. And miners\nwould only have additional centralization pressure proportional to what\ndrivechains they're opted into. The reason for that is that if a miner is\nopted into drivechain X, and propagation of transaction data for\ndrivechain X is significantly slower than the normal bitcoin network, a\nminer may not have the latest drivechain X block to merge mine on top of.\nHowever that miner can still mine bitcoin with no additional latency, and\nso that centralization pressure is minimal unless a significant fraction of\nthe miner's revenue comes from drivechains with slow data propagation.\nBeyond that, by my calculations, miner centralization is quite far from\nbeing significantly affected by blocksize increases. So unless drivechains\nbecome the dominant use case of the bitcoin blockchain, this really isn't\nsomething that I expect to cause any substantial miner centralization or\nother blocksize related problems.\n\nZmnSCPaj, are you arguing that drivechains are bad for bitcoin or are you\narguing that it would be unwise to opt into a drivechain? Those are very\ndifferent arguments. If drivechains compromised things for normal bitcoin\nnodes that ignore drivechains, then I agree that would be serious reason to\nreject drivechains outright and reject things that allow it to happen.\nHowever, if all you're saying is that people can shoot themselves in the\nfoot with drivechains, then avoiding drivechains should not be a\nsignificant design consideration for bitcoin but rather for those who might\nconsider spending their time working on drivechains.\n\nOn Thu, Feb 24, 2022 at 6:03 AM ZmnSCPxj via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Good morning aj,\n>\n> > > Logically, if the construct is general enough to form Drivechains, and\n> > > we rejected Drivechains, we should also reject the general construct.\n> >\n> > Not providing X because it can only be used for E, may generalise to not\n> > providing Y which can also only be used for E, but it doesn't necessarily\n> > generalise to not providing Z which can be used for both G and E.\n>\n> Does this not work only if the original objection to merging in BIP-300\n> was of the form:\n>\n> * X implements E.\n> * Z implements G and E.\n> * Therefore, we should not merge in X and instead should merge in the more\n> general construct Z.\n>\n> ?\n>\n> Where:\n>\n> * E = Drivechains\n> * X = BIP-300\n> * Z = some general computation facility\n> * G = some feature.\n>\n> But my understanding is that most of the NACKs on the BIP-300 were of the\n> form:\n>\n> * X implements E.\n> * E is bad.\n> * Therefore, we should not merge in X.\n>\n> If the above statement \"E is bad\" holds, then:\n>\n> * Z implements G and E.\n> * Therefore, we should not merge in Z.\n>\n> Where Z = something that implements recursive covenants.\n>\n> I think we really need someone who NACKed BIP-300 to speak up.\n> If my understanding is correct and that the original objection was\n> \"Drivechains are bad for reasons R[0], R[1]...\", then:\n>\n> * You can have either of these two positions:\n>   * R[0], R[1] ... are specious arguments and Drivechains are not bad,\n> therefore we can merge in a feature that enables Recursive Covenants ->\n> Turing-Completeness -> Drivechains.\n>     * Even if you NACKed before, you *are* allowed to change your mind and\n> move to this position.\n>   * R[0], R[1] ... are valid arguments are Drivechains are bad, therefore\n> we should **NOT** merge in a feature that implements Recursive Covenants ->\n> Turing-Completeness -> Drivechains.\n>\n> You cannot have it both ways.\n> Admittedly, there may be some set of restrictions that prevent\n> Turing-Completeness from implementing Drivechains, but you have to\n> demonstrate a proof of that set of restrictions existing.\n>\n> > I think it's pretty reasonable to say:\n> >\n> > a) adding dedicated consensus features for drivechains is a bad idea\n> > in the absence of widespread consensus that drivechains are likely\n> > to work as designed and be a benefit to bitcoin overall\n> >\n> > b) if you want to risk your own funds by leaving your coins on an\n> > exchange or using lightning or eltoo or tumbling/coinjoin or payment\n> > pools or drivechains or being #reckless in some other way, and aren't\n> > asking for consensus changes, that's your business\n>\n> *Shrug* I do not really see the distinction here --- in a world with\n> Drivechains, you are free to not put your coins in a Drivechain-backed\n> sidechain, too.\n>\n> (Admittedly, Drivechains does get into a Mutually Assured Destruction\n> argument, so that may not hold.\n> But if Drivechains going into a MAD argument is an objection, then I do\n> not see why covenant-based Drivechains would also not get into the same MAD\n> argument --- and if you want to avoid the MADness, you cannot support\n> recursive covenants, either.\n> Remember, 51% attackers can always censor the blockchain, regardless of\n> whether you put the Drivechain commitments into the coinbase, or in an\n> ostensibly-paid-by-somebody-else transaction.)\n>\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220225/80a77548/attachment.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-02-26T06:00:40",
                "message_text_only": "On Thu, Feb 24, 2022 at 12:03:32PM +0000, ZmnSCPxj via bitcoin-dev wrote:\n> > > Logically, if the construct is general enough to form Drivechains, and\n> > > we rejected Drivechains, we should also reject the general construct.\n> > Not providing X because it can only be used for E, may generalise to not\n> > providing Y which can also only be used for E, but it doesn't necessarily\n> > generalise to not providing Z which can be used for both G and E.\n> Does this not work only if the original objection to merging in BIP-300 was of the form:\n> * X implements E.\n> * Z implements G and E.\n> * Therefore, we should not merge in X and instead should merge in the more general construct Z.\n\nI'd describe the \"original objection\" more as \"E is not worth doing;\nX achieves nothing but E; therefore we should not work on or merge X\".\n\nWhether we should work on or eventually merge some other construct that\ndoes other things than E, depends on the (relative) merits of those\nother things.\n\n> I think we really need someone who NACKed BIP-300 to speak up.\n\nHere's some posts from 2017:\n\n] I think it's great that people want to experiment with things like\n] drivechains/sidechains and what not, but their security model is very\n] distinct from Bitcoin's and, given the current highly centralized\n] mining ecosystem, arguably not very good.  So positioning them as a\n] major solution for the Bitcoin project is the wrong way to go. Instead\n] we should support people trying cool stuff, at their own risk.\n\n - https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-July/014726.html\n\n] Regardless, people are free experiment and adopt such an approach. The\n] nice thing about it not being a hardfork is that it does not require\n] network-wide consensus to deploy. However, I don't think they offer a\n] security model that should be encouraged, and thus doesn't have a\n] place on a roadmap.\n\n - https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-July/014729.html\n\n> If my understanding is correct and that the original objection was \"Drivechains are bad for reasons R[0], R[1]...\", then:\n> * You can have either of these two positions:\n>   * R[0], R[1] ... are specious arguments and Drivechains are not bad [...]\n>   * R[0], R[1] ... are valid arguments are Drivechains are bad, therefore we should **NOT** merge in a feature that implements Recursive Covenants [...]\n> You cannot have it both ways.\n\nI guess you mean to say that I've got to pick one, rather than can't\npick both. But in any event, I don't pick either; my view is more along\nthe lines of:\n\n * drivechains shouldn't be used\n * it's okay if other people think drivechains are worth using, and go\n   ahead and do so, if they're not creating a direct burden on everyone\n   else\n\nThat's the same position I hold for other things, like using lightning\non mainnet in January 2018; or giving your bitcoin to an anonymous\ncustodian so it it can be borrowed via a flash loan on some novel third\nparty smart contract platform.\n\n> Admittedly, there may be some set of restrictions that prevent Turing-Completeness from implementing Drivechains, but you have to demonstrate a proof of that set of restrictions existing.\n\nLike I said; I don't think the drivechains game theory works without\nthe implicit threat of miner censorship, and therefore you need a\n\"from_coinbase\" flag as well as covenants. That's not a big objection,\nthough. (On the other hand, if I'm wrong and drivechains *do* work\nwithout that threat; then drivechains don't cause a block size increase,\nand can be safely ignored by miners and full node operators, and the\narguments against drivechains are specious; and implementing them purely\nvia covenants so miners aren't put in a privileged position seems an\nimprovement)\n\n> > I think it's pretty reasonable to say:\n> >\n> > a) adding dedicated consensus features for drivechains is a bad idea\n> > in the absence of widespread consensus that drivechains are likely\n> > to work as designed and be a benefit to bitcoin overall\n> >\n> > b) if you want to risk your own funds by leaving your coins on an\n> > exchange or using lightning or eltoo or tumbling/coinjoin or payment\n> > pools or drivechains or being #reckless in some other way, and aren't\n> > asking for consensus changes, that's your business\n> \n> *Shrug* I do not really see the distinction here --- in a world with Drivechains, you are free to not put your coins in a Drivechain-backed sidechain, too.\n\nWell, yes: I'm saying there's no distinction between putting funds in\ndrivechains and other #reckless things you might do with your money?\n\nMy opinion is (a) we should be conservative about adding new consensus\nfeatures because of the maintenance cost; (b) we should design\nconsensus/policy in a way to encourage minimising the externality costs\nusers impose on each other; and (c) we should make it as easy as possible\nto use bitcoin safely in general -- but if people *want* to be reckless,\neven knowing the consequences, that's fine.\n\n> (Admittedly, Drivechains does get into a Mutually Assured Destruction argument, so that may not hold.\n> But if Drivechains going into a MAD argument is an objection, then I do not see why covenant-based Drivechains would also not get into the same MAD argument --- and if you want to avoid the MADness, you cannot support recursive covenants, either.\n\nI think the argument you believe, but aren't quite actually making,\nis along the lines of:\n\n a) drivechain technology doen't just potentially harm people who use\n    them; it is an existential threat to bitcoin if used by anyone\n\n b) therefore the ability for anyone to implement them must be prevented\n\n c) (a) is well known and widely agreed upon by all reasonable\n    well-informed people\n\n(b) is definitely a reasonable consequence of (a), but I don't agree\nwith (a).  Drivechains have certainly been criticised as a bad idea,\nbut there are plenty of bad ideas that don't need to be outlawed.\n\nBut I think the simplest *method* of preventing drivechains from having\nsignificant adoption is just \"users encourage miners to steal funds\ndeposited into drivechains\" (eg, by declining to do a UASF to prevent\nsuch theft), which then obviously discourages people from putting funds\ninto drivechains. Since that can still be done even if bip300 or an\nimplementation of drivechains-via-covenants is deployed, I don't think\ndrivechains are an existential threat to bitcoin.\n\n> Remember, 51% attackers can always censor the blockchain, regardless of whether you put the Drivechain commitments into the coinbase, or in an ostensibly-paid-by-somebody-else transaction.)\n\nI think you could make the analogy between drivechains and covenants a\nfair bit stronger in the following way:\n\nThe idea behind drivechains and the liquid sidechain is, in both cases,\nthat funds can be moved to some other blockchain with its own rules, and\nthen moved back to the bitcoin blockchain, via the assistance of some\ngroup that will hopefully follow the stated rules of the sidechain. In\nliquid's case it's a group of semi-known functionaries who are also\ndirectly responsible for transactions appearing on the liquid sidechain,\nand it's done by them signing via multisig. For bip300, it's bitcoin\nminers, and done by putting entries in the coinbase.\n\nBut because just letting any miner alone immediately move funds\nwould be obviously too risky to consider, bip300 adds additional\nrestrictions, adding both multisig-like aspects, delays, and the ability\nto back-out/correct a theft attempt before it's final, which provides\nthe opportunity for honest participants to react to miners attempting to\ncheat and hopefully achieve a legitimate outcome instead. Whether that's\nenough is still debatable -- but it's certainly an improvement to go from\n\"too risky to consider\" to \"debatable\".\n\nBut the same incentive can apply to liquid too: it might be good to be\nable to have liquid funds encumbered on the bitcoin blockchain in such a\nway that it's even harder for people with liquid's private keys to cheat\nthan it currently is -- ie, it would be good to be able to specify more\n\"vault-like\" behaviours for the liquid funds, perhaps in relation to the\n\"backup recovery keys\" [0], eg.\n\nAs a result, while it's not obvious, I think it shouldn't be *surprising*\nthat the same technology that allows \"vaults\" also enables (something\nlike) drivechains -- since the goal in both cases is just constraining\nhow withdrawals work.\n\nCheers,\naj\n\n[0] https://medium.com/blockstream/patching-the-liquid-timelock-issue-b4b2f5f9a973"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-26T06:43:52",
                "message_text_only": "Good morning AJ,\n\n> ZmnSCPaj, are you arguing that drivechains\u00a0are bad for bitcoin or are you arguing that it would be unwise to opt into a drivechain? Those are very different arguments. If drivechains\u00a0compromised things for normal bitcoin nodes that ignore drivechains, then I agree that would be serious\u00a0reason to reject drivechains\u00a0outright and reject things that allow it to happen. However, if all you're saying is that people can shoot themselves in the foot with drivechains, then avoiding drivechains\u00a0should not be a significant design consideration for bitcoin but rather for those who might consider spending their time working on drivechains.\n\nNeither.\nMy argument is simply:\n\n* If Drivechains are bad for whatever reason, we should not add recursive covenants.\n* Otherwise, go ahead and add recursive covenants.\n\nDrivechains are not a scaling solution [FOOTNOTE 1] and I personally am interested only in scaling solutions, adding more non-scaling-useable functionality is not of interest to me and I do not really care (but I would *prefer* if people focus on scaling-useable functionality, like `SIGHASH_NOINPUT`, `OP_EVICT`, `OP_CTV`, `OP_TLUV` probably without the self-replace capability).\n\nI bring this up simply because I remembered those arguments against Drivechains, and as far as I could remember, those were the reasons for not adding Drivechains.\nBut if there is consensus that those arguments are bogus, then go ahead --- add Drivechains and/or recursive covenants.\nI do not intend to utilize them any time soon anyway.\n\nMy second position is that in general I am wary of adding Turing-completeness, due precisely to Principle of Least Power.\nA concern is that, since it turns out recursive covenants are sufficient to implement Drivechains, recursive covenants may also enable *other* techniques, currently unknown, which may have negative effects on Bitcoin, or which would be considered undesirable by a significant section of the userbase.\nOf course, I know of no such technique, but given that a technique (Drivechains) which before would have required its own consensus change, turns out to be implementable inside recursive covenants, then I wonder if there are other things that would have required their own consensus change that are now *also* implementable purely in recursive covenants.\n\nOf course, that is largely just stop energy, so if there is *now* consensus that Drivechains are not bad, go ahead, add recursive covenants (but please can we add `SIGHASH_NOINPUT` and `OP_CTV` first?).\n\nRegards,\nZmnSCPxj\n\n[FOOTNOTE 1] Sidechains are not a scaling solution, or at least, are beaten in raw scaling by Lightning.  Blockchains are inefficient (THAT IS PRECISELY THE PROBLEM WHY YOU NEED A SCALING SOLUTION FOR BITCOIN THAT WAS LIKE THE FIRST RESPONSE TO SATOSHI ON THE CYPHERPUNK MAILING LIST) and you have to show your transaction to everyone.  While sidechains imply that particular subsets are the only ones interested in particular transactions, compare how large a sidechain-participant-set would be expected to be, to how many people learn of a payment over the Lightning Network.  If you want a sidechain to be as popular as LN, then you expect its participant set to be about as large as LN as well, and on a sidechain, a transaction is published to all sidechain participants, but on the LN, only a tiny tiny tiny fraction of the network is involved in any payment.  Thus LN is a superior scaling solution.  Now you might conter-argue that you can have multiple smaller sidechains and just use HTLCs to trade across them (i.e. microchains).  I would then counter-counter-argue that bringing this to the most extreme conclusion, you would have tons of sidechains with only 2 participants each, and then you would pay by transferring across multiple participants in a chain of HTLCs and look, oh wow, surprise surprise, you just got the Lightning Network.  LN wins."
            },
            {
                "author": "Paul Sztorc",
                "date": "2022-02-27T00:58:01",
                "message_text_only": "On 2/26/2022 1:43 AM, ZmnSCPxj via bitcoin-dev wrote:\n\n> ...\n> Drivechains are not a scaling solution [FOOTNOTE 1] ...\n> I personally am interested only in scaling solutions, adding more non-scaling-useable functionality is not of interest to me and I do not really care\n> ...\n> But if there is consensus that those arguments are bogus, then go ahead --- add Drivechains and/or recursive covenants.\n> ...\n>\n> [FOOTNOTE 1] Sidechains are not a scaling solution ... Blockchains are inefficient ... and you have to show your transaction to everyone.\n> ...\n>   Now you might conter-argue that you can have multiple smaller sidechains and just use HTLCs to trade across them ... I would then counter-counter-argue that bringing this to the most extreme conclusion, you would have tons of sidechains with only 2 participants each ...\n\nDo you really hang your entire --\"sidechains are not a scaling solution\"-- argument on this frail logic?\n\nThe scaling strategy (in LN and DC) is the same: try NOT to \"show your transaction to everyone\". The details are of course different.\n\nI think largeblock sidechains should be reconsidered:\n  * They are not a blocksize increase.\n  * They endorse the principle of scaling in layers.\n  * They allow users to be different. Some can pay more (for more decentralization), some less (for less decentralization).\n     (We are currently gambling the entire future of BTC, on the premise that strong decentralization will always be needed at all points in time.)\n     (This leaves us vulnerable to a strategy where our adversaries temporarily favor/promote centralized chains, so as to \"domesticate\" / control these in the future.)\n  * We can learn from past mistakes -- when a new largeblock sidechain is needed, we can make a new one from scratch, using everything we know.\n  * Different teams can compete, releasing different chains independently; thus curtailing \"toxicity\".\n  * All of the fees, paid on all blockchains, arrive as revenue to the same group of miners, thus improving total hashrate/difficulty.\n  * Sidechains will organize geographically, which may help security (ie, USA could spitefully run full nodes of the \"China\" largeblock sidechain).\n  * Relative to LN, users enjoy: unlimited \"inbound liquidity\", can receive money while offline, no risk that the channel will close, etc.\n\nCertainly, sidechains are NOT for everyone. (Just as [I imagine] the LN is not for everyone.)\n\nHowever, in 2015, many hardfork-largeblockers said: \"we do not run a full node, full nodes are not important; we use SPV; read the whitepaper\" etc.\nThey used SPV completely; and wanted large blocks. Presumably they would be happy users of a largeblock sidechain. So it would be >0 users.\n\nSadly, this idea is neglected, (I think) because of its unfortunate resemblance to naive-largeblock-ism. This is irrational.\n\n***\n\nYou have emphasized the following relation: \"you have to show your transaction to everyone\" = \"thing doesn't scale\".\n\nHowever, in LN, there is one transaction which you must, in fact, \"show to everyone\": your channel-opener.\n\nAmusingly, in the largeblock sidechain, there is not. You can onboard using only the blockspace of the SC.\n(One \"rich guy\" can first shift 100k coins Main-to-Side, and he can henceforth onboard many users over there. Those users can then onboard new users, forever.)\n\nSo it would seem to me, that you are on the ropes, even by your own criterion. [Footnote 1]\n\n***\n\nPerhaps, someone will invent a way, to LN-onboard WITHOUT needing new layer1 bytes.\n\nIf so, a \"rich man\" could open a LN channel, and gradually transfer it to new people.\n\nSuch a technique would need to meet two requirements (or, so it seems to me):\n#1: The layer1 UTXO (that defines the channel) can never change (ie, the 32-bytes which define the p2sh/tapscript/covenant/whatever, must stay what-they-were when the channel was opened).\n#2: The new part-owners (who are getting coins from the rich man), will have new pubkeys which are NOT known, until AFTER the channel is opened and confirmed on the blockchain.\n\nNot sure how you would get both #1 and #2 at the same time. But I am not up to date on the latest LN research.\n\nPaul\n\n\n[Footnote 1]\nI am certainly not a LN expert, so perhaps this analysis is misconceived. But consider these \"best case scenario\" assumptions for LN:\n  * Each new channel-open consumes just 32 vbytes (since they are all done via one or more \"rich men\" who batches all these into one block, 24/7/365)\n  * Each new channel-open, onboards 5 users at once who are a permanent trust group / channel factory / what-have-you\n       (these five newcomers must coordinate with each other and the \"rich man\", presumably via calendly link or whatever, for their one shot at getting on the blockchain).\n  * That one single channel is able to meet 100% of the user's payment needs\n       (it never has any problems, with liquidity /balancing /routing /uptime /hotwallet-crashing /counterparty-fees /etc)\n       (and also, people do NOT desire >1 channel for other reasons: their alt nyms, small business, church, etc)\n  * 99.9% of the 1MB (vB) blocksize is used for channel-opens (the spare 1000 vb = the coinbase + the single \"rich man\"-input)\n  * World population becomes a fixed 8.2 billion (and henceforth stops growing)\n\nBy simple envelop math, 6*24*365*(((1000000*.999)/32)*5) / 8.2 billion = ~exactly one year to onboard everyone.\nBut if the above assumptions contain, say, two orders of magnitude of \"optimism\", then it would instead take 100 years.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220226/58dfa5d7/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-27T02:00:37",
                "message_text_only": "Good morning Paul,\n\n> ***\n>\n> You have emphasized the following relation: \"you have to show your transaction to everyone\" = \"thing doesn't scale\".\n>\n> However, in LN, there is one transaction which you must, in fact, \"show to everyone\": your channel-opener.\n>\n> Amusingly, in the largeblock sidechain, there is not. You can onboard using only the blockspace of the SC.\n> (One \"rich guy\" can first shift 100k coins Main-to-Side, and he can henceforth onboard many users over there. Those users can then onboard new users, forever.)\n>\n> So it would seem to me, that you are on the ropes, even by your own criterion. [Footnote 1]\n>\n> ***\n>\n> Perhaps, someone will invent a way, to LN-onboard WITHOUT needing new layer1 bytes.\n>\n> If so, a \"rich man\" could open a LN channel, and gradually transfer it to new people.\n>\n> Such a technique would need to meet two requirements (or, so it seems to me):\n> #1: The layer1 UTXO (that defines the channel) can never change (ie, the 32-bytes which define the p2sh/tapscript/covenant/whatever, must stay what-they-were when the channel was opened).\n> #2: The new part-owners (who are getting coins from the rich man), will have new pubkeys which are NOT known, until AFTER the channel is opened and confirmed on the blockchain.\n>\n> Not sure how you would get both #1 and #2 at the same time. But I am not up to date on the latest LN research.\n\nYes, using channel factories.\n\nA channel factory is a N-of-N where N >= 3, and which uses the same offchain technology to host multiple 2-of-2 channels.\nWe observe that, just as an offchain structure like a payment channel can host HTLCs, any offchain structure can host a lot of *other* contracts, because the offchain structure can always threaten to drop onchain to enforce any onchain-enforceable contract.\nBut an offchain structure is just another onchain contract!\nThus, an offchain structure can host many other offchain structures, and thus an N-of-N channel factory can host multiple 2-of-2 channel factories.\n\n(I know we discussed sidechains-within-sidechains before, or at least I mentioned that to you in direct correspondence, this is basically that idea brought to its logical conclusion.)\n\nThus, while you still have to give *one* transaction to all Bitcoin users, that single transaction can back several channels, up to (N * (N - 1)) / 2.\n\nIt is not quite matching your description --- the pubkeys of the peer participants need to be fixed beforehand.\nHowever, all it means is some additional pre-planning during setup with no scope for dynamic membership.\n\nAt least, you cannot dynamically change membership without onchain action.\nYou *can* change membership sets by publishing a one-input-one-output transaction onchain, but with Taproot, the new membership set is representable in a single 32-byte Taproot address onchain (admittedly, the transaction input is a txin and thus has overhead 32 bytes plus 1 byte for txout index, and you need 64 bytes signature for Taproot as well).\nThe advantage is that, in the meantime, if membership set is not changed, payments can occur *without* any data published on the blockchain (literally 0 data).\n\nWith sidechains, changing the ownership set requires that the sidechain produce a block.\nThat block requires a 32-byte commitment in the coinbase.\nWhat is more, if *any* transfers occur on the sidechain, they cannot be real without a sidechain block, that has to be committed on the mainchain.\n\nThus, while changing the membership set of a channel factory is more expensive (it requires a pointer to the previous txout, a 64-byte Taproot signature, and a new Taproot address), continuous operation does not publish any data at all.\nWhile in sidehchains, continuous operation and ordinary payments requires ideally one commitment of 32 bytes per mainchain block.\nContinuous operation of the sidechain then implies a constant stream of 32-byte commitments, whereas continuous operation of a channel factory, in the absence of membership set changes, has 0 bytes per block being published.\n\nWe assume that onboarding new members is much rarer than existing members actually paying each other in an actual economy (after the first burst of onboarding, new members will only arise in proportion to the birth rate, but typical economic transactions occur much more often), so optimizing for the continuous operation seems a better tradeoff.\n\n\nChannel factories have the nice properties:\n\n* N-of-N means that nobody can steal from you.\n  * Even with a 51% miner, nobody can steal from you as long as none of the N participants is the 51% miner, see the other thread.\n* Graceful degradation: even if if 1 of the N is offline, payments are done over the hosted 2-of-2s, and the balance of probability is that most of the 2-of-2s have both participants online and payments can continue to occur.\n\n--\n\nThe reason why channel factories do not exist *yet* is that the main offchain construction we have, Poon-Dryja, is 2-of-2.\nWe have Decker-Wattenhofer, which supports N >= 2, but it needs to publish a lot of onchain data in case of dispute, and has lousy UX due to how it uses delays (you can only be safely offline for some small number of blocks, but you have to wait out a large multiple of that parameter).\n\nWe also have the newer Decker-Russell-Osuntokun (\"eltoo\"), but that requires `SIGHASH_NOINPUT`, which is now today called `SIGHASH_ANYPREVOUT`.\n\n`OP_CTV` also is useful for publishing commitments-to-promised-outputs without having to publish outputs right now.\n\nThis is why I want to focus on getting both on Bitcoin first, *before* any recursive-contract-enabling technologies.\n\nAdmittedly, the recursive-covenant-enabling constructs look like they enable functionality equivalent to `SIGHASH_NOINPUT` and `OP_CTV`, though as I understand them, they would require more bytes than `SIGHASH_NOINPUT` or `OP_CTV`.\nAnd scaling is really improved by reducing the number of bytes published, so there is value in merging in `SIGHASH_ANYPREVOUT` and `OP_CTV` at some point, so why not now.\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-27T07:25:27",
                "message_text_only": "Good morning again Paul,\n\n> With sidechains, changing the ownership set requires that the sidechain produce a block.\n> That block requires a 32-byte commitment in the coinbase.\n> What is more, if any transfers occur on the sidechain, they cannot be real without a sidechain block, that has to be committed on the mainchain.\n\nThe above holds if the mainchain miners also act as sidechain validators.\nIf they are somehow separate (i.e. blind merge mining), then the `OP_BRIBE` transaction needed is also another transaction.\nAssuming the sidechain validator is using Taproot as well, it needs the 32+1 txin, a 64-byte signature, a 32-byte copy of the sidechain commitment that the miner is being bribed to put in the coinbase, and a txout for any change the sidechain validator has.\n\nThis is somewhat worse than the case for channel factories, even if you assume that every block, at least one channel factory has to do an onboarding event.\n\n> Thus, while changing the membership set of a channel factory is more expensive (it requires a pointer to the previous txout, a 64-byte Taproot signature, and a new Taproot address), continuous operation does not publish any data at all.\n> While in sidehchains, continuous operation and ordinary payments requires ideally one commitment of 32 bytes per mainchain block.\n> Continuous operation of the sidechain then implies a constant stream of 32-byte commitments, whereas continuous operation of a channel factory, in the absence of membership set changes, has 0 bytes per block being published.\n>\n> We assume that onboarding new members is much rarer than existing members actually paying each other in an actual economy (after the first burst of onboarding, new members will only arise in proportion to the birth rate, but typical economic transactions occur much more often), so optimizing for the continuous operation seems a better tradeoff.\n\nPerhaps more illustratively, with channel factories, different layers have different actions they can do, and the only one that needs to be broadcast widely are actions on the onchain layer:\n\n* Onchain: onboarding / deboarding\n* Channel Factory: channel topology change\n* Channel: payments\n\nThis is in contrast with merge-mined Sidechains, where *all* activity requires a commitment on the mainchain:\n\n* Onchain: onboarding / deboarding, payments\n\nWhile it is true that all onboarding, deboarding, and payments are summarized in a single commitment, notice how in LN-with-channel-factories, all onboarding / deboarding is *also* summarized, but payments *have no onchain impact*, at all.\n\nWithout channel factories, LN is only:\n\n* Onchain: onboarding / deboarding, channel topology change\n* Channel: payments\n\nSo even without channel factories there is already a win, although again, due to the large numbers of channels we need, a channel factory in practice will be needed to get significantly better scaling.\n\n\nFinally, in practice with Drivechains, starting a new sidechain requires implicit permission from the miners.\nWith LN, new channels and channel factories do not require any permission, as they are indistinguishable from ordinary transactions.\n(the gossip system does leak that a particular UTXO is a particular published channel, but gossip triggers after deep confirmation, at which point it would be too late for miners to censor the channel opening.\nThe miners can censor channel closure for published channels, admittedly, but at least you can *start* a new channel without being censored, which you cannot do with Drivechain sidechains.)\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-02-27T16:59:54",
                "message_text_only": "@Paul\n> I think largeblock sidechains should be reconsidered:\n> * They are not a blocksize increase.\n\nThis is short sighted. They would absolutely be a blocksize increase for\nthose following a large block sidechain. While sure, it wouldn't affect\nbitcoin users who don't follow that sidechain, its misleading to call it\n\"not a blocksize increase\" for everyone.\n\n> * They allow users to be different. Some can pay more (for more decentralization), some less (for less decentralization).\n\n> gambling the entire future of BTC, on the premise that strong decentralization will always be needed at all points in time.\n\nDecentralization isn't just something where more is more valuable and\nless is less valuable. Decentralization is either enough to stop a\nclass of attack or its not. Its pretty binary. If the decentralization\nis not enough, it would be a pretty huge catastrophe for those\ninvolved. Its pretty clear that making the blocksize eg 10 times\nlarger is a poor design choice. So advocating for such a thing on a\nsidechain is just as bad as advocating for it on an altcoin.\n\nEven if people only put a couple satoshis in such a sidechain at a\ntime, and don't feel the loss very much, the *world* would feel the\nloss. Eg if everyone had $1 in such a system, and someone stole it\nall, it would be a theft of billions of dollars. The fact that no\nindividual would feel much pain would make it not much less harmful to\nsociety.\n\n> We can learn from past mistakes -- when a new largeblock sidechain is needed, we can make a new one from scratch, using everything we know.\n\nIf there's some design principles that *allow* for safely increasing the\nblocksize substantially like that, then I'd advocate for it in bitcoin. But\nthe goal of sidechains should not be \"shoot from the hip and after everyone\non that sidechain gets burned we'll have learned valuable lessons\". That's\nnot how engineering works. That's akin to wreckless human experimentation.\n\n\n\nOn Sun, Feb 27, 2022 at 1:25 AM ZmnSCPxj via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Good morning again Paul,\n>\n> > With sidechains, changing the ownership set requires that the sidechain\n> produce a block.\n> > That block requires a 32-byte commitment in the coinbase.\n> > What is more, if any transfers occur on the sidechain, they cannot be\n> real without a sidechain block, that has to be committed on the mainchain.\n>\n> The above holds if the mainchain miners also act as sidechain validators.\n> If they are somehow separate (i.e. blind merge mining), then the\n> `OP_BRIBE` transaction needed is also another transaction.\n> Assuming the sidechain validator is using Taproot as well, it needs the\n> 32+1 txin, a 64-byte signature, a 32-byte copy of the sidechain commitment\n> that the miner is being bribed to put in the coinbase, and a txout for any\n> change the sidechain validator has.\n>\n> This is somewhat worse than the case for channel factories, even if you\n> assume that every block, at least one channel factory has to do an\n> onboarding event.\n>\n> > Thus, while changing the membership set of a channel factory is more\n> expensive (it requires a pointer to the previous txout, a 64-byte Taproot\n> signature, and a new Taproot address), continuous operation does not\n> publish any data at all.\n> > While in sidehchains, continuous operation and ordinary payments\n> requires ideally one commitment of 32 bytes per mainchain block.\n> > Continuous operation of the sidechain then implies a constant stream of\n> 32-byte commitments, whereas continuous operation of a channel factory, in\n> the absence of membership set changes, has 0 bytes per block being\n> published.\n> >\n> > We assume that onboarding new members is much rarer than existing\n> members actually paying each other in an actual economy (after the first\n> burst of onboarding, new members will only arise in proportion to the birth\n> rate, but typical economic transactions occur much more often), so\n> optimizing for the continuous operation seems a better tradeoff.\n>\n> Perhaps more illustratively, with channel factories, different layers have\n> different actions they can do, and the only one that needs to be broadcast\n> widely are actions on the onchain layer:\n>\n> * Onchain: onboarding / deboarding\n> * Channel Factory: channel topology change\n> * Channel: payments\n>\n> This is in contrast with merge-mined Sidechains, where *all* activity\n> requires a commitment on the mainchain:\n>\n> * Onchain: onboarding / deboarding, payments\n>\n> While it is true that all onboarding, deboarding, and payments are\n> summarized in a single commitment, notice how in LN-with-channel-factories,\n> all onboarding / deboarding is *also* summarized, but payments *have no\n> onchain impact*, at all.\n>\n> Without channel factories, LN is only:\n>\n> * Onchain: onboarding / deboarding, channel topology change\n> * Channel: payments\n>\n> So even without channel factories there is already a win, although again,\n> due to the large numbers of channels we need, a channel factory in practice\n> will be needed to get significantly better scaling.\n>\n>\n> Finally, in practice with Drivechains, starting a new sidechain requires\n> implicit permission from the miners.\n> With LN, new channels and channel factories do not require any permission,\n> as they are indistinguishable from ordinary transactions.\n> (the gossip system does leak that a particular UTXO is a particular\n> published channel, but gossip triggers after deep confirmation, at which\n> point it would be too late for miners to censor the channel opening.\n> The miners can censor channel closure for published channels, admittedly,\n> but at least you can *start* a new channel without being censored, which\n> you cannot do with Drivechain sidechains.)\n>\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220227/cd1866f0/attachment-0001.html>"
            },
            {
                "author": "Paul Sztorc",
                "date": "2022-02-27T23:50:54",
                "message_text_only": "On 2/27/2022 11:59 AM, Billy Tetrud via bitcoin-dev wrote:\n\n> @Paul\n> > I think largeblocksidechainsshould be reconsidered:\n> > * They are not a blocksize increase.\n> This is short sighted. They would absolutely be a blocksize increase \n> for those following a large block sidechain. While sure, it wouldn't \n> affect bitcoin users who don't follow that sidechain, its misleading \n> to call it \"not a blocksize increase\" for everyone.\n\nYour larger explanation is entirely correct.\n\nMany of the important anti-largeblock arguments are not relevant to the largeblock sidechain strategy, but some of them still are.\n\nMy concern is that people will jump to conclusions, and use the old 2015 arguments against \"a blocksize increase\" against this idea.\n\nHence my small bullet point.\n\n\n> > * They allow users to be different. Some can pay more (for more decentralization), some less (for less decentralization).\n> > gambling the entire future of BTC, on the premise that strong decentralization will always be needed at all points in time.\n> Decentralization isn't just something where more is more valuable and less is less valuable. Decentralization is either enough to stop a class of attack or its not. Its pretty binary. If the decentralization is not enough, it would be a pretty huge catastrophe for those involved. Its pretty clear that making the blocksize eg 10 times larger is a poor design choice. So advocating for such a thing on a sidechain is just as bad as advocating for it on an altcoin.\n> Even if people only put a couple satoshis in such a sidechain at a time, and don't feel the loss very much, the *world* would feel the loss. Eg if everyone had $1 in such a system, and someone stole it all, it would be a theft of billions of dollars. The fact that no individual would feel much pain would make it not much less harmful to society.\n\nI believe that you have missed my point. Let me try to explain it in more detail.\n\nFirst, imagine a magic spell is cast, which 100% prevents the \"class of attack\" which you mention. In that situation, all of the work that BTC does to remain decentralized, is a pure burden with absolutely no benefit whatsoever. Rational users will then become indifferent to centralization. Since decentralization has tradeoffs, users will tend to be drawn towards 'crypto' projects that have very low decentralization.\n\nNext, imagine that the spell is lifted, and the attacks start. Users will be, of course, drawn back towards BTC, and they will appreciate it for its decentralization.\n\nSo what's the problem? Well, I believe that money has very strong network effects. Therefore, I believe that \"user inertia\" will be much stronger than usual. At a certain critical mass it may be insurmountable. So, at certain points along the spectrum, users will \"clump up\" and get \"stuck\".\n\nThus, we may \"clump\" on a chain that is not the most decentralized one. And an adversary can use this to their advantage. They can \"grow\" the centralized chain at first, to help it, and help ensure that they do not have to deal with the most decentralized chain.\n\nThis entire issue is avoided completely, if all the chains --decentralized and centralized-- and in the same monetary unit. Then, the monetary network effects never interfere, and the decentralized chain is always guaranteed to exist.\n\n\nAs for the phrase \" Its pretty clear that making the blocksize eg 10 times larger is a poor design choice\" I think this entire way of reasoning about the blocksize, is one that only applies to a non-sidechain world.\n\nIn contrast, in a world where many chains can be created, it does not matter what Some Guy thinks is \"pretty clear\". The only thing that matters is that people can try things out, are rewarded for successes, and are punished for mistakes.\n\nSo: if someone makes a largeblock sidechain, and the design is bad, the chain fails, and their reputation suffers.\n\nIn my way-of-reasoning, someone is actually in the wrong, if they proactively censor an experiment of any type. If a creator is willing to stand behind something, then it should be tried.\n\nIn fact, it is often best for everyone (especially the end user), if a creator keeps their ideas secret (from the \"peer review\" community). That way they can at least get credit/glory. The soon-to-be-successful experiments of tomorrow, should be incomprehensible to the experts of today. That's what makes them innovations.\n\n\nFinally, to me it makes no difference if users have their funds stolen from a centralized Solana contract (because there is only one full node which the operator resets), or from a bip300 centralized bit-Solana sidechain (for the same reason). I don't see why the tears shed would be any different.\n\n> > We can learn from past mistakes -- when a new largeblock sidechain is needed, we can make a new one from scratch, using everything we know.\n> If there's some design principles that *allow* for safely increasing the blocksize substantially like that, then I'd advocate for it in bitcoin. But the goal of sidechains should not be \"shoot from the hip and after everyone on that sidechain gets burned we'll have learned valuable lessons\". That's not how engineering works. That's akin to wreckless human experimentation.\n\nAgain, we perhaps have a fundamental disagreement on this point.\n\nIn 2008 a FED chairman might have said to Satoshi, \"If there were design principles that *allowed* for private, digital, bearer-instrument, payments, then of course I'd advocate for it here at the FED. But the goal of bitcoin should not be 'shoot from the hip ...'. That's not how engineering works. That's akin to wreckless human experimentation.\"\n\nI think that the most dangerous experiment of all, is to adopt the 'reckless' policy of suppressing creativity.\n\nIf instead you said something like, \"If a 10x blocksize chain is ever demonstrated to have property XYZ, then I will repent my error by putting my own children to death\", then the audience would at least have some idea of your confidence and sincerity. But, again, a FED chairman could say exactly that, about Bitcoin. And they would still have been wrong. And even if they were right (on a fluke) they would still have been wrong to prevent the idea from being tried.\n\nCensorship (the suppression of ideas, merely because you disagree with them) is not only immoral, on this issue it is also largely pointless. Today, a Bitcoiner can sell their BTC for Solana, or BSV, and there is nothing anyone here can do about it. Altcoin Solana vs bip300 bit-Solana, would seem to be equivalently reckless to me. So, your implicit advice (of bureaucracy-based sidechain drop/add), seems to fail to meet your own criterion (of preventing human recklessness). And it certainly does other bad things for no reason (pumps an altcoin, decreases btc fee revenues /hashrate, etc).\n\n\nPaul\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220227/bb030454/attachment.html>"
            },
            {
                "author": "Paul Sztorc",
                "date": "2022-02-28T00:20:47",
                "message_text_only": "On 2/26/2022 9:00 PM, ZmnSCPxj wrote:\n\n> ...\n>> Such a technique would need to meet two requirements (or, so it seems to me):\n>> #1: The layer1 UTXO (that defines the channel) can never change (ie, the 32-bytes which define the p2sh/tapscript/covenant/whatever, must stay what-they-were when the channel was opened).\n>> #2: The new part-owners (who are getting coins from the rich man), will have new pubkeys which are NOT known, until AFTER the channel is opened and confirmed on the blockchain.\n>>\n>> Not sure how you would get both #1 and #2 at the same time. But I am not up to date on the latest LN research.\n> Yes, using channel factories.\n\nI think you may be wrong about this.\nChannel factories do not meet requirement #2, as they cannot grow to onboard new users (ie, new pubkeys).\nThe factory-open requires that people pay to (for example), a 5-of-5 multisig. So all 5 fixed pubkeys must be known, before the factory-open is confirmed, not after.\n\n\n> We assume that onboarding new members is much rarer than existing members actually paying each other\n\nImagine that Bitcoin could only onboard 5 new users per millennium, but once onboarded they had payment nirvana (could transact hundreds of trillions of times per second, privately, smart contracts, whatever).\nSadly, the payment nirvana would not matter. The low onboarding rate would kill the project.\n\nThe difference between the two rates [onboarding and payment], is not relevant. EACH rate must meet the design goal.\nIt is akin to saying: \" Our car shifts from park to drive in one-millionth of a second, but it can only shift into reverse once per year; but that is OK because 'we assume that going in reverse is much rarer than driving forward' \".\n\n\n> Continuous operation of the sidechain then implies a constant stream of 32-byte commitments, whereas continuous operation of a channel factory, in the absence of membership set changes, has 0 bytes per block being published.\n\nThat's true, but I think you have neglected to actually take out your calculator and run the numbers.\n\nHypothetically, 10 largeblock-sidechains would be 320 bytes per block (00.032%, essentially nothing).\nThose 10, could onboard 33% of the planet in a single month [footnote], even if each sc-onboard required an average of 800 sc-bytes.\n\nCertainly not a perfect idea, as the SC onboarding rate is the same as the payment rate. But once they are onboarded, those users can immediately join the LN *from* their sidechain. (All of the SC LNs would be interoperable.)\n\nSuch a strategy would take enormous pressure *off* of layer1 (relative to the \"LN only\" strategy). The layer1 blocksize could even **shrink** from 4 MB (wu) to 400 kb, or lower. That would cancel out the 320 bytes of overhead, many hundreds of times over.\n\nPaul\n\n[footnote] Envelope math, 10 sidechains, each 50 MB forever-fixed blocksize (which is a mere 12.5x our current 4M wu limit): 10 * 6*24*30 * ((50*1000*1000)/800) / 8.2 billion = .32926\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220227/3ca47607/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-28T06:49:22",
                "message_text_only": "Good morning Paul,\n\n> On 2/26/2022 9:00 PM, ZmnSCPxj wrote:\n>\n> > ...\n> >\n> > > Such a technique would need to meet two requirements (or, so it seems to me):\n> > > #1: The layer1 UTXO (that defines the channel) can never change (ie, the 32-bytes which define the p2sh/tapscript/covenant/whatever, must stay what-they-were when the channel was opened).\n> > > #2: The new part-owners (who are getting coins from the rich man), will have new pubkeys which are NOT known, until AFTER the channel is opened and confirmed on the blockchain.\n> > >\n> > > Not sure how you would get both #1 and #2 at the same time. But I am not up to date on the latest LN research.\n> >\n> > Yes, using channel factories.\n>\n> I think you may be wrong about this.\n> Channel factories do not meet requirement #2, as they cannot grow to onboard new users (ie, new pubkeys).\n> The factory-open requires that people pay to (for example), a 5-of-5 multisig. So all 5 fixed pubkeys must be known, before the factory-open is confirmed, not after.\n\nI am not wrong about this.\nYou can cut-through the closure of one channel factory with the opening of another channel factory with the same 5 fixed pubkeys *plus* an additional 100 new fixed pubkeys.\nWith `SIGHASH_ANYPREVOUT` (which we need to Decker-Russell-Osuntokun-based channel factories) you do not even need to make new signatures for the existing channels, you just reuse the existing channel signatures and whether or not the *single*, one-input-one-output, close+reopen transaction is confirmed or not, the existing channels remain usable (the signatures can be used on both pre-reopen and post-reopen).\n\nThat is why I said changing the membership set requires onchain action.\nBut the onchain action is *only* a 1-input-1-output transaction, and with Taproot the signature needed is just 64 bytes witness (1 weight unit per byte), I had several paragraphs describing that, did you not read them?\n\nNote as well that with sidechains, onboarding also requires action on the mainchain, in the form of a sideblock merge-mined on the mainchain.\n\n>\n> > We assume that onboarding new members is much rarer than existing members actually paying each other\n>\n> Imagine that Bitcoin could only onboard 5 new users per millennium, but once onboarded they had payment nirvana (could transact hundreds of trillions of times per second, privately, smart contracts, whatever).\n> Sadly, the payment nirvana would not matter. The low onboarding rate would kill the project.\n\nFortunately even without channel factories the onboarding rate of LN is much much higher than that.\nI mean, like, LN *is* live and *is* working, today, and (at least where I have looked, but I could be provincial) has a lot more onboarding activity than half-hearted sidechains like Liquid or Rootstock.\n\n> The difference between the two rates [onboarding and payment], is not relevant. EACH rate must meet the design goal.\n> It is akin to saying: \" Our car shifts from park to drive in one-millionth of a second, but it can only shift into reverse once per year; but that is OK because 'we assume that going in reverse is much rarer than driving forward' \".\n\nYour numbers absolutely suck and have no basis in reality, WTF.\nEven without batched channel openings and a typical tranaction of 2 inputs, 1 LN channel, and a change output, you can onboard ~1250 channels per mainchain block (admittedly, without any other activity).\nLet us assume every user needs 5 channels on average and that is still 250 users per 10 minutes.\nI expect channel factories to increase that by about 10x to 100x more, and then you are going to hit the issue of getting people to *use* Bitcoin rather than many users wanting to get in but being unable to due to block size limits.\n\n>\n> > Continuous operation of the sidechain then implies a constant stream of 32-byte commitments, whereas continuous operation of a channel factory, in the absence of membership set changes, has 0 bytes per block being published.\n>\n> That's true, but I think you have neglected to actually take out your calculator and run the numbers.\n>\n> Hypothetically, 10 largeblock-sidechains would be 320 bytes per block (00.032%, essentially nothing).\n> Those 10, could onboard 33% of the planet in a single month [footnote], even if each sc-onboard required an average of 800 sc-bytes.\n>\n> Certainly not a perfect idea, as the SC onboarding rate is the same as the payment rate. But once they are onboarded, those users can immediately join the LN *from* their sidechain. (All of the SC LNs would be interoperable.)\n>\n> Such a strategy would take enormous pressure *off* of layer1 (relative to the \"LN only\" strategy). The layer1 blocksize could even **shrink** from 4 MB (wu) to 400 kb, or lower. That would cancel out the 320 bytes of overhead, many hundreds of times over.\n>\n> Paul\n>\n> [footnote] Envelope math, 10 sidechains, each 50 MB forever-fixed blocksize (which is a mere 12.5x our current 4M wu limit): 10 * 6*24*30 * ((50*1000*1000)/800) / 8.2 billion = .32926\n\nYes, and 33% of the planet want to use Bitcoin in the next month.\n\nThe onboarding rate only needs to be as fast as the rate at which people want to join Bitcoin, and any security you sacrifice in order to get a higher number than that is security you are sacrificing needlessly for extra capacity you are unable to utilize.\n\nAs I pointed out in the other thread:\n\n* LN:\n  * Funds can be stolen IF:\n    * There is a 51% miner, AND\n    * The 51% miner is a member of a channel/channel factory you are in.\n* Drivechains:\n  * Funds can be stolen IF:\n    * There is a 51% miner.\n\nNow of course there is always the possibility that the 51% miner is in *every* channel factory globally.\nBut there is also the possibility that the 51% miner exists, but is *not* on every channel factory.\nIndeed, for any arbitrary channel or factory, I expect that the probability of the 51% miner being a member is less than 100%, thus the combined probability is lower than Drivechains.\n\nSo there is a real degradation of security in Drivechains, and if you compute the numbers, I am reasonably sure that 33% of the world is unlikely to want to use Bitcoin within one month.\nI mean we already had a pandemic and everyone going online and so on, and yet Bitcoin blockchain feerates are *still* small, I had to fix a bug in CLBOSS that came up only due to hitting the minimum feerate, so no --- people are not joining Bitcoin at a rate faster than Bitcoin + LN can handle it, even with a pretty good reason to move payments online.\n\nWorse, once 100% of the world is onboarded, the extra onboarding capacity is useless since the onboarding rate can only match the birth rate (including birth of legal persons such as corporations), which we expect is much lower than 33% increase per ***month***.\n\nYou are buying too much capacity at a real degradation in security, and I am not convinced the extra capacity is worth the loss of security.\n\nSeparating the onboarding rate from the payment rate is a *good thing*, because we can then design their structures differently.\nMake onboarding slow but secure (so that their money is very secure), but make payment rate faster and less secure (because in-flight payments are likely to be much smaller than the total owned funds).\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2022-02-28T07:55:29",
                "message_text_only": "> Continuous operation of the sidechain then implies a constant stream of 32-byte commitments, whereas continuous operation of a channel factory, in the absence of membership set changes, has 0 bytes per block being published.\n\nThe sidechain can push zero bytes on-chain, just by placing a sidechain hash in OP_RETURN inside TapScript. Then, every sidechain node can check that \"this sidechain hash is connected with this Taproot address\", without pushing 32 bytes on-chain.\n\nOn 2022-02-28 08:13:03 user ZmnSCPxj via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Good morning Paul,\n\n> On 2/26/2022 9:00 PM, ZmnSCPxj wrote:\n>\n> > ...\n> >\n> > > Such a technique would need to meet two requirements (or, so it seems to me):\n> > > #1: The layer1 UTXO (that defines the channel) can never change (ie, the 32-bytes which define the p2sh/tapscript/covenant/whatever, must stay what-they-were when the channel was opened).\n> > > #2: The new part-owners (who are getting coins from the rich man), will have new pubkeys which are NOT known, until AFTER the channel is opened and confirmed on the blockchain.\n> > >\n> > > Not sure how you would get both #1 and #2 at the same time. But I am not up to date on the latest LN research.\n> >\n> > Yes, using channel factories.\n>\n> I think you may be wrong about this.\n> Channel factories do not meet requirement #2, as they cannot grow to onboard new users (ie, new pubkeys).\n> The factory-open requires that people pay to (for example), a 5-of-5 multisig. So all 5 fixed pubkeys must be known, before the factory-open is confirmed, not after.\n\nI am not wrong about this.\nYou can cut-through the closure of one channel factory with the opening of another channel factory with the same 5 fixed pubkeys *plus* an additional 100 new fixed pubkeys.\nWith `SIGHASH_ANYPREVOUT` (which we need to Decker-Russell-Osuntokun-based channel factories) you do not even need to make new signatures for the existing channels, you just reuse the existing channel signatures and whether or not the *single*, one-input-one-output, close+reopen transaction is confirmed or not, the existing channels remain usable (the signatures can be used on both pre-reopen and post-reopen).\n\nThat is why I said changing the membership set requires onchain action.\nBut the onchain action is *only* a 1-input-1-output transaction, and with Taproot the signature needed is just 64 bytes witness (1 weight unit per byte), I had several paragraphs describing that, did you not read them?\n\nNote as well that with sidechains, onboarding also requires action on the mainchain, in the form of a sideblock merge-mined on the mainchain.\n\n>\n> > We assume that onboarding new members is much rarer than existing members actually paying each other\n>\n> Imagine that Bitcoin could only onboard 5 new users per millennium, but once onboarded they had payment nirvana (could transact hundreds of trillions of times per second, privately, smart contracts, whatever).\n> Sadly, the payment nirvana would not matter. The low onboarding rate would kill the project.\n\nFortunately even without channel factories the onboarding rate of LN is much much higher than that.\nI mean, like, LN *is* live and *is* working, today, and (at least where I have looked, but I could be provincial) has a lot more onboarding activity than half-hearted sidechains like Liquid or Rootstock.\n\n> The difference between the two rates [onboarding and payment], is not relevant. EACH rate must meet the design goal.\n> It is akin to saying: \" Our car shifts from park to drive in one-millionth of a second, but it can only shift into reverse once per year; but that is OK because 'we assume that going in reverse is much rarer than driving forward' \".\n\nYour numbers absolutely suck and have no basis in reality, WTF.\nEven without batched channel openings and a typical tranaction of 2 inputs, 1 LN channel, and a change output, you can onboard ~1250 channels per mainchain block (admittedly, without any other activity).\nLet us assume every user needs 5 channels on average and that is still 250 users per 10 minutes.\nI expect channel factories to increase that by about 10x to 100x more, and then you are going to hit the issue of getting people to *use* Bitcoin rather than many users wanting to get in but being unable to due to block size limits.\n\n>\n> > Continuous operation of the sidechain then implies a constant stream of 32-byte commitments, whereas continuous operation of a channel factory, in the absence of membership set changes, has 0 bytes per block being published.\n>\n> That's true, but I think you have neglected to actually take out your calculator and run the numbers.\n>\n> Hypothetically, 10 largeblock-sidechains would be 320 bytes per block (00.032%, essentially nothing).\n> Those 10, could onboard 33% of the planet in a single month [footnote], even if each sc-onboard required an average of 800 sc-bytes.\n>\n> Certainly not a perfect idea, as the SC onboarding rate is the same as the payment rate. But once they are onboarded, those users can immediately join the LN *from* their sidechain. (All of the SC LNs would be interoperable.)\n>\n> Such a strategy would take enormous pressure *off* of layer1 (relative to the \"LN only\" strategy). The layer1 blocksize could even **shrink** from 4 MB (wu) to 400 kb, or lower. That would cancel out the 320 bytes of overhead, many hundreds of times over.\n>\n> Paul\n>\n> [footnote] Envelope math, 10 sidechains, each 50 MB forever-fixed blocksize (which is a mere 12.5x our current 4M wu limit): 10 * 6*24*30 * ((50*1000*1000)/800) / 8.2 billion = .32926\n\nYes, and 33% of the planet want to use Bitcoin in the next month.\n\nThe onboarding rate only needs to be as fast as the rate at which people want to join Bitcoin, and any security you sacrifice in order to get a higher number than that is security you are sacrificing needlessly for extra capacity you are unable to utilize.\n\nAs I pointed out in the other thread:\n\n* LN:\n  * Funds can be stolen IF:\n    * There is a 51% miner, AND\n    * The 51% miner is a member of a channel/channel factory you are in.\n* Drivechains:\n  * Funds can be stolen IF:\n    * There is a 51% miner.\n\nNow of course there is always the possibility that the 51% miner is in *every* channel factory globally.\nBut there is also the possibility that the 51% miner exists, but is *not* on every channel factory.\nIndeed, for any arbitrary channel or factory, I expect that the probability of the 51% miner being a member is less than 100%, thus the combined probability is lower than Drivechains.\n\nSo there is a real degradation of security in Drivechains, and if you compute the numbers, I am reasonably sure that 33% of the world is unlikely to want to use Bitcoin within one month.\nI mean we already had a pandemic and everyone going online and so on, and yet Bitcoin blockchain feerates are *still* small, I had to fix a bug in CLBOSS that came up only due to hitting the minimum feerate, so no --- people are not joining Bitcoin at a rate faster than Bitcoin + LN can handle it, even with a pretty good reason to move payments online.\n\nWorse, once 100% of the world is onboarded, the extra onboarding capacity is useless since the onboarding rate can only match the birth rate (including birth of legal persons such as corporations), which we expect is much lower than 33% increase per ***month***.\n\nYou are buying too much capacity at a real degradation in security, and I am not convinced the extra capacity is worth the loss of security.\n\nSeparating the onboarding rate from the payment rate is a *good thing*, because we can then design their structures differently.\nMake onboarding slow but secure (so that their money is very secure), but make payment rate faster and less secure (because in-flight payments are likely to be much smaller than the total owned funds).\n\n\nRegards,\nZmnSCPxj\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Paul Sztorc",
                "date": "2022-02-28T22:54:47",
                "message_text_only": "On 2/28/2022 1:49 AM, ZmnSCPxj wrote:\n\n> ...\n>>>> ...\n>>>>\n>>>> Perhaps, someone will invent a way, to LN-onboard WITHOUT needing new layer1 bytes.\n>>>>\n>>>> If so, a \"rich man\" could open a LN channel, and gradually transfer it to new people.\n>>>>\n>>>> Such a technique would need to meet two requirements (or, so it seems to me):\n>>>> #1: The layer1 UTXO (that defines the channel) can never change (ie, the 32-bytes which define the p2sh/tapscript/covenant/whatever, must stay what-they-were when the channel was opened).\n>>>> #2: The new part-owners (who are getting coins from the rich man), will have new pubkeys which are NOT known, until AFTER the channel is opened and confirmed on the blockchain.\n>>>>\n>>>> Not sure how you would get both #1 and #2 at the same time. But I am not up to date on the latest LN research.\n>>> Yes, using channel factories.\n>> I think you may be wrong about this.\n>> ...\n> I am not wrong about this.\n\nWell, let's take a closer look then.\n\nThe topic was: \"a way, to LN-onboard [a new pubkey] WITHOUT needing new layer1 bytes\".\n\nBy which I meant, that I could generate a new pubkey right now, and add it to the LN, without any onchain action.\n\nI can shorten and restate the two requirements (and reorder them) as:\n#2: Can later add a new public key to the membership set.\n#1: Without an onchain action.\n\nAnd yet you yourself say, very clearly:\n\n> ... That is why I said changing the membership set requires onchain action.\n\nWhich would seem to directly contradict what you say about channel factories.\n\nUnless you can show me how to add my new pubkey_4, to a 3-of-3 channel factory opened last year. Without using an onchain action.\n\nYou seem to want to instead change the subject. (To something like: 'we can do better the rate (32 bytes per 5 onboards), from your footnote'.)\n\nWhich is fine. But it is not what I bought up.\n\n***\n\nIn general, you seem to have a future in mind, where new users onboard via factory.\nFor example, 50,000 new users want to onboard in the next block. These strangers, spontaneously organize into 1000 factories of 55 people each, (50 newbies with zero coins + 5 wealthier BTCs who have lots of coins). They then broadcast into the block and join Bitcoin.\nAnd this one factory provides them with many channels, so it can meet most/all of their needs.\n\nI am not here to critique factories. I was simply observing that your logic \"sidechains don't scale, because you have to share your messages\" is not quite airtight, because in the case of onboarding the situation is reversed and so supports the exact opposite conclusion.\nI believe I have made my point by now. It should be easy for people to see what each of us has in mind, and the strengths and weaknesses.\n\nI am curious about something, though. Maybe you can help me.\nPresumably there are risks to large factories. Perhaps an attacker could join each new factory with just $1 of BTC, spend this $1, and then refuse to cooperate with the factory any further. Thus they can disable the factory at a cost of $1 rented dollar.\nIf 1000 factories are opened per block, this would be 52.5 M factories per year, $52.5 million USD per year to disable all the factories out of spite. (All of which they would eventually get back.) I can think of a few people who might try it.\n\n> I mean, like, LN ... has a lot more onboarding activity than half-hearted sidechains like Liquid or Rootstock.\nI don't see the relevance of this. We are talking about the future (theoretical), not the past (empirical).\nFor example, someone could say \"Ethereum has a lot more onboarding activity than LN ...\" but this would also make no difference to anything.\n\n> ...The onboarding rate only needs to be as fast as the rate at which people want to join Bitcoin.\n> ...\n>\n> As I pointed out in the other thread:\n>\n> * LN:\n>    * Funds can be stolen IF:\n>      * There is a 51% miner, AND\n>      * The 51% miner is a member of a channel/channel factory you are in.\n> * Drivechains:\n>    * Funds can be stolen IF:\n>      * There is a 51% miner.\n> ...\n> So there is a real degradation of security in Drivechains, and if you compute the numbers, I am reasonably sure that 33% of the world is unlikely to want to use Bitcoin within one month.\n> I mean we already had a pandemic and everyone going online and so on, and yet Bitcoin blockchain feerates are *still* small, I had to fix a bug in CLBOSS that came up only due to hitting the minimum feerate, so no --- people are not joining Bitcoin at a rate faster than Bitcoin + LN can handle it, even with a pretty good reason to move payments online.\n>\n> Worse, once 100% of the world is onboarded, the extra onboarding capacity is useless since the onboarding rate can only match the birth rate (including birth of legal persons such as corporations), which we expect is much lower than 33% increase per ***month***.\n>\n> You are buying too much capacity at a real degradation in security, and I am not convinced the extra capacity is worth the loss of security.\n>\n> Separating the onboarding rate from the payment rate is a *good thing*, because we can then design their structures differently.\n> Make onboarding slow but secure (so that their money is very secure), but make payment rate faster and less secure (because in-flight payments are likely to be much smaller than the total owned funds).\n\nObviously I don't agree with any of these sentences (most are irrelevant, some false). But I would only be repeating myself.\n\nPaul\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220228/b74a2e78/attachment-0001.html>"
            },
            {
                "author": "Prayank",
                "date": "2022-02-26T07:47:14",
                "message_text_only": "Good morning ZmnSCPxj,\n\n> Of course, I know of no such technique, but given that a technique (Drivechains) which before would have required its own consensus change, turns out to be implementable inside recursive covenants, then I wonder if there are other things that would have required their own consensus change that are now *also* implementable purely in recursive covenants.\n\n\nAgree. I would be interested to know what is NOT possible once we have recursive covenants.\n\n> if there is *now* consensus that Drivechains are not bad, go ahead, add recursive covenants (but please can we add `SIGHASH_NOINPUT` and `OP_CTV` first?)\n\n\nAgree and I think everything can be done in separate soft forks.\n\n\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220226/42c42eac/attachment.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-02-26T16:18:55",
                "message_text_only": "> If Drivechains are bad for whatever reason, we should not add recursive\ncovenants.\n\nBad \"for who\" was the crux of my question to you. Even if drivechains are\nalways bad for their users, I don't think that's a good enough reason to\nblock things that could allow people to build user-space drivechains, as\nlong as it doesn't negatively affect normal Bitcoin users.\n\n> Drivechains are not a scaling solution\n\nI generally agree, more of a laboratory where many things (including\nscaling solutions) can be tested.\n\n> Principle of Least Power.\nA concern is that, since it turns out recursive covenants are sufficient to\nimplement Drivechains, recursive covenants may also enable *other*\ntechniques, currently unknown, which may have negative effects on Bitcoin,\nor which would be considered undesirable by a significant section of the\nuserbase.\n\nI think the principle of least power is a good one, but it cannot be dogma.\nI think your point about unknown consequences is reasonable and a study on\nthat kind of thing would be quite valuable. The community has discussed it\nmultiple times in the past, and so at least some thought has gone into it,\nwith nothing very strong in opposition that I know of. Has anyone made a\ngood summary/study of the kinds of things recursive covenants allows?\n\nOn Sat, Feb 26, 2022, 02:35 Prayank via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Good morning ZmnSCPxj,\n>\n> > Of course, I know of no such technique, but given that a technique\n> (Drivechains) which before would have required its own consensus change,\n> turns out to be implementable inside recursive covenants, then I wonder if\n> there are other things that would have required their own consensus change\n> that are now *also* implementable purely in recursive covenants.\n>\n>\n> Agree. I would be interested to know what is NOT possible once we have\n> recursive covenants.\n>\n> > if there is *now* consensus that Drivechains are not bad, go ahead, add\n> recursive covenants (but please can we add `SIGHASH_NOINPUT` and `OP_CTV`\n> first?)\n>\n>\n> Agree and I think everything can be done in separate soft forks.\n>\n>\n>\n>\n> --\n> Prayank\n>\n> A3B1 E430 2298 178F\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220226/f09205e1/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Recursive covenant opposition, or the absence thereof, was Re: TXHASH + CHECKSIGFROMSTACKVERIFY in lieu of CTV and ANYPREVOUT",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Prayank",
                "David A. Harding",
                "digital vagabond",
                "darosior",
                "Anthony Towns",
                "Lucky Star",
                "ZmnSCPxj",
                "vjudeu at gazeta.pl",
                "Paul Sztorc",
                "Billy Tetrud",
                "James O'Beirne"
            ],
            "messages_count": 27,
            "total_messages_chars_count": 143092
        }
    },
    {
        "title": "[bitcoin-dev] Recursive covenant opposition, or the absence thereof, was Re:  TXHASH + CHECKSIGFROMSTACKVERIFY in lieu of CTV and ANYPREVOUT",
        "thread_messages": [
            {
                "author": "Jeremy Rubin",
                "date": "2022-02-11T03:42:02",
                "message_text_only": "I don't have a specific response to share at this moment, but I may make\none later.\n\nBut for the sake of elevating the discourse, I'd encourage people\nresponding this to read through\nhttps://rubin.io/bitcoin/2021/12/04/advent-7/ as I think it has some\nhelpful terminology and categorizations.\n\nI bring this up because I think that recursion is often given as a\nshorthand for \"powerful\" because the types of operations that support\nrecursion typically also introduce open ended covenants, unless they are\ndesigned specially not to. As a trivial example a covenant that makes a\ncoin spendable from itself to itself entirely with no authorization is\nrecursive but fully enumerated in a sense and not particularly interesting\nor useful.\n\nTherefore when responding you might be careful to distinguish if it is just\nrecursion which you take issue with or open ended or some combination of\nproperties which severally might be acceptable.\n\nTL;DR there are different properties people might care about that get\nlumped in with recursion, it's good to be explicit if it is a recursion\nissue or something else.\n\nCheers,\n\nJeremy\n\n\nOn Thu, Feb 10, 2022, 4:55 PM David A. Harding <dave at dtrt.org> wrote:\n\n> On Mon, Feb 07, 2022 at 08:34:30PM -0800, Jeremy Rubin via bitcoin-dev\n> wrote:\n> > Whether [recursive covenants] is an issue or not precluding this sort\n> > of design or not, I defer to others.\n>\n> For reference, I believe the last time the merits of allowing recursive\n> covenants was discussed at length on this list[1], not a single person\n> replied to say that they were opposed to the idea.\n>\n> I would like to suggest that anyone opposed to recursive covenants speak\n> for themselves (if any intelligent such people exist).  Citing the risk\n> of recursive covenants without presenting a credible argument for the\n> source of that risk feels to me like (at best) stop energy[2] and (at\n> worst) FUD.\n>\n> -Dave\n>\n> [1]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-July/019203.html\n> [2]\n> http://radio-weblogs.com/0107584/stories/2002/05/05/stopEnergyByDaveWiner.html\n>     (thanks to AJ who told me about stop energy one time when I was\n>     producing it)\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220210/6b1fd62d/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Recursive covenant opposition, or the absence thereof, was Re:  TXHASH + CHECKSIGFROMSTACKVERIFY in lieu of CTV and ANYPREVOUT",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeremy Rubin"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2361
        }
    },
    {
        "title": "[bitcoin-dev] Why CTV, why now?",
        "thread_messages": [
            {
                "author": "Anthony Towns",
                "date": "2022-02-02T01:28:49",
                "message_text_only": "On Wed, Jan 05, 2022 at 02:44:54PM -0800, Jeremy via bitcoin-dev wrote:\n> CTV was an output of my personal \"research program\" on how to make simple\n> covenant types without undue validation burdens. It is designed to be the\n> simplest and least risky covenant specification you can do that still\n> delivers sufficient flexibility and power to build many useful applications.\n\nI believe the new elements opcodes [0] allow simulating CTV on the liquid\nblockchain (or liquid-testnet [1] if you'd rather use fake money but not\nuse Jeremy's CTV signet). It's very much not as efficient as having a\ndedicated opcode, of course, but I think the following script template\nwould work:\n\nINSPECTVERSION SHA256INITIALIZE\nINSPECTLOCKTIME SHA256UPDATEE\nINSPECTNUMINPUTS SCRIPTNUMTOLE64 SHA256UPDATE\nINSPECTNUMOUTPUTS SCRIPTNUMTOLE64 SHA256UPDATE\n\nPUSHCURRENTINPUTINDEX SCRIPTNUMTOLE64 SHA256UPDATE\nPUSHCURRENTINPUTINDEX INSPECTINPUTSEQUENCE SCRIPTNUMTOLE64 SHA256UPDATE\n\n{ for <x> in 0..<numoutputs-1>\n<x> INSPECTOUTPUTASSET CAT SHA256UPDATE\n<x> INSPECTOUTPUTVALUE DROP SIZE SCRIPTNUMTOLE64 SWAP CAT SHA256UPDATE\n<x> INSPECTOUTPUTNONCE SIZE SCRIPTNUMTOLE64 SWAP CAT SHA256UPDATE\n<x> INSPECTOUTPUTSCRIPTPUBKEY SWAP SIZE SCRIPTNUMTOLE64 SWAP CAT CAT SHA256UPDATE\n}\n\nSHA256FINALIZE <expectedhash> EQUAL\n\nProvided NUMINPUTS is one, this also means the txid of the spending tx is\nfixed, I believe (since these are tapoot only opcodes, scriptSig\nmalleability isn't possible); if NUMINPUTS is greater than one, you'd\nneed to limit what other inputs could be used somehow which would be\napplication specific, I think.\n\nI think that might be compatible with confidential assets/values, but\nI'm not really sure.\n\nI think it should be possible to use a similar approach with\nCHECKSIGFROMSTACK instead of \"<expectedhash> EQUAL\" to construct APO-style\nsignatures on elements/liquid. Though you'd probably want to have the\noutput inspction blocks wrapped with \"INSPECTNUMOUTPUTS <x> GREATERTHAN\nIF .. ENDIF\". (In that case, beginning with \"PUSH[FakeAPOSig] SHA256\nDUP SHA256INITIALIZE SHA256UPDATE\" might also be sensible, so you're\nnot signing something that might be misused in a different context later)\n\n\nAnyway, since liquid isn't congested, and mostly doesn't have lightning\nchannels built on top of it, probably the vaulting application is the\nonly interesting one to build on top on liquid today? There's apparently\nabout $120M worth of BTC and $36M worth of USDT on liquid, which seems\nlike it could justify some vault-related work. And real experience with\nCTV-like constructs seems like it would be very informative.\n\nCheers,\naj\n\n[0] https://github.com/ElementsProject/elements/blob/master/doc/tapscript_opcodes.md\n[1] https://liquidtestnet.com/"
            },
            {
                "author": "Jeremy Rubin",
                "date": "2022-02-02T01:43:38",
                "message_text_only": "I agree this emulation seems sound but also tap out at how the CT stuff\nworks with this type of covenant as well.\n\nHappy hacking!\n\nOn Tue, Feb 1, 2022, 5:29 PM Anthony Towns via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Wed, Jan 05, 2022 at 02:44:54PM -0800, Jeremy via bitcoin-dev wrote:\n> > CTV was an output of my personal \"research program\" on how to make simple\n> > covenant types without undue validation burdens. It is designed to be the\n> > simplest and least risky covenant specification you can do that still\n> > delivers sufficient flexibility and power to build many useful\n> applications.\n>\n> I believe the new elements opcodes [0] allow simulating CTV on the liquid\n> blockchain (or liquid-testnet [1] if you'd rather use fake money but not\n> use Jeremy's CTV signet). It's very much not as efficient as having a\n> dedicated opcode, of course, but I think the following script template\n> would work:\n>\n> INSPECTVERSION SHA256INITIALIZE\n> INSPECTLOCKTIME SHA256UPDATEE\n> INSPECTNUMINPUTS SCRIPTNUMTOLE64 SHA256UPDATE\n> INSPECTNUMOUTPUTS SCRIPTNUMTOLE64 SHA256UPDATE\n>\n> PUSHCURRENTINPUTINDEX SCRIPTNUMTOLE64 SHA256UPDATE\n> PUSHCURRENTINPUTINDEX INSPECTINPUTSEQUENCE SCRIPTNUMTOLE64 SHA256UPDATE\n>\n> { for <x> in 0..<numoutputs-1>\n> <x> INSPECTOUTPUTASSET CAT SHA256UPDATE\n> <x> INSPECTOUTPUTVALUE DROP SIZE SCRIPTNUMTOLE64 SWAP CAT SHA256UPDATE\n> <x> INSPECTOUTPUTNONCE SIZE SCRIPTNUMTOLE64 SWAP CAT SHA256UPDATE\n> <x> INSPECTOUTPUTSCRIPTPUBKEY SWAP SIZE SCRIPTNUMTOLE64 SWAP CAT CAT\n> SHA256UPDATE\n> }\n>\n> SHA256FINALIZE <expectedhash> EQUAL\n>\n> Provided NUMINPUTS is one, this also means the txid of the spending tx is\n> fixed, I believe (since these are tapoot only opcodes, scriptSig\n> malleability isn't possible); if NUMINPUTS is greater than one, you'd\n> need to limit what other inputs could be used somehow which would be\n> application specific, I think.\n>\n> I think that might be compatible with confidential assets/values, but\n> I'm not really sure.\n>\n> I think it should be possible to use a similar approach with\n> CHECKSIGFROMSTACK instead of \"<expectedhash> EQUAL\" to construct APO-style\n> signatures on elements/liquid. Though you'd probably want to have the\n> output inspction blocks wrapped with \"INSPECTNUMOUTPUTS <x> GREATERTHAN\n> IF .. ENDIF\". (In that case, beginning with \"PUSH[FakeAPOSig] SHA256\n> DUP SHA256INITIALIZE SHA256UPDATE\" might also be sensible, so you're\n> not signing something that might be misused in a different context later)\n>\n>\n> Anyway, since liquid isn't congested, and mostly doesn't have lightning\n> channels built on top of it, probably the vaulting application is the\n> only interesting one to build on top on liquid today? There's apparently\n> about $120M worth of BTC and $36M worth of USDT on liquid, which seems\n> like it could justify some vault-related work. And real experience with\n> CTV-like constructs seems like it would be very informative.\n>\n> Cheers,\n> aj\n>\n> [0]\n> https://github.com/ElementsProject/elements/blob/master/doc/tapscript_opcodes.md\n> [1] https://liquidtestnet.com/\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220201/54203ebd/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Why CTV, why now?",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Anthony Towns",
                "Jeremy Rubin"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 6187
        }
    },
    {
        "title": "[bitcoin-dev] non-default ports for automatic connections in Bitcoin P2P network",
        "thread_messages": [
            {
                "author": "Vasil Dimov",
                "date": "2022-02-02T13:30:47",
                "message_text_only": "Prayank, thanks for taking the time to inform the wider community.\n\nI just want to clarify to avoid confusion that this is about whether to\nopen automatic outgoing connections to a peer at addr:port if port is\nnot 8333. Right now, Bitcoin Core has a very very strong preference\ntowards peers that listen on port 8333. So, if one listens on !=8333\nthen he is practically not getting any incoming connections (from\nBitcoin Core).\n\nSee the PR for details and justifications:\nhttps://github.com/bitcoin/bitcoin/pull/23542.\n\n-- \nVasil Dimov\ngro.DSBeerF at dv\n%\nDiplomacy is the art of telling people to go to hell in such a way that\nthey ask for directions.\n                -- Winston Churchill\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 1528 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220202/c68b4724/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "non-default ports for automatic connections in Bitcoin P2P network",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Vasil Dimov"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 963
        }
    },
    {
        "title": "[bitcoin-dev] CTV Meeting #2 Summary & Minutes",
        "thread_messages": [
            {
                "author": "Jeremy Rubin",
                "date": "2022-02-02T20:04:59",
                "message_text_only": "This meeting was held January 25th, 2022. The meeting logs are available\nhttps://gnusha.org/ctv-bip-review/2022-01-25.log\n\nPlease review the agenda in conjunction with the notes:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019807.html\n\nFeel free to make any corrections if I did not summarize accurately.\n\nThe next meeting is next Tuesday at 12:00 PT. I will attempt to circulate a\npre-meeting agenda draft shortly.\n\nBest,\n\nJeremy\n\n*Bug Bounty Update:*\n\n   1. Basic Rules set, working to formalize the program.\n   2. It turns out that 1 person allocating ~$10k is easy, a multi\n   stakeholder organization requires more formality.\n   3. 501c3 status / tax deducitbility available.\n   4. See here for more details:\n   https://docs.google.com/document/d/1pN6YzQ6HlR8t_-ZZoEdTegt88w6gJzCkcA_a4IXpKAo/edit\n   5. Rules still subject to change, but issues found under the current\n   descriptions awarded in good faith by me/Ariel for now.\n\n\n\n*Notes from Feedback Review:*\n\n*Luke's Feedback:*\n\n   1. Sentiment that activation / CTV should be discussed somewhat\n   separately.\n   2. Sentiment that having more clear cut use cases is good, no agreement\n   about what venue / type of document those should be (no disagreement really\n   either, just that BIPs might be too formal, but blog posts might not be\n   formal enough).\n\n\n*James' Feedback:*\n\n   1. Sentiment that a minor slowdown isn't problematic, we've done it\n   before for other precomputations.\n   2. James was to spend a bit more time on benchmarking in a more modern\n   segment of the chain (the range he used was slightly irrelevant given low\n   segwit adoption period).\n   3. *After meeting: James' shows updates for CTV don't cause any notable\n   slowdown for current chain segments.*\n\n\n*Peter's Feedback:*\n\n   1. Denial-of-Service concerns seem largely addressed.\n   2. Comment on tests was a result of reviewing outdated branch, not PR.\n   3. Main feedback that \"sticks\" is wanting more use cases to be more clear\n\nI've seen some reviews that almost run into a kind of paradox of choice and\n> are turned off by all the different potential applications. This isn't\n> necessarily a bad thing. As we've seen with Taproot and now even CTV with\n> the DLC post, there are going to be use cases and standards nobody's\n> thought of yet, but having them out there all at once can be difficult for\n> some to digest\n\n\n\n*Sapio*\n\n   1. Sapio can be used today, without CTV.\n   2. Main change with CTV is more \"non-interactivity\".\n   3. Need for a more descriptive terms than \"non-interactive\", e.g.,\n   \"asynchronous non-blocking\", \"independently verifiable\", \"non-stallable\".\n   4. Composability is cool, but people aren't doing that much composable\n   stuff anyways so it's probably under-appreciated.\n\n\n\n*Vaults*\n\n   1. Very strong positive sentiment for Vaults.\n   2. CTV eliminates \"toxic waste\" from the setup of vaults with pre-signed\n   txns / requirement for a RNG.\n   3. CTV/Sapio composability makes vaults somewhat \"BIP Resistant\" because\n   vaults could be customized heavily per user, depending on needs.\n   4. CPFP for smart contracts is in not the best state, improving\n   CPFP/Package relay important for these designs.\n   5. The ability to *prove* vaults constructed correctly w/o toxic waste,\n   e.g., 30 years later, is pretty important for high security uses (as\n   opposed to assume w/ presigned).\n   6. More flexible vaults (e.g., withdraw up to X amount per step v.s.\n   fixed X per step) are desirable, but can be emulated by withdrawing X and\n   sending it somewhere else (e.g. another vault) without major loss of\n   security properties or network load -- more flexible vault covenants have\n   greater space/validation costs v.s. simpler CTV ones.\n\n\n\n*Congestion Control*\n\n   1. Sentiments shared that no one really cares about this issue and it's\n   bad marketing.\n   2. Layer 2 to 1 Index \"21i\" which is how long for a L2 (sidechain,\n   exchange, mining pools, etc) to clear all liabilities to end users (CTV\n   improves this to 1 block, currently clearing out and Exchange could take\n   weeks and also trigger \"thundering herd\" behaviors whereby if the expected\n   time to withdraw becomes too long, you then also need to withdraw).\n   3. Anecdotally, Exchanges seem less interested in congestion control,\n   Mining Pools and Lightning Channel openers seem more into it.\n\n\nMain Issues & Answers:\n\nQ: wallet complexity?\nA: Wallets largely already need to understand most of the logic for CTV,\nshould they be rational\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019756.html\n\nQ: uses more space overall\nA: Doesn't use more space than existing incentive compatible behavior on\nhow you might split up txns already, and even if it did, it's a small\nconstant factor more. See https://utxos.org/analysis/batching_sim/ for more\nanalysis.\n\nQ: block space is cheap right now, why do we need this?\nA: we do not want or expect blockspace to be cheap in the future, we should\nplan for that outcome.\n\nQ: What might adoption look like for businesses / how required is their\nadoption?\nA: Users can request payouts into their own CTV-trees w/o exchanges\nknowing. Exchanges do stand to benefit from this, so they might. They will\nneed to pick a SLA for users to receive until wallet software \"catches up\"\na bit more. SLA's and a gradual low-change path for changing industry norms\ndiscussed more in https://stephanlivera.com/episode/339/\n\nQ (unanswered): Can we show that CTV is the optimal congestion control?\nWhat else might work?\n\n*Payment Pools*\n\n   1. Basically a Congestion Control + Cooperative Close.\n   2. Compose with Channels as leaf nodes.\n   3. CoinJoins can be done into payment pools.\n   4. There are some high level design questions to ask of any payment pool\n   design (see minutes), CTV seems to have OK tradeoffs.\n   5. What is the \"Dunbar's Number\" for how big pools could be? If it's 10\n   users, different design tradeoffs can be made than if it is 100.\n   6. More study to be needed on fund availability tradeoffs between having\n   1 Pool of size O(M) per user, N pools per user of size O(G), etc.\n   7. CTV Pools particularly seem suited for participant privacy compared\n   to other proposals which require all parties knowing all balances for all\n   other parties to be secure.\n   8. Need to better model/discuss alternatives and costs of\n   failure scenarios, e.g. 1 Failure in a TLUV model could mean O(N log N)\n   chainload, unless you precommit to paths for every 1 failure case, 2\n   failure case, etc, which then blows up the costs of each transaction in the\n   unilateral withdraw case. CTV Pools, being simpler have a bit more\n   \"symmetry\" in kickout costs v.s. unilateral withdrawal.\n\n\n*General Discussion:*\n\n   1. Template covenants via APO can be made similar cost to CTV with the\n   addition of OP_GENERATOR (pushes G to the stack) and OP_CAT via `<half\n   sig> OP_G OP_CAT 0x01 OP_G OP_CAT CHECKSIG`, or without CAT by allowing\n   checksig to read R and S separately and getting rid of APO 0x01 prefix tags.\n\n\n\n\n\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220202/4eb51ab2/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "CTV Meeting #2 Summary & Minutes",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeremy Rubin"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 7297
        }
    },
    {
        "title": "[bitcoin-dev] BIP-119 CTV Meeting #3 Draft Agenda for Tuesday February 8th at 12:00 PT",
        "thread_messages": [
            {
                "author": "Jeremy Rubin",
                "date": "2022-02-02T20:29:19",
                "message_text_only": "Bitcoin Developers,\n\nThe 3rd instance of the recurring meeting is scheduled for Tuesday February\n8th at 12:00 PT in channel ##ctv-bip-review in libera.chat IRC server.\n\nThe meeting should take approximately 2 hours.\n\nThe topics proposed to be discussed are agendized below. Please review the\nagenda in advance of the meeting to make the best use of everyone's time.\n\nPlease send me any feedback, proposed topic changes, additions, or\nquestions you would like to pre-register on the agenda.\n\nI will send a reminder to this list with a finalized Agenda in advance of\nthe meeting.\n\nBest,\n\nJeremy\n\n- Bug Bounty Updates (10 Minutes)\n- Non-Interactive Lightning Channels (20 minutes)\n  + https://rubin.io/bitcoin/2021/12/11/advent-14/\n  + https://utxos.org/uses/non-interactive-channels/\n- CTV's \"Dramatic\" Improvement of DLCs (20 Minutes)\n  + Summary: https://zensored.substack.com/p/supercharging-dlcs-with-ctv\n  +\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019808.html\n  + https://rubin.io/bitcoin/2021/12/20/advent-23/\n- PathCoin (15 Minutes)\n  + Summary: A proposal of coins that can be transferred in an offline\nmanner by pre-compiling chains of transfers cleverly.\n  + https://gist.github.com/AdamISZ/b462838cbc8cc06aae0c15610502e4da\n  +\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019809.html\n- OP_TXHASH (30 Minutes)\n  + An alternative approach to OP_CTV + APO's functionality by programmable\ntx hash opcode.\n  + See discussion thread at:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019813.html\n- Emulating CTV for Liquid (10 Minutes)\n  +\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019851.html\n- General Discussion (15 Minutes)\n\nBest,\n\nJeremy\n\n\n\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220202/13198f50/attachment.html>"
            },
            {
                "author": "Jeremy Rubin",
                "date": "2022-02-07T19:10:41",
                "message_text_only": "Reminder:\n\nThis is in ~24 hours.\n\nThere have been no requests to add content to the agenda.\n\nBest,\n\nJeremy\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n\n\nOn Wed, Feb 2, 2022 at 12:29 PM Jeremy Rubin <jeremy.l.rubin at gmail.com>\nwrote:\n\n> Bitcoin Developers,\n>\n> The 3rd instance of the recurring meeting is scheduled for Tuesday\n> February 8th at 12:00 PT in channel ##ctv-bip-review in libera.chat IRC\n> server.\n>\n> The meeting should take approximately 2 hours.\n>\n> The topics proposed to be discussed are agendized below. Please review the\n> agenda in advance of the meeting to make the best use of everyone's time.\n>\n> Please send me any feedback, proposed topic changes, additions, or\n> questions you would like to pre-register on the agenda.\n>\n> I will send a reminder to this list with a finalized Agenda in advance of\n> the meeting.\n>\n> Best,\n>\n> Jeremy\n>\n> - Bug Bounty Updates (10 Minutes)\n> - Non-Interactive Lightning Channels (20 minutes)\n>   + https://rubin.io/bitcoin/2021/12/11/advent-14/\n>   + https://utxos.org/uses/non-interactive-channels/\n> - CTV's \"Dramatic\" Improvement of DLCs (20 Minutes)\n>   + Summary: https://zensored.substack.com/p/supercharging-dlcs-with-ctv\n>   +\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019808.html\n>   + https://rubin.io/bitcoin/2021/12/20/advent-23/\n> - PathCoin (15 Minutes)\n>   + Summary: A proposal of coins that can be transferred in an offline\n> manner by pre-compiling chains of transfers cleverly.\n>   + https://gist.github.com/AdamISZ/b462838cbc8cc06aae0c15610502e4da\n>   +\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019809.html\n> - OP_TXHASH (30 Minutes)\n>   + An alternative approach to OP_CTV + APO's functionality by\n> programmable tx hash opcode.\n>   + See discussion thread at:\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019813.html\n> - Emulating CTV for Liquid (10 Minutes)\n>   +\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019851.html\n> - General Discussion (15 Minutes)\n>\n> Best,\n>\n> Jeremy\n>\n>\n>\n>\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220207/3a9308bb/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "BIP-119 CTV Meeting #3 Draft Agenda for Tuesday February 8th at 12:00 PT",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeremy Rubin"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 4310
        }
    },
    {
        "title": "[bitcoin-dev] Unlimited covenants, was Re: CHECKSIGFROMSTACK/{Verify} BIP for Bitcoin",
        "thread_messages": [
            {
                "author": "Anthony Towns",
                "date": "2022-02-03T06:17:14",
                "message_text_only": "On Mon, Jul 05, 2021 at 09:46:21AM -0400, Matt Corallo via bitcoin-dev wrote:\n> More importantly, AJ's point here neuters anti-covanent arguments rather\n> strongly.\n>\n> On 7/5/21 01:04, Anthony Towns via bitcoin-dev wrote:\n> > In some sense multisig *alone* enables recursive covenants: a government\n> > that wants to enforce KYC can require that funds be deposited into\n> > a multisig of \"2 <recipient> <gov_key> 2 CHECKMULTISIG\", and that\n> > \"recipient\" has gone through KYC. Once deposited to such an address,\n> > the gov can refus to sign with gov_key unless the funds are being spent\n> > to a new address that follows the same rules.\n\nI couldn't remember where I'd heard this, but it looks like I came\nacross it via Andrew Poelstra's \"CAT and Schnorr Tricks II\" post [0]\n(Feb 2021), in which he credits Ethan Heilman for originally coming up\nwith the analogy (in 2019, cf [1]).\n\n[0] https://medium.com/blockstream/cat-and-schnorr-tricks-ii-2f6ede3d7bb5\n[1] https://twitter.com/Ethan_Heilman/status/1194624166093369345\n\nCheers,\naj"
            }
        ],
        "thread_summary": {
            "title": "Unlimited covenants, was Re: CHECKSIGFROMSTACK/{Verify} BIP for Bitcoin",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Anthony Towns"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1035
        }
    },
    {
        "title": "[bitcoin-dev] CTV dramatically improves DLCs",
        "thread_messages": [
            {
                "author": "Lloyd Fournier",
                "date": "2022-02-06T07:18:11",
                "message_text_only": "Hi Jeremy,\n\n\nOn Sat, 29 Jan 2022 at 04:21, Jeremy <jlrubin at mit.edu> wrote:\n\n> Lloyd,\n>\n> This is an excellent write up, the idea and benefits are clear.\n>\n> Is it correct that in the case of a 3/5th threshold it is a total 10x *\n> 30x = 300x improvement? Quite impressive.\n>\n\nYes I think so but I am mostly guessing these numbers. The improvement is\nseveral orders of magnitude. Enough to make almost any payout curve\npossible without UX degredation I think.\n\n\n> I have a few notes of possible added benefits / features of DLCs with CTV:\n>\n> 1) CTV also enables a \"trustless timeout\" branch, whereby you can have a\n> failover claim that returns funds to both sides.\n>\n> There are a few ways to do this:\n>\n> A) The simplest is just an oracle-free <STH(timeout tx)> CTV whereby the\n> timeout transaction has an absolute/relative timelock after the creation of\n> the DLC in question.\n>\n> B) An alternative approach I like is to have the base DLC have a branch\n> `<STH(begin timeout)> CTV` which pays into a DLC that is the exact same\n> except it removes the just-used branch and replaces it with `<STH(timeout\n> tx)> CTV` which contains a relative timelock R for the desired amount of\n> time to resolve. This has the advantage of always guaranteeing at least R\n> amount of time since the Oracles have been claimed to be non-live to\n> \"return funds\"  to parties participating\n>\n>\n> 2) CTV DLCs are non-interactive asynchronously third-party unilaterally\n> creatable.\n>\n> What I mean by this is that it is possible for a single party to create a\n> DLC on behalf of another user since there is no required per-instance\n> pre-signing or randomly generated state. E.g., if Alice wants to create a\n> DLC with Bob, and knows the contract details, oracles, and a key for Bob,\n> she can create the contract and pay to it unilaterally as a payment to Bob.\n>\n> This enables use cases like pay-to-DLC addresses. Pay-to-DLC addresses can\n> also be constructed and then sent (along with a specific amount) to a third\n> party service (such as an exchange or Lightning node) to create DLCs\n> without requiring the third party service to do anything other than make\n> the payment as requested.\n>\n\nThis is an interesting point -- I hadn't thought about interactivity prior\nto this.\n\nI agree CTV makes possible an on-chain DEX kind of thing where you put in\norders by sending txs to a DLC address generated from a maker's public key.\nYou could cancel the order by spending out of it via some cancel path. You\nneed to inform the maker of (i) your public key  (maybe you can use the\nsame public key as one of the inputs) and (ii) the amount the maker is\nmeant to put in (use fixed denominations?).\n\nAlthough that's cool I'm not really a big fan of \"putting the order book\non-chain\" ideas because it brings up some of the problems that EVM DEXs\nhave.\nI like centralized non-custodial order books.\nFor this I don't think that CTV makes a qualitative improvement given we\ncan use ANYONECANPAY to get some non-interactivity.\nFor example here's an alternative design:\n\nThe *taker*  provides a HTTP REST api where you (a maker) can:\n\n1. POST an order using SIGHASH_ANYONECANPAY signed inputs and contract\ndetails needed to generate the single output (the CTV DLC). The maker can\ntake the signatures and complete the transaction (they need to provide an\nexact input amount of course).\n2. DELETE an order -- the maker does some sort of revocation on the DLC\noutput e.g. signs something giving away all the coins in one of the\nbranches. If a malicious taker refuses to delete you just double spend one\nof your inputs.\n\nIf the taker wants to take a non-deleted order they *could* just finish the\ntransaction but if they still have a connection open with the maker then\nthey could re-contact them to do a normal tx signing (rather than useing\nthe ANYONECANPAY signatures).\nThe obvious advantage here is that there are no transactions on-chain\nunless the order is taken.\nAdditionally, the maker can send the same order to multiple takers -- the\ntakers will cancel each other's transactions should they broadcast the\ntransactions.\nLooking forward to see if you can come up with something better than this\nwith CTV.\nThe above is suboptimal as getting both sides to have a change output is\nhard but I think it's also difficult in your suggestion.\nIt might be better to use SIGHASH_SINGLE + ANYONECANPAY so the maker has to\nbe the one to provide the right input amount but the taker can choose their\nchange output and the fee...\n\n\n>\n> 3) CTV DLCs can be composed in interesting ways\n>\n> Options over DLCs open up many exciting types of instrument where Alice\n> can do things like:\n> A) Create a Option expiring in 1 week where Bob can add funds to pay a\n> premium and \"Open\" a DLC on an outcome closing in 1 year\n> B) Create an Option expiring in 1 week where one-of-many Bobs can pay the\n> premium (on-chain DEX?).\n>\n>  See https://rubin.io/bitcoin/2021/12/20/advent-23/ for more concrete\n> stuff around this.\n>\n> There are also opportunities for perpetual-like contracts where you could\n> combine into one logical DLC 12 DLCs closing 1 per month that can either be\n> payed out all at once at the end of the year, or profit pulled out\n> partially at any time earlier.\n>\n> 4) This satisfies (I think?) my request to make DLCs expressible as Sapio\n> contracts in https://rubin.io/bitcoin/2021/12/20/advent-23/\n>\n> 5) An additional performance improvement can be had for iterative DLCs in\n> Lightning where you might trade over a fixed set of attestation points with\n> variable payout curves (e.g., just modifying some set of the CTV points).\n> Defer to you on performance, but this could help enable some more HFT-y\n> experiences for DLCs in LN\n>\n\nI'm not sure what is meant concretely by (5) but I think overall\nperformance is ok here. You will always have 10mins or so to confirm the\nDLC so you can't be too fussy about performance!\n\nLL\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220206/93996aef/attachment.html>"
            },
            {
                "author": "Jeremy Rubin",
                "date": "2022-02-06T17:56:12",
                "message_text_only": "I'm not sure what is meant concretely by (5) but I think overall\nperformance is ok here. You will always have 10mins or so to confirm the\nDLC so you can't be too fussy about performance!\n\n\nI mean that if you think of the CIT points as being the X axis (or\nindependent axes if multivariate) of a contract, the Y axis is the\ndependent variable represented by the CTV hashes.\n\n\nFor a DLC living inside a lightning channel, which might be updated between\nparties e.g. every second, this means you only have to recompute the\ncheaper part of the DLC only if you update the payoff curves (y axis) only,\nand you only have to update the points whose y value changes.\n\nFor on chain DLCs this point is less relevant since the latency of block\nspace is larger.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220206/9a92d215/attachment.html>"
            },
            {
                "author": "Thibaut Le Guilly",
                "date": "2022-02-07T02:30:32",
                "message_text_only": "Hi all,\n\nA lot is being discussed but just wanted to react on some points.\n\n# CSFS\n\nLloyd, good point about CSFS not providing the same privacy benefits, and\nOP_CAT being required in addition. And thanks Philipp for the link to your\npost, it was an interesting read!\n\nJeremy\n>CSFS might have independent benefits, but in this case CTV is not being\nused in the Oracle part of the DLC, it's being used in the user generated\nmapping of Oracle result to Transaction Outcome.\n\nMy point was that CSFS could be used both in the oracle part but also in\nthe transaction restriction part (as in the post by Philipp), but again it\ndoes not really provide the same model as DLC as pointed out by Lloyd.\n\n# Performance\n\nRegarding how much performance benefit this CTV approach would provide,\nwithout considering the benefit of not having to transmit and store a large\nnumber of adaptor signatures, and without considering any further\noptimization of the anticipation points computation, I tried to get a rough\nestimate through some benchmarking. Basically, if I'm not mistaken, using\nCTV we would only have to compute the oracle anticipation points, without\nneeding any signing or verification. I've thus made a benchmark comparing\nthe current approach with signing + verification with only computing the\nanticipation points, for a single oracle with 17 digits and 10000 varying\npayouts (between 45000 and 55000). The results are below.\n\nWithout using parallelization:\nbaseline:                            [7.8658 s 8.1122 s 8.3419 s]\nno signing/no verification:  [321.52 ms 334.18 ms 343.65 ms]\n\nUsing parallelization:\nbaseline:                            [3.0030 s 3.1811 s 3.3851 s]\nno signing/no verification:  [321.52 ms 334.18 ms 343.65 ms]\n\nSo it seems like the performance improvement is roughly 24x for the serial\ncase and 10x for the parallel case.\n\nThe two benchmarks are available (how to run them is detailed in the README\nin the same folder):\n*\nhttps://github.com/p2pderivatives/rust-dlc/blob/ctv-bench-simulation-baseline/dlc-manager/benches/benchmarks.rs#L290\n*\nhttps://github.com/p2pderivatives/rust-dlc/blob/ctv-bench-simulation/dlc-manager/benches/benchmarks.rs#L290\n\nLet me know if you think that's a fair simulation or not. One thing I'd\nlike to see as well is what will be the impact of having a very large\ntaproot tree on the size of the witness data when spending script paths\nthat are low in the tree, and how it would affect the transaction fee. I\nmight try to experiment with that at some point.\n\nCheers,\n\nThibaut\n\nOn Mon, Feb 7, 2022 at 2:56 AM Jeremy Rubin via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> I'm not sure what is meant concretely by (5) but I think overall\n> performance is ok here. You will always have 10mins or so to confirm the\n> DLC so you can't be too fussy about performance!\n>\n>\n> I mean that if you think of the CIT points as being the X axis (or\n> independent axes if multivariate) of a contract, the Y axis is the\n> dependent variable represented by the CTV hashes.\n>\n>\n> For a DLC living inside a lightning channel, which might be updated\n> between parties e.g. every second, this means you only have to recompute\n> the cheaper part of the DLC only if you update the payoff curves (y axis)\n> only, and you only have to update the points whose y value changes.\n>\n> For on chain DLCs this point is less relevant since the latency of block\n> space is larger.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220207/c52fa87e/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "CTV dramatically improves DLCs",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Thibaut Le Guilly",
                "Lloyd Fournier",
                "Jeremy Rubin"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 10802
        }
    },
    {
        "title": "[bitcoin-dev] A suggestion to periodically destroy (or remove to secondary storage for Archiving reasons) dust, Non-standard UTXOs, and also detected burn",
        "thread_messages": [
            {
                "author": "shymaa arafat",
                "date": "2022-02-06T12:41:33",
                "message_text_only": "Dear Bitcoin Developers,\n\n-I think you may remember me sending to you about my proposal to partition\n( and other stuff all about) the UTXO set Merkle in bridge servers\nproviding proofs Stateless nodes.\n-While those previous suggestions might not have been on the most interest\nof core Developers, I think this one I happened to notice is:\n\n-When I contacted bitInfoCharts to divide the first interval of addresses,\nthey kindly did divided to 3 intervals. From here:\nhttps://bitinfocharts.com/top-100-richest-bitcoin-addresses.html\n-You can see that there are *more than* *3.1m addresses* holding \u2264 0.000001\nBTC (1000 Sat) with total value of *14.9BTC*; an average of *473 Sat* per\naddress.\n-Keeping in mind that an address can hold more than 1 UTXO; ie, this is\neven a lowerbound on the number of UTXOs holding such small values.\n-Noticing also that every lightning network transaction adds one dust UTXO\n(actually two one of which is instantly spent, and their dust limit is 333\nSat not even 546), ie, *this number of dust UTXOs will probably increase\nwith time.*\n.\n-Therefore, a simple solution would be to follow the difficulty adjustment\nidea and just *delete all those*, or at least remove them to secondary\nstorage for Archiving with extra cost to get them back, *along with\nnon-standard UTXOs and Burned ones* (at least for publicly known,\npublished, burn addresses). *Benefits are:*\n\n1- you will *relieve* the system state from the burden *of about 3.8m\nUTXOs *\n(*3.148952m*\n+ *0.45m* non-standard\n+ *0.178m* burned\nhttps://blockchair.com/bitcoin/address/1111111111111111111114oLvT2\nhttps://blockchair.com/bitcoin/address/1CounterpartyXXXXXXXXXXXXXXXUWLpVr\nas of today 6Feb2022)\n, a number that will probably increase with time.\n2-You will add to the *scarcity* of Bitcoin even with a very small amount\nlike 14.9 BTC.\n3-You will *remove* away *the risk of using* any of these kinds for\n*attacks* as happened before.\n.\n-Finally, the parameters could be studied for optimal values; I mean the\n1st delete, the periodical interval, and also the delete threshold (maybe\nall holding less than 1$ not just 546 Sat need to be deleted)\n.\nThat's all\nThank you very much\n.\nShymaa M Arafat\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220206/78172c6e/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2022-02-06T17:39:41",
                "message_text_only": "> Dear Bitcoin Developers,\n\n> -When I contacted bitInfoCharts to divide the first interval of addresses, they kindly did divided to 3 intervals. From here:\n> https://bitinfocharts.com/top-100-richest-bitcoin-addresses.html\n> -You can see that there are more than 3.1m addresses holding \u2264 0.000001 BTC (1000 Sat) with total value of 14.9BTC; an average of 473 Sat per address.\n\n> -Therefore, a simple solution would be to follow the difficulty adjustment idea and just delete all those\n\nThat would be a soft-fork, and arguably could be considered theft. While commonly (but non universally) implemented standardness rules may prevent spending them currently, there is no requirement that such a rule remain in place. Depending on how feerate economics work out in the future, such outputs may not even remain uneconomical to spend. Therefore, dropping them entirely from the UTXO set is potentially destroying potentially useful funds people own.\n\n> or at least remove them to secondary storage\n\nCommonly adopted Bitcoin full nodes already have two levels of storage effectively (disk and in-RAM cache). It may be useful to investigate using amount as a heuristic about what to keep and how long. IIRC, not even every full node implementation even uses a UTXO model.\n\n> for Archiving with extra cost to get them back, along with non-standard UTXOs and Burned ones (at least for publicly known, published, burn addresses).\n\nDo you mean this as a standardness rule, or a consensus rule?\n\n* As a standardness rule it's feasible, but it makes policy (further) deviate from economically rational behavior. There is no reason for miners to require a higher price for spending such outputs.\n* As a consensus rule, I expect something like this to be very controversial. There are currently no rules that demand any minimal fee for anything, and given uncertainly over how fee levels could evolve in the future, it's unclear what those rules, if any, should be.\n\nCheers,\n\n--\nPieter"
            },
            {
                "author": "Eric Voskuil",
                "date": "2022-02-06T19:14:28",
                "message_text_only": "> On Feb 6, 2022, at 10:52, Pieter Wuille via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> \ufeff\n>> Dear Bitcoin Developers,\n> \n>> -When I contacted bitInfoCharts to divide the first interval of addresses, they kindly did divided to 3 intervals. From here:\n>> https://bitinfocharts.com/top-100-richest-bitcoin-addresses.html\n>> -You can see that there are more than 3.1m addresses holding \u2264 0.000001 BTC (1000 Sat) with total value of 14.9BTC; an average of 473 Sat per address.\n> \n>> -Therefore, a simple solution would be to follow the difficulty adjustment idea and just delete all those\n> \n> That would be a soft-fork, and arguably could be considered theft. While commonly (but non universally) implemented standardness rules may prevent spending them currently, there is no requirement that such a rule remain in place. Depending on how feerate economics work out in the future, such outputs may not even remain uneconomical to spend. Therefore, dropping them entirely from the UTXO set is potentially destroying potentially useful funds people own.\n> \n>> or at least remove them to secondary storage\n> \n> Commonly adopted Bitcoin full nodes already have two levels of storage effectively (disk and in-RAM cache). It may be useful to investigate using amount as a heuristic about what to keep and how long. IIRC, not even every full node implementation even uses a UTXO model.\n\nYou recall correctly. Libbitcoin has never used a UTXO store. A full node has no logical need for an additional store of outputs, as transactions already contain them, and a full node requires all of them, spent or otherwise.\n\nThe hand-wringing over UTXO set size does not apply to full nodes, it is relevant only to pruning. Given linear worst case growth, even that is ultimately a non-issue.\n\n>> for Archiving with extra cost to get them back, along with non-standard UTXOs and Burned ones (at least for publicly known, published, burn addresses).\n> \n> Do you mean this as a standardness rule, or a consensus rule?\n> \n> * As a standardness rule it's feasible, but it makes policy (further) deviate from economically rational behavior. There is no reason for miners to require a higher price for spending such outputs.\n> * As a consensus rule, I expect something like this to be very controversial. There are currently no rules that demand any minimal fee for anything, and given uncertainly over how fee levels could evolve in the future, it's unclear what those rules, if any, should be.\n> \n> Cheers,\n> \n> --\n> Pieter\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-02-07T14:34:42",
                "message_text_only": "> every lightning network transaction adds one dust UTXO\n\nCould you clarify what you mean here? What dust do lightning transactions\ncreate?\n\nI do think that UTXO set size is something that will need to be addressed\nat some point. I liked the idea of utreexo or some other accumulator as the\nultimate solution to this problem. In the mean time, I kind of agree with\nEric that outputs unlikely to be spent can easily be stored off ram and so\nI wouldn't expect them to really be much of an issue to keep around. 3\nmillion utxos is only like 100MB. If software could be improved to move\ndust off ram, that sounds like a good win tho.\n\nOn Sun, Feb 6, 2022, 13:14 Eric Voskuil via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n>\n> > On Feb 6, 2022, at 10:52, Pieter Wuille via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >\n> > \ufeff\n> >> Dear Bitcoin Developers,\n> >\n> >> -When I contacted bitInfoCharts to divide the first interval of\n> addresses, they kindly did divided to 3 intervals. From here:\n> >> https://bitinfocharts.com/top-100-richest-bitcoin-addresses.html\n> >> -You can see that there are more than 3.1m addresses holding \u2264 0.000001\n> BTC (1000 Sat) with total value of 14.9BTC; an average of 473 Sat per\n> address.\n> >\n> >> -Therefore, a simple solution would be to follow the difficulty\n> adjustment idea and just delete all those\n> >\n> > That would be a soft-fork, and arguably could be considered theft. While\n> commonly (but non universally) implemented standardness rules may prevent\n> spending them currently, there is no requirement that such a rule remain in\n> place. Depending on how feerate economics work out in the future, such\n> outputs may not even remain uneconomical to spend. Therefore, dropping them\n> entirely from the UTXO set is potentially destroying potentially useful\n> funds people own.\n> >\n> >> or at least remove them to secondary storage\n> >\n> > Commonly adopted Bitcoin full nodes already have two levels of storage\n> effectively (disk and in-RAM cache). It may be useful to investigate using\n> amount as a heuristic about what to keep and how long. IIRC, not even every\n> full node implementation even uses a UTXO model.\n>\n> You recall correctly. Libbitcoin has never used a UTXO store. A full node\n> has no logical need for an additional store of outputs, as transactions\n> already contain them, and a full node requires all of them, spent or\n> otherwise.\n>\n> The hand-wringing over UTXO set size does not apply to full nodes, it is\n> relevant only to pruning. Given linear worst case growth, even that is\n> ultimately a non-issue.\n>\n> >> for Archiving with extra cost to get them back, along with non-standard\n> UTXOs and Burned ones (at least for publicly known, published, burn\n> addresses).\n> >\n> > Do you mean this as a standardness rule, or a consensus rule?\n> >\n> > * As a standardness rule it's feasible, but it makes policy (further)\n> deviate from economically rational behavior. There is no reason for miners\n> to require a higher price for spending such outputs.\n> > * As a consensus rule, I expect something like this to be very\n> controversial. There are currently no rules that demand any minimal fee for\n> anything, and given uncertainly over how fee levels could evolve in the\n> future, it's unclear what those rules, if any, should be.\n> >\n> > Cheers,\n> >\n> > --\n> > Pieter\n> >\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220207/42073d60/attachment.html>"
            },
            {
                "author": "shymaa arafat",
                "date": "2022-02-07T16:51:54",
                "message_text_only": "On Mon, Feb 7, 2022, 16:44 Billy Tetrud via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> > every lightning network transaction adds one dust UTXO\n>\n> Could you clarify what you mean here? What dust do lightning transactions\n> create?\n>\nI mean this msg\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-December/019636.html\nEven though, the writer clarified after my enquiry I still think it is the\nsame meaning most of the time only one will be spent. His words:\n..............\n*My statement was technically incorrect, it should have been \"most of the\ntime only one of them is spent\".*\n*But nothing prevents them to be both spent, or none of them to be spent.*\n*They are strictly equivalent, the only difference is the public key that\ncan sign for them: one of these outputs belongs to you, the other belongs\nto your peer.*\n\n*You really cannot distinguish anything when inserting them into the utxo\nset, they are perfectly symmetrical and you cannot know beforehand for sure\nwhich one will be spent.*\n*You can guess which one will be spent most of the time, but your heuristic\nwill never be 100% correct, so I don't think it's worth pursuing.*\n*.........*........\n\n>\n> I do think that UTXO set size is something that will need to be addressed\n> at some point. I liked the idea of utreexo or some other accumulator as the\n> ultimate solution to this problem. In the mean time, I kind of agree with\n> Eric that outputs unlikely to be spent can easily be stored off ram and so\n> I wouldn't expect them to really be much of an issue to keep around. 3\n> million utxos is only like 100MB. If software could be improved to move\n> dust off ram, that sounds like a good win tho.\n>\n> On Sun, Feb 6, 2022, 13:14 Eric Voskuil via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>>\n>>\n>> > On Feb 6, 2022, at 10:52, Pieter Wuille via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> >\n>> > \ufeff\n>> >> Dear Bitcoin Developers,\n>> >\n>> >> -When I contacted bitInfoCharts to divide the first interval of\n>> addresses, they kindly did divided to 3 intervals. From here:\n>> >> https://bitinfocharts.com/top-100-richest-bitcoin-addresses.html\n>> >> -You can see that there are more than 3.1m addresses holding \u2264\n>> 0.000001 BTC (1000 Sat) with total value of 14.9BTC; an average of 473 Sat\n>> per address.\n>> >\n>> >> -Therefore, a simple solution would be to follow the difficulty\n>> adjustment idea and just delete all those\n>> >\n>> > That would be a soft-fork, and arguably could be considered theft.\n>> While commonly (but non universally) implemented standardness rules may\n>> prevent spending them currently, there is no requirement that such a rule\n>> remain in place. Depending on how feerate economics work out in the future,\n>> such outputs may not even remain uneconomical to spend. Therefore, dropping\n>> them entirely from the UTXO set is potentially destroying potentially\n>> useful funds people own.\n>> >\n>> >> or at least remove them to secondary storage\n>> >\n>> > Commonly adopted Bitcoin full nodes already have two levels of storage\n>> effectively (disk and in-RAM cache). It may be useful to investigate using\n>> amount as a heuristic about what to keep and how long. IIRC, not even every\n>> full node implementation even uses a UTXO model.\n>>\n>> You recall correctly. Libbitcoin has never used a UTXO store. A full node\n>> has no logical need for an additional store of outputs, as transactions\n>> already contain them, and a full node requires all of them, spent or\n>> otherwise.\n>>\n>> The hand-wringing over UTXO set size does not apply to full nodes, it is\n>> relevant only to pruning. Given linear worst case growth, even that is\n>> ultimately a non-issue.\n>>\n>> >> for Archiving with extra cost to get them back, along with\n>> non-standard UTXOs and Burned ones (at least for publicly known, published,\n>> burn addresses).\n>> >\n>> > Do you mean this as a standardness rule, or a consensus rule?\n>> >\n>> > * As a standardness rule it's feasible, but it makes policy (further)\n>> deviate from economically rational behavior. There is no reason for miners\n>> to require a higher price for spending such outputs.\n>> > * As a consensus rule, I expect something like this to be very\n>> controversial. There are currently no rules that demand any minimal fee for\n>> anything, and given uncertainly over how fee levels could evolve in the\n>> future, it's unclear what those rules, if any, should be.\n>> >\n>> > Cheers,\n>> >\n>> > --\n>> > Pieter\n>> >\n>> > _______________________________________________\n>> > bitcoin-dev mailing list\n>> > bitcoin-dev at lists.linuxfoundation.org\n>> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220207/cc4fbf85/attachment-0001.html>"
            },
            {
                "author": "yanmaani at cock.li",
                "date": "2022-02-13T09:56:21",
                "message_text_only": "On 2022-02-07 14:34, Billy Tetrud via bitcoin-dev wrote:\n> I do think that UTXO set size is something that will need to be\n> addressed at some point. I liked the idea of utreexo or some other\n> accumulator as the ultimate solution to this problem.\n\nWhat about using economic incentives to disincentivize the creation of \nnew UTXOs? Currently, the fee is only charged per byte of space. What if \nyou instead charged a fee of (bytes*byte_weight + \nnet_utxos*utxo_weight)? For example, if utxo_weight=500, then a \ntransaction that creates 2 new UTXOs would cost as if it were 1 KB in \nsize. And a transaction that consolidated 2 UTXOs into one might even \nget a negative transaction fee (rebate).\n\nTechnologically, you'd implement this by lowering the block size cap by \nmax(0, net_utxos_created*utxo_weight). That would be a soft fork, if \nmaybe a contentious one. It's probably also a good idea to limit it at \n0, separate from consensus issues, because it means you're not \nguaranteed to get back whatever you put into it."
            },
            {
                "author": "shymaa arafat",
                "date": "2022-02-13T13:11:18",
                "message_text_only": "Are you big  Developers aware of what is said in this thread\nhttps://bitcointalk.org/index.php?topic=5385559.new#new\nThat \"Omni\" ALT coin, and all Alt coins and new protocols do create such\nextensive amount of dust that they are thinking of dividing 1 Satoshi to\nfractions or how to accept a UTXO with 0 value????\nIsn't that almost the definition of non-standard transactions; the famous\n2016 email?\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012715.html\n\n\n\nOn Sun, Feb 13, 2022, 13:02 yanmaani--- via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On 2022-02-07 14:34, Billy Tetrud via bitcoin-dev wrote:\n> > I do think that UTXO set size is something that will need to be\n> > addressed at some point. I liked the idea of utreexo or some other\n> > accumulator as the ultimate solution to this problem.\n>\n> What about using economic incentives to disincentivize the creation of\n> new UTXOs? Currently, the fee is only charged per byte of space. What if\n> you instead charged a fee of (bytes*byte_weight +\n> net_utxos*utxo_weight)? For example, if utxo_weight=500, then a\n> transaction that creates 2 new UTXOs would cost as if it were 1 KB in\n> size. And a transaction that consolidated 2 UTXOs into one might even\n> get a negative transaction fee (rebate).\n>\n> Technologically, you'd implement this by lowering the block size cap by\n> max(0, net_utxos_created*utxo_weight). That would be a soft fork, if\n> maybe a contentious one. It's probably also a good idea to limit it at\n> 0, separate from consensus issues, because it means you're not\n> guaranteed to get back whatever you put into it.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220213/ce129380/attachment-0001.html>"
            },
            {
                "author": "shymaa arafat",
                "date": "2022-02-13T05:19:04",
                "message_text_only": "I just want to add an alarming info to this thread...\n\n*There are at least 5.7m UTXOs\u22641000 Sat (~7%), *\n*8.04 m \u22641$ (10%), *\n*13.5m \u2264 0.0001BTC (17%)*\n\nIt seems that bitInfoCharts took my enquiry seriously and added a main link\nfor dust analysis:\nhttps://bitinfocharts.com/top-100-dustiest-bitcoin-addresses.html\nHere, you can see just *the first address contains more than 1.7m dust\nUTXOs*\n(ins-outs =1,712,706 with a few real UTXOs holding the bulk of 415 BTC)\nhttps://bitinfocharts.com/bitcoin/address/1HckjUpRGcrrRAtFaaCAUaGjsPx9oYmLaZ\n\n\u00bb\u00bb\u00bb\u00bb\u00bb\n That's alarming isn't it?, is it due to the lightning networks protocol or\ncould be some other weird activity going on?\n.\nThe following address are similar but less severe\n~394k UTXOs, 170k, 92k, 10*20k, 4or5 *14k,...etc\nadd at least 2.7m UTXOs coming from addresses with a higher balance to the\ninterval numbers here (calculated & mentioned in my previous email)\nhttps://bitinfocharts.com/top-100-richest-bitcoin-addresses.html\n\n\nI think it seems bitInfoCharts will probably make their own report about it\nsoon\n\nRegards\nShymaa M. Arafat\n\nOn Wed, Feb 9, 2022, 07:19 shymaa arafat <shymaa.arafat at gmail.com> wrote:\n\n> If 1 Sat reached 100$, you may adjust the delete( or call it omitting or\n> trimming) threshold, since you will need to acquire decimal places inside\n> the Sat variable too ( people may have TXs less than 100$)\n>\n> -Talking with today's numbers,\n> https://bitinfocharts.com/top-100-richest-bitcoin-addresses.html\n>\n> it is hard to imagine that someone's all holdings in Bitcoin is just \u22641000\n> Sat (3.15 m address) or even \u226410,000 Sat (4.1$, with currently 7.6m\n> addresses in addition to the 3.15m)\n> So we'll just incentivise those people to find a low fee time in say a 6\n> month interval and collect those UTXOs into one of at least 5$\n> (10.86m\u22644.1$) or 1$ (5.248m\u22641$) your decision.\n>\n> -During 4 days after showing the smaller intervals, those \u22641000Sat\n> increase by ~2K everyday with total holding increased by 0.01BTC. Addresses\n> in millions:\n> 3.148, 3.1509, 3.152895, 3.154398\n> Total BTC:\n> 14.91,14.92,14.93,14.94\n>\n> -The number of \u226410,000 Sat increases by 4-8 k per day.\n> Addresses in millions:\n> 7.627477, 7.631436, 7.639287, 7.644925\n> Total BTC\n> 333.5, 333.63, 333.89, 334.1\n>\n> -remember that no. of addresses is a lowerbound on no. of UTXOs; ie., the\n> real numbers could be even more.\n> .\n> + There's also non-standard & burned , yes they're about 0.6m UTXOs, but\n> they're misleading on the status of the value they hold.\n> .\n> At the end, I'm just suggesting...\n> .\n> Regards,\n> Shymaa\n>\n> On Wed, Feb 9, 2022, 00:16 <damian at willtech.com.au> wrote:\n>\n>> Good Morning,\n>>\n>> I wish to point out that because fees are variable there is no reason\n>> fees could not be less than 1 sat in future if fees climb. You may\n>> consider this optimistic but I recall in the first days of Bitcoin when\n>> fees were voluntary. It is not unreasonable provided the fungibility\n>> (money-like-quality) of Bitcoin is maintained for 1 sat to be worth over\n>> $100.00 in the future.\n>>\n>> KING JAMES HRMH\n>> Great British Empire\n>>\n>> Regards,\n>> The Australian\n>> LORD HIS EXCELLENCY JAMES HRMH (& HMRH)\n>> of Hougun Manor & Glencoe & British Empire\n>> MR. Damian A. James Williamson\n>> Wills\n>>\n>> et al.\n>>\n>>\n>> Willtech\n>> www.willtech.com.au\n>> www.go-overt.com\n>> duigco.org DUIGCO API\n>> and other projects\n>>\n>>\n>> m. 0487135719\n>> f. +61261470192\n>>\n>>\n>> This email does not constitute a general advice. Please disregard this\n>> email if misdelivered.\n>> --------------\n>> On 2022-02-06 09:39, Pieter Wuille via bitcoin-dev wrote:\n>> >> Dear Bitcoin Developers,\n>> >\n>> >> -When I contacted bitInfoCharts to divide the first interval of\n>> >> addresses, they kindly did divided to 3 intervals. From here:\n>> >> https://bitinfocharts.com/top-100-richest-bitcoin-addresses.html\n>> >> -You can see that there are more than 3.1m addresses holding \u2264\n>> >> 0.000001 BTC (1000 Sat) with total value of 14.9BTC; an average of 473\n>> >> Sat per address.\n>> >\n>> >> -Therefore, a simple solution would be to follow the difficulty\n>> >> adjustment idea and just delete all those\n>> >\n>> > That would be a soft-fork, and arguably could be considered theft.\n>> > While commonly (but non universally) implemented standardness rules\n>> > may prevent spending them currently, there is no requirement that such\n>> > a rule remain in place. Depending on how feerate economics work out in\n>> > the future, such outputs may not even remain uneconomical to spend.\n>> > Therefore, dropping them entirely from the UTXO set is potentially\n>> > destroying potentially useful funds people own.\n>> >\n>> >> or at least remove them to secondary storage\n>> >\n>> > Commonly adopted Bitcoin full nodes already have two levels of storage\n>> > effectively (disk and in-RAM cache). It may be useful to investigate\n>> > using amount as a heuristic about what to keep and how long. IIRC, not\n>> > even every full node implementation even uses a UTXO model.\n>> >\n>> >> for Archiving with extra cost to get them back, along with\n>> >> non-standard UTXOs and Burned ones (at least for publicly known,\n>> >> published, burn addresses).\n>> >\n>> > Do you mean this as a standardness rule, or a consensus rule?\n>> >\n>> > * As a standardness rule it's feasible, but it makes policy (further)\n>> > deviate from economically rational behavior. There is no reason for\n>> > miners to require a higher price for spending such outputs.\n>> > * As a consensus rule, I expect something like this to be very\n>> > controversial. There are currently no rules that demand any minimal\n>> > fee for anything, and given uncertainly over how fee levels could\n>> > evolve in the future, it's unclear what those rules, if any, should\n>> > be.\n>> >\n>> > Cheers,\n>> >\n>> > --\n>> > Pieter\n>> >\n>> > _______________________________________________\n>> > bitcoin-dev mailing list\n>> > bitcoin-dev at lists.linuxfoundation.org\n>> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220213/eb38bf91/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-18T03:36:19",
                "message_text_only": "Good morning shymaa,\n\n> I just want to add an alarming info to this thread...\n>\n> There are at least 5.7m UTXOs\u22641000 Sat (~7%),\u00a0\n> 8.04 m \u22641$ (10%),\u00a0\n> 13.5m \u2264 0.0001BTC (17%)\n>\n> It seems that bitInfoCharts took my enquiry seriously and added a main link for dust analysis:\n> https://bitinfocharts.com/top-100-dustiest-bitcoin-addresses.html\n> Here, you can see just the first address contains more than 1.7m dust UTXOs\n> (ins-outs =1,712,706 with a few real UTXOs holding the bulk of 415 BTC)\u00a0\n> https://bitinfocharts.com/bitcoin/address/1HckjUpRGcrrRAtFaaCAUaGjsPx9oYmLaZ\n>\n> \u00bb\u00bb\u00bb\u00bb\u00bb\n> \u00a0That's alarming isn't it?, is it due to the lightning networks protocol or could be some other weird activity going on?\n> .\n\nI believe some blockchain tracking analysts will \"dust\" addresses that were spent from (give them 546 sats), in the hope that lousy wallets will use the new 546-sat UTXO from the same address but spending to a different address and combining with *other* inputs with new addresses, thus allowing them to grow their datasets about fund ownership.\n\nIndeed JoinMarket has a policy to ignore-by-default UTXOs that pay to an address it already spent from, precisely due to this (apparently common, since my JoinMarket maker got dusted a number of times already) practice.\n\nI am personally unsure of how common this is but it seems likely that you can eliminate this effect by removing outputs of exactly 546 sats to reused addresses.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "A suggestion to periodically destroy (or remove to secondary storage for Archiving reasons) dust, Non-standard UTXOs, and also detected burn",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Eric Voskuil",
                "shymaa arafat",
                "yanmaani at cock.li",
                "ZmnSCPxj",
                "Pieter Wuille",
                "Billy Tetrud"
            ],
            "messages_count": 9,
            "total_messages_chars_count": 26957
        }
    },
    {
        "title": "[bitcoin-dev] CTV Meeting Notes #3",
        "thread_messages": [
            {
                "author": "Jeremy Rubin",
                "date": "2022-02-09T08:53:12",
                "message_text_only": "Bitcoin Developers,\n\nThe Third CTV meeting was held earlier today (Tuesday February 8th, 2022).\nYou can find the meeting log here:\nhttps://gnusha.org/ctv-bip-review/2022-02-08.log\n\nA best-effort summary:\n\n- Not much new to report on the Bounty\n- Non Interactive Lightning Channel Opens\n  Non interactive lightning Channel opens seems to work!\n  There are questions around being able operate a channel in a \"unipolar\"\nway for routing with the receiver's key offline, as HTLCs might require\nsync revocation. This is orthogonal to the opening of the channels.\n- DLCs w/ CTV\n  DLCs built with CTV does seem to be a \"key enabler\" for DLCs.\n  The non interactivity provides a dramatic speedup (30x - 300x depending\non multi-oracle setup)\n  Changes the client/server setup enable new use cases to explore, and\nsimplify the spec substantially.\n  Backfilling lets clients commit to the DLC faster and lazily backfill at\ncost of state storage.\n  For M-N oracles, precompiling N choose M groups + musig'ing the\nattestation points can possibly save some witness space because\nlog2(N)*32 + N*32 > log2(N*(N choose M))*32 for many values of N and M.\n- Pathcoin\n  Not well understood yet concretely.\n  Seems like the API of a \"a coin that 1-of-N can spend\" shared by N is\nnew/unique and not something LN can do (which always requires N online to\nsign txns)\n  Binary expansion of coins could allow arbitrary value transfer (binary\nexpansion can live in a CTV tree too).\n  Best way to think of Pathcoin at this point is an important theoretical\nresult that should open up new exploration/improvement\n- TXHash\n  Main concerns: more complexity, potential for recursion, script size\noverhead\n- Soft Forks, Generally\n  Big question: Are the fork processes themselves (e.g., BIP9/8/ST\nactiviations) riskier than the upgrades (CTV)?\n  On the one hand, validation rules are something we have to live with\nforever so they should be riskier. Soft fork rules and coordination might\nbe bad, but after activation they go away.\n  On the other hand, we can \"prove\" a technical upgrade correct, but\nsoft-fork signalling requires unprovable user behavior and coordination\n(e.g., actually upgrading).\n  If you perceive the forking mechanism as high risk, it makes sense to\nmake the upgrades have as much content as possible since you need to\njustify the high risk.\n  If you perceive the forking mechanism as low risk, it is fine to make the\nupgrades smaller and easier to prove safe since there's not a high cost to\nforking.\n- Elements CTV Emulation\n  Seems to be workable.\n  Questionable if any of the use cases one might want CTV for (Lightning,\nDLCs, Vaults) would have much demand on Liquid today.\n\nFeel free to correct me where I've not represented perspectives decently,\nas always the logs are the only true summary.\n\nBest,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220209/b8f256d0/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "CTV Meeting Notes #3",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeremy Rubin"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 3037
        }
    },
    {
        "title": "[bitcoin-dev] [Pre-BIP] Fee Accounts",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2022-02-10T06:58:56",
                "message_text_only": "On Sat, Jan 01, 2022 at 12:04:00PM -0800, Jeremy via bitcoin-dev wrote:\n> Happy new years devs,\n> \n> I figured I would share some thoughts for conceptual review that have been\n> bouncing around my head as an opportunity to clean up the fee paying\n> semantics in bitcoin \"for good\". The design space is very wide on the\n> approach I'll share, so below is just a sketch of how it could work which\n> I'm sure could be improved greatly.\n> \n> Transaction fees are an integral part of bitcoin.\n> \n> However, due to quirks of Bitcoin's transaction design, fees are a part of\n> the transactions that they occur in.\n> \n> While this works in a \"Bitcoin 1.0\" world, where all transactions are\n> simple on-chain transfers, real world use of Bitcoin requires support for\n> things like Fee Bumping stuck transactions, DoS resistant Payment Channels,\n> and other long lived Smart Contracts that can't predict future fee rates.\n> Having the fees paid in band makes writing these contracts much more\n> difficult as you can't merely express the logic you want for the\n> transaction, but also the fees.\n> \n> Previously, I proposed a special type of transaction called a \"Sponsor\"\n> which has some special consensus + mempool rules to allow arbitrarily\n> appending fees to a transaction to bump it up in the mempool.\n> \n> As an alternative, we could establish an account system in Bitcoin as an\n> \"extension block\".\n\n<snip>\n\n> This type of design works really well for channels because the addition of\n> fees to e.g. a channel state does not require any sort of pre-planning\n> (e.g. anchors) or transaction flexibility (SIGHASH flags). This sort of\n> design is naturally immune to pinning issues since you could offer to pay a\n> fee for any TXID and the number of fee adding offers does not need to be\n> restricted in the same way the descendant transactions would need to be.\n\nSo it's important to recognize that fee accounts introduce their own kind of\ntransaction pinning attacks: third parties would be able to attach arbitrary\nfees to any transaction without permission. This isn't necessarily a good\nthing: I don't want third parties to be able to grief my transaction engines by\ngetting obsolete transactions confirmed in liu of the replacments I actually\nwant confirmed. Eg a third party could mess up OpenTimestamps calendars at\nrelatively low cost by delaying the mining of timestamp txs.\n\nOf course, there's an obvious way to fix this: allow transactions to designate\na pubkey allowed to add further transaction fees if required. Which Bitcoin\nalready has in two forms: Replace-by-Fee and Child Pays for Parent.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220210/ddb4235b/attachment-0001.sig>"
            },
            {
                "author": "Jeremy Rubin",
                "date": "2022-02-10T08:08:59",
                "message_text_only": "That's not really pinning; painning usually refers to pinning something to\nthe bottom of the mempool whereas these mechanisms make it easier to\nguarantee that progress can be made on confirming the transactions you're\ninterested in.\n\nOften times in these protocols \"the call is coming inside the house\". It's\nnot a third party adding fees we are scared of, it's a direct party to the\nprotocol!\n\nSponsors or fee accounts would enable you to ensure the protocol you're\nworking on makes forward progress. For things like Eltoo the internal\nratchet makes this work well.\n\nProtocols which depend on in mempool replacements before confirmation\nalready must be happy (should they be secure) with any prior state being\nmined. If a third party pays the fee you might even be happier since the\nexecution wasn't on your dime.\n\nCheers,\n\nJeremy\n\nOn Wed, Feb 9, 2022, 10:59 PM Peter Todd via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Sat, Jan 01, 2022 at 12:04:00PM -0800, Jeremy via bitcoin-dev wrote:\n> > Happy new years devs,\n> >\n> > I figured I would share some thoughts for conceptual review that have\n> been\n> > bouncing around my head as an opportunity to clean up the fee paying\n> > semantics in bitcoin \"for good\". The design space is very wide on the\n> > approach I'll share, so below is just a sketch of how it could work which\n> > I'm sure could be improved greatly.\n> >\n> > Transaction fees are an integral part of bitcoin.\n> >\n> > However, due to quirks of Bitcoin's transaction design, fees are a part\n> of\n> > the transactions that they occur in.\n> >\n> > While this works in a \"Bitcoin 1.0\" world, where all transactions are\n> > simple on-chain transfers, real world use of Bitcoin requires support for\n> > things like Fee Bumping stuck transactions, DoS resistant Payment\n> Channels,\n> > and other long lived Smart Contracts that can't predict future fee rates.\n> > Having the fees paid in band makes writing these contracts much more\n> > difficult as you can't merely express the logic you want for the\n> > transaction, but also the fees.\n> >\n> > Previously, I proposed a special type of transaction called a \"Sponsor\"\n> > which has some special consensus + mempool rules to allow arbitrarily\n> > appending fees to a transaction to bump it up in the mempool.\n> >\n> > As an alternative, we could establish an account system in Bitcoin as an\n> > \"extension block\".\n>\n> <snip>\n>\n> > This type of design works really well for channels because the addition\n> of\n> > fees to e.g. a channel state does not require any sort of pre-planning\n> > (e.g. anchors) or transaction flexibility (SIGHASH flags). This sort of\n> > design is naturally immune to pinning issues since you could offer to\n> pay a\n> > fee for any TXID and the number of fee adding offers does not need to be\n> > restricted in the same way the descendant transactions would need to be.\n>\n> So it's important to recognize that fee accounts introduce their own kind\n> of\n> transaction pinning attacks: third parties would be able to attach\n> arbitrary\n> fees to any transaction without permission. This isn't necessarily a good\n> thing: I don't want third parties to be able to grief my transaction\n> engines by\n> getting obsolete transactions confirmed in liu of the replacments I\n> actually\n> want confirmed. Eg a third party could mess up OpenTimestamps calendars at\n> relatively low cost by delaying the mining of timestamp txs.\n>\n> Of course, there's an obvious way to fix this: allow transactions to\n> designate\n> a pubkey allowed to add further transaction fees if required. Which Bitcoin\n> already has in two forms: Replace-by-Fee and Child Pays for Parent.\n>\n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220210/bfed4525/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2022-02-18T23:50:07",
                "message_text_only": "On Thu, Feb 10, 2022 at 12:08:59AM -0800, Jeremy Rubin wrote:\n> That's not really pinning; painning usually refers to pinning something to\n> the bottom of the mempool whereas these mechanisms make it easier to\n> guarantee that progress can be made on confirming the transactions you're\n> interested in.\n\nAs I said, it's a new kind of pinning attack, distinct from other types of\npinning attack.\n\n> Often times in these protocols \"the call is coming inside the house\". It's\n> not a third party adding fees we are scared of, it's a direct party to the\n> protocol!\n\nOften times that is true. But other times that is not true! I gave examples of\nuse-cases where being able to arbitrary add fees to transactions is harmful;\nthe onus is on you to argue why that is acceptable to burden those users with a\nnew class of attack.\n\n> Sponsors or fee accounts would enable you to ensure the protocol you're\n> working on makes forward progress. For things like Eltoo the internal\n> ratchet makes this work well.\n> \n> Protocols which depend on in mempool replacements before confirmation\n> already must be happy (should they be secure) with any prior state being\n> mined. If a third party pays the fee you might even be happier since the\n> execution wasn't on your dime.\n\n\"Must be able to deal with\" is not the same thing as \"Must be happy\". While\nthose use-cases do have to deal with those exceptional cases happening\noccasionally, it's harmful if an attacker can harass you by making those\nexceptional cases happen frequently.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220218/ffb7a6b7/attachment-0001.sig>"
            },
            {
                "author": "Jeremy Rubin",
                "date": "2022-02-19T00:38:27",
                "message_text_only": "> As I said, it's a new kind of pinning attack, distinct from other types\nof pinning attack.\n\nI think pinning is \"formally defined\" as sequences of transactions which\nprevent or make it less likely for you to make any progress (in terms of\nunits of computation proceeding).\n\nSomething that only increases possibility to make progress cannot be\npinning.\n\nIf you want to call it something else, with a negative connotation, maybe\ncall it \"necromancing\" (bringing back txns that would otherwise be\nfeerate/fee irrational).\n\nI would posit that we should be wholly unconcerned with necromancing -- if\nyour protocol is particularly vulnerable to a third party necromancing then\nyour protocol is insecure and we shouldn't hamper Bitcoin's forward\nprogress on secure applications to service already insecure ones. Lightning\nis particularly necromancy resistant by design, but pinning vulnerable.\nThis is also true with things like coinjoins which are necromancy resistant\nbut pinning vulnerable.\n\nNecromancy in particular is something that isn't uniquely un-present in\nBitcoin today, and things like package relay and elimination of pinning are\ninherently at odds with making necromancy either for CPFP use cases.\n\nIn particular, for the use case you mentioned \"Eg a third party could mess\nup OpenTimestamps calendars at relatively low cost by delaying the mining\nof timestamp txs.\", this is incorrect. A third party can only accelerate\nthe mining on the timestamp transactions, but they *can* accelerate the\nmining of any such timestamp transaction. If you have a single output chain\nthat you're RBF'ing per block, then at most they can cause you to shift the\ncalendar commits forward one block. But again, they cannot pin you. If you\nwant to shift it back one block earlier, just offer a higher fee for the\nlater RBF'd calendar. Thus the interference is limited by how much you wish\nto pay to guarantee your commitment is in this block as opposed to the next.\n\nBy the way, you can already do out-of-band transaction fees to a very\nsimilar effect, google \"BTC transaction accelerator\". If the attack were at\nall valuable to perform, it could happen today.\n\nLastly, if you do get \"necromanced\" on an earlier RBF'd transaction by a\nthird party for OTS, you should be relatively happy because it cost you\nless fees overall, since the undoing of your later RBF surely returned some\nsatoshis to your wallet.\n\nBest,\n\nJeremy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220218/83410688/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2022-02-19T09:39:22",
                "message_text_only": "On Fri, Feb 18, 2022 at 04:38:27PM -0800, Jeremy Rubin wrote:\n> > As I said, it's a new kind of pinning attack, distinct from other types\n> of pinning attack.\n> \n> I think pinning is \"formally defined\" as sequences of transactions which\n> prevent or make it less likely for you to make any progress (in terms of\n> units of computation proceeding).\n\nMentioning \"computation\" when talking about transactions is misleading:\nblockchain transactions have nothing to do with computation.\n\n> Something that only increases possibility to make progress cannot be\n> pinning.\n\nIt is incorrect to say that all use-cases have the property that any version of\na transaction being mined is progress.\n\n> If you want to call it something else, with a negative connotation, maybe\n> call it \"necromancing\" (bringing back txns that would otherwise be\n> feerate/fee irrational).\n\nNecromancing might be a reasonable name for attacks that work by getting an\nout-of-date version of a tx mined.\n\n> In particular, for the use case you mentioned \"Eg a third party could mess\n> up OpenTimestamps calendars at relatively low cost by delaying the mining\n> of timestamp txs.\", this is incorrect. A third party can only accelerate\n> the mining on the timestamp transactions, but they *can* accelerate the\n> mining of any such timestamp transaction. If you have a single output chain\n> that you're RBF'ing per block, then at most they can cause you to shift the\n> calendar commits forward one block. But again, they cannot pin you. If you\n> want to shift it back one block earlier, just offer a higher fee for the\n> later RBF'd calendar. Thus the interference is limited by how much you wish\n> to pay to guarantee your commitment is in this block as opposed to the next.\n\nYour understanding of how OpenTimestamps calendars work appears to be\nincorrect. There is no chain of unconfirmed transactions. Rather, OTS calendars\nuse RBF to _update_ the timestamp tx with a new merkle tip hash for to all\noutstanding per-second commitments once per new block. In high fee situations\nit's normal for there to be dozens of versions of that same tx, each with a\nslightly higher feerate.\n\nOTS calendars can handle any of those versions getting mined. But older\nversions getting mined wastes money, as the remaining commitments still need to\nget mined in a subsequent transaction. Those remaining commitments are also\ndelayed by the time it takes for the next tx to get mined.\n\nThere are many use-cases beyond OTS with this issue. For example, some entities\nuse \"in-place\" replacement for update low-time-preference settlement\ntransactions by adding new txouts and updating existing ones. Older versions of\nthose settlement transactions getting mined rather than the newer version\nwastes money and delays settlement for the exact same reason it does in OTS.\n\nIf fee accounts or any similar mechanism get implemented, they absolutely\nshould be opt-in. Obviously, using a currently non-standard nVersion bit is a\npossible approach. Conversely, with CPFP it may be desirable in the settlement\ncase to be able to *prevent* outputs from being spent in the same block. Again,\nan nVersion bit is a possible approach.\n\n> By the way, you can already do out-of-band transaction fees to a very\n> similar effect, google \"BTC transaction accelerator\". If the attack were at\n> all valuable to perform, it could happen today.\n\nI just checked: all the BTC transaction accellerator services I could find look\nto be either scams, or very expensive. We need compelling reasons to make this\nnuisance attack significantly cheaper.\n\n> Lastly, if you do get \"necromanced\" on an earlier RBF'd transaction by a\n> third party for OTS, you should be relatively happy because it cost you\n> less fees overall, since the undoing of your later RBF surely returned some\n> satoshis to your wallet.\n\nAs I said above, no it doesn't.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220219/e3194806/attachment.sig>"
            },
            {
                "author": "Jeremy Rubin",
                "date": "2022-02-20T16:29:00",
                "message_text_only": "--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n\n\nOn Sat, Feb 19, 2022 at 1:39 AM Peter Todd <pete at petertodd.org> wrote:\n\n> On Fri, Feb 18, 2022 at 04:38:27PM -0800, Jeremy Rubin wrote:\n> > > As I said, it's a new kind of pinning attack, distinct from other types\n> > of pinning attack.\n> >\n> > I think pinning is \"formally defined\" as sequences of transactions which\n> > prevent or make it less likely for you to make any progress (in terms of\n> > units of computation proceeding).\n>\n> Mentioning \"computation\" when talking about transactions is misleading:\n> blockchain transactions have nothing to do with computation.\n>\n\nIt is in fact computation. Branding it as \"misleading\" is misleading... The\nrelevant literature is https://en.wikipedia.org/wiki/Non-blocking_algorithm,\nsponsors helps get rid of deadlocking so that any thread can be guaranteed\nto make progress. E.g., this is critical in Eltoo, which is effectively a\ncoordinated multi-party computation on-chain to compute the highest\nsequence number known by any worker.\n\nThat transactions are blobs of \"verification\" (which is also itself a\ncomputation) less so than dynamic computations is irrelevant to the fact\nthat series of transactions do represent computations.\n\n\n\n> > Something that only increases possibility to make progress cannot be\n> > pinning.\n>\n> It is incorrect to say that all use-cases have the property that any\n> version of\n> a transaction being mined is progress.\n>\n\nIt is progress, tautologically. Progress is formally definable as a\ntransaction of any kind getting mined. Pinning prevents progress by an\nadversarial worker. Sponsoring enables progress, but it may not be your\npreferred interleaving. That's OK, but it's inaccurate to say it is not\nprogress.\n\nYour understanding of how OpenTimestamps calendars work appears to be\n> incorrect. There is no chain of unconfirmed transactions. Rather, OTS\n> calendars\n> use RBF to _update_ the timestamp tx with a new merkle tip hash for to all\n> outstanding per-second commitments once per new block. In high fee\n> situations\n> it's normal for there to be dozens of versions of that same tx, each with a\n> slightly higher feerate.\n>\n\nI didn't claim there to be a chain of unconfirmed, I claimed that there\ncould be single output chain that you're RBF'ing one step per block.\n\nE.g., it could be something like\n\nA_0 -> {A_1 w/ CSV 1 block, OP_RETURN {blah, foo}}\nA_1 -> {A_2 w/ CSV 1 block, OP_RETURN {bar}}\n\nsuch that A_i provably can't have an unconfirmed descendant. The notion\nwould be that you're replacing one with another. E.g., if you're updating\nthe calendar like:\n\n\nVersion 0: A_0 -> {A_1 w/ CSV 1 block, OP_RETURN {blah, foo}}\nVersion 1: A_0 -> {A_1 w/ CSV 1 block, OP_RETURN {blah, foo, bar}}\nVersion 2: A_0 -> {A_1 w/ CSV 1 block, OP_RETURN {blah, foo, bar, delta}}\n\nand version 1 gets mined, then in A_1's spend you simply shift delta to\nthat (next) calendar.\n\nA_1 -> {A_2 w/ CSV 1 block, OP_RETURN {delta}}\n\nThus my claim that someone sponsoring a old version only can delay by 1\nblock the calendar commit.\n\n\n\n\n\n> OTS calendars can handle any of those versions getting mined. But older\n> versions getting mined wastes money, as the remaining commitments still\n> need to\n> get mined in a subsequent transaction. Those remaining commitments are also\n> delayed by the time it takes for the next tx to get mined.\n>\n> There are many use-cases beyond OTS with this issue. For example, some\n> entities\n> use \"in-place\" replacement for update low-time-preference settlement\n> transactions by adding new txouts and updating existing ones. Older\n> versions of\n> those settlement transactions getting mined rather than the newer version\n> wastes money and delays settlement for the exact same reason it does in\n> OTS.\n>\n>\n> > Lastly, if you do get \"necromanced\" on an earlier RBF'd transaction by a\n> > third party for OTS, you should be relatively happy because it cost you\n> > less fees overall, since the undoing of your later RBF surely returned\n> some\n> > satoshis to your wallet.\n>\n> As I said above, no it doesn't.\n>\n>\nIt does save money since you had to pay to RBF, the N+1st txn will be\npaying higher fee than the Nth. So if someone else sponsors an earlier\nversion, then you save whatever feerate/fee bumps you would have paid and\nthe funds are again in your change output (or something). You can apply\nthose change output savings to your next batch, which can include any\nentries that have been dropped .\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220220/8d19b01b/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Fee Accounts",
            "categories": [
                "bitcoin-dev",
                "Pre-BIP"
            ],
            "authors": [
                "Jeremy Rubin",
                "Peter Todd"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 20267
        }
    },
    {
        "title": "[bitcoin-dev] [Lightning-dev]  [Pre-BIP] Fee Accounts",
        "thread_messages": [
            {
                "author": "darosior",
                "date": "2022-02-19T17:20:19",
                "message_text_only": "> Necromancing might be a reasonable name for attacks that work by getting an\n> out-of-date version of a tx mined.\n\nIt's not an \"attack\"? There is no such thing as an out-of-date transaction, if\nyou signed and broadcasted it in the first place. You can't rely on the fact that\na replacement transaction would somehow invalidate a previous version of it.\n\n------- Original Message -------\n\nLe samedi 19 f\u00e9vrier 2022 \u00e0 10:39 AM, Peter Todd <pete at petertodd.org> a \u00e9crit :\n\n> On Fri, Feb 18, 2022 at 04:38:27PM -0800, Jeremy Rubin wrote:\n>\n> > > As I said, it's a new kind of pinning attack, distinct from other types\n> > >\n> > > of pinning attack.\n> >\n> > I think pinning is \"formally defined\" as sequences of transactions which\n> >\n> > prevent or make it less likely for you to make any progress (in terms of\n> >\n> > units of computation proceeding).\n>\n> Mentioning \"computation\" when talking about transactions is misleading:\n>\n> blockchain transactions have nothing to do with computation.\n>\n> > Something that only increases possibility to make progress cannot be\n> >\n> > pinning.\n>\n> It is incorrect to say that all use-cases have the property that any version of\n>\n> a transaction being mined is progress.\n>\n> > If you want to call it something else, with a negative connotation, maybe\n> >\n> > call it \"necromancing\" (bringing back txns that would otherwise be\n> >\n> > feerate/fee irrational).\n>\n> Necromancing might be a reasonable name for attacks that work by getting an\n>\n> out-of-date version of a tx mined.\n>\n> > In particular, for the use case you mentioned \"Eg a third party could mess\n> >\n> > up OpenTimestamps calendars at relatively low cost by delaying the mining\n> >\n> > of timestamp txs.\", this is incorrect. A third party can only accelerate\n> >\n> > the mining on the timestamp transactions, but they can accelerate the\n> >\n> > mining of any such timestamp transaction. If you have a single output chain\n> >\n> > that you're RBF'ing per block, then at most they can cause you to shift the\n> >\n> > calendar commits forward one block. But again, they cannot pin you. If you\n> >\n> > want to shift it back one block earlier, just offer a higher fee for the\n> >\n> > later RBF'd calendar. Thus the interference is limited by how much you wish\n> >\n> > to pay to guarantee your commitment is in this block as opposed to the next.\n>\n> Your understanding of how OpenTimestamps calendars work appears to be\n>\n> incorrect. There is no chain of unconfirmed transactions. Rather, OTS calendars\n>\n> use RBF to update the timestamp tx with a new merkle tip hash for to all\n>\n> outstanding per-second commitments once per new block. In high fee situations\n>\n> it's normal for there to be dozens of versions of that same tx, each with a\n>\n> slightly higher feerate.\n>\n> OTS calendars can handle any of those versions getting mined. But older\n>\n> versions getting mined wastes money, as the remaining commitments still need to\n>\n> get mined in a subsequent transaction. Those remaining commitments are also\n>\n> delayed by the time it takes for the next tx to get mined.\n>\n> There are many use-cases beyond OTS with this issue. For example, some entities\n>\n> use \"in-place\" replacement for update low-time-preference settlement\n>\n> transactions by adding new txouts and updating existing ones. Older versions of\n>\n> those settlement transactions getting mined rather than the newer version\n>\n> wastes money and delays settlement for the exact same reason it does in OTS.\n>\n> If fee accounts or any similar mechanism get implemented, they absolutely\n>\n> should be opt-in. Obviously, using a currently non-standard nVersion bit is a\n>\n> possible approach. Conversely, with CPFP it may be desirable in the settlement\n>\n> case to be able to prevent outputs from being spent in the same block. Again,\n>\n> an nVersion bit is a possible approach.\n>\n> > By the way, you can already do out-of-band transaction fees to a very\n> >\n> > similar effect, google \"BTC transaction accelerator\". If the attack were at\n> >\n> > all valuable to perform, it could happen today.\n>\n> I just checked: all the BTC transaction accellerator services I could find look\n>\n> to be either scams, or very expensive. We need compelling reasons to make this\n>\n> nuisance attack significantly cheaper.\n>\n> > Lastly, if you do get \"necromanced\" on an earlier RBF'd transaction by a\n> >\n> > third party for OTS, you should be relatively happy because it cost you\n> >\n> > less fees overall, since the undoing of your later RBF surely returned some\n> >\n> > satoshis to your wallet.\n>\n> As I said above, no it doesn't.\n>\n> ----------------------------------\n>\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n>\n> Lightning-dev mailing list\n>\n> Lightning-dev at lists.linuxfoundation.org\n>\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "Peter Todd",
                "date": "2022-02-19T20:35:20",
                "message_text_only": "On Sat, Feb 19, 2022 at 05:20:19PM +0000, darosior wrote:\n> > Necromancing might be a reasonable name for attacks that work by getting an\n> > out-of-date version of a tx mined.\n> \n> It's not an \"attack\"? There is no such thing as an out-of-date transaction, if\n> you signed and broadcasted it in the first place. You can't rely on the fact that\n> a replacement transaction would somehow invalidate a previous version of it.\n\nAnyone on the internet can send you a packet; a secure system must be able to\nreceive any packet without being compromised. Yet we still call packet floods\nas DoS attacks. And internet standards are careful to avoid making packet\nflooding cheaper than it currently is.\n\nThe same principal applies here: in many situations transactions _do_ become\nout of date, in the sense that you would rather a different transaction be\nmined instead, and the out-of-date tx being mined is expensive and annoying.\nWhile you have to account for the _possibility_ of any transaction you have\nsigned being mined, Bitcoin standards should avoid making unwanted necromancy a\ncheap and easy attack.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220219/d50565cf/attachment.sig>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-20T02:24:37",
                "message_text_only": "Good morning Peter and Jeremy,\n\n> On Sat, Feb 19, 2022 at 05:20:19PM +0000, darosior wrote:\n>\n> > > Necromancing might be a reasonable name for attacks that work by getting an\n> > > out-of-date version of a tx mined.\n> >\n> > It's not an \"attack\"? There is no such thing as an out-of-date transaction, if\n> > you signed and broadcasted it in the first place. You can't rely on the fact that\n> > a replacement transaction would somehow invalidate a previous version of it.\n>\n> Anyone on the internet can send you a packet; a secure system must be able to\n> receive any packet without being compromised. Yet we still call packet floods\n> as DoS attacks. And internet standards are careful to avoid making packet\n> flooding cheaper than it currently is.\n>\n> The same principal applies here: in many situations transactions do become\n> out of date, in the sense that you would rather a different transaction be\n> mined instead, and the out-of-date tx being mined is expensive and annoying.\n> While you have to account for the possibility of any transaction you have\n> signed being mined, Bitcoin standards should avoid making unwanted necromancy a\n> cheap and easy attack.\n>\n\nThis seems to me to restrict the only multiparty feebumping method to be some form of per-participant anchor outputs a la Lightning anchor commitments.\n\nNote that multiparty RBF is unreliable.\nWhile the initial multiparty signing of a transaction may succeed, at a later time with the transaction unconfirmed, one or more of the participants may regret cooperating in the initial signing and decide not to cooperate with the RBF.\nOr for that matter, a participant may, through complete accident, go offline.\n\nAnchor outputs can be keyed to only a specific participant, so feebumping of particular transaction can only be done by participants who have been authorized to feebump.\n\nPerhaps fee accounts can include some kind of proof-this-transaction-authorizes-this-fee-account?\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-20T02:39:50",
                "message_text_only": "Good morning Peter and Jeremy,\n\n> Good morning Peter and Jeremy,\n>\n> > On Sat, Feb 19, 2022 at 05:20:19PM +0000, darosior wrote:\n> >\n> > > > Necromancing might be a reasonable name for attacks that work by getting an\n> > > > out-of-date version of a tx mined.\n> > >\n> > > It's not an \"attack\"? There is no such thing as an out-of-date transaction, if\n> > > you signed and broadcasted it in the first place. You can't rely on the fact that\n> > > a replacement transaction would somehow invalidate a previous version of it.\n> >\n> > Anyone on the internet can send you a packet; a secure system must be able to\n> > receive any packet without being compromised. Yet we still call packet floods\n> > as DoS attacks. And internet standards are careful to avoid making packet\n> > flooding cheaper than it currently is.\n> > The same principal applies here: in many situations transactions do become\n> > out of date, in the sense that you would rather a different transaction be\n> > mined instead, and the out-of-date tx being mined is expensive and annoying.\n> > While you have to account for the possibility of any transaction you have\n> > signed being mined, Bitcoin standards should avoid making unwanted necromancy a\n> > cheap and easy attack.\n>\n> This seems to me to restrict the only multiparty feebumping method to be some form of per-participant anchor outputs a la Lightning anchor commitments.\n>\n> Note that multiparty RBF is unreliable.\n> While the initial multiparty signing of a transaction may succeed, at a later time with the transaction unconfirmed, one or more of the participants may regret cooperating in the initial signing and decide not to cooperate with the RBF.\n> Or for that matter, a participant may, through complete accident, go offline.\n>\n> Anchor outputs can be keyed to only a specific participant, so feebumping of particular transaction can only be done by participants who have been authorized to feebump.\n>\n> Perhaps fee accounts can include some kind of proof-this-transaction-authorizes-this-fee-account?\n\nFor example:\n\n* We reserve one Tapscript version for fee-account-authorization.\n  * Validation of this tapscript version always fails.\n* If a transaction wants to authorize a fee account, it should have at least one Taproot output.\n  * This Taproot output must have tapleaf with the fee-account-authorization Tapscript version.\n* In order for a fee account to feebump a transaction, it must also present the Taproot MAST path to the fee-account-authorization tapleaf of one output of that transaction.\n\nThis gives similar functionality to anchor outputs, without requiring an explicit output on the initial transaction, saving blockspace.\nIn particular, once the number of participants grows, the number of anchor outputs must grow linearly with the number of participants being authorized to feebump.\nOnly when the feerate turns out to be too low do we need to expose the authorization.\nRevelation of the fee-account-authorization is O(log N), and if only one participant decides to feebump, then only a single O(log N) MAST treepath is published.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Jeremy Rubin",
                "date": "2022-02-20T16:29:35",
                "message_text_only": "opt-in or explicit tagging of fee account is a bad design IMO.\n\nAs pointed out by James O'Beirne in the other email, having an explicit key\nrequired means you have to pre-plan.... suppose you're building a vault\nmeant to distribute funds over many years, do you really want a *specific*\nprecommitted key you have to maintain? What happens to your ability to bump\nshould it be compromised (which may be more likely if it's intended to be a\nhot-wallet function for bumping).\n\nFurthermore, it's quite often the case that someone might do a transaction\nthat pays you that is low fee that you want to bump but they choose to\nopt-out... then what? It's better that you should always be able to fee\nbump.\n\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n\n\nOn Sun, Feb 20, 2022 at 6:24 AM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning DA,\n>\n>\n> > Agreed, you cannot rely on a replacement transaction would somehow\n> > invalidate a previous version of it, it has been spoken into the gossip\n> > and exists there in mempools somewhere if it does, there is no guarantee\n> > that anyone has ever heard of the replacement transaction as there is no\n> > consensus about either the previous version of the transaction or its\n> > replacement until one of them is mined and the block accepted. -DA.\n>\n> As I understand from the followup from Peter, the point is not \"this\n> should never happen\", rather the point is \"this should not happen *more\n> often*.\"\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220220/c3047335/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-20T16:34:35",
                "message_text_only": "Good morning Jeremy,\n\n> opt-in or explicit tagging of fee account is a bad design IMO.\n>\n> As pointed out by James O'Beirne in the other email, having an explicit key required means you have to pre-plan.... suppose you're building a vault meant to distribute funds over many years, do you really want a *specific* precommitted\u00a0key you have to maintain? What happens to your ability to bump should it be compromised (which may be more likely if it's intended to be a hot-wallet function for bumping).\n>\n> Furthermore, it's quite often the case that someone might do a transaction that pays you that is low fee that you want to bump but they choose to opt-out... then what? It's better that you should always be able to fee bump.\n\nGood point.\n\nFor the latter case, CPFP would work and already exists.\n**Unless** you are doing something complicated and offchain-y and involves relative locktimes, of course.\n\n\nOnce could point out as well that Peter Todd gave just a single example, OpenTimeStamps, for this, and OpenTimeStamps is not the only user of the Bitcoin blockchain.\n\nSo we can consider: who benefits and who suffers, and does the benefit to the former outweigh the detriment of the latter?\n\n\nIt seems to me that the necromancing attack mostly can *only* target users of RBF that might want to *additionally* add outputs (or in the case of OTS, commitments) when RBF-ing.\nFor example, a large onchain-paying entity might lowball an onchain transaction for a few withdrawals, then as more withdrawals come in, bump up their feerate and add more withdrawals to the RBF-ed transaction.\nSuch an entity might prefer to confirm the latest RBF-ed transaction, as if an earlier transaction (which does not include some other withdrawals requested later) is necromanced, they would need to make an *entire* *other* transaction (which may be costlier!) to fulfill pending withdrawal requests.\n\nHowever, to my knowledge, there is no actual entity that *currently* acts this way (I do have some sketches for a wallet that can support this behavior, but it gets *complicated* due to having to keep track of reorgs as well... sigh).\n\nIn particular, I expect that many users do not really make outgoing payments often enough that they would actually benefit from such a wallet feature.\nInstead, they will generally make one payment at a time, or plan ahead and pay several in a batch at once, and even if they RBF, they would just keep the same set of outputs and just reduce their change output.\nFor such low-scale users, a rando third-party necromancing their old transactions could only make them happy, thus this nuisance attack cannot be executed.\n\nWe could also point out that this is really a nuisance attack and not an economic-theft attack.\nThe attacker cannot gain, and can only pay in order to impose costs on somebody else.\nRationally, the only winning move is not to play.\n\n\nSo --- has anyone actually implemented a Bitcoin wallet that has such a feature (i.e. make a lowball send transaction now, then you can add another send later and if the previous send transaction is unconfirmed, RBF it with a new transaction that has the previous send and the current send) and if so, can you open-source the code and show me?\n\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Fee Accounts",
            "categories": [
                "bitcoin-dev",
                "Lightning-dev",
                "Pre-BIP"
            ],
            "authors": [
                "ZmnSCPxj",
                "darosior",
                "Jeremy Rubin",
                "Peter Todd"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 16227
        }
    },
    {
        "title": "[bitcoin-dev] [Lightning-dev]    [Pre-BIP] Fee Accounts",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-20T14:24:22",
                "message_text_only": "Good morning DA,\n\n\n> Agreed, you cannot rely on a replacement transaction would somehow\n> invalidate a previous version of it, it has been spoken into the gossip\n> and exists there in mempools somewhere if it does, there is no guarantee\n> that anyone has ever heard of the replacement transaction as there is no\n> consensus about either the previous version of the transaction or its\n> replacement until one of them is mined and the block accepted. -DA.\n\nAs I understand from the followup from Peter, the point is not \"this should never happen\", rather the point is \"this should not happen *more often*.\"\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Fee Accounts",
            "categories": [
                "bitcoin-dev",
                "Lightning-dev",
                "Pre-BIP"
            ],
            "authors": [
                "ZmnSCPxj"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 623
        }
    },
    {
        "title": "[bitcoin-dev] [Lightning-dev] [Pre-BIP] Fee Accounts",
        "thread_messages": [
            {
                "author": "Jeremy Rubin",
                "date": "2022-02-20T16:45:35",
                "message_text_only": "Morning!\n\n>\n> For the latter case, CPFP would work and already exists.\n> **Unless** you are doing something complicated and offchain-y and involves\n> relative locktimes, of course.\n>\n>\nThe \"usual\" design I recommend for Vaults contains something that is like:\n\n{<maturity> CSV <pk_hot> CHECKSIG, <pk_cold> CHECKSIG}\nor\n{<maturity> CSV <pk_hot> CHECKSIG, <H(tx to: <pk_cold> CHECKSIG)> CTV}\n\n\nwhere after an output is created, it has to hit maturity before hot\nspendable but can be kicked to recovery any time before (optional: use CTV\nto actually transition on chain removing hot wallet, if cold key is hard to\naccess).\n\n\nNot that this means if you're waiting for one of these outputs to be\ncreated on chain, you cannot spend from the hot key since it needs to\nconfirm on chain first. Spending from the cold key for CPFP'ing the hot is\nan 'invalid move' (emergency key for non emergency sitch)\n\nThus in order to CPFP, you would need a separate output just for CPFPing\nthat is not subject to these restrictions, or some sort of RBF-able addable\ninput/output. Or, Sponsors.\n\n\nJeremy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220220/92d8f0e2/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Fee Accounts",
            "categories": [
                "bitcoin-dev",
                "Lightning-dev",
                "Pre-BIP"
            ],
            "authors": [
                "Jeremy Rubin"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1263
        }
    },
    {
        "title": "[bitcoin-dev] Advancing the security of Neutrino using minimally trusted oracles",
        "thread_messages": [
            {
                "author": "enclade",
                "date": "2022-02-10T10:02:08",
                "message_text_only": "The design document which inspired Neutrino outlined the use of oracles to provide a moderate level of confidence to lightweight clients in the filters they have received from an untrusted source. Current implementations of lightweight wallets using Neutrino either trust in a single source, or a sampling of untrusted peers for this information. The determinism of the filter headers allows for them to be simply and compactly attested by a potentially large number of authoritative sources with minimal loss in privacy. These sources could be exchanges, hardware wallet manufacturers, block explorers, or other well known parties.\n\nThe most obvious transport for these oracles is DNS, several[0][1] implementations of tools exist which provide either headers or raw filter data to clients by encoding it in record responses. With careful construction oracles can operate using DNS with extremely low resource requirements and attack surface, while providing a privacy maximizing service to their clients. For situations where DNS is not appropriate, other tools can aggregate the signatures into other formats as required.\n\nClients could consider their view of the current network state to be strong when several of their oracle sources present agreeing signatures, or display an error to their user if no suitable number of attestations could be found. Fault or fraud proofs can be generated by any party by simply collecting differing signatures, for example if an oracle was presenting disjoint filter headers from its peers the error would be readily apparent and provable.\n\n-\n\nHost names and their associated keys would be baked into the binaries of client software supporting the system, but their location and credentials could be attested in a text file of their primary domain. For example, a popular fictional exchange could advertise their ability to provide this service using RFC5785.\n\n # curl https://pizzabase.com/.well-known/neutrino.txt\n 03a34b99f22c790c4e36b2b3c2c35a36db06226e41c692fc82b8b56ac1c540c5bd at neutrino.pizzabase.com\n\nThe client would request its known sources for attestations, using the current unix timestamp as a nonce. Use of a lower precision (for example rounded to 60 seconds) allows the oracle to cache the result with a long TTL, while allowing a client to poll with relatively high frequency if required.\n\n # dig 6204dd70.neutrino.pizzabase.com\n # dig 6204dd70.neutrino.blockspaghettini.com\n # dig 6204dd70.neutrino.mtgnocchi.com\n\nOracles would return the current block hash, hash of the tip of the neutrino header chain, and a ECDSA signature over the data including the requesting quantized timestamp. In totality giving the client sufficient and portable evidence that their view of the state of the network has not been tampered with, while maintaining as much privacy as possible.\n\n-\n\nRFC.\n\n[0]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013417.html\n[1]: https://github.com/mempoolco/chaindnsd\n[2]: https://bitcoinheaders.net/"
            },
            {
                "author": "Devrandom",
                "date": "2022-02-10T21:07:14",
                "message_text_only": "This would be very useful for the Validating Lightning Signer project,\nsince we need to prove to a non-network connected signer that a UTXO has\nnot been spent.  It allows the signer to make sure the channel is still\nactive.\n\n( the related design doc is at\nhttps://gitlab.com/lightning-signer/docs/-/blob/master/oracle.md )\n\nI think it would be useful if the oracles were non-interactive, so that\nthey can communicate with the world over a one-way connection.  This would\nreduce their attack surface.  Instead of signing over a client-provided\ntimestamp, we could pre-quantize the timestamp and emit attestations for\neach quantum time step.\n\nOn Thu, Feb 10, 2022 at 11:10 AM enclade via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> The design document which inspired Neutrino outlined the use of oracles to\n> provide a moderate level of confidence to lightweight clients in the\n> filters they have received from an untrusted source. Current\n> implementations of lightweight wallets using Neutrino either trust in a\n> single source, or a sampling of untrusted peers for this information. The\n> determinism of the filter headers allows for them to be simply and\n> compactly attested by a potentially large number of authoritative sources\n> with minimal loss in privacy. These sources could be exchanges, hardware\n> wallet manufacturers, block explorers, or other well known parties.\n>\n> The most obvious transport for these oracles is DNS, several[0][1]\n> implementations of tools exist which provide either headers or raw filter\n> data to clients by encoding it in record responses. With careful\n> construction oracles can operate using DNS with extremely low resource\n> requirements and attack surface, while providing a privacy maximizing\n> service to their clients. For situations where DNS is not appropriate,\n> other tools can aggregate the signatures into other formats as required.\n>\n> Clients could consider their view of the current network state to be\n> strong when several of their oracle sources present agreeing signatures, or\n> display an error to their user if no suitable number of attestations could\n> be found. Fault or fraud proofs can be generated by any party by simply\n> collecting differing signatures, for example if an oracle was presenting\n> disjoint filter headers from its peers the error would be readily apparent\n> and provable.\n>\n> -\n>\n> Host names and their associated keys would be baked into the binaries of\n> client software supporting the system, but their location and credentials\n> could be attested in a text file of their primary domain. For example, a\n> popular fictional exchange could advertise their ability to provide this\n> service using RFC5785.\n>\n>  # curl https://pizzabase.com/.well-known/neutrino.txt\n>\n> 03a34b99f22c790c4e36b2b3c2c35a36db06226e41c692fc82b8b56ac1c540c5bd at neutrino.pizzabase.com\n>\n> The client would request its known sources for attestations, using the\n> current unix timestamp as a nonce. Use of a lower precision (for example\n> rounded to 60 seconds) allows the oracle to cache the result with a long\n> TTL, while allowing a client to poll with relatively high frequency if\n> required.\n>\n>  # dig 6204dd70.neutrino.pizzabase.com\n>  # dig 6204dd70.neutrino.blockspaghettini.com\n>  # dig 6204dd70.neutrino.mtgnocchi.com\n>\n> Oracles would return the current block hash, hash of the tip of the\n> neutrino header chain, and a ECDSA signature over the data including the\n> requesting quantized timestamp. In totality giving the client sufficient\n> and portable evidence that their view of the state of the network has not\n> been tampered with, while maintaining as much privacy as possible.\n>\n> -\n>\n> RFC.\n>\n> [0]:\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013417.html\n> [1]: https://github.com/mempoolco/chaindnsd\n> [2]: https://bitcoinheaders.net/\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220210/dcc0fea8/attachment-0001.html>"
            },
            {
                "author": "enclade",
                "date": "2022-02-11T02:39:15",
                "message_text_only": "That sounds completely reasonable.\n\nOriginally I had discussed privately making the protocol design completely interactive (client sends a nonce over DNS, oracle responds signing the nonce), but it was pointed out that making them use quantized timestamps mitigated a lot of the issues regarding denial of service, and allows for fault proofs to be significantly stronger.\n\nDelivering the oracle messages over a write only channel like Kryptoradio or Blockstream Satellite would scale extremely well too. When the oracles produce agreeing messages (hopefully, the majority of the time except on block boundaries) the additional data is only 64 bytes per additional signer, so it makes sense to broadcast any a client may want to trust.\n\n\n------- Original Message -------\n\nOn Thursday, February 10th, 2022 at 4:07 PM, Devrandom <c1.bitcoin at niftybox.net> wrote:\n\n> This would be very useful for the Validating Lightning Signer project, since we need to prove to a non-network connected signer that a UTXO has not been spent. It allows the signer to make sure the channel is still active.\n>\n> ( the related design doc is at https://gitlab.com/lightning-signer/docs/-/blob/master/oracle.md )\n>\n> I think it would be useful if the oracles were non-interactive, so that they can communicate with the world over a one-way connection. This would reduce their attack surface. Instead of signing over a client-provided timestamp, we could pre-quantize the timestamp and emit attestations for each quantum time step."
            }
        ],
        "thread_summary": {
            "title": "Advancing the security of Neutrino using minimally trusted oracles",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Devrandom",
                "enclade"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 8749
        }
    },
    {
        "title": "[bitcoin-dev] Thoughts on fee bumping",
        "thread_messages": [
            {
                "author": "James O'Beirne",
                "date": "2022-02-10T19:40:22",
                "message_text_only": "There's been much talk about fee-bumping lately, and for good reason -\ndynamic fee management is going to be a central part of bitcoin use as\nthe mempool fills up (lord willing) and right now fee-bumping is\nfraught with difficulty and pinning peril.\n\nGloria's recent post on the topic[0] was very lucid and highlights a\nlot of the current issues, as well as some proposals to improve the\nsituation.\n\nAs others have noted, the post was great. But throughout the course\nof reading it and the ensuing discussion, I became troubled by the\nincreasing complexity of both the status quo and some of the\nproposed remedies.\n\nLayering on special cases, more carve-outs, and X and Y percentage\nthresholds is going to make reasoning about the mempool harder than it\nalready is. Special consideration for \"what should be in the next\nblock\" and/or the caching of block templates seems like an imposing\ndependency, dragging in a bunch of state and infrastructure to a\nquestion that should be solely limited to mempool feerate aggregates\nand the feerate of the particular txn package a wallet is concerned\nwith.\n\nThis is bad enough for protocol designers and Core developers, but\nmaking the situation any more intractable for \"end-users\" and wallet\ndevelopers feels wrong.\n\nI thought it might be useful to step back and reframe. Here are a few\naims that are motivated chiefly by the quality of end-user experience,\nconstrained to obey incentive compatibility (i.e. miner reward, DoS\navoidance). Forgive the abstract dalliance for a moment; I'll talk\nthrough concretes afterwards.\n\n\n# Purely additive feerate bumps should never be impossible\n\nAny user should always be able to add to the incentive to mine any\ntransaction in a purely additive way. The countervailing force here\nends up being spam prevention (a la min-relay-fee) to prevent someone\nfrom consuming bandwidth and mempool space with a long series of\ninfinitesimal fee-bumps.\n\nA fee bump, naturally, should be given the same per-byte consideration\nas a normal Bitcoin transaction in terms of relay and block space,\nalthough it would be nice to come up with a more succinct\nrepresentation. This leads to another design principle:\n\n\n# The bandwidth and chain space consumed by a fee-bump should be minimal\n\nInstead of prompting a rebroadcast of the original transaction for\nreplacement, which contains a lot of data not new to the network, it\nmakes more sense to broadcast the \"diff\" which is the additive\ncontribution towards some txn's feerate.\n\nThis dovetails with the idea that...\n\n\n# Special transaction structure should not be required to bump fees\n\nIn an ideal design, special structural foresight would not be needed\nin order for a txn's feerate to be improved after broadcast.\n\nAnchor outputs specified solely for CPFP, which amount to many bytes of\nwasted chainspace, are a hack. It's probably uncontroversial at this\npoint to say that even RBF itself is kind of a hack - a special\nsequence number should not be necessary for post-broadcast contribution\ntoward feerate. Not to mention RBF's seemingly wasteful consumption of\nbandwidth due to the rebroadcast of data the network has already seen.\n\nIn a sane design, no structural foresight - and certainly no wasted\nbytes in the form of unused anchor outputs - should be needed in order\nto add to a miner's reward for confirming a given transaction.\n\nPlanning for fee-bumps explicitly in transaction structure also often\nwinds up locking in which keys are required to bump fees, at odds\nwith the idea that...\n\n\n# Feerate bumps should be able to come from anywhere\n\nOne of the practical downsides of CPFP that I haven't seen discussed in\nthis conversation is that it requires the transaction to pre-specify the\nkeys needed to sign for fee bumps. This is problematic if you're, for\nexample, using a vault structure that makes use of pre-signed\ntransactions.\n\nWhat if the key you specified n the anchor outputs for a bunch of\npre-signed txns is compromised? What if you'd like to be able to\ndynamically select the wallet that bumps fees? CPFP does you no favors\nhere.\n\nThere is of course a tension between allowing fee bumps to come from\nanywhere and the threat of pinning-like attacks. So we should venture\nto remove pinning as a possibility, in line with the first design\nprinciple I discuss.\n\n\n---\n\nComing down to earth, the \"tabula rasa\" thought experiment above has led\nme to favor an approach like the transaction sponsors design that Jeremy\nproposed in a prior discussion back in 2020[1].\n\nTransaction sponsors allow feerates to be bumped after a transaction's\nbroadcast, regardless of the structure of the original transaction.\nNo rebroadcast (wasted bandwidth) is required for the original txn data.\nNo wasted chainspace on only-maybe-used prophylactic anchor outputs.\n\nThe interface for end-users is very straightforward: if you want to bump\nfees, specify a transaction that contributes incrementally to package\nfeerate for some txid. Simple.\n\nIn the original discussion, there were a few main objections that I noted:\n\n1. In Jeremy's original proposal, only one sponsor txn per txid is\n   allowed by policy. A malicious actor could execute a pinning-like\n   attack by specifying an only-slightly-helpful feerate sponsor that\n   then precludes other larger bumps.\n\nI think there are some ways around this shortcoming. For example: what\nif, by policy, sponsor txns had additional constraints that\n\n  - each input must be signed {SIGHASH_SINGLE,SIGHASH_NONE}|ANYONECANPAY,\n  - the txn must be specified RBFable,\n  - a replacement for the sponsor txn must raise the sponsor feerate,\n    including ancestors (maybe this is inherent in \"is RBFable,\" but\n    I don't want to conflate absolute feerates into this).\n\nThat way, there is still at most a single sponsor txn per txid in the\nmempool, but anyone can \"mix in\" inputs which bump the effective\nfeerate of the sponsor.\n\nThis may not be the exact solution we want, but I think it demonstrates\nthat the sponsors design has some flexibility and merits some thinking.\n\nThe second objection about sponsors was\n\n2. (from Suhas) sponsors break the classic invariant: \"once a valid\n   transaction is created, it should not become invalid later on unless\n   the inputs are double-spent.\"\n\nThis doesn't seem like a huge concern to me if you consider the txid\nbeing sponsored as a sort of spiritual input to the sponsor. While the\ntheoretical objection against broadening where one has to look in a txn\nto determine its dependencies is understandable, I don't see what the\npractical cost here is.\n\nReorg complexity seems comparable if not identical, especially if we\nbroaden sponsor rules to allow blocks to contain sponsor txns that are\nboth for txids in the same block _or_ already included in the chain.\n\nThis theoretical concession seems preferable to heaping more rules onto\nan already labyrinthine mempool policy that is difficult for both\nimplementers and users to reason about practically and conceptually.\n\nA third objection that wasn't posed, IIRC, but almost certainly would\nbe:\n\n3. Transaction sponsors requires a soft-fork.\n\nSoft-forks are no fun, but I'll tell you what also isn't fun: being on\nthe hook to model (and sometimes implement) a dizzying potpourri of\nmempool policies and special-cases. Expecting wallet implementers to\nabide by a maze of rules faithfully in order to ensure txn broadcast and\nfee management invites bugs for perpetuity and network behavior that is\ndifficult to reason about a priori. Use of CPFP in the long-term also\nrisks needless chain waste.\n\nIf a soft-fork is the cost of cleaning up this essential process,\nconsideration should be given to paying it as a one-time cost. This\ntopic merits a separate post, but consider that in the 5 years leading\nup to the 2017 SegWit drama, we averaged about a soft-fork a year.\nUncontroversial, \"safe\" changes to the consensus protocol shouldn't be\nout of the question when significant practical benefit is plain to see.\n\n---\n\nI hope this message has added some framing to the discussion on fees,\nas well prompting other participants to go back and give the\ntransaction sponsor proposal a serious look. The sponsors interface is\nabout the simplest I can imagine for wallets, and it seems easy to\nreason about for implementers on Core and elsewhere.\n\nI'm not out to propose soft-forks lightly, but the current complexity\nin fee management feels untenable, and as evidenced by all the\ndiscussion lately, fees are an increasingly crucial part of the system.\n\n\n\n[0]:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019817.html\n[1]:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-September/018168.html\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220210/b6bf7054/attachment.html>"
            },
            {
                "author": "Greg Sanders",
                "date": "2022-02-10T23:09:06",
                "message_text_only": "One quick thought to the proposal and perhaps to sponsors in general(didn't\nhave time to go over original proposal again):\n\nSince sponsors can come from anywhere, the wallet application must have\naccess to the mempool to know what inputs must be double spent to RBF the\nsponsor transaction.\n\nSeems like an important difference to be considered.\n\nOn Fri, Feb 11, 2022 at 3:49 AM James O'Beirne via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> There's been much talk about fee-bumping lately, and for good reason -\n> dynamic fee management is going to be a central part of bitcoin use as\n> the mempool fills up (lord willing) and right now fee-bumping is\n> fraught with difficulty and pinning peril.\n>\n> Gloria's recent post on the topic[0] was very lucid and highlights a\n> lot of the current issues, as well as some proposals to improve the\n> situation.\n>\n> As others have noted, the post was great. But throughout the course\n> of reading it and the ensuing discussion, I became troubled by the\n> increasing complexity of both the status quo and some of the\n> proposed remedies.\n>\n> Layering on special cases, more carve-outs, and X and Y percentage\n> thresholds is going to make reasoning about the mempool harder than it\n> already is. Special consideration for \"what should be in the next\n> block\" and/or the caching of block templates seems like an imposing\n> dependency, dragging in a bunch of state and infrastructure to a\n> question that should be solely limited to mempool feerate aggregates\n> and the feerate of the particular txn package a wallet is concerned\n> with.\n>\n> This is bad enough for protocol designers and Core developers, but\n> making the situation any more intractable for \"end-users\" and wallet\n> developers feels wrong.\n>\n> I thought it might be useful to step back and reframe. Here are a few\n> aims that are motivated chiefly by the quality of end-user experience,\n> constrained to obey incentive compatibility (i.e. miner reward, DoS\n> avoidance). Forgive the abstract dalliance for a moment; I'll talk\n> through concretes afterwards.\n>\n>\n> # Purely additive feerate bumps should never be impossible\n>\n> Any user should always be able to add to the incentive to mine any\n> transaction in a purely additive way. The countervailing force here\n> ends up being spam prevention (a la min-relay-fee) to prevent someone\n> from consuming bandwidth and mempool space with a long series of\n> infinitesimal fee-bumps.\n>\n> A fee bump, naturally, should be given the same per-byte consideration\n> as a normal Bitcoin transaction in terms of relay and block space,\n> although it would be nice to come up with a more succinct\n> representation. This leads to another design principle:\n>\n>\n> # The bandwidth and chain space consumed by a fee-bump should be minimal\n>\n> Instead of prompting a rebroadcast of the original transaction for\n> replacement, which contains a lot of data not new to the network, it\n> makes more sense to broadcast the \"diff\" which is the additive\n> contribution towards some txn's feerate.\n>\n> This dovetails with the idea that...\n>\n>\n> # Special transaction structure should not be required to bump fees\n>\n> In an ideal design, special structural foresight would not be needed\n> in order for a txn's feerate to be improved after broadcast.\n>\n> Anchor outputs specified solely for CPFP, which amount to many bytes of\n> wasted chainspace, are a hack. It's probably uncontroversial at this\n> point to say that even RBF itself is kind of a hack - a special\n> sequence number should not be necessary for post-broadcast contribution\n> toward feerate. Not to mention RBF's seemingly wasteful consumption of\n> bandwidth due to the rebroadcast of data the network has already seen.\n>\n> In a sane design, no structural foresight - and certainly no wasted\n> bytes in the form of unused anchor outputs - should be needed in order\n> to add to a miner's reward for confirming a given transaction.\n>\n> Planning for fee-bumps explicitly in transaction structure also often\n> winds up locking in which keys are required to bump fees, at odds\n> with the idea that...\n>\n>\n> # Feerate bumps should be able to come from anywhere\n>\n> One of the practical downsides of CPFP that I haven't seen discussed in\n> this conversation is that it requires the transaction to pre-specify the\n> keys needed to sign for fee bumps. This is problematic if you're, for\n> example, using a vault structure that makes use of pre-signed\n> transactions.\n>\n> What if the key you specified n the anchor outputs for a bunch of\n> pre-signed txns is compromised? What if you'd like to be able to\n> dynamically select the wallet that bumps fees? CPFP does you no favors\n> here.\n>\n> There is of course a tension between allowing fee bumps to come from\n> anywhere and the threat of pinning-like attacks. So we should venture\n> to remove pinning as a possibility, in line with the first design\n> principle I discuss.\n>\n>\n> ---\n>\n> Coming down to earth, the \"tabula rasa\" thought experiment above has led\n> me to favor an approach like the transaction sponsors design that Jeremy\n> proposed in a prior discussion back in 2020[1].\n>\n> Transaction sponsors allow feerates to be bumped after a transaction's\n> broadcast, regardless of the structure of the original transaction.\n> No rebroadcast (wasted bandwidth) is required for the original txn data.\n> No wasted chainspace on only-maybe-used prophylactic anchor outputs.\n>\n> The interface for end-users is very straightforward: if you want to bump\n> fees, specify a transaction that contributes incrementally to package\n> feerate for some txid. Simple.\n>\n> In the original discussion, there were a few main objections that I noted:\n>\n> 1. In Jeremy's original proposal, only one sponsor txn per txid is\n>    allowed by policy. A malicious actor could execute a pinning-like\n>    attack by specifying an only-slightly-helpful feerate sponsor that\n>    then precludes other larger bumps.\n>\n> I think there are some ways around this shortcoming. For example: what\n> if, by policy, sponsor txns had additional constraints that\n>\n>   - each input must be signed {SIGHASH_SINGLE,SIGHASH_NONE}|ANYONECANPAY,\n>   - the txn must be specified RBFable,\n>   - a replacement for the sponsor txn must raise the sponsor feerate,\n>     including ancestors (maybe this is inherent in \"is RBFable,\" but\n>     I don't want to conflate absolute feerates into this).\n>\n> That way, there is still at most a single sponsor txn per txid in the\n> mempool, but anyone can \"mix in\" inputs which bump the effective\n> feerate of the sponsor.\n>\n> This may not be the exact solution we want, but I think it demonstrates\n> that the sponsors design has some flexibility and merits some thinking.\n>\n> The second objection about sponsors was\n>\n> 2. (from Suhas) sponsors break the classic invariant: \"once a valid\n>    transaction is created, it should not become invalid later on unless\n>    the inputs are double-spent.\"\n>\n> This doesn't seem like a huge concern to me if you consider the txid\n> being sponsored as a sort of spiritual input to the sponsor. While the\n> theoretical objection against broadening where one has to look in a txn\n> to determine its dependencies is understandable, I don't see what the\n> practical cost here is.\n>\n> Reorg complexity seems comparable if not identical, especially if we\n> broaden sponsor rules to allow blocks to contain sponsor txns that are\n> both for txids in the same block _or_ already included in the chain.\n>\n> This theoretical concession seems preferable to heaping more rules onto\n> an already labyrinthine mempool policy that is difficult for both\n> implementers and users to reason about practically and conceptually.\n>\n> A third objection that wasn't posed, IIRC, but almost certainly would\n> be:\n>\n> 3. Transaction sponsors requires a soft-fork.\n>\n> Soft-forks are no fun, but I'll tell you what also isn't fun: being on\n> the hook to model (and sometimes implement) a dizzying potpourri of\n> mempool policies and special-cases. Expecting wallet implementers to\n> abide by a maze of rules faithfully in order to ensure txn broadcast and\n> fee management invites bugs for perpetuity and network behavior that is\n> difficult to reason about a priori. Use of CPFP in the long-term also\n> risks needless chain waste.\n>\n> If a soft-fork is the cost of cleaning up this essential process,\n> consideration should be given to paying it as a one-time cost. This\n> topic merits a separate post, but consider that in the 5 years leading\n> up to the 2017 SegWit drama, we averaged about a soft-fork a year.\n> Uncontroversial, \"safe\" changes to the consensus protocol shouldn't be\n> out of the question when significant practical benefit is plain to see.\n>\n> ---\n>\n> I hope this message has added some framing to the discussion on fees,\n> as well prompting other participants to go back and give the\n> transaction sponsor proposal a serious look. The sponsors interface is\n> about the simplest I can imagine for wallets, and it seems easy to\n> reason about for implementers on Core and elsewhere.\n>\n> I'm not out to propose soft-forks lightly, but the current complexity\n> in fee management feels untenable, and as evidenced by all the\n> discussion lately, fees are an increasingly crucial part of the system.\n>\n>\n>\n> [0]:\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019817.html\n> [1]:\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-September/018168.html\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220211/c1c03c95/attachment.html>"
            },
            {
                "author": "darosior",
                "date": "2022-02-10T23:44:38",
                "message_text_only": "(I have not yet read the recent posts on RBF but i wanted to react on the \"additive feerate\".)\n\n> # Purely additive feerate bumps should never be impossible\n\nIt's not that simple. As a miner, if i have less than 1vMB of transactions in my mempool. I don't want a 10sats/vb transaction paying 100000sats by a 100sats/vb transaction paying only 10000sats.\n\nApart from that i very much agree with the approach of taking a step back and reframing, with CPFP being inadapted long term (wasteful, not useful for delegating fee bumping (i'm surprised i didn't mention it publicly but it makes it unsuitable for Revault for instance), and the current carve-out rule makes it only suitable for 2-party protocols), and the `diff` approach.\n\nAll that again with the caveat that i need to update myself on the recent proposals.\n\n-------- Original Message --------\nOn Feb 10, 2022, 20:40, James O'Beirne via bitcoin-dev wrote:\n\n> There's been much talk about fee-bumping lately, and for good reason -\n> dynamic fee management is going to be a central part of bitcoin use as\n> the mempool fills up (lord willing) and right now fee-bumping is\n> fraught with difficulty and pinning peril.\n>\n> Gloria's recent post on the topic[0] was very lucid and highlights a\n> lot of the current issues, as well as some proposals to improve the\n> situation.\n>\n> As others have noted, the post was great. But throughout the course\n> of reading it and the ensuing discussion, I became troubled by the\n> increasing complexity of both the status quo and some of the\n> proposed remedies.\n>\n> Layering on special cases, more carve-outs, and X and Y percentage\n> thresholds is going to make reasoning about the mempool harder than it\n> already is. Special consideration for \"what should be in the next\n> block\" and/or the caching of block templates seems like an imposing\n> dependency, dragging in a bunch of state and infrastructure to a\n> question that should be solely limited to mempool feerate aggregates\n> and the feerate of the particular txn package a wallet is concerned\n> with.\n>\n> This is bad enough for protocol designers and Core developers, but\n> making the situation any more intractable for \"end-users\" and wallet\n> developers feels wrong.\n>\n> I thought it might be useful to step back and reframe. Here are a few\n> aims that are motivated chiefly by the quality of end-user experience,\n> constrained to obey incentive compatibility (i.e. miner reward, DoS\n> avoidance). Forgive the abstract dalliance for a moment; I'll talk\n> through concretes afterwards.\n>\n> # Purely additive feerate bumps should never be impossible\n>\n> Any user should always be able to add to the incentive to mine any\n> transaction in a purely additive way. The countervailing force here\n> ends up being spam prevention (a la min-relay-fee) to prevent someone\n> from consuming bandwidth and mempool space with a long series of\n> infinitesimal fee-bumps.\n>\n> A fee bump, naturally, should be given the same per-byte consideration\n> as a normal Bitcoin transaction in terms of relay and block space,\n> although it would be nice to come up with a more succinct\n> representation. This leads to another design principle:\n>\n> # The bandwidth and chain space consumed by a fee-bump should be minimal\n>\n> Instead of prompting a rebroadcast of the original transaction for\n> replacement, which contains a lot of data not new to the network, it\n> makes more sense to broadcast the \"diff\" which is the additive\n> contribution towards some txn's feerate.\n>\n> This dovetails with the idea that...\n>\n> # Special transaction structure should not be required to bump fees\n>\n> In an ideal design, special structural foresight would not be needed\n> in order for a txn's feerate to be improved after broadcast.\n>\n> Anchor outputs specified solely for CPFP, which amount to many bytes of\n> wasted chainspace, are a hack. It's probably uncontroversial at this\n> point to say that even RBF itself is kind of a hack - a special\n> sequence number should not be necessary for post-broadcast contribution\n> toward feerate. Not to mention RBF's seemingly wasteful consumption of\n> bandwidth due to the rebroadcast of data the network has already seen.\n>\n> In a sane design, no structural foresight - and certainly no wasted\n> bytes in the form of unused anchor outputs - should be needed in order\n> to add to a miner's reward for confirming a given transaction.\n>\n> Planning for fee-bumps explicitly in transaction structure also often\n> winds up locking in which keys are required to bump fees, at odds\n> with the idea that...\n>\n> # Feerate bumps should be able to come from anywhere\n>\n> One of the practical downsides of CPFP that I haven't seen discussed in\n> this conversation is that it requires the transaction to pre-specify the\n> keys needed to sign for fee bumps. This is problematic if you're, for\n> example, using a vault structure that makes use of pre-signed\n> transactions.\n>\n> What if the key you specified n the anchor outputs for a bunch of\n> pre-signed txns is compromised? What if you'd like to be able to\n> dynamically select the wallet that bumps fees? CPFP does you no favors\n> here.\n>\n> There is of course a tension between allowing fee bumps to come from\n> anywhere and the threat of pinning-like attacks. So we should venture\n> to remove pinning as a possibility, in line with the first design\n> principle I discuss.\n>\n> ---\n>\n> Coming down to earth, the \"tabula rasa\" thought experiment above has led\n> me to favor an approach like the transaction sponsors design that Jeremy\n> proposed in a prior discussion back in 2020[1].\n>\n> Transaction sponsors allow feerates to be bumped after a transaction's\n> broadcast, regardless of the structure of the original transaction.\n> No rebroadcast (wasted bandwidth) is required for the original txn data.\n> No wasted chainspace on only-maybe-used prophylactic anchor outputs.\n>\n> The interface for end-users is very straightforward: if you want to bump\n> fees, specify a transaction that contributes incrementally to package\n> feerate for some txid. Simple.\n>\n> In the original discussion, there were a few main objections that I noted:\n>\n> 1. In Jeremy's original proposal, only one sponsor txn per txid is\n> allowed by policy. A malicious actor could execute a pinning-like\n> attack by specifying an only-slightly-helpful feerate sponsor that\n> then precludes other larger bumps.\n>\n> I think there are some ways around this shortcoming. For example: what\n> if, by policy, sponsor txns had additional constraints that\n>\n> - each input must be signed {SIGHASH_SINGLE,SIGHASH_NONE}|ANYONECANPAY,\n> - the txn must be specified RBFable,\n> - a replacement for the sponsor txn must raise the sponsor feerate,\n> including ancestors (maybe this is inherent in \"is RBFable,\" but\n> I don't want to conflate absolute feerates into this).\n>\n> That way, there is still at most a single sponsor txn per txid in the\n> mempool, but anyone can \"mix in\" inputs which bump the effective\n> feerate of the sponsor.\n>\n> This may not be the exact solution we want, but I think it demonstrates\n> that the sponsors design has some flexibility and merits some thinking.\n>\n> The second objection about sponsors was\n>\n> 2. (from Suhas) sponsors break the classic invariant: \"once a valid\n> transaction is created, it should not become invalid later on unless\n> the inputs are double-spent.\"\n>\n> This doesn't seem like a huge concern to me if you consider the txid\n> being sponsored as a sort of spiritual input to the sponsor. While the\n> theoretical objection against broadening where one has to look in a txn\n> to determine its dependencies is understandable, I don't see what the\n> practical cost here is.\n>\n> Reorg complexity seems comparable if not identical, especially if we\n> broaden sponsor rules to allow blocks to contain sponsor txns that are\n> both for txids in the same block _or_ already included in the chain.\n>\n> This theoretical concession seems preferable to heaping more rules onto\n> an already labyrinthine mempool policy that is difficult for both\n> implementers and users to reason about practically and conceptually.\n>\n> A third objection that wasn't posed, IIRC, but almost certainly would\n> be:\n>\n> 3. Transaction sponsors requires a soft-fork.\n>\n> Soft-forks are no fun, but I'll tell you what also isn't fun: being on\n> the hook to model (and sometimes implement) a dizzying potpourri of\n> mempool policies and special-cases. Expecting wallet implementers to\n> abide by a maze of rules faithfully in order to ensure txn broadcast and\n> fee management invites bugs for perpetuity and network behavior that is\n> difficult to reason about a priori. Use of CPFP in the long-term also\n> risks needless chain waste.\n>\n> If a soft-fork is the cost of cleaning up this essential process,\n> consideration should be given to paying it as a one-time cost. This\n> topic merits a separate post, but consider that in the 5 years leading\n> up to the 2017 SegWit drama, we averaged about a soft-fork a year.\n> Uncontroversial, \"safe\" changes to the consensus protocol shouldn't be\n> out of the question when significant practical benefit is plain to see.\n>\n> ---\n>\n> I hope this message has added some framing to the discussion on fees,\n> as well prompting other participants to go back and give the\n> transaction sponsor proposal a serious look. The sponsors interface is\n> about the simplest I can imagine for wallets, and it seems easy to\n> reason about for implementers on Core and elsewhere.\n>\n> I'm not out to propose soft-forks lightly, but the current complexity\n> in fee management feels untenable, and as evidenced by all the\n> discussion lately, fees are an increasingly crucial part of the system.\n>\n> [0]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019817.html\n> [1]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-September/018168.html\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220210/b9a80bc9/attachment-0001.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-02-17T14:32:25",
                "message_text_only": "On Thu, Feb 10, 2022 at 07:12:16PM -0500, Matt Corallo via bitcoin-dev wrote:\n> This is where *all* the complexity comes from. If our goal is to \"ensure a\n> bump increases a miner's overall revenue\" (thus not wasting relay for\n> everyone else), then we precisely *do* need\n> > Special consideration for \"what should be in the next\n> > block\" and/or the caching of block templates seems like an imposing\n> > dependency\n> Whether a transaction increases a miner's revenue depends precisely on\n> whether the transaction (package) being replaced is in the next block - if\n> it is, you care about the absolute fee of the package and its replacement.\n\nOn Thu, Feb 10, 2022 at 11:44:38PM +0000, darosior via bitcoin-dev wrote:\n> It's not that simple. As a miner, if i have less than 1vMB of transactions in my mempool. I don't want a 10sats/vb transaction paying 100000sats by a 100sats/vb transaction paying only 10000sats.\n\nIs it really true that miners do/should care about that?\n\nIf you did this particular example, the miner would be losing 90k sats\nin fees, which would be at most 1.44 *millionths* of a percent of the\nblock reward with the subsidy at 6.25BTC per block, even if there were\nno other transactions in the mempool. Even cumulatively, 10sats/vb over\n1MB versus 100sats/vb over 10kB is only a 1.44% loss of block revenue.\n\nI suspect the \"economically rational\" choice would be to happily trade\noff that immediate loss against even a small chance of a simpler policy\nencouraging higher adoption of bitcoin, _or_ a small chance of more\non-chain activity due to higher adoption of bitcoin protocols like\nlightning and thus a lower chance of an empty mempool in future.\n\nIf the network has an \"empty mempool\" (say less than 2MvB-10MvB of\nbacklog even if you have access to every valid 1+ sat/vB tx on any node\nconnected to the network), then I don't think you'll generally have txs\nwith fee rates greater than ~20 sat/vB (ie 20x the minimum fee rate),\nwhich means your maximum loss is about 3% of block revenue, at least\nwhile the block subsidy remains at 6.25BTC/block.\n\nCertainly those percentages can be expected to double every four years as\nthe block reward halves (assuming we don't also reduce the min relay fee\nand block min tx fee), but I think for both miners and network stability,\nit'd be better to have the mempool backlog increase over time, which\nwould both mean there's no/less need to worry about the special case of\nthe mempool being empty, and give a better incentive for people to pay\nhigher fees for quicker confirmations.\n\nIf we accept that logic (and assuming we had some additional policy\nto prevent p2p relay spam due to replacement txs), we could make\nthe mempool accept policy for replacements just be (something like)\n\"[package] feerate is greater than max(descendent fee rate)\", which\nseems like it'd be pretty straightforward to deal with in general?\n\n\n\nThinking about it a little more; I think the decision as to whether\nyou want to have a \"100kvB at 10sat/vb\" tx or a conflicting \"1kvB at\n100sat/vb\" tx in your mempool if you're going to take into account\nunrelated, lower fee rate txs that are also in the mempool makes block\nbuilding \"more\" of an NP-hard problem and makes the greedy solution\nwe've currently got much more suboptimal -- if you really want to do that\noptimally, I think you have to have a mempool that retains conflicting\ntxs and runs a dynamic programming solution to pick the best set, rather\nthan today's simple greedy algorithms both for building the block and\npopulating the mempool?\n\nFor example, if you had two such replacements come through the network,\na miner could want to flip from initially accepting the first replacement,\nto unaccepting it:\n\nInitial mempool: two big txs at 100k each, many small transactions at\n15s/vB and 1s/vB\n\n [100kvB at 20s/vB] [850kvB at 15s/vB] [100kvB at 12s/vB] [1000kvB at 1s/vB]\n   -> 0.148 BTC for 1MvB (100*20 + 850*15 + 50*1)\n\nReplacement for the 20s/vB tx paying a higher fee rate but lower total\nfee; that's worth including:\n\n [10kvB at 100s/vB] [850kvB at 15s/vB] [100kvB at 12s/vB [1000kvB at 1s/vB]\n   -> 0.1499 BTC for 1MvB (10*100 + 850*15 + 100*12 + 40*1)\n\nLater, replacement for the 12s/vB tx comes in, also paying higher fee\nrate but lower total fee. Worth including, but only if you revert the\noriginal replacement:\n\n [100kvB at 20s/vB] [50kvB at 20s/vB] [850kvB at 15s/vB] [1000kvB at 1s/vB]\n   -> 0.16 BTC for 1MvB (150*20 + 850*15)\n\n [10kvB at 100s/vB] [50kvB at 20s/vB] [850kvB at 15s/vB] [1000kvB at 1s/vB]\n   -> 0.1484 BTC for 1MvB (10*100 + 50*20 + 850*15 + 90*1)\n\nAlgorithms/mempool policies you might have, and their results with\nthis example:\n\n * current RBF rules: reject both replacements because they don't\n   increase the absolute fee, thus get the minimum block fees of\n   0.148 BTC\n\n * reject RBF unless it increases the fee rate, and get 0.1484 BTC in\n   fees\n\n * reject RBF if it's lower fee rate or immediately decreases the block\n   reward: so, accept the first replacement, but reject the second,\n   getting 0.1499 BTC\n\n * only discard a conflicting tx when it pays both a lower fee rate and\n   lower absolute fees, and choose amongst conflicting txs optimally\n   via some complicated tx allocation algorithm when generating a block,\n   and get 0.16 BTC\n\nIn this example, those techniques give 92.5%, 92.75%, 93.69% and 100% of\ntotal possible fees you could collect; and 99.813%, 99.819%, 99.84% and\n100% of the total possible block reward at 6.25BTC/block.\n\nIs there a plausible example where the difference isn't that marginal?\nSeems like the simplest solution of just checking the (package/descendent)\nfee rate increases works well enough here at least.\n\nIf 90kvB of unrelated txs at 14s/vB were then added to the mempool, then\nreplacing both txs becomes (just barely) optimal, meaning the smartest\npossible algorithm and the dumbest one of just considering the fee rate\nproduce the same result, while the others are worse:\n\n [10kvB at 100s/vB] [50kvB at 20s/vB] [850kvB at 15s/vB] [90kvB at 14s/vB]\n   -> 0.1601 BTC for 1MvB\n   (accepting both)\n\n [100kvB at 20s/vB] [50kvB at 20s/vB] [850kvB at 15s/vB] [90kvB at 14s/vB]\n   -> 0.1575 BTC for 1MvB \n   (accepting only the second replacement)\n\n [10kvB at 100s/vB] [850kvB at 15s/vB] [90kvB at 14s/vB] [100kvB at 12s/vB]\n   -> 0.1551 BTC for 1MvB\n   (first replacement only, optimal tx selection: 10*100, 850*15, 50*14, 100*12)\n\n [100kvB at 20s/vB] [850kvB at 15s/vB] [90kvB at 14s/vB] [100kvB at 12s/vB]\n   -> 0.1545 BTC for 1MvB\n   (accepting neither replacement)\n\n [10kvB at 100s/vB] [850kvB at 15s/vB] [90kvB at 14s/vB] [100kvB at 12s/vB]\n   -> 0.1506 BTC for 1MvB \n   (first replacement only, greedy tx selection: 10*100, 850*15, 90*14, 50*1)\n\nAlways accepting (package/descendent) fee rate increases removes the\npossibility of pinning entirely, I think -- you still have the problem\nwhere someone else might get a conflicting transaction confirmed first,\nbut they can't get a conflicting tx stuck in the mempool without\nconfirming if you're willing to pay enough to get it confirmed.\n\n\n\nNote that if we did have this policy, you could abuse it to cheaply drain\npeople's mempools: if there was a 300MB backlog, you could publish 2980\n100kB txs paying a fee rate just below the next block fee, meaning you'd\nkick out the previous backlog and your transactions take up all but the\ntop 2MB of the mempool; if you then replace them all with perhaps 2980\n100B txs paying a slightly higher fee rate, the default mempool will be\nleft with only 2.3MB, at an ultimate cost to you of only about 30% of a\nblock in fees, and you could then fill the mempool back up by spamming\n300MB of ultra low fee rate txs.\n\nI think spam prevention at the outbound relay level isn't enough to\nprevent that: an attacker could contact every public node and relay the\ntxs directly, clearing out the mempool of most public nodes directly. So\nyou'd want some sort of spam prevention on inbound txs too?\n\nSo I think you'd need to carefully think about relay spam before making\nthis sort of change.  Also, if we had tx rebroadcast implemented then\nhaving just a few nodes with large mempools might allow the network to\nrecover from this situation automatically.\n\nCheers,\naj"
            },
            {
                "author": "James O'Beirne",
                "date": "2022-02-17T18:18:11",
                "message_text_only": "> Is it really true that miners do/should care about that?\n\nDe facto, any miner running an unmodified version of bitcoind doesn't\ncare about anything aside from ancestor fee rate, given that the\nBlockAssembler as-written orders transactions for inclusion by\ndescending ancestor fee-rate and then greedily adds them to the block\ntemplate. [0]\n\nIf anyone has any indication that there are miners running forks of\nbitcoind that change this behavior, I'd be curious to know it.\n\nAlong the lines of what AJ wrote, optimal transaction selection is\nNP-hard (knapsack problem). Any time that a miner spends deciding how\nto assemble the next block is time not spent grinding on the nonce, and\nso I'm skeptical that miners in practice are currently doing anything\nthat isn't fast and simple like the default implementation: sorting\nfee-rate in descending order and then greedily packing.\n\nBut it would be interesting to hear evidence to the contrary.\n\n---\n\nYou can make the argument that transaction selection is just a function\nof mempool contents, and so mempool maintenance criteria might be the\nthing to look at. Mempool acceptance is gated based on a minimum\nfeerate[1].  Mempool eviction (when running low on space) happens on\nthe basis of max(self_feerate, descendant_feerate) [2]. So even in the\nmempool we're still talking in terms of fee rates, not absolute fees.\n\nThat presents us with the \"is/ought\" problem: just because the mempool\n*is* currently gating only on fee rate doesn't mean that's optimal. But\nif the whole point of the mempool is to hold transactions that will be\nmined, and if there's good reason that txns are chosen for mining based\non fee rate (it's quick and good enough), then it seems like fee rate\nis the approximation that should ultimately prevail for txn\nreplacement.\n\n\n[0]:\nhttps://github.com/bitcoin/bitcoin/blob/master/src/node/miner.cpp#L310-L320\n[1]:\nhttps://github.com/bitcoin/bitcoin/blob/master/src/txmempool.cpp#L1106\n[2]:\nhttps://github.com/bitcoin/bitcoin/blob/master/src/txmempool.cpp#L1138-L1144\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220217/178b279c/attachment.html>"
            },
            {
                "author": "darosior",
                "date": "2022-02-18T09:01:07",
                "message_text_only": "James,\n\nYou seem to imply that the scenario described isn't prevented today. It is. The mempool acceptance for a replacement not only\ndepend on the transaction feerate but also the transaction fee [0]. That's why i raised it in the first place...\n\nAntoine\n\n[0] https://github.com/bitcoin/bitcoin/blob/66636ca438cb65fb18bcaa4540856cef0cee2029/src/validation.cpp#L944-L947\n\nOf course if you are evicting transactions then you don't have the issue i mentioned, so it's fine doing so.\n-------- Original Message --------\nOn Feb 17, 2022, 19:18, James O'Beirne < james.obeirne at gmail.com> wrote:\n\n>> Is it really true that miners do/should care about that?\n>\n> De facto, any miner running an unmodified version of bitcoind doesn't\n> care about anything aside from ancestor fee rate, given that the\n> BlockAssembler as-written orders transactions for inclusion by\n> descending ancestor fee-rate and then greedily adds them to the block\n> template. [0]\n>\n> If anyone has any indication that there are miners running forks of\n> bitcoind that change this behavior, I'd be curious to know it.\n>\n> Along the lines of what AJ wrote, optimal transaction selection is\n> NP-hard (knapsack problem). Any time that a miner spends deciding how\n> to assemble the next block is time not spent grinding on the nonce, and\n> so I'm skeptical that miners in practice are currently doing anything\n> that isn't fast and simple like the default implementation: sorting\n> fee-rate in descending order and then greedily packing.\n>\n> But it would be interesting to hear evidence to the contrary.\n>\n> ---\n>\n> You can make the argument that transaction selection is just a function\n> of mempool contents, and so mempool maintenance criteria might be the\n> thing to look at. Mempool acceptance is gated based on a minimum\n> feerate[1]. Mempool eviction (when running low on space) happens on\n> the basis of max(self_feerate, descendant_feerate) [2]. So even in the\n> mempool we're still talking in terms of fee rates, not absolute fees.\n>\n> That presents us with the \"is/ought\" problem: just because the mempool\n> *is* currently gating only on fee rate doesn't mean that's optimal. But\n> if the whole point of the mempool is to hold transactions that will be\n> mined, and if there's good reason that txns are chosen for mining based\n> on fee rate (it's quick and good enough), then it seems like fee rate\n> is the approximation that should ultimately prevail for txn\n> replacement.\n>\n> [0]:\n> https://github.com/bitcoin/bitcoin/blob/master/src/node/miner.cpp#L310-L320\n> [1]:\n> https://github.com/bitcoin/bitcoin/blob/master/src/txmempool.cpp#L1106\n> [2]:\n> https://github.com/bitcoin/bitcoin/blob/master/src/txmempool.cpp#L1138-L1144\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220218/1da83193/attachment.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2022-02-18T00:35:01",
                "message_text_only": "While I roughly agree with the thesis that different replacement policies\noffer marginal block reward gains _in the current state_ of the ecosystem,\nI would be more conservative about extending the conclusions to the\nmedium/long-term future.\n\n> I suspect the \"economically rational\" choice would be to happily trade\n> off that immediate loss against even a small chance of a simpler policy\n> encouraging higher adoption of bitcoin, _or_ a small chance of more\n> on-chain activity due to higher adoption of bitcoin protocols like\n> lightning and thus a lower chance of an empty mempool in future.\n\nThis is making the assumption that the economic interests of the different\nclass of actors in the Bitcoin ecosystem are not only well-understood but\nalso aligned. We have seen in the past mining actors behaviors delaying the\nadoption of protocol upgrades which were expected to encourage higher\nadoption of Bitcoin. Further, if miners likely have an incentive to see an\nincrease of on-chain activity, there is also the possibility that lightning\nwill be so throughput-efficient to drain mempools backlog, to a point where\nthe block demand is not high enough to pay back the cost of mining hardware\nand operational infrastructure. Or at least not matching the return on\nmining investments expectations.\n\nOf course, it could be argued that as a utxo-sharing protocol like\nlightning just compresses the number of payments per block space unit, it\nlowers the fees burden, thus making Bitcoin as a payment system far more\nattractive for a wider population of users. In fine increasing the block\nspace demand and satisfying the miners.\n\nIn the state of today's knowledge, this hypothesis sounds the most\nplausible. Though, I would say it's better to be cautious until we\nunderstand better the interactions between the different layers of the\nBitcoin ecosystem ?\n\n> Certainly those percentages can be expected to double every four years as\n> the block reward halves (assuming we don't also reduce the min relay fee\n> and block min tx fee), but I think for both miners and network stability,\n> it'd be better to have the mempool backlog increase over time, which\n> would both mean there's no/less need to worry about the special case of\n> the mempool being empty, and give a better incentive for people to pay\n> higher fees for quicker confirmations.\n\nIntuitively, if we assume that liquidity isn't free on lightning [0], there\nshould be a subjective equilibrium where it's cheaper to open new channels\nto reduce one's own graph traversal instead of paying too high routing fees.\n\nAs the core of the network should start to be more busy, I think we should\nsee more LN actors doing that kind of arbitrage, guaranteeing in the\nlong-term mempools backlog.\n\n> If you really want to do that\n> optimally, I think you have to have a mempool that retains conflicting\n> txs and runs a dynamic programming solution to pick the best set, rather\n> than today's simple greedy algorithms both for building the block and\n> populating the mempool?\n\nAs of today, I think power efficiency of mining chips and access to\naffordable sources of energy are more significant factors of the\nrentability of mining operations rather than optimality of block\nconstruction/replacement policy. IMO, making the argument that small deltas\nin block reward gains aren't that much relevant.\n\nThat said, the former factors might become a commodity, and the latter one\nbecome a competitive advantage. It could incentivize the development of\nsuch greedy algorithms, potentially in a covert way as we have seen with\nAsicBoost ?\n\n> Is there a plausible example where the difference isn't that marginal?\n\nThe paradigm might change in the future. If we see the deployment of\nchannel factories/payment pools, we might have users competing to spend a\nshared-utxo with different liquidity needs and thus ready to overbid. Lack\nof a \"conflict pool\" logic might make you lose income.\n\n> Always accepting (package/descendent) fee rate increases removes the\npossibility of pinning entirely, I think\n\nI think the pinnings we're affected with today are due to the ability of a\nmalicious counterparty to halt the on-chain resolution of the channel. The\npresence of a  pinning commitment transaction with low-chance of\nconfirmation (abuse of BIP125 rule 3)\nprevents the honest counterparty to fee-bump her own version of the\ncommitment, thus redeeming a HTLC before timelock expiration. As long as\none commitment confirms, independently of who issued it, the pinning is\nover. I think moving to replace-by-feerate allows the honest counterparty\nto fee-bump her commitment, thus offering a compelling block space demand,\nor forces the malicious counterparty to enter in a fee race.\n\n\nTo gather my thinking on the subject, the replace-by-feerate policy could\nproduce lower fees blocks in the presence of today's environment of\nempty/low-fulfilled blocks. That said, the delta sounds marginal enough\nw.r.t other factors of mining business units\nto not be worried (or at least low-key) about the potential implications on\ncentralization. If the risk is perceived as too intolerable, it could be\nargued an intermediate solution would be to deploy a \"dual\" RBF policy\n(replace-by-fee for the top of the mempool, replace-by-feerate\nfor the remaining part).\n\nStill, I believe we might have to adopt more sophisticated replacement\npolicies in the long term to level the field among the mining ecosystem if\nblock construction/mempool acceptance strategies become a competitive\nfactor. Default to do so might provoke a latent centralization of mining\ndue to heterogeneity in the block reward offered. This heterogeneity would\nalso likely downgrade the safety of L2 nodes, as those actors wouldn't be\nable to know how to format their fee-bumpings, in the lack of _a_ mempool\nreplacement standard.\n\n> Note that if we did have this policy, you could abuse it to cheaply drain\n> people's mempools: if there was a 300MB backlog, you could publish 2980\n> 100kB txs paying a fee rate just below the next block fee, meaning you'd\n> kick out the previous backlog and your transactions take up all but the\n> top 2MB of the mempool; if you then replace them all with perhaps 2980\n> 100B txs paying a slightly higher fee rate, the default mempool will be\n> left with only 2.3MB, at an ultimate cost to you of only about 30% of a\n> block in fees, and you could then fill the mempool back up by spamming\n> 300MB of ultra low fee rate txs.\n\nI believe we might have bandwidth-bleeding issues with our current\nreplacement policy. I think it would be good to have a cost estimate of\nthem and ensure a newer replacement policy would stay in the same bounds.\n\n> I think spam prevention at the outbound relay level isn't enough to\n> prevent that: an attacker could contact every public node and relay the\n> txs directly, clearing out the mempool of most public nodes directly. So\n> you'd want some sort of spam prevention on inbound txs too?\n\nThat we have to think about replacement spam prevention sounds reasonable\nto me. I would be worried about utxo-based replacement limitations which\ncould be abused in the context of multi-party protocol (introducing a new\npinning vector). One solution\ncould be to have a per-party transaction \"tag\" and allocate a replacement\nslot in function ? Maybe preventing a malicious counterparty to abuse a\n\"global\" utxo slot during periods of low fees...\n\nAntoine\n\n[0] https://twitter.com/alexbosworth/status/1476946257939628035\n\nLe jeu. 17 f\u00e9vr. 2022 \u00e0 09:32, Anthony Towns via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> On Thu, Feb 10, 2022 at 07:12:16PM -0500, Matt Corallo via bitcoin-dev\n> wrote:\n> > This is where *all* the complexity comes from. If our goal is to \"ensure\n> a\n> > bump increases a miner's overall revenue\" (thus not wasting relay for\n> > everyone else), then we precisely *do* need\n> > > Special consideration for \"what should be in the next\n> > > block\" and/or the caching of block templates seems like an imposing\n> > > dependency\n> > Whether a transaction increases a miner's revenue depends precisely on\n> > whether the transaction (package) being replaced is in the next block -\n> if\n> > it is, you care about the absolute fee of the package and its\n> replacement.\n>\n> On Thu, Feb 10, 2022 at 11:44:38PM +0000, darosior via bitcoin-dev wrote:\n> > It's not that simple. As a miner, if i have less than 1vMB of\n> transactions in my mempool. I don't want a 10sats/vb transaction paying\n> 100000sats by a 100sats/vb transaction paying only 10000sats.\n>\n> Is it really true that miners do/should care about that?\n>\n> If you did this particular example, the miner would be losing 90k sats\n> in fees, which would be at most 1.44 *millionths* of a percent of the\n> block reward with the subsidy at 6.25BTC per block, even if there were\n> no other transactions in the mempool. Even cumulatively, 10sats/vb over\n> 1MB versus 100sats/vb over 10kB is only a 1.44% loss of block revenue.\n>\n> I suspect the \"economically rational\" choice would be to happily trade\n> off that immediate loss against even a small chance of a simpler policy\n> encouraging higher adoption of bitcoin, _or_ a small chance of more\n> on-chain activity due to higher adoption of bitcoin protocols like\n> lightning and thus a lower chance of an empty mempool in future.\n>\n> If the network has an \"empty mempool\" (say less than 2MvB-10MvB of\n> backlog even if you have access to every valid 1+ sat/vB tx on any node\n> connected to the network), then I don't think you'll generally have txs\n> with fee rates greater than ~20 sat/vB (ie 20x the minimum fee rate),\n> which means your maximum loss is about 3% of block revenue, at least\n> while the block subsidy remains at 6.25BTC/block.\n>\n> Certainly those percentages can be expected to double every four years as\n> the block reward halves (assuming we don't also reduce the min relay fee\n> and block min tx fee), but I think for both miners and network stability,\n> it'd be better to have the mempool backlog increase over time, which\n> would both mean there's no/less need to worry about the special case of\n> the mempool being empty, and give a better incentive for people to pay\n> higher fees for quicker confirmations.\n>\n> If we accept that logic (and assuming we had some additional policy\n> to prevent p2p relay spam due to replacement txs), we could make\n> the mempool accept policy for replacements just be (something like)\n> \"[package] feerate is greater than max(descendent fee rate)\", which\n> seems like it'd be pretty straightforward to deal with in general?\n>\n>\n>\n> Thinking about it a little more; I think the decision as to whether\n> you want to have a \"100kvB at 10sat/vb\" tx or a conflicting \"1kvB at\n> 100sat/vb\" tx in your mempool if you're going to take into account\n> unrelated, lower fee rate txs that are also in the mempool makes block\n> building \"more\" of an NP-hard problem and makes the greedy solution\n> we've currently got much more suboptimal -- if you really want to do that\n> optimally, I think you have to have a mempool that retains conflicting\n> txs and runs a dynamic programming solution to pick the best set, rather\n> than today's simple greedy algorithms both for building the block and\n> populating the mempool?\n>\n> For example, if you had two such replacements come through the network,\n> a miner could want to flip from initially accepting the first replacement,\n> to unaccepting it:\n>\n> Initial mempool: two big txs at 100k each, many small transactions at\n> 15s/vB and 1s/vB\n>\n>  [100kvB at 20s/vB] [850kvB at 15s/vB] [100kvB at 12s/vB] [1000kvB at\n> 1s/vB]\n>    -> 0.148 BTC for 1MvB (100*20 + 850*15 + 50*1)\n>\n> Replacement for the 20s/vB tx paying a higher fee rate but lower total\n> fee; that's worth including:\n>\n>  [10kvB at 100s/vB] [850kvB at 15s/vB] [100kvB at 12s/vB [1000kvB at 1s/vB]\n>    -> 0.1499 BTC for 1MvB (10*100 + 850*15 + 100*12 + 40*1)\n>\n> Later, replacement for the 12s/vB tx comes in, also paying higher fee\n> rate but lower total fee. Worth including, but only if you revert the\n> original replacement:\n>\n>  [100kvB at 20s/vB] [50kvB at 20s/vB] [850kvB at 15s/vB] [1000kvB at 1s/vB]\n>    -> 0.16 BTC for 1MvB (150*20 + 850*15)\n>\n>  [10kvB at 100s/vB] [50kvB at 20s/vB] [850kvB at 15s/vB] [1000kvB at 1s/vB]\n>    -> 0.1484 BTC for 1MvB (10*100 + 50*20 + 850*15 + 90*1)\n>\n> Algorithms/mempool policies you might have, and their results with\n> this example:\n>\n>  * current RBF rules: reject both replacements because they don't\n>    increase the absolute fee, thus get the minimum block fees of\n>    0.148 BTC\n>\n>  * reject RBF unless it increases the fee rate, and get 0.1484 BTC in\n>    fees\n>\n>  * reject RBF if it's lower fee rate or immediately decreases the block\n>    reward: so, accept the first replacement, but reject the second,\n>    getting 0.1499 BTC\n>\n>  * only discard a conflicting tx when it pays both a lower fee rate and\n>    lower absolute fees, and choose amongst conflicting txs optimally\n>    via some complicated tx allocation algorithm when generating a block,\n>    and get 0.16 BTC\n>\n> In this example, those techniques give 92.5%, 92.75%, 93.69% and 100% of\n> total possible fees you could collect; and 99.813%, 99.819%, 99.84% and\n> 100% of the total possible block reward at 6.25BTC/block.\n>\n> Is there a plausible example where the difference isn't that marginal?\n> Seems like the simplest solution of just checking the (package/descendent)\n> fee rate increases works well enough here at least.\n>\n> If 90kvB of unrelated txs at 14s/vB were then added to the mempool, then\n> replacing both txs becomes (just barely) optimal, meaning the smartest\n> possible algorithm and the dumbest one of just considering the fee rate\n> produce the same result, while the others are worse:\n>\n>  [10kvB at 100s/vB] [50kvB at 20s/vB] [850kvB at 15s/vB] [90kvB at 14s/vB]\n>    -> 0.1601 BTC for 1MvB\n>    (accepting both)\n>\n>  [100kvB at 20s/vB] [50kvB at 20s/vB] [850kvB at 15s/vB] [90kvB at 14s/vB]\n>    -> 0.1575 BTC for 1MvB\n>    (accepting only the second replacement)\n>\n>  [10kvB at 100s/vB] [850kvB at 15s/vB] [90kvB at 14s/vB] [100kvB at 12s/vB]\n>    -> 0.1551 BTC for 1MvB\n>    (first replacement only, optimal tx selection: 10*100, 850*15, 50*14,\n> 100*12)\n>\n>  [100kvB at 20s/vB] [850kvB at 15s/vB] [90kvB at 14s/vB] [100kvB at 12s/vB]\n>    -> 0.1545 BTC for 1MvB\n>    (accepting neither replacement)\n>\n>  [10kvB at 100s/vB] [850kvB at 15s/vB] [90kvB at 14s/vB] [100kvB at 12s/vB]\n>    -> 0.1506 BTC for 1MvB\n>    (first replacement only, greedy tx selection: 10*100, 850*15, 90*14,\n> 50*1)\n>\n> Always accepting (package/descendent) fee rate increases removes the\n> possibility of pinning entirely, I think -- you still have the problem\n> where someone else might get a conflicting transaction confirmed first,\n> but they can't get a conflicting tx stuck in the mempool without\n> confirming if you're willing to pay enough to get it confirmed.\n>\n>\n>\n> Note that if we did have this policy, you could abuse it to cheaply drain\n> people's mempools: if there was a 300MB backlog, you could publish 2980\n> 100kB txs paying a fee rate just below the next block fee, meaning you'd\n> kick out the previous backlog and your transactions take up all but the\n> top 2MB of the mempool; if you then replace them all with perhaps 2980\n> 100B txs paying a slightly higher fee rate, the default mempool will be\n> left with only 2.3MB, at an ultimate cost to you of only about 30% of a\n> block in fees, and you could then fill the mempool back up by spamming\n> 300MB of ultra low fee rate txs.\n>\n> I think spam prevention at the outbound relay level isn't enough to\n> prevent that: an attacker could contact every public node and relay the\n> txs directly, clearing out the mempool of most public nodes directly. So\n> you'd want some sort of spam prevention on inbound txs too?\n>\n> So I think you'd need to carefully think about relay spam before making\n> this sort of change.  Also, if we had tx rebroadcast implemented then\n> having just a few nodes with large mempools might allow the network to\n> recover from this situation automatically.\n>\n> Cheers,\n> aj\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220217/b4aa2987/attachment-0001.html>"
            },
            {
                "author": "James O'Beirne",
                "date": "2022-02-10T23:51:47",
                "message_text_only": "> It's not that simple. As a miner, if i have less than 1vMB of\ntransactions in my mempool. I don't want a 10sats/vb transaction paying\n100000sats by a 100sats/vb transaction paying only 10000sats.\n\nI don't understand why the \"<1vMB in the mempool\" case is even worth\nconsideration because the miner will just include the entire mempool in the\nnext block regardless of feerate.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220210/58bee534/attachment-0001.html>"
            },
            {
                "author": "darosior",
                "date": "2022-02-11T06:51:34",
                "message_text_only": "Well because in the example i gave you this decreases the miner's reward. The rule of increasing feerate you stated isn't always economically rationale.\n\nNote how it can also be extended, for instance if the miner only has 1.5vMB of txs and is not assured to receive enough transactions to fill 2 blocks he might be interested in maximizing absolute fees, not feerate.\n\nSure, we could make the argument that long term we need a large backlog of transactions anyways.. But that'd be unfortunately not in phase with today's reality.\n\n-------- Original Message --------\nOn Feb 11, 2022, 00:51, James O'Beirne wrote:\n\n>> It's not that simple. As a miner, if i have less than 1vMB of transactions in my mempool. I don't want a 10sats/vb transaction paying 100000sats by a 100sats/vb transaction paying only 10000sats.\n>\n> I don't understand why the \"<1vMB in the mempool\" case is even worth consideration because the miner will just include the entire mempool in the next block regardless of feerate.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220211/82a07832/attachment.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-02-12T19:44:41",
                "message_text_only": "With respect to the disagreement/misunderstanding about the  \"<1vMB in the\nmempool\" case, I think it's important to be clear about what the goals of\nrelay policy are. Should the goal be to only relay transactions that\nincrease miner revenue? Sure ideally, because we want to minimize load on\nthe network. But practically, getting that goal 100% probably involves\ntradeoffs of diminishing returns.\n\nThe only way to ensure that a transaction is only relayed when it increases\nminer revenue is to make relay rules exactly match miner inclusion rules.\nAnd since we don't want to (nor can we) force miners to do transaction\ninclusion the same as each other, we certainly can't realistically produce\nan environment where relay rules exactly match miner inclusion rules.\n\nSo I think the goal should *not *be strictly minimal relay, because it's\nnot practical and basically not even possible. Instead the goal should be\nsome close-enough approach.\n\nThis relates to the  \"<1vMB in the mempool\" case because the disagreement\nseems to be related to what trade offs to make. A simple rule that the\nfee-rate must be bumped by at least X satoshi would indeed allow the\nscenario darosior describes, where someone can broadcast one large\nlow-feerate transaction and then subsequently broadcast smaller but\nhigher-feerate transactions. The question is: is that really likely be a\nproblem? This can be framed by considering a couple cases:\n\n* The average case\n* The adversarial worst case\n\nIn the average case, no one is going to be broadcasting any transactions\nlike that because they don't need to. So in the average case, that scenario\ncan be ignored. In the adversarial case however, some large actor that\nsends lots of transactions could spam the network any time blockchain\ncongestion. What's the worst someone could do?\n\nWell if there's really simply not enough transactions to even fill the\nblock, without an absolute-fee bump requirement, a malicious actor could\ncreate a lot of spam. To the tune of over 8000 transactions (assuming a 1\nsat/vb relay rule) for an empty mempool where the malicious actor sends a\n2MB transaction with a 1 sat/vb fee, then a 1MB transaction with a 2\nsat/vb, then 666KB transaction for 3 sat/vb etc. But in considering that\nthis transaction would already take up the entire block, it would be just\nas effective for an attacker to send 8000 minimal sized transactions and\nhave them relayed. So this method of attack doesn't gain the attacker any\nadditional power to spam the network. Not to mention that nodes should be\neasily able to handle that load, so there's not much of an actual \"attack\"\nhappening here. Just an insignificant amount of avoidable extra spent\nelectricity and unnecessary internet traffic. Nothing that's going to make\nrunning a full node any harder.\n\nAnd in the case that there *are* enough transactions to fill the block\n(which I think is the normal case, and it really should become a rarity for\nthis not to the case in the future), higher feerate transactions are always\nbetter unless you already overpaid for fees. Sure you can overpay and then\nadd some spam by making successively higher feerate but smaller\ntransactions, but in that case you've basically paid for all that spam up\nfront with your original fee. So is it really spam? If you've covered the\ncost of it, then its not spam as much as it is stupid behavior.\n\nSo I'm inclined to agree with O'Beirne (and Lisa Neigut) that valid\ntransactions with feerate bumps should never be excluded from relay as long\nas the amount of the feerate bump is more than the node's minimum\ntransaction fee. Doing that would also get rid of the spectre of\ntransaction pinning.\n\n*I'm curious if there's some other type of scenario where removing the\nabsolute fee bump rule would cause nodes to relay more transactions than\nthey would relay in a full/congested mempool scenario*. We shouldn't care\nabout spam that only happens when the network is quiet and can't bring\nnetwork traffic above normal non-quiet loads because a case like that isn't\na dos risk.\n\nOn Fri, Feb 11, 2022 at 3:13 AM darosior via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Well because in the example i gave you this decreases the miner's reward.\n> The rule of increasing feerate you stated isn't always economically\n> rationale.\n>\n>\n> Note how it can also be extended, for instance if the miner only has\n> 1.5vMB of txs and is not assured to receive enough transactions to fill 2\n> blocks he might be interested in maximizing absolute fees, not feerate.\n>\n>\n> Sure, we could make the argument that long term we need a large backlog of\n> transactions anyways.. But that'd be unfortunately not in phase with\n> today's reality.\n>\n>\n> -------- Original Message --------\n> On Feb 11, 2022, 00:51, James O'Beirne < james.obeirne at gmail.com> wrote:\n>\n>\n> > It's not that simple. As a miner, if i have less than 1vMB of\n> transactions in my mempool. I don't want a 10sats/vb transaction paying\n> 100000sats by a 100sats/vb transaction paying only 10000sats.\n>\n> I don't understand why the \"<1vMB in the mempool\" case is even worth\n> consideration because the miner will just include the entire mempool in the\n> next block regardless of feerate.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220212/a4ddf265/attachment.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2022-02-11T00:12:16",
                "message_text_only": "This is great in theory, but I think it kinda misses *why* the complexity keeps creeping in. We \nagree on (most of) the goals here, but the problem is the goals explicitly lead to the complexity, \nits not some software engineering failure or imagination failure that leads to the complexity.\n\nOn 2/10/22 14:40, James O'Beirne via bitcoin-dev wrote:\n-snip-\n> # Purely additive feerate bumps should never be impossible\n> \n> Any user should always be able to add to the incentive to mine any\n> transaction in a purely additive way. The countervailing force here\n> ends up being spam prevention (a la min-relay-fee) to prevent someone\n> from consuming bandwidth and mempool space with a long series of\n> infinitesimal fee-bumps.\n> \n> A fee bump, naturally, should be given the same per-byte consideration\n> as a normal Bitcoin transaction in terms of relay and block space,\n> although it would be nice to come up with a more succinct\n> representation. This leads to another design principle:\n\nThis is where *all* the complexity comes from. If our goal is to \"ensure a bump increases a miner's \noverall revenue\" (thus not wasting relay for everyone else), then we precisely *do* need\n\n > Special consideration for \"what should be in the next\n > block\" and/or the caching of block templates seems like an imposing\n > dependency\n\nWhether a transaction increases a miner's revenue depends precisely on whether the transaction \n(package) being replaced is in the next block - if it is, you care about the absolute fee of the \npackage and its replacement. If it is not in the next block (or, really, not near a block boundary \nor further down in the mempool where you assume other transactions will appear around it over time), \nthen you care about the fee *rate*, not the fee difference.\n\n > # The bandwidth and chain space consumed by a fee-bump should be minimal\n >\n > Instead of prompting a rebroadcast of the original transaction for\n > replacement, which contains a lot of data not new to the network, it\n > makes more sense to broadcast the \"diff\" which is the additive\n > contribution towards some txn's feerate.\n\nThis entirely misses the network cost. Yes, sure, we can send \"diffs\", but if you send enough diffs \neventually you send a lot of data. We cannot simply ignore network-wide costs like total relay \nbandwidth (or implementation runtime DoS issues).\n\n> # Special transaction structure should not be required to bump fees\n> \n> In an ideal design, special structural foresight would not be needed\n> in order for a txn's feerate to be improved after broadcast.\n> \n> Anchor outputs specified solely for CPFP, which amount to many bytes of\n> wasted chainspace, are a hack. > It's probably uncontroversial at this\n\nThis has nothing to do with fee bumping, though, this is only solved with covenants or something in \nthat direction, not different relay policy.\n\n> Coming down to earth, the \"tabula rasa\" thought experiment above has led\n> me to favor an approach like the transaction sponsors design that Jeremy\n> proposed in a prior discussion back in 2020[1].\n\nHow does this not also fail your above criteria of not wasting block space?\n\nFurther, this doesn't solve pinning attacks at all. In lightning we want to be able to *replace* \nsomething in the mempool (or see it confirm soon, but that assumes we know exactly what transaction \nis in \"the\" mempool). Just being able to sponsor something doesn't help if you don't know what that \nthing is.\n\nMatt"
            },
            {
                "author": "James O'Beirne",
                "date": "2022-02-14T19:51:26",
                "message_text_only": "> This entirely misses the network cost. Yes, sure, we can send\n> \"diffs\", but if you send enough diffs eventually you send a lot of data.\n\nThe whole point of that section of the email was to consider the\nnetwork cost. There are many cases for which transmitting a\nsupplementary 1-in-1-out transaction (i.e. a sponsorship txn) is going\nto be more efficient from a bandwidth standpoint than rebroadcasting a\npotentially large txn during RBF.\n\n> > In an ideal design, special structural foresight would not be\n> > needed in order for a txn's feerate to be improved after broadcast.\n> >\n> > Anchor outputs specified solely for CPFP, which amount to many\n> > bytes of wasted chainspace, are a hack. > It's probably\n> > uncontroversial at this\n>\n> This has nothing to do with fee bumping, though, this is only solved\n> with covenants or something in that direction, not different relay\n> policy.\n\nMy post isn't only about relay policy; it's that txn\nsponsors allows for fee-bumping in cases where RBF isn't possible and\nCPFP would be wasteful, e.g. for a tree of precomputed vault\ntransactions or - maybe more generally - certain kinds of\ncovenants.\n\n> How does this not also fail your above criteria of not wasting block\n> space?\n\nIn certain cases (e.g. vault structures), using sponsorship txns to\nbump fees as-needed is more blockspace-efficient than including\nmostly-unused CPFP \"anchor\" outputs that pay to fee-management wallets.\nI'm betting there are other similar cases where CPFP anchors are\nincluded but not necessarily used, and amount to wasted blockspace.\n\n> Further, this doesn't solve pinning attacks at all. In lightning we\n> want to be able to *replace* something in the mempool (or see it\n> confirm soon, but that assumes we know exactly what transaction is in\n> \"the\" mempool). Just being able to sponsor something doesn't help if\n> you don't know what that thing is.\n\nWhen would you be trying to bump the fee on a transaction without\nknowing what it is? Seeing a specific transaction \"stuck\" in the\nmempool seems to be a prerequisite to bumping fees. I'm not sure what\nyou're getting at here.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220214/bafcc985/attachment.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2022-02-11T05:26:53",
                "message_text_only": "Hi James,\n\nI fully agree on the need to reframe the conversation around\nmempools/fee-bumping/L2s though please see my following comments, it's far\nfrom simple!\n\n> Layering on special cases, more carve-outs, and X and Y percentage\n> thresholds is going to make reasoning about the mempool harder than it\n> already is.\n\nI think that's true with a lot of (all ?) pieces of software, there is a\ntrend towards complexification. As new Bitcoin use-cases such as LN or\nvaults appear, it's not surprising to see the base layer upper interfaces\nchanging to support the requirements. Same with kernels, at beginning, you\ncan have a basic memory support with paging/memory rights/kernel allocators\nthen as you start to support more platforms/devices you might have to\nsupport swaps/DMA/VMs management...\n\nThat we should keep the complexity reasonably sane to enable human\nauditing, and maybe learn from the failures of systems engineering, that's\nsomething to muse on.\n\n> The countervailing force here ends up being spam prevention (a la\nmin-relay-fee)\n> to prevent someone from consuming bandwidth and mempool space with a long\nseries of\n> infinitesimal fee-bumps.\n\nI think here we should dissociate a) long-chain of transactions and b)\nhigh-number of repeated fee-bumps.\n\nFor a) _if_ SIGHASH_ANYPREVOUT is deployed and Eltoo adopted as a primary\nupdate mechanism for stateful L2s, one might envision long-chain of update\ntransactions servicing as a new pinning vector, where all the chain\nelements are offering a compelling feerate/fees. It might be solvable with\nsmarter mempool logic sorting the elements from the best offer to the lower\nones, though that issue would need more serious investigation.\n\nFor b) if we bound with a hard constant the number of RBF attempts, we\ndecrease the fault-tolerance of L2 transaction issuers. Some of them might\nconnect directly to the miners because they're offering higher number of\nincentive-compatible RBF attempts than vanilla full-nodes. That might\nprovoke a more or slow centralization of the transaction relay topology...\n\n> Instead of prompting a rebroadcast of the original transaction for\n> replacement, which contains a lot of data not new to the network, it\n> makes more sense to broadcast the \"diff\" which is the additive\n> contribution towards some txn's feerate.\n\nIn a distributed system such as the Bitcoin p2p network, you might have\ntransaction A and transaction B  broadcast at the same time and your peer\ntopology might fluctuate between original send and broadcast\nof the diff, you don't know who's seen what... You might inefficiently\nannounce diff A on top of B and diff B on top A. We might leverage set\nreconciliation there a la Erlay, though likely with increased round-trips.\n\n> It's probably uncontroversial at this\n> point to say that even RBF itself is kind of a hack - a special\n> sequence number should not be necessary for post-broadcast contribution\n> toward feerate.\n\nI think here we should dissociate the replace-by-fee mechanism itself from\nthe replacement signaling one. To have a functional RBF, you don't need\nsignaling at all, just consider all received transactions as replaceable.\nThe replacement signaling one has been historically motivated to protect\nthe applications relying on zero-confs (with all the past polemics about\nthe well-foundedness of such claims on other nodes' policy).\n\n> In a sane design, no structural foresight - and certainly no wasted\n>bytes in the form of unused anchor outputs - should be needed in order\n>to add to a miner's reward for confirming a given transaction.\n\nHave you heard about SIGHASH_GROUP [0] ? It would move away from the\ntransaction to enable arbitrary bundles of input/outputs. You will have\nyour pre-signed bundle of inputs/outputs enforcing your LN/vaults/L2 and\nthen at broadcast time, you can attach an input/output. I think it would\nanswer your structural foresight.\n\n> One of the practical downsides of CPFP that I haven't seen discussed in\n> this conversation is that it requires the transaction to pre-specify the\n> keys needed to sign for fee bumps. This is problematic if you're, for\n> example, using a vault structure that makes use of pre-signed\n> transactions.\n\nIt's true it requires to pre-specify the fee-bumping key. Though note the\nfee-bumping key can be fully separated from the \"vaults\"/\"channels\" set of\nmain keys and hosted on replicated infrastructure such as watchtowers.\n\n> The interface for end-users is very straightforward: if you want to bump\n> fees, specify a transaction that contributes incrementally to package\n> feerate for some txid. Simple.\n\nAs a L2 transaction issuer you can't be sure the transaction you wish to\npoint to is already in the mempool, or have not been replaced by your\ncounterparty spending the same shared-utxo, either competitively or\nmaliciously. So as a measure of caution, you should broadcast sponsor +\ntarget transactions in the same package, thus cancelling the bandwidth\nsaving (I think).\n\n> This theoretical concession seems preferable to heaping more rules onto\nan already labyrinthine mempool policy that is difficult for both\nimplementers and users to reason about practically and conceptually.\n\nI don't think a sponsor is a silver-bullet to solve all the L2-related\nmempool issues. It won't solve the most concerning pinning attacks, as I\nthink the bottleneck is replace-by-fee. Neither solve the issues encumbered\nby the L2s by the dust limit.\n\n> If a soft-fork is the cost of cleaning up this essential process,\n> consideration should be given to paying it as a one-time cost. This\n> topic merits a separate post, but consider that in the 5 years leading\n> up to the 2017 SegWit drama, we averaged about a soft-fork a year.\n> Uncontroversial, \"safe\" changes to the consensus protocol shouldn't be\n> out of the question when significant practical benefit is plain to see.\n\nZooming out, I think we're still early in solving those L2 issues, as the\nmost major second-layers are still in a design or deployment phase. We\nmight freeze our transaction propagation interface, and get short for some\nof the most interesting ones like channel factories and payment pools.\nFurther, I think we're not entirely sure how the mining ecosystem is going\nto behave once the reward is drained and their incentives towards L2\nconfirmations.\n\nStill, if we think we have a correct picture of the fee-bumping/mempools\nissues and are sufficiently confident with the stability of L2 designs, I\nthink the next step would be to come with quantitative modelling of each\nresources consumed by fee-bumping (CPU validation/bandwidth/signing\ninteractivity for the L2s...) and then score the \"next-gen\" fee-bumping\nprimitives.\n\n> I'm not out to propose soft-forks lightly, but the current complexity\n> in fee management feels untenable, and as evidenced by all the\n> discussion lately, fees are an increasingly crucial part of the system.\n\nOverall, I think that's a relevant discussion to have ecosystem-wise.\nThough there is a lot of context and I don't think there is a simple way\nforward. Maybe better to stick to an evolutionary development process with\nthose mempool/fee-bumping issues. We might envision two-or-three steps\nahead though unlikely more.\n\nCheers,\nAntoine\n\n[0] SIGHASH_GROUP described here\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-May/019031.html\nand roughly roughly implemented here :\nhttps://github.com/ariard/bitcoin/pull/1\n\nLe jeu. 10 f\u00e9vr. 2022 \u00e0 14:48, James O'Beirne via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> There's been much talk about fee-bumping lately, and for good reason -\n> dynamic fee management is going to be a central part of bitcoin use as\n> the mempool fills up (lord willing) and right now fee-bumping is\n> fraught with difficulty and pinning peril.\n>\n> Gloria's recent post on the topic[0] was very lucid and highlights a\n> lot of the current issues, as well as some proposals to improve the\n> situation.\n>\n> As others have noted, the post was great. But throughout the course\n> of reading it and the ensuing discussion, I became troubled by the\n> increasing complexity of both the status quo and some of the\n> proposed remedies.\n>\n> Layering on special cases, more carve-outs, and X and Y percentage\n> thresholds is going to make reasoning about the mempool harder than it\n> already is. Special consideration for \"what should be in the next\n> block\" and/or the caching of block templates seems like an imposing\n> dependency, dragging in a bunch of state and infrastructure to a\n> question that should be solely limited to mempool feerate aggregates\n> and the feerate of the particular txn package a wallet is concerned\n> with.\n>\n> This is bad enough for protocol designers and Core developers, but\n> making the situation any more intractable for \"end-users\" and wallet\n> developers feels wrong.\n>\n> I thought it might be useful to step back and reframe. Here are a few\n> aims that are motivated chiefly by the quality of end-user experience,\n> constrained to obey incentive compatibility (i.e. miner reward, DoS\n> avoidance). Forgive the abstract dalliance for a moment; I'll talk\n> through concretes afterwards.\n>\n>\n> # Purely additive feerate bumps should never be impossible\n>\n> Any user should always be able to add to the incentive to mine any\n> transaction in a purely additive way. The countervailing force here\n> ends up being spam prevention (a la min-relay-fee) to prevent someone\n> from consuming bandwidth and mempool space with a long series of\n> infinitesimal fee-bumps.\n>\n> A fee bump, naturally, should be given the same per-byte consideration\n> as a normal Bitcoin transaction in terms of relay and block space,\n> although it would be nice to come up with a more succinct\n> representation. This leads to another design principle:\n>\n>\n> # The bandwidth and chain space consumed by a fee-bump should be minimal\n>\n> Instead of prompting a rebroadcast of the original transaction for\n> replacement, which contains a lot of data not new to the network, it\n> makes more sense to broadcast the \"diff\" which is the additive\n> contribution towards some txn's feerate.\n>\n> This dovetails with the idea that...\n>\n>\n> # Special transaction structure should not be required to bump fees\n>\n> In an ideal design, special structural foresight would not be needed\n> in order for a txn's feerate to be improved after broadcast.\n>\n> Anchor outputs specified solely for CPFP, which amount to many bytes of\n> wasted chainspace, are a hack. It's probably uncontroversial at this\n> point to say that even RBF itself is kind of a hack - a special\n> sequence number should not be necessary for post-broadcast contribution\n> toward feerate. Not to mention RBF's seemingly wasteful consumption of\n> bandwidth due to the rebroadcast of data the network has already seen.\n>\n> In a sane design, no structural foresight - and certainly no wasted\n> bytes in the form of unused anchor outputs - should be needed in order\n> to add to a miner's reward for confirming a given transaction.\n>\n> Planning for fee-bumps explicitly in transaction structure also often\n> winds up locking in which keys are required to bump fees, at odds\n> with the idea that...\n>\n>\n> # Feerate bumps should be able to come from anywhere\n>\n> One of the practical downsides of CPFP that I haven't seen discussed in\n> this conversation is that it requires the transaction to pre-specify the\n> keys needed to sign for fee bumps. This is problematic if you're, for\n> example, using a vault structure that makes use of pre-signed\n> transactions.\n>\n> What if the key you specified n the anchor outputs for a bunch of\n> pre-signed txns is compromised? What if you'd like to be able to\n> dynamically select the wallet that bumps fees? CPFP does you no favors\n> here.\n>\n> There is of course a tension between allowing fee bumps to come from\n> anywhere and the threat of pinning-like attacks. So we should venture\n> to remove pinning as a possibility, in line with the first design\n> principle I discuss.\n>\n>\n> ---\n>\n> Coming down to earth, the \"tabula rasa\" thought experiment above has led\n> me to favor an approach like the transaction sponsors design that Jeremy\n> proposed in a prior discussion back in 2020[1].\n>\n> Transaction sponsors allow feerates to be bumped after a transaction's\n> broadcast, regardless of the structure of the original transaction.\n> No rebroadcast (wasted bandwidth) is required for the original txn data.\n> No wasted chainspace on only-maybe-used prophylactic anchor outputs.\n>\n> The interface for end-users is very straightforward: if you want to bump\n> fees, specify a transaction that contributes incrementally to package\n> feerate for some txid. Simple.\n>\n> In the original discussion, there were a few main objections that I noted:\n>\n> 1. In Jeremy's original proposal, only one sponsor txn per txid is\n>    allowed by policy. A malicious actor could execute a pinning-like\n>    attack by specifying an only-slightly-helpful feerate sponsor that\n>    then precludes other larger bumps.\n>\n> I think there are some ways around this shortcoming. For example: what\n> if, by policy, sponsor txns had additional constraints that\n>\n>   - each input must be signed {SIGHASH_SINGLE,SIGHASH_NONE}|ANYONECANPAY,\n>   - the txn must be specified RBFable,\n>   - a replacement for the sponsor txn must raise the sponsor feerate,\n>     including ancestors (maybe this is inherent in \"is RBFable,\" but\n>     I don't want to conflate absolute feerates into this).\n>\n> That way, there is still at most a single sponsor txn per txid in the\n> mempool, but anyone can \"mix in\" inputs which bump the effective\n> feerate of the sponsor.\n>\n> This may not be the exact solution we want, but I think it demonstrates\n> that the sponsors design has some flexibility and merits some thinking.\n>\n> The second objection about sponsors was\n>\n> 2. (from Suhas) sponsors break the classic invariant: \"once a valid\n>    transaction is created, it should not become invalid later on unless\n>    the inputs are double-spent.\"\n>\n> This doesn't seem like a huge concern to me if you consider the txid\n> being sponsored as a sort of spiritual input to the sponsor. While the\n> theoretical objection against broadening where one has to look in a txn\n> to determine its dependencies is understandable, I don't see what the\n> practical cost here is.\n>\n> Reorg complexity seems comparable if not identical, especially if we\n> broaden sponsor rules to allow blocks to contain sponsor txns that are\n> both for txids in the same block _or_ already included in the chain.\n>\n> This theoretical concession seems preferable to heaping more rules onto\n> an already labyrinthine mempool policy that is difficult for both\n> implementers and users to reason about practically and conceptually.\n>\n> A third objection that wasn't posed, IIRC, but almost certainly would\n> be:\n>\n> 3. Transaction sponsors requires a soft-fork.\n>\n> Soft-forks are no fun, but I'll tell you what also isn't fun: being on\n> the hook to model (and sometimes implement) a dizzying potpourri of\n> mempool policies and special-cases. Expecting wallet implementers to\n> abide by a maze of rules faithfully in order to ensure txn broadcast and\n> fee management invites bugs for perpetuity and network behavior that is\n> difficult to reason about a priori. Use of CPFP in the long-term also\n> risks needless chain waste.\n>\n> If a soft-fork is the cost of cleaning up this essential process,\n> consideration should be given to paying it as a one-time cost. This\n> topic merits a separate post, but consider that in the 5 years leading\n> up to the 2017 SegWit drama, we averaged about a soft-fork a year.\n> Uncontroversial, \"safe\" changes to the consensus protocol shouldn't be\n> out of the question when significant practical benefit is plain to see.\n>\n> ---\n>\n> I hope this message has added some framing to the discussion on fees,\n> as well prompting other participants to go back and give the\n> transaction sponsor proposal a serious look. The sponsors interface is\n> about the simplest I can imagine for wallets, and it seems easy to\n> reason about for implementers on Core and elsewhere.\n>\n> I'm not out to propose soft-forks lightly, but the current complexity\n> in fee management feels untenable, and as evidenced by all the\n> discussion lately, fees are an increasingly crucial part of the system.\n>\n>\n>\n> [0]:\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019817.html\n> [1]:\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-September/018168.html\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220211/48f8d470/attachment-0001.html>"
            },
            {
                "author": "James O'Beirne",
                "date": "2022-02-14T20:28:51",
                "message_text_only": "Thanks for your thoughtful reply Antoine.\n\n> In a distributed system such as the Bitcoin p2p network, you might\n> have transaction A and transaction B  broadcast at the same time and\n> your peer topology might fluctuate between original send and\n> broadcast of the diff, you don't know who's seen what... You might\n> inefficiently announce diff A on top of B and diff B on top A. We\n> might leverage set reconciliation there a la Erlay, though likely\n> with increased round-trips.\n\nIn the context of fee bumping, I don't see how this is a criticism\nunique to transaction sponsors, since it also applies to CPFP: if you\ntried to bump fees for transaction A with child txn B, if some mempool\nhasn't seen parent A, it will reject B.\n\n> Have you heard about SIGHASH_GROUP [0] ?\n\nI haven't - I'll spend some time reviewing this. Thanks.\n\n> > [me complaining CPFP requires lock-in to keys]\n>\n> It's true it requires to pre-specify the fee-bumping key. Though note\n> the fee-bumping key can be fully separated from the\n> \"vaults\"/\"channels\" set of main keys and hosted on replicated\n> infrastructure such as watchtowers.\n\nThis still doesn't address the issue I'm talking about, which is if you\npre-commit to some \"fee-bumping\" key in your CPFP outputs and that key\nends up being compromised. This isn't a matter of data availability or\nredundancy.\n\nNote that this failure may be unique to vault use cases, when you're\npre-generating potentially large numbers of transactions or covenants\nthat cannot be altered after the fact. If you generate vault txns that\nassume the use of some key for CPFP-based fee bumping and that key\nwinds up being compromised, that puts you in a an uncomfortable\nsituation: you can no longer bump fees on unvaulting transactions,\nrendering the vaults possibly unretrievable depending on the fee market.\n\n> As a L2 transaction issuer you can't be sure the transaction you wish\n> to point to is already in the mempool, or have not been replaced by\n> your counterparty spending the same shared-utxo, either competitively\n> or maliciously. So as a measure of caution, you should broadcast\n> sponsor + target transactions in the same package, thus cancelling\n> the bandwidth saving (I think).\n\nAs I mentioned in the reply to Matt's message, I'm not quite\nunderstanding this idea of wanting to bump the fee for something\nwithout knowing what it is; that doesn't make much sense to me.\nThe \"bump fee\" operation seems contingent on knowing\nwhat you want to bump.\n\nAnd if you're, say, trying to broadcast a lightning channel close and\nyou know you need to bump the fee right away, before even broadcasting\nit, either you're going to\n\n- reformulate the txn to bring up the fee rate (e.g. add inputs\n  with some yet-undeployed sighash) as you would have done with RBF, or\n\n- you'd have the same \"package relay\" problem with CPFP that you\n  would with transaction sponsors.\n\nSo I don't understand the objection here.\n\nAlso, I didn't mean to discourage existing work on package relay or\nfixing RBF, which seem clearly important. Maybe I should have noted\nthat explicitly in the original message\n\n> I don't think a sponsor is a silver-bullet to solve all the\n> L2-related mempool issues. It won't solve the most concerning pinning\n> attacks, as I think the bottleneck is replace-by-fee. Neither solve\n> the issues encumbered by the L2s by the dust limit.\n\nI'm not familiar with the L2 dust-limit issues, and I do think that\n\"fixing\" RBF behavior is *probably* worthwhile. Those issues aside, I\nthink the transaction sponsors idea may be closer to a silver bullet\nthan you're giving it credit for, because designing specifically for the\nfee-management use case has some big benefits.\n\nFor one, it makes migration easier. That is to say: there is none,\nwhereas there is existing RBF policy that needs consideration.\n\nBut maybe more importantly, transaction sponsors' limited use case also\nallows for specifying much more targeted \"replacement\" policy since\nsponsors are special-purpose transactions that only exist to\ndynamically bump feerate. E.g. my SIGHASH_{NONE,SINGLE}|ANYONECANPAY\nproposal might make complete sense for the sponsors/fee-management use\ncase, and clarify the replacement problem, but obviously wouldn't work\nfor more general transaction replacement. In other words, RBF's\ngeneral nature might make it a much harder problem to solve well.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220214/37a896eb/attachment.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2022-02-15T00:43:26",
                "message_text_only": "> In the context of fee bumping, I don't see how this is a criticism\n> unique to transaction sponsors, since it also applies to CPFP: if you\n> tried to bump fees for transaction A with child txn B, if some mempool\n> hasn't seen parent A, it will reject B.\n\nAgree, it's a comment raising the shenanigans of tx-diff-only propagation,\nafaict affecting equally all fee-bumping primitives. It wasn't a criticism\nspecific to transaction sponsors, as at that point of your post, sponsors\nare not introduced yet.\n\n> This still doesn't address the issue I'm talking about, which is if you\n> pre-commit to some \"fee-bumping\" key in your CPFP outputs and that key\n> ends up being compromised. This isn't a matter of data availability or\n> redundancy.\n\nI'm not sure about the real safety risk of the compromise of the anchor\noutput key. Of course, if your anchor output key is compromised and the\nbumped package is already public/known, an attacker can extend your package\nwith junk to neutralize your carve-out capability (I think). That said,\nthis issue sounds solved to me with package relay, as you can always\nbroadcast a new version of the package from the root UTXO, without\nattention to the carve-out limitation.\n\n(Side-note: I think we can slowly deprecate the carve-out once package\nrelay is deployed, as the fee-bumping flexibility of the latter is a\nsuperset of the former).\n\n> As I mentioned in the reply to Matt's message, I'm not quite\n> understanding this idea of wanting to bump the fee for something\n> without knowing what it is; that doesn't make much sense to me.\n> The \"bump fee\" operation seems contingent on knowing\n> what you want to bump.\n\n>From your post : \"No rebroadcast (wasted bandwidth) is required for the\noriginal txn data.\"\n\nI'm objecting to that supposed benefit of a transaction sponsor. If you\nhave transaction X and transaction Y spending the same UTXO, both of them\ncan be defined as \"the original txn data\". If you wish to fee-bump\ntransaction X with sponsor, how can you be sure that transaction\nY isn't present in the majority of network nodes, and X has _not_ been\ndropped since your last broadcast ? Otherwise iirc sponsor design, your\nsponsor transaction is going to be rejected.\n\nI think you can't, and thus preventively you should broadcast as a (new\ntype) of package the sponsoring/sponsored transaction.\n\nThat said, I'm not sure if that issue is equally affecting vaults than\npayment channels. With vaults, the tree of transactions is  known ahead,\nand there is no competition in the spends. Assuming the first broadcast has\nbeen efficient (and it could be a reasonable assumption thanks to mempool\nrebroadcast), the sponsor should propagate.\n\nSo I think here for the sake of sponsor efficiency analysis, we might have\nto class between the protocol with once-for-all-transaction-negotiation\n(vaults) and the ones with off-chain, dynamic re-negotiation (payment\nchannels, factories) ?\n\n> I'm not familiar with the L2 dust-limit issues, and I do think that\n> \"fixing\" RBF behavior is *probably* worthwhile.\n\nSadly, it sounds that \"fixing\" RBF behavior is a requirement to eradicate\nthe most advanced pinnings... That fix is independent of the fee-bumping\nprimitive considered.\n\n>  Those issues aside, I\n> think the transaction sponsors idea may be closer to a silver bullet\n> than you're giving it credit for, because designing specifically for the\n> fee-management use case has some big benefits.\n\nI don't deny the scheme is interesting, though I would argue SIGHASH_GROUP\nis more efficient, while offering more flexibility. In any case, I think we\nshould still pursue further the collections of problems and requirements\n(batching, key management, ...) that new fee-bumping primitives should aim\nto solve, before engaging more on the deployment of one of them [0].\n\n[0] In that sense see\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-May/019031.html\n\nLe lun. 14 f\u00e9vr. 2022 \u00e0 15:29, James O'Beirne <james.obeirne at gmail.com> a\n\u00e9crit :\n\n> Thanks for your thoughtful reply Antoine.\n>\n> > In a distributed system such as the Bitcoin p2p network, you might\n> > have transaction A and transaction B  broadcast at the same time and\n> > your peer topology might fluctuate between original send and\n> > broadcast of the diff, you don't know who's seen what... You might\n> > inefficiently announce diff A on top of B and diff B on top A. We\n> > might leverage set reconciliation there a la Erlay, though likely\n> > with increased round-trips.\n>\n> In the context of fee bumping, I don't see how this is a criticism\n> unique to transaction sponsors, since it also applies to CPFP: if you\n> tried to bump fees for transaction A with child txn B, if some mempool\n> hasn't seen parent A, it will reject B.\n>\n> > Have you heard about SIGHASH_GROUP [0] ?\n>\n> I haven't - I'll spend some time reviewing this. Thanks.\n>\n> > > [me complaining CPFP requires lock-in to keys]\n> >\n> > It's true it requires to pre-specify the fee-bumping key. Though note\n> > the fee-bumping key can be fully separated from the\n> > \"vaults\"/\"channels\" set of main keys and hosted on replicated\n> > infrastructure such as watchtowers.\n>\n> This still doesn't address the issue I'm talking about, which is if you\n> pre-commit to some \"fee-bumping\" key in your CPFP outputs and that key\n> ends up being compromised. This isn't a matter of data availability or\n> redundancy.\n>\n> Note that this failure may be unique to vault use cases, when you're\n> pre-generating potentially large numbers of transactions or covenants\n> that cannot be altered after the fact. If you generate vault txns that\n> assume the use of some key for CPFP-based fee bumping and that key\n> winds up being compromised, that puts you in a an uncomfortable\n> situation: you can no longer bump fees on unvaulting transactions,\n> rendering the vaults possibly unretrievable depending on the fee market.\n>\n> > As a L2 transaction issuer you can't be sure the transaction you wish\n> > to point to is already in the mempool, or have not been replaced by\n> > your counterparty spending the same shared-utxo, either competitively\n> > or maliciously. So as a measure of caution, you should broadcast\n> > sponsor + target transactions in the same package, thus cancelling\n> > the bandwidth saving (I think).\n>\n> As I mentioned in the reply to Matt's message, I'm not quite\n> understanding this idea of wanting to bump the fee for something\n> without knowing what it is; that doesn't make much sense to me.\n> The \"bump fee\" operation seems contingent on knowing\n> what you want to bump.\n>\n> And if you're, say, trying to broadcast a lightning channel close and\n> you know you need to bump the fee right away, before even broadcasting\n> it, either you're going to\n>\n> - reformulate the txn to bring up the fee rate (e.g. add inputs\n>   with some yet-undeployed sighash) as you would have done with RBF, or\n>\n> - you'd have the same \"package relay\" problem with CPFP that you\n>   would with transaction sponsors.\n>\n> So I don't understand the objection here.\n>\n> Also, I didn't mean to discourage existing work on package relay or\n> fixing RBF, which seem clearly important. Maybe I should have noted\n> that explicitly in the original message\n>\n> > I don't think a sponsor is a silver-bullet to solve all the\n> > L2-related mempool issues. It won't solve the most concerning pinning\n> > attacks, as I think the bottleneck is replace-by-fee. Neither solve\n> > the issues encumbered by the L2s by the dust limit.\n>\n> I'm not familiar with the L2 dust-limit issues, and I do think that\n> \"fixing\" RBF behavior is *probably* worthwhile. Those issues aside, I\n> think the transaction sponsors idea may be closer to a silver bullet\n> than you're giving it credit for, because designing specifically for the\n> fee-management use case has some big benefits.\n>\n> For one, it makes migration easier. That is to say: there is none,\n> whereas there is existing RBF policy that needs consideration.\n>\n> But maybe more importantly, transaction sponsors' limited use case also\n> allows for specifying much more targeted \"replacement\" policy since\n> sponsors are special-purpose transactions that only exist to\n> dynamically bump feerate. E.g. my SIGHASH_{NONE,SINGLE}|ANYONECANPAY\n> proposal might make complete sense for the sponsors/fee-management use\n> case, and clarify the replacement problem, but obviously wouldn't work\n> for more general transaction replacement. In other words, RBF's\n> general nature might make it a much harder problem to solve well.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220214/887db345/attachment-0001.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-02-15T17:09:56",
                "message_text_only": ">   If you wish to fee-bump transaction X with sponsor, how can you be sure\nthat transaction isn't present in the majority of network nodes, and X has\n_not_ been dropped since your last broadcast ?\n\nYou're right that you can't assume your target transaction hasn't been\ndropped. However, I assume when James said \"No rebroadcast (wasted\nbandwidth) is required for the original txn data\" he meant that in the\ncontext of the \"diff\" he was talking about. It would be easy enough to\nspecify a sponsorship transaction that points to a transaction with a\nspecific id without *requiring* that transaction to be rebroadcast. If your\npartner node has that transaction, no rebroadcast is necessary. If your\npartner node doesn't have it, they can request it. That way rebroadcast is\nonly done when necessary. Correct me if my understanding of your suggestion\nis wrong James.\n\n>> 2. (from Suhas) \"once a valid transaction is created, it should not\nbecome invalid later on unless the inputs are double-spent.\"\n> This doesn't seem like a huge concern to me\n\nI agree that this shouldn't be a concern. In fact, I've asked numerous\npeople in numerous places what practical downside there is to transactions\nthat become invalid, and I've heard basically radio silence other than one\noff hand remark by satoshi at the dawn of time which didn't seem to me to\nhave good reasoning. I haven't seen any downside whatsoever of transactions\nthat can become invalid for anyone waiting the standard 6 confirmations -\nthe reorg risks only exists for people not waiting for standard\nfinalization. So I don't think we should consider that aspect of a\nsponsorship transaction that can only be mined with the transaction it\nsponsors to be a problem unless a specific practical problem case can be\nidentified. Even if a significant such case was identified, an easy\nsolution would be to simply allow sponsorship transactions to be mined on\nor after the sponsored transaction is mined.\n\n\n\nOn Mon, Feb 14, 2022 at 7:10 PM Antoine Riard via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> > In the context of fee bumping, I don't see how this is a criticism\n> > unique to transaction sponsors, since it also applies to CPFP: if you\n> > tried to bump fees for transaction A with child txn B, if some mempool\n> > hasn't seen parent A, it will reject B.\n>\n> Agree, it's a comment raising the shenanigans of tx-diff-only propagation,\n> afaict affecting equally all fee-bumping primitives. It wasn't a criticism\n> specific to transaction sponsors, as at that point of your post, sponsors\n> are not introduced yet.\n>\n> > This still doesn't address the issue I'm talking about, which is if you\n> > pre-commit to some \"fee-bumping\" key in your CPFP outputs and that key\n> > ends up being compromised. This isn't a matter of data availability or\n> > redundancy.\n>\n> I'm not sure about the real safety risk of the compromise of the anchor\n> output key. Of course, if your anchor output key is compromised and the\n> bumped package is already public/known, an attacker can extend your package\n> with junk to neutralize your carve-out capability (I think). That said,\n> this issue sounds solved to me with package relay, as you can always\n> broadcast a new version of the package from the root UTXO, without\n> attention to the carve-out limitation.\n>\n> (Side-note: I think we can slowly deprecate the carve-out once package\n> relay is deployed, as the fee-bumping flexibility of the latter is a\n> superset of the former).\n>\n> > As I mentioned in the reply to Matt's message, I'm not quite\n> > understanding this idea of wanting to bump the fee for something\n> > without knowing what it is; that doesn't make much sense to me.\n> > The \"bump fee\" operation seems contingent on knowing\n> > what you want to bump.\n>\n> From your post : \"No rebroadcast (wasted bandwidth) is required for the\n> original txn data.\"\n>\n> I'm objecting to that supposed benefit of a transaction sponsor. If you\n> have transaction X and transaction Y spending the same UTXO, both of them\n> can be defined as \"the original txn data\". If you wish to fee-bump\n> transaction X with sponsor, how can you be sure that transaction\n> Y isn't present in the majority of network nodes, and X has _not_ been\n> dropped since your last broadcast ? Otherwise iirc sponsor design, your\n> sponsor transaction is going to be rejected.\n>\n> I think you can't, and thus preventively you should broadcast as a (new\n> type) of package the sponsoring/sponsored transaction.\n>\n> That said, I'm not sure if that issue is equally affecting vaults than\n> payment channels. With vaults, the tree of transactions is  known ahead,\n> and there is no competition in the spends. Assuming the first broadcast has\n> been efficient (and it could be a reasonable assumption thanks to mempool\n> rebroadcast), the sponsor should propagate.\n>\n> So I think here for the sake of sponsor efficiency analysis, we might have\n> to class between the protocol with once-for-all-transaction-negotiation\n> (vaults) and the ones with off-chain, dynamic re-negotiation (payment\n> channels, factories) ?\n>\n> > I'm not familiar with the L2 dust-limit issues, and I do think that\n> > \"fixing\" RBF behavior is *probably* worthwhile.\n>\n> Sadly, it sounds that \"fixing\" RBF behavior is a requirement to eradicate\n> the most advanced pinnings... That fix is independent of the fee-bumping\n> primitive considered.\n>\n> >  Those issues aside, I\n> > think the transaction sponsors idea may be closer to a silver bullet\n> > than you're giving it credit for, because designing specifically for the\n> > fee-management use case has some big benefits.\n>\n> I don't deny the scheme is interesting, though I would argue SIGHASH_GROUP\n> is more efficient, while offering more flexibility. In any case, I think we\n> should still pursue further the collections of problems and requirements\n> (batching, key management, ...) that new fee-bumping primitives should aim\n> to solve, before engaging more on the deployment of one of them [0].\n>\n> [0] In that sense see\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-May/019031.html\n>\n> Le lun. 14 f\u00e9vr. 2022 \u00e0 15:29, James O'Beirne <james.obeirne at gmail.com> a\n> \u00e9crit :\n>\n>> Thanks for your thoughtful reply Antoine.\n>>\n>> > In a distributed system such as the Bitcoin p2p network, you might\n>> > have transaction A and transaction B  broadcast at the same time and\n>> > your peer topology might fluctuate between original send and\n>> > broadcast of the diff, you don't know who's seen what... You might\n>> > inefficiently announce diff A on top of B and diff B on top A. We\n>> > might leverage set reconciliation there a la Erlay, though likely\n>> > with increased round-trips.\n>>\n>> In the context of fee bumping, I don't see how this is a criticism\n>> unique to transaction sponsors, since it also applies to CPFP: if you\n>> tried to bump fees for transaction A with child txn B, if some mempool\n>> hasn't seen parent A, it will reject B.\n>>\n>> > Have you heard about SIGHASH_GROUP [0] ?\n>>\n>> I haven't - I'll spend some time reviewing this. Thanks.\n>>\n>> > > [me complaining CPFP requires lock-in to keys]\n>> >\n>> > It's true it requires to pre-specify the fee-bumping key. Though note\n>> > the fee-bumping key can be fully separated from the\n>> > \"vaults\"/\"channels\" set of main keys and hosted on replicated\n>> > infrastructure such as watchtowers.\n>>\n>> This still doesn't address the issue I'm talking about, which is if you\n>> pre-commit to some \"fee-bumping\" key in your CPFP outputs and that key\n>> ends up being compromised. This isn't a matter of data availability or\n>> redundancy.\n>>\n>> Note that this failure may be unique to vault use cases, when you're\n>> pre-generating potentially large numbers of transactions or covenants\n>> that cannot be altered after the fact. If you generate vault txns that\n>> assume the use of some key for CPFP-based fee bumping and that key\n>> winds up being compromised, that puts you in a an uncomfortable\n>> situation: you can no longer bump fees on unvaulting transactions,\n>> rendering the vaults possibly unretrievable depending on the fee market.\n>>\n>> > As a L2 transaction issuer you can't be sure the transaction you wish\n>> > to point to is already in the mempool, or have not been replaced by\n>> > your counterparty spending the same shared-utxo, either competitively\n>> > or maliciously. So as a measure of caution, you should broadcast\n>> > sponsor + target transactions in the same package, thus cancelling\n>> > the bandwidth saving (I think).\n>>\n>> As I mentioned in the reply to Matt's message, I'm not quite\n>> understanding this idea of wanting to bump the fee for something\n>> without knowing what it is; that doesn't make much sense to me.\n>> The \"bump fee\" operation seems contingent on knowing\n>> what you want to bump.\n>>\n>> And if you're, say, trying to broadcast a lightning channel close and\n>> you know you need to bump the fee right away, before even broadcasting\n>> it, either you're going to\n>>\n>> - reformulate the txn to bring up the fee rate (e.g. add inputs\n>>   with some yet-undeployed sighash) as you would have done with RBF, or\n>>\n>> - you'd have the same \"package relay\" problem with CPFP that you\n>>   would with transaction sponsors.\n>>\n>> So I don't understand the objection here.\n>>\n>> Also, I didn't mean to discourage existing work on package relay or\n>> fixing RBF, which seem clearly important. Maybe I should have noted\n>> that explicitly in the original message\n>>\n>> > I don't think a sponsor is a silver-bullet to solve all the\n>> > L2-related mempool issues. It won't solve the most concerning pinning\n>> > attacks, as I think the bottleneck is replace-by-fee. Neither solve\n>> > the issues encumbered by the L2s by the dust limit.\n>>\n>> I'm not familiar with the L2 dust-limit issues, and I do think that\n>> \"fixing\" RBF behavior is *probably* worthwhile. Those issues aside, I\n>> think the transaction sponsors idea may be closer to a silver bullet\n>> than you're giving it credit for, because designing specifically for the\n>> fee-management use case has some big benefits.\n>>\n>> For one, it makes migration easier. That is to say: there is none,\n>> whereas there is existing RBF policy that needs consideration.\n>>\n>> But maybe more importantly, transaction sponsors' limited use case also\n>> allows for specifying much more targeted \"replacement\" policy since\n>> sponsors are special-purpose transactions that only exist to\n>> dynamically bump feerate. E.g. my SIGHASH_{NONE,SINGLE}|ANYONECANPAY\n>> proposal might make complete sense for the sponsors/fee-management use\n>> case, and clarify the replacement problem, but obviously wouldn't work\n>> for more general transaction replacement. In other words, RBF's\n>> general nature might make it a much harder problem to solve well.\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220215/02f88440/attachment-0001.html>"
            },
            {
                "author": "Russell O'Connor",
                "date": "2022-02-15T20:24:29",
                "message_text_only": "> >> 2. (from Suhas) \"once a valid transaction is created, it should not\n> become invalid later on unless the inputs are double-spent.\"\n> > This doesn't seem like a huge concern to me\n>\n> I agree that this shouldn't be a concern. In fact, I've asked numerous\n> people in numerous places what practical downside there is to transactions\n> that become invalid, and I've heard basically radio silence other than one\n> off hand remark by satoshi at the dawn of time which didn't seem to me to\n> have good reasoning. I haven't seen any downside whatsoever of transactions\n> that can become invalid for anyone waiting the standard 6 confirmations -\n> the reorg risks only exists for people not waiting for standard\n> finalization. So I don't think we should consider that aspect of a\n> sponsorship transaction that can only be mined with the transaction it\n> sponsors to be a problem unless a specific practical problem case can be\n> identified. Even if a significant such case was identified, an easy\n> solution would be to simply allow sponsorship transactions to be mined on\n> or after the sponsored transaction is mined.\n>\n\nThe downside is that in a 6 block reorg any transaction that is moved past\nits expiration date becomes invalid and all its descendants become invalid\ntoo.\n\nThe current consensus threshold for transactions to become invalid is a 100\nblock reorg, and I see no reason to change this threshold.  I promise to\npersonally build a wallet that always creates transactions on the verge of\nbecoming invalid should anyone ever implement a feature that violates this\ntx validity principle.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220215/12596d01/attachment.html>"
            },
            {
                "author": "James O'Beirne",
                "date": "2022-02-15T20:53:13",
                "message_text_only": "> The downside is that in a 6 block reorg any transaction that is moved\n> past its expiration date becomes invalid and all its descendants\n> become invalid too.\n\nWorth noting that the transaction sponsors design is no worse an\noffender on this count than, say, CPFP is, provided we adopt the change\nthat sponsored txids are required to be included in the current block\n*or* prior blocks. (The original proposal allowed current block only).\n\nIn other words, the sponsored txids are just \"virtual inputs\" to the\nsponsor transaction.\n\nThis is a much different case than e.g. transaction expiry based on\nwall-clock time or block height, which I agree complicates reorgs\nsignificantly.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220215/a98b30f4/attachment.html>"
            },
            {
                "author": "Jeremy Rubin",
                "date": "2022-02-15T21:37:43",
                "message_text_only": "James,\n\nUnfortunately, there are technical reasons for sponsors to not be monotone.\nMostly that it requires the maintenance of an additional permanent\nTX-Index, making Bitcoin's state grow at a much worse rate. Instead, you\ncould introduce a time-bound for inclusion, e.g. 100 blocks. However, this\ntime-bounded version has the issue that Roconnor raised which is that\nvalidity \"stops\" after a certain time, hurting reorganization.\n\nHowever, If you wanted to map this conceptually onto existing tx indexes,\nyou could have an output with exactly the script `<100 blocks> OP_CSV` and\nthen allow sponsor references to be pruned after that output is \"garbage\ncollected\" by pruning it out of a block. This would be a way that\nsponsorship would be opt-in (must have the flag output) and then sponsors\nobservations of txid existence would be only guaranteed to work for 100\nblocks after which it could be garbage collected by a miner.\n\nIt's not a huge leap to say that this behavior should be made entirely\n\"virtual\", as you are essentially arguing that there exists a transaction\ngraph we could construct that would be equivalent to the graph were we to\nactually have such an output / spends relationship. Since the property we\ncare about is about all graphs, that a specific one could exist that has\nthe same dependency / invalidity relationships during a reorg is important\nfor the theory of bitcoin transaction execution.\n\nSo it really isn't clear to me that we're hurting the transaction graph\nproperties that severely with changes in this family. It's also not clear\nto me that having a TXINDEX is a huge issue given that making a dust-out\nper tx would have the same impact (and people might do it if it's\nfunctionally useful, so just making it default behavior would at least help\nus optimize it to be done through e.g. a separate witness space/utreexo-y\nthing).\n\nAnother consideration is to make the outputs from sponsor txn subject to a\n100 block cool-off period. E.g., so even if you have your inverse timelock,\nadding a constraint that all outputs then have something similar to\nfCoinbase set on them (for spending timelocks only) would mean that little\nreorgs could not disturb the tx graph, although this poses a UX challenge\nfor wallets that aim to bump often (e.g., 1 bump per block would mean you\nneed to maintain 100 outputs).\n\nLastly, it's pretty clear from a UX perspective that I should not want to\npay miners who did *not* mine my transactions! Therefore, it would be\nnatural to see if you pay a high enough fee that users might want to cancel\ntheir (now very desirable) stale fee bumps by replacing it with something\nmore useful to them. So allowing sponsors to be in subsequent blocks might\nmake it rational for users to do more transactions, which increases the\ncosts of such an approach.\n\n\nAll things considered, I favor the simple version of just having sponsors\nonly valid for the block their target is co-resident in.\n\n\nJeremy\n\n\n\n\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n\nOn Tue, Feb 15, 2022 at 12:53 PM James O'Beirne via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> > The downside is that in a 6 block reorg any transaction that is moved\n> > past its expiration date becomes invalid and all its descendants\n> > become invalid too.\n>\n> Worth noting that the transaction sponsors design is no worse an\n> offender on this count than, say, CPFP is, provided we adopt the change\n> that sponsored txids are required to be included in the current block\n> *or* prior blocks. (The original proposal allowed current block only).\n>\n> In other words, the sponsored txids are just \"virtual inputs\" to the\n> sponsor transaction.\n>\n> This is a much different case than e.g. transaction expiry based on\n> wall-clock time or block height, which I agree complicates reorgs\n> significantly.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220215/e10f80ab/attachment-0001.html>"
            },
            {
                "author": "Jeremy Rubin",
                "date": "2022-02-15T21:38:11",
                "message_text_only": "The difference between sponsors and this issue is more subtle. The issue\nSuhas raised was with a variant of sponsors trying to address a second\ncriticism, not sponsors itself, which is secure against this.\n\nI think I can make this clear by defining a few different properties:\n\nStrong Reorgability: The transaction graph can be arbitrarily reorged into\nany series of blocks as long as dependency order/timelocks are respected.\nSimple Existential Reorgability: The transaction graph can be reorged into\na different series of blocks, and it is not computationally difficult to\nfind such an ordering.\nEpsilon-Strong Reorgability: The transaction graph can be arbitrarily\nreorged into any series of blocks as long as dependency order/timelocks are\nrespected, up to Epsilon blocks.\nEpsilon: Simple Existential Reorgability: The transaction graph can be\nreorged into a different series of blocks, and it is not computationally\ndifficult to find such an ordering, up to epsilon blocks.\nPerfect Reorgability: The transaction graph can be reorged into a different\nseries of blocks, but the transactions themselves are already locked in.\n\nPerfect Reorgability doesn't exist in Bitcoin because unconfirmed\ntransactions can be double spent which invalidates descendants. Notably,\nfor a subset of the graph which is CTV Congestion control tree expansions,\nperfect reorg ability would exist, so it's not just a bullshit concept to\nthink about :)\n\nThe sponsors proposal is a change from Epsilon-Strong Reorgability to\nEpsilon-Weak Reorgability. It's not clear to me that there is any\nfunctional reason to rely on Strongness when Bitcoin's reorgability is\nalready not Perfect, so a reorg generator with malicious intent can already\ndisturb the tx graph. Epsion-Weak Reorgability seems to be a sufficient\nproperty.\n\nDo you disagree with that?\n\nBest,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n\nOn Tue, Feb 15, 2022 at 12:25 PM Russell O'Connor via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n>\n>> >> 2. (from Suhas) \"once a valid transaction is created, it should not\n>> become invalid later on unless the inputs are double-spent.\"\n>> > This doesn't seem like a huge concern to me\n>>\n>> I agree that this shouldn't be a concern. In fact, I've asked numerous\n>> people in numerous places what practical downside there is to transactions\n>> that become invalid, and I've heard basically radio silence other than one\n>> off hand remark by satoshi at the dawn of time which didn't seem to me to\n>> have good reasoning. I haven't seen any downside whatsoever of transactions\n>> that can become invalid for anyone waiting the standard 6 confirmations -\n>> the reorg risks only exists for people not waiting for standard\n>> finalization. So I don't think we should consider that aspect of a\n>> sponsorship transaction that can only be mined with the transaction it\n>> sponsors to be a problem unless a specific practical problem case can be\n>> identified. Even if a significant such case was identified, an easy\n>> solution would be to simply allow sponsorship transactions to be mined on\n>> or after the sponsored transaction is mined.\n>>\n>\n> The downside is that in a 6 block reorg any transaction that is moved past\n> its expiration date becomes invalid and all its descendants become invalid\n> too.\n>\n> The current consensus threshold for transactions to become invalid is a\n> 100 block reorg, and I see no reason to change this threshold.  I promise\n> to personally build a wallet that always creates transactions on the verge\n> of becoming invalid should anyone ever implement a feature that violates\n> this tx validity principle.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220215/405f746f/attachment.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-02-16T02:54:28",
                "message_text_only": "@Jeremy\n\n > there are technical reasons for sponsors to not be monotone. Mostly that\nit requires the maintenance of an additional permanent TX-Index, making\nBitcoin's state grow at a much worse rate\n\nWhat do you mean by monotone in the context of sponsor transactions? And when\nyou say tx-index, do you mean an index for looking up a transaction by its\nID? Is that not already something nodes do?\n\n> The sponsors proposal is a change from Epsilon-Strong Reorgability to\nEpsilon-Weak Reorgability\n\nIt doesn't look like you defined that term in your list. Did you mean what\nyou listed as \"Epsilon: Simple Existential Reorgability\"? If so, I would\nsay that should be sufficient. I'm not sure I would even distinguish\nbetween the \"strong\" and \"simple\" versions of these things, tho you could\ntalk about things that make reorgs more or less computationally difficult\non a spectrum. As long as the computational difficulty isn't significant\nfor miners vs their other computational costs, the computation isn't really\na problem.\n\n@Russell\n> The current consensus threshold for transactions to become invalid is a\n100 block reorg\n\nWhat do you mean by this? The only 100 block period I'm aware of is the\ncoinbase cooldown period.\n\n>  I promise to personally build a wallet that always creates transactions\non the verge of becoming invalid should anyone ever implement a feature\nthat violates this tx validity principle.\n\nCould you explain how you would build a wallet like that with a sponsor\ntransaction as described by Jeremy? What damage do you think such a wallet\ncould do? As far as I can tell, such a wallet is very unlikely to do more\ndamage to the network than it does to the user of that wallet.\n\nOn Tue, Feb 15, 2022 at 3:39 PM Jeremy Rubin via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> The difference between sponsors and this issue is more subtle. The issue\n> Suhas raised was with a variant of sponsors trying to address a second\n> criticism, not sponsors itself, which is secure against this.\n>\n> I think I can make this clear by defining a few different properties:\n>\n> Strong Reorgability: The transaction graph can be arbitrarily reorged into\n> any series of blocks as long as dependency order/timelocks are respected.\n> Simple Existential Reorgability: The transaction graph can be reorged into\n> a different series of blocks, and it is not computationally difficult to\n> find such an ordering.\n> Epsilon-Strong Reorgability: The transaction graph can be arbitrarily\n> reorged into any series of blocks as long as dependency order/timelocks are\n> respected, up to Epsilon blocks.\n> Epsilon: Simple Existential Reorgability: The transaction graph can be\n> reorged into a different series of blocks, and it is not computationally\n> difficult to find such an ordering, up to epsilon blocks.\n> Perfect Reorgability: The transaction graph can be reorged into a\n> different series of blocks, but the transactions themselves are already\n> locked in.\n>\n> Perfect Reorgability doesn't exist in Bitcoin because unconfirmed\n> transactions can be double spent which invalidates descendants. Notably,\n> for a subset of the graph which is CTV Congestion control tree expansions,\n> perfect reorg ability would exist, so it's not just a bullshit concept to\n> think about :)\n>\n> The sponsors proposal is a change from Epsilon-Strong Reorgability to\n> Epsilon-Weak Reorgability. It's not clear to me that there is any\n> functional reason to rely on Strongness when Bitcoin's reorgability is\n> already not Perfect, so a reorg generator with malicious intent can already\n> disturb the tx graph. Epsion-Weak Reorgability seems to be a sufficient\n> property.\n>\n> Do you disagree with that?\n>\n> Best,\n>\n> Jeremy\n>\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n>\n> On Tue, Feb 15, 2022 at 12:25 PM Russell O'Connor via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>>\n>>\n>>> >> 2. (from Suhas) \"once a valid transaction is created, it should not\n>>> become invalid later on unless the inputs are double-spent.\"\n>>> > This doesn't seem like a huge concern to me\n>>>\n>>> I agree that this shouldn't be a concern. In fact, I've asked numerous\n>>> people in numerous places what practical downside there is to transactions\n>>> that become invalid, and I've heard basically radio silence other than one\n>>> off hand remark by satoshi at the dawn of time which didn't seem to me to\n>>> have good reasoning. I haven't seen any downside whatsoever of transactions\n>>> that can become invalid for anyone waiting the standard 6 confirmations -\n>>> the reorg risks only exists for people not waiting for standard\n>>> finalization. So I don't think we should consider that aspect of a\n>>> sponsorship transaction that can only be mined with the transaction it\n>>> sponsors to be a problem unless a specific practical problem case can be\n>>> identified. Even if a significant such case was identified, an easy\n>>> solution would be to simply allow sponsorship transactions to be mined on\n>>> or after the sponsored transaction is mined.\n>>>\n>>\n>> The downside is that in a 6 block reorg any transaction that is moved\n>> past its expiration date becomes invalid and all its descendants become\n>> invalid too.\n>>\n>> The current consensus threshold for transactions to become invalid is a\n>> 100 block reorg, and I see no reason to change this threshold.  I promise\n>> to personally build a wallet that always creates transactions on the verge\n>> of becoming invalid should anyone ever implement a feature that violates\n>> this tx validity principle.\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220215/a6789db0/attachment.html>"
            },
            {
                "author": "James O'Beirne",
                "date": "2022-02-16T19:18:47",
                "message_text_only": "> What do you mean by monotone in the context of sponsor transactions?\n\nI take this to mean that the validity of a sponsor txn is\n\"monotonically\" true at any point after the inclusion of the sponsored\ntxn in a block.\n\n> And when you say tx-index, do you mean an index for looking up a\n> transaction by its ID? Is that not already something nodes do?\n\nIndeed, not all nodes have this ability. Each bitcoind node has a map\nof unspent coins which can be referenced by outpoint i.e.(txid, index),\nbut the same isn't true for all historical transactions. I\n(embarrassingly) forgot this in the prior post.\n\nThe map of (txid -> transaction) for all time is a separate index that\nmust be enabled via the `-txindex=1` flag; it isn't enabled by default\nbecause it isn't required for consensus and its growth is unbounded.\n\n> > The current consensus threshold for transactions to become invalid\n> > is a 100 block reorg\n>\n> What do you mean by this? The only 100 block period I'm aware of is\n> the coinbase cooldown period.\n\nIf there were a reorg deeper than 100 blocks, it would permanently\ninvalidate any transactions spending the recently-matured coinbase\nsubsidy in any block between $new_reorg_tip and ($former_tip_height -\n100). These invalidated spends would not be able to be reorganized\ninto a new replacement chain.\n\nHow this differs in practice or principle from a \"regular\" double-spend\nvia reorg I'll leave for another message. I'm not sure that I understand\nthat myself. Personally I think if we hit a >100 block reorg, we've got\nbigger issues than coinbase invalidation.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220216/d93aabca/attachment.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-02-16T20:36:04",
                "message_text_only": ">  the validity of a sponsor txn is \"monotonically\" true at any point after\nthe inclusion of the sponsored txn in a block.\n\nOh I see his point now. If sponsors were valid at any point in the future,\nnot only would a utxo index be needed but an index of all transactions.\nYeah, that wouldn't be good. And the solution of bounding the sponsor\ntransaction to be valid in some window after the transaction is included\ndoesn't solve the original point of making sponsor transactions never\nbecome invalid. Thanks for the clarification James, and good point Jeremy.\n\nOn Wed, Feb 16, 2022 at 1:19 PM James O'Beirne <james.obeirne at gmail.com>\nwrote:\n\n> > What do you mean by monotone in the context of sponsor transactions?\n>\n> I take this to mean that the validity of a sponsor txn is\n> \"monotonically\" true at any point after the inclusion of the sponsored\n> txn in a block.\n>\n> > And when you say tx-index, do you mean an index for looking up a\n> > transaction by its ID? Is that not already something nodes do?\n>\n> Indeed, not all nodes have this ability. Each bitcoind node has a map\n> of unspent coins which can be referenced by outpoint i.e.(txid, index),\n> but the same isn't true for all historical transactions. I\n> (embarrassingly) forgot this in the prior post.\n>\n> The map of (txid -> transaction) for all time is a separate index that\n> must be enabled via the `-txindex=1` flag; it isn't enabled by default\n> because it isn't required for consensus and its growth is unbounded.\n>\n> > > The current consensus threshold for transactions to become invalid\n> > > is a 100 block reorg\n> >\n> > What do you mean by this? The only 100 block period I'm aware of is\n> > the coinbase cooldown period.\n>\n> If there were a reorg deeper than 100 blocks, it would permanently\n> invalidate any transactions spending the recently-matured coinbase\n> subsidy in any block between $new_reorg_tip and ($former_tip_height -\n> 100). These invalidated spends would not be able to be reorganized\n> into a new replacement chain.\n>\n> How this differs in practice or principle from a \"regular\" double-spend\n> via reorg I'll leave for another message. I'm not sure that I understand\n> that myself. Personally I think if we hit a >100 block reorg, we've got\n> bigger issues than coinbase invalidation.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220216/cbe69c55/attachment-0001.html>"
            },
            {
                "author": "Prayank",
                "date": "2022-02-18T00:54:47",
                "message_text_only": "> I suspect the \"economically rational\" choice would be to happily trade off that immediate loss against even a small chance of a simpler policy encouraging higher adoption of bitcoin, _or_ a small chance of more on-chain activity due to higher adoption of bitcoin protocols like lightning and thus a lower chance of an empty mempool in future.\n\nIs this another way of saying a few developers will decide RBF policy for miners and they should follow it because it is the only way bitcoin gets more adoption? On-chain activity is dependent on lot of things. I suspect any change in policy will change it any time soon and miners should have the freedom to decide things that aren't consensus rules.\n\nLightning network contributes to on-chain activity only with opening and closing of channels. Based on the chart I see in the below link for channels opened/closed per block, its contribution is less than 1% in fees:\n\nhttps://txstats.com/dashboard/db/lightning-network?orgId=1&from=now-6M&to=now\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220218/8da73372/attachment.html>"
            },
            {
                "author": "Prayank",
                "date": "2022-02-18T02:08:02",
                "message_text_only": "> If anyone has any indication that there are miners running forks of bitcoind that change this behavior, I'd be curious to know it.\nIt is possible because some mining pools use bitcoind with custom patches.\u00a0\n\nExample: https://twitter.com/0xB10C/status/1461392912600776707 (f2pool)\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220218/00738003/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Thoughts on fee bumping",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Prayank",
                "Antoine Riard",
                "Anthony Towns",
                "darosior",
                "Jeremy Rubin",
                "Russell O'Connor",
                "Matt Corallo",
                "Billy Tetrud",
                "James O'Beirne",
                "Greg Sanders"
            ],
            "messages_count": 25,
            "total_messages_chars_count": 135941
        }
    },
    {
        "title": "[bitcoin-dev] Sponsor transaction engineering, was Re:  Thoughts on fee bumping",
        "thread_messages": [
            {
                "author": "David A. Harding",
                "date": "2022-02-18T21:09:31",
                "message_text_only": "On Tue, Feb 15, 2022 at 01:37:43PM -0800, Jeremy Rubin via bitcoin-dev wrote:\n> Unfortunately, there are technical reasons for sponsors to not be monotone.\n> Mostly that it requires the maintenance of an additional permanent\n> TX-Index\n\nAlternatively, you could allow a miner to include a sponsor transaction\nin a later block than the sponsored transaction by providing an (SPV)\nmerkle inclusion proof that the sponsored transaction was a part of a\nprevious block on the same chain.[1]\n\nThis does raise the vbyte cost of including sponsor and sponsored\ntransactions in different blocks compared to including them both in the\nsame block, but I wonder if it mitigates the validity concern raised by\nSuhas Daftuar in the previous sponsor transaction thread.\n\n-Dave\n\n[1] Bitcoin Core stores the complete headers chain, so it already has\nthe information necessary to validate such a proof (and the\n`verifytxoutproof` RPC already does this).  Utreexo-style nodes might\nnot store old headers to save space, but I presume they could store a\nmerkle-like commitment to all headers they previously validated and then\nhave utreexo proofs include the necessary headers and intermediate\nhashes necessary to validate subsequent-block sponsor transactions.\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220218/106316d2/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "Sponsor transaction engineering, was Re:  Thoughts on fee bumping",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "David A. Harding"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1513
        }
    },
    {
        "title": "[bitcoin-dev] Lightning and other layer 2 projects with multiple RBF policies",
        "thread_messages": [
            {
                "author": "Prayank",
                "date": "2022-02-13T06:09:05",
                "message_text_only": "Hello World,\n\nThere was a discussion about improving fee estimation in Bitcoin Core last year in which 'instagibbs' mentioned that we cannot consider mempool as an orderbook in which which everyone is bidding for block space because nodes can use different relay policies: https://bitcoin-irc.chaincode.com/bitcoin-core-dev/2021-09-22#706294;\n\nAlthough I still don't consider fee rates used in last few blocks relevant for fee estimation, it is possible that we have nodes with different relay policies.\n\nSimilarly if we have different RBF policies being used by nodes in future, how would this affect the security of lightning network implementations and other layer 2 projects? \n\nBased on the things shared by 'aj' in \nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019846.html it is possible for an attacker to use a different RBF policy with some nodes, 10% hash power and affect the security of different projects that rely on default RBF policy in latest Bitcoin Core.\n\nThere was even a CVE in which RBF policy not being documented according to the implementation could affect the security of LN: \nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-May/018893.html\n\n1.Is Lightning Network and a few other layer 2 projects vulnerable to multiple RBF policies being used? \n\n2.With recent discussion to change things in default RBF policy used by Core, will we have multiple versions using different policies? Are users and especially miners incentivized to use different versions and policies? Do they have freedom to use different RBF policy?\n\n3.Are the recent improvements suggested for RBF policy only focused on Lightning Network and its security which will anyway remain same or become worse with multiple RBF policies?\n\nNote: Bitcoin Knots policy is fully configurable, even in the GUI - users can readily choose whatever policy *they* want.\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220213/2e657a89/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Lightning and other layer 2 projects with multiple RBF policies",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Prayank"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2101
        }
    },
    {
        "title": "[bitcoin-dev] [Lightning-dev] Lightning and other layer 2 projects with multiple RBF policies",
        "thread_messages": [
            {
                "author": "Michael Folkson",
                "date": "2022-02-13T15:46:43",
                "message_text_only": "Hi Prayank\n\n> 1.Is Lightning Network and a few other layer 2 projects vulnerable to multiple RBF policies being used?\n\nClearly the security of the Lightning Network and some other Layer 2 projects are at least impacted or partly dependent on policy rules in a way that the base blockchain/network isn't. As I (and others) have said on many occasions ideally this wouldn't be the case but it is best we can do with current designs. I (and others) take the view that this is not a reason to abandon those designs in the absence of an alternative that offers a strictly superior security model. Going back to a model where *all* activity is onchain (or even in less trust minimized protocols than Lightning) doesn't seem like the right approach to me.\n\n> 2.With recent discussion to change things in default RBF policy used by Core, will we have multiple versions using different policies? Are users and especially miners incentivized to use different versions and policies? Do they have freedom to use different RBF policy?\n\nWithout making policy rules effective consensus rules users (including miners) are free to run different policy rules. I think it is too early to say what the final incentives will be to run the same or differing policies. Research into Lightning security is still nascent and we have no idea whether alternative Layer 2 projects will thrive and whether they will have the same or conflicting security considerations to Lightning. I suspect as with defaults generally most users will run whatever the defaults are as they won't care to change them (or even be capable of changing them if they are very non-technical). But users who have a stake in the security of Lightning (or other Layer 2 projects) will clearly want to run whatever policy rules are beneficial to those protocols.\n\nAs you know the vast majority of the full nodes on the network currently run Bitcoin Core. Whether that will change in future and whether this a good thing or not is a whole other discussion. But the reality is that with such strong dominance there is the option to set defaults that are widely used. I think if certain defaults can bolster the security of Lightning (and possibly other Layer 2 projects) at no cost to full node users with no interest in those protocols we should discuss what those defaults should be.\n\n> 3.Are the recent improvements suggested for RBF policy only focused on Lightning Network and its security which will anyway remain same or become worse with multiple RBF policies?\n\nI think by nature of the Lightning Network being the most widely adopted Layer 2 project most of the focus has been on Lightning security. But contributors to other Layer 2 projects are free to flag and discuss security considerations that aren't Lightning specific.\n\n> Note: Bitcoin Knots policy is fully configurable, even in the GUI - users can readily choose whatever policy *they* want.\n\nThe maintainer(s) and contributors to Bitcoin Knots are free to determine what default policy rules they want to implement (and make it easier for users to change those defaults) in the absence of those policy rules being made effective consensus rules. I suspect there would be strong opposition to making some policy rules effective consensus rules but we are now venturing again into future speculation and none of us have a crystal ball. Certainly if you take the view that these policy rules should never be made effective consensus rules then the fact there is at least one implementation taking a contrasting approach to Core is a good thing.\n\n--\nMichael Folkson\nEmail: michaelfolkson at [protonmail.com](http://protonmail.com/)\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n\n------- Original Message -------\nOn Sunday, February 13th, 2022 at 6:09 AM, Prayank via Lightning-dev <lightning-dev at lists.linuxfoundation.org> wrote:\n\n> Hello World,\n>\n> There was a discussion about improving fee estimation in Bitcoin Core last year in which 'instagibbs' mentioned that we cannot consider mempool as an orderbook in which which everyone is bidding for block space because nodes can use different relay policies: https://bitcoin-irc.chaincode.com/bitcoin-core-dev/2021-09-22#706294;\n>\n> Although I still don't consider fee rates used in last few blocks relevant for fee estimation, it is possible that we have nodes with different relay policies.\n>\n> Similarly if we have different RBF policies being used by nodes in future, how would this affect the security of lightning network implementations and other layer 2 projects?\n>\n> Based on the things shared by 'aj' in\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019846.html it is possible for an attacker to use a different RBF policy with some nodes, 10% hash power and affect the security of different projects that rely on default RBF policy in latest Bitcoin Core.\n>\n> There was even a CVE in which RBF policy not being documented according to the implementation could affect the security of LN:\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-May/018893.html\n>\n> 1.Is Lightning Network and a few other layer 2 projects vulnerable to multiple RBF policies being used?\n>\n> 2.With recent discussion to change things in default RBF policy used by Core, will we have multiple versions using different policies? Are users and especially miners incentivized to use different versions and policies? Do they have freedom to use different RBF policy?\n>\n> 3.Are the recent improvements suggested for RBF policy only focused on Lightning Network and its security which will anyway remain same or become worse with multiple RBF policies?\n>\n> Note: Bitcoin Knots policy is fully configurable, even in the GUI - users can readily choose whatever policy *they* want.\n>\n> --\n> Prayank\n>\n> A3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220213/45f656e7/attachment-0001.html>"
            },
            {
                "author": "Prayank",
                "date": "2022-02-14T05:18:30",
                "message_text_only": "> I suspect as with defaults generally most users will run whatever the defaults are as they won't care to change them (or even be capable of changing them if they are very non-technical).\n \n\n30% nodes are using 0.21.1 right now whereas latest version was 22.0 and some are even running lower versions. Different versions in future with defaults might be running RBF v1 and RBF v2.\n> But users who have a stake in the security of Lightning (or other Layer 2 projects) will clearly want to run whatever policy rules are beneficial to those protocols.\n\n\nAgree and attackers will want to run the nodes with policy that helps them exploit bitcoin projects. Miners can run nodes with policy that helps them get more fees.\u00a0\n\n> As you know the vast majority of the full nodes on the network currently run Bitcoin Core. Whether that will change in future and whether this a good thing or not is a whole other discussion. But the reality is that with such strong dominance there is the option to set defaults that are widely used.\n\nBitcoin Core with different versions are used at any point and not sure if this will ever change.\n\nhttps://luke.dashjr.org/programs/bitcoin/files/charts/security.html\n\nhttps://www.shodan.io/search/facet.png?query=User-Agent%3A%2FSatoshi%2F+port%3A%228333%22&facet=product\n> I think if certain defaults can bolster the security of Lightning (and possibly other Layer 2 projects) at no cost to full node users with no interest in those protocols we should discuss what those defaults should be.\n\n\nThis is the assumption which I don't agree with and hence asked some questions in my email. A new RBF policy used by default in Core will not improve the security of projects that are vulnerable to multiple RBF policies or rely on these policies in a way that affects their security.\u00a0\n\nMaybe some experiments on signet might help in knowing more issues associated with multiple RBF policies.\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n\n\n\nFeb 13, 2022, 21:16 by michaelfolkson at protonmail.com:\n\n> Hi Prayank\n>\n> > 1.Is Lightning Network and a few other layer 2 projects vulnerable to multiple RBF policies being used?\n>\n> Clearly the security of the Lightning Network and some other Layer 2 projects are at least impacted or partly dependent on policy rules in a way that the base blockchain/network isn't. As I (and others) have said on many occasions ideally this wouldn't be the case but it is best we can do with current designs. I (and others) take the view that this is not a reason to abandon those designs in the absence of an alternative that offers a strictly superior security model. Going back to a model where *all* activity is onchain (or even in less trust minimized protocols than Lightning) doesn't seem like the right approach to me.\n>\n> > 2.With recent discussion to change things in default RBF policy used by Core, will we have multiple versions using different policies? Are users and especially miners incentivized to use different versions and policies? Do they have freedom to use different RBF policy?\n>\n> Without making policy rules effective consensus rules users (including miners) are free to run different policy rules. I think it is too early to say what the final incentives will be to run the same or differing policies. Research into Lightning security is still nascent and we have no idea whether alternative Layer 2 projects will thrive and whether they will have the same or conflicting security considerations to Lightning. \n>\n> As you know the vast majority of the full nodes on the network currently run Bitcoin Core. Whether that will change in future and whether this a good thing or not is a whole other discussion. But the reality is that with such strong dominance there is the option to set defaults that are widely used. I think if certain defaults can bolster the security of Lightning (and possibly other Layer 2 projects) at no cost to full node users with no interest in those protocols we should discuss what those defaults should be.\n>\n> > 3.Are the recent improvements suggested for RBF policy only focused on Lightning Network and its security which will anyway remain same or become worse with multiple RBF policies?\n>\n> I think by nature of the Lightning Network being the most widely adopted Layer 2 project most of the focus has been on Lightning security. But contributors to other Layer 2 projects are free to flag and discuss security considerations that aren't Lightning specific.\n>\n> > Note: Bitcoin Knots policy is fully configurable, even in the GUI - users can readily choose whatever policy *they* want.\n>\n> The maintainer(s) and contributors to Bitcoin Knots are free to determine what default policy rules they want to implement (and make it easier for users to change those defaults) in the absence of those policy rules being made effective consensus rules. I suspect there would be strong opposition to making some policy rules effective consensus rules but we are now venturing again into future speculation and none of us have a crystal ball. Certainly if you take the view that these policy rules should never be made effective consensus rules then the fact there is at least one implementation taking a contrasting approach to Core is a good thing.\n>\n> --\n> Michael Folkson\n> Email: michaelfolkson at > protonmail.com <http://protonmail.com/>> Keybase: michaelfolkson\n> PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n>\n>\n> ------- Original Message -------\n>  On Sunday, February 13th, 2022 at 6:09 AM, Prayank via Lightning-dev <lightning-dev at lists.linuxfoundation.org> wrote:\n>  \n>\n>> Hello World,\n>>\n>> There was a discussion about improving fee estimation in Bitcoin Core last year in which 'instagibbs' mentioned that we cannot consider mempool as an orderbook in which which everyone is bidding for block space because nodes can use different relay policies: https://bitcoin-irc.chaincode.com/bitcoin-core-dev/2021-09-22#706294;\n>>\n>> Although I still don't consider fee rates used in last few blocks relevant for fee estimation, it is possible that we have nodes with different relay policies.\n>>\n>> Similarly if we have different RBF policies being used by nodes in future, how would this affect the security of lightning network implementations and other layer 2 projects? \n>>\n>> Based on the things shared by 'aj' in \n>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019846.html it is possible for an attacker to use a different RBF policy with some nodes, 10% hash power and affect the security of different projects that rely on default RBF policy in latest Bitcoin Core.\n>>\n>> There was even a CVE in which RBF policy not being documented according to the implementation could affect the security of LN: \n>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-May/018893.html\n>>\n>> 1.Is Lightning Network and a few other layer 2 projects vulnerable to multiple RBF policies being used? \n>>\n>> 2.With recent discussion to change things in default RBF policy used by Core, will we have multiple versions using different policies? Are users and especially miners incentivized to use different versions and policies? Do they have freedom to use different RBF policy?\n>>\n>> 3.Are the recent improvements suggested for RBF policy only focused on Lightning Network and its security which will anyway remain same or become worse with multiple RBF policies?\n>>\n>> Note: Bitcoin Knots policy is fully configurable, even in the GUI - users can readily choose whatever policy *they* want.\n>>\n>> -- \n>> Prayank\n>>\n>> A3B1 E430 2298 178F\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220214/fbe5af9a/attachment-0001.html>"
            },
            {
                "author": "Michael Folkson",
                "date": "2022-02-14T17:02:06",
                "message_text_only": "> This is the assumption which I don't agree with and hence asked some questions in my email. A new RBF policy used by default in Core will not improve the security of projects that are vulnerable to multiple RBF policies or rely on these policies in a way that affects their security.\n\nRight, not immediately. If and when new policy rules are included in a Bitcoin Core release it would take a while before a significant majority of the network were running those new policy rules (barring some kind of urgency, an attacker exploiting a systemic security flaw etc). That's not an argument not to do it though if you take a longer term perspective on building the strongest possible foundation for Lightning or other Layer 2 projects. The security benefit would just be delayed until a significant majority of Bitcoin Core users upgraded to a version including those new policy rules.\n\n> Bitcoin Core with different versions are used at any point and not sure if this will ever change.\n\nSure there will always be some stray full nodes running extremely old versions but the general direction of travel is more and more full nodes upgrading to newer versions. A network where *all* full nodes are running the same policy rules is clearly not an option available to us without making policy rules effective consensus rules and forking/kicking those old versions off the network.\n\n> Maybe some experiments on signet might help in knowing more issues associated with multiple RBF policies.\n\nDefinitely agree. It is a really interesting research area and lots of opportunities for simulations and experiments on the default or custom signet networks. Especially if we fill blocks with auto-generated transactions and/or reduce block sizes and create an artificial fee market.\n\n--\nMichael Folkson\nEmail: michaelfolkson at [protonmail.com](http://protonmail.com/)\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n\n------- Original Message -------\nOn Monday, February 14th, 2022 at 5:18 AM, Prayank <prayank at tutanota.de> wrote:\n\n>> I suspect as with defaults generally most users will run whatever the defaults are as they won't care to change them (or even be capable of changing them if they are very non-technical).\n>\n> 30% nodes are using 0.21.1 right now whereas latest version was 22.0 and some are even running lower versions. Different versions in future with defaults might be running RBF v1 and RBF v2.\n>\n>> But users who have a stake in the security of Lightning (or other Layer 2 projects) will clearly want to run whatever policy rules are beneficial to those protocols.\n>\n> Agree and attackers will want to run the nodes with policy that helps them exploit bitcoin projects. Miners can run nodes with policy that helps them get more fees.\n>\n>> As you know the vast majority of the full nodes on the network currently run Bitcoin Core. Whether that will change in future and whether this a good thing or not is a whole other discussion. But the reality is that with such strong dominance there is the option to set defaults that are widely used.\n>\n> Bitcoin Core with different versions are used at any point and not sure if this will ever change.\n>\n> https://luke.dashjr.org/programs/bitcoin/files/charts/security.html\n>\n> https://www.shodan.io/search/facet.png?query=User-Agent%3A%2FSatoshi%2F+port%3A%228333%22&facet=product\n>\n>> I think if certain defaults can bolster the security of Lightning (and possibly other Layer 2 projects) at no cost to full node users with no interest in those protocols we should discuss what those defaults should be.\n>\n> This is the assumption which I don't agree with and hence asked some questions in my email. A new RBF policy used by default in Core will not improve the security of projects that are vulnerable to multiple RBF policies or rely on these policies in a way that affects their security.\n>\n> Maybe some experiments on signet might help in knowing more issues associated with multiple RBF policies.\n>\n> --\n> Prayank\n>\n> A3B1 E430 2298 178F\n>\n> Feb 13, 2022, 21:16 by michaelfolkson at protonmail.com:\n>\n>> Hi Prayank\n>>\n>>> 1.Is Lightning Network and a few other layer 2 projects vulnerable to multiple RBF policies being used?\n>>\n>> Clearly the security of the Lightning Network and some other Layer 2 projects are at least impacted or partly dependent on policy rules in a way that the base blockchain/network isn't. As I (and others) have said on many occasions ideally this wouldn't be the case but it is best we can do with current designs. I (and others) take the view that this is not a reason to abandon those designs in the absence of an alternative that offers a strictly superior security model. Going back to a model where *all* activity is onchain (or even in less trust minimized protocols than Lightning) doesn't seem like the right approach to me.\n>>\n>>> 2.With recent discussion to change things in default RBF policy used by Core, will we have multiple versions using different policies? Are users and especially miners incentivized to use different versions and policies? Do they have freedom to use different RBF policy?\n>>\n>> Without making policy rules effective consensus rules users (including miners) are free to run different policy rules. I think it is too early to say what the final incentives will be to run the same or differing policies. Research into Lightning security is still nascent and we have no idea whether alternative Layer 2 projects will thrive and whether they will have the same or conflicting security considerations to Lightning.\n>>\n>> As you know the vast majority of the full nodes on the network currently run Bitcoin Core. Whether that will change in future and whether this a good thing or not is a whole other discussion. But the reality is that with such strong dominance there is the option to set defaults that are widely used. I think if certain defaults can bolster the security of Lightning (and possibly other Layer 2 projects) at no cost to full node users with no interest in those protocols we should discuss what those defaults should be.\n>>\n>>> 3.Are the recent improvements suggested for RBF policy only focused on Lightning Network and its security which will anyway remain same or become worse with multiple RBF policies?\n>>\n>> I think by nature of the Lightning Network being the most widely adopted Layer 2 project most of the focus has been on Lightning security. But contributors to other Layer 2 projects are free to flag and discuss security considerations that aren't Lightning specific.\n>>\n>>> Note: Bitcoin Knots policy is fully configurable, even in the GUI - users can readily choose whatever policy *they* want.\n>>\n>> The maintainer(s) and contributors to Bitcoin Knots are free to determine what default policy rules they want to implement (and make it easier for users to change those defaults) in the absence of those policy rules being made effective consensus rules. I suspect there would be strong opposition to making some policy rules effective consensus rules but we are now venturing again into future speculation and none of us have a crystal ball. Certainly if you take the view that these policy rules should never be made effective consensus rules then the fact there is at least one implementation taking a contrasting approach to Core is a good thing.\n>>\n>> --\n>> Michael Folkson\n>> Email: michaelfolkson at [protonmail.com](http://protonmail.com/)\n>> Keybase: michaelfolkson\n>> PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n>>\n>> ------- Original Message -------\n>> On Sunday, February 13th, 2022 at 6:09 AM, Prayank via Lightning-dev <lightning-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> Hello World,\n>>>\n>>> There was a discussion about improving fee estimation in Bitcoin Core last year in which 'instagibbs' mentioned that we cannot consider mempool as an orderbook in which which everyone is bidding for block space because nodes can use different relay policies: https://bitcoin-irc.chaincode.com/bitcoin-core-dev/2021-09-22#706294;\n>>>\n>>> Although I still don't consider fee rates used in last few blocks relevant for fee estimation, it is possible that we have nodes with different relay policies.\n>>>\n>>> Similarly if we have different RBF policies being used by nodes in future, how would this affect the security of lightning network implementations and other layer 2 projects?\n>>>\n>>> Based on the things shared by 'aj' in\n>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019846.html it is possible for an attacker to use a different RBF policy with some nodes, 10% hash power and affect the security of different projects that rely on default RBF policy in latest Bitcoin Core.\n>>>\n>>> There was even a CVE in which RBF policy not being documented according to the implementation could affect the security of LN:\n>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-May/018893.html\n>>>\n>>> 1.Is Lightning Network and a few other layer 2 projects vulnerable to multiple RBF policies being used?\n>>>\n>>> 2.With recent discussion to change things in default RBF policy used by Core, will we have multiple versions using different policies? Are users and especially miners incentivized to use different versions and policies? Do they have freedom to use different RBF policy?\n>>>\n>>> 3.Are the recent improvements suggested for RBF policy only focused on Lightning Network and its security which will anyway remain same or become worse with multiple RBF policies?\n>>>\n>>> Note: Bitcoin Knots policy is fully configurable, even in the GUI - users can readily choose whatever policy *they* want.\n>>>\n>>> --\n>>> Prayank\n>>>\n>>> A3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220214/c2938382/attachment-0001.html>"
            },
            {
                "author": "Prayank",
                "date": "2022-02-14T17:59:37",
                "message_text_only": "> That's not an argument not to do it though if you take a longer term perspective on building the strongest possible foundation for Lightning or other Layer 2 projects. The security benefit would just be delayed until a significant majority of Bitcoin Core users upgraded to a version including those new policy rules.\n\n1.An attacker does not require significant majority for such attacks. \n2.We aren't fixing the things that are broken. We can change the policy in core several times and still not achieve the goal and maybe create new issues.\n\n> A network where *all* full nodes are running the same policy rules is clearly not an option available to us without making policy rules effective consensus rules and forking/kicking those old versions off the network.\n\nA network with a policy already widely used exists right now. \n\n> Definitely agree. It is a really interesting research area and lots of opportunities for simulations and experiments on the default or custom signet networks. Especially if we fill blocks with auto-generated transactions and/or reduce block sizes and create an artificial fee market.\n\nI don't think I can convince everyone to do this however it will be helpful. I will try a few things on regtest and share results if I find anything interesting.\n\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n\n\n\nFeb 14, 2022, 22:32 by michaelfolkson at protonmail.com:\n\n> > This is the assumption which I don't agree with and hence asked some questions in my email. A new RBF policy used by default in Core will not improve the security of projects that are vulnerable to multiple RBF policies or rely on these policies in a way that affects their security.\u00a0\n>\n> Right, not immediately. If and when new policy rules are included in a Bitcoin Core release it would take a while before a significant majority of the network were running those new policy rules (barring some kind of urgency, an attacker exploiting a systemic security flaw etc). That's not an argument not to do it though if you take a longer term perspective on building the strongest possible foundation for Lightning or other Layer 2 projects. The security benefit would just be delayed until a significant majority of Bitcoin Core users upgraded to a version including those new policy rules.\n>\n> >\u00a0> Bitcoin Core with different versions are used at any point and not sure if this will ever change.\n>\n> Sure there will always be some stray full nodes running extremely old versions but the general direction of travel is more and more full nodes upgrading to newer versions. A network where *all* full nodes are running the same policy rules is clearly not an option available to us without making policy rules effective consensus rules and forking/kicking those old versions off the network.\n>\n> >\u00a0> Maybe some experiments on signet might help in knowing more issues associated with multiple RBF policies.\n>\n> Definitely agree. It is a really interesting research area and lots of opportunities for simulations and experiments on the default or custom signet networks. Especially if we fill blocks with auto-generated transactions and/or reduce block sizes and create an artificial fee market.\n>\n> --\n> Michael Folkson\n> Email: michaelfolkson at > protonmail.com <http://protonmail.com/>> Keybase: michaelfolkson\n> PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n>\n>\n>\n> ------- Original Message -------\n>  On Monday, February 14th, 2022 at 5:18 AM, Prayank <prayank at tutanota.de> wrote:\n>  \n>\n>> > I suspect as with defaults generally most users will run whatever the defaults are as they won't care to change them (or even be capable of changing them if they are very non-technical).\n>>\n>>\n>> 30% nodes are using 0.21.1 right now whereas latest version was 22.0 and some are even running lower versions. Different versions in future with defaults might be running RBF v1 and RBF v2.\n>>\n>> > But users who have a stake in the security of Lightning (or other Layer 2 projects) will clearly want to run whatever policy rules are beneficial to those protocols.\n>>\n>>\n>> Agree and attackers will want to run the nodes with policy that helps them exploit bitcoin projects. Miners can run nodes with policy that helps them get more fees.\u00a0\n>>\n>> > As you know the vast majority of the full nodes on the network currently run Bitcoin Core. Whether that will change in future and whether this a good thing or not is a whole other discussion. But the reality is that with such strong dominance there is the option to set defaults that are widely used.\n>>\n>>\n>> Bitcoin Core with different versions are used at any point and not sure if this will ever change.\n>>\n>> https://luke.dashjr.org/programs/bitcoin/files/charts/security.html\n>>\n>> https://www.shodan.io/search/facet.png?query=User-Agent%3A%2FSatoshi%2F+port%3A%228333%22&facet=product\n>>\n>> > I think if certain defaults can bolster the security of Lightning (and possibly other Layer 2 projects) at no cost to full node users with no interest in those protocols we should discuss what those defaults should be.\n>>\n>>\n>> This is the assumption which I don't agree with and hence asked some questions in my email. A new RBF policy used by default in Core will not improve the security of projects that are vulnerable to multiple RBF policies or rely on these policies in a way that affects their security.\u00a0\n>>\n>> Maybe some experiments on signet might help in knowing more issues associated with multiple RBF policies.\n>>\n>> -- \n>> Prayank\n>>\n>> A3B1 E430 2298 178F\n>>\n>>\n>>\n>> Feb 13, 2022, 21:16 by michaelfolkson at protonmail.com:\n>>\n>>> Hi Prayank\n>>>\n>>> > 1.Is Lightning Network and a few other layer 2 projects vulnerable to multiple RBF policies being used?\n>>>\n>>> Clearly the security of the Lightning Network and some other Layer 2 projects are at least impacted or partly dependent on policy rules in a way that the base blockchain/network isn't. As I (and others) have said on many occasions ideally this wouldn't be the case but it is best we can do with current designs. I (and others) take the view that this is not a reason to abandon those designs in the absence of an alternative that offers a strictly superior security model. Going back to a model where *all* activity is onchain (or even in less trust minimized protocols than Lightning) doesn't seem like the right approach to me.\n>>>\n>>> > 2.With recent discussion to change things in default RBF policy used by Core, will we have multiple versions using different policies? Are users and especially miners incentivized to use different versions and policies? Do they have freedom to use different RBF policy?\n>>>\n>>> Without making policy rules effective consensus rules users (including miners) are free to run different policy rules. I think it is too early to say what the final incentives will be to run the same or differing policies. Research into Lightning security is still nascent and we have no idea whether alternative Layer 2 projects will thrive and whether they will have the same or conflicting security considerations to Lightning. \n>>>\n>>> As you know the vast majority of the full nodes on the network currently run Bitcoin Core. Whether that will change in future and whether this a good thing or not is a whole other discussion. But the reality is that with such strong dominance there is the option to set defaults that are widely used. I think if certain defaults can bolster the security of Lightning (and possibly other Layer 2 projects) at no cost to full node users with no interest in those protocols we should discuss what those defaults should be.\n>>>\n>>> > 3.Are the recent improvements suggested for RBF policy only focused on Lightning Network and its security which will anyway remain same or become worse with multiple RBF policies?\n>>>\n>>> I think by nature of the Lightning Network being the most widely adopted Layer 2 project most of the focus has been on Lightning security. But contributors to other Layer 2 projects are free to flag and discuss security considerations that aren't Lightning specific.\n>>>\n>>> > Note: Bitcoin Knots policy is fully configurable, even in the GUI - users can readily choose whatever policy *they* want.\n>>>\n>>> The maintainer(s) and contributors to Bitcoin Knots are free to determine what default policy rules they want to implement (and make it easier for users to change those defaults) in the absence of those policy rules being made effective consensus rules. I suspect there would be strong opposition to making some policy rules effective consensus rules but we are now venturing again into future speculation and none of us have a crystal ball. Certainly if you take the view that these policy rules should never be made effective consensus rules then the fact there is at least one implementation taking a contrasting approach to Core is a good thing.\n>>>\n>>> --\n>>> Michael Folkson\n>>> Email: michaelfolkson at >>> protonmail.com <http://protonmail.com/>>>> Keybase: michaelfolkson\n>>> PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n>>>\n>>>\n>>> ------- Original Message -------\n>>> On Sunday, February 13th, 2022 at 6:09 AM, Prayank via Lightning-dev <lightning-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>>\n>>>> Hello World,\n>>>>\n>>>> There was a discussion about improving fee estimation in Bitcoin Core last year in which 'instagibbs' mentioned that we cannot consider mempool as an orderbook in which which everyone is bidding for block space because nodes can use different relay policies: https://bitcoin-irc.chaincode.com/bitcoin-core-dev/2021-09-22#706294;\n>>>>\n>>>> Although I still don't consider fee rates used in last few blocks relevant for fee estimation, it is possible that we have nodes with different relay policies.\n>>>>\n>>>> Similarly if we have different RBF policies being used by nodes in future, how would this affect the security of lightning network implementations and other layer 2 projects? \n>>>>\n>>>> Based on the things shared by 'aj' in \n>>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019846.html it is possible for an attacker to use a different RBF policy with some nodes, 10% hash power and affect the security of different projects that rely on default RBF policy in latest Bitcoin Core.\n>>>>\n>>>> There was even a CVE in which RBF policy not being documented according to the implementation could affect the security of LN: \n>>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-May/018893.html\n>>>>\n>>>> 1.Is Lightning Network and a few other layer 2 projects vulnerable to multiple RBF policies being used? \n>>>>\n>>>> 2.With recent discussion to change things in default RBF policy used by Core, will we have multiple versions using different policies? Are users and especially miners incentivized to use different versions and policies? Do they have freedom to use different RBF policy?\n>>>>\n>>>> 3.Are the recent improvements suggested for RBF policy only focused on Lightning Network and its security which will anyway remain same or become worse with multiple RBF policies?\n>>>>\n>>>> Note: Bitcoin Knots policy is fully configurable, even in the GUI - users can readily choose whatever policy *they* want.\n>>>>\n>>>> -- \n>>>> Prayank\n>>>>\n>>>> A3B1 E430 2298 178F\n>>>>\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220214/efb7c1ba/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Lightning and other layer 2 projects with multiple RBF policies",
            "categories": [
                "bitcoin-dev",
                "Lightning-dev"
            ],
            "authors": [
                "Prayank",
                "Michael Folkson"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 35096
        }
    },
    {
        "title": "[bitcoin-dev] CTV Signet Parameters",
        "thread_messages": [
            {
                "author": "Jeremy Rubin",
                "date": "2022-02-17T21:58:38",
                "message_text_only": "Hi devs,\n\nI have been running a CTV signet for around a year and it's seen little\nuse. Early on I had some issues syncing new nodes, but I have verified\nsyncability to this signet using\nhttps://github.com/JeremyRubin/bitcoin/tree/checktemplateverify-signet-23.0-alpha.\nPlease use this signet!\n\n```\n[signet]\nsignetchallenge=512102946e8ba8eca597194e7ed90377d9bbebc5d17a9609ab3e35e706612ee882759351ae\naddnode=50.18.75.225\n```\n\nThis should be operational. Let me know if there are any issues you\nexperience (likely with signet itself, but CTV too).\n\nFeel free to also email me an address and I can send you some signet coins\n-- if anyone is interested in running an automatic faucet I would love help\nwith that and will send you a lot of coins.\n\nAJ Wrote (in another thread):\n\n>  I'd much rather see some real\n>   third-party experimentation *somewhere* public first, and Jeremy's CTV\n>   signet being completely empty seems like a bad sign to me. Maybe that\n>   means we should tentatively merge the feature and deploy it on the\n>   default global signet though?  Not really sure how best to get more\n>   real world testing; but \"deploy first, test later\" doesn't sit right.\n\nI agree that real experimentation would be great, and think that merging\nthe code (w/o activation) for signet would likely help users v.s. custom\nbuilds/parameters.\n\nI am unsure that \"learning in public\" is required -- personally I do\nexperiments on regtest regularly and on mainnet (using emulators) more\noccasionally. I think some of the difficulty is that for setting up signet\nstuff you need to wait e.g. 10 minutes for blocks and stuff, source faucet\ncoins, etc. V.s. regtest you can make tests that run automatically. Maybe\nseeing more regtest RPC test samples for regtests would be a sufficient\nin-between?\n\n\nBest,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220217/74f1db31/attachment-0001.html>"
            },
            {
                "author": "0x0ff",
                "date": "2022-02-18T11:13:31",
                "message_text_only": "Good day,\n\nI've setup the explorer for CTV Signet which is now up and running at [https://explorer.ctvsignet.com](https://explorer.ctvsignet.com/)\n\nBest,\n[@0x0ff](https://twitter.com/0x0ff_)\n\n------- Original Message -------\nOn Thursday, February 17th, 2022 at 9:58 PM, Jeremy Rubin via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi devs,\n>\n> I have been running a CTV signet for around a year and it's seen little use. Early on I had some issues syncing new nodes, but I have verified syncability to this signet using https://github.com/JeremyRubin/bitcoin/tree/checktemplateverify-signet-23.0-alpha. Please use this signet!\n>\n> ```\n> [signet]\n> signetchallenge=512102946e8ba8eca597194e7ed90377d9bbebc5d17a9609ab3e35e706612ee882759351ae\n> addnode=50.18.75.225\n> ```\n>\n> This should be operational. Let me know if there are any issues you experience (likely with signet itself, but CTV too).\n>\n> Feel free to also email me an address and I can send you some signet coins -- if anyone is interested in running an automatic faucet I would love help with that and will send you a lot of coins.\n>\n> AJ Wrote (in another thread):\n>\n>> I'd much rather see some real> third-party experimentation *somewhere* public first, and Jeremy's CTV\n>> signet being completely empty seems like a bad sign to me. Maybe that\n>> means we should tentatively merge the feature and deploy it on the\n>> default global signet though? Not really sure how best to get more\n>> real world testing; but \"deploy first, test later\" doesn't sit right.\n>\n> I agree that real experimentation would be great, and think that merging the code (w/o activation) for signet would likely help users v.s. custom builds/parameters.\n>\n> I am unsure that \"learning in public\" is required -- personally I do experiments on regtest regularly and on mainnet (using emulators) more occasionally. I think some of the difficulty is that for setting up signet stuff you need to wait e.g. 10 minutes for blocks and stuff, source faucet coins, etc. V.s. regtest you can make tests that run automatically. Maybe seeing more regtest RPC test samples for regtests would be a sufficient in-between?\n>\n> Best,\n>\n> Jeremy\n>\n> --\n> [@JeremyRubin](https://twitter.com/JeremyRubin)\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220218/e09ee432/attachment-0001.html>"
            },
            {
                "author": "Jeremy Rubin",
                "date": "2022-02-22T03:19:23",
                "message_text_only": "There's also now a faucet:\n\nhttps://faucet.ctvsignet.com\n\nthanks 0x0ff!\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n\n\nOn Fri, Feb 18, 2022 at 3:13 AM 0x0ff <0x0ff at onsats.org> wrote:\n\n> Good day,\n>\n> I've setup the explorer for CTV Signet which is now up and running at\n> https://explorer.ctvsignet.com\n>\n> Best,\n> @0x0ff <https://twitter.com/0x0ff_>\n>\n> ------- Original Message -------\n> On Thursday, February 17th, 2022 at 9:58 PM, Jeremy Rubin via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> Hi devs,\n>\n> I have been running a CTV signet for around a year and it's seen little\n> use. Early on I had some issues syncing new nodes, but I have verified\n> syncability to this signet using\n> https://github.com/JeremyRubin/bitcoin/tree/checktemplateverify-signet-23.0-alpha.\n> Please use this signet!\n>\n> ```\n> [signet]\n>\n> signetchallenge=512102946e8ba8eca597194e7ed90377d9bbebc5d17a9609ab3e35e706612ee882759351ae\n> addnode=50.18.75.225\n> ```\n>\n> This should be operational. Let me know if there are any issues you\n> experience (likely with signet itself, but CTV too).\n>\n> Feel free to also email me an address and I can send you some signet coins\n> -- if anyone is interested in running an automatic faucet I would love help\n> with that and will send you a lot of coins.\n>\n> AJ Wrote (in another thread):\n>\n> > I'd much rather see some real\n> > third-party experimentation *somewhere* public first, and Jeremy's CTV\n> > signet being completely empty seems like a bad sign to me. Maybe that\n> > means we should tentatively merge the feature and deploy it on the\n> > default global signet though? Not really sure how best to get more\n> > real world testing; but \"deploy first, test later\" doesn't sit right.\n>\n> I agree that real experimentation would be great, and think that merging\n> the code (w/o activation) for signet would likely help users v.s. custom\n> builds/parameters.\n>\n> I am unsure that \"learning in public\" is required -- personally I do\n> experiments on regtest regularly and on mainnet (using emulators) more\n> occasionally. I think some of the difficulty is that for setting up signet\n> stuff you need to wait e.g. 10 minutes for blocks and stuff, source faucet\n> coins, etc. V.s. regtest you can make tests that run automatically. Maybe\n> seeing more regtest RPC test samples for regtests would be a sufficient\n> in-between?\n>\n>\n> Best,\n>\n> Jeremy\n>\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220221/e60231fc/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "CTV Signet Parameters",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "0x0ff",
                "Jeremy Rubin"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 7112
        }
    },
    {
        "title": "[bitcoin-dev] `OP_EVICT`: An Alternative to `OP_TAPLEAFUPDATEVERIFY`",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-18T02:45:23",
                "message_text_only": "`OP_EVICT`: An Alternative to `OP_TAPLEAFUPDATEVERIFY`\n======================================================\n\nIn late 2021, `aj` proposed `OP_TAPLEAFUPDATEVERIFY` in order to\nimplement CoinPools and similar constructions.\n\n`Jeremy` observed that due to the use of Merkle tree paths, an\n`OP_TLUV` would require O(log N) hash revelations in order to\nreach a particular tapleaf, which, in the case of a CoinPool,\nwould then delete itself after spending only a particular amount\nof funds.\nHe then observed that `OP_CTV` trees also require a similar\nrevelation of O(log N) transactions, but with the advantage that\nonce revealed, the transactions can then be reused, thus overall\nthe expectation is that the number of total bytes onchain is\nlesser compared to `OP_TLUV`.\n\nAfter some thinking, I realized that it was the use of the\nMerkle tree to represent the promised-but-offchain outputs of\nthe CoinPool that lead to the O(log N) space usage.\nI then started thinking of alternative representations of\nsets of promised outputs, which would not require O(log N)\nrevelations by avoiding the tree structure.\n\nPromised Outputs\n----------------\n\nFundamentally, we can consider that a solution for scaling\nBitcoin would be to *promise* that some output *can* appear\nonchain at some point in the future, without requiring that the\noutput be shown onchain *right now*.\nThen, we can perform transactional cut-through on spends of the\npromised outputs, without requiring onchain activity (\"offchain\").\nOnly if something Really Bad (TM) happens do we need to actually\ndrop the latest set of promised outputs onchain, where it has to\nbe verified globally by all fullnodes (and would thus incur scaling\nand privacy costs).\n\nAs an example of the above paradigm, consider the Lightning\nNetwork.\nOutputs representing the money of each party in a channel are\npromised, and *can* appear onchain (via the unilateral close\nmechanism).\nIn the meantime, there is a mechanism for performing cut-through,\nallowing transfers between channel participants; any number of\ntransactions can be performed that are only \"solidified\" later,\nwithout expensive onchain activity.\n\nThus:\n\n* A CoinPool is really a way to commit to promised outputs.\n  To change the distribution of those promised outputs, the\n  CoinPool operators need to post an onchain transaction, but\n  that is only a 1-input-1-output transaction, and with Schnorr\n  signatures the single input requires only a single signature.\n  But in case something Really Bad (TM) happens, any participant\n  can unilaterally close the CoinPool, instantiating the promised\n  outputs.\n* A statechain is really just a CoinPool hosted inside a\n  Decker-Wattenhofer or Decker-Russell-Osuntokun construction.\n  This allows changing the distribution of those promised outputs\n  without using an onchain transaction --- instead, a new state\n  in the Decker-Wattenhofer/Decker-Russell-Osuntokun construction\n  is created containing the new state, which invalidates all older\n  states.\n  Again, any participant can unilaterally shut it down, exposing\n  the state of the inner CoinPool.\n* A channel factory is really just a statechain where the\n  promised outputs are not simple 1-of-1 single-owner outputs,\n  but are rather 2-of-2 channels.\n  This allows graceful degradation, where even if the statechain\n  (\"factory\") layer has missing participants, individual 2-of-2\n  channels can still continue operating as long as they do not\n  involve missing participants, without requiring all participants\n  to be online for large numbers of transactions.\n\nWe can then consider that the base CoinPool usage should be enough,\nas other mechanisms (`OP_CTV`+`OP_CSFS`, `SIGHASH_NOINPUT`) can be\nused to implement statechains and channels and channel factories.\n\nI therefore conclude that what we really need is \"just\" a way to\ncommit ourselves to exposing a set of promised outputs, with the\nproviso that if we all agree, we can change that set (without\nrequiring that the current or next set be exposed, for both\nscaling and privacy).\n\n(To Bitcoin Cashers: this is not an IOU, this is *committed* and\ncan be enforced onchain, that is enough to threaten your offchain\ncounterparties into behaving correctly.\nThey cannot gain anything by denying the outputs they promised,\nyou can always drop it onchain and have it enforced, thus it is\nnot just merely an IOU, as IOUs are not necessarily enforceable,\nbut this mechanism *would* be.\nBlockchain as judge+jury+executioner, not noisy marketplace.)\n\nImportantly: both `OP_CTV` and `OP_TLUV` force the user to\ndecide on a particular, but ultimately arbitrary, ordering for\npromised outputs.\nIn principle, a set of promised outputs, if the owners of those\noutputs are peers, does not have *any* inherent order.\nThus, I started to think about a commitment scheme that does not\nimpose any ordering during commitment.\n\nDigression: N-of-N With Eviction\n--------------------------------\n\nAn issue with using an N-of-N construction is that if any single\nparticipant is offline, the construction cannot advance its state.\n\nThis has lead to some peopple proposing to instead use K-of-N\nonce N reaches much larger than 2 participants for CoinPools/statechains/\nchannel factories.\n\nHowever, even so, K-of-N still requires that K participants remain\nonline, and the level K is a security parameter.\nIf less than K participants are online, then the construction\n*still* cannot advance its state.\n\nWorse, because K < N, a single participant can have its funds\noutright stolen by a quorum of K participants.\nThere is no way to prove that the other participants in the same\nconstruction are not really sockpuppets of the same real-world\nentity, thus it is entirely possible that the K quorum is actually\njust a single participant that is now capable of stealing the\nfunds of all the other participants.\nThe only way to avoid this is to use N-oF-N: N-of-N requires\n*your* keys, thus the coins are *your* coins.\nIn short: K-of-N, as it allows the state to be updated without your\nkeys (on the excuse that \"if you are offline, we need to be able to\nupdate state\"), is *not your keys not your coins*.\n\nK-of-N should really only be used if all N are your sockpuppets,\nand you want to HODL your funds.\nThis is the difference between consensus \"everyone must agree\" and\nvoting \"enough sockpuppets can be used to overpower you\".\n\nWith `OP_TLUV`, however, it is possible to create an \"N-of-N With\nEviction\" construction.\nWhen a participant in the N-of-N is offline, but the remaining\nparticipants want to advance the state of the construction, they\ninstead evict the offline participant, creating a smaller N-of-N\nwhere *all* participants are online, and continue operating.\n\nThis avoids the *not your keys not your coins* problem of K-of-N\nconstructions, while simultaneously providing a way to advance\nthe state without the full participant set being online.\n\nThe only real problem with `OP_TLUV` is that it takes O(log N)\nhash revelations to evict one participant, and each evicted\nparticipant requires one separate transaction.\n\nK-of-N has the \"advantage\" that even if you are offline, the state\ncan be advanced without evicting you.\nHowever, as noted, as the coins can be spent without your keys,\nthe coins are not your coins, thus this advantage may be considered\ndubious --- whether you are online or offline, a quorum of K can\noutright steal your coins.\nEviction here requires that your coins be returned to your control.\n\nCommitting To An Unordered Set\n------------------------------\n\nIn an N-of-N CoinPool/statechain/channel factory, the ownership\nof a single onchain UTXO is shared among N participants.\nThat is, there are a number of promised outputs, not exposed\nonchain, which the N participants agree on as the \"real\" current\nstate of the construction,\nHowever, the N participants can also agree to change the current\nstate of the construction, if all of them sign off on the change.\n\nEach of the promised outputs has a value, and the sum of all\npromised values is the value of the onchain UTXO.\nInterestingly, each of the promised outputs also has an SECP256K1\npoint that can be used as a public key, and the sum of all\npromised points is the point of the onchain UTXO.\n\nThus, the onchain UTXO can serve as a commitment to the sum of\nthe promised outputs.\nThe problem is committing to each of the individual promised\noutputs.\n\nWe can observe that a digital signature not only proves knowledge\nof a private key, it also commits to a particular message.\nThus, we can make each participant sign their own expected\npromised output, and share the signature for their promised\noutput.\n\nWhen a participant is to be evicted, the other participants\ntake the signature for the promised output of the to-be-evicted\nparticipant, and show it onchain, to attest to the output.\nThen, the onchain mechanism should then allow the rest of the\nfunds to be controlled by the N-of-N set minus the evicted\nparticipant.\n\n`OP_EVICT`\n----------\n\nWith all that, let me now propose the `OP_EVICT` opcode.\n\n`OP_EVICT` accepts a variable number of arguments.\n\n* The stack top is either the constant `1`, or an SECP256K1\n  point.\n  * If it is `1` that simply means \"use the Taproot internal\n    pubkey\", as is usual for `OP_CHECKSIG`.\n* The next stack item is a number, equal to the number of\n  outputs that were promised, and which will now be evicted.\n* The next stack items will alternate:\n  * A number indicating an output index.\n  * A signature for that output.\n  * Output indices must not be duplicated, and indicated\n    outputs must be SegWit v1 (\"Taproot\") outputs.\n    The public key of the output will be taken as the public\n    key for the corresponding signature, and the signature\n    only covers the output itself (i.e. value and\n    `scriptPubKey`).\n    This means the signature has no `SIGHASH`.\n  * As the signature covers the public key, this prevents\n    malleation of a signature using one public key to a\n    signature for another public key.\n* After that is another signature.\n  * This signature is checked using `OP_CHECKSIG` semantics\n    (including `SIGHASH` support).\n  * The public key is the input point (i.e. stack top)\n    **MINUS** all the public keys of the indicated outputs.\n\nAs a concrete example, suppose A, B, C, and D want to make a\nCoinPool (or offchain variant of such) with the following\ninitial state:\n\n* A := 10\n* B := 6\n* C := 4\n* D := 22\n\nLet us assume that A, B, C, and D have generated public\nkeys in such a way to avoid key cancellation (e.g.\nprecommitment, or the MuSig scheme).\n\nThe participants then generate promised outputs for the\nabove, and each of them shares signatures for the promised\noutputs:\n\n* sign(a, \"A := 10\")\n* sign(b, \"B := 6\")\n* sign(c, \"C := 4\")\n* sign(d, \"D := 22\")\n\nOnce that is done, they generate:\n\n* Q = A + B + C + D\n* P = h(Q|`<1> OP_EVICT`) * Q\n\nThen they spend their funds, creating a Taproot output:\n\n* P := 42\n\nIf all participants are online, they can move funds between\neach other (or to other addresses) by cooperatively signing\nusing the point P, and the magic of Taproot means that use\nof `OP_EVICT` is not visible.\n\nSuppose however that B is offline.\nThen A, C, and D then decide to evict B.\nTo do so, they create a transaction that has an output\nwith \"B := 6\", and they reveal the `OP_EVICT` Tapscript\nas well as sign(b, \"B := 6\").\nThis lets them change state and spend their funds without\nB being online.\nAnd B remains secure, as they cannot evict B except using\nthe pre-signed output, which B certifies as their expected\npromised output.\n\nNote that the opcode as described above allows for multiple\nevictions in the same transaction.\nIf B and C are offline, then the remaining participants\nsimply need to expose multiple outputs in the same\ntransaction.\n\nSecurity\n--------\n\nI am not a cryptographer.\nThus, the security of this scheme is a conjecture.\n\nAs long as key cancellation is protected against, it should\nbe secure.\nThe combined fund cannot be spent except if all participants\nagree.\nA smaller online participant set can be created only if a\nparticipant is evicted, and eviction will force the owned\nfunds of the evicted participant to be instantiated.\nThe other participants cannot synthesize an alternate\nsignature signing a different value without knowledge of the\nprivkey of the evicted participant.\n\nTo prevent signature replay, each update of an updateable\nscheme like CoinPool et al should use a different pubkey\nfor each participant for each state.\nAs the signature covers the pubkey, it should be safe to\nuse a non-hardened derivation scheme so that only a single\nroot privkey is needed.\n\nAdditional Discussion\n---------------------\n\n### Eviction Scheme\n\nWe can consider that the eviction scheme proposed here is the\nfollowing contract:\n\n* Either all of us agree on some transfer, OR,\n* Give me my funds and the rest of you can all go play with\n  your funds however you want.\n\nThe signature that commits to a promised output is then the\nagreement that the particular participant believes they are\nentitled to a particular amount.\n\nWe can consider that a participant can re-sign their output\nwith a different amount, but that is why `OP_EVICT` requires\nthe *other* participants to cooperatively sign as well.\nIf the other participants cooperatively sign, they effectively\nagree to the participant re-signing for a different amount,\nand thus actually covered by \"all of us agree\".\n\n### Pure SCRIPT Contracts\n\nA \"pure SCRIPT contract\" is a Taproot contract where the\nkeyspend path is not desired, and the contract is composed of\nTapscript branches.\n\nIn such a case, the expected technique would be for the\ncontract participants to agree on a NUMS point where none\nof the participants can know the scalar (private key) behind\nthe point, and to use that as the internal Taproot pubkey\n`Q`.\nFor complete protocols, the NUMS point can be a protocol-defined\nconstant.\n\nAs the `OP_EVICT` opcode requires that each promised output\nbe signed, on the face of it, this technique cannot be used\nfor `OP_EVICT`-promised outputs, as it is impossible to sign\nusing the NUMS point.\n\nHowever, we should note that the requirement of a \"pure SCRIPT\"\ncontract is that none of the participants can unilaterally\nsign an alternate spend.\nUsing an N-of-N of the participants as the Taproot internal\npubkey is sufficient to ensure this.\n\nAs a concrete example: suppose we want an HTLC, which has a\nhashlock branch requiring participant A, and a timelock branch\nrequiring participant B.\nSuch a simple scheme would not require that both A and B be\nable to cooperatively spend the output, thus we might have\npreferred the technique of using a NUMS point as Taproot\ninternal pubkey.\nBut using a NUMS point would not allow any signature, even the\n`OP_EVICT`-required signatures-of-promised-outputs.\n\nInstead of using a NUMS point for the Taproot internal pubkey,\nwe can use the sum of `A[tmp] + B[tmp]` (suitably protected\nagainst key cancellation).\nThen both A and B can cooperatively sign the promised output,\nand keep the promised output in an `OP_EVICT`-enforced UTXO.\nAfter creating the signature for the promised output, A and B\ncan ensure that the keypath branch cannot be used by securely\ndeleting the private keys for `A[tmp]` and `B[tmp]`\nrespectively.\n\n### Signature Half-Aggregation\n\nIt is possible to batch-validate, and as `OP_EVICT` must\nvalidate at least two signatures (an eviction and the\nsignature of the remaining) it makes sense to use batch\nvalidation for `OP_EVICT`.\n\nOf note is that Schnorr signatures allow for third-party\nhalf-aggregation, where the `s` components of multiple\nsignatures are summed together, but the `R` components\nare not.\n\n(Warning: I am not aware of any security proofs that\nhalf-aggregation is actually **safe**!\nIn particular, BIP-340 does not define half-aggregation,\nand its batch validation algorithm is not, to my naivete,\nextensible to half-aggregation.)\n\nBasically, if we are batch validating two signatures\n`(R[0], s[0])`, `(R[1], s[1])` of two messages `m[0]`\nand `m[1]` signed by two keys `A[0]` and `A[1]`, we\nwould do:\n\n* For `i = 0, 1`: `e[i] = h(R[i]|m[i])`\n* Check: `(s[0] + s[1]) * G` is equal to `R[0] + e[0] * A[0] + R[1] + e[1] * A[1]`.\n\nAs we can see, the `s` can be summed before being\nposted on the blockchain, as validators do not need\nindividual `s[i]`.\nHowever, `R` cannot be summed as each one needs to be\nhashed.\n\nThis half-aggregation is third-party, i.e. someone\nwithout any knowledge of any private keys can simply\nsum the `s` components of multiple signatures.\n\nAs `OP_EVICT` always validates at least two signatures,\nusing half-aggregation can remove at least 32 weight\nunits, and each additional promised output being evicted\nis another signature whose `s` can be added to the sum.\nOf course, **that depends on half-aggregation being\nsecure**.\n\n### Relationship to Other Opcodes\n\n`OP_CTV` does other things than this opcode, and cannot\nbe used as a direct alternative.\nIn particular while `OP_CTV` *can* commit to a set of\npromised outputs, if a promised output needs to be\npublished, the remaining funds are now distributed over a\nset of UTXOs.\nThus, \"reviving\" the CoinPool (or offchain variant thereof)\nrequires consuming multiple UTXOs, and the consumption of\nmultiple UTXOs is risky unless specifically designd for it.\n(In particular, if the UTXOs have different signer sets,\none signer set can initially cooperate to revive the\nCoinPool, then spend their UTXO to a different transaction,\nwhich if confirmed will invalidate the revival transaction.)\n\nThis opcode seems largely in direct competitiong with\n`OP_TLUV`, with largely the same design goal.\nIts advantage is reduced number of eviction transactions,\nas multiple evictions, plus the revival of the CoinPool,\ncan be put in a single transaction.\nIt has the disadvantage relative to `OP_TLUV` of requiring\npoint operations.\nI have not explored completely, but my instinct suggests\nthat `OP_TLUV` use may require at least one signature\nvalidation anyway.\n\nIt may be possible to implement `OP_EVICT` in terms of\n`OP_TX`/`OP_TXHASH`, `OP_CSFS`, and a point-subtraction\noperation.\nHowever, `OP_EVICT` allows for the trivial implementation\nof batch validation (and, if half-aggregation is safe, to\nuse half-aggregation instead), whereas we expect multiple\n`OP_CSFS` to be needed to implement this, without any\npossibility of batch validation.\nIt may be possible to design an `OP_CSFS` variant that\nallows batch validation, such as by extending the virtual\nmachine with an accumulator for pending signature\nvalidations."
            },
            {
                "author": "Erik Aronesty",
                "date": "2022-02-18T13:53:09",
                "message_text_only": "hey, i read that whole thing, but i'm confused as to why it's necessary\n\nseems like N of N participants can pre-sign an on-chain transfer of funds\nfor each participant to a new address that consists of (N-1) or (N-1)\nparticipants, of which each portion of the signature is encrypted for the\nsame (N-1) participants\n\nthen any (N-1) subset of participants can collude publish that transaction\nat any time to remove any other member from the pool\n\nall of the set up  (dkg for N-1), and transfer (encryption of partial sigs)\nis done offchain, and online with the participants that are online\n\n\n\nOn Thu, Feb 17, 2022 at 9:45 PM ZmnSCPxj via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> `OP_EVICT`: An Alternative to `OP_TAPLEAFUPDATEVERIFY`\n> ======================================================\n>\n> In late 2021, `aj` proposed `OP_TAPLEAFUPDATEVERIFY` in order to\n> implement CoinPools and similar constructions.\n>\n> `Jeremy` observed that due to the use of Merkle tree paths, an\n> `OP_TLUV` would require O(log N) hash revelations in order to\n> reach a particular tapleaf, which, in the case of a CoinPool,\n> would then delete itself after spending only a particular amount\n> of funds.\n> He then observed that `OP_CTV` trees also require a similar\n> revelation of O(log N) transactions, but with the advantage that\n> once revealed, the transactions can then be reused, thus overall\n> the expectation is that the number of total bytes onchain is\n> lesser compared to `OP_TLUV`.\n>\n> After some thinking, I realized that it was the use of the\n> Merkle tree to represent the promised-but-offchain outputs of\n> the CoinPool that lead to the O(log N) space usage.\n> I then started thinking of alternative representations of\n> sets of promised outputs, which would not require O(log N)\n> revelations by avoiding the tree structure.\n>\n> Promised Outputs\n> ----------------\n>\n> Fundamentally, we can consider that a solution for scaling\n> Bitcoin would be to *promise* that some output *can* appear\n> onchain at some point in the future, without requiring that the\n> output be shown onchain *right now*.\n> Then, we can perform transactional cut-through on spends of the\n> promised outputs, without requiring onchain activity (\"offchain\").\n> Only if something Really Bad (TM) happens do we need to actually\n> drop the latest set of promised outputs onchain, where it has to\n> be verified globally by all fullnodes (and would thus incur scaling\n> and privacy costs).\n>\n> As an example of the above paradigm, consider the Lightning\n> Network.\n> Outputs representing the money of each party in a channel are\n> promised, and *can* appear onchain (via the unilateral close\n> mechanism).\n> In the meantime, there is a mechanism for performing cut-through,\n> allowing transfers between channel participants; any number of\n> transactions can be performed that are only \"solidified\" later,\n> without expensive onchain activity.\n>\n> Thus:\n>\n> * A CoinPool is really a way to commit to promised outputs.\n>   To change the distribution of those promised outputs, the\n>   CoinPool operators need to post an onchain transaction, but\n>   that is only a 1-input-1-output transaction, and with Schnorr\n>   signatures the single input requires only a single signature.\n>   But in case something Really Bad (TM) happens, any participant\n>   can unilaterally close the CoinPool, instantiating the promised\n>   outputs.\n> * A statechain is really just a CoinPool hosted inside a\n>   Decker-Wattenhofer or Decker-Russell-Osuntokun construction.\n>   This allows changing the distribution of those promised outputs\n>   without using an onchain transaction --- instead, a new state\n>   in the Decker-Wattenhofer/Decker-Russell-Osuntokun construction\n>   is created containing the new state, which invalidates all older\n>   states.\n>   Again, any participant can unilaterally shut it down, exposing\n>   the state of the inner CoinPool.\n> * A channel factory is really just a statechain where the\n>   promised outputs are not simple 1-of-1 single-owner outputs,\n>   but are rather 2-of-2 channels.\n>   This allows graceful degradation, where even if the statechain\n>   (\"factory\") layer has missing participants, individual 2-of-2\n>   channels can still continue operating as long as they do not\n>   involve missing participants, without requiring all participants\n>   to be online for large numbers of transactions.\n>\n> We can then consider that the base CoinPool usage should be enough,\n> as other mechanisms (`OP_CTV`+`OP_CSFS`, `SIGHASH_NOINPUT`) can be\n> used to implement statechains and channels and channel factories.\n>\n> I therefore conclude that what we really need is \"just\" a way to\n> commit ourselves to exposing a set of promised outputs, with the\n> proviso that if we all agree, we can change that set (without\n> requiring that the current or next set be exposed, for both\n> scaling and privacy).\n>\n> (To Bitcoin Cashers: this is not an IOU, this is *committed* and\n> can be enforced onchain, that is enough to threaten your offchain\n> counterparties into behaving correctly.\n> They cannot gain anything by denying the outputs they promised,\n> you can always drop it onchain and have it enforced, thus it is\n> not just merely an IOU, as IOUs are not necessarily enforceable,\n> but this mechanism *would* be.\n> Blockchain as judge+jury+executioner, not noisy marketplace.)\n>\n> Importantly: both `OP_CTV` and `OP_TLUV` force the user to\n> decide on a particular, but ultimately arbitrary, ordering for\n> promised outputs.\n> In principle, a set of promised outputs, if the owners of those\n> outputs are peers, does not have *any* inherent order.\n> Thus, I started to think about a commitment scheme that does not\n> impose any ordering during commitment.\n>\n> Digression: N-of-N With Eviction\n> --------------------------------\n>\n> An issue with using an N-of-N construction is that if any single\n> participant is offline, the construction cannot advance its state.\n>\n> This has lead to some peopple proposing to instead use K-of-N\n> once N reaches much larger than 2 participants for CoinPools/statechains/\n> channel factories.\n>\n> However, even so, K-of-N still requires that K participants remain\n> online, and the level K is a security parameter.\n> If less than K participants are online, then the construction\n> *still* cannot advance its state.\n>\n> Worse, because K < N, a single participant can have its funds\n> outright stolen by a quorum of K participants.\n> There is no way to prove that the other participants in the same\n> construction are not really sockpuppets of the same real-world\n> entity, thus it is entirely possible that the K quorum is actually\n> just a single participant that is now capable of stealing the\n> funds of all the other participants.\n> The only way to avoid this is to use N-oF-N: N-of-N requires\n> *your* keys, thus the coins are *your* coins.\n> In short: K-of-N, as it allows the state to be updated without your\n> keys (on the excuse that \"if you are offline, we need to be able to\n> update state\"), is *not your keys not your coins*.\n>\n> K-of-N should really only be used if all N are your sockpuppets,\n> and you want to HODL your funds.\n> This is the difference between consensus \"everyone must agree\" and\n> voting \"enough sockpuppets can be used to overpower you\".\n>\n> With `OP_TLUV`, however, it is possible to create an \"N-of-N With\n> Eviction\" construction.\n> When a participant in the N-of-N is offline, but the remaining\n> participants want to advance the state of the construction, they\n> instead evict the offline participant, creating a smaller N-of-N\n> where *all* participants are online, and continue operating.\n>\n> This avoids the *not your keys not your coins* problem of K-of-N\n> constructions, while simultaneously providing a way to advance\n> the state without the full participant set being online.\n>\n> The only real problem with `OP_TLUV` is that it takes O(log N)\n> hash revelations to evict one participant, and each evicted\n> participant requires one separate transaction.\n>\n> K-of-N has the \"advantage\" that even if you are offline, the state\n> can be advanced without evicting you.\n> However, as noted, as the coins can be spent without your keys,\n> the coins are not your coins, thus this advantage may be considered\n> dubious --- whether you are online or offline, a quorum of K can\n> outright steal your coins.\n> Eviction here requires that your coins be returned to your control.\n>\n> Committing To An Unordered Set\n> ------------------------------\n>\n> In an N-of-N CoinPool/statechain/channel factory, the ownership\n> of a single onchain UTXO is shared among N participants.\n> That is, there are a number of promised outputs, not exposed\n> onchain, which the N participants agree on as the \"real\" current\n> state of the construction,\n> However, the N participants can also agree to change the current\n> state of the construction, if all of them sign off on the change.\n>\n> Each of the promised outputs has a value, and the sum of all\n> promised values is the value of the onchain UTXO.\n> Interestingly, each of the promised outputs also has an SECP256K1\n> point that can be used as a public key, and the sum of all\n> promised points is the point of the onchain UTXO.\n>\n> Thus, the onchain UTXO can serve as a commitment to the sum of\n> the promised outputs.\n> The problem is committing to each of the individual promised\n> outputs.\n>\n> We can observe that a digital signature not only proves knowledge\n> of a private key, it also commits to a particular message.\n> Thus, we can make each participant sign their own expected\n> promised output, and share the signature for their promised\n> output.\n>\n> When a participant is to be evicted, the other participants\n> take the signature for the promised output of the to-be-evicted\n> participant, and show it onchain, to attest to the output.\n> Then, the onchain mechanism should then allow the rest of the\n> funds to be controlled by the N-of-N set minus the evicted\n> participant.\n>\n> `OP_EVICT`\n> ----------\n>\n> With all that, let me now propose the `OP_EVICT` opcode.\n>\n> `OP_EVICT` accepts a variable number of arguments.\n>\n> * The stack top is either the constant `1`, or an SECP256K1\n>   point.\n>   * If it is `1` that simply means \"use the Taproot internal\n>     pubkey\", as is usual for `OP_CHECKSIG`.\n> * The next stack item is a number, equal to the number of\n>   outputs that were promised, and which will now be evicted.\n> * The next stack items will alternate:\n>   * A number indicating an output index.\n>   * A signature for that output.\n>   * Output indices must not be duplicated, and indicated\n>     outputs must be SegWit v1 (\"Taproot\") outputs.\n>     The public key of the output will be taken as the public\n>     key for the corresponding signature, and the signature\n>     only covers the output itself (i.e. value and\n>     `scriptPubKey`).\n>     This means the signature has no `SIGHASH`.\n>   * As the signature covers the public key, this prevents\n>     malleation of a signature using one public key to a\n>     signature for another public key.\n> * After that is another signature.\n>   * This signature is checked using `OP_CHECKSIG` semantics\n>     (including `SIGHASH` support).\n>   * The public key is the input point (i.e. stack top)\n>     **MINUS** all the public keys of the indicated outputs.\n>\n> As a concrete example, suppose A, B, C, and D want to make a\n> CoinPool (or offchain variant of such) with the following\n> initial state:\n>\n> * A := 10\n> * B := 6\n> * C := 4\n> * D := 22\n>\n> Let us assume that A, B, C, and D have generated public\n> keys in such a way to avoid key cancellation (e.g.\n> precommitment, or the MuSig scheme).\n>\n> The participants then generate promised outputs for the\n> above, and each of them shares signatures for the promised\n> outputs:\n>\n> * sign(a, \"A := 10\")\n> * sign(b, \"B := 6\")\n> * sign(c, \"C := 4\")\n> * sign(d, \"D := 22\")\n>\n> Once that is done, they generate:\n>\n> * Q = A + B + C + D\n> * P = h(Q|`<1> OP_EVICT`) * Q\n>\n> Then they spend their funds, creating a Taproot output:\n>\n> * P := 42\n>\n> If all participants are online, they can move funds between\n> each other (or to other addresses) by cooperatively signing\n> using the point P, and the magic of Taproot means that use\n> of `OP_EVICT` is not visible.\n>\n> Suppose however that B is offline.\n> Then A, C, and D then decide to evict B.\n> To do so, they create a transaction that has an output\n> with \"B := 6\", and they reveal the `OP_EVICT` Tapscript\n> as well as sign(b, \"B := 6\").\n> This lets them change state and spend their funds without\n> B being online.\n> And B remains secure, as they cannot evict B except using\n> the pre-signed output, which B certifies as their expected\n> promised output.\n>\n> Note that the opcode as described above allows for multiple\n> evictions in the same transaction.\n> If B and C are offline, then the remaining participants\n> simply need to expose multiple outputs in the same\n> transaction.\n>\n> Security\n> --------\n>\n> I am not a cryptographer.\n> Thus, the security of this scheme is a conjecture.\n>\n> As long as key cancellation is protected against, it should\n> be secure.\n> The combined fund cannot be spent except if all participants\n> agree.\n> A smaller online participant set can be created only if a\n> participant is evicted, and eviction will force the owned\n> funds of the evicted participant to be instantiated.\n> The other participants cannot synthesize an alternate\n> signature signing a different value without knowledge of the\n> privkey of the evicted participant.\n>\n> To prevent signature replay, each update of an updateable\n> scheme like CoinPool et al should use a different pubkey\n> for each participant for each state.\n> As the signature covers the pubkey, it should be safe to\n> use a non-hardened derivation scheme so that only a single\n> root privkey is needed.\n>\n> Additional Discussion\n> ---------------------\n>\n> ### Eviction Scheme\n>\n> We can consider that the eviction scheme proposed here is the\n> following contract:\n>\n> * Either all of us agree on some transfer, OR,\n> * Give me my funds and the rest of you can all go play with\n>   your funds however you want.\n>\n> The signature that commits to a promised output is then the\n> agreement that the particular participant believes they are\n> entitled to a particular amount.\n>\n> We can consider that a participant can re-sign their output\n> with a different amount, but that is why `OP_EVICT` requires\n> the *other* participants to cooperatively sign as well.\n> If the other participants cooperatively sign, they effectively\n> agree to the participant re-signing for a different amount,\n> and thus actually covered by \"all of us agree\".\n>\n> ### Pure SCRIPT Contracts\n>\n> A \"pure SCRIPT contract\" is a Taproot contract where the\n> keyspend path is not desired, and the contract is composed of\n> Tapscript branches.\n>\n> In such a case, the expected technique would be for the\n> contract participants to agree on a NUMS point where none\n> of the participants can know the scalar (private key) behind\n> the point, and to use that as the internal Taproot pubkey\n> `Q`.\n> For complete protocols, the NUMS point can be a protocol-defined\n> constant.\n>\n> As the `OP_EVICT` opcode requires that each promised output\n> be signed, on the face of it, this technique cannot be used\n> for `OP_EVICT`-promised outputs, as it is impossible to sign\n> using the NUMS point.\n>\n> However, we should note that the requirement of a \"pure SCRIPT\"\n> contract is that none of the participants can unilaterally\n> sign an alternate spend.\n> Using an N-of-N of the participants as the Taproot internal\n> pubkey is sufficient to ensure this.\n>\n> As a concrete example: suppose we want an HTLC, which has a\n> hashlock branch requiring participant A, and a timelock branch\n> requiring participant B.\n> Such a simple scheme would not require that both A and B be\n> able to cooperatively spend the output, thus we might have\n> preferred the technique of using a NUMS point as Taproot\n> internal pubkey.\n> But using a NUMS point would not allow any signature, even the\n> `OP_EVICT`-required signatures-of-promised-outputs.\n>\n> Instead of using a NUMS point for the Taproot internal pubkey,\n> we can use the sum of `A[tmp] + B[tmp]` (suitably protected\n> against key cancellation).\n> Then both A and B can cooperatively sign the promised output,\n> and keep the promised output in an `OP_EVICT`-enforced UTXO.\n> After creating the signature for the promised output, A and B\n> can ensure that the keypath branch cannot be used by securely\n> deleting the private keys for `A[tmp]` and `B[tmp]`\n> respectively.\n>\n> ### Signature Half-Aggregation\n>\n> It is possible to batch-validate, and as `OP_EVICT` must\n> validate at least two signatures (an eviction and the\n> signature of the remaining) it makes sense to use batch\n> validation for `OP_EVICT`.\n>\n> Of note is that Schnorr signatures allow for third-party\n> half-aggregation, where the `s` components of multiple\n> signatures are summed together, but the `R` components\n> are not.\n>\n> (Warning: I am not aware of any security proofs that\n> half-aggregation is actually **safe**!\n> In particular, BIP-340 does not define half-aggregation,\n> and its batch validation algorithm is not, to my naivete,\n> extensible to half-aggregation.)\n>\n> Basically, if we are batch validating two signatures\n> `(R[0], s[0])`, `(R[1], s[1])` of two messages `m[0]`\n> and `m[1]` signed by two keys `A[0]` and `A[1]`, we\n> would do:\n>\n> * For `i = 0, 1`: `e[i] = h(R[i]|m[i])`\n> * Check: `(s[0] + s[1]) * G` is equal to `R[0] + e[0] * A[0] + R[1] + e[1]\n> * A[1]`.\n>\n> As we can see, the `s` can be summed before being\n> posted on the blockchain, as validators do not need\n> individual `s[i]`.\n> However, `R` cannot be summed as each one needs to be\n> hashed.\n>\n> This half-aggregation is third-party, i.e. someone\n> without any knowledge of any private keys can simply\n> sum the `s` components of multiple signatures.\n>\n> As `OP_EVICT` always validates at least two signatures,\n> using half-aggregation can remove at least 32 weight\n> units, and each additional promised output being evicted\n> is another signature whose `s` can be added to the sum.\n> Of course, **that depends on half-aggregation being\n> secure**.\n>\n> ### Relationship to Other Opcodes\n>\n> `OP_CTV` does other things than this opcode, and cannot\n> be used as a direct alternative.\n> In particular while `OP_CTV` *can* commit to a set of\n> promised outputs, if a promised output needs to be\n> published, the remaining funds are now distributed over a\n> set of UTXOs.\n> Thus, \"reviving\" the CoinPool (or offchain variant thereof)\n> requires consuming multiple UTXOs, and the consumption of\n> multiple UTXOs is risky unless specifically designd for it.\n> (In particular, if the UTXOs have different signer sets,\n> one signer set can initially cooperate to revive the\n> CoinPool, then spend their UTXO to a different transaction,\n> which if confirmed will invalidate the revival transaction.)\n>\n> This opcode seems largely in direct competitiong with\n> `OP_TLUV`, with largely the same design goal.\n> Its advantage is reduced number of eviction transactions,\n> as multiple evictions, plus the revival of the CoinPool,\n> can be put in a single transaction.\n> It has the disadvantage relative to `OP_TLUV` of requiring\n> point operations.\n> I have not explored completely, but my instinct suggests\n> that `OP_TLUV` use may require at least one signature\n> validation anyway.\n>\n> It may be possible to implement `OP_EVICT` in terms of\n> `OP_TX`/`OP_TXHASH`, `OP_CSFS`, and a point-subtraction\n> operation.\n> However, `OP_EVICT` allows for the trivial implementation\n> of batch validation (and, if half-aggregation is safe, to\n> use half-aggregation instead), whereas we expect multiple\n> `OP_CSFS` to be needed to implement this, without any\n> possibility of batch validation.\n> It may be possible to design an `OP_CSFS` variant that\n> allows batch validation, such as by extending the virtual\n> machine with an accumulator for pending signature\n> validations.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220218/32e791c1/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-18T14:48:38",
                "message_text_only": "Good morning Erik,\n\n> hey, i read that whole thing, but i'm confused as to why it's necessary\n>\n> seems like N of N participants can pre-sign an on-chain transfer of funds for each participant to a new address that consists of (N-1) or (N-1) participants, of which each portion of the signature is encrypted for the same (N-1) participants\n>\n> then any (N-1) subset of participants can collude publish that transaction at any time to remove any other member from\u00a0the pool\n>\n> all of the set up\u00a0 (dkg for N-1), and transfer (encryption of partial sigs) is done offchain, and online with the participants\u00a0that are online\n\n\nAs I understand your counterproposal, it would require publishing one transaction per evicted participant.\nIn addition, each participant has to store `N!` possible orderings in which participants can be evicted, as you cannot predict the future and cannot predict which partiicpants will go offline first.\n\nFinally, please see also the other thread on lightning-dev: https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-February/003479.html\nIn this thread, I point out that if we ever use channel factories, it would be best if we treat each channel as a 2-of-2 that participates in an overall N-of-N (i.e. the N in the outer channel factory is composed of 2-of-2).\nFor example, instead of the channel factory being signed by participants `A`, `B`, `C`, `D`, instead the channel factory is signed by `AB`, `AC`, `AD`, `BC`, `BD`, `CD`, so that if e.g. participant B needs to be evicted, we can evict the signers `AB`, `BC`, and `BD`.\nThis means that for the channel factory case, already the number of \"participants\" is quadratic on the number of *actual* participants, which greatly increases the number of transactions that need to be evicted in one-eviction-at-a-time schemes (which is how I understand your proposal) as well as increasing the `N!` number of signatures that need to be exchanged during setup.\n\n\nBut yes, certainly that can work, just as pre-signed transactions can be used instead of `OP_CTV` or pretty much any non-`OP_CHECKMULTISIG` opcode, xref Smart Contracts Unchained.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Erik Aronesty",
                "date": "2022-02-18T15:50:02",
                "message_text_only": "> As I understand your counterproposal, it would require publishing one\ntransaction per evicted participant.\n\nif you also pre-sign (N-2, N-3, etc), you can avoid this\n\n> In addition, each participant has to store `N!` possible orderings in\nwhich participants can be evicted, as you cannot predict the future and\ncannot predict which partiicpants will go offline first.\n\nwhy would the ordering matter?  these are unordered pre commitments to move\nfunds, right?   you agree post the one that represents \"everyone that's\noffline\"\n\n> But yes, certainly that can work, just as pre-signed transactions can be\nused instead of `OP_CTV`\n\ni don't see how multiple users can securely share a channel (allowing\nmassive additional scaling with lighting) without op_ctv\n\n\nOn Fri, Feb 18, 2022 at 9:48 AM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Erik,\n>\n> > hey, i read that whole thing, but i'm confused as to why it's necessary\n> >\n> > seems like N of N participants can pre-sign an on-chain transfer of\n> funds for each participant to a new address that consists of (N-1) or (N-1)\n> participants, of which each portion of the signature is encrypted for the\n> same (N-1) participants\n> >\n> > then any (N-1) subset of participants can collude publish that\n> transaction at any time to remove any other member from the pool\n> >\n> > all of the set up  (dkg for N-1), and transfer (encryption of partial\n> sigs) is done offchain, and online with the participants that are online\n>\n>\n> As I understand your counterproposal, it would require publishing one\n> transaction per evicted participant.\n> In addition, each participant has to store `N!` possible orderings in\n> which participants can be evicted, as you cannot predict the future and\n> cannot predict which partiicpants will go offline first.\n>\n> Finally, please see also the other thread on lightning-dev:\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-February/003479.html\n> In this thread, I point out that if we ever use channel factories, it\n> would be best if we treat each channel as a 2-of-2 that participates in an\n> overall N-of-N (i.e. the N in the outer channel factory is composed of\n> 2-of-2).\n> For example, instead of the channel factory being signed by participants\n> `A`, `B`, `C`, `D`, instead the channel factory is signed by `AB`, `AC`,\n> `AD`, `BC`, `BD`, `CD`, so that if e.g. participant B needs to be evicted,\n> we can evict the signers `AB`, `BC`, and `BD`.\n> This means that for the channel factory case, already the number of\n> \"participants\" is quadratic on the number of *actual* participants, which\n> greatly increases the number of transactions that need to be evicted in\n> one-eviction-at-a-time schemes (which is how I understand your proposal) as\n> well as increasing the `N!` number of signatures that need to be exchanged\n> during setup.\n>\n>\n> But yes, certainly that can work, just as pre-signed transactions can be\n> used instead of `OP_CTV` or pretty much any non-`OP_CHECKMULTISIG` opcode,\n> xref Smart Contracts Unchained.\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220218/6b0a0376/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-18T16:06:39",
                "message_text_only": "Good morning Erik,\n\n> > As I understand your counterproposal, it would require publishing one transaction per evicted participant.\n>\n> if you also pre-sign (N-2, N-3, etc), you can avoid this\n\nIt also increases the combinatorial explosion.\n\n> > In addition, each participant has to store `N!` possible orderings in which participants can be evicted, as you cannot predict the future and cannot predict which partiicpants will go offline first.\n>\n> why would\u00a0the ordering matter?\u00a0 these are unordered pre commitments to move funds, right?\u00a0 \u00a0you agree post the one that represents \"everyone that's offline\"\n\nSuppose `B` is offline first, then the remaining `A` `C` and `D` publish the eviction transaction that evicts only `B`.\nWhat happens if `C` then goes offline?\nWe need to prepare for that case (and other cases where the participants go offline at arbitrary orders) and pre-sign a spend from the `ACD` set and evicts `C` as well, increasing combinatorial explosion.\nAnd so on.\n\nWe *could* use multiple Tapleaves, of the form `<A> OP_CHECKSIG <BCD> OP_CHECKSIG` for each participant.\nThen the per-participant `<A>` signature is signed with `SIGHASH_SINGLE|SIGHASH_ANYONECANPAY` and is pre-signed, while the remainder is signed by `<BCD>` with default `SIGHASH_ALL`.\nThen if one participant `B` is offline they can evict `B` and then the change is put into a new UTXO with a similar pre-signed scheme `<A> OP_CHECKSIG <CD> OP_CHECKSIG`.\nThis technique precludes pre-signing multiple evictions.\n\n>\n> > But yes, certainly that can work, just as pre-signed transactions can be used instead of `OP_CTV`\u00a0\n>\n> i don't see how multiple users can securely share a channel (allowing massive additional scaling with lighting) without op_ctv\n\nThey can, they just pre-sign, like you pointed out.\nThe same technique works --- `OP_CTV` just avoids having ridiculous amounts of combinatorial explosion and just requires `O(log n)` per eviction.\nRemember, this proposal can be used for channel factories just as well, as pointed out, so any objection to this proposal also applies to `OP_CTV`.\n\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Jonas Nick",
                "date": "2022-02-18T13:55:31",
                "message_text_only": "On the topic of half aggregation, Chalkias et al. gave a convincing security\nproof last year:\nhttps://eprint.iacr.org/2021/350\n\nAs an aside, half aggregation is not exactly the scheme in the OP because that\none is insecure. This does not affect Zmn's conclusion and was already\npointed out in the original half aggregation thread:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-May/014306.html\n\nIt is required that each of the \"s\"-values are multiplied with a different\nunpredictable value, for example like this:\nhttps://github.com/ElementsProject/cross-input-aggregation/blob/master/slides/2021-Q2-halfagg-impl.org#schnorr-signature-half-aggregation-1"
            },
            {
                "author": "Antoine Riard",
                "date": "2022-02-18T18:09:07",
                "message_text_only": "Hi Zeeman,\n\n> After some thinking, I realized that it was the use of the\n> Merkle tree to represent the promised-but-offchain outputs of\n> the CoinPool that lead to the O(log N) space usage.\n> I then started thinking of alternative representations of\n> sets of promised outputs, which would not require O(log N)\n> revelations by avoiding the tree structure.\n\nIn the context of payment pools, I think the O(log N) revelations can be\navoided already today by pre-signing all the combinations of\npromised-but-offchain outputs publications order. However, this approach\npresents a factorial complexity and appears as an intractable problem for\nhigh-number of pool users.\n\nI think this factorial complexity issue is the primary problem to enable\nscalable payment pools. This issue appears to be solvable by introducing an\naccumulator at the script interpreter level. IMO, the efficiency of the\naccumulated set representations comes as a second-order issue.\n\nIn the comparison of different covenant primitives, I believe we should ask\nfirst if the flexibility offered is enough to solve the factorial\ncomplexity. I would say performance trade-offs analysis can only be\nconducted in logically equivalent primitives.\n\n> A statechain is really just a CoinPool hosted inside a\n>  Decker-Wattenhofer or Decker-Russell-Osuntokun construction.\n\nNote, to the best of my knowledge, how to use LN-Penalty in the context of\nmulti-party construction is still an unsolved issue. If an invalidated\nstate is published on-chain, how do you guarantee that the punished output\nvalue is distributed \"fairly\" among the \"honest\" set of users ? At least\nwhere fairness is defined as a reasonable proportion of the balances they\nowned in the latest state.\n\n> (To Bitcoin Cashers: this is not an IOU, this is *committed* and\n> can be enforced onchain, that is enough to threaten your offchain\n> counterparties into behaving correctly.\n> They cannot gain anything by denying the outputs they promised,\n> you can always drop it onchain and have it enforced, thus it is\n> not just merely an IOU, as IOUs are not necessarily enforceable,\n> but this mechanism *would* be.\n> Blockchain as judge+jury+executioner, not noisy marketplace.)\n\nTo be fair towards the Bitcoin Cashers, I think there are still limitations\nof LN, we have not solved yet. Especially, w.r.t to mass exits from the\noff-chain layers to the chain, where the blocks would stay fulfilled longer\nthan the standard HTLC timelocks, at  a fee price point that the average\nuser can't buy... I'm not sure if we have outlawed the \"bank runs\" scenario\nyet of LN.\n\nI would say yes the Blockchain is a juge authority, but in the worst-case\nwe might be all in market competition to get enforcement.\n\n> In principle, a set of promised outputs, if the owners of those\n> outputs are peers, does not have *any* inherent order.\n> Thus, I started to think about a commitment scheme that does not\n> impose any ordering during commitment.\n\nI think we should dissociate a) *outputs publication ordering* from the b)\n*spends paths ordering* itself. Even if to each spend path a output\npublication is attached, the ordering constraint might not present the same\ncomplexity.\n\nUnder this distinction, are you sure that TLUV imposes an ordering on the\noutput publication ?\n\n> With `OP_TLUV`, however, it is possible to create an \"N-of-N With\n> Eviction\" construction.\n> When a participant in the N-of-N is offline, but the remaining\n> participants want to advance the state of the construction, they\n> instead evict the offline participant, creating a smaller N-of-N\n> where *all* participants are online, and continue operating.\n\nI think we should dissociate two types of pool spends : a) eviction by the\npool unanimity in case of irresponsive participants and b) unilateral\nwithdrawal by a participant because of the liquidity allocation policy. I\nthink the distinction is worthy, as the pool participant should be stable\nand the eviction not abused.\n\nI'm not sure if TLUV enables b), at least without transforming the\nunilateral withdrawal into an eviction. To ensure the TLUV operation is\ncorrect  (spent leaf is removed, withdrawing participant point removed,\netc), the script content must be inspected by *all* the participant.\nHowever, I believe\nknowledge of this content effectively allows you to play it out against the\npool at any time ? It's likely solvable at the price of a CHECKSIG.\n\n`OP_EVICT`\n----------\n\n>  * If it is `1` that simply means \"use the Taproot internal\n>    pubkey\", as is usual for `OP_CHECKSIG`.\n\nIIUC, this assumes the deployment of BIP118, where if the  public key is a\nsingle byte 0x01, the internal pubkey is used\nfor verification.\n\n>  * Output indices must not be duplicated, and indicated\n>    outputs must be SegWit v1 (\"Taproot\") outputs.\n\nI think public key duplication must not be verified. If a duplicated public\nkey is present, the point is subtracted twice from the internal pubkey and\ntherefore the aggregated\nkey remains unknown ? So it sounds to me safe against replay attacks.\n\n>  * The public key is the input point (i.e. stack top)\n>    **MINUS** all the public keys of the indicated outputs.\n\nCan you prevent eviction abuse where one counterparty threatens to evict\neveryone as all the output signatures are known among participants and free\nto sum ? (at least not considering fees)\n\n> Suppose however that B is offline.\n> Then A, C, and D then decide to evict B.\n> To do so, they create a transaction that has an output\n> with \"B := 6\", and they reveal the `OP_EVICT` Tapscript\n> as well as sign(b, \"B := 6\").\n> This lets them change state and spend their funds without\n> B being online.\n> And B remains secure, as they cannot evict B except using\n> the pre-signed output, which B certifies as their expected\n> promised output.\n\nI think in the context of (off-chain) payment pool, OP_EVICT requires\nparticipant cooperation *after* the state update to allow a single\nparticipant to withdraw her funds.\n\nI believe this is unsafe if we retain as an off-chain construction security\nrequirement that a participant should have the unilateral means to enforce\nthe latest agreed upon state at any time during the construction lifetime.\n\nI would say an OP_EVICT construction could solve the issue where the pool\nparticipants exchange pre-signatures of the internal pubkey with the\nwithdrawing participant point removed. However, I believe such fix would a)\nblock promised outputs batching (or at least in a pre-committed way like\nradix pools) and b) be grieved by the factorial complexity described above.\n\n> The combined fund cannot be spent except if all participants\n> agree.\n\nIf all participants agree minus the evicted ones, correct ? The output\npromises signatures are shared at state setup, therefore no additional\ncontribution from the evicted participant (I think).\n\n> To prevent signature replay, each update of an updateable\n> scheme like CoinPool et al should use a different pubkey\n> for each participant for each state.\n\nI'm not even sure if it's required with OP_EVICT, as the publication of the\npromised output are ultimately restrained by a signature of the updated\ninternal pubkey, this set of signers verify that promised output N does\nbind to the published state N ?\n\n> Its advantage is reduced number of eviction transactions,\n> as multiple evictions, plus the revival of the CoinPool,\n> can be put in a single transaction.\n> It has the disadvantage relative to `OP_TLUV` of requiring\n> point operations.\n> I have not explored completely, but my instinct suggests\n> that `OP_TLUV` use may require at least one signature\n> validation anyway.\n\nI believe you can slightly modify TLUV to make it functional for CoinPool\nrevival, where you want to prevent equivocation among the remaining set of\nsigners. Though, I'm leaning to agree that you may require at least one\nsignature validation  (first to restrain spend authorization inside the\npool participants, second to attach fees at broadcast-time).\n\n> It may be possible to design an `OP_CSFS` variant that\n> allows batch validation, such as by extending the virtual\n> machine with an accumulator for pending signature\n> validations.\n\nI agree that in the context of payment pools, aggregation of\nnon-cooperative unilateral spends is a scalability bottleneck, especially\nin the face of mempools congestion. If we rely on merkle\ntrees as the accumulator primitive, there is still the path to aggregate\nmany branches in-flight.\n\nAny misunderstandings of this proposal are my own.\n,\nAntoine\n\nLe jeu. 17 f\u00e9vr. 2022 \u00e0 21:45, ZmnSCPxj via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> `OP_EVICT`: An Alternative to `OP_TAPLEAFUPDATEVERIFY`\n> ======================================================\n>\n> In late 2021, `aj` proposed `OP_TAPLEAFUPDATEVERIFY` in order to\n> implement CoinPools and similar constructions.\n>\n> `Jeremy` observed that due to the use of Merkle tree paths, an\n> `OP_TLUV` would require O(log N) hash revelations in order to\n> reach a particular tapleaf, which, in the case of a CoinPool,\n> would then delete itself after spending only a particular amount\n> of funds.\n> He then observed that `OP_CTV` trees also require a similar\n> revelation of O(log N) transactions, but with the advantage that\n> once revealed, the transactions can then be reused, thus overall\n> the expectation is that the number of total bytes onchain is\n> lesser compared to `OP_TLUV`.\n>\n> After some thinking, I realized that it was the use of the\n> Merkle tree to represent the promised-but-offchain outputs of\n> the CoinPool that lead to the O(log N) space usage.\n> I then started thinking of alternative representations of\n> sets of promised outputs, which would not require O(log N)\n> revelations by avoiding the tree structure.\n>\n> Promised Outputs\n> ----------------\n>\n> Fundamentally, we can consider that a solution for scaling\n> Bitcoin would be to *promise* that some output *can* appear\n> onchain at some point in the future, without requiring that the\n> output be shown onchain *right now*.\n> Then, we can perform transactional cut-through on spends of the\n> promised outputs, without requiring onchain activity (\"offchain\").\n> Only if something Really Bad (TM) happens do we need to actually\n> drop the latest set of promised outputs onchain, where it has to\n> be verified globally by all fullnodes (and would thus incur scaling\n> and privacy costs).\n>\n> As an example of the above paradigm, consider the Lightning\n> Network.\n> Outputs representing the money of each party in a channel are\n> promised, and *can* appear onchain (via the unilateral close\n> mechanism).\n> In the meantime, there is a mechanism for performing cut-through,\n> allowing transfers between channel participants; any number of\n> transactions can be performed that are only \"solidified\" later,\n> without expensive onchain activity.\n>\n> Thus:\n>\n> * A CoinPool is really a way to commit to promised outputs.\n>   To change the distribution of those promised outputs, the\n>   CoinPool operators need to post an onchain transaction, but\n>   that is only a 1-input-1-output transaction, and with Schnorr\n>   signatures the single input requires only a single signature.\n>   But in case something Really Bad (TM) happens, any participant\n>   can unilaterally close the CoinPool, instantiating the promised\n>   outputs.\n> * A statechain is really just a CoinPool hosted inside a\n>   Decker-Wattenhofer or Decker-Russell-Osuntokun construction.\n>   This allows changing the distribution of those promised outputs\n>   without using an onchain transaction --- instead, a new state\n>   in the Decker-Wattenhofer/Decker-Russell-Osuntokun construction\n>   is created containing the new state, which invalidates all older\n>   states.\n>   Again, any participant can unilaterally shut it down, exposing\n>   the state of the inner CoinPool.\n> * A channel factory is really just a statechain where the\n>   promised outputs are not simple 1-of-1 single-owner outputs,\n>   but are rather 2-of-2 channels.\n>   This allows graceful degradation, where even if the statechain\n>   (\"factory\") layer has missing participants, individual 2-of-2\n>   channels can still continue operating as long as they do not\n>   involve missing participants, without requiring all participants\n>   to be online for large numbers of transactions.\n>\n> We can then consider that the base CoinPool usage should be enough,\n> as other mechanisms (`OP_CTV`+`OP_CSFS`, `SIGHASH_NOINPUT`) can be\n> used to implement statechains and channels and channel factories.\n>\n> I therefore conclude that what we really need is \"just\" a way to\n> commit ourselves to exposing a set of promised outputs, with the\n> proviso that if we all agree, we can change that set (without\n> requiring that the current or next set be exposed, for both\n> scaling and privacy).\n>\n> (To Bitcoin Cashers: this is not an IOU, this is *committed* and\n> can be enforced onchain, that is enough to threaten your offchain\n> counterparties into behaving correctly.\n> They cannot gain anything by denying the outputs they promised,\n> you can always drop it onchain and have it enforced, thus it is\n> not just merely an IOU, as IOUs are not necessarily enforceable,\n> but this mechanism *would* be.\n> Blockchain as judge+jury+executioner, not noisy marketplace.)\n>\n> Importantly: both `OP_CTV` and `OP_TLUV` force the user to\n> decide on a particular, but ultimately arbitrary, ordering for\n> promised outputs.\n> In principle, a set of promised outputs, if the owners of those\n> outputs are peers, does not have *any* inherent order.\n> Thus, I started to think about a commitment scheme that does not\n> impose any ordering during commitment.\n>\n> Digression: N-of-N With Eviction\n> --------------------------------\n>\n> An issue with using an N-of-N construction is that if any single\n> participant is offline, the construction cannot advance its state.\n>\n> This has lead to some peopple proposing to instead use K-of-N\n> once N reaches much larger than 2 participants for CoinPools/statechains/\n> channel factories.\n>\n> However, even so, K-of-N still requires that K participants remain\n> online, and the level K is a security parameter.\n> If less than K participants are online, then the construction\n> *still* cannot advance its state.\n>\n> Worse, because K < N, a single participant can have its funds\n> outright stolen by a quorum of K participants.\n> There is no way to prove that the other participants in the same\n> construction are not really sockpuppets of the same real-world\n> entity, thus it is entirely possible that the K quorum is actually\n> just a single participant that is now capable of stealing the\n> funds of all the other participants.\n> The only way to avoid this is to use N-oF-N: N-of-N requires\n> *your* keys, thus the coins are *your* coins.\n> In short: K-of-N, as it allows the state to be updated without your\n> keys (on the excuse that \"if you are offline, we need to be able to\n> update state\"), is *not your keys not your coins*.\n>\n> K-of-N should really only be used if all N are your sockpuppets,\n> and you want to HODL your funds.\n> This is the difference between consensus \"everyone must agree\" and\n> voting \"enough sockpuppets can be used to overpower you\".\n>\n> With `OP_TLUV`, however, it is possible to create an \"N-of-N With\n> Eviction\" construction.\n> When a participant in the N-of-N is offline, but the remaining\n> participants want to advance the state of the construction, they\n> instead evict the offline participant, creating a smaller N-of-N\n> where *all* participants are online, and continue operating.\n>\n> This avoids the *not your keys not your coins* problem of K-of-N\n> constructions, while simultaneously providing a way to advance\n> the state without the full participant set being online.\n>\n> The only real problem with `OP_TLUV` is that it takes O(log N)\n> hash revelations to evict one participant, and each evicted\n> participant requires one separate transaction.\n>\n> K-of-N has the \"advantage\" that even if you are offline, the state\n> can be advanced without evicting you.\n> However, as noted, as the coins can be spent without your keys,\n> the coins are not your coins, thus this advantage may be considered\n> dubious --- whether you are online or offline, a quorum of K can\n> outright steal your coins.\n> Eviction here requires that your coins be returned to your control.\n>\n> Committing To An Unordered Set\n> ------------------------------\n>\n> In an N-of-N CoinPool/statechain/channel factory, the ownership\n> of a single onchain UTXO is shared among N participants.\n> That is, there are a number of promised outputs, not exposed\n> onchain, which the N participants agree on as the \"real\" current\n> state of the construction,\n> However, the N participants can also agree to change the current\n> state of the construction, if all of them sign off on the change.\n>\n> Each of the promised outputs has a value, and the sum of all\n> promised values is the value of the onchain UTXO.\n> Interestingly, each of the promised outputs also has an SECP256K1\n> point that can be used as a public key, and the sum of all\n> promised points is the point of the onchain UTXO.\n>\n> Thus, the onchain UTXO can serve as a commitment to the sum of\n> the promised outputs.\n> The problem is committing to each of the individual promised\n> outputs.\n>\n> We can observe that a digital signature not only proves knowledge\n> of a private key, it also commits to a particular message.\n> Thus, we can make each participant sign their own expected\n> promised output, and share the signature for their promised\n> output.\n>\n> When a participant is to be evicted, the other participants\n> take the signature for the promised output of the to-be-evicted\n> participant, and show it onchain, to attest to the output.\n> Then, the onchain mechanism should then allow the rest of the\n> funds to be controlled by the N-of-N set minus the evicted\n> participant.\n>\n> `OP_EVICT`\n> ----------\n>\n> With all that, let me now propose the `OP_EVICT` opcode.\n>\n> `OP_EVICT` accepts a variable number of arguments.\n>\n> * The stack top is either the constant `1`, or an SECP256K1\n>   point.\n>   * If it is `1` that simply means \"use the Taproot internal\n>     pubkey\", as is usual for `OP_CHECKSIG`.\n> * The next stack item is a number, equal to the number of\n>   outputs that were promised, and which will now be evicted.\n> * The next stack items will alternate:\n>   * A number indicating an output index.\n>   * A signature for that output.\n>   * Output indices must not be duplicated, and indicated\n>     outputs must be SegWit v1 (\"Taproot\") outputs.\n>     The public key of the output will be taken as the public\n>     key for the corresponding signature, and the signature\n>     only covers the output itself (i.e. value and\n>     `scriptPubKey`).\n>     This means the signature has no `SIGHASH`.\n>   * As the signature covers the public key, this prevents\n>     malleation of a signature using one public key to a\n>     signature for another public key.\n> * After that is another signature.\n>   * This signature is checked using `OP_CHECKSIG` semantics\n>     (including `SIGHASH` support).\n>   * The public key is the input point (i.e. stack top)\n>     **MINUS** all the public keys of the indicated outputs.\n>\n> As a concrete example, suppose A, B, C, and D want to make a\n> CoinPool (or offchain variant of such) with the following\n> initial state:\n>\n> * A := 10\n> * B := 6\n> * C := 4\n> * D := 22\n>\n> Let us assume that A, B, C, and D have generated public\n> keys in such a way to avoid key cancellation (e.g.\n> precommitment, or the MuSig scheme).\n>\n> The participants then generate promised outputs for the\n> above, and each of them shares signatures for the promised\n> outputs:\n>\n> * sign(a, \"A := 10\")\n> * sign(b, \"B := 6\")\n> * sign(c, \"C := 4\")\n> * sign(d, \"D := 22\")\n>\n> Once that is done, they generate:\n>\n> * Q = A + B + C + D\n> * P = h(Q|`<1> OP_EVICT`) * Q\n>\n> Then they spend their funds, creating a Taproot output:\n>\n> * P := 42\n>\n> If all participants are online, they can move funds between\n> each other (or to other addresses) by cooperatively signing\n> using the point P, and the magic of Taproot means that use\n> of `OP_EVICT` is not visible.\n>\n> Suppose however that B is offline.\n> Then A, C, and D then decide to evict B.\n> To do so, they create a transaction that has an output\n> with \"B := 6\", and they reveal the `OP_EVICT` Tapscript\n> as well as sign(b, \"B := 6\").\n> This lets them change state and spend their funds without\n> B being online.\n> And B remains secure, as they cannot evict B except using\n> the pre-signed output, which B certifies as their expected\n> promised output.\n>\n> Note that the opcode as described above allows for multiple\n> evictions in the same transaction.\n> If B and C are offline, then the remaining participants\n> simply need to expose multiple outputs in the same\n> transaction.\n>\n> Security\n> --------\n>\n> I am not a cryptographer.\n> Thus, the security of this scheme is a conjecture.\n>\n> As long as key cancellation is protected against, it should\n> be secure.\n> The combined fund cannot be spent except if all participants\n> agree.\n> A smaller online participant set can be created only if a\n> participant is evicted, and eviction will force the owned\n> funds of the evicted participant to be instantiated.\n> The other participants cannot synthesize an alternate\n> signature signing a different value without knowledge of the\n> privkey of the evicted participant.\n>\n> To prevent signature replay, each update of an updateable\n> scheme like CoinPool et al should use a different pubkey\n> for each participant for each state.\n> As the signature covers the pubkey, it should be safe to\n> use a non-hardened derivation scheme so that only a single\n> root privkey is needed.\n>\n> Additional Discussion\n> ---------------------\n>\n> ### Eviction Scheme\n>\n> We can consider that the eviction scheme proposed here is the\n> following contract:\n>\n> * Either all of us agree on some transfer, OR,\n> * Give me my funds and the rest of you can all go play with\n>   your funds however you want.\n>\n> The signature that commits to a promised output is then the\n> agreement that the particular participant believes they are\n> entitled to a particular amount.\n>\n> We can consider that a participant can re-sign their output\n> with a different amount, but that is why `OP_EVICT` requires\n> the *other* participants to cooperatively sign as well.\n> If the other participants cooperatively sign, they effectively\n> agree to the participant re-signing for a different amount,\n> and thus actually covered by \"all of us agree\".\n>\n> ### Pure SCRIPT Contracts\n>\n> A \"pure SCRIPT contract\" is a Taproot contract where the\n> keyspend path is not desired, and the contract is composed of\n> Tapscript branches.\n>\n> In such a case, the expected technique would be for the\n> contract participants to agree on a NUMS point where none\n> of the participants can know the scalar (private key) behind\n> the point, and to use that as the internal Taproot pubkey\n> `Q`.\n> For complete protocols, the NUMS point can be a protocol-defined\n> constant.\n>\n> As the `OP_EVICT` opcode requires that each promised output\n> be signed, on the face of it, this technique cannot be used\n> for `OP_EVICT`-promised outputs, as it is impossible to sign\n> using the NUMS point.\n>\n> However, we should note that the requirement of a \"pure SCRIPT\"\n> contract is that none of the participants can unilaterally\n> sign an alternate spend.\n> Using an N-of-N of the participants as the Taproot internal\n> pubkey is sufficient to ensure this.\n>\n> As a concrete example: suppose we want an HTLC, which has a\n> hashlock branch requiring participant A, and a timelock branch\n> requiring participant B.\n> Such a simple scheme would not require that both A and B be\n> able to cooperatively spend the output, thus we might have\n> preferred the technique of using a NUMS point as Taproot\n> internal pubkey.\n> But using a NUMS point would not allow any signature, even the\n> `OP_EVICT`-required signatures-of-promised-outputs.\n>\n> Instead of using a NUMS point for the Taproot internal pubkey,\n> we can use the sum of `A[tmp] + B[tmp]` (suitably protected\n> against key cancellation).\n> Then both A and B can cooperatively sign the promised output,\n> and keep the promised output in an `OP_EVICT`-enforced UTXO.\n> After creating the signature for the promised output, A and B\n> can ensure that the keypath branch cannot be used by securely\n> deleting the private keys for `A[tmp]` and `B[tmp]`\n> respectively.\n>\n> ### Signature Half-Aggregation\n>\n> It is possible to batch-validate, and as `OP_EVICT` must\n> validate at least two signatures (an eviction and the\n> signature of the remaining) it makes sense to use batch\n> validation for `OP_EVICT`.\n>\n> Of note is that Schnorr signatures allow for third-party\n> half-aggregation, where the `s` components of multiple\n> signatures are summed together, but the `R` components\n> are not.\n>\n> (Warning: I am not aware of any security proofs that\n> half-aggregation is actually **safe**!\n> In particular, BIP-340 does not define half-aggregation,\n> and its batch validation algorithm is not, to my naivete,\n> extensible to half-aggregation.)\n>\n> Basically, if we are batch validating two signatures\n> `(R[0], s[0])`, `(R[1], s[1])` of two messages `m[0]`\n> and `m[1]` signed by two keys `A[0]` and `A[1]`, we\n> would do:\n>\n> * For `i = 0, 1`: `e[i] = h(R[i]|m[i])`\n> * Check: `(s[0] + s[1]) * G` is equal to `R[0] + e[0] * A[0] + R[1] + e[1]\n> * A[1]`.\n>\n> As we can see, the `s` can be summed before being\n> posted on the blockchain, as validators do not need\n> individual `s[i]`.\n> However, `R` cannot be summed as each one needs to be\n> hashed.\n>\n> This half-aggregation is third-party, i.e. someone\n> without any knowledge of any private keys can simply\n> sum the `s` components of multiple signatures.\n>\n> As `OP_EVICT` always validates at least two signatures,\n> using half-aggregation can remove at least 32 weight\n> units, and each additional promised output being evicted\n> is another signature whose `s` can be added to the sum.\n> Of course, **that depends on half-aggregation being\n> secure**.\n>\n> ### Relationship to Other Opcodes\n>\n> `OP_CTV` does other things than this opcode, and cannot\n> be used as a direct alternative.\n> In particular while `OP_CTV` *can* commit to a set of\n> promised outputs, if a promised output needs to be\n> published, the remaining funds are now distributed over a\n> set of UTXOs.\n> Thus, \"reviving\" the CoinPool (or offchain variant thereof)\n> requires consuming multiple UTXOs, and the consumption of\n> multiple UTXOs is risky unless specifically designd for it.\n> (In particular, if the UTXOs have different signer sets,\n> one signer set can initially cooperate to revive the\n> CoinPool, then spend their UTXO to a different transaction,\n> which if confirmed will invalidate the revival transaction.)\n>\n> This opcode seems largely in direct competitiong with\n> `OP_TLUV`, with largely the same design goal.\n> Its advantage is reduced number of eviction transactions,\n> as multiple evictions, plus the revival of the CoinPool,\n> can be put in a single transaction.\n> It has the disadvantage relative to `OP_TLUV` of requiring\n> point operations.\n> I have not explored completely, but my instinct suggests\n> that `OP_TLUV` use may require at least one signature\n> validation anyway.\n>\n> It may be possible to implement `OP_EVICT` in terms of\n> `OP_TX`/`OP_TXHASH`, `OP_CSFS`, and a point-subtraction\n> operation.\n> However, `OP_EVICT` allows for the trivial implementation\n> of batch validation (and, if half-aggregation is safe, to\n> use half-aggregation instead), whereas we expect multiple\n> `OP_CSFS` to be needed to implement this, without any\n> possibility of batch validation.\n> It may be possible to design an `OP_CSFS` variant that\n> allows batch validation, such as by extending the virtual\n> machine with an accumulator for pending signature\n> validations.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220218/c59ee569/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-18T23:39:49",
                "message_text_only": "Good morning ariard,\n\n\n> > A statechain is really just a CoinPool hosted inside a\n> > \u00a0Decker-Wattenhofer or Decker-Russell-Osuntokun construction.\n>\n> Note, to the best of my knowledge, how to use LN-Penalty in the context of multi-party construction is still an unsolved issue. If an invalidated state is published on-chain, how do you guarantee that the punished output value is distributed \"fairly\" among the \"honest\" set of users ? At least\n> where fairness is defined as a reasonable proportion of the balances they owned in the latest state.\n\nLN-Penalty I believe is what I call Poon-Dryja?\n\nBoth Decker-Wattenhofer (has no common colloquial name) and Decker-Russell-Osuntokun (\"eltoo\") are safe with N > 2.\nThe former has bad locktime tradeoffs in the unilateral close case, and the latter requires `SIGHASH_NOINPUT`/`SIGHASH_ANYPREVOUT`.\n\n\n> > In principle, a set of promised outputs, if the owners of those\n> > outputs are peers, does not have *any* inherent order.\n> > Thus, I started to think about a commitment scheme that does not\n> > impose any ordering during commitment.\n>\n> I think we should dissociate a) *outputs publication ordering* from the b) *spends paths ordering* itself. Even if to each spend path a output publication is attached, the ordering constraint might not present the same complexity.\n>\n> Under this distinction, are you sure that TLUV imposes an ordering on the output publication ?\n\nYes, because TLUV is based on tapleaf revelation.\nEach participant gets its own unique tapleaf that lets that participant get evicted.\n\nIn Taproot, the recommendation is to sort the hashes of each tapleaf before arranging them into a MAST that the Taproot address then commits to.\nThis sort-by-hash *is* the arbitrary ordering I refer to when I say that TLUV imposes an arbitrary ordering.\n(actually the only requirement is that pairs of scripts are sorted-by-hash, but it is just easier to sort the whole array by hash.)\n\nTo reveal a single participant in a TLUV-based CoinPool, you need to reveal O(log N) hashes.\nIt is the O(log N) space consumption I want to avoid with `OP_EVICT`, and I believe the reason for that O(log N) revelation is due precisely to the arbitrary but necessary ordering.\n\n> > With `OP_TLUV`, however, it is possible to create an \"N-of-N With\n> > Eviction\" construction.\n> > When a participant in the N-of-N is offline, but the remaining\n> > participants want to advance the state of the construction, they\n> > instead evict the offline participant, creating a smaller N-of-N\n> > where *all* participants are online, and continue operating.\n>\n> I think we should dissociate two types of pool spends : a) eviction by the pool unanimity in case of irresponsive participants and b) unilateral withdrawal by a participant because of the liquidity allocation policy. I think the distinction is worthy, as the pool participant should be stable and the eviction not abused.\n>\n> I'm not sure if TLUV enables b), at least without transforming the unilateral withdrawal into an eviction. To ensure the TLUV operation is correct\u00a0 (spent leaf is removed, withdrawing participant point removed, etc), the script content must be inspected by *all* the participant. However, I believe\n> knowledge of this content effectively allows you to play it out against the pool at any time ? It's likely solvable at the price of a CHECKSIG.\n\nIndeed, that distinction is important.\n`OP_TLUV` (and `OP_EVICT`, which is just a redesigned `OP_TLUV`) supports (a) but not (b).\n\n> `OP_EVICT`\n> ----------\n>\n> > \u00a0* If it is `1` that simply means \"use the Taproot internal\n> > \u00a0 \u00a0pubkey\", as is usual for `OP_CHECKSIG`.\n>\n> IIUC, this assumes the deployment of BIP118, where if the\u00a0 public key is a single byte 0x01, the internal pubkey is used\n> for verification.\n\nI thought it was part of Taproot?\n\n>\n> > \u00a0* Output indices must not be duplicated, and indicated\n> > \u00a0 \u00a0outputs must be SegWit v1 (\"Taproot\") outputs.\n>\n> I think public key duplication must not be verified. If a duplicated public key is present, the point is subtracted twice from the internal pubkey and therefore the aggregated\n> key remains unknown ? So it sounds to me safe against replay attacks.\n\nAh, right.\n\n> > \u00a0* The public key is the input point (i.e. stack top)\n> > \u00a0 \u00a0**MINUS** all the public keys of the indicated outputs.\n>\n> Can you prevent eviction abuse where one counterparty threatens to evict everyone as all the output signatures are known among participants and free to sum ? (at least not considering fees)\n\nNo, I considered onchain fees as the only mechanism to avoid eviction abuse.\nThe individual-evict signatures commit to fixed quantities.\nThe remaining change is then the only fund that can pay for onchain fees, so a single party evicting everyone else has to pay for the eviction of everyone else.\n\n\n> > Suppose however that B is offline.\n> > Then A, C, and D then decide to evict B.\n> > To do so, they create a transaction that has an output\n> > with \"B := 6\", and they reveal the `OP_EVICT` Tapscript\n> > as well as sign(b, \"B := 6\").\n> > This lets them change state and spend their funds without\n> > B being online.\n> > And B remains secure, as they cannot evict B except using\n> > the pre-signed output, which B certifies as their expected\n> > promised output.\n>\n> I think in the context of (off-chain) payment pool, OP_EVICT requires participant cooperation *after* the state update to allow a single participant to withdraw her funds.\n\nHow so?\n\nA single participant withdrawing their funds unilaterally can do so by evicting everyone else (and paying for those evictions, as sort of a \"nuisance fee\").\nThe signatures for each per-participant-eviction can be exchanged before the signature exchange for the Decker-Wattenhofer or Decker-Russell-Osuntokun.\n\n\n> > The combined fund cannot be spent except if all participants\n> > agree.\n>\n> If all participants agree minus the evicted ones, correct ? The output promises signatures are shared at state setup, therefore no additional contribution from the evicted participant (I think).\n\nYes.\n\n>\n> > To prevent signature replay, each update of an updateable\n> > scheme like CoinPool et al should use a different pubkey\n> > for each participant for each state.\n>\n> I'm not even sure if it's required with OP_EVICT, as the publication of the promised output are ultimately restrained by a signature of the updated internal pubkey, this set of signers verify that promised output N does bind to the published state N ?\n\nIf the internal pubkey is reused (for example, if all participants are online and want to change state cooperatively) then the component keys need to be re-tweaked each time.\n\nThe tweaking can be done with non-hardened derivation.\n\n\n> > Its advantage is reduced number of eviction transactions,\n> > as multiple evictions, plus the revival of the CoinPool,\n> > can be put in a single transaction.\n> > It has the disadvantage relative to `OP_TLUV` of requiring\n> > point operations.\n> > I have not explored completely, but my instinct suggests\n> > that `OP_TLUV` use may require at least one signature\n> > validation anyway.\n>\n> I believe you can slightly modify TLUV to make it functional for CoinPool revival, where you want to prevent equivocation among the remaining set of signers. Though, I'm leaning to agree that you may require at least one signature validation\u00a0 (first to restrain spend authorization inside the pool participants, second to attach fees at broadcast-time).\n\nYes, TLUV does have that advantage relative to CTV, and `OP_EVICT` is \"just\" a redesigned `OP_TLUV`.\n\nIn particular, I first developed my thoughts on revivable constructs with eviction of participants here: https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-February/003479.html\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Jeremy Rubin",
                "date": "2022-02-19T00:56:05",
                "message_text_only": "This is a fascinating post and I'm still chewing on it.\n\nChiming in with two points:\n\nPoint 1, note with respect to evictions, revivals, CTV, TLUV:\n\nCTV enables 1 person to be evicted in O(log N) or one person to leave in\nO(log N). TLUV enables 1 person to leave in O(1) O(log N) transactions, but\nevictions take (AFAICT?) O(N) O(log N) transactions because the un-live\nparty stays in the pool. Hence OP_EVICT helps also make it so you can kick\nsomeone out, rather than all having to leave, which is an improvement.\n\nCTV rejoins work as follows:\n\nsuppose you have a pool with 1 failure, you need to do log N txns to evict\nthe failure, which creates R * log_R(N) outputs, which can then do a\ntransaction to rejoin.\n\nFor example, suppose I had 64 people in a radix 4 tree. you'd have at the\ntop level 4 groups of 16, then 4 groups of 4 people, and then 1 to 4 txns.\nKicking 1 person out would make you do 3 txns, and create 12 outputs total.\nA transaction spending the 11 outputs that are live would capture 63 people\nback into the tree, and with CISA would not be terribly expensive. To be a\nbit more economical, you might prefer to just join the 3 outputs with 16\npeople in it, and yield 48 people in one pool. Alternatively, you can\nlazily re-join if fees make it worth it/piggybacking another transaction,\nor operate independently or try to find new, better, peers.\n\nOverall this is the type of application that necessitates *exact* byte\ncounting. Oftentimes things with CTV seem inefficient, but when you crunch\nthe numbers it turns out not to be so terrible. OP_EVICT seems promising in\nthis regard compared to TLUV or accumulators.\n\nAnother option is to randomize the CTV trees with multiple outputs per\nparty (radix Q), then you need to do Q times the evictions, but you end up\nwith sub-pools that contain more people/fractional liquidity (this might\nhappen naturally if CTV Pools have channels in them, so it's good to model).\n\n\nPoint 2, on Eltoo:\n\nOne point of discomfort I have with Eltoo that I think is not universal,\nbut is shared by some others, is that non-punitive channels may not be good\nfor high-value channels as you do want, especially in a congested\nblockspace world, punishments to incentivize correct behavior (otherwise\ncheating may look like a free option).\n\nThus I'm reluctant to fully embrace designs which do not permit nested\ntraditional punitive channels in favor of Eltoo, when Eltoo might not have\nproduct-market-fit for higher valued channels.\n\nIf someone had a punitive-eltoo variant that would ameliorate this concern\nalmost entirely.\n\nCheers,\n\nJeremy\n\n\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n\nOn Fri, Feb 18, 2022 at 3:40 PM ZmnSCPxj via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Good morning ariard,\n>\n>\n> > > A statechain is really just a CoinPool hosted inside a\n> > >  Decker-Wattenhofer or Decker-Russell-Osuntokun construction.\n> >\n> > Note, to the best of my knowledge, how to use LN-Penalty in the context\n> of multi-party construction is still an unsolved issue. If an invalidated\n> state is published on-chain, how do you guarantee that the punished output\n> value is distributed \"fairly\" among the \"honest\" set of users ? At least\n> > where fairness is defined as a reasonable proportion of the balances\n> they owned in the latest state.\n>\n> LN-Penalty I believe is what I call Poon-Dryja?\n>\n> Both Decker-Wattenhofer (has no common colloquial name) and\n> Decker-Russell-Osuntokun (\"eltoo\") are safe with N > 2.\n> The former has bad locktime tradeoffs in the unilateral close case, and\n> the latter requires `SIGHASH_NOINPUT`/`SIGHASH_ANYPREVOUT`.\n>\n>\n> > > In principle, a set of promised outputs, if the owners of those\n> > > outputs are peers, does not have *any* inherent order.\n> > > Thus, I started to think about a commitment scheme that does not\n> > > impose any ordering during commitment.\n> >\n> > I think we should dissociate a) *outputs publication ordering* from the\n> b) *spends paths ordering* itself. Even if to each spend path a output\n> publication is attached, the ordering constraint might not present the same\n> complexity.\n> >\n> > Under this distinction, are you sure that TLUV imposes an ordering on\n> the output publication ?\n>\n> Yes, because TLUV is based on tapleaf revelation.\n> Each participant gets its own unique tapleaf that lets that participant\n> get evicted.\n>\n> In Taproot, the recommendation is to sort the hashes of each tapleaf\n> before arranging them into a MAST that the Taproot address then commits to.\n> This sort-by-hash *is* the arbitrary ordering I refer to when I say that\n> TLUV imposes an arbitrary ordering.\n> (actually the only requirement is that pairs of scripts are\n> sorted-by-hash, but it is just easier to sort the whole array by hash.)\n>\n> To reveal a single participant in a TLUV-based CoinPool, you need to\n> reveal O(log N) hashes.\n> It is the O(log N) space consumption I want to avoid with `OP_EVICT`, and\n> I believe the reason for that O(log N) revelation is due precisely to the\n> arbitrary but necessary ordering.\n>\n> > > With `OP_TLUV`, however, it is possible to create an \"N-of-N With\n> > > Eviction\" construction.\n> > > When a participant in the N-of-N is offline, but the remaining\n> > > participants want to advance the state of the construction, they\n> > > instead evict the offline participant, creating a smaller N-of-N\n> > > where *all* participants are online, and continue operating.\n> >\n> > I think we should dissociate two types of pool spends : a) eviction by\n> the pool unanimity in case of irresponsive participants and b) unilateral\n> withdrawal by a participant because of the liquidity allocation policy. I\n> think the distinction is worthy, as the pool participant should be stable\n> and the eviction not abused.\n> >\n> > I'm not sure if TLUV enables b), at least without transforming the\n> unilateral withdrawal into an eviction. To ensure the TLUV operation is\n> correct  (spent leaf is removed, withdrawing participant point removed,\n> etc), the script content must be inspected by *all* the participant.\n> However, I believe\n> > knowledge of this content effectively allows you to play it out against\n> the pool at any time ? It's likely solvable at the price of a CHECKSIG.\n>\n> Indeed, that distinction is important.\n> `OP_TLUV` (and `OP_EVICT`, which is just a redesigned `OP_TLUV`) supports\n> (a) but not (b).\n>\n> > `OP_EVICT`\n> > ----------\n> >\n> > >  * If it is `1` that simply means \"use the Taproot internal\n> > >    pubkey\", as is usual for `OP_CHECKSIG`.\n> >\n> > IIUC, this assumes the deployment of BIP118, where if the  public key is\n> a single byte 0x01, the internal pubkey is used\n> > for verification.\n>\n> I thought it was part of Taproot?\n>\n> >\n> > >  * Output indices must not be duplicated, and indicated\n> > >    outputs must be SegWit v1 (\"Taproot\") outputs.\n> >\n> > I think public key duplication must not be verified. If a duplicated\n> public key is present, the point is subtracted twice from the internal\n> pubkey and therefore the aggregated\n> > key remains unknown ? So it sounds to me safe against replay attacks.\n>\n> Ah, right.\n>\n> > >  * The public key is the input point (i.e. stack top)\n> > >    **MINUS** all the public keys of the indicated outputs.\n> >\n> > Can you prevent eviction abuse where one counterparty threatens to evict\n> everyone as all the output signatures are known among participants and free\n> to sum ? (at least not considering fees)\n>\n> No, I considered onchain fees as the only mechanism to avoid eviction\n> abuse.\n> The individual-evict signatures commit to fixed quantities.\n> The remaining change is then the only fund that can pay for onchain fees,\n> so a single party evicting everyone else has to pay for the eviction of\n> everyone else.\n>\n>\n> > > Suppose however that B is offline.\n> > > Then A, C, and D then decide to evict B.\n> > > To do so, they create a transaction that has an output\n> > > with \"B := 6\", and they reveal the `OP_EVICT` Tapscript\n> > > as well as sign(b, \"B := 6\").\n> > > This lets them change state and spend their funds without\n> > > B being online.\n> > > And B remains secure, as they cannot evict B except using\n> > > the pre-signed output, which B certifies as their expected\n> > > promised output.\n> >\n> > I think in the context of (off-chain) payment pool, OP_EVICT requires\n> participant cooperation *after* the state update to allow a single\n> participant to withdraw her funds.\n>\n> How so?\n>\n> A single participant withdrawing their funds unilaterally can do so by\n> evicting everyone else (and paying for those evictions, as sort of a\n> \"nuisance fee\").\n> The signatures for each per-participant-eviction can be exchanged before\n> the signature exchange for the Decker-Wattenhofer or\n> Decker-Russell-Osuntokun.\n>\n>\n> > > The combined fund cannot be spent except if all participants\n> > > agree.\n> >\n> > If all participants agree minus the evicted ones, correct ? The output\n> promises signatures are shared at state setup, therefore no additional\n> contribution from the evicted participant (I think).\n>\n> Yes.\n>\n> >\n> > > To prevent signature replay, each update of an updateable\n> > > scheme like CoinPool et al should use a different pubkey\n> > > for each participant for each state.\n> >\n> > I'm not even sure if it's required with OP_EVICT, as the publication of\n> the promised output are ultimately restrained by a signature of the updated\n> internal pubkey, this set of signers verify that promised output N does\n> bind to the published state N ?\n>\n> If the internal pubkey is reused (for example, if all participants are\n> online and want to change state cooperatively) then the component keys need\n> to be re-tweaked each time.\n>\n> The tweaking can be done with non-hardened derivation.\n>\n>\n> > > Its advantage is reduced number of eviction transactions,\n> > > as multiple evictions, plus the revival of the CoinPool,\n> > > can be put in a single transaction.\n> > > It has the disadvantage relative to `OP_TLUV` of requiring\n> > > point operations.\n> > > I have not explored completely, but my instinct suggests\n> > > that `OP_TLUV` use may require at least one signature\n> > > validation anyway.\n> >\n> > I believe you can slightly modify TLUV to make it functional for\n> CoinPool revival, where you want to prevent equivocation among the\n> remaining set of signers. Though, I'm leaning to agree that you may require\n> at least one signature validation  (first to restrain spend authorization\n> inside the pool participants, second to attach fees at broadcast-time).\n>\n> Yes, TLUV does have that advantage relative to CTV, and `OP_EVICT` is\n> \"just\" a redesigned `OP_TLUV`.\n>\n> In particular, I first developed my thoughts on revivable constructs with\n> eviction of participants here:\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-February/003479.html\n>\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220218/a48f8546/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-19T01:17:20",
                "message_text_only": "Good morning Jeremy,\n\n> This is a fascinating post and I'm still chewing on it.\n>\n> Chiming in with two points:\n>\n> Point 1, note with respect to evictions, revivals, CTV, TLUV:\n>\n> CTV enables 1 person to be evicted in O(log N) or one person to leave in O(log N). TLUV enables 1 person to leave in O(1) O(log N) transactions, but evictions take (AFAICT?) O(N) O(log N) transactions because the un-live party stays in the pool. Hence OP_EVICT helps also make it so you can kick someone out, rather than all having to leave, which is an improvement.\n>\n> CTV rejoins work as follows:\n>\n> suppose you have a pool with 1 failure, you need to do log N txns to evict the failure, which creates R * log_R(N) outputs, which can then do a transaction to rejoin.\n>\n> For example, suppose I had 64 people in a radix 4 tree. you'd have at the top level 4 groups of 16, then 4 groups of 4 people, and then 1 to 4 txns. Kicking 1 person out would make you do 3 txns, and create 12 outputs total. A transaction spending the 11 outputs that are live would capture 63 people back into the tree, and with CISA would not be terribly expensive. To be a bit more economical, you might prefer to just join the 3 outputs with 16 people in it, and yield 48 people in one pool. Alternatively, you can lazily re-join if fees make it worth it/piggybacking another transaction, or operate independently or try to find new, better, peers.\n>\n> Overall this is the type of application that necessitates *exact* byte counting. Oftentimes things with CTV seem inefficient, but when you crunch the numbers\u00a0it turns out not to be so terrible. OP_EVICT seems promising in this regard compared to TLUV or accumulators.\n>\n> Another option is to randomize the CTV trees with multiple outputs per party (radix Q), then you need to do Q times the evictions, but you end up with sub-pools that contain more people/fractional liquidity (this might happen naturally if CTV Pools have channels in them, so it's good to model).\n\nDo note that a weakness of CTV is that you *have to* split up the CoinPool into many smaller pools, and re-merging them requires waiting for onchain confirmation.\nThis overall means you have no real incentive to revive the original CoinPool minus evicted parties.\n`OP_EVICT` lets the CoinPool revival be made into the same transaction that performs the evict.\n\n> Point 2, on Eltoo:\n>\n> One point of discomfort I have with Eltoo that I think is not universal, but is shared by some others, is that non-punitive channels may not be good for high-value channels as you do want, especially in a congested blockspace world, punishments to incentivize correct behavior (otherwise cheating may look like a free option).\n>\n> Thus I'm reluctant to fully embrace designs which do not permit nested traditional punitive channels in favor of Eltoo, when Eltoo might not have product-market-fit for higher valued channels.\n>\n> If someone had a punitive-eltoo variant that would ameliorate this concern almost entirely.\n\nUnfortunately, it seems the way to any kind of N > 2 construction *with* penalty would require bonds, such as the recent PathCoin idea (which is an N > 2 construction *with* penalty, and is definitely offchain for much of its operation).\n\nHaving a Decker-Russell-Osuntokun \"factory\" layer that hosts multiple Poon-Dryja channels is not quite a solution; if old state on Decker-Russell-Osuntokun layer pushes through, then its obsolete Poon-Dryja channels will have all states invalid and unclaimable, but in case of Sybil where some participants are sockpuppets, it would still be possible for a thief to claim the funds from an \"invalidated\" Poon-Dryja channel if that channel is with a sockpuppet.\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Greg Sanders",
                "date": "2022-02-19T01:46:07",
                "message_text_only": "> One point of discomfort I have with Eltoo that I think is not universal,\nbut is shared by some others, is that non-punitive channels may not be good\nfor high-value channels as you do want, especially in a congested\nblockspace world, punishments to incentivize correct behavior (otherwise\ncheating may look like a free option).\n\nWithout derailing the conversation too far, \"fully\" punitive channels also\nmake large value channels more dangerous from the perspective of bugs\ncausing old states to be published. High value channels you'll need to have\nvery high uptime. If you're available, your counterparty is incentivized to\ndo a mutual close to reduce fees and remove timelocks on outputs. I think\nthese tradeoffs will result in both types existing for N==2.\n\nOn Sat, Feb 19, 2022 at 8:56 AM Jeremy Rubin via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> This is a fascinating post and I'm still chewing on it.\n>\n> Chiming in with two points:\n>\n> Point 1, note with respect to evictions, revivals, CTV, TLUV:\n>\n> CTV enables 1 person to be evicted in O(log N) or one person to leave in\n> O(log N). TLUV enables 1 person to leave in O(1) O(log N) transactions, but\n> evictions take (AFAICT?) O(N) O(log N) transactions because the un-live\n> party stays in the pool. Hence OP_EVICT helps also make it so you can kick\n> someone out, rather than all having to leave, which is an improvement.\n>\n> CTV rejoins work as follows:\n>\n> suppose you have a pool with 1 failure, you need to do log N txns to evict\n> the failure, which creates R * log_R(N) outputs, which can then do a\n> transaction to rejoin.\n>\n> For example, suppose I had 64 people in a radix 4 tree. you'd have at the\n> top level 4 groups of 16, then 4 groups of 4 people, and then 1 to 4 txns.\n> Kicking 1 person out would make you do 3 txns, and create 12 outputs total.\n> A transaction spending the 11 outputs that are live would capture 63 people\n> back into the tree, and with CISA would not be terribly expensive. To be a\n> bit more economical, you might prefer to just join the 3 outputs with 16\n> people in it, and yield 48 people in one pool. Alternatively, you can\n> lazily re-join if fees make it worth it/piggybacking another transaction,\n> or operate independently or try to find new, better, peers.\n>\n> Overall this is the type of application that necessitates *exact* byte\n> counting. Oftentimes things with CTV seem inefficient, but when you crunch\n> the numbers it turns out not to be so terrible. OP_EVICT seems promising in\n> this regard compared to TLUV or accumulators.\n>\n> Another option is to randomize the CTV trees with multiple outputs per\n> party (radix Q), then you need to do Q times the evictions, but you end up\n> with sub-pools that contain more people/fractional liquidity (this might\n> happen naturally if CTV Pools have channels in them, so it's good to model).\n>\n>\n> Point 2, on Eltoo:\n>\n> One point of discomfort I have with Eltoo that I think is not universal,\n> but is shared by some others, is that non-punitive channels may not be good\n> for high-value channels as you do want, especially in a congested\n> blockspace world, punishments to incentivize correct behavior (otherwise\n> cheating may look like a free option).\n>\n> Thus I'm reluctant to fully embrace designs which do not permit nested\n> traditional punitive channels in favor of Eltoo, when Eltoo might not have\n> product-market-fit for higher valued channels.\n>\n> If someone had a punitive-eltoo variant that would ameliorate this concern\n> almost entirely.\n>\n> Cheers,\n>\n> Jeremy\n>\n>\n>\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n>\n> On Fri, Feb 18, 2022 at 3:40 PM ZmnSCPxj via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Good morning ariard,\n>>\n>>\n>> > > A statechain is really just a CoinPool hosted inside a\n>> > >  Decker-Wattenhofer or Decker-Russell-Osuntokun construction.\n>> >\n>> > Note, to the best of my knowledge, how to use LN-Penalty in the context\n>> of multi-party construction is still an unsolved issue. If an invalidated\n>> state is published on-chain, how do you guarantee that the punished output\n>> value is distributed \"fairly\" among the \"honest\" set of users ? At least\n>> > where fairness is defined as a reasonable proportion of the balances\n>> they owned in the latest state.\n>>\n>> LN-Penalty I believe is what I call Poon-Dryja?\n>>\n>> Both Decker-Wattenhofer (has no common colloquial name) and\n>> Decker-Russell-Osuntokun (\"eltoo\") are safe with N > 2.\n>> The former has bad locktime tradeoffs in the unilateral close case, and\n>> the latter requires `SIGHASH_NOINPUT`/`SIGHASH_ANYPREVOUT`.\n>>\n>>\n>> > > In principle, a set of promised outputs, if the owners of those\n>> > > outputs are peers, does not have *any* inherent order.\n>> > > Thus, I started to think about a commitment scheme that does not\n>> > > impose any ordering during commitment.\n>> >\n>> > I think we should dissociate a) *outputs publication ordering* from the\n>> b) *spends paths ordering* itself. Even if to each spend path a output\n>> publication is attached, the ordering constraint might not present the same\n>> complexity.\n>> >\n>> > Under this distinction, are you sure that TLUV imposes an ordering on\n>> the output publication ?\n>>\n>> Yes, because TLUV is based on tapleaf revelation.\n>> Each participant gets its own unique tapleaf that lets that participant\n>> get evicted.\n>>\n>> In Taproot, the recommendation is to sort the hashes of each tapleaf\n>> before arranging them into a MAST that the Taproot address then commits to.\n>> This sort-by-hash *is* the arbitrary ordering I refer to when I say that\n>> TLUV imposes an arbitrary ordering.\n>> (actually the only requirement is that pairs of scripts are\n>> sorted-by-hash, but it is just easier to sort the whole array by hash.)\n>>\n>> To reveal a single participant in a TLUV-based CoinPool, you need to\n>> reveal O(log N) hashes.\n>> It is the O(log N) space consumption I want to avoid with `OP_EVICT`, and\n>> I believe the reason for that O(log N) revelation is due precisely to the\n>> arbitrary but necessary ordering.\n>>\n>> > > With `OP_TLUV`, however, it is possible to create an \"N-of-N With\n>> > > Eviction\" construction.\n>> > > When a participant in the N-of-N is offline, but the remaining\n>> > > participants want to advance the state of the construction, they\n>> > > instead evict the offline participant, creating a smaller N-of-N\n>> > > where *all* participants are online, and continue operating.\n>> >\n>> > I think we should dissociate two types of pool spends : a) eviction by\n>> the pool unanimity in case of irresponsive participants and b) unilateral\n>> withdrawal by a participant because of the liquidity allocation policy. I\n>> think the distinction is worthy, as the pool participant should be stable\n>> and the eviction not abused.\n>> >\n>> > I'm not sure if TLUV enables b), at least without transforming the\n>> unilateral withdrawal into an eviction. To ensure the TLUV operation is\n>> correct  (spent leaf is removed, withdrawing participant point removed,\n>> etc), the script content must be inspected by *all* the participant.\n>> However, I believe\n>> > knowledge of this content effectively allows you to play it out against\n>> the pool at any time ? It's likely solvable at the price of a CHECKSIG.\n>>\n>> Indeed, that distinction is important.\n>> `OP_TLUV` (and `OP_EVICT`, which is just a redesigned `OP_TLUV`) supports\n>> (a) but not (b).\n>>\n>> > `OP_EVICT`\n>> > ----------\n>> >\n>> > >  * If it is `1` that simply means \"use the Taproot internal\n>> > >    pubkey\", as is usual for `OP_CHECKSIG`.\n>> >\n>> > IIUC, this assumes the deployment of BIP118, where if the  public key\n>> is a single byte 0x01, the internal pubkey is used\n>> > for verification.\n>>\n>> I thought it was part of Taproot?\n>>\n>> >\n>> > >  * Output indices must not be duplicated, and indicated\n>> > >    outputs must be SegWit v1 (\"Taproot\") outputs.\n>> >\n>> > I think public key duplication must not be verified. If a duplicated\n>> public key is present, the point is subtracted twice from the internal\n>> pubkey and therefore the aggregated\n>> > key remains unknown ? So it sounds to me safe against replay attacks.\n>>\n>> Ah, right.\n>>\n>> > >  * The public key is the input point (i.e. stack top)\n>> > >    **MINUS** all the public keys of the indicated outputs.\n>> >\n>> > Can you prevent eviction abuse where one counterparty threatens to\n>> evict everyone as all the output signatures are known among participants\n>> and free to sum ? (at least not considering fees)\n>>\n>> No, I considered onchain fees as the only mechanism to avoid eviction\n>> abuse.\n>> The individual-evict signatures commit to fixed quantities.\n>> The remaining change is then the only fund that can pay for onchain fees,\n>> so a single party evicting everyone else has to pay for the eviction of\n>> everyone else.\n>>\n>>\n>> > > Suppose however that B is offline.\n>> > > Then A, C, and D then decide to evict B.\n>> > > To do so, they create a transaction that has an output\n>> > > with \"B := 6\", and they reveal the `OP_EVICT` Tapscript\n>> > > as well as sign(b, \"B := 6\").\n>> > > This lets them change state and spend their funds without\n>> > > B being online.\n>> > > And B remains secure, as they cannot evict B except using\n>> > > the pre-signed output, which B certifies as their expected\n>> > > promised output.\n>> >\n>> > I think in the context of (off-chain) payment pool, OP_EVICT requires\n>> participant cooperation *after* the state update to allow a single\n>> participant to withdraw her funds.\n>>\n>> How so?\n>>\n>> A single participant withdrawing their funds unilaterally can do so by\n>> evicting everyone else (and paying for those evictions, as sort of a\n>> \"nuisance fee\").\n>> The signatures for each per-participant-eviction can be exchanged before\n>> the signature exchange for the Decker-Wattenhofer or\n>> Decker-Russell-Osuntokun.\n>>\n>>\n>> > > The combined fund cannot be spent except if all participants\n>> > > agree.\n>> >\n>> > If all participants agree minus the evicted ones, correct ? The output\n>> promises signatures are shared at state setup, therefore no additional\n>> contribution from the evicted participant (I think).\n>>\n>> Yes.\n>>\n>> >\n>> > > To prevent signature replay, each update of an updateable\n>> > > scheme like CoinPool et al should use a different pubkey\n>> > > for each participant for each state.\n>> >\n>> > I'm not even sure if it's required with OP_EVICT, as the publication of\n>> the promised output are ultimately restrained by a signature of the updated\n>> internal pubkey, this set of signers verify that promised output N does\n>> bind to the published state N ?\n>>\n>> If the internal pubkey is reused (for example, if all participants are\n>> online and want to change state cooperatively) then the component keys need\n>> to be re-tweaked each time.\n>>\n>> The tweaking can be done with non-hardened derivation.\n>>\n>>\n>> > > Its advantage is reduced number of eviction transactions,\n>> > > as multiple evictions, plus the revival of the CoinPool,\n>> > > can be put in a single transaction.\n>> > > It has the disadvantage relative to `OP_TLUV` of requiring\n>> > > point operations.\n>> > > I have not explored completely, but my instinct suggests\n>> > > that `OP_TLUV` use may require at least one signature\n>> > > validation anyway.\n>> >\n>> > I believe you can slightly modify TLUV to make it functional for\n>> CoinPool revival, where you want to prevent equivocation among the\n>> remaining set of signers. Though, I'm leaning to agree that you may require\n>> at least one signature validation  (first to restrain spend authorization\n>> inside the pool participants, second to attach fees at broadcast-time).\n>>\n>> Yes, TLUV does have that advantage relative to CTV, and `OP_EVICT` is\n>> \"just\" a redesigned `OP_TLUV`.\n>>\n>> In particular, I first developed my thoughts on revivable constructs with\n>> eviction of participants here:\n>> https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-February/003479.html\n>>\n>>\n>> Regards,\n>> ZmnSCPxj\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220219/cddea35d/attachment-0001.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-02-19T07:21:56",
                "message_text_only": "> \"fully\" punitive channels also make large value channels more dangerous\nfrom the perspective of bugs causing old states to be published\n\nWouldn't it be ideal to have the penalty be to pay for a single extra\ntransaction fee? That way there is a penalty so cheating attempts aren't\nfree (for someone who wants to close a channel anyway) and yet a single fee\nisn't going to be much of a concern in the accidental publishing case. It\nstill perplexes me why eltoo chose no penalty at all vs a small penalty\nlike that.\n\nOn Fri, Feb 18, 2022, 19:46 Greg Sanders via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> > One point of discomfort I have with Eltoo that I think is not\n> universal, but is shared by some others, is that non-punitive channels may\n> not be good for high-value channels as you do want, especially in a\n> congested blockspace world, punishments to incentivize correct behavior\n> (otherwise cheating may look like a free option).\n>\n> Without derailing the conversation too far, \"fully\" punitive channels also\n> make large value channels more dangerous from the perspective of bugs\n> causing old states to be published. High value channels you'll need to have\n> very high uptime. If you're available, your counterparty is incentivized to\n> do a mutual close to reduce fees and remove timelocks on outputs. I think\n> these tradeoffs will result in both types existing for N==2.\n>\n> On Sat, Feb 19, 2022 at 8:56 AM Jeremy Rubin via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> This is a fascinating post and I'm still chewing on it.\n>>\n>> Chiming in with two points:\n>>\n>> Point 1, note with respect to evictions, revivals, CTV, TLUV:\n>>\n>> CTV enables 1 person to be evicted in O(log N) or one person to leave in\n>> O(log N). TLUV enables 1 person to leave in O(1) O(log N) transactions, but\n>> evictions take (AFAICT?) O(N) O(log N) transactions because the un-live\n>> party stays in the pool. Hence OP_EVICT helps also make it so you can kick\n>> someone out, rather than all having to leave, which is an improvement.\n>>\n>> CTV rejoins work as follows:\n>>\n>> suppose you have a pool with 1 failure, you need to do log N txns to\n>> evict the failure, which creates R * log_R(N) outputs, which can then do a\n>> transaction to rejoin.\n>>\n>> For example, suppose I had 64 people in a radix 4 tree. you'd have at the\n>> top level 4 groups of 16, then 4 groups of 4 people, and then 1 to 4 txns.\n>> Kicking 1 person out would make you do 3 txns, and create 12 outputs total.\n>> A transaction spending the 11 outputs that are live would capture 63 people\n>> back into the tree, and with CISA would not be terribly expensive. To be a\n>> bit more economical, you might prefer to just join the 3 outputs with 16\n>> people in it, and yield 48 people in one pool. Alternatively, you can\n>> lazily re-join if fees make it worth it/piggybacking another transaction,\n>> or operate independently or try to find new, better, peers.\n>>\n>> Overall this is the type of application that necessitates *exact* byte\n>> counting. Oftentimes things with CTV seem inefficient, but when you crunch\n>> the numbers it turns out not to be so terrible. OP_EVICT seems promising in\n>> this regard compared to TLUV or accumulators.\n>>\n>> Another option is to randomize the CTV trees with multiple outputs per\n>> party (radix Q), then you need to do Q times the evictions, but you end up\n>> with sub-pools that contain more people/fractional liquidity (this might\n>> happen naturally if CTV Pools have channels in them, so it's good to model).\n>>\n>>\n>> Point 2, on Eltoo:\n>>\n>> One point of discomfort I have with Eltoo that I think is not universal,\n>> but is shared by some others, is that non-punitive channels may not be good\n>> for high-value channels as you do want, especially in a congested\n>> blockspace world, punishments to incentivize correct behavior (otherwise\n>> cheating may look like a free option).\n>>\n>> Thus I'm reluctant to fully embrace designs which do not permit nested\n>> traditional punitive channels in favor of Eltoo, when Eltoo might not have\n>> product-market-fit for higher valued channels.\n>>\n>> If someone had a punitive-eltoo variant that would ameliorate this\n>> concern almost entirely.\n>>\n>> Cheers,\n>>\n>> Jeremy\n>>\n>>\n>>\n>> --\n>> @JeremyRubin <https://twitter.com/JeremyRubin>\n>>\n>> On Fri, Feb 18, 2022 at 3:40 PM ZmnSCPxj via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> Good morning ariard,\n>>>\n>>>\n>>> > > A statechain is really just a CoinPool hosted inside a\n>>> > >  Decker-Wattenhofer or Decker-Russell-Osuntokun construction.\n>>> >\n>>> > Note, to the best of my knowledge, how to use LN-Penalty in the\n>>> context of multi-party construction is still an unsolved issue. If an\n>>> invalidated state is published on-chain, how do you guarantee that the\n>>> punished output value is distributed \"fairly\" among the \"honest\" set of\n>>> users ? At least\n>>> > where fairness is defined as a reasonable proportion of the balances\n>>> they owned in the latest state.\n>>>\n>>> LN-Penalty I believe is what I call Poon-Dryja?\n>>>\n>>> Both Decker-Wattenhofer (has no common colloquial name) and\n>>> Decker-Russell-Osuntokun (\"eltoo\") are safe with N > 2.\n>>> The former has bad locktime tradeoffs in the unilateral close case, and\n>>> the latter requires `SIGHASH_NOINPUT`/`SIGHASH_ANYPREVOUT`.\n>>>\n>>>\n>>> > > In principle, a set of promised outputs, if the owners of those\n>>> > > outputs are peers, does not have *any* inherent order.\n>>> > > Thus, I started to think about a commitment scheme that does not\n>>> > > impose any ordering during commitment.\n>>> >\n>>> > I think we should dissociate a) *outputs publication ordering* from\n>>> the b) *spends paths ordering* itself. Even if to each spend path a output\n>>> publication is attached, the ordering constraint might not present the same\n>>> complexity.\n>>> >\n>>> > Under this distinction, are you sure that TLUV imposes an ordering on\n>>> the output publication ?\n>>>\n>>> Yes, because TLUV is based on tapleaf revelation.\n>>> Each participant gets its own unique tapleaf that lets that participant\n>>> get evicted.\n>>>\n>>> In Taproot, the recommendation is to sort the hashes of each tapleaf\n>>> before arranging them into a MAST that the Taproot address then commits to.\n>>> This sort-by-hash *is* the arbitrary ordering I refer to when I say that\n>>> TLUV imposes an arbitrary ordering.\n>>> (actually the only requirement is that pairs of scripts are\n>>> sorted-by-hash, but it is just easier to sort the whole array by hash.)\n>>>\n>>> To reveal a single participant in a TLUV-based CoinPool, you need to\n>>> reveal O(log N) hashes.\n>>> It is the O(log N) space consumption I want to avoid with `OP_EVICT`,\n>>> and I believe the reason for that O(log N) revelation is due precisely to\n>>> the arbitrary but necessary ordering.\n>>>\n>>> > > With `OP_TLUV`, however, it is possible to create an \"N-of-N With\n>>> > > Eviction\" construction.\n>>> > > When a participant in the N-of-N is offline, but the remaining\n>>> > > participants want to advance the state of the construction, they\n>>> > > instead evict the offline participant, creating a smaller N-of-N\n>>> > > where *all* participants are online, and continue operating.\n>>> >\n>>> > I think we should dissociate two types of pool spends : a) eviction by\n>>> the pool unanimity in case of irresponsive participants and b) unilateral\n>>> withdrawal by a participant because of the liquidity allocation policy. I\n>>> think the distinction is worthy, as the pool participant should be stable\n>>> and the eviction not abused.\n>>> >\n>>> > I'm not sure if TLUV enables b), at least without transforming the\n>>> unilateral withdrawal into an eviction. To ensure the TLUV operation is\n>>> correct  (spent leaf is removed, withdrawing participant point removed,\n>>> etc), the script content must be inspected by *all* the participant.\n>>> However, I believe\n>>> > knowledge of this content effectively allows you to play it out\n>>> against the pool at any time ? It's likely solvable at the price of a\n>>> CHECKSIG.\n>>>\n>>> Indeed, that distinction is important.\n>>> `OP_TLUV` (and `OP_EVICT`, which is just a redesigned `OP_TLUV`)\n>>> supports (a) but not (b).\n>>>\n>>> > `OP_EVICT`\n>>> > ----------\n>>> >\n>>> > >  * If it is `1` that simply means \"use the Taproot internal\n>>> > >    pubkey\", as is usual for `OP_CHECKSIG`.\n>>> >\n>>> > IIUC, this assumes the deployment of BIP118, where if the  public key\n>>> is a single byte 0x01, the internal pubkey is used\n>>> > for verification.\n>>>\n>>> I thought it was part of Taproot?\n>>>\n>>> >\n>>> > >  * Output indices must not be duplicated, and indicated\n>>> > >    outputs must be SegWit v1 (\"Taproot\") outputs.\n>>> >\n>>> > I think public key duplication must not be verified. If a duplicated\n>>> public key is present, the point is subtracted twice from the internal\n>>> pubkey and therefore the aggregated\n>>> > key remains unknown ? So it sounds to me safe against replay attacks.\n>>>\n>>> Ah, right.\n>>>\n>>> > >  * The public key is the input point (i.e. stack top)\n>>> > >    **MINUS** all the public keys of the indicated outputs.\n>>> >\n>>> > Can you prevent eviction abuse where one counterparty threatens to\n>>> evict everyone as all the output signatures are known among participants\n>>> and free to sum ? (at least not considering fees)\n>>>\n>>> No, I considered onchain fees as the only mechanism to avoid eviction\n>>> abuse.\n>>> The individual-evict signatures commit to fixed quantities.\n>>> The remaining change is then the only fund that can pay for onchain\n>>> fees, so a single party evicting everyone else has to pay for the eviction\n>>> of everyone else.\n>>>\n>>>\n>>> > > Suppose however that B is offline.\n>>> > > Then A, C, and D then decide to evict B.\n>>> > > To do so, they create a transaction that has an output\n>>> > > with \"B := 6\", and they reveal the `OP_EVICT` Tapscript\n>>> > > as well as sign(b, \"B := 6\").\n>>> > > This lets them change state and spend their funds without\n>>> > > B being online.\n>>> > > And B remains secure, as they cannot evict B except using\n>>> > > the pre-signed output, which B certifies as their expected\n>>> > > promised output.\n>>> >\n>>> > I think in the context of (off-chain) payment pool, OP_EVICT requires\n>>> participant cooperation *after* the state update to allow a single\n>>> participant to withdraw her funds.\n>>>\n>>> How so?\n>>>\n>>> A single participant withdrawing their funds unilaterally can do so by\n>>> evicting everyone else (and paying for those evictions, as sort of a\n>>> \"nuisance fee\").\n>>> The signatures for each per-participant-eviction can be exchanged before\n>>> the signature exchange for the Decker-Wattenhofer or\n>>> Decker-Russell-Osuntokun.\n>>>\n>>>\n>>> > > The combined fund cannot be spent except if all participants\n>>> > > agree.\n>>> >\n>>> > If all participants agree minus the evicted ones, correct ? The output\n>>> promises signatures are shared at state setup, therefore no additional\n>>> contribution from the evicted participant (I think).\n>>>\n>>> Yes.\n>>>\n>>> >\n>>> > > To prevent signature replay, each update of an updateable\n>>> > > scheme like CoinPool et al should use a different pubkey\n>>> > > for each participant for each state.\n>>> >\n>>> > I'm not even sure if it's required with OP_EVICT, as the publication\n>>> of the promised output are ultimately restrained by a signature of the\n>>> updated internal pubkey, this set of signers verify that promised output N\n>>> does bind to the published state N ?\n>>>\n>>> If the internal pubkey is reused (for example, if all participants are\n>>> online and want to change state cooperatively) then the component keys need\n>>> to be re-tweaked each time.\n>>>\n>>> The tweaking can be done with non-hardened derivation.\n>>>\n>>>\n>>> > > Its advantage is reduced number of eviction transactions,\n>>> > > as multiple evictions, plus the revival of the CoinPool,\n>>> > > can be put in a single transaction.\n>>> > > It has the disadvantage relative to `OP_TLUV` of requiring\n>>> > > point operations.\n>>> > > I have not explored completely, but my instinct suggests\n>>> > > that `OP_TLUV` use may require at least one signature\n>>> > > validation anyway.\n>>> >\n>>> > I believe you can slightly modify TLUV to make it functional for\n>>> CoinPool revival, where you want to prevent equivocation among the\n>>> remaining set of signers. Though, I'm leaning to agree that you may require\n>>> at least one signature validation  (first to restrain spend authorization\n>>> inside the pool participants, second to attach fees at broadcast-time).\n>>>\n>>> Yes, TLUV does have that advantage relative to CTV, and `OP_EVICT` is\n>>> \"just\" a redesigned `OP_TLUV`.\n>>>\n>>> In particular, I first developed my thoughts on revivable constructs\n>>> with eviction of participants here:\n>>> https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-February/003479.html\n>>>\n>>>\n>>> Regards,\n>>> ZmnSCPxj\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220219/22b64517/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-19T11:41:42",
                "message_text_only": "Good morning Billy,\n\n> >\u00a0\"fully\" punitive channels also make large value channels more dangerous from the perspective of bugs causing old\u00a0states to be published\n>\n> Wouldn't it be ideal to have the penalty be to pay for a single extra transaction fee? That way there is a penalty so cheating attempts aren't free (for someone who wants to close a channel anyway) and yet a single fee isn't going to be much of a concern in the accidental publishing case. It still perplexes me why eltoo chose no penalty at all vs a small penalty like that.\n\nNothing in the Decker-Russell-Osunstokun paper *prevents* that --- you could continue to retain per-participant versions of update+state transactions (congruent to the per-participant commitment transactions of Poon-Dryja) and have each participant hold a version that deducts the fee from their main owned funds.\nThe Decker-Russell-Osuntokun paper simply focuses on the mechanism by itself without regard to fees, on the understanding that the reader already knows fees exist and need to be paid.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-02-19T21:59:41",
                "message_text_only": "Thanks for the clarification ZmnSCPxj!\n\nOn Sat, Feb 19, 2022 at 5:41 AM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Billy,\n>\n> > > \"fully\" punitive channels also make large value channels more\n> dangerous from the perspective of bugs causing old states to be published\n> >\n> > Wouldn't it be ideal to have the penalty be to pay for a single extra\n> transaction fee? That way there is a penalty so cheating attempts aren't\n> free (for someone who wants to close a channel anyway) and yet a single fee\n> isn't going to be much of a concern in the accidental publishing case. It\n> still perplexes me why eltoo chose no penalty at all vs a small penalty\n> like that.\n>\n> Nothing in the Decker-Russell-Osunstokun paper *prevents* that --- you\n> could continue to retain per-participant versions of update+state\n> transactions (congruent to the per-participant commitment transactions of\n> Poon-Dryja) and have each participant hold a version that deducts the fee\n> from their main owned funds.\n> The Decker-Russell-Osuntokun paper simply focuses on the mechanism by\n> itself without regard to fees, on the understanding that the reader already\n> knows fees exist and need to be paid.\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220219/93018285/attachment.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2022-02-22T00:17:52",
                "message_text_only": "Hi Zeeman,\n\n> To reveal a single participant in a TLUV-based CoinPool, you need to\nreveal O(log N) hashes.\n> It is the O(log N) space consumption I want to avoid with `OP_EVICT`, and\nI believe the reason for that O(log N) revelation is due precisely to the\narbitrary but necessary ordering.\n\nAFAIU the TLUV proposal, it removes the constraint in the *outputs\npublication ordering*, once they have all been generated. The tree update\nmechanism ensure that whatever the order of dependency :\n- the spend path can't be replayed because the user leaf is removed\n- the key path can be re-used by remaining participant because the\nwithdrawing user point is removed\n\nHowever, I agree that TLUV enforces a constraint in the *spends path\nordering* for the reason you raise.\n\nI think `OP_EVICT` also removes the constraint in the *outputs publication\nordering*. AFAIU, opcode semantics you can mark as indicated any subset of\nthem. Further, it also solves the *spends paths ordering* as you don't need\nto reveal O(log N) hashes anymore.\n\nHowever, I don't think it's solving the *outputs publication ordering*\nissues with the same non-cooperative property of TLUV. TLUV doesn't assume\ncooperation among the construction participants once the Taproot tree is\nsetup. EVICT assumes cooperation among the remaining construction\nparticipants to satisfy the final CHECKSIG.\n\nSo that would be a feature difference between TLUV and EVICT, I think ?\n\n> I thought it was part of Taproot?\n\nI checked BIP342 again, *as far as I can read* (unreliable process), it\nsounds like it was proposed by BIP118 only.\n\n> No, I considered onchain fees as the only mechanism to avoid eviction\nabuse.\n\nI'm unsure about the game-theory robustness of such abuse deterrent\nmechanisms... As the pool off-chain payments are cheaper, you might break\nyour counterparty economic predictions by forcing them to go on-chain\nbefore fee spikes and thus increasing their liquidity operational costs. Or\nevicting them as a time where the fees are lower than they have paid to\nget-in.\n\n> A single participant withdrawing their funds unilaterally can do so by\nevicting everyone else (and paying for those evictions, as sort of a\n\"nuisance fee\").\n\nI see, I'm more interested in the property of a single participant\nwithdrawing their funds, without affecting the stability of the off-chain\npool and without cooperation with other users. This is currently a\nrestriction of the channel factories fault-tolerance. If one channel goes\non-chain, all the outputs are published.\n\nAntoine\n\nLe ven. 18 f\u00e9vr. 2022 \u00e0 18:39, ZmnSCPxj <ZmnSCPxj at protonmail.com> a \u00e9crit :\n\n> Good morning ariard,\n>\n>\n> > > A statechain is really just a CoinPool hosted inside a\n> > >  Decker-Wattenhofer or Decker-Russell-Osuntokun construction.\n> >\n> > Note, to the best of my knowledge, how to use LN-Penalty in the context\n> of multi-party construction is still an unsolved issue. If an invalidated\n> state is published on-chain, how do you guarantee that the punished output\n> value is distributed \"fairly\" among the \"honest\" set of users ? At least\n> > where fairness is defined as a reasonable proportion of the balances\n> they owned in the latest state.\n>\n> LN-Penalty I believe is what I call Poon-Dryja?\n>\n> Both Decker-Wattenhofer (has no common colloquial name) and\n> Decker-Russell-Osuntokun (\"eltoo\") are safe with N > 2.\n> The former has bad locktime tradeoffs in the unilateral close case, and\n> the latter requires `SIGHASH_NOINPUT`/`SIGHASH_ANYPREVOUT`.\n>\n>\n> > > In principle, a set of promised outputs, if the owners of those\n> > > outputs are peers, does not have *any* inherent order.\n> > > Thus, I started to think about a commitment scheme that does not\n> > > impose any ordering during commitment.\n> >\n> > I think we should dissociate a) *outputs publication ordering* from the\n> b) *spends paths ordering* itself. Even if to each spend path a output\n> publication is attached, the ordering constraint might not present the same\n> complexity.\n> >\n> > Under this distinction, are you sure that TLUV imposes an ordering on\n> the output publication ?\n>\n> Yes, because TLUV is based on tapleaf revelation.\n> Each participant gets its own unique tapleaf that lets that participant\n> get evicted.\n>\n> In Taproot, the recommendation is to sort the hashes of each tapleaf\n> before arranging them into a MAST that the Taproot address then commits to.\n> This sort-by-hash *is* the arbitrary ordering I refer to when I say that\n> TLUV imposes an arbitrary ordering.\n> (actually the only requirement is that pairs of scripts are\n> sorted-by-hash, but it is just easier to sort the whole array by hash.)\n>\n> To reveal a single participant in a TLUV-based CoinPool, you need to\n> reveal O(log N) hashes.\n> It is the O(log N) space consumption I want to avoid with `OP_EVICT`, and\n> I believe the reason for that O(log N) revelation is due precisely to the\n> arbitrary but necessary ordering.\n>\n> > > With `OP_TLUV`, however, it is possible to create an \"N-of-N With\n> > > Eviction\" construction.\n> > > When a participant in the N-of-N is offline, but the remaining\n> > > participants want to advance the state of the construction, they\n> > > instead evict the offline participant, creating a smaller N-of-N\n> > > where *all* participants are online, and continue operating.\n> >\n> > I think we should dissociate two types of pool spends : a) eviction by\n> the pool unanimity in case of irresponsive participants and b) unilateral\n> withdrawal by a participant because of the liquidity allocation policy. I\n> think the distinction is worthy, as the pool participant should be stable\n> and the eviction not abused.\n> >\n> > I'm not sure if TLUV enables b), at least without transforming the\n> unilateral withdrawal into an eviction. To ensure the TLUV operation is\n> correct  (spent leaf is removed, withdrawing participant point removed,\n> etc), the script content must be inspected by *all* the participant.\n> However, I believe\n> > knowledge of this content effectively allows you to play it out against\n> the pool at any time ? It's likely solvable at the price of a CHECKSIG.\n>\n> Indeed, that distinction is important.\n> `OP_TLUV` (and `OP_EVICT`, which is just a redesigned `OP_TLUV`) supports\n> (a) but not (b).\n>\n> > `OP_EVICT`\n> > ----------\n> >\n> > >  * If it is `1` that simply means \"use the Taproot internal\n> > >    pubkey\", as is usual for `OP_CHECKSIG`.\n> >\n> > IIUC, this assumes the deployment of BIP118, where if the  public key is\n> a single byte 0x01, the internal pubkey is used\n> > for verification.\n>\n> I thought it was part of Taproot?\n>\n> >\n> > >  * Output indices must not be duplicated, and indicated\n> > >    outputs must be SegWit v1 (\"Taproot\") outputs.\n> >\n> > I think public key duplication must not be verified. If a duplicated\n> public key is present, the point is subtracted twice from the internal\n> pubkey and therefore the aggregated\n> > key remains unknown ? So it sounds to me safe against replay attacks.\n>\n> Ah, right.\n>\n> > >  * The public key is the input point (i.e. stack top)\n> > >    **MINUS** all the public keys of the indicated outputs.\n> >\n> > Can you prevent eviction abuse where one counterparty threatens to evict\n> everyone as all the output signatures are known among participants and free\n> to sum ? (at least not considering fees)\n>\n> No, I considered onchain fees as the only mechanism to avoid eviction\n> abuse.\n> The individual-evict signatures commit to fixed quantities.\n> The remaining change is then the only fund that can pay for onchain fees,\n> so a single party evicting everyone else has to pay for the eviction of\n> everyone else.\n>\n>\n> > > Suppose however that B is offline.\n> > > Then A, C, and D then decide to evict B.\n> > > To do so, they create a transaction that has an output\n> > > with \"B := 6\", and they reveal the `OP_EVICT` Tapscript\n> > > as well as sign(b, \"B := 6\").\n> > > This lets them change state and spend their funds without\n> > > B being online.\n> > > And B remains secure, as they cannot evict B except using\n> > > the pre-signed output, which B certifies as their expected\n> > > promised output.\n> >\n> > I think in the context of (off-chain) payment pool, OP_EVICT requires\n> participant cooperation *after* the state update to allow a single\n> participant to withdraw her funds.\n>\n> How so?\n>\n> A single participant withdrawing their funds unilaterally can do so by\n> evicting everyone else (and paying for those evictions, as sort of a\n> \"nuisance fee\").\n> The signatures for each per-participant-eviction can be exchanged before\n> the signature exchange for the Decker-Wattenhofer or\n> Decker-Russell-Osuntokun.\n>\n>\n> > > The combined fund cannot be spent except if all participants\n> > > agree.\n> >\n> > If all participants agree minus the evicted ones, correct ? The output\n> promises signatures are shared at state setup, therefore no additional\n> contribution from the evicted participant (I think).\n>\n> Yes.\n>\n> >\n> > > To prevent signature replay, each update of an updateable\n> > > scheme like CoinPool et al should use a different pubkey\n> > > for each participant for each state.\n> >\n> > I'm not even sure if it's required with OP_EVICT, as the publication of\n> the promised output are ultimately restrained by a signature of the updated\n> internal pubkey, this set of signers verify that promised output N does\n> bind to the published state N ?\n>\n> If the internal pubkey is reused (for example, if all participants are\n> online and want to change state cooperatively) then the component keys need\n> to be re-tweaked each time.\n>\n> The tweaking can be done with non-hardened derivation.\n>\n>\n> > > Its advantage is reduced number of eviction transactions,\n> > > as multiple evictions, plus the revival of the CoinPool,\n> > > can be put in a single transaction.\n> > > It has the disadvantage relative to `OP_TLUV` of requiring\n> > > point operations.\n> > > I have not explored completely, but my instinct suggests\n> > > that `OP_TLUV` use may require at least one signature\n> > > validation anyway.\n> >\n> > I believe you can slightly modify TLUV to make it functional for\n> CoinPool revival, where you want to prevent equivocation among the\n> remaining set of signers. Though, I'm leaning to agree that you may require\n> at least one signature validation  (first to restrain spend authorization\n> inside the pool participants, second to attach fees at broadcast-time).\n>\n> Yes, TLUV does have that advantage relative to CTV, and `OP_EVICT` is\n> \"just\" a redesigned `OP_TLUV`.\n>\n> In particular, I first developed my thoughts on revivable constructs with\n> eviction of participants here:\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-February/003479.html\n>\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220221/6bf7adec/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-23T11:42:54",
                "message_text_only": "Good morning Antoine,\n\n> TLUV doesn't assume cooperation among the construction participants once the Taproot tree is setup. EVICT assumes cooperation among the remaining construction participants to satisfy the final CHECKSIG.\n>\n> So that would be a feature difference between TLUV and EVICT, I think ?\n\n`OP_TLUV` leaves the transaction output with the remaining Tapleaves intact, and, optionally, with a point subtracted from Taproot internal pubkey.\n\nIn order to *truly* revive the construct, you need a separate transaction that spends that change output, and puts it back into a new construct.\n\nSee: https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-February/003479.html\nI describe how this works.\n\nThat `OP_EVICT` does another `CHECKSIG` simply cuts through the separate transaction that `OP_TLUV` would require in order to revive the construct.\n\n> > I thought it was part of Taproot?\n>\n> I checked BIP342 again, *as far as I can read* (unreliable process), it sounds like it was proposed by BIP118 only.\n\n*shrug* Okay!\n\n> > A single participant withdrawing their funds unilaterally can do so by evicting everyone else (and paying for those evictions, as sort of a \"nuisance fee\").\n>\n> I see, I'm more interested in the property of a single participant withdrawing their funds, without affecting the stability of the off-chain pool and without cooperation with other users. This is currently a restriction of the channel factories fault-tolerance. If one channel goes on-chain, all the outputs are published.\n\nSee also: https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-February/003479.html\n\nGenerally, the reason for a channel to go *onchain*, instead of just being removed inside the channel factory and its funds redistributed elsewhere, is that an HTLC/PTLC is about to time out.\nThe blockchain is really the only entity that can reliably enforce timeouts.\n\nAnd, from the above link:\n\n> * If a channel has an HTLC/PTLC time out:\n>   * If the participant to whom the HTLC/PTLC is offered is\n>     offline, that may very well be a signal that it is unlikely\n>     to come online soon.\n>     The participant has strong incentives to come online before\n>     the channel is forcibly closed due to the HTLC/PTLC timeout,\n>     so if it is not coming online, something is very wrong with\n>     that participant and we should really evict the participant.\n>   * If the participant to whom the HTLC/PTLC is offered is\n>     online, then it is not behaving properly and we should\n>     really evict the participant.\n\nNote the term \"evict\" as well --- the remaining participants that are presumably still behaving correctly (i.e. not letting HTLC/PTLC time out) evict the participants that *are*, and that is what `OP_EVICT` does, as its name suggests.\n\nIndeed, I came up with `OP_EVICT` *after* musing the above link.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "`OP_EVICT`: An Alternative to `OP_TAPLEAFUPDATEVERIFY`",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Antoine Riard",
                "Billy Tetrud",
                "ZmnSCPxj",
                "Erik Aronesty",
                "Jonas Nick",
                "Jeremy Rubin",
                "Greg Sanders"
            ],
            "messages_count": 16,
            "total_messages_chars_count": 140800
        }
    },
    {
        "title": "[bitcoin-dev] Stumbling into a contentious soft fork activation attempt",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2022-02-18T23:41:02",
                "message_text_only": "On Tue, Jan 18, 2022 at 02:57:30AM +0100, Prayank wrote:\n> Hi Peter,\n> \n> > that current lacks compelling use-cases clearly beneficial to all users\n> \n> All the use cases shared in below links look compelling enough to me and we can do anything that a programmer could think of using such restrictions:\n> \n>  https://utxos.org/uses/\n> \n> https://rubin.io/archive/\n\nAgain, what I said was \"compelling use-cases _clearly_ beneficial to _all_\nusers\", not just a small subset. I neither think the use-cases in those links\nare clearly compelling in the current form, and they of course, don't benefit\nall users. Indeed, the Drivechains use-case arguably *harms* all users, as\nDrivechains is arguably harmful to the security of Bitcoin as a whole.\nSimilarly, the various new uses for on-chain transactions mentioned as a\nuse-case arguably harms all existing users by competing for scarce blockchain\nspace - note how ETH has quite high on chain fees for basic transactions,\nbecause there are so many use-cases where the per-tx value can afford much\nhigher fees. That kind of expansion of use-case also arguably harms Bitcoin as\na whole by providing more fuel for a future contentious blocksize debate.\n\nBitcoin is an almost $1 trillion dollar system. We have to very carefully weigh\nthe benefits of making core consensus changes to that system against the risks.\nBoth for each proposal in isolation, as well as the precedent making that\nchange sets.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220218/6499c346/attachment.sig>"
            },
            {
                "author": "Erik Aronesty",
                "date": "2022-02-20T18:35:15",
                "message_text_only": "> note how ETH has quite high on chain fees for basic transactions,\n> because there are so many use-cases where the per-tx value can afford much\n> higher fees. That kind of expansion of use-case also arguably harms\nBitcoin as\n> a whole by providing more fuel for a future contentious blocksize debate.\n\ni second this argument\n\nideally, all extensions should be explicit use cases, not generic/implicit\nlayers that can be exploited for unknown and possibly harmful use cases\n\nalso timing is critical for all bitcoin innovation.   look at how lightning\nate up fees\n\nto keep bitcoin stable, we can't \"scale\" too quickly either\n\ni'm a fan of, eventually (timing is critical), a lightning-compatible\nmimblewible+dandelion on-chain soft fork can reduce tx size, move us from\nl2 to l3, vastly improve privacy, and get more small transactions off-chain.\n\nbut it probably shouldn't be released for another 2 years\n\n\nOn Fri, Feb 18, 2022 at 6:41 PM Peter Todd via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Tue, Jan 18, 2022 at 02:57:30AM +0100, Prayank wrote:\n> > Hi Peter,\n> >\n> > > that current lacks compelling use-cases clearly beneficial to all users\n> >\n> > All the use cases shared in below links look compelling enough to me and\n> we can do anything that a programmer could think of using such restrictions:\n> >\n> >  https://utxos.org/uses/\n> >\n> > https://rubin.io/archive/\n>\n> Again, what I said was \"compelling use-cases _clearly_ beneficial to _all_\n> users\", not just a small subset. I neither think the use-cases in those\n> links\n> are clearly compelling in the current form, and they of course, don't\n> benefit\n> all users. Indeed, the Drivechains use-case arguably *harms* all users, as\n> Drivechains is arguably harmful to the security of Bitcoin as a whole.\n> Similarly, the various new uses for on-chain transactions mentioned as a\n> use-case arguably harms all existing users by competing for scarce\n> blockchain\n> space - note how ETH has quite high on chain fees for basic transactions,\n> because there are so many use-cases where the per-tx value can afford much\n> higher fees. That kind of expansion of use-case also arguably harms\n> Bitcoin as\n> a whole by providing more fuel for a future contentious blocksize debate.\n>\n> Bitcoin is an almost $1 trillion dollar system. We have to very carefully\n> weigh\n> the benefits of making core consensus changes to that system against the\n> risks.\n> Both for each proposal in isolation, as well as the precedent making that\n> change sets.\n>\n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220220/13f2c635/attachment.html>"
            },
            {
                "author": "Prayank",
                "date": "2022-02-21T03:03:07",
                "message_text_only": "> note how ETH has quite high on chain fees for basic transactions,> because there are so many use-cases where the per-tx value can afford much> higher fees. That kind of expansion of use-case also arguably harms Bitcoin as> a whole by providing more fuel for a future contentious blocksize debate.\n>i second this argument\n\nI disagree with this argument, Satoshi won't agree with it either if still active and it make no sense. Fees will be the incentives for miners as subsidy decreases after every 210,000 blocks and it will depend on demand for block space.\n\nThere is nothing harmful in it just because something similar is happening in an altcoin which has several other issues. Example: if a user has to pay fees with 100 sat/vbyte fee rate to open and close channels it will be good for Bitcoin in long term.\n\nIf this is the reason to stop/delay improvements in bitcoin, maybe it applies for Taproot as well although I don't remember reading such things in your posts or maybe missed it.\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n\n\n\nFeb 21, 2022, 00:05 by erik at q32.com:\n\n> > note how ETH has quite high on chain fees for basic transactions,\n> > because there are so many use-cases where the per-tx value can afford much\n> > higher fees. That kind of expansion of use-case also arguably harms Bitcoin as\n> > a whole by providing more fuel for a future contentious blocksize debate.\n>\n> i second this argument\n>\n> ideally, all extensions should be explicit use cases, not generic/implicit layers that can be exploited for unknown and possibly harmful use cases\n>\n> also timing is critical for all bitcoin innovation.\u00a0 \u00a0look at how lightning ate up fees\n>\n> to keep bitcoin stable, we can't \"scale\" too quickly either\n>\n> i'm a fan of, eventually (timing is critical), a lightning-compatible mimblewible+dandelion\u00a0on-chain soft fork can reduce tx size, move us from l2 to l3, vastly improve privacy, and get more small transactions off-chain.\n>\n> but it probably shouldn't be released for another 2 years\n>\n>\n> On Fri, Feb 18, 2022 at 6:41 PM Peter Todd via bitcoin-dev <> bitcoin-dev at lists.linuxfoundation.org> > wrote:\n>\n>> On Tue, Jan 18, 2022 at 02:57:30AM +0100, Prayank wrote:\n>>  > Hi Peter,\n>>  > \n>>  > > that current lacks compelling use-cases clearly beneficial to all users\n>>  > \n>>  > All the use cases shared in below links look compelling enough to me and we can do anything that a programmer could think of using such restrictions:\n>>  > \n>>  >\u00a0 >> https://utxos.org/uses/\n>>  > \n>>  > >> https://rubin.io/archive/\n>>  \n>>  Again, what I said was \"compelling use-cases _clearly_ beneficial to _all_\n>>  users\", not just a small subset. I neither think the use-cases in those links\n>>  are clearly compelling in the current form, and they of course, don't benefit\n>>  all users. Indeed, the Drivechains use-case arguably *harms* all users, as\n>>  Drivechains is arguably harmful to the security of Bitcoin as a whole.\n>>  Similarly, the various new uses for on-chain transactions mentioned as a\n>>  use-case arguably harms all existing users by competing for scarce blockchain\n>>  space - note how ETH has quite high on chain fees for basic transactions,\n>>  because there are so many use-cases where the per-tx value can afford much\n>>  higher fees. That kind of expansion of use-case also arguably harms Bitcoin as\n>>  a whole by providing more fuel for a future contentious blocksize debate.\n>>  \n>>  Bitcoin is an almost $1 trillion dollar system. We have to very carefully weigh\n>>  the benefits of making core consensus changes to that system against the risks.\n>>  Both for each proposal in isolation, as well as the precedent making that\n>>  change sets.\n>>  \n>>  -- \n>>  >> https://petertodd.org>>  'peter'[:-1]@>> petertodd.org <http://petertodd.org>\n>>  _______________________________________________\n>>  bitcoin-dev mailing list\n>>  >> bitcoin-dev at lists.linuxfoundation.org\n>>  >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220221/45889d89/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-21T09:02:06",
                "message_text_only": "Good morning Prayank,\n\n(offlist)\n\n>  Satoshi\n\nI object to the invocation of Satoshi here, and in general.\nIf Satoshi wants to participate in Bitcoin development today, he can speak for himself.\nIf Satoshi refuses to participate in Bitcoin development today, who cares what his opinion is?\nSatoshi is dead, long live Bitcoin.\n\n\nAside from that, I am otherwise thinking about the various arguments being presented.\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-21T09:11:11",
                "message_text_only": "> Good morning Prayank,\n>\n> (offlist)\n\n<facepalm>\n\nMy apologies.\nI pushed the wrong button, I should have pressed \"Reply\" and not \"Reply All\".\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Prayank",
                "date": "2022-02-21T09:48:26",
                "message_text_only": "Goog morning ZmnSCPxj,\n\nContext: https://bitcointalk.org/index.php?topic=48.msg329#msg329\n\nMaybe I should have rephrased it and quote Satoshi. I agree I should not speak for others and it was not my intention in the email.\n\n> If Satoshi refuses to participate in Bitcoin development today, who cares what his opinion is?\n\nI care about the opinions especially if consensus rules are not changed and remain same as far as subsidy is concerned.\n\n> Satoshi is dead, long live Bitcoin.\n\nI object to such assumptions about the founder of Bitcoin. Satoshi is more than a pseudonym and will stay alive forever.\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n\n\n\nFeb 21, 2022, 14:32 by ZmnSCPxj at protonmail.com:\n\n> Good morning Prayank,\n>\n> (offlist)\n>\n>> Satoshi\n>>\n>\n> I object to the invocation of Satoshi here, and in general.\n> If Satoshi wants to participate in Bitcoin development today, he can speak for himself.\n> If Satoshi refuses to participate in Bitcoin development today, who cares what his opinion is?\n> Satoshi is dead, long live Bitcoin.\n>\n>\n> Aside from that, I am otherwise thinking about the various arguments being presented.\n>\n>\n> Regards,\n> ZmnSCPxj\n>\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220221/8542f685/attachment.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-02-22T12:57:15",
                "message_text_only": "> look at how lightning ate up fees to keep bitcoin stable, we can't\n\"scale\" too quickly either\n\nI strongly disagree with this. We should be scaling Bitcoin as fast as we\ncan. There is no reason to delay scaling for the purposes of keeping fees\nhigh. If we need fees to be higher, we can lower the block size or increase\nthe default minimum relay fee rate.\n\nAlso, the idea that use of the LN is there primary cause of recent low fees\nis highly dubious.\n\n> the various new uses for on-chain transactions mentioned as a use-case\narguably harms all existing users by competing for scarce blockchain space\n\nReminds me of that old saying, \"nobody goes there anymore, it's too\ncrowded\". ; )\n\n\nOn Mon, Feb 21, 2022, 03:54 Prayank via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Goog morning ZmnSCPxj,\n>\n> Context: https://bitcointalk.org/index.php?topic=48.msg329#msg329\n>\n> Maybe I should have rephrased it and quote Satoshi. I agree I should not\n> speak for others and it was not my intention in the email.\n>\n> > If Satoshi refuses to participate in Bitcoin development today, who\n> cares what his opinion is?\n>\n> I care about the opinions especially if consensus rules are not changed\n> and remain same as far as subsidy is concerned.\n>\n> > Satoshi is dead, long live Bitcoin.\n>\n> I object to such assumptions about the founder of Bitcoin. Satoshi is more\n> than a pseudonym and will stay alive forever.\n>\n> --\n> Prayank\n>\n> A3B1 E430 2298 178F\n>\n>\n>\n> Feb 21, 2022, 14:32 by ZmnSCPxj at protonmail.com:\n>\n> Good morning Prayank,\n>\n> (offlist)\n>\n> Satoshi\n>\n>\n> I object to the invocation of Satoshi here, and in general.\n> If Satoshi wants to participate in Bitcoin development today, he can speak\n> for himself.\n> If Satoshi refuses to participate in Bitcoin development today, who cares\n> what his opinion is?\n> Satoshi is dead, long live Bitcoin.\n>\n>\n> Aside from that, I am otherwise thinking about the various arguments being\n> presented.\n>\n>\n> Regards,\n> ZmnSCPxj\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220222/313a76dc/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-21T09:09:09",
                "message_text_only": "Good morning,\n\n\n> If this is the reason to stop/delay improvements in bitcoin, maybe it applies for Taproot as well although I don't remember reading such things in your posts or maybe missed it.\n\nPerhaps a thing to note, is that if it allows us to move some activity off-chain, and reduce activity on the blockchain, then the increase in functionality does *not* translate to a requirement of block size increase.\n\nSo for example:\n\n* Taproot, by allowing the below improvements, is good:\n  * Schnorr multisignatures that allow multiple users to publish a single signature, reducing block size usage for large participant sets.\n  * MAST, which allows eliding branches of complicated SCRIPTs that are not executed, reducing block size usage for complex contracts.\n* `SIGHASH_ANYPREVOUT`, by enabling an offchain updateable multiparty (N > 2) cryptocurrency system (Decker-Russell-Osuntokun), is also good, as it allows us to make channel factories without having to suffer the bad tradeoffs of Decker-Wattenhofer.\n* `OP_CTV`, by enabling commit-to-unpublished-promised-outputs, is also good, as it allows opportunities for transactional cut-through without having to publish promised outputs *right now*.\n\nSo I do not think the argument should really object to any of the above, either --- all these improvements increase the functionality of Bitcoin, but also allow opportunities to use the blockchain as judge+jury+executioner instead of noisy marketplace.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Stumbling into a contentious soft fork activation attempt",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Prayank",
                "Peter Todd",
                "ZmnSCPxj",
                "Erik Aronesty",
                "Billy Tetrud"
            ],
            "messages_count": 8,
            "total_messages_chars_count": 14650
        }
    },
    {
        "title": "[bitcoin-dev] PathCoin",
        "thread_messages": [
            {
                "author": "AdamISZ",
                "date": "2022-02-20T18:26:30",
                "message_text_only": "An update, after some feedback, and me using the odd hour here and there to try to push this idea forward a bit:\n\n1. I think (though I'm not 100% certain) that you can get rid of the fidelity bond requirement entirely with an eltoo/D-R-O type mechanism, assuming APOAS.\n2. With a relaxation of online-ness requirement (see below) I think you can jump steps in the path.\n\n(Before I get into all this you might reasonably ask - well, with eltoo mechanisms we can just do a very large multiparty channel no? And not have this severe utxo denomination issue, nor fixed paths? All true, so that's probably way more interesting, but in this, we're looking for one property in particular - ability to pass coins without *anyone* needing to sign with a hot wallet - let alone *everyone* needing to sign.)\n\n1. No fidelity bond:\n\nThe first of these two is more technically hairy, but, setting the stage again:\n\nSay 100 keyholders in initial setup, A1 .. A100 (larger numbers this time to keep scale more realistically in mind). A1 funds a script U which is a musig key prepared for this as N of N between these 100.\n\nAs before, they need 100 separate tapscript leafs (in case we need different keysets for each, but I think we don't and it's inefficient, h/t Jeremy Rubin for pointing that out) or more simply, 100 separate Musig2 protocol runs, in each one they are signing a tx to each of them individually, but not completing the run, i.e. only certain parties share their signature partials. Who shares what is shown in the tables in the gist linked below (i.e. this is not changing that part of the mechanism) (there would be around 5000 signature partials shared in the setup). As before, adaptors for individual partial sigs will be shared by A1, A2 etc when the pass on the coin from An to An+1.\n\nBut the difference now is that they do not post a fidelity bond. So what does this adaptor, verifiably, enforce, if the \"wrong\" signature is used? Suggestion here is: the destination each party A_x is signing the coin over is not just exclusive ownership, but (A_x + TL OR CTV(back to script U) + T_x). Translating the rough pseudo-script: if A_x has transferred the coin but then 'illegally' broadcasts, they, having revealed the adaptor t_x verifiably connected to T_x, will allow the coin spent from U to be passed directly back into U. Using APOAS for the signatures, as with eltoo, would mean that the existing prepared set of signatures for the initial funding of U, still applies. I wave hands here about btc amount being fixed, and fees - I presume that SIGHASH_SINGLE, as in the eltoo paper (or?), handles all that - and there may need to be finesse there to create economic disincentive to wrongly publish.\nGoing further with the eltoo mechanism - for this to work we would similarly use a state number enforcing ordering (and hence APOAS). The valid-by-protocol current owner of the pathcoin would still be the only one able to successfully spend it after the miscreant action led to no successful theft. I presume the same nLockTime trick works.\n\nI may have got some or all of that wrong, but if it's correct in general outline, it trades off: timelocked ownership of the pathcoin (previously timelocked ownership of the fidelity bond), but it means you don't have to post collateral upfront, which was *one* factor that made the thing hugely impractical at any scale. So it's barely a tradeoff and seems a huge win, if functional.\n\nImportant caveat: though it would remove the collateral-posting requirement, it wouldn't remove the timelock aspect: so we're still only able to operate this in a pre-agreed time window.\n\n2. Jumping over hops:\n\nThis is more of an actual tradeoff, but I can imagine it being necessary: For a fixed path of 100 users we would clearly get far too little utility from a fixed path between them. The factorial blowup has been noted many times. What isn't yet clear to me is: if you had fairly long paths like this and were able to jump over, i.e. in A, B, C, D, E, A could pay anyone, B could pay (C, D, E), C could pay (D, E) etc., if this extra flexibility were combined with cleverly arranged lists of other paths, might we have a somewhat useful system? Let me suggest a way that it's *kind of possible* to do it, and leave the combinatorial discussion for later:\n\nNothing fancy: just notice, let's say A87 wants to receive the coin from a pseudonymous user AX who is not specifying their position in the ordering (but they have to be X < 87): what A87 needs is a full set of revocations of all owners before 87, along with a pre-authorization of all receivers post-87. In some logical sense that is \"coming from\" A86, because A86 has to be included in that set, but it needn't literally be A86 doing the paying, I'd claim: suppose it's actually A85. A85 only needs to get A86's agreement to make the payment, i.e. A86 can voluntarily revoke their right to this pathcoin, as they never owned it - they can send, to A85, the set: adaptor sigma'_86 (that reveals t_86 if the real partial sig, sigma_86_86 were revealed), and their authorizations to spend it forwards (basically sigma_86_87, sigma_86_88 .. sigma_86_100), and A85 can combine that with the rest of the set needed to pass on to A87. The recipient, A87, needn't even know which participant earlier in the path, sent to them (at least, I think so, but if that's not true it doesn't make it useless).\n\nThe problem here is obvious: we were hoping that Bob could pay Carol (and etc etc) without anything but a transfer of info from Bob to Carol; nobody else should have to be involved. But I think we could at least conceive that software running this protocol could stay online - it wouldn't, notice, need to be running a hot wallet, because we're talking about the case of a user not holding any funds, just some pre-prepared signature data. If a request comes in to A86 to use it, it could accept and then just forget about this particular pathcoin (one would imagine it maintaining state for many of them at once, of course). I'd note that unfortunately I don't think outsourcing makes sense here: a recipient can only truly know that they can't receive the coin if they themselves are directly sending out the revocation data (adaptor, etc.). Perhaps arguable; if outsourced the scheme seems a lot more practical.\n\nA failure of this mechanism due to offline-ness is unfortunate because we lose hopping functionality, but at least it's not a security risk. Maybe just try another pathcoin.\n\n3.? Moving to new keyholders?\n\nBut there wasn't a 3 :) I'm just not sure about this, but is there a way to have one recipient in A1..A100 send out to a new pathcoin group **off-chain** (B1..B100 say), in a way that makes sense and is useful? I *think* it would require the 'baggage' of the ~ 1 MB of data from the A100 set's payment history be forwarded for each payment in the new group. (What's the real size? I think: max 100 adaptors, plus 100 scriptpubkeys (representing revocation), max 10K partial signatures but probably a lot less, so < 1MB from the whole thing I believe). Also nice is that the monitoring on chain of the whole A history is just one utxo, the one that funded U initially. *Not* so nice, is that the original timelock carries over to the new group, who would have to use a shorter one ...\n\nNot sure, but I might update and change the gist to include this new line of thinking, in particular in (1) above .. at least if it makes sense :)\n\nRegards,\nwaxwing / AdamISZ\n\n\nSent with ProtonMail Secure Email.\n\n------- Original Message -------\n\nOn Monday, January 24th, 2022 at 14:43, AdamISZ via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hello list,\n>\n> I took the time to write up this rather out-there idea:\n>\n> Imagine you wanted to send a coin just like email, i.e. just transfer data to the counterparty.\n>\n> Clearly this is in general entirely impossible; but with what restrictions and assumptions could you create a toy version of it?\n>\n> See this gist for a detailed build up of the idea:\n>\n> https://gist.github.com/AdamISZ/b462838cbc8cc06aae0c15610502e4da\n>\n> Basically: using signature adaptors and CTV or a similar covenant, you could create a fully trustless transfer of control of a utxo from one party to another with no interaction with the rest of the group, at the time of transfer (modulo of course lots and lots of one-time setup).\n>\n> The limitations are extreme and as you'd imagine. In the gist I feel like I got round one of them, but not the others.\n>\n> (I very briefly mention comparison to e.g. statechains or payment pools; they are making other tradeoffs against the 'digital cash' type of goal. There is no claim that this 'pathcoin' idea is even viable yet, let alone better than those ideas).\n>\n> Publishing this because I feel like it's the kind of thing imaginative minds like the ones here, may be able to develop further. Possibly!\n>\n> waxwing / AdamISZ\n>\n> bitcoin-dev mailing list\n>\n> bitcoin-dev at lists.linuxfoundation.org\n>\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "PathCoin",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "AdamISZ"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 9064
        }
    },
    {
        "title": "[bitcoin-dev] A Dive into CoinPool : Bitcoin Balances for Billions",
        "thread_messages": [
            {
                "author": "Antoine Riard",
                "date": "2022-02-21T13:16:06",
                "message_text_only": "Hi,\n\nWe (Gleb+ me) would like to present the following of our research on\npayment pools [0].\n\nAbstract:\n\nCoinPool is a new multi-party construction to improve Bitcoin onboarding\nand transactional scaling by orders of magnitude.\nCoinPool allows many users to share a UTXO and make instant off-chain\ntransfers inside the UTXO while allowing withdrawals at any time without\npermission from other users.\nIn-pool accounts can be used for advanced protocols (e.g., payment\nchannels). Connecting them to other CoinPool instances, or even to the\nLightning Network, makes in-pool funds highly liquid.\nCoinPool construction relies on SIGHASH_GROUP, SIGHASH_ANYPREVOUT and\nOP_MERKLESUB changes to Bitcoin. It also assumes a high degree of\ninteractivity between pool participants.\n\nCoinPool provides an interesting alternative to the LN: it allows locking\nmore people in a single UTXO and potentially lets them stay in the same\nUTXO for longer. In the end, this expands Bitcoin payment throughput, via\nbetter use of the block space.\nCoinPool accounts can be also plugged into the LN, making them\ncomplementary and benefiting from each other.\n\nCoinPool explores what can be achieved with covenants, lately explored by a\nfew of us. It is exploration \u201cin-depth\u201d: what kind of protocol could be\nachieved by Merkle tree subtraction check.\nWe hope this work can inform thinking on future softforks.\n\nWe think the 7.9 billion people could be distributed across 1000-sized\nCoinPool instances. Assuming perfect cooperation among the participant, a\nliquidity exhaustion rate of 6 months and a refulfillment footprint of 100\ninputs (at 106 bytes each), 167 GB of blockchain space would be required by\nyear to enable everyone in the world to transact on Bitcoin in a\nnon-custodial fashion, unless one order of magnitude beyond the current\nblock size. By fine-tuning the pools off-chain sustainability  parameters\nfurther, it is realistic to think to satisfy current full-node validation\nrequirements, thus preserving the decentralization of the network.\n.\nWe're eager to hear everyone's feedback, what we missed, what can be\nimproved. We hope the ideas presented sound interesting to the community.\nIf so, we acknowledge it will likely take a decade of patient engineering\nbefore we see mature payment pools in the wild.\n\nThe paper is available here :\nhttps://coinpool.dev/v0.1.pdf [1]\n\nThe OP_MERKLESUB and SIGASH_GROUP BIPs are available here:\nhttps://github.com/ariard/bips/blob/coinpool-bips/bip-group.mediawiki\nhttps://github.com/ariard/bips/blob/coinpool-bips/bip-merklesub.mediawiki\n\nThe code for the pool withdraw tree is available here:\nhttps://github.com/ariard/bitcoin/tree/2022-02-coinpool-withdraw\n\nThe transaction/scripts formats for the CoinPool transaction are available\nhere:\nhttps://gist.github.com/ariard/713ce396281163337c175d9122163e8f\n\nSincerely,\nGleb & Antoine\n\nPS: Thanks to the reviewers.\n\n[0]\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-June/017964.html\n[1] Always have a backup plan in Bitcoin :\nhttps://github.com/coinpool-dev/paper/blob/master/coinpool-v0.1.pdf\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220221/f231c535/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "A Dive into CoinPool : Bitcoin Balances for Billions",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Antoine Riard"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 3266
        }
    },
    {
        "title": "[bitcoin-dev] BIP-119 CTV Meeting #4 Draft Agenda for Tuesday February 22nd at 12:00 PT",
        "thread_messages": [
            {
                "author": "Jeremy Rubin",
                "date": "2022-02-22T03:36:05",
                "message_text_only": "Hi All,\n\nApologies for the late posting of the agenda. The 4th CTV meeting will be\nheld tomorrow at 12:00 PT in ##ctv-bip-review in Libera.chat.\n\nTomorrow the conversation will be slightly more tutorial focused. If you\nhave time in advance of the meeting, it might be good to do some of this in\nadvance.\n\n1) Discussion: What is the goal of Signet? (20 minutes)\n    - Do we have a \"decision function\" of observations from a test network?\n    - What applications should be prototyped/fleshed out?\n    - What level of fleshed out matters?\n    - Should we add other experiments in the mix on this net, like\nAPO/Sponsors?\n    - Should we get e.g. lightning working on this signet?\n2) Connecting to CTV Signet Tutorial (10 mins)\n\nWe'll make sure everyone who wants to be on it is on it & debug any issues.\n\n*Ahead of Meeting: Build this\nbranch https://github.com/JeremyRubin/bitcoin/tree/checktemplateverify-signet-23.0-alpha\n<https://github.com/JeremyRubin/bitcoin/tree/checktemplateverify-signet-23.0-alpha>*\n\nConnect to:\n```\n[signet]\nsignetchallenge=512102946e8ba8eca597194e7ed90377d9bbebc5d17a9609ab3e35e706612ee882759351ae\naddnode=50.18.75.225\n```\n\n3) Receiving Coins / Sending Coins (5 mins)\nThere's now a faucet for this signet: https://faucet.ctvsignet.com\nAnd also an explorer: https://explorer.ctvsignet.com\n\n4) Sapio tutorial (25 minutes)\n\n*Ahead of meeting, if you have time: skim https://learn.sapio-lang.org\n<https://learn.sapio-lang.org> & download/build the sapio cli & plugin\nexamples*\n\nWe'll try to get everyone building and sending a basic application (e.g.\ncongestion control tree or vault) on the signet (instructions to be posted\nbefore meeting).\n\nWe won't use Sapio Studio, just the Sapio CLI.\n\n5) Sapio Q&A (30 mins)\n\nAfter some experience playing with Sapio, more general discussion about the\nproject and what it may accomplish\n\n6) General Discussion (30 minutes)\n\n\nBest,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220221/79265c98/attachment.html>"
            },
            {
                "author": "Jeremy Rubin",
                "date": "2022-02-22T18:05:21",
                "message_text_only": "Hi Devs,\n\nAs promised, a Sapio Tutorial. In this tutorial we'll walk through how to\nuse the Sapio CLI to generate contracts and play with them on the network.\nWe'll use a congestion control tree because it's very simple! We will walk\nthrough this step-by-step during the meeting today.\n\n-1. Install JQ (json manipulating tool) if you don't have it / other things\nneeded to run a bitcoin node.\n0. Set up a node as described above.  You'll likely want settings like this\nin your bitcoin.conf too:\n[signet]\n# generate this yourself\n\nrpcauth=generateme:fromtherpcauth.pyfile\ntxindex=1\nsignetchallenge=512102946e8ba8eca597194e7ed90377d9bbebc5d17a9609ab3e35e706612ee882759351ae\n\nrpcport=18332\nrpcworkqueue=1000\nfallbackfee=0.0002\n\nGet coins https://faucet.ctvsignet.com/ / DM me\n\n1. Follow the install instructions on\nhttps://learn.sapio-lang.org/ch01-01-installation.html You can skip the the\nsapio-studio part / pod part and just do the Local Quickstart up until\n\"Instantiate a contract from the plugin\". You'll also want to run *cargo\nbuild --release* from the root directory to build the sapio-cli.\n\n\n2. Open up the site https://rjsf-team.github.io/react-jsonschema-form/\n3. Run *sapio-cli contract api --file\nplugin-example/target/wasm32-unknown-unknown/debug/sapio_wasm_plugin_example.wasm*\n4. Copy the resulting JSON into the RJSF site\n5. Fill out the form as you wish. You should see a JSON like\n{\n\"context\": {\n\"amount\": 3,\n\"network\": \"Signet\",\n\"effects\": {\n\"effects\": {}\n}\n},\n\"arguments\": {\n\"TreePay\": {\n\"fee_sats_per_tx\": 1000,\n\"participants\": [\n{\n\"address\": \"tb1pwqchwp3zur2ewuqsvg0mcl34pmcyxzqn9x8vn0p5a4hzckmujqpqp2dlma\",\n\"amount\": 1\n},\n{\n\"address\": \"tb1pwqchwp3zur2ewuqsvg0mcl34pmcyxzqn9x8vn0p5a4hzckmujqpqp2dlma\",\n\"amount\": 1\n}\n],\n\"radix\": 2\n}\n}\n}\n\nYou may have to delete some extra fields (that site is a little buggy).\n\nOptionally, just modify the JSON above directly.\n\n6. Copy the JSON and paste it into a file ARGS.json\n7. Find your sapio-cli config file (mine is at\n~/.config/sapio-cli/config.json). Modify it to look like (enter your\nrpcauth credentials):\n{\n \"main\": null,\n \"testnet\": null,\n \"signet\": {\n   \"active\": true,\n   \"api_node\": {\n     \"url\": \"http://0.0.0.0:18332\",\n     \"auth\": {\n       \"UserPass\": [\n         \"YOUR RPC NAME\",\n         \"YOUR PASSWORD HERE\"\n       ]\n     }\n   },\n   \"emulator_nodes\": {\n     \"enabled\": false,\n     \"emulators\": [],\n     \"threshold\": 1\n   },\n   \"plugin_map\": {}\n },\n \"regtest\": null\n}\n\n8. Create a contract template:\n*cat ARGS.json| ./target/release/sapio-cli contract create  --file\nplugin-example/target/wasm32-unknown-unknown/debug/sapio_wasm_plugin_example.wasm\n | jq > UNBOUND.json*\n9. Get a proposed funding & binding of the template to that utxo:\n\n*cat UNBOUND.json| ./target/release/sapio-cli contract bind | jq >\nBOUND.json*\n10. Finalize the funding tx:\n\n*cat BOUND.json | jq \".program[\\\"funding\\\"].txs[0].linked_psbt.psbt\" |\nxargs echo | xargs -I% ./bitcoin-cli -signet utxoupdatepsbt % |  xargs -I%\n./bitcoin-cli -signet walletprocesspsbt % | jq \".psbt\" | xargs -I%\n./bitcoin-cli -signet finalizepsbt % | jq \".hex\"*\n\n11. Review the hex transaction/make sure you want this contract... and then\nsend to network:\n\n\n\n*./bitcoin-cli -signet sendrawtransaction\n020000000001015e69106b2eb00d668d945101ed3c0102cf35aba738ee6520fc2603bd60a872ea0000000000feffffff02e8c5eb0b000000002200203d00d88fd664cbfaf8a1296d3f717625595d2980976bbf4feeb10ab090180ccdcb3faefd020000002251208f7e5e50ce7f65debe036a90641a7e4d719d65d621426fd6589e5ec1c5969e200140a348a8711cb389bdb3cc0b1050961e588bb42cb5eb429dd0a415b7b9c712748fa4d5dfe2bb9c4dc48b31a7e3d1a66d9104bbb5936698f8ef8a92ac27a650663500000000*\n\n\n12. Send the other transactions:\n\n*cat BOUND.json| jq .program | jq \".[].txs[0].linked_psbt.psbt\" | xargs -I%\n./target/release/sapio-cli psbt finalize --psbt %  | xargs -I%\n./bitcoin-cli -signet sendrawtransaction %*\n\n\n\nNow what?\n\n- Maybe load up the Sapio Studio and try it through the GUI?\n- Modify the congestion control tree code and recompile it?\n- How big of a tree can you make (I did about 6000 last night)?\n- Try out other contracts?\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n\n\nOn Mon, Feb 21, 2022 at 7:36 PM Jeremy Rubin <jeremy.l.rubin at gmail.com>\nwrote:\n\n> Hi All,\n>\n> Apologies for the late posting of the agenda. The 4th CTV meeting will be\n> held tomorrow at 12:00 PT in ##ctv-bip-review in Libera.chat.\n>\n> Tomorrow the conversation will be slightly more tutorial focused. If you\n> have time in advance of the meeting, it might be good to do some of this in\n> advance.\n>\n> 1) Discussion: What is the goal of Signet? (20 minutes)\n>     - Do we have a \"decision function\" of observations from a test network?\n>     - What applications should be prototyped/fleshed out?\n>     - What level of fleshed out matters?\n>     - Should we add other experiments in the mix on this net, like\n> APO/Sponsors?\n>     - Should we get e.g. lightning working on this signet?\n> 2) Connecting to CTV Signet Tutorial (10 mins)\n>\n> We'll make sure everyone who wants to be on it is on it & debug any issues.\n>\n> *Ahead of Meeting: Build this\n> branch https://github.com/JeremyRubin/bitcoin/tree/checktemplateverify-signet-23.0-alpha\n> <https://github.com/JeremyRubin/bitcoin/tree/checktemplateverify-signet-23.0-alpha>*\n>\n> Connect to:\n> ```\n> [signet]\n>\n> signetchallenge=512102946e8ba8eca597194e7ed90377d9bbebc5d17a9609ab3e35e706612ee882759351ae\n> addnode=50.18.75.225\n> ```\n>\n> 3) Receiving Coins / Sending Coins (5 mins)\n> There's now a faucet for this signet: https://faucet.ctvsignet.com\n> And also an explorer: https://explorer.ctvsignet.com\n>\n> 4) Sapio tutorial (25 minutes)\n>\n> *Ahead of meeting, if you have time: skim https://learn.sapio-lang.org\n> <https://learn.sapio-lang.org> & download/build the sapio cli & plugin\n> examples*\n>\n> We'll try to get everyone building and sending a basic application (e.g.\n> congestion control tree or vault) on the signet (instructions to be posted\n> before meeting).\n>\n> We won't use Sapio Studio, just the Sapio CLI.\n>\n> 5) Sapio Q&A (30 mins)\n>\n> After some experience playing with Sapio, more general discussion about\n> the project and what it may accomplish\n>\n> 6) General Discussion (30 minutes)\n>\n>\n> Best,\n>\n> Jeremy\n>\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220222/ef26b30c/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "BIP-119 CTV Meeting #4 Draft Agenda for Tuesday February 22nd at 12:00 PT",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeremy Rubin"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 8592
        }
    },
    {
        "title": "[bitcoin-dev] BIP-119 CTV Meeting #4 Notes",
        "thread_messages": [
            {
                "author": "Jeremy Rubin",
                "date": "2022-02-22T22:30:17",
                "message_text_only": "Today's meeting was a bit of a different format than usual, the prime focus\nwas on getting CTV Signet up and running and testing out some contracts.\n\nIn terms of discussion, there was some talk about what the goals of a\nsignet should be, but no conclusions were really reached. It is very good a\nsignet exists, but it's unclear how much people will be interested in\nSignet with CTV v.s. if it had a lot of other forks to play with. Further,\nother fork ideas are a lot greener w.r.t. infrastructure available.\n\nIn the tutorial section, we walked through the guide posted on the list.\nThere were a myriad of difficulties with local environments and brittle\nbash scripts provided for the tutorial, as well a confusion around using\nold versions of sapio-cli (spoiler: it's alpha software, need to always be\non the latest).\n\nDespite difficulties, multiple participants finished the tutorial during\nthe session, some of their transactions can be seen below:\n\nhttps://explorer.ctvsignet.com/tx/62292138c2f55713c3c161bd7ab36c7212362b648cf3f054315853a081f5808e\nhttps://explorer.ctvsignet.com/tx/5ff08dcc8eb17979a22be471db1d9f0eb8dc49b4dd015fb08bac34be1ed03a10\n\nIn future weeks the tutorials will continue & more contracts can be tried\nout. This tutorial was also focused on using the CLI, which is harder,\nwhereas future tutorials will use the GUI as well but won't be as prime for\nunderstanding all the \"moving parts\".\n\nBest,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220222/41b9bcbf/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "BIP-119 CTV Meeting #4 Notes",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeremy Rubin"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1659
        }
    },
    {
        "title": "[bitcoin-dev] Draft-BIP: Ordinal Numbers",
        "thread_messages": [
            {
                "author": "Casey Rodarmor",
                "date": "2022-02-23T00:43:52",
                "message_text_only": "Good afternoon list,\n\nI've been working on a scheme of stable public identifiers that can be\nused for a variety of purposes.\n\nThe scheme is extremely simple and does not require protocol-level\nchanges, but since different applications and wallets might use such\nidentifiers, standardizing and publishing the scheme as a BIP seems\nwarranted. The draft-BIP is hosted on GitHub, as well as reproduced\nbelow:\n\nhttps://github.com/casey/ord/blob/master/bip.mediawiki\n\nBriefly, newly mined satoshis are sequentially numbered in the order in\nwhich they are mined. These numbers are called \"ordinal numbers\" or\n\"ordinals\". When satoshis are spent in a transaction, the input satoshi\nordinal numbers are assigned to output satoshis using a simple\nfirst-in-first-out algorithm.\n\nAt any time, the output that contains an ordinal can be determined, and\nthe public key associated with that output can be used to sign\nchallenges or perform actions related to the ordinal that it contains.\n\nSuch identifiers could be used for a variety of purposes, such as user\naccounts, PKI roots, or to issue stablecoins or NFTs. The scheme\ncomposes nicely with other Bitcoin applications, such as the Lightning\nNetwork or state chains.\n\nI'm also working on an command-line tool that builds an index of ordinal\nranges to answer queries about the whereabouts of a particular ordinal,\nor the ordinals contained in a particular output:\n\nhttps://github.com/casey/ord/\n\nThe index is well tested but needs to be optimized before it can index\nthe main chain in a reasonable amount of time and space. It's written in\nRust, by myself and Liam Scalzulli.\n\nI'm eager for feedback, both here, and on GitHub:\n\nhttps://github.com/casey/ord/discussions/126\n\nBest regards,\nCasey Rodarmor\n\nPS After finishing the current draft, I discovered that a variation of\nthis scheme was independently proposed a little under a decade ago by\njl2012 on BitcoinTalk:\n\nhttps://bitcointalk.org/index.php?topic=117224.0\n\n---\n\n<pre>\n  BIP: ?\n  Layer: Applications\n  Title: Ordinal Numbers\n  Author: Casey Rodarmor <casey at rodarmor.com>\n  Comments-Summary: No comments yet.\n  Comments-URI: https://github.com/casey/ord/discussions/126\n  Status: Draft\n  Type: Informational\n  Created: 2022-02-02\n  License: PD\n</pre>\n\n== Introduction ==\n\n=== Abstract ===\n\nThis document defines a scheme for numbering and tracking satoshis\nacross transactions. These numbers, \"ordinal numbers\" in the language of\nthis document, can be used as a useful primitive for a diverse range of\napplications, including NFTs, reputation systems, and Lightning\nNetwork-compatible stablecoins.\n\n=== Copyright ===\n\nThis work is placed in the public domain.\n\n=== Motivation ===\n\nBitcoin has no notion of a stable, public account or identity. Addresses\nare single-use, and wallet accounts, while permanent, are not publicly\nvisible. Additionally, the use of addresses or public keys as\nidentifiers precludes private key rotation or transfer of ownership.\n\nMany applications, some of which are detailed in this document, require\nstable, public identifiers tracking identity or ownership. This proposal\nis motivated by the desire to provide such a system of identifiers.\n\n== Description ==\n\n=== Design ===\n\nEvery satoshi is serially numbered, starting at 0, in the order in which\nit is mined. These numbers are termed \"ordinal numbers\", or \"ordinals\",\nas they are ordinal numbers in the mathematical sense. The word\n\"ordinal\" is nicely unambiguous, as it is not used elsewhere in the\nBitcoin protocol[0].\n\nThe ordinal numbers of transaction inputs are transferred to outputs in\nfirst-in-first-out order, according to the size and order of the\ntransactions inputs and outputs.\n\nIf a transaction is mined with the same transaction ID as outputs\ncurrently in the UTXO set, following the behavior of Bitcoin Core, the\nnew transaction outputs displace the older UTXO set entries, destroying\nthe ordinals contained in any unspent outputs of the first transaction.\n\nFor the purposes of the assignment algorithm, the coinbase transaction\nis considered to have an implicit input equal in size to the subsidy,\nfollowed by an input for every fee-paying transaction in the block, in\nthe order that those transactions appear in the block. The implicit\nsubsidy input carries the block's newly created ordinals. The implicit\nfee inputs carry the ordinals that were paid as fees in the block's\ntransactions.\n\nUnderpaying the subsidy does not change the ordinal numbers of satoshis\nmined in subsequent blocks. Ordinals depend only on how many satoshis\ncould have been mined, not how many actually were.\n\nAt any given time, the output in which an ordinal resides can be\nidentified. The public key associated with this output can be used to\nsign messages, such as ownership challenges, concerning to the ordinals\nit contains. The specification of a standardized message format for such\npurposes is deferred to a later BIP.\n\nOrdinal aware software should not mix outputs containing meaningful\nordinals with outputs used for other purposes to avoid inadvertent loss\nof valuable ordinals, or privacy leaks allowing links between funds. For\nthis reason, ordinal aware software using BIP-32 hierarchical\ndeterministic key generation should use a key derivation path specific\nto ordinals.\n\nThe suggested key derivation path is `m/44'/7303780'/0'/0`. This\nsuggested derivation path has not been standardized and may change in\nthe future[1].\n\n=== Specification ===\n\nOrdinals are created and assigned with the following algorithm:\n\n    # subsidy of block at given height\n    def subsidy(height):\n      return 50 * 100_000_000 >> int(height / 210_000)\n\n    # first ordinal of subsidy of block at given height\n    def first_ordinal(height):\n      start = 0\n      for height in range(height):\n        start += subsidy(height)\n      return start\n\n    # assign ordinals in given block\n    def assign_ordinals(block):\n      first = first_ordinal(block.height)\n      last = first + subsidy(block.height)\n      coinbase_ordinals = list(range(first, last))\n\n      for transaction in block.transactions[1:]:\n        ordinals = []\n        for input in transaction.inputs:\n          ordinals.extend(input.ordinals)\n\n        for output in transaction.outputs:\n          output.ordinals = ordinals[:output.value]\n          del ordinals[:output.value]\n\n        coinbase_ordinals.extend(ordinals)\n\n      for output in block.transaction[0].outputs:\n        output.ordinals = coinbase_ordinals[:output.value]\n        del coinbase_ordinals[:output.value]\n\n=== Terminology and Notation ===\n\nOrdinals may be written as the ordinal number followed by the\nRomance-language ordinal indicator \u00b0, for example 13\u00b0.\n\nA satpoint may be used to indicate the location of an ordinal within an\noutput. A satpoint consists of an outpoint, i.e., a transaction ID and\noutput index, with the addition of the offset of the ordinal within that\noutput. For example, if the ordinal in question is at offset 6 in the\nfirst output of a transaction can be written as:\n\n    680df1e4d43016571e504b0b142ee43c5c0b83398a97bdcfd94ea6f287322d22:0:6\n\nA slot may be used to indicate the output of an ordinal without\nreferring to a transaction ID, by substituting the block height and\ntransaction index within the block for the transaction ID. It is written\nas a dotted quad. For example, the ordinal at offset 100 in the output\nat offset 1, in the coinbase transaction of block 83 can be written as:\n\n    83.0.1.100\n\nSatoshis with ordinals that are not valuable or notable can be referred\nto as cardinal, as their identity does not matter, only the amount. A\ncardinal output is one whose ordinals are unimportant for the purpose at\nhand, for example an output used only to provide padding to avoid\ncreating a transaction with an output below the dust limit.\n\n== Discussion ==\n\n=== Rationale ===\n\nOrdinal numbers are designed to be orthogonal to other aspects of the\nBitcoin protocol, and can thus be used in conjunction with other\nlayer-one techniques and applications, even ones that were not designed\nwith ordinal numbers in mind.\n\nOrdinal satoshis can be secured using current and future script types.\nThey can be held by single-signature wallets, multi-signature wallets,\ntime-locked, and height-locked in all the usual ways.\n\nThis orthogonality also allows them to be used with layer-two\napplications. A stablecoin issuer can promise to allow redemption of\nspecific ranges of ordinals for $1 United States dollar each. Lightning\nNetwork nodes can then be used to create a USD-denominated Lightning\nNetwork, using existing software with very modest modifications.\n\nBy assigning ordinal numbers to all satoshis without need for an\nexplicit creation step, the anonymity set of ordinal number users is\nmaximized.\n\nSince an ordinal number has an output that contains it, and an output\nhas a public key that controls it, the owner of an ordinal can respond\nto challenges by signing messages using the public key associated with\nthe controlling UTXO. Additionally, an ordinal can change hands, or its\nprivate key can be rotated without a change of ownership, by\ntransferring it to a new output.\n\nOrdinals require no changes to blocks, transactions, or network\nprotocols, and can thus be immediately adopted, or ignored, without\nimpacting existing users.\n\nOrdinals do not have an explicit on-chain footprint. However, a valid\nobjection is that adoption of ordinals will increase demand for outputs,\nand thus increase the size of the UTXO set that full nodes must track.\nSee the objections section below.\n\nThe ordinal number scheme is extremely simple. The specification above\nis 15 lines of code.\n\nOrdinals are fairly assigned. They are not premined, and are assigned\nproportionally to existing bitcoin holders.\n\nOrdinals are as granular as possible, as bitcoin is not capable of\ntracking ownership of sub-satoshi values.\n\n=== Transfer and the Dust Limit ===\n\nAny ordinal transfer can be accomplished in a single transaction, but\nthe resulting transaction may contain outputs below the dust limit, and\nthus be non-standard and difficult to get included in a block. Consider\na scenario where Alice owns an output containing the range of ordinals\n[0,10], the current dust limit is 5 satoshis, and Alice wishes to send\nsend ordinals 4\u00b0 and 6\u00b0 to Bob, but retain ordinal 5\u00b0. Alice could\nconstruct a transaction with three outputs of size 5, 1, and 5,\ncontaining ordinals [0,4], 5, and [6,10]. The second output is under the\ndust limit, and so such a transaction would be non-standard.\n\nThis transfer, and indeed any transfer, can be accomplished by breaking\nthe transfer into multiple transactions, with each transaction\nperforming one or more splits and merging in padding outputs as needed.\n\nTo wit, Alice could perform the desired transfer in two transactions.\nThe first transaction would send ordinals [0,4] to Bob, and return as\nchange ordinals [5,10] to Alice. The second transaction would take as\ninputs an output of at least 4 satoshis, the change input, and an\nadditional input of at least one satoshi; and create an output of size 5\nto Bob's address, and the remainder as a change output. Both\ntransactions avoid creating any non-standard outputs, but still\naccomplish the same desired transfer of ordinals.\n\n=== Objections ===\n\n- Privacy: Ordinal numbers are public and thus reduce user privacy.\n\n  The applications using ordinal numbers required them to be public, and\n  reduce the privacy of only those users that decide to use them.\n\n  Fungibility: Ordinal numbers reduce the fungibility of Bitcoin, as\n  ordinals received in a transaction may carry with them some public\n  history.\n\n  As anyone can send anyone else any ordinals, any reasonable person\n  will assume that a new owner of a particular ordinal cannot be\n  understood to be the old owner, or have any particular relationship\n  with the old owner.\n\n- Congestion: Adoption of ordinal numbers will increase the demand for\n  transactions, and drive up fees.\n\n  Since Bitcoin requires the development of a robust fee market, this is\n  a strong positive of the proposal.\n\n- UTXO set bloat: Adoption of ordinal numbers will increase the demand\n  for entries in the UTXO set, and thus increase the size of the UTXO\n  set, which all full nodes are required to track.\n\n  The dust limit, which makes outputs with small values difficult to\n  create, should encourage users to create non-dust outputs, and to\n  clean them up once they no longer have use for the ordinals that they\n  contain.\n\n=== Security ===\n\nThe public key associated with an ordinal may change. This requires\nactively following the blockchain to keep up with key changes, and\nrequires care compared to a system where public keys are static.\nHowever, a system with static public keys suffers from an inability for\nkeys to be rotated or accounts to change hands.\n\nOrdinal-aware software must avoid destroying ordinals by unintentionally\nrelinquishing them in a transaction, either to a non-controlled output\nor by using them as fees.\n\n=== Privacy considerations ===\n\nOrdinals are opt-in, and should not impact the privacy of existing\nusers.\n\nOrdinals are themselves public, however, this is required by the fact\nthat many of the applications that they are intended to enable require\npublic identifiers.\n\nOrdinal aware software should never mix satoshis which might have some\npublicly visible data associated with their ordinals with satoshis\nintended for use in payments or savings, since this would associate that\npublicly visible data with the users otherwise pseudonymous wallet\noutputs.\n\n=== Fungibility considerations ===\n\nSince any ordinal can be sent to any address at any time, ordinals that\nare transferred, even those with some public history, should be\nconsidered to be fungible with other satoshis with no such history.\n\n=== Backward compatibility ===\n\nOrdinal numbers are fully backwards compatible and require no changes to\nthe bitcoin network.\n\n=== Compatibility with Existing and Envisaged Applications ===\n\nOrdinals are compatible with many current and planned applications.\n\n==== Covenants ====\n\nSince ordinals are borne by outputs, they can be encumbered by\ncovenants. BIP-119* specifies OP_CTV, which constraints outputs by\npre-committing to a spending transaction template. This template commits\nto the number, value, and order of spending transaction outputs, which\nallows constraining how specific ordinals are spent in future\ntransactions.\n\nhttps://github.com/bitcoin/bips/blob/master/bip-0119.mediawiki\n\n==== The Lightning Network ====\n\nThe Lightning Network cannot be used to selectively transfer individual\nnon-fungible ordinals, however it can be used to transfer arbitrary\namounts of fungible ordinals. Channels can be created with inputs whose\nordinals are all colored coins of the same type, for example colored\ncoins honored for redemption by a stablecoin issuer. These channels can\nbe used to conduct instant, low-fee USD-denominated off-chain payments,\nand would require only modest changes to existing Lightning Network\nnodes.\n\nOn channel close, fees would have to be paid by child-pays-for-parent,\nto avoid paying stablecoin ordinals as fees.\n\n==== Opendimes and Casascius coins ====\n\nPhysical transfer of ordinals can be facilitated by loading them onto\nbitcoin bearer artifacts, such as Opendimes and Casascius coins.\n\n==== RGB ====\n\nRGB is a proposed scheme for using sequences of single-use seals to\ndefine state transitions of off-chain, client-side-validated state\nmachines, for example smart contract platforms. Such chains of\nsingle-use seals could be addressed by an ordinal contained in the\noutput that starts the chain of single-use seals.\n\nhttps://rgb-org.github.io/\n\n==== State Chains ====\n\nThe state chain proposal facilitates off-chain transfer of whole\noutputs, which could contain ordinals with specific meanings, for\nexample stable coins or NFTs, allowing off-chain transfer of such\ndigital assets.\n\nhttps://github.com/RubenSomsen/rubensomsen.github.io/blob/master/img/statechains.pdf\n\n== Applications ==\n\n=== Accounts and Authentication ===\n\nOrdinal numbers can serve as the basis for account and authentication\nschemes. The account issuer associates a newly created account with an\nordinal number in an output controlled by the account owner. The account\nowner can then log in and take actions related to the account by signing\nmessages with the private key associated with the public key associated\nwith the output that contains the account ordinal. This key is only\nknown to the account owner, preventing unauthorized access.\n\nBy transferring the ordinal to a new output, the owner can rotate their\nprivate key, or transfer the account to a new owner. Transferring an\nordinal requires creating a transaction signed by the current outputs\nprivate key, preventing unauthorized transfer of accounts.\n\n=== Colored Coins ===\n\nOrdinals can be used as the basis for colored coin schemes. Unlike other\ncolored coin schemes which use additional outputs or require\nmanipulation of other parts of a transaction, ordinal-based colored coin\nschemes can take advantage of the full range of available script types,\nand other base-layer bitcoin features.\n\n=== The DNS ===\n\nThe DNS root of trust could be defined not as a specific set of public\nkeys, but as a specific set of ordinals, which would allow for easy key\nrotation and updates to the set.\n\n=== Name Services ===\n\nA scheme, not described in this document, could be used to assign names\nto ordinals based on their number. These names could then be used as\naccount names. Many such names would be gibberish, but many would be\nhuman readable. A scheme which enumerated strings of the ASCII\ncharacters `a` through `z` would assign as names all length-10 and\nshorter permutations of these characters.\n\n=== NFTs ===\n\nAn artist can issue an NFT by signing a message containing a hash of a\nwork of art that they have created, along with the number of a\nparticular ordinal. The owner of that ordinal is the owner of that NFT,\nallowing ownership to be proven, and the NFT to be bought and sold, and\notherwise change hands.\n\nSuch NFTs could be used for art, in-game assets, membership systems, or\nany other kind of digital asset.\n\nThe signed message, which may contain arbitrary attributes and metadata,\nis not sensitive and can be widely disseminated and replicated, to\nensure it is not lost.\n\nScarcity of such NFTs can be guaranteed by including in the NFT messages\nthe total number of NFTs to be issued. If this promise is violated, the\nset of issued NFTs serves as an easy-to-verify fraud proof that the\nissuance limit was exceeded.\n\nA judicious NFT issuer will create a new private key to sign a new set\nof NFTs and destroy it afterwards, to ensure the limited nature of the\nNFT set. Multi-party-computation can be used to provide additional\nassurances that overissuance cannot occur.\n\n=== PKI ===\n\nInstead of individual public keys serving as roots of trust for PKI\nsystems, individual ordinals could be used, allowing for key rotation.\n\n=== Rare Sats ===\n\nOrdinal numbers are unique, which might encourage collectors and\nspeculators to collect particular ordinals. Examples of potentially\ncollectable ordinals include:\n\n* The first ordinal in a block, difficulty adjustment period, or halving\nepoch.\n* Ordinals consisting only of a single repeating digit.\n* Ordinals with a large number of 8s, commonly held to be a lucky digit.\n* Low ordinals mined early in bitcoin's history.\n* Ordinals that were part of unusual blocks or transactions.\n\n=== Reputation Systems ===\n\nOrdinal numbers can serve as the basis for persistent reputation\nsystems, for example one of Lightning Network node operators. Unlike the\ncurrent system of associating reputation with public keys, an\nordinal-based reputation system allows for key rotation and reputation\ntransfer.\n\n=== Stablecoins ===\n\nA stablecoin issuer could promise to allow redemption of a range of\nordinals for one United States dollar each, minus the price of one\nsatoshi times the number of satoshis so redeemed. Such ordinals could be\ntransacted on-chain and on a slightly modified Lightning Network, as\nwell as other layers.\n\n=== Voting and DAOs ===\n\nA DAO or other organization may decide to allocate voting rights\nproportionally to ownership of a predetermined range of ordinals. Voting\nrights can thus be made transferable, and voting may be conducted by\nsigning messages using public keys associated with the outputs holding\nvote-bearing ordinals.\n\n== Reference implementation ==\n\nThis document, along with an implementation of an ordinal index that\ntracks the position of ordinals in the main chain, is available on\nGitHub: https://github.com/casey/ord\n\n== References ==\n\nA variation of this scheme was independently invented a decade ago by\njl2012 on BitcoinTalk: https://bitcointalk.org/index.php?topic=117224.0\n\nFor other colored coin proposals see the Bitcoin Wiki entry:\nhttps://en.bitcoin.it/wiki/Colored_Coins\n\nFor aliases, an implementation of short on-chain identifiers, see BIP\n15.\n\n[0] With the exception of being word #1405 in the BIP-39 Portuguese word\n    list. Me perdoe!\n[1] 7303780 is the decimal representation of the ASCII string 'ord'.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220222/04d7630c/attachment-0001.html>"
            },
            {
                "author": "damian at willtech.com.au",
                "date": "2022-02-23T07:02:03",
                "message_text_only": "Well done, your bip looks well presented for discussion. You say to \nnumber each satoshi created? For a 50 BTC block reward that is \n5,000,000,000 ordinal numbers, and when some BTC is transferred to \nanother UTXO how do you determine which ordinal numbers, say if I create \na transaction to pay-to another UTXO. The system sounds expensive \neventually to cope with approximately 2,100,000,000,000,000 ordinals. If \nI understand ordinals 0 to 5,000,000,000 as assigned to the first \nBitcoin created from mining block-reward. Say if I send some Bitcoin to \nanother UTXO then first-in-first-out algorithm splits those up to assign \n1 to 100,000,000 to the 1 BTC that I sent, and 100,000,001 to \n5,000,000,000 are assigned to the change plus if any fee?-DA.\n\nOn 2022-02-23 11:43, Casey Rodarmor via bitcoin-dev wrote:\n> Briefly, newly mined satoshis are sequentially numbered in the order\n> in\n> which they are mined. These numbers are called \"ordinal numbers\" or\n> \"ordinals\". When satoshis are spent in a transaction, the input\n> satoshi\n> ordinal numbers are assigned to output satoshis using a simple\n> first-in-first-out algorithm."
            },
            {
                "author": "damian at willtech.com.au",
                "date": "2022-02-23T07:24:52",
                "message_text_only": "At the moment it is indisputable that a particular satoshi cannot be \nproven, an amount of Bitcoin is a bag of satoshi's and no-one can tell \nwhich ones are any particular ones **so even if you used the system of \nordinals privately, and it might make interesting for research, I cannot \nsee that it would be sensible to be adopted** as it can only cause \ntrouble. If I receive some Bitcoin I cannot know if some or any of those \nhave been at any point in the past been stolen, I assume the transaction \nis honest, and in all likelihood it is likely that it is. The least \nreasonable thing I could expect is some claimed former holder of some \nordianls turning up to challenge me that it was their stolen Bitcoin was \nsome of what I received.\n\nNACK\n\n-DA.\n\nOn 2022-02-23 18:02, damian at willtech.com.au wrote:\n> Well done, your bip looks well presented for discussion. You say to\n> number each satoshi created? For a 50 BTC block reward that is\n> 5,000,000,000 ordinal numbers, and when some BTC is transferred to\n> another UTXO how do you determine which ordinal numbers, say if I\n> create a transaction to pay-to another UTXO. The system sounds\n> expensive eventually to cope with approximately 2,100,000,000,000,000\n> ordinals. If I understand ordinals 0 to 5,000,000,000 as assigned to\n> the first Bitcoin created from mining block-reward. Say if I send some\n> Bitcoin to another UTXO then first-in-first-out algorithm splits those\n> up to assign 1 to 100,000,000 to the 1 BTC that I sent, and\n> 100,000,001 to 5,000,000,000 are assigned to the change plus if any\n> fee?-DA.\n> \n> On 2022-02-23 11:43, Casey Rodarmor via bitcoin-dev wrote:\n>> Briefly, newly mined satoshis are sequentially numbered in the order\n>> in\n>> which they are mined. These numbers are called \"ordinal numbers\" or\n>> \"ordinals\". When satoshis are spent in a transaction, the input\n>> satoshi\n>> ordinal numbers are assigned to output satoshis using a simple\n>> first-in-first-out algorithm."
            },
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2022-02-24T07:02:06",
                "message_text_only": "> The system sounds expensive eventually to cope with approximately 2,100,000,000,000,000 ordinals.\nWhat about zero satoshis? There are transactions, where zero satoshis are created or moved. Typical users cannot do that, but miners can, we currently have such transactions in the blockchain, for example 9f0b871e28fa19e2308e2fa74243bf2dcf23b160754df847d5f1e41aabe499d1 (check the last two inputs).\n\nOn 2022-02-24 01:53:36 user damian--- via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Well done, your bip looks well presented for discussion. You say to \nnumber each satoshi created? For a 50 BTC block reward that is \n5,000,000,000 ordinal numbers, and when some BTC is transferred to \nanother UTXO how do you determine which ordinal numbers, say if I create \na transaction to pay-to another UTXO. The system sounds expensive \neventually to cope with approximately 2,100,000,000,000,000 ordinals. If \nI understand ordinals 0 to 5,000,000,000 as assigned to the first \nBitcoin created from mining block-reward. Say if I send some Bitcoin to \nanother UTXO then first-in-first-out algorithm splits those up to assign \n1 to 100,000,000 to the 1 BTC that I sent, and 100,000,001 to \n5,000,000,000 are assigned to the change plus if any fee?-DA.\n\nOn 2022-02-23 11:43, Casey Rodarmor via bitcoin-dev wrote:\n> Briefly, newly mined satoshis are sequentially numbered in the order\n> in\n> which they are mined. These numbers are called \"ordinal numbers\" or\n> \"ordinals\". When satoshis are spent in a transaction, the input\n> satoshi\n> ordinal numbers are assigned to output satoshis using a simple\n> first-in-first-out algorithm.\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Casey Rodarmor",
                "date": "2022-02-24T07:17:02",
                "message_text_only": "\u200bWhat about zero satoshis?\n\n\nA zero satoshi input or output carries no ordinals, so an ordinal index can\nignore them.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220224/fabae806/attachment.html>"
            },
            {
                "author": "Casey Rodarmor",
                "date": "2022-02-23T07:10:58",
                "message_text_only": "\u200bWell done, your bip looks well presented for discussion.\n\n\nThank you!\n\nYou say to number each satoshi created? For a 50 BTC block reward that is\n> 5,000,000,000 ordinal numbers, and when some BTC is transferred to another\n> UTXO how do you determine which ordinal numbers, say if I create a\n> transaction to pay-to another UTXO.\n>\n\nIt uses a first-in-first out algorithm, so the first ordinal number of the\nfirst input becomes the first ordinal number of the first output.\n\nThe system sounds expensive eventually to cope with approximately\n> 2,100,000,000,000,000 ordinals.\n>\n\nA full index is expensive, but it doesn't have to track 2.1 individual\nentries, it only has to track contiguous ordinal ranges, which scales with\nthe number of outputs\u2013all outputs, not just unspent outputs\u2013since an output\nmight split an ordinal range.\n\nIf I understand ordinals 0 to 5,000,000,000 as assigned to the first\n> Bitcoin created from mining block-reward. Say if I send some Bitcoin to\n> another UTXO then first-in-first-out algorithm splits those up to assign 1\n> to 100,000,000 to the 1 BTC that I sent, and 100,000,001 to 5,000,000,000\n> are assigned to the change plus if any fee?-DA.\n>\n\nThat's correct, assuming that the 1 BTC output is first, and the 4 BTC\noutput is second. Although it's actually 0 to 99,999,999 that go to the\nfirst output, and 100,000,000 to 499,999,999 that are assigned to the\nsecond output, less any fees.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220223/4ff3cf7a/attachment-0001.html>"
            },
            {
                "author": "Casey Rodarmor",
                "date": "2022-02-23T07:31:49",
                "message_text_only": "\u200bThe least reasonable thing I could expect is some claimed former holder of\nsome ordianls turning up to challenge me that it was their stolen Bitcoin\nwas some of what I received.\n\n\nI think it's unlikely that this would come to pass. A previous owner of an\nordinal wouldn't have any particular reason to expect that they should own\nit after they transfer it. Similar to how noting a dollar bill's serial\nnumber doesn't give you a claim to it after you spend it. From the BIP:\n\n\u200bSince any ordinal can be sent to any address at any time, ordinals that\nare transferred, even those with some public history, should be considered\nto be fungible with other satoshis with no such history.\n<https://github.com/casey/ord/blob/master/bip.mediawiki#backward-compatibility>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220223/cb083aa3/attachment.html>"
            },
            {
                "author": "damian at willtech.com.au",
                "date": "2022-02-24T02:34:39",
                "message_text_only": "Not all people who have been stolen from believe that they have lost the \nright and title to what has been stolen and in many cases they have not. \nI do not excuse Bitcoin that it is impossible to have any individual \nBitcoin identified but also I do not care, if I receive Bitcoin honestly \nI do not care what their history was. What if they were taken from a \nbrothel? It is not a matter for an ordinal to determine if a satoshi is \nfungible. It is truth in effect that each satoshi is newly created to \nthe new UTXO and the old satoshi destroyed. -DA.\n\n  On 2022-02-23 18:31, Casey Rodarmor wrote:\n>> \u200bThe least reasonable thing I could expect is some claimed former\n>> holder of some ordianls turning up to challenge me that it was their\n>> stolen Bitcoin was some of what I received.\n> \n> I think it's unlikely that this would come to pass. A previous owner\n> of an ordinal wouldn't have any particular reason to expect that they\n> should own it after they transfer it. Similar to how noting a dollar\n> bill's serial number doesn't give you a claim to it after you spend\n> it. From the BIP:\n> \n>> \u200bSince any ordinal can be sent to any address at any time,\n>> ordinals that are transferred, even those with some public history,\n>> should be considered to be fungible with other satoshis with no such\n>> history. [1]\n> \n> \n> \n> Links:\n> ------\n> [1] \n> https://github.com/casey/ord/blob/master/bip.mediawiki#backward-compatibility"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-02-24T15:55:57",
                "message_text_only": "I think the proposal is interesting in that it could be an interesting way\nto solve the dust problem. While most solutions to dust focus on reducing\nhow much are created and encouraging consolidating utxos to avoid them\nbecoming dust, this proposal could utilize dust for valuable purposes. Why\nuse valuable Bitcoin for NFTs or colored coins when dust can be split into\nit's unit satoshis and used with no loss of utility?\n\nSimple and elegant. I like it. If we're giving ACKs: ACK. Tho TBH I don't\nsee any reason NACK this - seems like this doesn't affect consensus,\ndoesn't affect relay, doesn't affect anything except people that run this\nalgorithm on the blockchain. If people want to do something like this,\npeople are going to do it whether or not the bitcoin community wants them\nto. A standard would be good rather than everyone doing their own thing.\n\nOne thought I had was: what happens if/when it comes to pass that we\nincrease payment precision by going sub-satoshi on chain? It seems like it\nwould be fairly simple to extend that to ordinals by having fraction\nordinals like 1.1 or 4.85. Could be an interesting thought to add to the\nproposal.\n\n> If a transaction is mined with the same transaction ID as outputs\ncurrently in the UTXO set, following the behavior of Bitcoin Core, the new\ntransaction outputs displace the older UTXO set entries, destroying the\nordinals contained in any unspent outputs of the first transaction.\n\nWhat you mean by \"the same transaction id\" here is unclear. I was\ninterpreting the proposal to mean that UTXOs are all assigned a set of\nordinals, and when that UTXO is spent, it transfers it's ordinals to\noutputs in the transaction the UTXO is spent in. Is that what you mean by\nthis sentence? If so, I'd suggest rewording.\n\n@Damian\n> If I receive some Bitcoin I cannot know if some or any of those have been\nat any point in the past been stolen, I assume the transaction is honest,\nand in all likelihood it is likely that it is.\n\nThis isn't true at all. Some bitcoins are indeed known to be stolen and\neven blacklisted by some companies/governments. I don't see how ordinals\nchanges anything related to this.\n\n@vjudeu\n> What about zero satoshis?\n\nThose could be used for NFTs but not something like colored coins. It would\nbe a strict subset of ability, tho its an interesting idea in its own\nright.\n\n\n\n\nOn Thu, Feb 24, 2022, 02:15 damian--- via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Not all people who have been stolen from believe that they have lost the\n> right and title to what has been stolen and in many cases they have not.\n> I do not excuse Bitcoin that it is impossible to have any individual\n> Bitcoin identified but also I do not care, if I receive Bitcoin honestly\n> I do not care what their history was. What if they were taken from a\n> brothel? It is not a matter for an ordinal to determine if a satoshi is\n> fungible. It is truth in effect that each satoshi is newly created to\n> the new UTXO and the old satoshi destroyed. -DA.\n>\n>   On 2022-02-23 18:31, Casey Rodarmor wrote:\n> >> \u200bThe least reasonable thing I could expect is some claimed former\n> >> holder of some ordianls turning up to challenge me that it was their\n> >> stolen Bitcoin was some of what I received.\n> >\n> > I think it's unlikely that this would come to pass. A previous owner\n> > of an ordinal wouldn't have any particular reason to expect that they\n> > should own it after they transfer it. Similar to how noting a dollar\n> > bill's serial number doesn't give you a claim to it after you spend\n> > it. From the BIP:\n> >\n> >> \u200bSince any ordinal can be sent to any address at any time,\n> >> ordinals that are transferred, even those with some public history,\n> >> should be considered to be fungible with other satoshis with no such\n> >> history. [1]\n> >\n> >\n> >\n> > Links:\n> > ------\n> > [1]\n> >\n> https://github.com/casey/ord/blob/master/bip.mediawiki#backward-compatibility\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220224/35332576/attachment-0001.html>"
            },
            {
                "author": "Casey Rodarmor",
                "date": "2022-02-24T21:03:54",
                "message_text_only": "> One thought I had was: what happens if/when it comes to pass that we\nincrease payment precision by going sub-satoshi on chain? It seems like it\nwould be fairly simple to extend that to ordinals by having fraction\nordinals like 1.1 or 4.85. Could be an interesting thought to add to the\nproposal.\n\nI think it's probably premature to make a concrete proposal, since any\nproposal made now might be inapplicable to the actual form that a precision\nincrease takes.\n\n> What you mean by \"the same transaction id\" here is unclear. I was\ninterpreting the proposal to mean that UTXOs are all assigned a set of\nordinals, and when that UTXO is spent, it transfers it's ordinals to\noutputs in the transaction the UTXO is spent in. Is that what you mean by\nthis sentence? If so, I'd suggest rewording.\n\nThere are two pairs of old transactions with duplicate IDs, from blocks\n91812 and 91842, and 91722 91880. (It's no longer possible to create\ntransactions with duplicate IDs, since the BIP 34 soft fork that required\nthe height be included in coinbase transaction inputs, making them have\nguaranteed unique IDs.)\n\nThis section of the spec defines what ordinal ranges such duplicate\ntransactions contain. It tries to match the behavior of Bitcoin Core, which\nconsiders the second transaction with a given ID to render unspendable\ncurrent UTXOs created by a transaction with the same ID.\n\nI'll add some detail to this part of the BIP, and talk about why this rule\nis needed.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220224/efa3472c/attachment.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-02-25T04:59:56",
                "message_text_only": ">  what if/when we introduce some Monero-like system and hide coin amounts?\n\nI really don't see a world where bitcoin goes that route. Hiding coin\namounts would make it impossible to audit the blockchain and verify that\nthere hasn't been inflation and the emission schedule is on schedule. It\nwould inherently remove unconditional soundness from bitcoin and replace it\nwith computational soundness. Even if bitcoin did adopt it, it would keep\nbackwards compatibility with old style addresses which could continue to\nuse ordinals.\n\nOn Thu, Feb 24, 2022 at 3:03 PM Casey Rodarmor <casey at rodarmor.com> wrote:\n\n> > One thought I had was: what happens if/when it comes to pass that we\n> increase payment precision by going sub-satoshi on chain? It seems like it\n> would be fairly simple to extend that to ordinals by having fraction\n> ordinals like 1.1 or 4.85. Could be an interesting thought to add to the\n> proposal.\n>\n> I think it's probably premature to make a concrete proposal, since any\n> proposal made now might be inapplicable to the actual form that a precision\n> increase takes.\n>\n> > What you mean by \"the same transaction id\" here is unclear. I was\n> interpreting the proposal to mean that UTXOs are all assigned a set of\n> ordinals, and when that UTXO is spent, it transfers it's ordinals to\n> outputs in the transaction the UTXO is spent in. Is that what you mean by\n> this sentence? If so, I'd suggest rewording.\n>\n> There are two pairs of old transactions with duplicate IDs, from blocks\n> 91812 and 91842, and 91722 91880. (It's no longer possible to create\n> transactions with duplicate IDs, since the BIP 34 soft fork that required\n> the height be included in coinbase transaction inputs, making them have\n> guaranteed unique IDs.)\n>\n> This section of the spec defines what ordinal ranges such duplicate\n> transactions contain. It tries to match the behavior of Bitcoin Core, which\n> considers the second transaction with a given ID to render unspendable\n> current UTXOs created by a transaction with the same ID.\n>\n> I'll add some detail to this part of the BIP, and talk about why this rule\n> is needed.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220224/775c5d38/attachment-0001.html>"
            },
            {
                "author": "AdamISZ",
                "date": "2022-02-25T11:17:29",
                "message_text_only": "> I really don't see a world where bitcoin goes that route. Hiding coin amounts would make it impossible to audit the blockchain and verify that there hasn't been inflation and the emission schedule is on schedule. It would inherently remove unconditional soundness from bitcoin and replace it with computational soundness. Even if bitcoin did adopt it, it would keep backwards compatibility with old style addresses which could continue to use ordinals.\n\nNit: it isn't technically correct to say that amount hiding \"inherently removes unconditional soundness\". Such commitments can be either perfectly hiding or perfectly binding; it isn't even logically possible for them to be both, sadly. But we are not forced to choose perfect binding; El Gamal commitments, for example, are perfectly binding but only computationally hiding."
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-02-25T15:56:24",
                "message_text_only": "> El Gamal commitments, for example, are perfectly binding but only\ncomputationally hiding.\n\nThat's very interesting. I stand corrected in that respect. Thanks for the\ninformation Adam!\n\nOn Fri, Feb 25, 2022, 05:17 AdamISZ <AdamISZ at protonmail.com> wrote:\n\n> > I really don't see a world where bitcoin goes that route. Hiding coin\n> amounts would make it impossible to audit the blockchain and verify that\n> there hasn't been inflation and the emission schedule is on schedule. It\n> would inherently remove unconditional soundness from bitcoin and replace it\n> with computational soundness. Even if bitcoin did adopt it, it would keep\n> backwards compatibility with old style addresses which could continue to\n> use ordinals.\n>\n> Nit: it isn't technically correct to say that amount hiding \"inherently\n> removes unconditional soundness\". Such commitments can be either perfectly\n> hiding or perfectly binding; it isn't even logically possible for them to\n> be both, sadly. But we are not forced to choose perfect binding; El Gamal\n> commitments, for example, are perfectly binding but only computationally\n> hiding.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220225/8c71070c/attachment-0001.html>"
            },
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2022-02-24T17:52:39",
                "message_text_only": "> what happens if/when it comes to pass that we increase payment precision by going sub-satoshi on chain?\nWhen we talk about future improvements, there could be even bigger problem with ordinal numbers: what if/when we introduce some Monero-like system and hide coin amounts? (for example by using zero satoshi, because we have to use something that will be backward-compatible). Zero is quite interesting amount, because it means \"skip amount checking for old clients\". That can be used in many ways to introduce many protocols (and also to add fractional satoshis on-chain, because 0.4 satoshis could be represented as zero), so if that amounts will be simply ignored, then I wonder how it would be possible to connect some future protocol based on that with ordinal numbers.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220224/7038c088/attachment.html>"
            },
            {
                "author": "Casey Rodarmor",
                "date": "2022-02-24T21:02:05",
                "message_text_only": "> When we talk about future improvements, there could be even bigger\nproblem with ordinal numbers: what if/when we introduce some Monero-like\nsystem and hide coin amounts? (for example by using zero satoshi, because\nwe have to use something that will be backward-compatible). Zero is quite\ninteresting amount, because it means \"skip amount checking for old\nclients\". That can be used in many ways to introduce many protocols (and\nalso to add fractional satoshis on-chain, because 0.4 satoshis could be\nrepresented as zero), so if that amounts will be simply ignored, then I\nwonder how it would be possible to connect some future protocol based on\nthat with ordinal numbers.\n\nOrdinal numbers are inherently public, so it seems reasonable that they\ndon't work with transactions that are private or have obfuscated values.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220224/fad28767/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Draft-BIP: Ordinal Numbers",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "damian at willtech.com.au",
                "vjudeu at gazeta.pl",
                "Casey Rodarmor",
                "AdamISZ",
                "Billy Tetrud"
            ],
            "messages_count": 15,
            "total_messages_chars_count": 42907
        }
    },
    {
        "title": "[bitcoin-dev] OP_RETURN inside TapScript",
        "thread_messages": [
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2022-02-24T09:02:08",
                "message_text_only": "Since Taproot was activated, we no longer need separate OP_RETURN outputs to be pushed on-chain. If we want to attach any data to a transaction, we can create \"OP_RETURN <anything>\" as a branch in the TapScript. In this way, we can store that data off-chain and we can always prove that they are connected with some taproot address, that was pushed on-chain. Also, we can store more than 80 bytes for \"free\", because no such taproot branch will be ever pushed on-chain and used as an input. That means we can use \"OP_RETURN <1.5 GB of data>\", create some address having that taproot branch, and later prove to anyone that such \"1.5 GB of data\" is connected with our taproot address.\n\u00a0\nCurrently in Bitcoin Core we have \"data\" field in \"createrawtransaction\". Should the implementation be changed to place that data in a TapScript instead of creating separate OP_RETURN output? What do you think?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220224/e4160455/attachment.html>"
            },
            {
                "author": "Ruben Somsen",
                "date": "2022-02-24T10:08:22",
                "message_text_only": "Note this has always been possible, and is not specifically related to\ntapscript. As long as you're committing to an ECC point, you can tweak it\nto commit data inside it (i.e. pay-to-contract). This includes P2PK and\nP2PKH.\n\nCommitting to 1.5GB of data has equally been possible with OP_RETURN\n<hash>, or even an entire merkle tree of hashes, as is the case with Todd's\nopentimestamps.\n\nAlso, tweaking an ECC point (this includes tapscript) in non-deterministic\nways also makes it harder to recover from backup, because you can't recover\nthe key without knowing the full commitment.\n\nFurthermore, the scheme is not actually equivalent to op_return, because\nit requires the user to communicate out-of-band to reveal the commitment,\nwhereas with op_return the data is immediately visible (while not popular,\nBIP47 and various colored coin protocols rely on this).\n\nCheers,\nRuben\n\n\nOn Thu, Feb 24, 2022 at 10:19 AM vjudeu via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Since Taproot was activated, we no longer need separate OP_RETURN outputs\n> to be pushed on-chain. If we want to attach any data to a transaction, we\n> can create \"OP_RETURN <anything>\" as a branch in the TapScript. In this\n> way, we can store that data off-chain and we can always prove that they are\n> connected with some taproot address, that was pushed on-chain. Also, we can\n> store more than 80 bytes for \"free\", because no such taproot branch will be\n> ever pushed on-chain and used as an input. That means we can use \"OP_RETURN\n> <1.5 GB of data>\", create some address having that taproot branch, and\n> later prove to anyone that such \"1.5 GB of data\" is connected with our\n> taproot address.\n>\n> Currently in Bitcoin Core we have \"data\" field in \"createrawtransaction\".\n> Should the implementation be changed to place that data in a TapScript\n> instead of creating separate OP_RETURN output? What do you think?\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220224/e1bb7a97/attachment-0001.html>"
            },
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2022-02-24T13:27:16",
                "message_text_only": "> Also, tweaking an ECC point (this includes tapscript) in non-deterministic ways also makes it harder to recover from backup, because you can't recover the key without knowing the full commitment.\nI don't think so. You can spend coins from taproot by key or by script. If you spend by key, making backup is simple, we have WIF for that. If you spend by script, you only need a part of the tree. So, you can \"recover the key without knowing the full commitment\", because you can spend coins \"without knowing the full commitment\". On-chain, you never reveal your \"OP_RETURN <data>\" or \"OP_RETURN <hash>\" or \"<tapbranch> <tapbranch> <tapbranch> OP_RETURN <chunk_of_data>\". Those additional branches are stored only by those who wants their data to be connected with some key, knowing the full script is not needed, because it is not needed for on-chain validation.\n> Furthermore, the scheme is not actually equivalent to op_return, because it requires the user to communicate out-of-band to reveal the commitment, whereas with op_return the data is immediately visible (while not popular, BIP47 and various colored coin protocols rely on this).\nYes, but storing that additional data on-chain is not needed. It is expensive. By paying one satoshi per byte, you would pay 0.01 BTC for pushing 1 MB of data. That means 1 BTC for 100 MB of data, so 15 BTC for that 1.5 GB file. And in practice it is the absolute minimum, because you have to wrap your data somehow, you cannot just push 1.5 GB file. By placing that in TapScript, you can use your taproot public key as usual and attach any data into your key for \"free\", because it takes zero additional bytes on-chain.\nOn 2022-02-24 11:08:39 user Ruben Somsen <rsomsen at gmail.com> wrote:\nNote this has\u00a0always been possible, and is not specifically related to tapscript. As long as you're committing to an ECC point, you can tweak it to commit data inside it (i.e. pay-to-contract). This includes P2PK and P2PKH.\n\u00a0\nCommitting to 1.5GB of data has equally been possible with OP_RETURN <hash>, or even an entire merkle tree of hashes, as is the case with Todd's opentimestamps.\n\u00a0\nAlso, tweaking an ECC point (this includes tapscript)\u00a0in non-deterministic ways also makes it harder to recover from backup, because you can't recover the key without knowing the full commitment.\n\u00a0\nFurthermore, the scheme is not actually equivalent to op_return, because it\u00a0requires the user to communicate out-of-band to reveal the commitment, whereas with op_return the data is immediately visible (while not popular, BIP47 and various colored coin protocols rely on this).\n\u00a0\nCheers,\nRuben\n\u00a0\nOn Thu, Feb 24, 2022 at 10:19 AM vjudeu via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\nSince Taproot was activated, we no longer need separate OP_RETURN outputs to be pushed on-chain. If we want to attach any data to a transaction, we can create \"OP_RETURN <anything>\" as a branch in the TapScript. In this way, we can store that data off-chain and we can always prove that they are connected with some taproot address, that was pushed on-chain. Also, we can store more than 80 bytes for \"free\", because no such taproot branch will be ever pushed on-chain and used as an input. That means we can use \"OP_RETURN <1.5 GB of data>\", create some address having that taproot branch, and later prove to anyone that such \"1.5 GB of data\" is connected with our taproot address.\n\u00a0\nCurrently in Bitcoin Core we have \"data\" field in \"createrawtransaction\". Should the implementation be changed to place that data in a TapScript instead of creating separate OP_RETURN output? What do you think?\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220224/532d2b75/attachment-0001.html>"
            },
            {
                "author": "Ruben Somsen",
                "date": "2022-02-24T14:01:58",
                "message_text_only": "In Q = P + hash(P||commitment)G you cannot spend from Q without knowing\nboth the private key of P as well as the commitment (i.e. 32 bytes,\nassuming the commitment itself is another hash). This is generally not a\nproblem for tapscript, as the scripts are deterministically generated (i.e.\nbacking up the policy once is sufficient), but what you are suggesting is\nnot deterministic. Hope that clarifies things.\n\nOn Thu, Feb 24, 2022 at 2:27 PM <vjudeu at gazeta.pl> wrote:\n\n> > Also, tweaking an ECC point (this includes tapscript) in\n> non-deterministic ways also makes it harder to recover from backup, because\n> you can't recover the key without knowing the full commitment.\n>\n> I don't think so. You can spend coins from taproot by key or by script. If\n> you spend by key, making backup is simple, we have WIF for that. If you\n> spend by script, you only need a part of the tree. So, you can \"recover the\n> key without knowing the full commitment\", because you can spend coins\n> \"without knowing the full commitment\". On-chain, you never reveal your\n> \"OP_RETURN <data>\" or \"OP_RETURN <hash>\" or \"<tapbranch> <tapbranch>\n> <tapbranch> OP_RETURN <chunk_of_data>\". Those additional branches are\n> stored only by those who wants their data to be connected with some key,\n> knowing the full script is not needed, because it is not needed for\n> on-chain validation.\n>\n> > Furthermore, the scheme is not actually equivalent to op_return, because\n> it requires the user to communicate out-of-band to reveal the commitment,\n> whereas with op_return the data is immediately visible (while not popular,\n> BIP47 and various colored coin protocols rely on this).\n>\n> Yes, but storing that additional data on-chain is not needed. It is\n> expensive. By paying one satoshi per byte, you would pay 0.01 BTC for\n> pushing 1 MB of data. That means 1 BTC for 100 MB of data, so 15 BTC for\n> that 1.5 GB file. And in practice it is the absolute minimum, because you\n> have to wrap your data somehow, you cannot just push 1.5 GB file. By\n> placing that in TapScript, you can use your taproot public key as usual and\n> attach any data into your key for \"free\", because it takes zero additional\n> bytes on-chain.\n>\n> On 2022-02-24 11:08:39 user Ruben Somsen <rsomsen at gmail.com> wrote:\n>\n> Note this has always been possible, and is not specifically related to\n> tapscript. As long as you're committing to an ECC point, you can tweak it\n> to commit data inside it (i.e. pay-to-contract). This includes P2PK and\n> P2PKH.\n>\n> Committing to 1.5GB of data has equally been possible with OP_RETURN\n> <hash>, or even an entire merkle tree of hashes, as is the case with Todd's\n> opentimestamps.\n>\n> Also, tweaking an ECC point (this includes tapscript) in non-deterministic\n> ways also makes it harder to recover from backup, because you can't recover\n> the key without knowing the full commitment.\n>\n> Furthermore, the scheme is not actually equivalent to op_return, because\n> it requires the user to communicate out-of-band to reveal the commitment,\n> whereas with op_return the data is immediately visible (while not popular,\n> BIP47 and various colored coin protocols rely on this).\n>\n> Cheers,\n> Ruben\n>\n>\n> On Thu, Feb 24, 2022 at 10:19 AM vjudeu via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org\n> <http://../NowaWiadomosc/Do/QlIkBFQ6QUFhIVRZX192dnQBeCtCchE6GhA5LFpLCUc7EVZQVl9dQRIXXR8NCBMbCwIGChJXQFxcXEgcFh8UVVVDEyBdVkE9JVRdEwFhYXVlblhVIkosEAszLR5BQVV7U0MID0BAQUgIGh0RHgAMGAMXBQJfW1sdXRQUQUoDQlAiBFY8>>\n> wrote:\n>\n>> Since Taproot was activated, we no longer need separate OP_RETURN outputs\n>> to be pushed on-chain. If we want to attach any data to a transaction, we\n>> can create \"OP_RETURN <anything>\" as a branch in the TapScript. In this\n>> way, we can store that data off-chain and we can always prove that they are\n>> connected with some taproot address, that was pushed on-chain. Also, we can\n>> store more than 80 bytes for \"free\", because no such taproot branch will be\n>> ever pushed on-chain and used as an input. That means we can use \"OP_RETURN\n>> <1.5 GB of data>\", create some address having that taproot branch, and\n>> later prove to anyone that such \"1.5 GB of data\" is connected with our\n>> taproot address.\n>>\n>> Currently in Bitcoin Core we have \"data\" field in \"createrawtransaction\".\n>> Should the implementation be changed to place that data in a TapScript\n>> instead of creating separate OP_RETURN output? What do you think?\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> <http://../NowaWiadomosc/Do/QlIkBFQ6QUFhIVRZX192dnQBeCtCchE6GhA5LFpLCUc7EVZQVl9dQRIXXR8NCBMbCwIGChJXQFxcXEgcFh8UVVVDEyBdVkE9JVRdEwFhYXVlblhVIkosEAszLR5BQVV7U0MID0BAQUgIGh0RHgAMGAMXBQJfW1sdXRQUQUoDQlAiBFY8>\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220224/598293c1/attachment.html>"
            },
            {
                "author": "Zac Greenwood",
                "date": "2022-02-24T21:40:57",
                "message_text_only": "Reducing the footprint of storing data on-chain might better be achieved by\n*supporting* it.\n\nCurrently storing data is wasteful because it is embedded inside an\nOP_RETURN within a transaction structure. As an alternative, by supporting\nstoring of raw data without creating a transaction, waste can be reduced.\n\nStoring data in this way must only be marginally cheaper per on-chain byte\nthan the current method  using OP_RETURN by applying the appropriate\nweight-per-byte for on-chain data.\n\nThe intended result is a smaller footprint for on-chain data without making\nit cheaper (except marginally in order to disincentivize the use of\nOP_RETURN).\n\nZac\n\n\nOn Thu, 24 Feb 2022 at 10:19, vjudeu via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Since Taproot was activated, we no longer need separate OP_RETURN outputs\n> to be pushed on-chain. If we want to attach any data to a transaction, we\n> can create \"OP_RETURN <anything>\" as a branch in the TapScript. In this\n> way, we can store that data off-chain and we can always prove that they are\n> connected with some taproot address, that was pushed on-chain. Also, we can\n> store more than 80 bytes for \"free\", because no such taproot branch will be\n> ever pushed on-chain and used as an input. That means we can use \"OP_RETURN\n> <1.5 GB of data>\", create some address having that taproot branch, and\n> later prove to anyone that such \"1.5 GB of data\" is connected with our\n> taproot address.\n>\n> Currently in Bitcoin Core we have \"data\" field in \"createrawtransaction\".\n> Should the implementation be changed to place that data in a TapScript\n> instead of creating separate OP_RETURN output? What do you think?\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220224/da19f3fb/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-25T00:04:54",
                "message_text_only": "Good morning Zac,\n\n> Reducing the footprint of storing data on-chain might better be achieved by *supporting* it.\n>\n> Currently storing data is wasteful because it is embedded inside an OP_RETURN within a transaction structure. As an alternative, by supporting storing of raw data without creating a transaction, waste can be reduced.\n\nIf the data is not embedded inside a transaction, how would I be able to pay a miner to include the data on the blockchain?\n\nI need a transaction in order to pay a miner anyway, so why not just embed it into the same transaction I am using to pay the miner?\n(i.e. the current design)\n\n\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Zac Greenwood",
                "date": "2022-02-25T01:12:34",
                "message_text_only": "Hi ZmnSCPxj,\n\nAny benefits of my proposal depend on my presumption that using a standard\ntransaction for storing data must be inefficient. Presumably a transaction\ntakes up significantly more on-chain space than the data it carries within\nits OP_RETURN. Therefore, not requiring a standard transaction for data\nstorage should be more efficient. Facilitating data storage within some\nspecialized, more space-efficient data structure at marginally lower fee\nper payload-byte should enable reducing the footprint of storing data\non-chain.\n\nIn case storing data through OP_RETURN embedded within a transaction is\noptimal in terms of on-chain footprint then my proposal doesn\u2019t seem useful.\n\nZac\n\nOn Fri, 25 Feb 2022 at 01:05, ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Zac,\n>\n> > Reducing the footprint of storing data on-chain might better be achieved\n> by *supporting* it.\n> >\n> > Currently storing data is wasteful because it is embedded inside an\n> OP_RETURN within a transaction structure. As an alternative, by supporting\n> storing of raw data without creating a transaction, waste can be reduced.\n>\n> If the data is not embedded inside a transaction, how would I be able to\n> pay a miner to include the data on the blockchain?\n>\n> I need a transaction in order to pay a miner anyway, so why not just embed\n> it into the same transaction I am using to pay the miner?\n> (i.e. the current design)\n>\n>\n>\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220225/6ee277d1/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-25T03:19:34",
                "message_text_only": "Good morning Zac,\n\n> Hi\u00a0ZmnSCPxj,\n>\n> Any benefits of my proposal depend on my presumption that using a standard transaction for storing data must be inefficient. Presumably a transaction takes up significantly more on-chain space than the data it carries within its OP_RETURN. Therefore, not requiring a standard transaction for data storage should be more efficient. Facilitating data storage within some specialized, more space-efficient data structure at marginally lower fee per payload-byte should enable reducing the footprint of storing data on-chain.\n>\n> In case storing data through OP_RETURN embedded within a transaction is optimal in terms of on-chain footprint then my proposal doesn\u2019t seem useful.\n\nYou need to have some assurance that, if you pay a fee, this data gets on the blockchain.\nAnd you also need to pay a fee for the blockchain space.\nIn order to do that, you need to indicate an existing UTXO, and of course you have to provably authorize the spend of that UTXO.\nBut that is already an existing transaction structure, the transaction input.\nIf you are not going to pay an entire UTXO for it, you need a transaction output as well to store the change.\n\nYour signature needs to cover the data being published, and it is more efficient to have a single signature that covers the transaction input, the transaction output, and the data being published.\nWe already have a structure for that, the transaction.\n\nSo an `OP_RETURN` transaction output is added and you put published data there, and existing constructions make everything Just Work (TM).\n\nNow I admit we can shave off some bytes.\nPure published data does not need an amount, and using a transaction output means there is always an amount field.\nWe do not want the `OP_RETURN` opcode itself, though if the data is variable-size we do need an equivalent to the `OP_PUSH` opcode (which has many variants depending on the size of the data).\n\nBut that is not really a lot of bytes, and adding a separate field to the transaction would require a hardfork.\nWe cannot use the SegWit technique of just adding a new field that is not serialized for `txid` and `wtxid` calculations, but is committed in a new id, let us call it `dtxid`, and a new Merkle Tree added to the coinbase.\nIf we *could*, then a separate field for data publication would be softforkable, but the technique does not apply here.\nThe reason we cannot use that technique is that we want to save bytes by having the signature cover the data to be published, and signatures need to be validated by pre-softfork nodes looking at just the data committed to in `wtxid`.\nIf you have a separate signature that is in the `dtxid`, then you spend more actual bytes to save a few bytes.\n\nSaving a few bytes for an application that is arguably not the \"job\" of Bitcoin (Bitcoin is supposed to be for value transfer, not data archiving) is not enough to justify a **hard**fork.\nAnd any softfork seems likely to spend more bytes than what it could save.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Zac Greenwood",
                "date": "2022-02-25T07:15:06",
                "message_text_only": "Hi ZmnSCPxj,\n\nTo me it seems that more space can be saved.\n\nThe data-\u201ctransaction\u201d need not specify any output. The network could\nsubtract the fee amount of the transaction directly from the specified\nUTXO. A fee also need not to be specified. It can be calculated in advance\nboth by the network and the transaction sender based on the size of the\ndata.\n\nThe calculation of the fee should be such that it only marginally cheaper\nto use this new construct over using one or more transactions. For\ninstance, sending 81 bytes should cost as much as two OP_RETURN\ntransactions (minus some marginal discount to incentivize the use of this\nmore efficient way to store data).\n\nIf the balance of the selected UTXO is insufficient to pay for the data\nthen the transaction will be invalid.\n\nI can\u2019t judge whether this particular approach would require a hardfork,\nsadly.\n\nZac\n\n\nOn Fri, 25 Feb 2022 at 04:19, ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Zac,\n>\n> > Hi ZmnSCPxj,\n> >\n> > Any benefits of my proposal depend on my presumption that using a\n> standard transaction for storing data must be inefficient. Presumably a\n> transaction takes up significantly more on-chain space than the data it\n> carries within its OP_RETURN. Therefore, not requiring a standard\n> transaction for data storage should be more efficient. Facilitating data\n> storage within some specialized, more space-efficient data structure at\n> marginally lower fee per payload-byte should enable reducing the footprint\n> of storing data on-chain.\n> >\n> > In case storing data through OP_RETURN embedded within a transaction is\n> optimal in terms of on-chain footprint then my proposal doesn\u2019t seem useful.\n>\n> You need to have some assurance that, if you pay a fee, this data gets on\n> the blockchain.\n> And you also need to pay a fee for the blockchain space.\n> In order to do that, you need to indicate an existing UTXO, and of course\n> you have to provably authorize the spend of that UTXO.\n> But that is already an existing transaction structure, the transaction\n> input.\n> If you are not going to pay an entire UTXO for it, you need a transaction\n> output as well to store the change.\n>\n> Your signature needs to cover the data being published, and it is more\n> efficient to have a single signature that covers the transaction input, the\n> transaction output, and the data being published.\n> We already have a structure for that, the transaction.\n>\n> So an `OP_RETURN` transaction output is added and you put published data\n> there, and existing constructions make everything Just Work (TM).\n>\n> Now I admit we can shave off some bytes.\n> Pure published data does not need an amount, and using a transaction\n> output means there is always an amount field.\n> We do not want the `OP_RETURN` opcode itself, though if the data is\n> variable-size we do need an equivalent to the `OP_PUSH` opcode (which has\n> many variants depending on the size of the data).\n>\n> But that is not really a lot of bytes, and adding a separate field to the\n> transaction would require a hardfork.\n> We cannot use the SegWit technique of just adding a new field that is not\n> serialized for `txid` and `wtxid` calculations, but is committed in a new\n> id, let us call it `dtxid`, and a new Merkle Tree added to the coinbase.\n> If we *could*, then a separate field for data publication would be\n> softforkable, but the technique does not apply here.\n> The reason we cannot use that technique is that we want to save bytes by\n> having the signature cover the data to be published, and signatures need to\n> be validated by pre-softfork nodes looking at just the data committed to in\n> `wtxid`.\n> If you have a separate signature that is in the `dtxid`, then you spend\n> more actual bytes to save a few bytes.\n>\n> Saving a few bytes for an application that is arguably not the \"job\" of\n> Bitcoin (Bitcoin is supposed to be for value transfer, not data archiving)\n> is not enough to justify a **hard**fork.\n> And any softfork seems likely to spend more bytes than what it could save.\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220225/f0f711c1/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-25T12:48:11",
                "message_text_only": "Good morning Zac,\n\n> Hi ZmnSCPxj,\n>\n> To me it seems that more space can be saved.\n>\n> The data-\u201ctransaction\u201d need not specify any output. The network could subtract the fee amount of the transaction directly from the specified UTXO.\n\nThat is not how UTXO systems like Bitcoin work.\nEither you consume the entire UTXO (take away the \"U\" from the \"UTXO\") completely and in full, or you do not touch the UTXO (and cannot get fees from it).\n\n> A fee also need not to be specified.\n\nFees are never explicit in Bitcoin; it is always the difference between total input amount minus the total output amount.\n\n> It can be calculated in advance both by the network and the transaction sender based on the size of the data.\n\nIt is already implicitly calculated by the difference between the total input amount minus the total output amount.\n\nYou seem to misunderstand as well.\nFee rate is computed from the fee (computed from total input minus total output) divided by the transaction weight.\nNodes do not compute fees from feerate and weight.\n\n> The calculation of the fee should be such that it only marginally cheaper to use this new construct over using one or more transactions. For instance, sending 81 bytes should cost as much as two OP_RETURN transactions (minus some marginal discount to incentivize the use of this more efficient way to store data).\n\nDo you want to change weight calculations?\n*reducing* weight calculations is a hardfork, increasing it is a softfork.\n\n> If the balance of the selected UTXO is insufficient to pay for the data then the transaction will be invalid.\n>\n> I can\u2019t judge whether this particular approach would require a hardfork, sadly.\n\nSee above note, if you want to somehow reduce the weight of the data so as to reduce the cost of data relative to `OP_RETURN`, that is a hardfork.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Zac Greenwood",
                "date": "2022-02-25T13:53:57",
                "message_text_only": "Hi ZmnSCPxj,\n\n> Either you consume the entire UTXO (take away the \"U\" from the \"UTXO\")\ncompletely and in full, or you do not touch the UTXO\n\nOk, so enabling spending a UTXO partly would be a significant departure\nfrom the systems\u2019 design philosophy.\n\nI have been unclear about the fee part. In my proposal there\u2019s only one\ninput and zero outputs, so normally there would be no way to set any fee.\nOne could add a fee field although that would be slightly wasteful \u2014 it may\nbe sufficient to just specify the fee *rate*, for instance 0-255\nsat/payload_byte, requiring only one byte for the fee. The calculation of\nthe actual fee can be performed by both the network and the sender. The fee\nequals payload_size*feerate +\nan-amount-calculated-by-preset-rules-such-that-it-raises-the-cost-of-the-transaction-to-only-marginally-less-than-what-it\nwould-have-cost-to-store-the-same-amount-of-data-using-one-or-more-OP_RETURN-transactions.\n\nHowever explicitly specifying the fee amount is probably preferable for the\nsake of transparency.\n\nI wonder if this proposal could technically work. I fully recognize though\nthat even if it would, it has close to zero chances becoming reality as it\nbreaks the core design based on *U*TXOs (and likely also a lot of existing\nsoftware) \u2014 thank you for pointing that out and for your helpful feedback.\n\nZac\n\n\nOn Fri, 25 Feb 2022 at 13:48, ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Zac,\n>\n> > Hi ZmnSCPxj,\n> >\n> > To me it seems that more space can be saved.\n> >\n> > The data-\u201ctransaction\u201d need not specify any output. The network could\n> subtract the fee amount of the transaction directly from the specified UTXO.\n>\n> That is not how UTXO systems like Bitcoin work.\n> Either you consume the entire UTXO (take away the \"U\" from the \"UTXO\")\n> completely and in full, or you do not touch the UTXO (and cannot get fees\n> from it).\n>\n> > A fee also need not to be specified.\n>\n> Fees are never explicit in Bitcoin; it is always the difference between\n> total input amount minus the total output amount.\n>\n> > It can be calculated in advance both by the network and the transaction\n> sender based on the size of the data.\n>\n> It is already implicitly calculated by the difference between the total\n> input amount minus the total output amount.\n>\n> You seem to misunderstand as well.\n> Fee rate is computed from the fee (computed from total input minus total\n> output) divided by the transaction weight.\n> Nodes do not compute fees from feerate and weight.\n>\n> > The calculation of the fee should be such that it only marginally\n> cheaper to use this new construct over using one or more transactions. For\n> instance, sending 81 bytes should cost as much as two OP_RETURN\n> transactions (minus some marginal discount to incentivize the use of this\n> more efficient way to store data).\n>\n> Do you want to change weight calculations?\n> *reducing* weight calculations is a hardfork, increasing it is a softfork.\n>\n> > If the balance of the selected UTXO is insufficient to pay for the data\n> then the transaction will be invalid.\n> >\n> > I can\u2019t judge whether this particular approach would require a hardfork,\n> sadly.\n>\n> See above note, if you want to somehow reduce the weight of the data so as\n> to reduce the cost of data relative to `OP_RETURN`, that is a hardfork.\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220225/f60e1849/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "OP_RETURN inside TapScript",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Zac Greenwood",
                "vjudeu at gazeta.pl",
                "ZmnSCPxj",
                "Ruben Somsen"
            ],
            "messages_count": 11,
            "total_messages_chars_count": 29296
        }
    },
    {
        "title": "[bitcoin-dev] A Comparison Of LN and Drivechain Security In The Presence Of 51% Attackers",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-24T12:49:00",
                "message_text_only": "Good morning lightning-dev and bitcoin-dev,\n\nRecently, some dumb idiot, desperate to prove that recursive covenants are somehow a Bad Thing (TM), [necromanced Drivechains][0], which actually caused Paul Sztorc to [revive][1] and make the following statement:\n\n> As is well known, it is easy for 51% hashrate to double-spend in the LN, by censoring 'justice transactions'. Moreover, miners seem likely to evade retribution if they do this, as they can restrain the scale, timing, victims, circumstances etc of the attack.\n\nLet me state that, as a supposed expert developer of the Lightning Network (despite the fact that I probably spend more time ranting on the lists than actually doing something useful like improve C-Lightning or CLBOSS), the above statement is unequivocally ***true***.\n\nHowever, I believe that the following important points must be raised:\n\n* A 51% miner can only attack LN channels it is a participant in.\n* A 51% miner can simultaneously attack all Drivechain-based sidechains and steal all of their funds.\n\nIn order for \"justice transactions\" to come into play, an attacker has to have an old state of a channel.\nAnd only the channel participants have access to old state (modulo bugs and operator error on not being careful of toxic waste, but those are arguably as out of scope as operator error not keeping your privkey safe, or bugs that reveal your privkey).\n\nIf the 51% miner is not a participant on a channel, then it simply has no access to old state of the channel and cannot even *start* the above theft attack.\nIf the first step fails, then the fact that the 51% miner can perform the second step is immaterial.\n\nNow, this is not a perfect protection!\nWe should note that miners are anonymous and it is possible that there is already a 51% miner, and that that 51% miner secretly owns almost all nodes on the LN.\nHowever, even this also means there is some probability that, if you picked a node at random to make a channel with, then there is some probability that it is *not* a 51% miner and you are *still* safe from the 51% miner.\n\nThus, LN usage is safer than Drivechain usage.\nOn LN, if you make a channel to some LN node, there is a probability that you make a channel with a non-51%-miner, and if you luck into that, your funds are still safe from the above theft attack, because the 51% miner cannot *start* the attack by getting old state and publishing it onchain.\nOn Drivechain, if you put your funds in *any* sidechain, a 51% miner has strong incentive to attack all sidechains and steal all the funds simultaneously.\n\n--\n\nNow, suppose we have:\n\n* a 51% miner\n* Alice\n* Bob\n\nAnd that 51% miner != Alice, Alice != Bob, and Bob != 51% miner.\n\nWe could ask: Suppose Alice wants to attack Bob, could Alice somehow convince 51% miner to help it steal from Bob?\n\nFirst, we should observe that *all* economically-rational actors have a *time preference*.\nThat is, N sats now is better than N sats tomorrow.\nIn particular, both the 51% miner *and* Alice the attacker have this time preference, as does victim Bob.\n\nWe can observe that in order for Alice to benefit from the theft, it has to *wait out* the `OP_CSV` before it can finalize the theft.\nAlice can offer fees to the miner only after the `OP_CSV` delay.\n\nHowever, Bob can offer fees *right now* on the justice transaction.\nAnd the 51% miner, being economically rational, would prefer the *right now* funds to the *maybe later* promise by Alice.\n\nIndeed, if Bob offered a justice transaction paying the channel amount minus 1 satoshi (i.e. Bob keeps 1 satoshi), then Alice has to beat that by offering the entire channel amount to the 51% miner.\nBut the 51% miner would then have to wait out the `OP_CSV` delay before it gets the funds.\nIts time preference may be large enough (if the `OP_CSV` delay is big enough) that it would rather side with Bob, who can pay channel amount - 1 right now, than Alice who promises to pay channel amount later.\n\n\"But Zeeman, Alice could offer to pay now from some onchain funds Alice has, and Alice can recoup the losses later!\"\nBut remember, Alice *also* has a time preference!\nLet us consider the case where Alice promises to bribe 51% miner *now*, on the promise that 51% miner will block the Bob justice transaction and *then* Alice gets to enjoy the entire channel amount later.\nBob can counter by offering channel amount - 1 right now on the justice transaction.\nThe only way for Alice to beat that is to offer channel amount right now, in which case 51% miner will now side with Alice.\n\nBut what happens to Alice in that case?\nIt loses out on channel amount right now, and then has to wait `OP_CSV` delay, to get the exact same amount later!\nIt gets no benefit, so this is not even an investment.\nIt is just enforced HODLing, but Alice can do that using `OP_CLTV` already.\n\nWorse, Alice has to trust that 51% miner will indeed block the justice transaction.\nBut if 51% miner is unscrupulous, it could do:\n\n* Get the bribe from Alice right now.\n* After the bribe from Alice confirms, confirm the justice transaction (which has a bribe from Bob).\n* Thus:\n  * Alice loses the channel amount.\n  * Bob keeps 1 satoshi.\n  * 51% miner gets channel amount + channel amount - 1.\n\nNow of course, we can eliminate the need for trust by using some kind of smart contract.\nUnfortunately for Alice, there is no contract that Alice and 51% miner can engage in, to ensure that 51% miner will block the justice transaction, which itself does *not* require that 51% miner wait out the `OP_CSV` delay.\nEither the payment from Alice to 51% miner is delayed (and the 51% miner suffers the time preference discount) or the 51% miner has to offer a bond that only gets released after the Alice theft succeeds (and again the 51% miner suffers the time preference discount on that bond).\n\nThus, due to the `OP_CSV` delay, the honest participant always has the upper hand, even in a 51% miner scenario.\nIf your channel is *not* with the 51% miner, your funds are still safe.\n\n--\n\nNow, we might consider, what if the 51% miner always blocks *all* Lightning-related transactions?\nIn that case, it loses out on any bribes that any LN participants would offer.\n\nFurther, with Taproot, a mutual LN channel close is indistinguishable from a singlesig spend.\nThus, not all LN-related transactions can be censored by the 51% miner.\nExtensive use of Taproot Tapleaves can also make it difficult for a 51% miner to differentiate between LN and other protocols (though that *does* mean we should probably e.g. coordiante with other protocols like CoinSwap, CoinPool etc. so that the \"shape\" of Taproot Tapleaves is consistent across protocols).\n\n--\n\nA final note: in the presence of channel factories, the *entire* factory is at risk if at least one participant is the 51% miner or a sockpuppet thereof.\nThus, channel factories trade off even further scaling, at the cost of reduced protection against 51% miners.\n\n\nRegards,\nZmnSCPxj\n\n[0]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019976.html\n[1]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019978.html"
            },
            {
                "author": "Paul Sztorc",
                "date": "2022-02-24T21:39:40",
                "message_text_only": "On 2/24/2022 7:49 AM, ZmnSCPxj via bitcoin-dev wrote:\n...\n\n>> ... it is easy for 51% hashrate to double-spend in the LN ...\n> ... the above statement is unequivocally ***true***.\n\nBoth LN and Drivechain are vulnerable to miner-theft; and both use their design to deter theft.\n\n> However, I believe that the following important points must be raised:\n>\n> * A 51% miner can only attack LN channels it is a participant in.\n> * A 51% miner can simultaneously attack all Drivechain-based sidechains and steal all of their funds.\n\nIn LN, the main obstacle is that your miner-coalition must first join the channel.\n\nIn DC, the main obstacle is that your miner-coalition must construct a txn obeying the Bip300 rules. Knowing that SPV proofs allow miner-theft, the Bip300 rules are designed specifically to try to thwart miner-theft.\n\n***\n\nI don't think I can stop people from being ignorant about Drivechain. But I can at least allow the Drivechain-knowledgable to identify each other.\n\nSo here below, I present a little \"quiz\". If you can answer all of these questions, then you basically understand Drivechain:\n\n0. We could change DC to make miner-theft impossible, by making it a layer1 consensus rule that miners never steal. Why is this cure worse than the disease?\n1. If 100% hashrate wanted to steal coins from a DC sidechain *as quickly as possible*, how long would this take (in blocks)?\n2. Per sidechain per year (ie, per 52560 blocks), how many DC withdrawals can take place (maximum)? How many can be attempted?\n      (Ie, how does the 'train track metaphor' work, from ~1h5m in the \"Overview and Misconceptions\" video)?\n3. Only two types of people should ever be using the DC withdrawal system at all.\n   3a. Which two?\n   3b. How is everyone else, expected to move their coins from chain to chain?\n   3c. (Obviously, this improves UX.) But why does it also improve security?\n--\n4. What do the parameters b and m stand for (in the DC security model)?\n5. How can m possibly be above 1? Give an example of a sidechain-attribute which may cause this situation to arise.\n6. For which range of m, is DC designed to deter sc-theft?\n7. If DC could be changed to magically deter theft across all ranges of m, why would that be bad for sidechain users in general?\n--\n8. If imminent victims of a DC-based theft, used a mainchain UASF to prohibit the future theft-withdrawal, then how would this affect non-DC users?\n9. In what ways might the BTC network one day become uncompetitive? And how is this different from caring about a sidechain's m and b?\n--\n10. If DC were successful, Altcoin-investors would be harmed. Two Maximalist-groups would also be slightly harmed -- who are these?\n\n***\n\n> Thus, LN usage is safer than Drivechain usage.\n\nNeither LN nor DC, are intended for use by everyone in every circumstance.\n\nDC can simulate a zcash sidechain, but it can not allow for instant off-chain payments. So DC-vs-LN would never be an apples-to-apples comparison, on any criterion.\n\nThe end user should be free to decide, what risks they take with their money. Today, users can sell their BTC for Solana (or BSV or whatever). So, to me it seems clear that they should be \"allowed\" to spend their BTC to a Bip300 script, just as they are allowed to open a LN channel.\n\n-Paul\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220224/29005c37/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-26T07:39:36",
                "message_text_only": "Good morning Paul,\n\n\n> I don't think I can stop people from being ignorant about Drivechain. But I can at least allow the Drivechain-knowledgable to identify each other.\n>\n> So here below, I present a little \"quiz\". If you can answer all of these questions, then you basically understand Drivechain:\n>\n> 0. We could change DC to make miner-theft impossible, by making it a layer1 consensus rule that miners never steal. Why is this cure worse than the disease?\n\nNow miners are forced to look at all sideblocks, not optionally do so if it is profitable for them.\n\n> 1. If 100% hashrate wanted to steal coins from a DC sidechain *as quickly as possible*, how long would this take (in blocks)?\n\n13,150 (I think this is how you changed it after feedback from this list, I think I remember it was ~3000 before or thereabouts.)\n\n> 2. Per sidechain per year (ie, per 52560 blocks), how many DC withdrawals can take place (maximum)? How many can be attempted?\n>      (Ie, how does the 'train track metaphor' work, from ~1h5m in the \"Overview and Misconceptions\" video)?\n\nI hate watching videos, I can read faster than anyone can talk (except maybe Laolu, he speaks faster than I can process, never mind read).\n\n~4 times (assuming 52560 block per year, which may vary due to new miners, hashrate drops, etc)\n\n> 3. Only two types of people should ever be using the DC withdrawal system at all.\n>   3a. Which two?\n\na.  Miners destroying the sidechain because the sidechain is no longer viable.\nb.  Aggregators of sidechain-to-minechain transfers and large whales.\n\n>   3b. How is everyone else, expected to move their coins from chain to chain?\n\nCross-system atomic swaps.\n(I use \"System\" here since the same mechanism works for Lightning channels, and channels are not blockchains.)\n\n>   3c. (Obviously, this improves UX.) But why does it also improve security?\n\nDrivechain-based pegged transfers are aggregates of many smaller transfers and thus every transfer out from the sidechain contributes its \"fee\" to the security of the peg.\n\n> --\n> 4. What do the parameters b and m stand for (in the DC security model)?\n\nm is how much people want to kill a sidechain, 0 = everybody would be sad if it died and would rather burn all their BTC forever than continue living, 1 = do not care, > 1 people want to actively kill the sidechain.\n\nb is how much profit a mainchain miner expects from supporting a sidechain (do not remember the unit though).\nSomething like u = a + b where a is the mainchain, b is the sidechain, u is the total profit.\nOr fees?  Something like that.\n\n> 5. How can m possibly be above 1? Give an example of a sidechain-attribute which may cause this situation to arise.\n\nThe sidechain is a total scam.\nA bug may be found in the sidechain that completely negates any security it might have, thus removing any desire to protect the sidechain and potentially make users want to destroy it completely rather than let it continue.\nPeople end up hating sidechains completely.\n\n> 6. For which range of m, is DC designed to deter sc-theft?\n\nm <= 1\n\n> 7. If DC could be changed to magically deter theft across all ranges of m, why would that be bad for sidechain users in general?\n\nBecause the sidechain would already be part of mainchain consensus.\n\n> --\n> 8. If imminent victims of a DC-based theft, used a mainchain UASF to prohibit the future theft-withdrawal, then how would this affect non-DC users?\n\nIf the non-DC users do not care, then they are unaffected.\nIf the non-DC users want to actively kill the sidechain, they will counterattack with an opposite UASF and we have a chainsplit and sadness and mutual destruction and death and a new subreddit.\n\n> 9. In what ways might the BTC network one day become uncompetitive? And how is this different from caring about a sidechain's m and b?\n\nIf it does not enable scaling technology fast enough to actually be able to enable hyperbitcoinization.\n\nSidechains are not a scaling solution, so caring about m and b is different because your focus is not on scaling.\n\n> --\n> 10. If DC were successful, Altcoin-investors would be harmed. Two Maximalist-groups would also be slightly harmed -- who are these?\n\nDunno!\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-02-26T14:58:12",
                "message_text_only": "> m is how much people want to kill a sidechain, 0 = everybody would be sad\nif it died and would rather burn all their BTC forever than continue living\n\nMath is brutal\n\nOn Sat, Feb 26, 2022, 01:39 ZmnSCPxj via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n> Good morning Paul,\n>\n>\n> > I don't think I can stop people from being ignorant about Drivechain.\n> But I can at least allow the Drivechain-knowledgable to identify each other.\n> >\n> > So here below, I present a little \"quiz\". If you can answer all of these\n> questions, then you basically understand Drivechain:\n> >\n> > 0. We could change DC to make miner-theft impossible, by making it a\n> layer1 consensus rule that miners never steal. Why is this cure worse than\n> the disease?\n>\n> Now miners are forced to look at all sideblocks, not optionally do so if\n> it is profitable for them.\n>\n> > 1. If 100% hashrate wanted to steal coins from a DC sidechain *as\n> quickly as possible*, how long would this take (in blocks)?\n>\n> 13,150 (I think this is how you changed it after feedback from this list,\n> I think I remember it was ~3000 before or thereabouts.)\n>\n> > 2. Per sidechain per year (ie, per 52560 blocks), how many DC\n> withdrawals can take place (maximum)? How many can be attempted?\n> >      (Ie, how does the 'train track metaphor' work, from ~1h5m in the\n> \"Overview and Misconceptions\" video)?\n>\n> I hate watching videos, I can read faster than anyone can talk (except\n> maybe Laolu, he speaks faster than I can process, never mind read).\n>\n> ~4 times (assuming 52560 block per year, which may vary due to new miners,\n> hashrate drops, etc)\n>\n> > 3. Only two types of people should ever be using the DC withdrawal\n> system at all.\n> >   3a. Which two?\n>\n> a.  Miners destroying the sidechain because the sidechain is no longer\n> viable.\n> b.  Aggregators of sidechain-to-minechain transfers and large whales.\n>\n> >   3b. How is everyone else, expected to move their coins from chain to\n> chain?\n>\n> Cross-system atomic swaps.\n> (I use \"System\" here since the same mechanism works for Lightning\n> channels, and channels are not blockchains.)\n>\n> >   3c. (Obviously, this improves UX.) But why does it also improve\n> security?\n>\n> Drivechain-based pegged transfers are aggregates of many smaller transfers\n> and thus every transfer out from the sidechain contributes its \"fee\" to the\n> security of the peg.\n>\n> > --\n> > 4. What do the parameters b and m stand for (in the DC security model)?\n>\n> m is how much people want to kill a sidechain, 0 = everybody would be sad\n> if it died and would rather burn all their BTC forever than continue\n> living, 1 = do not care, > 1 people want to actively kill the sidechain.\n>\n> b is how much profit a mainchain miner expects from supporting a sidechain\n> (do not remember the unit though).\n> Something like u = a + b where a is the mainchain, b is the sidechain, u\n> is the total profit.\n> Or fees?  Something like that.\n>\n> > 5. How can m possibly be above 1? Give an example of a\n> sidechain-attribute which may cause this situation to arise.\n>\n> The sidechain is a total scam.\n> A bug may be found in the sidechain that completely negates any security\n> it might have, thus removing any desire to protect the sidechain and\n> potentially make users want to destroy it completely rather than let it\n> continue.\n> People end up hating sidechains completely.\n>\n> > 6. For which range of m, is DC designed to deter sc-theft?\n>\n> m <= 1\n>\n> > 7. If DC could be changed to magically deter theft across all ranges of\n> m, why would that be bad for sidechain users in general?\n>\n> Because the sidechain would already be part of mainchain consensus.\n>\n> > --\n> > 8. If imminent victims of a DC-based theft, used a mainchain UASF to\n> prohibit the future theft-withdrawal, then how would this affect non-DC\n> users?\n>\n> If the non-DC users do not care, then they are unaffected.\n> If the non-DC users want to actively kill the sidechain, they will\n> counterattack with an opposite UASF and we have a chainsplit and sadness\n> and mutual destruction and death and a new subreddit.\n>\n> > 9. In what ways might the BTC network one day become uncompetitive? And\n> how is this different from caring about a sidechain's m and b?\n>\n> If it does not enable scaling technology fast enough to actually be able\n> to enable hyperbitcoinization.\n>\n> Sidechains are not a scaling solution, so caring about m and b is\n> different because your focus is not on scaling.\n>\n> > --\n> > 10. If DC were successful, Altcoin-investors would be harmed. Two\n> Maximalist-groups would also be slightly harmed -- who are these?\n>\n> Dunno!\n>\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220226/cf55492c/attachment.html>"
            },
            {
                "author": "Paul Sztorc",
                "date": "2022-02-27T00:42:22",
                "message_text_only": "Not bad, but not particularly good either.\n\nDefinitely correct:\n \u00a0 1\u00a0 (plus extra credit, it was originally 1008+2016),\n \u00a0 3a \"whales\"\n \u00a0 3b (atomic swaps is the \"official\" answer, but otc trading is also \nacceptable, or just \"trade\" in general)\n \u00a0 6\n \u00a0 9\u00a0 part one\n\nClose, but not quite right:\n \u00a0 2\u00a0 (part one \"~4\" is correct, but you didn't answer part two)\n \u00a0 3a \"attacker- miners\" is not the way I see it at all\n \u00a0 3c true, but I was talking about withdrawal security, not hashrate, \n[this is related to the 3a \"attacker miners\" mis-answer]\n \u00a0 4\u00a0 ? you seem to have not very seriously answered this. The \nparameters are spelled out in the original Nov 2015 post\n\nSome kind of miscommunication may have happened:\n \u00a0 8 -- I was more thinking, what happens if the UASF fails (in \nthwarting miners) vs succeeds. (I take it for granted that non-DC users \nwill prefer to do nothing, and prefer to be unaffected.)\n\nSeems wrong to me:\n \u00a0 0\u00a0 seems like a pretty big misunderstanding happened here, or else \nyou mistakenly typo'd the wrong word\n \u00a0 5\u00a0 (you started with m=1 examples, which is not what was requested; \nand finished with something not a sidechain attribute)\n \u00a0 7\u00a0 [related to the miss on #5] ; it is not a re-ask of question #0\n \u00a0 9\u00a0 part two is wrong\n \u00a010\u00a0 you did not answer\n\nPaul\n\n\nOn 2/26/2022 2:39 AM, ZmnSCPxj wrote:\n> Good morning Paul,\n>\n>\n>> I don't think I can stop people from being ignorant about Drivechain. But I can at least allow the Drivechain-knowledgable to identify each other.\n>>\n>> So here below, I present a little \"quiz\". If you can answer all of these questions, then you basically understand Drivechain:\n>>\n>> 0. We could change DC to make miner-theft impossible, by making it a layer1 consensus rule that miners never steal. Why is this cure worse than the disease?\n> Now miners are forced to look at all sideblocks, not optionally do so if it is profitable for them.\n>\n>> 1. If 100% hashrate wanted to steal coins from a DC sidechain *as quickly as possible*, how long would this take (in blocks)?\n> 13,150 (I think this is how you changed it after feedback from this list, I think I remember it was ~3000 before or thereabouts.)\n>\n>> 2. Per sidechain per year (ie, per 52560 blocks), how many DC withdrawals can take place (maximum)? How many can be attempted?\n>>       (Ie, how does the 'train track metaphor' work, from ~1h5m in the \"Overview and Misconceptions\" video)?\n> I hate watching videos, I can read faster than anyone can talk (except maybe Laolu, he speaks faster than I can process, never mind read).\n>\n> ~4 times (assuming 52560 block per year, which may vary due to new miners, hashrate drops, etc)\n>\n>> 3. Only two types of people should ever be using the DC withdrawal system at all.\n>>    3a. Which two?\n> a.  Miners destroying the sidechain because the sidechain is no longer viable.\n> b.  Aggregators of sidechain-to-minechain transfers and large whales.\n>\n>>    3b. How is everyone else, expected to move their coins from chain to chain?\n> Cross-system atomic swaps.\n> (I use \"System\" here since the same mechanism works for Lightning channels, and channels are not blockchains.)\n>\n>>    3c. (Obviously, this improves UX.) But why does it also improve security?\n> Drivechain-based pegged transfers are aggregates of many smaller transfers and thus every transfer out from the sidechain contributes its \"fee\" to the security of the peg.\n>\n>> --\n>> 4. What do the parameters b and m stand for (in the DC security model)?\n> m is how much people want to kill a sidechain, 0 = everybody would be sad if it died and would rather burn all their BTC forever than continue living, 1 = do not care, > 1 people want to actively kill the sidechain.\n>\n> b is how much profit a mainchain miner expects from supporting a sidechain (do not remember the unit though).\n> Something like u = a + b where a is the mainchain, b is the sidechain, u is the total profit.\n> Or fees?  Something like that.\n>\n>> 5. How can m possibly be above 1? Give an example of a sidechain-attribute which may cause this situation to arise.\n> The sidechain is a total scam.\n> A bug may be found in the sidechain that completely negates any security it might have, thus removing any desire to protect the sidechain and potentially make users want to destroy it completely rather than let it continue.\n> People end up hating sidechains completely.\n>\n>> 6. For which range of m, is DC designed to deter sc-theft?\n> m <= 1\n>\n>> 7. If DC could be changed to magically deter theft across all ranges of m, why would that be bad for sidechain users in general?\n> Because the sidechain would already be part of mainchain consensus.\n>\n>> --\n>> 8. If imminent victims of a DC-based theft, used a mainchain UASF to prohibit the future theft-withdrawal, then how would this affect non-DC users?\n> If the non-DC users do not care, then they are unaffected.\n> If the non-DC users want to actively kill the sidechain, they will counterattack with an opposite UASF and we have a chainsplit and sadness and mutual destruction and death and a new subreddit.\n>\n>> 9. In what ways might the BTC network one day become uncompetitive? And how is this different from caring about a sidechain's m and b?\n> If it does not enable scaling technology fast enough to actually be able to enable hyperbitcoinization.\n>\n> Sidechains are not a scaling solution, so caring about m and b is different because your focus is not on scaling.\n>\n>> --\n>> 10. If DC were successful, Altcoin-investors would be harmed. Two Maximalist-groups would also be slightly harmed -- who are these?\n> Dunno!\n>\n>\n> Regards,\n> ZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "A Comparison Of LN and Drivechain Security In The Presence Of 51% Attackers",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "Billy Tetrud",
                "Paul Sztorc"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 25380
        }
    },
    {
        "title": "[bitcoin-dev] Documenting the lifetime of a transaction during mempool congestion from the perspective of a rational user",
        "thread_messages": [
            {
                "author": "Billy Tetrud",
                "date": "2022-02-26T05:35:51",
                "message_text_only": "The crux of the type of situation you're talking about is where a source\nthat might revert their payment by rbf double spending sends you money. You\nmentioned this situation is \"not unlikely\". What kind of prevalence does\nthis happen with today?\n\nAlso my question is, if you've been paid by someone like this, what right\ndo you really have to this money? Is the other side buying something from\nyou? No one should be considering something actually bought unless it's got\nsufficient confirmations. Anyone following that rule isn't losing anything\nby simply waiting for the transaction to confirm. If the transaction is\ndouble spent, who cares?\n\nThis seems like a situation where adding software and ui complexity is not\nworth it to reach the maximization you're talking about. It feels more like\nopportunistic stealing than actual commerce. Maybe it would be a social\ngood to prevent attempted scammers from scamming people, but the only\npeople who would be affected are 0 conf people. And that problem can be\nsolved much more easily and generally (eg by clear messaging around\ntransaction finalization) than by complicating coin selection.\n\nOn Thu, Jan 13, 2022, 15:07 Jeremy via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Devs,\n>\n> This email is primarily about existing wallet behaviors and user\n> preferences, and not about CTV. However, towards the end I will describe\n> the relevance of CTV, but the email is worth reading even if you have no\n> interest in CTV as the problems described exist today.\n>\n> One point of confusion I've seen while discussing CTV based congestion\n> control is that it requires a bunch of new wallet software.\n>\n> Most of the software requirements that would make CTV work well are things\n> that either already exist in Bitcoin Core, or are 'bugs' (where bug is\n> defined as deviation from rational utility maximizing behavior) that should\n> be fixed *whether or not CTV exists.*\n>\n> In this post, I am going to walk through what I expect rational behavior\n> to be for a not unlikely circumstance.\n>\n> First, let's define what rational behavior for a wallet is. A rational\n> wallet should have a few goals:\n>\n> 1) Maximize 'fully trusted' balance (fully confirmed + unconfirmed change\n> outputs from our own txns)\n> 2) Process payments requested by the owner within the \"urgency budget\"\n> requested by the user.\n> 3) Maximize \"privacy\" (this is a vague goal, so we'll largely ignore it\n> here.).\n>\n> Rational wallet behavior may not be possible without metadata. For\n> example, a rational wallet might prompt the user for things like \"how much\n> do you trust the sender of this payment to not RBF this transaction?\", or\n> \"how much do you trust this sender to not double spend you?\". For example,\n> a self-transfer from cold wallet to hot wallet could have a trust score of\n> 1, whereas a payment from an anonymous source would have a trust score of\n> 0. Exchanges where you have a legal agreement to not RBF might sit\n> somewhere in between. Other pieces of exogenous information that could\n> inform wallet behavior include \"has hashrate decreased recently, making\n> longer reorgs likely\".\n>\n> In the model above, a user does not request transactions, they request\n> payments. The rational wallet serves as an agent to assist the user in\n> completing these payments. For example, if I have a wallet with a single\n> unconfirmed output, and I spend from it to pay Alice, if the unconfirmed\n> gets replaced, my wallet should track that it was replaced and prompt me to\n> re-sign a new transaction. Rational wallets that maximize balance should be\n> careful to ensure that replaced payments are exclusive, guaranteed either\n> through sufficient confirmations or 'impossibility proofs' by reusing an\n> input (preventing double-send behavior).\n>\n> -----------------------------\n>\n> Now that we've sketched out a basic framework for what a rational wallet\n> should be doing, we can describe what the process of receiving a payment is.\n>\n> Suppose I have a wallet with a bevy of fully confirmed coins such that for\n> my future payments I am sufficiently funded.\n>\n> Then, I receive a payment from a highly trusted source (e.g., self\n> transfer) that is unconfirmed.\n>\n> I then seek to make an outgoing payment. I should have no preference\n> towards or against spending the unconfirmed transfer, I should simply\n> account for it's cost in coin selection of CPFP-ing the parent transaction.\n> If fees are presently historically low, I may have a preference to spend it\n> so as to not have a higher fee later (consolidation).\n>\n> Later, I receive payment from an untrusted source (e.g., an anonymous\n> donation to me). I have no reason to trust this won't be double spent.\n> Perhaps I can even observe that this output has been RBF'd many times\n> previously. I do not count on this money arriving. The feerate on the\n> transaction suggests it won't be confirmed immediately.\n>\n> In order to maximize balance, I should prioritize spending from this\n> output (even if I don't have a payment I desire to make) in order to CPFP\n> it to the top of the mempool and guarantee I am paid. This is inherently\n> \"free\" since my cost to CPFP would be checked to be lower than the funds I\n> am receiving, and I have no expected value to receive the payment if it is\n> not confirmed. If I do have a transaction I desire to do, I should\n> prioritize spending this output at that time. If not, I would do a CPFP\n> just in favor of balance maximizing. Perhaps I can piggyback something\n> useful, like speculatively opening a lightning channel.\n>\n> If I just self-spend to CPFP, it is very simple since the only party set\n> up for disappointment is myself (note: I patched the behavior in this case\n> to accurately *not* count this as a trusted balance in\n> https://github.com/bitcoin/bitcoin/pull/16766, since a parent could\n> disrupt this). However, if I try to make a payment, my wallet agent must\n> somehow prompt me to re-sign or automatically sign an alternative payment\n> once it is proven (e.g. 6 blocks) I won't receive the output, or to re-sign\n> on a mutually exclusive output (e.g., fee bumping RBF) such that issuing\n> two payments will not causes a double-send error. This adds complexity to\n> both the user story and logic, but is still rational.\n>\n> Now, suppose that I receive a new payment from  a **trusted** source that\n> is a part of a \"long chain\". A long chain is a series of transactions that\n> are all unconfirmed in the mempool. This long-chain is in the bottom of the\n> mempool, and is not expected to confirm any time soon.\n>\n> My wallet should be configured such that it saves not only all ancestors\n> of the transaction paying me, but also all descendants of the root\n> unconfirmed transaction paying me. If I do not save all ancestor\n> transactions, then it would be impossible for me to claim this payment at a\n> future date, and would violate balance maximization. But why save\n> descendants, if they do not concern me? Descendant transactions are\n> critical for balance maximization. Someone else's spend of an output is a\n> \"free\" CPFP subsidy for driving my transaction to completion (perhaps\n> \"descendants that increase the feerate of any parent\" is the correct thing\n> to save). Therefore if I want to maximize balance, I would rather keep\n> these transactions around should I ever need to rebroadcast the\n> transactions as it should be cheaper than having to CPFP by myself.\n>\n> Now, suppose that I receive a similar payment in a longchain from a series\n> of untrusted sources. The same arguments apply, but now I may have even\n> higher incentive to prioritize spending this coin since, if sender's trust\n> scores are independent, my total trust in that payment is decomposed\n> worst-case geometrically. It may not be a good assumption that trust scores\n> are independent, since a long chain might be generated as e.g. a series of\n> batch payments from one service provider.\n>\n> Briefly mentioned above is rebroadcasting. This is sort of an orthogonal\n> behavior to the above, but it is \"simple\" to explain. Wallet agents should\n> retransmit txns to the network if they believe other nodes are not aware of\n> them and they are likely to go into a block. This encapsulates personal\n> transactions as well as arbitrary transactions from third parties. There\n> are many privacy implications of rebroadcasting that are out of scope for\n> this post.\n>\n> -----------------\n>\n> All of the behaviors documented above are things that should happen today\n> if you would like to have a rational wallet that maximizes your balance and\n> makes payments.\n>\n> The reasons we don't do some of these things are, as far as I can tell:\n>\n> 1) Engineering Complexity\n> 2) UX Complexity (simpler to make unconfirmed outputs \"unspendable\" to\n> minimize transaction reissuing)\n> 3) Mempool backlog is low, things are confirmed quickly if a sender pays a\n> relatively low fee\n>\n> Certain wallets already have parts of this functionality baked in to an\n> extent. For example, in Lightning Channels, you will drive payments to\n> completion to prevent HTLC timeouts during contested closes (HTLCs == low\n> trust score payments).\n>\n> Should Bitcoin see development of a more robust fee market, it is highly\n> likely the rational behaviors described above would be emergent among all\n> bitcoin wallets (who would want to use a Bitcoin wallet that gets you less\n> money over time?). This email is not just a \"Bitcoin Core\" thing, hence not\n> being an issue on Bitcoin Core where there are currently deviations from\n> the above behaviors.\n>\n> -----------------\n>\n> What's CTV got to do with it?\n>\n> A common critique of congestion control using CTV is that it complicates\n> wallet behavior because congestion control is designed to be useful in the\n> circumstances above. CTV and congestion control do not cause these\n> conditions. These conditions already exist whether or not we have\n> congestion control.\n>\n> Where congestion control helps is that, in a world with a full mempool,\n> you have fewer payments that are *actually* unconfirmed because exchanges\n> that batch can fully confirm a constant sized root transaction and the\n> sub-trees of transactions locked in by CTV can be treated as fully trusted.\n> This helps reduce the need for the (rational) behavior of CPFP bumping your\n> own payments on receipt from lower trust senders. Further, the expansion of\n> the transaction tree can be done by other users receiving, so you have an\n> incentive to wait for funds to mature as someone else might pay for\n> expansion. These two factors mean that CTV congestion control can exert a\n> dramatic back pressure on transaction urgency by unbundling the blockspace\n> demand for spending and receiving coins. There are other synergies -- such\n> as non-interactive channel opens -- that further improve the amount of\n> reduction of time-preference in full on-chain resolution.\n>\n> I hope this email helps clarify why CTV Congestion Control isn't\n> particularly creating a wallet architecture problem, it's helping to solve\n> an existing one.\n>\n> Best,\n>\n> Jeremy\n>\n>\n>\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n> <https://twitter.com/JeremyRubin>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220225/41175dc5/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Documenting the lifetime of a transaction during mempool congestion from the perspective of a rational user",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Billy Tetrud"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 11568
        }
    },
    {
        "title": "[bitcoin-dev] `OP_FOLD`: A Looping Construct For Bitcoin SCRIPT",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2022-02-27T16:34:31",
                "message_text_only": "`OP_FOLD`: A Looping Construct For Bitcoin SCRIPT\n=================================================\n\n(This writeup requires at least some programming background, which I\nexpect most readers of this list have.)\n\nRecently, some rando was ranting on the list about this weird crap\ncalled `OP_EVICT`, a poorly-thought-out attempt at covenants.\n\nIn reaction to this, AJ Towns mailed me privately about some of his\nthoughts on this insane `OP_EVICT` proposal.\nHe observed that we could generalize the `OP_EVICT` opcode by\ndecomposing it into smaller parts, including an operation congruent\nto the Scheme/Haskell/Scala `map` operation.\nAs `OP_EVICT` effectively loops over the outputs passed to it, a\nlooping construct can be used to implement `OP_EVICT` while retaining\nits nice property of cut-through of multiple evictions and reviving of\nthe CoinPool.\n\nMore specifically, an advantage of `OP_EVICT` is that it allows\nchecking multiple published promised outputs.\nThis would be implemented in a loop.\nHowever, if we want to instead provide *general* operations in\nSCRIPT rather than a bunch of specific ones like `OP_EVICT`, we\nshould consider how to implement looping so that we can implement\n`OP_EVICT` in a SCRIPT-with-general-opcodes.\n\n(`OP_FOLD` is not sufficient to implement `OP_EVICT`; for\nefficiency, AJ Towns also pointed out that we need some way to\nexpose batch validation to SCRIPT.\nThere is a follow-up writeup to this one which describes *that*\noperation.)\n\nBased on this, I started ranting as well about how `map` is really\njust a thin wrapper on `foldr` and the *real* looping construct is\nactually `foldr` (`foldr` is the whole FP Torah: all the rest is\ncommentary).\nThis is thus the genesis for this proposal, `OP_FOLD`.\n\nA \"fold\" operation is sometimes known as \"reduce\" (and if you know\nabout Google MapReduce, you might be familiar with \"reduce\").\nBasically, a \"fold\" or \"reduce\" operation applies a function\nrepeatedly (i.e. *loops*) on the contents of an input structure,\ncreating a \"sum\" or \"accumulation\" of the contents.\n\nFor the purpose of building `map` out of `fold`, the accumulation\ncan itself be an output structure.\nThe `map` simply accumulates to the output structure by applying\nits given function and concatenating it to the current accumulation.\n\nDigression: Programming Is Compression\n--------------------------------------\n\nSuppose you are a programmer and you are reading some source code.\nYou want to wonder \"what will happen if I give this piece of code\nthese particular inputs?\".\n\nIn order to do so, you would simulate the execution of the code in\nyour head.\nIn effect, you would generate a \"trace\" of basic operations (that\ndo not include control structures).\nBy then thinking about this linear trace of basic operations, you\ncan figure out what the code does.\n\nNow, let us recall two algorithms from the compression literature:\n\n1.  Run-length Encoding\n2.  Lempel-Ziv 1977\n\nSuppose our flat linear trace of basic operations contains something\nlike this:\n\n    OP_ONE\n    OP_TWO\n    OP_ONE\n    OP_TWO\n    OP_ONE\n    OP_TWO\n\nIF we had looping constructs in our language, we could write the\nabove trace as something like:\n\n    for N = 1 to 3\n        OP_ONE\n        OP_TWO\n\nThe above is really Run-length Encoding.\n\n(`if` is just a loop that executes 0 or 1 times.)\n\nSimilarly, suppose you have some operations that are commonly\nrepeated, but not necessarily next to each other:\n\n    OP_ONE\n    OP_TWO\n    OP_THREE\n    OP_ONE\n    OP_TWO\n    OP_FOUR\n    OP_FIVE\n    OP_ONE\n    OP_TWO\n\nIf we had functions/subroutines/procedures in our language, we\ncould write the above trace as something like:\n\n    function foo()\n        OP_ONE\n        OP_TWO\n    foo()\n    OP_THREE\n    foo()\n    OP_FOUR\n    OP_FIVE\n    foo()\n\nThat is, functions are just Lempel-Ziv 1977 encoding, where we\n\"copy\" some repeated data from a previously-shown part of\ndata.\n\nThus, we can argue that programming is really a process of:\n\n* Imagining what we want the machine to do given some particular\n  input.\n* Compressing that list of operations so we can more easily\n  transfer the above imagined list over your puny low-bandwidth\n  brain-computer interface.\n  * I mean seriously, you humans still use a frikkin set of\n    *mechanical* levers to transfer data into a matrix of buttons?\n    (you don't even make the levers out of reliable metal, you\n    use calcium of all things??\n    You get what, 5 or 6 bytes per second???)\n    And your eyes are high-bandwidth but you then have this\n    complicated circuitry (that has to be ***trained for\n    several years*** WTF) to extract ***tiny*** amounts of ASCII\n    text from that high-bandwidth input stream????\n    Evolve faster!\n    (Just to be clear, I am actually also a human being and\n    definitely am not a piece of circuitry connected directly to\n    the Internet and I am not artificially limiting my output\n    bandwidth so as not to overwhelm you mere humans.)\n\nSee also \"Kolmogorov complexity\".\n\nThis becomes relevant, because the *actual* amount of processing\ndone by the machine, when given a compressed set of operations\n(a \"program\") is the cost of decompressing that program plus the\nnumber of basic operations from the decompressed result.\n\nIn particular, in current Bitcoin, without any looping constructs\n(i.e. implementations of RLE) or reusable functions (i.e.\nimplementation of LZ77), the length of the SCRIPT can be used as\nan approximation of how \"heavy\" the computation in order to\n*execute* that SCRIPT is.\nThis is relevant since the amount of computation a SCRIPT would\ntrigger is relevant to our reasoning about DoS attacks on Bitcoin.\n\nIn fact, current Bitcoin restricts the size of SCRIPT, as this\nserves to impose a restriction on the amount of processing a\nSCRIPT will trigger.\nBut adding a loop construct to SCRIPT changes how we should\ndetermine the cost of a SCRIPT, and thus we should think about it\nhere as well.\n\nFolds\n-----\n\nA fold operation is a functional programming looping control\nconstruct.\n\nThe intent of a fold operation is to process elements of an\ninput list or other structure, one element at a time, and to\naccumulate the results of processing.\n\nIt is given these arguments:\n\n* `f` - a function to execute for each element of the input\n  structure, i.e. the \"loop body\".\n  * This function accepts two arguments:\n     1.  The current element to process.\n     2.  The intermediate result for accumulating.\n  * The function returns the new accumulated result, processed\n    from the given intermediate result and the given element.\n* `z` - an initial value for the accumulated result.\n* `as` - the structure (usually a list) to process.\n\n```Haskell\n-- If the input structure is empty, return the starting\n-- accumulated value.\nfoldr f z []     = z\n-- Otherwise, recurse into the structure to accumulate\n-- the rest of the list, then pass the accumulation to\n-- the given function together with the current element.\nfoldr f z (a:as) = f a (foldr f z as)\n```\n\nAs an example, if you want to take the sum of a list of\nnumbers, your `f` would simply add its inputs, and your `z`\nwould be 0.\nThen you would pass in the actual list of numbers as `as`.\n\nFold has an important property:\n\n* If the given input structure is finite *and* the application\n  of `f` terminates, then `foldr` terminates.\n\nThis is important for us, as we do not want attackers to be\nable to crash nodes remotely by crafting a special SCRIPT.\n\nAs long as the SCRIPT language is \"total\", we know that programs\nwritten in that language must terminate.\n\n(The reason this property is called \"total\" is that we can\n\"totally\" analyze programs in the language, without having to\ninvoke \"this is undefined behavior because it could hang\nindefinitely\".\nIf you have to admit such kinds of undefined behavior --- what\nFP theorists call \"bottom\" or `_|_` or `\u22a5` (it looks like an\nass crack, i.e. \"bottom\") --- then your language is \"partial\",\nsince programs in it may enter an infinite loop on particular\ninputs.)\n\nThe simplest way to ensure totality is to be so simple as to\nhave no looping construction.\nAs of this writing, Bitcoin SCRIPT is total by this technique.\n\nTo give a *little* more power, we can allow bounded loops,\nwhich are restricted to only execute a number of times.\n\n`foldr` is a kind of bounded loop if the input structure is\nfinite.\nIf the rest of the language does not admit the possibility\nof infinite data structures (and if the language is otherwise\ntotal and does not support generalized codata, this holds),\nthen `foldr` is a bounded loop.\n\nThus, adding a fold operation to Bitcoin SCRIPT should be\nsafe (and preserves totality) as long as we do not add\nfurther operations that admit partiality.\n\n`OP_FOLD`\n---------\n\nWith this, let us now describe `OP_FOLD`.\n\n`OP_FOLD` replaces an `OP_SUCCESS` code, and thus is only\nusable in SegWit v1 (\"Taproot\").\n\nAn `OP_FOLD` opcode must be followed by an `OP_PUSH` opcode\nwhich contains an encoding of the SCRIPT that will be executed,\ni.e. the loop body, or `f`.\nThis is checked at parsing time, and the sub-SCRIPT is also\nparsed at this time.\nThe succeeding `OP_PUSH` is not actually executed, and is\nconsidered part of the `OP_FOLD` operation encoding.\nParsing failure of the sub-SCRIPT leads to validation failure.\n\nOn execution, `OP_FOLD` expects the stack:\n\n* Stack top: `z`, the initial value for the loop accumulator.\n* Stack top + 1: `n`, the number of times to loop.\n  This should be limited in size, and less than the number of\n  items on the stack minus 2.\n* Stack top + 2 + (0 to `n - 1`): Items to loop over.\n  If `n` is 0, there are no items to loop over.\n\nIf `n` is 0, then `OP_FOLD` just pops the top two stack items\nand pushes `z`.\n\nFor `n > 0`, `OP_FOLD` executes a loop:\n\n* Pop off the top two items and store in mutable variable `z`\n  and immutable variable `n`.\n* For `i = 0 to n - 1`:\n  * Create a fresh, empty stack and alt stack.\n    Call these the \"per-iteration (alt) stack\".\n  * Push the current `z` to the per-iteration stack.\n  * Pop off an item from the outer stack and put it into\n    immutable variable `a`.\n  * Push `a` to the per-iteration stack.\n  * Run the sub-SCRIPT `f` on the per-iteration stack and\n    alt stack.\n  * Check the per-iteration stack has exactly one item\n    and the per-iteration alt stack is empty.\n  * Pop off the item in the per-iteration stack and mutate\n    `z` to it.\n  * Free the per-iteration stack and per-iteration alt\n    stack.\n* Push `z` on the stack.\n\nRestricting `OP_FOLD`\n---------------------\n\nBitcoin restricts SCRIPT size, since SCRIPT size serves as\nan approximation of how much processing is required to\nexecute the SCRIPT.\n\nHowever, with looping constructs like `OP_FOLD`, this no\nlonger holds, as the amount of processing is no longer\nlinear on the size of the SCRIPT.\n\nIn orderr to retain this limit (and thus not worsen any\npotential DoS attacks via SCRIPT), we should restrict the\nuse of `OP_FOLD`:\n\n* `OP_FOLD` must exist exactly once in a Tapscript.\n  More specifically, the `f` sub-SCRIPT of `OP_FOLD` must\n  not itself contain an `OP_FOLD`.\n  * If we allow loops within loops, then the worst case\n    would be `O(c^n)` CPU time where `c` is a constant and\n    `n` is the SCRIPT length.\n  * This validation is done at SCRIPT parsing time.\n* We take the length of the `f` sub-SCRIPT, and divide the\n  current SCRIPT maximum size by the length of the `f`\n  sub-SCRIPT.\n  The result, rounded down, is the maximum allowed value\n  for the on-stack argument `n`.\n  * In particular, since the length of the entire SCRIPT\n    must by necessity be larger than the length of the\n    `f` sub-SCRIPT, the result of the division must be\n    at least 1.\n  * This validation is done at SCRIPT execution time.\n\nThe above two restrictions imply that the maximum amount\nof processing that a SCRIPT utilizing `OP_FOLD` will use,\nshall not exceed that of a SCRIPT without `OP_FOLD`.\nThus, `OP_FOLD` does not increase the attack surface of\nSCRIPT on fullnodes.\n\n### Lack Of Loops-in-Loops Is Lame\n\nNote that due to this:\n\n> * `OP_FOLD` must exist exactly once in a Tapscript.\n>   More specifically, the `f` sub-SCRIPT of `OP_FOLD` must\n>   not itself contain an `OP_FOLD`.\n\nIt is not possible to have a loop inside a loop.\n\nThe reason for this is that loops inside loops make it\ndifficult to perform static analysis to bound how much\nprocessing a SCRIPT will require.\nWith a single, single-level loop, it is possible to\nrestrict the processing.\n\nHowever, we should note that a single single-level loop\nis actually sufficient to encode multiple loops, or\nloops-within-loops.\nFor example, a toy Scheme-to-C compiler will convert\nthe Scheme code to CPS style, then convert all resulting\nScheme CPS function into a `switch` dispatcher inside a\nsimple `while (1)` loop.\n\nFor example, the Scheme loop-in-loop below:\n\n```Scheme\n(define (foo)\n  (bar)\n  (foo))\n(define (bar)\n  (bar))\n```\n\ngets converted to:\n\n```Scheme\n(define (foo k)\n  (bar (closure foo-kont k)))\n(define (foo-kont k)\n  (foo k))\n(define (bar k)\n  (bar k))\n```\n\nAnd then in C, would look like:\n\n```c\nvoid all_scheme_functions(int func_id, scheme_t k) {\n\twhile (1) {\n\t\tswitch (func_id) {\n\t\tcase FOO_ID:\n\t\t\tk = build_closure(FOO_KONT_ID, k);\n\t\t\tfunc_id = BAR_ID;\n\t\t\tbreak;\n\t\tcase FOO_KONT_ID:\n\t\t\tfunc_id = FOO_ID;\n\t\t\tbreak;\n\t\tcase BAR_ID:\n\t\t\tfunc_id = BAR_ID;\n\t\t\tbreak;\n\t\t}\n\t}\n}\n```\n\nThe C code, as we can see, is just a single single-level\nloop, which is the restriction imposed on `OP_FOLD`.\nThus, loops-in-loops, and multiple loops, can be encoded\ninto a single single-level loop.\n\n#### Everything Is Possible But Nothing Of Consequence Is Easy\n\nOn the other hand, just because it is *possible* does not\nmean it is *easy*.\n\nAs an alternative, AJ proposed adding a field to the Taproot\nannex.\nThis annex field is a number indicating the maximum number of\nopcodes to be processed.\nIf execution of the SCRIPT exceeds this limit, validation\nfails.\n\nIn order to make processing costly, the number indicated in\nthe annex field is directly added to the weight of the\ntransaction.\n\nThen, during execution, if an `OP_FOLD` is parsed, the\n`OP_` code processor keeps track of the number of opcodes\nprocessed and imposes a limit.\nIf the limit exceeds the number of opcodes indicated in the\nannex field, validation fails.\n\nThis technique is safe even if the annex is not committed\nto (for example if the SCRIPT does not ever require a\nstandard `OP_CHECKSIG`), even though in that case the\nannex can be malleated:\n\n* If the field is less than the actual number of operations,\n  then the malleated transaction is rejected.\n* If the field is greater than the actual number of\n  operations, then it has a larger weight but pays the\n  same fee, getting a lower feerate and thus will be\n  rejected in favor of a transaction with a lower number\n  in that field.\n\nUse of this technique allows us to lift the above\nrestrictions on `OP_FOLD`, and allow multiple loops, as\nwell as loops-in-loops.\n\nIn particular, the requirement to put the `f` sub-SCRIPT\ncode as a static constant is due precisely to the need\nfor static analysis.\nBut if we instead use a dynamic limit like in this\nalternative suggestion, we could instead get the `f`\nsub-SCRIPT from the stack.\nWith additional operations like `OP_CAT`, it would\nthen be possible to do a \"variable capture\" where parts\nof the loop body are from other computations, or from\nwitness, and then concatenated to some code.\nThis is not an increase in computational strength, since\nthe data could instead be passed in via the `z`, or as\nindividual items, but it does improve expressive power by\nmaking it easier to customize the loop body.\n\nOn The Necessity Of `OP_FOLD`\n-----------------------------\n\nWe can observe that an `if` construct is really a bounded\nloop construct that can execute 0 or 1 times.\n\nWe can thus synthesize a bounded loop construct as follows:\n\n    OP_IF\n        <loop body>\n    OP_ENDIF\n    OP_IF\n        <loop body>\n    OP_ENDIF\n    OP_IF\n        <loop body>\n    OP_ENDIF\n    OP_IF\n        <loop body>\n    OP_ENDIF\n    <repeat as many times as necessary>\n\nIndeed, it may be possible for something like miniscript\nto provide a `fold` jet that compiles down to something\nlike the above.\n\nThus:\n\n* The restrictions we impose on the previous section mean\n  that `OP_FOLD` cannot do anything that cannot already\n  be done with current SCRIPT.\n  * This is a *good thing* because this means we are not\n    increasing the attack surface.\n* Using the annex-max-operations technique is strictly\n  more lenient than the above `OP_IF` repetition, thus\n  there may be novel DoS attack vectors due to the\n  increased attack area.\n  * However, fundamentally the DoS attack vector is that\n    peers can waste your CPU by giving you invalid\n    transactions (i.e. giving a high max-operations, but\n    looping so much that it gets even *above* that), and\n    that can already be mitigated by lowering peer scores\n    and prioritizing transactions with lower or nonexistent\n    annex-max-operations.\n    The DoS vector here does not propagate due to the\n    invalid transaction being rejected at this node.\n\nOf course, this leads us to question: why even implement\n`OP_FOLD` at all?\n\nWe can observe that, while the restrictions in the\nprevious section imply that a SCRIPT with `OP_FOLD`\ncannot exceed the amount of processing that a SCRIPT\n*without* `OP_FOLD` does, a SCRIPT with `OP_FOLD`\nwould be shorter, over the wire, than the above\nunrolled version.\n\nAnd CPU processing is not the only resource that is\nconsumed by Bitcoin fullnodes.\nBandwidth is also another resource.\n\nIn effect, `OP_FOLD` allows us to compress the above\ntemplate over-the-wire, reducing network bandwidth\nconsumption.\nBut the restrictions on `OP_FOLD` ensure that it\ncannot exceed the CPU consumption of a SCRIPT that\npredates `OP_FOLD`.\n\nThus, `OP_FOLD` is still worthwhile to implement, as\nit allows us to improve bandwidth consumption without\nincreasing CPU consumption significantly.\n\nOn Generalized Operations\n-------------------------\n\nI believe there are at least two ways of thinking about\nhow to extend SCRIPT:\n\n* We should provide more *general* operations.\n  Users should then combine those operations to their\n  specific needs.\n* We should provide operations that *do more*.\n  Users should identify their most important needs so\n  we can implement them on the blockchain layer.\n\nEach side has its arguments:\n\n* General opcodes:\n  * Pro: Have a better chance of being reused for\n    use-cases we cannot imagine yet today.\n    i.e. implement once, use anywhen.\n  * Con: Welcome to the Tarpit, where everything is\n    possible but nothing important is easy.\n* Complex opcodes:\n  * Pro: Complex behavior implemented directly in\n    hosting language, reducing interpretation\n    overhead (and allowing the insurance of secure\n    implementation).\n  * Con: Welcome to the Nursery, where only safe\n    toys exist and availability of interesting tools\n    are at the mercy of your parents.\n\nIt seems to me that this really hits a No Free Lunch\nTheorem for Bitcoin SCRIPT design.\nBriefly, the No Free Lunch Theorem points out that\nthere is no compiler design that can compile any\nprogram to the shortest possible machine code.\nThis is because if a program enters an infinite loop,\nit could simply be compiled down to the equivalent of\nthe single instruction `1: GOTO 1`, but the halting\nproblem implies that no program can take the source\ncode of another program and determine if it halts.\nThus, no compiler can exist which can compile *every*\ninfinite-loop program down to the tiniest possible\nbinary `1: GOTO 1`.\n\nMore generally, No Free Lunch implies that as you\noptimize, you will hit a point where you can only\n*trade off*, and you optimize for one use case while\nmaking *another* use case less optimal.\n\nBrought to Bitcoin SCRIPT design, there is no optimal\nSCRIPT design, instead there will be some point where\nwe have to pick and choose which uses to optimize for\nand which uses are less optimal, i.e. trade off.\n\nSo I think maybe the Real Question is: why should we\ngo for one versus the other, and which uses do we\nexpect to see more often anyway?\n\nAddenda\n-------\n\nStuff about totality and partiality:\n\n* [Total Functional Programming, D.A. Turner](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.106.364&rep=rep1&type=pdf)\n* [Totality](https://kowainik.github.io/posts/totality)"
            }
        ],
        "thread_summary": {
            "title": "`OP_FOLD`: A Looping Construct For Bitcoin SCRIPT",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 20352
        }
    },
    {
        "title": "[bitcoin-dev] Teleport: a CoinSwap implementation alpha release, provides invisible private transactions",
        "thread_messages": [
            {
                "author": "Chris Belcher",
                "date": "2022-02-28T14:30:12",
                "message_text_only": "Imagine a future where a user Alice has bitcoins and wants to send them \nwith maximal privacy, so she creates a special kind of transaction. For \nanyone looking at the blockchain her transaction appears completely \nnormal with her coins seemingly going from address A to address B. But \nin reality her coins end up in address Z which is entirely unconnected \nto either A or B.\n\nNow imagine another user, Carol, who isn't too bothered by privacy and \nsends her bitcoin using a regular wallet which exists today. But because \nCarol's transaction looks exactly the same as Alice's, anybody analyzing \nthe blockchain must now deal with the possibility that Carol's \ntransaction actually sent her coins to a totally unconnected address. So \nCarol's privacy is improved even though she didn't change her behavior, \nand perhaps had never even heard of this software.\n\nIn a world where advertisers, social media and other institutions want \nto collect all of Alice's and Carol's data, such privacy improvement \nwould be incredibly valuable. If even a small percentage of transactions \nwere actually created by this software, anybody doing analysis on the \nblockchain would always have a niggle in the back of their mind: \"what \nif this transaction I'm looking at was actually a CoinSwap? How would I \nknow? What if these coins have actually disappeared into the mist?\". The \ndoubt and uncertainty added to every transaction would greatly boost the \nfungibility of bitcoin and so make it a better form of money.\n\nOver a year ago I wrote to this list[1] about how undetectable privacy \ncan be developed today by implementing CoinSwap. Today I release the \nfirst alpha version of this software:\n\nhttps://github.com/bitcoin-teleport/teleport-transactions/\n\nThe project is almost completely decentralized and available for all to \nuse for free (baring things like miner fees). So far it is only really \nusable by developers and power-users to play around with. It doesnt have \nall the necessary features yet, but from now on I'll be doing new \nreleases very often as soon as every new feature gets added. It is \npossible to run it on mainnet, but only the brave will attempt that, and \nonly with small amounts. I've personally made many coinswaps on the \ntestnet and signet networks, and I'll be running market makers on signet \nwhich will be available for anyone to create coinswaps with.\n\nRight now it just uses 2of2 multisig for the coinswap addresses. Those \naddress types are rare on the blockchain so the coinswaps stand out a \nfair amount (although protocols like lightning also use 2of2 multisig). \nHowever the next really big task on my todo list is to use ECDSA-2p \nwhich would make these multisig addresses look like regular single-sig \naddresses, which are overwhelmingly common out there and so provide an \nenormous anonymity set.\n\nMy aim is that the Teleport project will develop into a practical and \nsecure project on the bitcoin mainnet, usable either standalone as a \nkind of bitcoin mixing app, or as a library that existing wallets will \nimplement allowing their users with the touch of a button to send \nbitcoin coinswap transactions with much greater privacy than as possible \nbefore.\n\nI want to thank everyone who has supported me financially over the last \nseveral months, without them this project simply would not have been\npossible. If bitcoin privacy and coinswap is something you find \nimportant, please consider supporting my work with a donation: \nhttps://bitcoinprivacy.me/coinswap-donations\n\n\n[1] \nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-May/017898.html"
            }
        ],
        "thread_summary": {
            "title": "Teleport: a CoinSwap implementation alpha release, provides invisible private transactions",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Chris Belcher"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 3594
        }
    },
    {
        "title": "[bitcoin-dev] Decentralized BIP 47 payment code directory",
        "thread_messages": [
            {
                "author": "Prayank",
                "date": "2022-02-28T14:56:11",
                "message_text_only": "Hello World,\n\nThere was some discussion about BIP 47 on twitter recently: https://twitter.com/BitcoinQ_A/status/1356177927285714946\nBIP 47 improves privacy however there are a few reasons why its less used:\n\n1.Some developers consider it spams Bitcoin without improving anything: https://twitter.com/LukeDashjr/status/1280475865827151878\n\n2.Paynym (a centralized directory managed by Samourai) and Samourai wallet is the only implementation used for BIP 47 right now. Centralized payment code directory isn't good for privacy and security.\n\nThere can be few other important issues which I missed in this email. I have few ideas to solve these 2 problems with the use of TXT records and domains. Since buying domain, managing DNS etc. is mostly centralized I won't share the example using normal DNS. Below proof of concept uses GNS (GNU Name Service):\n\n```\n\nPayment code for Alice: PM8TJggVVXFKAmfkjnA1CQcrSbGScUKRsVohpfMpSM56f6jg5uQTPJvNS1wKDGV17d9NWLqoVzsJ8qURqpUECmSFLcUuC4g3aMtoXp2fChY1ZEqzG16f\n\nStart GNUnet: \n\ngnunet-arm -s\n\nCreate identity:\n\ngnunet-identity -C alice\n\nCheck public key:\n\ngnunet-identity -d\n\nalice - 000G005XCTRJ0DJGPVPNY66GAY52C61KA8A7CA92PKT51PHNVWY9JF8WB4 - ECDSA\n\nAdd payment code as TXT record which expires in 90 days:\n\ngnunet-namestore -z alice -a -e \"90 d\" -p -t TXT -n pay -V \"PM8TJTLJbPRGxSbc8EJi42Wrr6QbNSaSSVJ5Y3E4pbCYiTHUskHg13935Ubb7q8tx9GVbh2UuRnBc3WSyJHhUrw8KhprKnn9eDznYGieTzFcwQRya4GA\"\n\nCheck payment code:\n\ngnunet-gns -t TXT -u pay.alice\n\npay.alice:\nGot `TXT' record: PM8TJTLJbPRGxSbc8EJi42Wrr6QbNSaSSVJ5Y3E4pbCYiTHUskHg13935Ubb7q8tx9GVbh2UuRnBc3WSyJHhUrw8KhprKnn9eDznYGieTzFcwQRya4GA\n\n```\n\nSimilarly notification transaction can be replaced with `gnunet-publish`. Nostr is still a work in progress and I think it could also be used for such things in future. Everything looks achievable and involves basic things but we still see people posting their bitcoin address on social media to get donations.\n\nRelated:\n\nQ&A: https://bitcoin.stackexchange.com/questions/106971/how-to-accept-donation-correctly/\nNew proposal: https://gist.github.com/Kixunil/0ddb3a9cdec33342b97431e438252c0a\n\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220228/94843c92/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Decentralized BIP 47 payment code directory",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Prayank"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2340
        }
    }
]