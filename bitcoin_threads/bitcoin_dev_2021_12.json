[
    {
        "title": "[bitcoin-dev] New Portuguese & Spanish translations of Learning Bitcoin self-paced course",
        "thread_messages": [
            {
                "author": "Christopher Allen",
                "date": "2021-12-01T17:54:12",
                "message_text_only": "Blockchain Commons has recently released two translations of our free,\nself-paced, \"Learning Bitcoin from the Command Line\" course, into Spanish\nand Portuguese:\n\n\n   - Portuguese Translation:\n   https://github.com/BlockchainCommons/Learning-Bitcoin-from-the-Command-Line/tree/master/pt\n   - Spanish Translation:\n   https://github.com/BlockchainCommons/Learning-Bitcoin-from-the-Command-Line/tree/master/es\n\nLearning Bitcoin from the Command Line teaches about Bitcoin development\nstarting with bitcoin-cli and moving on to using computer languages to\naccess the RPC API. We\u2019ve always intended that it provide a pathway for\ndevelopers to join the broader Bitcoin ecosystem, and we\u2019ve seen personal\nsuccess toward that goal, with most of our international interns getting\ntheir start with our course, and with many of them having since found\nemployment in the field.\n\nHaving more educated people in the field not only helps everyone looking\nfor developers, but it also will make it that much easier for us to make\nthe next big transition, such as the Taproot transition that we\u2019re\ncurrently working on.\n\nWith 460 million native speakers of Spanish and 230 million native speakers\nof Portuguese, and with 29 different countries where one or both is an\nofficial language, we think these new translations will considerably widen\nthe scope of Learning Bitcoin\u2019s coverage and invite many new developers to\nwork together with all of us on Bitcoin, using the international language\nof computer code. Of course, this year\u2019s decision by El Salvador to adopt\nBitcoin as an official currency makes it even more obvious why these sorts\nof translations are important.\n\nHere\u2019s what\u2019s next for Learning Bitcoin from the Command Line.\n\n   1. Learning Bitcoin from the Command Line v3.0\n\nOur current iteration of Learning Bitcoin from the Command Line is now a\nfull year old, so we want to update it to talk about the newest Bitcoin\nwork, including Taproot, Schnorr signatures, miniscript, and more. Our\ncurrent outline for v3.0 is found here (though it\u2019s likely to change some\nas we dive fully into the latest bitcoin-core releases):\n\nhttps://github.com/BlockchainCommons/Learning-Bitcoin-from-the-Command-Line/blob/master/TODO-30.md\n\nWe\u2019d love your expertise on anything you think we\u2019re missing, or getting\nwrong, for the v3.0 update. Please feel free to respond here or write us an\nissue, either telling us of any problems with the current course (including\nthings that have just gotten out of date) or things that we should have in\nv3.0 that we\u2019re not currently outlining.\n\nhttps://github.com/BlockchainCommons/Learning-Bitcoin-from-the-Command-Line/issues\n\n   1. Learning Bitcoin from the Command Line Seminars\n\nWe are considering offering some brief, weekly seminars in 2022, looking at\nindividual sections of Learning Bitcoin from the Command Line and answering\nquestions. If this interests you, or you\u2019d like to help support it, please\nlet us know.\n\nThank you to everyone who worked on the translations of Learning Bitcoin:\nIan Culp, Maxi Goyheneche, Said Rahal, C\u00e9sar A. Vallero, and Javier Vargas\nfor our Spanish translation; Namcios, Korea, Luke Pavsky, and hgrams for\nthe Portuguese translation.\n\nTo continue this work, we are looking for monthly patronage to support\nLearning Bitcoin. If you think increasing the pool of Bitcoin developers is\nimportant, please consider becoming a patron of Blockchain Commons, and let\nus know it\u2019s because of your interest in this course.\n\nhttps://github.com/sponsors/BlockchainCommons\n\nThanks for your interest!\n\n-- Christopher Allen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211201/eb429a8d/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "New Portuguese & Spanish translations of Learning Bitcoin self-paced course",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Christopher Allen"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 3745
        }
    },
    {
        "title": "[bitcoin-dev] Pawn (chess piece) | Breaking bitcoin by playing chess",
        "thread_messages": [
            {
                "author": "Prayank",
                "date": "2021-12-04T04:36:02",
                "message_text_only": "Hello World,\n\nLink with what, why and how: https://gist.github.com/prayank23/22763f48199ed106e59801be43ad4efc\n\nTwo related things that I found: \n\n1.Koala Studio tried chess on LN in 2019 but shutdown in August 2019\n2.Etleneum still has chess but works differently\n\nPrimary goal of this project can be different and focus on testing Bitcoin transactions. Secondary goal is to have fun and contribute in increasing demand for block space. Maybe an app for developers to play chess, friendly competitions, learn and share new things. \n\nIf chess sounds boring it can be replaced with any 2 player game that works for such setup and can be played with patience over few hours/days.\n\nSpam? Sorry zero fee transactions do not work anymore. In fact, nothing below 1 sat/vbyte fee rate would work and all transactions will pay fees that are required long term. OP_RETURN is used by many projects and excluded from UTXO set. Let me know if something looks wrong. I won't be working on this as busy with another project and recently started contributing in Wasabi.\n\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211204/b04a0d5c/attachment.html>"
            },
            {
                "author": "LORD HIS EXCELLENCY JAMES HRMH",
                "date": "2021-12-04T10:00:13",
                "message_text_only": "The frivolous use of block space - ie. to increase the demand for block space -  is not encouraged. Although it is possible you may write chess moves on a wrap of dollar bills and send them to your friends, nowhere that I know of has this been recorded in a ledger as a valid past time.\n\nKING JAMES HRMH\nGreat British Empire\n\nRegards,\nThe Australian\nLORD HIS EXCELLENCY JAMES HRMH (& HMRH)\nof Hougun Manor & Glencoe & British Empire\nMR. Damian A. James Williamson\nWills\n\net al.\n\n\nWilltech\nwww.willtech.com.au\nwww.go-overt.com\nduigco.org DUIGCO API\nand other projects\n\n\nm. 0487135719\nf. +61261470192\n\n\nThis email does not constitute a general advice. Please disregard this email if misdelivered.\n________________________________\n\nFrom: bitcoin-dev <bitcoin-dev-bounces at lists.linuxfoundation.org> on behalf of Prayank via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>\nSent: Saturday, 4 December 2021 3:36 PM\nTo: Lightning Dev <lightning-dev at lists.linuxfoundation.org>\nCc: Bitcoin Dev <bitcoin-dev at lists.linuxfoundation.org>\nSubject: [bitcoin-dev] Pawn (chess piece) | Breaking bitcoin by playing chess\n\nHello World,\n\nLink with what, why and how: https://gist.github.com/prayank23/22763f48199ed106e59801be43ad4efc\n\nTwo related things that I found:\n\n1.Koala Studio tried chess on LN in 2019 but shutdown in August 2019\n2.Etleneum still has chess but works differently\n\nPrimary goal of this project can be different and focus on testing Bitcoin transactions. Secondary goal is to have fun and contribute in increasing demand for block space. Maybe an app for developers to play chess, friendly competitions, learn and share new things.\n\nIf chess sounds boring it can be replaced with any 2 player game that works for such setup and can be played with patience over few hours/days.\n\nSpam? Sorry zero fee transactions do not work anymore. In fact, nothing below 1 sat/vbyte fee rate would work and all transactions will pay fees that are required long term. OP_RETURN is used by many projects and excluded from UTXO set. Let me know if something looks wrong. I won't be working on this as busy with another project and recently started contributing in Wasabi.\n\n\n--\nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211204/0a21b13b/attachment.html>"
            },
            {
                "author": "Prayank",
                "date": "2021-12-04T15:28:32",
                "message_text_only": "Can you share the email address to get approval or permission for this type of bitcoin transactions? i.e. opening and closing of LN channels or OP_RETURN. I will keep that in Cc next time.\n\nI can write chess moves on a dollar bill and send to my friends but it does not solve any of the problems. Bitcoin's blockchain or ledger is for transactions. As long as a transaction is valid, standard and paying fees nobody should have issues with what is being achieved with the transaction.\n\nThanks\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n\n\n\nDec 4, 2021, 15:30 by willtech at live.com.au:\n\n> The frivolous use of block space - ie. to increase the demand for block space -\u00a0 is not encouraged. Although it is possible you may write chess moves on a wrap of dollar bills and send them to your friends, nowhere that I know of has this been recorded in a ledger as a valid past time.\n>\n> KING JAMES HRMH \n> Great British Empire\n>\n> Regards,\n> The Australian\n> LORD HIS EXCELLENCY JAMES HRMH (& HMRH)\n> of Hougun Manor & Glencoe & British Empire\n> MR. Damian A. James Williamson\n> Wills\n>\n> et al.\n>\n> \u00a0\n> Willtech\n> www.willtech.com.au\n> www.go-overt.com\n> duigco.org DUIGCO API\n> and other projects\n> \u00a0\n>\n> m. 0487135719\n> f. +61261470192\n>\n>\n> This email does not constitute a general advice. Please disregard this email if misdelivered.\n>  \n>\n>\n> From:>  bitcoin-dev <bitcoin-dev-bounces at lists.linuxfoundation.org> on behalf of Prayank via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>\n>  \n> Sent:>  Saturday, 4 December 2021 3:36 PM\n>  > To:>  Lightning Dev <lightning-dev at lists.linuxfoundation.org>\n>  > Cc:>  Bitcoin Dev <bitcoin-dev at lists.linuxfoundation.org>\n>  > Subject:>  [bitcoin-dev] Pawn (chess piece) | Breaking bitcoin by playing chess>  > \u00a0\n> Hello World,\n>\n> Link with what, why and how: https://gist.github.com/prayank23/22763f48199ed106e59801be43ad4efc\n>\n> Two related things that I found: \n>\n> 1.> Koala Studio tried chess on LN in 2019 but shutdown in August 2019\n> 2.Etleneum still has chess but works differently\n>\n> Primary goal of this project can be different and focus on testing Bitcoin transactions. Secondary goal is to have fun and contribute in increasing demand for block space. Maybe an app for developers to play chess, friendly competitions, learn and share new things. \n>\n> If chess sounds boring it can be replaced with any 2 player game that works for such setup and can be played with patience over few hours/days.\n>\n> Spam? Sorry zero fee transactions do not work anymore. In fact, nothing below 1 sat/vbyte fee rate would work and all transactions will pay fees that are required long term. OP_RETURN is used by many projects and excluded from UTXO set. Let me know if something looks wrong. I won't be working on this as busy with another project and recently started contributing in Wasabi.\n>\n>\n> -- \n> Prayank\n>\n> A3B1 E430 2298 178F\n>\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211204/0788929e/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Pawn (chess piece) | Breaking bitcoin by playing chess",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Prayank",
                "LORD HIS EXCELLENCY JAMES HRMH"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 6724
        }
    },
    {
        "title": "[bitcoin-dev] Sending OP_RETURN via Bitcoin Lightning",
        "thread_messages": [
            {
                "author": "H\u00e9ctor Jos\u00e9 C\u00e1rdenas Pacheco",
                "date": "2021-12-06T09:54:30",
                "message_text_only": "Hello all,\n\nI\u2019ve been thinking about how OP_RETURN is being used to create and trade NFTs on Bitcoin (think RarePepes, SoG and other new ones) and was wondering if it\u2019s possible to make transactions with this opcode via Lightning.\n\nMore specific questions could be:\nCan opcodes like OP_RETURN be inside a channel\u2019s opening or closing transaction?\nIf so, could that OP_RETURN change hands within that channel or network of channels?\nIf possible, could the OP_RETURN be divisible? Could one person send a piece of a OP_RETURN just like one can do right now on the primary ledger or would it need to maintain the OP_RETURN code intact?\nI\u2019m assuming that, if possible, this would need a protocol layer parallel to Bitcoin/Lightning that stores and reads all Bitcoin transactions and the ones which involve the node's channels as well as the ones with the OP_RETURN, just like CounterParty does right now with the primary ledger.\n\nThank in advance.\n\u2014\u2014\nH\u00e9ctor C\u00e1rdenas\n@hcarpach\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211206/96193038/attachment.html>"
            },
            {
                "author": "Karl",
                "date": "2021-12-06T10:20:55",
                "message_text_only": "Hi,\n\nI'm not a bitcoin developer.\n\nOn Mon, Dec 6, 2021, 5:05 AM H\u00e9ctor Jos\u00e9 C\u00e1rdenas Pacheco via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hello all,\n>\n> I\u2019ve been thinking about how OP_RETURN is being used to create and trade\n> NFTs on Bitcoin (think RarePepes, SoG and other new ones) and was wondering\n> if it\u2019s possible to\n>\n\nDo you have a link to any of these protocols?\n\nmake transactions with this opcode via Lightning.\n>\n> More specific questions could be:\n>\n>    1. Can opcodes like OP_RETURN be inside a channel\u2019s opening or closing\n>    transaction?\n>    2. If so, could that OP_RETURN change hands within that channel or\n>    network of channels?\n>\n> OP_RETURNs do not have ownership according to the bitcoin network.  It is\nnot hard to define a protocol that associates an OP_RETURN with ownership,\nand ownership could then be transferred via lightning by sending associated\ncurrency via lightning.  Robustness improvements seem possible.\n\n\n>    1. If possible, could the OP_RETURN be divisible? Could one person\n>    send a piece of a OP_RETURN just like one can do right now on the primary\n>    ledger or would it need to maintain the OP_RETURN code intact?\n>\n> OP_RETURNs themselves do not have ownership, but you can define a protocol\nthat gives them divisible ownership, including via lightning.\n\nI\u2019m assuming that, if possible, this would need a protocol layer parallel\n> to Bitcoin/Lightning that stores and reads all Bitcoin transactions and the\n> ones which involve the node's channels as well as the ones with the\n> OP_RETURN, just like CounterParty does right now with the primary ledger.\n>\n> Thank in advance.\n> \u2014\u2014\n>\n> *H\u00e9ctor C\u00e1rdenas*@hcarpach\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211206/f3253ec4/attachment.html>"
            },
            {
                "author": "Christian Moss",
                "date": "2021-12-06T12:38:30",
                "message_text_only": "Hi, it is not really possible in the way you think, mainly because\nlightning relies on liquidity to work, i.,e. lots of bitcoin locked up in\nchannels to allow liquidity, NFTs are not liquid, so if you have 1 NFT then\nit would be impossible to send on the network\n\nI think the best off chain solution to NFTs on bitcoin is using Ruben\nSomsens state chain protocol, which allows you to swap utxos off chain, and\nthose off chain utxos could harbour an op return/nft\n\nOn Mon, Dec 6, 2021 at 10:36 AM Karl via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi,\n>\n> I'm not a bitcoin developer.\n>\n> On Mon, Dec 6, 2021, 5:05 AM H\u00e9ctor Jos\u00e9 C\u00e1rdenas Pacheco via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Hello all,\n>>\n>> I\u2019ve been thinking about how OP_RETURN is being used to create and trade\n>> NFTs on Bitcoin (think RarePepes, SoG and other new ones) and was wondering\n>> if it\u2019s possible to\n>>\n>\n> Do you have a link to any of these protocols?\n>\n> make transactions with this opcode via Lightning.\n>>\n>> More specific questions could be:\n>>\n>>    1. Can opcodes like OP_RETURN be inside a channel\u2019s opening or\n>>    closing transaction?\n>>    2. If so, could that OP_RETURN change hands within that channel or\n>>    network of channels?\n>>\n>> OP_RETURNs do not have ownership according to the bitcoin network.  It is\n> not hard to define a protocol that associates an OP_RETURN with ownership,\n> and ownership could then be transferred via lightning by sending associated\n> currency via lightning.  Robustness improvements seem possible.\n>\n>\n>>    1. If possible, could the OP_RETURN be divisible? Could one person\n>>    send a piece of a OP_RETURN just like one can do right now on the primary\n>>    ledger or would it need to maintain the OP_RETURN code intact?\n>>\n>> OP_RETURNs themselves do not have ownership, but you can define a\n> protocol that gives them divisible ownership, including via lightning.\n>\n> I\u2019m assuming that, if possible, this would need a protocol layer parallel\n>> to Bitcoin/Lightning that stores and reads all Bitcoin transactions and the\n>> ones which involve the node's channels as well as the ones with the\n>> OP_RETURN, just like CounterParty does right now with the primary ledger.\n>>\n>> Thank in advance.\n>> \u2014\u2014\n>>\n>> *H\u00e9ctor C\u00e1rdenas*@hcarpach\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211206/7533655c/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Sending OP_RETURN via Bitcoin Lightning",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Christian Moss",
                "Karl",
                "H\u00e9ctor Jos\u00e9 C\u00e1rdenas Pacheco"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 6113
        }
    },
    {
        "title": "[bitcoin-dev] [Lightning-dev] Sending OP_RETURN via Bitcoin Lightning",
        "thread_messages": [
            {
                "author": "Martin Habov\u0161tiak",
                "date": "2021-12-06T11:31:29",
                "message_text_only": "I recommend you researching RGB: https://rgb-org.github.io/\n\nOn Mon, Dec 6, 2021, 11:21 Karl <gmkarl at gmail.com> wrote:\n\n> Hi,\n>\n> I'm not a bitcoin developer.\n>\n> On Mon, Dec 6, 2021, 5:05 AM H\u00e9ctor Jos\u00e9 C\u00e1rdenas Pacheco via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Hello all,\n>>\n>> I\u2019ve been thinking about how OP_RETURN is being used to create and trade\n>> NFTs on Bitcoin (think RarePepes, SoG and other new ones) and was wondering\n>> if it\u2019s possible to\n>>\n>\n> Do you have a link to any of these protocols?\n>\n> make transactions with this opcode via Lightning.\n>>\n>> More specific questions could be:\n>>\n>>    1. Can opcodes like OP_RETURN be inside a channel\u2019s opening or\n>>    closing transaction?\n>>    2. If so, could that OP_RETURN change hands within that channel or\n>>    network of channels?\n>>\n>> OP_RETURNs do not have ownership according to the bitcoin network.  It is\n> not hard to define a protocol that associates an OP_RETURN with ownership,\n> and ownership could then be transferred via lightning by sending associated\n> currency via lightning.  Robustness improvements seem possible.\n>\n>\n>>    1. If possible, could the OP_RETURN be divisible? Could one person\n>>    send a piece of a OP_RETURN just like one can do right now on the primary\n>>    ledger or would it need to maintain the OP_RETURN code intact?\n>>\n>> OP_RETURNs themselves do not have ownership, but you can define a\n> protocol that gives them divisible ownership, including via lightning.\n>\n> I\u2019m assuming that, if possible, this would need a protocol layer parallel\n>> to Bitcoin/Lightning that stores and reads all Bitcoin transactions and the\n>> ones which involve the node's channels as well as the ones with the\n>> OP_RETURN, just like CounterParty does right now with the primary ledger.\n>>\n>> Thank in advance.\n>> \u2014\u2014\n>>\n>> *H\u00e9ctor C\u00e1rdenas*@hcarpach\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211206/d86e56cb/attachment.html>"
            },
            {
                "author": "Christian Moss",
                "date": "2021-12-06T16:35:19",
                "message_text_only": "As far as I understand it, RGB doesn't scale NFTs as each\ntransaction to transfer ownership of an NFT would require an onchain\ntransaction\n\nOn Mon, Dec 6, 2021 at 3:44 PM Martin Habov\u0161tiak via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> I recommend you researching RGB: https://rgb-org.github.io/\n>\n> On Mon, Dec 6, 2021, 11:21 Karl <gmkarl at gmail.com> wrote:\n>\n>> Hi,\n>>\n>> I'm not a bitcoin developer.\n>>\n>> On Mon, Dec 6, 2021, 5:05 AM H\u00e9ctor Jos\u00e9 C\u00e1rdenas Pacheco via bitcoin-dev\n>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> Hello all,\n>>>\n>>> I\u2019ve been thinking about how OP_RETURN is being used to create and trade\n>>> NFTs on Bitcoin (think RarePepes, SoG and other new ones) and was wondering\n>>> if it\u2019s possible to\n>>>\n>>\n>> Do you have a link to any of these protocols?\n>>\n>> make transactions with this opcode via Lightning.\n>>>\n>>> More specific questions could be:\n>>>\n>>>    1. Can opcodes like OP_RETURN be inside a channel\u2019s opening or\n>>>    closing transaction?\n>>>    2. If so, could that OP_RETURN change hands within that channel or\n>>>    network of channels?\n>>>\n>>> OP_RETURNs do not have ownership according to the bitcoin network.  It\n>> is not hard to define a protocol that associates an OP_RETURN with\n>> ownership, and ownership could then be transferred via lightning by sending\n>> associated currency via lightning.  Robustness improvements seem possible.\n>>\n>>\n>>>    1. If possible, could the OP_RETURN be divisible? Could one person\n>>>    send a piece of a OP_RETURN just like one can do right now on the primary\n>>>    ledger or would it need to maintain the OP_RETURN code intact?\n>>>\n>>> OP_RETURNs themselves do not have ownership, but you can define a\n>> protocol that gives them divisible ownership, including via lightning.\n>>\n>> I\u2019m assuming that, if possible, this would need a protocol layer parallel\n>>> to Bitcoin/Lightning that stores and reads all Bitcoin transactions and the\n>>> ones which involve the node's channels as well as the ones with the\n>>> OP_RETURN, just like CounterParty does right now with the primary ledger.\n>>>\n>>> Thank in advance.\n>>> \u2014\u2014\n>>>\n>>> *H\u00e9ctor C\u00e1rdenas*@hcarpach\n>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211206/c62a093a/attachment-0001.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2021-12-09T09:12:59",
                "message_text_only": "On Mon, Dec 06, 2021 at 04:35:19PM +0000, Christian Moss via bitcoin-dev wrote:\n> As far as I understand it, RGB doesn't scale NFTs as each\n> transaction to transfer ownership of an NFT would require an onchain\n> transaction\n\nRGB intends to scale NFTs and similar things in the future via scalable\nsingle-use-seals: https://petertodd.org/2017/scalable-single-use-seal-asset-transfer\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211209/468e4620/attachment.sig>"
            },
            {
                "author": "Christian Moss",
                "date": "2021-12-09T09:49:11",
                "message_text_only": "pete at petertodd.org, so single use seals require an onchain transaction to\npost the proof of publication to the ledger (assuming bitcoin is used as\nthe ledger) when an asset is transferred, but it can scale because you can\nbatch many proofs (transfer of ownerships) into a merkle tree and just add\nthe merkle root into the single tx going into the ledger?\n\nOn Thu, Dec 9, 2021 at 9:13 AM Peter Todd <pete at petertodd.org> wrote:\n\n> On Mon, Dec 06, 2021 at 04:35:19PM +0000, Christian Moss via bitcoin-dev\n> wrote:\n> > As far as I understand it, RGB doesn't scale NFTs as each\n> > transaction to transfer ownership of an NFT would require an onchain\n> > transaction\n>\n> RGB intends to scale NFTs and similar things in the future via scalable\n> single-use-seals:\n> https://petertodd.org/2017/scalable-single-use-seal-asset-transfer\n>\n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211209/9c5f1126/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2021-12-09T10:07:37",
                "message_text_only": "On Thu, Dec 09, 2021 at 09:49:11AM +0000, Christian Moss wrote:\n> pete at petertodd.org, so single use seals require an onchain transaction to\n> post the proof of publication to the ledger (assuming bitcoin is used as\n> the ledger) when an asset is transferred, but it can scale because you can\n> batch many proofs (transfer of ownerships) into a merkle tree and just add\n> the merkle root into the single tx going into the ledger?\n\nExactly. And since the aggregation is trustless with respect to validity, users\ncan choose what kind of censorship risk they're willing to take (as well as\nmitigate it with \"multisig\" schemes that use multiple aggregators in parallel).\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211209/1f650ac2/attachment.sig>"
            },
            {
                "author": "Alex Schoof",
                "date": "2021-12-09T12:12:45",
                "message_text_only": "The multisig scheme is interesting. From my understanding of Single Use\nSeals, since seal n commits to seal n+1, for the on-chain aggregation seals\nyou would want to pick some common aggregation service provider ahead of\ntime and if that provider disappears, you\u2019re stuck and cant close the next\nseal. If instead you say \u201cthis seal commits to three of the five of these\nnext seals\u201d then you mitigate both availability and censorship risk. Am I\ngetting that right?\n\nAlex\n\nOn Thu, Dec 9, 2021 at 5:23 AM Peter Todd <pete at petertodd.org> wrote:\n\n> On Thu, Dec 09, 2021 at 09:49:11AM +0000, Christian Moss wrote:\n> > pete at petertodd.org, so single use seals require an onchain transaction\n> to\n> > post the proof of publication to the ledger (assuming bitcoin is used as\n> > the ledger) when an asset is transferred, but it can scale because you\n> can\n> > batch many proofs (transfer of ownerships) into a merkle tree and just\n> add\n> > the merkle root into the single tx going into the ledger?\n>\n> Exactly. And since the aggregation is trustless with respect to validity,\n> users\n> can choose what kind of censorship risk they're willing to take (as well as\n> mitigate it with \"multisig\" schemes that use multiple aggregators in\n> parallel).\n>\n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-- \n\n\nAlex Schoof\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211209/0a9cde55/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2021-12-09T12:56:19",
                "message_text_only": "On Thu, Dec 09, 2021 at 07:12:45AM -0500, Alex Schoof wrote:\n> The multisig scheme is interesting. From my understanding of Single Use\n> Seals, since seal n commits to seal n+1, for the on-chain aggregation seals\n> you would want to pick some common aggregation service provider ahead of\n> time and if that provider disappears, you\u2019re stuck and cant close the next\n> seal. If instead you say \u201cthis seal commits to three of the five of these\n> next seals\u201d then you mitigate both availability and censorship risk. Am I\n> getting that right?\n\nRe: \"some common aggregation service provider\", you might be misunderstanding\nthe protocol: since seals are trustless with regard to validity, I can validate\nyour seal, regardless of which aggregation service you use.\n\nBut other than that, I think we're on the same page!\n\nA concrete example would be an exchange: they do a lot of transactions, so they\ncould choose to be their own aggregator, and wouldn't need any multisig at all\nbecause they can trust themselves not to censor themselves. :) Meanwhile, one\nof their customers might use 3-of-5 as you suggest, as they only do a few\ntransactions a month.\n\nInterestingly, in some scenarios it might be worthwhile to both run your own\naggregator, and use multisig. Eg Alice could use a 2-of-3 with two third-party\naggregators, and her own aggregation chain. If both third-parties are up, she\ndoes no on-chain transactions at all; if one third-party is down, she can use\nher own, and the remaining third-party. Thus she would only do an on-chain\ntransaction to defeat censorship/failure.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211209/6edd19ec/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "Sending OP_RETURN via Bitcoin Lightning",
            "categories": [
                "bitcoin-dev",
                "Lightning-dev"
            ],
            "authors": [
                "Christian Moss",
                "Martin Habov\u0161tiak",
                "Peter Todd",
                "Alex Schoof"
            ],
            "messages_count": 7,
            "total_messages_chars_count": 11779
        }
    },
    {
        "title": "[bitcoin-dev] A fee-bumping model",
        "thread_messages": [
            {
                "author": "Gloria Zhao",
                "date": "2021-12-07T17:24:33",
                "message_text_only": "Hi Darosior and Ariard,\n\nThank you for your work looking into fee-bumping so thoroughly, and for\nsharing your results. I agree about fee-bumping's importance in contract\nsecurity and feel that it's often under-prioritized. In general, what\nyou've described in this post, to me, is strong motivation for some of the\nproposed changes to RBF we've been discussing. Mostly, I have some\nquestions.\n\n> The part of Revault we are interested in for this study is the delegation\nprocess, and more\n> specifically the application of spending policies by network monitors\n(watchtowers).\n\nI'd like to better understand how fee-bumping would be used, i.e. how the\nwatchtower model works:\n- Do all of the vault parties both deposit to the vault and a refill/fee to\nthe watchtower, is there a reward the watchtower collects for a successful\nCancel, or something else? (Apologies if there's a thorough explanation\nsomewhere that I haven't already seen).\n- Do we expect watchtowers tracking multiple vaults to be batching multiple\nCancel transaction fee-bumps?\n- Do we expect vault users to be using multiple watchtowers for a better\ntrust model? If so, and we're expecting batched fee-bumps, won't those\nconflict?\n\n> For Revault we can afford to introduce malleability in the Cancel\ntransaction since there is no\n> second-stage transaction depending on its txid. Therefore it is\npre-signed with ANYONECANPAY. We\n> can't use ANYONECANPAY|SINGLE since it would open a pinning vector [3].\nNote how we can't leverage\n> the carve out rule, and neither can any other more-than-two-parties\ncontract.\n\nWe've already talked about this offline, but I'd like to point out here\nthat even transactions signed with ANYONECANPAY|ALL can be pinned by RBF\nunless we add an ancestor score rule. [0], [1] (numbers are inaccurate,\nCancel Tx feerates wouldn't be that low, but just to illustrate what the\nattack would look like)\n\n[0]:\nhttps://user-images.githubusercontent.com/25183001/135104603-9e775062-5c8d-4d55-9bc9-6e9db92cfe6d.png\n[1]:\nhttps://user-images.githubusercontent.com/25183001/145044333-2f85da4a-af71-44a1-bc21-30c388713a0d.png\n\n> can't use ANYONECANPAY|SINGLE since it would open a pinning vector [3].\nNote how we can't leverage\n> the carve out rule, and neither can any other more-than-two-parties\ncontract.\n\nWell stated about CPFP carve out. I suppose the generalization is that\nallowing n extra ancestorcount=2 descendants to a transaction means it can\nhelp contracts with <=n+1 parties (more accurately, outputs)? I wonder if\nit's possible to devise a different approach for limiting\nancestors/descendants, e.g. by height/width/branching factor of the family\ninstead of count... :shrug:\n\n> You could keep a single large UTxO and peel it as you need to sponsor\ntransactions. But this means\n> that you need to create a coin of a specific value according to your need\nat the current feerate\n> estimation, hope to have it confirmed in a few blocks (at least for now!\n[5]), and hope that the\n> value won't be obsolete by the time it confirmed.\n\nIIUC, a Cancel transaction can be generalized as a 1-in-1-out where the\ninput is presigned with counterparties, SIGHASH_ANYONECANPAY. The fan-out\nUTXO pool approach is a clever solution. I also think this smells like a\ncase where improving lower-level RBF rules is more appropriate than\nrequiring applications to write workarounds and generate extra\ntransactions. Seeing that the BIP125#2 (no new unconfirmed inputs)\nrestriction really hurts in this case, if that rule were removed, would you\nbe able to simply keep the 1 big UTXO per vault and cut out the exact\nnValue you need to fee-bump Cancel transactions? Would that feel less like\n\"burning\" for the sake of fee-bumping?\n\n> First of all, when to fee-bump? At fixed time intervals? At each block\nconnection? It sounds like,\n> given a large enough timelock, you could try to greed by \"trying your\nluck\" at a lower feerate and\n> only re-bumping every N blocks. You would then start aggressively bumping\nat every block after M\n> blocks have passed.\n\nI'm wondering if you also considered other questions like:\n- Should a fee-bumping strategy be dependent upon the rate of incoming\ntransactions? To me, it seems like the two components are (1) what's in the\nmempool and (2) what's going to trickle into the mempool between now and\nthe target block. The first component is best-effort keeping\nincentive-compatible mempool; historical data and crystal ball look like\nthe only options for incorporating the 2nd component.\n- Should the fee-bumping strategy depend on how close you are to your\ntimelock expiry? (though this seems like a potential privacy leak, and the\ngame theory could get weird as you mentioned).\n- As long as you have a good fee estimator (i.e. given a current mempool,\ncan get an accurate feerate given a % probability of getting into target\nblock n), is there any reason to devise a fee-bumping strategy beyond\npicking a time interval?\n\nIt would be interesting to see stats on the spread of feerates in blocks\nduring periods of fee fluctuation.\n\n> > In the event that you notice a consequent portion of the block is\nfilled with transactions paying\n> > less than your own, you might want to start panicking and bump your\ntransaction fees by a certain\n> > percentage with no consideration for your fee estimator. You might skew\nminers incentives in doing\n> > so: if you increase the fees by a factor of N, any miner with a\nfraction larger than 1/N of the\n> > network hashrate now has an incentive to censor your transaction at\nfirst to get you to panic.\n\n> Yes I think miner-harvesting attacks should be weighed carefully in the\ndesign of offchain contracts fee-bumping strategies, at least in the future\nwhen the mining reward exhausts further.\n\nMiner-harvesting (such cool naming!) is interesting, but I want to clarify\nthe value of N - I don't think it's the factor by which you increase the\nfees on just your transaction.\n\nTo codify: your transaction pays a fee of `f1` right now and might pay a\nfee of `f2` in a later block that the miner expects to mine with 1/N\nprobability. The economically rational miner isn't incentivized if simply\n`f2 = N * f1` unless their mempool is otherwise empty.\nBy omitting your transaction in this block, the miner can include another\ntransaction/package paying `g1` fees instead, so they lose `f1-g1` in fees\nright now. In the future block, they have the choice between collecting\n`f2` or `g2` (from another transaction/package) in fees, so their gain is\n`max(f2-g2, 0)`.\nSo the equation is more like: a miner with 1/N of the hashrate, employing\nthis censorship strategy, gains only if `max(f2-g2, 0) > N * (f1-g1)`. More\nbroadly, the miner only profits if `f2` is significantly higher than `g2`\nand `f1` is about the same feerate as everything else in your mempool: it\nseems like they're betting on how much you _overshoot_, not how much you\nbump.\n\nIn general, I agree it would really suck to inadvertently create a game\nwhere miners can drive feerates up by triggering desperation-driven\nfee-bumping procedures. I guess this is a reason to avoid\nincreasingly-aggressive feebumping, or strategies where we predictably\novershoot.\n\nSlightly related question: in contracts, generally, the timelock deadline\nis revealed in the script, so the miner knows how \"desperate\" we are right?\nIs that a problem? For Revault, if your Cancel transaction is a keypath\nspend (I think I remember reading that somewhere?) and you don't reveal the\nscript, they don't see your timelock deadline yes?\n\nAgain, thanks for the digging and sharing. :)\n\nBest,\nGloria\n\nOn Tue, Nov 30, 2021 at 3:27 PM darosior via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi Antoine,\n>\n> Thanks for your comment. I believe for Lightning it's simpler with regard\n> to the management of the UTxO pool, but harder with regard to choosing\n> a threat model.\n> Responses inline.\n>\n>\n> For any opened channel, ensure the confirmation of a Commitment\n> transaction and the children HTLC-Success/HTLC-Timeout transactions. Note,\n> in the Lightning security game you have to consider (at least) 4 types of\n> players moves and incentives : your node, your channel counterparties, the\n> miners, the crowd of bitcoin users. The number of the last type of players\n> is unknown from your node, however it should not be forgotten you're in\n> competition for block space, therefore their block demands bids should be\n> anticipated and reacted to in consequence. With that remark in mind,\n> implications for your LN fee-bumping strategy will be raised afterwards.\n>\n> For a LN service provider, on-chain overpayments are bearing on your\n> operational costs, thus downgrading your economic competitiveness. For the\n> average LN user, overpayment might price out outside a LN non-custodial\n> deployment, as you don't have the minimal security budget to be on your own.\n>\n>\n> I think this problem statement can be easily generalised to any offchain\n> contract. And your points stand for all of them.\n> \"For any opened contract, ensure at any point the confirmation of a (set\n> of) transaction(s) in a given number of blocks\"\n>\n>\n> Same issue with Lightning, we can be pinned today on the basis of\n> replace-by-fee rule 3. We can be also blinded by network mempool\n> partitions, a pinning counterparty can segregate all the full-nodes  in as\n> many subsets by broadcasting a revoked Commitment transaction different for\n> each. For Revault, I think you can also do unlimited partitions by mutating\n> the ANYONECANPAY-input of the Cancel.\n>\n>\n> Well you can already do unlimited partitions by adding different inputs to\n> it. You could malleate the witness, but since we are using Miniscript i'm\n> confident you would only be able in a marginal way.\n>\n>\n> That said, if you have a distributed towers deployment, spread across the\n> p2p network topology, and they can't be clustered together through\n> cross-layers or intra-layer heuristics, you should be able to reliably\n> observe such partitions. I think such distributed monitors are deployed by\n> few L1 merchants accepting 0-conf to detect naive double-spend.\n>\n>\n> We should aim to more than 0-conf (in)security level..\n> It seems to me the only policy-level mitigation for RBF pinning around the\n> \"don't decrease the abolute fees of a less-than-a-block mempool\" would be\n> to drop the requirement on increasing absolute fees if the mempool is \"full\n> enough\" (and the feerate increases exponentially, of course).\n> Another approach could be by introducing new consensus rules as proposed\n> by Jeremy last year [0]. If we go in the realm of new consensus rules, then\n> i think that simply committing to a maximum tx size would fix pinning by\n> RBF rule 3. Could be in the annex, or in the unused sequence bits (although\n> they currently are by Lightning, meh). You could also check in the output\n> script that the input commits to this.\n>\n> [0]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-September/018168.html\n>\n>\n> Have we already discussed a fee-bumping \"shared cache\", a CPFP variation ?\n> Strawman idea: Alice and Bob commit collateral inputs to a separate UTXO\n> from the main \"offchain contract\" one. This UTXO is locked by a multi-sig.\n> For any Commitment transaction pre-signed, also counter-sign a CPFP with\n> top mempool feerate included, spending a Commitment anchor output and the\n> shared-cache UTXO. If the fees spike,  you can re-sign a high-feerate CPFP,\n> assuming interactivity. As the CPFP is counter-signed by everyone, the\n> outputs can be CSV-1 encumbered to prevent pinnings. If the share-cache is\n> feeded at parity, there shouldn't be an incentive to waste or maliciously\n> inflate the feerate. I think this solution can be easily generalized to\n> more than 2 counterparties by using a multi-signature scheme. Big issue, if\n> the feerate is short due to fee spikes and you need to re-sign a\n> higher-feerate CPFP, you're trusting your counterparty to interact, though\n> arguably not worse than the current update fee mechanism.\n>\n>\n> It really looks just like `update_fee`. Except maybe with the property\n> that you have the channel liquidity not depend on the onchain feerate.\n> In any case, for Lightning i think it's a bad idea to re-introduce trust\n> on this side post anchor outputs. For Revault it's clearly out of the\n> question to introduce trust in your counterparties (why would you bother\n> having a fee-bumping mechanism in the first place then?). Probably the same\n> holds for all offchain contracts.\n>\n>\n> > For Lightning, it'd mean keeping an equivalent amount of funds as the\n> sum of all your\n> channels balances sitting there unallocated \"just in case\". This is not\n> reasonable.\n>\n> Agree, game-theory wise, you would like to keep a full fee-bumping\n> reserve, ready to burn as much in fees as the contested HTLC value, as it's\n> the maximum gain of your counterparty. Though perfect equilibrium is hard\n> to achieve because your malicious counterparty might have an edge pushing\n> you to broadcast your Commitment first by witholding HTLC resolution.\n>\n> Fractional fee-bumping reserves are much more realistic to expect in the\n> LN network. Lower fee-bumping reserve, higher liquidity deployed, in theory\n> higher routing fees. By observing historical feerates, average offchain\n> balances at risk and routing fees expected gains, you should be able to\n> discover an equilibrium where higher levels of reserve aren't worth the\n> opportunity cost. I guess this  equilibrium could be your LN fee-bumping\n> reserve max feerate.\n>\n> Note, I think the LN approach is a bit different from what suits a custody\n> protocol like Revault,  as you compute a direct return of the frozen\n> fee-bumping liquidity. With Revault, if you have numerous bitcoins\n> protected, it's might be more interesting to adopt a \"buy the mempool,\n> stupid\" strategy than risking fund safety for few percentages of interest\n> returns.\n>\n>\n> True for routing nodes. For wallets (if receiving funds), it's not about\n> an investment: just users expectations to being able to transact without\n> risking to lose their funds (ie being able to enforce their contract\n> onchain). Although wallets they are much less at risk.\n>\n>\n> This is where the \"anticipate the crowd of bitcoin users move\" point can\n> be laid out. As the crowd of bitcoin users' fee-bumping reserves are\n> ultimately unknown from your node knowledge, you should be ready to be a\n> bit more conservative than the vanilla fee-bumping strategies shipped by\n> default. In case of massive mempool congestion, your additional\n> conservatism might get your time-sensitive transactions and game on the\n> crowd of bitcoin users. First Problem: if all offchain bitcoin software\n> adopt that strategy we might inflate the worst-case feerate rate at the\n> benefit of the miners, without holistically improving block throughput.\n> Second problem : your class of offchain bitcoin softwares might have\n> ridiculous fee-bumping reserve compared\n> to other classes of offchain bitcoin softwares (Revault > Lightning) and\n> just be priced out bydesign in case of mempool congestion. Third problem :\n> as the number of offchain bitcoin applications should go up with time, your\n> fee-bumping reserve levels based from historical data might be always late\n> by one \"bank-run\" scenario.\n>\n>\n> Black swan event 2.0? Just rule n\u00b03 is inherent to any kind of fee\n> estimation.\n>\n> For Lightning, if you're short in fee-bumping reserves you might still do\n> preemptive channel closures, either cooperatively or unilaterally and get\n> back the off-chain liquidity to protect the more economically interesting\n> channels. Though again, that kind of automatic behavior might be compelling\n> at the individual node-level, but make the mempol congestion worse\n> holistically.\n>\n>\n> Yeah so we are back to the \"fractional reserve\" model: you can only\n> enforce X% of the offchain contracts your participate in.. Actually it's\n> even an added assumption: that you still have operating contracts, with\n> honest counterparties.\n>\n>\n> In case of massive mempool congestion, you might try to front-run the\n> crowd of bitcoin users relying on block connections for fee-bumping, and\n> thus start your fee-bumping as soon as you observe feerate groups\n> fluctuations in your local mempool(s).\n>\n>\n> I don't think any kind of mempool-based estimate generalizes well, since\n> at any point the expected time before the next block is 10 minutes (and a\n> lot can happen in 10min).\n>\n> Also you might proceed your fee-bumping ticks on a local clock instead of\n> block connections in case of time-dilation or deeper eclipse attacks of\n> your local node. Your view of the chain might be compromised but not your\n> ability to broadcast transactions thanks to emergency channels (in the\n> non-LN sense...though in fact quid of txn wrapped in onions ?) of\n> communication.\n>\n>\n> Oh, yeah, i didn't explicit \"not getting eclipsed\" (or more generally\n> \"data availability\") as an assumption since it's generally one made by\n> participants of any offchain contract. In this case you can't even have\n> decent fee estimation, so you are screwed anyways.\n>\n>\n> Yes, stay open the question on how you enforce this block insurance\n> market. Reputation, which might be to avoid due to the latent\n> centralization effect, might be hard to stack and audit reliably for an\n> emergency mechanism running, hopefully, once in a halvening period. Maybe\n> maybe some cryptographic or economically based mechanism on slashing or\n> swaps could be found...\n>\n>\n> Unfortunately, given current mining centralisation, pools are in a very\n> good position to offer pretty decent SLAs around that. With a block space\n> insurance, you of course don't need all these convoluted fee-bumping hacks.\n> I'm very concerned that large stakeholders of the \"offchain contracts\n> ecosystem\" would just go this (easier) way and further increase mining\n> centralisation pressure.\n>\n> I agree that a cryptography-based scheme around this type of insurance\n> services would be the best way out.\n>\n>\n> Antoine\n>\n> Le lun. 29 nov. 2021 \u00e0 09:34, darosior via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>\n>> Hi everyone,\n>>\n>> Fee-bumping is paramount to the security of many protocols building on\n>> Bitcoin, as they require the\n>> confirmation of a transaction (which might be presigned) before the\n>> expiration of a timelock at any\n>> point after the establishment of the contract.\n>>\n>> The part of Revault using presigned transactions (the delegation from a\n>> large to a smaller multisig)\n>> is no exception. We have been working on how to approach this for a while\n>> now and i'd like to share\n>> what we have in order to open a discussion on this problem so central to\n>> what seem to be The Right\n>> Way [0] to build on Bitcoin but which has yet to be discussed in details\n>> (at least publicly).\n>>\n>> I'll discuss what we came up with for Revault (at least for what will be\n>> its first iteration) but my\n>> intent with posting to the mailing list is more to frame the questions to\n>> this problem we are all\n>> going to face rather than present the results of our study tailored to\n>> the Revault usecase.\n>> The discussion is still pretty Revault-centric (as it's the case study)\n>> but hopefully this can help\n>> future protocol designers and/or start a discussion around what\n>> everyone's doing for existing ones.\n>>\n>>\n>> ## 1. Reminder about Revault\n>>\n>> The part of Revault we are interested in for this study is the delegation\n>> process, and more\n>> specifically the application of spending policies by network monitors\n>> (watchtowers).\n>> Coins are received on a large multisig. Participants of this large\n>> multisig create 2 [1]\n>> transactions. The Unvault, spending a deposit UTxO, creates an output\n>> paying either to the small\n>> multisig after a timelock or to the large multisig immediately. The\n>> Cancel, spending the Unvault\n>> output through the non-timelocked path, creates a new deposit UTxO.\n>> Participants regularly exchange the Cancel transaction signatures for\n>> each deposit, sharing the\n>> signatures with the watchtowers they operate. They then optionally [2]\n>> sign the Unvault transaction\n>> and share the signatures with the small multisig participants who can in\n>> turn use them to proceed\n>> with a spending. Watchtowers can enforce spending policies (say, can't\n>> Unvault outside of business\n>> hours) by having the Cancel transaction be confirmed before the\n>> expiration of the timelock.\n>>\n>>\n>> ## 2. Problem statement\n>>\n>> For any delegated vault, ensure the confirmation of a Cancel transaction\n>> in a configured number of\n>> blocks at any point. In so doing, minimize the overpayments and the UTxO\n>> set footprint. Overpayments\n>> increase the burden on the watchtower operator by increasing the required\n>> frequency of refills of the\n>> fee-bumping wallet, which is already the worst user experience. You are\n>> likely to manage a number of\n>> UTxOs with your number of vaults, which comes at a cost for you as well\n>> as everyone running a full\n>> node.\n>>\n>> Note that this assumes miners are economically rationale, are\n>> incentivized by *public* fees and that\n>> you have a way to propagate your fee-bumped transaction to them. We also\n>> don't consider the block\n>> space bounds.\n>>\n>> In the previous paragraph and the following text, \"vault\" can generally\n>> be replaced with \"offchain\n>> contract\".\n>>\n>>\n>> ## 3. With presigned transactions\n>>\n>> As you all know, the first difficulty is to get to be able to\n>> unilaterally enforce your contract\n>> onchain. That is, any participant must be able to unilaterally bump the\n>> fees of a transaction even\n>> if it was co-signed by other participants.\n>>\n>> For Revault we can afford to introduce malleability in the Cancel\n>> transaction since there is no\n>> second-stage transaction depending on its txid. Therefore it is\n>> pre-signed with ANYONECANPAY. We\n>> can't use ANYONECANPAY|SINGLE since it would open a pinning vector [3].\n>> Note how we can't leverage\n>> the carve out rule, and neither can any other more-than-two-parties\n>> contract.\n>> This has a significant implication for the rest, as we are entirely\n>> burning fee-bumping UTxOs.\n>>\n>> This opens up a pinning vector, or at least a significant nuisance: any\n>> other party can largely\n>> increase the absolute fee without increasing the feerate, leveraging the\n>> RBF rules to prevent you\n>> from replacing it without paying an insane fee. And you might not see it\n>> in your own mempool and\n>> could only suppose it's happening by receiving non-full blocks or with\n>> transactions paying a lower\n>> feerate.\n>> Unfortunately i know of no other primitive that can be used by\n>> multi-party (i mean, >2) presigned\n>> transactions protocols for fee-bumping that aren't (more) vulnerable to\n>> pinning.\n>>\n>>\n>> ## 4. We are still betting on future feerate\n>>\n>> The problem is still missing one more constraint. \"Ensuring confirmation\n>> at any time\" involves ensuring\n>> confirmation at *any* feerate, which you *cannot* do. So what's the\n>> limit? In theory you should be ready\n>> to burn as much in fees as the value of the funds you want to get out of\n>> the contract. So... For us\n>> it'd mean keeping for each vault an equivalent amount of funds sitting\n>> there on the watchtower's hot\n>> wallet. For Lightning, it'd mean keeping an equivalent amount of funds as\n>> the sum of all your\n>> channels balances sitting there unallocated \"just in case\". This is not\n>> reasonable.\n>>\n>> So you need to keep a maximum feerate, above which you won't be able to\n>> ensure the enforcement of\n>> all your contracts onchain at the same time. We call that the \"reserve\n>> feerate\" and you can have\n>> different strategies for choosing it, for instance:\n>> - The 85th percentile over the last year of transactions feerates\n>> - The maximum historical feerate\n>> - The maximum historical feerate adjusted in dollars (makes more sense\n>> but introduces a (set of?)\n>>   trusted oracle(s) in a security-critical component)\n>> - Picking a random high feerate (why not? It's an arbitrary assumption\n>> anyways)\n>>\n>> Therefore, even if we don't have to bet on the broadcast-time feerate\n>> market at signing time anymore\n>> (since we can unilaterally bump), we still need some kind of prediction\n>> in preparation of making\n>> funds available to bump the fees at broadcast time.\n>> Apart from judging that 500sat/vb is probably more reasonable than\n>> 10sat/vbyte, this unfortunately\n>> sounds pretty much crystal-ball-driven.\n>>\n>> We currently use the maximum of the 95th percentiles over 90-days windows\n>> over historical block chain\n>> feerates. [4]\n>>\n>>\n>> ## 5. How much funds does my watchtower need?\n>>\n>> That's what we call the \"reserve\". Depending on your reserve feerate\n>> strategy it might vary over\n>> time. This is easier to reason about with a per-contract reserve. For\n>> Revault it's pretty\n>> straightforward since the Cancel transaction size is static:\n>> `reserve_feerate * cancel_size`. For\n>> other protocols with dynamic transaction sizes (or even packages of\n>> transactions) it's less so. For\n>> your Lightning channel you would probably take the maximum size of your\n>> commitment transaction\n>> according to your HTLC exposure settings + the size of as many\n>> `htlc_success` transaction?\n>>\n>> Then you either have your software or your user guesstimate how many\n>> offchain contracts the\n>> watchtower will have to watch, time that by the per-contract reserve and\n>> refill this amount (plus\n>> some slack in practice). Once again, a UX tradeoff (not even mentioning\n>> the guesstimation UX):\n>> overestimating leads to too many unallocated funds sitting on a hot\n>> wallet, underestimating means\n>> (at best) inability to participate in new contracts or being \"at risk\"\n>> (not being able to enforce\n>> all your contracts onchain at your reserve feerate) before a new refill.\n>>\n>> For vaults you likely have large-value UTxOs and small transactions (the\n>> Cancel is one-in one-out in\n>> Revault). For some other applications with large transactions and\n>> lower-value UTxOs on average it's\n>> likely that only part of the offchain contracts might be enforceable at a\n>> reasonable feerate. Is it\n>> reasonable?\n>>\n>>\n>> ## 6. UTxO pool layout\n>>\n>> Now that you somehow managed to settle on a refill amount, how are you\n>> going to use these funds?\n>> Also, you'll need to manage your pool across time (consolidating small\n>> coins, and probably fanning\n>> out large ones).\n>>\n>> You could keep a single large UTxO and peel it as you need to sponsor\n>> transactions. But this means\n>> that you need to create a coin of a specific value according to your need\n>> at the current feerate\n>> estimation, hope to have it confirmed in a few blocks (at least for now!\n>> [5]), and hope that the\n>> value won't be obsolete by the time it confirmed. Also, you'd have to do\n>> that for any number of\n>> Cancel, chaining feebump coin creation transactions off the change of the\n>> previous ones or replacing\n>> them with more outputs. Both seem to become really un-manageable (and\n>> expensive) in many edge-cases,\n>> shortening the time you have to confirm the actual Cancel transaction and\n>> creating uncertainty about\n>> the reserve (how much is my just-in-time fanout going to cost me in fees\n>> that i need to refill in\n>> advance on my watchtower wallet?).\n>> This is less of a concern for protocols using CPFP to sponsor\n>> transactions, but they rely on a\n>> policy rule specific to 2-parties contracts.\n>>\n>> Therefore for Revault we fan-out the coins per-vault in advance. We do so\n>> at refill time so the\n>> refiller can give an excess to pay for the fees of the fanout transaction\n>> (which is reasonable since\n>> it will occur just after the refilling transaction confirms). When the\n>> watchtower is asked to watch\n>> for a new delegated vault it will allocate coins from the pool of\n>> fanned-out UTxOs to it (failing\n>> that, it would refuse the delegation).\n>> What is a good distribution of UTxOs amounts per vault? We want to\n>> minimize the number of coins,\n>> still have coins small enough to not overpay (remember, we can't have\n>> change) and be able to bump a\n>> Cancel up to the reserve feerate using these coins. The two latter\n>> constraints are directly in\n>> contradiction as the minimal value of a coin usable at the reserve\n>> feerate (paying for its own input\n>> fee + bumping the feerate by, say, 5sat/vb) is already pretty high.\n>> Therefore we decided to go with\n>> two distributions per vault. The \"reserve distribution\" alone ensures\n>> that we can bump up to the\n>> reserve feerate and is usable for high feerates. The \"bonus distribution\"\n>> is not, but contains\n>> smaller coins useful to prevent overpayments during low and medium fee\n>> periods (which is most of the\n>> time).\n>> Both distributions are based on a basic geometric suite [6]. Each value\n>> is half the previous one.\n>> This exponentially decreases the value, limiting the number of coins. But\n>> this also allows for\n>> pretty small coins to exist and each coin's value is equal to the sum of\n>> the smaller coins,\n>> or smaller by at most the value of the smallest coin. Therefore bounding\n>> the maximum overpayment to\n>> the smallest coin's value [7].\n>>\n>> For the management of the UTxO pool across time we merged the\n>> consolidation with the fanout. When\n>> fanning out a refilled UTxO, we scan the pool for coins that need to be\n>> consolidated according to a\n>> heuristic. An instance of a heuristic is \"the coin isn't allocated and\n>> would not have been able to\n>> increase the fee at the median feerate over the past 90 days of blocks\".\n>> We had this assumption that feerate would tend to go up with time and\n>> therefore discarded having to\n>> split some UTxOs from the pool. We however overlooked that a large\n>> increase in the exchange price of\n>> BTC as we've seen during the past year could invalidate this assumption\n>> and that should arguably be\n>> reconsidered.\n>>\n>>\n>> ## 7. Bumping and re-bumping\n>>\n>> First of all, when to fee-bump? At fixed time intervals? At each block\n>> connection? It sounds like,\n>> given a large enough timelock, you could try to greed by \"trying your\n>> luck\" at a lower feerate and\n>> only re-bumping every N blocks. You would then start aggressively bumping\n>> at every block after M\n>> blocks have passed. But that's actually a bet (in disguised?) that the\n>> next block feerate in M blocks\n>> will be lower than the current one. In the absence of any predictive\n>> model it is more reasonable to\n>> just start being aggressive immediately.\n>> You probably want to base your estimates on `estimatesmartfee` and as a\n>> consequence you would re-bump\n>> (if needed )after each block connection, when your estimates get updated\n>> and you notice your\n>> transaction was not included in the block.\n>>\n>> In the event that you notice a consequent portion of the block is filled\n>> with transactions paying\n>> less than your own, you might want to start panicking and bump your\n>> transaction fees by a certain\n>> percentage with no consideration for your fee estimator. You might skew\n>> miners incentives in doing\n>> so: if you increase the fees by a factor of N, any miner with a fraction\n>> larger than 1/N of the\n>> network hashrate now has an incentive to censor your transaction at first\n>> to get you to panic. Also\n>> note this can happen if you want to pay the absolute fees for the\n>> 'pinning' attack mentioned in\n>> section #2, and that might actually incentivize miners to perform it\n>> themselves..\n>>\n>> The gist is that the most effective way to bump and rebump (RBF the\n>> Cancel tx) seems to just be to\n>> consider the `estimatesmartfee 2 CONSERVATIVE` feerate at every block\n>> your tx isn't included in, and\n>> to RBF it if the feerate is higher.\n>> In addition, we fallback to a block chain based estimation when estimates\n>> aren't available (eg if\n>> the user stopped their WT for say a hour and we come back up): we use the\n>> 85th percentile over the\n>> feerates in the last 6 blocks. Sure, miners can try to have an influence\n>> on that by stuffing their\n>> blocks with large fee self-paying transactions, but they would need to:\n>> 1. Be sure to catch a significant portion of the 6 blocks (at least 2,\n>> actually)\n>> 2. Give up on 25% of the highest fee-paying transactions (assuming they\n>> got the 6 blocks, it's\n>>    proportionally larger and incertain as they get less of them)\n>> 3. Hope that our estimator will fail and we need to fall back to the\n>> chain-based estimation\n>>\n>>\n>> ## 8. Our study\n>>\n>> We essentially replayed the historical data with different deployment\n>> configurations (number of\n>> participants and timelock) and probability of an event occurring (event\n>> being say an Unvault, an\n>> invalid Unvault, a new delegation, ..). We then observed different\n>> metrics such as the time at risk\n>> (when we can't enforce all our contracts at the reserve feerate at the\n>> same time), or the\n>> operational cost.\n>> We got the historical fee estimates data from Statoshi [9], Txstats [10]\n>> and the historical chain\n>> data from Riccardo Casatta's `blocks_iterator` [11]. Thanks!\n>>\n>> The (research-quality..) code can be found at\n>> https://github.com/revault/research under the section\n>> \"Fee bumping\". Again it's very Revault specific, but at least the data\n>> can probably be reused for\n>> studying other protocols.\n>>\n>>\n>> ## 9. Insurances\n>>\n>> Of course, given it's all hacks and workarounds and there is no good\n>> answer to \"what is a reasonable\n>> feerate up to which we need to make contracts enforceable onchain?\",\n>> there is definitely room for an\n>> insurance market. But this enters the realm of opinions. Although i do\n>> have some (having discussed\n>> this topic for the past years with different people), i would like to\n>> keep this post focused on the\n>> technical aspects of this problem.\n>>\n>>\n>>\n>> [0] As far as i can tell, having offchain contracts be enforceable\n>> onchain by confirming a\n>> transaction before the expiration of a timelock is a widely agreed-upon\n>> approach. And i don't think\n>> we can opt for any other fundamentally different one, as you want to know\n>> you can claim back your\n>> coins from a contract after a deadline before taking part in it.\n>>\n>> [1] The Real Revault (tm) involves more transactions, but for the sake of\n>> conciseness i only\n>> detailed a minimum instance of the problem.\n>>\n>> [2] Only presigning part of the Unvault transactions allows to only\n>> delegate part of the coins,\n>> which can be abstracted as \"delegate x% of your stash\" in the user\n>> interface.\n>>\n>> [3]\n>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-May/017835.html\n>>\n>> [4]\n>> https://github.com/revault/research/blob/1df953813708287c32a15e771ba74957ec44f354/feebumping/model/statemachine.py#L323-L329\n>>\n>> [5] https://github.com/bitcoin/bitcoin/pull/23121\n>>\n>> [6]\n>> https://github.com/revault/research/blob/1df953813708287c32a15e771ba74957ec44f354/feebumping/model/statemachine.py#L494-L507\n>>\n>> [7] Of course this assumes a combinatorial coin selection, but i believe\n>> it's ok given we limit the\n>> number of coins beforehand.\n>>\n>> [8] Although there is the argument to outbid a censorship, anyone\n>> censoring you isn't necessarily a\n>> miner.\n>>\n>> [9] https://www.statoshi.info/\n>>\n>> [10] https://www.statoshi.info/\n>>\n>> [11] https://github.com/RCasatta/blocks_iterator\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211207/79ed7638/attachment-0001.html>"
            },
            {
                "author": "darosior",
                "date": "2021-12-08T14:51:25",
                "message_text_only": "Hi Gloria,\n\nI agree with regard to the RBF changes. To me, we should (obviously?) do ancestor feerate instead of requiring confirmed inputs (#23121).\nHowever, we are yet to come with a reasonable policy-only fix to \"rule 3 pinning\".\n\nResponses inline!\n\n>> The part of Revault we are interested in for this study is the delegation process, and more\n>> specifically the application of spending policies by network monitors (watchtowers).\n>\n> I'd like to better understand how fee-bumping would be used, i.e. how the watchtower model works:\n> - Do all of the vault parties both deposit to the vault and a refill/fee to the watchtower, is there a reward the watchtower collects for a successful Cancel, or something else? (Apologies if there's a thorough explanation somewhere that I haven't already seen).\n> - Do we expect watchtowers tracking multiple vaults to be batching multiple Cancel transaction fee-bumps?\n> - Do we expect vault users to be using multiple watchtowers for a better trust model? If so, and we're expecting batched fee-bumps, won't those conflict?\n\nGrossly, it could be described as \"enforce spending policied on 10BTC worth of delegated coins by allocating 5mBTC to 3 different watchtowers\".\nEach participant that will be delegating coins is expected to run a number of watchtowers. They should ideally be replicated (for full disclosure if\nit wasn't obvious providing replication is the business model of the company behind the Revault project :p).\nYou can't batch fee-bumps as you *could* (maybe not *would*) do with anchor outputs channels, since the latter use CPFP and we \"sponsor\"\n(or whatever the name of \"supplementing the fees of a transaction by adding inputs\" is).\nIn the current model, watchtowers enforcing the same policy do compete in that they broadcast conflicting transactions since they attach different\nfee-bumping inputs. Ideally we could sign the added feebumping inputs themselves with ACP so they are allowed to cooperate. However doing that\nwould allow anyone on the network to snip the added fees to perform a \"RBF rule-3 pinning\".\nFinally, there could be concerns around game theory of letting others' watchtowers feebump for you. I'm convinced however that in our case the fee\nis completely dwarfed by the value at stake. Trying to delay your own WT's fee-bump reaction to hope someone else will pay the 10k sats for enforcing\na 1BTC contract, hmmm, i wouldn't do that.\n\n>> For Revault we can afford to introduce malleability in the Cancel transaction since there is no\n>> second-stage transaction depending on its txid. Therefore it is pre-signed with ANYONECANPAY. We\n>> can't use ANYONECANPAY|SINGLE since it would open a pinning vector [3]. Note how we can't leverage\n>> the carve out rule, and neither can any other more-than-two-parties contract.\n>\n> We've already talked about this offline, but I'd like to point out here that even transactions signed with ANYONECANPAY|ALL can be pinned by RBF unless we add an ancestor score rule. [0], [1] (numbers are inaccurate, Cancel Tx feerates wouldn't be that low, but just to illustrate what the attack would look like)\n\nThanks for expliciting that, i should have mentioned it. For everyone reading the PR is at https://github.com/bitcoin/bitcoin/pull/23121 .\n\n>> can't use ANYONECANPAY|SINGLE since it would open a pinning vector [3]. Note how we can't leverage\n>> the carve out rule, and neither can any other more-than-two-parties contract.\n>\n> Well stated about CPFP carve out. I suppose the generalization is that allowing n extra ancestorcount=2 descendants to a transaction means it can help contracts with <=n+1 parties (more accurately, outputs)? I wonder if it's possible to devise a different approach for limiting ancestors/descendants, e.g. by height/width/branching factor of the family instead of count... :shrug:\n\nI don't think so, because you want any party involved in the contract to be able to unilaterally enforce it. With >2 anchor outputs any 2-parties can\ncollude against the other one(s) by pinning the transaction using the first party's output to hit the descendant chain limit and the second one to trigger\nthe carve-out.\n\nIdeally i think it'd be better that all contracts move toward using sponsoring (\"tx malleation\") when we can (ie for all transactions that are at the end of\nthe chain, or post-ANYPREVOUT any transaction really) instead of CPFP for fee-bumping because:\n1. It's way easier to reason about wrt mempool DOS protections (the fees don't depend on a chain of childs, it's just a function of the transaction alone)\n2. It's more space efficient (and thereby economical): you don't need to create a new transaction (or a set of new txs) to bump your fees.\n\nUnfortunately, having to use ACP instead of ACP|SINGLE is a showstopper. Managing a fee-bumping UTxO pool is a massive burden.\nOn a side note, thinking back about ACP|SINGLE vs ACP i'm not so sure anymore the latter opens up more pinning vectors than the former..\n\n> IIUC, a Cancel transaction can be generalized as a 1-in-1-out where the input is presigned with counterparties, SIGHASH_ANYONECANPAY. The fan-out UTXO pool approach is a clever solution. I also think this smells like a case where improving lower-level RBF rules is more appropriate than requiring applications to write workarounds and generate extra transactions. Seeing that the BIP125#2 (no new unconfirmed inputs) restriction really hurts in this case, if that rule were removed, would you be able to simply keep the 1 big UTXO per vault and cut out the exact nValue you need to fee-bump Cancel transactions? Would that feel less like \"burning\" for the sake of fee-bumping?\n\nI am not sure. It's a question i raised when i was made aware of your finding of the \"no unconfirmed\" rule defect and your proposal to move to ancestor\nscore instead. Without further consideration i'd say yes, but this needs more research. I'm also biased as i really want to get rid of this coin pool for both\nthe complexity and the social cost..\n\n>> First of all, when to fee-bump? At fixed time intervals? At each block connection? It sounds like,\n>> given a large enough timelock, you could try to greed by \"trying your luck\" at a lower feerate and\n>> only re-bumping every N blocks. You would then start aggressively bumping at every block after M\n>> blocks have passed.\n>\n> I'm wondering if you also considered other questions like:\n> - Should a fee-bumping strategy be dependent upon the rate of incoming transactions? To me, it seems like the two components are (1) what's in the mempool and (2) what's going to trickle into the mempool between now and the target block. The first component is best-effort keeping incentive-compatible mempool; historical data and crystal ball look like the only options for incorporating the 2nd component.\n> - Should the fee-bumping strategy depend on how close you are to your timelock expiry? (though this seems like a potential privacy leak, and the game theory could get weird as you mentioned).\n> - As long as you have a good fee estimator (i.e. given a current mempool, can get an accurate feerate given a % probability of getting into target block n), is there any reason to devise a fee-bumping strategy beyond picking a time interval?\n\nI think (again, ideally) applications should take `estimatesmarfee` as a black box, and not look into the mempool by themselves. Now whether we should\ntake into account the mempool data for short target estimation, i don't know. The first issue that comes to mind is how to measure whether your mempool\nis \"up-to-date\" (i mean if you have most of the current unconfirmed transactions). Weak blocks were mentionned elsewhere, and i think they can help for\nthis (you don't influence your estimate by the rate of new unconfirmed transactions you hear about, but what miners are currently working on). Now, sure,\nthe expected time before the next block would be 10min. But for a short target estimate it still seems better to base your estimate on the most up-to-date\ndata you can get? (maybe not? Can a statistician chime in?)\n\nThis section was about arguing that it doesn't make sense to start low and get to next-block feerate as you approach your timelock expiration. Is your\nquestion about whether it makes sense to start, as you get closer to timelock maturation, feebumping not on the basis of what your fee estimator gives\nyou but blindly i don't believe it does. If all you have is a fee-bumping method, all confirmation problems look like a fee-paying one :p.\nYou are already assuming your fee estimator is working and isn't being manipulated. If it does, and you don't get confirmed after X blocks and as many\nre-bumping attempts the problem is elsewhere imo.\n\n> It would be interesting to see stats on the spread of feerates in blocks during periods of fee fluctuation.\n>\n>> > In the event that you notice a consequent portion of the block is filled with transactions paying\n>> > less than your own, you might want to start panicking and bump your transaction fees by a certain\n>> > percentage with no consideration for your fee estimator. You might skew miners incentives in doing\n>> > so: if you increase the fees by a factor of N, any miner with a fraction larger than 1/N of the\n>> > network hashrate now has an incentive to censor your transaction at first to get you to panic.\n>\n>> Yes I think miner-harvesting attacks should be weighed carefully in the design of offchain contracts fee-bumping strategies, at least in the future when the mining reward exhausts further.\n>\n> Miner-harvesting (such cool naming!) is interesting, but I want to clarify the value of N - I don't think it's the factor by which you increase the fees on just your transaction.\n> To codify: your transaction pays a fee of `f1` right now and might pay a fee of `f2` in a later block that the miner expects to mine with 1/N probability. The economically rational miner isn't incentivized if simply `f2 = N * f1` unless their mempool is otherwise empty.\n> By omitting your transaction in this block, the miner can include another transaction/package paying `g1` fees instead, so they lose `f1-g1` in fees right now. In the future block, they have the choice between collecting `f2` or `g2` (from another transaction/package) in fees, so their gain is `max(f2-g2, 0)`.\n> So the equation is more like: a miner with 1/N of the hashrate, employing this censorship strategy, gains only if `max(f2-g2, 0) > N * (f1-g1)`. More broadly, the miner only profits if `f2` is significantly higher than `g2` and `f1` is about the same feerate as everything else in your mempool: it seems like they're betting on how much you _overshoot_, not how much you bump.\n\nRight. I was talking in the worst case where they don't have a replacement package with a feerate of `g1`. They are even more incentivized to try that if they do.\nSince `f1` is already expected to be the next block feerate, by how much you bump is technically by how much your overshoot. So much for dismissing your fee\nestimator!\n\n> Slightly related question: in contracts, generally, the timelock deadline is revealed in the script, so the miner knows how \"desperate\" we are right?\n\nFor P2WSH, yes.\n\n> Is that a problem?\n\nI don't think so. As long as we don't bump the feerate by the N factor mentioned above, they have an incentive to try to take the fees while they still can (or someone else will).\n\n> For Revault, if your Cancel transaction is a keypath spend (I think I remember reading that somewhere?) and you don't reveal the script, they don't see your timelock deadline yes?\n\nIt *could* be once we move to Taproot (2weeks). Yep! They would only know about it on a successful spend which would reveal the branch with the timelock. It's\na good point in that the attack above could be made impractical through privacy. Although i don't think it's realistic: due to the necessary script-path spends it would\nbe trivial to cluster coins managed by a Revault setup and deduce whether a given transaction is a Cancel with very high accuracy.\n\n> Again, thanks for the digging and sharing. :)\n\nThanks for the interest and getting me to re-think through this!\n\nBest,\nAntoine\n\n> Best,\n> Gloria\n>\n> On Tue, Nov 30, 2021 at 3:27 PM darosior via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Hi Antoine,\n>>\n>> Thanks for your comment. I believe for Lightning it's simpler with regard to the management of the UTxO pool, but harder with regard to choosing\n>> a threat model.\n>> Responses inline.\n>>\n>>> For any opened channel, ensure the confirmation of a Commitment transaction and the children HTLC-Success/HTLC-Timeout transactions. Note, in the Lightning security game you have to consider (at least) 4 types of players moves and incentives : your node, your channel counterparties, the miners, the crowd of bitcoin users. The number of the last type of players is unknown from your node, however it should not be forgotten you're in competition for block space, therefore their block demands bids should be anticipated and reacted to in consequence. With that remark in mind, implications for your LN fee-bumping strategy will be raised afterwards.\n>>>\n>>> For a LN service provider, on-chain overpayments are bearing on your operational costs, thus downgrading your economic competitiveness. For the average LN user, overpayment might price out outside a LN non-custodial deployment, as you don't have the minimal security budget to be on your own.\n>>\n>> I think this problem statement can be easily generalised to any offchain contract. And your points stand for all of them.\n>> \"For any opened contract, ensure at any point the confirmation of a (set of) transaction(s) in a given number of blocks\"\n>>\n>>> Same issue with Lightning, we can be pinned today on the basis of replace-by-fee rule 3. We can be also blinded by network mempool partitions, a pinning counterparty can segregate all the full-nodes in as many subsets by broadcasting a revoked Commitment transaction different for each. For Revault, I think you can also do unlimited partitions by mutating the ANYONECANPAY-input of the Cancel.\n>>\n>> Well you can already do unlimited partitions by adding different inputs to it. You could malleate the witness, but since we are using Miniscript i'm confident you would only be able in a marginal way.\n>>\n>>> That said, if you have a distributed towers deployment, spread across the p2p network topology, and they can't be clustered together through cross-layers or intra-layer heuristics, you should be able to reliably observe such partitions. I think such distributed monitors are deployed by few L1 merchants accepting 0-conf to detect naive double-spend.\n>>\n>> We should aim to more than 0-conf (in)security level..\n>> It seems to me the only policy-level mitigation for RBF pinning around the \"don't decrease the abolute fees of a less-than-a-block mempool\" would be to drop the requirement on increasing absolute fees if the mempool is \"full enough\" (and the feerate increases exponentially, of course).\n>> Another approach could be by introducing new consensus rules as proposed by Jeremy last year [0]. If we go in the realm of new consensus rules, then i think that simply committing to a maximum tx size would fix pinning by RBF rule 3. Could be in the annex, or in the unused sequence bits (although they currently are by Lightning, meh). You could also check in the output script that the input commits to this.\n>>\n>> [0] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-September/018168.html\n>>\n>>> Have we already discussed a fee-bumping \"shared cache\", a CPFP variation ? Strawman idea: Alice and Bob commit collateral inputs to a separate UTXO from the main \"offchain contract\" one. This UTXO is locked by a multi-sig. For any Commitment transaction pre-signed, also counter-sign a CPFP with top mempool feerate included, spending a Commitment anchor output and the shared-cache UTXO. If the fees spike, you can re-sign a high-feerate CPFP, assuming interactivity. As the CPFP is counter-signed by everyone, the outputs can be CSV-1 encumbered to prevent pinnings. If the share-cache is feeded at parity, there shouldn't be an incentive to waste or maliciously inflate the feerate. I think this solution can be easily generalized to more than 2 counterparties by using a multi-signature scheme. Big issue, if the feerate is short due to fee spikes and you need to re-sign a higher-feerate CPFP, you're trusting your counterparty to interact, though arguably not worse than the current update fee mechanism.\n>>\n>> It really looks just like `update_fee`. Except maybe with the property that you have the channel liquidity not depend on the onchain feerate.\n>> In any case, for Lightning i think it's a bad idea to re-introduce trust on this side post anchor outputs. For Revault it's clearly out of the question to introduce trust in your counterparties (why would you bother having a fee-bumping mechanism in the first place then?). Probably the same holds for all offchain contracts.\n>>\n>>>> For Lightning, it'd mean keeping an equivalent amount of funds as the sum of all your\n>>> channels balances sitting there unallocated \"just in case\". This is not reasonable.\n>>>\n>>> Agree, game-theory wise, you would like to keep a full fee-bumping reserve, ready to burn as much in fees as the contested HTLC value, as it's the maximum gain of your counterparty. Though perfect equilibrium is hard to achieve because your malicious counterparty might have an edge pushing you to broadcast your Commitment first by witholding HTLC resolution.\n>>>\n>>> Fractional fee-bumping reserves are much more realistic to expect in the LN network. Lower fee-bumping reserve, higher liquidity deployed, in theory higher routing fees. By observing historical feerates, average offchain balances at risk and routing fees expected gains, you should be able to discover an equilibrium where higher levels of reserve aren't worth the opportunity cost. I guess this equilibrium could be your LN fee-bumping reserve max feerate.\n>>>\n>>> Note, I think the LN approach is a bit different from what suits a custody protocol like Revault, as you compute a direct return of the frozen fee-bumping liquidity. With Revault, if you have numerous bitcoins protected, it's might be more interesting to adopt a \"buy the mempool, stupid\" strategy than risking fund safety for few percentages of interest returns.\n>>\n>> True for routing nodes. For wallets (if receiving funds), it's not about an investment: just users expectations to being able to transact without risking to lose their funds (ie being able to enforce their contract onchain). Although wallets they are much less at risk.\n>>\n>>> This is where the \"anticipate the crowd of bitcoin users move\" point can be laid out. As the crowd of bitcoin users' fee-bumping reserves are ultimately unknown from your node knowledge, you should be ready to be a bit more conservative than the vanilla fee-bumping strategies shipped by default. In case of massive mempool congestion, your additional conservatism might get your time-sensitive transactions and game on the crowd of bitcoin users. First Problem: if all offchain bitcoin software adopt that strategy we might inflate the worst-case feerate rate at the benefit of the miners, without holistically improving block throughput. Second problem : your class of offchain bitcoin softwares might have ridiculous fee-bumping reserve compared\n>>> to other classes of offchain bitcoin softwares (Revault > Lightning) and just be priced out bydesign in case of mempool congestion. Third problem : as the number of offchain bitcoin applications should go up with time, your fee-bumping reserve levels based from historical data might be always late by one \"bank-run\" scenario.\n>>\n>> Black swan event 2.0? Just rule n\u00b03 is inherent to any kind of fee estimation.\n>>\n>>> For Lightning, if you're short in fee-bumping reserves you might still do preemptive channel closures, either cooperatively or unilaterally and get back the off-chain liquidity to protect the more economically interesting channels. Though again, that kind of automatic behavior might be compelling at the individual node-level, but make the mempol congestion worse holistically.\n>>\n>> Yeah so we are back to the \"fractional reserve\" model: you can only enforce X% of the offchain contracts your participate in.. Actually it's even an added assumption: that you still have operating contracts, with honest counterparties.\n>>\n>>> In case of massive mempool congestion, you might try to front-run the crowd of bitcoin users relying on block connections for fee-bumping, and thus start your fee-bumping as soon as you observe feerate groups fluctuations in your local mempool(s).\n>>\n>> I don't think any kind of mempool-based estimate generalizes well, since at any point the expected time before the next block is 10 minutes (and a lot can happen in 10min).\n>>\n>>> Also you might proceed your fee-bumping ticks on a local clock instead of block connections in case of time-dilation or deeper eclipse attacks of your local node. Your view of the chain might be compromised but not your ability to broadcast transactions thanks to emergency channels (in the non-LN sense...though in fact quid of txn wrapped in onions ?) of communication.\n>>\n>> Oh, yeah, i didn't explicit \"not getting eclipsed\" (or more generally \"data availability\") as an assumption since it's generally one made by participants of any offchain contract. In this case you can't even have decent fee estimation, so you are screwed anyways.\n>>\n>>> Yes, stay open the question on how you enforce this block insurance market. Reputation, which might be to avoid due to the latent centralization effect, might be hard to stack and audit reliably for an emergency mechanism running, hopefully, once in a halvening period. Maybe maybe some cryptographic or economically based mechanism on slashing or swaps could be found...\n>>\n>> Unfortunately, given current mining centralisation, pools are in a very good position to offer pretty decent SLAs around that. With a block space insurance, you of course don't need all these convoluted fee-bumping hacks.\n>> I'm very concerned that large stakeholders of the \"offchain contracts ecosystem\" would just go this (easier) way and further increase mining centralisation pressure.\n>>\n>> I agree that a cryptography-based scheme around this type of insurance services would be the best way out.\n>>\n>>> Antoine\n>>>\n>>> Le lun. 29 nov. 2021 \u00e0 09:34, darosior via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>>\n>>>> Hi everyone,\n>>>>\n>>>> Fee-bumping is paramount to the security of many protocols building on Bitcoin, as they require the\n>>>> confirmation of a transaction (which might be presigned) before the expiration of a timelock at any\n>>>> point after the establishment of the contract.\n>>>>\n>>>> The part of Revault using presigned transactions (the delegation from a large to a smaller multisig)\n>>>> is no exception. We have been working on how to approach this for a while now and i'd like to share\n>>>> what we have in order to open a discussion on this problem so central to what seem to be The Right\n>>>> Way [0] to build on Bitcoin but which has yet to be discussed in details (at least publicly).\n>>>>\n>>>> I'll discuss what we came up with for Revault (at least for what will be its first iteration) but my\n>>>> intent with posting to the mailing list is more to frame the questions to this problem we are all\n>>>> going to face rather than present the results of our study tailored to the Revault usecase.\n>>>> The discussion is still pretty Revault-centric (as it's the case study) but hopefully this can help\n>>>> future protocol designers and/or start a discussion around what everyone's doing for existing ones.\n>>>>\n>>>> ## 1. Reminder about Revault\n>>>>\n>>>> The part of Revault we are interested in for this study is the delegation process, and more\n>>>> specifically the application of spending policies by network monitors (watchtowers).\n>>>> Coins are received on a large multisig. Participants of this large multisig create 2 [1]\n>>>> transactions. The Unvault, spending a deposit UTxO, creates an output paying either to the small\n>>>> multisig after a timelock or to the large multisig immediately. The Cancel, spending the Unvault\n>>>> output through the non-timelocked path, creates a new deposit UTxO.\n>>>> Participants regularly exchange the Cancel transaction signatures for each deposit, sharing the\n>>>> signatures with the watchtowers they operate. They then optionally [2] sign the Unvault transaction\n>>>> and share the signatures with the small multisig participants who can in turn use them to proceed\n>>>> with a spending. Watchtowers can enforce spending policies (say, can't Unvault outside of business\n>>>> hours) by having the Cancel transaction be confirmed before the expiration of the timelock.\n>>>>\n>>>> ## 2. Problem statement\n>>>>\n>>>> For any delegated vault, ensure the confirmation of a Cancel transaction in a configured number of\n>>>> blocks at any point. In so doing, minimize the overpayments and the UTxO set footprint. Overpayments\n>>>> increase the burden on the watchtower operator by increasing the required frequency of refills of the\n>>>> fee-bumping wallet, which is already the worst user experience. You are likely to manage a number of\n>>>> UTxOs with your number of vaults, which comes at a cost for you as well as everyone running a full\n>>>> node.\n>>>>\n>>>> Note that this assumes miners are economically rationale, are incentivized by *public* fees and that\n>>>> you have a way to propagate your fee-bumped transaction to them. We also don't consider the block\n>>>> space bounds.\n>>>>\n>>>> In the previous paragraph and the following text, \"vault\" can generally be replaced with \"offchain\n>>>> contract\".\n>>>>\n>>>> ## 3. With presigned transactions\n>>>>\n>>>> As you all know, the first difficulty is to get to be able to unilaterally enforce your contract\n>>>> onchain. That is, any participant must be able to unilaterally bump the fees of a transaction even\n>>>> if it was co-signed by other participants.\n>>>>\n>>>> For Revault we can afford to introduce malleability in the Cancel transaction since there is no\n>>>> second-stage transaction depending on its txid. Therefore it is pre-signed with ANYONECANPAY. We\n>>>> can't use ANYONECANPAY|SINGLE since it would open a pinning vector [3]. Note how we can't leverage\n>>>> the carve out rule, and neither can any other more-than-two-parties contract.\n>>>> This has a significant implication for the rest, as we are entirely burning fee-bumping UTxOs.\n>>>>\n>>>> This opens up a pinning vector, or at least a significant nuisance: any other party can largely\n>>>> increase the absolute fee without increasing the feerate, leveraging the RBF rules to prevent you\n>>>> from replacing it without paying an insane fee. And you might not see it in your own mempool and\n>>>> could only suppose it's happening by receiving non-full blocks or with transactions paying a lower\n>>>> feerate.\n>>>> Unfortunately i know of no other primitive that can be used by multi-party (i mean, >2) presigned\n>>>> transactions protocols for fee-bumping that aren't (more) vulnerable to pinning.\n>>>>\n>>>> ## 4. We are still betting on future feerate\n>>>>\n>>>> The problem is still missing one more constraint. \"Ensuring confirmation at any time\" involves ensuring\n>>>> confirmation at *any* feerate, which you *cannot* do. So what's the limit? In theory you should be ready\n>>>> to burn as much in fees as the value of the funds you want to get out of the contract. So... For us\n>>>> it'd mean keeping for each vault an equivalent amount of funds sitting there on the watchtower's hot\n>>>> wallet. For Lightning, it'd mean keeping an equivalent amount of funds as the sum of all your\n>>>> channels balances sitting there unallocated \"just in case\". This is not reasonable.\n>>>>\n>>>> So you need to keep a maximum feerate, above which you won't be able to ensure the enforcement of\n>>>> all your contracts onchain at the same time. We call that the \"reserve feerate\" and you can have\n>>>> different strategies for choosing it, for instance:\n>>>> - The 85th percentile over the last year of transactions feerates\n>>>> - The maximum historical feerate\n>>>> - The maximum historical feerate adjusted in dollars (makes more sense but introduces a (set of?)\n>>>> trusted oracle(s) in a security-critical component)\n>>>> - Picking a random high feerate (why not? It's an arbitrary assumption anyways)\n>>>>\n>>>> Therefore, even if we don't have to bet on the broadcast-time feerate market at signing time anymore\n>>>> (since we can unilaterally bump), we still need some kind of prediction in preparation of making\n>>>> funds available to bump the fees at broadcast time.\n>>>> Apart from judging that 500sat/vb is probably more reasonable than 10sat/vbyte, this unfortunately\n>>>> sounds pretty much crystal-ball-driven.\n>>>>\n>>>> We currently use the maximum of the 95th percentiles over 90-days windows over historical block chain\n>>>> feerates. [4]\n>>>>\n>>>> ## 5. How much funds does my watchtower need?\n>>>>\n>>>> That's what we call the \"reserve\". Depending on your reserve feerate strategy it might vary over\n>>>> time. This is easier to reason about with a per-contract reserve. For Revault it's pretty\n>>>> straightforward since the Cancel transaction size is static: `reserve_feerate * cancel_size`. For\n>>>> other protocols with dynamic transaction sizes (or even packages of transactions) it's less so. For\n>>>> your Lightning channel you would probably take the maximum size of your commitment transaction\n>>>> according to your HTLC exposure settings + the size of as many `htlc_success` transaction?\n>>>>\n>>>> Then you either have your software or your user guesstimate how many offchain contracts the\n>>>> watchtower will have to watch, time that by the per-contract reserve and refill this amount (plus\n>>>> some slack in practice). Once again, a UX tradeoff (not even mentioning the guesstimation UX):\n>>>> overestimating leads to too many unallocated funds sitting on a hot wallet, underestimating means\n>>>> (at best) inability to participate in new contracts or being \"at risk\" (not being able to enforce\n>>>> all your contracts onchain at your reserve feerate) before a new refill.\n>>>>\n>>>> For vaults you likely have large-value UTxOs and small transactions (the Cancel is one-in one-out in\n>>>> Revault). For some other applications with large transactions and lower-value UTxOs on average it's\n>>>> likely that only part of the offchain contracts might be enforceable at a reasonable feerate. Is it\n>>>> reasonable?\n>>>>\n>>>> ## 6. UTxO pool layout\n>>>>\n>>>> Now that you somehow managed to settle on a refill amount, how are you going to use these funds?\n>>>> Also, you'll need to manage your pool across time (consolidating small coins, and probably fanning\n>>>> out large ones).\n>>>>\n>>>> You could keep a single large UTxO and peel it as you need to sponsor transactions. But this means\n>>>> that you need to create a coin of a specific value according to your need at the current feerate\n>>>> estimation, hope to have it confirmed in a few blocks (at least for now! [5]), and hope that the\n>>>> value won't be obsolete by the time it confirmed. Also, you'd have to do that for any number of\n>>>> Cancel, chaining feebump coin creation transactions off the change of the previous ones or replacing\n>>>> them with more outputs. Both seem to become really un-manageable (and expensive) in many edge-cases,\n>>>> shortening the time you have to confirm the actual Cancel transaction and creating uncertainty about\n>>>> the reserve (how much is my just-in-time fanout going to cost me in fees that i need to refill in\n>>>> advance on my watchtower wallet?).\n>>>> This is less of a concern for protocols using CPFP to sponsor transactions, but they rely on a\n>>>> policy rule specific to 2-parties contracts.\n>>>>\n>>>> Therefore for Revault we fan-out the coins per-vault in advance. We do so at refill time so the\n>>>> refiller can give an excess to pay for the fees of the fanout transaction (which is reasonable since\n>>>> it will occur just after the refilling transaction confirms). When the watchtower is asked to watch\n>>>> for a new delegated vault it will allocate coins from the pool of fanned-out UTxOs to it (failing\n>>>> that, it would refuse the delegation).\n>>>> What is a good distribution of UTxOs amounts per vault? We want to minimize the number of coins,\n>>>> still have coins small enough to not overpay (remember, we can't have change) and be able to bump a\n>>>> Cancel up to the reserve feerate using these coins. The two latter constraints are directly in\n>>>> contradiction as the minimal value of a coin usable at the reserve feerate (paying for its own input\n>>>> fee + bumping the feerate by, say, 5sat/vb) is already pretty high. Therefore we decided to go with\n>>>> two distributions per vault. The \"reserve distribution\" alone ensures that we can bump up to the\n>>>> reserve feerate and is usable for high feerates. The \"bonus distribution\" is not, but contains\n>>>> smaller coins useful to prevent overpayments during low and medium fee periods (which is most of the\n>>>> time).\n>>>> Both distributions are based on a basic geometric suite [6]. Each value is half the previous one.\n>>>> This exponentially decreases the value, limiting the number of coins. But this also allows for\n>>>> pretty small coins to exist and each coin's value is equal to the sum of the smaller coins,\n>>>> or smaller by at most the value of the smallest coin. Therefore bounding the maximum overpayment to\n>>>> the smallest coin's value [7].\n>>>>\n>>>> For the management of the UTxO pool across time we merged the consolidation with the fanout. When\n>>>> fanning out a refilled UTxO, we scan the pool for coins that need to be consolidated according to a\n>>>> heuristic. An instance of a heuristic is \"the coin isn't allocated and would not have been able to\n>>>> increase the fee at the median feerate over the past 90 days of blocks\".\n>>>> We had this assumption that feerate would tend to go up with time and therefore discarded having to\n>>>> split some UTxOs from the pool. We however overlooked that a large increase in the exchange price of\n>>>> BTC as we've seen during the past year could invalidate this assumption and that should arguably be\n>>>> reconsidered.\n>>>>\n>>>> ## 7. Bumping and re-bumping\n>>>>\n>>>> First of all, when to fee-bump? At fixed time intervals? At each block connection? It sounds like,\n>>>> given a large enough timelock, you could try to greed by \"trying your luck\" at a lower feerate and\n>>>> only re-bumping every N blocks. You would then start aggressively bumping at every block after M\n>>>> blocks have passed. But that's actually a bet (in disguised?) that the next block feerate in M blocks\n>>>> will be lower than the current one. In the absence of any predictive model it is more reasonable to\n>>>> just start being aggressive immediately.\n>>>> You probably want to base your estimates on `estimatesmartfee` and as a consequence you would re-bump\n>>>> (if needed )after each block connection, when your estimates get updated and you notice your\n>>>> transaction was not included in the block.\n>>>>\n>>>> In the event that you notice a consequent portion of the block is filled with transactions paying\n>>>> less than your own, you might want to start panicking and bump your transaction fees by a certain\n>>>> percentage with no consideration for your fee estimator. You might skew miners incentives in doing\n>>>> so: if you increase the fees by a factor of N, any miner with a fraction larger than 1/N of the\n>>>> network hashrate now has an incentive to censor your transaction at first to get you to panic. Also\n>>>> note this can happen if you want to pay the absolute fees for the 'pinning' attack mentioned in\n>>>> section #2, and that might actually incentivize miners to perform it themselves..\n>>>>\n>>>> The gist is that the most effective way to bump and rebump (RBF the Cancel tx) seems to just be to\n>>>> consider the `estimatesmartfee 2 CONSERVATIVE` feerate at every block your tx isn't included in, and\n>>>> to RBF it if the feerate is higher.\n>>>> In addition, we fallback to a block chain based estimation when estimates aren't available (eg if\n>>>> the user stopped their WT for say a hour and we come back up): we use the 85th percentile over the\n>>>> feerates in the last 6 blocks. Sure, miners can try to have an influence on that by stuffing their\n>>>> blocks with large fee self-paying transactions, but they would need to:\n>>>> 1. Be sure to catch a significant portion of the 6 blocks (at least 2, actually)\n>>>> 2. Give up on 25% of the highest fee-paying transactions (assuming they got the 6 blocks, it's\n>>>> proportionally larger and incertain as they get less of them)\n>>>> 3. Hope that our estimator will fail and we need to fall back to the chain-based estimation\n>>>>\n>>>> ## 8. Our study\n>>>>\n>>>> We essentially replayed the historical data with different deployment configurations (number of\n>>>> participants and timelock) and probability of an event occurring (event being say an Unvault, an\n>>>> invalid Unvault, a new delegation, ..). We then observed different metrics such as the time at risk\n>>>> (when we can't enforce all our contracts at the reserve feerate at the same time), or the\n>>>> operational cost.\n>>>> We got the historical fee estimates data from Statoshi [9], Txstats [10] and the historical chain\n>>>> data from Riccardo Casatta's `blocks_iterator` [11]. Thanks!\n>>>>\n>>>> The (research-quality..) code can be found at https://github.com/revault/research under the section\n>>>> \"Fee bumping\". Again it's very Revault specific, but at least the data can probably be reused for\n>>>> studying other protocols.\n>>>>\n>>>> ## 9. Insurances\n>>>>\n>>>> Of course, given it's all hacks and workarounds and there is no good answer to \"what is a reasonable\n>>>> feerate up to which we need to make contracts enforceable onchain?\", there is definitely room for an\n>>>> insurance market. But this enters the realm of opinions. Although i do have some (having discussed\n>>>> this topic for the past years with different people), i would like to keep this post focused on the\n>>>> technical aspects of this problem.\n>>>>\n>>>> [0] As far as i can tell, having offchain contracts be enforceable onchain by confirming a\n>>>> transaction before the expiration of a timelock is a widely agreed-upon approach. And i don't think\n>>>> we can opt for any other fundamentally different one, as you want to know you can claim back your\n>>>> coins from a contract after a deadline before taking part in it.\n>>>>\n>>>> [1] The Real Revault (tm) involves more transactions, but for the sake of conciseness i only\n>>>> detailed a minimum instance of the problem.\n>>>>\n>>>> [2] Only presigning part of the Unvault transactions allows to only delegate part of the coins,\n>>>> which can be abstracted as \"delegate x% of your stash\" in the user interface.\n>>>>\n>>>> [3] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-May/017835.html\n>>>>\n>>>> [4] https://github.com/revault/research/blob/1df953813708287c32a15e771ba74957ec44f354/feebumping/model/statemachine.py#L323-L329\n>>>>\n>>>> [5] https://github.com/bitcoin/bitcoin/pull/23121\n>>>>\n>>>> [6] https://github.com/revault/research/blob/1df953813708287c32a15e771ba74957ec44f354/feebumping/model/statemachine.py#L494-L507\n>>>>\n>>>> [7] Of course this assumes a combinatorial coin selection, but i believe it's ok given we limit the\n>>>> number of coins beforehand.\n>>>>\n>>>> [8] Although there is the argument to outbid a censorship, anyone censoring you isn't necessarily a\n>>>> miner.\n>>>>\n>>>> [9] https://www.statoshi.info/\n>>>>\n>>>> [10] https://www.statoshi.info/\n>>>>\n>>>> [11] https://github.com/RCasatta/blocks_iterator\n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211208/06c111d0/attachment-0001.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2021-12-08T23:56:39",
                "message_text_only": "Hi Antoine,\n\n> It seems to me the only policy-level mitigation for RBF pinning around\nthe \"don't decrease the abolute fees of a less-than-a-block mempool\" would\nbe to drop the requirement on increasing absolute fees if the mempool is\n\"full enough\" (and the feerate increases exponentially, of course).\n\nYes, it's hard to say the \"less-than-a-block-mempool\" scenario is long-term\nrealistic. In the future, you can expect liquidity operations to be\ntriggered as soon as the network mempools start to be empty.  At a given\nblock space price, there is always room to improve your routing topology.\n\nThat said, you would like the default block construction strategy to be\n\"all-weather\" economically aligned. To build such a more robust strategy, I\nthink a miner would have interest to level the  \"full enough\" bar.\n\nI still think a policy-level mitigation is possible, where you have a\nreplace-by-fee rate above X MB of blocks and replace-by-fee under X.\nResponsibility is on the L2 fee-bumper to guarantee the  honest bid is in\nthe X MB of blocks or the malicious pinning attacker has to overbid.\n\nAt first sight, yes committing the maximum tx size in the annex covered by\nyour counterparty signature should still allow you to add high-feerate\ninput. Though niice if we can save a consensus rule to fix pinnings.\n\n> In any case, for Lightning i think it's a bad idea to re-introduce trust\non this side post anchor outputs. For Revault it's clearly out of the\nquestion to introduce trust in your counterparties (why would you bother\nhaving a fee-bumping mechanism in the >first place then?). Probably the\nsame holds for all offchain contracts.\n\nYeah it was a strawman exercise on the question \"not knowledge of other\nprimitive that can be used by multi-party\" :) I wouldn't recommend that\nkind of fee-bumping \"shared cache\" scheme for a  trust-minimized setup.\nMaybe interesting for watchtowers/LSP topologies.\n\n> Black swan event 2.0? Just rule n\u00b03 is inherent to any kind of fee\nestimation.\n\nIt's just the old good massive mempool congestion systemic risk known since\nthe LN whitepaper. AFAIK, anchor output fee-bumping schemes have not really\nstarted the work to be robust against that. What I'm aiming to point out is\nthat it might be even harder to build a fault-tolerant fee-bumping strategy\nbecause of the \"limited rationality\" of your local node towards the\nbehaviors of the other bitcoin users in face of this phenomena. Would be\nnice to have more research on that front.\n\n> I don't think any kind of mempool-based estimate generalizes well, since\nat any point the expected time before the next block is 10 minutes (and a\nlot can happen in 10min).\n\nSure, you might be off-bid because of block variance, though if you're\nready to pay multiple RBF penalties which are linear, you might adjust your\nshots in function of \"real-time\" mempool congestion.\n\n> I'm very concerned that large stakeholders of the \"offchain contracts\necosystem\" would just go this (easier) way and further increase mining\ncentralisation pressure.\n\n*back on the whiteboard sweating on a consensus-enforced timestop primitive*\n\nCheers,\nAntoine\n\nLe mar. 30 nov. 2021 \u00e0 10:19, darosior <darosior at protonmail.com> a \u00e9crit :\n\n> Hi Antoine,\n>\n> Thanks for your comment. I believe for Lightning it's simpler with regard\n> to the management of the UTxO pool, but harder with regard to choosing\n> a threat model.\n> Responses inline.\n>\n>\n> For any opened channel, ensure the confirmation of a Commitment\n> transaction and the children HTLC-Success/HTLC-Timeout transactions. Note,\n> in the Lightning security game you have to consider (at least) 4 types of\n> players moves and incentives : your node, your channel counterparties, the\n> miners, the crowd of bitcoin users. The number of the last type of players\n> is unknown from your node, however it should not be forgotten you're in\n> competition for block space, therefore their block demands bids should be\n> anticipated and reacted to in consequence. With that remark in mind,\n> implications for your LN fee-bumping strategy will be raised afterwards.\n>\n> For a LN service provider, on-chain overpayments are bearing on your\n> operational costs, thus downgrading your economic competitiveness. For the\n> average LN user, overpayment might price out outside a LN non-custodial\n> deployment, as you don't have the minimal security budget to be on your own.\n>\n>\n> I think this problem statement can be easily generalised to any offchain\n> contract. And your points stand for all of them.\n> \"For any opened contract, ensure at any point the confirmation of a (set\n> of) transaction(s) in a given number of blocks\"\n>\n>\n> Same issue with Lightning, we can be pinned today on the basis of\n> replace-by-fee rule 3. We can be also blinded by network mempool\n> partitions, a pinning counterparty can segregate all the full-nodes  in as\n> many subsets by broadcasting a revoked Commitment transaction different for\n> each. For Revault, I think you can also do unlimited partitions by mutating\n> the ANYONECANPAY-input of the Cancel.\n>\n>\n> Well you can already do unlimited partitions by adding different inputs to\n> it. You could malleate the witness, but since we are using Miniscript i'm\n> confident you would only be able in a marginal way.\n>\n>\n> That said, if you have a distributed towers deployment, spread across the\n> p2p network topology, and they can't be clustered together through\n> cross-layers or intra-layer heuristics, you should be able to reliably\n> observe such partitions. I think such distributed monitors are deployed by\n> few L1 merchants accepting 0-conf to detect naive double-spend.\n>\n>\n> We should aim to more than 0-conf (in)security level..\n> It seems to me the only policy-level mitigation for RBF pinning around the\n> \"don't decrease the abolute fees of a less-than-a-block mempool\" would be\n> to drop the requirement on increasing absolute fees if the mempool is \"full\n> enough\" (and the feerate increases exponentially, of course).\n> Another approach could be by introducing new consensus rules as proposed\n> by Jeremy last year [0]. If we go in the realm of new consensus rules, then\n> i think that simply committing to a maximum tx size would fix pinning by\n> RBF rule 3. Could be in the annex, or in the unused sequence bits (although\n> they currently are by Lightning, meh). You could also check in the output\n> script that the input commits to this.\n>\n> [0]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-September/018168.html\n>\n>\n> Have we already discussed a fee-bumping \"shared cache\", a CPFP variation ?\n> Strawman idea: Alice and Bob commit collateral inputs to a separate UTXO\n> from the main \"offchain contract\" one. This UTXO is locked by a multi-sig.\n> For any Commitment transaction pre-signed, also counter-sign a CPFP with\n> top mempool feerate included, spending a Commitment anchor output and the\n> shared-cache UTXO. If the fees spike,  you can re-sign a high-feerate CPFP,\n> assuming interactivity. As the CPFP is counter-signed by everyone, the\n> outputs can be CSV-1 encumbered to prevent pinnings. If the share-cache is\n> feeded at parity, there shouldn't be an incentive to waste or maliciously\n> inflate the feerate. I think this solution can be easily generalized to\n> more than 2 counterparties by using a multi-signature scheme. Big issue, if\n> the feerate is short due to fee spikes and you need to re-sign a\n> higher-feerate CPFP, you're trusting your counterparty to interact, though\n> arguably not worse than the current update fee mechanism.\n>\n>\n> It really looks just like `update_fee`. Except maybe with the property\n> that you have the channel liquidity not depend on the onchain feerate.\n> In any case, for Lightning i think it's a bad idea to re-introduce trust\n> on this side post anchor outputs. For Revault it's clearly out of the\n> question to introduce trust in your counterparties (why would you bother\n> having a fee-bumping mechanism in the first place then?). Probably the same\n> holds for all offchain contracts.\n>\n>\n> > For Lightning, it'd mean keeping an equivalent amount of funds as the\n> sum of all your\n> channels balances sitting there unallocated \"just in case\". This is not\n> reasonable.\n>\n> Agree, game-theory wise, you would like to keep a full fee-bumping\n> reserve, ready to burn as much in fees as the contested HTLC value, as it's\n> the maximum gain of your counterparty. Though perfect equilibrium is hard\n> to achieve because your malicious counterparty might have an edge pushing\n> you to broadcast your Commitment first by witholding HTLC resolution.\n>\n> Fractional fee-bumping reserves are much more realistic to expect in the\n> LN network. Lower fee-bumping reserve, higher liquidity deployed, in theory\n> higher routing fees. By observing historical feerates, average offchain\n> balances at risk and routing fees expected gains, you should be able to\n> discover an equilibrium where higher levels of reserve aren't worth the\n> opportunity cost. I guess this  equilibrium could be your LN fee-bumping\n> reserve max feerate.\n>\n> Note, I think the LN approach is a bit different from what suits a custody\n> protocol like Revault,  as you compute a direct return of the frozen\n> fee-bumping liquidity. With Revault, if you have numerous bitcoins\n> protected, it's might be more interesting to adopt a \"buy the mempool,\n> stupid\" strategy than risking fund safety for few percentages of interest\n> returns.\n>\n>\n> True for routing nodes. For wallets (if receiving funds), it's not about\n> an investment: just users expectations to being able to transact without\n> risking to lose their funds (ie being able to enforce their contract\n> onchain). Although wallets they are much less at risk.\n>\n>\n> This is where the \"anticipate the crowd of bitcoin users move\" point can\n> be laid out. As the crowd of bitcoin users' fee-bumping reserves are\n> ultimately unknown from your node knowledge, you should be ready to be a\n> bit more conservative than the vanilla fee-bumping strategies shipped by\n> default. In case of massive mempool congestion, your additional\n> conservatism might get your time-sensitive transactions and game on the\n> crowd of bitcoin users. First Problem: if all offchain bitcoin software\n> adopt that strategy we might inflate the worst-case feerate rate at the\n> benefit of the miners, without holistically improving block throughput.\n> Second problem : your class of offchain bitcoin softwares might have\n> ridiculous fee-bumping reserve compared\n> to other classes of offchain bitcoin softwares (Revault > Lightning) and\n> just be priced out bydesign in case of mempool congestion. Third problem :\n> as the number of offchain bitcoin applications should go up with time, your\n> fee-bumping reserve levels based from historical data might be always late\n> by one \"bank-run\" scenario.\n>\n>\n> Black swan event 2.0? Just rule n\u00b03 is inherent to any kind of fee\n> estimation.\n>\n> For Lightning, if you're short in fee-bumping reserves you might still do\n> preemptive channel closures, either cooperatively or unilaterally and get\n> back the off-chain liquidity to protect the more economically interesting\n> channels. Though again, that kind of automatic behavior might be compelling\n> at the individual node-level, but make the mempol congestion worse\n> holistically.\n>\n>\n> Yeah so we are back to the \"fractional reserve\" model: you can only\n> enforce X% of the offchain contracts your participate in.. Actually it's\n> even an added assumption: that you still have operating contracts, with\n> honest counterparties.\n>\n>\n> In case of massive mempool congestion, you might try to front-run the\n> crowd of bitcoin users relying on block connections for fee-bumping, and\n> thus start your fee-bumping as soon as you observe feerate groups\n> fluctuations in your local mempool(s).\n>\n>\n> I don't think any kind of mempool-based estimate generalizes well, since\n> at any point the expected time before the next block is 10 minutes (and a\n> lot can happen in 10min).\n>\n> Also you might proceed your fee-bumping ticks on a local clock instead of\n> block connections in case of time-dilation or deeper eclipse attacks of\n> your local node. Your view of the chain might be compromised but not your\n> ability to broadcast transactions thanks to emergency channels (in the\n> non-LN sense...though in fact quid of txn wrapped in onions ?) of\n> communication.\n>\n>\n> Oh, yeah, i didn't explicit \"not getting eclipsed\" (or more generally\n> \"data availability\") as an assumption since it's generally one made by\n> participants of any offchain contract. In this case you can't even have\n> decent fee estimation, so you are screwed anyways.\n>\n>\n> Yes, stay open the question on how you enforce this block insurance\n> market. Reputation, which might be to avoid due to the latent\n> centralization effect, might be hard to stack and audit reliably for an\n> emergency mechanism running, hopefully, once in a halvening period. Maybe\n> maybe some cryptographic or economically based mechanism on slashing or\n> swaps could be found...\n>\n>\n> Unfortunately, given current mining centralisation, pools are in a very\n> good position to offer pretty decent SLAs around that. With a block space\n> insurance, you of course don't need all these convoluted fee-bumping hacks.\n> I'm very concerned that large stakeholders of the \"offchain contracts\n> ecosystem\" would just go this (easier) way and further increase mining\n> centralisation pressure.\n>\n> I agree that a cryptography-based scheme around this type of insurance\n> services would be the best way out.\n>\n>\n> Antoine\n>\n> Le lun. 29 nov. 2021 \u00e0 09:34, darosior via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>\n>> Hi everyone,\n>>\n>> Fee-bumping is paramount to the security of many protocols building on\n>> Bitcoin, as they require the\n>> confirmation of a transaction (which might be presigned) before the\n>> expiration of a timelock at any\n>> point after the establishment of the contract.\n>>\n>> The part of Revault using presigned transactions (the delegation from a\n>> large to a smaller multisig)\n>> is no exception. We have been working on how to approach this for a while\n>> now and i'd like to share\n>> what we have in order to open a discussion on this problem so central to\n>> what seem to be The Right\n>> Way [0] to build on Bitcoin but which has yet to be discussed in details\n>> (at least publicly).\n>>\n>> I'll discuss what we came up with for Revault (at least for what will be\n>> its first iteration) but my\n>> intent with posting to the mailing list is more to frame the questions to\n>> this problem we are all\n>> going to face rather than present the results of our study tailored to\n>> the Revault usecase.\n>> The discussion is still pretty Revault-centric (as it's the case study)\n>> but hopefully this can help\n>> future protocol designers and/or start a discussion around what\n>> everyone's doing for existing ones.\n>>\n>>\n>> ## 1. Reminder about Revault\n>>\n>> The part of Revault we are interested in for this study is the delegation\n>> process, and more\n>> specifically the application of spending policies by network monitors\n>> (watchtowers).\n>> Coins are received on a large multisig. Participants of this large\n>> multisig create 2 [1]\n>> transactions. The Unvault, spending a deposit UTxO, creates an output\n>> paying either to the small\n>> multisig after a timelock or to the large multisig immediately. The\n>> Cancel, spending the Unvault\n>> output through the non-timelocked path, creates a new deposit UTxO.\n>> Participants regularly exchange the Cancel transaction signatures for\n>> each deposit, sharing the\n>> signatures with the watchtowers they operate. They then optionally [2]\n>> sign the Unvault transaction\n>> and share the signatures with the small multisig participants who can in\n>> turn use them to proceed\n>> with a spending. Watchtowers can enforce spending policies (say, can't\n>> Unvault outside of business\n>> hours) by having the Cancel transaction be confirmed before the\n>> expiration of the timelock.\n>>\n>>\n>> ## 2. Problem statement\n>>\n>> For any delegated vault, ensure the confirmation of a Cancel transaction\n>> in a configured number of\n>> blocks at any point. In so doing, minimize the overpayments and the UTxO\n>> set footprint. Overpayments\n>> increase the burden on the watchtower operator by increasing the required\n>> frequency of refills of the\n>> fee-bumping wallet, which is already the worst user experience. You are\n>> likely to manage a number of\n>> UTxOs with your number of vaults, which comes at a cost for you as well\n>> as everyone running a full\n>> node.\n>>\n>> Note that this assumes miners are economically rationale, are\n>> incentivized by *public* fees and that\n>> you have a way to propagate your fee-bumped transaction to them. We also\n>> don't consider the block\n>> space bounds.\n>>\n>> In the previous paragraph and the following text, \"vault\" can generally\n>> be replaced with \"offchain\n>> contract\".\n>>\n>>\n>> ## 3. With presigned transactions\n>>\n>> As you all know, the first difficulty is to get to be able to\n>> unilaterally enforce your contract\n>> onchain. That is, any participant must be able to unilaterally bump the\n>> fees of a transaction even\n>> if it was co-signed by other participants.\n>>\n>> For Revault we can afford to introduce malleability in the Cancel\n>> transaction since there is no\n>> second-stage transaction depending on its txid. Therefore it is\n>> pre-signed with ANYONECANPAY. We\n>> can't use ANYONECANPAY|SINGLE since it would open a pinning vector [3].\n>> Note how we can't leverage\n>> the carve out rule, and neither can any other more-than-two-parties\n>> contract.\n>> This has a significant implication for the rest, as we are entirely\n>> burning fee-bumping UTxOs.\n>>\n>> This opens up a pinning vector, or at least a significant nuisance: any\n>> other party can largely\n>> increase the absolute fee without increasing the feerate, leveraging the\n>> RBF rules to prevent you\n>> from replacing it without paying an insane fee. And you might not see it\n>> in your own mempool and\n>> could only suppose it's happening by receiving non-full blocks or with\n>> transactions paying a lower\n>> feerate.\n>> Unfortunately i know of no other primitive that can be used by\n>> multi-party (i mean, >2) presigned\n>> transactions protocols for fee-bumping that aren't (more) vulnerable to\n>> pinning.\n>>\n>>\n>> ## 4. We are still betting on future feerate\n>>\n>> The problem is still missing one more constraint. \"Ensuring confirmation\n>> at any time\" involves ensuring\n>> confirmation at *any* feerate, which you *cannot* do. So what's the\n>> limit? In theory you should be ready\n>> to burn as much in fees as the value of the funds you want to get out of\n>> the contract. So... For us\n>> it'd mean keeping for each vault an equivalent amount of funds sitting\n>> there on the watchtower's hot\n>> wallet. For Lightning, it'd mean keeping an equivalent amount of funds as\n>> the sum of all your\n>> channels balances sitting there unallocated \"just in case\". This is not\n>> reasonable.\n>>\n>> So you need to keep a maximum feerate, above which you won't be able to\n>> ensure the enforcement of\n>> all your contracts onchain at the same time. We call that the \"reserve\n>> feerate\" and you can have\n>> different strategies for choosing it, for instance:\n>> - The 85th percentile over the last year of transactions feerates\n>> - The maximum historical feerate\n>> - The maximum historical feerate adjusted in dollars (makes more sense\n>> but introduces a (set of?)\n>>   trusted oracle(s) in a security-critical component)\n>> - Picking a random high feerate (why not? It's an arbitrary assumption\n>> anyways)\n>>\n>> Therefore, even if we don't have to bet on the broadcast-time feerate\n>> market at signing time anymore\n>> (since we can unilaterally bump), we still need some kind of prediction\n>> in preparation of making\n>> funds available to bump the fees at broadcast time.\n>> Apart from judging that 500sat/vb is probably more reasonable than\n>> 10sat/vbyte, this unfortunately\n>> sounds pretty much crystal-ball-driven.\n>>\n>> We currently use the maximum of the 95th percentiles over 90-days windows\n>> over historical block chain\n>> feerates. [4]\n>>\n>>\n>> ## 5. How much funds does my watchtower need?\n>>\n>> That's what we call the \"reserve\". Depending on your reserve feerate\n>> strategy it might vary over\n>> time. This is easier to reason about with a per-contract reserve. For\n>> Revault it's pretty\n>> straightforward since the Cancel transaction size is static:\n>> `reserve_feerate * cancel_size`. For\n>> other protocols with dynamic transaction sizes (or even packages of\n>> transactions) it's less so. For\n>> your Lightning channel you would probably take the maximum size of your\n>> commitment transaction\n>> according to your HTLC exposure settings + the size of as many\n>> `htlc_success` transaction?\n>>\n>> Then you either have your software or your user guesstimate how many\n>> offchain contracts the\n>> watchtower will have to watch, time that by the per-contract reserve and\n>> refill this amount (plus\n>> some slack in practice). Once again, a UX tradeoff (not even mentioning\n>> the guesstimation UX):\n>> overestimating leads to too many unallocated funds sitting on a hot\n>> wallet, underestimating means\n>> (at best) inability to participate in new contracts or being \"at risk\"\n>> (not being able to enforce\n>> all your contracts onchain at your reserve feerate) before a new refill.\n>>\n>> For vaults you likely have large-value UTxOs and small transactions (the\n>> Cancel is one-in one-out in\n>> Revault). For some other applications with large transactions and\n>> lower-value UTxOs on average it's\n>> likely that only part of the offchain contracts might be enforceable at a\n>> reasonable feerate. Is it\n>> reasonable?\n>>\n>>\n>> ## 6. UTxO pool layout\n>>\n>> Now that you somehow managed to settle on a refill amount, how are you\n>> going to use these funds?\n>> Also, you'll need to manage your pool across time (consolidating small\n>> coins, and probably fanning\n>> out large ones).\n>>\n>> You could keep a single large UTxO and peel it as you need to sponsor\n>> transactions. But this means\n>> that you need to create a coin of a specific value according to your need\n>> at the current feerate\n>> estimation, hope to have it confirmed in a few blocks (at least for now!\n>> [5]), and hope that the\n>> value won't be obsolete by the time it confirmed. Also, you'd have to do\n>> that for any number of\n>> Cancel, chaining feebump coin creation transactions off the change of the\n>> previous ones or replacing\n>> them with more outputs. Both seem to become really un-manageable (and\n>> expensive) in many edge-cases,\n>> shortening the time you have to confirm the actual Cancel transaction and\n>> creating uncertainty about\n>> the reserve (how much is my just-in-time fanout going to cost me in fees\n>> that i need to refill in\n>> advance on my watchtower wallet?).\n>> This is less of a concern for protocols using CPFP to sponsor\n>> transactions, but they rely on a\n>> policy rule specific to 2-parties contracts.\n>>\n>> Therefore for Revault we fan-out the coins per-vault in advance. We do so\n>> at refill time so the\n>> refiller can give an excess to pay for the fees of the fanout transaction\n>> (which is reasonable since\n>> it will occur just after the refilling transaction confirms). When the\n>> watchtower is asked to watch\n>> for a new delegated vault it will allocate coins from the pool of\n>> fanned-out UTxOs to it (failing\n>> that, it would refuse the delegation).\n>> What is a good distribution of UTxOs amounts per vault? We want to\n>> minimize the number of coins,\n>> still have coins small enough to not overpay (remember, we can't have\n>> change) and be able to bump a\n>> Cancel up to the reserve feerate using these coins. The two latter\n>> constraints are directly in\n>> contradiction as the minimal value of a coin usable at the reserve\n>> feerate (paying for its own input\n>> fee + bumping the feerate by, say, 5sat/vb) is already pretty high.\n>> Therefore we decided to go with\n>> two distributions per vault. The \"reserve distribution\" alone ensures\n>> that we can bump up to the\n>> reserve feerate and is usable for high feerates. The \"bonus distribution\"\n>> is not, but contains\n>> smaller coins useful to prevent overpayments during low and medium fee\n>> periods (which is most of the\n>> time).\n>> Both distributions are based on a basic geometric suite [6]. Each value\n>> is half the previous one.\n>> This exponentially decreases the value, limiting the number of coins. But\n>> this also allows for\n>> pretty small coins to exist and each coin's value is equal to the sum of\n>> the smaller coins,\n>> or smaller by at most the value of the smallest coin. Therefore bounding\n>> the maximum overpayment to\n>> the smallest coin's value [7].\n>>\n>> For the management of the UTxO pool across time we merged the\n>> consolidation with the fanout. When\n>> fanning out a refilled UTxO, we scan the pool for coins that need to be\n>> consolidated according to a\n>> heuristic. An instance of a heuristic is \"the coin isn't allocated and\n>> would not have been able to\n>> increase the fee at the median feerate over the past 90 days of blocks\".\n>> We had this assumption that feerate would tend to go up with time and\n>> therefore discarded having to\n>> split some UTxOs from the pool. We however overlooked that a large\n>> increase in the exchange price of\n>> BTC as we've seen during the past year could invalidate this assumption\n>> and that should arguably be\n>> reconsidered.\n>>\n>>\n>> ## 7. Bumping and re-bumping\n>>\n>> First of all, when to fee-bump? At fixed time intervals? At each block\n>> connection? It sounds like,\n>> given a large enough timelock, you could try to greed by \"trying your\n>> luck\" at a lower feerate and\n>> only re-bumping every N blocks. You would then start aggressively bumping\n>> at every block after M\n>> blocks have passed. But that's actually a bet (in disguised?) that the\n>> next block feerate in M blocks\n>> will be lower than the current one. In the absence of any predictive\n>> model it is more reasonable to\n>> just start being aggressive immediately.\n>> You probably want to base your estimates on `estimatesmartfee` and as a\n>> consequence you would re-bump\n>> (if needed )after each block connection, when your estimates get updated\n>> and you notice your\n>> transaction was not included in the block.\n>>\n>> In the event that you notice a consequent portion of the block is filled\n>> with transactions paying\n>> less than your own, you might want to start panicking and bump your\n>> transaction fees by a certain\n>> percentage with no consideration for your fee estimator. You might skew\n>> miners incentives in doing\n>> so: if you increase the fees by a factor of N, any miner with a fraction\n>> larger than 1/N of the\n>> network hashrate now has an incentive to censor your transaction at first\n>> to get you to panic. Also\n>> note this can happen if you want to pay the absolute fees for the\n>> 'pinning' attack mentioned in\n>> section #2, and that might actually incentivize miners to perform it\n>> themselves..\n>>\n>> The gist is that the most effective way to bump and rebump (RBF the\n>> Cancel tx) seems to just be to\n>> consider the `estimatesmartfee 2 CONSERVATIVE` feerate at every block\n>> your tx isn't included in, and\n>> to RBF it if the feerate is higher.\n>> In addition, we fallback to a block chain based estimation when estimates\n>> aren't available (eg if\n>> the user stopped their WT for say a hour and we come back up): we use the\n>> 85th percentile over the\n>> feerates in the last 6 blocks. Sure, miners can try to have an influence\n>> on that by stuffing their\n>> blocks with large fee self-paying transactions, but they would need to:\n>> 1. Be sure to catch a significant portion of the 6 blocks (at least 2,\n>> actually)\n>> 2. Give up on 25% of the highest fee-paying transactions (assuming they\n>> got the 6 blocks, it's\n>>    proportionally larger and incertain as they get less of them)\n>> 3. Hope that our estimator will fail and we need to fall back to the\n>> chain-based estimation\n>>\n>>\n>> ## 8. Our study\n>>\n>> We essentially replayed the historical data with different deployment\n>> configurations (number of\n>> participants and timelock) and probability of an event occurring (event\n>> being say an Unvault, an\n>> invalid Unvault, a new delegation, ..). We then observed different\n>> metrics such as the time at risk\n>> (when we can't enforce all our contracts at the reserve feerate at the\n>> same time), or the\n>> operational cost.\n>> We got the historical fee estimates data from Statoshi [9], Txstats [10]\n>> and the historical chain\n>> data from Riccardo Casatta's `blocks_iterator` [11]. Thanks!\n>>\n>> The (research-quality..) code can be found at\n>> https://github.com/revault/research under the section\n>> \"Fee bumping\". Again it's very Revault specific, but at least the data\n>> can probably be reused for\n>> studying other protocols.\n>>\n>>\n>> ## 9. Insurances\n>>\n>> Of course, given it's all hacks and workarounds and there is no good\n>> answer to \"what is a reasonable\n>> feerate up to which we need to make contracts enforceable onchain?\",\n>> there is definitely room for an\n>> insurance market. But this enters the realm of opinions. Although i do\n>> have some (having discussed\n>> this topic for the past years with different people), i would like to\n>> keep this post focused on the\n>> technical aspects of this problem.\n>>\n>>\n>>\n>> [0] As far as i can tell, having offchain contracts be enforceable\n>> onchain by confirming a\n>> transaction before the expiration of a timelock is a widely agreed-upon\n>> approach. And i don't think\n>> we can opt for any other fundamentally different one, as you want to know\n>> you can claim back your\n>> coins from a contract after a deadline before taking part in it.\n>>\n>> [1] The Real Revault (tm) involves more transactions, but for the sake of\n>> conciseness i only\n>> detailed a minimum instance of the problem.\n>>\n>> [2] Only presigning part of the Unvault transactions allows to only\n>> delegate part of the coins,\n>> which can be abstracted as \"delegate x% of your stash\" in the user\n>> interface.\n>>\n>> [3]\n>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-May/017835.html\n>>\n>> [4]\n>> https://github.com/revault/research/blob/1df953813708287c32a15e771ba74957ec44f354/feebumping/model/statemachine.py#L323-L329\n>>\n>> [5] https://github.com/bitcoin/bitcoin/pull/23121\n>>\n>> [6]\n>> https://github.com/revault/research/blob/1df953813708287c32a15e771ba74957ec44f354/feebumping/model/statemachine.py#L494-L507\n>>\n>> [7] Of course this assumes a combinatorial coin selection, but i believe\n>> it's ok given we limit the\n>> number of coins beforehand.\n>>\n>> [8] Although there is the argument to outbid a censorship, anyone\n>> censoring you isn't necessarily a\n>> miner.\n>>\n>> [9] https://www.statoshi.info/\n>>\n>> [10] https://www.statoshi.info/\n>>\n>> [11] https://github.com/RCasatta/blocks_iterator\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211208/99481646/attachment-0001.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2021-12-09T00:55:23",
                "message_text_only": "Hi Gloria,\n\nFor LN, I think 3 tower rewards models have been discussed : per-penalty\non-chain bounty/per-job micropayment/customer subscription. If curious, see\nthe wip specification :\nhttps://github.com/sr-gi/bolt13/blob/master/13-watchtowers.md\n\n> - Do we expect watchtowers tracking multiple vaults to be batching\nmultiple\n> Cancel transaction fee-bumps?\n\nFor LN, I can definitely see LSP to batch closure of their spokes, with one\nCPFP spending multiple anchors outputs of commitment transactions, and\nRBF'ing when needed.\n\n> - Do we expect vault users to be using multiple watchtowers for a better\n> trust model? If so, and we're expecting batched fee-bumps, won't those\n> conflict?\n\nEven worse, a malicious counterparty could force an unilateral closure by\nthe honest participant and observe the fee-bumping transaction propagation\nby the towers to discover their full-nodes topologies. Might be good to\nhave an ordering algo among your towers to select who is fee-bumping first,\nand broadcast all when you're reaching near timelock expiration.\n\n> Well stated about CPFP carve out. I suppose the generalization is that\n> allowing n extra ancestorcount=2 descendants to a transaction means it can\n> help contracts with <=n+1 parties (more accurately, outputs)? I wonder if\n> it's possible to devise a different approach for limiting\n> ancestors/descendants, e.g. by height/width/branching factor of the family\n> instead of count... :shrug:\n\nI think CPFP carve out can be deprecated once package relay and a\npinning-hardened RBF is deployed ?  Like if your counterparty is abusing\nthe ancestors/descendants limits, your RBF'ed package should evict the\nmalicious pinning starting by the root commitment transaction (I think).\nAnd I believe it can be generalized to n-parties contracts, if your\ntransaction includes one \"any-contract-can-spend\" anchor ouput.\n\n> - Should the fee-bumping strategy depend on how close you are to your\n> timelock expiry? (though this seems like a potential privacy leak, and the\n> game theory could get weird as you mentioned).\n\nYes, at first it's hard to predict how tight it is going to be and it's\nnice to save on fees. At some point, you might fall-back off this\nfee-bumping warm up-phase to accelerate the rate and start to be more\naggressive. In that direction, see DLC spec fee-bumping recommendation :\nhttps://github.com/discreetlogcontracts/dlcspecs/blob/master/Non-Interactive-Protocol.md\n\nNote, at least for LN, the transaction weight isn't proportional with the\nvalue at stake, and there  is a focal point where it's more interesting to\nsave fee reserves rather than keep bumping.\n\n> - As long as you have a good fee estimator (i.e. given a current mempool,\ncan get an accurate feerate given a % probability of getting into target\nblock n), is there any reason to devise a fee-bumping strategy beyond\npicking a time interval?\n\nYou might be a LSP, you observe rapid changes in the global network HTLC\ntraffic and would like to react in consequence. You accelerate the\nfee-bumping to free/reallocate your liquidity elsewhere.\n\n> So the equation is more like: a miner with 1/N of the hashrate, employing\nthis censorship strategy, gains only if `max(f2-g2, 0) > N * (f1-g1)`. More\nbroadly, the miner only profits if `f2` is significantly higher than `g2\n\nThis is where it becomes hard. From your \"limited rationality\" of a\nfee-bumping node `g2` is unknown, And you might be incentivized to\novershoot to front-run `g2` issuer (?)\n\n> In general, I agree it would really suck to inadvertently create a game\nwhere miners can drive feerates up by triggering desperation-driven\nfee-bumping procedures. I guess this is a reason to avoid\nincreasingly-aggressive feebumping, or strategies where we predictably\novershoot.\n\nGood topic of research! Few other vectors of analysis :\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2020-February/002569.html\n\nCheers,\nAntoine\n\nLe mar. 7 d\u00e9c. 2021 \u00e0 12:24, Gloria Zhao <gloriajzhao at gmail.com> a \u00e9crit :\n\n> Hi Darosior and Ariard,\n>\n> Thank you for your work looking into fee-bumping so thoroughly, and for\n> sharing your results. I agree about fee-bumping's importance in contract\n> security and feel that it's often under-prioritized. In general, what\n> you've described in this post, to me, is strong motivation for some of the\n> proposed changes to RBF we've been discussing. Mostly, I have some\n> questions.\n>\n> > The part of Revault we are interested in for this study is the\n> delegation process, and more\n> > specifically the application of spending policies by network monitors\n> (watchtowers).\n>\n> I'd like to better understand how fee-bumping would be used, i.e. how the\n> watchtower model works:\n> - Do all of the vault parties both deposit to the vault and a refill/fee\n> to the watchtower, is there a reward the watchtower collects for a\n> successful Cancel, or something else? (Apologies if there's a thorough\n> explanation somewhere that I haven't already seen).\n> - Do we expect watchtowers tracking multiple vaults to be batching\n> multiple Cancel transaction fee-bumps?\n> - Do we expect vault users to be using multiple watchtowers for a better\n> trust model? If so, and we're expecting batched fee-bumps, won't those\n> conflict?\n>\n> > For Revault we can afford to introduce malleability in the Cancel\n> transaction since there is no\n> > second-stage transaction depending on its txid. Therefore it is\n> pre-signed with ANYONECANPAY. We\n> > can't use ANYONECANPAY|SINGLE since it would open a pinning vector [3].\n> Note how we can't leverage\n> > the carve out rule, and neither can any other more-than-two-parties\n> contract.\n>\n> We've already talked about this offline, but I'd like to point out here\n> that even transactions signed with ANYONECANPAY|ALL can be pinned by RBF\n> unless we add an ancestor score rule. [0], [1] (numbers are inaccurate,\n> Cancel Tx feerates wouldn't be that low, but just to illustrate what the\n> attack would look like)\n>\n> [0]:\n> https://user-images.githubusercontent.com/25183001/135104603-9e775062-5c8d-4d55-9bc9-6e9db92cfe6d.png\n> [1]:\n> https://user-images.githubusercontent.com/25183001/145044333-2f85da4a-af71-44a1-bc21-30c388713a0d.png\n>\n> > can't use ANYONECANPAY|SINGLE since it would open a pinning vector [3].\n> Note how we can't leverage\n> > the carve out rule, and neither can any other more-than-two-parties\n> contract.\n>\n> Well stated about CPFP carve out. I suppose the generalization is that\n> allowing n extra ancestorcount=2 descendants to a transaction means it can\n> help contracts with <=n+1 parties (more accurately, outputs)? I wonder if\n> it's possible to devise a different approach for limiting\n> ancestors/descendants, e.g. by height/width/branching factor of the family\n> instead of count... :shrug:\n>\n> > You could keep a single large UTxO and peel it as you need to sponsor\n> transactions. But this means\n> > that you need to create a coin of a specific value according to your\n> need at the current feerate\n> > estimation, hope to have it confirmed in a few blocks (at least for now!\n> [5]), and hope that the\n> > value won't be obsolete by the time it confirmed.\n>\n> IIUC, a Cancel transaction can be generalized as a 1-in-1-out where the\n> input is presigned with counterparties, SIGHASH_ANYONECANPAY. The fan-out\n> UTXO pool approach is a clever solution. I also think this smells like a\n> case where improving lower-level RBF rules is more appropriate than\n> requiring applications to write workarounds and generate extra\n> transactions. Seeing that the BIP125#2 (no new unconfirmed inputs)\n> restriction really hurts in this case, if that rule were removed, would you\n> be able to simply keep the 1 big UTXO per vault and cut out the exact\n> nValue you need to fee-bump Cancel transactions? Would that feel less like\n> \"burning\" for the sake of fee-bumping?\n>\n> > First of all, when to fee-bump? At fixed time intervals? At each block\n> connection? It sounds like,\n> > given a large enough timelock, you could try to greed by \"trying your\n> luck\" at a lower feerate and\n> > only re-bumping every N blocks. You would then start aggressively\n> bumping at every block after M\n> > blocks have passed.\n>\n> I'm wondering if you also considered other questions like:\n> - Should a fee-bumping strategy be dependent upon the rate of incoming\n> transactions? To me, it seems like the two components are (1) what's in the\n> mempool and (2) what's going to trickle into the mempool between now and\n> the target block. The first component is best-effort keeping\n> incentive-compatible mempool; historical data and crystal ball look like\n> the only options for incorporating the 2nd component.\n> - Should the fee-bumping strategy depend on how close you are to your\n> timelock expiry? (though this seems like a potential privacy leak, and the\n> game theory could get weird as you mentioned).\n> - As long as you have a good fee estimator (i.e. given a current mempool,\n> can get an accurate feerate given a % probability of getting into target\n> block n), is there any reason to devise a fee-bumping strategy beyond\n> picking a time interval?\n>\n> It would be interesting to see stats on the spread of feerates in blocks\n> during periods of fee fluctuation.\n>\n> > > In the event that you notice a consequent portion of the block is\n> filled with transactions paying\n> > > less than your own, you might want to start panicking and bump your\n> transaction fees by a certain\n> > > percentage with no consideration for your fee estimator. You might\n> skew miners incentives in doing\n> > > so: if you increase the fees by a factor of N, any miner with a\n> fraction larger than 1/N of the\n> > > network hashrate now has an incentive to censor your transaction at\n> first to get you to panic.\n>\n> > Yes I think miner-harvesting attacks should be weighed carefully in the\n> design of offchain contracts fee-bumping strategies, at least in the future\n> when the mining reward exhausts further.\n>\n> Miner-harvesting (such cool naming!) is interesting, but I want to clarify\n> the value of N - I don't think it's the factor by which you increase the\n> fees on just your transaction.\n>\n> To codify: your transaction pays a fee of `f1` right now and might pay a\n> fee of `f2` in a later block that the miner expects to mine with 1/N\n> probability. The economically rational miner isn't incentivized if simply\n> `f2 = N * f1` unless their mempool is otherwise empty.\n> By omitting your transaction in this block, the miner can include another\n> transaction/package paying `g1` fees instead, so they lose `f1-g1` in fees\n> right now. In the future block, they have the choice between collecting\n> `f2` or `g2` (from another transaction/package) in fees, so their gain is\n> `max(f2-g2, 0)`.\n> So the equation is more like: a miner with 1/N of the hashrate, employing\n> this censorship strategy, gains only if `max(f2-g2, 0) > N * (f1-g1)`. More\n> broadly, the miner only profits if `f2` is significantly higher than `g2`\n> and `f1` is about the same feerate as everything else in your mempool: it\n> seems like they're betting on how much you _overshoot_, not how much you\n> bump.\n>\n> In general, I agree it would really suck to inadvertently create a game\n> where miners can drive feerates up by triggering desperation-driven\n> fee-bumping procedures. I guess this is a reason to avoid\n> increasingly-aggressive feebumping, or strategies where we predictably\n> overshoot.\n>\n> Slightly related question: in contracts, generally, the timelock deadline\n> is revealed in the script, so the miner knows how \"desperate\" we are right?\n> Is that a problem? For Revault, if your Cancel transaction is a keypath\n> spend (I think I remember reading that somewhere?) and you don't reveal the\n> script, they don't see your timelock deadline yes?\n>\n> Again, thanks for the digging and sharing. :)\n>\n> Best,\n> Gloria\n>\n> On Tue, Nov 30, 2021 at 3:27 PM darosior via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Hi Antoine,\n>>\n>> Thanks for your comment. I believe for Lightning it's simpler with regard\n>> to the management of the UTxO pool, but harder with regard to choosing\n>> a threat model.\n>> Responses inline.\n>>\n>>\n>> For any opened channel, ensure the confirmation of a Commitment\n>> transaction and the children HTLC-Success/HTLC-Timeout transactions. Note,\n>> in the Lightning security game you have to consider (at least) 4 types of\n>> players moves and incentives : your node, your channel counterparties, the\n>> miners, the crowd of bitcoin users. The number of the last type of players\n>> is unknown from your node, however it should not be forgotten you're in\n>> competition for block space, therefore their block demands bids should be\n>> anticipated and reacted to in consequence. With that remark in mind,\n>> implications for your LN fee-bumping strategy will be raised afterwards.\n>>\n>> For a LN service provider, on-chain overpayments are bearing on your\n>> operational costs, thus downgrading your economic competitiveness. For the\n>> average LN user, overpayment might price out outside a LN non-custodial\n>> deployment, as you don't have the minimal security budget to be on your own.\n>>\n>>\n>> I think this problem statement can be easily generalised to any offchain\n>> contract. And your points stand for all of them.\n>> \"For any opened contract, ensure at any point the confirmation of a (set\n>> of) transaction(s) in a given number of blocks\"\n>>\n>>\n>> Same issue with Lightning, we can be pinned today on the basis of\n>> replace-by-fee rule 3. We can be also blinded by network mempool\n>> partitions, a pinning counterparty can segregate all the full-nodes  in as\n>> many subsets by broadcasting a revoked Commitment transaction different for\n>> each. For Revault, I think you can also do unlimited partitions by mutating\n>> the ANYONECANPAY-input of the Cancel.\n>>\n>>\n>> Well you can already do unlimited partitions by adding different inputs\n>> to it. You could malleate the witness, but since we are using Miniscript\n>> i'm confident you would only be able in a marginal way.\n>>\n>>\n>> That said, if you have a distributed towers deployment, spread across the\n>> p2p network topology, and they can't be clustered together through\n>> cross-layers or intra-layer heuristics, you should be able to reliably\n>> observe such partitions. I think such distributed monitors are deployed by\n>> few L1 merchants accepting 0-conf to detect naive double-spend.\n>>\n>>\n>> We should aim to more than 0-conf (in)security level..\n>> It seems to me the only policy-level mitigation for RBF pinning around\n>> the \"don't decrease the abolute fees of a less-than-a-block mempool\" would\n>> be to drop the requirement on increasing absolute fees if the mempool is\n>> \"full enough\" (and the feerate increases exponentially, of course).\n>> Another approach could be by introducing new consensus rules as proposed\n>> by Jeremy last year [0]. If we go in the realm of new consensus rules, then\n>> i think that simply committing to a maximum tx size would fix pinning by\n>> RBF rule 3. Could be in the annex, or in the unused sequence bits (although\n>> they currently are by Lightning, meh). You could also check in the output\n>> script that the input commits to this.\n>>\n>> [0]\n>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-September/018168.html\n>>\n>>\n>> Have we already discussed a fee-bumping \"shared cache\", a CPFP variation\n>> ? Strawman idea: Alice and Bob commit collateral inputs to a separate UTXO\n>> from the main \"offchain contract\" one. This UTXO is locked by a multi-sig.\n>> For any Commitment transaction pre-signed, also counter-sign a CPFP with\n>> top mempool feerate included, spending a Commitment anchor output and the\n>> shared-cache UTXO. If the fees spike,  you can re-sign a high-feerate CPFP,\n>> assuming interactivity. As the CPFP is counter-signed by everyone, the\n>> outputs can be CSV-1 encumbered to prevent pinnings. If the share-cache is\n>> feeded at parity, there shouldn't be an incentive to waste or maliciously\n>> inflate the feerate. I think this solution can be easily generalized to\n>> more than 2 counterparties by using a multi-signature scheme. Big issue, if\n>> the feerate is short due to fee spikes and you need to re-sign a\n>> higher-feerate CPFP, you're trusting your counterparty to interact, though\n>> arguably not worse than the current update fee mechanism.\n>>\n>>\n>> It really looks just like `update_fee`. Except maybe with the property\n>> that you have the channel liquidity not depend on the onchain feerate.\n>> In any case, for Lightning i think it's a bad idea to re-introduce trust\n>> on this side post anchor outputs. For Revault it's clearly out of the\n>> question to introduce trust in your counterparties (why would you bother\n>> having a fee-bumping mechanism in the first place then?). Probably the same\n>> holds for all offchain contracts.\n>>\n>>\n>> > For Lightning, it'd mean keeping an equivalent amount of funds as the\n>> sum of all your\n>> channels balances sitting there unallocated \"just in case\". This is not\n>> reasonable.\n>>\n>> Agree, game-theory wise, you would like to keep a full fee-bumping\n>> reserve, ready to burn as much in fees as the contested HTLC value, as it's\n>> the maximum gain of your counterparty. Though perfect equilibrium is hard\n>> to achieve because your malicious counterparty might have an edge pushing\n>> you to broadcast your Commitment first by witholding HTLC resolution.\n>>\n>> Fractional fee-bumping reserves are much more realistic to expect in the\n>> LN network. Lower fee-bumping reserve, higher liquidity deployed, in theory\n>> higher routing fees. By observing historical feerates, average offchain\n>> balances at risk and routing fees expected gains, you should be able to\n>> discover an equilibrium where higher levels of reserve aren't worth the\n>> opportunity cost. I guess this  equilibrium could be your LN fee-bumping\n>> reserve max feerate.\n>>\n>> Note, I think the LN approach is a bit different from what suits a\n>> custody protocol like Revault,  as you compute a direct return of the\n>> frozen fee-bumping liquidity. With Revault, if you have numerous bitcoins\n>> protected, it's might be more interesting to adopt a \"buy the mempool,\n>> stupid\" strategy than risking fund safety for few percentages of interest\n>> returns.\n>>\n>>\n>> True for routing nodes. For wallets (if receiving funds), it's not about\n>> an investment: just users expectations to being able to transact without\n>> risking to lose their funds (ie being able to enforce their contract\n>> onchain). Although wallets they are much less at risk.\n>>\n>>\n>> This is where the \"anticipate the crowd of bitcoin users move\" point can\n>> be laid out. As the crowd of bitcoin users' fee-bumping reserves are\n>> ultimately unknown from your node knowledge, you should be ready to be a\n>> bit more conservative than the vanilla fee-bumping strategies shipped by\n>> default. In case of massive mempool congestion, your additional\n>> conservatism might get your time-sensitive transactions and game on the\n>> crowd of bitcoin users. First Problem: if all offchain bitcoin software\n>> adopt that strategy we might inflate the worst-case feerate rate at the\n>> benefit of the miners, without holistically improving block throughput.\n>> Second problem : your class of offchain bitcoin softwares might have\n>> ridiculous fee-bumping reserve compared\n>> to other classes of offchain bitcoin softwares (Revault > Lightning) and\n>> just be priced out bydesign in case of mempool congestion. Third problem :\n>> as the number of offchain bitcoin applications should go up with time, your\n>> fee-bumping reserve levels based from historical data might be always late\n>> by one \"bank-run\" scenario.\n>>\n>>\n>> Black swan event 2.0? Just rule n\u00b03 is inherent to any kind of fee\n>> estimation.\n>>\n>> For Lightning, if you're short in fee-bumping reserves you might still do\n>> preemptive channel closures, either cooperatively or unilaterally and get\n>> back the off-chain liquidity to protect the more economically interesting\n>> channels. Though again, that kind of automatic behavior might be compelling\n>> at the individual node-level, but make the mempol congestion worse\n>> holistically.\n>>\n>>\n>> Yeah so we are back to the \"fractional reserve\" model: you can only\n>> enforce X% of the offchain contracts your participate in.. Actually it's\n>> even an added assumption: that you still have operating contracts, with\n>> honest counterparties.\n>>\n>>\n>> In case of massive mempool congestion, you might try to front-run the\n>> crowd of bitcoin users relying on block connections for fee-bumping, and\n>> thus start your fee-bumping as soon as you observe feerate groups\n>> fluctuations in your local mempool(s).\n>>\n>>\n>> I don't think any kind of mempool-based estimate generalizes well, since\n>> at any point the expected time before the next block is 10 minutes (and a\n>> lot can happen in 10min).\n>>\n>> Also you might proceed your fee-bumping ticks on a local clock instead of\n>> block connections in case of time-dilation or deeper eclipse attacks of\n>> your local node. Your view of the chain might be compromised but not your\n>> ability to broadcast transactions thanks to emergency channels (in the\n>> non-LN sense...though in fact quid of txn wrapped in onions ?) of\n>> communication.\n>>\n>>\n>> Oh, yeah, i didn't explicit \"not getting eclipsed\" (or more generally\n>> \"data availability\") as an assumption since it's generally one made by\n>> participants of any offchain contract. In this case you can't even have\n>> decent fee estimation, so you are screwed anyways.\n>>\n>>\n>> Yes, stay open the question on how you enforce this block insurance\n>> market. Reputation, which might be to avoid due to the latent\n>> centralization effect, might be hard to stack and audit reliably for an\n>> emergency mechanism running, hopefully, once in a halvening period. Maybe\n>> maybe some cryptographic or economically based mechanism on slashing or\n>> swaps could be found...\n>>\n>>\n>> Unfortunately, given current mining centralisation, pools are in a very\n>> good position to offer pretty decent SLAs around that. With a block space\n>> insurance, you of course don't need all these convoluted fee-bumping hacks.\n>> I'm very concerned that large stakeholders of the \"offchain contracts\n>> ecosystem\" would just go this (easier) way and further increase mining\n>> centralisation pressure.\n>>\n>> I agree that a cryptography-based scheme around this type of insurance\n>> services would be the best way out.\n>>\n>>\n>> Antoine\n>>\n>> Le lun. 29 nov. 2021 \u00e0 09:34, darosior via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>\n>>> Hi everyone,\n>>>\n>>> Fee-bumping is paramount to the security of many protocols building on\n>>> Bitcoin, as they require the\n>>> confirmation of a transaction (which might be presigned) before the\n>>> expiration of a timelock at any\n>>> point after the establishment of the contract.\n>>>\n>>> The part of Revault using presigned transactions (the delegation from a\n>>> large to a smaller multisig)\n>>> is no exception. We have been working on how to approach this for a\n>>> while now and i'd like to share\n>>> what we have in order to open a discussion on this problem so central to\n>>> what seem to be The Right\n>>> Way [0] to build on Bitcoin but which has yet to be discussed in details\n>>> (at least publicly).\n>>>\n>>> I'll discuss what we came up with for Revault (at least for what will be\n>>> its first iteration) but my\n>>> intent with posting to the mailing list is more to frame the questions\n>>> to this problem we are all\n>>> going to face rather than present the results of our study tailored to\n>>> the Revault usecase.\n>>> The discussion is still pretty Revault-centric (as it's the case study)\n>>> but hopefully this can help\n>>> future protocol designers and/or start a discussion around what\n>>> everyone's doing for existing ones.\n>>>\n>>>\n>>> ## 1. Reminder about Revault\n>>>\n>>> The part of Revault we are interested in for this study is the\n>>> delegation process, and more\n>>> specifically the application of spending policies by network monitors\n>>> (watchtowers).\n>>> Coins are received on a large multisig. Participants of this large\n>>> multisig create 2 [1]\n>>> transactions. The Unvault, spending a deposit UTxO, creates an output\n>>> paying either to the small\n>>> multisig after a timelock or to the large multisig immediately. The\n>>> Cancel, spending the Unvault\n>>> output through the non-timelocked path, creates a new deposit UTxO.\n>>> Participants regularly exchange the Cancel transaction signatures for\n>>> each deposit, sharing the\n>>> signatures with the watchtowers they operate. They then optionally [2]\n>>> sign the Unvault transaction\n>>> and share the signatures with the small multisig participants who can in\n>>> turn use them to proceed\n>>> with a spending. Watchtowers can enforce spending policies (say, can't\n>>> Unvault outside of business\n>>> hours) by having the Cancel transaction be confirmed before the\n>>> expiration of the timelock.\n>>>\n>>>\n>>> ## 2. Problem statement\n>>>\n>>> For any delegated vault, ensure the confirmation of a Cancel transaction\n>>> in a configured number of\n>>> blocks at any point. In so doing, minimize the overpayments and the UTxO\n>>> set footprint. Overpayments\n>>> increase the burden on the watchtower operator by increasing the\n>>> required frequency of refills of the\n>>> fee-bumping wallet, which is already the worst user experience. You are\n>>> likely to manage a number of\n>>> UTxOs with your number of vaults, which comes at a cost for you as well\n>>> as everyone running a full\n>>> node.\n>>>\n>>> Note that this assumes miners are economically rationale, are\n>>> incentivized by *public* fees and that\n>>> you have a way to propagate your fee-bumped transaction to them. We also\n>>> don't consider the block\n>>> space bounds.\n>>>\n>>> In the previous paragraph and the following text, \"vault\" can generally\n>>> be replaced with \"offchain\n>>> contract\".\n>>>\n>>>\n>>> ## 3. With presigned transactions\n>>>\n>>> As you all know, the first difficulty is to get to be able to\n>>> unilaterally enforce your contract\n>>> onchain. That is, any participant must be able to unilaterally bump the\n>>> fees of a transaction even\n>>> if it was co-signed by other participants.\n>>>\n>>> For Revault we can afford to introduce malleability in the Cancel\n>>> transaction since there is no\n>>> second-stage transaction depending on its txid. Therefore it is\n>>> pre-signed with ANYONECANPAY. We\n>>> can't use ANYONECANPAY|SINGLE since it would open a pinning vector [3].\n>>> Note how we can't leverage\n>>> the carve out rule, and neither can any other more-than-two-parties\n>>> contract.\n>>> This has a significant implication for the rest, as we are entirely\n>>> burning fee-bumping UTxOs.\n>>>\n>>> This opens up a pinning vector, or at least a significant nuisance: any\n>>> other party can largely\n>>> increase the absolute fee without increasing the feerate, leveraging the\n>>> RBF rules to prevent you\n>>> from replacing it without paying an insane fee. And you might not see it\n>>> in your own mempool and\n>>> could only suppose it's happening by receiving non-full blocks or with\n>>> transactions paying a lower\n>>> feerate.\n>>> Unfortunately i know of no other primitive that can be used by\n>>> multi-party (i mean, >2) presigned\n>>> transactions protocols for fee-bumping that aren't (more) vulnerable to\n>>> pinning.\n>>>\n>>>\n>>> ## 4. We are still betting on future feerate\n>>>\n>>> The problem is still missing one more constraint. \"Ensuring confirmation\n>>> at any time\" involves ensuring\n>>> confirmation at *any* feerate, which you *cannot* do. So what's the\n>>> limit? In theory you should be ready\n>>> to burn as much in fees as the value of the funds you want to get out of\n>>> the contract. So... For us\n>>> it'd mean keeping for each vault an equivalent amount of funds sitting\n>>> there on the watchtower's hot\n>>> wallet. For Lightning, it'd mean keeping an equivalent amount of funds\n>>> as the sum of all your\n>>> channels balances sitting there unallocated \"just in case\". This is not\n>>> reasonable.\n>>>\n>>> So you need to keep a maximum feerate, above which you won't be able to\n>>> ensure the enforcement of\n>>> all your contracts onchain at the same time. We call that the \"reserve\n>>> feerate\" and you can have\n>>> different strategies for choosing it, for instance:\n>>> - The 85th percentile over the last year of transactions feerates\n>>> - The maximum historical feerate\n>>> - The maximum historical feerate adjusted in dollars (makes more sense\n>>> but introduces a (set of?)\n>>>   trusted oracle(s) in a security-critical component)\n>>> - Picking a random high feerate (why not? It's an arbitrary assumption\n>>> anyways)\n>>>\n>>> Therefore, even if we don't have to bet on the broadcast-time feerate\n>>> market at signing time anymore\n>>> (since we can unilaterally bump), we still need some kind of prediction\n>>> in preparation of making\n>>> funds available to bump the fees at broadcast time.\n>>> Apart from judging that 500sat/vb is probably more reasonable than\n>>> 10sat/vbyte, this unfortunately\n>>> sounds pretty much crystal-ball-driven.\n>>>\n>>> We currently use the maximum of the 95th percentiles over 90-days\n>>> windows over historical block chain\n>>> feerates. [4]\n>>>\n>>>\n>>> ## 5. How much funds does my watchtower need?\n>>>\n>>> That's what we call the \"reserve\". Depending on your reserve feerate\n>>> strategy it might vary over\n>>> time. This is easier to reason about with a per-contract reserve. For\n>>> Revault it's pretty\n>>> straightforward since the Cancel transaction size is static:\n>>> `reserve_feerate * cancel_size`. For\n>>> other protocols with dynamic transaction sizes (or even packages of\n>>> transactions) it's less so. For\n>>> your Lightning channel you would probably take the maximum size of your\n>>> commitment transaction\n>>> according to your HTLC exposure settings + the size of as many\n>>> `htlc_success` transaction?\n>>>\n>>> Then you either have your software or your user guesstimate how many\n>>> offchain contracts the\n>>> watchtower will have to watch, time that by the per-contract reserve and\n>>> refill this amount (plus\n>>> some slack in practice). Once again, a UX tradeoff (not even mentioning\n>>> the guesstimation UX):\n>>> overestimating leads to too many unallocated funds sitting on a hot\n>>> wallet, underestimating means\n>>> (at best) inability to participate in new contracts or being \"at risk\"\n>>> (not being able to enforce\n>>> all your contracts onchain at your reserve feerate) before a new refill.\n>>>\n>>> For vaults you likely have large-value UTxOs and small transactions (the\n>>> Cancel is one-in one-out in\n>>> Revault). For some other applications with large transactions and\n>>> lower-value UTxOs on average it's\n>>> likely that only part of the offchain contracts might be enforceable at\n>>> a reasonable feerate. Is it\n>>> reasonable?\n>>>\n>>>\n>>> ## 6. UTxO pool layout\n>>>\n>>> Now that you somehow managed to settle on a refill amount, how are you\n>>> going to use these funds?\n>>> Also, you'll need to manage your pool across time (consolidating small\n>>> coins, and probably fanning\n>>> out large ones).\n>>>\n>>> You could keep a single large UTxO and peel it as you need to sponsor\n>>> transactions. But this means\n>>> that you need to create a coin of a specific value according to your\n>>> need at the current feerate\n>>> estimation, hope to have it confirmed in a few blocks (at least for now!\n>>> [5]), and hope that the\n>>> value won't be obsolete by the time it confirmed. Also, you'd have to do\n>>> that for any number of\n>>> Cancel, chaining feebump coin creation transactions off the change of\n>>> the previous ones or replacing\n>>> them with more outputs. Both seem to become really un-manageable (and\n>>> expensive) in many edge-cases,\n>>> shortening the time you have to confirm the actual Cancel transaction\n>>> and creating uncertainty about\n>>> the reserve (how much is my just-in-time fanout going to cost me in fees\n>>> that i need to refill in\n>>> advance on my watchtower wallet?).\n>>> This is less of a concern for protocols using CPFP to sponsor\n>>> transactions, but they rely on a\n>>> policy rule specific to 2-parties contracts.\n>>>\n>>> Therefore for Revault we fan-out the coins per-vault in advance. We do\n>>> so at refill time so the\n>>> refiller can give an excess to pay for the fees of the fanout\n>>> transaction (which is reasonable since\n>>> it will occur just after the refilling transaction confirms). When the\n>>> watchtower is asked to watch\n>>> for a new delegated vault it will allocate coins from the pool of\n>>> fanned-out UTxOs to it (failing\n>>> that, it would refuse the delegation).\n>>> What is a good distribution of UTxOs amounts per vault? We want to\n>>> minimize the number of coins,\n>>> still have coins small enough to not overpay (remember, we can't have\n>>> change) and be able to bump a\n>>> Cancel up to the reserve feerate using these coins. The two latter\n>>> constraints are directly in\n>>> contradiction as the minimal value of a coin usable at the reserve\n>>> feerate (paying for its own input\n>>> fee + bumping the feerate by, say, 5sat/vb) is already pretty high.\n>>> Therefore we decided to go with\n>>> two distributions per vault. The \"reserve distribution\" alone ensures\n>>> that we can bump up to the\n>>> reserve feerate and is usable for high feerates. The \"bonus\n>>> distribution\" is not, but contains\n>>> smaller coins useful to prevent overpayments during low and medium fee\n>>> periods (which is most of the\n>>> time).\n>>> Both distributions are based on a basic geometric suite [6]. Each value\n>>> is half the previous one.\n>>> This exponentially decreases the value, limiting the number of coins.\n>>> But this also allows for\n>>> pretty small coins to exist and each coin's value is equal to the sum of\n>>> the smaller coins,\n>>> or smaller by at most the value of the smallest coin. Therefore bounding\n>>> the maximum overpayment to\n>>> the smallest coin's value [7].\n>>>\n>>> For the management of the UTxO pool across time we merged the\n>>> consolidation with the fanout. When\n>>> fanning out a refilled UTxO, we scan the pool for coins that need to be\n>>> consolidated according to a\n>>> heuristic. An instance of a heuristic is \"the coin isn't allocated and\n>>> would not have been able to\n>>> increase the fee at the median feerate over the past 90 days of blocks\".\n>>> We had this assumption that feerate would tend to go up with time and\n>>> therefore discarded having to\n>>> split some UTxOs from the pool. We however overlooked that a large\n>>> increase in the exchange price of\n>>> BTC as we've seen during the past year could invalidate this assumption\n>>> and that should arguably be\n>>> reconsidered.\n>>>\n>>>\n>>> ## 7. Bumping and re-bumping\n>>>\n>>> First of all, when to fee-bump? At fixed time intervals? At each block\n>>> connection? It sounds like,\n>>> given a large enough timelock, you could try to greed by \"trying your\n>>> luck\" at a lower feerate and\n>>> only re-bumping every N blocks. You would then start aggressively\n>>> bumping at every block after M\n>>> blocks have passed. But that's actually a bet (in disguised?) that the\n>>> next block feerate in M blocks\n>>> will be lower than the current one. In the absence of any predictive\n>>> model it is more reasonable to\n>>> just start being aggressive immediately.\n>>> You probably want to base your estimates on `estimatesmartfee` and as a\n>>> consequence you would re-bump\n>>> (if needed )after each block connection, when your estimates get updated\n>>> and you notice your\n>>> transaction was not included in the block.\n>>>\n>>> In the event that you notice a consequent portion of the block is filled\n>>> with transactions paying\n>>> less than your own, you might want to start panicking and bump your\n>>> transaction fees by a certain\n>>> percentage with no consideration for your fee estimator. You might skew\n>>> miners incentives in doing\n>>> so: if you increase the fees by a factor of N, any miner with a fraction\n>>> larger than 1/N of the\n>>> network hashrate now has an incentive to censor your transaction at\n>>> first to get you to panic. Also\n>>> note this can happen if you want to pay the absolute fees for the\n>>> 'pinning' attack mentioned in\n>>> section #2, and that might actually incentivize miners to perform it\n>>> themselves..\n>>>\n>>> The gist is that the most effective way to bump and rebump (RBF the\n>>> Cancel tx) seems to just be to\n>>> consider the `estimatesmartfee 2 CONSERVATIVE` feerate at every block\n>>> your tx isn't included in, and\n>>> to RBF it if the feerate is higher.\n>>> In addition, we fallback to a block chain based estimation when\n>>> estimates aren't available (eg if\n>>> the user stopped their WT for say a hour and we come back up): we use\n>>> the 85th percentile over the\n>>> feerates in the last 6 blocks. Sure, miners can try to have an influence\n>>> on that by stuffing their\n>>> blocks with large fee self-paying transactions, but they would need to:\n>>> 1. Be sure to catch a significant portion of the 6 blocks (at least 2,\n>>> actually)\n>>> 2. Give up on 25% of the highest fee-paying transactions (assuming they\n>>> got the 6 blocks, it's\n>>>    proportionally larger and incertain as they get less of them)\n>>> 3. Hope that our estimator will fail and we need to fall back to the\n>>> chain-based estimation\n>>>\n>>>\n>>> ## 8. Our study\n>>>\n>>> We essentially replayed the historical data with different deployment\n>>> configurations (number of\n>>> participants and timelock) and probability of an event occurring (event\n>>> being say an Unvault, an\n>>> invalid Unvault, a new delegation, ..). We then observed different\n>>> metrics such as the time at risk\n>>> (when we can't enforce all our contracts at the reserve feerate at the\n>>> same time), or the\n>>> operational cost.\n>>> We got the historical fee estimates data from Statoshi [9], Txstats [10]\n>>> and the historical chain\n>>> data from Riccardo Casatta's `blocks_iterator` [11]. Thanks!\n>>>\n>>> The (research-quality..) code can be found at\n>>> https://github.com/revault/research under the section\n>>> \"Fee bumping\". Again it's very Revault specific, but at least the data\n>>> can probably be reused for\n>>> studying other protocols.\n>>>\n>>>\n>>> ## 9. Insurances\n>>>\n>>> Of course, given it's all hacks and workarounds and there is no good\n>>> answer to \"what is a reasonable\n>>> feerate up to which we need to make contracts enforceable onchain?\",\n>>> there is definitely room for an\n>>> insurance market. But this enters the realm of opinions. Although i do\n>>> have some (having discussed\n>>> this topic for the past years with different people), i would like to\n>>> keep this post focused on the\n>>> technical aspects of this problem.\n>>>\n>>>\n>>>\n>>> [0] As far as i can tell, having offchain contracts be enforceable\n>>> onchain by confirming a\n>>> transaction before the expiration of a timelock is a widely agreed-upon\n>>> approach. And i don't think\n>>> we can opt for any other fundamentally different one, as you want to\n>>> know you can claim back your\n>>> coins from a contract after a deadline before taking part in it.\n>>>\n>>> [1] The Real Revault (tm) involves more transactions, but for the sake\n>>> of conciseness i only\n>>> detailed a minimum instance of the problem.\n>>>\n>>> [2] Only presigning part of the Unvault transactions allows to only\n>>> delegate part of the coins,\n>>> which can be abstracted as \"delegate x% of your stash\" in the user\n>>> interface.\n>>>\n>>> [3]\n>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-May/017835.html\n>>>\n>>> [4]\n>>> https://github.com/revault/research/blob/1df953813708287c32a15e771ba74957ec44f354/feebumping/model/statemachine.py#L323-L329\n>>>\n>>> [5] https://github.com/bitcoin/bitcoin/pull/23121\n>>>\n>>> [6]\n>>> https://github.com/revault/research/blob/1df953813708287c32a15e771ba74957ec44f354/feebumping/model/statemachine.py#L494-L507\n>>>\n>>> [7] Of course this assumes a combinatorial coin selection, but i believe\n>>> it's ok given we limit the\n>>> number of coins beforehand.\n>>>\n>>> [8] Although there is the argument to outbid a censorship, anyone\n>>> censoring you isn't necessarily a\n>>> miner.\n>>>\n>>> [9] https://www.statoshi.info/\n>>>\n>>> [10] https://www.statoshi.info/\n>>>\n>>> [11] https://github.com/RCasatta/blocks_iterator\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211208/fabce717/attachment-0001.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2021-12-09T13:50:33",
                "message_text_only": "On Mon, Nov 29, 2021 at 02:27:23PM +0000, darosior via bitcoin-dev wrote:\n> ## 2. Problem statement\n> \n> For any delegated vault, ensure the confirmation of a Cancel transaction in a configured number of\n> blocks at any point. In so doing, minimize the overpayments and the UTxO set footprint. Overpayments\n> increase the burden on the watchtower operator by increasing the required frequency of refills of the\n> fee-bumping wallet, which is already the worst user experience. You are likely to manage a number of\n> UTxOs with your number of vaults, which comes at a cost for you as well as everyone running a full\n> node.\n> \n> Note that this assumes miners are economically rationale, are incentivized by *public* fees and that\n> you have a way to propagate your fee-bumped transaction to them. We also don't consider the block\n> space bounds.\n> \n> In the previous paragraph and the following text, \"vault\" can generally be replaced with \"offchain\n> contract\".\n\nFor this section I think it'd help if you re-wrote it mathematically in terms\nof probabilities, variance, and costs. It's impossible to ensure confirmation\nwith 100% probability, so obviously we can start by asking what is the cost of\nfailing to get a confirmation by the deadline?\n\nNow suppose that cost is X. Note how the lowest _variance_ approach would be to\npay a fee of X immediately: that would get us the highest possible probability\nof securing a confirmation prior to the deadline, without paying more than that\nconfirmation is worth.\n\nOf course, this is silly! So the next step is to trade off some variance -\nhigher probability of failure - for lower expected cost. A trivial way to do\nthat could be to just bump the fee linearly between now and that deadline. That\nlowers expected cost. But does increase the probability of failure. We can of\ncourse account for this in an expected cost.\n\n\nOne final nuance here is if this whole process is visible in the UI we might\nwant to take into account user discomfort: if I know the process could fail,\nthe user will probably be happier if it succeeds quickly, even if the\nprobability of success in the future is still very high.\n\n\nFWIW the approach taken by the OpenTimestamps calendars is a trivial linear\nincrease. While they don't have deadlines in the same sense as your\napplication, there is a trade-off between cost and confirmation time. So our\nstrategy is to spend money at a constant rate by simply bumping the fee by the\nsame amount at every new block. I could improve this by using knowledge of the\nmempool. But so far I haven't bothered.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211209/7d38040e/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "A fee-bumping model",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "darosior",
                "Gloria Zhao",
                "Antoine Riard",
                "Peter Todd"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 151992
        }
    },
    {
        "title": "[bitcoin-dev] [Bitcoin Advent Calendar] What's Smart about Smart Contracts",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-07T23:29:27",
                "message_text_only": "Hi!\n\nOver the next month I'm doing a one-a-day blog post series till Christmas,\nand I think some of the posts might be appropriate for discussion here.\n\nUnfortunately I forgot to start the calendar series syndicated here too...\nThe first few posts are less bitcoin development related and philosophical,\nso I think we could skip them and start around Day 6 and I'll post the rest\nup to Day 10 here today (and do every day starting tomorrow). You can see\nan archive of all posts at https://rubin.io/archive/. Every post will have\n[Bitcoin Advent Calendar] if you wish to filter it :(.\n\n---------------------------------\n\nHere's the day 6 post: https://rubin.io/bitcoin/2021/12/03/advent-6/, the\ntopic is why smart contracts (in extended form) may be a critical precursor\nto securing Bitcoin's future rather than something we should do after\nmaking the base layer more robust.\n\nCheers,\n\nJeremy Rubin\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211207/8d8df500/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-12-08T00:32:37",
                "message_text_only": "Good morning Jeremy,\n\n>\n> Here's the day 6 post: https://rubin.io/bitcoin/2021/12/03/advent-6/, the topic is why smart contracts (in extended form) may be a critical precursor to securing Bitcoin's future rather than something we should do after making the base layer more robust.\n\n\n*This* particular post seems to contain more polemic than actual content.\nThis is the first post I read of the series, so maybe it is just a \"breather\" post between content posts?\n\nIn any case, given the subject line, it seems a waste not to discuss the actual \"smart\" in \"smart\" contract...\n\n## Why would a \"Smart\" contract be \"Smart\"?\n\nA \"smart\" contract is simply one that somehow self-enforces rather than requires a third party to enforce it.\nIt is \"smart\" because its execution is done automatically.\n\nConsider the humble HTLC.\nIt is simply a contract which says:\n\n* If B can provide the preimage for this hash H, it gets the money from A.\n* If the time L arrives without B claiming this fund, A gets its money back.\n\nWhy would an HTLC self-enforce?\nWhy would a simple paper contract with the above wording, signed and notarized, be insufficient?\n\nAn HTLC self-enforces because given the Bitcoin network, it is not possible to violate and transfer the funds outside of the HTLC specification.\nWhereas a paper contract can be mere ink on a page, if sufficient firepower is directed at the people (judges, lawyers, etc.) that would ensure its faithful execution.\nYou puny humans are notoriously squishy and easily destroyed.\n\nBut we must warn as well that the Bitcoin network is *also* run by people.\nThus, a \"smart\" contract is only \"smart\" to a degree, and that degree is dependent on how easily it is for the \"justice system\" that enforces the contract to be subverted.\nAfter all, a \"smart\" contract is software, and software must run on some hardware in order to execute.\n\nThus, even existing paper contracts are \"smart\" to a degree, too.\nIt is simply that the hardware they run on top of --- a bunch of puny humans --- is far less reliable than cold silicon (so upgrade your compute substrate already, puny humans!).\nOur hope with the Bitcoin experiment is that we might actually be able to make it much harder to subvert contracts running on the Bitcoin network.\n\nIt is that difficulty of subversion which determines the \"smart\"ness of a smart contract.\nBitcoin is effectively a massive RAID1 on several dozen thousands of redundant compute hardware, ensuring that the execution of every contract is faithful to the Bitcoin SCRIPT programming model.\n\nThis is why the reticence of Bitcoin node operators to change the programming model is a welcome feature of the network.\nAny change to the programming model risks the introduction of bugs to the underlying virtual machine that the Bitcoin network presents to contract makers.\nAnd without that strong reticence, we risk utterly demolishing the basis of the \"smart\"ness of \"smart\" contracts --- if a \"smart\" contract cannot reliably be executed, it cannot self-enforce, and if it cannot self-enforce, it is no longer particularly \"smart\".\n\n## The N-of-N Rule\n\nWhat is a \"contract\", anyway?\n\nA \"contract\" is an agreement between two or more parties.\nYou do not make a contract to yourself, since (we assume) you are completely a single unit (in practice, humans are internally divided into smaller compute modules with slightly different incentives (note: I did not get this information by *personally* dissecting the brains of any humans), hence the \"we assume\").\n\nThus, a contract must by necessity require N participants.\n\nThis is of interest since in a reliability perspective, we often accept k-of-n.\nFor example, we might run a computation on three different pieces of hardware, and if only one diverges, we accept the result of the other two as true and the diverging hardware as faulty.\n\nHowever, the above 2-of-3 example has a hidden assumption: that all three pieces of hardware are actually owned and operated by a single entity.\n\nA contract has N participants, and is not governed by a single entity.\nThus, it cannot use k-of-n replication.\n\nContracts require N-of-N replication.\nIn Bitcoin terms, that is what we mean by \"consensus\" --- that all Bitcoin network participants can agree that some transfer is \"valid\".\n\nSimilarly, L2 layers, to be able to host properly \"smart\" contracts, require N-of-N agreement.\nFor example, a Lightning Network channel can properly host \"smart\" HTLCs, as the channel is controlled via 2-of-2 agreement.\n\nLesser L2 layers which support k-of-n thus have degraded \"smartness\", as a quorum of k participants can evict the n-k and deny the execution of the smart contract.\nBut with an N-of-N, *you* are a participant and your input is necessary for the execution of the smart contract, thus you can be *personally* assured that the smart contract *will* be executed faithfully.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Jeremy",
                "date": "2021-12-08T01:11:55",
                "message_text_only": "--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n\nHi!\n\nOn Tue, Dec 7, 2021 at 4:33 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Jeremy,\n>\n> >\n> > Here's the day 6 post: https://rubin.io/bitcoin/2021/12/03/advent-6/,\n> the topic is why smart contracts (in extended form) may be a critical\n> precursor to securing Bitcoin's future rather than something we should do\n> after making the base layer more robust.\n>\n>\n> *This* particular post seems to contain more polemic than actual content.\n> This is the first post I read of the series, so maybe it is just a\n> \"breather\" post between content posts?\n>\n\nThe series in general is intended to be a bit more on the approachable side\nthan hardcore detail.\n\n\n\n>\n> In any case, given the subject line, it seems a waste not to discuss the\n> actual \"smart\" in \"smart\" contract...\n>\n>\nYeah maybe a better title would be \"The Case for Enhanced Functionality in\nBitcoin\" -- it's not really about smart contracts per se, but the thing\nthat people are calling smart contracts in the broader community. This gets\ndown to prescriptive v.s. descriptive lingo and it's not really a debate I\ncare much for :)\n\n\n\n\n> ## Why would a \"Smart\" contract be \"Smart\"?\n>\n> A \"smart\" contract is simply one that somehow self-enforces rather than\n> requires a third party to enforce it.\n> It is \"smart\" because its execution is done automatically.\n>\n\nThere are no automatic executing smart contracts on any platform I'm aware\nof. Bitcoin requires TX submission, same with Eth.\n\nEnforcement and execution are different subjects.\n\n\n> Consider the humble HTLC.\n> *<snip>*\n> This is why the reticence of Bitcoin node operators to change the\n> programming model is a welcome feature of the network.\n> Any change to the programming model risks the introduction of bugs to the\n> underlying virtual machine that the Bitcoin network presents to contract\n> makers.\n> And without that strong reticence, we risk utterly demolishing the basis\n> of the \"smart\"ness of \"smart\" contracts --- if a \"smart\" contract cannot\n> reliably be executed, it cannot self-enforce, and if it cannot\n> self-enforce, it is no longer particularly \"smart\".\n>\n\nI don't think that anywhere in the post I advocated for playing fast and\nloose with the rules to introduce any sort of unreliability.\n\nWhat I'm saying is more akin to we can actually improve the \"hardware\" that\nBitcoin runs on to the extent that it actually does give us better ability\nto adjudicate the transfers of value, and we should absolutely and\naggressively pursue that rather than keeping Bitcoin running on a set\nmechanisms that are insufficient to reach the scale, privacy, self custody,\nand decentralization goals we have.\n\n\n\n> ## The N-of-N Rule\n>\n> What is a \"contract\", anyway?\n>\n> A \"contract\" is an agreement between two or more parties.\n> You do not make a contract to yourself, since (we assume) you are\n> completely a single unit (in practice, humans are internally divided into\n> smaller compute modules with slightly different incentives (note: I did not\n> get this information by *personally* dissecting the brains of any humans),\n> hence the \"we assume\").\n\n\n\n> Thus, a contract must by necessity require N participants\n\n\nThis is getting too pedantic about contracts. If you want to go there,\nyou're also missing \"consideration\".\n\nSmart Contracts are really just programs. And you absolutely can enter\nsmart contracts with yourself solely, for example, Vaults (as covered in\nday 10) are an example where you form a contract where you are intended to\nbe the only party.\n\nYou could make the claim that a vault is just an open contract between you\nand some future would be hacker, but the intent is that the contract is\nthere to just safeguard you and those terms should mostly never execute. +\nyou usually want to define contract participants as not universally\nquantified...\n\n>\n> This is of interest since in a reliability perspective, we often accept\n> k-of-n.\n> <snip>\n> But with an N-of-N, *you* are a participant and your input is necessary\n> for the execution of the smart contract, thus you can be *personally*\n> assured that the smart contract *will* be executed faithfully.\n>\n>\nYes I agree that N-N or K-N have uses -- Sapio is designed to work with\narbitrary thresholds in lieu of CTV/other covenant proposals which can be\nused to emulate arbitrary business logic :)\n\n\nHowever, the benefit of the contracts without that is non-interactivity of\nsending. Having everyone online is a major obstacle for things like\ndecentralized coordination free mining pools (kinda, the whole coordination\nfree part). So if you just say \"always do N-of-N\" you basically lose the\nentire thread of\"smart contract capabilities improving the four pillars\n(covered in earlier posts) which solidifies bitcoin's adjudication of\ntransfers of value.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211207/4df0986a/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-12-08T23:23:46",
                "message_text_only": "Good morning Jeremy,\n\n\n> > ## Why would a \"Smart\" contract be \"Smart\"?\n> >\n> > A \"smart\" contract is simply one that somehow self-enforces rather than requires a third party to enforce it.\n> > It is \"smart\" because its execution is done automatically.\n>\n> There are no automatic executing smart contracts on any platform I'm aware of. Bitcoin requires TX submission, same with Eth.\n>\n> Enforcement and execution are different subjects.\n\nNothing really prevents a cryptocurrency system from recording a \"default branch\" and enforcing that later.\nIn Bitcoin terms, nothing fundamentally prevents this redesign:\n\n* A confirmed transaction can include one or more transactions (not part of its wtxid or txid) which spend an output of that confirmed transaction.\n  * Like SegWit, they can be put in a new region that is not visible to pre-softfork nodes, but this new section is committed to in the coinbase.\n* Those extra transactions must be `nLockTime`d to a future blockheight.\n* When the future blockheight arrives, we add those transactions to the mempool.\n  * If the TXO is already spent by then, then they are not put in the mempool.\n\nThat way, at least the timelocked branch can be automatically executed, because the tx can be submitted \"early\".\nThe only real limitation against the above is the amount of resources it would consume on typical nodes.\n\nEven watchtower behavior can be programmed directly into the blockchain layer, i.e. we can put encrypted blobs into the same extra blockspace, with a partial txid key that triggers decryption and putting those transactions in the mempool, etc.\nThus, the line between execution and enforcement blurs.\n\n\nBut that is really beside the point.\n\nThe Real Point is that \"smart\"ness is not a Boolean flag, but a spectrum.\nThe above feature would allow for more \"smart\"ness in contracts, at the cost of increased resource utilization at each node.\nIn this point-of-view, even a paper contract is \"smart\", though less \"smart\" than a typical Bitcoin HTLC.\n\n> > Consider the humble HTLC.\n> > <snip>\n> > This is why the reticence of Bitcoin node operators to change the programming model is a welcome feature of the network.\n> > Any change to the programming model risks the introduction of bugs to the underlying virtual machine that the Bitcoin network presents to contract makers.\n> > And without that strong reticence, we risk utterly demolishing the basis of the \"smart\"ness of \"smart\" contracts --- if a \"smart\" contract cannot reliably be executed, it cannot self-enforce, and if it cannot self-enforce, it is no longer particularly \"smart\".\n>\n> I don't think that anywhere in the post I advocated for playing fast and loose with the rules to introduce any sort of unreliability.\n\nThis is not a criticism of your post, merely an amusing article that fits the post title better.\n\n> What I'm saying is more akin to we can actually improve the \"hardware\" that Bitcoin runs on to the extent that it actually does give us better ability to adjudicate the transfers of value, and we should absolutely and aggressively pursue that rather than keeping Bitcoin running on a set mechanisms that are insufficient to reach the scale, privacy, self custody, and decentralization goals we have.\n\nAgreed.\n\n> \u00a0\n>\n> > ## The N-of-N Rule\n> >\n> > What is a \"contract\", anyway?\n> >\n> > A \"contract\" is an agreement between two or more parties.\n> > You do not make a contract to yourself, since (we assume) you are completely a single unit (in practice, humans are internally divided into smaller compute modules with slightly different incentives (note: I did not get this information by *personally* dissecting the brains of any humans), hence the \"we assume\").\n>\n> \u00a0\n>\n> > Thus, a contract must by necessity require N participants\n>\n> This is getting too pedantic about contracts. If you want to go there, you're also missing \"consideration\".\n>\n> Smart Contracts are really just programs. And you absolutely can enter smart contracts with yourself solely, for example, Vaults (as covered in day 10) are an example where you form a contract where you are intended to be the only party.\n\nNo, because a vault is a contract between your self-of-today and your self-of-tomorrow, with your self-of-today serving as an attorney-in-place of your self-of-tomorrow.\nAfter all, at the next Planck Interval you will die and be replaced with a new entity that only *mostly* agrees with you.\n\n> You could make the claim that a vault is just an open contract between you and some future would be hacker, but the intent is that the contract is there to just safeguard you and those terms should mostly never execute.\u00a0+ you usually want to define contract participants as not universally quantified...\n>\n> > This is of interest since in a reliability perspective, we often accept k-of-n.\n> > <snip>\n> > But with an N-of-N, *you* are a participant and your input is necessary for the execution of the smart contract, thus you can be *personally* assured that the smart contract *will* be executed faithfully.\n>\n> Yes I agree that N-N or K-N have uses -- Sapio is designed to work with arbitrary thresholds in lieu of CTV/other covenant proposals which can be used to emulate arbitrary business logic :)\n>\n> However, the benefit of the contracts without that is non-interactivity of sending. Having everyone online is a major obstacle for things like decentralized coordination free mining pools (kinda, the whole coordination free part). So if you just say \"always do N-of-N\" you basically lose the entire thread of\"smart contract capabilities improving the four pillars (covered in earlier posts) which solidifies bitcoin's adjudication of transfers of value.\n\nThe point really is \"buyer beware\".\nAny k-of-n where you do not puppet at least (n - k + 1) allows you to be evicted and your assets seized by somebody else puppeting k entities.\nBut if you trust that the other entities will not steal from you --- if you do not need the *definite* assurance that the smart contract *will* be executed faithfully --- then go ahead --- do k-of-n.\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Prayank",
                "date": "2021-12-23T09:42:02",
                "message_text_only": "Hi Jeremy,\n\n> Here's the day 6 post: https://rubin.io/bitcoin/2021/12/03/advent-6/, the\ntopic is why smart contracts (in extended form) may be a critical precursor\nto securing Bitcoin's future rather than something we should do after\nmaking the base layer more robust.\n\nThere are few comparisons in this post and links that I consider misleading or incomplete. I had already tweeted this but such discussions are better archived here:\n\nDifference between Bitcoin and Ethereum that is not mentioned on the website which should be considered while looking at fees: 1. Size of blocks added to chain everyday (600 MB) 2. Block limit (500 MB per 10 mins) 3. UTXO vs Account model 4. Failed transactions that pay fees (50k per day) 5. Will these fancy smart contracts work without nodes? No. Where are these nodes running? AWS and Infura has nice articles to highlight their importance 6. Who is paying the fees? Stablecoins, DEX, NFT platforms, CEX and VCs\n\nThere can be lot of other differences that affect the fee market including lot of users in Bitcoin obsessed\u00a0with supply and hodling. Things that have changed in last few years: 1. Darknet markets using Monero 2. Stablecoins stopped using Omni and lot of alternatives exist right now 3. Most of the transactions are related to exchanges. They have started using their own tokens, less exchanges support layer 2 for Bitcoin and users are forced to withdraw some altcoin even if they need bitcoin. 4. Newbies are reading influencers like Elon Musk and happy with their doggy coins to get rich quick/rekt. 5. Bitcoin users or influencers declared DeFi a scam and even sidechains like Liquid, Rootstock do not qualify their purity tests. Projects like DLCs are still not used in any projects with good volume.\n\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211223/115e25b3/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "What's Smart about Smart Contracts",
            "categories": [
                "bitcoin-dev",
                "Bitcoin Advent Calendar"
            ],
            "authors": [
                "ZmnSCPxj",
                "Jeremy",
                "Prayank"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 19127
        }
    },
    {
        "title": "[bitcoin-dev] [Bitcoin Advent Calendar] Review of Smart Contract Concepts",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-07T23:31:06",
                "message_text_only": "This post covers some high-level smart contract concepts that different\nopcodes or proposals could have (or not).\n\nhttps://rubin.io/bitcoin/2021/12/04/advent-7/\n\nInterested to hear about other properties that you think are relevant!\n\nBest,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211207/278d94b2/attachment.html>"
            },
            {
                "author": "Prayank",
                "date": "2021-12-23T11:55:31",
                "message_text_only": "Hi Jeremy,\n\n> This post covers some high-level smart contract concepts that different\nopcodes or proposals could have (or not).\nhttps://rubin.io/bitcoin/2021/12/04/advent-7/\n\nInteresting post. I love the concept of recursion in programming. There is one Indian movie called 'Karthik calling Karthik' which is one of the ways I remember this concept. \n\n> Recursive is pretty much just a fancy way of saying \u201cloops\u201d. This is sometimesalso called \u201cTuring Complete\u201d.\n\nRecently asked one dumb question on Stakexchange after reading a comment on reddit, maybe you can add anything new in this:\n\nhttps://bitcoin.stackexchange.com/questions/111337/loops-in-bitcoin-scripting\n\n> Here, the contract terminates after one canceled request by moving the coinelsewhere.  It\u2019s possible to emulate recursive behavior a limited amount by\u201cunrolling\u201d a loop.\n\nI think this is what I did in the above link where for loop was replaced with if-else statements.\n\n> However, unrolling has it\u2019s limits. When choices(action A or B) are introduced, unrolling can be less effective since you haveand exponential blowup (that means unrolling even like 32 steps might be toomany). However, there are some tricks that can be employed by a clever andcareful programmer to reduce this complexity through, for example, memoization.\n\nAgree with limits and possibility of optimization.\n\n> The key difference being that in the fully enumerated case we must know the exact specifics of the contract and how it will execute, and in the open ended contract case there are bits and pieces we can dynamically specify. If Alice is paid 1 BTC by December 25th, 2021 Midnight, then transfer 100 tokensto one of Bob\u2019s Address B1, B2, or B3 at Bob\u2019s discretion.\n\nInteresting\n\n> Signing the transaction fee rate as a function of locktime\n\nTIL\n\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211223/d53b094f/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Review of Smart Contract Concepts",
            "categories": [
                "bitcoin-dev",
                "Bitcoin Advent Calendar"
            ],
            "authors": [
                "Jeremy",
                "Prayank"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 2532
        }
    },
    {
        "title": "[bitcoin-dev] [Bitcoin Advent Calendar] Sapio Primer",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-07T23:37:42",
                "message_text_only": "This post covers a basic intro to Sapio and links to more complete docs.\nhttps://rubin.io/bitcoin/2021/12/06/advent-9/\n\nI've previously shared Sapio on this list, and there's been a lot of\nprogress since then! I think Sapio is a fantastic system to express Bitcoin\nideas in, even if you don't want to use it for your production\nimplementation. Most of the future posts in the series will make heavy use\nof Sapio so it's worth getting comfortable with, at least for reading.\n\nCheers,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211207/b9350e90/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Sapio Primer",
            "categories": [
                "bitcoin-dev",
                "Bitcoin Advent Calendar"
            ],
            "authors": [
                "Jeremy"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 758
        }
    },
    {
        "title": "[bitcoin-dev] [Bitcoin Advent Calendar] Vaults",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-07T23:40:46",
                "message_text_only": "Last one for today -- sorry for the overload, I had meant to post as the\nseries kicked off...\n\nThis post covers building various vaults/better cold storage using sapio\nhttps://rubin.io/bitcoin/2021/12/07/advent-10/.\n\nIn an earlier post I motivated why self-custody is so critical (see\nhttps://rubin.io/bitcoin/2021/11/30/advent-3/); this post demonstrates how\nSapio + CTV can dramatically enhance what users can do.\n\nCheers, you'll see me in the inbox tomorrow,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211207/d0903acc/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Vaults",
            "categories": [
                "bitcoin-dev",
                "Bitcoin Advent Calendar"
            ],
            "authors": [
                "Jeremy"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 737
        }
    },
    {
        "title": "[bitcoin-dev] [Bitcoin Advent Calendar] Contract Primitives and Upgrades to Bitcoin",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-08T00:36:35",
                "message_text_only": "This post is a mini high level SoK covering basic details of a number of\ndifferent new proposed primitives that folks might find useful -- I think\nthere's less to discuss around this post, since it is at a higher level and\nthe parts contained here could be discussed separately.\n\nIf something isn't on this list, it's an oversight by me and I'd love to\nadd it. The subjective criteria for inclusion/exclusion is if it seems\nsomething the community is actively considering and is relatively well\nresearched.\n\nPost here: https://rubin.io/bitcoin/2021/12/05/advent-8/\n\nbest,\n\nJeremy\n\n(sorry it's out of order sent from the wrong email so it bounced, this is\nday 8)\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211207/2f71f61a/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Contract Primitives and Upgrades to Bitcoin",
            "categories": [
                "bitcoin-dev",
                "Bitcoin Advent Calendar"
            ],
            "authors": [
                "Jeremy"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 929
        }
    },
    {
        "title": "[bitcoin-dev] Take 2: Removing the Dust Limit",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-08T01:28:42",
                "message_text_only": "Bitcoin Devs (+cc lightning-dev),\n\nEarlier this year I proposed allowing 0 value outputs and that was shot\ndown for various reasons, see\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-August/019307.html\n\nI think that there can be a simple carve out now that package relay is\nbeing launched based on my research into covenants from 2017\nhttps://rubin.io/public/pdfs/multi-txn-contracts.pdf.\n\nEssentially, if we allow 0 value outputs BUT require as a matter of policy\n(or consensus, but policy has major advantages) that the output be used as\nan Intermediate Output (that is, in order for the transaction to be\ncreating it to be in the mempool it must be spent by another tx)  with the\nadditional rule that the parent must have a higher feerate after CPFP'ing\nthe parent than the parent alone we can both:\n\n1) Allow 0 value outputs for things like Anchor Outputs (very good for not\ngetting your eltoo/Decker channels pinned by junk witness data using Anchor\nInputs, very good for not getting your channels drained by at-dust outputs)\n2) Not allow 0 value utxos to proliferate long\n3) It still being valid for a 0 value that somehow gets created to be spent\nby the fee paying txn later\n\nJust doing this as a mempool policy also has the benefits of not\nintroducing any new validation rules. Although in general the IUTXO concept\nis very attractive, it complicates mempool :(\n\nI understand this may also be really helpful for CTV based contracts (like\nvault continuation hooks) as well as things like spacechains.\n\nSuch a rule -- if it's not clear -- presupposes a fully working package\nrelay system.\n\nI believe that this addresses all the issues with allowing 0 value outputs\nto be created for the narrow case of immediately spendable outputs.\n\nCheers,\n\nJeremy\n\np.s. why another post today? Thank Greg\nhttps://twitter.com/JeremyRubin/status/1468390561417547780\n\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211207/1a309947/attachment-0001.html>"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2021-12-08T08:34:32",
                "message_text_only": "Hi Jeremy,\n\nRight now, lightning anchor outputs use a 330 sats amount. Each commitment\ntransaction has two such outputs, and only one of them is spent to help the\ntransaction get confirmed, so the other stays there and bloats the utxo set.\nWe allow anyone to spend them after a csv of 16 blocks, in the hope that\nsomeone will claim a batch of them when the fees are low and remove them\nfrom the utxo set. However, that trick wouldn't work with 0-value outputs,\nas\nno-one would ever claim them (doesn't make economical sense).\n\nWe actually need to have two of them to avoid pinning: each participant is\nable to spend only one of these outputs while the parent tx is unconfirmed.\nI believe N-party protocols would likely need N such outputs (not sure).\n\nYou mention a change to the carve-out rule, can you explain it further?\nI believe it would be a necessary step, otherwise 0-value outputs for\nCPFP actually seem worse than low-value ones...\n\nThanks,\nBastien\n\nLe mer. 8 d\u00e9c. 2021 \u00e0 02:29, Jeremy via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Bitcoin Devs (+cc lightning-dev),\n>\n> Earlier this year I proposed allowing 0 value outputs and that was shot\n> down for various reasons, see\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-August/019307.html\n>\n> I think that there can be a simple carve out now that package relay is\n> being launched based on my research into covenants from 2017\n> https://rubin.io/public/pdfs/multi-txn-contracts.pdf.\n>\n> Essentially, if we allow 0 value outputs BUT require as a matter of policy\n> (or consensus, but policy has major advantages) that the output be used as\n> an Intermediate Output (that is, in order for the transaction to be\n> creating it to be in the mempool it must be spent by another tx)  with the\n> additional rule that the parent must have a higher feerate after CPFP'ing\n> the parent than the parent alone we can both:\n>\n> 1) Allow 0 value outputs for things like Anchor Outputs (very good for not\n> getting your eltoo/Decker channels pinned by junk witness data using Anchor\n> Inputs, very good for not getting your channels drained by at-dust outputs)\n> 2) Not allow 0 value utxos to proliferate long\n> 3) It still being valid for a 0 value that somehow gets created to be\n> spent by the fee paying txn later\n>\n> Just doing this as a mempool policy also has the benefits of not\n> introducing any new validation rules. Although in general the IUTXO concept\n> is very attractive, it complicates mempool :(\n>\n> I understand this may also be really helpful for CTV based contracts (like\n> vault continuation hooks) as well as things like spacechains.\n>\n> Such a rule -- if it's not clear -- presupposes a fully working package\n> relay system.\n>\n> I believe that this addresses all the issues with allowing 0 value outputs\n> to be created for the narrow case of immediately spendable outputs.\n>\n> Cheers,\n>\n> Jeremy\n>\n> p.s. why another post today? Thank Greg\n> https://twitter.com/JeremyRubin/status/1468390561417547780\n>\n>\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n> <https://twitter.com/JeremyRubin>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211208/58427839/attachment.html>"
            },
            {
                "author": "Jeremy",
                "date": "2021-12-08T17:18:49",
                "message_text_only": "Bastien,\n\nThe issue is that with Decker Channels you either use SIGHASH_ALL / APO and\ndon't allow adding outs (this protects against certain RBF pinning on the\nroot with bloated wtxid data) and have anchor outputs or you do allow them\nand then are RBF pinnable (but can have change).\n\nAssuming you use anchor outs, then you really can't use dust-threshold\noutputs as it either breaks the ratcheting update validity (if the specific\namount paid to output matters) OR it allows many non-latest updates to\nfully drain the UTXO of any value.\n\nYou can get around the needing for N of them by having a congestion-control\ntree setup in theory; then you only need log(n) data for one bumper, and\n(say) 1.25x the data if all N want to bump. This can be a nice trade-off\nbetween letting everyone bump and not. Since these could be chains of\nIUTXO, they don't need to carry any weight directly.\n\nThe carve out would just be to ensure that CPFP 0 values are known how to\nbe spent.\n\n\n\n\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211208/8dfaefad/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Take 2: Removing the Dust Limit",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Bastien TEINTURIER",
                "Jeremy"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 6826
        }
    },
    {
        "title": "[bitcoin-dev] [Lightning-dev]  Take 2: Removing the Dust Limit",
        "thread_messages": [
            {
                "author": "Ruben Somsen",
                "date": "2021-12-08T10:46:22",
                "message_text_only": "Hi Jeremy,\n\nI brought up the exact same thing at coredev, but unfortunately I came up\nwith a way in which the 0 sat output could still enter the UTXO set under\nthose rules:\n\n- Parent P1 (0 sat per byte) has 2 outputs, one is 0 sat\n- Child C1 spends the 0 sat output for a combined feerate of 1 sat per byte\nand they enter the mempool as a package\n- Child C2 spends the other output of P1 with a really high feerate and\nenters the mempool\n- Fees rise and child C1 falls out of the mempool, leaving the 0 sat output\nunspent\n\nFor this to not be a problem, the 0 sat output needs to provably be the\nonly spendable output. As you pointed out to me a few days ago, having a\nrelative timelock on the other outputs would do the trick (and this happens\nto be true for spacechains), but that will only be provable if all script\nconditions are visible prior to spending time (ruling out p2sh and taproot,\nand conflicting with standardness rules for transactions).\n\nIt's worth noting out that you can't really make a policy rule that says\nthe 0 sat output can't be left unspent (i.e. C1 can't be evicted without\nalso evicting P1), as this would not mirror economically rational behavior\nfor miners (they would get more fees if they evicted C1, so we must assume\nthey will, if the transaction ever reaches them).\n\nThis last example really points out the tricky situation we're dealing\nwith. In my opinion, we'd only want to relay 0 sat outputs if we can\nguarantee that it's never economically profitable to mine them without them\ngetting spent in the same block.\n\nFinally, here's a timestamped link to a diagram that shows where 0 sat\noutputs would be helpful for spacechains (otherwise someone would have to\npay the dust up front for countless outputs):\nhttps://youtu.be/N2ow4Q34Jeg?t=2556\n\nCheers,\nRuben\n\n\n\n\nOn Wed, Dec 8, 2021 at 9:35 AM Bastien TEINTURIER <bastien at acinq.fr> wrote:\n\n> Hi Jeremy,\n>\n> Right now, lightning anchor outputs use a 330 sats amount. Each commitment\n> transaction has two such outputs, and only one of them is spent to help the\n> transaction get confirmed, so the other stays there and bloats the utxo\n> set.\n> We allow anyone to spend them after a csv of 16 blocks, in the hope that\n> someone will claim a batch of them when the fees are low and remove them\n> from the utxo set. However, that trick wouldn't work with 0-value outputs,\n> as\n> no-one would ever claim them (doesn't make economical sense).\n>\n> We actually need to have two of them to avoid pinning: each participant is\n> able to spend only one of these outputs while the parent tx is unconfirmed.\n> I believe N-party protocols would likely need N such outputs (not sure).\n>\n> You mention a change to the carve-out rule, can you explain it further?\n> I believe it would be a necessary step, otherwise 0-value outputs for\n> CPFP actually seem worse than low-value ones...\n>\n> Thanks,\n> Bastien\n>\n> Le mer. 8 d\u00e9c. 2021 \u00e0 02:29, Jeremy via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>\n>> Bitcoin Devs (+cc lightning-dev),\n>>\n>> Earlier this year I proposed allowing 0 value outputs and that was shot\n>> down for various reasons, see\n>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-August/019307.html\n>>\n>> I think that there can be a simple carve out now that package relay is\n>> being launched based on my research into covenants from 2017\n>> https://rubin.io/public/pdfs/multi-txn-contracts.pdf.\n>>\n>> Essentially, if we allow 0 value outputs BUT require as a matter of\n>> policy (or consensus, but policy has major advantages) that the output be\n>> used as an Intermediate Output (that is, in order for the transaction to be\n>> creating it to be in the mempool it must be spent by another tx)  with the\n>> additional rule that the parent must have a higher feerate after CPFP'ing\n>> the parent than the parent alone we can both:\n>>\n>> 1) Allow 0 value outputs for things like Anchor Outputs (very good for\n>> not getting your eltoo/Decker channels pinned by junk witness data using\n>> Anchor Inputs, very good for not getting your channels drained by at-dust\n>> outputs)\n>> 2) Not allow 0 value utxos to proliferate long\n>> 3) It still being valid for a 0 value that somehow gets created to be\n>> spent by the fee paying txn later\n>>\n>> Just doing this as a mempool policy also has the benefits of not\n>> introducing any new validation rules. Although in general the IUTXO concept\n>> is very attractive, it complicates mempool :(\n>>\n>> I understand this may also be really helpful for CTV based contracts\n>> (like vault continuation hooks) as well as things like spacechains.\n>>\n>> Such a rule -- if it's not clear -- presupposes a fully working package\n>> relay system.\n>>\n>> I believe that this addresses all the issues with allowing 0 value\n>> outputs to be created for the narrow case of immediately spendable outputs.\n>>\n>> Cheers,\n>>\n>> Jeremy\n>>\n>> p.s. why another post today? Thank Greg\n>> https://twitter.com/JeremyRubin/status/1468390561417547780\n>>\n>>\n>> --\n>> @JeremyRubin <https://twitter.com/JeremyRubin>\n>> <https://twitter.com/JeremyRubin>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211208/7634150b/attachment-0001.html>"
            },
            {
                "author": "Jeremy",
                "date": "2021-12-08T17:41:34",
                "message_text_only": "IMO this is not a big problem. The problem is not if a 0 value ever enters\nthe mempool, it's if it is never spent. And even if C2/P1 goes in, C1 still\ncan be spent. In fact, it increases it's feerate with P1's confirmation so\nit's somewhat likely it would go in. C2 further has to be pretty expensive\ncompared to C1 in order to be mined when C2 would not be, so the user\ntrying to do this has to pay for it.\n\nIf we're worried it might never be spent again since no incentive, it's\nrational for miners *and users who care about bloat* to save to disk the\ntransaction spending it to resurrect it. The way this can be broken is if\nthe txn has two inputs and that input gets spent separately.\n\nThat said, I think if we can say that taking advantage of keeping the 0\nvalue output will cost you more than if you just made it above dust\nthreshold, it shouldn't be economically rational to not just do a dust\nthreshold value output instead.\n\nSo I'm not sure the extent to which we should bend backwards to make 0\nvalue outputs impossible v.s. making them inconvenient enough to not be\npopular.\n\n\n\n-------------------------------------\nConsensus changes below:\n-------------------------------------\n\nAnother possibility is to have a utxo with drop semantics; if UTXO X with\nsome flag on it is not spent in the block it is created, it expires and can\nnever be spent. This is essentially an inverse timelock, but severely\nlimited to one block and mempool evictions can be handled as if a conflict\nwere mined.\n\nThese types of 0 value outputs could be present just for attaching fee in\nthe mempool but be treated like an op_return otherwise. We could add two\ncases for this: one bare segwit version (just the number, no data) and one\nthat's equivalent to taproot. This covers OP_TRUE anchors very efficiently\nand ones that require a signature as well.\n\nThis is relatively similar to how Transaction Sponsors works, but without\nfull tx graph de-linkage... obviously I think if we'll entertain a\nconsensus change, sponsors makes more sense, but expiring utxos doesn't\nchange as many properties of the tx-graph validation so might be simpler.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211208/5bf1bd84/attachment.html>"
            },
            {
                "author": "Ruben Somsen",
                "date": "2021-12-08T22:51:50",
                "message_text_only": "Hi Jeremy,\n\nThanks for sharing your thoughts.\n\nTo summarize your arguments: the intentionally malicious path to getting\nthe 0 sat output confirmed without being spent is uneconomical compared to\nsimply creating dust outputs. And even if it does happen, the tx spending\nfrom the 0 sat output may still be valid (as long as none of its inputs get\nspent elsewhere) and could eventually get confirmed.\n\nI think those are good points. I do still see a possibility where a user\nnon-maliciously happens to behave in a way that causes all of the above to\nhappen, but it does seem somewhat unlikely.\n\nIt could happen if all of the following occurs:\n1. Another output happens to get spent at a higher feerate (e.g. because an\nabsolute timelock expires and the output gets used)\n2. The tx spending the 0 sat output then happens to not make it into the\nblock due to the lower fees\n3. The user then happens to invalidate the tx that was spending from the 0\nsat output (seems rational at that point)\n\nAssuming this is the only scenario (I am at least not currently aware of\nothers), the question then becomes whether the above is acceptable in order\nto avoid a soft fork.\n\nCheers,\nRuben\n\n\nOn Wed, Dec 8, 2021 at 6:41 PM Jeremy <jlrubin at mit.edu> wrote:\n\n> IMO this is not a big problem. The problem is not if a 0 value ever enters\n> the mempool, it's if it is never spent. And even if C2/P1 goes in, C1 still\n> can be spent. In fact, it increases it's feerate with P1's confirmation so\n> it's somewhat likely it would go in. C2 further has to be pretty expensive\n> compared to C1 in order to be mined when C2 would not be, so the user\n> trying to do this has to pay for it.\n>\n> If we're worried it might never be spent again since no incentive, it's\n> rational for miners *and users who care about bloat* to save to disk the\n> transaction spending it to resurrect it. The way this can be broken is if\n> the txn has two inputs and that input gets spent separately.\n>\n> That said, I think if we can say that taking advantage of keeping the 0\n> value output will cost you more than if you just made it above dust\n> threshold, it shouldn't be economically rational to not just do a dust\n> threshold value output instead.\n>\n> So I'm not sure the extent to which we should bend backwards to make 0\n> value outputs impossible v.s. making them inconvenient enough to not be\n> popular.\n>\n>\n>\n> -------------------------------------\n> Consensus changes below:\n> -------------------------------------\n>\n> Another possibility is to have a utxo with drop semantics; if UTXO X with\n> some flag on it is not spent in the block it is created, it expires and can\n> never be spent. This is essentially an inverse timelock, but severely\n> limited to one block and mempool evictions can be handled as if a conflict\n> were mined.\n>\n> These types of 0 value outputs could be present just for attaching fee in\n> the mempool but be treated like an op_return otherwise. We could add two\n> cases for this: one bare segwit version (just the number, no data) and one\n> that's equivalent to taproot. This covers OP_TRUE anchors very efficiently\n> and ones that require a signature as well.\n>\n> This is relatively similar to how Transaction Sponsors works, but without\n> full tx graph de-linkage... obviously I think if we'll entertain a\n> consensus change, sponsors makes more sense, but expiring utxos doesn't\n> change as many properties of the tx-graph validation so might be simpler.\n>\n>\n>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211208/8f8b4e60/attachment-0001.html>"
            },
            {
                "author": "damian at willtech.com.au",
                "date": "2021-12-09T06:27:04",
                "message_text_only": "Good Afternoon,\n\n'Avoiding a soft-fork' is a political concession. Consensus is none of \nthat.\n\nKING JAMES HRMH\nGreat British Empire\n\nRegards,\nThe Australian\nLORD HIS EXCELLENCY JAMES HRMH (& HMRH)\nof Hougun Manor & Glencoe & British Empire\nMR. Damian A. James Williamson\nWills\n\net al.\n\n\nWilltech\nwww.willtech.com.au\nwww.go-overt.com\nand other projects\n\nearn.com/willtech\nlinkedin.com/in/damianwilliamson\n\n\nm. 0487135719\nf. +61261470192\n\n\nThis email does not constitute a general advice. Please disregard this \nemail if misdelivered.\nOn 2021-12-08 14:51, Ruben Somsen via bitcoin-dev wrote:\n> Hi Jeremy,\n> \n> Thanks for sharing your thoughts.\n> \n> To summarize your arguments: the intentionally malicious path to\n> getting the 0 sat output confirmed without being spent is uneconomical\n> compared to simply creating dust outputs. And even if it does happen,\n> the tx spending from the 0 sat output may still be valid (as long as\n> none of its inputs get spent elsewhere) and could eventually get\n> confirmed.\n> \n> I think those are good points. I do still see a possibility where a\n> user non-maliciously happens to behave in a way that causes all of the\n> above to happen, but it does seem somewhat unlikely.\n> \n> It could happen if all of the following occurs:\n> 1. Another output happens to get spent at a higher feerate (e.g.\n> because an absolute timelock expires and the output gets used)\n> 2. The tx spending the 0 sat output then happens to not make it into\n> the block due to the lower fees\n> 3. The user then happens to invalidate the tx that was spending from\n> the 0 sat output (seems rational at that point)\n> \n> Assuming this is the only scenario (I am at least not currently aware\n> of others), the question then becomes whether the above is acceptable\n> in order to avoid a soft fork.\n> \n> Cheers,\n> Ruben\n> \n> On Wed, Dec 8, 2021 at 6:41 PM Jeremy <jlrubin at mit.edu> wrote:\n> \n>> IMO this is not a big problem. The problem is not if a 0 value ever\n>> enters the mempool, it's if it is never spent. And even if C2/P1\n>> goes in, C1 still can be spent. In fact, it increases it's feerate\n>> with P1's confirmation so it's somewhat likely it would go in. C2\n>> further has to be pretty expensive compared to C1 in order to be\n>> mined when C2 would not be, so the user trying to do this has to pay\n>> for it.\n>> \n>> If we're worried it might never be spent again since no incentive,\n>> it's rational for miners *and users who care about bloat* to save to\n>> disk the transaction spending it to resurrect it. The way this can\n>> be broken is if the txn has two inputs and that input gets spent\n>> separately.\n>> \n>> That said, I think if we can say that taking advantage of keeping\n>> the 0 value output will cost you more than if you just made it above\n>> dust threshold, it shouldn't be economically rational to not just do\n>> a dust threshold value output instead.\n>> \n>> So I'm not sure the extent to which we should bend backwards to make\n>> 0 value outputs impossible v.s. making them inconvenient enough to\n>> not be popular.\n>> \n>> -------------------------------------\n>> Consensus changes below:\n>> -------------------------------------\n>> \n>> Another possibility is to have a utxo with drop semantics; if UTXO X\n>> with some flag on it is not spent in the block it is created, it\n>> expires and can never be spent. This is essentially an inverse\n>> timelock, but severely limited to one block and mempool evictions\n>> can be handled as if a conflict were mined.\n>> \n>> These types of 0 value outputs could be present just for attaching\n>> fee in the mempool but be treated like an op_return otherwise. We\n>> could add two cases for this: one bare segwit version (just the\n>> number, no data) and one that's equivalent to taproot. This covers\n>> OP_TRUE anchors very efficiently and ones that require a signature\n>> as well.\n>> \n>> This is relatively similar to how Transaction Sponsors works, but\n>> without full tx graph de-linkage... obviously I think if we'll\n>> entertain a consensus change, sponsors makes more sense, but\n>> expiring utxos doesn't change as many properties of the tx-graph\n>> validation so might be simpler.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "Take 2: Removing the Dust Limit",
            "categories": [
                "bitcoin-dev",
                "Lightning-dev"
            ],
            "authors": [
                "Jeremy",
                "damian at willtech.com.au",
                "Ruben Somsen"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 15945
        }
    },
    {
        "title": "[bitcoin-dev] [Bitcoin Advent Calendar] Inheritance Schemes",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-08T19:20:54",
                "message_text_only": "Devs,\n\nFor today's post, something near and dear to our hearts: giving our sats to\nour loved ones after we kick the bucket.\n\nsee: https://rubin.io/bitcoin/2021/12/08/advent-11/\n\nSome interesting primitives, hopefully enough to spark a discussion around\ndifferent inheritance schemes that might be useful.\n\nOne note I think is particularly discussion worthy is how the UTXO model\nmakes inheritance backups sort of fundamentally difficult to do v.s. a\nmonolithic account model.\n\nCheers,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211208/f422a334/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Inheritance Schemes",
            "categories": [
                "bitcoin-dev",
                "Bitcoin Advent Calendar"
            ],
            "authors": [
                "Jeremy"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 760
        }
    },
    {
        "title": "[bitcoin-dev] [Bitcoin Advent Calendar]: Congestion Control",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-09T18:24:44",
                "message_text_only": "Today's post is a follow up to some older content about congestion control\n& CTV.\n\nIt's written (as with the rest of the series) to be a bit more approachable\nthan technical, but there are code samples in Sapio of constructing a\npayout tree.\n\ntoday's post:\nhttps://rubin.io/bitcoin/2021/12/09/advent-12/\n\nolder posts:\n- https://utxos.org/analysis/bip_simulation/\n- https://utxos.org/analysis/batching_sim/\n\nGenerally, I think the importance and potential of congestion control is\ncurrently understated. The next couple posts will build on this with Coin\nPools, Mining Pools, and Lighting which also leverage congestion control\nstructures with multi-party opt-outs for added punch. But even in the base\ncase, these congestion control primitives can be really important for large\nvolume large value businesses to close out liabilities reliably without\nbeing impacted too much by transient chain weather. Those types of demand\n(high volume, high value) aren't served well by the lightning network\n(ever) since the large values of flows would be difficult to route and\nmight prefer being deposited directly into cold storage given the amounts\nat stake.\n\nbest,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211209/f7e175bc/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": ": Congestion Control",
            "categories": [
                "bitcoin-dev",
                "Bitcoin Advent Calendar"
            ],
            "authors": [
                "Jeremy"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1436
        }
    },
    {
        "title": "[bitcoin-dev] Rebroadcast mechanism in Bitcoin P2P network",
        "thread_messages": [
            {
                "author": "Prayank",
                "date": "2021-12-10T15:13:02",
                "message_text_only": "Hello World,\n\nI had started working on this blog dedicated to Hal Finney in August: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-August/019367.html\n\nI have been able to track more than 10 Issues and Pull Requests from different Bitcoin projects that are focused on privacy. Wrote 3 blog posts and will write more often as I learn new things. There is a section called 'Hall of Fame' and 7 developers are listed in hof who worked on one or more pull requests that helped improve privacy in Bitcoin projects: Andrew Chow, chimp1984, jmacxx, Luke Dashjr, Samuel Dobson, Vasil Dimov and wpaulino.\n\nLast post is about 'Rebroadcast mechanism' used in Bitcoin full node implementations: https://prayank23.github.io/camouflage//blog/rebroadcast/\n\nProblem: Rebroadcast mechanism used in Bitcoin Core and Knots, rebroadcasts only our transactions. This helps spy nodes to link bitcoin addresses with IP addresses and also know that wallets are enabled for a node.\n\nSolution by Amiti Uttarwar: New rebroadcast mechanism in which transactions are re-broadcasted based on fee rate and mempool age.\n\nI have shared other details, my opinion and links to comments by Suhas Daftuar in the blog post since related pull request has been in draft mode for some time now.\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211210/994e89f4/attachment.html>"
            },
            {
                "author": "damian at willtech.com.au",
                "date": "2021-12-11T03:49:55",
                "message_text_only": "Good Afternoon,\n\nThank-you for your good work cataloguing and writing about the \ncontributions to Bitcoin.\n\nIt is that the solution to privacy is to use privacy-enhancing network \ncommunications, such as TOR. I am not against a mechanism to rebroadcast \ntransactions more robustly if the mempool of adjoining nodes has \nforgotten about them, but the truth is, all transactions originate from \nsome node, and there are methods that allow an individual node to be \nidentified as the likely source of a transaction unless privacy-enabled \nnetworks are utilised. Having a different method to cause rebroadcast \ndoes not obfuscate the origin.\n\nKING JAMES HRMH\nGreat British Empire\n\nRegards,\nThe Australian\nLORD HIS EXCELLENCY JAMES HRMH (& HMRH)\nof Hougun Manor & Glencoe & British Empire\nMR. Damian A. James Williamson\nWills\n\net al.\n\n\nWilltech\nwww.willtech.com.au\nwww.go-overt.com\nduigco.org DUIGCO API\nand other projects\n\n\nm. 0487135719\nf. +61261470192\n\n\nThis email does not constitute a general advice. Please disregard this \nemail if misdelivered.\nOn 2021-12-10 07:13, Prayank via bitcoin-dev wrote:\n> Hello World,\n> \n> I had started working on this blog dedicated to Hal Finney in August:\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-August/019367.html\n> \n> I have been able to track more than 10 Issues and Pull Requests from\n> different Bitcoin projects that are focused on privacy. Wrote 3 blog\n> posts and will write more often as I learn new things. There is a\n> section called 'Hall of Fame' and 7 developers are listed in hof who\n> worked on one or more pull requests that helped improve privacy in\n> Bitcoin projects: Andrew Chow, chimp1984, jmacxx, Luke Dashjr, Samuel\n> Dobson, Vasil Dimov and wpaulino.\n> \n> Last post is about 'Rebroadcast mechanism' used in Bitcoin full node\n> implementations:\n> https://prayank23.github.io/camouflage//blog/rebroadcast/\n> \n> Problem: Rebroadcast mechanism used in Bitcoin Core and Knots,\n> rebroadcasts only our transactions. This helps spy nodes to link\n> bitcoin addresses with IP addresses and also know that wallets are\n> enabled for a node.\n> \n> Solution by Amiti Uttarwar: New rebroadcast mechanism in which\n> transactions are re-broadcasted based on fee rate and mempool age.\n> \n> I have shared other details, my opinion and links to comments by Suhas\n> Daftuar in the blog post since related pull request has been in draft\n> mode for some time now.\n> \n> --\n> \n> Prayank\n> \n> A3B1 E430 2298 178F\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Pieter Wuille",
                "date": "2021-12-11T16:21:21",
                "message_text_only": "> It is that the solution to privacy is to use privacy-enhancing network\n> communications, such as TOR. I am not against a mechanism to rebroadcast\n> transactions more robustly if the mempool of adjoining nodes has\n> forgotten about them, but the truth is, all transactions originate from\n> some node, and there are methods that allow an individual node to be\n> identified as the likely source of a transaction unless privacy-enabled\n> networks are utilised. Having a different method to cause rebroadcast\n> does not obfuscate the origin.\n\nYou're talking about distinct aspects of transaction privacy.\n\nThe rebroadcasting approach as it exists on the network, where wallets are responsible for their own rebroadcasting, directly reveals to your peers a relation between nodes and transactions: whenever any node relays the same transaction twice, it almost certainly implies they are the origin.\n\nThis is just a node-transaction relation, and not necessarily IP-transaction relation. The latter can indeed be avoided by only connecting over Tor, or using other privacy networks, but just hiding the relation with IP addresses isn't sufficient (and has its own downsides; e.g. Tor-only connectivity is far more susceptible to partition/Eclipse/DoS attacks). For example seeing the same node (even without knowing its IP) rebroadcast two transaction lets an observe infer a relation between those transactions, and that too is a privacy leak.\n\nI believe moving to a model where mempools/nodes themselves are responsible for rebroadcasting is a great solution to improving this specific problem, simply because if everyone rebroadcasts, the original author doing it too does not stand out anymore. It isn't \"fixing privacy\", it's fixing a specific leak, one of many, but this isn't a black and white property.\n\nCheers,\n\n--\nPieter"
            },
            {
                "author": "Aymeric Vitte",
                "date": "2021-12-12T11:48:18",
                "message_text_only": "Indeed, I reiterate that using the Tor network for Bitcoin or whatever\nprotocol not related to the Tor Browser (ie browsing and HS) does not\nmake sense, for plenty of reasons\n\nBut using the Tor protocol outside of the Tor network (and inside\nbrowsers for wallets for example) does:\nhttps://github.com/Ayms/node-Tor#presentation and\nhttps://github.com/Ayms/node-Tor#phase-4-and-phase-5, anonymizing nodes\ncan just be already existing bitcoin nodes, example:\nhttps://github.com/bitcoin/bitcoin/pull/18988#issuecomment-646564853\n\n\nLe 11/12/2021 \u00e0 17:21, Pieter Wuille via bitcoin-dev a \u00e9crit :\n>> It is that the solution to privacy is to use privacy-enhancing network\n>> communications, such as TOR. I am not against a mechanism to rebroadcast\n>> transactions more robustly if the mempool of adjoining nodes has\n>> forgotten about them, but the truth is, all transactions originate from\n>> some node, and there are methods that allow an individual node to be\n>> identified as the likely source of a transaction unless privacy-enabled\n>> networks are utilised. Having a different method to cause rebroadcast\n>> does not obfuscate the origin.\n> You're talking about distinct aspects of transaction privacy.\n>\n> The rebroadcasting approach as it exists on the network, where wallets are responsible for their own rebroadcasting, directly reveals to your peers a relation between nodes and transactions: whenever any node relays the same transaction twice, it almost certainly implies they are the origin.\n>\n> This is just a node-transaction relation, and not necessarily IP-transaction relation. The latter can indeed be avoided by only connecting over Tor, or using other privacy networks, but just hiding the relation with IP addresses isn't sufficient (and has its own downsides; e.g. Tor-only connectivity is far more susceptible to partition/Eclipse/DoS attacks). For example seeing the same node (even without knowing its IP) rebroadcast two transaction lets an observe infer a relation between those transactions, and that too is a privacy leak.\n>\n> I believe moving to a model where mempools/nodes themselves are responsible for rebroadcasting is a great solution to improving this specific problem, simply because if everyone rebroadcasts, the original author doing it too does not stand out anymore. It isn't \"fixing privacy\", it's fixing a specific leak, one of many, but this isn't a black and white property.\n>\n> Cheers,\n>\n> --\n> Pieter\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211212/21aee049/attachment.html>"
            },
            {
                "author": "Karl",
                "date": "2021-12-12T13:38:18",
                "message_text_only": "On Sun, Dec 12, 2021, 7:42 AM Aymeric Vitte via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Indeed, I reiterate that using the Tor network for Bitcoin or whatever\n> protocol not related to the Tor Browser (ie browsing and HS) does not make\n> sense, for plenty of reasons\n>\n\nPlease cite this.  It is very hard to believe.\n\nPersonally, I have encountered network blocking of bitcoin peers, and Tor\nis one way to reconnect with the network when this happens.\n\n\nRegardless, reasonable rebroadcasting of nonlocal transactions is a\nhands-down good thing.  This does not make them anonymous, but it does make\nit a little harder to track their origin, and additionally it makes their\ntransmission more robust.\n\nEvery extra measure is a good thing, as everything eventually fails.\n\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211212/b747ded8/attachment.html>"
            },
            {
                "author": "Aymeric Vitte",
                "date": "2021-12-12T14:23:44",
                "message_text_only": "Using the Tor network to bypass censorship for bitcoin can work but is a\nvery poor solution, the Tor network is very centralized, very small,\nwatched and controlled, with plenty of features that do not apply to\nother protocols than those made to be used with the Tor Browser, Pieter\ngave a simple example, that you can solve easily changing the circuits,\nthe problem remains that you really need to be a super expert to escape\nall the dangers of the Tor network, not even sure it's possible unless\nyou use something else than the Tor project code\n\nBelieve it or not, node-Tor is a more than ten years old project (and\nnot a duplicate of the Tor network), so I know what I am talking about,\ndifferent studies of mine show also that the more you try to hide the\nmore you can get caught, even on really decentralized networks like\nbittorrent, unlike another common belief that in such big networks it's\ndifficult to track/deanonymize peers, it is not\n\nExtra measures like rebroadcasting can maybe add something, but back to\nthe previous sentence, extra measures can also help to catch/track you\nif not well designed/thought\n\nWhat I am proposing since years, not only to bitcoin, is to use the Tor\nprotocol independently of the Tor network, and from the browsers also\nacting as nodes (not to be misunderstood with the Tor Browser, this has\nnothing to do) probably someone one day will understand it\n\nLe 12/12/2021 \u00e0 14:38, Karl a \u00e9crit :\n>\n>\n> On Sun, Dec 12, 2021, 7:42 AM Aymeric Vitte via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org\n> <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n>\n>     Indeed, I reiterate that using the Tor network for Bitcoin or\n>     whatever protocol not related to the Tor Browser (ie browsing and\n>     HS) does not make sense, for plenty of reasons\n>\n>\n> Please cite this.  It is very hard to believe.\n>\n> Personally, I have encountered network blocking of bitcoin peers, and\n> Tor is one way to reconnect with the network when this happens.\n>\n>\n> Regardless, reasonable rebroadcasting of nonlocal transactions is a\n> hands-down good thing.  This does not make them anonymous, but it does\n> make it a little harder to track their origin, and additionally it\n> makes their transmission more robust.\n>\n> Every extra measure is a good thing, as everything eventually fails.\n>\n\n-- \nSophia-Antipolis, France\nLinkedIn: https://fr.linkedin.com/in/aymeric-vitte-05855b26\nGitHub : https://www.github.com/Ayms\nMove your coins by yourself (browser version): https://peersm.com/wallet\nBitcoin transactions made simple: https://github.com/Ayms/bitcoin-transactions\ntorrent-live: https://github.com/Ayms/torrent-live\nnode-Tor : https://www.github.com/Ayms/node-Tor\nZcash wallets made simple: https://github.com/Ayms/zcash-wallets\nBitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets\nAnti-spies and private torrents, dynamic blocklist: http://torrent-live.peersm.com\nPeersm : http://www.peersm.com\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211212/4e661983/attachment-0001.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2021-12-12T15:15:16",
                "message_text_only": "On Sunday, December 12th, 2021 at 9:23 AM, Aymeric Vitte via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Using the Tor network to bypass censorship for bitcoin can work but is a very poor solution, the Tor network is very centralized, very small, watched and controlled, with plenty of features that do not apply to other protocols than those made to be used with the Tor Browser, Pieter gave a simple example, that you can solve easily changing the circuits, the problem remains that you really need to be a super expert to escape all the dangers of the Tor network, not even sure it's possible unless you use something else than the Tor project code\n\nFWIW, I wasn't talking about anything related to Tor's protocol or organization at all. What I meant is that because creating a hidden service has ~0 cost, it is trivial for anyone to spin up an arbitrary number of Bitcoin hidden services. Thus, if one runs a node that only connects to hidden services, it is fairly easily eclipsable.\n\nIt's just one example of a downside of (a particular way of) using Tor. That doesn't mean I recommend against using Tor for Bitcoin traffic at all; my point was simply that there are trade-offs, and aspects of privacy of the P2P protocol that Tor does not address, and thus one shouldn't assume that all problems are solved by \"just use Tor\".\n\nCheers,\n\n--\nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211212/f92b2ade/attachment.html>"
            },
            {
                "author": "Aymeric Vitte",
                "date": "2021-12-13T12:40:51",
                "message_text_only": "> It's just one example of a downside of (a particular way of) using\n> Tor. That doesn't mean I recommend against using Tor for Bitcoin\n> traffic at all; my point was simply that there are trade-offs, and\n> aspects of privacy of the P2P protocol that Tor does not address, and\n> thus one shouldn't assume that all problems are solved by \"just use Tor\".\nThere are many downsides since the default behavior of the Tor network\ndoes not apply to p2p networks, another example is a bitcoin node\nexiting transactions (I thought you were referring to this), since the\nsame Tor circuit is used during some time most likely the transactions\nare related to the same node even if we don't know its IP\n\nAccording to the bitcoin github example discussion link I gave, I am not\nsaying that Tor network nodes should not be used, I am saying that they\nshould be used \u00e0 la node-Tor, or more precisely like the github example\nand http://www.peersm.com/Convergence-2020.pdf, one of the main\ndifferences are how behave the first node (ie the originating bitcoin\nnode), HS/RDV, nb of hops, hybrid nodes\n\nAnother drawback is that bitcoin community lets bitcoin nodes operators\nplay the way they like with torrc\n\n@Prayank, regarding js/webrtc my previous answer was not partial, please\nemail in private if you need more, it's just a part of the project (but\nimportant since disruptive), which is already advertised widely\n(bitcoin, ipfs, covid apps, videoconf, etc, there are plenty of links on\ngithub, lists, specs discussion, probably I should reference them), the\nanswer is always the same: \"very interesting, go ahead\", but no, it is\ndesigned to be integrated by the projects, not by myself, and the only\nthing missing to get rid of myself is to release phase4"
            },
            {
                "author": "damian at willtech.com.au",
                "date": "2021-12-12T22:32:21",
                "message_text_only": "Good Afternoon,\n\nYou are right, of course, I did nothing to differentiate between the \nprivacy of the connection of the node, the identification of the public \nIP of the node, and the suspected original of a transaction.\n\nIf I understand, the reason for only the originating node to rebroadcast \nwas because only that node can be authoritative,  but that logic is \nfallible once the transaction is signed - none of the nodes apart from \nthe origin know about the transaction but they always manage to gossip.\n\nAnyway, it is concept ACK from me and I know it has been a concern that \nI have raised previously, I presume some pseudo-random and lengthening \nper attempt length of time between receiving gossip about a transaction \nand rebroadcasting attempts. I have always worked with \n`mempoolexpiry=2160` and `maxmempool=900` and so far as I can presume \nmempool has never been full.\n\nRegards,\nThe Australian\nLORD HIS EXCELLENCY JAMES HRMH (& HMRH)\nof Hougun Manor & Glencoe & British Empire\nMR. Damian A. James Williamson\nWills\n\net al.\n\n\nWilltech\nwww.willtech.com.au\nwww.go-overt.com\nduigco.org DUIGCO API\nand other projects\n\n\nm. 0487135719\nf. +61261470192\n\n\nThis email does not constitute a general advice. Please disregard this \nemail if misdelivered.\nOn 2021-12-11 08:21, Pieter Wuille via bitcoin-dev wrote:\n>> It is that the solution to privacy is to use privacy-enhancing network\n>> communications, such as TOR. I am not against a mechanism to \n>> rebroadcast\n>> transactions more robustly if the mempool of adjoining nodes has\n>> forgotten about them, but the truth is, all transactions originate \n>> from\n>> some node, and there are methods that allow an individual node to be\n>> identified as the likely source of a transaction unless \n>> privacy-enabled\n>> networks are utilised. Having a different method to cause rebroadcast\n>> does not obfuscate the origin.\n> \n> You're talking about distinct aspects of transaction privacy.\n> \n> The rebroadcasting approach as it exists on the network, where wallets\n> are responsible for their own rebroadcasting, directly reveals to your\n> peers a relation between nodes and transactions: whenever any node\n> relays the same transaction twice, it almost certainly implies they\n> are the origin.\n> \n> This is just a node-transaction relation, and not necessarily\n> IP-transaction relation. The latter can indeed be avoided by only\n> connecting over Tor, or using other privacy networks, but just hiding\n> the relation with IP addresses isn't sufficient (and has its own\n> downsides; e.g. Tor-only connectivity is far more susceptible to\n> partition/Eclipse/DoS attacks). For example seeing the same node (even\n> without knowing its IP) rebroadcast two transaction lets an observe\n> infer a relation between those transactions, and that too is a privacy\n> leak.\n> \n> I believe moving to a model where mempools/nodes themselves are\n> responsible for rebroadcasting is a great solution to improving this\n> specific problem, simply because if everyone rebroadcasts, the\n> original author doing it too does not stand out anymore. It isn't\n> \"fixing privacy\", it's fixing a specific leak, one of many, but this\n> isn't a black and white property.\n> \n> Cheers,\n> \n> --\n> Pieter\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Prayank",
                "date": "2021-12-12T18:49:37",
                "message_text_only": "Hi Aymeric,\n> What I am proposing since years, not only to bitcoin, is to use the Tor\nprotocol independently of the Tor network, and from the browsers alsoacting as nodes (not to be misunderstood with the Tor Browser, this hasnothing to do) probably someone one day will understand it\nI understand the concept and like it. However had some issues with use of JavaScript and WebRTC which were partially answered in https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-August/019373.html\n\nI like that you don't give up, passionate about privacy, nodes and contributing to Bitcoin. Not sure if you have commits in Bitcoin Core repository which is one of the weird requirements to get free tickets for open source stage of https://b.tc/conference/\n\nI think presenting your idea with some demo, talking to other developers in community IRL would help your project. If you agree and interested to participate, please apply here: https://b.tc/conference/open-source\n\nI have already requested few people and recommend you to share things about your project in conference. Let me know if you need sponsors for flight tickets as well.\n\nHappy Weekend!\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211212/fd6607c2/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Rebroadcast mechanism in Bitcoin P2P network",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "damian at willtech.com.au",
                "Karl",
                "Aymeric Vitte",
                "Pieter Wuille",
                "Prayank"
            ],
            "messages_count": 10,
            "total_messages_chars_count": 20965
        }
    },
    {
        "title": "[bitcoin-dev] [Bitcoin Advent Calendar] Payment Pools/ Coin Pools",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-10T23:01:34",
                "message_text_only": "This post showcases building payment pools / coin pools* in Sapio!\n\nhttps://rubin.io/bitcoin/2021/12/10/advent-13/\n\nThere will be many more posts in the series that will take this concept a\nlot further and showcase some more advanced things that can be built.\n\nI think that payment pools are incredibly exciting -- we know that it's\ngoing to be tough to give every human a UTXO, even with Lightning. Payment\nPools promise to help compress that chain load into single utxos so that\nusers can be perfectly secure with a proof root and just need to do some\ntransactions to recover their coins. While channels could live inside of\npayment pools, scaling via payment pools without nested channels can be\nnice because there is no degradation of assumptions for the coins inside\nbeing able to broadcast transactions quickly.\n\nPayment pools in Sapio also provide a natural evolution path for things\nlike Rollups (they're essentially federated rollups with unilateral exits),\nwhere state transitions in pools could one day be enforced by either\ncovenants or some sort of ZK system in place of N-of-N signatures.\n\nHopefully this stimulates some folks to muck around with Sapio and\nexperiment creating their own custom Payment Pools! I'd love to see someone\nhack some kind of EVM into the state transition function of a payment pool\n;)\n\nCheers,\n\nJeremy\n\n* we should probably nail down some terminology -- I think Payment Pools /\nCoin Pools are kinda \"generic\" names for the technique, but we should give\nspecific protocols more specific names like payment channels : lightning\nnetwork.\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211210/4358fb3d/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Payment Pools/ Coin Pools",
            "categories": [
                "bitcoin-dev",
                "Bitcoin Advent Calendar"
            ],
            "authors": [
                "Jeremy"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1842
        }
    },
    {
        "title": "[bitcoin-dev] [Bitcoin Advent Calendar] Payment Channels in a CTV+Sapio World",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-11T18:01:31",
                "message_text_only": "hola devs,\n\nThis post details more formally a basic version of payment channels built\non top of CTV/Sapio and the implications of having non-interactive channel\ncreation.\n\nhttps://rubin.io/bitcoin/2021/12/11/advent-14/\n\nI'm personally incredibly bullish on where this concept can go since it\nwould make channel opening much more efficient, especially when paired with\nthe payment pool concept shared the other day.\n\nBest,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211211/1e9e50c8/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Payment Channels in a CTV+Sapio World",
            "categories": [
                "bitcoin-dev",
                "Bitcoin Advent Calendar"
            ],
            "authors": [
                "Jeremy"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 697
        }
    },
    {
        "title": "[bitcoin-dev] [Bitcoin Advent Calendar] Decentralized Coordination Free Mining Pools",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-12T16:43:12",
                "message_text_only": "Howdy, welcome to day 15!\n\nToday's post covers a form of a mining pool that can be operated as sort of\na map-reduce over blocks without any \"infrastructure\".\n\nhttps://rubin.io/bitcoin/2021/12/12/advent-15/\n\nThere's still some really open-ended questions (perhaps for y'all to\nconsider) around how to select an analyze the choice of window and payout\nfunctions, but something like this could alleviate a lot of the\ncentralization pressures typically faced by pools.\n\nNotably, compared to previous attempts, combining the payment pool payout\nwith this concept means that there is practically very little on-chain\noverhead from this approach as the chain-load\nfor including payouts in every block is deferred for future cooperation\namong miners. Although that can be considered cooperation itself, if you\nthink of it like a pipeline, the cooperation happens out of band from\nmining and block production so it really is coordination free to mine.\n\n\nCheers,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211212/ac3c5c3d/attachment.html>"
            },
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2021-12-12T23:14:45",
                "message_text_only": "> how to select an analyze the choice of window\nCurrently, we need 100 blocks to spend the coinbase transaction and I think that should be our \"window\".\n> and payout functions\nSomething like \"miner-based difficulty\" should do the trick. So, each miner is trying to produce its own block, with its own transactions, and its own coinbase reward (based on those transactions, if we want to think ahead and do it right from the start, we should be ready for situation where the basic block reward is zero and the whole coinbase is based only on transaction fees). So, each miner can mine a block with its own coinbase amount (based on transaction fees). Then, that miner should multiply the target by the number of satoshis collected in the coinbase transaction to get \"target per satoshi\". Then, by dividing this target by its block hash, it would produce the number of satoshis that miner should receive.\nSome example:\ndifficulty: 170ba21f\ntarget: 0000000000000000000ba21f0000000000000000000000000000000000000000\ncoinbase: 6.27930034 BTC (627930034 satoshis = 0x256d73b2 satoshis)\ntargetPerSatoshi: 0000000000000000000ba21f0000000000000000000000000000000000000000*0x256d73b2\ntargetPerSatoshi: 000000000001b367c41da68e0000000000000000000000000000000000000000\nsampleShare: 0000000000000000b613738816247a7f4d357cae555996519cf5b543e9b3554b\nminerReward: targetPerSatoshi/sampleShare=0x2642e (156718 satoshis = 0.00156718 BTC for this share)\nBecause we assume that the basic reward will be zero, we assume that all miners will include their own set of transactions. That means, to check if the miner really should receive that reward, checking all transactions is required. Assuming that most of the miners will have similar transactions in their mempools, for each share there is a need to only check transactions that were unknown by that miner. For all other previously validated transactions, miners can store a table like: \"<txid> <fee>\" and then quickly validate if the amount specified in the coinbase transaction is correct.\nTo avoid \"share spam\", we can use something like \"miner-based difficulty\" mentioned above. Everyone knows the network difficulty, but not all miners are directly connected. So, for each connection with each miner in our decentralized pool, we can define a difficulty for each connection. In this way, each node can specify the absolute minimum difficulty, where paying any reward is above the dust limit, and where including that miner makes sense. Then, each miner can produce shares and adjust miner-based difficulty, just to produce for example one share per 10 minutes (or per 30 seconds if we have enough resources to fully validate each share from each miner we are connected with in that time).\nIf we want to include really small miners (like CPU miners), then we need a way to allow sub-satoshi payments. That means, each small miner should mine to a single N-of-N taproot-based multisig, where the whole pot is then splitted between N miners in LN. That means, for example one output of 1000 satoshis can be shared between one million small CPU miners. Then, our target from example above is denominated in millisatoshis.\ntargetPerSatoshi: 000000000001b367c41da68e0000000000000000000000000000000000000000*0x3e8 (1000 in decimal)\ntargetPerMillisatoshi: 0000000006a4cd5613d29ab00000000000000000000000000000000000000000\nOn 2021-12-12 17:43:39 user Jeremy via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\nHowdy, welcome to day 15!\n\u00a0\nToday's post covers a form of a mining pool that can be operated as sort of a map-reduce over blocks without any \"infrastructure\".\n\u00a0\nhttps://rubin.io/bitcoin/2021/12/12/advent-15/\n\u00a0\nThere's still some really open-ended questions (perhaps for y'all to consider) around how to select an analyze the choice of window and payout functions, but something like this could alleviate a lot of the centralization pressures typically faced by pools.\n\u00a0\nNotably, compared to previous attempts, combining the payment pool payout with this concept means that there is practically very little on-chain overhead from this approach as the chain-load\nfor including payouts in every block is deferred for future cooperation among miners. Although that can be considered cooperation itself, if you think of it like a pipeline, the cooperation happens out of band from mining and block production so it really is coordination free to mine.\n\u00a0\nCheers,\n\u00a0\nJeremy\n--\n@JeremyRubin\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211213/a6680906/attachment.html>"
            },
            {
                "author": "Jeremy",
                "date": "2021-12-13T01:31:42",
                "message_text_only": "Hey there!\n\nThanks for your response!\n\nOne of the reasons to pick a longer window of, say, a couple difficulty\nperiods would be that you can make participation in the pool hedge you\nagainst hashrate changes.\n\nYou're absolutely spot on to think about the impact of pooling w.r.t.\nvariance when fees > subsidy. That's not really in the analysis I had in\nthe (old) post, but when the block revenues swing, dcfmp over longer\nperiods can really smooth out the revenues for miners in a great way. This can\nalso help with the \"mind the gap\" problem when there isn't a backlog of\ntransactions, since producing an empty block still has some value (in order\nto incentivize mining transaction at all and not cheating, we need to\nreward txn inclusion as I think you're trying to point out.\n\nSadly, I've read the rest of your email a couple times and I don't really\nget what you're proposing at all. It jumps right into \"things you could\ncompute\". Can you maybe try stating the goals of your payout function, and\nthen demonstrate how what you're proposing meets that? E.g., we want to pay\nmore to miners that do x?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211212/092788f5/attachment.html>"
            },
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2021-12-13T14:10:49",
                "message_text_only": "> Can you maybe try stating the goals of your payout function, and then demonstrate how what you're proposing meets that?\n\u00a0\nThe goals are quite simple: if you are a solo miner, you are trying to mine a block that meets the network difficulty. If you are using some kind of pool, then you are trying to mine N times easier blocks and receive N times lower reward for doing that. If many miners work on similar transactions, then each miner can validate each transaction once and assign transaction fee to transaction id, in this way the coinbase reward can be quickly checked, because you have to check only those transactions, which were unknown to you and for example included only by this miner and not broadcasted. Assuming that most of the transactions will be the same and included by most of the miners, that verification would be quick and can be simplified only to checking \"what is different from what I am mining\".\nAlso, to determine the proper amount of shares received, you can assign a difficulty for each miner. So, if you are connected to eight mining nodes, you can assign a difficulty to each of them, just to limit how much work for each share they can produce to have it accepted and included for payments. It is needed to avoid spamming by producing a lot of shares at difficulty one by bigger miners, they should find it more profitable to create bigger shares, because by accumulating them, it is cheaper to receive one bigger payment than a lot of smaller payments.\nOn 2021-12-13 14:59:58 user Jeremy <jlrubin at mit.edu> wrote:\nHey there!\n\u00a0\nThanks for your response!\n\u00a0\nOne of the reasons to pick a longer window of, say, a couple difficulty periods would be that you can make participation in the pool hedge you against hashrate changes.\n\u00a0\nYou're absolutely spot on to think about the impact of pooling w.r.t. variance when fees > subsidy. That's not really in the analysis I had in the (old) post, but when the block revenues swing, dcfmp over longer periods can really smooth out the revenues for miners in a great way. This\u00a0can also help with the \"mind the gap\" problem when there isn't a backlog of transactions, since producing an empty block still has some value (in order to incentivize mining transaction at all and not cheating, we need to reward txn inclusion as I think you're trying to point out.\n\u00a0\nSadly, I've read the rest of your email a couple times and I don't really get what you're proposing at all. It jumps right into \"things you could compute\". Can you maybe try stating the goals of your payout function, and then demonstrate how what you're proposing meets that? E.g., we want to pay more to miners that do x?\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211213/ac9d832f/attachment.html>"
            },
            {
                "author": "Jeremy",
                "date": "2021-12-14T19:39:06",
                "message_text_only": "Bitcoin didn't invent the concept of pooling:\nhttps://en.wikipedia.org/wiki/Pooling_(resource_management). This is a\nBitcoin Mining Pool, although it may not be your favorite kind, which is\nfixated on specific properties of computing contributions before finding a\nblock. Pooling is just a general technique for aggregating resources to\naccomplish something. If you have another name like pooling that is in\ncommon use for this type of activity I would be more than happy to adopt it.\n\nThis sort of pool can hedge not only against fee rates but also against\nincreases in hashrate since your historical rate 'carries' into the future\nas a function of the window. Further, windows and reward functions can be\ndefined in a myriad of ways that could, e.g., pay less to blocks found in\nmore rapid succession, contributing to the smoothing functionality.\n\nWith respect to sub-block pooling, as described in the article, this sort\nof design also helps with micro-pools being able to split resources\nnon-custodially in every block as a part of the higher order DCFMP. The\npoint is not, as noted, to enable solo mining an S9, but to decrease the\nsize of the minimum viable pool. It's also possible to add, without much\nvalidation or data, some 'uncle block' type mechanism in an incentive\ncompatible way (e.g., add 10 pow-heavy headers on the last block for cost\n48 bytes header + 32 bytes payout key) such that there's an incentive to\ninclude the heaviest ones you've seen, not just your own, that are worth\nfurther study and consideration (particularly because it's non-consensus,\nonly for opt-in participation in the pool).\n\nWith respect to space usage, it seems you wholly reject the viability of a\npayment pool mechanism to cut-through chain space. Is this a critique that\nholds for all Payment Pools, or just in the context of mining? Is there a\nparticular reason why you think it infeasible that \"strongly online\"\ncounterparties would be able to coordinate more efficiently? Is it\npreferable for miners, the nexus of decentralization for Bitcoin, to prefer\nto use custodial services for pooling (which may require KYC/AM) over\nbearing a cost of some extra potential chainload?\n\nLastly, with respect to complexity, the proposal is actually incredibly\nsimple when you take it in a broader context. Non Interactive Channels and\nPayment Pools are useful by themselves, so are the operations to merge them\nand swap balance across them. Therefore most of the complexity in this\nproposal is relying on tools we'll likely see in everyday use in any case,\nDCFMP or no.\n\nJeremy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211214/8d63da6e/attachment.html>"
            },
            {
                "author": "Jeremy",
                "date": "2021-12-14T19:50:33",
                "message_text_only": "I've received some confused messages that whatever I was replying to didn't\ncome through, I've reproduced Bob's e-mail below that I was responding to\nfor context:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n*This, quite simply, is not a \"pool\". A pool is by definition a tool to\nreduceprofit variance by miners by collecting \"weak blocks\" that do not\nmeet thedifficulty target, so as to get a better statistical measure of\neach miner'shashrate, which is used to subdivide profits. These are called\n\"shares\" and areentirely absent here.The only available information here to\ndecide payouts is the blocks themselves,I do not have any higher statistics\nmeasurement to subdivide payments. If Iexpect to earn 3 blocks within the\nwindow, sometimes I will earn 2 and sometimesI will earn 4. Whether I keep\nthe entire coinbase in those 2-4 blocks, or I have100 other miners paying\nme 1/100 as much 100 times, my payment is the same andmust be proportional\nto the number of blocks I mine in the window.  My varianceis not\nreduced.Further, by making miners pay other miners within the window N,\nthis results inN^2 payments to miners which otherwise would have had N\ncoinbase payments. So,this is extremely block-space inefficient for no good\nreason. P2Pool had thesame problem and generated giant coinbases which\ncompeted with fee revenue.\"Congestion control\" makes this somewhat worse\nsince is it is an absoluteincrease in the block space consumed for these\nN^2 payments.The only thing this proposal does do is smooth out fee\nrevenue. While hedging onfee revenue is valuable, this is an extremely\ncomplicated and expensive way togo about it, that simultaneously *reduces*\nfee revenue due to all the extrablock space used for miner payouts.*\n\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211214/9be6323e/attachment.html>"
            },
            {
                "author": "Bob McElrath",
                "date": "2021-12-15T00:12:00",
                "message_text_only": "You are hand waving. Attempting to redefine terms to justify your argument is\nintellectually dishonest. Bitcoin pools have *always* been about variance\nreduction. Your window function fundamentally CANNOT be used to hedge hashrate.\nVarious suggestions below introduce dangerous new games that might be played by\nminers.\n\nThe fact is that the half-baked design you posted is less than useless, and\ndoesn't do anything that anyone wants.\n\nYou are trying to justify CTV by making it be all things to all people. \"When\nall you have is a hammer, every problem looks like a nail\".  Instead I humbly\nsuggest that you pick ONE problem for which CTV is demonstrably the right and\nbest solution, instead of snowing us with a ton of half-baked things that\n*could* be done, and often don't even require CTV, and some (like this one)\nfundamentally don't work. I do like some of your ideas, but if you had to pick\njust one \"use case\", which would it be?\n\nJeremy [jlrubin at mit.edu] wrote:\n> Bitcoin didn't invent the concept of pooling: https://en.wikipedia.org/wiki/\n> Pooling_(resource_management). This is a Bitcoin Mining Pool, although it may\n> not be your favorite kind, which is fixated on specific properties of computing\n> contributions before finding a block. Pooling is just a general technique for\n> aggregating resources to accomplish something. If you have another name like\n> pooling that is in common use for this type of activity I would be more than\n> happy to adopt it.\n> \n> This sort of pool can hedge not only against fee rates but also against\n> increases in hashrate since your historical rate 'carries' into the future as a\n> function of the window. Further, windows and reward functions can be defined in\n> a myriad of ways that could, e.g., pay less to blocks found in more rapid\n> succession, contributing to the smoothing functionality.\n> \n> With respect to sub-block pooling, as described in the article, this sort of\n> design also helps with micro-pools being able to split resources\n> non-custodially in every block as a part of the higher order DCFMP. The point\n> is not, as noted, to enable solo mining an S9, but to decrease the size of the\n> minimum viable pool. It's also possible to add, without much validation or\n> data, some 'uncle block' type mechanism in an incentive compatible way (e.g.,\n> add 10 pow-heavy headers on the last block for cost 48 bytes header + 32 bytes\n> payout key) such that there's an incentive to include the heaviest ones you've\n> seen, not just your own, that are worth further study and consideration\n> (particularly because it's non-consensus, only for opt-in participation in the\n> pool).\n> \n> With respect to space usage, it seems you wholly reject the viability of a\n> payment pool mechanism to cut-through chain space. Is this a critique that\n> holds for all Payment Pools, or just in the context of mining? Is there a\n> particular reason why you think it infeasible that \"strongly online\"\n> counterparties would be able to coordinate more efficiently? Is it preferable\n> for miners, the nexus of decentralization for Bitcoin, to prefer to use\n> custodial services for pooling (which may require KYC/AM) over bearing a cost\n> of some extra potential chainload?\n> \n> Lastly, with respect to complexity, the proposal is actually incredibly simple\n> when you take it in a broader context. Non Interactive Channels and Payment\n> Pools are useful\u00a0by themselves, so are the operations to merge them and swap\n> balance across them. Therefore most of the complexity in this proposal is\n> relying on tools we'll likely see in everyday use in any case, DCFMP or no.\n> \n> Jeremy\n> !DSPAM:61b8f2f5321461582627336!\n--\nCheers, Bob McElrath\n\n\"For every complex problem, there is a solution that is simple, neat, and wrong.\"\n    -- H. L. Mencken"
            },
            {
                "author": "Billy Tetrud",
                "date": "2021-12-15T17:25:15",
                "message_text_only": "Looks like an interesting proposal, but it doesn't seem to quite match the\ngoals you mentioned. As you do mention, this mining pool coordination\ndoesn't get rid of the need for mining pools in the first place. So it\ndoesn't satisfy item 1 on your goal list afaict.\n\nThe primary benefits over what we have today that I can see are:\n1. increased payout regularity, which lowers the viable size of mining\npools, and\n2. Lower on chain footprint through combining pay outs from multiple pools.\n\nAm I missing some?\n\nThese are interesting benefits, but it would be nice if your post was\nclearer on that, since the goals list is not the same as the list of\npotential benefits of this kind of design.\n\nAs far as enabling solo mining, what if this concept were used off chain?\nHave a public network of solo miners who publish \"weak blocks\" to that\nnetwork, and the next 100 (or 1000 etc) nice miners pay you out as long as\nyou're also being nice by following the protocol? All the nice\noptimizations you mentioned about eg combined taproot payouts would apply i\nthink. The only goals this wouldn't satisfy are 3 and 5 since an extra\nnetwork is needed, but to be fair, your proposal requires pools which all\nneed their own extra network anyways.\n\nThe missing piece here would be an ordering of weak blocks to make the\nwindow possible. Or at least a way to determine what blocks should\ndefinitely be part of a particular block's pay out. I could see this being\ndone by a separate ephemeral blockchain (which starts fresh after each\nBitcoin block) that keeps track of which weak blocks have been submitted,\npotentially using the pow already in each block to secure it. Granted that\npiece is a bit half baked, but it seems quite solvable. Wdyt?\n\nOne thing that jumped out at me as not safe is throwing block rewards into\na channel and being able to spend them immediately. There's a reason block\nrewards aren't spendable for a while, and channels don't solve that\nproblem, do they? Why not simply reduce the on chain wait time for spending\nblock rewards at that point? Seems like the consequences would be the same.\n\nOn Tue, Dec 14, 2021, 16:12 Bob McElrath via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> You are hand waving. Attempting to redefine terms to justify your argument\n> is\n> intellectually dishonest. Bitcoin pools have *always* been about variance\n> reduction. Your window function fundamentally CANNOT be used to hedge\n> hashrate.\n> Various suggestions below introduce dangerous new games that might be\n> played by\n> miners.\n>\n> The fact is that the half-baked design you posted is less than useless, and\n> doesn't do anything that anyone wants.\n>\n> You are trying to justify CTV by making it be all things to all people.\n> \"When\n> all you have is a hammer, every problem looks like a nail\".  Instead I\n> humbly\n> suggest that you pick ONE problem for which CTV is demonstrably the right\n> and\n> best solution, instead of snowing us with a ton of half-baked things that\n> *could* be done, and often don't even require CTV, and some (like this one)\n> fundamentally don't work. I do like some of your ideas, but if you had to\n> pick\n> just one \"use case\", which would it be?\n>\n> Jeremy [jlrubin at mit.edu] wrote:\n> > Bitcoin didn't invent the concept of pooling:\n> https://en.wikipedia.org/wiki/\n> > Pooling_(resource_management). This is a Bitcoin Mining Pool, although\n> it may\n> > not be your favorite kind, which is fixated on specific properties of\n> computing\n> > contributions before finding a block. Pooling is just a general\n> technique for\n> > aggregating resources to accomplish something. If you have another name\n> like\n> > pooling that is in common use for this type of activity I would be more\n> than\n> > happy to adopt it.\n> >\n> > This sort of pool can hedge not only against fee rates but also against\n> > increases in hashrate since your historical rate 'carries' into the\n> future as a\n> > function of the window. Further, windows and reward functions can be\n> defined in\n> > a myriad of ways that could, e.g., pay less to blocks found in more rapid\n> > succession, contributing to the smoothing functionality.\n> >\n> > With respect to sub-block pooling, as described in the article, this\n> sort of\n> > design also helps with micro-pools being able to split resources\n> > non-custodially in every block as a part of the higher order DCFMP. The\n> point\n> > is not, as noted, to enable solo mining an S9, but to decrease the size\n> of the\n> > minimum viable pool. It's also possible to add, without much validation\n> or\n> > data, some 'uncle block' type mechanism in an incentive compatible way\n> (e.g.,\n> > add 10 pow-heavy headers on the last block for cost 48 bytes header + 32\n> bytes\n> > payout key) such that there's an incentive to include the heaviest ones\n> you've\n> > seen, not just your own, that are worth further study and consideration\n> > (particularly because it's non-consensus, only for opt-in participation\n> in the\n> > pool).\n> >\n> > With respect to space usage, it seems you wholly reject the viability of\n> a\n> > payment pool mechanism to cut-through chain space. Is this a critique\n> that\n> > holds for all Payment Pools, or just in the context of mining? Is there a\n> > particular reason why you think it infeasible that \"strongly online\"\n> > counterparties would be able to coordinate more efficiently? Is it\n> preferable\n> > for miners, the nexus of decentralization for Bitcoin, to prefer to use\n> > custodial services for pooling (which may require KYC/AM) over bearing a\n> cost\n> > of some extra potential chainload?\n> >\n> > Lastly, with respect to complexity, the proposal is actually incredibly\n> simple\n> > when you take it in a broader context. Non Interactive Channels and\n> Payment\n> > Pools are useful by themselves, so are the operations to merge them and\n> swap\n> > balance across them. Therefore most of the complexity in this proposal is\n> > relying on tools we'll likely see in everyday use in any case, DCFMP or\n> no.\n> >\n> > Jeremy\n> > !DSPAM:61b8f2f5321461582627336!\n> --\n> Cheers, Bob McElrath\n>\n> \"For every complex problem, there is a solution that is simple, neat, and\n> wrong.\"\n>     -- H. L. Mencken\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211215/9210a8de/attachment.html>"
            },
            {
                "author": "Jeremy",
                "date": "2021-12-15T18:39:28",
                "message_text_only": "Hi Billy!\n\nThanks for your response. Some replies inline:\n\n\nOn Wed, Dec 15, 2021 at 10:01 AM Billy Tetrud via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Looks like an interesting proposal, but it doesn't seem to quite match the\n> goals you mentioned. As you do mention, this mining pool coordination\n> doesn't get rid of the need for mining pools in the first place. So it\n> doesn't satisfy item 1 on your goal list afaict.\n>\n\nIt does, actually :) Point 1 was\n\n   1. Funds should not be centrally custodied, ever, if at all\n\nAnd for top-level pool participants there is never any central custody.\nWhat the windows are there (100 blocks, 2016, 4032, etc) is up to the\nspecific implementation which sets limits on how small you can be to\nparticipate.\n\nFurther, for the entities that are too small:\n\nfrom the article:\n*> **The blocks that they mine should use a taproot address/key which is a\nmultisig of some portion of the workshares, that gets included in the\ntop-level pool as a part of Payment Pool.*\n\nThe micro-pools embed a multisig of top-contributors, 'reputable' members,\nor on a rotating basis, as a leaf node to the parent. They then opt-out of\nhaving their leaf channel-ized, as noted.\n\nThis would be fully non-custodial if we always included all miners. The\nissue is that opens up DoS if one miner goes away, so you do want to anchor\naround a few.\n\nIn this mode, you can set the protocol up such that immediately after\ngetting a reward in a block, you should see the chosen nodes for multi-sigs\ndistribute the spoils according to the schedule that is agreed on in the\nblock causing the share to be granted.\n\nthe main issue is data availability, without extra in-band storage local\nmining pools have to track the work shares (which can be committed to in a\nblock) locally for auditing.\n\nThis is not fully non-custodial, but it doesn't have to be centrally\ncustodied by one party. We can multisig immediately after every block (and\nnodes should quit their pool if they don't get sigs quickly perhaps).\nFurther, nodes can hash into multiple pools dividing their risk (modulo\nsybil attack) across many pools.\n\nIf we had stronger covenants (CAT, AMOUNT, DIVIDE/MUL), we could make every\nleaf node commit to payment pools that operate on percents instead of fixed\namounts and we'd be able to handle this in a manner that the payment pools\nwork no matter what amount is assigned to them.\n\n\n\nThe primary benefits over what we have today that I can see are:\n> 1. increased payout regularity, which lowers the viable size of mining\n> pools, and\n> 2. Lower on chain footprint through combining pay outs from multiple pools.\n>\n> Am I missing some?\n>\n> These are interesting benefits, but it would be nice if your post was\n> clearer on that, since the goals list is not the same as the list of\n> potential benefits of this kind of design.\n>\n\nI think I hit all the benefits mentioned:\n\n1. Funds should not be centrally custodied, ever, if at all.\nsee above -- we can do better for smaller miners, but we hit this for\nminers above the threshold.\n\n2. No KYC/AML.\nsee above, payouts are done 'decentralized' by every miner mining to the\npayout\n\n3. No \u201cExtra network\u201d software required.\nyou need the WASM, but do not need any networked software to participate,\nso there are no DoS concerns from participating.\n\nYou do need extra software to e.g. use channels or cut-through multiple\npools, but only after the fact of minding.\n\n4. No blockchain bloat.\n\nVery little, if cut-through + LN works.\n\n\n5. No extra infrastructure.\n\nNot much needed, if anything. I don't really know what 'infrastructure'\nmeans, but I kind of imagined it to mean 'big expensive things' that would\nmake it hard to partake.\n\n\n6. The size of a viable pool should be smaller. Remember our singer \u2013 if\nyou just pool with one other songwriter it doesn\u2019t make your expected time\ntill payout in your lifetime. So bigger the pools, more regular the\npayouts. We want the smallest possible \u201cunits of control\u201d with the most\nregular payouts possible.\n\nI think this works, roughly?\n\n\n> As far as enabling solo mining, what if this concept were used off chain?\n> Have a public network of solo miners who publish \"weak blocks\" to that\n> network, and the next 100 (or 1000 etc) nice miners pay you out as long as\n> you're also being nice by following the protocol? All the nice\n> optimizations you mentioned about eg combined taproot payouts would apply i\n> think. The only goals this wouldn't satisfy are 3 and 5 since an extra\n> network is needed, but to be fair, your proposal requires pools which all\n> need their own extra network anyways.\n>\n> The missing piece here would be an ordering of weak blocks to make the\n> window possible. Or at least a way to determine what blocks should\n> definitely be part of a particular block's pay out. I could see this being\n> done by a separate ephemeral blockchain (which starts fresh after each\n> Bitcoin block) that keeps track of which weak blocks have been submitted,\n> potentially using the pow already in each block to secure it. Granted that\n> piece is a bit half baked, but it seems quite solvable. Wdyt?\n>\n>\nYeah, it's worth thinking more about 100%. This post wasn't a deployable\nthing, more an exposition of a technique. I'd love to see a weak-block\nbased pool, the main issue as noted is the extra software component + data\navailability, but perhaps that's solvable!\n\n\n\n> One thing that jumped out at me as not safe is throwing block rewards into\n> a channel and being able to spend them immediately. There's a reason block\n> rewards aren't spendable for a while, and channels don't solve that\n> problem, do they? Why not simply reduce the on chain wait time for spending\n> block rewards at that point? Seems like the consequences would be the same.\n>\n\nMiners could already do this if they mine to e.g. a multisig (trustlessly\nif they form blocks with their counterparty and pre-sign before hashing).\nAlso in lightning we don't generally have to check that our routes channels\nexist, we don't care as long as they are happy. Thus it doesn't \"hurt\"\nanyone except for the miners who are taking the not fully locked in funds\nrisk, a risk they already take. But that risk can't infect the rest of\nBitcoin's users.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211215/ebd6b6af/attachment-0001.html>"
            },
            {
                "author": "Bob McElrath",
                "date": "2021-12-15T18:51:41",
                "message_text_only": "You basically described Braidpool:\n    https://github.com/pool2win/braidpool\n    https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-August/019371.html\n\nWe're working on this actively and will have some updates soon. Additional\ncontributors are most welcome.\n\nTo your points below:\n1. Increased payout regularity does not lower the viable size of mining pools,\n    because smaller mining pools using this mechanism still have higher variance.\n2. The on-chain footprint is *higher* due to the increased payout regularity.\n\nFor a talk a while back I computed that if you want to have a 1% annual variance\non your profits, you need a pool that is about 20% of the network. The only way\nto get this is with smaller, faster blocks or \"shares\". \n    https://www.youtube.com/watch?v=91WKy7RYHD4\n\nThere is a discussion forum at:\n    https://matrix.to/#/#braidpool:matrix.org\nand a mailing list:\n    https://sourceforge.net/p/braidpool/mailman/\n\nAll of the existing discussion has been happening privately unfortunately but\nI'll try to start using Matrix. ;-)\n\nWe've been discussing alternatives for both fee-rate and hashrate derivatives\nlately. I'm not opposed to using CTV for some of the things in braidpool, if it\nmakes sense. Payment pools and unilateral channel openings may be interesting in\nthis context.\n\nP.S. if anyone wants me to write up a blurb of exactly *why* a construction\nwithout shares cannot be used for hashrate derivatives I can do that, just ask.\nIt comes down to maximum likelihood estimators for the Poisson distribution...\n\nBilly Tetrud [billy.tetrud at gmail.com] wrote:\n> Looks like an interesting proposal, but it doesn't seem to quite match the\n> goals you mentioned. As you do mention, this mining pool coordination doesn't\n> get rid of the need for mining pools in the first place. So it doesn't satisfy\n> item 1 on your goal list afaict.\u00a0\n> \n> The primary benefits over what we have today that I can see are:\n> 1. increased payout regularity, which lowers the viable size of mining pools,\n> and\n> 2. Lower on chain footprint through combining pay outs from multiple pools.\n> \n> Am I missing some?\n> \n> These are interesting benefits, but it would be nice if your post was clearer\n> on that, since the goals list is not the same as the list of potential benefits\n> of this kind of design.\n> \n> As far as enabling solo mining, what if this concept were used off chain? Have\n> a public network of solo miners who publish \"weak blocks\" to that network, and\n> the next 100 (or 1000 etc) nice miners pay you out as long as you're also being\n> nice by following the protocol? All the nice optimizations you mentioned about\n> eg combined taproot payouts would apply i think. The only goals this wouldn't\n> satisfy are 3 and 5 since an extra network is needed, but to be fair, your\n> proposal requires pools which all need their own extra network anyways.\u00a0\n> \n> The missing piece here would be an ordering of weak blocks to make the window\n> possible. Or at least a way to determine what blocks should definitely be part\n> of a particular block's pay out. I could see this being done by a separate\n> ephemeral blockchain (which starts fresh after each Bitcoin block) that keeps\n> track of which weak blocks have been submitted, potentially using the pow\n> already in each block to secure it. Granted that piece is a bit half baked, but\n> it seems quite solvable. Wdyt?\n> \n> One thing that jumped out at me as not safe is throwing block rewards into a\n> channel and being able to spend them immediately. There's a reason block\n> rewards aren't spendable for a while, and channels don't solve that problem, do\n> they? Why not simply reduce the on chain wait time for spending block rewards\n> at that point? Seems like the consequences would be the same.\n> \n> On Tue, Dec 14, 2021, 16:12 Bob McElrath via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n>     You are hand waving. Attempting to redefine terms to justify your argument\n>     is\n>     intellectually dishonest. Bitcoin pools have *always* been about variance\n>     reduction. Your window function fundamentally CANNOT be used to hedge\n>     hashrate.\n>     Various suggestions below introduce dangerous new games that might be\n>     played by\n>     miners.\n> \n>     The fact is that the half-baked design you posted is less than useless, and\n>     doesn't do anything that anyone wants.\n> \n>     You are trying to justify CTV by making it be all things to all people.\n>     \"When\n>     all you have is a hammer, every problem looks like a nail\".\u00a0 Instead I\n>     humbly\n>     suggest that you pick ONE problem for which CTV is demonstrably the right\n>     and\n>     best solution, instead of snowing us with a ton of half-baked things that\n>     *could* be done, and often don't even require CTV, and some (like this one)\n>     fundamentally don't work. I do like some of your ideas, but if you had to\n>     pick\n>     just one \"use case\", which would it be?\n> \n>     Jeremy [jlrubin at mit.edu] wrote:\n>     > Bitcoin didn't invent the concept of pooling: https://en.wikipedia.org/\n>     wiki/\n>     > Pooling_(resource_management). This is a Bitcoin Mining Pool, although it\n>     may\n>     > not be your favorite kind, which is fixated on specific properties of\n>     computing\n>     > contributions before finding a block. Pooling is just a general technique\n>     for\n>     > aggregating resources to accomplish something. If you have another name\n>     like\n>     > pooling that is in common use for this type of activity I would be more\n>     than\n>     > happy to adopt it.\n>     >\n>     > This sort of pool can hedge not only against fee rates but also against\n>     > increases in hashrate since your historical rate 'carries' into the\n>     future as a\n>     > function of the window. Further, windows and reward functions can be\n>     defined in\n>     > a myriad of ways that could, e.g., pay less to blocks found in more rapid\n>     > succession, contributing to the smoothing functionality.\n>     >\n>     > With respect to sub-block pooling, as described in the article, this sort\n>     of\n>     > design also helps with micro-pools being able to split resources\n>     > non-custodially in every block as a part of the higher order DCFMP. The\n>     point\n>     > is not, as noted, to enable solo mining an S9, but to decrease the size\n>     of the\n>     > minimum viable pool. It's also possible to add, without much validation\n>     or\n>     > data, some 'uncle block' type mechanism in an incentive compatible way\n>     (e.g.,\n>     > add 10 pow-heavy headers on the last block for cost 48 bytes header + 32\n>     bytes\n>     > payout key) such that there's an incentive to include the heaviest ones\n>     you've\n>     > seen, not just your own, that are worth further study and consideration\n>     > (particularly because it's non-consensus, only for opt-in participation\n>     in the\n>     > pool).\n>     >\n>     > With respect to space usage, it seems you wholly reject the viability of\n>     a\n>     > payment pool mechanism to cut-through chain space. Is this a critique\n>     that\n>     > holds for all Payment Pools, or just in the context of mining? Is there a\n>     > particular reason why you think it infeasible that \"strongly online\"\n>     > counterparties would be able to coordinate more efficiently? Is it\n>     preferable\n>     > for miners, the nexus of decentralization for Bitcoin, to prefer to use\n>     > custodial services for pooling (which may require KYC/AM) over bearing a\n>     cost\n>     > of some extra potential chainload?\n>     >\n>     > Lastly, with respect to complexity, the proposal is actually incredibly\n>     simple\n>     > when you take it in a broader context. Non Interactive Channels and\n>     Payment\n>     > Pools are useful\u00a0by themselves, so are the operations to merge them and\n>     swap\n>     > balance across them. Therefore most of the complexity in this proposal is\n>     > relying on tools we'll likely see in everyday use in any case, DCFMP or\n>     no.\n>     >\n>     > Jeremy\n>     >\n>     --\n>     Cheers, Bob McElrath\n> \n>     \"For every complex problem, there is a solution that is simple, neat, and\n>     wrong.\"\n>     \u00a0 \u00a0 -- H. L. Mencken\n> \n>     _______________________________________________\n>     bitcoin-dev mailing list\n>     bitcoin-dev at lists.linuxfoundation.org\n>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> !DSPAM:61ba2512470948607217095!\n--\nCheers, Bob McElrath\n\n\"For every complex problem, there is a solution that is simple, neat, and wrong.\"\n    -- H. L. Mencken"
            },
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2021-12-16T09:35:04",
                "message_text_only": "> The missing piece here would be an ordering of weak blocks to make the window possible. Or at least a way to determine what blocks should definitely be part of a particular block's pay out. I could see this being done by a separate ephemeral blockchain (which starts fresh after each Bitcoin block) that keeps track of which weak blocks have been submitted, potentially using the pow already in each block to secure it. Granted that piece is a bit half baked, but it seems quite solvable. Wdyt?\n\u00a0\nI thought about something like that, but there is one problem: how many block headers should be stored per one \"superblock\"? Currently, we have single block header, where the whole coinbase transaction is taken by some mining pool or solo miner. But instead, each miner could submit its own block header. Then, we can collect all headers with the same previous block hash, and distribute block reward between all coinbase transactions in those headers. One \"superblock\" then would be created in a similar way as existing blocks, we would just have block headers instead of transactions. If most transactions inside those blocks will be the same, then each block could be expressed just as a set of transaction hashes, only coinbase transactions or custom, non-broadcasted transactions included by miners will be revealed, everything else will be known.\n> One thing that jumped out at me as not safe is throwing block rewards into a channel and being able to spend them immediately. There's a reason block rewards aren't spendable for a while, and channels don't solve that problem, do they? Why not simply reduce the on chain wait time for spending block rewards at that point? Seems like the consequences would be the same.\nAll coinbase rewards are unspendable for 100 blocks, it is enforced by consensus. It does not matter if there are outputs owned directly by miners, or if there is one huge N-of-N taproot multisig for the whole pool, where every miner signed the closing transaction. The only option to take coins faster I can see is swapping the coins by some LN transaction. But then, the other party can check if some deposit to the LN channel is a part of the coinbase transaction or not, and then decide if it is acceptable to do the swap.\nOn 2021-12-15 19:00:44 user Billy Tetrud via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\nLooks like an interesting proposal, but it doesn't seem to quite match the goals you mentioned. As you do mention, this mining pool coordination doesn't get rid of the need for mining pools in the first place. So it doesn't satisfy item 1 on your goal list afaict.\u00a0 \u00a0\nThe primary benefits over what we have today that I can see are:\n1. increased payout regularity, which lowers the viable size of mining pools, and\n2. Lower on chain footprint through combining pay outs from multiple pools.\n\u00a0\nAm I missing some?\n\u00a0\nThese are interesting benefits, but it would be nice if your post was clearer on that, since the goals list is not the same as the list of potential benefits of this kind of design.\n\u00a0\nAs far as enabling solo mining, what if this concept were used off chain? Have a public network of solo miners who publish \"weak blocks\" to that network, and the next 100 (or 1000 etc) nice miners pay you out as long as you're also being nice by following the protocol? All the nice optimizations you mentioned about eg combined taproot payouts would apply i think. The only goals this wouldn't satisfy are 3 and 5 since an extra network is needed, but to be fair, your proposal requires pools which all need their own extra network anyways.\u00a0\n\u00a0\nThe missing piece here would be an ordering of weak blocks to make the window possible. Or at least a way to determine what blocks should definitely be part of a particular block's pay out. I could see this being done by a separate ephemeral blockchain (which starts fresh after each Bitcoin block) that keeps track of which weak blocks have been submitted, potentially using the pow already in each block to secure it. Granted that piece is a bit half baked, but it seems quite solvable. Wdyt?\n\u00a0\nOne thing that jumped out at me as not safe is throwing block rewards into a channel and being able to spend them immediately. There's a reason block rewards aren't spendable for a while, and channels don't solve that problem, do they? Why not simply reduce the on chain wait time for spending block rewards at that point? Seems like the consequences would be the same.\nOn Tue, Dec 14, 2021, 16:12 Bob McElrath via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\nYou are hand waving. Attempting to redefine terms to justify your argument is\nintellectually dishonest. Bitcoin pools have *always* been about variance\nreduction. Your window function fundamentally CANNOT be used to hedge hashrate.\nVarious suggestions below introduce dangerous new games that might be played by\nminers.\nThe fact is that the half-baked design you posted is less than useless, and\ndoesn't do anything that anyone wants.\nYou are trying to justify CTV by making it be all things to all people. \"When\nall you have is a hammer, every problem looks like a nail\".\u00a0 Instead I humbly\nsuggest that you pick ONE problem for which CTV is demonstrably the right and\nbest solution, instead of snowing us with a ton of half-baked things that\n*could* be done, and often don't even require CTV, and some (like this one)\nfundamentally don't work. I do like some of your ideas, but if you had to pick\njust one \"use case\", which would it be?\nJeremy [jlrubin at mit.edu] wrote:\n> Bitcoin didn't invent the concept of pooling: https://en.wikipedia.org/wiki/\n> Pooling_(resource_management). This is a Bitcoin Mining Pool, although it may\n> not be your favorite kind, which is fixated on specific properties of computing\n> contributions before finding a block. Pooling is just a general technique for\n> aggregating resources to accomplish something. If you have another name like\n> pooling that is in common use for this type of activity I would be more than\n> happy to adopt it.\n>\n> This sort of pool can hedge not only against fee rates but also against\n> increases in hashrate since your historical rate 'carries' into the future as a\n> function of the window. Further, windows and reward functions can be defined in\n> a myriad of ways that could, e.g., pay less to blocks found in more rapid\n> succession, contributing to the smoothing functionality.\n>\n> With respect to sub-block pooling, as described in the article, this sort of\n> design also helps with micro-pools being able to split resources\n> non-custodially in every block as a part of the higher order DCFMP. The point\n> is not, as noted, to enable solo mining an S9, but to decrease the size of the\n> minimum viable pool. It's also possible to add, without much validation or\n> data, some 'uncle block' type mechanism in an incentive compatible way (e.g.,\n> add 10 pow-heavy headers on the last block for cost 48 bytes header + 32 bytes\n> payout key) such that there's an incentive to include the heaviest ones you've\n> seen, not just your own, that are worth further study and consideration\n> (particularly because it's non-consensus, only for opt-in participation in the\n> pool).\n>\n> With respect to space usage, it seems you wholly reject the viability of a\n> payment pool mechanism to cut-through chain space. Is this a critique that\n> holds for all Payment Pools, or just in the context of mining? Is there a\n> particular reason why you think it infeasible that \"strongly online\"\n> counterparties would be able to coordinate more efficiently? Is it preferable\n> for miners, the nexus of decentralization for Bitcoin, to prefer to use\n> custodial services for pooling (which may require KYC/AM) over bearing a cost\n> of some extra potential chainload?\n>\n> Lastly, with respect to complexity, the proposal is actually incredibly simple\n> when you take it in a broader context. Non Interactive Channels and Payment\n> Pools are useful\u00a0by themselves, so are the operations to merge them and swap\n> balance across them. Therefore most of the complexity in this proposal is\n> relying on tools we'll likely see in everyday use in any case, DCFMP or no.\n>\n> Jeremy\n> !DSPAM:61b8f2f5321461582627336!\n--\nCheers, Bob McElrath\n\"For every complex problem, there is a solution that is simple, neat, and wrong.\"\n\u00a0 \u00a0 -- H. L. Mencken\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211216/672b3d09/attachment-0001.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2021-12-16T16:57:03",
                "message_text_only": "@Jeremy\n>   for top-level pool participants there is never any central custody.\n\nI definitely see that. That was actually what I meant when I said the goals\naren't the same as benefits. While your idea definitely satisfies all your\ngoals in a modular way, the fact that it relies on pools means that unless\nthe pools can also satisfy the goals, the total system also doesn't satisfy\nthe goals (even tho the piece of that system you designed does).\n\n> Thus it doesn't \"hurt\" anyone except for the miners who are taking the\nnot fully locked in funds risk\n\nTrue, it only potentially hurts whoever the channel partner is accepting\nthe unspendable coins. And no one can really stop anyone from taking that\nrisk if they really want to. But in that case, its not exactly a fully\nfunctional channel, since recourse mechanisms couldn't be performed.\nWouldn't that open such a channel up to a pretty bad theft possibility?\n\n@Bob\n> Increased payout regularity does not lower the viable size of mining\npools, because smaller mining pools using this mechanism still have higher\nvariance.\n\nYes, smaller mining pools will always have higher variance. However, lower\nvariance has diminishing benefits. Below a certain amount of variance, less\nvariance isn't very valuable. So increased payout regularity does indeed\nlower the viable size of mining pools because a given low-enough level of\nvariance can be achieved with less pool hashpower.\n\n> The on-chain footprint is *higher* due to the increased payout regularity.\n\nThat's a reasonable point. However, I think there is a difference here\nbetween the regularity of rewards vs payouts. Rewards for each miner can be\nmore regular without necessarily increasing the number of on-chain payouts.\nIn fact, theoretically, an individual miner could let their rewards\naccumulate in a pool over many rewards and only redeem when they need the\ncoins for something. The incentive is there for each miner to be judicious\non how much onchain space they take up.\n\n@vjudeu\n\n> how many block headers should be stored per one \"superblock\"?\n\nI was thinking that this would be a separate blockchain with separate\nheaders that progress linearly like a normal blockchain. A block creator\nwould collect together as many blocks that haven't been collected yet into\nthe next superblock (and maybe receive a reward proportional to how many /\nhow much weight they include). This could be done using merge mining, or it\ncould be done using a signing scheme (eg where the block creator signs to\nsay \"I created this superblock\" and have mechanisms to punish those who\nsign multiple superblocks at the same height. For merge mining, I could\neven imagine the data necessary to validate that it has been merge mined\ncould be put into a taproot script branch (creating an invalid script, but\na valid hash of the superblock).\n\n> we can collect all headers with the same previous block hash, and\ndistribute block reward between all coinbase transactions in those headers\n\nExactly.\n\n> we would just have block headers instead of transactions\n\nYeah, I think that would be the way to go. Really, you could even just use\nhashes of the block headers. But the size doesn't matter much because it\nwould be both a small blockchain and an ephemeral one (which can be fully\ndiscarded after all parties have been paid out, or at least their payout\nhas been committed to on the bitcoin blockchain).\n\nOn Thu, Dec 16, 2021 at 1:35 AM <vjudeu at gazeta.pl> wrote:\n\n> > The missing piece here would be an ordering of weak blocks to make the\n> window possible. Or at least a way to determine what blocks should\n> definitely be part of a particular block's pay out. I could see this being\n> done by a separate ephemeral blockchain (which starts fresh after each\n> Bitcoin block) that keeps track of which weak blocks have been submitted,\n> potentially using the pow already in each block to secure it. Granted that\n> piece is a bit half baked, but it seems quite solvable. Wdyt?\n>\n> I thought about something like that, but there is one problem: how many\n> block headers should be stored per one \"superblock\"? Currently, we have\n> single block header, where the whole coinbase transaction is taken by some\n> mining pool or solo miner. But instead, each miner could submit its own\n> block header. Then, we can collect all headers with the same previous block\n> hash, and distribute block reward between all coinbase transactions in\n> those headers. One \"superblock\" then would be created in a similar way as\n> existing blocks, we would just have block headers instead of transactions.\n> If most transactions inside those blocks will be the same, then each block\n> could be expressed just as a set of transaction hashes, only coinbase\n> transactions or custom, non-broadcasted transactions included by miners\n> will be revealed, everything else will be known.\n>\n> > One thing that jumped out at me as not safe is throwing block rewards\n> into a channel and being able to spend them immediately. There's a reason\n> block rewards aren't spendable for a while, and channels don't solve that\n> problem, do they? Why not simply reduce the on chain wait time for spending\n> block rewards at that point? Seems like the consequences would be the same.\n>\n> All coinbase rewards are unspendable for 100 blocks, it is enforced by\n> consensus. It does not matter if there are outputs owned directly by\n> miners, or if there is one huge N-of-N taproot multisig for the whole pool,\n> where every miner signed the closing transaction. The only option to take\n> coins faster I can see is swapping the coins by some LN transaction. But\n> then, the other party can check if some deposit to the LN channel is a part\n> of the coinbase transaction or not, and then decide if it is acceptable to\n> do the swap.\n>\n> On 2021-12-15 19:00:44 user Billy Tetrud via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> Looks like an interesting proposal, but it doesn't seem to quite match the\n> goals you mentioned. As you do mention, this mining pool coordination\n> doesn't get rid of the need for mining pools in the first place. So it\n> doesn't satisfy item 1 on your goal list afaict.\n>\n> The primary benefits over what we have today that I can see are:\n> 1. increased payout regularity, which lowers the viable size of mining\n> pools, and\n> 2. Lower on chain footprint through combining pay outs from multiple pools.\n>\n> Am I missing some?\n>\n> These are interesting benefits, but it would be nice if your post was\n> clearer on that, since the goals list is not the same as the list of\n> potential benefits of this kind of design.\n>\n> As far as enabling solo mining, what if this concept were used off chain?\n> Have a public network of solo miners who publish \"weak blocks\" to that\n> network, and the next 100 (or 1000 etc) nice miners pay you out as long as\n> you're also being nice by following the protocol? All the nice\n> optimizations you mentioned about eg combined taproot payouts would apply i\n> think. The only goals this wouldn't satisfy are 3 and 5 since an extra\n> network is needed, but to be fair, your proposal requires pools which all\n> need their own extra network anyways.\n>\n> The missing piece here would be an ordering of weak blocks to make the\n> window possible. Or at least a way to determine what blocks should\n> definitely be part of a particular block's pay out. I could see this being\n> done by a separate ephemeral blockchain (which starts fresh after each\n> Bitcoin block) that keeps track of which weak blocks have been submitted,\n> potentially using the pow already in each block to secure it. Granted that\n> piece is a bit half baked, but it seems quite solvable. Wdyt?\n>\n> One thing that jumped out at me as not safe is throwing block rewards into\n> a channel and being able to spend them immediately. There's a reason block\n> rewards aren't spendable for a while, and channels don't solve that\n> problem, do they? Why not simply reduce the on chain wait time for spending\n> block rewards at that point? Seems like the consequences would be the same.\n>\n> On Tue, Dec 14, 2021, 16:12 Bob McElrath via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org\n> <http://../NowaWiadomosc/Do/QlIkBFQ6QUFhIVRZX192dnQBeCtCchE6GhA5LFpLCUc7EVZQVl9dQRIXXR8NCBMbCwIGChJXQFxcXEgcFh8UVVVDEyBdVkE9JVRdEwFhYXVlblhVIkosEAszLR5BQVV7U0MID0BAQUgIGh0RHgAMGAMXBQJfW1sdXRQUQUoDQlAiBFY8>>\n> wrote:\n>\n>> You are hand waving. Attempting to redefine terms to justify your\n>> argument is\n>> intellectually dishonest. Bitcoin pools have *always* been about variance\n>> reduction. Your window function fundamentally CANNOT be used to hedge\n>> hashrate.\n>> Various suggestions below introduce dangerous new games that might be\n>> played by\n>> miners.\n>>\n>> The fact is that the half-baked design you posted is less than useless,\n>> and\n>> doesn't do anything that anyone wants.\n>>\n>> You are trying to justify CTV by making it be all things to all people.\n>> \"When\n>> all you have is a hammer, every problem looks like a nail\".  Instead I\n>> humbly\n>> suggest that you pick ONE problem for which CTV is demonstrably the right\n>> and\n>> best solution, instead of snowing us with a ton of half-baked things that\n>> *could* be done, and often don't even require CTV, and some (like this\n>> one)\n>> fundamentally don't work. I do like some of your ideas, but if you had to\n>> pick\n>> just one \"use case\", which would it be?\n>>\n>> Jeremy [jlrubin at mit.edu\n>> <http://../NowaWiadomosc/Do/QlIkBFQ6QUFhIVRZX192dnQBeCtCchEyHxYvIVpLARduChoQSFZQR0NWQVZWJUNRXwMSCRMTBgcWASdWVkpbCxUTQwoWQUdjKVBMGFY3MWMWeU9QBAZtNw%3D%3D>]\n>> wrote:\n>> > Bitcoin didn't invent the concept of pooling:\n>> https://en.wikipedia.org/wiki/\n>> > Pooling_(resource_management). This is a Bitcoin Mining Pool, although\n>> it may\n>> > not be your favorite kind, which is fixated on specific properties of\n>> computing\n>> > contributions before finding a block. Pooling is just a general\n>> technique for\n>> > aggregating resources to accomplish something. If you have another name\n>> like\n>> > pooling that is in common use for this type of activity I would be more\n>> than\n>> > happy to adopt it.\n>> >\n>> > This sort of pool can hedge not only against fee rates but also against\n>> > increases in hashrate since your historical rate 'carries' into the\n>> future as a\n>> > function of the window. Further, windows and reward functions can be\n>> defined in\n>> > a myriad of ways that could, e.g., pay less to blocks found in more\n>> rapid\n>> > succession, contributing to the smoothing functionality.\n>> >\n>> > With respect to sub-block pooling, as described in the article, this\n>> sort of\n>> > design also helps with micro-pools being able to split resources\n>> > non-custodially in every block as a part of the higher order DCFMP. The\n>> point\n>> > is not, as noted, to enable solo mining an S9, but to decrease the size\n>> of the\n>> > minimum viable pool. It's also possible to add, without much validation\n>> or\n>> > data, some 'uncle block' type mechanism in an incentive compatible way\n>> (e.g.,\n>> > add 10 pow-heavy headers on the last block for cost 48 bytes header +\n>> 32 bytes\n>> > payout key) such that there's an incentive to include the heaviest ones\n>> you've\n>> > seen, not just your own, that are worth further study and consideration\n>> > (particularly because it's non-consensus, only for opt-in participation\n>> in the\n>> > pool).\n>> >\n>> > With respect to space usage, it seems you wholly reject the viability\n>> of a\n>> > payment pool mechanism to cut-through chain space. Is this a critique\n>> that\n>> > holds for all Payment Pools, or just in the context of mining? Is there\n>> a\n>> > particular reason why you think it infeasible that \"strongly online\"\n>> > counterparties would be able to coordinate more efficiently? Is it\n>> preferable\n>> > for miners, the nexus of decentralization for Bitcoin, to prefer to use\n>> > custodial services for pooling (which may require KYC/AM) over bearing\n>> a cost\n>> > of some extra potential chainload?\n>> >\n>> > Lastly, with respect to complexity, the proposal is actually incredibly\n>> simple\n>> > when you take it in a broader context. Non Interactive Channels and\n>> Payment\n>> > Pools are useful by themselves, so are the operations to merge them and\n>> swap\n>> > balance across them. Therefore most of the complexity in this proposal\n>> is\n>> > relying on tools we'll likely see in everyday use in any case, DCFMP or\n>> no.\n>> >\n>> > Jeremy\n>> > !DSPAM:61b8f2f5321461582627336!\n>> --\n>> Cheers, Bob McElrath\n>>\n>> \"For every complex problem, there is a solution that is simple, neat, and\n>> wrong.\"\n>>     -- H. L. Mencken\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> <http://../NowaWiadomosc/Do/QlIkBFQ6QUFhIVRZX192dnQBeCtCchE6GhA5LFpLCUc7EVZQVl9dQRIXXR8NCBMbCwIGChJXQFxcXEgcFh8UVVVDEyBdVkE9JVRdEwFhYXVlblhVIkosEAszLR5BQVV7U0MID0BAQUgIGh0RHgAMGAMXBQJfW1sdXRQUQUoDQlAiBFY8>\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211216/d18efb77/attachment-0001.html>"
            },
            {
                "author": "Jeremy",
                "date": "2021-12-17T00:37:09",
                "message_text_only": "high level response:\n\nincluding a small number of block headers (10?) directly as op_return\nmetadata (or something) doesn't have that high overhead necessarily, but\ncould be super effective at helping miners participate with lower hashrate.\nthe reason to include this as on-chain data is so that the mining pool\ndoesn't require any external network software.\n\nthis would balance out the issues if the data is somewhat bounded (e.g., 10\nheaders). what's nice is this data has no consensus meaning as it's client\nside validated by the DCFMP block filter.\n\ninterestingly, the participating pools could 'vote' on how difficult shares\nshould be as a metaparameter to the pool over blocks... but analysis gets\nmore complex with that.\n\ncheers,\n\njeremy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211216/b0d4f253/attachment.html>"
            },
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2021-12-17T06:37:17",
                "message_text_only": "> I was thinking that this would be a separate blockchain with separate headers that progress linearly like a normal blockchain.\nExactly, that's what I called \"superblocks\", where you have a separate chain, just to keep block headers instead of transactions.\n> A block creator would collect together as many blocks that haven't been collected yet into the next superblock (and maybe receive a reward proportional to how many / how much weight they include).\nYou cannot \"catch them all\". If you do, you can end up with a lot of block headers, where each of them has difficulty equal to one. You need some limit, you can limit amount of blocks, you can assign some minimal difficulty, it does not matter that much, but some limit is needed, also because mining on top of the latest superblock should be more profitable than replacing someone else's reward in the previous superblock by your own reward and getting a bigger share in the previous superblock.\n> This could be done using merge mining, or it could be done using a signing scheme (eg where the block creator signs to say \"I created this superblock\" and have mechanisms to punish those who sign multiple superblocks at the same height.\nI would pick merge mining, because it is more compatible with existing mining scheme. Signing sounds more like Proof of Stake and I am trying to avoid that solution. Also, there is no need to sign anything, because you are solo mining where you have your own coinbase transaction or you are mining in a pool, where you have some shared address, and then you cannot produce any incompatible superblock, because the protocol can tell you, which address you should use (and if it is N-of-N taproot multisig and you have some closing transaction, then you can safely mine it).\n> Really, you could even just use hashes of the block headers.\nReplacing transactions with block headers will do the same trick. Each transaction is first hashed with double SHA-256, in exactly the same way as block headers are. If you replace transactions with block headers, you would get a superblock header, then varint saying how many block headers are there, and then you can place all block headers. During superblock merkle tree construction, you will hash all block headers (so you will get block hashes as leaves), and then you will combine block hashes in the same way as transaction hashes are combined.\n>From the Script point of view, you can always use \"OP_SIZE 80 OP_EQUALVERIFY OP_HASH256 <hash> OP_EQUAL\". Then, you can just change the size, just to show which object is hashed. Value 80 will work for block headers, small values below 520 will work for small transactions, value 64 will work for any merkle tree proof, no matter if it is for superblock or normal block. Also, by using block headers instead of hashes, you can prove that at least a proper amount of work was done to produce it, because if you use just hashes, then they could be random.\nOn 2021-12-16 17:57:23 user Billy Tetrud <billy.tetrud at gmail.com> wrote:\n@Jeremy\n>\u00a0 \u00a0for top-level pool participants there is never any central custody.\n\u00a0\nI definitely see that. That was actually what I meant when I said the goals aren't the same as benefits. While your idea definitely satisfies all your goals in a modular way, the fact that it relies on pools means that unless the pools can also satisfy the goals, the total system also doesn't satisfy the goals (even tho the piece of that system you designed does).\u00a0\n\u00a0\n>\u00a0Thus it doesn't \"hurt\" anyone except for the miners who are taking the not fully locked in funds risk\n\u00a0\nTrue, it only potentially hurts whoever the channel partner is accepting the unspendable coins. And no one can really stop anyone from taking that risk if they really want to. But in that case, its not exactly a fully functional channel, since recourse mechanisms couldn't be performed. Wouldn't that open such a channel up to a pretty bad theft possibility?\n\u00a0\n@Bob\n>\u00a0Increased payout regularity does not lower the viable size of mining pools, because smaller mining pools using this mechanism still have higher variance.\n\u00a0\nYes, smaller mining pools will always have higher variance. However, lower variance has diminishing benefits. Below a certain amount of variance, less variance isn't very valuable. So increased payout regularity does indeed lower the viable size of mining pools because a given low-enough level of variance can be achieved with less pool hashpower.\n\u00a0\n> The on-chain footprint is *higher* due to the increased payout regularity.\n\u00a0\nThat's a reasonable point. However, I think there is a difference here between the regularity of rewards vs payouts. Rewards for each miner can be more regular without necessarily increasing the number of on-chain payouts. In fact, theoretically, an individual miner could let their rewards accumulate in a pool over many rewards and only redeem when they need the coins for something. The incentive is there for each miner to be judicious on how much onchain space they take up.\n\u00a0\n@vjudeu\n\u00a0\n> how many block headers should be stored per one \"superblock\"?\n\u00a0\nI was thinking that this would be a separate blockchain with separate headers that progress linearly like a normal blockchain. A block creator would collect together as many blocks that haven't been collected yet into the next superblock (and maybe receive a reward proportional to how many / how much weight they include). This could be done using merge mining, or it could be done using a signing scheme (eg where the block creator signs to say \"I created this superblock\" and have mechanisms to punish those who sign multiple superblocks at the same height. For merge mining, I could even imagine the data necessary to validate that it has been merge mined could be put into a taproot script branch (creating an invalid script, but a valid hash of the superblock).\u00a0\n\u00a0\n> we can collect all headers with the same previous block hash, and distribute block reward between all coinbase transactions in those headers\n\u00a0\nExactly.\n\u00a0\n> we would just have block headers instead of transactions\n\u00a0\nYeah, I think that would be the way to go. Really, you could even just use hashes of the block headers. But the size doesn't matter much because it would be both a small blockchain and an ephemeral one (which can be fully discarded after all parties have been paid out, or at least their payout has been committed to on the bitcoin blockchain).\u00a0\nOn Thu, Dec 16, 2021 at 1:35 AM <vjudeu at gazeta.pl> wrote:\n> The missing piece here would be an ordering of weak blocks to make the window possible. Or at least a way to determine what blocks should definitely be part of a particular block's pay out. I could see this being done by a separate ephemeral blockchain (which starts fresh after each Bitcoin block) that keeps track of which weak blocks have been submitted, potentially using the pow already in each block to secure it. Granted that piece is a bit half baked, but it seems quite solvable. Wdyt?\n\u00a0\nI thought about something like that, but there is one problem: how many block headers should be stored per one \"superblock\"? Currently, we have single block header, where the whole coinbase transaction is taken by some mining pool or solo miner. But instead, each miner could submit its own block header. Then, we can collect all headers with the same previous block hash, and distribute block reward between all coinbase transactions in those headers. One \"superblock\" then would be created in a similar way as existing blocks, we would just have block headers instead of transactions. If most transactions inside those blocks will be the same, then each block could be expressed just as a set of transaction hashes, only coinbase transactions or custom, non-broadcasted transactions included by miners will be revealed, everything else will be known.\n> One thing that jumped out at me as not safe is throwing block rewards into a channel and being able to spend them immediately. There's a reason block rewards aren't spendable for a while, and channels don't solve that problem, do they? Why not simply reduce the on chain wait time for spending block rewards at that point? Seems like the consequences would be the same.\nAll coinbase rewards are unspendable for 100 blocks, it is enforced by consensus. It does not matter if there are outputs owned directly by miners, or if there is one huge N-of-N taproot multisig for the whole pool, where every miner signed the closing transaction. The only option to take coins faster I can see is swapping the coins by some LN transaction. But then, the other party can check if some deposit to the LN channel is a part of the coinbase transaction or not, and then decide if it is acceptable to do the swap.\nOn 2021-12-15 19:00:44 user Billy Tetrud via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\nLooks like an interesting proposal, but it doesn't seem to quite match the goals you mentioned. As you do mention, this mining pool coordination doesn't get rid of the need for mining pools in the first place. So it doesn't satisfy item 1 on your goal list afaict.\u00a0 \u00a0\nThe primary benefits over what we have today that I can see are:\n1. increased payout regularity, which lowers the viable size of mining pools, and\n2. Lower on chain footprint through combining pay outs from multiple pools.\n\u00a0\nAm I missing some?\n\u00a0\nThese are interesting benefits, but it would be nice if your post was clearer on that, since the goals list is not the same as the list of potential benefits of this kind of design.\n\u00a0\nAs far as enabling solo mining, what if this concept were used off chain? Have a public network of solo miners who publish \"weak blocks\" to that network, and the next 100 (or 1000 etc) nice miners pay you out as long as you're also being nice by following the protocol? All the nice optimizations you mentioned about eg combined taproot payouts would apply i think. The only goals this wouldn't satisfy are 3 and 5 since an extra network is needed, but to be fair, your proposal requires pools which all need their own extra network anyways.\u00a0\n\u00a0\nThe missing piece here would be an ordering of weak blocks to make the window possible. Or at least a way to determine what blocks should definitely be part of a particular block's pay out. I could see this being done by a separate ephemeral blockchain (which starts fresh after each Bitcoin block) that keeps track of which weak blocks have been submitted, potentially using the pow already in each block to secure it. Granted that piece is a bit half baked, but it seems quite solvable. Wdyt?\n\u00a0\nOne thing that jumped out at me as not safe is throwing block rewards into a channel and being able to spend them immediately. There's a reason block rewards aren't spendable for a while, and channels don't solve that problem, do they? Why not simply reduce the on chain wait time for spending block rewards at that point? Seems like the consequences would be the same.\nOn Tue, Dec 14, 2021, 16:12 Bob McElrath via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\nYou are hand waving. Attempting to redefine terms to justify your argument is\nintellectually dishonest. Bitcoin pools have *always* been about variance\nreduction. Your window function fundamentally CANNOT be used to hedge hashrate.\nVarious suggestions below introduce dangerous new games that might be played by\nminers.\nThe fact is that the half-baked design you posted is less than useless, and\ndoesn't do anything that anyone wants.\nYou are trying to justify CTV by making it be all things to all people. \"When\nall you have is a hammer, every problem looks like a nail\".\u00a0 Instead I humbly\nsuggest that you pick ONE problem for which CTV is demonstrably the right and\nbest solution, instead of snowing us with a ton of half-baked things that\n*could* be done, and often don't even require CTV, and some (like this one)\nfundamentally don't work. I do like some of your ideas, but if you had to pick\njust one \"use case\", which would it be?\nJeremy [jlrubin at mit.edu] wrote:\n> Bitcoin didn't invent the concept of pooling: https://en.wikipedia.org/wiki/\n> Pooling_(resource_management). This is a Bitcoin Mining Pool, although it may\n> not be your favorite kind, which is fixated on specific properties of computing\n> contributions before finding a block. Pooling is just a general technique for\n> aggregating resources to accomplish something. If you have another name like\n> pooling that is in common use for this type of activity I would be more than\n> happy to adopt it.\n>\n> This sort of pool can hedge not only against fee rates but also against\n> increases in hashrate since your historical rate 'carries' into the future as a\n> function of the window. Further, windows and reward functions can be defined in\n> a myriad of ways that could, e.g., pay less to blocks found in more rapid\n> succession, contributing to the smoothing functionality.\n>\n> With respect to sub-block pooling, as described in the article, this sort of\n> design also helps with micro-pools being able to split resources\n> non-custodially in every block as a part of the higher order DCFMP. The point\n> is not, as noted, to enable solo mining an S9, but to decrease the size of the\n> minimum viable pool. It's also possible to add, without much validation or\n> data, some 'uncle block' type mechanism in an incentive compatible way (e.g.,\n> add 10 pow-heavy headers on the last block for cost 48 bytes header + 32 bytes\n> payout key) such that there's an incentive to include the heaviest ones you've\n> seen, not just your own, that are worth further study and consideration\n> (particularly because it's non-consensus, only for opt-in participation in the\n> pool).\n>\n> With respect to space usage, it seems you wholly reject the viability of a\n> payment pool mechanism to cut-through chain space. Is this a critique that\n> holds for all Payment Pools, or just in the context of mining? Is there a\n> particular reason why you think it infeasible that \"strongly online\"\n> counterparties would be able to coordinate more efficiently? Is it preferable\n> for miners, the nexus of decentralization for Bitcoin, to prefer to use\n> custodial services for pooling (which may require KYC/AM) over bearing a cost\n> of some extra potential chainload?\n>\n> Lastly, with respect to complexity, the proposal is actually incredibly simple\n> when you take it in a broader context. Non Interactive Channels and Payment\n> Pools are useful\u00a0by themselves, so are the operations to merge them and swap\n> balance across them. Therefore most of the complexity in this proposal is\n> relying on tools we'll likely see in everyday use in any case, DCFMP or no.\n>\n> Jeremy\n> !DSPAM:61b8f2f5321461582627336!\n--\nCheers, Bob McElrath\n\"For every complex problem, there is a solution that is simple, neat, and wrong.\"\n\u00a0 \u00a0 -- H. L. Mencken\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211217/84b4855e/attachment-0001.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2021-12-20T17:18:44",
                "message_text_only": "> you can assign some minimal difficulty,\n\nI was assuming that would be part of the plan.\n\n> Signing sounds more like Proof of Stake\n\nAssociating signing with proof of stake and thereby concluding that signing\nis something to avoid sounds like superstitious thinking. A signing scheme\nwith proof of work would clearly not be proof of stake. I would suggest not\ndismissing a design out of hand like that. The benefit of that over merge\nmining would be no extra on chain foot print. What do you think the\ndownsides might be?\n\n> if you use just hashes, then they could be random.\n\nYou're right. Nodes would of course need to validate the Bitcoin block\nheaders being included, so i concede hashing them doesn't gain you anything.\n\n\nOn Thu, Dec 16, 2021, 22:37 <vjudeu at gazeta.pl> wrote:\n\n> > I was thinking that this would be a separate blockchain with separate\n> headers that progress linearly like a normal blockchain.\n>\n> Exactly, that's what I called \"superblocks\", where you have a separate\n> chain, just to keep block headers instead of transactions.\n>\n> > A block creator would collect together as many blocks that haven't been\n> collected yet into the next superblock (and maybe receive a reward\n> proportional to how many / how much weight they include).\n>\n> You cannot \"catch them all\". If you do, you can end up with a lot of block\n> headers, where each of them has difficulty equal to one. You need some\n> limit, you can limit amount of blocks, you can assign some minimal\n> difficulty, it does not matter that much, but some limit is needed, also\n> because mining on top of the latest superblock should be more profitable\n> than replacing someone else's reward in the previous superblock by your own\n> reward and getting a bigger share in the previous superblock.\n>\n> > This could be done using merge mining, or it could be done using a\n> signing scheme (eg where the block creator signs to say \"I created this\n> superblock\" and have mechanisms to punish those who sign multiple\n> superblocks at the same height.\n>\n> I would pick merge mining, because it is more compatible with existing\n> mining scheme. Signing sounds more like Proof of Stake and I am trying to\n> avoid that solution. Also, there is no need to sign anything, because you\n> are solo mining where you have your own coinbase transaction or you are\n> mining in a pool, where you have some shared address, and then you cannot\n> produce any incompatible superblock, because the protocol can tell you,\n> which address you should use (and if it is N-of-N taproot multisig and you\n> have some closing transaction, then you can safely mine it).\n>\n> > Really, you could even just use hashes of the block headers.\n>\n> Replacing transactions with block headers will do the same trick. Each\n> transaction is first hashed with double SHA-256, in exactly the same way as\n> block headers are. If you replace transactions with block headers, you\n> would get a superblock header, then varint saying how many block headers\n> are there, and then you can place all block headers. During superblock\n> merkle tree construction, you will hash all block headers (so you will get\n> block hashes as leaves), and then you will combine block hashes in the same\n> way as transaction hashes are combined.\n>\n> From the Script point of view, you can always use \"OP_SIZE 80\n> OP_EQUALVERIFY OP_HASH256 <hash> OP_EQUAL\". Then, you can just change the\n> size, just to show which object is hashed. Value 80 will work for block\n> headers, small values below 520 will work for small transactions, value 64\n> will work for any merkle tree proof, no matter if it is for superblock or\n> normal block. Also, by using block headers instead of hashes, you can prove\n> that at least a proper amount of work was done to produce it, because if\n> you use just hashes, then they could be random.\n>\n> On 2021-12-16 17:57:23 user Billy Tetrud <billy.tetrud at gmail.com> wrote:\n>\n> @Jeremy\n> >   for top-level pool participants there is never any central custody.\n>\n> I definitely see that. That was actually what I meant when I said the\n> goals aren't the same as benefits. While your idea definitely satisfies all\n> your goals in a modular way, the fact that it relies on pools means that\n> unless the pools can also satisfy the goals, the total system also doesn't\n> satisfy the goals (even tho the piece of that system you designed does).\n>\n> > Thus it doesn't \"hurt\" anyone except for the miners who are taking the\n> not fully locked in funds risk\n>\n> True, it only potentially hurts whoever the channel partner is accepting\n> the unspendable coins. And no one can really stop anyone from taking that\n> risk if they really want to. But in that case, its not exactly a fully\n> functional channel, since recourse mechanisms couldn't be performed.\n> Wouldn't that open such a channel up to a pretty bad theft possibility?\n>\n> @Bob\n> > Increased payout regularity does not lower the viable size of mining\n> pools, because smaller mining pools using this mechanism still have higher\n> variance.\n>\n> Yes, smaller mining pools will always have higher variance. However, lower\n> variance has diminishing benefits. Below a certain amount of variance, less\n> variance isn't very valuable. So increased payout regularity does indeed\n> lower the viable size of mining pools because a given low-enough level of\n> variance can be achieved with less pool hashpower.\n>\n> > The on-chain footprint is *higher* due to the increased payout\n> regularity.\n>\n> That's a reasonable point. However, I think there is a difference here\n> between the regularity of rewards vs payouts. Rewards for each miner can be\n> more regular without necessarily increasing the number of on-chain payouts.\n> In fact, theoretically, an individual miner could let their rewards\n> accumulate in a pool over many rewards and only redeem when they need the\n> coins for something. The incentive is there for each miner to be judicious\n> on how much onchain space they take up.\n>\n> @vjudeu\n>\n> > how many block headers should be stored per one \"superblock\"?\n>\n> I was thinking that this would be a separate blockchain with separate\n> headers that progress linearly like a normal blockchain. A block creator\n> would collect together as many blocks that haven't been collected yet into\n> the next superblock (and maybe receive a reward proportional to how many /\n> how much weight they include). This could be done using merge mining, or it\n> could be done using a signing scheme (eg where the block creator signs to\n> say \"I created this superblock\" and have mechanisms to punish those who\n> sign multiple superblocks at the same height. For merge mining, I could\n> even imagine the data necessary to validate that it has been merge mined\n> could be put into a taproot script branch (creating an invalid script, but\n> a valid hash of the superblock).\n>\n> > we can collect all headers with the same previous block hash, and\n> distribute block reward between all coinbase transactions in those headers\n>\n> Exactly.\n>\n> > we would just have block headers instead of transactions\n>\n> Yeah, I think that would be the way to go. Really, you could even just use\n> hashes of the block headers. But the size doesn't matter much because it\n> would be both a small blockchain and an ephemeral one (which can be fully\n> discarded after all parties have been paid out, or at least their payout\n> has been committed to on the bitcoin blockchain).\n>\n> On Thu, Dec 16, 2021 at 1:35 AM <vjudeu at gazeta.pl\n> <http://../NowaWiadomosc/Do/QlIkBFQ6QUFhIVRZX192dnQBeCtCchEuGRE%2BJkYAEBM5BgkBElIaQgpBQUFBVCVGX18dBRtTEQcBF1UyQUoDEQ0TRQYNQUdjI1hCU0cyajZIblhVZRQcVlEe>>\n> wrote:\n>\n>> > The missing piece here would be an ordering of weak blocks to make the\n>> window possible. Or at least a way to determine what blocks should\n>> definitely be part of a particular block's pay out. I could see this being\n>> done by a separate ephemeral blockchain (which starts fresh after each\n>> Bitcoin block) that keeps track of which weak blocks have been submitted,\n>> potentially using the pow already in each block to secure it. Granted that\n>> piece is a bit half baked, but it seems quite solvable. Wdyt?\n>>\n>> I thought about something like that, but there is one problem: how many\n>> block headers should be stored per one \"superblock\"? Currently, we have\n>> single block header, where the whole coinbase transaction is taken by some\n>> mining pool or solo miner. But instead, each miner could submit its own\n>> block header. Then, we can collect all headers with the same previous block\n>> hash, and distribute block reward between all coinbase transactions in\n>> those headers. One \"superblock\" then would be created in a similar way as\n>> existing blocks, we would just have block headers instead of transactions.\n>> If most transactions inside those blocks will be the same, then each block\n>> could be expressed just as a set of transaction hashes, only coinbase\n>> transactions or custom, non-broadcasted transactions included by miners\n>> will be revealed, everything else will be known.\n>>\n>> > One thing that jumped out at me as not safe is throwing block rewards\n>> into a channel and being able to spend them immediately. There's a reason\n>> block rewards aren't spendable for a while, and channels don't solve that\n>> problem, do they? Why not simply reduce the on chain wait time for spending\n>> block rewards at that point? Seems like the consequences would be the same.\n>>\n>> All coinbase rewards are unspendable for 100 blocks, it is enforced by\n>> consensus. It does not matter if there are outputs owned directly by\n>> miners, or if there is one huge N-of-N taproot multisig for the whole pool,\n>> where every miner signed the closing transaction. The only option to take\n>> coins faster I can see is swapping the coins by some LN transaction. But\n>> then, the other party can check if some deposit to the LN channel is a part\n>> of the coinbase transaction or not, and then decide if it is acceptable to\n>> do the swap.\n>>\n>> On 2021-12-15 19:00:44 user Billy Tetrud via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org\n>> <http://../NowaWiadomosc/Do/QlIkBFQ6QUFhIVRZX192dnQBeCtCchE6GhA5LFpLCUc7EVZQVl9dQRIXXR8NCBMbCwIGChJXQFxcXEgcFh8UVVVDEyBdVkE9JVRdEwFhYXVlblhVIkosEAszLR5BQVV7U0MID0BAQUgIGh0RHgAMGAMXBQJfW1sdXRQUQUoDQlAiBFY8>>\n>> wrote:\n>>\n>> Looks like an interesting proposal, but it doesn't seem to quite match\n>> the goals you mentioned. As you do mention, this mining pool coordination\n>> doesn't get rid of the need for mining pools in the first place. So it\n>> doesn't satisfy item 1 on your goal list afaict.\n>>\n>> The primary benefits over what we have today that I can see are:\n>> 1. increased payout regularity, which lowers the viable size of mining\n>> pools, and\n>> 2. Lower on chain footprint through combining pay outs from multiple\n>> pools.\n>>\n>> Am I missing some?\n>>\n>> These are interesting benefits, but it would be nice if your post was\n>> clearer on that, since the goals list is not the same as the list of\n>> potential benefits of this kind of design.\n>>\n>> As far as enabling solo mining, what if this concept were used off chain?\n>> Have a public network of solo miners who publish \"weak blocks\" to that\n>> network, and the next 100 (or 1000 etc) nice miners pay you out as long as\n>> you're also being nice by following the protocol? All the nice\n>> optimizations you mentioned about eg combined taproot payouts would apply i\n>> think. The only goals this wouldn't satisfy are 3 and 5 since an extra\n>> network is needed, but to be fair, your proposal requires pools which all\n>> need their own extra network anyways.\n>>\n>> The missing piece here would be an ordering of weak blocks to make the\n>> window possible. Or at least a way to determine what blocks should\n>> definitely be part of a particular block's pay out. I could see this being\n>> done by a separate ephemeral blockchain (which starts fresh after each\n>> Bitcoin block) that keeps track of which weak blocks have been submitted,\n>> potentially using the pow already in each block to secure it. Granted that\n>> piece is a bit half baked, but it seems quite solvable. Wdyt?\n>>\n>> One thing that jumped out at me as not safe is throwing block rewards\n>> into a channel and being able to spend them immediately. There's a reason\n>> block rewards aren't spendable for a while, and channels don't solve that\n>> problem, do they? Why not simply reduce the on chain wait time for spending\n>> block rewards at that point? Seems like the consequences would be the same.\n>>\n>> On Tue, Dec 14, 2021, 16:12 Bob McElrath via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org\n>> <http://../NowaWiadomosc/Do/QlIkBFQ6QUFhIVRZX192dnQBeCtCchE6GhA5LFpLCUc7EVZQVl9dQRIXXR8NCBMbCwIGChJXQFxcXEgcFh8UVVVDEyBdVkE9JVRdEwFhYXVlblhVIkosEAszLR5BQVV7U0MID0BAQUgIGh0RHgAMGAMXBQJfW1sdXRQUQUoDQlAiBFY8>>\n>> wrote:\n>>\n>>> You are hand waving. Attempting to redefine terms to justify your\n>>> argument is\n>>> intellectually dishonest. Bitcoin pools have *always* been about variance\n>>> reduction. Your window function fundamentally CANNOT be used to hedge\n>>> hashrate.\n>>> Various suggestions below introduce dangerous new games that might be\n>>> played by\n>>> miners.\n>>>\n>>> The fact is that the half-baked design you posted is less than useless,\n>>> and\n>>> doesn't do anything that anyone wants.\n>>>\n>>> You are trying to justify CTV by making it be all things to all people.\n>>> \"When\n>>> all you have is a hammer, every problem looks like a nail\".  Instead I\n>>> humbly\n>>> suggest that you pick ONE problem for which CTV is demonstrably the\n>>> right and\n>>> best solution, instead of snowing us with a ton of half-baked things that\n>>> *could* be done, and often don't even require CTV, and some (like this\n>>> one)\n>>> fundamentally don't work. I do like some of your ideas, but if you had\n>>> to pick\n>>> just one \"use case\", which would it be?\n>>>\n>>> Jeremy [jlrubin at mit.edu\n>>> <http://../NowaWiadomosc/Do/QlIkBFQ6QUFhIVRZX192dnQBeCtCchEyHxYvIVpLARduChoQSFZQR0NWQVZWJUNRXwMSCRMTBgcWASdWVkpbCxUTQwoWQUdjKVBMGFY3MWMWeU9QBAZtNw%3D%3D>]\n>>> wrote:\n>>> > Bitcoin didn't invent the concept of pooling:\n>>> https://en.wikipedia.org/wiki/\n>>> > Pooling_(resource_management). This is a Bitcoin Mining Pool, although\n>>> it may\n>>> > not be your favorite kind, which is fixated on specific properties of\n>>> computing\n>>> > contributions before finding a block. Pooling is just a general\n>>> technique for\n>>> > aggregating resources to accomplish something. If you have another\n>>> name like\n>>> > pooling that is in common use for this type of activity I would be\n>>> more than\n>>> > happy to adopt it.\n>>> >\n>>> > This sort of pool can hedge not only against fee rates but also against\n>>> > increases in hashrate since your historical rate 'carries' into the\n>>> future as a\n>>> > function of the window. Further, windows and reward functions can be\n>>> defined in\n>>> > a myriad of ways that could, e.g., pay less to blocks found in more\n>>> rapid\n>>> > succession, contributing to the smoothing functionality.\n>>> >\n>>> > With respect to sub-block pooling, as described in the article, this\n>>> sort of\n>>> > design also helps with micro-pools being able to split resources\n>>> > non-custodially in every block as a part of the higher order DCFMP.\n>>> The point\n>>> > is not, as noted, to enable solo mining an S9, but to decrease the\n>>> size of the\n>>> > minimum viable pool. It's also possible to add, without much\n>>> validation or\n>>> > data, some 'uncle block' type mechanism in an incentive compatible way\n>>> (e.g.,\n>>> > add 10 pow-heavy headers on the last block for cost 48 bytes header +\n>>> 32 bytes\n>>> > payout key) such that there's an incentive to include the heaviest\n>>> ones you've\n>>> > seen, not just your own, that are worth further study and consideration\n>>> > (particularly because it's non-consensus, only for opt-in\n>>> participation in the\n>>> > pool).\n>>> >\n>>> > With respect to space usage, it seems you wholly reject the viability\n>>> of a\n>>> > payment pool mechanism to cut-through chain space. Is this a critique\n>>> that\n>>> > holds for all Payment Pools, or just in the context of mining? Is\n>>> there a\n>>> > particular reason why you think it infeasible that \"strongly online\"\n>>> > counterparties would be able to coordinate more efficiently? Is it\n>>> preferable\n>>> > for miners, the nexus of decentralization for Bitcoin, to prefer to use\n>>> > custodial services for pooling (which may require KYC/AM) over bearing\n>>> a cost\n>>> > of some extra potential chainload?\n>>> >\n>>> > Lastly, with respect to complexity, the proposal is actually\n>>> incredibly simple\n>>> > when you take it in a broader context. Non Interactive Channels and\n>>> Payment\n>>> > Pools are useful by themselves, so are the operations to merge them\n>>> and swap\n>>> > balance across them. Therefore most of the complexity in this proposal\n>>> is\n>>> > relying on tools we'll likely see in everyday use in any case, DCFMP\n>>> or no.\n>>> >\n>>> > Jeremy\n>>> > !DSPAM:61b8f2f5321461582627336!\n>>> --\n>>> Cheers, Bob McElrath\n>>>\n>>> \"For every complex problem, there is a solution that is simple, neat,\n>>> and wrong.\"\n>>>     -- H. L. Mencken\n>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> <http://../NowaWiadomosc/Do/QlIkBFQ6QUFhIVRZX192dnQBeCtCchE6GhA5LFpLCUc7EVZQVl9dQRIXXR8NCBMbCwIGChJXQFxcXEgcFh8UVVVDEyBdVkE9JVRdEwFhYXVlblhVIkosEAszLR5BQVV7U0MID0BAQUgIGh0RHgAMGAMXBQJfW1sdXRQUQUoDQlAiBFY8>\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211220/c859e9f2/attachment-0001.html>"
            },
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2021-12-23T11:56:18",
                "message_text_only": "> Associating signing with proof of stake and thereby concluding that signing is something to avoid sounds like superstitious thinking.\nIf you introduce signing into mining, then you will have cases, where someone is powerful enough to produce blocks, but cannot, because signing is needed. Then, your consensus is no longer \"the heaviest chain\", but \"the heaviest signed chain\". That means, your computing power is no longer enough by itself (as today), because to make a block, you also need some kind of \"permission to mine\", because first you sign things (like in signet) and then you mine them. That kind of being \"reliably unreliable\" may be ok for testing, but not for the main network.\n> A signing scheme with proof of work would clearly not be proof of stake.\nIt is not that far from Proof of Stake as you may think. In Proof of Stake, you sign your whale balance with thousands of BTC, and then you can mine a new block, just by putting your coins at stake. In Proof of Work with signing, you also make a signature, and even if there is no amount in satoshis for your signing key, it is still somehow checked, who can sign a block and how many signatures are needed. So, is your private key that allows you to extend the chain is really worth zero satoshis, because there is no balance connected with your address/public key? Not really, it is worth a lot of coins, because it can be used to control the chain.\n> The benefit of that over merge mining would be no extra on chain foot print.\nYour signatures would be that \"extra on chain foot print\". In signet, by default you have 1-of-2 multisig, so you have one signature for every block. In the main chain, there is no need for any foot print, because you can reveal taproot public key and hide other things in tapscript, so your foot print is present only when people will not cooperate.\nOn 2021-12-20 18:18:58 user Billy Tetrud <billy.tetrud at gmail.com> wrote:\n> you can assign some minimal difficulty,\n\u00a0\nI was assuming that would be part of the plan.\n\u00a0\n>\u00a0Signing sounds more like Proof of Stake\n\u00a0\nAssociating signing with proof of stake and thereby concluding that signing is something to avoid sounds like superstitious thinking. A signing scheme with proof of work would clearly not be proof of stake. I would suggest not dismissing a design out of hand like that. The benefit of that over merge mining would be no extra on chain foot print. What do you think the downsides might be?\n\u00a0\n> if you use just hashes, then they could be random.\n\u00a0\nYou're right. Nodes would of course need to validate the Bitcoin block headers being included, so i concede hashing them doesn't gain you anything.\nOn Thu, Dec 16, 2021, 22:37 <vjudeu at gazeta.pl> wrote:\n> I was thinking that this would be a separate blockchain with separate headers that progress linearly like a normal blockchain.\nExactly, that's what I called \"superblocks\", where you have a separate chain, just to keep block headers instead of transactions.\n> A block creator would collect together as many blocks that haven't been collected yet into the next superblock (and maybe receive a reward proportional to how many / how much weight they include).\nYou cannot \"catch them all\". If you do, you can end up with a lot of block headers, where each of them has difficulty equal to one. You need some limit, you can limit amount of blocks, you can assign some minimal difficulty, it does not matter that much, but some limit is needed, also because mining on top of the latest superblock should be more profitable than replacing someone else's reward in the previous superblock by your own reward and getting a bigger share in the previous superblock.\n> This could be done using merge mining, or it could be done using a signing scheme (eg where the block creator signs to say \"I created this superblock\" and have mechanisms to punish those who sign multiple superblocks at the same height.\nI would pick merge mining, because it is more compatible with existing mining scheme. Signing sounds more like Proof of Stake and I am trying to avoid that solution. Also, there is no need to sign anything, because you are solo mining where you have your own coinbase transaction or you are mining in a pool, where you have some shared address, and then you cannot produce any incompatible superblock, because the protocol can tell you, which address you should use (and if it is N-of-N taproot multisig and you have some closing transaction, then you can safely mine it).\n> Really, you could even just use hashes of the block headers.\nReplacing transactions with block headers will do the same trick. Each transaction is first hashed with double SHA-256, in exactly the same way as block headers are. If you replace transactions with block headers, you would get a superblock header, then varint saying how many block headers are there, and then you can place all block headers. During superblock merkle tree construction, you will hash all block headers (so you will get block hashes as leaves), and then you will combine block hashes in the same way as transaction hashes are combined.\n>From the Script point of view, you can always use \"OP_SIZE 80 OP_EQUALVERIFY OP_HASH256 <hash> OP_EQUAL\". Then, you can just change the size, just to show which object is hashed. Value 80 will work for block headers, small values below 520 will work for small transactions, value 64 will work for any merkle tree proof, no matter if it is for superblock or normal block. Also, by using block headers instead of hashes, you can prove that at least a proper amount of work was done to produce it, because if you use just hashes, then they could be random.\nOn 2021-12-16 17:57:23 user Billy Tetrud <billy.tetrud at gmail.com> wrote:\n@Jeremy\n>\u00a0 \u00a0for top-level pool participants there is never any central custody.\n\u00a0\nI definitely see that. That was actually what I meant when I said the goals aren't the same as benefits. While your idea definitely satisfies all your goals in a modular way, the fact that it relies on pools means that unless the pools can also satisfy the goals, the total system also doesn't satisfy the goals (even tho the piece of that system you designed does).\u00a0\n\u00a0\n>\u00a0Thus it doesn't \"hurt\" anyone except for the miners who are taking the not fully locked in funds risk\n\u00a0\nTrue, it only potentially hurts whoever the channel partner is accepting the unspendable coins. And no one can really stop anyone from taking that risk if they really want to. But in that case, its not exactly a fully functional channel, since recourse mechanisms couldn't be performed. Wouldn't that open such a channel up to a pretty bad theft possibility?\n\u00a0\n@Bob\n>\u00a0Increased payout regularity does not lower the viable size of mining pools, because smaller mining pools using this mechanism still have higher variance.\n\u00a0\nYes, smaller mining pools will always have higher variance. However, lower variance has diminishing benefits. Below a certain amount of variance, less variance isn't very valuable. So increased payout regularity does indeed lower the viable size of mining pools because a given low-enough level of variance can be achieved with less pool hashpower.\n\u00a0\n> The on-chain footprint is *higher* due to the increased payout regularity.\n\u00a0\nThat's a reasonable point. However, I think there is a difference here between the regularity of rewards vs payouts. Rewards for each miner can be more regular without necessarily increasing the number of on-chain payouts. In fact, theoretically, an individual miner could let their rewards accumulate in a pool over many rewards and only redeem when they need the coins for something. The incentive is there for each miner to be judicious on how much onchain space they take up.\n\u00a0\n@vjudeu\n\u00a0\n> how many block headers should be stored per one \"superblock\"?\n\u00a0\nI was thinking that this would be a separate blockchain with separate headers that progress linearly like a normal blockchain. A block creator would collect together as many blocks that haven't been collected yet into the next superblock (and maybe receive a reward proportional to how many / how much weight they include). This could be done using merge mining, or it could be done using a signing scheme (eg where the block creator signs to say \"I created this superblock\" and have mechanisms to punish those who sign multiple superblocks at the same height. For merge mining, I could even imagine the data necessary to validate that it has been merge mined could be put into a taproot script branch (creating an invalid script, but a valid hash of the superblock).\u00a0\n\u00a0\n> we can collect all headers with the same previous block hash, and distribute block reward between all coinbase transactions in those headers\n\u00a0\nExactly.\n\u00a0\n> we would just have block headers instead of transactions\n\u00a0\nYeah, I think that would be the way to go. Really, you could even just use hashes of the block headers. But the size doesn't matter much because it would be both a small blockchain and an ephemeral one (which can be fully discarded after all parties have been paid out, or at least their payout has been committed to on the bitcoin blockchain).\u00a0\nOn Thu, Dec 16, 2021 at 1:35 AM <vjudeu at gazeta.pl> wrote:\n> The missing piece here would be an ordering of weak blocks to make the window possible. Or at least a way to determine what blocks should definitely be part of a particular block's pay out. I could see this being done by a separate ephemeral blockchain (which starts fresh after each Bitcoin block) that keeps track of which weak blocks have been submitted, potentially using the pow already in each block to secure it. Granted that piece is a bit half baked, but it seems quite solvable. Wdyt?\n\u00a0\nI thought about something like that, but there is one problem: how many block headers should be stored per one \"superblock\"? Currently, we have single block header, where the whole coinbase transaction is taken by some mining pool or solo miner. But instead, each miner could submit its own block header. Then, we can collect all headers with the same previous block hash, and distribute block reward between all coinbase transactions in those headers. One \"superblock\" then would be created in a similar way as existing blocks, we would just have block headers instead of transactions. If most transactions inside those blocks will be the same, then each block could be expressed just as a set of transaction hashes, only coinbase transactions or custom, non-broadcasted transactions included by miners will be revealed, everything else will be known.\n> One thing that jumped out at me as not safe is throwing block rewards into a channel and being able to spend them immediately. There's a reason block rewards aren't spendable for a while, and channels don't solve that problem, do they? Why not simply reduce the on chain wait time for spending block rewards at that point? Seems like the consequences would be the same.\nAll coinbase rewards are unspendable for 100 blocks, it is enforced by consensus. It does not matter if there are outputs owned directly by miners, or if there is one huge N-of-N taproot multisig for the whole pool, where every miner signed the closing transaction. The only option to take coins faster I can see is swapping the coins by some LN transaction. But then, the other party can check if some deposit to the LN channel is a part of the coinbase transaction or not, and then decide if it is acceptable to do the swap.\nOn 2021-12-15 19:00:44 user Billy Tetrud via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\nLooks like an interesting proposal, but it doesn't seem to quite match the goals you mentioned. As you do mention, this mining pool coordination doesn't get rid of the need for mining pools in the first place. So it doesn't satisfy item 1 on your goal list afaict.\u00a0 \u00a0\nThe primary benefits over what we have today that I can see are:\n1. increased payout regularity, which lowers the viable size of mining pools, and\n2. Lower on chain footprint through combining pay outs from multiple pools.\n\u00a0\nAm I missing some?\n\u00a0\nThese are interesting benefits, but it would be nice if your post was clearer on that, since the goals list is not the same as the list of potential benefits of this kind of design.\n\u00a0\nAs far as enabling solo mining, what if this concept were used off chain? Have a public network of solo miners who publish \"weak blocks\" to that network, and the next 100 (or 1000 etc) nice miners pay you out as long as you're also being nice by following the protocol? All the nice optimizations you mentioned about eg combined taproot payouts would apply i think. The only goals this wouldn't satisfy are 3 and 5 since an extra network is needed, but to be fair, your proposal requires pools which all need their own extra network anyways.\u00a0\n\u00a0\nThe missing piece here would be an ordering of weak blocks to make the window possible. Or at least a way to determine what blocks should definitely be part of a particular block's pay out. I could see this being done by a separate ephemeral blockchain (which starts fresh after each Bitcoin block) that keeps track of which weak blocks have been submitted, potentially using the pow already in each block to secure it. Granted that piece is a bit half baked, but it seems quite solvable. Wdyt?\n\u00a0\nOne thing that jumped out at me as not safe is throwing block rewards into a channel and being able to spend them immediately. There's a reason block rewards aren't spendable for a while, and channels don't solve that problem, do they? Why not simply reduce the on chain wait time for spending block rewards at that point? Seems like the consequences would be the same.\nOn Tue, Dec 14, 2021, 16:12 Bob McElrath via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\nYou are hand waving. Attempting to redefine terms to justify your argument is\nintellectually dishonest. Bitcoin pools have *always* been about variance\nreduction. Your window function fundamentally CANNOT be used to hedge hashrate.\nVarious suggestions below introduce dangerous new games that might be played by\nminers.\nThe fact is that the half-baked design you posted is less than useless, and\ndoesn't do anything that anyone wants.\nYou are trying to justify CTV by making it be all things to all people. \"When\nall you have is a hammer, every problem looks like a nail\".\u00a0 Instead I humbly\nsuggest that you pick ONE problem for which CTV is demonstrably the right and\nbest solution, instead of snowing us with a ton of half-baked things that\n*could* be done, and often don't even require CTV, and some (like this one)\nfundamentally don't work. I do like some of your ideas, but if you had to pick\njust one \"use case\", which would it be?\nJeremy [jlrubin at mit.edu] wrote:\n> Bitcoin didn't invent the concept of pooling: https://en.wikipedia.org/wiki/\n> Pooling_(resource_management). This is a Bitcoin Mining Pool, although it may\n> not be your favorite kind, which is fixated on specific properties of computing\n> contributions before finding a block. Pooling is just a general technique for\n> aggregating resources to accomplish something. If you have another name like\n> pooling that is in common use for this type of activity I would be more than\n> happy to adopt it.\n>\n> This sort of pool can hedge not only against fee rates but also against\n> increases in hashrate since your historical rate 'carries' into the future as a\n> function of the window. Further, windows and reward functions can be defined in\n> a myriad of ways that could, e.g., pay less to blocks found in more rapid\n> succession, contributing to the smoothing functionality.\n>\n> With respect to sub-block pooling, as described in the article, this sort of\n> design also helps with micro-pools being able to split resources\n> non-custodially in every block as a part of the higher order DCFMP. The point\n> is not, as noted, to enable solo mining an S9, but to decrease the size of the\n> minimum viable pool. It's also possible to add, without much validation or\n> data, some 'uncle block' type mechanism in an incentive compatible way (e.g.,\n> add 10 pow-heavy headers on the last block for cost 48 bytes header + 32 bytes\n> payout key) such that there's an incentive to include the heaviest ones you've\n> seen, not just your own, that are worth further study and consideration\n> (particularly because it's non-consensus, only for opt-in participation in the\n> pool).\n>\n> With respect to space usage, it seems you wholly reject the viability of a\n> payment pool mechanism to cut-through chain space. Is this a critique that\n> holds for all Payment Pools, or just in the context of mining? Is there a\n> particular reason why you think it infeasible that \"strongly online\"\n> counterparties would be able to coordinate more efficiently? Is it preferable\n> for miners, the nexus of decentralization for Bitcoin, to prefer to use\n> custodial services for pooling (which may require KYC/AM) over bearing a cost\n> of some extra potential chainload?\n>\n> Lastly, with respect to complexity, the proposal is actually incredibly simple\n> when you take it in a broader context. Non Interactive Channels and Payment\n> Pools are useful\u00a0by themselves, so are the operations to merge them and swap\n> balance across them. Therefore most of the complexity in this proposal is\n> relying on tools we'll likely see in everyday use in any case, DCFMP or no.\n>\n> Jeremy\n> !DSPAM:61b8f2f5321461582627336!\n--\nCheers, Bob McElrath\n\"For every complex problem, there is a solution that is simple, neat, and wrong.\"\n\u00a0 \u00a0 -- H. L. Mencken\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211223/2b2e0e8c/attachment-0001.html>"
            },
            {
                "author": "Jeremy",
                "date": "2021-12-23T19:05:22",
                "message_text_only": "> If you introduce signing into mining, then you will have cases, where\n> someone is powerful enough to produce blocks, but cannot, because signing\n> is needed. Then, your consensus is no longer \"the heaviest chain\", but \"the\n> heaviest signed chain\". That means, your computing power is no longer\n> enough by itself (as today), because to make a block, you also need some\n> kind of \"permission to mine\", because first you sign things (like in\n> signet) and then you mine them. That kind of being \"reliably unreliable\"\n> may be ok for testing, but not for the main network.\n\n\nthis is a really great point worth underscoring. this is the 'key\ningredient' for DCFMP, which is that there is no signing or other network\nsystem that is 'in the way' of normal bitcoin mining, just an opt-in set of\nrules for sharing the bounties of your block in exchange for future shares.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211223/9dd4e268/attachment.html>"
            },
            {
                "author": "Bob McElrath",
                "date": "2021-12-14T23:33:05",
                "message_text_only": "This, quite simply, is not a \"pool\". A pool is by definition a tool to reduce\nprofit variance by miners by collecting \"weak blocks\" that do not meet the\ndifficulty target, so as to get a better statistical measure of each miner's\nhashrate, which is used to subdivide profits. These are called \"shares\" and are\nentirely absent here.\n\nThe only available information here to decide payouts is the blocks themselves,\nI do not have any higher statistics measurement to subdivide payments. If I\nexpect to earn 3 blocks within the window, sometimes I will earn 2 and sometimes\nI will earn 4. Whether I keep the entire coinbase in those 2-4 blocks, or I have\n100 other miners paying me 1/100 as much 100 times, my payment is the same and\nmust be proportional to the number of blocks I mine in the window.  My variance\nis not reduced.\n\nFurther, by making miners pay other miners within the window N, this results in\nN^2 payments to miners which otherwise would have had N coinbase payments. So,\nthis is extremely block-space inefficient for no good reason. P2Pool had the\nsame problem and generated giant coinbases which competed with fee revenue.\n\"Congestion control\" makes this somewhat worse since is it is an absolute\nincrease in the block space consumed for these N^2 payments.\n\nThe only thing this proposal does do is smooth out fee revenue. While hedging on\nfee revenue is valuable, this is an extremely complicated and expensive way to\ngo about it, that simultaneously *reduces* fee revenue due to all the extra\nblock space used for miner payouts.\n\nJeremy via bitcoin-dev [bitcoin-dev at lists.linuxfoundation.org] wrote:\n> Howdy, welcome to day 15!\n> \n> Today's post covers a form of a mining pool that can be operated as sort of a\n> map-reduce over blocks without any \"infrastructure\".\n> \n> https://rubin.io/bitcoin/2021/12/12/advent-15/\n> \n> There's still some really open-ended questions (perhaps for y'all to consider)\n> around how to select an analyze the choice of window and payout functions, but\n> something like this could alleviate a lot of the centralization pressures\n> typically faced by pools.\n> \n> Notably, compared to previous attempts, combining the payment pool payout with\n> this concept means that there is practically very little on-chain overhead from\n> this approach as the chain-load\n> for including payouts in every block is deferred for future cooperation among\n> miners. Although that can be considered cooperation itself, if you think of it\n> like a pipeline, the cooperation happens out of band from mining and block\n> production so it really is coordination free to mine.\n> \n> \n> Cheers,\n> \n> Jeremy\n> \n> --\n> @JeremyRubin \n> !DSPAM:61b626be345321821816715!\n\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> \n> !DSPAM:61b626be345321821816715!\n\n--\nCheers, Bob McElrath\n\n\"For every complex problem, there is a solution that is simple, neat, and wrong.\"\n    -- H. L. Mencken"
            },
            {
                "author": "yanmaani at cock.li",
                "date": "2021-12-15T21:10:51",
                "message_text_only": "How does this differ from p2pool?\n\nIf you've just re-invented p2pool, shouldn't you credit their prior art?\n\nMonero is doing their implementation of p2pool. They have viable solo \nmining, as far as I understand. The basic idea is you have several \nP2pools. If you have a block time of 10 minutes, p2pool has 20% of \nhashrate, and there's 100 p2pool chains, each chain gets 0.2% of net \nhash. If you're OK with 20s block times (orphans aren't really a big \nproblem), you need (20/600) * (0.02/100) = 0.00067% of network hash to \nget a payout every 10m."
            },
            {
                "author": "Jeremy",
                "date": "2021-12-15T21:53:50",
                "message_text_only": "I could add a comparison to p2pool if you want, but bear in mind this is a\nblog post designed to introduce a complex topic to a wide audience, not a\nliterature review of all possible designs and prior art.\n\nIn particular, while P2Pool and DCFMP share a goal (decentralize mining),\nthe approaches to them bear very little similarity as DCFMP is focused on\nmaking the pooling a pure client side validatable function of the existing\nchain, and not create a major risk to mining centralization with a reliance\non a new network running on top of Bitcoin. DCFMP also lacks the core value\nprop of P2Pool which is higher resolution on share assignment.\n\nFurther, DCFMP's core innovations are Payment Pool and non interactive\nchannel based, something the P2Pool does not have, but could adopt, in\ntheory, to solve their payout problems[^note]. I still believe that making\na unified layer of networked software all miners are running on top of\nBitcoin in the loop of mining is a major risk and architecturally bad idea,\nhence my advocacy for doing such designs as micro pools inside a DCFMP; It\nwould be possible to make the \"micropools\" run on a P2Pool like software,\nthe DCFMP allows for smaller P2Pools to aggregate their hashrate\ntrustlessly with the main DCFMP shares.\n\n\n\n[^note]: for what it's worth, I was not familiar with p2pool very much\nbefore I came up with DCFMP. The lineage of my conceptual work was\ndeterminism, payment pools, and then realizing they could do something for\nmining.\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n\n\nOn Wed, Dec 15, 2021 at 1:11 PM <yanmaani at cock.li> wrote:\n\n> How does this differ from p2pool?\n>\n> If you've just re-invented p2pool, shouldn't you credit their prior art?\n>\n> Monero is doing their implementation of p2pool. They have viable solo\n> mining, as far as I understand. The basic idea is you have several\n> P2pools. If you have a block time of 10 minutes, p2pool has 20% of\n> hashrate, and there's 100 p2pool chains, each chain gets 0.2% of net\n> hash. If you're OK with 20s block times (orphans aren't really a big\n> problem), you need (20/600) * (0.02/100) = 0.00067% of network hash to\n> get a payout every 10m.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211215/3e190ead/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Decentralized Coordination Free Mining Pools",
            "categories": [
                "bitcoin-dev",
                "Bitcoin Advent Calendar"
            ],
            "authors": [
                "Jeremy",
                "yanmaani at cock.li",
                "vjudeu at gazeta.pl",
                "Bob McElrath",
                "Billy Tetrud"
            ],
            "messages_count": 20,
            "total_messages_chars_count": 120815
        }
    },
    {
        "title": "[bitcoin-dev] [Bitcoin Advent Calendar] Composability in Sapio Contracts",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-13T18:25:33",
                "message_text_only": "Devs,\n\nHere's today's post: https://rubin.io/bitcoin/2021/12/13/advent-16/\n\nIt covers how you can use Sapio modules composably. This is an active area\nof research for the Sapio platform, so definitely welcome and appreciate\nideas and feedback.\n\nOne area I'm particularly happy with but also unhappy with is the\n\"JSONSchema Type System\". It is remarkably flexible, which is useful, but a\nbetter type system would be able to enforce guarantees more strongly. Of\ncourse, comparing to things like ERC-20, Eth interfaces aren't particularly\nbinding (functions could do anything) so maybe it's OK. If you have\nthoughts on better ways to accomplish this, would love to think it through\nmore deeply. I'm particularly excited about ways to introduce more formal\ncorrectness.\n\nCheers,\n\nJeremy\n\np.s. -- feel free to send me any general feedback on the series out\nof band. There's a couple posts in the pipeline that are a bit less\ndevelopment focused like the earlier posts I excluded, and I could filter\nthem if folks are feeling like it's too much information, but I'd bias\ntowards posting the remaining pieces as they come for continuity. Let me\nknow if you feel strongly about a couple posts that might be a topical\nreach for this list.\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211213/f9d8d86d/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Composability in Sapio Contracts",
            "categories": [
                "bitcoin-dev",
                "Bitcoin Advent Calendar"
            ],
            "authors": [
                "Jeremy"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1497
        }
    },
    {
        "title": "[bitcoin-dev] [Bitcoin Advent Calendar] A Defense of Having Fun (and maybe staying poor)",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-14T18:37:59",
                "message_text_only": "Hi Devs,\n\nToday's post is a little more philosophical and less technical. Based on\nthe private feedback I received (from >1 persons, perhaps surprisingly)\nI'll continue to syndicate the remaining posts to this list.\n\nHere it is: https://rubin.io/bitcoin/2021/12/14/advent-17/\n\nTo having a little fun every now and again, as a treat,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211214/d26aab52/attachment.html>"
            },
            {
                "author": "Prayank",
                "date": "2021-12-23T14:09:13",
                "message_text_only": "Hi Jeremy,\n\n> Eugene just dropped a project he\u2019s been working on, and it\u2019s really freakin\u2019 cool. He basically implemented a human v. chess engine in Solidity that mints beautiful interactive NFTs of representations of the contract\u2019s internal states.\n\nNot sure why NFT is involved here but experiment looks interesting. Maybe Eugene should try few things on Lightning as well: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-December/019619.html\n\n> Why isn\u2019t Eugene working on Bitcoin?\n\nThere can be many reasons however Eugene can answer this question better.\n\n> Working on Bitcoin is can be fun. But mostly it\u2019s not. My post yesterday? Theone describing new techniques to make Bitcoin more decentralized? I had a lotof fun writing it. And then someone claimed that my work is \u201cvery dangerous\u201d toBitcoin.\n\nI don't support what someone said about you in IRC however people do say crazy things online which has nothing to do with Bitcoin. Bitcoin can be different things for everyone.\n\n> Bitcoin development has a bit of a burnout problem, with multiple contributors stepping down their engagement recently. A likely cause is the struggle it takes to ship even the smallest features, not to mention the monumental effort it takes to ship a single large project.\n\nI am not sure if this is the reason for people who will be less active in Bitcoin development now. And its a part of life, people will come and go. Show must go on. There will always be another developer who is more passionate and got more ideas to contribute. Funding in open source projects is an issue which exists outside Bitcoin as well and it is being addresses by several individuals and organizations.\n> It\u2019s hard to tell people, especially younger folk just entering the space, to work on Bitcoin full-time. What I say is as follows:\n\nIt is hard but not impossible. I tried recently in a meetup in India. Ethereum is not the answer to all the problems in the worlds. If someone has issues with Bitcoin development, they can be solved.\n\n> I don\u2019t think I\u2019m going to convince you here to care about NFTs. But I am \u2013hopefully \u2013 going to convince you to care about NFTs the phenomenon.\n\nTo be honest, its the trend right now and people care about it to get rich. There is nothing new or innovative about it. \n\n> For example, scaling challenge in Ethereum have led to the development of Zero Knowledge Roll-Ups, privacy issues things like Tornado Cash, and more.\n\nRollups are memes and there are several articles, threads, research etc. to read about this. Most of their implementations are not even decentralized. Tornado Cash is not good privacy and even became worse after their governance token.\n\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211223/51e3702f/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "A Defense of Having Fun (and maybe staying poor)",
            "categories": [
                "bitcoin-dev",
                "Bitcoin Advent Calendar"
            ],
            "authors": [
                "Jeremy",
                "Prayank"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 3503
        }
    },
    {
        "title": "[bitcoin-dev] Bitcoin is a protocol",
        "thread_messages": [
            {
                "author": "James Lu",
                "date": "2021-12-15T08:25:08",
                "message_text_only": "Bitcoin is a protocol. Protocols should be:\n\nSecure;\nBackwards compatible;\nForward compatible;\nand agreed by consensus\n\nFor Bitcoin, these properties are particularly important.\n\nThe fourth one is important not just because Bitcoin is a payment network,\nbut because more eyes on code creates security. More eyes on code may help\nprotocol design.\n\nTaproot is good.\nSapio proposal is good.\n\nLet\u2019s work together for constructive, positive, secure, and forward\ncompatible upgrades to Bitcoin.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211215/baabcba3/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Bitcoin is a protocol",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "James Lu"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 671
        }
    },
    {
        "title": "[bitcoin-dev] [Bitcoin Advent Calendar] Sapio Studio Payment Pool Walthrough",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-15T18:02:23",
                "message_text_only": "Hi Devs,\n\nToday's post is showing off how the Sapio Studio, the GUI smart contract\ncomposer for Sapio, functions.\nhttps://rubin.io/bitcoin/2021/12/15/advent-18/\n\nIn contrast to other posts this is mostly pictures.\n\nThis is a part of the project that could definitely use some development\nassistance if anyone is interested in pushing the frontier of bitcoin\nwallet functionality :)\n\nBest,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211215/ba365020/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Sapio Studio Payment Pool Walthrough",
            "categories": [
                "bitcoin-dev",
                "Bitcoin Advent Calendar"
            ],
            "authors": [
                "Jeremy"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 664
        }
    },
    {
        "title": "[bitcoin-dev] [Bitcoin Advent Calendar] Part One: Implementing NFTs in Sapio",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-17T00:49:30",
                "message_text_only": "I know NFTs are controversial, but here's my take on them in Sapio:\n\nhttps://rubin.io/bitcoin/2021/12/16/advent-19/\n\nIf you don't like NFTs, don't worry: the results and techniques are\nentirely generalizable here and can apply to many other types of things\nthat aren't stupid JPGs.\n\nE.g.,\n\n- If you squint, Lightning Channels are NFTs: I have a channel with someone\nand I can't transfer it to a third party fungibly because both the\nremaining side and entering side want to know about the counterparty\nreputation.\n- DLCs are NFTs because I want to know not just counterparties, but also\nwhich oracles.\n- Colored Coins/Tokens, definitionally, are not NFTs, but fractional shares\nof an NFT are Colored Coins, so NFT research might yield new results for\nColored Coins.\n\nAdvancing the state of the art for NFTs advances the state of the art for\nall sorts of other purposes, while letting us have a little fun. This is a\nstrong callback to https://rubin.io/bitcoin/2021/12/14/advent-17/ and\nhttps://rubin.io/bitcoin/2021/12/03/advent-6/ if you want to read more on\nwhy things like NFTs are cool even if JPGs are lame.\n\nCheers,\n\nJeremy\n\n\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211216/2634db55/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Part One: Implementing NFTs in Sapio",
            "categories": [
                "bitcoin-dev",
                "Bitcoin Advent Calendar"
            ],
            "authors": [
                "Jeremy"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1399
        }
    },
    {
        "title": "[bitcoin-dev] Bitcoin Knots \"Steel Rope\" LTS 21.2.knots20210629 released",
        "thread_messages": [
            {
                "author": "Luke Dashjr",
                "date": "2021-12-17T06:21:06",
                "message_text_only": "Bitcoin Knots version 21.2.knots20210629 is now available from:\n\n  https://bitcoinknots.org/files/21.x/21.2.knots20210629/\n\nThis Long Term Support (LTS) \"Steel Rope\" release is based on the unchanged\nBitcoin Knots feature set from 2021 June 29th, with only bug fixes and updated\ntranslations.\n\nPlease report bugs using the issue tracker at GitHub:\n\n  https://github.com/bitcoinknots/bitcoin/issues\n\nTo receive security and update notifications, please subscribe to:\n\n  https://bitcoinknots.org/list/announcements/join/\n\nFor the full release notes and change log, see:\n\nhttps://github.com/bitcoinknots/bitcoin/blob/v21.2.knots20210629/doc/release-notes.md\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 1528 bytes\nDesc: This is a digitally signed message part.\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211217/40240a62/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "Bitcoin Knots \"Steel Rope\" LTS 21.2.knots20210629 released",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Luke Dashjr"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 955
        }
    },
    {
        "title": "[bitcoin-dev] [Bitcoin Advent Calendar] Oracles, Bonds, and Attestation Chains",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-17T18:24:01",
                "message_text_only": "Today's post is pretty cool: it details how covenants like CTV can be used\nto improve on-chain bitcoin signing oracles by solving the timeout/rollover\nissue and solving the miner/oracle collusion issue on punishment. This\nissue is similar to the Blockstream Liquid Custody Federation rollover bug\nfrom a while back (which this type of design also helps to fix).\n\nhttps://rubin.io/bitcoin/2021/12/17/advent-20/\n\nIt also describes:\n- how a protocol on top can make 'branch free' attestation chains where if\nyou equivocate your funds get burned.\n- lightly, various uses for these chained attestations\n\nIn addition, Robin Linus has a great whitepaper he put out getting much\nmore in the weeds on the concepts described in the post, it's linked in the\nfirst bit of the post.\n\ncheers,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211217/aafa4fd6/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-12-18T01:00:14",
                "message_text_only": "Good morning Jeremy,\n\n> Today's post is pretty cool: it details how covenants like CTV can be used to improve on-chain bitcoin signing oracles by solving the timeout/rollover issue and solving the miner/oracle collusion issue on punishment. This issue is similar to the Blockstream Liquid Custody Federation rollover bug from a while back (which this type of design also helps to fix).\n>\n> https://rubin.io/bitcoin/2021/12/17/advent-20/\n>\n> It also describes:\n> - how a protocol on top can make 'branch free' attestation chains where if you equivocate your funds get burned.\n> - lightly, various uses for these chained attestations\n>\n> In addition, Robin Linus has a great whitepaper he put out getting much more in the weeds on the concepts described in the post, it's linked in the first bit of the post.\n\nNice, bonds are significantly better if you can ensure that the bonder cannot recover their funds.\nWithout a covenant the best you could do would be to have the bonder risk loss of funds on equivocation, not have the bonder actually definitely lose funds.\n\nWe should note that \"equivocate\" is not \"lie\".\nAn oracle can still lie, it just needs to consistently lie (i.e. not equivocate).\n\nAs an example, if the oracle is a signer for a federated sidechain, it could still sign an invalid sidechain block that inflates the sidecoin supply.\nIt is simply prevented from later denying this by signing an alternative valid sidechain block and acting as if it never signed the invalid sidechain block.\nBut if it sticks to its guns, then the sidechain simply stops operation with everyone owning sidecoins losing their funds (and if the oracle already exited the sidechain, its bond remains safe, as it did not equivocate, it only lied).\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Jeremy",
                "date": "2021-12-18T02:00:32",
                "message_text_only": "Yep, these are great points. There is no way to punish signing the wrong\nthing directly, just not changing your answers without risk to funds.\n\nOne of the interesting things is that upon a single equivocation you get\nunbounded equivocation by 3rd parties, e.g., you can completely rewrite the\nentire signature chain!\n\nAnother interesting point: if you use a musig key for your staking key that\nis musig(a,b,c) you can sign with a until you equivocate once, then switch\nto b, then c. Three strikes and you're out! IDK what that could be used for.\n\nLastly, while you can't punish lying, you could say \"only the stakers who\nsign with the majority get allocated reward tokens for that slot\". So you\ncould equivocate to switch and get tokens, but you'd burn your collateral\nfor them. But this does make an incentive for the stakers to try to sign\nthe \"correct\" statement in line with peers.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211217/24fdc3d2/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-12-18T03:49:15",
                "message_text_only": "Good morning Jeremy,\n\n\n> Another interesting point: if you use a musig key for your staking key that is musig(a,b,c) you can sign with a until you equivocate once, then switch to b, then c. Three strikes and you're out! IDK what that could be used for.\n\nYou could say \"oops, I made a mistake, can I correct it by equivocating just this time?\".\nThree strikes and you are out.\n\n> Lastly, while you can't punish lying, you could say \"only the stakers who sign with the majority get allocated reward tokens for that slot\". So you could equivocate to switch and get tokens, but you'd burn your collateral for them. But this does make an incentive for the stakers to try to sign the \"correct\" statement in line with peers.\n\nNote the quote marks around \"correct\" --- the majority of peers could be conspiring to lie, too.\nConspiracy theory time.....\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Oracles, Bonds, and Attestation Chains",
            "categories": [
                "bitcoin-dev",
                "Bitcoin Advent Calendar"
            ],
            "authors": [
                "ZmnSCPxj",
                "Jeremy"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 4738
        }
    },
    {
        "title": "[bitcoin-dev] Globally Broadcasting Workshares to Improve Finality Heuristics",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-17T18:53:55",
                "message_text_only": "An interesting concept occurred to me today while chatting with Nic Carter.\n\nIf we set Bitcoin Core up to gossip headers for work shares (e.g., expected\n500 headers per block would have 20kb overhead, assuming we don't need to\nsend the prev hash) we'd be able to have more accurate finality estimates\nand warnings if we see hashrate abandoning our chain tip. This is\nobservable regardless of if dishonest miners choose not to publish their\nwork on non tip shares, since you can notice the missing work.\n\nIn the GUI, we could give users an additional warning if they are\naccepting a payment during a sudden hashrate decrease that they might wait\nlonger.\n\nHas this been discussed before?\n\nCheers,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211217/fbca153e/attachment.html>"
            },
            {
                "author": "Newsletter Catcher",
                "date": "2021-12-18T08:36:07",
                "message_text_only": "It's not exactly what you're looking for but this is very similar to the\npremise of Bobtail, which was presented at Scaling Bitcoin a few years ago:\nhttps://arxiv.org/abs/1709.08750\n\nOn Fri, Dec 17, 2021, 10:54 Jeremy via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> An interesting concept occurred to me today while chatting with Nic Carter.\n>\n> If we set Bitcoin Core up to gossip headers for work shares (e.g.,\n> expected 500 headers per block would have 20kb overhead, assuming we don't\n> need to send the prev hash) we'd be able to have more accurate finality\n> estimates and warnings if we see hashrate abandoning our chain tip. This is\n> observable regardless of if dishonest miners choose not to publish their\n> work on non tip shares, since you can notice the missing work.\n>\n> In the GUI, we could give users an additional warning if they are\n> accepting a payment during a sudden hashrate decrease that they might wait\n> longer.\n>\n> Has this been discussed before?\n>\n> Cheers,\n>\n> Jeremy\n>\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n> <https://twitter.com/JeremyRubin>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211218/422c0b92/attachment.html>"
            },
            {
                "author": "Rune K. Svendsen",
                "date": "2021-12-18T13:25:08",
                "message_text_only": "Hi Jeremy,\n\nIf I understand you correctly, then I believe I touch upon this concept here: https://bitcointalk.org/index.php?topic=97153.msg1309930#msg1309930\n\n\n/Rune"
            }
        ],
        "thread_summary": {
            "title": "Globally Broadcasting Workshares to Improve Finality Heuristics",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeremy",
                "Rune K. Svendsen",
                "Newsletter Catcher"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 2616
        }
    },
    {
        "title": "[bitcoin-dev] Proposal: Full-RBF in Bitcoin Core 24.0",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-18T16:51:46",
                "message_text_only": "Small idea:\n\nease into getting rid of full-rbf by keeping the flag working, but make\nenforcement of non-replaceability something that happens n seconds after\nfirst seen.\n\nthis reduces the ability to partition the mempools by broadcasting\nirreplaceable conflicts all at once, and slowly eases clients off of\nrelying on non-RBF.\n\nwe might start with 60 seconds, and then double every release till we get\nto 600 at which point we disable it.\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n\n\nOn Tue, Jun 15, 2021 at 10:00 AM Antoine Riard via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi,\n>\n> I'm writing to propose deprecation of opt-in RBF in favor of full-RBF as\n> the Bitcoin Core's default replacement policy in version 24.0. As a\n> reminder, the next release is 22.0, aimed for August 1st, assuming\n> agreement is reached, this policy change would enter into deployment phase\n> a year from now.\n>\n> Even if this replacement policy has been deemed as highly controversial a\n> few years ago, ongoing and anticipated changes in the Bitcoin ecosystem are\n> motivating this proposal.\n>\n> # RBF opt-out as a DoS Vector against Multi-Party Funded Transactions\n>\n> As explained in \"On Mempool Funny Games against Multi-Party Funded\n> Transactions'', 2nd issue [0], an attacker can easily DoS a multi-party\n> funded transactions by propagating an RBF opt-out double-spend of its\n> contributed input before the honest transaction is broadcasted by the\n> protocol orchester. DoSes are qualified in the sense of either an attacker\n> wasting timevalue of victim's inputs or forcing exhaustion of the\n> fee-bumping  reserve.\n>\n> This affects a series of Bitcoin protocols such as Coinjoin, onchain DLCs\n> and dual-funded LN channels. As those protocols are still in the early\n> phase of deployment, it doesn't seem to have been executed in the wild for\n> now.  That said, considering that dual-funded are more efficient from a\n> liquidity standpoint, we can expect them to be widely relied on, once\n> Lightning enters in a more mature phase. At that point, it should become\n> economically rational for liquidity service providers to launch those DoS\n> attacks against their competitors to hijack user traffic.\n>\n> Beyond that, presence of those DoSes will complicate the design and\n> deployment of multi-party Bitcoin protocols such as payment\n> pools/multi-party channels. Note, Lightning Pool isn't affected as there is\n> a preliminary stage where batch participants are locked-in their funds\n> within an account witnessScript shared with the orchestrer.\n>\n> Of course, even assuming full-rbf, propagation of the multi-party funded\n> transactions can still be interfered with by an attacker, simply\n> broadcasting a double-spend with a feerate equivalent to the honest\n> transaction. However, it tightens the attack scenario to a scorched earth\n> approach, where the attacker has to commit equivalent fee-bumping reserve\n> to maintain the pinning and might lose the \"competing\" fees to miners.\n>\n> # RBF opt-out as a Mempools Partitions Vector\n>\n> A longer-term issue is the risk of mempools malicious partitions, where an\n> attacker exploits network topology or divergence in mempools policies to\n> partition network mempools in different subsets. From then a wide range of\n> attacks can be envisioned such as package pinning [1], artificial\n> congestion to provoke LN channels closure or manipulation of\n> fee-estimator's feerate (the Core's one wouldn't be affected as it relies\n> on block confirmation, though other fee estimators designs deployed across\n> the ecosystem are likely going to be affected).\n>\n> Traditionally, mempools partitions have been gauged as a spontaneous\n> outcome of a distributed systems like Bitcoin p2p network and I'm not aware\n> it has been studied in-depth for adversarial purposes. Though, deployment\n> of second-layer\n> protocols, heavily relying on sanity of a local mempool for fee-estimation\n> and robust propagation of their time-sensitive transactions might lead to\n> reconsider this position. Acknowledging this, RBF opt-out is a low-cost\n> partitioning tool, of which the existence nullifies most of potential\n> progresses to mitigate malicious partitioning.\n>\n>\n> To resume, opt-in RBF doesn't suit well deployment of robust second-layers\n> protocol, even if those issues are still early and deserve more research.\n> At the same time, I believe a meaningful subset of the ecosystem  are still\n> relying\n> on 0-confs transactions, even if their security is relying on far weaker\n> assumptions (opt-in RBF rule is a policy rule, not a consensus one) [2] A\n> rapid change of Core's mempool rules would be harming their quality of\n> services and should be\n> weighed carefully. On the other hand, it would be great to nudge them\n> towards more secure handling of their 0-confs flows [3]\n>\n> Let's examine what could be deployed ecosystem-wise as enhancements to the\n> 0-confs security model.\n>\n> # Proactive security models : Double-spend Monitoring/Receiver-side\n> Fee-Topping with Package Relay\n>\n> From an attacker viewpoint, opt-in RBF isn't a big blocker to successful\n> double-spends. Any motivated attacker can modify Core to mass-connect to a\n> wide portion of the network, announce txA to this subset, announce txA' to\n> the\n> merchant. TxA' propagation will be encumbered by the privacy-preserving\n> inventory timers (`OUTBOUND_INVENTORY_BROADCAST_INTERVAL`), of which an\n> attacker has no care to respect.\n>\n> To detect a successful double-spend attempt, a Bitcoin service should run\n> few full-nodes with well-spread connection graphs and unlinkable between\n> them, to avoid being identified then maliciously partitioned from the rest\n> of the network.\n>\n> I believe this tactic is already deployed by few Bitcoin services, and\n> even one can throw flame at it because it over consumes network resources\n> (bandwidth, connection slots, ...), it does procure a security advantage to\n> the ones doing it.\n>\n> One further improvement on top of this protection could be to react after\n> the double-spend detection by attaching a CPFP to the merchant transaction,\n> with a higher package feerate than the double-spend. Expected deployment of\n> package-relay as a p2p mechanism/mempool policy in Bitcoin Core should\n> enable it to do so.\n>\n> # Reactive security models : EconomicReputation-based Compensations\n>\n> Another approach could be to react after the fact if a double-spend has\n> been qualified. If the sender is already known to the service provider, the\n> service account can be slashed.  If the sender is a low-trusted\n> counterparty to the merchant, \"side-trust\" models could be relied on. For\n> e.g a LN pubkey with a stacked reputation from your autopilot, LSATs, stake\n> certificates, a HTLC-as-a-fidelity-bond, ... The space is quite wide there\n> but I foresee those trust-minimized, decentralized solutions being adopted\n> by the LN ecosystem to patch the risks when you enter in a channel/HTLC\n> operation with an anonymous counterparty.\n>\n> What other cool new tools could be considered to enhance 0-confs security ?\n>\n> To conclude, let's avoid replaying the contentious threads of a few years\n> ago. What this new thread highlights is the fact that a transaction\n> relay/mempool acceptance policy might be beneficial to some class of\n> already-deployed\n> Bitcoin applications while being detrimental to newer ones. How do we\n> preserve the current interests of 0-confs users while enabling upcoming\n> interests of fancy L2s to flourish is a good conversation to have. I think.\n>\n> If there is ecosystem agreement on switching to full-RBF, but 0.24 sounds\n> too early, let's defer it to 0.25 or 0.26. I don't think Core has a\n> consistent deprecation process w.r.t to policy rules heavily relied-on by\n> Bitcoin users, if we do so let sets a precedent satisfying as many folks as\n> we can.\n>\n> Cheers,\n> Antoine\n>\n> [0]\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2021-May/003033.html\n>\n> [1] See scenario 3 :\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-June/002758.html\n>\n> [2] https://github.com/bitcoin/bitcoin/pull/10823#issuecomment-466485121\n>\n> [3] And the LN ecosystem does have an interest to fix zero-confs security,\n> if \"turbo-channels\"-like become normalized for mobile nodes\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211218/d033f8d9/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2021-12-18T17:52:07",
                "message_text_only": "On Sat, Dec 18, 2021 at 08:51:46AM -0800, Jeremy via bitcoin-dev wrote:\n> Small idea:\n> \n> ease into getting rid of full-rbf by keeping the flag working, but make\n> enforcement of non-replaceability something that happens n seconds after\n> first seen.\n> \n> this reduces the ability to partition the mempools by broadcasting\n> irreplaceable conflicts all at once, and slowly eases clients off of\n> relying on non-RBF.\n> \n> we might start with 60 seconds, and then double every release till we get\n> to 600 at which point we disable it.\n\nMaking replacability turn on _after_ an expiry time is reached has been\nsuggested before, IIRC by Matt Corallo. However I believe the approach of\nenabling full-rbf _until_ a time is reached is clever and novel.\n\nI'd suggest doing both at once. Long-running txs are certainly useful. But if a\ntx hasn't been mined in a few blocks, it certainly can't be relied on for\nzeroconf.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211218/4f6f02f0/attachment-0001.sig>"
            },
            {
                "author": "damian at willtech.com.au",
                "date": "2021-12-20T02:30:57",
                "message_text_only": "Good Afternoon,\n\nThere is no such thing in Bitcoin as zeroconf but any individual may use \ngossip from mempool if they choose it to prefer it could be possible a \ntransaction could exist in the future. You are talking about the \nmempool. The mempool exists on gossip. There are no transactions until \nthey are mined and included in a block and information can disappear \nfrom the mempool. This is Bitcoin where we scientifically make a \nconsensus to assure fungibility.\n\nKING JAMES HRMH\nGreat British Empire\n\nRegards,\nThe Australian\nLORD HIS EXCELLENCY JAMES HRMH (& HMRH)\nof Hougun Manor & Glencoe & British Empire\nMR. Damian A. James Williamson\nWills\n\net al.\n\n\nWilltech\nwww.willtech.com.au\nwww.go-overt.com\nduigco.org DUIGCO API\nand other projects\n\n\nm. 0487135719\nf. +61261470192\n\n\nThis email does not constitute a general advice. Please disregard this \nemail if misdelivered.\nOn 2021-12-18 09:52, Peter Todd via bitcoin-dev wrote:\n> On Sat, Dec 18, 2021 at 08:51:46AM -0800, Jeremy via bitcoin-dev wrote:\n>> Small idea:\n>> \n>> ease into getting rid of full-rbf by keeping the flag working, but \n>> make\n>> enforcement of non-replaceability something that happens n seconds \n>> after\n>> first seen.\n>> \n>> this reduces the ability to partition the mempools by broadcasting\n>> irreplaceable conflicts all at once, and slowly eases clients off of\n>> relying on non-RBF.\n>> \n>> we might start with 60 seconds, and then double every release till we \n>> get\n>> to 600 at which point we disable it.\n> \n> Making replacability turn on _after_ an expiry time is reached has been\n> suggested before, IIRC by Matt Corallo. However I believe the approach \n> of\n> enabling full-rbf _until_ a time is reached is clever and novel.\n> \n> I'd suggest doing both at once. Long-running txs are certainly useful. \n> But if a\n> tx hasn't been mined in a few blocks, it certainly can't be relied on \n> for\n> zeroconf.\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Antoine Riard",
                "date": "2021-12-19T18:55:01",
                "message_text_only": "> we might start with 60 seconds, and then double every release till we get\nto 600 at which point we disable it.\n\nThis is clearly new. However, I'm not sure if it's solving multi-party\nfunding transaction DoS, which was of the motivation to propose to\ndeprecate opt-in RBF. The malicious counterparty can broadcast its\nlow-feerate, opt-out spending of its own collateral input far before to\nengage in the cooperative funding.\n\nWhen the funding transaction starts to propagate, the opt-out has been\n\"first seen\" for a while, the replaceability is turned off, the honest\nfunding is bounced off ?\n\n\nTaking opportunity to laid out another proposal which has whispered to me\noffline :\n\n\"(what) if the nversion of outputs (which is set by their creating\ntransaction) were inspected and\ntriggered any spend of the output to be required to be flagged to be\nreplaceable-- as a standardness rule?\"\n\nWhile working to solve the DoS, I believe this approach is introducing an\noverhead cost in the funding of multi-party transactions, as from now on,\nyou have to sanitize your collateral inputs by sending them first to a\nreplaceable nVersion outputs ? (iirc, this is done by Lightning Pool, where\nyou have a first step where the inputs are locked in a 2-of-2 with the\norchester before to engage in the batch execution tx).\n\nCurrent state of the discussion is to introduce a `fullrbf` config-knob\nturned to false, see more context here :\nhttps://gnusha.org/bitcoin-core-dev/2021-10-21.log. Proposing an\nimplementation soon.\n\nAntoine\n\nLe sam. 18 d\u00e9c. 2021 \u00e0 11:51, Jeremy <jlrubin at mit.edu> a \u00e9crit :\n\n> Small idea:\n>\n> ease into getting rid of full-rbf by keeping the flag working, but make\n> enforcement of non-replaceability something that happens n seconds after\n> first seen.\n>\n> this reduces the ability to partition the mempools by broadcasting\n> irreplaceable conflicts all at once, and slowly eases clients off of\n> relying on non-RBF.\n>\n> we might start with 60 seconds, and then double every release till we get\n> to 600 at which point we disable it.\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n> <https://twitter.com/JeremyRubin>\n>\n>\n> On Tue, Jun 15, 2021 at 10:00 AM Antoine Riard via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Hi,\n>>\n>> I'm writing to propose deprecation of opt-in RBF in favor of full-RBF as\n>> the Bitcoin Core's default replacement policy in version 24.0. As a\n>> reminder, the next release is 22.0, aimed for August 1st, assuming\n>> agreement is reached, this policy change would enter into deployment phase\n>> a year from now.\n>>\n>> Even if this replacement policy has been deemed as highly controversial a\n>> few years ago, ongoing and anticipated changes in the Bitcoin ecosystem are\n>> motivating this proposal.\n>>\n>> # RBF opt-out as a DoS Vector against Multi-Party Funded Transactions\n>>\n>> As explained in \"On Mempool Funny Games against Multi-Party Funded\n>> Transactions'', 2nd issue [0], an attacker can easily DoS a multi-party\n>> funded transactions by propagating an RBF opt-out double-spend of its\n>> contributed input before the honest transaction is broadcasted by the\n>> protocol orchester. DoSes are qualified in the sense of either an attacker\n>> wasting timevalue of victim's inputs or forcing exhaustion of the\n>> fee-bumping  reserve.\n>>\n>> This affects a series of Bitcoin protocols such as Coinjoin, onchain DLCs\n>> and dual-funded LN channels. As those protocols are still in the early\n>> phase of deployment, it doesn't seem to have been executed in the wild for\n>> now.  That said, considering that dual-funded are more efficient from a\n>> liquidity standpoint, we can expect them to be widely relied on, once\n>> Lightning enters in a more mature phase. At that point, it should become\n>> economically rational for liquidity service providers to launch those DoS\n>> attacks against their competitors to hijack user traffic.\n>>\n>> Beyond that, presence of those DoSes will complicate the design and\n>> deployment of multi-party Bitcoin protocols such as payment\n>> pools/multi-party channels. Note, Lightning Pool isn't affected as there is\n>> a preliminary stage where batch participants are locked-in their funds\n>> within an account witnessScript shared with the orchestrer.\n>>\n>> Of course, even assuming full-rbf, propagation of the multi-party funded\n>> transactions can still be interfered with by an attacker, simply\n>> broadcasting a double-spend with a feerate equivalent to the honest\n>> transaction. However, it tightens the attack scenario to a scorched earth\n>> approach, where the attacker has to commit equivalent fee-bumping reserve\n>> to maintain the pinning and might lose the \"competing\" fees to miners.\n>>\n>> # RBF opt-out as a Mempools Partitions Vector\n>>\n>> A longer-term issue is the risk of mempools malicious partitions, where\n>> an attacker exploits network topology or divergence in mempools policies to\n>> partition network mempools in different subsets. From then a wide range of\n>> attacks can be envisioned such as package pinning [1], artificial\n>> congestion to provoke LN channels closure or manipulation of\n>> fee-estimator's feerate (the Core's one wouldn't be affected as it relies\n>> on block confirmation, though other fee estimators designs deployed across\n>> the ecosystem are likely going to be affected).\n>>\n>> Traditionally, mempools partitions have been gauged as a spontaneous\n>> outcome of a distributed systems like Bitcoin p2p network and I'm not aware\n>> it has been studied in-depth for adversarial purposes. Though, deployment\n>> of second-layer\n>> protocols, heavily relying on sanity of a local mempool for\n>> fee-estimation and robust propagation of their time-sensitive transactions\n>> might lead to reconsider this position. Acknowledging this, RBF opt-out is\n>> a low-cost partitioning tool, of which the existence nullifies most of\n>> potential progresses to mitigate malicious partitioning.\n>>\n>>\n>> To resume, opt-in RBF doesn't suit well deployment of robust\n>> second-layers protocol, even if those issues are still early and deserve\n>> more research. At the same time, I believe a meaningful subset of the\n>> ecosystem  are still relying\n>> on 0-confs transactions, even if their security is relying on far weaker\n>> assumptions (opt-in RBF rule is a policy rule, not a consensus one) [2] A\n>> rapid change of Core's mempool rules would be harming their quality of\n>> services and should be\n>> weighed carefully. On the other hand, it would be great to nudge them\n>> towards more secure handling of their 0-confs flows [3]\n>>\n>> Let's examine what could be deployed ecosystem-wise as enhancements to\n>> the 0-confs security model.\n>>\n>> # Proactive security models : Double-spend Monitoring/Receiver-side\n>> Fee-Topping with Package Relay\n>>\n>> From an attacker viewpoint, opt-in RBF isn't a big blocker to successful\n>> double-spends. Any motivated attacker can modify Core to mass-connect to a\n>> wide portion of the network, announce txA to this subset, announce txA' to\n>> the\n>> merchant. TxA' propagation will be encumbered by the privacy-preserving\n>> inventory timers (`OUTBOUND_INVENTORY_BROADCAST_INTERVAL`), of which an\n>> attacker has no care to respect.\n>>\n>> To detect a successful double-spend attempt, a Bitcoin service should run\n>> few full-nodes with well-spread connection graphs and unlinkable between\n>> them, to avoid being identified then maliciously partitioned from the rest\n>> of the network.\n>>\n>> I believe this tactic is already deployed by few Bitcoin services, and\n>> even one can throw flame at it because it over consumes network resources\n>> (bandwidth, connection slots, ...), it does procure a security advantage to\n>> the ones doing it.\n>>\n>> One further improvement on top of this protection could be to react after\n>> the double-spend detection by attaching a CPFP to the merchant transaction,\n>> with a higher package feerate than the double-spend. Expected deployment of\n>> package-relay as a p2p mechanism/mempool policy in Bitcoin Core should\n>> enable it to do so.\n>>\n>> # Reactive security models : EconomicReputation-based Compensations\n>>\n>> Another approach could be to react after the fact if a double-spend has\n>> been qualified. If the sender is already known to the service provider, the\n>> service account can be slashed.  If the sender is a low-trusted\n>> counterparty to the merchant, \"side-trust\" models could be relied on. For\n>> e.g a LN pubkey with a stacked reputation from your autopilot, LSATs, stake\n>> certificates, a HTLC-as-a-fidelity-bond, ... The space is quite wide there\n>> but I foresee those trust-minimized, decentralized solutions being adopted\n>> by the LN ecosystem to patch the risks when you enter in a channel/HTLC\n>> operation with an anonymous counterparty.\n>>\n>> What other cool new tools could be considered to enhance 0-confs security\n>> ?\n>>\n>> To conclude, let's avoid replaying the contentious threads of a few years\n>> ago. What this new thread highlights is the fact that a transaction\n>> relay/mempool acceptance policy might be beneficial to some class of\n>> already-deployed\n>> Bitcoin applications while being detrimental to newer ones. How do we\n>> preserve the current interests of 0-confs users while enabling upcoming\n>> interests of fancy L2s to flourish is a good conversation to have. I think.\n>>\n>> If there is ecosystem agreement on switching to full-RBF, but 0.24 sounds\n>> too early, let's defer it to 0.25 or 0.26. I don't think Core has a\n>> consistent deprecation process w.r.t to policy rules heavily relied-on by\n>> Bitcoin users, if we do so let sets a precedent satisfying as many folks as\n>> we can.\n>>\n>> Cheers,\n>> Antoine\n>>\n>> [0]\n>> https://lists.linuxfoundation.org/pipermail/lightning-dev/2021-May/003033.html\n>>\n>> [1] See scenario 3 :\n>> https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-June/002758.html\n>>\n>> [2] https://github.com/bitcoin/bitcoin/pull/10823#issuecomment-466485121\n>>\n>> [3] And the LN ecosystem does have an interest to fix zero-confs\n>> security, if \"turbo-channels\"-like become normalized for mobile nodes\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211219/55df5c1e/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Proposal: Full-RBF in Bitcoin Core 24.0",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeremy",
                "damian at willtech.com.au",
                "Antoine Riard",
                "Peter Todd"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 22530
        }
    },
    {
        "title": "[bitcoin-dev] [Bitcoin Advent Calendar] Packaging Sapio Applications",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-18T21:14:16",
                "message_text_only": "hi devs,\n\ntoday's topic is packaging Sapio applications. maybe a bit more annoying\nthan usual, but important.\n\nhttps://rubin.io/bitcoin/2021/12/18/advent-21/\n\n\nI think WASM is really really cool! It's definitely been very helpful for\nSapio. It'd be kinda neat if at some point software like Bitcoin Core could\nrun Sapio modules natively and offer users extended functionality based on\nthat. For now I'm building out the wallet as Sapio Studio, but a boy can\ndream. I know there are some bitcoiners (in particular, the rust-bitcoiners\n& rust-lightning) who like WASM for shipping stuff to browsers!\n\nWASM is also something I've been thinking about w.r.t. how we ship\nconsensus upgrades. It would be kinda groovy if we could implement the\nsemantics of pieces of bitcoin code as WASM modules... e.g., imagine pieces\nof consensus being able to be compiled to and run through a WASM system, it\nwould help guarantee that those pieces of the code are entirely\ndeterministic. Maybe something for Simplicity to consider WASM being the\nhost language for JET extensions!\n\n\nCheers,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211218/0deffeca/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Packaging Sapio Applications",
            "categories": [
                "bitcoin-dev",
                "Bitcoin Advent Calendar"
            ],
            "authors": [
                "Jeremy"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1345
        }
    },
    {
        "title": "[bitcoin-dev] [Bitcoin Advent Calendar] NFTs Part Two: Auctions, Royalties, Mints, Generative, Game Items",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-20T02:37:39",
                "message_text_only": "Hi Devs!\n\nMore on NFTs today! Code demos of dutch auctions of NFTs + royalties, and\nthen discussion of a few other concepts I'm excited about.\n\nhttps://rubin.io/bitcoin/2021/12/19/advent-22/\n\nParticularly novel is the combination of attestation chains, lightning\ninvoices, and NFTs to create off-chain updatable and on-chain sellable\nin-game items.\n\nTill tomorrow!\n\nCheers,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211219/54ba2655/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "NFTs Part Two: Auctions, Royalties, Mints, Generative, Game Items",
            "categories": [
                "bitcoin-dev",
                "Bitcoin Advent Calendar"
            ],
            "authors": [
                "Jeremy"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 654
        }
    },
    {
        "title": "[bitcoin-dev] [Bitcoin Advent Calendar] Derivatives and Options",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-21T01:17:34",
                "message_text_only": "Hi Devs,\n\nToday's post is on building options/derivatives in Sapio!\n\nhttps://rubin.io/bitcoin/2021/12/20/advent-23\n\nEnjoy!\n\nCheers,\n\nJeremy\n\n\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211220/11983870/attachment.html>"
            },
            {
                "author": "Prayank",
                "date": "2021-12-24T16:42:42",
                "message_text_only": "Hi Jeremy,\n\n> Wheres the info come from? Well, multiple places. We could get it from a third party (maybe using anattestation chain of some sort?), or there are certain ways it could beself-referential (like for powswap <https://powswap.com>).\n\n> Now let\u2019s define a threshold oracle \u2013 we wouldn\u2019t want to trust just onelousy oracle, so let\u2019s trust M out of N of them!\n\nSimilar approach is used in discreet log contracts for multi oracles. There is even a project for P2P derivatives but it was not used for any real trades on mainnet or further developed. What difference would OP_CTV make in this project if its implemented in Bitcoin?\nhttps://github.com/p2pderivatives/p2pderivatives-client\n\nhttps://github.com/p2pderivatives/p2pderivatives-server\n\nhttps://github.com/p2pderivatives/p2pderivatives-oracle\n\n> Does this NEED CTV?\nNo, not in particular. Most of this stuff could be done with online signer server federation between you and counterparty. CTV makes some stuff nicer though, and opens up new possibilities for opening these contracts unilaterally.\n\nNicer? How would unilateral derivatives work because my understanding was that you always need a peer to take the other side of the trade. I wish we could discuss this topic in a trading community with some Bitcoiners that even had some programming knowledge.\n\nDerivatives are interesting and less explored or used in Bitcoin projects. They could be useful in solving lot of problems.\n\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211224/97c13df5/attachment.html>"
            },
            {
                "author": "email at yancy.lol",
                "date": "2021-12-26T20:49:30",
                "message_text_only": "Prayank,\n\nI believe the p2pderivatives DLC application is still under active \ndevelopment here (single oracle):\nhttps://github.com/p2pderivatives/rust-dlc\n\nI was once involved in the project in a galaxy far far away but haven't \nkept up with the project.  Also, I'm a few days behind in the Bitcoin \nAdvent Calendar :)\n\nCheers,\n-Yancy\n\n\nOn 2021-12-24 17:42, Prayank via bitcoin-dev wrote:\n> Hi Jeremy,\n> \n>> Wheres the info come from? Well, multiple places. We could get it\n> from a third party (maybe using an attestation chain of some sort?),\n> or there are certain ways it could be self-referential (like for\n> powswap [1]).\n> \n>> Now let\u2019s define a threshold oracle \u2013 we wouldn\u2019t want to\n> trust just one lousy oracle, so let\u2019s trust M out of N of them!\n> \n> Similar approach is used in discreet log contracts for multi oracles.\n> There is even a project for P2P derivatives but it was not used for\n> any real trades on mainnet or further developed. What difference would\n> OP_CTV make in this project if its implemented in Bitcoin?\n> \n> https://github.com/p2pderivatives/p2pderivatives-client\n> \n> https://github.com/p2pderivatives/p2pderivatives-server\n> \n> https://github.com/p2pderivatives/p2pderivatives-oracle\n> \n>> Does this NEED CTV?\n> \n> No, not in particular. Most of this stuff could be done with online\n> signer server federation between you and counterparty. CTV makes some\n> stuff nicer though, and opens up new possibilities for opening these\n> contracts unilaterally.\n> \n> Nicer? How would unilateral derivatives work because my understanding\n> was that you always need a peer to take the other side of the trade. I\n> wish we could discuss this topic in a trading community with some\n> Bitcoiners that even had some programming knowledge.\n> \n> Derivatives are interesting and less explored or used in Bitcoin\n> projects. They could be useful in solving lot of problems.\n> \n> --\n> \n> Prayank\n> \n> A3B1 E430 2298 178F\n> \n> \n> Links:\n> ------\n> [1] https://powswap.com\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Thibaut Le Guilly",
                "date": "2021-12-27T12:05:36",
                "message_text_only": "Hi all,\n\nDid someone say rust-dlc? Just kidding, but wanted to mention that indeed\nit's under active development, supports multi oracle contracts and many\nother cool things (pretty much everything you can find in the dlc specs)!\n\nOtherwise nice article Jeremy. Maybe you should drop by our monthly DLC\nspec meeting one of these days. I'm sure everybody would be happy to hear\nhow we could improve the Bitcoin derivatives ecosystem with CTV and what\ninfrastructures or code could be reused from DLCs.\n\nCheers,\n\nThibaut\n\nOn Mon, Dec 27, 2021 at 7:39 AM yancy via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Prayank,\n>\n> I believe the p2pderivatives DLC application is still under active\n> development here (single oracle):\n> https://github.com/p2pderivatives/rust-dlc\n>\n> I was once involved in the project in a galaxy far far away but haven't\n> kept up with the project.  Also, I'm a few days behind in the Bitcoin\n> Advent Calendar :)\n>\n> Cheers,\n> -Yancy\n>\n>\n> On 2021-12-24 17:42, Prayank via bitcoin-dev wrote:\n> > Hi Jeremy,\n> >\n> >> Wheres the info come from? Well, multiple places. We could get it\n> > from a third party (maybe using an attestation chain of some sort?),\n> > or there are certain ways it could be self-referential (like for\n> > powswap [1]).\n> >\n> >> Now let\u2019s define a threshold oracle \u2013 we wouldn\u2019t want to\n> > trust just one lousy oracle, so let\u2019s trust M out of N of them!\n> >\n> > Similar approach is used in discreet log contracts for multi oracles.\n> > There is even a project for P2P derivatives but it was not used for\n> > any real trades on mainnet or further developed. What difference would\n> > OP_CTV make in this project if its implemented in Bitcoin?\n> >\n> > https://github.com/p2pderivatives/p2pderivatives-client\n> >\n> > https://github.com/p2pderivatives/p2pderivatives-server\n> >\n> > https://github.com/p2pderivatives/p2pderivatives-oracle\n> >\n> >> Does this NEED CTV?\n> >\n> > No, not in particular. Most of this stuff could be done with online\n> > signer server federation between you and counterparty. CTV makes some\n> > stuff nicer though, and opens up new possibilities for opening these\n> > contracts unilaterally.\n> >\n> > Nicer? How would unilateral derivatives work because my understanding\n> > was that you always need a peer to take the other side of the trade. I\n> > wish we could discuss this topic in a trading community with some\n> > Bitcoiners that even had some programming knowledge.\n> >\n> > Derivatives are interesting and less explored or used in Bitcoin\n> > projects. They could be useful in solving lot of problems.\n> >\n> > --\n> >\n> > Prayank\n> >\n> > A3B1 E430 2298 178F\n> >\n> >\n> > Links:\n> > ------\n> > [1] https://powswap.com\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211227/a1d82a30/attachment.html>"
            },
            {
                "author": "Jeremy",
                "date": "2021-12-24T17:17:16",
                "message_text_only": "On Fri, Dec 24, 2021, 8:42 AM Prayank <prayank at tutanota.de> wrote:\n\n> Hi Jeremy,\n>\n> > Wheres the info come from? Well, multiple places. We could get it from a\n> third party (maybe using an attestation chain of some sort?), or there are\n> certain ways it could be self-referential (like for powswap\n> <https://powswap.com>).\n>\n> > Now let\u2019s define a threshold oracle \u2013 we wouldn\u2019t want to trust just one\n> lousy oracle, so let\u2019s trust M out of N of them!\n>\n> Similar approach is used in discreet log contracts for multi oracles.\n> There is even a project for P2P derivatives but it was not used for any\n> real trades on mainnet or further developed. What difference would OP_CTV\n> make in this project if its implemented in Bitcoin?\n>\n> https://github.com/p2pderivatives/p2pderivatives-client\n>\n> https://github.com/p2pderivatives/p2pderivatives-server\n>\n> https://github.com/p2pderivatives/p2pderivatives-oracle\n>\n\nDiscussed a bit here\nhttps://twitter.com/JeremyRubin/status/1473175356366458883?t=7U4vI4CYIM82vNc8T8n6_g&s=19\n\n\nA core benefit is unilateral opens. I.e. you can pay someone into a\nderivative without them being online.\n\n\nFor example, you want to receive your payment in a Bitcoin backed Magnesium\nrisk reversal in exchange for some phys magnesium. I can create the\ncontract with your signing keys offline.\n\n>\n>\n> > Does this NEED CTV?\n> No, not in particular. Most of this stuff could be done with online signer\n> server federation between you and counterparty. CTV makes some stuff nicer\n> though, and opens up new possibilities for opening these contracts\n> unilaterally.\n>\n> Nicer? How would unilateral derivatives work because my understanding was\n> that you always need a peer to take the other side of the trade. I wish we\n> could discuss this topic in a trading community with some Bitcoiners that\n> even had some programming knowledge.\n>\n> Derivatives are interesting and less explored or used in Bitcoin projects.\n> They could be useful in solving lot of problems.\n>\n>\nI have a decent understanding of a bit of the trading world and can answer\nmost questions you have, or point you to someone else who would.\n\n\nThe way a unilateral option would work is that I can create a payment to\nyou paying you into an Option expiring next week that gives you the right\nto purchase from me a magnesium risk reversal contract that settles next\nmonth.\n\n\n\nAn example where this type of pattern must be used is in conjunction with\nDCFMP and PowSwap where miners could commit to, instead of just keys,\n'trade specs' and an Automatic market maker inside the DCFMP could attempt\nto match that miner to a counterparty who wants the opposite hashrate\nhedge. The need to exchange signatures would make this unviable otherwise.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211224/c594082e/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Derivatives and Options",
            "categories": [
                "bitcoin-dev",
                "Bitcoin Advent Calendar"
            ],
            "authors": [
                "Thibaut Le Guilly",
                "Jeremy",
                "Prayank",
                "email at yancy.lol"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 10429
        }
    },
    {
        "title": "[bitcoin-dev] [Bitcoin Advent Calendar] POWSWAP: Oracle Free Bitcoin Hashrate Derivatives",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-22T00:25:04",
                "message_text_only": "Hi devs,\n\nToday's post details how to make fully trustless hashrate derivative\ncontracts that can be embedded on-chain, inside of channels, options, or\ninside of DCFMPs. These contracts can be used today without CTV, but they\nobviously get better with CTV :)\n\nenjoy: https://rubin.io/bitcoin/2021/12/21/advent-24/\n\nI have not done any work to analyze the profitability of these contracts or\nhow you might price and risk them, or if a two sided market among miners\nactually exists. That's not really my expertise.\n\nBut maybe someone can figure that out and let us all know :)\n\ncheers,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211221/ec303376/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "POWSWAP: Oracle Free Bitcoin Hashrate Derivatives",
            "categories": [
                "bitcoin-dev",
                "Bitcoin Advent Calendar"
            ],
            "authors": [
                "Jeremy"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 859
        }
    },
    {
        "title": "[bitcoin-dev] [Bitcoin Advent Calendar] Decentralized Autonomous Organizations (DAOs) Will Save Bitcoin",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-23T03:47:06",
                "message_text_only": "Hi Devs,\n\nEnjoy! https://rubin.io/bitcoin/2021/12/22/advent-25/\n\nI'm really excited about opportunities for capital formation to happen\nnatively in Bitcoin. This is actually a really big deal and something (I\nthink) to pay close attention to. This is basically like running a little\ncompany with shareholders inside of Bitcoin, which to me really helps us\ninhabit the \"be your own bank\" part of Bitcoin. None of this particularly\nrequires CTV, but it does require the type of composable and flexible\nsoftware that I aspire to deliver with Sapio.\n\nbusiness matter:\n\nThere are two more posts, and they will both be focused on getting this\nstuff out into the wild more. If you particularly have thoughts on BIP-119\nactivation I would love to hear them publicly, or at your preference,\nprivately.\n\nIf you like or dislike BIP-119 and wish to \"soft-signal\" yes or no\npublicly, you may do so on https://utxos.org/signals by editing the\nappropriate file(s) and making a PR. Alternatively, comment somewhere\npublicly I can link to, send it to me, and I will make the edits.\n\nedit links:\n- for individuals/devs:\nhttps://github.com/JeremyRubin/utxos.org/edit/master/data/devs.yaml\n- organizations:\nhttps://github.com/JeremyRubin/utxos.org/edit/master/data/bizs.yaml\n- miners/pools:\nhttps://github.com/JeremyRubin/utxos.org/edit/master/data/hashratesnapshot.json\n\nBest,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211222/1ec13f4d/attachment.html>"
            },
            {
                "author": "Oscar Lafarga",
                "date": "2021-12-23T18:10:48",
                "message_text_only": ">\n> None of this particularly requires CTV, but it does require the type of\n> composable and flexible software that I aspire to deliver with Sapio.\n\n\n Does this imply that there is some kind of Sapio client to be run\nalongside a Bitcoin full node similar to how a Lightning node would\noperate? If so, are the computation, bandwidth, and liveness requirements\nfor someone running Sapio contracts more or less comparable to Lightning?\n\nThe implementation approach seems pretty interesting overall and reminds me\nof concepts like bitcoin bug bounties (https://bitcoinacks.com/ for\nexample) but perhaps with the potential for more sophisticated\nfunctionality.\n\nWill try to take a closer look at the https://github.com/sapio-lang/sapio\nif that seems like a recommended starting point.\n\nThanks!\n\nOn Wed, Dec 22, 2021 at 10:47 PM Jeremy via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi Devs,\n>\n> Enjoy! https://rubin.io/bitcoin/2021/12/22/advent-25/\n>\n> I'm really excited about opportunities for capital formation to happen\n> natively in Bitcoin. This is actually a really big deal and something (I\n> think) to pay close attention to. This is basically like running a little\n> company with shareholders inside of Bitcoin, which to me really helps us\n> inhabit the \"be your own bank\" part of Bitcoin. None of this particularly\n> requires CTV, but it does require the type of composable and flexible\n> software that I aspire to deliver with Sapio.\n>\n> business matter:\n>\n> There are two more posts, and they will both be focused on getting this\n> stuff out into the wild more. If you particularly have thoughts on BIP-119\n> activation I would love to hear them publicly, or at your preference,\n> privately.\n>\n> If you like or dislike BIP-119 and wish to \"soft-signal\" yes or no\n> publicly, you may do so on https://utxos.org/signals by editing the\n> appropriate file(s) and making a PR. Alternatively, comment somewhere\n> publicly I can link to, send it to me, and I will make the edits.\n>\n> edit links:\n> - for individuals/devs:\n> https://github.com/JeremyRubin/utxos.org/edit/master/data/devs.yaml\n> - organizations:\n> https://github.com/JeremyRubin/utxos.org/edit/master/data/bizs.yaml\n> - miners/pools:\n> https://github.com/JeremyRubin/utxos.org/edit/master/data/hashratesnapshot.json\n>\n> Best,\n>\n> Jeremy\n>\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n> <https://twitter.com/JeremyRubin>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n\n\n-- \nOscar Lafarga\nhttps://www.setlife.network\n<https://www.setdev.io/>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211223/1e853899/attachment-0001.html>"
            },
            {
                "author": "Jeremy",
                "date": "2021-12-23T19:30:50",
                "message_text_only": "Oscar,\n\nSapio is essentially a 'Compiler toolchain' you run it once and then send\nmoney to the contract. This is like Solidity in Ethereum.\n\nSapio Studio is a GUI for interacting with the outputs of a Sapio contract.\nThis is like Metamask/web3.js in Ethereum.\n\nIt's really not comparable to Lightning.\n\nrecommend starting with learn.sapio-lang.org :)\n\nCheers,\n\nJeremy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211223/d74044ef/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Decentralized Autonomous Organizations (DAOs) Will Save Bitcoin",
            "categories": [
                "bitcoin-dev",
                "Bitcoin Advent Calendar"
            ],
            "authors": [
                "Jeremy",
                "Oscar Lafarga"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 5048
        }
    },
    {
        "title": "[bitcoin-dev] [Bitcoin Advent Calendar] History and Future of Sapio",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-23T19:00:48",
                "message_text_only": "Hi devs,\n\nThis post details a little on the origins of Sapio as well as features that\nare in development this year (other than bugfixes).\n\nhttps://rubin.io/bitcoin/2021/12/23/advent-26/\n\ncheers,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211223/0350e0c4/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "History and Future of Sapio",
            "categories": [
                "bitcoin-dev",
                "Bitcoin Advent Calendar"
            ],
            "authors": [
                "Jeremy"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 470
        }
    },
    {
        "title": "[bitcoin-dev] [Bitcoin Advent Calendar] RoadMap or Load o' Crap?",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-24T21:49:19",
                "message_text_only": "Devs,\n\nFor the final post of the advent calendar, a discussion around feasible\nschedules for deployment of upgrades to Bitcoin and the relevant window of\nopportunity for BIP-119 in 2022.\n\nHopefully this serves as a good launch point for discussion.\n\nhttps://rubin.io/bitcoin/2021/12/24/advent-27/\n\nMerry Christmas to those who celebrate, thanks for coming along for the\nadvent calendar ride!\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211224/d9d22d22/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "RoadMap or Load o' Crap?",
            "categories": [
                "bitcoin-dev",
                "Bitcoin Advent Calendar"
            ],
            "authors": [
                "Jeremy"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 667
        }
    },
    {
        "title": "[bitcoin-dev] BIP-119 Deployment and Review Workshops",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-12-30T20:28:48",
                "message_text_only": "Dear Bitcoin Developers,\n\nI've selected a time (every 2 weeks on Tuesday 12:00 PM PT starting January\n11th in Liber ##ctv-bip-review) to host a recurring meeting to discuss\nBIP-119 review and deployment topics.\n\nBefore the meeting, please send me any topics you would like to cover & I\nwill circulate a preliminary agenda incorporating feedback in advance of\nthe meeting.\n\nThe rough plan will be to coordinate review, testing, and iron out any\nwrinkles around release timelines and procedures based on my request for\ncomment here https://rubin.io/bitcoin/2021/12/24/advent-27/. Meetings will\nbe moderated tightly to remain on topic, but all relevant topics will be\nwelcome (similar to how I moderated the meetings I hosted for Taproot).\n\nParticipation in these meetings does not signify any sort of endorsement of\nBIP-119's inclusion, nor a specific roadmap, but rather to carry on the\n'gradient descent process' of consensus in an open, interactive, and\nproductive format. Logs will be available via gnusha.\n\nHappy New Year,\n\nJeremy\n\np.s. If you plan to attend feel free to drop me a private note / reply\nhere, otherwise I may ping you to ask if you can attend.\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211230/db7bf110/attachment.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: not available\nType: text/calendar\nSize: 999 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211230/db7bf110/attachment.ics>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: BIP-119 Events_9k5gabum1lca4vs00rsk9bcrhk at group.calendar.google.com.ics\nType: application/ics\nSize: 1041 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211230/db7bf110/attachment.bin>"
            },
            {
                "author": "Heritage Samuel Falodun",
                "date": "2021-12-31T16:56:17",
                "message_text_only": "Okay Jeremy, I plan to attend.\n\nOn Thu, Dec 30, 2021 at 9:29 PM Jeremy via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Dear Bitcoin Developers,\n>\n> I've selected a time (every 2 weeks on Tuesday 12:00 PM PT starting\n> January 11th in Liber ##ctv-bip-review) to host a recurring meeting to\n> discuss BIP-119 review and deployment topics.\n>\n> Before the meeting, please send me any topics you would like to cover & I\n> will circulate a preliminary agenda incorporating feedback in advance of\n> the meeting.\n>\n> The rough plan will be to coordinate review, testing, and iron out any\n> wrinkles around release timelines and procedures based on my request for\n> comment here https://rubin.io/bitcoin/2021/12/24/advent-27/. Meetings\n> will be moderated tightly to remain on topic, but all relevant topics will\n> be welcome (similar to how I moderated the meetings I hosted for Taproot).\n>\n> Participation in these meetings does not signify any sort of endorsement\n> of BIP-119's inclusion, nor a specific roadmap, but rather to carry on the\n> 'gradient descent process' of consensus in an open, interactive, and\n> productive format. Logs will be available via gnusha.\n>\n> Happy New Year,\n>\n> Jeremy\n>\n> p.s. If you plan to attend feel free to drop me a private note / reply\n> here, otherwise I may ping you to ask if you can attend.\n>\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n> <https://twitter.com/JeremyRubin>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-- \nHeritage Samuel Falodun\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211231/b8b99b7d/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "BIP-119 Deployment and Review Workshops",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeremy",
                "Heritage Samuel Falodun"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 3854
        }
    },
    {
        "title": "[bitcoin-dev] On the regularity of soft forks",
        "thread_messages": [
            {
                "author": "Keagan McClelland",
                "date": "2021-12-31T03:10:48",
                "message_text_only": ">  But whether or not it is a basic principle of general software\nengineering kind of misses the point. Security critical software clearly\nisn't engineered in the same way as a new social media app. Bugs are easily\nreverted in a new social media app.On top of that we aren't just dealing\nwith security critical software. One of the most important objectives is to\nkeep all the nodes on the network in consensus. Introducing a consensus\nchange before we are comfortable there is community consensus for it is a\nmassive effective bug in itself. The network can split in multiple ways\ne.g. part of the network disagrees on whether to activate the consensus\nchange, part of the network disagrees on how to resist that consensus\nchange, part of the network disagrees on how to activate that consensus\nchange etc\n\n>  A consensus change is extremely hard to revert and probably requires a\nhard fork, a level of central coordination we generally attempt to avoid\nand a speed of deployment that we also attempt to avoid.\n\nThis seems to assert the idea that soft forks are all the same: they are\nnot. For instance a soft fork, lowering the block subsidy is completely\ndifferent than changing the semantics of an OP_NOP to have semantics that\nmay reject a subset of the witnesses that attest to the transactions\npermissibility. As a result, reversion means two entirely different things\nin these contexts. While a strict reversion of both soft forks is by\ndefinition a hard fork, the requirement of reversion as a result of\nundesired behavior is not the same. In the case of opcodes, there is almost\nnever a requirement to revert it. If you don't like the way the opcodes\nbehave, then you just don't use them. If you don't like the reduction of\nthe block subsidy, well that's a much bigger problem.\n\nI make this point to elucidate the idea that we cannot treat SoftForks\u2122 as\na single monolithic idea. Perhaps we need to come up with better\nterminology to be specific about what each fork actually is. The soft vs.\nhard distinction is a critical one but it is not enough and treating soft\nforks that are noninvasive such as OP_NOP tightenings. This has been\nproposed before [1], and while I do not necessarily think the terms cited\nare necessarily complete, they admit the low resolution of our current\nterminology.\n\n> Soft fork features can (and should) obviously be tested thoroughly on\ntestnet, signet, custom signets, sidechains etc on a standalone basis and a\nbundled basis.\n\nI vehemently disagree that any consensus changes should be bundled,\nespecially when it comes to activation parameters. When we start to bundle\nthings, we amplify the community resources needed to do review, not reduce\nthem. I suspect your opinion here is largely informed by your frustration\nwith the Taproot Activation procedure that you underwent earlier this year.\nThis is understandable. However, let me present the alternative case. If we\nstart to bundle features, the review of the features gets significantly\nharder. As the Bitcoin project scales, the ability of any one developer to\nunderstand the entire codebase declines. Bundling changes reduces the\nnumber of people who are qualified to review a particular proposal, and\neven worse, intimidates people who may be willing and able to review\nlogically distinct portions of the proposal, resulting in lower amounts of\nreview overall. This will likely have the opposite effect of what you seem\nto desire. BIP8 and BIP9 give us the ability to have multiple independent\nsoft forks in flight at once. Choosing to bundle them instead makes little\nsense when we do not have to. Bundling them will inevitably degenerate into\npolitical horse trading and everyone will be worse off for it.\n\n> part of the network disagrees on whether to activate the consensus\nchange, part of the network disagrees on how to resist that consensus\nchange, part of the network disagrees on how to activate that consensus\nchange etc\n\nDisagreements, and by extension, forks are a part of Bitcoin. What is\nimportant is that they are well defined and clean. This is the reason why\nthe mandatory signaling period exists in BIP8/9, so that clients that\nintend to reject the soft fork change have a very easy means of doing so in\na clean break where consensus is clearly divergent. In accordance with\nthis, consensus changes should be sequenced so that people can decide which\nsides of the forks they want to follow and that the economic reality can\nreorganize around that. If choose to bundle them, you have one of two\noutcomes: either consensus atomizes into a mist where people have different\nideas of which subsets of a soft fork bundle they want to adopt, or what\nlikely comes after is a reconvergence on the old client with none of the\nsoft fork rules in place. This will lead to significantly more confusion as\nwell given that with sufficient miner consensus some of the rules may stick\nanyway even if the rest of the user base reconverges on the old client.\n\nIt is quite likely less damaging to consensus to have frequent but strictly\nsequenced soft forks so that if one of the new rules is contentious the\nbreak can happen cleanly. That said, if Core or any other client wishes to\ncut a release of the software with the parameters bundled into a single\nrelease, that is a significantly more palatable state of affairs, as you\ncan still pipeline signaling and activation. However, the protocol itself\nadopting a tendency to activate unrelated proposals in bundles is a recipe\nfor disaster.\n\n\nRespectfully,\nKeagan\n\n\n[1] https://www.truthcoin.info/blog/protocol-upgrade-terminology\n\nOn Sat, Oct 16, 2021 at 12:57 PM Michael Folkson via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> > Interesting discussion. Correct me if I'm wrong: but putting too many\n> features together in one shot just can't make things harder to debug in\n> production if something very unexpected happens. It's a basic principle\n> of software engineering.\n>\n> Soft fork features can (and should) obviously be tested thoroughly on\n> testnet, signet, custom signets, sidechains etc on a standalone basis and a\n> bundled basis. But whether or not it is a basic principle of general\n> software engineering kind of misses the point. Security critical software\n> clearly isn't engineered in the same way as a new social media app. Bugs\n> are easily reverted in a new social media app. A consensus change is\n> extremely hard to revert and probably requires a hard fork, a level of\n> central coordination we generally attempt to avoid and a speed of\n> deployment that we also attempt to avoid. On top of that we aren't just\n> dealing with security critical software. One of the most important\n> objectives is to keep all the nodes on the network in consensus.\n> Introducing a consensus change before we are comfortable there is community\n> consensus for it is a massive effective bug in itself. The network can\n> split in multiple ways e.g. part of the network disagrees on whether to\n> activate the consensus change, part of the network disagrees on how to\n> resist that consensus change, part of the network disagrees on how to\n> activate that consensus change etc\n>\n> In addition, a social media app can experiment in production whether\n> Feature A works, whether Feature B works or whether Feature A and B work\n> best together. In Bitcoin if we activate consensus Feature A, later decide\n> we want consensus Feature B but find out that by previously activating\n> Feature A we can't have Feature B (it is now unsafe to activate it) or its\n> design now has to be suboptimal because we have to ensure it can safely\n> work in the presence of Feature A we have made a mistake by activating\n> Feature A in the first place. Decentralized security critical consensus\n> changes are an emerging field in itself and really can't be treated like\n> any other software project. This will become universally understood I'm\n> sure over time.\n>\n> --Michael Folkson\n> Email: michaelfolkson at protonmail.com\n> Keybase: michaelfolkson\n> PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n>\n>\n> \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n> On Friday, October 15th, 2021 at 1:43 AM, Felipe Micaroni Lalli via\n> bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> Interesting discussion. Correct me if I'm wrong: but putting too many\n> features together in one shot just can't make things harder to debug in\n> production if something very unexpected happens. It's a basic principle\n> of software engineering.\n>\n> Change. Deploy. Nothing bad happened? Change it a little more. Deployment.\n> Or: Change, change, change. Deploy. Did something bad happen? What change\n> caused the problem?\n>\n> On Thu, Oct 14, 2021 at 8:53 PM Anthony Towns via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> On Mon, Oct 11, 2021 at 12:12:58PM -0700, Jeremy via bitcoin-dev wrote:\n>> > > ... in this post I will argue against frequent soft forks with a\n>> single or\n>> > minimal\n>> > > set of features and instead argue for infrequent soft forks with\n>> batches\n>> > > of features.\n>> > I think this type of development has been discussed in the past and has\n>> been\n>> > rejected.\n>>\n>> > AJ: - improvements: changes might not make everyone better off, but we\n>> >    don't want changes to screw anyone over either -- pareto\n>> >    improvements in economics, \"first, do no harm\", etc. (if we get this\n>> >    right, there's no need to make compromises and bundle multiple\n>> >    flawed proposals so that everyone's an equal mix of happy and\n>> >    miserable)\n>>\n>> I don't think your conclusion above matches my opinion, for what it's\n>> worth.\n>>\n>> If you've got two features, A and B, where the game theory is:\n>>\n>>  If A happens, I'm +100, You're -50\n>>  If B happens, I'm -50, You're +100\n>>\n>> then even though A+B is +50, +50, then I do think the answer should\n>> generally be \"think harder and come up with better proposals\" rather than\n>> \"implement A+B as a bundle that makes us both +50\".\n>>\n>> _But_ if the two features are more like:\n>>\n>>   If C happens, I'm +100, You're +/- 0\n>>   If D happens, I'm +/- 0, You're +100\n>>\n>> then I don't have a problem with bundling them together as a single\n>> simultaneous activation of both C and D.\n>>\n>> Also, you can have situations where things are better together,\n>> that is:\n>>\n>>   If E happens, we're both at +100\n>>   If F happens, we're both at +50\n>>   If E+F both happen, we're both at +9000\n>>\n>> In general, I think combining proposals when the combination is better\n>> than the individual proposals were is obviously good; and combining\n>> related proposals into a single activation can be good if it is easier\n>> to think about the ideas as a set.\n>>\n>> It's only when you'd be rejecting the proposal on its own merits that\n>> I think combining it with others is a bad idea in principle.\n>>\n>> For specific examples, we bundled schnorr, Taproot, MAST, OP_SUCCESSx\n>> and CHECKSIGADD together because they do have synergies like that; we\n>> didn't bundle ANYPREVOUT and graftroot despite the potential synergies\n>> because those features needed substantially more study.\n>>\n>> The nulldummy soft-fork (bip 147) was deployed concurrently with\n>> the segwit soft-fork (bip 141, 143), but I don't think there was any\n>> particular synergy or need for those things to be combined, it just\n>> reduced the overhead of two sets of activation signalling to one.\n>>\n>> Note that the implementation code for nulldummy had already been merged\n>> and were applied as relay policy well before activation parameters were\n>> defined (May 2014 via PR#3843 vs Sep 2016 for PR#8636) let alone becoming\n>> an active soft fork.\n>>\n>> Cheers,\n>> aj\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211230/6bc5d2f3/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "On the regularity of soft forks",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Keagan McClelland"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 12247
        }
    },
    {
        "title": "[bitcoin-dev] Covenants and capabilities in the UTXO model",
        "thread_messages": [
            {
                "author": "Bram Cohen",
                "date": "2021-12-31T23:22:08",
                "message_text_only": "There are a few different approaches to adding covenants and capabilities\nto the UTXO model with varying tradeoffs. It turns out that it can be done\nwhile making very few but not quite zero compromises to practices Bitcoin\nhas been following so far.\n\nFirst, the good news: Full support for both capabilities and covenants can\nbe added without changing the UTXO model whatsoever by adding some more\nprogrammatic capabilities to the language and doing some programmatic\ntricks. Since scriptpubkeys/scriptsigs continue to run ephemerally at\nvalidation time full turing completeness is much less dangerous than people\nfear. The main thing missing from what's expressed in transactions\nthemselves is a coherent notion of a single parent of each output instead\nof the all-inputs-lead-to-all-outputs approach of transactions currently.\nIt would also probably be a good idea to add in a bunch of special purpose\nopcodes for making coherent statements about transactions since in Bitcoin\nthey're a very complex and hard to parse format.\n\nNow for the controversial stuff. Once you start implementing complex\ngeneral purpose functionality it tends to get very expensive very fast and\nis likely impractical unless there's a way to compress or at least\nde-duplicate snippets of code which are repeated on chain. Currently\nBitcoin has a strong policy that deciding which transactions to let into a\nblock for maximum fee is a strictly linear optimization problem and while\nit's possible to keep things mostly that way making it completely strict is\nunlikely to workable. About as close as you can get is to make it so that\neach block can reference code snippets in previous blocks for\ndeduplication, so at least the optimization is linear for each block by\nitself.\n\nHaving covenants and capabilities at all is controversial in and of itself.\nWith covenants the main issue is whether they're opt-in or opt-out. For a\npayment to someone to come with a rider where they could accept it and\nthink their system was working properly for a while until you exercised\nsome kind of retroactive veto on new action or even clawback would\nobviously be unacceptable behavior. But for payments to come with covenants\nbut the recipient not even be able to parse them unless they're fully\nbuying into that behavior is much more reasonable.\n\nThe main issue which people have raised with capabilities is that if you\nwere to have colored coins whose value was substantially greater than the\nchain they were tokenized on then that could potentially create a business\nmodel for attacking the underlying chain. While this is a real concern\ntokenized assets have been out for a while now and have never come close to\ncausing this to happen, so maybe people aren't so worried about it now.\n\nGiven all the above caveats it turns out one weird trick is all you need to\nsupport general purpose capabilities: for a UTXO to have a capability its\nscriptpubkey asserts that its parent must either be the originator of that\ncapability or also conform to the same parent-asserting format. More\ncomplex functionality such as supporting on-chain verifiable colored coins\ncan also be done but it follows the same pattern: Capabilities are\nimplemented as backwards pointing covenants.\n\nIf you'd like to see a fleshed out implementation of these ideas (albeit in\na slightly different model) there's quite a bit of stuff on chialisp.com\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211231/dac00321/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Covenants and capabilities in the UTXO model",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Bram Cohen"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 3564
        }
    }
]