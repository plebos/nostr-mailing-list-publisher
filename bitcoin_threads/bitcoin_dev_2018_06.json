[
    {
        "title": "[bitcoin-dev] Should Graftroot be optional?",
        "thread_messages": [
            {
                "author": "Pieter Wuille",
                "date": "2018-06-01T00:25:04",
                "message_text_only": "On Fri, May 25, 2018 at 3:14 AM, Johnson Lau <jl2012 at xbt.hk> wrote:\n> A graftroot design like this is a strict subset of existing signature checking rules. If this is dangerous, the existing signature checking rules must be dangerous.\n\nWhile you may be right in this situation, I'm not sure that conclusion\nfollows from your argument. Whether or not a construction is safe does\nnot just depend on the consensus rules, but also on how it is used.\nOtherwise you could as well argue that since OP_TRUE is possible right\nnow which is obviously insecure, nothing more dangerous can be\naccomplished through any soft fork.\n\nThe best argument for why Graftroot does not need to be optional I\nthink was how Greg put it: \"since the signer(s) could have signed an\narbitrary transaction instead, being able to delegate is strictly less\npowerful.\".\n\nCheers,\n\n-- \nPieter"
            },
            {
                "author": "Tim Ruffing",
                "date": "2018-06-06T12:48:01",
                "message_text_only": "I haven't read the original Graftroot thread, so maybe all of this has\nb\neen discussed already or is just wrong... Please correct me if this\nis\nthe case.\n\nOn Thu, 2018-05-31 at 17:25 -0700, Pieter Wuille via bitcoin-dev wrote:\n> The best argument for why Graftroot does not need to be optional I\n> think was how Greg put it: \"since the signer(s) could have signed an\n> arbitrary transaction instead, being able to delegate is strictly\n> less\n> powerful.\".\n\nI'm trying to get a more abstract view of the problem. One issue with\nGreg's argument is the following:\n\nIf g-script is a script (containing a public key) that allows for\nGraftroot spending, then the following \"flow\" of coins is valid: \n\n  g-script --g-sig--> script1 ---tx2---> script2\n\nHere, g-sig is a Graftroot signature on script1 and tx2 is a\ntransaction that fulfills script1 and sends to script2. In other words,\nthe only transaction involved here is tx2; it provides g-sig, script1,\nand a solution for tx1, and it spends to script2.\n\nNow Greg's argument (as I understand it) is that this can be already\ndone without Grafroot with two transactions, namely a normal\ntransaction tx1 that spends g-script normally and tx2 that spends tx1\nto script1.\n\n  g-script ---tx1---> script1 ---tx2---> script2.\n\nSo far, so good. A difference however is that g-sig *alone* can't be\ncommitted to the chain but tx1 alone can be committed to the chain.\nThat means\n\n  g-script --g-sig--> script1             (*)\n\nis \"incomplete\" but\n\n  g-script ---tx1---> script1             (**)\n\nis a perfectly valid transaction that can be committed to the chain. So\nI think Graftroot delegation is not \"strictly less powerful\" than just\nusing a normal transaction: Graftroot enables to delegate in a way such\nthat the delegation itself cannot be fixed in the chain. I think this\nis not possible currently. (Okay, you can just pass around the secret\nkeys but has other problems obviously).\n\nDoes this have practical implications?\nI don't see any but maybe this helps someone to identify an undesirable\nimplication.\n\nOne way to be on the safe side and probably make Greg's argument go\nthrough is to just define the semantics such that (*) is allowed, i.e.,\ncall g-sig a \"Graftroot transaction\" and give it transaction semantics.\nThis provides a new perspective on Graftroot: Then Graftroot does not\nintroduce new semantics but (*) is just an optimized version of (**)\nthat uses fewer bytes and may be better for privacy. \n\nInterestingly Andrew's blind-sig example and Johnson's fix (g-sig signs\nthe outpoint) are just a special case. If g-sig has transaction\nsemantics, it must sign the outpoint (and other stuff).\n\nNow you can say that this is not really useful: if g-sig is essentially\na full transaction that can committed to the blockchain, then it needs\nto specify inputs, outputs etc. So all the optimizations are lost and\nthose were the reason we want to introduce Grafroot in the first place.\n\nBut one observation here is that g-sig only needs to be a full\ntransaction if it's used standalone as in (*). If we want to have \n\n  g-script --g-sig--> script1 ---tx2---> script2\n\n(and this should be the common case) then just the bare signature  and\nscript1 suffices, as in the Graftroot proposal. In some sense, inputs\nand outputs of the Graftroot transaction are just implicit in this\ncase.\n\nAnother way to look at this that instead of providing a transaction\nwith g-sig, script1, and a solution for script1, you can also choose to\nprovide a transaction with only g-sig and script1 (and don't solve\nscript1), which then just sends to script1.\n\nI'm not saying that it's worth the hassle to add this possibility\nwithout being aware of a problem that arises if we don't add it -- but\nmaybe my thoughts provide another perspective on the issue.\n\nBest,\nTim"
            },
            {
                "author": "Pieter Wuille",
                "date": "2018-06-06T17:04:23",
                "message_text_only": "On Wed, Jun 6, 2018 at 5:48 AM, Tim Ruffing via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> On Thu, 2018-05-31 at 17:25 -0700, Pieter Wuille via bitcoin-dev wrote:\n>> The best argument for why Graftroot does not need to be optional I\n>> think was how Greg put it: \"since the signer(s) could have signed an\n>> arbitrary transaction instead, being able to delegate is strictly\n>> less\n>> powerful.\".\n\n...\n\n> So\n> I think Graftroot delegation is not \"strictly less powerful\" than just\n> using a normal transaction: Graftroot enables to delegate in a way such\n> that the delegation itself cannot be fixed in the chain. I think this\n> is not possible currently. (Okay, you can just pass around the secret\n> keys but has other problems obviously).\n>\n> Does this have practical implications?\n> I don't see any but maybe this helps someone to identify an undesirable\n> implication.\n\nInteresting point; I don't see any relevant implications to this\neither, but it's indeed good to point out this as a distinction.\n\n> One way to be on the safe side and probably make Greg's argument go\n> through is to just define the semantics such that (*) is allowed, i.e.,\n> call g-sig a \"Graftroot transaction\" and give it transaction semantics.\n> This provides a new perspective on Graftroot: Then Graftroot does not\n> introduce new semantics but (*) is just an optimized version of (**)\n> that uses fewer bytes and may be better for privacy.\n\nSo you're saying: the Graftroot signature data could be made identical\nto the signature hash of an implicit 1-input-1-output transaction\nspending the coin and creating a new output with the delegated script\nas sPK, and the same amount.\n\nI like that idea, but I don't think it can be *exactly* that. If it's\npossible to take a Graftroot signature and instead construct a\ntransaction with it, you have inherently introduced a malleability.\nThe created outpoint will be different in both cases (different txid),\nmeaning that a chain of dependent unconfirmed transactions may be\nbroken by giving one participant the ability to choose between\nGraftroot delegation or actual spending.\n\nTwo points here: (1) the implicit transaction would be 0 fee (unless\nwe somehow assign a portion of the fee to the delegation itself for\npurposes of sighash computing), and (2) this sounds very similar to\nthe issue SIGHASH_NOINPUT is intended to solve. About that...\n\n> Interestingly Andrew's blind-sig example and Johnson's fix (g-sig signs\n> the outpoint) are just a special case. If g-sig has transaction\n> semantics, it must sign the outpoint (and other stuff).\n\nYou're right when you're comparing with existing transaction sighash\nsemantics, but not when SIGHASH_NOINPUT would exist. If that were the\ncase, the only real difference is your point above of not being able\nto commit the implicit transaction separately. In other words, we're\nback to something Johnson pointed out earlier: some of the perceived\nproblems with Graftroot are also issues with SIGHASH_NOINPUT.\n\nI wonder if we can make this explicit: Graftroot spending becomes a\nspecial sighash flag (which possibly is only allowed at the top level)\n- it builds an implicit transaction which moves all the coins to a\nnewly provided script, computes the sighash of that transaction\n(taking all of the Graftroot signature's sighash flags into account -\nincluding potentially SIGHASH_NOINPUT), and requires a signature with\nthat. The delegated script is then evaluated in the context of that\nimplicit transaction.\n\nHowever, in order to avoid the malleability issue I think the actual\nsignature should still be different - possibly by simply passing\nthrough the Graftroot sighash flag into the sighash being computed.\n\nCheers,\n\n-- \nPieter"
            },
            {
                "author": "Tim Ruffing",
                "date": "2018-06-06T21:25:33",
                "message_text_only": "What you're saying makes sense.\n\nBy the way, an even stronger reason why you shouldn't be able to\n\"repurpose\" just a Graftroot signature as a transaction: You may want\nto reveal to others that you've delegated. But if an observer sees the\ndelegation (literally the Graftroot signature), this observer could\nsend the Graftroot signature to the network (and lock out the other\ndelegates and the initial owner). So you would need to keep the\nsignature itself secret, otherwise we can't call this delegation.\n\nSo it may sense to consider the idea of an implicit transaction for the\ncase when one really solves the delegated script (as you mentioned) but\nonly in this case.\n\nTim\n\n\nOn Wed, 2018-06-06 at 10:04 -0700, Pieter Wuille wrote:\n> On Wed, Jun 6, 2018 at 5:48 AM, Tim Ruffing via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > On Thu, 2018-05-31 at 17:25 -0700, Pieter Wuille via bitcoin-dev\n> > wrote:\n> > > The best argument for why Graftroot does not need to be optional\n> > > I\n> > > think was how Greg put it: \"since the signer(s) could have signed\n> > > an\n> > > arbitrary transaction instead, being able to delegate is strictly\n> > > less\n> > > powerful.\".\n> \n> ...\n> \n> > So\n> > I think Graftroot delegation is not \"strictly less powerful\" than\n> > just\n> > using a normal transaction: Graftroot enables to delegate in a way\n> > such\n> > that the delegation itself cannot be fixed in the chain. I think\n> > this\n> > is not possible currently. (Okay, you can just pass around the\n> > secret\n> > keys but has other problems obviously).\n> > \n> > Does this have practical implications?\n> > I don't see any but maybe this helps someone to identify an\n> > undesirable\n> > implication.\n> \n> Interesting point; I don't see any relevant implications to this\n> either, but it's indeed good to point out this as a distinction.\n> \n> > One way to be on the safe side and probably make Greg's argument go\n> > through is to just define the semantics such that (*) is allowed,\n> > i.e.,\n> > call g-sig a \"Graftroot transaction\" and give it transaction\n> > semantics.\n> > This provides a new perspective on Graftroot: Then Graftroot does\n> > not\n> > introduce new semantics but (*) is just an optimized version of\n> > (**)\n> > that uses fewer bytes and may be better for privacy.\n> \n> So you're saying: the Graftroot signature data could be made\n> identical\n> to the signature hash of an implicit 1-input-1-output transaction\n> spending the coin and creating a new output with the delegated script\n> as sPK, and the same amount.\n> \n> I like that idea, but I don't think it can be *exactly* that. If it's\n> possible to take a Graftroot signature and instead construct a\n> transaction with it, you have inherently introduced a malleability.\n> The created outpoint will be different in both cases (different\n> txid),\n> meaning that a chain of dependent unconfirmed transactions may be\n> broken by giving one participant the ability to choose between\n> Graftroot delegation or actual spending.\n> \n> Two points here: (1) the implicit transaction would be 0 fee (unless\n> we somehow assign a portion of the fee to the delegation itself for\n> purposes of sighash computing), and (2) this sounds very similar to\n> the issue SIGHASH_NOINPUT is intended to solve. About that...\n> \n> > Interestingly Andrew's blind-sig example and Johnson's fix (g-sig\n> > signs\n> > the outpoint) are just a special case. If g-sig has transaction\n> > semantics, it must sign the outpoint (and other stuff).\n> \n> You're right when you're comparing with existing transaction sighash\n> semantics, but not when SIGHASH_NOINPUT would exist. If that were the\n> case, the only real difference is your point above of not being able\n> to commit the implicit transaction separately. In other words, we're\n> back to something Johnson pointed out earlier: some of the perceived\n> problems with Graftroot are also issues with SIGHASH_NOINPUT.\n> \n> I wonder if we can make this explicit: Graftroot spending becomes a\n> special sighash flag (which possibly is only allowed at the top\n> level)\n> - it builds an implicit transaction which moves all the coins to a\n> newly provided script, computes the sighash of that transaction\n> (taking all of the Graftroot signature's sighash flags into account -\n> including potentially SIGHASH_NOINPUT), and requires a signature with\n> that. The delegated script is then evaluated in the context of that\n> implicit transaction.\n> \n> However, in order to avoid the malleability issue I think the actual\n> signature should still be different - possibly by simply passing\n> through the Graftroot sighash flag into the sighash being computed.\n> \n> Cheers,\n>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-06-20T12:12:28",
                "message_text_only": "Good morning Pieter and Tim and all,\n\nMy understanding is that the idea now being discussed by Pieter and Tim is that the Graftroot signature is not `sign(P, script)` but instead `sign(P, sighash(tx))`, where `tx` is an \"ordinary\" transaction that spends the outpoint that pays to `P`, and a single output whose `scriptPubKey` is the Graftroot `script` and contains the entire value of the outpoint, and `sighash()` is the standard SegWit transaction digest algorithm used for signing (and is affected by other flags in the signature).\n\nThis has the advantage that the Graftroot signature commits to a single outpoint and cannot be used to spend all outpoints that happen to pay to the same `P` public key.\n\nHowever I believe the ability to \"immanentize\" a Graftroot signature as a signature for a 1-input 1-output transaction is unsafe (so a Graftroot signature should probably not be \"the same\" as a signature for a 1-input 1-output transaction like the above).\n\nLet us consider a simple CoinSwap protocol.  Let us focus on one half of the procedure, which is a simple ZKCP, i.e. Alice pays Bob for a hash preimage, with a timeout imposed so that Bob needs to provide the preimage, within a specified time.\n\n1.  Alice and Bob generate a shared public key P which requires k_a (Alice secret key) and k_b (Bob secret key) to sign.\n\n2.  Alice creates but does not sign a funding transaction that pays to public key P and gives its txid to Bob.  Alice also provides a standard P2WPKH return address that Alice controls.\n\n3.  Bob creates a `nLockTime`-encumbered transaction (the timeout backoff transaction) on the agreed timeout, spending the above txid outpoint to the Alice return address, and provides its half of the signature to P signing the timeout backoff transaction to Alice.\n\n4.  Alice keeps the above signature (verifying it is to the correct `nLockTime` and Alice return address), then signs and broadcasts the funding transaction.  Both wait for the funding transaction to confirm deeply.\n\n5.  Alice then signs a Graftroot to the script `{ OP_HASH <hash> OP_EQUALVERIFY <P_b> OP_CHECKSIG }` and gives its half of the signature to P signing the Graftroot to Bob.  Bob keeps this signature.\n\n6.  Bob provides the preimage to the hash directly to Alice and a standard P2WPKH destination address that Bob controls.\n\n7.  Alice then signs a direct spend of the funding transaction outpoint (one that is not encumbered by `nLockTime`), spending the funding txid outpoint to the Bob destination address, and provides its half of the signature to P signing this transaction.  This completes Alice participation in the protocol (it has now received the preimage).\n\n8.  Bob completes the signature to the destination transaction and broadcasts it to the blockchain layer.\n\nIf Alice or Bob stalls at step 5 or earlier then the transaction does not occur (Alice does not learn the preimage, Bob gets no money).\n\nIf Bob stalls at step 6, Alice can use the timeout backoff.\n\nIf Alice stalls at step 7, Bob can use the Graftroot signed at step 5 to claim its funds as long as the timeout is not reached.\n\nNow if Graftroot signature is \"actually\" just a standard signature of a transaction that is elided from the blockchain, however, it means that this elided transaction can be immanentized on the blockchain with the specified script.  Even if this transaction has e.g. no fee then Bob could collude with a miner via sidefees to get the (valid) transaction onchain.\n\nSo Bob could take the signature made at 5 to create a transaction spending to the specified script, and prevent Alice from claiming the funds using the timeout backoff transaction.  Then Bob forever controls the UTXO and Alice cannot back out of the transaction, so even if the knowledge of the preimage ceases to be interesting, Alice has already paid Bob and Bob can provide the preimage at its leisure rather than constrained by the timeout.\n\nThus we should somehow disallow immanentizing the Graftroot signature.\n\nAn idea is that the Graftroot signature should sign a transaction with a specific special `nVersion`, that is then soft-forked to be invalid onchain (i.e. the `nVersion` is reserved for Graftroot and it is invalid for a transaction onchain to use that `nVersion`).  So the Graftroot signature can be used as a Graftroot spend, but not as a immanentized signature on an actual onchain transaction that could disable the timeout backoff transaction.\n\nUtilities that can sign an arbitrary message using your private keys could check if the first four bytes match the Graftroot `nVersion` and refuse to sign such messages to prevent inadvertently giving a Graftroot signature.\n\nAlternatively, we note that the \"transaction\" signed by Graftroot will not be referred to onchain anyway, and we could use a completely different `sighash()` algorithm, e.g. it could just be the outpoint being spent and the script to be executed, i.e. `sign(P, concat(txid, outnum, script))`.  This reduces code reuse, though.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2018-06-20T14:30:55",
                "message_text_only": "On Wed, Jun 20, 2018 at 12:12 PM, ZmnSCPxj via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> This has the advantage that the Graftroot signature commits to a single outpoint and cannot be used to spend all outpoints that happen to pay to the same `P` public key.\n\nIf it isn't possible to make a graftroot signature independent of the\noutpoint then the functionality is _greatly_ reduced to the point of\nlargely mooting it-- because you could no longer prepare the grafts\nbefore the coins to be spent existed, and meaning you must stay online\nand sign new grafts as coins show up. In my view graft's two main\ngains are being able to delegate before coins exist and making the\nconditional transfer atomic (e.g. compared to just pre-signing a\ntransaction).  Making outpoint binding optional, so that you could\nchoose to either sign for particular outputs or in a blanket way would\nbe a lot more useful.\n\nThough I had assumed outpoint binding could best be achieved by\nchecking the outpoint in the graft-script-- this is general for\nwhatever kinds of arbitrary graft conditions you might want to specify\n(e.g. locktimes, value checks, or conditions on subsequent outputs)...\nbut perhaps binding a particular outpoint is enough of a special case\nthat it's worth avoiding the overhead of expressing a match condition\nin the script, since that would probably end up blowing 36 bytes for\nthe match condition in the witness when instead it could just be\ncovered by the signature, and people should probably prefer to do\noutput binding grafts whenever its reasonably possible."
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-06-21T07:09:14",
                "message_text_only": "Good morning Greg,\n\n\n> On Wed, Jun 20, 2018 at 12:12 PM, ZmnSCPxj via bitcoin-dev\n> \n> bitcoin-dev at lists.linuxfoundation.org wrote:\n> \n> > This has the advantage that the Graftroot signature commits to a single outpoint and cannot be used to spend all outpoints that happen to pay to the same `P` public key.\n> \n> If it isn't possible to make a graftroot signature independent of the\n> \n> outpoint then the functionality is greatly reduced to the point of\n> \n> largely mooting it-- because you could no longer prepare the grafts\n> \n> before the coins to be spent existed, and meaning you must stay online\n> \n> and sign new grafts as coins show up. In my view graft's two main\n> \n> gains are being able to delegate before coins exist and making the\n> \n> conditional transfer atomic (e.g. compared to just pre-signing a\n> \n> transaction). Making outpoint binding optional, so that you could\n> \n> choose to either sign for particular outputs or in a blanket way would\n> \n> be a lot more useful.\n> \n\nPerhaps `SIGHASH_NOINPUT` can do this? One can argue that the option to not commit a signature to refer to a specific outpoint is orthogonal to the option to Graftroot, so having a separate flag for that makes sense.\n\nThe proposal could then be:\n\n1. Define a transaction `nVersion` reserved for Graftroot. Transactions with that `nVersion` are disallowed in blocks.\n2. If a next-SegWit-version P2WPKH (or P2WPK) is spent, and the top witness stack item is a signature with `SIGHASH_GRAFTROOT` flag, then this is a Graftroot spend.\n3. The signature signs an imaginary 1-input 1-output tx, with the input copied from the spending tx, the output value being the entire output being spent, and the output `scriptPubKey` being the Graftroot script (second to top witness stack). The imaginary tx has the Graftroot-reserved `nVersion`.\n4. The Graftroot signature has its other flags `SIGHASH_NOINPUT` evaluated also when verifying it signs the imaginary tx.\n5. The Graftroot signature and the Graftroot script are popped and the script executed in the context of the original Graftroot-spending tx.\n\n\nThis lets users select whether committing to a specific outpoint is needed or not, independently of Graftroot.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Anthony Towns",
                "date": "2018-06-27T07:29:09",
                "message_text_only": "On Thu, May 31, 2018 at 05:25:04PM -0700, Pieter Wuille via bitcoin-dev wrote:\n> The best argument for why Graftroot does not need to be optional I\n> think was how Greg put it: \"since the signer(s) could have signed an\n> arbitrary transaction instead, being able to delegate is strictly less\n> powerful.\".\n\nThis seems persuasive to me. I think you could implement graftroot in\na way that makes this explicit:\n\n * A graftroot input has >=2 items on the witness stack, a signature,\n   a script (S), and possibly witness elements for the script. The\n   signature has a SIGHASH_GRAFTROOT bit set.\n\n * To validate the signature, a virtual transaction is constructed:\n\n     nVersion = 1\n     locktime = 0\n     inputs = [(txhash, txoutidx, 0, \"\", 0xffffffff)]\n     outputs = [(txvalue, len(S), S)]\n     locktime = 0\n\n   The signature is then checked against the virtual transaction.\n\n * If the signature is valid, the virtual transaction is discarded, and\n   the script and witness elements are checked against the original tx.\n\nI think this approach (or one like it) would make it clear that\ngraftroot is a simple optimisation, rather than changing the security\nparameters. Some caveats:\n\n * You'd presumably want to disallow signatures with SIGHASH_GRAFTROOT\n   from being used in signatures in scripts, so as not to end up having\n   to support recursive graftroot.\n\n * Checking the script/witness against the original transaction instead\n   of the virtual one cheats a bit, but something like it is necessary\n   to ensure locktime/csv checks in the script S behave sanely. You\n   could have the virtual transaction be treated as being confirmed in\n   the same block as the original transaction instead though, I think.\n\n * You would need to use SIGHASH_NOINPUT (or similar) in conjuction\n   to allow graftroot delegation prior to constructing the tx (otherwise\n   the signature would be committing to txhash/txoutidx). BIP118 would\n   still commit to txvalue, but would otherwise work fine, I think.\n\nCheers,\naj"
            }
        ],
        "thread_summary": {
            "title": "Should Graftroot be optional?",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Anthony Towns",
                "Tim Ruffing",
                "ZmnSCPxj",
                "Gregory Maxwell",
                "Pieter Wuille"
            ],
            "messages_count": 8,
            "total_messages_chars_count": 23850
        }
    },
    {
        "title": "[bitcoin-dev] BIP 158 Flexibility and Filter Size",
        "thread_messages": [
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-06-01T02:52:48",
                "message_text_only": "Hi y'all,\n\nI've made a PR to the BIP repo to modify BIP 158 based on this thread, and\nother recent threads giving feedback on the current version of the BIP:\n\n  * https://github.com/bitcoin/bips/pull/687\n\nI've also updated the test vectors based on the current parameters (and\nfilter format), and also the code used to generate the test vectors. Due to\nthe change in parametrization, the test vectors now target (P=19 M=784931),\nand there're no longer any cases related to extended filters.\n\nOne notable thing that I left off is the proposed change to use the previous\noutput script rather than the outpoint. Modifying the filters in this\nfashion would be a downgrade in the security model for light clients, as it\nwould allow full nodes to lie by omission, just as they can with BIP 37. As\nis now, if nodes present conflicting information, then the light client can\ndownload the target block, fully reconstruct the filter itself, then ban any\nnodes which advertised the incorrect filter. The inclusion of the filter\nheader checkpoints make it rather straight forward for light clients to\nbisect the state to find the conflicting advertisement, and it's strongly\nrecommended that they do so.\n\nTo get a feel for the level of impact these changes would have on existing\napplications that depend on the txid being included in the filter, I've\nimplemented these changes across btcutil, btcd, btcwallet, and lnd (which\npreviously relied on the txid for confirmation notifications). For lnd at\nleast, the code impact was rather minimal, as we use the pkScript for\nmatching a block, but then still scan the block manually to find the precise\ntransaction (by txid) that we were interested in (if it's there).\n\n-- Laolu\n\n\nOn Mon, May 28, 2018 at 9:01 PM Olaoluwa Osuntokun <laolu32 at gmail.com>\nwrote:\n\n> > The additional benefit of the input script/outpoint filter is to watch\n> for\n> > unexpected spends (coins getting stolen or spent from another wallet) or\n> > transactions without a unique change or output address. I think this is a\n> > reasonable implementation, and it would be nice to be able to download\n> that\n> > filter without any input elements.\n>\n> As someone who's implemented a complete integration of the filtering\n> technique into an existing wallet, and a higher application I disagree.\n> There's not much gain to be had in splitting up the filters: it'll result\n> in\n> additional round trips (to fetch these distinct filter) during normal\n> operation, complicate routine seed rescanning logic, and also is\n> detrimental\n> to privacy if one is fetching blocks from the same peer as they've\n> downloaded the filters from.\n>\n> However, I'm now convinced that the savings had by including the prev\n> output\n> script (addr re-use and outputs spent in the same block as they're created)\n> outweigh the additional booking keeping required in an implementation (when\n> extracting the precise tx that matched) compared to using regular outpoint\n> as we do currently. Combined with the recently proposed re-parametrization\n> of the gcs parameters[1], the filter size should shrink by quite a bit!\n>\n> I'm very happy with the review the BIPs has been receiving as of late. It\n> would've been nice to have this 1+ year ago when the draft was initially\n> proposed, but better late that never!\n>\n> Based on this thread, [1], and discussions on various IRC channels, I plan\n> to make the following modifications to the BIP:\n>\n>   1. use P=2^19 and M=784931 as gcs parameters, and also bind these to the\n>      filter instance, so future filter types may use distinct parameters\n>   2. use the prev output script rather than the prev input script in the\n>      regular filter\n>   3. remove the txid from the regular filter(as with some extra\n> book-keeping\n>      the output script is enough)\n>   4. do away with the extended filter all together, as our original use\n> case\n>      for it has been nerfed as the filter size grew too large when doing\n>      recursive parsing. instead we watch for the outpoint being spent and\n>      extract the pre-image from it if it matches now\n>\n> The resulting changes should slash the size of the filters, yet still\n> ensure\n> that they're useful enough for our target use case.\n>\n> [1]:\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-May/016029.html\n>\n> -- Laolu\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180531/5499982f/attachment-0001.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2018-06-01T04:15:13",
                "message_text_only": "On Fri, Jun 1, 2018 at 2:52 AM, Olaoluwa Osuntokun via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> One notable thing that I left off is the proposed change to use the previous\n> output script rather than the outpoint. Modifying the filters in this\n> fashion would be a downgrade in the security model for light clients, as it\n\nOnly if you make a very strong assumption about the integrity of the\nnodes the client is talkign to. A typical network attacker (e.g.\nsomeone on your lan or wifi segmet, or someone who has compromised or\noperates an upstream router) can be all of your peers.\n\nThe original propsal for using these kinds of maps was that their\ndigests could eventually be commited and then checked against the\ncommitment, matching the same general security model used otherwise in\nSPV.\n\nUnfortunately, using the scripts instead of the outpoints takes us\nfurther away from a design that is optimized for committing (or, for\nthat matter, use purely locally by a wallet)..."
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-06-02T00:01:43",
                "message_text_only": "> A typical network attacker (e.g.  someone on your lan or wifi segmet, or\n> someone who has compromised or operates an upstream router) can be all of\n> your peers.\n\nThis is true, but it cannot make us accept any invalid filters unless the\nattacker is also creating invalid blocks w/ valid PoW.\n\n> The original propsal for using these kinds of maps was that their digests\n> could eventually be commited and then checked against the commitment,\n> matching the same general security model used otherwise in SPV.\n\nIndeed, but no such proposal for committing the filters has emerged yet.\nSlinging filters with new p2p messages requires much less coordination that\nadding a new committed structure to Bitcoin. One could imagine that if\nconsensus exists to add new committed structures, then there may also be\ninitiatives to start to commit sig-ops, block weight, utxo's etc. As a\nresult one could imagine a much longer deployment cycle compared to a pure\np2p roll out in the near term, and many applications are looking for a\nviable alternative to BIP 37.\n\n> Unfortunately, using the scripts instead of the outpoints takes us further\n> away from a design that is optimized for committing (or, for that matter,\n> use purely locally by a wallet)...\n\nI agree that using the prev input scripts would indeed be optimal from a\nsize perspective when the filters are to be committed. The current proposal\nmakes way for future filter types and it's likely the case that only the\nmost optimal filters should be committed (while other more niche filters\nperhaps, remain only on the p2p level).\n\n-- Laolu\n\n\nOn Thu, May 31, 2018 at 9:14 PM Gregory Maxwell <gmaxwell at gmail.com> wrote:\n\n> On Fri, Jun 1, 2018 at 2:52 AM, Olaoluwa Osuntokun via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > One notable thing that I left off is the proposed change to use the\n> previous\n> > output script rather than the outpoint. Modifying the filters in this\n> > fashion would be a downgrade in the security model for light clients, as\n> it\n>\n> Only if you make a very strong assumption about the integrity of the\n> nodes the client is talkign to. A typical network attacker (e.g.\n> someone on your lan or wifi segmet, or someone who has compromised or\n> operates an upstream router) can be all of your peers.\n>\n> The original propsal for using these kinds of maps was that their\n> digests could eventually be commited and then checked against the\n> commitment, matching the same general security model used otherwise in\n> SPV.\n>\n> Unfortunately, using the scripts instead of the outpoints takes us\n> further away from a design that is optimized for committing (or, for\n> that matter, use purely locally by a wallet)...\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180601/12faacf0/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2018-06-02T00:22:25",
                "message_text_only": "On Sat, Jun 2, 2018 at 12:01 AM, Olaoluwa Osuntokun <laolu32 at gmail.com> wrote:\n>> A typical network attacker (e.g.  someone on your lan or wifi segmet, or\n>> someone who has compromised or operates an upstream router) can be all of\n>> your peers.\n>\n> This is true, but it cannot make us accept any invalid filters unless the\n> attacker is also creating invalid blocks w/ valid PoW.\n\nI wish that were the true, but absent commitments that wouldn't be the\ncase unless you were always downloading all the blocks-- since you\nwouldn't have any sign that there was something wrong with the\nfilter-- and downloading all the blocks would moot using the filters\nin the first place. :)\n\nOr have I misunderstood you massively here?\n\nFor segwit originally I had proposed adding additional commitments\nthat would make it possible to efficiently prove invalidity of a\nblock; but that got stripped because many people were of the view that\nthe \"assume you have at least one honest peer who saw that block and\nrejected it to tell you that the block was invalid\" security\nassumption was of dubious value. Maybe it's more justifiable to make\nuse of a dubious assumption for a P2P feature than for a consensus\nfeature?  Perhaps,  I'd rather have both filter types from day one so\nthat things not implementing the comparison techniques don't get the\nefficiency loss or the extra work to change filter types for a\nconsensus one.\n\n[I think now that we're much closer to a design that would be worth\nmaking a consensus committed version of than we were a few months ago\nnow, since we are effectively already on a second generation of the\ndesign with the various improvements lately]"
            },
            {
                "author": "Jim Posen",
                "date": "2018-06-02T02:02:38",
                "message_text_only": "To address the at-least-one-honest peer security assumption for light\nclients, I think this is a rather good security model for light clients.\nFirst it significantly reduces the chances that an attacker can eclipse a\nclient just by chance, and clients can implement measures like ensuring\nconnectivity to peers from different subnets. But even if, as you suggest,\na network attacker controls the target's local network, peers still can\nhave good security guarantees by requiring authenticated connections to\nsemi-trusted peers. A client can select a set of N servers that it believes\nwill not collude to attack it, and only sync filters if connected to a\nthreshold of them. So even if the network is malicious, the attacker cannot\nforge the authenticated responses. The level of trust in these designated\nparties again is quite low because only one has to be honest. This would\nrequire something like BIP 150.\n\nEven if clients are uncomfortable with whitelisting required peers, it\ncould have a policy of requiring a certain number of connections to peers\nthat have honestly served it filters in the past. This is sort of like\ntrust-on-first-use. This type of scheme, however, would require nodes to\nadvertise a pubkey per address, which BIP 150/151 does not support at\npresent.\n\nAll in all, I think this is an acceptable security model for light clients.\nWithout the ability to verify filter validity, a client would have to stop\nsyncing altogether in the presence of just one malicious peer, which is\nunacceptable.\n\nThe other concern you raise, Greg, is using a filter for P2P communications\nthat we expect may be replaced in the future. You also raise the point that\nfull node wallets can use the smaller filters for rescans because the\nfilter validity is not in question. I'd perfectly fine with the idea of\ndefining two filter types in the BIP, one that is output script + outpoint\nand the other output script + prev script. But I imagine some people would\nobject to the idea of full nodes storing two different filters that overlap\nin contents. If we had to pick just one though, I'm strongly in support of\noutput script + outpoint so that BIP 157 can be deployed ASAP without a\nconsensus change. It's entirely possible we will learn even more about\noptimal filter design through deployment and adoption.\n\nOn Fri, Jun 1, 2018 at 5:22 PM Gregory Maxwell <greg at xiph.org> wrote:\n\n> On Sat, Jun 2, 2018 at 12:01 AM, Olaoluwa Osuntokun <laolu32 at gmail.com>\n> wrote:\n> >> A typical network attacker (e.g.  someone on your lan or wifi segmet, or\n> >> someone who has compromised or operates an upstream router) can be all\n> of\n> >> your peers.\n> >\n> > This is true, but it cannot make us accept any invalid filters unless the\n> > attacker is also creating invalid blocks w/ valid PoW.\n>\n> I wish that were the true, but absent commitments that wouldn't be the\n> case unless you were always downloading all the blocks-- since you\n> wouldn't have any sign that there was something wrong with the\n> filter-- and downloading all the blocks would moot using the filters\n> in the first place. :)\n>\n> Or have I misunderstood you massively here?\n>\n> For segwit originally I had proposed adding additional commitments\n> that would make it possible to efficiently prove invalidity of a\n> block; but that got stripped because many people were of the view that\n> the \"assume you have at least one honest peer who saw that block and\n> rejected it to tell you that the block was invalid\" security\n> assumption was of dubious value. Maybe it's more justifiable to make\n> use of a dubious assumption for a P2P feature than for a consensus\n> feature?  Perhaps,  I'd rather have both filter types from day one so\n> that things not implementing the comparison techniques don't get the\n> efficiency loss or the extra work to change filter types for a\n> consensus one.\n>\n> [I think now that we're much closer to a design that would be worth\n> making a consensus committed version of than we were a few months ago\n> now, since we are effectively already on a second generation of the\n> design with the various improvements lately]\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180601/b99c927d/attachment-0001.html>"
            },
            {
                "author": "David A. Harding",
                "date": "2018-06-02T12:41:57",
                "message_text_only": "On Fri, Jun 01, 2018 at 07:02:38PM -0700, Jim Posen via bitcoin-dev wrote:\n> Without the ability to verify filter validity, a client would have to stop\n> syncing altogether in the presence of just one malicious peer, which is\n> unacceptable.\n\nI'm confused about why this would be the case.  If Alice's node\ngenerates filters accurately and Mallory's node generates filters\ninaccurately, and they both send their filters to Bob, won't Bob be able\nto download any blocks either filter indicates are relevant to his\nwallet?\n\nIf Bob downloads a block that contains one of his transactions based on\nAlice's filter indicating a possible match at a time when Mallory's\nfilter said there was no match, then this false negative is perfect\nevidence of deceit on Mallory's part[1] and Bob can ban her.\n\nIf Bob downloads a block that doesn't contain any of his transactions\nbased on Mallory's filter indicating a match at a time when Alice's\nfilter said there was no match, then this false positive can be recorded\nand Bob can eventually ban Mallory should the false positive rate\nexceeds some threshold.\n\nUntil Mallory is eventually banned, it seems to me that the worst she\ncan do is waste Bob's bandwidth and that of any nodes serving him\naccurate information, such as Alice's filters and the blocks Bob\nis misled into downloading to check for matches.  The amount of\nattacker:defender asymetry in the bandwidth wasted increases if\nMallory's filters become less accurate, but this also increases her\nfalse positive rate and reduces the number of filters that need to be\nseen before Bob bans her, so it seems to me (possibly naively) that this\nis not a significant DoS vector.\n\n-Dave\n\n[1] Per BIP158 saying, \"a Golomb-coded set (GCS), which matches all\nitems in the set with probability 1\"\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180602/9556f60f/attachment.sig>"
            },
            {
                "author": "Tamas Blummer",
                "date": "2018-06-02T22:02:11",
                "message_text_only": "Without block commitment mobiles would have to use trusted filter provider or implement a complex data hungry algorithm and still remain as insecure as with BIP 37.\n\nYears of experience implementing wallets with BIP 37 taught us that an outpoint + output script filter is useful. Committing such a filter to the block can not be an error.\n\nWe could roll this out on P2P prior to a soft fork adding the commitment, but I would not expect its use to pick up before that.\nTherafter BIP 37 could be rightfully decommissioned, herby offering both security and privacy enhancement at modest data cost.\n\nTamas Blummer\n\n> On Jun 2, 2018, at 14:41, David A. Harding via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> On Fri, Jun 01, 2018 at 07:02:38PM -0700, Jim Posen via bitcoin-dev wrote:\n>> Without the ability to verify filter validity, a client would have to stop\n>> syncing altogether in the presence of just one malicious peer, which is\n>> unacceptable.\n> \n> I'm confused about why this would be the case.  If Alice's node\n> generates filters accurately and Mallory's node generates filters\n> inaccurately, and they both send their filters to Bob, won't Bob be able\n> to download any blocks either filter indicates are relevant to his\n> wallet?\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 529 bytes\nDesc: Message signed with OpenPGP\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180603/6623601b/attachment.sig>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2018-06-03T00:28:33",
                "message_text_only": "On Sat, Jun 2, 2018 at 10:02 PM, Tamas Blummer via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Years of experience implementing wallets with BIP 37\n\npretty much us that all these filter things are a total waste of time.\nBIP37 use is nearly dead on the network-- monitor your own nodes to\nsee the actual use of the filters: it's very low.  I see under average\nof 1 peer per day using it.\n\nMoreover the primary complaint from users about BIP37 vs the\nalternatives they're choosing over it (electrum and web wallets) is\nthat the sync time is too long-- something BIP158 doesn't improve.\n\nSo if we were going to go based on history we wouldn't bother with on\nP2P at all.   But I think the history's lesson here may mostly be an\naccident, and that the the non-use of BIP37 is  due more to the low\nquality and/or abandoned status of most BIP37 implementing software,\nrather than a fundamental lack of utility.   Though maybe we do find\nout that once someone bothers implementing a PIR based scanning\nmechanism (as electrum has talked about on and off for a while now)\nwe'll lose another advantage.\n\nBIP37 also got a number of things wrong-- what went into the filters\nwas a big element in that (causing massive pollution of matches due to\nuseless data), along with privacy etc.  This kind of approach will\nhave the best chances if it doesn't repeat the mistakes... but also\nit'll have the best chances if it has good security, and getting SPV-\nequivalent security will require committing the filters, but\ncommitting them is a big step because then the behaviour becomes\nconsensus normative-- it's worth spending a few months of extra\niteration getting the design as good as possible before doing that\n(which is what we've been seeing lately)."
            },
            {
                "author": "Tamas Blummer",
                "date": "2018-06-03T05:14:34",
                "message_text_only": "Lighter but SPV secure nodes (filter committed) would help the network (esp. Layer 2) to grow mesh like, but add more user that blindly follow POW.\n\nOn longer term most users' security will be determined by either trusted hubs or POW.\nI do not know which is worse, but we should at least offer the choice to the user, therefore commit filters.\n\nTamas Blummer\n\n> On Jun 3, 2018, at 02:28, Gregory Maxwell <greg at xiph.org> wrote:\n> \n> pretty much us that all these filter things are a total waste of time.\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 529 bytes\nDesc: Message signed with OpenPGP\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180603/0f5d908f/attachment.sig>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2018-06-03T06:11:56",
                "message_text_only": "On Sat, Jun 2, 2018, 22:56 Tamas Blummer via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Lighter but SPV secure nodes (filter committed) would help the network\n> (esp. Layer 2) to grow mesh like, but add more user that blindly follow POW.\n>\n> On longer term most users' security will be determined by either trusted\n> hubs or POW.\n> I do not know which is worse, but we should at least offer the choice to\n> the user, therefore commit filters.\n>\n\nI don't think that's the point of discussion here. Of course, in order to\nhave filters that verifiably don't lie by omission, the filters need to be\ncommitted to by blocks.\n\nThe question is what data that filter should contain.\n\nThere are two suggestions:\n(a) The scriptPubKeys of the block's outputs, and prevouts of the block's\ninputs.\n(b) The scriptPubKeys of the block's outputs, and scriptPubKeys of outputs\nbeing spent by the block's inputs.\n\nThe advantage of (a) is that it can be verified against a full block\nwithout access to the outputs being spent by it. This allows light clients\nto ban nodes that give them incorrect filters, but they do need to actually\nsee the blocks (partially defeating the purpose of having filters in the\nfirst place).\n\nThe advantage of (b) is that it is more compact (scriot reuse, and outputs\nspent within the same block as they are created). It also had the advantage\nof being more easily usable for scanning of a wallet's transactions. Using\n(a) for that in some cases may need to restart and refetch when an output\nis discovered, to go test for its spending (whose outpoint is not known\nahead of time). Especially when fetching multiple filters at a time this\nmay be an issue.\n\nI think both of these potentially good arguments. However, once a committed\nfilter exists, the advantage of (a) goes away completely - validation of\ncommitted filters is trivial and can be done without needing the full\nblocks in the first place.\n\nSo I think the question is do we aim for an uncommitted (a) first and a\ncommitted (b) later, or go for (b) immediately?\n\nCheers,\n\n-- \nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180602/ddc0b29e/attachment-0001.html>"
            },
            {
                "author": "Tamas Blummer",
                "date": "2018-06-03T16:44:04",
                "message_text_only": "I processed bitcoin history assuming filters using with P=19 M=784931.\n\nFindings:\n- Output script + spent script filters (Wuille\u2019s (b)) have sizes of ca. 0.2% of block size.\n- Output script + spent script filters (Wuille\u2019s (b)) are ca. 10% smaller than output script + spent outpoint filters (Wuille's (a)). Savings here however trend lower since years.\n\nGraphs attached.\n\nTamas Blummer\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180603/02d00008/attachment-0001.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: scriptfilter.png\nType: image/png\nSize: 55464 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180603/02d00008/attachment-0002.png>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: scriptssaving.png\nType: image/png\nSize: 59097 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180603/02d00008/attachment-0003.png>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 529 bytes\nDesc: Message signed with OpenPGP\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180603/02d00008/attachment-0001.sig>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-06-08T05:03:04",
                "message_text_only": "Hi sipa,\n\n> The advantage of (a) is that it can be verified against a full block\nwithout\n> access to the outputs being spent by it\n>\n> The advantage of (b) is that it is more compact (scriot reuse, and outputs\n> spent within the same block as they are created).\n\nThanks for this breakdown. I think you've accurately summarized the sole\nremaining discussing point in this thread.\n\nAs someone who's written and reviews code integrating the proposal all the\nway up the stack (from node to wallet, to application), IMO, there's no\nimmediate cost to deferring the inclusion/creation of a filter that includes\nprev scripts (b) instead of the outpoint as the \"regular\" filter does now.\nSwitching to prev script in the _short term_ would be costly for the set of\napplications already deployed (or deployed in a minimal or flag flip gated\nfashion) as the move from prev script to outpoint is a cascading one that\nimpacts wallet operation, rescans, HD seed imports, etc.\n\nMaintaining the outpoint also allows us to rely on a \"single honest peer\"\nsecurity model in the short term. In the long term the main barrier to\ncommitting the filters isn't choosing what to place in the filters (as once\nyou have the gcs code, adding/removing elements is a minor change), but the\nactual proposal to add new consensus enforced commitments to Bitcoin in the\nfirst place. Such a proposal would need to be generalized enough to allow\nseveral components to be committed, likely have versioning, and also provide\nthe necessary extensibility to allow additional items to be committed in the\nfuture. To my knowledge no such soft-fork has yet been proposed in a serious\nmanner, although we have years of brainstorming on the topic. The timeline\nof the drafting, design, review, and deployment of such a change would\nlikely be measures in years, compared to the immediate deployment of the\ncurrent p2p filter model proposed in the BIP.\n\nAs a result, I see no reason to delay the p2p filter deployment (with the\noutpoint) in the short term, as the long lead time a soft-fork to add\nextensible commitments to Bitcoin would give application+wallet authors\nample time to switch to the new model. Also there's no reason that full-node\nwallets which wish to primarily use the filters for rescan purposes can't\njust construct them locally for this particular use case independent of\nwhat's currently deployed on the p2p network.\n\nFinally, I've addressed the remaining comments on my PR modifying the BIP\nfrom my last message.\n\n-- Laolu\n\nOn Sat, Jun 2, 2018 at 11:12 PM Pieter Wuille via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Sat, Jun 2, 2018, 22:56 Tamas Blummer via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Lighter but SPV secure nodes (filter committed) would help the network\n>> (esp. Layer 2) to grow mesh like, but add more user that blindly follow POW.\n>>\n>> On longer term most users' security will be determined by either trusted\n>> hubs or POW.\n>> I do not know which is worse, but we should at least offer the choice to\n>> the user, therefore commit filters.\n>>\n>\n> I don't think that's the point of discussion here. Of course, in order to\n> have filters that verifiably don't lie by omission, the filters need to be\n> committed to by blocks.\n>\n> The question is what data that filter should contain.\n>\n> There are two suggestions:\n> (a) The scriptPubKeys of the block's outputs, and prevouts of the block's\n> inputs.\n> (b) The scriptPubKeys of the block's outputs, and scriptPubKeys of outputs\n> being spent by the block's inputs.\n>\n> The advantage of (a) is that it can be verified against a full block\n> without access to the outputs being spent by it. This allows light clients\n> to ban nodes that give them incorrect filters, but they do need to actually\n> see the blocks (partially defeating the purpose of having filters in the\n> first place).\n>\n> The advantage of (b) is that it is more compact (scriot reuse, and outputs\n> spent within the same block as they are created). It also had the advantage\n> of being more easily usable for scanning of a wallet's transactions. Using\n> (a) for that in some cases may need to restart and refetch when an output\n> is discovered, to go test for its spending (whose outpoint is not known\n> ahead of time). Especially when fetching multiple filters at a time this\n> may be an issue.\n>\n> I think both of these potentially good arguments. However, once a\n> committed filter exists, the advantage of (a) goes away completely -\n> validation of committed filters is trivial and can be done without needing\n> the full blocks in the first place.\n>\n> So I think the question is do we aim for an uncommitted (a) first and a\n> committed (b) later, or go for (b) immediately?\n>\n> Cheers,\n>\n> --\n> Pieter\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180607/835cc319/attachment-0001.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2018-06-08T16:14:41",
                "message_text_only": "On Fri, Jun 8, 2018 at 5:03 AM, Olaoluwa Osuntokun via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> As someone who's written and reviews code integrating the proposal all the\n> way up the stack (from node to wallet, to application), IMO, there's no\n> immediate cost to deferring the inclusion/creation of a filter that includes\n> prev scripts (b) instead of the outpoint as the \"regular\" filter does now.\n> Switching to prev script in the _short term_ would be costly for the set of\n> applications already deployed (or deployed in a minimal or flag flip gated\n> fashion) as the move from prev script to outpoint is a cascading one that\n> impacts wallet operation, rescans, HD seed imports, etc.\n\nIt seems to me that you're making the argument against your own case\nhere: I'm reading this as a \"it's hard to switch so it should be done\nthe inferior way\".  That in argument against adopting the inferior\nversion, as that will contribute more momentum to doing it in a way\nthat doesn't make sense long term.\n\n> Such a proposal would need to be generalized enough to allow several components to be committed,\n\nI don't agree at all, and I can't see why you say so.\n\n> likely have versioning,\n\nThis is inherent in how e.g. the segwit commitment is encoded, the\ninitial bytes are an identifying cookies. Different commitments would\nhave different cookies.\n\n> and also provide the necessary extensibility to allow additional items to be committed in the future\n\nWhat was previously proposed is that the commitment be required to be\nconsistent if present but not be required to be present.  This would\nallow changing whats used by simply abandoning the old one.  Sparsity\nin an optional commitment can be addressed when there is less than\n100% participation by having each block that includes a commitment\ncommit to the missing filters ones from their immediate ancestors.\n\nAdditional optionality can be provided by the other well known\nmechanisms,  e.g. have the soft fork expire at a block 5 years out\npast deployment, and continue to soft-fork it in for a longer term so\nlong as its in use (or eventually without expiration if its clear that\nit's not going away).\n\n> wallets which wish to primarily use the filters for rescan purposes can't\n> just construct them locally for this particular use case independent of\n> what's currently deployed on the p2p network.\n\nAbsolutely, but given the failure of BIP37 on the network-- and the\napparent strong preference of end users for alternatives that don't\nscan (e.g. electrum and web wallets)-- supporting making this\navailable via P2P was already only interesting to many as a nearly\nfree side effect of having filters for local scanning.  If it's a\ndifferent filter, it's no longer attractive.\n\nIt seems to me that some people have forgotten that this whole idea\nwas originally proposed to be a committed data-- but with an added\nadvantage of permitting expirementation ahead of the commitment.\n\n> Maintaining the outpoint also allows us to rely on a \"single honest peer\"security model in the short term.\n\nYou can still scan blocks directly when peers disagree on the filter\ncontent, regardless of how the filter is constructed-- yes, it uses\nmore bandwidth if you're attacked, but it makes the attack ineffective\nand using outpoints considerably increases bandwidth for everyone\nwithout an attack.  These ineffective (except for increasing\nbandwidth) attacks would have to be common to offset the savings. It\nseems to me this point is being overplayed, especially considering the\ncurrent state of non-existing validation in SPV software (if SPV\nsoftware doesn't validate anything else they could be validating, why\nwould they implement a considerable amount of logic for this?)."
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-06-08T23:35:29",
                "message_text_only": "> That in argument against adopting the inferior version, as that will\n> contribute more momentum to doing it in a way that doesn't make sense long\n> term.\n\nThat was moreso an attempt at a disclosure, rather than may argument. But\nalso as noted further up in the thread, both approaches have a trade off:\none is better for light clients in a p2p \"one honest peer mode\", while the\nother is more compact, but is less verifiable for the light clients. They're\n\"inferior\" in different ways.\n\nMy argument goes more like: moving to prev scripts means clients cannot\nverify in full unless a block message is added to include the prev outs.\nThis is a downgrade assuming a \"one honest peer\" model for the p2p\ninteractions. A commitment removes this drawback, but ofc requires a soft\nfork. Soft forks take a \"long\" time to deploy. So what's the cost in using\nthe current filter (as it lets the client verify the filter if they want to,\nor in an attempted \"bamboozlement\" scenario) in the short term (as we don't\nyet have a proposal for committing the filters) which would allow us to\nexperiment more with the technique on mainnet before making the step up to\ncommitting the filter. Also, depending on the way the commitment is done,\nthe filters themselves would need to be modified.\n\n> I don't agree at all, and I can't see why you say so.\n\nSure it doesn't _have_ to, but from my PoV as \"adding more commitments\" is\non the top of every developers wish list for additions to Bitcoin, it would\nmake sense to coordinate on an \"ultimate\" extensible commitment once, rather\nthan special case a bunch of distinct commitments. I can see arguments for\neither really.\n\n> This is inherent in how e.g. the segwit commitment is encoded, the initial\n> bytes are an identifying cookies. Different commitments would have\ndifferent\n> cookies.\n\nIndeed, if the filter were to be committed, using an output on the coinbase\nwould be a likely candidate. However, I see two issues with this:\n\n  1. The current filter format (even moving to prevouts) cannot be committed\n     in this fashion as it indexes each of the coinbase output scripts. This\n     creates a circular dependency: the commitment is modified by the\n     filter, which is modified by the commitment (the filter atm indexes the\n     commitment). So we'd need to add a special case to skip outputs with a\n     particular witness magic. However, we don't know what that witness\n     magic looks like (as there's no proposal). As a result, the type\n     filters that can be served over the p2p network may be distinct from\n     the type of filters that are to be committed, as the commitment may\n     have an impact on the filter itself.\n\n  2. Since the coinbase transaction is the first in a block, it has the\n     longest merkle proof path. As a result, it may be several hundred bytes\n     (and grows with future capacity increases) to present a proof to the\n     client. Depending on the composition of blocks, this may outweigh the\n     gains had from taking advantage of the additional compression the prev\n     outs allow.\n\nIn regards to the second item above, what do you think of the old Tier Nolan\nproposal [1] to create a \"constant\" sized proof for future commitments by\nconstraining the size of the block and placing the commitments within the\nlast few transactions in the block?\n\n> but with an added advantage of permitting expirementation ahead of the\n> commitment.\n\nIndeed! To my knowledge, lnd is the only software deployed that even has\ncode to experiment with the filtering proposal in general. Also, as I\npointed out above, we may require an additional modification in order to be\nable to commit the filter. The nature of that modification may depend on how\nthe filter is to be committed. As a result, why hinder experimentation today\n(since it might need to be changed anyway, and as you point out the filter\nbeing committed can even be swapped) by delaying until we know what the\ncommitment will look like?\n\n> You can still scan blocks directly when peers disagree on the filter\n> content, regardless of how the filter is constructed\n\nBut the difference is that one options lets you fully construct the filter\nfrom a block, while the other requires additional data.\n\n> but it makes the attack ineffective and using outpoints considerably\nincreases\n> bandwidth for everyone without an attack\n\nSo should we optimize for the ability to validate in a particular model\n(better\nsecurity), or lower bandwidth in this case? It may also be the case that the\noverhead of receiving proofs of the commitment outweigh the savings\ndepending\non block composition (ofc entire block that re-uses the same address is\nsuper\nsmall).\n\n> It seems to me this point is being overplayed, especially considering the\n> current state of non-existing validation in SPV software (if SPV software\n> doesn't validate anything else they could be validating, why would they\n> implement a considerable amount of logic for this?).\n\nI don't think its fair to compare those that wish to implement this proposal\n(and actually do the validation) to the legacy SPV software that to my\nknowledge is all but abandoned. The project I work on that seeks to deploy\nthis proposal (already has, but mainnet support is behind a flag as I\nanticipated further modifications) indeed has implemented the \"considerable\"\namount of logic to check for discrepancies and ban peers trying to bamboozle\nthe light clients. I'm confident that the other projects seeking to\nimplement\nthis (rust-bitcoin-spv, NBitcoin, bcoin, maybe missing a few too) won't\nfind it\ntoo difficult to implement \"full\" validation, as they're bitcoin developers\nwith quite a bit of experience.\n\nI think we've all learned from the past defects of past light clients, and\ndon't seek to repeat history by purposefully implementing as little\nvalidation\nas possible. With these new projects by new authors, I think we have an\nopprotunity to implement light clients \"correctly\" this time around.\n\n[1]:\nhttps://github.com/TierNolan/bips/blob/00a8d3e1ac066ce3728658c6c40240e1c2ab859e/bip-aux-header.mediawiki\n\n-- Laolu\n\n\nOn Fri, Jun 8, 2018 at 9:14 AM Gregory Maxwell <greg at xiph.org> wrote:\n\n> On Fri, Jun 8, 2018 at 5:03 AM, Olaoluwa Osuntokun via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > As someone who's written and reviews code integrating the proposal all\n> the\n> > way up the stack (from node to wallet, to application), IMO, there's no\n> > immediate cost to deferring the inclusion/creation of a filter that\n> includes\n> > prev scripts (b) instead of the outpoint as the \"regular\" filter does\n> now.\n> > Switching to prev script in the _short term_ would be costly for the set\n> of\n> > applications already deployed (or deployed in a minimal or flag flip\n> gated\n> > fashion) as the move from prev script to outpoint is a cascading one that\n> > impacts wallet operation, rescans, HD seed imports, etc.\n>\n> It seems to me that you're making the argument against your own case\n> here: I'm reading this as a \"it's hard to switch so it should be done\n> the inferior way\".  That in argument against adopting the inferior\n> version, as that will contribute more momentum to doing it in a way\n> that doesn't make sense long term.\n>\n> > Such a proposal would need to be generalized enough to allow several\n> components to be committed,\n>\n> I don't agree at all, and I can't see why you say so.\n>\n> > likely have versioning,\n>\n> This is inherent in how e.g. the segwit commitment is encoded, the\n> initial bytes are an identifying cookies. Different commitments would\n> have different cookies.\n>\n> > and also provide the necessary extensibility to allow additional items\n> to be committed in the future\n>\n> What was previously proposed is that the commitment be required to be\n> consistent if present but not be required to be present.  This would\n> allow changing whats used by simply abandoning the old one.  Sparsity\n> in an optional commitment can be addressed when there is less than\n> 100% participation by having each block that includes a commitment\n> commit to the missing filters ones from their immediate ancestors.\n>\n> Additional optionality can be provided by the other well known\n> mechanisms,  e.g. have the soft fork expire at a block 5 years out\n> past deployment, and continue to soft-fork it in for a longer term so\n> long as its in use (or eventually without expiration if its clear that\n> it's not going away).\n>\n> > wallets which wish to primarily use the filters for rescan purposes can't\n> > just construct them locally for this particular use case independent of\n> > what's currently deployed on the p2p network.\n>\n> Absolutely, but given the failure of BIP37 on the network-- and the\n> apparent strong preference of end users for alternatives that don't\n> scan (e.g. electrum and web wallets)-- supporting making this\n> available via P2P was already only interesting to many as a nearly\n> free side effect of having filters for local scanning.  If it's a\n> different filter, it's no longer attractive.\n>\n> It seems to me that some people have forgotten that this whole idea\n> was originally proposed to be a committed data-- but with an added\n> advantage of permitting expirementation ahead of the commitment.\n>\n> > Maintaining the outpoint also allows us to rely on a \"single honest\n> peer\"security model in the short term.\n>\n> You can still scan blocks directly when peers disagree on the filter\n> content, regardless of how the filter is constructed-- yes, it uses\n> more bandwidth if you're attacked, but it makes the attack ineffective\n> and using outpoints considerably increases bandwidth for everyone\n> without an attack.  These ineffective (except for increasing\n> bandwidth) attacks would have to be common to offset the savings. It\n> seems to me this point is being overplayed, especially considering the\n> current state of non-existing validation in SPV software (if SPV\n> software doesn't validate anything else they could be validating, why\n> would they implement a considerable amount of logic for this?).\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180608/347a6539/attachment-0001.html>"
            },
            {
                "author": "David A. Harding",
                "date": "2018-06-09T10:34:45",
                "message_text_only": "On Fri, Jun 08, 2018 at 04:35:29PM -0700, Olaoluwa Osuntokun via bitcoin-dev wrote:\n>   2. Since the coinbase transaction is the first in a block, it has the\n>      longest merkle proof path. As a result, it may be several hundred bytes\n>      (and grows with future capacity increases) to present a proof to the\n>      client.\n\nI'm not sure why commitment proof size is a significant issue.  Doesn't\nthe current BIP157 protocol have each filter commit to the filter for\nthe previous block?  If that's the case, shouldn't validating the\ncommitment at the tip of the chain (or buried back whatever number of\nblocks that the SPV client trusts) obliviate the need to validate the\ncommitments for any preceeding blocks in the SPV trust model?\n\n> Depending on the composition of blocks, this may outweigh the gains\n> had from taking advantage of the additional compression the prev outs\n> allow.\n\nI think those are unrelated points.  The gain from using a more\nefficient filter is saved bytes.  The gain from using block commitments\nis SPV-level security---that attacks have a definite cost in terms of\ngenerating proof of work instead of the variable cost of network\ncompromise (which is effectively free in many situations).\n\nComparing the extra bytes used by block commitments to the reduced bytes\nsaved by prevout+output filters is like comparing the extra bytes used\nto download all blocks for full validation to the reduced bytes saved by\nonly checking headers and merkle inclusion proofs in simplified\nvalidation.  Yes, one uses more bytes than the other, but they're\ncompletely different security models and so there's no normative way for\none to \"outweigh the gains\" from the other.\n\n> So should we optimize for the ability to validate in a particular\n> model (better security), or lower bandwidth in this case?\n\nIt seems like you're claiming better security here without providing any\nevidence for it.  The security model is \"at least one of my peers is\nhonest.\"  In the case of outpoint+output filters, when a client receives\nadvertisements for different filters from different peers, it:\n\n    1. Downloads the corresponding block\n    2. Locally generates the filter for that block\n    3. Kicks any peers that advertised a different filter than what it\n       generated locally\n\nThis ensures that as long as the client has at least one honest peer, it\nwill see every transaction affecting its wallet.  In the case of\nprevout+output filters, when a client receives advertisements for\ndifferent filters from different peers, it:\n\n    1. Downloads the corresponding block and checks it for wallet\n       transactions as if there had been a filter match\n\nThis also ensures that as long as the client has at least one honest\npeer, it will see every transaction affecting its wallet.  This is\nequivilant security.\n\nIn the second case, it's possible for the client to eventually\nprobabalistically determine which peer(s) are dishonest and kick them.\nThe most space efficient of these protocols may disclose some bits of\nevidence for what output scripts the client is looking for, but a\nslightly less space-efficient protocol simply uses randomly-selected\noutputs saved from previous blocks to make the probabalistic\ndetermination (rather than the client's own outputs) and so I think\nshould be quite private.  Neither protocol seems significantly more\ncomplicated than keeping an associative array recording the number of\nfalse positive matches for each peer's filters.\n\n-Dave\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180609/ff14b111/attachment.sig>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-06-12T23:51:29",
                "message_text_only": "> Doesn't the current BIP157 protocol have each filter commit to the filter\n> for the previous block?\n\nYep!\n\n> If that's the case, shouldn't validating the commitment at the tip of the\n> chain (or buried back whatever number of blocks that the SPV client\ntrusts)\n> obliviate the need to validate the commitments for any preceeding blocks\nin\n> the SPV trust model?\n\nYeah, just that there'll be a gap between the p2p version, and when it's\nultimately committed.\n\n> It seems like you're claiming better security here without providing any\n> evidence for it.\n\nWhat I mean is that one allows you to fully verify the filter, while the\nother allows you to only validate a portion of the filter and requires other\nadded heuristics.\n\n> In the case of prevout+output filters, when a client receives\nadvertisements\n> for different filters from different peers, it:\n\nAlternatively, they can decompress the filter and at least verify that\nproper _output scripts_ have been included. Maybe this is \"good enough\"\nuntil its committed. If a command is added to fetch all the prev outs along\nw/ a block (which would let you do another things like verify fees), then\nthey'd be able to fully validate the filter as well.\n\n-- Laolu\n\n\nOn Sat, Jun 9, 2018 at 3:35 AM David A. Harding <dave at dtrt.org> wrote:\n\n> On Fri, Jun 08, 2018 at 04:35:29PM -0700, Olaoluwa Osuntokun via\n> bitcoin-dev wrote:\n> >   2. Since the coinbase transaction is the first in a block, it has the\n> >      longest merkle proof path. As a result, it may be several hundred\n> bytes\n> >      (and grows with future capacity increases) to present a proof to the\n> >      client.\n>\n> I'm not sure why commitment proof size is a significant issue.  Doesn't\n> the current BIP157 protocol have each filter commit to the filter for\n> the previous block?  If that's the case, shouldn't validating the\n> commitment at the tip of the chain (or buried back whatever number of\n> blocks that the SPV client trusts) obliviate the need to validate the\n> commitments for any preceeding blocks in the SPV trust model?\n>\n> > Depending on the composition of blocks, this may outweigh the gains\n> > had from taking advantage of the additional compression the prev outs\n> > allow.\n>\n> I think those are unrelated points.  The gain from using a more\n> efficient filter is saved bytes.  The gain from using block commitments\n> is SPV-level security---that attacks have a definite cost in terms of\n> generating proof of work instead of the variable cost of network\n> compromise (which is effectively free in many situations).\n>\n> Comparing the extra bytes used by block commitments to the reduced bytes\n> saved by prevout+output filters is like comparing the extra bytes used\n> to download all blocks for full validation to the reduced bytes saved by\n> only checking headers and merkle inclusion proofs in simplified\n> validation.  Yes, one uses more bytes than the other, but they're\n> completely different security models and so there's no normative way for\n> one to \"outweigh the gains\" from the other.\n>\n> > So should we optimize for the ability to validate in a particular\n> > model (better security), or lower bandwidth in this case?\n>\n> It seems like you're claiming better security here without providing any\n> evidence for it.  The security model is \"at least one of my peers is\n> honest.\"  In the case of outpoint+output filters, when a client receives\n> advertisements for different filters from different peers, it:\n>\n>     1. Downloads the corresponding block\n>     2. Locally generates the filter for that block\n>     3. Kicks any peers that advertised a different filter than what it\n>        generated locally\n>\n> This ensures that as long as the client has at least one honest peer, it\n> will see every transaction affecting its wallet.  In the case of\n> prevout+output filters, when a client receives advertisements for\n> different filters from different peers, it:\n>\n>     1. Downloads the corresponding block and checks it for wallet\n>        transactions as if there had been a filter match\n>\n> This also ensures that as long as the client has at least one honest\n> peer, it will see every transaction affecting its wallet.  This is\n> equivilant security.\n>\n> In the second case, it's possible for the client to eventually\n> probabalistically determine which peer(s) are dishonest and kick them.\n> The most space efficient of these protocols may disclose some bits of\n> evidence for what output scripts the client is looking for, but a\n> slightly less space-efficient protocol simply uses randomly-selected\n> outputs saved from previous blocks to make the probabalistic\n> determination (rather than the client's own outputs) and so I think\n> should be quite private.  Neither protocol seems significantly more\n> complicated than keeping an associative array recording the number of\n> false positive matches for each peer's filters.\n>\n> -Dave\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180612/295d1a52/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2018-06-09T15:45:54",
                "message_text_only": "> So what's the cost in using\n> the current filter (as it lets the client verify the filter if they want to,\n\nAn example of that cost is you arguing against specifying and\nsupporting the design that is closer to one that would be softforked,\nwhich increases the time until we can make these filters secure\nbecause it slows convergence on the design of what would get\ncommitted.\n\n>> I don't agree at all, and I can't see why you say so.\n>\n> Sure it doesn't _have_ to, but from my PoV as \"adding more commitments\" is\n> on the top of every developers wish list for additions to Bitcoin, it would\n> make sense to coordinate on an \"ultimate\" extensible commitment once, rather\n> than special case a bunch of distinct commitments. I can see arguments for\n> either really.\n\nWe have an extensible commitment style via BIP141 already. I don't see\nwhy this in particular demands a new one.\n\n>   1. The current filter format (even moving to prevouts) cannot be committed\n>      in this fashion as it indexes each of the coinbase output scripts. This\n>      creates a circular dependency: the commitment is modified by the\n>      filter,\n\nGreat point, but it should probably exclude coinbase OP_RETURN output.\nThis would exclude the current BIP141 style commitment and likely any\nother.\n\nShould I start a new thread on excluding all OP_RETURN outputs from\nBIP-158 filters for all transactions? -- they can't be spent, so\nincluding them just pollutes the filters.\n\n>   2. Since the coinbase transaction is the first in a block, it has the\n>      longest merkle proof path. As a result, it may be several hundred bytes\n>      (and grows with future capacity increases) to present a proof to the\n\nIf 384 bytes is a concern, isn't 3840 bytes (the filter size\ndifference is in this ballpark) _much_ more of a concern?  Path to the\ncoinbase transaction increases only logarithmically so further\ncapacity increases are unlikely to matter much, but the filter size\nincreases linearly and so it should be much more of a concern.\n\n> In regards to the second item above, what do you think of the old Tier Nolan\n> proposal [1] to create a \"constant\" sized proof for future commitments by\n> constraining the size of the block and placing the commitments within the\n> last few transactions in the block?\n\nI think it's a fairly ugly hack. esp since it requires that mining\ntemplate code be able to stuff the block if they just don't know\nenough actual transactions-- which means having a pool of spendable\noutputs in order to mine, managing private keys, etc... it also\nrequires downstream software not tinker with the transaction count\n(which I wish it didn't but as of today it does). A factor of two\ndifference in capacity-- if you constrain to get the smallest possible\nproof-- is pretty stark, optimal txn selection with this cardinality\nconstraint would be pretty weird. etc.\n\nIf the community considers tree depth for proofs like that to be such\na concern to take on technical debt for that structure, we should\nprobably be thinking about more drastic (incompatible) changes... but\nI don't think it's actually that interesting.\n\n> I don't think its fair to compare those that wish to implement this proposal\n> (and actually do the validation) to the legacy SPV software that to my\n> knowledge is all but abandoned. The project I work on that seeks to deploy\n\nYes, maybe it isn't.  But then that just means we don't have good information.\n\nWhen a lot of people were choosing electrum over SPV wallets when\nthose SPV wallets weren't abandoned, sync time was frequently cited as\nan actual reason. BIP158 makes that worse, not better.   So while I'm\nhopeful, I'm also somewhat sceptical.  Certainly things that reduce\nthe size of the 158 filters make them seem more likely to be a success\nto me.\n\n> too difficult to implement \"full\" validation, as they're bitcoin developers\n> with quite a bit of experience.\n\n::shrugs:: Above you're also arguing against fetching down to the\ncoinbase transaction to save a couple hundred bytes a block, which\nmakes it impossible to validate a half dozen other things (including\nas mentioned in the other threads depth fidelity of returned proofs).\nThere are a lot of reasons why things don't get implemented other than\nexperience! :)"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-06-12T23:58:50",
                "message_text_only": "> An example of that cost is you arguing against specifying and supporting\nthe\n> design that is closer to one that would be softforked, which increases the\n> time until we can make these filters secure because it\n> slows convergence on the design of what would get committed\n\nAgreed, since the commitment is just flat out better, and also also less\ncode to validate compared to the cross p2p validation, the filter should be\nas close to the committed version. This way, wallet and other apps don't\nneed to modify their logic in X months when the commitment is rolled out.\n\n> Great point, but it should probably exclude coinbase OP_RETURN output.\n> This would exclude the current BIP141 style commitment and likely any\n> other.\n\nDefinitely. I chatted offline with sipa recently, and he suggested this as\nwell. Upside is that the filters will get even smaller, and also the first\nfilter type becomes even more of a \"barebones\" wallet filter. If folks\nreaally want to also search OP_RETURN in the filter (as no widely deployed\napplications I know of really use it), then an additional filter type can be\nadded in the future. It would need to be special cased to filter out the\ncommitment itself.\n\nAlright, color me convinced! I'll further edit my open BIP 158 PR to:\n\n  * exclude all OP_RETURN\n  * switch to prev scripts instead of outpoints\n  * update the test vectors to include the prev scripts from blocks in\n    addition to the block itself\n\n-- Laolu\n\n\nOn Sat, Jun 9, 2018 at 8:45 AM Gregory Maxwell <greg at xiph.org> wrote:\n\n> > So what's the cost in using\n> > the current filter (as it lets the client verify the filter if they want\n> to,\n>\n> An example of that cost is you arguing against specifying and\n> supporting the design that is closer to one that would be softforked,\n> which increases the time until we can make these filters secure\n> because it slows convergence on the design of what would get\n> committed.\n>\n> >> I don't agree at all, and I can't see why you say so.\n> >\n> > Sure it doesn't _have_ to, but from my PoV as \"adding more commitments\"\n> is\n> > on the top of every developers wish list for additions to Bitcoin, it\n> would\n> > make sense to coordinate on an \"ultimate\" extensible commitment once,\n> rather\n> > than special case a bunch of distinct commitments. I can see arguments\n> for\n> > either really.\n>\n> We have an extensible commitment style via BIP141 already. I don't see\n> why this in particular demands a new one.\n>\n> >   1. The current filter format (even moving to prevouts) cannot be\n> committed\n> >      in this fashion as it indexes each of the coinbase output scripts.\n> This\n> >      creates a circular dependency: the commitment is modified by the\n> >      filter,\n>\n> Great point, but it should probably exclude coinbase OP_RETURN output.\n> This would exclude the current BIP141 style commitment and likely any\n> other.\n>\n> Should I start a new thread on excluding all OP_RETURN outputs from\n> BIP-158 filters for all transactions? -- they can't be spent, so\n> including them just pollutes the filters.\n>\n> >   2. Since the coinbase transaction is the first in a block, it has the\n> >      longest merkle proof path. As a result, it may be several hundred\n> bytes\n> >      (and grows with future capacity increases) to present a proof to the\n>\n> If 384 bytes is a concern, isn't 3840 bytes (the filter size\n> difference is in this ballpark) _much_ more of a concern?  Path to the\n> coinbase transaction increases only logarithmically so further\n> capacity increases are unlikely to matter much, but the filter size\n> increases linearly and so it should be much more of a concern.\n>\n> > In regards to the second item above, what do you think of the old Tier\n> Nolan\n> > proposal [1] to create a \"constant\" sized proof for future commitments by\n> > constraining the size of the block and placing the commitments within the\n> > last few transactions in the block?\n>\n> I think it's a fairly ugly hack. esp since it requires that mining\n> template code be able to stuff the block if they just don't know\n> enough actual transactions-- which means having a pool of spendable\n> outputs in order to mine, managing private keys, etc... it also\n> requires downstream software not tinker with the transaction count\n> (which I wish it didn't but as of today it does). A factor of two\n> difference in capacity-- if you constrain to get the smallest possible\n> proof-- is pretty stark, optimal txn selection with this cardinality\n> constraint would be pretty weird. etc.\n>\n> If the community considers tree depth for proofs like that to be such\n> a concern to take on technical debt for that structure, we should\n> probably be thinking about more drastic (incompatible) changes... but\n> I don't think it's actually that interesting.\n>\n> > I don't think its fair to compare those that wish to implement this\n> proposal\n> > (and actually do the validation) to the legacy SPV software that to my\n> > knowledge is all but abandoned. The project I work on that seeks to\n> deploy\n>\n> Yes, maybe it isn't.  But then that just means we don't have good\n> information.\n>\n> When a lot of people were choosing electrum over SPV wallets when\n> those SPV wallets weren't abandoned, sync time was frequently cited as\n> an actual reason. BIP158 makes that worse, not better.   So while I'm\n> hopeful, I'm also somewhat sceptical.  Certainly things that reduce\n> the size of the 158 filters make them seem more likely to be a success\n> to me.\n>\n> > too difficult to implement \"full\" validation, as they're bitcoin\n> developers\n> > with quite a bit of experience.\n>\n> ::shrugs:: Above you're also arguing against fetching down to the\n> coinbase transaction to save a couple hundred bytes a block, which\n> makes it impossible to validate a half dozen other things (including\n> as mentioned in the other threads depth fidelity of returned proofs).\n> There are a lot of reasons why things don't get implemented other than\n> experience! :)\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180612/d1dbb394/attachment-0001.html>"
            },
            {
                "author": "Tamas Blummer",
                "date": "2018-06-03T16:50:17",
                "message_text_only": "Correction:\n- Output script + spent script filters (Wuille\u2019s (b)) have sizes of ca. 2% of block size.\n\nTamas Blummer\n\n> On Jun 3, 2018, at 18:44, Tamas Blummer <tamas.blummer at gmail.com> wrote:\n> \n> I processed bitcoin history assuming filters using with P=19 M=784931.\n> \n> Findings:\n> - Output script + spent script filters (Wuille\u2019s (b)) have sizes of ca. 0.2% of block size.\n> - Output script + spent script filters (Wuille\u2019s (b)) are ca. 10% smaller than output script + spent outpoint filters (Wuille's (a)). Savings here however trend lower since years.\n> \n> Graphs attached.\n> \n> Tamas Blummer\n> \n> <scriptfilter.png><scriptssaving.png>\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180603/99540bb3/attachment-0001.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 529 bytes\nDesc: Message signed with OpenPGP\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180603/99540bb3/attachment-0001.sig>"
            },
            {
                "author": "Riccardo Casatta",
                "date": "2018-06-04T08:42:10",
                "message_text_only": "I was wondering why this multi-layer multi-block filter proposal isn't\ngetting any comment,\nis it because not asking all filters is leaking information?\n\nThanks\n\nIl giorno ven 18 mag 2018 alle ore 08:29 Karl-Johan Alm via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> ha scritto:\n\n> On Fri, May 18, 2018 at 12:25 AM, Matt Corallo via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > In general, I'm concerned about the size of the filters making existing\n> > SPV clients less willing to adopt BIP 158 instead of the existing bloom\n> > filter garbage and would like to see a further exploration of ways to\n> > split out filters to make them less bandwidth intensive. Some further\n> > ideas we should probably play with before finalizing moving forward is\n> > providing filters for certain script templates, eg being able to only\n> > get outputs that are segwit version X or other similar ideas.\n>\n> There is also the idea of multi-block filters. The idea is that light\n> clients would download a pair of filters for blocks X..X+255 and\n> X+256..X+511, check if they have any matches and then grab pairs for\n> any that matched, e.g. X..X+127 & X+128..X+255 if left matched, and\n> iterate down until it ran out of hits-in-a-row or it got down to\n> single-block level.\n>\n> This has an added benefit where you can accept a slightly higher false\n> positive rate for bigger ranges, because the probability of a specific\n> entry having a false positive in each filter is (empirically speaking)\n> independent. I.e. with a FP probability of 1% in the 256 range block\n> and a FP probability of 0.1% in the 128 range block would mean the\n> probability is actually 0.001%.\n>\n> Wrote about this here: https://bc-2.jp/bfd-profile.pdf (but the filter\n> type is different in my experiments)\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n\n\n-- \nRiccardo Casatta - @RCasatta <https://twitter.com/RCasatta>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180604/8e42f18b/attachment.html>"
            },
            {
                "author": "Jim Posen",
                "date": "2018-06-05T01:08:01",
                "message_text_only": ">\n> I was wondering why this multi-layer multi-block filter proposal isn't\n> getting any comment,\n> is it because not asking all filters is leaking information?\n>\n\nIt's an interesting idea, but it adds more complexity to the client and\ncould be added later on if clients adopt BIP 157 and complain about\nbandwidth. It also derives all bandwidth gains from address reuse. So I'm\nhesitant to make the complexity tradeoff for bandwidth savings due to a\nbehavior that is actively discouraged.\n\nOn another note, I've been thinking that block TXO commitments could\nresolve the issue we are facing now with deciding between the prev script\napproach and outpoint. The whole argument for outpoints is that there are\ncompact-ish (<1 MiB) proofs of filter validity, which is not currently\npossible if the filters included prev output data. Such proofs would be\nfeasible if blocks headers (well, actually coinbase txs) had a commitment\nto the Merkle root of all newly created outputs in the block.\n\nThis idea has been tossed around before in the context of fraud proofs and\nTXO bitfields, and seems to unlock a whole bunch of other P2P commitments.\nFor example, if we wanted to do P2P commitments (BIP 157-style) to the\ndistribution of tx fees in a block, one could use block TXO commitments to\nprove correctness of fees for non-segwit txs. It also enables block\nvalidity proofs (assuming parent blocks are valid), which are not as\npowerful as invalidity/fraud proofs, but interesting nonetheless.\n\nThis would require a new getdata type BLOCK_WITH_PREVOUTS or something. I\nassume for most coinbase-tx-committed proposals, we'll also need a new\ngetcoinbases/coinbases that requests the coinbase tx and Merkle branch for\na range of headers as well. But with these additions, we could start\nserving more block-derived data to light clients under the BIP 157\nat-least-one-honest-peer assumption.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180604/ec43b862/attachment-0001.html>"
            },
            {
                "author": "Karl-Johan Alm",
                "date": "2018-06-05T04:33:06",
                "message_text_only": "On Tue, Jun 5, 2018 at 10:08 AM, Jim Posen <jim.posen at gmail.com> wrote:\n> It also derives all bandwidth gains from address reuse. So I'm\n> hesitant to make the complexity tradeoff for bandwidth savings due to a\n> behavior that is actively discouraged.\n\nI don't understand this comment. The bandwidth gains are not from\naddress reuse, they are from the observed property that false\npositives are independent between two filters. I.e. clients that\nconnect once a day will probably download 2-3 filters at most, if they\nhad nothing relevant in the last ~144 blocks.\n\n-Kalle."
            },
            {
                "author": "Jim Posen",
                "date": "2018-06-05T17:22:04",
                "message_text_only": ">\n> I don't understand this comment. The bandwidth gains are not from\n> address reuse, they are from the observed property that false\n> positives are independent between two filters. I.e. clients that\n> connect once a day will probably download 2-3 filters at most, if they\n> had nothing relevant in the last ~144 blocks.\n>\n\nYour multi-layer digest proposal (https://bc-2.jp/bfd-profile.pdf) uses a\ndifferent type of filter which seems more like a compressed Bloom filter if\nI understand it correctly. Appendix A shows how the FP rate increases with\nthe number of elements.\n\nWith the Golomb-Coded Sets, the filter size increases linearly in the\nnumber of elements for a fixed FP rate. So currently we are targeting an\n~1/2^20 rate (actually 1/784931 now), and filter sizes are ~20 bits * N for\nN elements. With a 1-layer digest covering let's say 16 blocks, you could\ndrop the FP rate on the digest filters and the block filters each to ~10\nbits per element, I think, to get the same FP rate for a given block by\nyour argument of independence. But the digest is only half the size of the\n16 combined filters and there's a high probability of downloading the other\nhalf anyway. So unless there is greater duplication of elements in the\ndigest filters, it's not clear to me that there are great bandwidth\nsavings. But maybe there are. Even so, I think we should just ship the\nblock filters and consider multi-layer digests later.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180605/1d6de3bd/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2018-06-05T17:52:29",
                "message_text_only": "On Tue, Jun 5, 2018 at 1:08 AM, Jim Posen via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> hesitant to make the complexity tradeoff for bandwidth savings due to a\n> behavior that is actively discouraged.\n\nAs an important point of clarification here. If scripts are used to\nidentify inputs and outputs, then no use is required for that savings.\nEach coin spent was created once, so in an absurd hypothetical you can\nget a 2:1 change in bits set without any reuse at all.   I don't know\nwhat portion of coins created are spent in the same 144 block\nwindow..."
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-06-06T01:12:55",
                "message_text_only": "It isn't being discussed atm (but was discussed 1 year ago when the BIP\ndraft was originally published), as we're in the process of removing items\nor filters that aren't absolutely necessary. We're now at the point where\nthere're no longer any items we can remove w/o making the filters less\ngenerally useful which signals a stopping point so we can begin widespread\ndeployment.\n\nIn terms of a future extension, BIP 158 already defines custom filter types,\nand BIP 157 allows filters to be fetched in batch based on the block height\nand numerical range. The latter feature can later be modified to return a\nsingle composite filter rather than several individual filters.\n\n-- Laolu\n\n\nOn Mon, Jun 4, 2018 at 7:28 AM Riccardo Casatta via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> I was wondering why this multi-layer multi-block filter proposal isn't\n> getting any comment,\n> is it because not asking all filters is leaking information?\n>\n> Thanks\n>\n> Il giorno ven 18 mag 2018 alle ore 08:29 Karl-Johan Alm via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> ha scritto:\n>\n>> On Fri, May 18, 2018 at 12:25 AM, Matt Corallo via bitcoin-dev\n>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> > In general, I'm concerned about the size of the filters making existing\n>> > SPV clients less willing to adopt BIP 158 instead of the existing bloom\n>> > filter garbage and would like to see a further exploration of ways to\n>> > split out filters to make them less bandwidth intensive. Some further\n>> > ideas we should probably play with before finalizing moving forward is\n>> > providing filters for certain script templates, eg being able to only\n>> > get outputs that are segwit version X or other similar ideas.\n>>\n>> There is also the idea of multi-block filters. The idea is that light\n>> clients would download a pair of filters for blocks X..X+255 and\n>> X+256..X+511, check if they have any matches and then grab pairs for\n>> any that matched, e.g. X..X+127 & X+128..X+255 if left matched, and\n>> iterate down until it ran out of hits-in-a-row or it got down to\n>> single-block level.\n>>\n>> This has an added benefit where you can accept a slightly higher false\n>> positive rate for bigger ranges, because the probability of a specific\n>> entry having a false positive in each filter is (empirically speaking)\n>> independent. I.e. with a FP probability of 1% in the 256 range block\n>> and a FP probability of 0.1% in the 128 range block would mean the\n>> probability is actually 0.001%.\n>>\n>> Wrote about this here: https://bc-2.jp/bfd-profile.pdf (but the filter\n>> type is different in my experiments)\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n>\n> --\n> Riccardo Casatta - @RCasatta <https://twitter.com/RCasatta>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180605/7afa073e/attachment.html>"
            },
            {
                "author": "Riccardo Casatta",
                "date": "2018-06-06T15:14:17",
                "message_text_only": "Sorry if I continue on the subject even if\n\u200bcustom filter types are considered in BIP 157/158\n.\nI am doing it\n because\n\u200b:\n1)\u200b\nwith a fixed target FP=2^-20  (or 1/784931)\n\u200b and the multi layer filtering maybe it's reasonable to consider less than\n~20 bits for the golomb encoding of the per-block filter (one day committed\nin the blockchain)\n2) based on the answer received, privacy leak if downloading a subset of\nfilters doesn't look a concern\n3)\nAs far as I know, anyone is considering to use a map instead of a filter\nfor the upper layers of the filter\u200b.\n\nSimplistic example:\nSuppose to have a 2 blocks blockchain, every block contains N items for the\nfilter:\n1) In the current discussed filter we have 2 filters of 20N bits\n2) In a two layer solution, we have 1 map of (10+1)2N bits and 2 filters of\n10N bits\nThe additional bit in the map discriminate if the match is in the first or\nin the second block.\nSupposing to have 1 match in the two blocks, the filter size downloaded in\nthe first case is always 40N bits, while the expected downloaded size in\nthe second case is 22N+2^-10*10N+10N ~= 32N with the same FP because\nindependence.\nThis obviously isn't a full analysis of the methodology, the expected\ndownloaded size in the second case could go from the best case 22N bits to\nthe worst case of 42N bits...\n\n@Gregory\n> I don't know what portion of coins created are spent in the same 144 block\nwindow...\n\nAbout 50%\nsource code <https://github.com/RCasatta/coincount>\n\n>From block 393216 to 458752  (still waiting for results on all the\nblockchain)\nTotal outputs 264185587\nsize: 2 spent: 11791058 ratio:0.04463172322871649\nsize: 4 spent: 29846090 ratio:0.11297395266305728\nsize: 16 spent: 72543182 ratio:0.2745917475051355\nsize: 64 spent: 113168726 ratio:0.4283682818775424\nsize: 144 spent: 134294070 ratio:0.508332311103709\nsize: 256 spent: 148824781 ratio:0.5633342177747191\nsize: 1024 spent: 179345566 ratio:0.6788620379960395\nsize: 4096 spent: 205755628 ratio:0.7788298761355213\nsize: 16384 spent: 224448158 ratio:0.849585174379706\n\nAnother point to consider is that if we don't want the full transaction\nhistory of our wallet but only the UTXO, the upper layer map could contain\nonly the item which are not already spent in the considered window. As we\ncan see from the previous result if the window is 16384 ~85% of the\nelements are already spent suggesting a very high time locality. (apart\n144, I choose power of 2 windows so there are an integer number of bits in\nthe map)\n\nIt's possible we need ~20 bits anyway for the per-block filters because\nthere are always connected wallets which one synced, always download the\nlast filter, anyway the upper layer map looks very promising for longer\nsync.\n\nIl giorno mer 6 giu 2018 alle ore 03:13 Olaoluwa Osuntokun <\nlaolu32 at gmail.com> ha scritto:\n\n> It isn't being discussed atm (but was discussed 1 year ago when the BIP\n> draft was originally published), as we're in the process of removing items\n> or filters that aren't absolutely necessary. We're now at the point where\n> there're no longer any items we can remove w/o making the filters less\n> generally useful which signals a stopping point so we can begin widespread\n> deployment.\n>\n> In terms of a future extension, BIP 158 already defines custom filter\n> types,\n> and BIP 157 allows filters to be fetched in batch based on the block height\n> and numerical range. The latter feature can later be modified to return a\n> single composite filter rather than several individual filters.\n>\n> -- Laolu\n>\n>\n> On Mon, Jun 4, 2018 at 7:28 AM Riccardo Casatta via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> I was wondering why this multi-layer multi-block filter proposal isn't\n>> getting any comment,\n>> is it because not asking all filters is leaking information?\n>>\n>> Thanks\n>>\n>> Il giorno ven 18 mag 2018 alle ore 08:29 Karl-Johan Alm via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> ha scritto:\n>>\n>>> On Fri, May 18, 2018 at 12:25 AM, Matt Corallo via bitcoin-dev\n>>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>> > In general, I'm concerned about the size of the filters making existing\n>>> > SPV clients less willing to adopt BIP 158 instead of the existing bloom\n>>> > filter garbage and would like to see a further exploration of ways to\n>>> > split out filters to make them less bandwidth intensive. Some further\n>>> > ideas we should probably play with before finalizing moving forward is\n>>> > providing filters for certain script templates, eg being able to only\n>>> > get outputs that are segwit version X or other similar ideas.\n>>>\n>>> There is also the idea of multi-block filters. The idea is that light\n>>> clients would download a pair of filters for blocks X..X+255 and\n>>> X+256..X+511, check if they have any matches and then grab pairs for\n>>> any that matched, e.g. X..X+127 & X+128..X+255 if left matched, and\n>>> iterate down until it ran out of hits-in-a-row or it got down to\n>>> single-block level.\n>>>\n>>> This has an added benefit where you can accept a slightly higher false\n>>> positive rate for bigger ranges, because the probability of a specific\n>>> entry having a false positive in each filter is (empirically speaking)\n>>> independent. I.e. with a FP probability of 1% in the 256 range block\n>>> and a FP probability of 0.1% in the 128 range block would mean the\n>>> probability is actually 0.001%.\n>>>\n>>> Wrote about this here: https://bc-2.jp/bfd-profile.pdf (but the filter\n>>> type is different in my experiments)\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>\n>>\n>> --\n>> Riccardo Casatta - @RCasatta <https://twitter.com/RCasatta>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n\n-- \nRiccardo Casatta - @RCasatta <https://twitter.com/RCasatta>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180606/a82a07c6/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "BIP 158 Flexibility and Filter Size",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Tamas Blummer",
                "David A. Harding",
                "Riccardo Casatta",
                "Olaoluwa Osuntokun",
                "Gregory Maxwell",
                "Pieter Wuille",
                "Jim Posen",
                "Karl-Johan Alm"
            ],
            "messages_count": 26,
            "total_messages_chars_count": 80199
        }
    },
    {
        "title": "[bitcoin-dev] SIGHASH2 for version 1 witness programme",
        "thread_messages": [
            {
                "author": "Russell O'Connor",
                "date": "2018-06-01T15:03:46",
                "message_text_only": "On Thu, May 31, 2018 at 2:35 PM, Johnson Lau via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n>   Double SHA256 of the serialization of:\n>\n\nShould we replace the Double SHA256 with a Single SHA256?  There is no\npossible length extension attack here.  Or are we speculating that there is\na robustness of Double SHA256 in the presence of SHA256 breaking?\n\nI suggest putting `sigversion` at the beginning instead of the end of the\nformat.  Because its value is constant, the beginning of the SHA-256\ncomputation could be pre-computed in advance.  Furthermore, if we make the\n`sigversion` exactly 64-bytes long then the entire first block of the\nSHA-256 compression function could be pre-computed.\n\nCan we add CHECKSIGFROMSTACK or do you think that would go into a separate\nBIP?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180601/6880054c/attachment.html>"
            },
            {
                "author": "Johnson Lau",
                "date": "2018-06-01T17:03:05",
                "message_text_only": "> On 1 Jun 2018, at 11:03 PM, Russell O'Connor <roconnor at blockstream.io> wrote:\n> \n> \n> \n> On Thu, May 31, 2018 at 2:35 PM, Johnson Lau via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n> \n>   Double SHA256 of the serialization of:\n> \n> Should we replace the Double SHA256 with a Single SHA256?  There is no possible length extension attack here.  Or are we speculating that there is a robustness of Double SHA256 in the presence of SHA256 breaking?\n> \n> I suggest putting `sigversion` at the beginning instead of the end of the format.  Because its value is constant, the beginning of the SHA-256 computation could be pre-computed in advance.  Furthermore, if we make the `sigversion` exactly 64-bytes long then the entire first block of the SHA-256 compression function could be pre-computed.\n> \n> Can we add CHECKSIGFROMSTACK or do you think that would go into a separate BIP?\n\nI think it\u2019s just a tradition to use double SHA256. One reason we might want to keep dSHA256 is a blind signature might be done by giving only the single SHA256 hash to the signer. At the same time, a non-Bitcoin signature scheme might use SHA512-SHA256. So a blind signer could distinguish the message type without learning the message.\n\nsigversion is a response to Peter Todd\u2019s comments on BIP143: https://petertodd.org/2016/segwit-consensus-critical-code-review#bip143-transaction-signature-verification <https://petertodd.org/2016/segwit-consensus-critical-code-review#bip143-transaction-signature-verification>\n\nI make it a 0x01000000 at the end of the message because the last 4 bytes has been the nHashType in the legacy/BIP143 protocol. Since the maximum legacy nHashType is 0xff, no collision could ever occur.\n\nPutting a 64-byte constant at the beginning should also work, since a collision means SHA256 is no longer preimage resistance. I don\u2019t know much about SHA256 optimisation. How good it is as we put a 64-byte constant at the beginning, while we also make the message 64-byte longer?\n\nFor CHECKSIGFROMSTACK (CSFS), I think the question is whether we want to make it as a separate opcode, or combine that with CHECKSIG. If it is a separate opcode, I think it should be a separate BIP. If it is combined with CHECKSIG, we could do something like this: If the bit 10 of SIGHASH2 is set, CHECKSIG will pop one more item from stack, and serialize its content with the transaction digest. Any thought?\n\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180602/72a9454f/attachment.html>"
            },
            {
                "author": "Russell O'Connor",
                "date": "2018-06-01T18:15:32",
                "message_text_only": "On Fri, Jun 1, 2018 at 1:03 PM, Johnson Lau <jl2012 at xbt.hk> wrote:\n\n> On 1 Jun 2018, at 11:03 PM, Russell O'Connor <roconnor at blockstream.io>\n> wrote:\n> On Thu, May 31, 2018 at 2:35 PM, Johnson Lau via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>>\n>>   Double SHA256 of the serialization of:\n>>\n>\n> Should we replace the Double SHA256 with a Single SHA256?  There is no\n> possible length extension attack here.  Or are we speculating that there is\n> a robustness of Double SHA256 in the presence of SHA256 breaking?\n>\n> I suggest putting `sigversion` at the beginning instead of the end of the\n> format.  Because its value is constant, the beginning of the SHA-256\n> computation could be pre-computed in advance.  Furthermore, if we make the\n> `sigversion` exactly 64-bytes long then the entire first block of the\n> SHA-256 compression function could be pre-computed.\n>\n> Can we add CHECKSIGFROMSTACK or do you think that would go into a separate\n> BIP?\n>\n>\n> I think it\u2019s just a tradition to use double SHA256. One reason we might\n> want to keep dSHA256 is a blind signature might be done by giving only the\n> single SHA256 hash to the signer. At the same time, a non-Bitcoin signature\n> scheme might use SHA512-SHA256. So a blind signer could distinguish the\n> message type without learning the message.\n>\n> sigversion is a response to Peter Todd\u2019s comments on BIP143:\n> https://petertodd.org/2016/segwit-consensus-critical-code-review#bip143-\n> transaction-signature-verification\n>\n> I make it a 0x01000000 at the end of the message because the last 4 bytes\n> has been the nHashType in the legacy/BIP143 protocol. Since the maximum\n> legacy nHashType is 0xff, no collision could ever occur.\n>\n> Putting a 64-byte constant at the beginning should also work, since a\n> collision means SHA256 is no longer preimage resistance. I don\u2019t know much\n> about SHA256 optimisation. How good it is as we put a 64-byte constant at\n> the beginning, while we also make the message 64-byte longer?\n>\n\nIn theory, having a fixed 64 byte constant at the beginning results in zero\noverhead for those 64 bytes.  An implementation would just start the usual\nSHA-256 algorithm with a different pre-computed and fixed initial value\nthan SHA-256's standard initial value.  The SHA-256 padding counter would\nalso need to start at 64*8 bits rather than starting at 0 bits.  In\npractice, assuming a OpenSSL-like implementation of SHA-256, it should be\neasy to implement this optimization. One would replace SHA256_Init call\nwith a variant that initializes the SHA256_CTX to this pre-computed value\nand sets SHA256_CTX's num counter to the appropriate value.  Non-optimized\nimplementations can still just add the 64 byte prefix and use any SHA-256\nimplementation.\n\nFor CHECKSIGFROMSTACK (CSFS), I think the question is whether we want to\n> make it as a separate opcode, or combine that with CHECKSIG. If it is a\n> separate opcode, I think it should be a separate BIP. If it is combined\n> with CHECKSIG, we could do something like this: If the bit 10 of SIGHASH2\n> is set, CHECKSIG will pop one more item from stack, and serialize its\n> content with the transaction digest. Any thought?\n>\n\nI prefer a different opcode for CHECKSIGFROMSTACK because I dislike opcodes\nthat pop a non-static number of elements off the stack.  Popping a dynamic\nnumber of stack elements makes it more difficult to validate that a Script\npubkey doesn't allow any funny business.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180601/db9109ff/attachment.html>"
            },
            {
                "author": "Johnson Lau",
                "date": "2018-06-01T18:45:57",
                "message_text_only": "> On 2 Jun 2018, at 2:15 AM, Russell O'Connor <roconnor at blockstream.io> wrote:\n> \n> \n> I prefer a different opcode for CHECKSIGFROMSTACK because I dislike opcodes that pop a non-static number of elements off the stack.  Popping a dynamic number of stack elements makes it more difficult to validate that a Script pubkey doesn't allow any funny business.\n\n\nAgreed. This is one of the reasons I think we should remove CHECKMULTISIG in the new script system\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180602/9d05b74e/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "SIGHASH2 for version 1 witness programme",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Russell O'Connor",
                "Johnson Lau"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 7912
        }
    },
    {
        "title": "[bitcoin-dev] New serialization/encoding format for key material",
        "thread_messages": [
            {
                "author": "Jonas Schnelli",
                "date": "2018-06-03T16:51:09",
                "message_text_only": "Hi\n\nThe BIP proposal is now available here:\nhttps://gist.github.com/jonasschnelli/68a2a5a5a5b796dc9992f432e794d719\n\nReference C code is available here:\nhttps://github.com/jonasschnelli/bech32_keys\n\nFeedback, criticism, etc. welcome!\n\nThanks\n\u2014\nJonas\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: Message signed with OpenPGP\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180603/32ff5291/attachment.sig>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2018-06-03T19:23:17",
                "message_text_only": "On Sun, Jun 3, 2018 at 9:51 AM, Jonas Schnelli via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Hi\n>\n> The BIP proposal is now available here:\n> https://gist.github.com/jonasschnelli/68a2a5a5a5b796dc9992f432e794d719\n>\n> Reference C code is available here:\n> https://github.com/jonasschnelli/bech32_keys\n>\n> Feedback, criticism, etc. welcome!\n\nFirst of all, thanks for working on this.\n\nI have some concerns about the use of Bech32. It is designed for\ndetecting 3 errors up to length 1023 (but is then picked specifically\nto support 4 errors up to length 89). However, for error correction\nthis translates to just being able to efficiently correct 1 error\n(3/2, rounded down) up to length 1023. You can of course always try\nall combinations of up to N changes to the input (for any N), testing\nthe checksum, and comparing the results against the UTXO set or other\nwallet information that may have been recovered. However, the checksum\nat best gives you a small constant speedup here, not a fundamentally\nimproved way for recovery.\n\nHowever, we can design other base32 BCH codes easily with different\nproperties. As we mostly care about efficient algorithms for recovery\n(and not just error detection properties), it seems more important to\nhave good design strength (as opposed to picking a code from a large\nset which happens to have better properties, but without efficient\nalgorithm, like Bech32).\n\nThis is what I find for codes designed for length 93 (the first length\nfor which efficient codes exist with length long enough to support 256\nbits of data):\n* correct 1 error = 3 checksum characters\n* correct 2 errors = 6 checksum characters\n* correct 3 errors = 10 checksum characters\n* correct 4 errors = 13 checksum characters\n* correct 5 errors = 16 checksum characters\n* ...\n* correct 8 errors = 26 checksum characters (~ length * 1.5)\n* correct 11 errors = 36 checksum characters (~ maximum length without\npushing checksum + data over 93 characters)\n\nFor codes designed for length 341 (the first length enough to support\n512 bits of data):\n* correct 1 error = 3 checksum characters\n* correct 2 errors = 7 checksum characters\n* correct 3 errors = 11 checksum characters\n* correct 4 errors = 15 checksum characters\n* correct 5 errors = 19 checksum characters\n* ...\n* correct 7 errors = 26 checksum characters (~ length * 1.25)\n* correct 13 errors = 51 checksum characters (~ length * 1.5)\n* correct 28 errors = 102 checksum characters (~ length * 2)\n\nSo it really boils down to a trade-off between length of the code, and\nrecovery properties.\n\nThese two sets of codes are distinct (a code designed for length 93\nhas zero error correction properties when going above 93), so either\nwe can pick a separate code for the two purposes, or be limited to the\nsecond set.\n\nIf there is interest, I can construct a code + implementation for any\nof these in a few days probably, once the requirements are clear.\n\nCheers,\n\n-- \nPieter"
            },
            {
                "author": "Jonas Schnelli",
                "date": "2018-06-03T21:30:48",
                "message_text_only": "> I have some concerns about the use of Bech32. It is designed for\n> detecting 3 errors up to length 1023 (but is then picked specifically\n> to support 4 errors up to length 89). However, for error correction\n> this translates to just being able to efficiently correct 1 error\n> (3/2, rounded down) up to length 1023. You can of course always try\n> all combinations of up to N changes to the input (for any N), testing\n> the checksum, and comparing the results against the UTXO set or other\n> wallet information that may have been recovered. However, the checksum\n> at best gives you a small constant speedup here, not a fundamentally\n> improved way for recovery.\n\nThanks Peter\n\nI removed the part in the proposals that made false claims about the error\ncorrection or cpu-intense key recovery.\n\nI wrote some test code and figured out that my Core i7 machine can\ndo 31\u2019775 operations per seconds of a addr-derivation-comparison\n(bech32 decode, bip32 ckd, hash160, Base58check).\nThis is non-optimized code running non-parallelized.\n\nJust in case someone wants to do more math here.\n\nWithout knowing to much about BCHs, ideally there would be a code that\nincludes the fact that computational costs for error correction can be very\nhigh during a disaster recovery and that we can probably assume that the\nuser can provide a derivation element like a used address or pubkey.\n\nDeriving one million child keys and comparing them against an address\ntable will take less than a minute on consumer systems.\n\n> * correct 7 errors = 26 checksum characters (~ length * 1.25)\n> \n> So it really boils down to a trade-off between length of the code, and\n> recovery properties.\n\nI think 5% error correction (7 errors at 555bits) with a 26 char checksum is\nprobably an acceptable tradeoff.\n\nResulting string with 26 checksum chars (mockup):\nxp1qqqqqq8z4rsgv54z9a92yla4m2yrsqdlwdl7gn6qldvwkuh3zrg66z8ad2snf832tgaxcuv3kmwugzl5x8wtnkj2q3a03ky0kg8p7dvv4czpjqgvv4zgnvv4zgnvv4zgnvv4zgngn\n(140 chars)\n\nVersus the bech32 (6 char checksum):\nxp1qqqqqq8z4rsgv54z9a92yla4m2yrsqdlwdl7gn6qldvwkuh3zrg66z8ad2snf832tgaxcuv3kmwugzl5x8wtnkj2q3a03ky0kg8p7dvv4czpjqgvv4zgn\n(120 chars)\n\nVersus an xpriv:\nxprv9wHokC2KXdTSpEepFcu53hMDUHYfAtTaLEJEMyxBPAMf78hJg17WhL5FyeDUQH5KWmGjGgEb2j74gsZqgupWpPbZgP6uFmP8MYEy5BNbyET\n(111 chars)\n\nNot sure if the additional 20 characters make the UX worse.\nTyping in +20 chars in a disaster recovery is probably acceptable.\n\n> If there is interest, I can construct a code + implementation for any\n> of these in a few days probably, once the requirements are clear.\n\n\nYes. Please.\nLets first wait for more feedback about the error robustness though.\n\nThanks\n-\nJonas\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: Message signed with OpenPGP\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180603/2669b658/attachment.sig>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2018-06-13T02:44:47",
                "message_text_only": "On Sun, Jun 3, 2018 at 2:30 PM, Jonas Schnelli <dev at jonasschnelli.ch> wrote:\n>> If there is interest, I can construct a code + implementation for any\n>> of these in a few days probably, once the requirements are clear.\n>\n> Yes. Please.\n\nHere is an example BCH code for base32 data which adds 27 checksum\ncharacters, and can correct up to 7 errors occurring in strings up to\nlength 1023 (including the checksum characters themselves):\nhttps://gist.github.com/sipa/d62f94faa1dcfd9ee4012d4c88955ba6\n\nIt can encode sequences of integers (between 0 and 31):\n\nref.py encode 13 7 22 23 11 29 21 15 3 26 20 26 4 7 6 11 19 1 6 8 31 13 4 19\n\n> d8khta40r656y8xtnpxgldyne96vsfr83uch908se82g98rmnaa\n\nDecode it again:\n\nref.py decode d8khta40r656y8xtnpxgldyne96vsfr83uch908se82g98rmnaa\n\n> Decoded: 13 7 22 23 11 29 21 15 3 26 20 26 4 7 6 11 19 1 6 8 31 13 4 19\n\nOr correct errors:\n\nref.py decode d8khta50r656y8xtmpxhlcyne96vsfr84udh908se82g98rmnat\n\n> Errors found: d8khta?0r656y8xt?px?l?yne96vsfr8?u?h908se82g98rmna?\n> Correction:   d8khta40r656y8xtnpxgldyne96vsfr83uch908se82g98rmnaa\n> Decoded: 13 7 22 23 11 29 21 15 3 26 20 26 4 7 6 11 19 1 6 8 31 13 4 19\n\nThe code above is just a randomly picked BCH code, and has no special\nproperties beyond the ones it is designed for.\n\nI can easily generate similar code for BCH codes with different properties.\n\nCheers,\n\n-- \nPieter"
            },
            {
                "author": "Russell O'Connor",
                "date": "2018-06-15T15:54:30",
                "message_text_only": "> For codes designed for length 341 (the first length enough to support\n> 512 bits of data):\n> * correct 1 error = 3 checksum characters\n> * correct 2 errors = 7 checksum characters\n> * correct 3 errors = 11 checksum characters\n> * correct 4 errors = 15 checksum characters\n> * correct 5 errors = 19 checksum characters\n> * ...\n> * correct 7 errors = 26 checksum characters (~ length * 1.25)\n> * correct 13 errors = 51 checksum characters (~ length * 1.5)\n> * correct 28 errors = 102 checksum characters (~ length * 2)\n>\n> So it really boils down to a trade-off between length of the code, and\n> recovery properties.\n>\n\nAt the risk of making the proposal more complex, I wonder if it might be\nbetter to support multiple checksum variants?  The trade-off between code\nlength and recovery seems to be largely determined by the user's medium of\nstorage, which is likely to vary from person to person.  I personally would\nprobably be interested in the 51 or even 102 character checksums variants.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180615/0d043e8d/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2018-06-23T19:49:54",
                "message_text_only": "On Fri, Jun 15, 2018 at 8:54 AM, Russell O'Connor\n<roconnor at blockstream.io> wrote:\n>\n>> For codes designed for length 341 (the first length enough to support\n>> 512 bits of data):\n>> * correct 1 error = 3 checksum characters\n>> * correct 2 errors = 7 checksum characters\n>> * correct 3 errors = 11 checksum characters\n>> * correct 4 errors = 15 checksum characters\n>> * correct 5 errors = 19 checksum characters\n>> * ...\n>> * correct 7 errors = 26 checksum characters (~ length * 1.25)\n>> * correct 13 errors = 51 checksum characters (~ length * 1.5)\n>> * correct 28 errors = 102 checksum characters (~ length * 2)\n>>\n>> So it really boils down to a trade-off between length of the code, and\n>> recovery properties.\n>\n>\n> At the risk of making the proposal more complex, I wonder if it might be\n> better to support multiple checksum variants?  The trade-off between code\n> length and recovery seems to be largely determined by the user's medium of\n> storage, which is likely to vary from person to person.  I personally would\n> probably be interested in the 51 or even 102 character checksums variants.\n\nHere are some more numbers then. It's important to note that the\nnumber of correctable errors includes errors inside the checksum\ncharacters themselves. So if you want to aim for a certain percentage\nof correctable characters, the numbers go up much more dramatically.\n\nFor codes restricted to 341 characters total (including the checksum\ncharacters), and assuming 103 data characters (enough for 512 bits):\n* With 26 checksum characters (adding 25%, 20% of overall string), 7\nerrors can be corrected (5% of overall string)\n* With 62 checksum characters (adding 60%, 38% of overall string), 17\nerrors can be corrected (10% of overall string)\n* With 116 checksum characters (adding 113%, 53% of overall string),\n33 errors can be corrected (15% of overall string)\n* With 195 checksum characters (adding 189%, 65% of overall string),\n60 errors can be corrected (20% of overall string)\n\nFor codes restricted to 1023 characters total (including the checksum\ncharacters), and assuming 103 data characters (enough for 512 bits):\n* With 27 checksum characters (adding 26%, 21% of overall string), 7\nerrors can be corrected (5% of overall string)\n* With 64 checksum characters (adding 62%, 38% of overall string), 17\nerrors can be corrected (10% of overall string)\n* With 127 checksum characters (adding 123%, 57% of overall string),\n36 errors can be corrected (15% of overall string)\n* With 294 checksum characters (adding 285%, 74% of overall string),\n80 errors can be corrected (20% of overall string)\n* With 920 checksum characters (adding 893%, 90% of overall string),\n255 errors can be corrected (25% of overall string)\n\nI'll gladly construct reference source code for any of these.\n\nCheers,\n\n-- \nPieter"
            },
            {
                "author": "Russell O'Connor",
                "date": "2018-06-13T14:58:33",
                "message_text_only": "On Tue, May 29, 2018 at 5:13 AM, Jonas Schnelli via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi\n>\n> If 520 bits are present, first 256 bits are the BIP32 chain code, to\n> second 264\n> bits (33 bytes) define the public key (according to BIP32)\n>\n\nIn a 33 byte compressed public key, only 1 bit from the first byte conveys\ninformation.  The other 7 bits can be discarded.  This will allow you to\nreduce the bech32 encoded result by one character.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180613/e2042928/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "New serialization/encoding format for key material",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Russell O'Connor",
                "Pieter Wuille",
                "Jonas Schnelli"
            ],
            "messages_count": 7,
            "total_messages_chars_count": 12420
        }
    },
    {
        "title": "[bitcoin-dev] BIP proposal - Dandelion: Privacy Preserving Transaction Propagation",
        "thread_messages": [
            {
                "author": "Bradley Denby",
                "date": "2018-06-04T20:29:50",
                "message_text_only": "Hello all,\n\nWe now have an arXiv preprint of our latest findings available, which\nprovides additional details regarding Dandelion:\nhttps://arxiv.org/pdf/1805.11060.pdf\n\nNote that Dandelion's precision guarantees are at the population level,\nwhile the recall guarantees can be interpreted as individual guarantees.\nExpected recall is equivalent to the probability of an adversary\nassociating a single transaction with a given source.\n\nSince these guarantees are probabilistic, a node cannot be sure whether all\nof its peers are monitoring it. Dandelion does not protect against these\nadversaries, and individuals who are worried about targeted deanonymization\nshould still use Tor.\n\nOne way to conceptualize Dandelion is as a \"public health\" fix or an\n\"anonymity vaccination.\" Higher adoption leads to greater benefits, even\nfor those who are not using Tor. Individuals who adopt Dandelion benefit\nbecause their transactions make at least one hop before diffusing (or more\nas adoption increases).\n\nNevertheless, the probabilistic nature of the guarantees means that they\nare not absolute. We have shown that any solution based only on routing\ncannot be absolute due to fundamental lower bounds on precision and recall.\n\nThank you to Eric Voskuil, Pieter Wuille, Suhas Daftuar, Christian Decker,\nand Tim Ruffing for the recent feedback!\n\nOn Thu, May 10, 2018 at 8:59 AM, Bradley Denby <bdenby at cmu.edu> wrote:\n\n> Hi all,\n>\n> We're writing with an update on the Dandelion project. As a reminder,\n> Dandelion\n> is a practical, lightweight privacy solution that provides Bitcoin users\n> formal\n> anonymity guarantees. While other privacy solutions aim to protect\n> individual\n> users, Dandelion protects privacy by limiting the capability of\n> adversaries to\n> deanonymize the entire network.\n>\n> Bitcoin's transaction spreading protocol is vulnerable to deanonymization\n> attacks. When a node generates a transaction without Dandelion, it\n> transmits\n> that transaction to its peers with independent, exponential delays. This\n> approach, known as diffusion in academia, allows network adversaries to\n> link\n> transactions to IP addresses.\n>\n> Dandelion prevents this class of attacks by sending transactions over a\n> randomly\n> selected path before diffusion. Transactions travel along this path during\n> the\n> \"stem phase\" and are then diffused during the \"fluff phase\" (hence the name\n> Dandelion). We have shown that this routing protocol provides near-optimal\n> anonymity guarantees among schemes that do not introduce additional\n> encryption\n> mechanisms.\n>\n> Since the last time we contacted the list, we have:\n>  - Completed additional theoretical analysis and simulations\n>  - Built a working prototype\n>    (https://github.com/mablem8/bitcoin/tree/dandelion)\n>  - Built a test suite for the prototype\n>    (https://github.com/mablem8/bitcoin/blob/dandelion/test/fun\n> ctional/p2p_dandelion.py)\n>  - Written detailed documentation for the new implementation\n>    (https://github.com/mablem8/bips/blob/master/bip-dandelion/\n> dandelion-reference-documentation.pdf)\n>\n> Among other things, one question we've addressed in our additional\n> analysis is\n> how to route messages during the stem phase. For example, if two Dandelion\n> transactions arrive at a node from different inbound peers, to which\n> Dandelion\n> destination(s) should these transactions be sent? We have found that some\n> choices are much better than others.\n>\n> Consider the case in which each Dandelion transaction is forwarded to a\n> Dandelion destination selected uniformly at random. We have shown that this\n> approach results in a fingerprint attack allowing network-level botnet\n> adversaries to achieve total deanonymization of the P2P network after\n> observing\n> less than ten transactions per node.\n>\n> To avoid this issue, we suggest \"per-inbound-edge\" routing. Each inbound\n> peer is\n> assigned a particular Dandelion destination. Each Dandelion transaction\n> that\n> arrives via this peer is forwarded to the same Dandelion destination.\n> Per-inbound-edge routing breaks the described attack by blocking an\n> adversary's\n> ability to construct useful fingerprints.\n>\n> This iteration of Dandelion has been tested on our own small network, and\n> we\n> would like to get the implementation in front of a wider audience. An\n> updated\n> BIP document with further details on motivation, specification,\n> compatibility,\n> and implementation is located here:\n> https://github.com/mablem8/bips/blob/master/bip-dandelion.mediawiki\n>\n> We would like to thank the Bitcoin Core developers and Gregory Maxwell in\n> particular for their insightful comments, which helped to inform this\n> implementation and some of the follow-up work we conducted. We would also\n> like\n> to thank the Mimblewimble development community for coining the term\n> \"stempool,\"\n> which we happily adopted for this implementation.\n>\n> All the best,\n> Brad Denby <bdenby at cmu.edu>\n> Andrew Miller <soc1024 at illinois.edu>\n> Giulia Fanti <gfanti at andrew.cmu.edu>\n> Surya Bakshi <sbakshi3 at illinois.edu>\n> Shaileshh Bojja Venkatakrishnan <shaileshh.bv at gmail.com>\n> Pramod Viswanath <pramodv at illinois.edu>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180604/3c3f9692/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2018-06-06T04:01:00",
                "message_text_only": "On Thu, May 10, 2018 at 5:59 AM, Bradley Denby via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Hi all,\n>\n> ...\n>\n> This iteration of Dandelion has been tested on our own small network, and we\n> would like to get the implementation in front of a wider audience. An\n> updated\n> BIP document with further details on motivation, specification,\n> compatibility,\n> and implementation is located here:\n> https://github.com/mablem8/bips/blob/master/bip-dandelion.mediawiki\n\nHi Bradley,\n\nthank you for working on this and going as far as implementing the\nentire protocol. It looks like a very well-worked out idea already,\nand its semantics can probably be adopted pretty much as-is. It would\nbe very exciting to bring these kinds of privacy improvements to\nBitcoin's P2P protocol.\n\nI do have a number of comments on the specification and suggested\nimplementation in Bitcoin Core. I'm dumping my thoughts here, though\nat this stage the specification is probably more important. The\nimplementation can be discussed more thoroughly when there is a PR\nopen.\n\nSpecification\n\n* Overall, I think it would be worthwhile to describe the intended\nnode behavior in the BIP, at a higher level than Bitcoin Core\npatchsets, but more detailed than what is in the BIP now. The\npatch-based descriptions are both hard to read for developers working\non different systems who are unfamiliar with the Core codebase, and\ndon't make it clear to what extent implementation decisions are local\npolicy (which can be changed without network coordination), and which\nfollow from security or privacy arguments for the protocol.\n\n* Interaction with feefilter (BIP 133) and Bloom filter (BIP 37). When\npeers have given us filters on what transactions they will accept,\nshould Dandelion transactions be subject to the same? Should it\ninfluence the choice of route? One simple possibility is perhaps to\navoid choosing BIP37 peers as Dandelion routes, and treat transactions\nthat do not pass the feefilter for its\nwould-be-outgoing-Dandelion-route as an automatic fluff - justified by\nnoting that relaying a transaction close to what fee is acceptable to\nthe network's mempools is already less likely to get good privacy due\nto reduced chances of propagation.\n\n* Mempool dependant transactions. It looks like the current\nimplementation accepts Dandelion transactions which are dependant on\nother Dandelion (stempool) transactions and on confirmed blockchain\ntransactions, but not ones that are dependant on other unconfirmed\nnormal mempool transactions. Is this intentional, or resulting from a\ndifficulty in implementing this? Should the correct behaviour be\nspecified, or left free for nodes to decide?\n\n* Orphan transactions. It looks like the current implementation\nassumes no orphan transactions, but in a dynamic network (especially\nwith occasionally shuffling of Dandelion routes), I expect that\nsometimes a dependent transaction will go on a different route than\nits parent. Do you have any thoughts about that (even if not addressed\nin a very implementation). Could we have a Dandelion-orphan-pool of\ntransactions, similar to the normal mempool has a set of orphan\ntransactions?\n\n* Preferred connections. Should we bias the outgoing connection peer\nselection code to prefer Dandelion-capable peers when the number is\ntoo low?\n\nImplementation\n\n* How do we control the size of the stempool? Should acceptance of a\ntransaction to the normal mempool and/or blockchain result in eviction\nof it (and conflicts) from the stempool? The existing code\nintentionally has an upper bound on the size of the mempool to assure\npredictable resource usage - the introduction of the stempool\nshouldn't change that.\n\n* I don't think you need to fully materialize all the routes. Instead,\nyou can just maintain a vector of 2 selected Dandelion-supporting\npeers (and if one disconnects, replace just that one with another\none). To map incoming peers to an index in that list of peers, you can\nuse deterministic randomness (see SipHasher in the source code) with\nthe incoming node_id as data and a single global secret nonce (chosen\nat startup, and reset on reshuffle).\n\n* setDandelionInventoryKnown looks like it can grow unboundedly. A\nrolling Bloom filter (like used for filterInventoryKnown) is perhaps\neasier to guarantee predictable memory usage for.\n\n* Use a scheduler job instead of a separate thread for shuffling the\nroutes (extra threads use unnecessarily large amounts of memory).\n\n* (nit) coding style: doc/developer-notes.md has a number of\nguidelines on coding style you may want to check out.\n\nCheers,\n\n-- \nPieter"
            },
            {
                "author": "Bradley Denby",
                "date": "2018-06-11T14:31:09",
                "message_text_only": "Thanks for the comments Pieter!\n\nWe can make descriptions for the intended node behaviors more clear in the\nBIP.\n\nRegarding interaction with BIPs 37 and 133, we have found that if Dandelion\nrouting decisions are based on self-reported features, malicious nodes can\noften exploit that to launch serious deanonymization attacks. As a result,\nwe recommend not allowing fee filters from peers to influence the choice of\nroute. Your suggestion of automatically fluffing is a good solution.\nAnother (similar) option would be to apply fee filters in the stempool.\nThis would prevent the tx from propagating in stem phase, so eventually an\nembargo timer on the stem will expire and the transaction will fluff. This\nis slower than auto-fluffing, but requires (slightly) less code.\n\nRegarding mempool-dependent transactions, the reference implementation adds\nany mempool transactions to the stempool but not vice-versa so that the\nstempool becomes a superset of the mempool. In other words, information is\nfree to flow from the mempool to the stempool. Information does not flow\nfrom the stempool to the mempool except when a transaction fluffs. As a\nresult, a node's stempool should accept and propagate Dandelion\ntransactions that depend on other unconfirmed normal mempool transactions.\nThe behavior you described is not intended; if you have any tests\ndemonstrating this behavior, would you mind sharing them?\n\nOrphans: stem orphans can occur when a node on the stem shuffles its route\nbetween sending dependent transactions. One way to deal with this issue\nwould be to re-broadcast all previous Dandelion transactions that have not\nbeen fluffed after Dandelion route shuffling. This could add a fair amount\nof data and logic. This re-broadcast method also telegraphs the fact that a\nDandelion shuffle has taken place and can result in bursts of transactions\ndepending on traffic patterns. A second option (which we used in the\nreference implementation) is to wait for the fluff phase to begin, at which\npoint the orphans will be resolved. This should happen within 15 seconds\nfor most transactions. Do you have any thoughts on which option would be\nmore palatable? Or if there are other options we have missed?\n\nRegarding preferred connections, we have found that making Dandelion\nrouting decisions based on claims made by peer nodes can cause problems and\ntherefore would recommend against biasing the peer selection code.\n\nOn the implementation side:\n\n* We apply the same logic to the stempool as the mempool in the reference\nimplementation. The stempool should remain a superset of the mempool to\nallow for proper handling of mempool-dependent transactions.\n\n* We'll take a look at setDandelionInventoryKnown.\n\n* We will look into using scheduler jobs instead of a separate\nthread--could you point us towards somewhere else in the code that uses a\nscheduler job?\n\nBased on the feedback we have received so far, we are planning to\nprioritize writing up a clearer spec for node behavior in the BIP. Does\nthat seem reasonable, or are there other issues that are more pressing at\nthis point?\n\nCheers\n\nOn Wed, Jun 6, 2018 at 12:01 AM, Pieter Wuille <pieter.wuille at gmail.com>\nwrote:\n\n> On Thu, May 10, 2018 at 5:59 AM, Bradley Denby via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > Hi all,\n> >\n> > ...\n> >\n> > This iteration of Dandelion has been tested on our own small network,\n> and we\n> > would like to get the implementation in front of a wider audience. An\n> > updated\n> > BIP document with further details on motivation, specification,\n> > compatibility,\n> > and implementation is located here:\n> > https://github.com/mablem8/bips/blob/master/bip-dandelion.mediawiki\n>\n> Hi Bradley,\n>\n> thank you for working on this and going as far as implementing the\n> entire protocol. It looks like a very well-worked out idea already,\n> and its semantics can probably be adopted pretty much as-is. It would\n> be very exciting to bring these kinds of privacy improvements to\n> Bitcoin's P2P protocol.\n>\n> I do have a number of comments on the specification and suggested\n> implementation in Bitcoin Core. I'm dumping my thoughts here, though\n> at this stage the specification is probably more important. The\n> implementation can be discussed more thoroughly when there is a PR\n> open.\n>\n> Specification\n>\n> * Overall, I think it would be worthwhile to describe the intended\n> node behavior in the BIP, at a higher level than Bitcoin Core\n> patchsets, but more detailed than what is in the BIP now. The\n> patch-based descriptions are both hard to read for developers working\n> on different systems who are unfamiliar with the Core codebase, and\n> don't make it clear to what extent implementation decisions are local\n> policy (which can be changed without network coordination), and which\n> follow from security or privacy arguments for the protocol.\n>\n> * Interaction with feefilter (BIP 133) and Bloom filter (BIP 37). When\n> peers have given us filters on what transactions they will accept,\n> should Dandelion transactions be subject to the same? Should it\n> influence the choice of route? One simple possibility is perhaps to\n> avoid choosing BIP37 peers as Dandelion routes, and treat transactions\n> that do not pass the feefilter for its\n> would-be-outgoing-Dandelion-route as an automatic fluff - justified by\n> noting that relaying a transaction close to what fee is acceptable to\n> the network's mempools is already less likely to get good privacy due\n> to reduced chances of propagation.\n>\n> * Mempool dependant transactions. It looks like the current\n> implementation accepts Dandelion transactions which are dependant on\n> other Dandelion (stempool) transactions and on confirmed blockchain\n> transactions, but not ones that are dependant on other unconfirmed\n> normal mempool transactions. Is this intentional, or resulting from a\n> difficulty in implementing this? Should the correct behaviour be\n> specified, or left free for nodes to decide?\n>\n> * Orphan transactions. It looks like the current implementation\n> assumes no orphan transactions, but in a dynamic network (especially\n> with occasionally shuffling of Dandelion routes), I expect that\n> sometimes a dependent transaction will go on a different route than\n> its parent. Do you have any thoughts about that (even if not addressed\n> in a very implementation). Could we have a Dandelion-orphan-pool of\n> transactions, similar to the normal mempool has a set of orphan\n> transactions?\n>\n> * Preferred connections. Should we bias the outgoing connection peer\n> selection code to prefer Dandelion-capable peers when the number is\n> too low?\n>\n> Implementation\n>\n> * How do we control the size of the stempool? Should acceptance of a\n> transaction to the normal mempool and/or blockchain result in eviction\n> of it (and conflicts) from the stempool? The existing code\n> intentionally has an upper bound on the size of the mempool to assure\n> predictable resource usage - the introduction of the stempool\n> shouldn't change that.\n>\n> * I don't think you need to fully materialize all the routes. Instead,\n> you can just maintain a vector of 2 selected Dandelion-supporting\n> peers (and if one disconnects, replace just that one with another\n> one). To map incoming peers to an index in that list of peers, you can\n> use deterministic randomness (see SipHasher in the source code) with\n> the incoming node_id as data and a single global secret nonce (chosen\n> at startup, and reset on reshuffle).\n>\n> * setDandelionInventoryKnown looks like it can grow unboundedly. A\n> rolling Bloom filter (like used for filterInventoryKnown) is perhaps\n> easier to guarantee predictable memory usage for.\n>\n> * Use a scheduler job instead of a separate thread for shuffling the\n> routes (extra threads use unnecessarily large amounts of memory).\n>\n> * (nit) coding style: doc/developer-notes.md has a number of\n> guidelines on coding style you may want to check out.\n>\n> Cheers,\n>\n> --\n> Pieter\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180611/d971d843/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2018-06-12T01:05:14",
                "message_text_only": "On Mon, Jun 11, 2018, 07:37 Bradley Denby via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Thanks for the comments Pieter!\n>\n> We can make descriptions for the intended node behaviors more clear in the\n> BIP.\n>\n> Regarding interaction with BIPs 37 and 133, we have found that if\n> Dandelion routing decisions are based on self-reported features, malicious\n> nodes can often exploit that to launch serious deanonymization attacks. As\n> a result, we recommend not allowing fee filters from peers to influence the\n> choice of route. Your suggestion of automatically fluffing is a good\n> solution. Another (similar) option would be to apply fee filters in the\n> stempool. This would prevent the tx from propagating in stem phase, so\n> eventually an embargo timer on the stem will expire and the transaction\n> will fluff. This is slower than auto-fluffing, but requires (slightly) less\n> code.\n>\n\nI understand the argument about not making routing decisions based on\nself-reported features, but I would expect it to only matter if done\nselectively? Allowing a node to opt out of Dandelion entirely should always\nbe possible regardless - as they can always indicate not supporting it.\n\nThe reason for my suggestion was that most full nodes on the network use\nfeefilter, while only (from the perspective of Dandelion uninteresting)\nlight nodes and blocksonly nodes generally use Bloom filters.\n\nJust dropping stem transactions that would otherwise be sent to a Dandelion\npeer which fails its filter, and relying on embargo seems fine. But perhaps\nthis option is something to describe in the BIP (\"Nodes MAY choose to\neither drop stem transactions or immediately start diffusion when a\ntransaction would otherwise be sent to a Dandelion node whose filter is not\nsatisfied for that transaction. A node SHOULD NOT make any routing\ndecisions based on the transaction itself, and thus SHOULD NOT try to find\nan alternative Dandelion node to forward to\" for example).\n\nRegarding mempool-dependent transactions, the reference implementation adds\n> any mempool transactions to the stempool but not vice-versa so that the\n> stempool becomes a superset of the mempool. In other words, information is\n> free to flow from the mempool to the stempool. Information does not flow\n> from the stempool to the mempool except when a transaction fluffs. As a\n> result, a node's stempool should accept and propagate Dandelion\n> transactions that depend on other unconfirmed normal mempool transactions.\n> The behavior you described is not intended; if you have any tests\n> demonstrating this behavior, would you mind sharing them?\n>\n\nOh, I see! I was just judging based on the spec code you published, but I\nmust have missed this. Yes, that makes perfect sense. There may be some\nissues with this having a significant impact on stempool memory usage, but\nlet's discuss this later on implementation.\n\nOrphans: stem orphans can occur when a node on the stem shuffles its route\n> between sending dependent transactions. One way to deal with this issue\n> would be to re-broadcast all previous Dandelion transactions that have not\n> been fluffed after Dandelion route shuffling. This could add a fair amount\n> of data and logic. This re-broadcast method also telegraphs the fact that a\n> Dandelion shuffle has taken place and can result in bursts of transactions\n> depending on traffic patterns. A second option (which we used in the\n> reference implementation) is to wait for the fluff phase to begin, at which\n> point the orphans will be resolved. This should happen within 15 seconds\n> for most transactions. Do you have any thoughts on which option would be\n> more palatable? Or if there are other options we have missed?\n>\n\nAnother option (just brainstorming, I may be missing something here), is to\nremember which peer each stempool transaction was forwarded to. When a\ndependent stem transaction arrives, it is always sent to (one of?) the\npeers its dependencies were sent to, even if a reshuffle happened in\nbetween.\n\nThinking more about it, relying on embargo is probably fine - it'll just\nresult in slightly lowered average stem length, and perhaps multiple\nsimultaneous fluffs starting?\n\nRegarding preferred connections, we have found that making Dandelion\n> routing decisions based on claims made by peer nodes can cause problems and\n> therefore would recommend against biasing the peer selection code.\n>\n\nOh, I don't mean routing decisions, but connections in general.\n\nOn the implementation side:\n>\n\nLet's discuss these later.\n\n\n> Based on the feedback we have received so far, we are planning to\n> prioritize writing up a clearer spec for node behavior in the BIP. Does\n> that seem reasonable, or are there other issues that are more pressing at\n> this point?\n>\n\nI think that's the primary thing to focus on at this point, but perhaps\nothers on this list feel different.\n\nCheers,\n\n-- \nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180611/308dd73e/attachment-0001.html>"
            },
            {
                "author": "Bradley Denby",
                "date": "2018-06-26T00:12:02",
                "message_text_only": "On Mon, Jun 11, 2018 at 9:05 PM, Pieter Wuille <pieter.wuille at gmail.com>\nwrote:\n\n> On Mon, Jun 11, 2018, 07:37 Bradley Denby via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Thanks for the comments Pieter!\n>>\n>> We can make descriptions for the intended node behaviors more clear in\n>> the BIP.\n>>\n>> Regarding interaction with BIPs 37 and 133, we have found that if\n>> Dandelion routing decisions are based on self-reported features, malicious\n>> nodes can often exploit that to launch serious deanonymization attacks. As\n>> a result, we recommend not allowing fee filters from peers to influence the\n>> choice of route. Your suggestion of automatically fluffing is a good\n>> solution. Another (similar) option would be to apply fee filters in the\n>> stempool. This would prevent the tx from propagating in stem phase, so\n>> eventually an embargo timer on the stem will expire and the transaction\n>> will fluff. This is slower than auto-fluffing, but requires (slightly) less\n>> code.\n>>\n>\n> I understand the argument about not making routing decisions based on\n> self-reported features, but I would expect it to only matter if done\n> selectively? Allowing a node to opt out of Dandelion entirely should always\n> be possible regardless - as they can always indicate not supporting it.\n>\n\nThat's right, the idea is to choose Dandelion relays independently from\nwhether they support Dandelion. If the chosen nodes do not support\nDandelion, then the transactions are fluffed. Otherwise, the transactions\nare relayed along a stem.\n\n\n>\n> The reason for my suggestion was that most full nodes on the network use\n> feefilter, while only (from the perspective of Dandelion uninteresting)\n> light nodes and blocksonly nodes generally use Bloom filters.\n>\n> Just dropping stem transactions that would otherwise be sent to a\n> Dandelion peer which fails its filter, and relying on embargo seems fine.\n> But perhaps this option is something to describe in the BIP (\"Nodes MAY\n> choose to either drop stem transactions or immediately start diffusion when\n> a transaction would otherwise be sent to a Dandelion node whose filter is\n> not satisfied for that transaction. A node SHOULD NOT make any routing\n> decisions based on the transaction itself, and thus SHOULD NOT try to find\n> an alternative Dandelion node to forward to\" for example).\n>\n\nThanks for the suggestion, we've updated the BIP with RFC 2119 language.\n\n\n>\n> Regarding mempool-dependent transactions, the reference implementation\n>> adds any mempool transactions to the stempool but not vice-versa so that\n>> the stempool becomes a superset of the mempool. In other words, information\n>> is free to flow from the mempool to the stempool. Information does not flow\n>> from the stempool to the mempool except when a transaction fluffs. As a\n>> result, a node's stempool should accept and propagate Dandelion\n>> transactions that depend on other unconfirmed normal mempool transactions.\n>> The behavior you described is not intended; if you have any tests\n>> demonstrating this behavior, would you mind sharing them?\n>>\n>\n> Oh, I see! I was just judging based on the spec code you published, but I\n> must have missed this. Yes, that makes perfect sense. There may be some\n> issues with this having a significant impact on stempool memory usage, but\n> let's discuss this later on implementation.\n>\n> Orphans: stem orphans can occur when a node on the stem shuffles its route\n>> between sending dependent transactions. One way to deal with this issue\n>> would be to re-broadcast all previous Dandelion transactions that have not\n>> been fluffed after Dandelion route shuffling. This could add a fair amount\n>> of data and logic. This re-broadcast method also telegraphs the fact that a\n>> Dandelion shuffle has taken place and can result in bursts of transactions\n>> depending on traffic patterns. A second option (which we used in the\n>> reference implementation) is to wait for the fluff phase to begin, at which\n>> point the orphans will be resolved. This should happen within 15 seconds\n>> for most transactions. Do you have any thoughts on which option would be\n>> more palatable? Or if there are other options we have missed?\n>>\n>\n> Another option (just brainstorming, I may be missing something here), is\n> to remember which peer each stempool transaction was forwarded to. When a\n> dependent stem transaction arrives, it is always sent to (one of?) the\n> peers its dependencies were sent to, even if a reshuffle happened in\n> between.\n>\n> Thinking more about it, relying on embargo is probably fine - it'll just\n> result in slightly lowered average stem length, and perhaps multiple\n> simultaneous fluffs starting?\n>\n\nThat's right, the stem length would be slightly shorter because of the time\nspent waiting for the parent transaction, and you could get multiple\nsimultaneous fluffs. If this is acceptable, it is probably the simplest\nsolution.\n\n\n>\n> Regarding preferred connections, we have found that making Dandelion\n>> routing decisions based on claims made by peer nodes can cause problems and\n>> therefore would recommend against biasing the peer selection code.\n>>\n>\n> Oh, I don't mean routing decisions, but connections in general.\n>\n\nAh ok. Even biasing a node's connections to prefer Dandelion nodes could be\nproblematic, especially in the early-deployment stage. For instance, a set\nof malicious nodes could run Dandelion at the beginning; since there are\nfew honest nodes running Dandelion, the malicious nodes would draw a\ndisproportionate fraction of peer connections. This could have implications\nfor anonymity as well as eclipsing attacks. So we would suggest not\nchanging the peer connection strategy. In fact, we found that even when\nthere are very few nodes running Dandelion, this Dandelion-agnostic\nconnection strategy still provides some benefit over the current mechanism.\n\n\n>\n> On the implementation side:\n>>\n>\n> Let's discuss these later.\n>\n>\n>> Based on the feedback we have received so far, we are planning to\n>> prioritize writing up a clearer spec for node behavior in the BIP. Does\n>> that seem reasonable, or are there other issues that are more pressing at\n>> this point?\n>>\n>\n> I think that's the primary thing to focus on at this point, but perhaps\n> others on this list feel different.\n>\n\nWe've updated the BIP with RFC 2119 statements. Thanks for the feedback!\n\n\n>\n> Cheers,\n>\n> --\n> Pieter\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180625/319d09a2/attachment-0001.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2018-06-26T05:20:02",
                "message_text_only": "On Tue, Jun 26, 2018 at 12:12 AM, Bradley Denby via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> That's right, the idea is to choose Dandelion relays independently from\n> whether they support Dandelion. If the chosen nodes do not support\n> Dandelion, then the transactions are fluffed. Otherwise, the transactions\n> are relayed along a stem.\n\nI don't see any problem with doing that... Although an additional\ncountermeasure we're likely to take against attacks on partial\ndeployment is that we'd likely make the wallet's use of stem\nforwarding be a configuration option which is initially hidden and set\nto off.  In a subsistent release after dandelion propagation is widely\ndeployed we'd unhide the option and default it to on.   This way users\ndon't begin using it until the deployment is relatively dense.\n\nI believe this approach is a is sufficient such that it could always\nselect out-peers that were dandelion capable without harm,  but at the\nsame time I also don't see a reason that we can't do both.\n\n(in fact, for privacy reasons we might want to three-stage the\ndeployment, with the use of dandelion by wallets having a setting of\noff, sometimes, or always so that attackers can't so easily correlate\nthe use of dandelion with upgrades.)"
            }
        ],
        "thread_summary": {
            "title": "BIP proposal - Dandelion: Privacy Preserving Transaction Propagation",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Bradley Denby",
                "Gregory Maxwell",
                "Pieter Wuille"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 31033
        }
    },
    {
        "title": "[bitcoin-dev] BIP suggestion: PoW proportional to block transaction sum",
        "thread_messages": [
            {
                "author": "Thomas Guyot-Sionnest",
                "date": "2018-06-05T10:50:35",
                "message_text_only": "On 30/05/18 12:17 PM, Darren Weber via bitcoin-dev wrote:\n>\n> Apologies for brevity, noob here and just throwing out an idea in case\n> it's useful (probably already covered somewhere, but I haven't got\n> time to do all the necessary background research).\n>\n> From https://github.com/bitcoin/bitcoin/issues/13342\n>\n> Suggestion:\u00a0 To make it more difficult for a malicious attacker to\n> reap quick rewards by double-spending large amounts with a relatively\n> brief majority of the network hashing power, introduce a hash workload\n> that is proportional to the sum of transactions in a block (probably\n> the sum of the absolute values, and a \"proportionality function\" could\n> be linear or exponential).\u00a0 The motivation is to make it more\n> difficult for malicious attacks to hash-power their way through a few\n> large transactions.\u00a0 Obviously, there are costs in greater transaction\n> delays (and fees?) for larger amounts (absolute value).\n>\n> If there is original value in the idea, I can try to make time to\n> follow-up with a better BIP proposal.\n>\nHi Darren,\n\nI'm wondering how do you think this can be implemented... The problem\nbeing that you cannot just decide to exclude transactions because you\nfound a lesser difficulty hash since that hash includes all transactions\nalready... Miners will either include or not these transactions based on\neconomical value, and since most of the rewards still comes from block\nrewards there would be very little right now except with very high fees.\n\nEven worse, it may have detrimental side-effects: since there is no\ndistinctions between destination and change addresses, one can only\nassume the transaction amount is the full input amount. Therefore users\nwould be inclined to keep large amount in lots of smaller addresses to\navoid being penalized on small transactions, increasing the UTXO size\nfor everybody.\n\nAnd besides, this is a huge change to swallow, requiring very good\nconsensus and a hard fork. IMHO I wouldn't even waste time on this.\n\nRegards,\n\n-- \nThomas"
            },
            {
                "author": "Darren Weber",
                "date": "2018-06-06T21:01:22",
                "message_text_only": "Hi Thomas,\n\nThanks for considering this suggestion.  You've raised some interesting\npoints (and concluded that it could be very difficult to implement).  I'm\nnot yet at a point where I could answer any questions about implementation\ndetails with any authority.  With that caveat, your points are worth\nconsidering further and I will dwell on it for a bit.\n\nBest regards,\nDarren\n\n\nOn Tue, Jun 5, 2018 at 3:50 AM, Thomas Guyot-Sionnest <dermoth at aei.ca>\nwrote:\n\n> On 30/05/18 12:17 PM, Darren Weber via bitcoin-dev wrote:\n> >\n> > Apologies for brevity, noob here and just throwing out an idea in case\n> > it's useful (probably already covered somewhere, but I haven't got\n> > time to do all the necessary background research).\n> >\n> > From https://github.com/bitcoin/bitcoin/issues/13342\n> >\n> > Suggestion:  To make it more difficult for a malicious attacker to\n> > reap quick rewards by double-spending large amounts with a relatively\n> > brief majority of the network hashing power, introduce a hash workload\n> > that is proportional to the sum of transactions in a block (probably\n> > the sum of the absolute values, and a \"proportionality function\" could\n> > be linear or exponential).  The motivation is to make it more\n> > difficult for malicious attacks to hash-power their way through a few\n> > large transactions.  Obviously, there are costs in greater transaction\n> > delays (and fees?) for larger amounts (absolute value).\n> >\n> > If there is original value in the idea, I can try to make time to\n> > follow-up with a better BIP proposal.\n> >\n> Hi Darren,\n>\n> I'm wondering how do you think this can be implemented... The problem\n> being that you cannot just decide to exclude transactions because you\n> found a lesser difficulty hash since that hash includes all transactions\n> already... Miners will either include or not these transactions based on\n> economical value, and since most of the rewards still comes from block\n> rewards there would be very little right now except with very high fees.\n>\n> Even worse, it may have detrimental side-effects: since there is no\n> distinctions between destination and change addresses, one can only\n> assume the transaction amount is the full input amount. Therefore users\n> would be inclined to keep large amount in lots of smaller addresses to\n> avoid being penalized on small transactions, increasing the UTXO size\n> for everybody.\n>\n> And besides, this is a huge change to swallow, requiring very good\n> consensus and a hard fork. IMHO I wouldn't even waste time on this.\n>\n> Regards,\n>\n> --\n> Thomas\n>\n>\n>\n\n\n-- \nDarren L. Weber, Ph.D.\nhttp://psdlw.users.sourceforge.net/\nhttp://psdlw.users.sourceforge.net/wordpress/\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180606/817306c9/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "BIP suggestion: PoW proportional to block transaction sum",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Thomas Guyot-Sionnest",
                "Darren Weber"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 4874
        }
    },
    {
        "title": "[bitcoin-dev] [BIP Proposal] BetterHash Mining Protocol Replacements",
        "thread_messages": [
            {
                "author": "Matt Corallo",
                "date": "2018-06-05T18:44:57",
                "message_text_only": "Been working on this one for a while, so its already been through a few\nrounds of feeback (thanks to all those who already have provided feedback)!\n\nAt a high level, this meets a few goals:\n\n1) Replace getblocktemplate with something that is both more performant\n(no JSON encoding, no full transactions sent over the wire to update a\njob, hence we can keep the same CTransactionRef in Bitcoin Core making\nlots of validation things way faster), more robust for consensus changes\n(no need to add protocol changes to add commitments ala SegWit in the\nfuture), and moves more block-switching logic inside of the work\nprovider (allowing Bitcoin Core to better optimize work switching as it\nknows more than an outside pool server, specifically we can play more\ngames with how we do mempool eviction, empty block mining, and not\nmining fresh transactions more easily by moving to a more \"push\" model\nfrom the normal \"pull\" getblocktemplate implementation).\n\n2) Replace Stratum with something more secure (sign messages when\napplicable, without adding too much overhead to the pool), simpler to\nimplement (not JSON-wrapped-hex, no 32-byte-swapped-per-4-byte-byteorder\ninsanity), and better-defined (a clearly written spec, encompassing the\nvarious things shoved backwards into stratum like suggested difficulty\nin the password field and device identification by setting user to\n\"user.device\") with VENDOR_MESSAGEs provided for extensibility instead\nof conflicting specifications from various different vendors.\n\n3) Provide the ability for a pool to accept work which the users of the\npool selected the transactions for, providing strong decentralization\npressure by removing the network-level centralization attacks pools can\ndo (or be compromised and used to perform) while still allowing them\nfull control of payout management and variance reduction.\n\nWhile (1) and (2) stand on their own, making it all one set of protocols\nto provide (3) provides at least the opportunity for drastically better\ndecentralization in Bitcoin mining in the future.\n\nThe latest version of the full BIP draft can be found at\nhttps://github.com/TheBlueMatt/bips/blob/betterhash/bip-XXXX.mediawiki\nand implementations of the work-generation part at\nhttps://github.com/TheBlueMatt/bitcoin/commits/2018-02-miningserver and\npool/proxy parts at https://github.com/TheBlueMatt/mining-proxy (though\nnote that both implementations are currently on a slightly out-of-date\nversion of the protocol, I hope to get them brought up to date in the\ncoming day or two and make them much more full-featured. The whole stack\nhas managed to mine numerous testnet blocks on several different types\nof hardware).\n\nMatt"
            },
            {
                "author": "Chris Pacia",
                "date": "2018-06-06T01:26:35",
                "message_text_only": "Really like that you're moving forward with this. A few months ago I was \nworking on something similar as it seemed like nobody else was interested.\n\nIn regards to the specific proposal, would it make sense to offer a tx \nsubscription endpoint in addition to TRANSACTION_DATA_REQUEST? Such an \nendpoint could respond to the subscription with the current full list of \ntransactions and then push the diff every time a new template is pushed. \nA client that wants to inspect and modify the transactions would use \nquite a bit less data than polling the request endpoint.\n\n\nOn 06/05/2018 02:44 PM, Matt Corallo via bitcoin-dev wrote:\n> Been working on this one for a while, so its already been through a few\n> rounds of feeback (thanks to all those who already have provided feedback)!\n>\n> At a high level, this meets a few goals:\n>\n> 1) Replace getblocktemplate with something that is both more performant\n> (no JSON encoding, no full transactions sent over the wire to update a\n> job, hence we can keep the same CTransactionRef in Bitcoin Core making\n> lots of validation things way faster), more robust for consensus changes\n> (no need to add protocol changes to add commitments ala SegWit in the\n> future), and moves more block-switching logic inside of the work\n> provider (allowing Bitcoin Core to better optimize work switching as it\n> knows more than an outside pool server, specifically we can play more\n> games with how we do mempool eviction, empty block mining, and not\n> mining fresh transactions more easily by moving to a more \"push\" model\n> from the normal \"pull\" getblocktemplate implementation).\n>\n> 2) Replace Stratum with something more secure (sign messages when\n> applicable, without adding too much overhead to the pool), simpler to\n> implement (not JSON-wrapped-hex, no 32-byte-swapped-per-4-byte-byteorder\n> insanity), and better-defined (a clearly written spec, encompassing the\n> various things shoved backwards into stratum like suggested difficulty\n> in the password field and device identification by setting user to\n> \"user.device\") with VENDOR_MESSAGEs provided for extensibility instead\n> of conflicting specifications from various different vendors.\n>\n> 3) Provide the ability for a pool to accept work which the users of the\n> pool selected the transactions for, providing strong decentralization\n> pressure by removing the network-level centralization attacks pools can\n> do (or be compromised and used to perform) while still allowing them\n> full control of payout management and variance reduction.\n>\n> While (1) and (2) stand on their own, making it all one set of protocols\n> to provide (3) provides at least the opportunity for drastically better\n> decentralization in Bitcoin mining in the future.\n>\n> The latest version of the full BIP draft can be found at\n> https://github.com/TheBlueMatt/bips/blob/betterhash/bip-XXXX.mediawiki\n> and implementations of the work-generation part at\n> https://github.com/TheBlueMatt/bitcoin/commits/2018-02-miningserver and\n> pool/proxy parts at https://github.com/TheBlueMatt/mining-proxy (though\n> note that both implementations are currently on a slightly out-of-date\n> version of the protocol, I hope to get them brought up to date in the\n> coming day or two and make them much more full-featured. The whole stack\n> has managed to mine numerous testnet blocks on several different types\n> of hardware).\n>\n> Matt\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Matt Corallo",
                "date": "2018-06-06T19:16:09",
                "message_text_only": "Clients \"inspecting and modifying the transactions\" is explicitly *not*\nsupported. There should be more than enough features for clients to get\nbitcoind to generate the exact block they want already available via\nBitcoin Core. The only reason transactions are exposed over the work\nprotocol at all, really, is so that clients can generate weak blocks to\nbe sent to the pool for efficient client -> pool block relay, not sure\nthat's worth bothering to add a whole new endpoint for, sounds\nneedlessly complicated (and the spec is already more than complicated\nenough, sadly).\n\nMatt\n\nOn 06/05/18 21:26, Chris Pacia via bitcoin-dev wrote:\n> Really like that you're moving forward with this. A few months ago I was\n> working on something similar as it seemed like nobody else was interested.\n> \n> In regards to the specific proposal, would it make sense to offer a tx\n> subscription endpoint in addition to TRANSACTION_DATA_REQUEST? Such an\n> endpoint could respond to the subscription with the current full list of\n> transactions and then push the diff every time a new template is pushed.\n> A client that wants to inspect and modify the transactions would use\n> quite a bit less data than polling the request endpoint.\n> \n> \n> On 06/05/2018 02:44 PM, Matt Corallo via bitcoin-dev wrote:\n>> Been working on this one for a while, so its already been through a few\n>> rounds of feeback (thanks to all those who already have provided\n>> feedback)!\n>>\n>> At a high level, this meets a few goals:\n>>\n>> 1) Replace getblocktemplate with something that is both more performant\n>> (no JSON encoding, no full transactions sent over the wire to update a\n>> job, hence we can keep the same CTransactionRef in Bitcoin Core making\n>> lots of validation things way faster), more robust for consensus changes\n>> (no need to add protocol changes to add commitments ala SegWit in the\n>> future), and moves more block-switching logic inside of the work\n>> provider (allowing Bitcoin Core to better optimize work switching as it\n>> knows more than an outside pool server, specifically we can play more\n>> games with how we do mempool eviction, empty block mining, and not\n>> mining fresh transactions more easily by moving to a more \"push\" model\n>> from the normal \"pull\" getblocktemplate implementation).\n>>\n>> 2) Replace Stratum with something more secure (sign messages when\n>> applicable, without adding too much overhead to the pool), simpler to\n>> implement (not JSON-wrapped-hex, no 32-byte-swapped-per-4-byte-byteorder\n>> insanity), and better-defined (a clearly written spec, encompassing the\n>> various things shoved backwards into stratum like suggested difficulty\n>> in the password field and device identification by setting user to\n>> \"user.device\") with VENDOR_MESSAGEs provided for extensibility instead\n>> of conflicting specifications from various different vendors.\n>>\n>> 3) Provide the ability for a pool to accept work which the users of the\n>> pool selected the transactions for, providing strong decentralization\n>> pressure by removing the network-level centralization attacks pools can\n>> do (or be compromised and used to perform) while still allowing them\n>> full control of payout management and variance reduction.\n>>\n>> While (1) and (2) stand on their own, making it all one set of protocols\n>> to provide (3) provides at least the opportunity for drastically better\n>> decentralization in Bitcoin mining in the future.\n>>\n>> The latest version of the full BIP draft can be found at\n>> https://github.com/TheBlueMatt/bips/blob/betterhash/bip-XXXX.mediawiki\n>> and implementations of the work-generation part at\n>> https://github.com/TheBlueMatt/bitcoin/commits/2018-02-miningserver and\n>> pool/proxy parts at https://github.com/TheBlueMatt/mining-proxy (though\n>> note that both implementations are currently on a slightly out-of-date\n>> version of the protocol, I hope to get them brought up to date in the\n>> coming day or two and make them much more full-featured. The whole stack\n>> has managed to mine numerous testnet blocks on several different types\n>> of hardware).\n>>\n>> Matt\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "BetterHash Mining Protocol Replacements",
            "categories": [
                "bitcoin-dev",
                "BIP Proposal"
            ],
            "authors": [
                "Chris Pacia",
                "Matt Corallo"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 10718
        }
    },
    {
        "title": "[bitcoin-dev] Disallow insecure use of SIGHASH_SINGLE",
        "thread_messages": [
            {
                "author": "Chris Stewart",
                "date": "2018-06-06T00:17:52",
                "message_text_only": "Do you have any thoughts on expanding this to SIGHASH_NONE? Perhaps someone\nelse on the dev list can enlighten me, but is there a current use case for\nSIGHASH_NONE that would suffer from it being non standard?\n\n-Chris\n\n\nOn Thu, May 31, 2018 at 1:53 PM, Johnson Lau via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> I\u2019ve made a PR to add a new policy to disallow using SIGHASH_SINGLE\n> without matched output:\n>\n> https://github.com/bitcoin/bitcoin/pull/13360\n>\n> Signature of this form is insecure, as it commits to no output while users\n> might think it commits to one. It is even worse in non-segwit scripts,\n> which is effectively SIGHASH_NOINPUT|SIGHASH_NONE, so any UTXO of the same\n> key could be stolen. (It\u2019s restricted to only one UTXO in segwit, but it\u2019s\n> still like a SIGHASH_NONE.)\n>\n> This is one of the earliest unintended consensus behavior. Since these\n> signatures are inherently unsafe, I think it does no harm to disable this\n> unintended \u201cfeature\u201d with a softfork. But since these signatures are\n> currently allowed, the first step is to make them non-standard.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180605/090a210f/attachment-0001.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2018-06-06T00:43:26",
                "message_text_only": "On Tue, Jun 05, 2018 at 07:17:52PM -0500, Chris Stewart via bitcoin-dev wrote:\n> Do you have any thoughts on expanding this to SIGHASH_NONE? Perhaps someone\n> else on the dev list can enlighten me, but is there a current use case for\n> SIGHASH_NONE that would suffer from it being non standard?\n\nSIGHASH_NONE is important as it's the only way that a multisig signers can\nrelinquish the need for them to sign without giving up the private key.\n\nFWIW the SIGHASH_SINGLE bug can be used in similar ways too.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 488 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180605/418825dc/attachment.sig>"
            },
            {
                "author": "Peter Todd",
                "date": "2018-06-06T00:49:01",
                "message_text_only": "On Fri, Jun 01, 2018 at 02:53:01AM +0800, Johnson Lau via bitcoin-dev wrote:\n> I\u2019ve made a PR to add a new policy to disallow using SIGHASH_SINGLE without matched output:\n> \n> https://github.com/bitcoin/bitcoin/pull/13360\n> \n> Signature of this form is insecure, as it commits to no output while users might think it commits to one. It is even worse in non-segwit scripts, which is effectively SIGHASH_NOINPUT|SIGHASH_NONE, so any UTXO of the same key could be stolen. (It\u2019s restricted to only one UTXO in segwit, but it\u2019s still like a SIGHASH_NONE.)\n> \n> This is one of the earliest unintended consensus behavior. Since these signatures are inherently unsafe, I think it does no harm to disable this unintended \u201cfeature\u201d with a softfork. But since these signatures are currently allowed, the first step is to make them non-standard.\n\nI don't see why we should bother to soft fork this out on the basis of\nsecurity, given that there are many other ways to insecurely use private keys\n(e.g. reused nonces). Maybe soft-fork it out on the basis of code complexity,\nbut this sounds like a lot of work.\n\nAlso, I have to wonder if it's just as likely the devs might think the\nnon-standardness means it is secure.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 488 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180605/c5beda70/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "Disallow insecure use of SIGHASH_SINGLE",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Chris Stewart",
                "Peter Todd"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 3841
        }
    },
    {
        "title": "[bitcoin-dev] UHS: Full-node security without maintaining a full UTXO set",
        "thread_messages": [
            {
                "author": "Sjors Provoost",
                "date": "2018-06-07T09:39:59",
                "message_text_only": "eMMC storage, which low end devices often use, come in 2x increments. Running a pruned full node on 8 GB is difficult if not impossible (the UTXO set peaked at 3.5 GB in January, but a full node stores additional stuff).\n\nHowever, 16 GB is only \u20ac10 more expensive and presumably standard by the time this would be rolled out.\n\nOn AWS every GB of SSD storage avoided saves $1 per year, not end of the world stuff, but not negligible either. Outbound traffic costs $0.10 / GB (ignoring free allowance), so when uploading 200 GB per year, the 5% would offset $1 of storage cost savings.\n\nThe above seems marginal, probably not worth it unless there\u2019s really no downside.\n\nWhat I find attractive about this proposal is the ability to squeeze more out of limited RAM (typically only 1 or 2 GB on these low end devices). I\u2019d have to test Cory\u2019s branch to see if that actually matters in practice.\n\nIt\u2019s also useful to distinguish benefits during initial sync from ongoing operation. The former I\u2019ve almost given up on for  low end devices (can take weeks), in favor of doing it on a faster computer and copying the result. The latter needs far less RAM, so perhaps this proposal doesn\u2019t help much there, but that would be useful to measure.\n\nDid you try the recent SHA256 optimizations on your branch?\n\nSjors\n\n> Op 17 mei 2018, om 18:56 heeft Gregory Maxwell via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> het volgende geschreven:\n> \n> On Wed, May 16, 2018 at 4:36 PM, Cory Fields via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> Tl;dr: Rather than storing all unspent outputs, store their hashes.\n> \n> My initial thoughts are it's not _completely_ obvious to me that a 5%\n> ongoing bandwidth increase is actually a win to get something like a\n> 40% reduction in the size of a pruned node (and less than a 1%\n> reduction in an archive node) primarily because I've not seen size of\n> a pruned node cited as a usage limiting factor basically anywhere. I\n> would assume it is a win but wouldn't be shocked to see a careful\n> analysis that concluded it wasn't.\n> \n> But perhaps more interestingly, I think the overhead is not really 5%,\n> but it's 5% measured in the context of the phenomenally inefficient tx\n> mechanisms ( https://bitcointalk.org/index.php?topic=1377345.0 ).\n> Napkin math on the size of a txn alone tells me it's more like a 25%\n> increase if you just consider size of tx vs size of\n> tx+scriptpubkeys,amounts.  If I'm not missing something there, I think\n> that would get in into a very clear not-win range.\n> \n> On the positive side is that it doesn't change the blockchain\n> datastructure, so it's something implementations could do without\n> marrying the network to it forever.\n>"
            },
            {
                "author": "Jim Posen",
                "date": "2018-06-10T23:07:07",
                "message_text_only": "I generally like the direction of this proposal in terms of allowing full\nnodes to run with a different storage/bandwidth tradeoff. Cory, were this\nimplemented, would you expect Core to support both operating modes (full\nUTXO set and UHS) depending on user configuration, or would UHS be\nmandatory?\n\nAlso, given that Bram Cohen's TXO bitfield proposal was an inspiration for\nthis, could you comment on why the UHS is preferable to that approach? An\nalternative that goes even further in the direction of more bandwidth, less\nstorage, would be for nodes to simply maintain a Merkle Mountain Range over\nall TXOs in order of creation and a spentness bitfield. Blocks could be\nrequested with the prev outputs and a Merkle proof linking them into the\nMMR root. Since the Merkle proof is deterministic, it could be computed by\narchive nodes and miners and saved alongside the block data for relay.\nAnother benefit of this is the TXO MMR root may be independently useful if\ncommitted into the coinbase transaction.\n\nOn Thu, Jun 7, 2018 at 7:02 AM Sjors Provoost via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> eMMC storage, which low end devices often use, come in 2x increments.\n> Running a pruned full node on 8 GB is difficult if not impossible (the UTXO\n> set peaked at 3.5 GB in January, but a full node stores additional stuff).\n>\n> However, 16 GB is only \u20ac10 more expensive and presumably standard by the\n> time this would be rolled out.\n>\n> On AWS every GB of SSD storage avoided saves $1 per year, not end of the\n> world stuff, but not negligible either. Outbound traffic costs $0.10 / GB\n> (ignoring free allowance), so when uploading 200 GB per year, the 5% would\n> offset $1 of storage cost savings.\n>\n> The above seems marginal, probably not worth it unless there\u2019s really no\n> downside.\n>\n> What I find attractive about this proposal is the ability to squeeze more\n> out of limited RAM (typically only 1 or 2 GB on these low end devices). I\u2019d\n> have to test Cory\u2019s branch to see if that actually matters in practice.\n>\n> It\u2019s also useful to distinguish benefits during initial sync from ongoing\n> operation. The former I\u2019ve almost given up on for  low end devices (can\n> take weeks), in favor of doing it on a faster computer and copying the\n> result. The latter needs far less RAM, so perhaps this proposal doesn\u2019t\n> help much there, but that would be useful to measure.\n>\n> Did you try the recent SHA256 optimizations on your branch?\n>\n> Sjors\n>\n> > Op 17 mei 2018, om 18:56 heeft Gregory Maxwell via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> het volgende geschreven:\n> >\n> > On Wed, May 16, 2018 at 4:36 PM, Cory Fields via bitcoin-dev\n> > <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >> Tl;dr: Rather than storing all unspent outputs, store their hashes.\n> >\n> > My initial thoughts are it's not _completely_ obvious to me that a 5%\n> > ongoing bandwidth increase is actually a win to get something like a\n> > 40% reduction in the size of a pruned node (and less than a 1%\n> > reduction in an archive node) primarily because I've not seen size of\n> > a pruned node cited as a usage limiting factor basically anywhere. I\n> > would assume it is a win but wouldn't be shocked to see a careful\n> > analysis that concluded it wasn't.\n> >\n> > But perhaps more interestingly, I think the overhead is not really 5%,\n> > but it's 5% measured in the context of the phenomenally inefficient tx\n> > mechanisms ( https://bitcointalk.org/index.php?topic=1377345.0 ).\n> > Napkin math on the size of a txn alone tells me it's more like a 25%\n> > increase if you just consider size of tx vs size of\n> > tx+scriptpubkeys,amounts.  If I'm not missing something there, I think\n> > that would get in into a very clear not-win range.\n> >\n> > On the positive side is that it doesn't change the blockchain\n> > datastructure, so it's something implementations could do without\n> > marrying the network to it forever.\n> >\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180610/609b21d1/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "UHS: Full-node security without maintaining a full UTXO set",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jim Posen",
                "Sjors Provoost"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 7050
        }
    },
    {
        "title": "[bitcoin-dev] Trusted merkle tree depth for safe tx inclusion proofs without a soft fork",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2018-06-07T17:13:11",
                "message_text_only": "It's well known that the Bitcoin merkle tree algorithm fails to distinguish\nbetween inner nodes and 64 byte transactions, as both txs and inner nodes are\nhashed the same way. This potentially poses a problem for tx inclusion proofs,\nas a miner could (with ~60 bits of brute forcing) create a transaction that\ncommitted to a transaction that was not in fact in the blockchain.\n\nSince odd-numbered inner/leaf nodes are concatenated with themselves and hashed\ntwice, the depth of all leaves (txs) in the tree is fixed.\n\nIt occured to me that if the depth of the merkle tree is known, this\nvulnerability can be trivially avoided by simply comparing the length of the\nmerkle path to that known depth. For pruned nodes, if the depth is saved prior\nto pruning the block contents itself, this would allow for completely safe\nverification of tx inclusion proofs, without a soft-fork; storing this data in\nthe block header database would be a simple thing to do.\n\nLite client verification without a trusted source of known-valid headers is\ndangerous anyway, so this protection makes for a fairly simple addition to any\nlite client protocol.\n\n\n# Brute Force Cost Assuming a Valid Tx\n\nConsider the following 64 byte transaction:\n\n    tx = CTransaction([CTxIn(COutPoint(b'\\xaa'*32,0xbbbbbbbb),nSequence=0xcccccccc)],[CTxOut(2**44-1,CScript([b'\\xdd\\xdd\\xdd']))],nLockTime=2**31-1)\n\nIf we serialize it, the last 32 bytes are:\n\n    aaaaaaaaaa bbbbbbbb 00 cccccccc 01 ffffffffff0f0000 04 03dddddd ffffff7f\n    \u21b3prevhash\u21b2 \u21b3 n    \u21b2    \u21b3 seq  \u21b2    \u21b3 nValue       \u21b2    \u21b3 pubk \u21b2 \u21b3lockt \u21b2\n                        \u21b3 sig_len   \u21b3num_txouts         \u21b3scriptPubKey_len\n\nOf those fields, we have free choice of the following bits:\n\nprevhash:  40 - prev tx fully brute-forcable, as tx can be created to match\nprev_n:    16 - can create a tx with up to about 2^16 outputs\nseq:       32 - fully brute-forcable in nVersion=1 txs\nnValue:    44 - assuming attacker has access to 175,921 BTC, worth ~1.3 billion right now\npubk:      32 - fully brute-forcable if willing to lose BTC spent; all scriptPubKeys are valid\nnLockTime: 31 - valid time-based nLockTime\nTotal: 195 bits free choice \u2192 61 bits need to be brute-forced\n\nAdditionally, this can be improved slightly by a few more bits by checking for\nvalid scriptSig/scriptPubKey combinations other than a zero-length scriptSig;\nthe attacker can also avoid creating an unspendable output this way, and\nrecover their funds by spending it in the same block with a third transaction.\nAn obvious implementation making use of this would be to check that the high\nbits of prevout.n are zero first, prior to doing more costly checks.\n\nFinally, if inflation is not controlled - and thus nValue can be set freely -\nnote how the brute force is trivial. There may very well exist crypto-currencies\nfor which this brute-force is much easier than it is on Bitcoin!\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 488 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180607/14f8671f/attachment-0001.sig>"
            },
            {
                "author": "Bram Cohen",
                "date": "2018-06-07T21:15:35",
                "message_text_only": "Are you proposing a soft fork to include the number of transactions in a\nblock in the block headers to compensate for the broken Merkle format? That\nsounds like a good idea.\n\nOn Thu, Jun 7, 2018 at 10:13 AM, Peter Todd via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> It's well known that the Bitcoin merkle tree algorithm fails to distinguish\n> between inner nodes and 64 byte transactions, as both txs and inner nodes\n> are\n> hashed the same way. This potentially poses a problem for tx inclusion\n> proofs,\n> as a miner could (with ~60 bits of brute forcing) create a transaction that\n> committed to a transaction that was not in fact in the blockchain.\n>\n> Since odd-numbered inner/leaf nodes are concatenated with themselves and\n> hashed\n> twice, the depth of all leaves (txs) in the tree is fixed.\n>\n> It occured to me that if the depth of the merkle tree is known, this\n> vulnerability can be trivially avoided by simply comparing the length of\n> the\n> merkle path to that known depth. For pruned nodes, if the depth is saved\n> prior\n> to pruning the block contents itself, this would allow for completely safe\n> verification of tx inclusion proofs, without a soft-fork; storing this\n> data in\n> the block header database would be a simple thing to do.\n>\n> Lite client verification without a trusted source of known-valid headers is\n> dangerous anyway, so this protection makes for a fairly simple addition to\n> any\n> lite client protocol.\n>\n>\n> # Brute Force Cost Assuming a Valid Tx\n>\n> Consider the following 64 byte transaction:\n>\n>     tx = CTransaction([CTxIn(COutPoint(b'\\xaa'*32,0xbbbbbbbb),\n> nSequence=0xcccccccc)],[CTxOut(2**44-1,CScript([b'\\\n> xdd\\xdd\\xdd']))],nLockTime=2**31-1)\n>\n> If we serialize it, the last 32 bytes are:\n>\n>     aaaaaaaaaa bbbbbbbb 00 cccccccc 01 ffffffffff0f0000 04 03dddddd\n> ffffff7f\n>     \u21b3prevhash\u21b2 \u21b3 n    \u21b2    \u21b3 seq  \u21b2    \u21b3 nValue       \u21b2    \u21b3 pubk \u21b2 \u21b3lockt\n> \u21b2\n>                         \u21b3 sig_len   \u21b3num_txouts         \u21b3scriptPubKey_len\n>\n> Of those fields, we have free choice of the following bits:\n>\n> prevhash:  40 - prev tx fully brute-forcable, as tx can be created to match\n> prev_n:    16 - can create a tx with up to about 2^16 outputs\n> seq:       32 - fully brute-forcable in nVersion=1 txs\n> nValue:    44 - assuming attacker has access to 175,921 BTC, worth ~1.3\n> billion right now\n> pubk:      32 - fully brute-forcable if willing to lose BTC spent; all\n> scriptPubKeys are valid\n> nLockTime: 31 - valid time-based nLockTime\n> Total: 195 bits free choice \u2192 61 bits need to be brute-forced\n>\n> Additionally, this can be improved slightly by a few more bits by checking\n> for\n> valid scriptSig/scriptPubKey combinations other than a zero-length\n> scriptSig;\n> the attacker can also avoid creating an unspendable output this way, and\n> recover their funds by spending it in the same block with a third\n> transaction.\n> An obvious implementation making use of this would be to check that the\n> high\n> bits of prevout.n are zero first, prior to doing more costly checks.\n>\n> Finally, if inflation is not controlled - and thus nValue can be set\n> freely -\n> note how the brute force is trivial. There may very well exist\n> crypto-currencies\n> for which this brute-force is much easier than it is on Bitcoin!\n>\n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180607/6531c188/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2018-06-07T22:20:28",
                "message_text_only": "On Thu, Jun 07, 2018 at 02:15:35PM -0700, Bram Cohen wrote:\n> Are you proposing a soft fork to include the number of transactions in a\n> block in the block headers to compensate for the broken Merkle format? That\n> sounds like a good idea.\n> \n> On Thu, Jun 7, 2018 at 10:13 AM, Peter Todd via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> > It's well known that the Bitcoin merkle tree algorithm fails to distinguish\n> > between inner nodes and 64 byte transactions, as both txs and inner nodes\n> > are\n> > hashed the same way. This potentially poses a problem for tx inclusion\n> > proofs,\n> > as a miner could (with ~60 bits of brute forcing) create a transaction that\n> > committed to a transaction that was not in fact in the blockchain.\n> >\n> > Since odd-numbered inner/leaf nodes are concatenated with themselves and\n> > hashed\n> > twice, the depth of all leaves (txs) in the tree is fixed.\n> >\n> > It occured to me that if the depth of the merkle tree is known, this\n> > vulnerability can be trivially avoided by simply comparing the length of\n> > the\n> > merkle path to that known depth. For pruned nodes, if the depth is saved\n> > prior\n> > to pruning the block contents itself, this would allow for completely safe\n> > verification of tx inclusion proofs, without a soft-fork; storing this\n                                         ^^^^^^^^^^^^^^^^^^^\n\nRe-read my post: I specifically said you do not need a soft-fork to implement\nthis. In fact, I think you can argue that this is an accidental feature, not a\nbug, as it further encourages the use of safe full verifiaction rather than\nunsafe lite clients.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 488 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180607/4fc61b12/attachment.sig>"
            },
            {
                "author": "Bram Cohen",
                "date": "2018-06-09T03:29:30",
                "message_text_only": "So are you saying that if fully validating nodes wish to prune they can\nmaintain the ability to validate old transactions by cacheing the number of\ntransactions in each previous block?\n\nOn Thu, Jun 7, 2018 at 3:20 PM, Peter Todd <pete at petertodd.org> wrote:\n\n> On Thu, Jun 07, 2018 at 02:15:35PM -0700, Bram Cohen wrote:\n> > Are you proposing a soft fork to include the number of transactions in a\n> > block in the block headers to compensate for the broken Merkle format?\n> That\n> > sounds like a good idea.\n> >\n> > On Thu, Jun 7, 2018 at 10:13 AM, Peter Todd via bitcoin-dev <\n> > bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >\n> > > It's well known that the Bitcoin merkle tree algorithm fails to\n> distinguish\n> > > between inner nodes and 64 byte transactions, as both txs and inner\n> nodes\n> > > are\n> > > hashed the same way. This potentially poses a problem for tx inclusion\n> > > proofs,\n> > > as a miner could (with ~60 bits of brute forcing) create a transaction\n> that\n> > > committed to a transaction that was not in fact in the blockchain.\n> > >\n> > > Since odd-numbered inner/leaf nodes are concatenated with themselves\n> and\n> > > hashed\n> > > twice, the depth of all leaves (txs) in the tree is fixed.\n> > >\n> > > It occured to me that if the depth of the merkle tree is known, this\n> > > vulnerability can be trivially avoided by simply comparing the length\n> of\n> > > the\n> > > merkle path to that known depth. For pruned nodes, if the depth is\n> saved\n> > > prior\n> > > to pruning the block contents itself, this would allow for completely\n> safe\n> > > verification of tx inclusion proofs, without a soft-fork; storing this\n>                                          ^^^^^^^^^^^^^^^^^^^\n>\n> Re-read my post: I specifically said you do not need a soft-fork to\n> implement\n> this. In fact, I think you can argue that this is an accidental feature,\n> not a\n> bug, as it further encourages the use of safe full verifiaction rather than\n> unsafe lite clients.\n>\n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180608/8320bbb2/attachment.html>"
            },
            {
                "author": "Sergio Demian Lerner",
                "date": "2018-06-09T11:03:53",
                "message_text_only": "Hi Peter,\nWe reported this as CVE-2017-12842, although it may have been known by\ndevelopers before us.\nThere are hundreds of SPV wallets out there, without even considering other\nmore sensitive systems relying on SPV proofs.\nAs I said we, at RSK, discovered this problem in 2017. For RSK it's very\nimportant this is fixed because our SPV bridge uses SPV proofs.\nI urge all people participating in this mailing list and the rest of the\nBitcoin community to work on this issue for the security and clean-design\nof Bitcoin.\n\nMy suggestion is to use in the version bits 4 bits indicating the tree\ndepth (-1), as a soft-fork, so\n00=1 depth,\n0F = 16 depth (maximum 64K transactions). Very clean.\n\nThe other option is to ban transaction with size lower or equal to 64.\n\nBest regards,\n Sergio.\n\nOn Sat, Jun 9, 2018 at 5:31 AM Bram Cohen via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> So are you saying that if fully validating nodes wish to prune they can\n> maintain the ability to validate old transactions by cacheing the number of\n> transactions in each previous block?\n>\n> On Thu, Jun 7, 2018 at 3:20 PM, Peter Todd <pete at petertodd.org> wrote:\n>\n>> On Thu, Jun 07, 2018 at 02:15:35PM -0700, Bram Cohen wrote:\n>> > Are you proposing a soft fork to include the number of transactions in a\n>> > block in the block headers to compensate for the broken Merkle format?\n>> That\n>> > sounds like a good idea.\n>> >\n>> > On Thu, Jun 7, 2018 at 10:13 AM, Peter Todd via bitcoin-dev <\n>> > bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> >\n>> > > It's well known that the Bitcoin merkle tree algorithm fails to\n>> distinguish\n>> > > between inner nodes and 64 byte transactions, as both txs and inner\n>> nodes\n>> > > are\n>> > > hashed the same way. This potentially poses a problem for tx inclusion\n>> > > proofs,\n>> > > as a miner could (with ~60 bits of brute forcing) create a\n>> transaction that\n>> > > committed to a transaction that was not in fact in the blockchain.\n>> > >\n>> > > Since odd-numbered inner/leaf nodes are concatenated with themselves\n>> and\n>> > > hashed\n>> > > twice, the depth of all leaves (txs) in the tree is fixed.\n>> > >\n>> > > It occured to me that if the depth of the merkle tree is known, this\n>> > > vulnerability can be trivially avoided by simply comparing the length\n>> of\n>> > > the\n>> > > merkle path to that known depth. For pruned nodes, if the depth is\n>> saved\n>> > > prior\n>> > > to pruning the block contents itself, this would allow for completely\n>> safe\n>> > > verification of tx inclusion proofs, without a soft-fork; storing this\n>>                                          ^^^^^^^^^^^^^^^^^^^\n>>\n>> Re-read my post: I specifically said you do not need a soft-fork to\n>> implement\n>> this. In fact, I think you can argue that this is an accidental feature,\n>> not a\n>> bug, as it further encourages the use of safe full verifiaction rather\n>> than\n>> unsafe lite clients.\n>>\n>> --\n>> https://petertodd.org 'peter'[:-1]@petertodd.org\n>>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180609/b0d55588/attachment-0001.html>"
            },
            {
                "author": "Sergio Demian Lerner",
                "date": "2018-06-09T12:21:17",
                "message_text_only": "Also it must be noted that an attacker having only 1.3M USD that can\nbrute-force 72 bits (4 days of hashing on capable ASICs) can perform the\nsame attack, so the attack is entirely feasible and no person should accept\nmore than 1M USD using a SPV wallet.\n\nAlso the attack can be repeated: once you create the \"extension point\"\nblock, you can attack more and more parties without any additional\ncomputation.\n\n\nOn Sat, Jun 9, 2018 at 1:03 PM Sergio Demian Lerner <\nsergio.d.lerner at gmail.com> wrote:\n\n> Hi Peter,\n> We reported this as CVE-2017-12842, although it may have been known by\n> developers before us.\n> There are hundreds of SPV wallets out there, without even considering\n> other more sensitive systems relying on SPV proofs.\n> As I said we, at RSK, discovered this problem in 2017. For RSK it's very\n> important this is fixed because our SPV bridge uses SPV proofs.\n> I urge all people participating in this mailing list and the rest of the\n> Bitcoin community to work on this issue for the security and clean-design\n> of Bitcoin.\n>\n> My suggestion is to use in the version bits 4 bits indicating the tree\n> depth (-1), as a soft-fork, so\n> 00=1 depth,\n> 0F = 16 depth (maximum 64K transactions). Very clean.\n>\n> The other option is to ban transaction with size lower or equal to 64.\n>\n> Best regards,\n>  Sergio.\n>\n> On Sat, Jun 9, 2018 at 5:31 AM Bram Cohen via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> So are you saying that if fully validating nodes wish to prune they can\n>> maintain the ability to validate old transactions by cacheing the number of\n>> transactions in each previous block?\n>>\n>> On Thu, Jun 7, 2018 at 3:20 PM, Peter Todd <pete at petertodd.org> wrote:\n>>\n>>> On Thu, Jun 07, 2018 at 02:15:35PM -0700, Bram Cohen wrote:\n>>> > Are you proposing a soft fork to include the number of transactions in\n>>> a\n>>> > block in the block headers to compensate for the broken Merkle format?\n>>> That\n>>> > sounds like a good idea.\n>>> >\n>>> > On Thu, Jun 7, 2018 at 10:13 AM, Peter Todd via bitcoin-dev <\n>>> > bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>> >\n>>> > > It's well known that the Bitcoin merkle tree algorithm fails to\n>>> distinguish\n>>> > > between inner nodes and 64 byte transactions, as both txs and inner\n>>> nodes\n>>> > > are\n>>> > > hashed the same way. This potentially poses a problem for tx\n>>> inclusion\n>>> > > proofs,\n>>> > > as a miner could (with ~60 bits of brute forcing) create a\n>>> transaction that\n>>> > > committed to a transaction that was not in fact in the blockchain.\n>>> > >\n>>> > > Since odd-numbered inner/leaf nodes are concatenated with themselves\n>>> and\n>>> > > hashed\n>>> > > twice, the depth of all leaves (txs) in the tree is fixed.\n>>> > >\n>>> > > It occured to me that if the depth of the merkle tree is known, this\n>>> > > vulnerability can be trivially avoided by simply comparing the\n>>> length of\n>>> > > the\n>>> > > merkle path to that known depth. For pruned nodes, if the depth is\n>>> saved\n>>> > > prior\n>>> > > to pruning the block contents itself, this would allow for\n>>> completely safe\n>>> > > verification of tx inclusion proofs, without a soft-fork; storing\n>>> this\n>>>                                          ^^^^^^^^^^^^^^^^^^^\n>>>\n>>> Re-read my post: I specifically said you do not need a soft-fork to\n>>> implement\n>>> this. In fact, I think you can argue that this is an accidental feature,\n>>> not a\n>>> bug, as it further encourages the use of safe full verifiaction rather\n>>> than\n>>> unsafe lite clients.\n>>>\n>>> --\n>>> https://petertodd.org 'peter'[:-1]@petertodd.org\n>>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180609/ee578469/attachment-0001.html>"
            },
            {
                "author": "Sergio Demian Lerner",
                "date": "2018-06-09T12:24:49",
                "message_text_only": "Here is our internal report, if you want more details on the problem.\n (our reported attack complexity may slightly differ from what Peter has\nprovided, but the attack complexity depends on the funds the attacker is\nwilling to lock).\n\nregards,\n Sergio.\n\nOn Sat, Jun 9, 2018 at 2:21 PM Sergio Demian Lerner <\nsergio.d.lerner at gmail.com> wrote:\n\n> Also it must be noted that an attacker having only 1.3M USD that can\n> brute-force 72 bits (4 days of hashing on capable ASICs) can perform the\n> same attack, so the attack is entirely feasible and no person should accept\n> more than 1M USD using a SPV wallet.\n>\n> Also the attack can be repeated: once you create the \"extension point\"\n> block, you can attack more and more parties without any additional\n> computation.\n>\n>\n> On Sat, Jun 9, 2018 at 1:03 PM Sergio Demian Lerner <\n> sergio.d.lerner at gmail.com> wrote:\n>\n>> Hi Peter,\n>> We reported this as CVE-2017-12842, although it may have been known by\n>> developers before us.\n>> There are hundreds of SPV wallets out there, without even considering\n>> other more sensitive systems relying on SPV proofs.\n>> As I said we, at RSK, discovered this problem in 2017. For RSK it's very\n>> important this is fixed because our SPV bridge uses SPV proofs.\n>> I urge all people participating in this mailing list and the rest of the\n>> Bitcoin community to work on this issue for the security and clean-design\n>> of Bitcoin.\n>>\n>> My suggestion is to use in the version bits 4 bits indicating the tree\n>> depth (-1), as a soft-fork, so\n>> 00=1 depth,\n>> 0F = 16 depth (maximum 64K transactions). Very clean.\n>>\n>> The other option is to ban transaction with size lower or equal to 64.\n>>\n>> Best regards,\n>>  Sergio.\n>>\n>> On Sat, Jun 9, 2018 at 5:31 AM Bram Cohen via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> So are you saying that if fully validating nodes wish to prune they can\n>>> maintain the ability to validate old transactions by cacheing the number of\n>>> transactions in each previous block?\n>>>\n>>> On Thu, Jun 7, 2018 at 3:20 PM, Peter Todd <pete at petertodd.org> wrote:\n>>>\n>>>> On Thu, Jun 07, 2018 at 02:15:35PM -0700, Bram Cohen wrote:\n>>>> > Are you proposing a soft fork to include the number of transactions\n>>>> in a\n>>>> > block in the block headers to compensate for the broken Merkle\n>>>> format? That\n>>>> > sounds like a good idea.\n>>>> >\n>>>> > On Thu, Jun 7, 2018 at 10:13 AM, Peter Todd via bitcoin-dev <\n>>>> > bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>> >\n>>>> > > It's well known that the Bitcoin merkle tree algorithm fails to\n>>>> distinguish\n>>>> > > between inner nodes and 64 byte transactions, as both txs and inner\n>>>> nodes\n>>>> > > are\n>>>> > > hashed the same way. This potentially poses a problem for tx\n>>>> inclusion\n>>>> > > proofs,\n>>>> > > as a miner could (with ~60 bits of brute forcing) create a\n>>>> transaction that\n>>>> > > committed to a transaction that was not in fact in the blockchain.\n>>>> > >\n>>>> > > Since odd-numbered inner/leaf nodes are concatenated with\n>>>> themselves and\n>>>> > > hashed\n>>>> > > twice, the depth of all leaves (txs) in the tree is fixed.\n>>>> > >\n>>>> > > It occured to me that if the depth of the merkle tree is known, this\n>>>> > > vulnerability can be trivially avoided by simply comparing the\n>>>> length of\n>>>> > > the\n>>>> > > merkle path to that known depth. For pruned nodes, if the depth is\n>>>> saved\n>>>> > > prior\n>>>> > > to pruning the block contents itself, this would allow for\n>>>> completely safe\n>>>> > > verification of tx inclusion proofs, without a soft-fork; storing\n>>>> this\n>>>>                                          ^^^^^^^^^^^^^^^^^^^\n>>>>\n>>>> Re-read my post: I specifically said you do not need a soft-fork to\n>>>> implement\n>>>> this. In fact, I think you can argue that this is an accidental\n>>>> feature, not a\n>>>> bug, as it further encourages the use of safe full verifiaction rather\n>>>> than\n>>>> unsafe lite clients.\n>>>>\n>>>> --\n>>>> https://petertodd.org 'peter'[:-1]@petertodd.org\n>>>>\n>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180609/9f4f5b1f/attachment-0001.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: Vulnerability in Bitcoin Merkle Tree Design.pdf\nType: application/pdf\nSize: 402454 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180609/9f4f5b1f/attachment-0001.pdf>"
            },
            {
                "author": "Peter Todd",
                "date": "2018-06-09T12:45:16",
                "message_text_only": "On Sat, Jun 09, 2018 at 02:21:17PM +0200, Sergio Demian Lerner wrote:\n> Also it must be noted that an attacker having only 1.3M USD that can\n> brute-force 72 bits (4 days of hashing on capable ASICs) can perform the\n> same attack, so the attack is entirely feasible and no person should accept\n> more than 1M USD using a SPV wallet.\n\nThat doesn't make any sense. Against a SPV wallet you don't need that attack;\nwith that kind of budget you can fool it by just creating a fake block at far\nless cost, along with a sybil attack. Sybils aren't difficult to pull off when\nyou have the budget to be greating fake blocks.\n\n> Also the attack can be repeated: once you create the \"extension point\"\n> block, you can attack more and more parties without any additional\n> computation.\n\nThat's technically incorrect: txouts can only be spent once, so you'll need to\ndo 2^40 work each time you want to repeat the attack to grind the matching part\nof the prevout again.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 488 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180609/fd6ded5c/attachment.sig>"
            },
            {
                "author": "Sergio Demian Lerner",
                "date": "2018-06-09T12:51:55",
                "message_text_only": "Yo can fool a SPV wallet even if it requires a thousands confirmations\nusing this attack, and you don't need a Sybil attack, so yes, it impacts\nSPV wallets also. The protections a SPV node should have to prevent this\nattack are  different, so it must be considered separately.\n\nIt should be said that a SPV node can avoid accepting payments if any\nMerkle node is at the same time a valid transaction, and that basically\nalmost eliminates the problem.\n\nSPV Wallet would reject valid payments with a astonishingly low probability.\n\n\n\nOn Sat, Jun 9, 2018 at 2:45 PM Peter Todd <pete at petertodd.org> wrote:\n\n> On Sat, Jun 09, 2018 at 02:21:17PM +0200, Sergio Demian Lerner wrote:\n> > Also it must be noted that an attacker having only 1.3M USD that can\n> > brute-force 72 bits (4 days of hashing on capable ASICs) can perform the\n> > same attack, so the attack is entirely feasible and no person should\n> accept\n> > more than 1M USD using a SPV wallet.\n>\n> That doesn't make any sense. Against a SPV wallet you don't need that\n> attack;\n> with that kind of budget you can fool it by just creating a fake block at\n> far\n> less cost, along with a sybil attack. Sybils aren't difficult to pull off\n> when\n> you have the budget to be greating fake blocks.\n>\n> > Also the attack can be repeated: once you create the \"extension point\"\n> > block, you can attack more and more parties without any additional\n> > computation.\n>\n> That's technically incorrect: txouts can only be spent once, so you'll\n> need to\n> do 2^40 work each time you want to repeat the attack to grind the matching\n> part\n> of the prevout again.\n>\n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180609/567e1527/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2018-06-09T12:50:58",
                "message_text_only": "On Sat, Jun 09, 2018 at 01:03:53PM +0200, Sergio Demian Lerner wrote:\n> Hi Peter,\n> We reported this as CVE-2017-12842, although it may have been known by\n> developers before us.\n\nIt's been known so long ago that I incorrectly thought the attack was ok to\ndiscuss in public; I had apparently incorrectly remembered a conversation I had\nwith Greg Maxwell over a year ago where I thought he said it was fine to\ndiscuss because it was well known.\n\nMy apologies to anyone who thinks my post was jumping the gun by discussing\nthis in public; cats out of the bag now anyway.\n\n> There are hundreds of SPV wallets out there, without even considering other\n> more sensitive systems relying on SPV proofs.\n> As I said we, at RSK, discovered this problem in 2017. For RSK it's very\n> important this is fixed because our SPV bridge uses SPV proofs.\n> I urge all people participating in this mailing list and the rest of the\n> Bitcoin community to work on this issue for the security and clean-design\n> of Bitcoin.\n\nMy post is arguing that we *don't* need to fix the attack, because we can make\npruned nodes invulerable to it while retaining the ability to verify merkle\npath tx inclusion proofs.\n\nAs for SPV, there is no attack to fix: they can be attacked at much lower cost\nby simply generating fake blocks.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 488 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180609/14afa2c5/attachment.sig>"
            },
            {
                "author": "Peter Todd",
                "date": "2018-06-09T13:02:55",
                "message_text_only": "On Sat, Jun 09, 2018 at 02:51:55PM +0200, Sergio Demian Lerner wrote:\n> Yo can fool a SPV wallet even if it requires a thousands confirmations\n> using this attack, and you don't need a Sybil attack, so yes, it impacts\n> SPV wallets also. The protections a SPV node should have to prevent this\n> attack are  different, so it must be considered separately.\n\nThere's hardly any cases where \"thousands of confirmations\" change anything.\n\nAnyway, SPV is a discredited concept and we shouldn't be concerning ourselves\nwith it.\n\n> It should be said that a SPV node can avoid accepting payments if any\n> Merkle node is at the same time a valid transaction, and that basically\n> almost eliminates the problem.\n\nIndeed it does: between the number of txouts, scriptSig length, scriptPubKey\nlength, and the upper bits of nValue we have ~32 known bits that we can use to\ndistinguish between inner nodes and transactions. That's a false positive rate\nof under one in a billion, so no issues there.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 488 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180609/3ac37781/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "Trusted merkle tree depth for safe tx inclusion proofs without a soft fork",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Bram Cohen",
                "Sergio Demian Lerner",
                "Peter Todd"
            ],
            "messages_count": 11,
            "total_messages_chars_count": 29296
        }
    },
    {
        "title": "[bitcoin-dev] Why not archive the backend of Bitcoin blockchain?",
        "thread_messages": [
            {
                "author": "Kulpreet Singh",
                "date": "2018-06-12T08:40:18",
                "message_text_only": "Apologies for a noob question.\n\nBut if I understand correctly, lightning nodes need to check if a\ncounterparty is broadcasting an old channel state and in response broadcast\na penalty/justice transaction. Does that mean lightning nodes only need to\nwatch for transactions that come after the funding transaction? Is that the\nonly reason lightning needs to run bitcoind with txindex?\n\nIf that is the case, and a lightning node only needs to query transactions\nbroadcast after the funding transaction, then a pruned bitcoind instance\nwith txindex might be a bit handy.\n\nAlso from [1] it seems that indexing pruned nodes is not supported because\nit doesn't make sense, not that it was infeasible. Now with the lightning\nrequirements, does an indexed pruned node start to make sense?\n\nOnce again, please forgive my naive understanding of some of the issues\ninvolved and thanks for your patience.\n\nRegards\nKulpreet\n\n[1]\nhttps://bitcoin.stackexchange.com/questions/52889/bitcoin-core-txindex-vs-default-mode-vs-pruned-mode-in-depth#52894\n\n\nOn Thu, 10 May 2018, 14:47 Patrick Shirkey via bitcoin-dev, <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n> > On 3/17/18, someone posted on the Lightning-dev list, \"Can I try\n> > Lightning without running a fully-fledged bitcoin block chain? (Yubin\n> > Ruan).\"  The inquirer was asking because he didn't have much space to\n> > store the entire blockchain on his laptop.\n> >\n> > I replied:\n> >\n> > \"Developers,\n> >\n> > On THIS note and slightly off-topic but relevant, why can't chunks of\n> > blockchain peel off the backend periodically and be archived, say on\n> > minimum of 150 computers across 7 continents?\n> >\n> > It seems crazy to continue adding on to an increasingly long chain to\n> > infinity if the old chapters (i.e. more than, say, 2 years old) could be\n> > stored in an evenly distributed manner across the planet. The same 150\n> > computers would not need to store every chapter either, just the index\n> > would need to be widely distributed in order to reconnect with a chapter\n> > if needed. Then maybe it is no longer a limitation in the future for\n> > people like Yubin. \"\n> >\n> > It was suggested by a couple of lightning developers that I post this\n> > idea on the bitcoin-dev list.  So, here I post :).\n> >\n>\n> You can already use the \"prune\" flag to get a snapshot of the blockchain\n> but it is incompatible with \"txindex\" and \"rescan\" so maybe that is and\n> issue for lightning nodes?\n>\n>\n>\n>\n> --\n> Patrick Shirkey\n> Boost Hardware\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180612/242c770f/attachment.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2018-06-13T13:33:33",
                "message_text_only": "Kulpreet Singh via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>\nwrites:\n> But if I understand correctly, lightning nodes need to check if a\n> counterparty is broadcasting an old channel state and in response\n> broadcast a penalty/justice transaction. Does that mean lightning\n> nodes only need to watch for transactions that come after the funding\n> transaction? Is that the only reason lightning needs to run bitcoind\n> with txindex?\n\nYes, Lightning nodes need to monitor the network for transactions that\nthey need to react to. This is basically tailing the blockchain and\nlooking for anything suspicious. The `bitcoind` sitting next to the\nlightning node however does not need to keep an index of the\ntransactions, at least for c-lightning, because we just ask for the full\nblock that then gets scanned for transactions of interest and then we\ndiscard the rest of the block. We never ask for a specific transaction\nfrom `bitcoind` and therefore we don't need to run with `-txindex`.\n\n> If that is the case, and a lightning node only needs to query\n> transactions broadcast after the funding transaction, then a pruned\n> bitcoind instance with txindex might be a bit handy.\n\nPruned nodes should work, as long as the current blockchain head that\nthe lightning node has seen does not fall into the pruned range, since\nin that case it won't be able to fetch and process the blocks anymore.\n\n> Also from [1] it seems that indexing pruned nodes is not supported\n> because it doesn't make sense, not that it was infeasible. Now with\n> the lightning requirements, does an indexed pruned node start to make\n> sense?\n\nI don't think we should ever require `-txindex` to run a lightning node\n(I know some implementations did in the past), since that'd be a very\nonerous requirement to run a lightning node. Tailing the blockchain is\nmore than sufficient to get the necessary data, and hopefully we can get\nour reliance on `bitcoind` down to a minimum in the future.\n\n> Once again, please forgive my naive understanding of some of the issues\n> involved and thanks for your patience.\n\nAbsolutely no problem, it is a common misconception that `-txindex` is\nrequired to run a lightning node in all cases :-)\n\nCheers,\nChristian"
            },
            {
                "author": "Brian Lockhart",
                "date": "2018-06-13T15:32:11",
                "message_text_only": "Somewhat related question -\n\nIn the interest of avoiding running multiple bitcoind full nodes - is there\na method to allow a Lightning node to point to / access a separate\nalready-existing node, vs. requiring it to have its own dedicated local\ninstance of bitcoind running?\n\nI.e. if I already have a full bitcoin node running, could I use RPC calls\nor something to tell my Lightning node to use that node, instead of\nspinning up *another* full node? I\u2019m currently minimizing the network\nthrashing by whitelisting my LN bitcoind node to only point to my existing\nfull node for updates, but if I could just point my whole LN node at it,\nthat\u2019s save on disk storage etc. etc. etc.\n\n\nApologies if this is already in there (or has been added) and I missed it\nbecause I haven\u2019t kept up with release notes\u2026\n\n\n\n\nOn June 13, 2018 at 6:35:49 AM, Christian Decker via bitcoin-dev (\nbitcoin-dev at lists.linuxfoundation.org) wrote:\n\nKulpreet Singh via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>\nwrites:\n> But if I understand correctly, lightning nodes need to check if a\n> counterparty is broadcasting an old channel state and in response\n> broadcast a penalty/justice transaction. Does that mean lightning\n> nodes only need to watch for transactions that come after the funding\n> transaction? Is that the only reason lightning needs to run bitcoind\n> with txindex?\n\nYes, Lightning nodes need to monitor the network for transactions that\nthey need to react to. This is basically tailing the blockchain and\nlooking for anything suspicious. The `bitcoind` sitting next to the\nlightning node however does not need to keep an index of the\ntransactions, at least for c-lightning, because we just ask for the full\nblock that then gets scanned for transactions of interest and then we\ndiscard the rest of the block. We never ask for a specific transaction\nfrom `bitcoind` and therefore we don't need to run with `-txindex`.\n\n> If that is the case, and a lightning node only needs to query\n> transactions broadcast after the funding transaction, then a pruned\n> bitcoind instance with txindex might be a bit handy.\n\nPruned nodes should work, as long as the current blockchain head that\nthe lightning node has seen does not fall into the pruned range, since\nin that case it won't be able to fetch and process the blocks anymore.\n\n> Also from [1] it seems that indexing pruned nodes is not supported\n> because it doesn't make sense, not that it was infeasible. Now with\n> the lightning requirements, does an indexed pruned node start to make\n> sense?\n\nI don't think we should ever require `-txindex` to run a lightning node\n(I know some implementations did in the past), since that'd be a very\nonerous requirement to run a lightning node. Tailing the blockchain is\nmore than sufficient to get the necessary data, and hopefully we can get\nour reliance on `bitcoind` down to a minimum in the future.\n\n> Once again, please forgive my naive understanding of some of the issues\n> involved and thanks for your patience.\n\nAbsolutely no problem, it is a common misconception that `-txindex` is\nrequired to run a lightning node in all cases :-)\n\nCheers,\nChristian\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180613/3b904f54/attachment-0001.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2018-06-13T16:17:20",
                "message_text_only": "Brian Lockhart <brianlockhart at gmail.com> writes:\n> In the interest of avoiding running multiple bitcoind full nodes - is there\n> a method to allow a Lightning node to point to / access a separate\n> already-existing node, vs. requiring it to have its own dedicated local\n> instance of bitcoind running?\n>\n> I.e. if I already have a full bitcoin node running, could I use RPC calls\n> or something to tell my Lightning node to use that node, instead of\n> spinning up *another* full node? I\u2019m currently minimizing the network\n> thrashing by whitelisting my LN bitcoind node to only point to my existing\n> full node for updates, but if I could just point my whole LN node at it,\n> that\u2019s save on disk storage etc. etc. etc.\n\nCertainly, that's supported by all 3 implementations:\n\n - With c-lightning you can either configure `bitcoin-cli` to connect to\n   a remote node with the `rpcconnect`, `rpcuser`, and `rpcpassword`\n   options in the `bitcoin.conf` file (at which point all calls to\n   `bitcoin-cli` will use that node) or you can use the following\n   command line options when starting `lightningd`: `--bitcoin-rpcuser`,\n   `--bitcoin-rpcpassword` and `--bitcoin-rpcconnect`\n - lnd allows you to specify the node to connect to using the command\n   line options `--bitcoind.rpchost`, `--bitcoind.rpcuser`, and\n   `--bitcoind.rpcpass`.\n - Eclair requires you to edit the configuration file [1] before\n   compiling afaik\n\nCheers,\nChristian\n\n\n[1] https://github.com/ACINQ/eclair/blob/master/eclair-core/src/main/resources/reference.conf"
            },
            {
                "author": "Brian Lockhart",
                "date": "2018-06-14T23:24:33",
                "message_text_only": "Resolved -> RTFM\n\nWorked like a champ, first try. Thanks! Wish I had thought to look into that sooner! My c-lightning node\u2019s resource footprint is even smaller now. Unfairly small.\n\nAnd now that I\u2019ve unexpectedly regained ~180 GB worth of free SSD space from not having that extra full node, I\u2019m feeling wealthy. Accordingly, I\u2019m off to squander my newfound riches over at satoshis\u2019s place. )\n\n\n\n> On Jun 13, 2018, at 9:17 AM, Christian Decker <decker.christian at gmail.com> wrote:\n> \n> Brian Lockhart <brianlockhart at gmail.com> writes:\n>> In the interest of avoiding running multiple bitcoind full nodes - is there\n>> a method to allow a Lightning node to point to / access a separate\n>> already-existing node, vs. requiring it to have its own dedicated local\n>> instance of bitcoind running?\n>> \n>> I.e. if I already have a full bitcoin node running, could I use RPC calls\n>> or something to tell my Lightning node to use that node, instead of\n>> spinning up *another* full node? I\u2019m currently minimizing the network\n>> thrashing by whitelisting my LN bitcoind node to only point to my existing\n>> full node for updates, but if I could just point my whole LN node at it,\n>> that\u2019s save on disk storage etc. etc. etc.\n> \n> Certainly, that's supported by all 3 implementations:\n> \n> - With c-lightning you can either configure `bitcoin-cli` to connect to\n>   a remote node with the `rpcconnect`, `rpcuser`, and `rpcpassword`\n>   options in the `bitcoin.conf` file (at which point all calls to\n>   `bitcoin-cli` will use that node) or you can use the following\n>   command line options when starting `lightningd`: `--bitcoin-rpcuser`,\n>   `--bitcoin-rpcpassword` and `--bitcoin-rpcconnect`\n> - lnd allows you to specify the node to connect to using the command\n>   line options `--bitcoind.rpchost`, `--bitcoind.rpcuser`, and\n>   `--bitcoind.rpcpass`.\n> - Eclair requires you to edit the configuration file [1] before\n>   compiling afaik\n> \n> Cheers,\n> Christian\n> \n> \n> [1] https://github.com/ACINQ/eclair/blob/master/eclair-core/src/main/resources/reference.conf"
            }
        ],
        "thread_summary": {
            "title": "Why not archive the backend of Bitcoin blockchain?",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Kulpreet Singh",
                "Brian Lockhart",
                "Christian Decker"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 12204
        }
    },
    {
        "title": "[bitcoin-dev] Bitcoin Core 0.16.1 released",
        "thread_messages": [
            {
                "author": "Wladimir J. van der Laan",
                "date": "2018-06-15T16:28:01",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA512\n\nBitcoin Core version 0.16.1 is now available from:\n\n  <https://bitcoincore.org/bin/bitcoin-core-0.16.1/>\n\nOr through bittorrent:\n\n  magnet:?xt=urn:btih:91069028aaf9f6bb3279e71bfd9ab164922e578e&dn=bitcoin-core-0.16.1&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969&tr=udp%3A%2F%2Ftracker.leechers-paradise.org%3A6969&tr=udp%3A%2F%2Fzer0day.ch%3A1337&tr=udp%3A%2F%2Fexplodie.org%3A6969 \n\nThis is a new minor version release, with various bugfixes\nas well as updated translations.\n\nPlease report bugs using the issue tracker at GitHub:\n\n  <https://github.com/bitcoin/bitcoin/issues>\n\nTo receive security and update notifications, please subscribe to:\n\n  <https://bitcoincore.org/en/list/announcements/join/>\n\nHow to Upgrade\n==============\n\nIf you are running an older version, shut it down. Wait until it has completely\nshut down (which might take a few minutes for older versions), then run the\ninstaller (on Windows) or just copy over `/Applications/Bitcoin-Qt` (on Mac)\nor `bitcoind`/`bitcoin-qt` (on Linux).\n\nThe first time you run version 0.15.0 or newer, your chainstate database will be converted to a\nnew format, which will take anywhere from a few minutes to half an hour,\ndepending on the speed of your machine.\n\nNote that the block database format also changed in version 0.8.0 and there is no\nautomatic upgrade code from before version 0.8 to version 0.15.0 or higher. Upgrading\ndirectly from 0.7.x and earlier without re-downloading the blockchain is not supported.\nHowever, as usual, old wallet versions are still supported.\n\nDowngrading warning\n- -------------------\n\nWallets created in 0.16 and later are not compatible with versions prior to 0.16\nand will not work if you try to use newly created wallets in older versions. Existing\nwallets that were created with older versions are not affected by this.\n\nCompatibility\n==============\n\nBitcoin Core is extensively tested on multiple operating systems using\nthe Linux kernel, macOS 10.8+, and Windows Vista and later. Windows XP is not supported.\n\nBitcoin Core should also work on most other Unix-like systems but is not\nfrequently tested on them.\n\nNotable changes\n===============\n\nMiner block size removed\n- ------------------------\n\nThe `-blockmaxsize` option for miners to limit their blocks' sizes was\ndeprecated in version 0.15.1, and has now been removed. Miners should use the\n`-blockmaxweight` option if they want to limit the weight of their blocks'\nweights.\n\n0.16.1 change log\n- ------------------\n\n### Policy\n- - #11423 `d353dd1` [Policy] Several transaction standardness rules (jl2012)\n\n### Mining\n- - #12756 `e802c22` [config] Remove blockmaxsize option (jnewbery)\n\n### Block and transaction handling\n- - #13199 `c71e535` Bugfix: ensure consistency of m_failed_blocks after reconsiderblock (sdaftuar)\n- - #13023 `bb79aaf` Fix some concurrency issues in ActivateBestChain() (skeees)\n\n### P2P protocol and network code\n- - #12626 `f60e84d` Limit the number of IPs addrman learns from each DNS seeder (EthanHeilman)\n\n### Wallet\n- - #13265 `5d8de76` Exit SyncMetaData if there are no transactions to sync (laanwj)\n- - #13030 `5ff571e` Fix zapwallettxes/multiwallet interaction. (jnewbery)\n\n### GUI\n- - #12999 `1720eb3` Show the Window when double clicking the taskbar icon (ken2812221)\n- - #12650 `f118a7a` Fix issue: \"default port not shown correctly in settings dialog\" (251Labs)\n- - #13251 `ea487f9` Rephrase Bech32 checkbox texts, and enable it with legacy address default (fanquake)\n\n### Build system\n- - #12474 `b0f692f` Allow depends system to support armv7l (hkjn)\n- - #12585 `72a3290` depends: Switch to downloading expat from GitHub (fanquake)\n- - #12648 `46ca8f3` test: Update trusted git root (MarcoFalke)\n- - #11995 `686cb86` depends: Fix Qt build with Xcode 9 (fanquake)\n- - #12636 `845838c` backport: #11995 Fix Qt build with Xcode 9 (fanquake)\n- - #12946 `e055bc0` depends: Fix Qt build with XCode 9.3 (fanquake)\n- - #12998 `7847b92` Default to defining endian-conversion DECLs in compat w/o config (TheBlueMatt)\n\n### Tests and QA\n- - #12447 `01f931b` Add missing signal.h header (laanwj)\n- - #12545 `1286f3e` Use wait_until to ensure ping goes out (Empact)\n- - #12804 `4bdb0ce` Fix intermittent rpc_net.py failure. (jnewbery)\n- - #12553 `0e98f96` Prefer wait_until over polling with time.sleep (Empact)\n- - #12486 `cfebd40` Round target fee to 8 decimals in assert_fee_amount (kallewoof)\n- - #12843 `df38b13` Test starting bitcoind with -h and -version (jnewbery)\n- - #12475 `41c29f6` Fix python TypeError in script.py (MarcoFalke)\n- - #12638 `0a76ed2` Cache only chain and wallet for regtest datadir (MarcoFalke)\n- - #12902 `7460945` Handle potential cookie race when starting node (sdaftuar)\n- - #12904 `6c26df0` Ensure bitcoind processes are cleaned up when tests end (sdaftuar)\n- - #13049 `9ea62a3` Backports (MarcoFalke)\n- - #13201 `b8aacd6` Handle disconnect_node race (sdaftuar)\n\n### Miscellaneous\n- - #12518 `a17fecf` Bump leveldb subtree (MarcoFalke)\n- - #12442 `f3b8d85` devtools: Exclude patches from lint-whitespace (MarcoFalke)\n- - #12988 `acdf433` Hold cs_main while calling UpdatedBlockTip() signal (skeees)\n- - #12985 `0684cf9` Windows: Avoid launching as admin when NSIS installer ends. (JeremyRand)\n\n### Documentation\n- - #12637 `60086dd` backport: #12556 fix version typo in getpeerinfo RPC call help (fanquake)\n- - #13184 `4087dd0` RPC Docs: `gettxout*`: clarify bestblock and unspent counts (harding)\n- - #13246 `6de7543` Bump to Ubuntu Bionic 18.04 in build-windows.md (ken2812221)\n- - #12556 `e730b82` Fix version typo in getpeerinfo RPC call help (tamasblummer)\n\nCredits\n=======\n\nThanks to everyone who directly contributed to this release:\n\n- - 251\n- - Ben Woosley\n- - Chun Kuan Lee\n- - David A. Harding\n- - e0\n- - fanquake\n- - Henrik Jonsson\n- - JeremyRand\n- - Jesse Cohen\n- - John Newbery\n- - Johnson Lau\n- - Karl-Johan Alm\n- - Luke Dashjr\n- - MarcoFalke\n- - Matt Corallo\n- - Pieter Wuille\n- - Suhas Daftuar\n- - Tamas Blummer\n- - Wladimir J. van der Laan\n\nAs well as everyone that helped translating on [Transifex](https://www.transifex.com/projects/p/bitcoin/).\n-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v2\n\niQEcBAEBCgAGBQJbI+haAAoJEB5K7WKYbNJdLhwH/2wQbLV/PcQ+dISESPz77b5m\nEsPg4vx3OyD0zshLhqvXZ2tykWVOK6Ft9qWYXfD2ze8OtlMKFaGhOFbaaotNv8f4\ngBW2hBzLfi+WjAjhluYL8NuORlHn+CNTxbFN+xUj5n7y9/6mmcnJaWoYToKbxs5D\nfSM3rQPkrnXf1ar6rnzIxjLHwxzX+lMkh8bzfdyBCmq2HNnUag4tBVLwY/GsIo3p\nUc6MqPReuHnzdKpfG/yo01XngXY7Op3kbnWy/+ZxMtB0JAnHs7YW5PW8vglPeivJ\nZR0nh0n/DmWf7S214Spv/n0ZkZk3s8nsx9ZlZxUlzrjAmZZueW+hylddbsLGvo8=\n=bDHY\n-----END PGP SIGNATURE-----"
            }
        ],
        "thread_summary": {
            "title": "Bitcoin Core 0.16.1 released",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Wladimir J. van der Laan"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 6690
        }
    },
    {
        "title": "[bitcoin-dev] BIP 174 thoughts",
        "thread_messages": [
            {
                "author": "Pieter Wuille",
                "date": "2018-06-15T23:34:40",
                "message_text_only": "Hello all,\n\ngiven some recent work and discussions around BIP 174 (Partially\nSigned Bitcoin Transaction Format) I'd like to bring up a few ideas.\n\nFirst of all, it's unclear to me to what extent projects have already\nworked on implementations, and thus to what extent the specification\nis still subject to change. A response of \"this is way too late\" is\nperfectly fine.\n\nSo here goes:\n\n* Key-value map model or set model.\n\nThis was suggested in this thread:\nhttps://twitter.com/matejcik/status/1002618633472892929\n\nThe motivation behind using a key-value model rather than a simple\nlist of records was that PSBTs can be duplicated (given to multiple\npeople for signing, for example), and merged back together after\nsigning. With a generic key-value model, any implementation can remove\nthe duplication even if they don't understand fields that may have\nbeen added in future extensions.\n\nHowever, almost the same can be accomplished by using the simpler set\nmodel (the file consists of a set of records, with no duplication\nallowed). This would mean that it would technically be legal to have\ntwo partial signatures with the same key for the same input, if a\nnon-deterministic signer is used.\n\nOn the other hand, this means that certain data currently encoded\ninside keys can be dropped, reducing the PSBT size. This is\nparticularly true for redeemscripts and witnessscripts, as they can\njust be computed by the client when deserializing. The two types could\neven be merged into just \"scripts\" records - as they don't need to be\nseparated based on the way they're looked up (Hash160 for P2SH, SHA256\nfor P2WSH). The same could be done for the BIP32 derivation paths,\nthough this may be expensive, as the client would need to derive all\nkeys before being able to figure out which one(s) it needs.\n\nOne exception is the \"transaction\" record, which needs to be unique.\nThat can either be done by adding an exception (\"there can only be one\ntransaction record\"), or by encoding it separately outside the normal\nrecords (that may also be useful to make it clear that it is always\nrequired).\n\n* Ability for Combiners to verify two PSBT are for the same transaction\n\nClearly two PSBTs for incompatible transactions cannot be combined,\nand this should not be allowed.\n\nIt may be easier to enforce this if the \"transaction\" record inside a\nPSBT was required to be in a canonical form, meaning with empty\nscriptSigs and witnesses. In order to do so, there could be per-input\nrecords for \"finalized scriptSig\" and \"finalized witness\". Actually\nplacing those inside the transaction itself would only be allowed when\nall inputs are finalized.\n\n* Optional signing\n\nI think all operation for the Signer responsibility should be\noptional. This will inevitably lead to incompatibilities, but with the\nintent of being forward compatible with future developments, I don't\nthink it is possible to require every implementation to support the\nsame set of scripts or contracts. For example, some signers may only\nimplement single-key P2PKH, or may only support signing SegWit inputs.\nIt's the user's responsibility to find compatible signers (which is\ngenerally not an issue, as the different participants in a setup\nnecessarily have this figured out before being able to create an\naddress). This does mean that there can't be an unconditional test\nvector that specifies the produced signature in certain circumstances,\nbut there could be \"For implementations that choose to implement\nsigning for P2PKH inputs using RFC6979, the expected output given\ninput X and access to key Y is Z\".\n\nOn the other hand, the Combiner and Finalizer roles can probably be\nspecified much more accurately than they are now.\n\n* Derivation from xpub or fingerprint\n\nFor BIP32 derivation paths, the spec currently only encodes the 32-bit\nfingerprint of the parent or master xpub. When the Signer only has a\nsingle xprv from which everything is derived, this is obviously\nsufficient. When there are many xprv, or when they're not available\nindexed by fingerprint, this may be less convenient for the signer.\nFurthermore, it violates the \"PSBT contains all information necessary\nfor signing, excluding private keys\" idea - at least if we don't treat\nthe chaincode as part of the private key.\n\nFor that reason I would suggest that the derivation paths include the\nfull public key and chaincode of the parent or master things are\nderived from. This does mean that the Creator needs to know the full\nxpub which things are derived from, rather than just its fingerprint.\n\n* Generic key offset derivation\n\nWhenever a BIP32 derivation path does not include any hardened steps,\nthe entirety of the derivation can be conveyed as \"The private key for\nP is equal to the private key for Q plus x\", with P and Q points and x\na scalar. This representation is more flexible (it also supports\npay-to-contract derivations), more efficient, and more compact. The\ndownside is that it requires the Signer to support such derivation,\nwhich I don't believe any current hardware devices do.\n\nWould it make sense to add this as an additional derivation method?\n\n* Hex encoding?\n\nThis is a very minor thing. But presumably there will be some standard\nway to store PSBTs as text for copy-pasting - either prescribed by the\nspec, or de facto. These structures may become pretty large, so\nperhaps it's worth choosing something more compact than hexadecimal -\nfor example Base64 or even Z85 (https://rfc.zeromq.org/spec:32/Z85/).\n\nCheers,\n\n-- \nPieter"
            },
            {
                "author": "Peter D. Gray",
                "date": "2018-06-16T15:00:40",
                "message_text_only": "On Fri, Jun 15, 2018 at 04:34:40PM -0700, Pieter Wuille wrote:\n...\n> First of all, it's unclear to me to what extent projects have already\n> worked on implementations, and thus to what extent the specification\n> is still subject to change. A response of \"this is way too late\" is\n> perfectly fine.\n...\n\nThe new Coldcard hardware wallet is based on PSBT (ie. BIP 174 as\npublished), and we consider it \"PSBT Native\". It can add signatures\nto PSBT files delivered on MicroSD card and/or over USB, and is\nable to finalize PSBT files for lots of simple cases. It already\nworks well against the existing BIP174 pull request.\n\nI think the BIP174 spec is reasonable as it is, and should only be\nchanged in a forwards-compatible way from this point... but obviously\nI'm biased.\n\nAs for your specific comments, I don't have strong feelings really.\n\n---\nPeter D. Gray  ||  Founder, Coinkite  ||  Twitter: @dochex  ||  GPG: A3A31BAD 5A2A5B10\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180616/7ae9d1a2/attachment.sig>"
            },
            {
                "author": "Jonas Schnelli",
                "date": "2018-06-19T09:38:24",
                "message_text_only": "> * Key-value map model or set model.\n> * Ability for Combiners to verify two PSBT are for the same transaction\n> * Optional signing\n> * Derivation from xpub or fingerprint\n> * Generic key offset derivation\n> * Hex encoding?\n\nI think all of Pieters points are valid and reasonable thought, though I\u2019m unsure if it would be worth changing the existing-implementation-breaking things like the k/v set model.\nAFAIK things like non-hex-encoding or generic key offset derivation are extensions and would not break existing implementations.\n\nFurther thoughts on BIP174 from my side.\n\nKey derivation in multisig:\nFrom my understanding, the signers and the creator must have agreed \u2013 in advance to the PSBT use case \u2013 on a key derivation scheme.\nBIP32 derivation is assumed, but may not always be the case.\nSharing xpubs (the chaincode) may be a concern in non-trust-relationships between signer(s) and the creator (regarding Pieters xpub/fingerprint concerns).\nProviding the type 0x03, the bip32 derivation path is one form of a support to faster (or computational possible) derivation of the required keys for signing a particular input.\nFrom my point of view, it is a support of additional metadata shared between creator and signer and provided from the creator to the signer for faster (or computation possible) key deviation.\n\nI think it could be more flexible (generic) in BIP174.\nIt could be just a single child key {32-bit int}, or just a keypath ({32-bit int}]{32-bit int}\u2026) which is very likely sufficient for a HWW to derive the relevant key without the creation of a lookup-window or other \u201emaps\".\nIt could even be an enciphered payload which was shared during address/redeem-script generation and \u201eloops\u201c back during a signing request.\n\nMaybe I\u2019m overcomplicating things, but for practical multisig with HWWs, a simple BIP32-child-key-index or BIP32-keypath derivation support field should be sufficient.\nA generic \u201ederivation support field\u201c, provided from the signer to the creator during address-generation that just \u201eloops\u201c back during the PSBT use-cases is probably a overkill.\n\n\nThanks\n\u2014\n/jonas\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: Message signed with OpenPGP\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180619/e76fa257/attachment-0001.sig>"
            },
            {
                "author": "matejcik",
                "date": "2018-06-19T14:20:03",
                "message_text_only": "Hello,\n\nFirst of all, we'd like to apologize for such a late feedback, since\nthere is a PR for this already. We've come up with a few more notes on\nthis, so we are introducing those in this message and replying on\nPieter's points in another one.\n\n\n1) Why isn't the global type 0x03 (BIP-32 path) per-input? How do we\nknow, which BIP-32 path goes to which input? The only idea that comes to\nmy mind is that we should match the input's scriptPubKey's pubkey to\nthis 0x03's key (the public key).\n\nIf our understanding is correct, the BIP-32 path is global to save space\nin case two inputs share the same BIP-32 path? How often does that\nhappen? And in case it does, doesn't it mean an address reuse which is\ndiscouraged?\n\nAlso, we believe that if the public key is to be used as \"spent to by an\noutput\" it should be in an output section. If the public key is to be\nused to sign an input, it should be in the input section. Again, how\noften are those the same? We understand creating another section might\nbe cumbersome, but it'd significantly increase clarity to have global,\ninput and output section.\n\nAlternately, we could keep \u201cspend to\u201d public keys in the global section,\nand put the input public keys to the per-input sections. This is less\nclear, but doesn\u2019t introduce another section. A question to consider is,\nwill there be more per-output data? If yes, it might make sense to have\nan output section.\n\n\n2) The global items 0x01 (redeem script) and 0x02 (witness script) are\nsomewhat confusing. Let's consider only the redeem script (0x01) to make\nit simple. The value description says: \"A redeem script that will be\nneeded to sign a Pay-To-Script-Hash input or is spent to by an output.\".\nDoes this mean that the record includes both input's redeem script\n(because we need to sign it), but also a redeem script for the output\n(to verify we are sending to a correct P2SH)? To mix those two seems\nreally confusing.\n\nYet again, adding a new output section would make this more readable. We\nwould include the input\u2019s redeem script in the input section and the\noutput\u2019s redeem script again in the output section, because they\u2019ll most\nlikely differ anyway.\n\nThe rationale says that the scripts are global to avoid duplication.\nHowever, how often is this the case? The scripts include a hash of some\nOP codes and the recipient's public key for example. So a) how often are\ntwo scripts equal to justify this? b) if they're the same, doesn't it\nyet again signalize address reuse?\n\n\n3) The sighash type 0x03 says the sighash is only a recommendation. That\nseems rather ambiguous. If the field is specified shouldn't it be binding?\n\n\n4) Is it a good idea to skip records which types we are unaware of? We\ncan't come up with a reasonable example, but intuitively this seems as a\npotential security issue. We think we should consider  introducing a\nflag, which would define if the record is \"optional\". In case the signer\nencounters a record it doesn't recognize and such flag is not set, it\naborts the procedure. If we assume the set model we could change the\nstructure to <type><optional flag><length>{data}. We are not keen on\nthis, but we wanted to include this idea to see what you think.\n\n-----------\n\nIn general, the standard is trying to be very space-conservative,\nhowever is that really necessary? We would argue for clarity and ease of\nuse over space constraints. We think more straightforward approach is\ndesired, although more space demanding. What are the arguments to make\nthis as small as possible? If we understand correctly, this format is\nnot intended for blockchain nor for persistent storage, so size doesn\u2019t\nmatter nearly as much.\n\n\nThank you,\n\nTomas Susanka\nJan Matejek\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 819 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180619/57fc30e2/attachment.sig>"
            },
            {
                "author": "Jonas Schnelli",
                "date": "2018-06-19T15:20:34",
                "message_text_only": "I agree with matejcik\u2019s point 1 to 3 and especially with point 4.\nThe mandatory flag (or optional-flag) makes much sense to me.\n\n> -----------\n> \n> In general, the standard is trying to be very space-conservative,\n> however is that really necessary? We would argue for clarity and ease of\n> use over space constraints. We think more straightforward approach is\n> desired, although more space demanding. What are the arguments to make\n> this as small as possible? If we understand correctly, this format is\n> not intended for blockchain nor for persistent storage, so size doesn\u2019t\n> matter nearly as much.\n\nI don\u2019t see any reasons why space would be an issue.\n\nHWWs probably can\u2019t handle PBST natively since it is not optimised for\npresenting various informations in a signing-verification.\n\nA single stream-in of a PSBT through USB (or similar channel) will not work in\nmany cases since HWW come often with very restrictive RAM constraints.\n\nFurthermore, I forget to mention in my last mail, that registering (or defining)\na mime-type for PSBT would probably a great usability feature.\n(Send PSBT by email/messanger and with dbl-click to open feature, etc.)\n\n\n/jonas\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: Message signed with OpenPGP\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180619/c263f67b/attachment.sig>"
            },
            {
                "author": "Peter D. Gray",
                "date": "2018-06-21T20:28:15",
                "message_text_only": "On Tue, Jun 19, 2018 at 05:20:34PM +0200, Jonas Schnelli wrote:\n...\n> \n> I don\u2019t see any reasons why space would be an issue.\n> \n> HWWs probably can\u2019t handle PBST natively since it is not optimised for\n> presenting various informations in a signing-verification.\n\nThe Coldcard hardware wallet is PSBT native and does work directly from PSBT.\n\n> A single stream-in of a PSBT through USB (or similar channel) will not work in\n> many cases since HWW come often with very restrictive RAM constraints.\n\nFor the Coldcard, we expect a PSBT to be 'uploaded' over USB (can\nalso be provided on MicroSD card) and we work in-place with it,\nscanning over it a few different times. If the user approves the\ntransaction, we produce a signed PSBT or final transaction and that\ngets downloaded.\n\nWe support 256k byte PSBT files with hundreds of inputs/outputs\n(IIRC, and exact limits still TBD) and are operating in a system\nwith only 25k free RAM after startup.\n\n> Furthermore, I forget to mention in my last mail, that registering (or defining)\n> a mime-type for PSBT would probably a great usability feature.\n> (Send PSBT by email/messanger and with dbl-click to open feature, etc.)\n\n+1 for mimetype, especially since it's a binary format.\n\n---\nPeter D. Gray  ||  Founder, Coinkite  ||  Twitter: @dochex  ||  GPG: A3A31BAD 5A2A5B10\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180621/be77e362/attachment.sig>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2018-06-19T17:16:51",
                "message_text_only": "On Tue, Jun 19, 2018 at 7:20 AM, matejcik via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n\nThanks for your comments so far. I'm very happy to see people dig into\nthe details, and consider alternative approaches.\n\n> 1) Why isn't the global type 0x03 (BIP-32 path) per-input? How do we\n> know, which BIP-32 path goes to which input? The only idea that comes to\n> my mind is that we should match the input's scriptPubKey's pubkey to\n> this 0x03's key (the public key).\n\n> If our understanding is correct, the BIP-32 path is global to save space\n> in case two inputs share the same BIP-32 path? How often does that\n> happen? And in case it does, doesn't it mean an address reuse which is\n> discouraged?\n\nYes, the reason is address reuse. It may be discouraged, but it still\nhappens in practice (and unfortunately it's very hard to prevent\npeople from sending to the same address twice).\n\nIt's certainly possible to make them per-input (and even per-output as\nsuggested below), but I don't think it gains you much. At least when a\nsigner supports any kind of multisig, it needs to match up public keys\nwith derivation paths. If several can be provided, looking them up\nfrom a global table or a per-input table shouldn't fundamentally\nchange anything.\n\nHowever, perhaps it makes sense to get rid of the global section\nentirely, and make the whole format a transaction plus per-input and\nper-output extra fields. This would result in duplication in case of\nkey reuse, but perhaps that's worth the complexity reduction.\n\n> 2) The global items 0x01 (redeem script) and 0x02 (witness script) are\n> somewhat confusing. Let's consider only the redeem script (0x01) to make\n> it simple. The value description says: \"A redeem script that will be\n> needed to sign a Pay-To-Script-Hash input or is spent to by an output.\".\n> Does this mean that the record includes both input's redeem script\n> (because we need to sign it), but also a redeem script for the output\n> (to verify we are sending to a correct P2SH)? To mix those two seems\n> really confusing.\n>\n> Yet again, adding a new output section would make this more readable. We\n> would include the input\u2019s redeem script in the input section and the\n> output\u2019s redeem script again in the output section, because they\u2019ll most\n> likely differ anyway.\n\nI think here it makes sense because there can actually only be (up to)\none redeemscript and (up to) one witnessscript. So if we made those\nper-input and per-output, it may simplify signers as they don't need a\ntable lookup to find the correct one. That would also mean we can drop\ntheir hashes, even if we keep a key-value model.\n\n> 3) The sighash type 0x03 says the sighash is only a recommendation. That\n> seems rather ambiguous. If the field is specified shouldn't it be binding?\n\nPerhaps, yes.\n\n> 4) Is it a good idea to skip records which types we are unaware of? We\n> can't come up with a reasonable example, but intuitively this seems as a\n> potential security issue. We think we should consider  introducing a\n> flag, which would define if the record is \"optional\". In case the signer\n> encounters a record it doesn't recognize and such flag is not set, it\n> aborts the procedure. If we assume the set model we could change the\n> structure to <type><optional flag><length>{data}. We are not keen on\n> this, but we wanted to include this idea to see what you think.\n\nOriginally there was at least this intuition for why it shouldn't be\nnecessary: the resulting signature for an input is either valid or\ninvalid. Adding information to a PSBT (which is what signers do)\neither helps with that or not. The worst case is that they simply\ndon't have enough information to produce a signature together. But an\nignored unknown field being present should never result in signing the\nwrong thing (they can always see the transaction being signed), or\nfailing to sign if signing was possible in the first place. Another\nway of looking at it, the operation of a signer is driven by queries:\nit looks at the scriptPubKey of the output being spent, sees it is\nP2SH, looks for the redeemscript, sees it is P2WSH, looks for the\nwitnessscript, sees it is multisig, looks for other signers'\nsignatures, finds enough for the threshold, and proceeds to sign and\ncreate a full transaction. If at any point one of those things is\nmissing or not comprehensible to the signer, he simply fails and\ndoesn't modify the PSBT.\n\nHowever, if the sighash request type becomes mandatory, perhaps this\nis not the case anymore, as misinterpreting something like this could\nindeed result in an incorrect signature.\n\nIf we go down this route, if a field is marked as mandatory, can you\nstill act as a combiner for it? Future extensions should always\nmaintain the invariant that a simple combiner which just merges all\nthe fields and deduplicates should always be correct, I think. So such\na mandatory field should only apply to signers?\n\n> In general, the standard is trying to be very space-conservative,\n> however is that really necessary? We would argue for clarity and ease of\n> use over space constraints. We think more straightforward approach is\n> desired, although more space demanding. What are the arguments to make\n> this as small as possible? If we understand correctly, this format is\n> not intended for blockchain nor for persistent storage, so size doesn\u2019t\n> matter nearly as much.\n\nI wouldn't say it's trying very hard to be space-conservative. The\ndesign train of thought started from \"what information does a signer\nneed\", and found a signer would need information on the transaction to\nsign, and on scripts to descend into, information on keys to derive,\nand information on signatures provided by other participants. Given\nthat some of this information was global (at least the transaction),\nand some of this information was per-input (at least the signatures),\nseparate scopes were needed for those. Once you have a global scope,\nand you envision a signer which looks up scripts and keys in a map of\nknown ones (like the signing code in Bitcoin Core), there is basically\nno downside to make the keys and scripts global - while giving space\nsavings for free to deduplication.\n\nHowever, perhaps that's not the right way to think about things, and\nthe result is simpler if we only keep the transaction itself global,\nand everything else per-input (and per-output).\n\nI think there are good reasons to not be gratuitously large (I expect\nthat at least while experimenting, people will copy-paste these things\na lot and page-long copy pastes become unwieldy quickly), but maybe\nnot at the cost of structural complexity.\n\nOn Tue, Jun 19, 2018 at 7:22 AM, matejcik via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> hello,\n> this is our second e-mail with replies to Pieter's suggestions.\n>\n> On 16.6.2018 01:34, pieter.wuille at gmail.com (Pieter Wuille) wrote:\n>> * Key-value map model or set model.\n\n> Just to note, we should probably use varint for the <type> field - this\n> allows us, e.g., to create \u201cnamespaces\u201d for future extensions by using\n> one byte as namespace identifier and one as field identifier.\n\nAgree, but this doesn't actually need to be specified right now. As\nthe key's (and or value's) interpretation (including the type) is\ncompletely unspecified, an extension can just start using 2-byte keys\n(as long as the first byte of those 2 isn't used by another field\nalready).\n\n>> One exception is the \"transaction\" record, which needs to be unique.\n>> That can either be done by adding an exception (\"there can only be one\n>> transaction record\"), or by encoding it separately outside the normal\n>> records (that may also be useful to make it clear that it is always\n>> required).\n>\n> This seems to be the case for some fields already - i.e., an input field\n> must have exactly one of Non-witness UTXO or Witness Output. So \u201cadding\n> an exception\u201d is probably just a matter of language?\n\nHmm, I wouldn't say so. Perhaps the transaction's inputs and outputs\nare chosen by one entity, and then sent to another entity which has\naccess to the UTXOs or previous transactions. So while the UTXOs must\nbe present before signing, I wouldn't say the file format itself must\nenforce that the UTXOs are present.\n\nHowever, perhaps we do want to enforce at-most one UTXO per input. If\nthere are more potential extensions like this, perhaps a key-value\nmodel is better, as it's much easier to enforce no duplicate keys than\nit is to add field-specific logic to combiners (especially for\nextensions they don't know about yet).\n\n> We\u2019d also like to note that the \u201cnumber of inputs\u201d field should be\n> mandatory - and as such, possibly also a candidate for outside-record field.\n\nIf we go with the \"not put signatures/witnesses inside the transaction\nuntil all of them are finalized\" suggestion, perhaps the number of\ninputs field can be dropped. There would be always one exactly for\neach input (but some may have the \"final script/witness\" field and\nothers won't).\n\n>> * Derivation from xpub or fingerprint\n>>\n>> For BIP32 derivation paths, the spec currently only encodes the 32-bit\n>> fingerprint of the parent or master xpub. When the Signer only has a\n>> single xprv from which everything is derived, this is obviously\n>> sufficient. When there are many xprv, or when they're not available\n>> indexed by fingerprint, this may be less convenient for the signer.\n>> Furthermore, it violates the \"PSBT contains all information necessary\n>> for signing, excluding private keys\" idea - at least if we don't treat\n>> the chaincode as part of the private key.\n>>\n>> For that reason I would suggest that the derivation paths include the\n>> full public key and chaincode of the parent or master things are\n>> derived from. This does mean that the Creator needs to know the full\n>> xpub which things are derived from, rather than just its fingerprint.\n>\n>\n> We don\u2019t understand the rationale for this idea. Do you see a scenario\n> where an index on master fingerprint is not available but index by xpubs\n> is? In our envisioned use cases at least, indexing private keys by xpubs\n> (as opposed to deriving from a BIP32 path) makes no sense.\n\nLet me elaborate.\n\nRight now, the BIP32 fields are of the form <master\nfingerprint><childidx><childidx><childidx>...\n\nInstead, I suggest fields of the form <master pubkey><master\nchaincode><childidx><childidx><childidx>...\n\nThe fingerprint in this case is identical to the first 32 bit of the\nHash160 of <master pubkey>, so certainly no information is lost by\nmaking this change.\n\nThis may be advantageous for three reasons:\n* It permits signers to have ~thousands of master keys (at which point\n32-bit fingerprints would start having reasonable chance for\ncollisions, meaning multiple derivation attempts would be needed to\nfigure out which one to use).\n* It permits signers to index their master keys by whatever they like\n(for example, SHA256 rather than Hash160 or prefix thereof).\n* It permits signers who don't store a chaincode at all, and just\nprotect a single private key.\n\nCheers,\n\n-- \nPieter"
            },
            {
                "author": "matejcik",
                "date": "2018-06-21T11:29:44",
                "message_text_only": "On 19.6.2018 19:16, Pieter Wuille wrote:\n>> 1) Why isn't the global type 0x03 (BIP-32 path) per-input? How do we\n>> know, which BIP-32 path goes to which input? The only idea that comes to\n>> my mind is that we should match the input's scriptPubKey's pubkey to\n>> this 0x03's key (the public key).\n> \n>> If our understanding is correct, the BIP-32 path is global to save space\n>> in case two inputs share the same BIP-32 path? How often does that\n>> happen? And in case it does, doesn't it mean an address reuse which is\n>> discouraged?\n> \n> Yes, the reason is address reuse. It may be discouraged, but it still\n> happens in practice (and unfortunately it's very hard to prevent\n> people from sending to the same address twice).\n> \n> It's certainly possible to make them per-input (and even per-output as\n> suggested below), but I don't think it gains you much. At least when a\n> signer supports any kind of multisig, it needs to match up public keys\n> with derivation paths. If several can be provided, looking them up\n> from a global table or a per-input table shouldn't fundamentally\n> change anything.\n\nSo here\u2019s a thing I\u2019m still confused about.\n\nImagine two cases, for a naive Signer:\n- either all data is global\n- or most data is per input.\n\nNow, the general signing flow is this:\n1. Pre-serialize the transaction\n2. Prepare the current input - fill out scriptPubKey (or equivalent for\nsegwit)\n3. find a secret key\n4. output public key + signature\n\nStep (3) is the main issue here.\n\nIn the case of everything per-input, the naive Signer can do this:\n1. (in the global section) pre-serialize the transaction\n2. (in each input) find and fill out scriptPubKey from the provided UTXO\n3. (for a given BIP32 path) check if the master fingerprint matches\nmine, if yes, derive secret key, output pubkey, signature\n4. goto 3 (more keys per input), goto 2 (next input)\n\nNote that this flow works perfectly for multisig; it\u2019s going to be the\njob of a Finalizer to build the final scriptSig, but each input can have\nmultiple partial signatures -- and, interestingly, the naive Signer\ndoesn\u2019t even need to know about multisig.\n\nA less naive Signer will want to check things, maybe derive a scriptSig\nitself and check if it matches the given hash, etc., but it can do this\nall in place. You go linearly through the signing flow and place a\ncouple strategic assertions along the way.\n\nHowever, if the data is global, as is now, it gets more complicated:\n1. (in the global section) pre-serialize the transaction, prefill lookup\ntables\n2. (for a given BIP32 path) check if mine, then derive public key and\nstore in a dictionary\n3. (for each input) find _and parse_ scriptPubKey, extract (PK or)\nscript hash\n4. lookup redeem script based on script-hash; if not found, goto 2; if\nfound, parse out public key\n5. lookup public key in the BIP32 dictionary; if not found, goto 2\n6. output pubkey, signature\n\nIn addition to being more steps and lookups, it requires the Signer to\nunderstand the redeem script. A strict Signer will want that anyway, but\nin the first case, the Signer can regenerate the scripts and compare\nspecificaly the ones it's working with; here, you need to parse them\neven before you know what you're comparing to.\n\nIs there something I\u2019m missing? Because as I see it, there is literally\nno advantage to the more complicated flow; that\u2019s why we assumed that\nthe format is space-saving, because saving space was the only reason we\ncould imagine.\n\n> If we go down this route, if a field is marked as mandatory, can you\n> still act as a combiner for it? Future extensions should always\n> maintain the invariant that a simple combiner which just merges all\n> the fields and deduplicates should always be correct, I think. So such\n> a mandatory field should only apply to signers?\n\n(...)\n\n> However, perhaps we do want to enforce at-most one UTXO per input. If\n> there are more potential extensions like this, perhaps a key-value\n> model is better, as it's much easier to enforce no duplicate keys than\n> it is to add field-specific logic to combiners (especially for\n> extensions they don't know about yet).\n\nIn general, you seem to focus a lot on the role of Combiners, esp.\nsimple Combiners. To me, that doesn\u2019t look like a significant role. As I\nenvision it, a Combiner really doesn\u2019t need to do anything more\ncomplicated than merge and deduplicate records, simply based on the\nuniqueness of the whole record.\nIt\u2019s the Finalizer\u2019s job to reconstruct and validate the result. Also\nISTM if something messes up the PSBT (such as including multiple\nconflicting fields anywhere), it\u2019s OK to leave it to Finalizer to fail.\n\nAre the Combiners supposed to be separate from Finalizers? (Is there a\nrisk of a Combiner passing along a bad PSBT, Finalizer rejecting it, and\nthe other parties not finding out?)\n\n\n> If we go with the \"not put signatures/witnesses inside the transaction\n> until all of them are finalized\" suggestion, perhaps the number of\n> inputs field can be dropped. There would be always one exactly for\n> each input (but some may have the \"final script/witness\" field and\n> others won't).\n\nStrongly agree with this. A guarantee that number of inputs in the\ntransaction corresponds to number of input fields for PBST looks cleaner\nthan specifying it separately. This way we can also drop the \"input index\".\n\n\n> Right now, the BIP32 fields are of the form <master\n> fingerprint><childidx><childidx><childidx>...\n> \n> Instead, I suggest fields of the form <master pubkey><master\n> chaincode><childidx><childidx><childidx>...\n> \n> The fingerprint in this case is identical to the first 32 bit of the\n> Hash160 of <master pubkey>, so certainly no information is lost by\n> making this change.\n> \n> This may be advantageous for three reasons:\n> * It permits signers to have ~thousands of master keys (at which point\n> 32-bit fingerprints would start having reasonable chance for\n> collisions, meaning multiple derivation attempts would be needed to\n> figure out which one to use).\n> * It permits signers to index their master keys by whatever they like\n> (for example, SHA256 rather than Hash160 or prefix thereof)> * It permits signers who don't store a chaincode at all, and just\n> protect a single private key.\n\nI like this last usecase a lot, but perhaps that's a role for a\n\"sub-Creator\"? see below.\n\nAlso, is there a reason to publish the chain code, wouldn't just the\npublic key be sufficient to accomplish all three usecases you list?\nI sort of dislike the notion that you need to give all this information\nto a possibly untrusted Creator.\n\n\nAn aside to this in particular, I\u2019ve been thinking about the requirement\nto share derivation paths and public keys with the Creator. The spec\nassumes that this will happen; you\u2019re talking about providing full\nxpub+chaincode too. At least, the Creator must prefill BIP32 paths and\nmaster key fingerprints. Possibly also prefill public keys in the redeem\nscripts?\n\nThis might not be an improvement proposal, but a point worth being\nraised and maybe explained in the spec. Perhaps the original Creator\ndoesn\u2019t have access to this data, and delegates this to some\n\u201csub-Creators\u201d  - I imagine a coordinator sending a PSBT to signing\nparties, each of which acts as a sub-Creator (fills out derivation paths\nand public keys) and a Signer (forwarding to a HWW). Some of the\ndiscussion even suggests some sort of generic \u201ckey derivation field\u201d\nwith arbitrary contents - fingerprint + bip32 path? xpub + chain code?\nderivation points? encrypted xprv?\n\nthank you for your comments\n\nregards\nm.\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 819 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180621/92c941f5/attachment-0001.sig>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2018-06-21T17:39:57",
                "message_text_only": "On Thu, Jun 21, 2018 at 4:29 AM, matejcik <jan.matejek at satoshilabs.com> wrote:\n> In the case of everything per-input, the naive Signer can do this:\n> 1. (in the global section) pre-serialize the transaction\n> 2. (in each input) find and fill out scriptPubKey from the provided UTXO\n> 3. (for a given BIP32 path) check if the master fingerprint matches\n> mine, if yes, derive secret key, output pubkey, signature\n> 4. goto 3 (more keys per input), goto 2 (next input)\n>\n> Note that this flow works perfectly for multisig; it\u2019s going to be the\n> job of a Finalizer to build the final scriptSig, but each input can have\n> multiple partial signatures -- and, interestingly, the naive Signer\n> doesn\u2019t even need to know about multisig.\n\nAh, you're thinking of an even simpler signer than I was imagining. I\ndon't think this works in general, because the hash being signed\ndepends on the structure of the script. For example, if it is P2SH, it\nis the redeemscript that goes into the scriptCode serialization rather\nthan the scriptPubKey. If it is segwit, BIP143 serialization needs to\nbe used, etc. It may work if your signing is restricted to just one of\nthose structures, though.\n\n> A less naive Signer will want to check things, maybe derive a scriptSig\n> itself and check if it matches the given hash, etc., but it can do this\n> all in place. You go linearly through the signing flow and place a\n> couple strategic assertions along the way.\n\nRight - but I think anything but the simplest signer must do this,\njust to be able to distinguish between different kinds of signature\nhashing.\n\nBut you're right, having per-input redeemscript/witnessscript\nsimplifies things still - instead of needing to look a script hash in\na map, you can just compare it with *the* redeemscript/witnessscript.\n\n> However, if the data is global, as is now, it gets more complicated:\n> 1. (in the global section) pre-serialize the transaction, prefill lookup\n> tables\n> 2. (for a given BIP32 path) check if mine, then derive public key and\n> store in a dictionary\n> 3. (for each input) find _and parse_ scriptPubKey, extract (PK or)\n> script hash\n> 4. lookup redeem script based on script-hash; if not found, goto 2; if\n> found, parse out public key\n> 5. lookup public key in the BIP32 dictionary; if not found, goto 2\n> 6. output pubkey, signature\n\nI understand your point now. I hadn't considered the possibility of\njust signing with all BIP32 derivation paths given for which the\nmaster matches, instead of extracting pubkeys/pkhs from the script.\nThat's a major simplification for signers indeed. I do think you need\nsome conditions before to determine the script structure (see above),\nbut this is a good point in favour of making the derivation paths\nper-input.\n\n> In general, you seem to focus a lot on the role of Combiners, esp.\n> simple Combiners. To me, that doesn\u2019t look like a significant role. As I\n> envision it, a Combiner really doesn\u2019t need to do anything more\n> complicated than merge and deduplicate records, simply based on the\n> uniqueness of the whole record.\n\nIt's more a side-effect of focusing on forward compatibility. I expect\nthat we will have transactions with inputs spending different kinds of\noutputs, and some signers may not be able to understand all of them.\nHowever, as long as the design goal of having Combiners function\ncorrectly for things they don't understand, everything should be able\nto work together fine.\n\n> It\u2019s the Finalizer\u2019s job to reconstruct and validate the result. Also\n> ISTM if something messes up the PSBT (such as including multiple\n> conflicting fields anywhere), it\u2019s OK to leave it to Finalizer to fail.\n\nAgree.\n\n> An aside to this in particular, I\u2019ve been thinking about the requirement\n> to share derivation paths and public keys with the Creator. The spec\n> assumes that this will happen; you\u2019re talking about providing full\n> xpub+chaincode too. At least, the Creator must prefill BIP32 paths and\n> master key fingerprints. Possibly also prefill public keys in the redeem\n> scripts?\n>\n> This might not be an improvement proposal, but a point worth being\n> raised and maybe explained in the spec. Perhaps the original Creator\n> doesn\u2019t have access to this data, and delegates this to some\n> \u201csub-Creators\u201d  - I imagine a coordinator sending a PSBT to signing\n> parties, each of which acts as a sub-Creator (fills out derivation paths\n> and public keys) and a Signer (forwarding to a HWW). Some of the\n> discussion even suggests some sort of generic \u201ckey derivation field\u201d\n> with arbitrary contents - fingerprint + bip32 path? xpub + chain code?\n> derivation points? encrypted xprv?\n\nThat makes sense - I think we've already touched this when discussing\nthe requirement for UTXOs to be added. Perhaps those aren't added by\nthe Creator, but by some index server. The same could be true for the\nscripts or derivations paths.\n\nAnd indeed, most of the information in the derivation paths is\neffectively opaque to the Creator - it's just some data given out by\nthe Signer about its keys that gets passed back to it so it can\nidentify the key. There is benefit in keeping it in a fixed structure\n(like xpub/chaincode, or fingerprint + derivation indexes), to\nguarantee compatibility between multiple signer implementations with\naccess to the same key.\n\nOn Tue, Jun 19, 2018 at 5:39 PM, Jason Les via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>Hex encoding?\n>\n> I was hoping for some standard here was well and I agree using something\n> more compact than hex is important. My understanding is Z85 is better for\n> use with JSON than Base64, which is probably a good benefit to have here.\n\nBoth Base64 and Z85 can be stored in JSON strings without quoting\n(neither uses quotation characters or backslashes), but Z85 is\nslightly more compact (Z85 is 5 characters for 4 bytes, Base64 is 4\ncharacters for 3 bytes). Both use non-alphanumeric characters, so I\ndon't think there is much difference w.r.t. copy-pastability either.\nZ85 is far less common though.\n\nOn Thu, Jun 21, 2018 at 4:44 AM, Tomas Susanka via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> I think here it makes sense because there can actually only be (up to)\n>> one redeemscript and (up to) one witnessscript. So if we made those\n>> per-input and per-output, it may simplify signers as they don't need a\n>> table lookup to find the correct one. That would also mean we can drop\n>> their hashes, even if we keep a key-value model.\n> Yes, indeed. Just to clarify: in the first sentence you mean \"per\n> output\", right? There can actually only be (up to) one redeemscript and\n> (up to) one witnessscript *per output*.\n\nUp to one per output, and up to one per input - indeed.\n\nOn Thu, Jun 21, 2018 at 7:32 AM, Tomas Susanka via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>> A question to consider is,\n>>> will there be more per-output data? If yes, it might make sense to have\n>>> an output section.\n>> I think it is unlikely that there would be anymore per-output data.\n>\n> Hmm, upon further reflection, maybe it's not even worth including *any*\n> per-output data, aside from what the original transaction contains.\n>\n> The output redeem script is either:\n> - unknown, because we have received only an address from the receiver\n> - or it is known, because it is ours and in that case it doesn\u2019t make\n> sense to include it in PSBT\n>\n> We got stuck on the idea of the Creator providing future (output)\n> redeem/witness scripts. But that seems to be a minority use case and can\n> be solved efficiently via the same channels that coordinate the PSBT\n> creation. Sorry to change opinions so quickly on this one.\n\nPerhaps you're missing the reason for having output scripts? It is so\nthat signers that wish to known the amounts transferred can be told\nwhich outputs of the to-be transaction are change, and thus shouldn't\nbe counted towards the balance. By providing the scripts and\nderivation paths in a PSBT, the Creator can prove to the Signer that\ncertain outputs do not actually move funds to some other entity.\n\n\nBased on the points before, my preference is having everything\nper-input and per-output except the transaction (with empty\nscriptSig/witness) itself, and having exactly one set/map per input\nand output (which may include a \"finalized scriptSig/witness field\"\nfor finalized inputs). The overhead of having at least one separator\nbyte for every input and output in the transaction is at most a few\npercent compared to the data in the transaction itself. If size is\nreally an issue (but I think we've already established that small size\ngains aren't worth much extra complexity), we could also serialize the\ntransaction without scriptSigs/witnesses (which are at least one byte\neach, and guaranteed to be empty).\n\nI'm unsure about typed record vs. key-value model. If we'd go with a\nper-input script approach, the key would just be a single byte (\"the\nredeemscript\" and \"the witnessscript\"), so the advantage of being able\nto drop the script hashes applies equally to both models. After that,\nit seems the only difference seems to be that a well-defined prefix of\nthe records is enforced to be unique as opposed to the entire record\nbeing enforced to be unique. I don't think there is much difference in\ncomplexity, as Combiners and Signers still need to enforce some kind\nof uniqueness even in a typed records model.\n\nCheers,\n\n-- \nPieter"
            },
            {
                "author": "Tomas Susanka",
                "date": "2018-06-21T11:44:37",
                "message_text_only": "Hi,\n\nOn 19.6.2018 19:16, Pieter Wuille via bitcoin-dev wrote:\n> Yes, the reason is address reuse. It may be discouraged, but it still\n> happens in practice (and unfortunately it's very hard to prevent\n> people from sending to the same address twice).\n>\n> It's certainly possible to make them per-input (and even per-output as\n> suggested below), but I don't think it gains you much. At least when a\n> signer supports any kind of multisig, it needs to match up public keys\n> with derivation paths. If several can be provided, looking them up\n> from a global table or a per-input table shouldn't fundamentally\n> change anything.\n>\n> However, perhaps it makes sense to get rid of the global section\n> entirely, and make the whole format a transaction plus per-input and\n> per-output extra fields. This would result in duplication in case of\n> key reuse, but perhaps that's worth the complexity reduction.\nI think having a global section with just one record (the transaction)\nis just fine, in case we come up with some other fields later on which\nwould fit the global section. Otherwise I totally agree.\n>> 2) The global items 0x01 (redeem script) and 0x02 (witness script) are\n>> somewhat confusing. Let's consider only the redeem script (0x01) to make\n>> it simple. The value description says: \"A redeem script that will be\n>> needed to sign a Pay-To-Script-Hash input or is spent to by an output.\".\n>> Does this mean that the record includes both input's redeem script\n>> (because we need to sign it), but also a redeem script for the output\n>> (to verify we are sending to a correct P2SH)? To mix those two seems\n>> really confusing.\n>>\n>> Yet again, adding a new output section would make this more readable. We\n>> would include the input\u2019s redeem script in the input section and the\n>> output\u2019s redeem script again in the output section, because they\u2019ll most\n>> likely differ anyway.\n> I think here it makes sense because there can actually only be (up to)\n> one redeemscript and (up to) one witnessscript. So if we made those\n> per-input and per-output, it may simplify signers as they don't need a\n> table lookup to find the correct one. That would also mean we can drop\n> their hashes, even if we keep a key-value model.\nYes, indeed. Just to clarify: in the first sentence you mean \"per\noutput\", right? There can actually only be (up to) one redeemscript and\n(up to) one witnessscript *per output*.\n>> 4) Is it a good idea to skip records which types we are unaware of? We\n>> can't come up with a reasonable example, but intuitively this seems as a\n>> potential security issue. We think we should consider  introducing a\n>> flag, which would define if the record is \"optional\". In case the signer\n>> encounters a record it doesn't recognize and such flag is not set, it\n>> aborts the procedure. If we assume the set model we could change the\n>> structure to <type><optional flag><length>{data}. We are not keen on\n>> this, but we wanted to include this idea to see what you think.\n> Originally there was at least this intuition for why it shouldn't be\n> necessary: the resulting signature for an input is either valid or\n> invalid. Adding information to a PSBT (which is what signers do)\n> either helps with that or not. The worst case is that they simply\n> don't have enough information to produce a signature together. But an\n> ignored unknown field being present should never result in signing the\n> wrong thing (they can always see the transaction being signed), or\n> failing to sign if signing was possible in the first place. Another\n> way of looking at it, the operation of a signer is driven by queries:\n> it looks at the scriptPubKey of the output being spent, sees it is\n> P2SH, looks for the redeemscript, sees it is P2WSH, looks for the\n> witnessscript, sees it is multisig, looks for other signers'\n> signatures, finds enough for the threshold, and proceeds to sign and\n> create a full transaction. If at any point one of those things is\n> missing or not comprehensible to the signer, he simply fails and\n> doesn't modify the PSBT.\nThe rationale behind this was, what if at some point we come up with a\nPSBT record, which forbids some kind of operation or alters some\nbehaviour. In another words, by omitting such record the signer would\ncreate a signature, which is valid, but actually signed something\ndifferent than the Creator intended.\n\n> However, if the sighash request type becomes mandatory, perhaps this\n> is not the case anymore, as misinterpreting something like this could\n> indeed result in an incorrect signature.\nI believe this use case illustrates it quite well. Let\u2019s suppose the\nsighash record is binding and the Signer does not know it. The Creator\ncreates a PSBT with sighash set SIGHASH_SINGLE. The Signer sings the\ntransaction with SIGHASH_ALL, because they are not aware of such field.\nThis results in a valid signature, however not what the Creator intended\nit to be.\n\n>> We\u2019d also like to note that the \u201cnumber of inputs\u201d field should be\n>> mandatory - and as such, possibly also a candidate for outside-record field.\n> If we go with the \"not put signatures/witnesses inside the transaction\n> until all of them are finalized\" suggestion, perhaps the number of\n> inputs field can be dropped. There would be always one exactly for\n> each input (but some may have the \"final script/witness\" field and\n> others won't).\nAgree. I'm be fine with dropping the field completely in that case.\n\n\nThanks,\nTomas"
            },
            {
                "author": "matejcik",
                "date": "2018-06-19T14:22:30",
                "message_text_only": "hello,\nthis is our second e-mail with replies to Pieter's suggestions.\n\nOn 16.6.2018 01:34, pieter.wuille at gmail.com (Pieter Wuille) wrote:\n> * Key-value map model or set model.\n> \n> This was suggested in this thread:\n> https://twitter.com/matejcik/status/1002618633472892929\n> \n> The motivation behind using a key-value model rather than a simple\n> list of records was that PSBTs can be duplicated (given to multiple\n> people for signing, for example), and merged back together after\n> signing. With a generic key-value model, any implementation can remove\n> the duplication even if they don't understand fields that may have\n> been added in future extensions.\n> \n> However, almost the same can be accomplished by using the simpler set\n> model (the file consists of a set of records, with no duplication\n> allowed). This would mean that it would technically be legal to have\n> two partial signatures with the same key for the same input, if a\n> non-deterministic signer is used.\n\nStrongly agree with this.\n\nJust to note, we should probably use varint for the <type> field - this\nallows us, e.g., to create \u201cnamespaces\u201d for future extensions by using\none byte as namespace identifier and one as field identifier.\n\n> \n> On the other hand, this means that certain data currently encoded\n> inside keys can be dropped, reducing the PSBT size. This is\n> particularly true for redeemscripts and witnessscripts, as they can\n> just be computed by the client when deserializing. The two types could\n> even be merged into just \"scripts\" records - as they don't need to be\n> separated based on the way they're looked up (Hash160 for P2SH, SHA256\n> for P2WSH). The same could be done for the BIP32 derivation paths,\n> though this may be expensive, as the client would need to derive all\n> keys before being able to figure out which one(s) it needs.\n\nIt could be nice if the output scripts records would be ordered the same\nas their corresponding outputs. But what if the Creator doesn\u2019t want to\ninclude a script for an output? Perhaps the Script record should have a\n<vout> field to match it to the appropriate output.\n\nAs for input scripts, we suggest that they are per-input and not\nincluded in the global record, see the other thread.\n\n> \n> One exception is the \"transaction\" record, which needs to be unique.\n> That can either be done by adding an exception (\"there can only be one\n> transaction record\"), or by encoding it separately outside the normal\n> records (that may also be useful to make it clear that it is always\n> required).\n\nThis seems to be the case for some fields already - i.e., an input field\nmust have exactly one of Non-witness UTXO or Witness Output. So \u201cadding\nan exception\u201d is probably just a matter of language?\n\n\nWe\u2019d also like to note that the \u201cnumber of inputs\u201d field should be\nmandatory - and as such, possibly also a candidate for outside-record field.\n\n> \n> * Ability for Combiners to verify two PSBT are for the same transaction\n> \n> Clearly two PSBTs for incompatible transactions cannot be combined,\n> and this should not be allowed.\n> \n> It may be easier to enforce this if the \"transaction\" record inside a\n> PSBT was required to be in a canonical form, meaning with empty\n> scriptSigs and witnesses. In order to do so, there could be per-input\n> records for \"finalized scriptSig\" and \"finalized witness\". Actually\n> placing those inside the transaction itself would only be allowed when\n> all inputs are finalized.\n\nAgreed! Also increases clarity, which is desired.\n\n> * Derivation from xpub or fingerprint\n> \n> For BIP32 derivation paths, the spec currently only encodes the 32-bit\n> fingerprint of the parent or master xpub. When the Signer only has a\n> single xprv from which everything is derived, this is obviously\n> sufficient. When there are many xprv, or when they're not available\n> indexed by fingerprint, this may be less convenient for the signer.\n> Furthermore, it violates the \"PSBT contains all information necessary\n> for signing, excluding private keys\" idea - at least if we don't treat\n> the chaincode as part of the private key.\n> \n> For that reason I would suggest that the derivation paths include the\n> full public key and chaincode of the parent or master things are\n> derived from. This does mean that the Creator needs to know the full\n> xpub which things are derived from, rather than just its fingerprint.\n\n\nWe don\u2019t understand the rationale for this idea. Do you see a scenario\nwhere an index on master fingerprint is not available but index by xpubs\nis? In our envisioned use cases at least, indexing private keys by xpubs\n(as opposed to deriving from a BIP32 path) makes no sense.\n\nMaybe this folds into the proposal for generic derivation below, or\nsomething like implementation-specific derivation methods?\n\nbest regards\n\nJan Matejek\nTomas Susanka\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 819 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180619/70447694/attachment-0001.sig>"
            },
            {
                "author": "Achow101",
                "date": "2018-06-21T00:39:05",
                "message_text_only": "Hi,\n\nOn June 15, 2018 4:34 PM, Pieter Wuille via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> \n> Hello all,\n> \n> given some recent work and discussions around BIP 174 (Partially\n> Signed Bitcoin Transaction Format) I'd like to bring up a few ideas.\n> First of all, it's unclear to me to what extent projects have already\n> worked on implementations, and thus to what extent the specification\n> is still subject to change. A response of \"this is way too late\" is\n> perfectly fine.\n\nWhile I agree that the BIP itself should be revised to reflect these suggestions, I fear that it may be too late. I know of a few other developers who have implemented BIP 174 already but have not yet responded to this email.\n\n>     \n> -   Generic key offset derivation\n>\n>     Whenever a BIP32 derivation path does not include any hardened steps,\n>     the entirety of the derivation can be conveyed as \"The private key for\n>     P is equal to the private key for Q plus x\", with P and Q points and x\n>     a scalar. This representation is more flexible (it also supports\n>     pay-to-contract derivations), more efficient, and more compact. The\n>     downside is that it requires the Signer to support such derivation,\n>     which I don't believe any current hardware devices do.\n>     Would it make sense to add this as an additional derivation method?\n\nWhile this is a good idea, I'm not sure that implementers would understand this as it requires knowing the cryptography which makes this possible. As an optional feature, not all wallets would understand it, and those that do could create PSBTs which other wallets do not understand and thus cannot sign even if they have the private keys and actually can sign.\n\n>     \n> -   Hex encoding?\n>     \n>     This is a very minor thing. But presumably there will be some standard\n>     way to store PSBTs as text for copy-pasting - either prescribed by the\n>     spec, or de facto. These structures may become pretty large, so\n>     perhaps it's worth choosing something more compact than hexadecimal -\n>     for example Base64 or even Z85 (https://rfc.zeromq.org/spec:32/Z85/).\n\nAgreed. Are there any encodings that do not have double click breaking characters?\n\n\nOn June 19, 2018 2:38 AM, Jonas Schnelli via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> I think it could be more flexible (generic) in BIP174.\n> It could be just a single child key {32-bit int}, or just a keypath ({32-bit int}]{32-bit int}\u2026) which is very likely sufficient for a HWW to derive the relevant key without the creation of a lookup-window or other \u201emaps\".\n\nThis ignores all of the other times that a BIP32 keypath needs to be provided. It is not only used for multisig, there may be other times that there are multiple derivation paths and master keys due to multiple inputs and such. Adding a field specific to multisig and HWW only seems pointless and redundant to me.\n\nOn June 19, 2018 2:38 AM, Jonas Schnelli via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n> A question to consider is,\n> will there be more per-output data? If yes, it might make sense to have\n> an output section.\n\nI think it is unlikely that there would be anymore per-output data.\n\n> 3) The sighash type 0x03 says the sighash is only a recommendation. That\n>seems rather ambiguous. If the field is specified shouldn't it be binding?\n\nI disagree. It is up to the signer to decide what they wish to sign, not for the creator to specify what to sign. The creator can ask the signer to sign something in a particular way, but it is ultimately up to the signer to decide.\n\n> 4) Is it a good idea to skip records which types we are unaware of? We\n> can't come up with a reasonable example, but intuitively this seems as a\n> potential security issue. We think we should consider  introducing a\n> flag, which would define if the record is \"optional\". In case the signer\n> encounters a record it doesn't recognize and such flag is not set, it\n> aborts the procedure. If we assume the set model we could change the\n> structure to <type><optional flag><length>{data}. We are not keen on\n> this, but we wanted to include this idea to see what you think.\n\nThe idea behind skipping unknown types is to allow forward compatibility. A combiner written now should be able to combine transactions created in the future with new types as combining is really only just merging the maps together.\n\n> In general, the standard is trying to be very space-conservative,\n> however is that really necessary? We would argue for clarity and ease of\n> use over space constraints. We think more straightforward approach is\n> desired, although more space demanding. What are the arguments to make\n> this as small as possible? If we understand correctly, this format is\n> not intended for blockchain nor for persistent storage, so size doesn\u2019t\n> matter nearly as much.\n\nSize is not really a constraint, but we do not want to be unnecessarily large. The PSBT still has to be transmitted to other people. It will likely be used by copy and pasting the string into a text box. Copying and pasting very long strings of text can be annoying and cumbersome. So the goal is to keep the format still relatively clear while avoiding the duplication of data.\n\n\nAndrew"
            },
            {
                "author": "Tomas Susanka",
                "date": "2018-06-21T14:32:07",
                "message_text_only": "Hello,\n\nFirst of all, let me thank you for all the hard work you and others have\nput into this.\n\n\nOn 21.6.2018 02:39, Achow101 via bitcoin-dev wrote:\n> While I agree that the BIP itself should be revised to reflect these suggestions, I fear that it may be too late. I know of a few other developers who have implemented BIP 174 already but have not yet responded to this email.\n\nWe do realize that this discussion should have happened earlier, however\nagreeing on a good standard should be the number one priority for all\nthe parties involved.\n\nThe fact that someone already implemented this is indeed unfortunate,\nbut I don't think we should lower our demands on the standard just\nbecause of a bad timing.\n\n>> A question to consider is,\n>> will there be more per-output data? If yes, it might make sense to have\n>> an output section.\n> I think it is unlikely that there would be anymore per-output data.\n\nHmm, upon further reflection, maybe it's not even worth including *any*\nper-output data, aside from what the original transaction contains.\n\nThe output redeem script is either:\n- unknown, because we have received only an address from the receiver\n- or it is known, because it is ours and in that case it doesn\u2019t make\nsense to include it in PSBT\n\nWe got stuck on the idea of the Creator providing future (output)\nredeem/witness scripts. But that seems to be a minority use case and can\nbe solved efficiently via the same channels that coordinate the PSBT\ncreation. Sorry to change opinions so quickly on this one.\n\n>\n>> 3) The sighash type 0x03 says the sighash is only a recommendation. That\n>> seems rather ambiguous. If the field is specified shouldn't it be binding?\n> I disagree. It is up to the signer to decide what they wish to sign, not for the creator to specify what to sign. The creator can ask the signer to sign something in a particular way, but it is ultimately up to the signer to decide.\n\nThis seems very ambiguous. The Signer always has the option of not\nsigning. *What* to sign is a matter of coordination between the parties;\notherwise, you could make all the fields advisory and let anyone sign\nanything they like?\n\nWe don't understand the usecase for a field that is advisory but not\nbinding. On what basis would you choose to respect or disregard the\nadvisory field? Either one party has a preference, in which case they\nhave to coordinate with the other anyway - or they don't, in which case\nthey simply leave the field out.\n\n> Size is not really a constraint, but we do not want to be unnecessarily large. The PSBT still has to be transmitted to other people. It will likely be used by copy and pasting the string into a text box. Copying and pasting very long strings of text can be annoying and cumbersome. So the goal is to keep the format still relatively clear while avoiding the duplication of data.\n\nI agree. Just to put some numbers on this: if we expect a 5-part\nderivation path, and add the master key fingerprint, that is 4 + 5*4 =\n24 bytes (~32 base64 letters) per input and signer. I'd argue this is\nnot significant.\nIf we used full xpub, per Pieter's suggestion, that would grow to 32 +\n32 + 5*4 = 84 bytes (~112 letters) per input/signer, which is quite a lot.\n\nOn the other hand, keeping the BIP32 paths per-input means that we don't\nneed to include the public key (as in the lookup key), so that's 32\nbytes down per path. In general, all the keys can be fully reconstructed\nfrom their values:\n\nredeem script key = hash160(value)\nwitness script key = sha256(value)\nbip32 key = derive(value)\n\nThe one exception is a partial signature. But even in that case we\nexpect that a given public key will always correspond to the same\nsignature, so we can act as if the public key is not part of the \"key\".\nIn other words, we can move the public key to the value part of the record.\n\nThis holds true unless there's some non-deterministic signing scheme,\n*and* multiple Signers sign with the same public key, which is what\nPieter was alluding to on Twitter\n(https://twitter.com/pwuille/status/1002627925110185984). Still, I would\nargue (as he also suggested) that keeping the format more complex to\nsupport this particular use case is probably not worth it.\n\nAlso, we can mostly ignore deduplication of witness/redeem scripts.\nThese still need to be included in the resulting transaction, duplicated\nif necessary, so I think counting their repetition against the size of\nPSBT isn't worth it.\n\n\nBest,\nTomas"
            },
            {
                "author": "Greg Sanders",
                "date": "2018-06-21T15:40:04",
                "message_text_only": ">Hmm, upon further reflection, maybe it's not even worth including *any*\nper-output data, aside from what the original transaction contains.\n\n>The output redeem script is either:\n- unknown, because we have received only an address from the receiver\n- or it is known, because it is ours and in that case it doesn\u2019t make\nsense to include it in PSBT\n\nSigners are an extremely heterogeneous bunch. A signer may need to\nintrospect on the script, such as \"this is a 2-of-3,\nand I'm one of the keys\". Even in basic p2pkh settings not adding any\noutput information rules out things like change\ndetection on any conceivable hardware wallet, or even simple software\nwallets that don't carry significant state.\n\nOn Thu, Jun 21, 2018 at 10:35 AM Tomas Susanka via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hello,\n>\n> First of all, let me thank you for all the hard work you and others have\n> put into this.\n>\n>\n> On 21.6.2018 02:39, Achow101 via bitcoin-dev wrote:\n> > While I agree that the BIP itself should be revised to reflect these\n> suggestions, I fear that it may be too late. I know of a few other\n> developers who have implemented BIP 174 already but have not yet responded\n> to this email.\n>\n> We do realize that this discussion should have happened earlier, however\n> agreeing on a good standard should be the number one priority for all\n> the parties involved.\n>\n> The fact that someone already implemented this is indeed unfortunate,\n> but I don't think we should lower our demands on the standard just\n> because of a bad timing.\n>\n> >> A question to consider is,\n> >> will there be more per-output data? If yes, it might make sense to have\n> >> an output section.\n> > I think it is unlikely that there would be anymore per-output data.\n>\n> Hmm, upon further reflection, maybe it's not even worth including *any*\n> per-output data, aside from what the original transaction contains.\n>\n> The output redeem script is either:\n> - unknown, because we have received only an address from the receiver\n> - or it is known, because it is ours and in that case it doesn\u2019t make\n> sense to include it in PSBT\n>\n> We got stuck on the idea of the Creator providing future (output)\n> redeem/witness scripts. But that seems to be a minority use case and can\n> be solved efficiently via the same channels that coordinate the PSBT\n> creation. Sorry to change opinions so quickly on this one.\n>\n> >\n> >> 3) The sighash type 0x03 says the sighash is only a recommendation. That\n> >> seems rather ambiguous. If the field is specified shouldn't it be\n> binding?\n> > I disagree. It is up to the signer to decide what they wish to sign, not\n> for the creator to specify what to sign. The creator can ask the signer to\n> sign something in a particular way, but it is ultimately up to the signer\n> to decide.\n>\n> This seems very ambiguous. The Signer always has the option of not\n> signing. *What* to sign is a matter of coordination between the parties;\n> otherwise, you could make all the fields advisory and let anyone sign\n> anything they like?\n>\n> We don't understand the usecase for a field that is advisory but not\n> binding. On what basis would you choose to respect or disregard the\n> advisory field? Either one party has a preference, in which case they\n> have to coordinate with the other anyway - or they don't, in which case\n> they simply leave the field out.\n>\n> > Size is not really a constraint, but we do not want to be unnecessarily\n> large. The PSBT still has to be transmitted to other people. It will likely\n> be used by copy and pasting the string into a text box. Copying and pasting\n> very long strings of text can be annoying and cumbersome. So the goal is to\n> keep the format still relatively clear while avoiding the duplication of\n> data.\n>\n> I agree. Just to put some numbers on this: if we expect a 5-part\n> derivation path, and add the master key fingerprint, that is 4 + 5*4 =\n> 24 bytes (~32 base64 letters) per input and signer. I'd argue this is\n> not significant.\n> If we used full xpub, per Pieter's suggestion, that would grow to 32 +\n> 32 + 5*4 = 84 bytes (~112 letters) per input/signer, which is quite a lot.\n>\n> On the other hand, keeping the BIP32 paths per-input means that we don't\n> need to include the public key (as in the lookup key), so that's 32\n> bytes down per path. In general, all the keys can be fully reconstructed\n> from their values:\n>\n> redeem script key = hash160(value)\n> witness script key = sha256(value)\n> bip32 key = derive(value)\n>\n> The one exception is a partial signature. But even in that case we\n> expect that a given public key will always correspond to the same\n> signature, so we can act as if the public key is not part of the \"key\".\n> In other words, we can move the public key to the value part of the record.\n>\n> This holds true unless there's some non-deterministic signing scheme,\n> *and* multiple Signers sign with the same public key, which is what\n> Pieter was alluding to on Twitter\n> (https://twitter.com/pwuille/status/1002627925110185984). Still, I would\n> argue (as he also suggested) that keeping the format more complex to\n> support this particular use case is probably not worth it.\n>\n> Also, we can mostly ignore deduplication of witness/redeem scripts.\n> These still need to be included in the resulting transaction, duplicated\n> if necessary, so I think counting their repetition against the size of\n> PSBT isn't worth it.\n>\n>\n> Best,\n> Tomas\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180621/b752efa0/attachment.html>"
            },
            {
                "author": "Peter D. Gray",
                "date": "2018-06-21T19:56:54",
                "message_text_only": "On Thu, Jun 21, 2018 at 04:32:07PM +0200, Tomas Susanka wrote:\n...\n> First of all, let me thank you for all the hard work you and others have\n> put into this.\n> \n> On 21.6.2018 02:39, Achow101 via bitcoin-dev wrote:\n> > While I agree that the BIP itself should be revised to reflect these suggestions, I fear that it may be too late. I know of a few other developers who have implemented BIP 174 already but have not yet responded to this email.\n> \n> We do realize that this discussion should have happened earlier, however\n> agreeing on a good standard should be the number one priority for all\n> the parties involved.\n> \n> The fact that someone already implemented this is indeed unfortunate,\n> but I don't think we should lower our demands on the standard just\n> because of a bad timing.\n\nWe all want a \"good\" standard but we have that already, IMHO.\n\nWhat you are really saying is you want a \"better\" standard, and I\nwould argue that's our enemy right now. It's just too easy to propose a\nfew tweaks, with \"wouldn't it be better if...\" \n\nI feel strongly we are entering the \"design by committee\" territory with BIP174.\n\nI have personally implemented this spec on an embedded micro, as\nthe signer and finalizer roles, and written multiple parsers for\nit as well. There is nothing wrong with it, and it perfectly meets\nmy needs as a hardware wallet.\n\nSo, there is a good proposal already spec'ed and implemented by\nmultiple parties. Andrew has been very patiently shepherding the PR\nfor over six months already.\n\nPSBT is something we need, and has been missing from the ecosystem\nfor a long time. Let's push this out and start talking about future\nversions after we learn from this one.\n\n---\nPeter D. Gray  ||  Founder, Coinkite  ||  Twitter: @dochex  ||  GPG: A3A31BAD 5A2A5B10\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180621/8d63430a/attachment.sig>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2018-06-21T21:39:20",
                "message_text_only": "On Thu, Jun 21, 2018 at 7:56 PM, Peter D. Gray via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> PSBT is something we need, and has been missing from the ecosystem\n> for a long time. Let's push this out and start talking about future\n> versions after we learn from this one.\n\nWhen you implement proposals that have little to no public discussion\nabout them you take the risk that your work needs to be changed when\nother people do actually begin reviewing the work.  It is incredibly\ndemoralizing as a designer and a reviewer to have proposals that were\nput out for discussion show up implemented in things with these vested\ninterests then insisting that they not be refined further.  I think\nkind of handling is toxic to performing development in public.\n\nAlthough it's silly enough that it won't happen, I think our industry\nwould be better off if there was a social norm that anytime someone\ninsists an unfinished proposal shouldn't be changed because they\nalready implemented it that the spec should _always_ be changed, in\norder to discourage further instances of that conduct."
            },
            {
                "author": "Pieter Wuille",
                "date": "2018-06-22T19:10:15",
                "message_text_only": "On Thu, Jun 21, 2018 at 12:56 PM, Peter D. Gray via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> I have personally implemented this spec on an embedded micro, as\n> the signer and finalizer roles, and written multiple parsers for\n> it as well. There is nothing wrong with it, and it perfectly meets\n> my needs as a hardware wallet.\n\nThis is awesome to hear. We need to hear from people who have comments\nor issues they encounter while implementing, but also cases where\nthings are fine as is.\n\n> So, there is a good proposal already spec'ed and implemented by\n> multiple parties. Andrew has been very patiently shepherding the PR\n> for over six months already.\n>\n> PSBT is something we need, and has been missing from the ecosystem\n> for a long time. Let's push this out and start talking about future\n> versions after we learn from this one.\n\nI understand you find the suggestions being brought up in this thread\nto be bikeshedding over details, and I certainly agree that \"changing\nX will gratuitously cause us more work\" is a good reason not to make\nbreaking changes to minutiae. However, at least abstractly speaking,\nit would be highly unfortunate if the fact that someone implemented a\ndraft specification results in a vested interest against changes which\nmay materially improve the standard.\n\nIn practice, the process surrounding BIPs' production readiness is not\nnearly as clear as it could be, and there are plenty of BIPs actually\ndeployed in production which are still marked as draft. So in reality,\ntruth is that this thread is \"late\", and also why I started the\ndiscussion by asking what the state of implementations was. As a\nresult, the discussion should be \"which changes are worth the hassle\",\nand not \"what other ideas can we throw in\" - and some of the things\nbrought up are certainly the latter.\n\nSo to get back to the question what changes are worth the hassle - I\nbelieve the per-input derivation paths suggested by matejcik may be\none. As is written right now, I believe BIP174 requires Signers to\npretty much always parse or template match the scripts involved. This\nmeans it is relatively hard to implement a Signer which is compatible\nwith many types of scripts - including ones that haven't been\nconsidered yet. However, if derivation paths are per-input, a signer\ncan just produce partial signatures for all keys it has the master\nfor. As long as the Finalizer understands the script type, this would\nmean that Signers will work with any script. My guess is that this\nwould be especially relevant to devices where the Signer\nimplementation is hard to change, like when it is implemented in a\nhardware signer directly.\n\nWhat do you think?\n\nCheers,\n\n-- \nPieter"
            },
            {
                "author": "Achow101",
                "date": "2018-06-22T22:28:33",
                "message_text_only": "Hi all,\n\nAfter reading the comments here about BIP 174, I would like to propose the following changes:\n\n- Moving redeemScripts, witnessScripts, and BIP 32 derivation paths to per-input and per-output data\n\nI think that by moving these three fields into input and output specific maps, the format will be\neasier to read and simpler for signers to parse. Instead of having to be able to parse entire\nscripts and extract pubkeys, the signer can simply look at which pubkeys are provided in the inputs\nand sign the input based upon the presence of a pubkey for which the signer has a privkey.\n\nA neat trick that fits well with this model is that a plain pubkey (one that is not part of a BIP 32\nderivation) can still be put in a BIP 32 derivation path field where the value is just the fingerprint\nof the pubkey itself. This would indicate that no derivation needs to be done from the master key, and\nthe master key is just the specified key itself.\n\nAdditionally, by having the redeemScript and witnessScript readily available in the input, signers\ndo not need to construct a map to find a redeemScript or witnessScript and can instead just look\ndirectly in the input data. There is also no need to include the hashes of these scripts, so the key\nis just the type. This also allows us to enforce the requirement for only one redeemScript and one\nwitnessScript per input easily by continuing to follow the generic rule of unique keys.\n\nBy using input specific and output specific fields, there is no need for the input index and the input\ncount types as all inputs will be accounted for.\n\n- Finalized scriptSig and scriptWitness fields\n\nTo determine whether two PSBTs are the same, we can compare the unsigned transaction. To ensure that the\nunsigned transactions are the same for two PSBTs with data for the same tx, we cannot put scriptSigs or\nscriptWitnesses into it. Thus for each input, two new fields have been added to store the finalized scriptSig\nand finalized scriptWitness.\n\n- Mandatory sighash\n\nThe sighash type field will be changed from a recommendation to a requirement. Signatures will need to \nuse the specified sighash type for that input. If a Signer cannot sign for a particular sighash type, it\nmust not add a partial signature.\n\n- Encoding\n\nI have decided that PSBTs should either be in binary or encoded as a Base64 string. For the latter, several\nBitcoin clients already support Base64 encoding of data (for signed messages) so this will not add any extra\ndependencies like Z85 would.\n\n\nA draft of the revised BIP can be found here: https://github.com/achow101/bips/blob/bip174-rev/bip-0174.mediawiki\nIf these changes are satisfactory, I will open a PR to the BIPs repo to update the BIP tomorrow. I will also\ncreate test vectors and update the implementation PR'ed to Core.\n\nAndrew"
            },
            {
                "author": "William Casarin",
                "date": "2018-06-23T17:00:18",
                "message_text_only": "Achow101 via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> writes:\n\n> I have decided that PSBTs should either be in binary or encoded as a\n> Base64 string. For the latter, several Bitcoin clients already support\n> Base64 encoding of data (for signed messages) so this will not add any\n> extra dependencies like Z85 would.\n\nSince we're still considering the encoding, I wonder if it would be a\ngood idea to have a human-readible part like lightning invoices[1]?\n\nlightning invoice \n\n  vvvvvv\n  lnbc1pvjluezpp5qqqsyqcyq5rqwzqfqqqsyqcyq5rqwzqfqqqsyqcyq5rqwzqfqypqdpl2pkx2ctnv5sxxmmwwd5kgetjypeh2ursdae8g6twvus8g6rfwvs8qun0dfjkxaq8rkx3yf5tcsyz3d73gafnh3cax9rn449d9p5uxz9ezhhypd0elx87sjle52x86fux2ypatgddc6k63n7erqz25le42c4u4ecky03ylcqca784w\n\npsbt?\n\n  vvvv\n  psbtcHNidP8BAHwCAAAAAi6MfY03xCfgYOwALsHCvDAZb8L3XWqIRMvANlHAgUMKAQAAAAD/////lqBODMY283eTPj2TrMxif6rNvNtaliTfG0kL0EXyTSwAAAAAAP////8B4CvlDgAAAAAXqRS1O7DcHbjI2APj4594TULkc3/6DYcAAAAAFQEgNzbDwGBTiW1wQc6PW64992zEkUdSIQPIcnzjXxyT6wviFAbumpI8iSGf6cnoUEyDFKaiLRKVwCEDx03HEMQH19tuBB7iEtmFzSgm2T+AbtRJErmh2mkcl3NSrhUB87qKEg2WCuB9Hb5vDDf7TJJtdtUiACCo9ERnvxcdUUmRU+AcC9YpEQn8OL0hs8MiTJ3GtXWQ3yECqPREZ78XHVFJkVPgHAvWKREJ/Di9IbPDIkydxrV1kN9HUiEC6A3sMdFnhlwWhenXqSkeZqTqIsZc/uMkKJoWZ8zaO4chAljLvDyylai+usIzqtx3c5eIBJk3mL5TkKtET23UxTJ+Uq4AAQD9/wACAAAAAYst0vc10KkzivlkAqipHkhBzT/tiCNi5zKfsE8f9lMlAAAAAGpHMEQCIHe+3+qZEMm6TgDeyUHazpdPi0c0mZLF1DEsHPV5bM5VAiBhZOa//3rBFZAGTKVxWDcJM3yKOJc9sucPTp2Ts7zOHQEhAy1kRHRZeE43yy3aNmxpetu9yKrirW23TtLa3jnXWIL6/v///wOCtCoEAAAAABl2qRTaUzZI/TOdV5d5DmuxZn2ehv37aIisgPD6AgAAAAAXqRQgNzbDwGBTiW1wQc6PW64992zEkYcAtMQEAAAAABepFLU7sNwduMjYA+Pjn3hNQuRzf/oNh54vEwAAAQEgAMLrCwAAAAAXqRTzuooSDZYK4H0dvm8MN/tMkm121YcA\n\nThen perhaps you could drop the magic code as well?\n\nAlso we could do a base encoding that excludes + and / characters, such\nas base62 (gmp-style). It's easier to copy/paste (double clicking a\nstring stops at / or + in base64 encodings).\n\nexample human readible part + base62\n\n  psbtWzecLNK5WdwZyceXUYVo0E1TqgLWF0jRXsrLnsuGifjOJl5QHEaaeQbfHfzvbYH85uhUUvpfNc2RYnBqM9E4UqOjzRzDg4QGypL2bxoEsUmmAqOC7nRoN8SuftftuFBI9YabFjVZC9ykOIuJaMzanmKHxnhuh55Hh4mxpwDsvkGFHEHzYHJfkIAiaCEmpdxVBD3qvXNlspDwLKkssUlmDjH7X9zCGyTBE90XvwNdrwM63Q4T45GQbe3c4oQlzCnJuHf5FLnH2oR70hgxIoM01af35iJpZRZAGITtdnKvm9PbH3huEf7TXTzXuNLB9XFh50UlGvnPKcIfFHvgzTSqeN3NmXdzPzsNSRY83BnfHFtTIZnczIyDi5oWsi0sL8f5ABUqGHD61GXDXJGcsqWOjiW6zjhz1L2IKN6OdSVGBFf7C7gH2EYvkWJcKYcJ34gBGsLuXYCU8vzauxEYXXlOXohQ1qKj6Eb0DqOyroRD57uw9fG1e3ueCGlBKmyTI4z4Q1JQXSuLYzBGPlBpVuSZmDBUe28b1EVetJbP9rQ5r6aKsuNX1GToXq1KY5Xh5hsMixJ2o8kG8IBKQSZBRaxjiVEQDWoN3FED869vNHiQtgSLjbqQFZRJuDK0UTMfQCtcg7NdYulPxbUYFNF5Ug6wCvWrTpX1SdbDgGOqZel4ibM18fk9uSIIVDFK9XbenLH3NBOKj0hkxgvrbICZMWBc8GW78TLV4acO75tFBt4a4ziH0wztWGbEEGIAZTDaGmJ51omiRNUVfIX6fO9CeN3Nx3c7Ja2hAjMqQcYcKHEK8tFtLuUdR2jqLuGXOPV4gsqJb8TdkKGEZaA0RRqwHm6HG86OCOEGYqptt43iljv52qkh4znyekJI2mYPItcaw11tsxHaRQcs8Us9Ehlbf6ngmIW6tlo\n\nbase64: 920 bytes\nbase62: 927 bytes\n\nCheers,\n\n\n[1] https://github.com/lightningnetwork/lightning-rfc/blob/master/11-payment-encoding.md#human-readable-part\n\n\n--\nhttps://jb55.com"
            },
            {
                "author": "Andrew Chow",
                "date": "2018-06-23T20:33:11",
                "message_text_only": "On 06/23/2018 10:00 AM, William Casarin wrote:\n> Since we're still considering the encoding, I wonder if it would be a\n> good idea to have a human-readible part like lightning invoices[1]?\nI don't think that is necessary.\n> Then perhaps you could drop the magic code as well?\nThe magic is still necessary for the binary format in order to prevent\nnormal transaction deserializers from accidentally deserializing a psbt.\n> Also we could do a base encoding that excludes + and / characters, such\n> as base62 (gmp-style). It's easier to copy/paste (double clicking a\n> string stops at / or + in base64 encodings).\nWhile that would be ideal, I think it is better to use an encoding that\nmost wallets already support. Most wallets already have Base64 decoding\navailable so that they can decode signed messages which also use Base64\nencoding. I think it is unnecessary to introduce another encoding.\n\n\nOn 06/23/2018 11:27 AM, Peter D. Gray wrote:\n> Personally, I don't think you should spec an encoding. It should be binary only and hex for developers and JSON interfaces. My thinking is that PSBT's are not user-visible things. They have a short lifetime and are nothing something that is \"stored\" or referenced much later. Hex is good enough and has no downsides as an excoding except for density.\nI think what will end up happening though is that, at least in the\nbeginning, PSBTs will primarily be strings that people end up copy and\npasting. Since a PSBT can get pretty large, the strings are rather\ncumbersome to move around, especially as hex. At least with Base64 the\nstrings will be smaller.\n> On the other hand, suggesting a filename extension (probably .PSBT?) and a mime-type to match, are helpful since wallets and such will want to register with their operating systems to become handlers of those mimetypes. Really that's a lot more important for interoperability at this point, than an encoding.\nAgreed. I will include those in the BIP.\n> Looking forward to test vectors, and I might have more to say once my code can handle them (again).\n>\n> Feedback on the BIP as it stands now: \n>\n> - Appendix A needs an update, and I suggest defining symbols (PK_PARTIAL_SIG == 0x02) for the numeric key values. This helps implementers as we don't all define our own symbols and/or use numeric constants in our code.\nOkay.\n> - Those tables are just not working. Might want to reformat as descriptive lists, point form, or generally anything else... sorry.\nI will try my best to fix that. Mediawiki sucks...\n\nAndrew"
            },
            {
                "author": "Andrea",
                "date": "2018-06-24T08:19:00",
                "message_text_only": "Hi, \n\nI think in the revised spec we can remove completely the \"global types\" as a map or even as typed record. Since there is only one type (the transaction) and it's compulsory to have one (and only one) we could just drop the definition of global type and the key associated with it, simply after the header + separator there must be a transaction.\u200b\u200b Having read all the discussion i also agree having per-input key derivation and per-output data is a lot more handy for signers, no special feeling regarding the encoding.Looking forward for the test vectors and the new spec.\n\nCheers, Andrea.\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n\nOn June 23, 2018 10:33 PM, Andrew Chow via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> \u200b\u200b\n> \n> On 06/23/2018 10:00 AM, William Casarin wrote:\n> \n> > Since we're still considering the encoding, I wonder if it would be a\n> > \n> > good idea to have a human-readible part like lightning invoices[1]?\n> \n> I don't think that is necessary.\n> \n> > Then perhaps you could drop the magic code as well?\n> \n> The magic is still necessary for the binary format in order to prevent\n> \n> normal transaction deserializers from accidentally deserializing a psbt.\n> \n> > Also we could do a base encoding that excludes + and / characters, such\n> > \n> > as base62 (gmp-style). It's easier to copy/paste (double clicking a\n> > \n> > string stops at / or + in base64 encodings).\n> \n> While that would be ideal, I think it is better to use an encoding that\n> \n> most wallets already support. Most wallets already have Base64 decoding\n> \n> available so that they can decode signed messages which also use Base64\n> \n> encoding. I think it is unnecessary to introduce another encoding.\n> \n> On 06/23/2018 11:27 AM, Peter D. Gray wrote:\n> \n> > Personally, I don't think you should spec an encoding. It should be binary only and hex for developers and JSON interfaces. My thinking is that PSBT's are not user-visible things. They have a short lifetime and are nothing something that is \"stored\" or referenced much later. Hex is good enough and has no downsides as an excoding except for density.\n> \n> I think what will end up happening though is that, at least in the\n> \n> beginning, PSBTs will primarily be strings that people end up copy and\n> \n> pasting. Since a PSBT can get pretty large, the strings are rather\n> \n> cumbersome to move around, especially as hex. At least with Base64 the\n> \n> strings will be smaller.\n> \n> > On the other hand, suggesting a filename extension (probably .PSBT?) and a mime-type to match, are helpful since wallets and such will want to register with their operating systems to become handlers of those mimetypes. Really that's a lot more important for interoperability at this point, than an encoding.\n> \n> Agreed. I will include those in the BIP.\n> \n> > Looking forward to test vectors, and I might have more to say once my code can handle them (again).\n> > \n> > Feedback on the BIP as it stands now:\n> > \n> > -   Appendix A needs an update, and I suggest defining symbols (PK_PARTIAL_SIG == 0x02) for the numeric key values. This helps implementers as we don't all define our own symbols and/or use numeric constants in our code.\n> \n> Okay.\n> \n> > -   Those tables are just not working. Might want to reformat as descriptive lists, point form, or generally anything else... sorry.\n> \n> I will try my best to fix that. Mediawiki sucks...\n> \n> Andrew\n> \n> bitcoin-dev mailing list\n> \n> bitcoin-dev at lists.linuxfoundation.org\n> \n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Peter D. Gray",
                "date": "2018-06-23T18:27:15",
                "message_text_only": "On Fri, Jun 22, 2018 at 06:28:33PM -0400, Achow101 wrote:\n> After reading the comments here about BIP 174, I would like to propose the following changes:\n> \n> - Moving redeemScripts, witnessScripts, and BIP 32 derivation paths to per-input and per-output data\n...\n\nI like this. I agree it's making things easier and it's more flexible.\n\n> - Finalized scriptSig and scriptWitness fields\n> \n> To determine whether two PSBTs are the same, we can compare the unsigned transaction. To ensure that the\n> unsigned transactions are the same for two PSBTs with data for the same tx, we cannot put scriptSigs or\n> scriptWitnesses into it. Thus for each input, two new fields have been added to store the finalized scriptSig\n> and finalized scriptWitness.\n...\n\nTo be honest, I don't understand the reasons/implications of this change, but it seems harmless.\n\n> - Mandatory sighash\n...\n\nGood improvement.\n\n> - Encoding\n> \n> I have decided that PSBTs should either be in binary or encoded as a Base64 string. For the latter, several\n> Bitcoin clients already support Base64 encoding of data (for signed messages) so this will not add any extra\n> dependencies like Z85 would.\n...\n\nPersonally, I don't think you should spec an encoding. It should be binary only and hex for developers and JSON interfaces. My thinking is that PSBT's are not user-visible things. They have a short lifetime and are nothing something that is \"stored\" or referenced much later. Hex is good enough and has no downsides as an excoding except for density.\n\nOn the other hand, suggesting a filename extension (probably .PSBT?) and a mime-type to match, are helpful since wallets and such will want to register with their operating systems to become handlers of those mimetypes. Really that's a lot more important for interoperability at this point, than an encoding.\n\n> A draft of the revised BIP can be found here: https://github.com/achow101/bips/blob/bip174-rev/bip-0174.mediawiki\n> If these changes are satisfactory, I will open a PR to the BIPs repo to update the BIP tomorrow. I will also\n> create test vectors and update the implementation PR'ed to Core.\n...\n\nLooking forward to test vectors, and I might have more to say once my code can handle them (again).\n\nFeedback on the BIP as it stands now: \n\n- Appendix A needs an update, and I suggest defining symbols (PK_PARTIAL_SIG == 0x02) for the numeric key values. This helps implementers as we don't all define our own symbols and/or use numeric constants in our code.\n\n- Those tables are just not working. Might want to reformat as descriptive lists, point form, or generally anything else... sorry.\n\n> Andrew\n> _______________________________________________\n\n---\nPeter D. Gray  ||  Founder, Coinkite  ||  Twitter: @dochex  ||  GPG: A3A31BAD 5A2A5B10\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 496 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180623/9c0b1846/attachment.sig>"
            },
            {
                "author": "Tomas Susanka",
                "date": "2018-06-25T19:47:59",
                "message_text_only": "Hi,\n\nthis is great.\n\nOn 23.6.2018 00:28, Achow101 via bitcoin-dev wrote:\n\n> Hi all,\n>\n> After reading the comments here about BIP 174, I would like to propose the following changes:\n\nFrom my perspective those are exactly the points I have felt strongly\nabout. I still think \"typed records\" would be a better choice, but it's\nsomething I'm willing to compromise on. As I'm looking at the draft, we\ncurrently have 13 records and only 3 of them have keys... Matejcik was a\nbit keener on this, so we'll try to discuss this more during the week\nand we also look at the draft more carefully to see if we can come up\nwith some nit-picks.\n\n> - Encoding\n>\n> I have decided that PSBTs should either be in binary or encoded as a Base64 string. For the latter, several\n> Bitcoin clients already support Base64 encoding of data (for signed messages) so this will not add any extra\n> dependencies like Z85 would.\n\nI agree. If we're arguing for not using protobuf, because it is a\ndependency, we shouldn't add dependency for some lesser-known encoding\nformat.\n\nAs was partially brought up by William, shouldn't we consider using\nbech32? It doesn't break on double-click and it is a dependency for\nnative Segwit addresses anyway, so wallets might already support it or\nthey will at some point. But we should probably run some numbers on this\nfirst, since bech32 will obviously be larger than base64.\n\n\nOn 24.6.2018 10:28, Andrew Chow via bitcoin-dev wrote:\n\n> I disagree with the idea that global types can be removed. Firstly, it\n> is less of a breaking change to leave it there than to remove it\n> entirely. Secondly, there may be a point in the future where global\n> types would be useful/necessary. By having it still be there, we allow\n> for future extensibility.\n\nI agree. It doesn't hurt if the global section stays and it is more\nforward-looking.\n\n\nBest,\nTomas\n\nPS: This email didn't get through at first, so I hope this isn't a repost.\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180625/b930777e/attachment.html>"
            },
            {
                "author": "Jonas Schnelli",
                "date": "2018-06-25T20:10:12",
                "message_text_only": "Hi\n> As was partially brought up by William, shouldn't we consider using\n> bech32? It doesn't break on double-click and it is a dependency for\n> native Segwit addresses anyway, so wallets might already support it or\n> they will at some point. But we should probably run some numbers on this\n> first, since bech32 will obviously be larger than base64.\nI don\u2019t think bech32 is a fit here.\nBech32 is a BCH where the error detecting properties are optimised for 1023 chars max and in the special case of the Bech32 BCH, error detection of 4 chars are guaranteed with a max length of 90 chars.\n\n/jonas\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180625/88470c4c/attachment.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: Message signed with OpenPGP\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180625/88470c4c/attachment.sig>"
            },
            {
                "author": "Achow101",
                "date": "2018-06-25T20:30:28",
                "message_text_only": "Hi,\n\nOn June 25, 2018 12:47 PM, Tomas Susanka via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> From my perspective those are exactly the points I have felt strongly\n> about. I still think \"typed records\" would be a better choice, but it's\n> something I'm willing to compromise on. As I'm looking at the draft, we\n> currently have 13 records and only 3 of them have keys... Matejcik was a\n> bit keener on this, so we'll try to discuss this more during the week\n> and we also look at the draft more carefully to see if we can come up\n> with some nit-picks.\n\nSo there are a few reasons for not using typed records. Firstly, it is less of a breaking change to retain the key-value map model.\n\nSecondly, it is easier to enforce uniqueness for certain things. For example, in each input, we only want to have one redeemScript and one witnessScript. With a typed records set, we would have to say that only on record of each type is allowed, which means that combiners need to understand types and be able to partially parse the records. However with a key-value model, we can more generically say that every key-value pair must have a unique key which means that combiners do not need to know anything about types and just needs to enforce key uniqueness. Since the type is the only thing in the key for redeemScripts and witnessScripts, this uniqueness automatically applies to this, as well as for other key-value pairs.\n\nLastly, the typed records model does not save a lot of space in a transaction. Each record has at most one extra byte in the key-value model, with records that must also have keys having no space savings. The data inside each key-value pair far exceeds one byte, so on additional byte per key-value pair isn't all that big of a deal, IMO.\n\nAndrew\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180625/a2dc6274/attachment.html>"
            },
            {
                "author": "matejcik",
                "date": "2018-06-26T15:33:14",
                "message_text_only": "hello,\n\nin general, I agree with my colleague Tomas, the proposed changes are\ngood and achieve the most important things that we wanted. We'll review\nthe proposal in more detail later.\n\nFor now a couple minor things I have run into:\n\n- valid test vector 2 (\"one P2PKH input and one P2SH-P2WPKH input\")\nseems broken, at least its hex version; a delimiter seems to be missing,\nmisplaced or corrupted\n- at least the first signing vector is not updated, but you probably\nknow that\n- BIP32 derivation fields don't specify size of the \"master public key\",\nwhich would make it un-parsable :)\n- \"Transaction format is specified as follows\" and its table need to be\nupdated.\n\n\nI'm still going to argue against the key-value model though.\n\nIt's true that this is not significant in terms of space. But I'm more\nconcerned about human readability, i.e., confusing future implementers.\nAt this point, the key-value model is there \"for historical reasons\",\nexcept these aren't valid even before finalizing the format. The\noriginal rationale for using key-values seems to be gone (no key-based\nlookups are necessary). As for combining and deduplication, whether key\ndata is present or not is now purely a stand-in for a \"repeatable\" flag.\nWe could just as easily say, e.g., that the high bit of \"type\" specifies\nwhether this record can be repeated.\n\n(Moreover, as I wrote previously, the Combiner seems like a weirdly\nplaced role. I still don't see its significance and why is it important\nto correctly combine PSBTs by agents that don't understand them. If you\nhave a usecase in mind, please explain.\nISTM a Combiner could just as well combine based on whole-record\nuniqueness, and leave the duplicate detection to the Finalizer. In case\nthe incoming PSBTs have incompatible unique fields, the Combiner would\nhave to fail anyway, so the Finalizer might as well do it. Perhaps it\nwould be good to leave out the Combiner role entirely?)\n\nThere's two remaining types where key data is used: BIP32 derivations\nand partial signatures. In case of BIP32 derivation, the key data is\nredundant ( pubkey = derive(value) ), so I'd argue we should leave that\nout and save space. In case of partial signatures, it's simple enough to\nmake the pubkey part of the value.\n\nRe breaking change, we are proposing breaking changes anyway, existing\ncode *will* need to be touched, and given that this is a hand-parsed\nformat, changing `parse_keyvalue` to `parse_record` seems like a small\nmatter?\n\n---\n\nAt this point I'm obliged to again argue for using protobuf.\n\nThing is: BIP174 *is basically protobuf* (v2) as it stands. If I'm\nsuccesful in convincing you to switch to a record set model, it's going\nto be \"protobuf with different varint\".\n\nI mean this very seriously: (the relevant subset of) protobuf is a set\nof records in the following format:\n<record type><varint field length><field data>\nRecord types can repeat, the schema specifies whether a field is\nrepeatable or not - if it's not, the last parsed value is used.\n\nBIP174 is a ad-hoc format, simple to parse by hand; but that results in\n_having to_ parse it by hand. In contrast, protobuf has a huge\ncollection of implementations that will do the job of sorting record\ntypes into relevant struct fields, proper delimiting of records, etc.\n...while at the same time, implementing \"protobuf-based-BIP174\" by hand\nis roughly equally difficult as implementing the current BIP174.\n\nN.B., it's possible to write a parser for protobuf-BIP174 without\nneeding a general protobuf library. Protobuf formats are designed with\nforwards- and backwards- compatibility in mind, so having a hand-written\nimplementation should not lead to incompatibilities.\n\nI did an experiment with this and other variants of the BIP174 format.\nYou can see them here:\n[1] https://github.com/matejcik/bip174-playground\nsee in particular:\n[2] https://github.com/matejcik/bip174-playground/blob/master/bip174.proto\n\nThe tool at [1] does size comparisons. On the test vectors, protobuf is\nslightly smaller than key-value, and roughly equal to record-set, losing\nout a little when BIP32 fields are used.\n(I'm also leaving out key-data for BIP32 fields.)\n\nThere's some technical points to consider about protobuf, too:\n\n- I decided to structure the message as a single \"PSBT\" type, where\n\"InputType\" and \"OutputType\" are repeatable embedded fields. This seemed\nbetter in terms of extensibility - we could add more sections in the\nfuture. But the drawback is that you need to know the size of\nInput/OutputType record in advance.\nThe other option is sending the messages separated by NUL bytes, same as\nnow, in which case you don't need to specify size.\n\n- in InputType, i'm using \"uint32\" for sighash. This type is not\nlength-delimited, so non-protobuf consumers would have to understand it\nspecially. We could also declare that all fields must be\nlength-delimited[1] and implement sighash as a separate message type\nwith one field.\n\n- non-protobuf consumers will also need to understand both protobuf\nvarint and bitcoin compact uint, which is a little ugly\n\nbest regards\nmatejcik\n\n\n[1] https://developers.google.com/protocol-buffers/docs/encoding#structure\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 819 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180626/a1c4e6e5/attachment.sig>"
            },
            {
                "author": "William Casarin",
                "date": "2018-06-26T16:58:18",
                "message_text_only": "matejcik via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> writes:\n\n> BIP174 is a ad-hoc format, simple to parse by hand; but that results\n> in _having to_ parse it by hand. In contrast, protobuf has a huge\n> collection of implementations that will do the job of sorting record\n> types into relevant struct fields, proper delimiting of records, etc.\n\nseems a bit overkill for how simple the format is, and pulling in a\nlarge dependency just for this is a bit silly. Although making it\nprotobuf-compatible is an interesting idea, but I fear would be more\nwork than is worth? I haven't looked closed enough at the protobuf\nencoding to be sure.\n\n> ...while at the same time, implementing \"protobuf-based-BIP174\" by\n> hand is roughly equally difficult as implementing the current BIP174.\n\nas a data point, I was able to build a simple serializer[1] in about an\nafternoon. I would much prefer to use this lib in say, clightning (my\noriginal goal), without having to have the larger protobuf dependency.\n\nCheers,\n\n[1] https://github.com/jb55/libpsbt\n\n\n--\nhttps://jb55.com"
            },
            {
                "author": "Marek Palatinus",
                "date": "2018-06-26T17:11:05",
                "message_text_only": "On Tue, Jun 26, 2018 at 6:58 PM, William Casarin via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> seems a bit overkill for how simple the format is, and pulling in a\n> large dependency just for this is a bit silly. Although making it\n> protobuf-compatible is an interesting idea, but I fear would be more\n> work than is worth? I haven't looked closed enough at the protobuf\n> encoding to be sure.\n>\n> > ...while at the same time, implementing \"protobuf-based-BIP174\" by\n> > hand is roughly equally difficult as implementing the current BIP174.\n>\n> as a data point, I was able to build a simple serializer[1] in about an\n> afternoon. I would much prefer to use this lib in say, clightning (my\n> original goal), without having to have the larger protobuf dependency.\n>\n>\nThat was exactly matejcik's point; you can easily write protobuf-compatible\nencoder/decoder for such simple structure in about an afternoon, if you\nneed. Or you can use existing protobuf parsers in matter of minute, if you\ndon't care about dependencies.\n\nAlso, many projects already have protobuf parsers, so it work in other way,\ntoo; you need BIP174 parser as extra dependency/library, although you\nalready use protobuf library (like Trezor device does). For needs of\nBIP174, the difference between ad-hoc format and protobuf is neglible, so\nit is a mistake to introduce yet another format.\n\nslush\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180626/b277b69a/attachment.html>"
            },
            {
                "author": "matejcik",
                "date": "2018-06-27T14:11:49",
                "message_text_only": "hello,\n\nOn 26.6.2018 18:58, William Casarin wrote:\n> as a data point, I was able to build a simple serializer[1] in about an\n> afternoon. I would much prefer to use this lib in say, clightning (my\n> original goal), without having to have the larger protobuf dependency.\n\nTo drive my point home, here's a PR converting the `writer` part of your\ncode to a protobuf-compatible version. It took me less than an hour to\nwrite, the bigger part of which was spent orienting myself in unfamiliar\ncode. I assume I could do `reader` in less than that, if your\ndeserialization code was complete.\n\nhttps://github.com/jb55/libpsbt/pull/3/files\n\nThis code produces PSBTs that my \"bip174 playground\" can correctly parse.\n\nregards\nm.\n\n> \n> Cheers,\n> \n> [1] https://github.com/jb55/libpsbt\n> \n> \n> --\n> https://jb55.com\n> \n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 819 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180627/4d38374e/attachment.sig>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2018-06-26T20:30:04",
                "message_text_only": "On Tue, Jun 26, 2018 at 8:33 AM, matejcik via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> I'm still going to argue against the key-value model though.\n>\n> It's true that this is not significant in terms of space. But I'm more\n> concerned about human readability, i.e., confusing future implementers.\n> At this point, the key-value model is there \"for historical reasons\",\n> except these aren't valid even before finalizing the format. The\n> original rationale for using key-values seems to be gone (no key-based\n> lookups are necessary). As for combining and deduplication, whether key\n> data is present or not is now purely a stand-in for a \"repeatable\" flag.\n> We could just as easily say, e.g., that the high bit of \"type\" specifies\n> whether this record can be repeated.\n\nI understand this is a philosophical point, but to me it's the\nopposite. The file conveys \"the script is X\", \"the signature for key X\nis Y\", \"the derivation for key X is Y\" - all extra metadata added to\ninputs of the form \"the X is Y\". In a typed record model, you still\nhave Xes, but they are restricted to a single number (the record\ntype). In cases where that is insufficient, your solution is adding a\nrepeatable flag to switch from \"the first byte needs to be unique\" to\n\"the entire record needs to be unique\". Why just those two? It seems\nmuch more natural to have a length that directly tells you how many of\nthe first bytes need to be unique (which brings you back to the\nkey-value model).\n\nSince the redundant script hashes were removed by making the scripts\nper-input, I think the most compelling reason (size advantages) for a\nrecord based model is gone.\n\n> (Moreover, as I wrote previously, the Combiner seems like a weirdly\n> placed role. I still don't see its significance and why is it important\n> to correctly combine PSBTs by agents that don't understand them. If you\n> have a usecase in mind, please explain.\n\nForward compatibility with new script types. A transaction may spend\ninputs from different outputs, with different script types. Perhaps\nsome of these are highly specialized things only implemented by some\nsoftware (say HTLCs of a particular structure), in non-overlapping\nways where no piece of software can handle all scripts involved in a\nsingle transaction. If Combiners cannot deal with unknown fields, they\nwon't be able to deal with unknown scripts. That would mean that\ncombining must be done independently by Combiner implementations for\neach script type involved. As this is easily avoided by adding a\nslight bit of structure (parts of the fields that need to be unique -\n\"keys\"), this seems the preferable option.\n\n> ISTM a Combiner could just as well combine based on whole-record\n> uniqueness, and leave the duplicate detection to the Finalizer. In case\n> the incoming PSBTs have incompatible unique fields, the Combiner would\n> have to fail anyway, so the Finalizer might as well do it. Perhaps it\n> would be good to leave out the Combiner role entirely?)\n\nNo, a Combiner can pick any of the values in case different PSBTs have\ndifferent values for the same key. That's the point: by having a\nkey-value structure the choice of fields can be made such that\nCombiners don't need to care about the contents. Finalizers do need to\nunderstand the contents, but they only operate once at the end.\nCombiners may be involved in any PSBT passing from one entity to\nanother.\n\n> There's two remaining types where key data is used: BIP32 derivations\n> and partial signatures. In case of BIP32 derivation, the key data is\n> redundant ( pubkey = derive(value) ), so I'd argue we should leave that\n> out and save space. In case of partial signatures, it's simple enough to\n> make the pubkey part of the value.\n\nIn case of BIP32 derivation, computing the pubkeys is possibly\nexpensive. A simple signer can choose to just sign with whatever keys\nare present, but they're not the only way to implement a signer, and\neven less the only software interacting with this format. Others may\nwant to use a matching approach to find keys that are relevant;\nwithout pubkeys in the format, they're forced to perform derivations\nfor all keys present.\n\nAnd yes, it's simple enough to make the key part of the value\neverywhere, but in that case it becomes legal for a PSBT to contain\nmultiple signatures for a key, for example, and all software needs to\ndeal with that possibility. With a stronger uniqueness constraint,\nonly Combiners need to deal with repetitions.\n\n> Thing is: BIP174 *is basically protobuf* (v2) as it stands. If I'm\n> succesful in convincing you to switch to a record set model, it's going\n> to be \"protobuf with different varint\".\n\nIf you take the records model, and then additionally drop the\nwhole-record uniqueness constraint, yes, though that seems pushing it\na bit by moving even more guarantees from the file format to\napplication level code. I'd like to hear opinions of other people who\nhave worked on implementations about changing this.\n\nCheers,\n\n-- \nPieter"
            },
            {
                "author": "matejcik",
                "date": "2018-06-27T14:04:06",
                "message_text_only": "hello,\n\nOn 26.6.2018 22:30, Pieter Wuille wrote:\n>> (Moreover, as I wrote previously, the Combiner seems like a weirdly\n>> placed role. I still don't see its significance and why is it important\n>> to correctly combine PSBTs by agents that don't understand them. If you\n>> have a usecase in mind, please explain.\n> \n> Forward compatibility with new script types. A transaction may spend\n> inputs from different outputs, with different script types. Perhaps\n> some of these are highly specialized things only implemented by some\n> software (say HTLCs of a particular structure), in non-overlapping\n> ways where no piece of software can handle all scripts involved in a\n> single transaction. If Combiners cannot deal with unknown fields, they\n> won't be able to deal with unknown scripts.\n\nRecord-based Combiners *can* deal with unknown fields. Either by\nincluding both versions, or by including one selected at random. This is\nthe same in k-v model.\n\n> combining must be done independently by Combiner implementations for\n> each script type involved. As this is easily avoided by adding a\n> slight bit of structure (parts of the fields that need to be unique -\n> \"keys\"), this seems the preferable option.\n\nIIUC, you're proposing a \"semi-smart Combiner\" that understands and\nprocesses some fields but not others? That doesn't seem to change\nthings. Either the \"dumb\" combiner throws data away before the \"smart\"\none sees it, or it needs to include all of it anyway.\n\n> No, a Combiner can pick any of the values in case different PSBTs have\n> different values for the same key. That's the point: by having a\n> key-value structure the choice of fields can be made such that\n> Combiners don't need to care about the contents. Finalizers do need to\n> understand the contents, but they only operate once at the end.\n> Combiners may be involved in any PSBT passing from one entity to\n> another.\n\nYes. Combiners don't need to care about the contents.\nSo why is it important that a Combiner properly de-duplicates the case\nwhere keys are the same but values are different? This is a job that,\nAFAICT so far, can be safely left to someone along the chain who\nunderstands that particular record.\n\nSay we have field F(key,value), and several Signers produce F(1,1),\nF(1,2), F(1,3).\n\nA key-based Combiner will pick exactly one to pass along. A record-based\nCombiner will pass all three.\n\nIt seems that you consider the latter PSBT \"invalid\". But it is well\nformed and doesn't contain duplicate records. A Finalizer, or a\ndifferent Combiner that understands field F, can as well have the rule\n\"throw away all but one\" for this case.\n\nTo repeat and restate my central question:\nWhy is it important, that an agent which doesn't understand a particular\nfield structure, can nevertheless make decisions about its inclusion or\nomission from the result (based on a repeated prefix)?\n\nActually, I can imagine the opposite: having fields with same \"key\"\n(identifying data), and wanting to combine their \"values\" intelligently\nwithout losing any of the data. Say, two Signers producing separate\nparts of a combined-signature under the same common public key?\n\n> In case of BIP32 derivation, computing the pubkeys is possibly\n> expensive. A simple signer can choose to just sign with whatever keys\n> are present, but they're not the only way to implement a signer, and\n> even less the only software interacting with this format. Others may\n> want to use a matching approach to find keys that are relevant;\n> without pubkeys in the format, they're forced to perform derivations\n> for all keys present.\n\nI'm going to search for relevant keys by comparing master fingerprint; I\nwould expect HWWs generally don't have index based on leaf pubkeys.\nOTOH, Signers with lots of keys probably aren't resource-constrained and\ncan do the derivations in case of collisions.\n\nAlso, you need to do the derivation and checking anyway, because what if\nthere is a mismatch between the key and the value?\n\nI liked @achow101's idea about supporting non-derived keys, but I\nassumed that you would match them based on the master fingerprint too?\n\nI wouldn't be against including the full master public key (probably\nwithout chaincode) instead of the fingerprint, as you proposed earlier.\nBut including both the leaf pubkey and the fingerprint seems weird.\n\n> If you take the records model, and then additionally drop the\n> whole-record uniqueness constraint, yes, though that seems pushing it\n> a bit by moving even more guarantees from the file format to\n> application level code.\n\nThe \"file format\" makes no guarantees, because the parsing code and\napplication code is the same anyway. You could say I'm proposing to\nseparate these concerns ;)\n\nregards\nm.\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 819 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180627/386870f6/attachment.sig>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2018-06-27T15:06:39",
                "message_text_only": "On Wed, Jun 27, 2018, 07:04 matejcik <jan.matejek at satoshilabs.com> wrote:\n\n> hello,\n>\n> On 26.6.2018 22:30, Pieter Wuille wrote:\n> >> (Moreover, as I wrote previously, the Combiner seems like a weirdly\n> >> placed role. I still don't see its significance and why is it important\n> >> to correctly combine PSBTs by agents that don't understand them. If you\n> >> have a usecase in mind, please explain.\n> >\n> > Forward compatibility with new script types. A transaction may spend\n> > inputs from different outputs, with different script types. Perhaps\n> > some of these are highly specialized things only implemented by some\n> > software (say HTLCs of a particular structure), in non-overlapping\n> > ways where no piece of software can handle all scripts involved in a\n> > single transaction. If Combiners cannot deal with unknown fields, they\n> > won't be able to deal with unknown scripts.\n>\n> Record-based Combiners *can* deal with unknown fields. Either by\n> including both versions, or by including one selected at random. This is\n> the same in k-v model.\n>\n\nYes, I wasn't claiming otherwise. This was just a response to your question\nwhy it is important that Combiners can process unknown fields. It is not an\nargument in favor of one model or the other.\n\n> combining must be done independently by Combiner implementations for\n> > each script type involved. As this is easily avoided by adding a\n> > slight bit of structure (parts of the fields that need to be unique -\n> > \"keys\"), this seems the preferable option.\n>\n> IIUC, you're proposing a \"semi-smart Combiner\" that understands and\n> processes some fields but not others? That doesn't seem to change\n> things. Either the \"dumb\" combiner throws data away before the \"smart\"\n> one sees it, or it needs to include all of it anyway.\n>\n\nNo, I'm exactly arguing against smartness in the Combiner. It should always\nbe possible to implement a Combiner without any script specific logic.\n\n> No, a Combiner can pick any of the values in case different PSBTs have\n> > different values for the same key. That's the point: by having a\n> > key-value structure the choice of fields can be made such that\n> > Combiners don't need to care about the contents. Finalizers do need to\n> > understand the contents, but they only operate once at the end.\n> > Combiners may be involved in any PSBT passing from one entity to\n> > another.\n>\n> Yes. Combiners don't need to care about the contents.\n> So why is it important that a Combiner properly de-duplicates the case\n> where keys are the same but values are different? This is a job that,\n> AFAICT so far, can be safely left to someone along the chain who\n> understands that particular record.\n>\n\nThat's because PSBTs can be copied, signed, and combined back together. A\nCombiner which does not deduplicate (at all) would end up having every\noriginal record present N times, one for each copy, a possibly large blowup.\n\nFor all fields I can think of right now, that type of deduplication can be\ndone through whole-record uniqueness.\n\nThe question whether you need whole-record uniqueness or specified-length\nuniqueness (=what is offered by a key-value model) is a philosophical one\n(as I mentioned before). I have a preference for stronger invariants on the\nfile format, so that it becomes illegal for a PSBT to contain multiple\nsignatures for the same key for example, and implementations do not need to\ndeal with the case where multiple are present.\n\nIt seems that you consider the latter PSBT \"invalid\". But it is well\n> formed and doesn't contain duplicate records. A Finalizer, or a\n> different Combiner that understands field F, can as well have the rule\n> \"throw away all but one\" for this case.\n>\n\nIt's not about considering. We're writing a specification. Either it is\nmade invalid, or not.\n\nIn a key-value model you can have dumb combiners that must pick one of the\nkeys in case of duplication, and remove the necessity of dealing with\nduplication from all other implementations (which I consider to be a good\nthing). In a record-based model you cannot guarantee deduplication of\nrecords that permit repetition per type, because a dumb combiner cannot\nunderstand what part is supposed to be unique. As a result, a record-based\nmodel forces you to let all implementations deal with e.g. multiple partial\nsignatures for a single key. This is a minor issue, but in my view shows\nhow records are a less than perfect match for the problem at hand.\n\nTo repeat and restate my central question:\n> Why is it important, that an agent which doesn't understand a particular\n> field structure, can nevertheless make decisions about its inclusion or\n> omission from the result (based on a repeated prefix)?\n>\n\nAgain, because otherwise you may need a separate Combiner for each type of\nscript involved. That would be unfortunate, and is very easily avoided.\n\nActually, I can imagine the opposite: having fields with same \"key\"\n> (identifying data), and wanting to combine their \"values\" intelligently\n> without losing any of the data. Say, two Signers producing separate\n> parts of a combined-signature under the same common public key?\n>\n\nThat can always be avoided by using different identifying information as\nkey for these fields. In your example, assuming you're talking about some\nform of threshold signature scheme, every party has their own \"shard\" of\nthe key, which still uniquely identifies the participant. If they have no\ndata that is unique to the participant, they are clones, and don't need to\ninteract regardless.\n\n> In case of BIP32 derivation, computing the pubkeys is possibly\n> > expensive. A simple signer can choose to just sign with whatever keys\n> > are present, but they're not the only way to implement a signer, and\n> > even less the only software interacting with this format. Others may\n> > want to use a matching approach to find keys that are relevant;\n> > without pubkeys in the format, they're forced to perform derivations\n> > for all keys present.\n>\n> I'm going to search for relevant keys by comparing master fingerprint; I\n> would expect HWWs generally don't have index based on leaf pubkeys.\n> OTOH, Signers with lots of keys probably aren't resource-constrained and\n> can do the derivations in case of collisions.\n>\n\nPerhaps you want to avoid signing with keys that are already signed with?\nIf you need to derive all the keys before even knowing what was already\nsigned with, you've already performed 80% of the work.\n\n> If you take the records model, and then additionally drop the\n> > whole-record uniqueness constraint, yes, though that seems pushing it\n> > a bit by moving even more guarantees from the file format to\n> > application level code.\n>\n> The \"file format\" makes no guarantees, because the parsing code and\n> application code is the same anyway. You could say I'm proposing to\n> separate these concerns ;)\n>\n\nOf course a file format can make guarantees. If certain combinations of\ndata in it do not satsify the specification, the file is illegal, and\nimplementations do not need to deal with it. Stricter file formats are\neasier to deal with, because there are less edge cases to consider.\n\nTo your point: proto v2 afaik has no way to declare \"whole record\nuniqueness\", so either you drop that (which I think is unacceptable - see\nthe copy/sign/combine argument above), or you deal with it in your\napplication code.\n\nCheers,\n\n-- \nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180627/fecd0346/attachment-0001.html>"
            },
            {
                "author": "Achow101",
                "date": "2018-06-29T19:12:27",
                "message_text_only": "Hi,\n\nI do not think that protobuf is the way to go for this. Not only is it another dependency\nwhich many wallets do not want to add (e.g. Armory has not added BIP 70 support because\nof its dependency on protobuf), but it is a more drastic change than the currently proposed\nchanges. The point of this email thread isn't to rewrite and design a new BIP (which is effectively\nwhat is currently going on). The point is to modify and improve the current one. In particular,\nwe do not want such drastic changes that people who have already implemented the current\nBIP would have to effectively rewrite everything from scratch again.\n\nI believe that this discussion has become bikeshedding and is really no longer constructive. Neither\nof us are going to convince the other to use or not use protobuf. ASeeing how no one else\nhas really participated in this discussion about protobuf and key uniqueness, I do not think\nthat these suggested changes are really necessary nor useful to others. It boils down to personal preference\nrather than technical merit. As such, I have opened a PR to the BIPs repo (https://github.com/bitcoin/bips/pull/694)\nwhich contains the changes that I proposed in an earlier email.\n\nAdditionally, because there have been no objections to the currently proposed changes, I propose\nto move the BIP from Draft to Proposed status.\n\nAndrew\n\n\n\u200b\u200b\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n\nOn June 29, 2018 2:53 AM, matejcik via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> \u200b\u200b\n> \n> Short version:\n> \n> -   I propose that conflicting \"values\" for the same \"key\" are considered\n>     \n>     invalid.\n>     \n> -   Let's not optimize for invalid data.\n> -   Given that, there's an open question on how to handle invalid data\n>     \n>     when encountered\n>     \n>     In general, I don't think it's possible to enforce correctness at the\n>     \n>     format level. You still need application level checks - and that calls\n>     \n>     into question what we gain by trying to do this on the format level.\n>     \n>     Long version:\n>     \n>     Let's look at this from a different angle.\n>     \n>     There are roughly two possible \"modes\" for the format with regard to\n>     \n>     possibly-conflicting data. Call them \"permissive\" and \"restrictive\".\n>     \n>     The spec says:\n>     \n>     \"\"\"\n>     \n>     Keys within each scope should never be duplicated; all keys in the\n>     \n>     format are unique. PSBTs containing duplicate keys are invalid. However\n>     \n>     implementors will still need to handle events where keys are duplicated\n>     \n>     when combining transactions with duplicated fields. In this event, the\n>     \n>     software may choose whichever value it wishes.\n>     \n>     \"\"\"\n>     \n>     The last sentence of this paragraph sets the mode to permissive:\n>     \n>     duplicate values are pretty much OK. If you see them, just pick one.\n>     \n>     You seem to argue that Combiners, in particular simple ones that don't\n>     \n>     understand field semantics, should merge keys permissively, but\n>     \n>     deduplicate values restrictively.\n>     \n>     IOW: if you receive two different values for the same key, just pick\n>     \n>     whichever, but $deity forbid you include both!\n>     \n>     This choice doesn't make sense to me.\n>     \n>     What would make sense is fully restrictive mode: receiving two\n>     \n>     different values for the same key is a fatal condition with no recovery.\n>     \n>     If you have a non-deterministic scheme, put a differentiator in the key.\n>     \n>     Or all the data, for that matter.\n>     \n>     (Incidentally, this puts key-aware and keyless Combiners on the same\n>     \n>     footing. As long as all participants uphold the protocol, different\n>     \n>     value = different key = different full record.)\n>     \n>     Given that, it's nice to have the Combiner perform the task of detecting\n>     \n>     this and failing. But not at all necessary. As the quoted paragraph\n>     \n>     correctly notes, consumers still need to handle PSBTs with duplicate keys.\n>     \n>     (In this context, your implied permissive/restrictive Combiner is\n>     \n>     optimized for dealing with invalid data. That seems like a wrong\n>     \n>     optimization.)\n>     \n>     A reasonable point to decide is whether the handling at the consumer\n>     \n>     should be permissive or restrictive. Personally I'm OK with either. I'd\n>     \n>     go with the following change:\n>     \n>     \"\"\"\n>     \n>     In this event, the software MAY reject the transaction as invalid. If it\n>     \n>     decides to accept it, it MUST choose the last value encountered.\n>     \n>     \"\"\"\n>     \n>     (deterministic way of choosing, instead of \"whichever you like\")\n>     \n>     We could also drop the first part, explicitly allowing consumers to\n>     \n>     pick, and simplifying the Combiner algorithm to `sort -u`.\n>     \n>     Note that this sort of \"picking\" will probably be implicit. I'd expect\n>     \n>     the consumer to look like this:\n>     \n> \n>     for key, value in parse(nextRecord()):\n>       data[key] = value\n>     \n> \n> Or we could drop the second part and switch MAY to MUST, for a fully\n> \n> restrictive mode - which, funnily enough, still lets the Combiner work\n> \n> as `sort -u`.\n> \n> To see why, remember that distinct values for the same key are not\n> \n> allowed in fully restrictive mode. If a Combiner encounters two\n> \n> conflicting values F(1) and F(2), it should fail -- but if it doesn't,\n> \n> it includes both and the same failure WILL happen on the fully\n> \n> restrictive consumer.\n> \n> This was (or is) my point of confusion re Combiners: the permissive key\n> \n> -   restrictive value mode of operation doesn't seem to help subsequent\n>     \n>     consumers in any way.\n>     \n>     Now, for the fully restrictive consumer, the key-value model is indeed\n>     \n>     advantageous (and this is the only scenario that I can imagine in which\n>     \n>     it is advantageous), because you can catch key duplication on the parser\n>     \n>     level.\n>     \n>     But as it turns out, it's not enough. Consider the following records:\n>     \n>     key(<PSBT_IN_REDEEM_SCRIPT> + abcde), value(<some redeem script>)\n>     \n> \n> and:\n> \n> key(<PSBT_IN_REDEEM_SCRIPT> + fghij), value(<some other redeem script>)\n> \n> A purely syntactic Combiner simply can't handle this case. The\n> \n> restrictive consumer needs to know whether the key is supposed to be\n> \n> repeating or not.\n> \n> We could fix this, e.g., by saying that repeating types must have high\n> \n> bit set and non-repeating must not. We also don't have to, because the\n> \n> worst failure here is that a consumer passes an invalid record to a\n> \n> subsequent one and the failure happens one step later.\n> \n> At this point it seems weird to be concerned about the \"unique key\"\n> \n> correctness, which is a very small subset of possibly invalid inputs. As\n> \n> a strict safety measure, I'd instead propose that a consumer MUST NOT\n> \n> operate on inputs or outputs, unless it understand ALL included fields -\n> \n> IOW, if you're signing a particular input, all fields in said input are\n> \n> mandatory. This prevents a situation where a simple Signer processes an\n> \n> input incorrectly based on incomplete set of fields, while still\n> \n> allowing Signers with different capabilities within the same PSBT.\n> \n> (The question here is whether to have either a flag or a reserved range\n> \n> for \"optional fields\" that can be safely ignored by consumers that don't\n> \n> understand them, but provide data for consumers who do.)\n> \n> > > To repeat and restate my central question: Why is it important,\n> > > \n> > > that an agent which doesn't understand a particular field\n> > > \n> > > structure, can nevertheless make decisions about its inclusion or\n> > > \n> > > omission from the result (based on a repeated prefix)?\n> > \n> > Again, because otherwise you may need a separate Combiner for each\n> > \n> > type of script involved. That would be unfortunate, and is very\n> > \n> > easily avoided.\n> \n> This is still confusing to me, and I would really like to get to the\n> \n> same page on this particular thing, because a lot of the debate hinges\n> \n> on it. I think I covered most of it above, but there are still pieces to\n> \n> clarify.\n> \n> As I understand it, the Combiner role (actually all the roles) is mostly\n> \n> an algorithm, with the implication that it can be performed\n> \n> independently by a separate agent, say a network node.\n> \n> So there's two types of Combiners:\n> \n> a) Combiner as a part of an intelligent consumer -- the usual scenario\n> \n> is a Creator/Combiner/Finalizer/Extractor being one participant, and\n> \n> Updater/Signers as other participants.\n> \n> In this case, the discussion of \"simple Combiners\" is actually talking\n> \n> about intelligent Combiners which don't understand new fields and must\n> \n> correctly pass them on. I argue that this can safely be done without\n> \n> loss of any important properties.\n> \n> b) Combiner as a separate service, with no understanding of semantics.\n> \n> Although parts of the debate seem to assume this scenario, I don't think\n> \n> it's worth considering. Again, do you have an usecase in mind for it?\n> \n> You also insist on enforcing a limited form of correctness on the\n> \n> Combiner level, but that is not worth it IMHO, as discussed above.\n> \n> Or am I missing something else?\n> \n> > Perhaps you want to avoid signing with keys that are already signed\n> > \n> > with? If you need to derive all the keys before even knowing what\n> > \n> > was already signed with, you've already performed 80% of the work.\n> \n> This wouldn't concern me at all, honestly. If the user sends an already\n> \n> signed PSBT to the same signer, IMHO it is OK to sign again; the\n> \n> slowdown is a fault of the user/workflow. You could argue that signing\n> \n> again is the valid response. Perhaps the Signer should even \"consume\"\n> \n> its keys and not pass them on after producing a signature? That seems\n> \n> like a sensible rule.\n> \n> > To your point: proto v2 afaik has no way to declare \"whole record\n> > \n> > uniqueness\", so either you drop that (which I think is unacceptable\n> > \n> > -   see the copy/sign/combine argument above), or you deal with it in\n> >     \n> >     your application code.\n> >     \n> \n> Yes. My argument is that \"whole record uniqueness\" isn't in fact an\n> \n> important property, because you need application-level checks anyway.\n> \n> Additionally, protobuf provides awareness of which fields are repeated\n> \n> and which aren't, and implicitly implements the \"pick last\" resolution\n> \n> strategy for duplicates.\n> \n> The simplest possible protobuf-based Combiner will:\n> \n> -   assume all fields are repeating\n> -   concatenate and parse\n> -   deduplicate and reserialize.\n>     \n>     More knowledgeable Combiner will intelligently handle non-repeating\n>     \n>     fields, but still has to assume that unknown fields are repeating and\n>     \n>     use the above algorithm.\n>     \n>     For \"pick last\" strategy, a consumer can simply parse the message and\n>     \n>     perform appropriate application-level checks.\n>     \n>     For \"hard-fail\" strategy, it must parse all fields as repeating and\n>     \n>     check that there's only one of those that are supposed to be unique.\n>     \n>     This is admittedly more work, and yes, protobuf is not perfectly suited\n>     \n>     for this task.\n>     \n>     But:\n>     \n>     One, this work must be done by hand anyway, if we go with a custom\n>     \n>     hand-parsed format. There is a protobuf implementation for every\n>     \n>     conceivable platform, we'll never have the same amount of BIP174 parsing\n>     \n>     code.\n>     \n>     (And if you're hand-writing a parser in order to avoid the dependency,\n>     \n>     you can modify it to do the checks at parser level. Note that this is\n>     \n>     not breaking the format! The modifed parser will consume well-formed\n>     \n>     protobuf and reject that which is valid protobuf but invalid bip174 - a\n>     \n>     correct behavior for a bip174 parser.)\n>     \n>     Two, it is my opinion that this is worth it in order to have a standard,\n>     \n>     well described, well studied and widely implemented format.\n>     \n>     Aside: I ha that there is no advantage to a record-set based\n>     \n>     custom format by itself, so IMHO the choice is between protobuf vs\n>     \n>     a custom key-value format. Additionally, it's even possible to implement\n>     \n>     a hand-parsable key-value format in terms of protobuf -- again, arguing\n>     \n>     that \"standardness\" of protobuf is valuable in itself.\n>     \n>     regards\n>     \n>     m.\n>     \n> \n> bitcoin-dev mailing list\n> \n> bitcoin-dev at lists.linuxfoundation.org\n> \n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Peter D. Gray",
                "date": "2018-06-29T20:31:21",
                "message_text_only": "...\n> I believe that this discussion has become bikeshedding and is really no longer constructive.\n...\n\nThanks for saying this Andrew! I agree with your point of view, and personally I'm ready to lock this BIP down ... or at least move it to the next level of approval.\n\n...\n>  I propose to move the BIP from Draft to Proposed status.\n\nYes please, all praise the BIP gods!\n\n---\nPeter D. Gray  ||  Founder, Coinkite  ||  Twitter: @dochex  ||  GPG: A3A31BAD 5A2A5B10"
            },
            {
                "author": "Achow101",
                "date": "2018-06-26T21:56:26",
                "message_text_only": "Hi,\n\nOn June 26, 2018 8:33 AM, matejcik via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> \u200b\u200b\n> \n> hello,\n> \n> in general, I agree with my colleague Tomas, the proposed changes are\n> \n> good and achieve the most important things that we wanted. We'll review\n> \n> the proposal in more detail later.\n> \n> For now a couple minor things I have run into:\n> \n> -   valid test vector 2 (\"one P2PKH input and one P2SH-P2WPKH input\")\n>     \n>     seems broken, at least its hex version; a delimiter seems to be missing,\n>     \n>     misplaced or corrupted\n\nFixed\n\n>     \n> -   at least the first signing vector is not updated, but you probably\n>     \n>     know that\n\nI updated all of the tests yesterday so they should be correct now. I will be adding more tests\nthis week.\n\n>     \n> -   BIP32 derivation fields don't specify size of the \"master public key\",\n>     \n>     which would make it un-parsable :)\n\nOops, that's actually supposed to be master key fingerprint, not master public key. I have updated\nthe BIP to reflect this.\n\n>     \n> -   \"Transaction format is specified as follows\" and its table need to be\n>     \n>     updated.\n\nDone.\n\n>     \n>     I'm still going to argue against the key-value model though.\n>     \n>     It's true that this is not significant in terms of space. But I'm more\n>     \n>     concerned about human readability, i.e., confusing future implementers.\n>     \n>     At this point, the key-value model is there \"for historical reasons\",\n>     \n>     except these aren't valid even before finalizing the format. The\n>     \n>     original rationale for using key-values seems to be gone (no key-based\n>     \n>     lookups are necessary). As for combining and deduplication, whether key\n>     \n>     data is present or not is now purely a stand-in for a \"repeatable\" flag.\n>     \n>     We could just as easily say, e.g., that the high bit of \"type\" specifies\n>     \n>     whether this record can be repeated.\n>     \n>     (Moreover, as I wrote previously, the Combiner seems like a weirdly\n>     \n>     placed role. I still don't see its significance and why is it important\n>     \n>     to correctly combine PSBTs by agents that don't understand them. If you\n>     \n>     have a usecase in mind, please explain.\n\nThere is a diagram in the BIP that explains this. The combiner's job is to combine two PSBTs that\nare for the same transaction but contain different data such as signatures. It is easier to implement\na combiner that does not need to understand the types at all, and such combiners are forwards compatible,\nso new types do not require new combiner implementations.\n\n>     \n>     ISTM a Combiner could just as well combine based on whole-record\n>     \n>     uniqueness, and leave the duplicate detection to the Finalizer. In case\n>     \n>     the incoming PSBTs have incompatible unique fields, the Combiner would\n>     \n>     have to fail anyway, so the Finalizer might as well do it. Perhaps it\n>     \n>     would be good to leave out the Combiner role entirely?)\n\nA transaction that contains duplicate keys would be completely invalid. Furthermore, in the set of typed\nrecords model, having more than one redeemScript and witnessScript should be invalid, so a combiner\nwould still need to understand what types are in order to avoid this situation. Otherwise it would produce\nan invalid PSBT.\n\nI also dislike the idea of having type specific things like \"only one redeemScript\" where a more generic\nthing would work.\n\n>     \n>     There's two remaining types where key data is used: BIP32 derivations\n>     \n>     and partial signatures. In case of BIP32 derivation, the key data is\n>     \n>     redundant ( pubkey = derive(value) ), so I'd argue we should leave that\n>     \n>     out and save space. \n\nI think it is still necessary to include the pubkey as not all signers who can sign for a given pubkey may\nknow the derivation path. For example, if a privkey is imported into a wallet, that wallet does not necessarily\nknow the master key fingerprint for the privkey nor would it necessarily have the master key itself in\norder to derive the privkey. But it still has the privkey and can still sign for it.\n\n>     \n>     Re breaking change, we are proposing breaking changes anyway, existing\n>     \n>     code will need to be touched, and given that this is a hand-parsed\n>     \n>     format, changing `parse_keyvalue` to `parse_record` seems like a small\n>     \n>     matter?\n\nThe point is to not make it difficult for existing implementations to change. Mostly what has been done now is just\nmoving things around, not an entire format change itself. Changing to a set of typed records model require more\nchanges and is a complete restructuring of the format, not just moving things around.\n\nAdditionally, I think that the current model is fairly easy to hand parse. I don't think a record set model would make\nit easier to follow. Furthermore, moving to Protobuf would make it harder to hand parse as varints are slightly more\nconfusing in protobuf than with Compact size uints. And with the key-value model, you don't need to know the types\nto know whether something is valid. You don't need to interpret any data.\n\n\n\nAndrew"
            },
            {
                "author": "William Casarin",
                "date": "2018-06-27T06:09:09",
                "message_text_only": "Hey Andrew,\n\nIf I'm reading the spec right: the way it is designed right now, you\ncould create hundreds of thousands of zero bytes in the input or output\nkey-value arrays. As far as I can tell this would be considered valid,\nas it is simply a large array of empty dictionaries. Is this right? I'm\nworried about buffer overflows in cases where someone sends a large blob\nof zeros to an unsuspecting implementation.\n\n\nAlso, the extensibility section reads:\n\n> Additional key-value maps with different types for the key-value pairs\n> can be added on to the end of the format.\n\n\"different types for the key-value pairs\", is this referring to new\ntypes beyond the current global, input and output types?\n\n> The number of each map that follows must be specified in the globals\n> section\n\nIs this out of date? Since there is only one type in the global section\nnow (tx).\n\n> so that parsers will know when to use different definitions of the\n> data types\n\nI'm not sure what this means.\n\n\nThanks!\n\nWill\n\n\n--\nhttps://jb55.com"
            },
            {
                "author": "Andrea",
                "date": "2018-06-27T13:39:02",
                "message_text_only": "Hi William, Andrew, list,\n\nAs noted by William there are some types missing in the global-types definition, because the number of each map for I/O must be known to the parser in order to use the correct definitions for the types. At the moment a parser reading a key-value record does not know whether it should read it as per-input type or per-output, a way to address this is to declare in advance the number of maps and ensure they are ordered (inputs first). If you haven't already worked out some types for that i propose using:\n\nNumber of inputs\n- key (None, only the type): PSBT_GLOBAL_INPUT_NUMBER = 0x01  \n- value: Varint \n\nNumber of outputs\n- key (none, only the type): PSBT_GLOBAL_OUTPUT_NUMBER = 0x02\n- value: Varint\n\nOn another note I think we can set a hard limit on the size of the PSBT, currently is 'legal' to produce a very large PSBT with an excessive number of Inputs and Outputs. By excessive I mean that even in the best case scenario (using the smallest possible scriptPubKey as in P2WPKH) it is possible to create a PSBT that would certainly create an invalid transaction (because of its size) when finalized. I haven't found anything related to this in the previous discussions, please ignore this if it was already proposed/discussed.\n\n\nCheers, Andrea.\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n\nOn June 27, 2018 8:09 AM, William Casarin via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> \u200b\u200b\n> \n> Hey Andrew,\n> \n> If I'm reading the spec right: the way it is designed right now, you\n> \n> could create hundreds of thousands of zero bytes in the input or output\n> \n> key-value arrays. As far as I can tell this would be considered valid,\n> \n> as it is simply a large array of empty dictionaries. Is this right? I'm\n> \n> worried about buffer overflows in cases where someone sends a large blob\n> \n> of zeros to an unsuspecting implementation.\n> \n> Also, the extensibility section reads:\n> \n> > Additional key-value maps with different types for the key-value pairs\n> > \n> > can be added on to the end of the format.\n> \n> \"different types for the key-value pairs\", is this referring to new\n> \n> types beyond the current global, input and output types?\n> \n> > The number of each map that follows must be specified in the globals\n> > \n> > section\n> \n> Is this out of date? Since there is only one type in the global section\n> \n> now (tx).\n> \n> > so that parsers will know when to use different definitions of the\n> > \n> > data types\n> \n> I'm not sure what this means.\n> \n> Thanks!\n> \n> Will\n> \n> \n> ------------------------------------------------\n> \n> https://jb55.com\n> \n> bitcoin-dev mailing list\n> \n> bitcoin-dev at lists.linuxfoundation.org\n> \n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Achow101",
                "date": "2018-06-27T17:55:59",
                "message_text_only": "Hi,\u200b\n\nOn June 26, 2018 11:09 PM, William Casarin <jb55 at jb55.com> wrote:\n\n> \u200b\u200b\n> \n> Hey Andrew,\n> \n> If I'm reading the spec right: the way it is designed right now, you\n> \n> could create hundreds of thousands of zero bytes in the input or output\n> \n> key-value arrays. As far as I can tell this would be considered valid,\n> \n> as it is simply a large array of empty dictionaries. Is this right? I'm\n> \n> worried about buffer overflows in cases where someone sends a large blob\n> \n> of zeros to an unsuspecting implementation.\n\nNo, that is incorrect. That whole paragraph is actually outdated, it was intended\nfor the possibility of adding output maps, which we have already done. I have \nremoved it from the BIP.\n\nHowever, it is possible for a PSBT to contain very large unknown key-value pairs \nwhich could potentially cause a problem. But I do not think that large PSBTs should \nreally be a problem as they are really something that the user has to enter rather \nthan something received remotely without user interaction.\n\n\n\nOn June 27, 2018 6:39 AM, Andrea via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> \u200b\u200b\n> \n> Hi William, Andrew, list,\n> \n> As noted by William there are some types missing in the global-types definition, because the number of each map for I/O must be known to the parser in order to use the correct definitions for the types. At the moment a parser reading a key-value record does not know whether it should read it as per-input type or per-output, a way to address this is to declare in advance the number of maps and ensure they are ordered (inputs first). If you haven't already worked out some types for that i propose using:\n> \n\nParsers actually do know because that information is present in the unsigned transaction \nat the beginning of each PSBT. Since each input and output must be accounted for,\na parser can just parse the unsigned transaction and from there it can know how\nmany inputs and outputs to expect. If it sees more or less, it should throw an error\nas the transaction is invalid.\n\nOf course this implies that implementations will need to parse the unsigned transaction,\nbut not all actually need to. Combiners do not need to, they just need to merge the\nmaps together and follow the key uniqueness rule. They don't really need to know\nor care about the number of inputs and outputs, just that the PSBTs being merged\nshare the same unsigned transaction and have the same number of maps.\n\nOther roles need to understand the unsigned transaction anyways, so they still need\nto parse it thus this isn't really a problem for those roles.\n\n>     \n>     On another note I think we can set a hard limit on the size of the PSBT, currently is 'legal' to produce a very large PSBT with an excessive number of Inputs and Outputs. By excessive I mean that even in the best case scenario (using the smallest possible scriptPubKey as in P2WPKH) it is possible to create a PSBT that would certainly create an invalid transaction (because of its size) when finalized. I haven't found anything related to this in the previous discussions, please ignore this if it was already proposed/discussed.\n>     \n\nI don't think such a limitation is practical or useful. A transaction can currently have, at most,\n~24000 inputs and ~111000 outputs (+/- a few hundred) which is well beyond any useful limit.\nAdditionally, such limits may not be as extensible for future work. It is hard to determine what\nis a reasonable limit on transaction size, and I don't think it is useful to have a limit. At worst\nwe would simply create an invalid transaction if it were too large.\n\n\nAndrew"
            },
            {
                "author": "Rodolfo Novak",
                "date": "2018-06-28T20:42:09",
                "message_text_only": "Hello Folks,\n\nThanks for expediting this debate, I understand some still disagree about how this final spec should look like.\n\n1. Coldcard's plugin for Electrum using the original BIP spec is ready, https://github.com/spesmilo/electrum/pull/4470 <https://github.com/spesmilo/electrum/pull/4470>\n2. Our hardware is ready with this spec (src will be public soon)\n3. Samourai Wallet and Sentinel also are ready with the current spec.\n\nWe intend to upgrade it once the final spec is ready, but I need to ship Coldcard.\n\nCheers,\n\n\u211d.\n\nRodolfo Novak  ||  Founder, Coinkite  ||  twitter @nvk  ||  GPG: B444CDDA\n\n> On Jun 27, 2018, at 13:55, Achow101 via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> Hi,\u200b\n> \n> On June 26, 2018 11:09 PM, William Casarin <jb55 at jb55.com> wrote:\n> \n>> \u200b\u200b\n>> \n>> Hey Andrew,\n>> \n>> If I'm reading the spec right: the way it is designed right now, you\n>> \n>> could create hundreds of thousands of zero bytes in the input or output\n>> \n>> key-value arrays. As far as I can tell this would be considered valid,\n>> \n>> as it is simply a large array of empty dictionaries. Is this right? I'm\n>> \n>> worried about buffer overflows in cases where someone sends a large blob\n>> \n>> of zeros to an unsuspecting implementation.\n> \n> No, that is incorrect. That whole paragraph is actually outdated, it was intended\n> for the possibility of adding output maps, which we have already done. I have \n> removed it from the BIP.\n> \n> However, it is possible for a PSBT to contain very large unknown key-value pairs \n> which could potentially cause a problem. But I do not think that large PSBTs should \n> really be a problem as they are really something that the user has to enter rather \n> than something received remotely without user interaction.\n> \n> \n> \n> On June 27, 2018 6:39 AM, Andrea via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n>> \u200b\u200b\n>> \n>> Hi William, Andrew, list,\n>> \n>> As noted by William there are some types missing in the global-types definition, because the number of each map for I/O must be known to the parser in order to use the correct definitions for the types. At the moment a parser reading a key-value record does not know whether it should read it as per-input type or per-output, a way to address this is to declare in advance the number of maps and ensure they are ordered (inputs first). If you haven't already worked out some types for that i propose using:\n>> \n> \n> Parsers actually do know because that information is present in the unsigned transaction \n> at the beginning of each PSBT. Since each input and output must be accounted for,\n> a parser can just parse the unsigned transaction and from there it can know how\n> many inputs and outputs to expect. If it sees more or less, it should throw an error\n> as the transaction is invalid.\n> \n> Of course this implies that implementations will need to parse the unsigned transaction,\n> but not all actually need to. Combiners do not need to, they just need to merge the\n> maps together and follow the key uniqueness rule. They don't really need to know\n> or care about the number of inputs and outputs, just that the PSBTs being merged\n> share the same unsigned transaction and have the same number of maps.\n> \n> Other roles need to understand the unsigned transaction anyways, so they still need\n> to parse it thus this isn't really a problem for those roles.\n> \n>> \n>>    On another note I think we can set a hard limit on the size of the PSBT, currently is 'legal' to produce a very large PSBT with an excessive number of Inputs and Outputs. By excessive I mean that even in the best case scenario (using the smallest possible scriptPubKey as in P2WPKH) it is possible to create a PSBT that would certainly create an invalid transaction (because of its size) when finalized. I haven't found anything related to this in the previous discussions, please ignore this if it was already proposed/discussed.\n>> \n> \n> I don't think such a limitation is practical or useful. A transaction can currently have, at most,\n> ~24000 inputs and ~111000 outputs (+/- a few hundred) which is well beyond any useful limit.\n> Additionally, such limits may not be as extensible for future work. It is hard to determine what\n> is a reasonable limit on transaction size, and I don't think it is useful to have a limit. At worst\n> we would simply create an invalid transaction if it were too large.\n> \n> \n> Andrew\n> \n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180628/176ddd27/attachment.html>"
            },
            {
                "author": "Jason Les",
                "date": "2018-06-20T00:39:46",
                "message_text_only": "On Fri, Jun 15, 2018 at 04:34:40PM -0700, Pieter Wuille wrote:\n...\n> First of all, it's unclear to me to what extent projects have already\n> worked on implementations, and thus to what extent the specification\n> is still subject to change. A response of \"this is way too late\" is\n> perfectly fine.\n...\n\nI am working on a python implementation of BIP 174 as part of a multi-sig hardware wallet I have in the works. I am not so far along as to say that all these changes are way too late, but I\u2019ll comment on the following:\n\n> Key-value map model or set model\nI believe the key-value map model should be kept because as Jonas Schnelli said it\u2019s probably not worth significantly breaking existing implementations like Peter Gray\u2019s, or maybe more who have not spoken up. However, I appreciate the benefit of the set model particularly in regards to size consideration and the merging of redeemscripts and witnesscripts into single \u201cscripts\u201d records.\n\n>Ability for Combiners to verify two PSBT are for the same transaction\nThis idea makes a lot of sense, much more intuitive.\n\n>Hex encoding?\nI was hoping for some standard here was well and I agree using something more compact than hex is important. My understanding is Z85 is better for use with JSON than Base64, which is probably a good benefit to have here.\n\nI will continue developing under the current spec and if there ends up being consensus for some of the changes here I\u2019m fine with re-doing the work. I will be following this thread closer now.\n\nBest,\nJason Les\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180619/f735692a/attachment.html>"
            },
            {
                "author": "Andrew Chow",
                "date": "2018-06-24T08:28:26",
                "message_text_only": "I disagree with the idea that global types can be removed. Firstly, it\nis less of a breaking change to leave it there than to remove it\nentirely. Secondly, there may be a point in the future where global\ntypes would be useful/necessary. By having it still be there, we allow\nfor future extensibility.\n\nAndrew\n\n\nOn 06/24/2018 01:19 AM, Andrea wrote:\n> Hi, \n>\n> I think in the revised spec we can remove completely the \"global types\" as a map or even as typed record. Since there is only one type (the transaction) and it's compulsory to have one (and only one) we could just drop the definition of global type and the key associated with it, simply after the header + separator there must be a transaction.\u200b\u200b Having read all the discussion i also agree having per-input key derivation and per-output data is a lot more handy for signers, no special feeling regarding the encoding.Looking forward for the test vectors and the new spec.\n>\n> Cheers, Andrea.\n>\n> \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n>\n> On June 23, 2018 10:33 PM, Andrew Chow via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> \u200b\u200b\n>>\n>> On 06/23/2018 10:00 AM, William Casarin wrote:\n>>\n>>> Since we're still considering the encoding, I wonder if it would be a\n>>>\n>>> good idea to have a human-readible part like lightning invoices[1]?\n>> I don't think that is necessary.\n>>\n>>> Then perhaps you could drop the magic code as well?\n>> The magic is still necessary for the binary format in order to prevent\n>>\n>> normal transaction deserializers from accidentally deserializing a psbt.\n>>\n>>> Also we could do a base encoding that excludes + and / characters, such\n>>>\n>>> as base62 (gmp-style). It's easier to copy/paste (double clicking a\n>>>\n>>> string stops at / or + in base64 encodings).\n>> While that would be ideal, I think it is better to use an encoding that\n>>\n>> most wallets already support. Most wallets already have Base64 decoding\n>>\n>> available so that they can decode signed messages which also use Base64\n>>\n>> encoding. I think it is unnecessary to introduce another encoding.\n>>\n>> On 06/23/2018 11:27 AM, Peter D. Gray wrote:\n>>\n>>> Personally, I don't think you should spec an encoding. It should be binary only and hex for developers and JSON interfaces. My thinking is that PSBT's are not user-visible things. They have a short lifetime and are nothing something that is \"stored\" or referenced much later. Hex is good enough and has no downsides as an excoding except for density.\n>> I think what will end up happening though is that, at least in the\n>>\n>> beginning, PSBTs will primarily be strings that people end up copy and\n>>\n>> pasting. Since a PSBT can get pretty large, the strings are rather\n>>\n>> cumbersome to move around, especially as hex. At least with Base64 the\n>>\n>> strings will be smaller.\n>>\n>>> On the other hand, suggesting a filename extension (probably .PSBT?) and a mime-type to match, are helpful since wallets and such will want to register with their operating systems to become handlers of those mimetypes. Really that's a lot more important for interoperability at this point, than an encoding.\n>> Agreed. I will include those in the BIP.\n>>\n>>> Looking forward to test vectors, and I might have more to say once my code can handle them (again).\n>>>\n>>> Feedback on the BIP as it stands now:\n>>>\n>>> -   Appendix A needs an update, and I suggest defining symbols (PK_PARTIAL_SIG == 0x02) for the numeric key values. This helps implementers as we don't all define our own symbols and/or use numeric constants in our code.\n>> Okay.\n>>\n>>> -   Those tables are just not working. Might want to reformat as descriptive lists, point form, or generally anything else... sorry.\n>> I will try my best to fix that. Mediawiki sucks...\n>>\n>> Andrew\n>>\n>> bitcoin-dev mailing list\n>>\n>> bitcoin-dev at lists.linuxfoundation.org\n>>\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Andrea",
                "date": "2018-06-24T09:00:53",
                "message_text_only": "Keeping it for future extensions is a good point, my understanding was that since we always need exactly one transaction it could be part of the definition of PSBT instead of being a key-value (although it is more of a breaking change). \n\n\nCheers, Andrea.\n\n\u200b\u200b\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n\nOn June 24, 2018 10:28 AM, Andrew Chow <achow101-lists at achow101.com> wrote:\n\n> \u200b\u200b\n> \n> I disagree with the idea that global types can be removed. Firstly, it\n> \n> is less of a breaking change to leave it there than to remove it\n> \n> entirely. Secondly, there may be a point in the future where global\n> \n> types would be useful/necessary. By having it still be there, we allow\n> \n> for future extensibility.\n> \n> Andrew\n> \n> On 06/24/2018 01:19 AM, Andrea wrote:\n> \n> > Hi,\n> > \n> > I think in the revised spec we can remove completely the \"global types\" as a map or even as typed record. Since there is only one type (the transaction) and it's compulsory to have one (and only one) we could just drop the definition of global type and the key associated with it, simply after the header + separator there must be a transaction. Having read all the discussion i also agree having per-input key derivation and per-output data is a lot more handy for signers, no special feeling regarding the encoding.Looking forward for the test vectors and the new spec.\n> > \n> > Cheers, Andrea.\n> > \n> > \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n> > \n> > On June 23, 2018 10:33 PM, Andrew Chow via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:\n> > \n> > > On 06/23/2018 10:00 AM, William Casarin wrote:\n> > > \n> > > > Since we're still considering the encoding, I wonder if it would be a\n> > > > \n> > > > good idea to have a human-readible part like lightning invoices[1]?\n> > > > \n> > > > I don't think that is necessary.\n> > > \n> > > > Then perhaps you could drop the magic code as well?\n> > > > \n> > > > The magic is still necessary for the binary format in order to prevent\n> > > \n> > > normal transaction deserializers from accidentally deserializing a psbt.\n> > > \n> > > > Also we could do a base encoding that excludes + and / characters, such\n> > > > \n> > > > as base62 (gmp-style). It's easier to copy/paste (double clicking a\n> > > > \n> > > > string stops at / or + in base64 encodings).\n> > > > \n> > > > While that would be ideal, I think it is better to use an encoding that\n> > > \n> > > most wallets already support. Most wallets already have Base64 decoding\n> > > \n> > > available so that they can decode signed messages which also use Base64\n> > > \n> > > encoding. I think it is unnecessary to introduce another encoding.\n> > > \n> > > On 06/23/2018 11:27 AM, Peter D. Gray wrote:\n> > > \n> > > > Personally, I don't think you should spec an encoding. It should be binary only and hex for developers and JSON interfaces. My thinking is that PSBT's are not user-visible things. They have a short lifetime and are nothing something that is \"stored\" or referenced much later. Hex is good enough and has no downsides as an excoding except for density.\n> > > > \n> > > > I think what will end up happening though is that, at least in the\n> > > \n> > > beginning, PSBTs will primarily be strings that people end up copy and\n> > > \n> > > pasting. Since a PSBT can get pretty large, the strings are rather\n> > > \n> > > cumbersome to move around, especially as hex. At least with Base64 the\n> > > \n> > > strings will be smaller.\n> > > \n> > > > On the other hand, suggesting a filename extension (probably .PSBT?) and a mime-type to match, are helpful since wallets and such will want to register with their operating systems to become handlers of those mimetypes. Really that's a lot more important for interoperability at this point, than an encoding.\n> > > > \n> > > > Agreed. I will include those in the BIP.\n> > > \n> > > > Looking forward to test vectors, and I might have more to say once my code can handle them (again).\n> > > > \n> > > > Feedback on the BIP as it stands now:\n> > > > \n> > > > -   Appendix A needs an update, and I suggest defining symbols (PK_PARTIAL_SIG == 0x02) for the numeric key values. This helps implementers as we don't all define our own symbols and/or use numeric constants in our code.\n> > > >     \n> > > >     Okay.\n> > > >     \n> > > \n> > > > -   Those tables are just not working. Might want to reformat as descriptive lists, point form, or generally anything else... sorry.\n> > > >     \n> > > >     I will try my best to fix that. Mediawiki sucks...\n> > > >     \n> > > \n> > > Andrew\n> > > \n> > > bitcoin-dev mailing list\n> > > \n> > > bitcoin-dev at lists.linuxfoundation.org\n> > > \n> > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "BIP 174 thoughts",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Greg Sanders",
                "William Casarin",
                "Jason Les",
                "Peter D. Gray",
                "Achow101",
                "Rodolfo Novak",
                "Andrea",
                "Gregory Maxwell",
                "Marek Palatinus",
                "Tomas Susanka",
                "Andrew Chow",
                "Pieter Wuille",
                "Jonas Schnelli",
                "matejcik"
            ],
            "messages_count": 42,
            "total_messages_chars_count": 164093
        }
    },
    {
        "title": "[bitcoin-dev]  BIP 174 thoughts",
        "thread_messages": [
            {
                "author": "matejcik",
                "date": "2018-06-29T09:53:34",
                "message_text_only": "Short version:\n\n- I propose that conflicting \"values\" for the same \"key\" are considered\ninvalid.\n- Let's not optimize for invalid data.\n- Given that, there's an open question on how to handle invalid data\nwhen encountered\n\nIn general, I don't think it's possible to enforce correctness at the\nformat level. You still need application level checks - and that calls\ninto question what we gain by trying to do this on the format level.\n\n\nLong version:\n\n\nLet's look at this from a different angle.\n\nThere are roughly two possible \"modes\" for the format with regard to\npossibly-conflicting data. Call them \"permissive\" and \"restrictive\".\n\nThe spec says:\n\"\"\"\nKeys within each scope should never be duplicated; all keys in the\nformat are unique. PSBTs containing duplicate keys are invalid. However\nimplementors will still need to handle events where keys are duplicated\nwhen combining transactions with duplicated fields. In this event, the\nsoftware may choose whichever value it wishes.\n\"\"\"\nThe last sentence of this paragraph sets the mode to permissive:\nduplicate values are pretty much OK. If you see them, just pick one.\n\nYou seem to argue that Combiners, in particular simple ones that don't\nunderstand field semantics, should merge _keys_ permissively, but\ndeduplicate _values_ restrictively.\nIOW: if you receive two different values for the same key, just pick\nwhichever, but $deity forbid you include both!\n\nThis choice doesn't make sense to me.\n\nWhat _would_ make sense is fully restrictive mode: receiving two\ndifferent values for the same key is a fatal condition with no recovery.\nIf you have a non-deterministic scheme, put a differentiator in the key.\nOr all the data, for that matter.\n(Incidentally, this puts key-aware and keyless Combiners on the same\nfooting. As long as all participants uphold the protocol, different\nvalue = different key = different full record.)\n\nGiven that, it's nice to have the Combiner perform the task of detecting\nthis and failing. But not at all necessary. As the quoted paragraph\ncorrectly notes, consumers *still need to handle* PSBTs with duplicate keys.\n(In this context, your implied permissive/restrictive Combiner is\noptimized for dealing with invalid data. That seems like a wrong\noptimization.)\n\nA reasonable point to decide is whether the handling at the consumer\nshould be permissive or restrictive. Personally I'm OK with either. I'd\ngo with the following change:\n\"\"\"\nIn this event, the software MAY reject the transaction as invalid. If it\ndecides to accept it, it MUST choose the last value encountered.\n\"\"\"\n(deterministic way of choosing, instead of \"whichever you like\")\n\nWe could also drop the first part, explicitly allowing consumers to\npick, and simplifying the Combiner algorithm to `sort -u`.\nNote that this sort of \"picking\" will probably be implicit. I'd expect\nthe consumer to look like this:\n```\nfor key, value in parse(nextRecord()):\n  data[key] = value\n```\n\nOr we could drop the second part and switch MAY to MUST, for a fully\nrestrictive mode - which, funnily enough, still lets the Combiner work\nas `sort -u`.\nTo see why, remember that distinct values for the same key are not\nallowed in fully restrictive mode. If a Combiner encounters two\nconflicting values F(1) and F(2), it should fail -- but if it doesn't,\nit includes both and the same failure WILL happen on the fully\nrestrictive consumer.\n\nThis was (or is) my point of confusion re Combiners: the permissive key\n+ restrictive value mode of operation doesn't seem to help subsequent\nconsumers in any way.\n\n\nNow, for the fully restrictive consumer, the key-value model is indeed\nadvantageous (and this is the only scenario that I can imagine in which\nit is advantageous), because you can catch key duplication on the parser\nlevel.\n\nBut as it turns out, it's not enough. Consider the following records:\nkey(<PSBT_IN_REDEEM_SCRIPT> + abcde), value(<some redeem script>)\nand:\nkey(<PSBT_IN_REDEEM_SCRIPT> + fghij), value(<some other redeem script>)\n\nA purely syntactic Combiner simply can't handle this case. The\nrestrictive consumer needs to know whether the key is supposed to be\nrepeating or not.\nWe could fix this, e.g., by saying that repeating types must have high\nbit set and non-repeating must not. We also don't have to, because the\nworst failure here is that a consumer passes an invalid record to a\nsubsequent one and the failure happens one step later.\n\nAt this point it seems weird to be concerned about the \"unique key\"\ncorrectness, which is a very small subset of possibly invalid inputs. As\na strict safety measure, I'd instead propose that a consumer MUST NOT\noperate on inputs or outputs, unless it understand ALL included fields -\nIOW, if you're signing a particular input, all fields in said input are\nmandatory. This prevents a situation where a simple Signer processes an\ninput incorrectly based on incomplete set of fields, while still\nallowing Signers with different capabilities within the same PSBT.\n\n(The question here is whether to have either a flag or a reserved range\nfor \"optional fields\" that can be safely ignored by consumers that don't\nunderstand them, but provide data for consumers who do.)\n\n\n>> To repeat and restate my central question: Why is it important, \n>> that an agent which doesn't understand a particular field \n>> structure, can nevertheless make decisions about its inclusion or \n>> omission from the result (based on a repeated prefix)?\n>> \n> \n> Again, because otherwise you may need a separate Combiner for each \n> type of script involved. That would be unfortunate, and is very \n> easily avoided.\n\nThis is still confusing to me, and I would really like to get to the\nsame page on this particular thing, because a lot of the debate hinges\non it. I think I covered most of it above, but there are still pieces to\nclarify.\n\nAs I understand it, the Combiner role (actually all the roles) is mostly\nan algorithm, with the implication that it can be performed\nindependently by a separate agent, say a network node.\n\nSo there's two types of Combiners:\n\na) Combiner as a part of an intelligent consumer -- the usual scenario\nis a Creator/Combiner/Finalizer/Extractor being one participant, and\nUpdater/Signers as other participants.\n\nIn this case, the discussion of \"simple Combiners\" is actually talking\nabout intelligent Combiners which don't understand new fields and must\ncorrectly pass them on. I argue that this can safely be done without\nloss of any important properties.\n\nb) Combiner as a separate service, with no understanding of semantics.\nAlthough parts of the debate seem to assume this scenario, I don't think\nit's worth considering. Again, do you have an usecase in mind for it?\n\nYou also insist on enforcing a limited form of correctness on the\nCombiner level, but that is not worth it IMHO, as discussed above.\n\nOr am I missing something else?\n\n\n> Perhaps you want to avoid signing with keys that are already signed \n> with? If you need to derive all the keys before even knowing what\n> was already signed with, you've already performed 80% of the work.\n\nThis wouldn't concern me at all, honestly. If the user sends an already\nsigned PSBT to the same signer, IMHO it is OK to sign again; the\nslowdown is a fault of the user/workflow. You could argue that signing\nagain is the valid response. Perhaps the Signer should even \"consume\"\nits keys and not pass them on after producing a signature? That seems\nlike a sensible rule.\n\n\n> To your point: proto v2 afaik has no way to declare \"whole record \n> uniqueness\", so either you drop that (which I think is unacceptable\n> - see the copy/sign/combine argument above), or you deal with it in \n> your application code.\n\nYes. My argument is that \"whole record uniqueness\" isn't in fact an\nimportant property, because you need application-level checks anyway.\nAdditionally, protobuf provides awareness of which fields are repeated\nand which aren't, and implicitly implements the \"pick last\" resolution\nstrategy for duplicates.\n\nThe simplest possible protobuf-based Combiner will:\n- assume all fields are repeating\n- concatenate and parse\n- deduplicate and reserialize.\n\nMore knowledgeable Combiner will intelligently handle non-repeating\nfields, but still has to assume that unknown fields are repeating and\nuse the above algorithm.\n\nFor \"pick last\" strategy, a consumer can simply parse the message and\nperform appropriate application-level checks.\n\nFor \"hard-fail\" strategy, it must parse all fields as repeating and\ncheck that there's only one of those that are supposed to be unique.\nThis is admittedly more work, and yes, protobuf is not perfectly suited\nfor this task.\n\nBut:\n\nOne, this work must be done by hand anyway, if we go with a custom\nhand-parsed format. There is a protobuf implementation for every\nconceivable platform, we'll never have the same amount of BIP174 parsing\ncode.\n(And if you're hand-writing a parser in order to avoid the dependency,\nyou can modify it to do the checks at parser level. Note that this is\nnot breaking the format! The modifed parser will consume well-formed\nprotobuf and reject that which is valid protobuf but invalid bip174 - a\ncorrect behavior for a bip174 parser.)\n\nTwo, it is my opinion that this is worth it in order to have a standard,\nwell described, well studied and widely implemented format.\n\n\nAside: I ha that there is no advantage to a record-set based\ncustom format by itself, so IMHO the choice is between protobuf vs\na custom key-value format. Additionally, it's even possible to implement\na hand-parsable key-value format in terms of protobuf -- again, arguing\nthat \"standardness\" of protobuf is valuable in itself.\n\nregards\nm.\n\n\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 819 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180629/358c1483/attachment-0001.sig>"
            }
        ],
        "thread_summary": {
            "title": "BIP 174 thoughts",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "matejcik"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 9909
        }
    },
    {
        "title": "[bitcoin-dev] Alert key retirement?",
        "thread_messages": [
            {
                "author": "Bryan Bishop",
                "date": "2018-06-17T17:46:00",
                "message_text_only": "Alert key has yet to be disclosed. The alert system itself has been retired\nfor quite a while now. More information about this can be found here:\nhttps://bitcoin.org/en/alert/2016-11-01-alert-retirement\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-September/013104.html\n\nRecently it was suggested to me that it would be helpful to wait for v0.13\nend-of-life before revealing the alert keys.\n\nI am seeking information regarding anyone that has requested final alert\nmessages for any particular projects that may have copied the bitcoin alert\npubkey.\n\nThank you.\n\n- Bryan\nhttp://heybryan.org/\n1 512 203 0507\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180617/5f87e0d9/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Alert key retirement?",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Bryan Bishop"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 804
        }
    },
    {
        "title": "[bitcoin-dev] Miner dilution attack on Bitcoin - is that something plausible?",
        "thread_messages": [
            {
                "author": "\u0410\u0440\u0442\u0451\u043c \u041b\u0438\u0442\u0432\u0438\u043d\u043e\u0432\u0438\u0447",
                "date": "2018-06-18T18:34:37",
                "message_text_only": "Dilution is a potential attack i randomly came up with in a Twitter\narguement and couldn't find any references to or convincing arguments of it\nbeing implausible.\n\nSuppose a malicious actor were to acquire a majority of hash power, and\nproceed to use that hash power to produce valid, but empty blocks.\n\nAs far as i understand it, this would effectively reduce the block rate by\nhalf or more and since nodes can't differentiate block relay and block\nproduction there would be nothing they can do to adjust difficulty or black\nlist the attacker.\n\nAt a rough estimate of $52 per TH equipment cost (Antminer pricing) and\n12.5 BTC per 10 minutes power cost we are looking at an order of $2 billion\nof equipment and $0.4 billion a month of power costs (ignoring block\nreward) to maintain an attack - easily within means of even a minor\ngovernment-scale actor.\n\nIs that a plausible scenario, or am i chasing a mirage? If it is plausible,\nwhat could be done to mitigate it?\n\n\n-Artem\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180618/92b9dd8a/attachment.html>"
            },
            {
                "author": "Alexander Leishman",
                "date": "2018-06-18T18:47:40",
                "message_text_only": "Well miners already regularly mine empty blocks. However, it is usually in\nthe economic interest of the miners to collect transaction fees. This\nincentive should hopefully be enough to prevent miners from choosing to\nproduce many empty blocks.\n\nIf a nation state attacker decides to allocate billions in resources to\nattack Bitcoin, then that is a bigger discussion. The risk there is\ndouble-spends, not empty blocks.\n\n-Alex\n\n\n\nOn Mon, Jun 18, 2018 at 11:39 AM \u0410\u0440\u0442\u0451\u043c \u041b\u0438\u0442\u0432\u0438\u043d\u043e\u0432\u0438\u0447 via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Dilution is a potential attack i randomly came up with in a Twitter\n> arguement and couldn't find any references to or convincing arguments of it\n> being implausible.\n>\n> Suppose a malicious actor were to acquire a majority of hash power, and\n> proceed to use that hash power to produce valid, but empty blocks.\n>\n> As far as i understand it, this would effectively reduce the block rate by\n> half or more and since nodes can't differentiate block relay and block\n> production there would be nothing they can do to adjust difficulty or black\n> list the attacker.\n>\n> At a rough estimate of $52 per TH equipment cost (Antminer pricing) and\n> 12.5 BTC per 10 minutes power cost we are looking at an order of $2 billion\n> of equipment and $0.4 billion a month of power costs (ignoring block\n> reward) to maintain an attack - easily within means of even a minor\n> government-scale actor.\n>\n> Is that a plausible scenario, or am i chasing a mirage? If it is\n> plausible, what could be done to mitigate it?\n>\n>\n> -Artem\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180618/35c7a580/attachment.html>"
            },
            {
                "author": "Eric Voskuil",
                "date": "2018-06-19T13:54:27",
                "message_text_only": "https://github.com/libbitcoin/libbitcoin/wiki/Other-Means-Principle\n\n>> On Mon, Jun 18, 2018 at 11:39 AM \u0410\u0440\u0442\u0451\u043c \u041b\u0438\u0442\u0432\u0438\u043d\u043e\u0432\u0438\u0447 via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> Dilution is a potential attack i randomly came up with in a Twitter arguement and couldn't find any references to or convincing arguments of it being implausible.\n>> \n>> Suppose a malicious actor were to acquire a majority of hash power, and proceed to use that hash power to produce valid, but empty blocks.\n>> \n>> As far as i understand it, this would effectively reduce the block rate by half or more and since nodes can't differentiate block relay and block production there would be nothing they can do to adjust difficulty or black list the attacker.\n>> \n>> At a rough estimate of $52 per TH equipment cost (Antminer pricing) and 12.5 BTC per 10 minutes power cost we are looking at an order of $2 billion of equipment and $0.4 billion a month of power costs (ignoring block reward) to maintain an attack - easily within means of even a minor government-scale actor.\n>> \n>> Is that a plausible scenario, or am i chasing a mirage? If it is plausible, what could be done to mitigate it?\n>> \n>> \n>> -Artem\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180619/5cd952ac/attachment.html>"
            },
            {
                "author": "Laszlo Hanyecz",
                "date": "2018-06-18T18:49:09",
                "message_text_only": "On 2018-06-18 18:34, \u0410\u0440\u0442\u0451\u043c \u041b\u0438\u0442\u0432\u0438\u043d\u043e\u0432\u0438\u0447 via bitcoin-dev wrote:\n> Suppose a malicious actor were to acquire a majority of hash power, and\n> proceed to use that hash power to produce valid, but empty blocks.\n\nhttps://github.com/libbitcoin/libbitcoin/wiki/Empty-Block-Fallacy"
            },
            {
                "author": "Bram Cohen",
                "date": "2018-06-18T20:40:26",
                "message_text_only": "Not sure what you're saying here. The block rate can't be particularly\nincreased or decreased in the long run due to the work difficulty\nadjustment getting you roughly back where you started no matter what.\nSomeone could DOS the system by producing empty blocks, sure, that's a\ncentral attack of what can happen when someone does a 51% attack with no\nspecial countermeasures other than everything that Bitcoin does at its\ncore. An attacker or group of attackers could conspire to reduce block\nsizes in order to increase transaction fees, in fact they could do that\nwith a miner activated soft fork. That appears both doable and given past\nthings which have happened with transaction fees in the past potentially\nlucrative, particularly as block rewards fall in the future. Please don't\ntell the big mining pools about it.\n\nOn Mon, Jun 18, 2018 at 11:39 AM \u0410\u0440\u0442\u0451\u043c \u041b\u0438\u0442\u0432\u0438\u043d\u043e\u0432\u0438\u0447 via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Dilution is a potential attack i randomly came up with in a Twitter\n> arguement and couldn't find any references to or convincing arguments of it\n> being implausible.\n>\n> Suppose a malicious actor were to acquire a majority of hash power, and\n> proceed to use that hash power to produce valid, but empty blocks.\n>\n> As far as i understand it, this would effectively reduce the block rate by\n> half or more and since nodes can't differentiate block relay and block\n> production there would be nothing they can do to adjust difficulty or black\n> list the attacker.\n>\n> At a rough estimate of $52 per TH equipment cost (Antminer pricing) and\n> 12.5 BTC per 10 minutes power cost we are looking at an order of $2 billion\n> of equipment and $0.4 billion a month of power costs (ignoring block\n> reward) to maintain an attack - easily within means of even a minor\n> government-scale actor.\n>\n> Is that a plausible scenario, or am i chasing a mirage? If it is\n> plausible, what could be done to mitigate it?\n>\n>\n> -Artem\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180618/98b4fa44/attachment.html>"
            },
            {
                "author": "Bryan Bishop",
                "date": "2018-06-18T20:51:33",
                "message_text_only": "On Mon, Jun 18, 2018 at 3:40 PM, Bram Cohen via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Not sure what you're saying here. The block rate can't be particularly\n> increased or decreased in the long run due to the work difficulty\n> adjustment getting you roughly back where you started no matter what.\n> Someone could DOS the system by producing empty blocks, sure, that's a\n> central attack of what can happen when someone does a 51% attack with no\n> special countermeasures other than everything that Bitcoin does at its\n> core. An attacker or group of attackers could conspire to reduce block\n> sizes in order to increase transaction fees, in fact they could do that\n> with a miner activated soft fork. That appears both doable and given past\n> things which have happened with transaction fees in the past potentially\n> lucrative, particularly as block rewards fall in the future. Please don't\n> tell the big mining pools about it.\n>\n\nBram, actually I thought the previous discussions determined that less than\n51% hashrate would be required for certain soft-hard-forks employing empty\nblocks?\n\nI don't have a specific reference:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-February/012377.html\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-February/012457.html\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-December/013332.html\n\n- Bryan\nhttp://heybryan.org/\n1 512 203 0507\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180618/dfdb18a4/attachment.html>"
            },
            {
                "author": "Richard Hein",
                "date": "2018-06-19T18:58:01",
                "message_text_only": "It's important therefore to ensure that everyone can make ASICs, IMHO.\n\nOn Mon, Jun 18, 2018 at 2:34 PM, \u0410\u0440\u0442\u0451\u043c \u041b\u0438\u0442\u0432\u0438\u043d\u043e\u0432\u0438\u0447 via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Dilution is a potential attack i randomly came up with in a Twitter\n> arguement and couldn't find any references to or convincing arguments of it\n> being implausible.\n>\n> Suppose a malicious actor were to acquire a majority of hash power, and\n> proceed to use that hash power to produce valid, but empty blocks.\n>\n> As far as i understand it, this would effectively reduce the block rate by\n> half or more and since nodes can't differentiate block relay and block\n> production there would be nothing they can do to adjust difficulty or black\n> list the attacker.\n>\n> At a rough estimate of $52 per TH equipment cost (Antminer pricing) and\n> 12.5 BTC per 10 minutes power cost we are looking at an order of $2 billion\n> of equipment and $0.4 billion a month of power costs (ignoring block\n> reward) to maintain an attack - easily within means of even a minor\n> government-scale actor.\n>\n> Is that a plausible scenario, or am i chasing a mirage? If it is\n> plausible, what could be done to mitigate it?\n>\n>\n> -Artem\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180619/e96223a6/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Miner dilution attack on Bitcoin - is that something plausible?",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Laszlo Hanyecz",
                "Bryan Bishop",
                "Eric Voskuil",
                "Alexander Leishman",
                "Richard Hein",
                "\u0410\u0440\u0442\u0451\u043c \u041b\u0438\u0442\u0432\u0438\u043d\u043e\u0432\u0438\u0447",
                "Bram Cohen"
            ],
            "messages_count": 7,
            "total_messages_chars_count": 10464
        }
    },
    {
        "title": "[bitcoin-dev] [Lightning-dev] eltoo: A Simplified update Mechanism for Lightning and Off-Chain Contracts",
        "thread_messages": [
            {
                "author": "Rusty Russell",
                "date": "2018-06-22T00:32:01",
                "message_text_only": "\"David A. Harding\" <dave at dtrt.org> writes:\n> On Tue, Jun 19, 2018 at 02:02:51PM -0400, David A. Harding wrote:\n>> Anyone can rewrite a SIGHASH_NOINPUT input's outpoint, but the actual\n>> transaction containing the settlement is expected to have (at least) two\n>> inputs, with the second one originating the fees.  That second input's\n>> signature is (I assume) using SIGHASH_ALL to commit to all outpoints in\n>> the transaction, so it can't be arbitrarily rewritten by a third-party\n>> to apply to a different state outpoint\n>\n> I realized that the fee-paying input could possibly be signed with\n> SIGHASH_ALL|SIGHASH_ANYONECANPAY to allow anyone to arbitrarily\n> rewrite the other input signed with SIGHASH_NOINPUT.  However, this\n> reminded me of the well-known DoS against transactions signed with\n> SIGHASH_ANYONECANPAY[1], which seems to apply generally against\n> SIGHASH_NOINPUT as well and may allow theft from HTLCs.\n\nYes, RBF Rule #3 again :( It makes RBF unusable in adversarial\nconditions, and it's not miner incentive-compatible.\n\nThe only mitigations I have been able to come up with are:\n\n1. Reduce the RBF grouping depth to 2, not 10.  This doesn't help\n   here though, since you can still have ~infinite fan-out of txs\n   (create 1000 outputs, spend each with a 400ksipa tx).\n\n2. Revert #3 to a simple \"greater feerate\" rule, but delay propagation\n   proportional to tx weight, say 60 seconds (fuzzed) for a 400 ksipa\n   tx.  That reduces your ability to spam the network (you can always\n   connect directly to nodes and waste their time and bandwidth, but you\n   can do that pretty much today).\n\nFrankly, I'd also like a similar mechanism to not reject low-fee txs\n(above 250 satoshi per ksipa) but simply not propagate them.  Drop them\nafter 60 seconds if there's no CPFP to increase their effective feerate.\nThat would allow us to use CPFP on lightning commitment txs today,\nwithout having to guess what fees will be sometime in the future.\n\nCheers,\nRusty.\n\n> ## DoS against Eltoo settlements\n>\n> Alice and Mallory have a channel with some state updates.  Alice tries\n> to initiate a cooperate close, but Mallory stalls and instead broadcasts\n> the trigger transaction and the first state (state 0).  Notably, the\n> first state is bundled into a very large vsize transaction with a low\n> feerate.  State 1 is added to another very large low-feerate\n> transaction, as are states 2 through 9. \n>\n> Alice could in theory RBF the state 0 transaction, but per BIP125 rule\n> #3, she needs to pay an absolute fee greater than all the transactions\n> being replaced (not just a higher feerate).  That could cost a lot.\n> Alice could also create a transaction that binds the final state to the\n> state 9 transaction and attempt CPFP, but increasing the feerate for the\n> transaction ancestor group to a satisfactory degree would cost the same\n> amount as RBF.\n>\n> So Alice is stuck waiting for states 0-9 to confirm before the final\n> state can be confirmed.  During recent periods of full mempools on\n> default nodes, the waiting time for 10 nBTC/vbyte transactions has been\n> more than two weeks.\n>\n> ## HTLC theft\n>\n> If Mallory is able to introduce significant settlement delays, HTLC\n> security is compromised.  For example, imagine this route:\n>\n>     Mallory <-> Alice <-> Bob\n>\n> Mallory orders a widget from Bob and pays via LN by sending 1 BTC to\n> Alice hashlocked and timelocked, which Alice forwards to Bob also\n> hashlocked and timelocked.  Mallory releases the preimage to Bob, who\n> claims the funds from Alice and ships the widget, giving Alice the\n> preimage.\n>\n> At this point, Mallory broadcasts the transactions described in the\n> preceding section.\n>\n> If the low feerate of states 0-9 prevent them from confirming before the\n> timeout, Mallory can create a transaction containing a dishonest final\n> state that executes the refund branch.  Like before, she can bury this\n> in an ancestor transaction chain that would be cost prohibitive for Alice\n> to RBF.\n>\n> Considered independently, this is a very expensive attack for Mallory,\n> and so perhaps impractical.  But Mallory can join forces with someone\n> already creating large low-feerate consolidation transactions.  Better\n> yet, from Mallory's perspective, she can execute the attack against\n> hundreds of channels at once (creating long chains of ancestor\n> transactions that are large in aggregate rather than individually\n> large), using the aggregate size of all the victims' channels against\n> each of the individual victims.\n>\n> Thanks,\n>\n> -Dave\n>\n> [1] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2014-August/006438.html\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            }
        ],
        "thread_summary": {
            "title": "eltoo: A Simplified update Mechanism for Lightning and Off-Chain Contracts",
            "categories": [
                "bitcoin-dev",
                "Lightning-dev"
            ],
            "authors": [
                "Rusty Russell"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 4821
        }
    },
    {
        "title": "[bitcoin-dev] BetterHash status",
        "thread_messages": [
            {
                "author": "Casciano, Anthony",
                "date": "2018-06-26T14:32:06",
                "message_text_only": "What is the status of Matt Corallo's \"BetterHash\" BIP??   I recommend it\n\ngoes into production sooner than later.  Any 2nd's ?\n\n\nThanks in advance!\n\nTony Cash\n\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180626/96b932ab/attachment.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2018-06-26T14:44:02",
                "message_text_only": "Things go into production when people decide to adopt them, not before. You're welcome to contribute to the implementation at https://github.com/TheBlueMatt/mining-proxy\n\nOn June 26, 2018 2:32:06 PM UTC, \"Casciano,\tAnthony via bitcoin-dev\" <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>What is the status of Matt Corallo's \"BetterHash\" BIP??   I recommend\n>it\n>\n>goes into production sooner than later.  Any 2nd's ?\n>\n>\n>Thanks in advance!\n>\n>Tony Cash\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180626/e2a4a965/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "BetterHash status",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Casciano, Anthony",
                "Matt Corallo"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 983
        }
    },
    {
        "title": "[bitcoin-dev] BIP039 - How to add a Portuguese wordlist?",
        "thread_messages": [
            {
                "author": "Breno Brito",
                "date": "2018-06-26T15:58:53",
                "message_text_only": "Hello,\n\nSince Portuguese is considered the 6th most spoken language in the world\nand is an official language in 10 countries, I'd like to propose the\nexpansion of the BIP039 wordlist to Portuguese or help if someone had\nalready proposed it. What should I do?\n\nRegards,\nBreno\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180626/a744fa69/attachment.html>"
            },
            {
                "author": "AJ West",
                "date": "2018-06-30T15:12:32",
                "message_text_only": "Hi Breno,\n\nThere has been discussion on various ways to improve multilingual usage of\nBIP39. https://lists.linuxfoundation.org/pipermail/bitcoin-dev/\n2018-January/015500.html\n\nMy takeaway is that we should be implementing a standard way to convert any\nwords into a hex string, regardless of language or character encoding. In\nthat discussion there was some notion of \"who needs anything but English\nwords anyway?\" rhetoric, so I'm happy to see yet again that there exists an\nobvious need to support all languages for a standard in wallet seed\ngeneration.\n\nI realise that doesn't help you today considering there is no such\nmultilingual standard or official Portuguese wordlist, but I wanted to\nexpress that this is an ongoing topic of discussion and it's clear we need\nto start choosing/codifying a standard in this regard.\n\nThat said, here's somebody who has started a Portuguese wordlist so maybe\nyou can start from there.\nhttps://github.com/bitpay/bitcore-mnemonic/issues/52\n\nRegards,\nAJ\n\n\n\n\nOn Tue, Jun 26, 2018 at 11:58 AM, Breno Brito via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hello,\n>\n> Since Portuguese is considered the 6th most spoken language in the world\n> and is an official language in 10 countries, I'd like to propose the\n> expansion of the BIP039 wordlist to Portuguese or help if someone had\n> already proposed it. What should I do?\n>\n> Regards,\n> Breno\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180630/d3405407/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "BIP039 - How to add a Portuguese wordlist?",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Breno Brito",
                "AJ West"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 2230
        }
    },
    {
        "title": "[bitcoin-dev] Testnet block generation",
        "thread_messages": [
            {
                "author": "Mattia Rossi",
                "date": "2018-06-28T13:19:46",
                "message_text_only": "Hi all,\n\nMaybe is a common 'issue' but why the testnet3 of bitcoin network\ngenerate a block so fast?\nat lead 6 block per minute.\n\nIs this a normal behavior?\n\nThanks in advance."
            },
            {
                "author": "Jameson Lopp",
                "date": "2018-06-28T17:33:58",
                "message_text_only": "This is normal behavior due to a special rule on testnet. For a detailed\nexplanation you can read\nhttps://web.archive.org/web/20160910173004/https://blog.blocktrail.com/2015/04/in-the-darkest-depths-of-testnet3-16k-blocks-were-found-in-1-day/\n\n- Jameson\n\nOn Thu, Jun 28, 2018 at 9:22 AM Mattia Rossi via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi all,\n>\n> Maybe is a common 'issue' but why the testnet3 of bitcoin network\n> generate a block so fast?\n> at lead 6 block per minute.\n>\n> Is this a normal behavior?\n>\n> Thanks in advance.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180628/9251fbf6/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Testnet block generation",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Mattia Rossi",
                "Jameson Lopp"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 1105
        }
    },
    {
        "title": "[bitcoin-dev] Graftroot: Private and efficient surrogate scripts under the taproot assumption",
        "thread_messages": [
            {
                "author": "Sjors Provoost",
                "date": "2018-06-30T11:49:36",
                "message_text_only": "> Op 22 feb. 2018, om 13:19 heeft Ryan Grant via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> het volgende geschreven:\n> \n> On Fri, Feb 9, 2018 at 7:29 AM, Jeremy <jlrubin at mit.edu> wrote:\n>> utility of this construction can be improved if we introduce functionality\n>> that makes a script invalid after a certain time\n> \n> Tagging this thread with \"nExpiryTime\".  Search archives for more.\n\nFill-or-kill transaction was the term used on the list:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-September/011042.html <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-September/011042.html>\n\nSjors\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180630/249bd366/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Graftroot: Private and efficient surrogate scripts under the taproot assumption",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Sjors Provoost"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 817
        }
    }
]