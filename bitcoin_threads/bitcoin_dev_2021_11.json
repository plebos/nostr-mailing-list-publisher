[
    {
        "title": "[bitcoin-dev] Covenant opcode proposal OP_CONSTRAINDESTINATION (an alternative to OP_CTV)",
        "thread_messages": [
            {
                "author": "Billy Tetrud",
                "date": "2021-11-01T01:19:42",
                "message_text_only": "FYI I broke out the fee limiting functionality from OP_CD into an opcode\ncalled OP_LIMITFEECONTRIBUTION\n<https://github.com/fresheneesz/bip-efficient-bitcoin-vaults/blob/main/lfc/bip-limit-fee-contribution.md>\nas\nJeremy suggested.\n\nOn Fri, Jul 30, 2021 at 1:42 PM Billy Tetrud <billy.tetrud at gmail.com> wrote:\n\n> Thanks for taking another look Jeremy. That's an interesting idea to split\n> it up into simpler opcodes, however there are some\n> limitations/considerations there.\n>\n> For example, with output addresses, I added specifying amounts to outputs\n> in order to make script evaluation simpler and eliminate a potential DOS\n> vector. I wrote about this in the section 'Specifying values sent to each\n> output\n> <https://github.com/fresheneesz/bip-efficient-bitcoin-vaults/blob/main/cd/bip-constraindestination.md#specifying-values-sent-to-each-output>'.\n> Originally, I designed OP_CD without specifying what amounts an input\n> contributes to what outputs, but it seemed like this would require\n> calculating various combinations of inequalities, which could get expensive\n> in scenarios where many inputs had overlapping destinations. See the\n> examples under the OP_CD section in this commit\n> <https://github.com/fresheneesz/bip-efficient-bitcoin-vaults/commit/9b2257410b5f0fc991f68e774c3faf601c02cc5d>\n> .\n>\n> Maybe there's an elegant and cheap way of verifying that a number of\n> inputs that have destination address limitations is within limits, but if\n> so I don't know how to do that. If there was a good way to do that, then I\n> wouldn't want to propose the ability to validate that specific amounts go\n> to specific outputs. So unless there's a simple and dos-vector-free way of\n> evaluating what addresses an input goes to without knowing what amounts an\n> input contributes to each output, I don't think these functionalities\n> should be separated.\n>\n> And about a fee-limit opcode, that could certainly be done on its own.\n> However, a version of OP_CD that doesn't specify fees would have to take\n> the fee-limit into account, and the calculation for the stand-alone\n> fee-limit operation would be moot for that output.\n>\n> So I think it could make sense to split the fee limit off from the rest of\n> OP_CD. I'm curious to know what others think of that.\n>\n> > all transactions are twice as large as they might otherwise need to be\n> for simple things like congestion control trees, since you have to repeat\n> all of the output data twice\n>\n> Well, the transaction wouldn't be quite twice as large. Each output would\n> add 9 bytes to the transaction, and outputs already are a minimum of about\n> 30 bytes I think? So for transactions with a lot of outputs, it could make\n> the transaction about 1/3 larger. I'll add a section on this into my\n> proposal.\n>\n> Perhaps it would be a reasonable optimization to allow omitting an output\n> value in cases where the entire output amount is contributed by that input.\n> This would reduce the overhead of specifying output amounts to 2 bytes for\n> most outputs (1 byte for the index, another to indicate the full value),\n> meaning that it would only make the transaction about 7% larger. What do\n> you think about that idea?\n>\n> On Wed, Jul 28, 2021 at 3:30 PM Jeremy via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> High level feedback:\n>>\n>> you should spec out the opcodes as separate pieces of functionality as it\n>> sounds like OP_CD is really 3 or 4 opcodes in one (e.g., amounts to\n>> outputs, output addresses, something with fees).\n>>\n>> One major drawback of your approach is that all transactions are twice as\n>> large as they might otherwise need to be for simple things like congestion\n>> control trees, since you have to repeat all of the output data twice.\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211031/be0e60ae/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Covenant opcode proposal OP_CONSTRAINDESTINATION (an alternative to OP_CTV)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Billy Tetrud"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 4136
        }
    },
    {
        "title": "[bitcoin-dev] BIP341 test vectors for wallet implementations",
        "thread_messages": [
            {
                "author": "Pieter Wuille",
                "date": "2021-11-01T18:48:49",
                "message_text_only": "Hi all,\n\nI wanted to bring some attention to a set of test vectors I'm proposing to add to BIP341 in https://github.com/bitcoin/bips/pull/1225.\n\nThese are focused on wallet implementations, covering Merkle root / tweak / scriptPubKey computation from key/scripts, sigmsg/sighash/signature computation for key path spending, and control block computation for script path spending.\n\nGiven the short time that remains before BIP341's activation, I think this may be helpful to people working on implementations, even before it's merged. All values in it are automatically generated from an actual scenario tested against Bitcoin Core (which involves constructing and mining transactions with specified data).\n\nThe tests are mostly focused on features that are likely useful/testable in implementations right now, and e.g. excludes sighashes that involve annexes for that reason. It's also specific to BIP341, and doesn't cover BIP342 script semantics. Still, if anyone relies on features that are useful, but aren't covered, I'm happy to add more scenarios.\n\nCheers,\n\n--\nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211101/10da2406/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "BIP341 test vectors for wallet implementations",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Pieter Wuille"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1257
        }
    },
    {
        "title": "[bitcoin-dev] death to the mempool, long live the mempool",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2021-11-03T10:12:57",
                "message_text_only": "Good morning list,\n\n\n\n> I describe here an algorithm based on semispace GC, but the GC algorithm space is well-studied and other algorithms may also be devised (in particular, spam is likely to match quite well with \"infant mortality\" concept in GC, i.e. \"most objects die young\", so some kind of nursery / generational GC may work better against spam in practice).\n\n\nThis \"nursery GC\" variation would be very similar to these \"pre-mempool\" / \"memswamp\" proposals, as well:\n\n* https://github.com/bitcoin/bitcoin/issues/14895#issuecomment-665975441\n* https://github.com/bitcoin/bitcoin/issues/14895#issuecomment-666267749\n\nIt may be better to continue thinking along those lines than to consider this GC concept.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "death to the mempool, long live the mempool",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 730
        }
    },
    {
        "title": "[bitcoin-dev] Neutrino, Taproot, and The Evolution of BiPs 157/158",
        "thread_messages": [
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2021-11-04T22:01:13",
                "message_text_only": "Hi y'all,\n\nIf you're an active user of neutrino [8], then you probably heard about what\nwent down over the past week or so on testnet as related to Taproot. First,\ni just wanted to reassure everyone that nothing is fundamentally broken with\nBIP 157/158 as it relates to taproot, and we already have a mitigation patch\nin place for the issue we encountered.\n\nThe rest of this mail is structured in a FAQ style to make it easy to skim\nand extract the information that may be relevant to the reader.\n\n## What happened on testnet?\n\nNeutrino nodes on testnet rejected a filter (thinking it was invalid) due\nto this transaction spending a taproot input [1]. This was due to a faulty\nheuristics in the neutrino _client code_ (not part of the protocol) that\nattempted to verify the contents of a filter more completely.\n\nIn retrospect, the heuristic in question wasn't full proof, as it attempted\nto derive the _pk script_ of a transaction based on its input\nwitness/sigScript. This worked pretty well in the context of segwit v0, but\nit isn't possible to exhaustively do as we don't know what future spends\nwill look like.\n\n## Is neutrino broken?\n\nNo, the client side is fine, and the protocol is fine.\n\nThe problematic heuristic has been removed in this PR [2], which will be\nincluded in lnd 0.14, and has been tagged with neutrino 0.13 [3].\n\nTo dig into _why_ we attempted to use such a heuristic, we'll need to\nrevisit how BIP 158 works briefly. For each block, we insert the `pkScript`s\nof all the outputs, and also the prev out's pkScript (the script being\nspent) as well. This lets the filter compress script re-use in both inputs\nand outputs, and also makes it possible to implement some protocols in a\nmore light-client friendly manner (for example Loop uses this to has the\nclient watch HTLC _scripts_ being spent, as it doesn't necessarily know the\ntxid/outpoint).\n\nThe one issue with this, is that while clients can ensure that all the\n`pkScripts` of outputs have been inserted, they can't do the same for the\ninputs (which is why we added that heuristic in the client code). Luckily we\nknow how to properly fix this at the protocol level, more on that below.\n\n## How can I make sure my neutrino clients handle the Taproot upgrade on\nmainnet smoothly?\n\nUpgrade to 0.14 (assuming it's out in time), or apply this small patch [4].\nThe patch just demotes an error case to a warning message, so anyone running\na fork should be able to easily apply the fix.\n\nAlongside, optionally extend these filter header guides [7].\n\nWe're looking into some intermediate ground where we can verify the scripts\nthat we know are relevant to the node.\n\n## How will BIP 158/157 evolve post taproot?\n\nIn terms of adding more taproot specific functionality, I've had a number of\nitems in my laundry list such as:\n\n  * creating new segwit-only filters with re-parameterized fp rates (also\n    examine other filter types such as pure outpoints, etc)\n\n  * creating filters that include witness data to allow matching on\n    internal/external keys, the control block, merkle root, annex, etc\n\n  * add a new protocol extension to btcd (with a corresponding BIP) to\n    allow notes to fetch block undo data (as described here [5]) to fully\n    verify fetched filters or a node needs to reconcile conflicting filters\n\n  * new filters that span across multiple blocks as previously worked on by\n    Kalle Alm (couldn't find a link to his PR when typing this...)\n\nMake further progress towards a proposal that allows filters to be committed\neither as a soft-fork, or a \"velvet fork\" [6] where miners optionally\ncommit to\nthe past filter header chain.\n\n\n-- Laolu\n\n[1]:\nhttps://mempool.space/testnet/tx/4b425a1f5c0fcf4794c48b810c53078773fb768acd2be1398e3f561cc3f19fb8\n[2]: https://github.com/lightninglabs/neutrino/pull/234\n[3]: https://github.com/lightninglabs/neutrino/releases/tag/v0.13.0\n[4]: https://github.com/lightninglabs/neutrino/pull/234/files\n[5]:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-February/016649.html\n[6]: https://eprint.iacr.org/2018/087\n[7]:\nhttps://github.com/lightninglabs/neutrino/blob/5e09bd9b5d65e90c6ff07aa11b3b9d80d42afb86/chainsync/filtercontrol.go#L15\n[8]: https://github.com/lightninglabs/neutrino\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211104/e5d7b150/attachment.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2021-11-04T22:07:52",
                "message_text_only": "> In terms of adding more taproot specific functionality, I've had a number\nof\n> items in my laundry list such as:\n\nForgot to add this other item (also the list wasn't meant to be only tapoot\nstuff):\n  * reviving old projects to include a micropayment-for-data layer to\n    incentivize nodes to serve the filters and other data\n\nOn Thu, Nov 4, 2021 at 3:01 PM Olaoluwa Osuntokun <laolu32 at gmail.com> wrote:\n\n> Hi y'all,\n>\n> If you're an active user of neutrino [8], then you probably heard about\n> what\n> went down over the past week or so on testnet as related to Taproot. First,\n> i just wanted to reassure everyone that nothing is fundamentally broken\n> with\n> BIP 157/158 as it relates to taproot, and we already have a mitigation\n> patch\n> in place for the issue we encountered.\n>\n> The rest of this mail is structured in a FAQ style to make it easy to skim\n> and extract the information that may be relevant to the reader.\n>\n> ## What happened on testnet?\n>\n> Neutrino nodes on testnet rejected a filter (thinking it was invalid) due\n> to this transaction spending a taproot input [1]. This was due to a faulty\n> heuristics in the neutrino _client code_ (not part of the protocol) that\n> attempted to verify the contents of a filter more completely.\n>\n> In retrospect, the heuristic in question wasn't full proof, as it attempted\n> to derive the _pk script_ of a transaction based on its input\n> witness/sigScript. This worked pretty well in the context of segwit v0, but\n> it isn't possible to exhaustively do as we don't know what future spends\n> will look like.\n>\n> ## Is neutrino broken?\n>\n> No, the client side is fine, and the protocol is fine.\n>\n> The problematic heuristic has been removed in this PR [2], which will be\n> included in lnd 0.14, and has been tagged with neutrino 0.13 [3].\n>\n> To dig into _why_ we attempted to use such a heuristic, we'll need to\n> revisit how BIP 158 works briefly. For each block, we insert the\n> `pkScript`s\n> of all the outputs, and also the prev out's pkScript (the script being\n> spent) as well. This lets the filter compress script re-use in both inputs\n> and outputs, and also makes it possible to implement some protocols in a\n> more light-client friendly manner (for example Loop uses this to has the\n> client watch HTLC _scripts_ being spent, as it doesn't necessarily know the\n> txid/outpoint).\n>\n> The one issue with this, is that while clients can ensure that all the\n> `pkScripts` of outputs have been inserted, they can't do the same for the\n> inputs (which is why we added that heuristic in the client code). Luckily\n> we\n> know how to properly fix this at the protocol level, more on that below.\n>\n> ## How can I make sure my neutrino clients handle the Taproot upgrade on\n> mainnet smoothly?\n>\n> Upgrade to 0.14 (assuming it's out in time), or apply this small patch [4].\n> The patch just demotes an error case to a warning message, so anyone\n> running\n> a fork should be able to easily apply the fix.\n>\n> Alongside, optionally extend these filter header guides [7].\n>\n> We're looking into some intermediate ground where we can verify the scripts\n> that we know are relevant to the node.\n>\n> ## How will BIP 158/157 evolve post taproot?\n>\n> In terms of adding more taproot specific functionality, I've had a number\n> of\n> items in my laundry list such as:\n>\n>   * creating new segwit-only filters with re-parameterized fp rates (also\n>     examine other filter types such as pure outpoints, etc)\n>\n>   * creating filters that include witness data to allow matching on\n>     internal/external keys, the control block, merkle root, annex, etc\n>\n>   * add a new protocol extension to btcd (with a corresponding BIP) to\n>     allow notes to fetch block undo data (as described here [5]) to fully\n>     verify fetched filters or a node needs to reconcile conflicting filters\n>\n>   * new filters that span across multiple blocks as previously worked on by\n>     Kalle Alm (couldn't find a link to his PR when typing this...)\n>\n> Make further progress towards a proposal that allows filters to be\n> committed\n> either as a soft-fork, or a \"velvet fork\" [6] where miners optionally\n> commit to\n> the past filter header chain.\n>\n>\n> -- Laolu\n>\n> [1]:\n> https://mempool.space/testnet/tx/4b425a1f5c0fcf4794c48b810c53078773fb768acd2be1398e3f561cc3f19fb8\n> [2]: https://github.com/lightninglabs/neutrino/pull/234\n> [3]: https://github.com/lightninglabs/neutrino/releases/tag/v0.13.0\n> [4]: https://github.com/lightninglabs/neutrino/pull/234/files\n> [5]:\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-February/016649.html\n> [6]: https://eprint.iacr.org/2018/087\n> [7]:\n> https://github.com/lightninglabs/neutrino/blob/5e09bd9b5d65e90c6ff07aa11b3b9d80d42afb86/chainsync/filtercontrol.go#L15\n> [8]: https://github.com/lightninglabs/neutrino\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211104/f67e66e8/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Neutrino, Taproot, and The Evolution of BiPs 157/158",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Olaoluwa Osuntokun"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 9394
        }
    },
    {
        "title": "[bitcoin-dev] bitcoin.org missing bitcoin core version 22.0",
        "thread_messages": [
            {
                "author": "Prayank",
                "date": "2021-11-05T08:17:22",
                "message_text_only": "Hi Kate,\n\n> He is taking the most sensible way forward, decreasing bus factor.\n\nAgree. Work being shared with other maintainers is an improvement.\n\n> Read: https://laanwj.github.io/2021/01/21/decentralize.html\n\nInteresting blog post. First paragraph talks about strange expectations, not sure what other people expected however I expected present maintainers will always have respect for the Founder of Bitcoin, keep important docs in repository, website etc. forever and respond with appropriate things if any rich scammers try to remove anything important. Anyway that chapter is over and this PR will always remain in history for others to see and make their own opinions about it: https://github.com/bitcoin-core/bitcoincore.org/pull/740\n\nWhat followed it (whitepaper being shared on different websites) was true decentralization and we need something similar in other aspects of full node implementations. Few things that can improve decentralization:\n\n1.More people using alternative full node implementations. Right now 98% of nodes use Bitcoin Core.\n2.More people like Luke Dashjr and Amir Taaki who do not simp for anyone. Being a contributor or maintainer in Bitcoin full node implementation is different from other open source projects. It was never going to be easy and it will get difficult with time,\n3.More people from different countries getting involved in important roles.\n4.Few anons.\n5.Individuals and organizations who fund different Bitcoin projects should consider contributing in alternative. full node implementations as well. Maybe start with Bitcoin Knots.\n\nI am sure lot of people will find this controversial or disagree with it however this is my opinion and things that I think can improve Bitcoin. Will quote something from my recent medium post about a dev meetup and Knots:\n\nAccepting the problems, looking for solutions and trying to improve things is the best approach we as engineers can follow to do better things in Bitcoin. Irrational optimism is as toxic as irrational pessimism.\n\nhttps://prayankgahlot.medium.com/op-halloween21-and-bitcoin-knots-b8a4da4fa0bd\n\nOnly ~1337 blocks left for Taproot to activate. So cheers to another soft fork being a success and Bitcoin improving regularly. Thanks to everyone who contributed including reviewers. Hoping most of the people will start using latest version of Bitcoin Core or other full node implementations soon.\n.\n-- \nPrayank\n\nA3B1 E430 2298 178F\n\n\n\nOct 21, 2021, 01:48 by mercedes.catherine.salazar at gmail.com:\n\n> Hi Owen,\n>\n> On Wed, Oct 20, 2021 at 9:25 PM Owen Gunden via bitcoin-dev <> bitcoin-dev at lists.linuxfoundation.org> > wrote:\n>\n>> On Wed, Oct 20, 2021 at 04:47:17PM +0200, Prayank wrote:\n>>  > > It seems confusing to have two sites that seemingly both represent\n>>  > > bitcoin core.\n>>  >\n>>  > There is only one website which represents Bitcoin Core full node\n>>  > implementation. You can download Bitcoin Core from\n>>  > >> https://bitcoincore.org\n>>  \n>>  I also notice that, as of 22.0, Wladimir is no longer signing the\n>>  releases, and I have no trust in my gpg network of the people who seem\n>>  to have replaced him.\n>>\n>\n> He is taking the most sensible way forward, decreasing bus factor.\n>\n> Read:\u00a0> https://laanwj.github.io/2021/01/21/decentralize.html\n> \u00a0\n>\n>>\n>> Given the level of security at stake here, my eyebrows are raised at\n>>  this combination of items changing (new website + new gpg signers at the\n>>  same time).\n>>\n>\n> Don't worry and build your own release;\n> but if you do, always verify the tree hash.\n> Trust signed annotated tags.\n> Cheers!\n> \u00a0\n>\n>>\n>> _______________________________________________\n>>  bitcoin-dev mailing list\n>>  >> bitcoin-dev at lists.linuxfoundation.org\n>>  >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211105/cd5ec286/attachment-0001.html>"
            },
            {
                "author": "damian at willtech.com.au",
                "date": "2021-11-05T10:52:00",
                "message_text_only": "Good Afternoon,\n\nTalk about the present and the future is misguided if it concludes that \nthe decentralisation of a project itself is responsible even if the \noperation of the project is to produce a specification, and a software \ncapable of performing transactions in a decentralised manner. There is \nno advantage or disadvantage to the project to have several choices of \nthe software implementation unless buffoons are misguided and wish to \nattack fungibility or consensus, which the operation of consensus must \ndefend, in which case use [Bitcoin Core at version \n21](https://github.com/bitcoin/bitcoin/tree/0.21) and [create a \nfork](https://docs.github.com/en/get-started/quickstart/fork-a-repo) and \nstart developing. As you can see [Bitcoin Core version 20 has been \nmodified](https://github.com/bitcoin/bitcoin/branches) and no longer \npasses checks similarly with version 18 which makes it more difficult to \ncompile make build. A person't own contributions are theirs, to delegate \nis responsible, to try and create a seven headed lion to keep under \ncommand like one would a gryphon is an infirmity.\n\nCreating a properly delegated organisation, being a foundation for the \npurpose of the maintenance of the consensus of Bitcoin a fungible \ndecentralised currency and the Bitcoin Core software is the sort of \nthing I would easily consider but it is far more centralised to be an \noperable concern. In short, as at the beginning, Bitcoin needs a few \nheroes that can understand the whitepaper and write software and if the \nwork is too much one organisational foundation that does is correctly \nstewardship. To decentralise one's own work only requires that there are \nseveral offices available for those you employ to delegate to, but still \nthere requires for your project a headship.\n\nOnce the currency is fungible as you should now consider, just today \nthere was a public announcement that the Commonwealth Bank Of Australia \nis going to allow buying and selling Bitcoin as one of its services, the \nprimary requirements are to defend the existing consensus as the right \nof trade in every UTXO has value because of that consensus, and \nparticularly because of the ease of that trade.\n\nI do not expect anybody is eligible to be dictated to in the Bitcoin \nproject, but people, be sensible, everybody has worked hard and the \nproject is becoming a success. Opportunities to further develop the \nsoftware of the coin and the protocol do not form part of the current \nvalue, hardly. Several groaning seven headed lion's are \ncounterproductive if all they do is search for opportunities for change. \nChange is the diametrically opposed of consensus and is consensus that \nbrings fungibility. You can already make any manner of instrument for \nsmart contract with your solicitor's office. What is necessary is the \noperation of the Bitcoin protocol. The alternative to the deliberately \nchosen consensus is a democracy, and you cannot build any software with \ndemocracy, and people do not know what is good for them or they could \ngovern themselves. A few lessons in history assures government is \nnecessary to enshrine law and insist as the delegates of democracy for \nits operation, to ensure a civilised and decent civility can exist.\n\nPlease, if you have concerned for Bitcoin, revoke your positions until \nyou fight for consensus.\n\nKING JAMES HRMH\nGreat British Empire\n\nRegards,\nThe Australian\nLORD HIS EXCELLENCY JAMES HRMH (& HMRH)\nof Hougun Manor & Glencoe & British Empire\nMR. Damian A. James Williamson\nWills\n\net al.\n\n\nWilltech\nwww.willtech.com.au\nwww.go-overt.com\nduigco.org DUIGCO API\nand other projects\n\n\nm. 0487135719\nf. +61261470192\n\n\nThis email does not constitute a general advice. Please disregard this \nemail if misdelivered.\nOn 2021-11-05 01:17, Prayank via bitcoin-dev wrote:\n> Hi Kate,\n> \n>> He is taking the most sensible way forward, decreasing bus factor.\n> \n> Agree. Work being shared with other maintainers is an improvement.\n> \n>> Read: https://laanwj.github.io/2021/01/21/decentralize.html\n> \n> Interesting blog post. First paragraph talks about strange\n> expectations, not sure what other people expected however I expected\n> present maintainers will always have respect for the Founder of\n> Bitcoin, keep important docs in repository, website etc. forever and\n> respond with appropriate things if any rich scammers try to remove\n> anything important. Anyway that chapter is over and this PR will\n> always remain in history for others to see and make their own opinions\n> about it: https://github.com/bitcoin-core/bitcoincore.org/pull/740\n> \n> What followed it (whitepaper being shared on different websites) was\n> true decentralization and we need something similar in other aspects\n> of full node implementations. Few things that can improve\n> decentralization:\n> \n> 1.More people using alternative full node implementations. Right now\n> 98% of nodes use Bitcoin Core.\n> \n> 2.More people like Luke Dashjr and Amir Taaki who do not simp for\n> anyone. Being a contributor or maintainer in Bitcoin full node\n> implementation is different from other open source projects. It was\n> never going to be easy and it will get difficult with time,\n> \n> 3.More people from different countries getting involved in important\n> roles.\n> \n> 4.Few anons.\n> \n> 5.Individuals and organizations who fund different Bitcoin projects\n> should consider contributing in alternative. full node implementations\n> as well. Maybe start with Bitcoin Knots.\n> \n> I am sure lot of people will find this controversial or disagree with\n> it however this is my opinion and things that I think can improve\n> Bitcoin. Will quote something from my recent medium post about a dev\n> meetup and Knots:\n> \n> Accepting the problems, looking for solutions and trying to improve\n> things is the best approach we as engineers can follow to do better\n> things in Bitcoin. Irrational optimism is as toxic as irrational\n> pessimism.\n> \n> https://prayankgahlot.medium.com/op-halloween21-and-bitcoin-knots-b8a4da4fa0bd\n> \n> Only ~1337 blocks left for Taproot to activate. So cheers to another\n> soft fork being a success and Bitcoin improving regularly. Thanks to\n> everyone who contributed including reviewers. Hoping most of the\n> people will start using latest version of Bitcoin Core or other full\n> node implementations soon.\n> \n> .\n> \n> --\n> \n> Prayank\n> \n> A3B1 E430 2298 178F\n> \n> Oct 21, 2021, 01:48 by mercedes.catherine.salazar at gmail.com:\n> \n>> Hi Owen,\n>> \n>> On Wed, Oct 20, 2021 at 9:25 PM Owen Gunden via bitcoin-dev\n>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> \n>>> On Wed, Oct 20, 2021 at 04:47:17PM +0200, Prayank wrote:\n>>> \n>>>>> It seems confusing to have two sites that seemingly both\n>>> represent\n>>> \n>>>>> bitcoin core.\n>>> \n>>>> \n>>> \n>>>> There is only one website which represents Bitcoin Core full\n>>> node\n>>> \n>>>> implementation. You can download Bitcoin Core from\n>>> \n>>>> https://bitcoincore.org\n>>> \n>>> I also notice that, as of 22.0, Wladimir is no longer signing the\n>>> \n>>> releases, and I have no trust in my gpg network of the people who\n>>> seem\n>>> \n>>> to have replaced him.\n>> \n>> He is taking the most sensible way forward, decreasing bus factor.\n>> \n>> Read: https://laanwj.github.io/2021/01/21/decentralize.html\n>> \n>>> Given the level of security at stake here, my eyebrows are raised\n>>> at\n>>> \n>>> this combination of items changing (new website + new gpg signers\n>>> at the\n>>> \n>>> same time).\n>> \n>> Don't worry and build your own release;\n>> \n>> but if you do, always verify the tree hash.\n>> \n>> Trust signed annotated tags.\n>> \n>> Cheers!\n>> \n>>> _______________________________________________\n>>> \n>>> bitcoin-dev mailing list\n>>> \n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> \n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "yanmaani at cock.li",
                "date": "2021-11-05T14:45:36",
                "message_text_only": "On 2021-11-05 08:17, Prayank via bitcoin-dev wrote:\n> What followed it (whitepaper being shared on different websites) was\n> true decentralization and we need something similar in other aspects\n> of full node implementations. Few things that can improve\n> decentralization:\n> \n> 1.More people using alternative full node implementations. Right now\n> 98% of nodes use Bitcoin Core.\n\nUnfortunately, this isn't really possible. If they did that, you could \nget consensus splits. This is why all the other stuff is so important - \nif Bitcoin is subverted via soft-fork, you *can't* just run your own \nfork.\n\nTheoretically, I suppose you could run two implementations and do \nsomething if they differ, but what?\n1. Bitcoin Core and <AltImpl> both say block is valid -> valid\n2. Bitcoin Core and <AltImpl> both say block is invalid -> invalid\n3. Bitcoin Core says valid, <AltImpl> says invalid -> valid (or get \nforked off)\n4. Bitcoin Core says invalid, <AltImpl> says valid -> invalid (or \nhardfork)\n\n> 2.More people like Luke Dashjr and Amir Taaki who do not simp for\n> anyone. Being a contributor or maintainer in Bitcoin full node\n> implementation is different from other open source projects. It was\n> never going to be easy and it will get difficult with time,\n\nThis is all about the money - it's easy to have people be independent \nwhen their source of money is independent. But nobody's crazy enough to \nbite the hand that feeds them, and you couldn't really build a system on \nthat basis. Our best hope is gentle hands, or contributors wealthy \nenough not to have to care.\n\n(Whatever happened to Amir Taaki, by the way?)\n\n> 3.More people from different countries getting involved in important\n> roles.\n\nIsn't Bitcoin already plenty distributed? Funding people in \nunder-represented countries seems to me like a textbook exercise in \n'box-ticking, but moreover, I'd frankly rather have reasonably well-off \nguys from Western Europe/America who have the financial backbone to not \nworry that much about attacks to their funding, than mercenaries who \nhave to follow orders or get fired. Even if they're from West \nUzbekistan.\n\n(Maybe they need a union?)\n\n> 4.Few anons.\n\nGonna guess you mean \"a few anons,\" not fewer anons.\n\nAgain, problem is money. These days, nobody threatens anyone with \nanything substantive, like murder - the threats all involve cutting off \nsome funding. So having anonymous people being funded by non-robust \nsources doesn't really buy you that much, because the weakest link will \npretty much never be the de-jure, legal freedom of an individual.\n\nHaving a system that allows people to fund anonymous people better would \nbe interesting, but it has some challenges with trust and so on.\n\n> 5.Individuals and organizations who fund different Bitcoin projects\n> should consider contributing in alternative. full node implementations\n> as well. Maybe start with Bitcoin Knots.\n\nSee above. Bitcoin Knots isn't really independent. btcd in Go is, so I \nguess they could try that. But at the end of the day, it wouldn't help - \nbtcd has to be bug-for-bug compatible with Core, and it couldn't really \nbe any other way.\n\nFor my $0.05, what's needed is more \"hard money\" - if people could make \ndonations into a fund, with the fund then paying out to developers, and \nthat fund be controlled in a civilized and non-centralized way (that's \nthe hard part!), this would somewhat insulate developers from people \nthreatening to stop their contributions to The Fund, at the price of \nhaving developers being able to be coerced by The Fund.\n\nYou could also look into a system like Monero's CCS. But at the end of \nthe day, funding is really a very difficult problem, no matter how you \nslice it. The money still has to enter the system somehow. Since Bitcoin \nis a public good, you can't really capture its value, and this means \nindividuals who can (e.g. by malicious activity) will always have the \nleg up."
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-11-08T03:02:33",
                "message_text_only": "Good morning yanmaani,\n\n\n> > 3.More people from different countries getting involved in important\n> > roles.\n>\n> Isn't Bitcoin already plenty distributed? Funding people in\n> under-represented countries seems to me like a textbook exercise in\n> 'box-ticking, but moreover, I'd frankly rather have reasonably well-off\n> guys from Western Europe/America who have the financial backbone to not\n> worry that much about attacks to their funding, than mercenaries who\n> have to follow orders or get fired. Even if they're from West\n> Uzbekistan.\n>\n> (Maybe they need a union?)\n>\n> > 4.Few anons.\n>\n> Gonna guess you mean \"a few anons,\" not fewer anons.\n>\n> Again, problem is money. These days, nobody threatens anyone with\n> anything substantive, like murder - the threats all involve cutting off\n> some funding. So having anonymous people being funded by non-robust\n> sources doesn't really buy you that much, because the weakest link will\n> pretty much never be the de-jure, legal freedom of an individual.\n>\n> Having a system that allows people to fund anonymous people better would\n> be interesting, but it has some challenges with trust and so on.\n\n<ZmnSCPxj coughs quietly in GMT+8 timezone>\n\nOn the other hand, one can argue that \"ZmnSCPxj\" at this point is a bonafide name (that happens to once have been simply a random sequence of letters) rather than an anonymous cover.\n\nAs to box-ticking: the simple fact of the matter is that smart humans will arise everywhere.\nIt is to the interest of Bitcoin users that more of thhem contribute to the success of Bitcoin.\nThe alternative by default is that smart people who became smart because they happened to start out with the disadvantage of having to fight tooth and claw in a bad environment will be quite willing to do anything, including attack Bitcoin, just to get out of such a situation.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Prayank",
                "date": "2021-11-09T12:49:14",
                "message_text_only": "Hi Yanmaani,\n\n> Unfortunately, this isn't really possible. If they did that, you could get consensus splits. This is why all the other stuff is so important - if Bitcoin is subverted via soft-fork, you *can't* just run your own fork.\n\nI am aware of this problem however looking for solutions or workarounds. Is it possible to have one library for things related to consensus which is used by all full node implementations? There are lot of other things in any full node implementation apart from consensus related code.\n\n> This is all about the money - it's easy to have people be independent when their source of money is independent. But nobody's crazy enough to bite the hand that feeds them, and you couldn't really build a system on that basis. Our best hope is gentle hands, or contributors wealthy enough not to have to care.\n\nSorry neither I agree with this nor its \"all about money\" for me. I am assuming there are few others as well with similar thoughts.\n\nI had shared few reasons why someone might contribute to Bitcoin Core: https://bitcoin.stackexchange.com/a/108017/\n\n> Isn't Bitcoin already plenty distributed? Funding people in under-represented countries seems to me like a textbook exercise in 'box-ticking, but moreover, I'd frankly rather have reasonably well-off guys from Western Europe/America who have the financial backbone to not worry that much about attacks to their funding, than mercenaries who have to follow orders or get fired. Even if they're from West Uzbekistan.\n\nSorry I don't agree with this approach. Its not about 'box-ticking' textbook exercise but to make the attacks more difficult by any governments or people with malicious intent. Few years back when I was in college and Tor wasn't as famous as it is now, we used normal socks proxies for pentesting. My mentor Godzilla had suggested us to use IP of different countries not because it is some textbook exercise but it helps when someone tries to trace your requests. Similarly, if we have people from different countries in different full node implementations as maintainers it will be difficult for people to try crazy things.\n\n> See above. Bitcoin Knots isn't really independent.\n\nI understand there are other implementations like btcd, bcoin, gocoin, libbitcoin, bitcore etc. and Knots is a derivative of Core. However there are lot of differences and it changes your experience as a user while running node and using it for different things. I have mentioned few things in the medium post shared in last email.\n\n> You could also look into a system like Monero's CCS.\n\nYes I like it and suggested once in bitcoin.org repository even though it can be done by anyone as there is no official website for Bitcoin: https://github.com/bitcoin-dot-org/Bitcoin.org/issues/3545\n\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n\n\n\nNov 5, 2021, 20:15 by yanmaani at cock.li:\n\n> On 2021-11-05 08:17, Prayank via bitcoin-dev wrote:\n>\n>> What followed it (whitepaper being shared on different websites) was\n>> true decentralization and we need something similar in other aspects\n>> of full node implementations. Few things that can improve\n>> decentralization:\n>>\n>> 1.More people using alternative full node implementations. Right now\n>> 98% of nodes use Bitcoin Core.\n>>\n>\n> Unfortunately, this isn't really possible. If they did that, you could get consensus splits. This is why all the other stuff is so important - if Bitcoin is subverted via soft-fork, you *can't* just run your own fork.\n>\n> Theoretically, I suppose you could run two implementations and do something if they differ, but what?\n> 1. Bitcoin Core and <AltImpl> both say block is valid -> valid\n> 2. Bitcoin Core and <AltImpl> both say block is invalid -> invalid\n> 3. Bitcoin Core says valid, <AltImpl> says invalid -> valid (or get forked off)\n> 4. Bitcoin Core says invalid, <AltImpl> says valid -> invalid (or hardfork)\n>\n>> 2.More people like Luke Dashjr and Amir Taaki who do not simp for\n>> anyone. Being a contributor or maintainer in Bitcoin full node\n>> implementation is different from other open source projects. It was\n>> never going to be easy and it will get difficult with time,\n>>\n>\n> This is all about the money - it's easy to have people be independent when their source of money is independent. But nobody's crazy enough to bite the hand that feeds them, and you couldn't really build a system on that basis. Our best hope is gentle hands, or contributors wealthy enough not to have to care.\n>\n> (Whatever happened to Amir Taaki, by the way?)\n>\n>> 3.More people from different countries getting involved in important\n>> roles.\n>>\n>\n> Isn't Bitcoin already plenty distributed? Funding people in under-represented countries seems to me like a textbook exercise in 'box-ticking, but moreover, I'd frankly rather have reasonably well-off guys from Western Europe/America who have the financial backbone to not worry that much about attacks to their funding, than mercenaries who have to follow orders or get fired. Even if they're from West Uzbekistan.\n>\n> (Maybe they need a union?)\n>\n>> 4.Few anons.\n>>\n>\n> Gonna guess you mean \"a few anons,\" not fewer anons.\n>\n> Again, problem is money. These days, nobody threatens anyone with anything substantive, like murder - the threats all involve cutting off some funding. So having anonymous people being funded by non-robust sources doesn't really buy you that much, because the weakest link will pretty much never be the de-jure, legal freedom of an individual.\n>\n> Having a system that allows people to fund anonymous people better would be interesting, but it has some challenges with trust and so on.\n>\n>> 5.Individuals and organizations who fund different Bitcoin projects\n>> should consider contributing in alternative. full node implementations\n>> as well. Maybe start with Bitcoin Knots.\n>>\n>\n> See above. Bitcoin Knots isn't really independent. btcd in Go is, so I guess they could try that. But at the end of the day, it wouldn't help - btcd has to be bug-for-bug compatible with Core, and it couldn't really be any other way.\n>\n> For my $0.05, what's needed is more \"hard money\" - if people could make donations into a fund, with the fund then paying out to developers, and that fund be controlled in a civilized and non-centralized way (that's the hard part!), this would somewhat insulate developers from people threatening to stop their contributions to The Fund, at the price of having developers being able to be coerced by The Fund.\n>\n> You could also look into a system like Monero's CCS. But at the end of the day, funding is really a very difficult problem, no matter how you slice it. The money still has to enter the system somehow. Since Bitcoin is a public good, you can't really capture its value, and this means individuals who can (e.g. by malicious activity) will always have the leg up.\n>\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211109/96c00682/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "bitcoin.org missing bitcoin core version 22.0",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "yanmaani at cock.li",
                "ZmnSCPxj",
                "Prayank",
                "damian at willtech.com.au"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 24777
        }
    },
    {
        "title": "[bitcoin-dev] Providing \"non-inclusion proofs\" to \"append to the right Merkles\" with minimized overhead",
        "thread_messages": [
            {
                "author": "shymaa arafat",
                "date": "2021-11-11T22:28:39",
                "message_text_only": "Dear Bitcoin Developers,\nHi everybody,\nPlease allow me to suggest this idea, hope you find it worth reading &\ncommenting....\n\nWhile append to the right UTXO Merkles have the much needed advantage of\nlocality of reference that leads to shorter batch proofs, they're\ncriticized by some for not providing non-inclusion proofs. This is a\nsuggestion to slightly change the needed anyway map/dictionary to map the\nUTXOS to their positions as leaves given their hash value, so that it can\nprovide non-inclusion proofs....\n\n1-A 2\u2076\u2074 (could be tuned*) pointer vector is allocated to put the  UTXO in a\nbucket according to its hash.\n2-Due to the Cryptographic hash robustness & bit randomness UTXO hashes\nshould fall uniformly as all buckets are equiprobable and each bucket is\nexpected to contain 1-2 UTXOS (less than Go lang map load factor which is\n6.5-8, 2\u00b3\u00b2 entry may lead to the same)\n3-Insertion is adding a node (in order)to the bucket linked list, deletion\nis deleting a node from the bucket list; the vector bucket pointer maybe\nadjusted in the process from or to NULL value.\n.\nThe strucutre should be as in attached figure:\nVector of pointers to buckets that could be linked lists with the following\nfields in each node:\n-A pointer to the UTXO leaf in the main Merkle\n-The value of the remaining hash bits, or could be omitted with its value\nwith the leaf node.\n-two bit flag that will be explained shortly.\n.\n4-To save computation time, we may calculate and send the accumulated root\nvalue of this secondary tree only once per block; either at the end if\nvalid or when encountering an invalid UTXO to send the non-inclusion proof\nand abort the whole block. Like the Tx Merkle, maybe no need to store the\ntree or maybe it will save time for the non changed parts (experimental).\n5-The non-inclusion proof will be send according to the previous block\naccumulated root, so during batch preparation a newly inserted  UTXO will\nbe flagged 1, a deleted UTXO is  flagged -1, and 0 means value the same as\nprevious block.\n6-If the block is valid, update and reset flags to 0 while computing the\nnew root. A hash of a bucket is the concatenation of all its nodes(expected\nto be2), and empty buckets are substituted by a default NULL hash value.\n7-If we had an invalid UTXO, we have two cases:\na)-The hash doesn't exist in any case ( not even with a deleted flag), in\nthis case we send the usual non-inclusion proof considering old status\nbefore any UTXO from this block.\nb)-The UTXO hash is there, but has a deleted flag (it was spent before\nduring this block); ie, a double spend within the same block.\nNow, in this case I do have some doubts and suggestions or comments are\nwelcomed:\nI think we should resend the previous inclusion proof along saying\nsomething like \"TX ID ....spent it with proof....\"\nI guess this an implementation detail of how to point out to a previous\nproof in the same block\n( I assume such a block is going to be aborted anyways after the invalid\nUTXO and old status will be resumed, if this is wrong please say so)\n.\nThat's all, thank you for your time,\nShymaa M. Arafat\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211112/d58c7732/attachment-0001.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: IMG_20211109_003226.jpg\nType: image/jpeg\nSize: 182701 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211112/d58c7732/attachment-0001.jpg>"
            }
        ],
        "thread_summary": {
            "title": "Providing \"non-inclusion proofs\" to \"append to the right Merkles\" with minimized overhead",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "shymaa arafat"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 3553
        }
    },
    {
        "title": "[bitcoin-dev] Bitcoin Knots 22.0.knots20211108 released",
        "thread_messages": [
            {
                "author": "Luke Dashjr",
                "date": "2021-11-12T01:53:29",
                "message_text_only": "Bitcoin Knots version 22.0.knots20211108 is now available from:\n\n  https://bitcoinknots.org/files/22.x/22.0.knots20211108/\n\nThis release includes new features, various bug fixes and performance \nimprovements, as well as updated translations.\n\nNote that I also plan to release a Long-Term Support 21.2 in the near future, \nfor users who prefer to minimise new feature risks. If you prefer to use a \nLTS branch, I recommend *not* upgrading to 22.x and waiting for 21.2 instead.\n\nPlease report bugs using the issue tracker at GitHub:\n\n  https://github.com/bitcoinknots/bitcoin/issues\n\nTo receive security and update notifications, please subscribe to:\n\n  https://bitcoinknots.org/list/announcements/join/\n\nFor the full release notes and change log, see:\n\nhttps://github.com/bitcoinknots/bitcoin/blob/v22.0.knots20211108/doc/release-notes.md\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 1528 bytes\nDesc: This is a digitally signed message part.\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211112/6b53e516/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "Bitcoin Knots 22.0.knots20211108 released",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Luke Dashjr"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1138
        }
    },
    {
        "title": "[bitcoin-dev] Taproot activates - A time/block line",
        "thread_messages": [
            {
                "author": "Michael Folkson",
                "date": "2021-11-15T14:42:44",
                "message_text_only": "I thought I would collect together the highlights from Taproot activating for those strange Bitcoin history buffs (including myself). If you\u2019d rather watch in video form 0xB10C provided an excellent livestream [0] showing blocks and transactions in the run up to and directly after Taproot activating.\n\nBack in July [1] 0xB10C (with the help of F2Pool) spent from Taproot (P2TR) outputs months before Taproot activated. Taproot rules weren\u2019t being enforced and hence these spends were treated as anyone-can-spend. Hence I won\u2019t treat these as the \u201cfirst\u201d Taproot spends as they didn\u2019t need to meet Taproot consensus rules.\n\nBlock 709631: The last block before Taproot rules were enforced was mined by AntPool. By this point there were various P2TR outputs in blocks, some timelocked for higher than block 709631 but others not timelocked. These non-timelocked P2TR outputs were in effect anyone-can-spend like the ones 0xB10C spent from in July as Taproot rules were not yet being enforced. No P2TR outputs were spent immediately before activation as far as I can tell.\n\nBlock 709632: The first block where full nodes started enforcing Taproot rules was mined by F2Pool. It seems [2] F2Pool wasn\u2019t enforcing Taproot rules and did not include any Taproot spends (some with high fee rates) in this block. More seriously an invalid Taproot spend included in this block could have cost them the block reward and caused a re-org when full nodes (and miners) enforcing Taproot rules rejected this block. Given they didn\u2019t include any attempted Taproot spends (valid or invalid) in the block this protected them from that but it does lead to discussions for a later time on whether Speedy Trial signaling was effective if some mining pools signaled readiness months previous but then did not enforce the new soft fork rules on activation.\n\nBlock 709633: Mined by AntPool. Similar to F2Pool they didn\u2019t include any Taproot spends in this block and one would assume that they also weren\u2019t enforcing Taproot rules though this has not been confirmed.\n\nBlock 709634: Also mined by AntPool.\n\nBlock 709635: The first block which included (numerous) valid Taproot spends (and no invalid Taproot spends) was mined by Foundry USA.\n\nThe first Taproot spend [3] was completed by bitbug42. It was a key path spend and the OP_RETURN in this first Taproot spend contained the message \u201cI like Schnorr sigs and I cannot lie\u201d.\n\nAndrew Chow had the second Taproot spend [4] but the first script path spend. He also spent from two Taproot vanity addresses beginning bc1ptapr00t which were presumably generated using his Rust Bitcoin Vanity Address Generator [5].\n\nPieter Wuille had the third Taproot spend [6]. He also had the bronze medal for SegWit spends on SegWit activation in 2017 [7], he was third then too. However, his vanity address game stepped up for Taproot. For SegWit he spent from one vanity address beginning 35Segwit. This time he spent from vanity addresses beginning bc1ptapr00tearly, bc1pay2tapr00t, bc1pmyusetapr00t, bc1partytaptap and this isn\u2019t including all the vanity addresses that were sent to. He (with Greg Maxwell\u2019s help) searched 2.4 quadrillion keys in a week [8].\n\nBitGo had the fourth Taproot spend [9] and the first Taproot multisig spend via the script path. The script used OP_CHECKSIG and OP_CHECKSIGVERIFY which have been modified with the Taproot upgrade to check Schnorr signatures but it didn\u2019t use the new opcode OP_CHECKSIGADD that has been introduced for threshold (and multi) signatures. It contained an OP_RETURN message with \u201cThx Satoshi! \u221e/21mil First Taproot multisig spend -BitGo\u201d\n\njaoNoctus claimed the fifth Taproot spend and narcelio and ottosch claimed the sixth and seventh Taproot spends.\n\nThe first use of OP_CHECKSIGADD on mainnet was completed [10] by Alekos Filini using modified code [11] from the BDK (Bitcoin Dev Kit) library.\n\nAny errors are my own and I will gladly correct. Thanks to 0xB10C, Greg Maxwell, Pieter Wuille, Murch, Andrew Chow, BitGo, Daniel McNally, Rearden Code, Alekos Filini, Chun, pinheadmz for highlighting these various events online. Thanks to the various block explorers (Esplora, mempool.space etc) that were very useful for monitoring Taproot activation. And thanks to the community as a whole for participating and engaging with this successful upgrade. Without participation and engagement these upgrades are meaningless and it is a great sign for Bitcoin\u2019s future that there was so much interest and scrutiny of this upgrade.\n\n[0]: https://www.youtube.com/watch?v=1jijKVB1cNA, https://www.twitch.tv/0xb10c/video/1204909643\n[1]: https://b10c.me/blog/007-spending-p2tr-pre-activation/\n[2]: https://twitter.com/satofishi/status/1459868549674065926?s=20\n[3]: 33e794d097969002ee05d336686fc03c9e15a597c1b9827669460fac98799036\n[4]: 37777defed8717c581b4c0509329550e344bdc14ac38f71fc050096887e535c8\n[5]: https://github.com/achow101/rust-vanitygen\n[6]: 83c8e0289fecf93b5a284705396f5a652d9886cbd26236b0d647655ad8a37d82\n[7]: https://twitter.com/0xB10C/status/1459592608305582090?s=20\n[8]: https://twitter.com/pwuille/status/1459778731548057603?s=20\n[9]: 905ecdf95a84804b192f4dc221cfed4d77959b81ed66013a7e41a6e61e7ed530\n[10]: 2eb8dbaa346d4be4e82fe444c2f0be00654d8cfd8c4a9a61b11aeaab8c00b272\n[11]: https://gist.github.com/afilini/7c2c33af095ea975f52f5d68302c91d6\n\n--\n\nMichael Folkson\nEmail: michaelfolkson at protonmail.com\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211115/bb3c2818/attachment.html>"
            },
            {
                "author": "0xB10C",
                "date": "2021-11-24T17:29:54",
                "message_text_only": "Thanks, Michael, for writing this up. I agree that it's good to archive\nevents like, for example, soft-fork activations in an ML post.\n\nAll bigger pools have now included multiple P2TR spends in their blocks.\nI have a few comments on what happened at F2Pool and to some extend also\nat AntPool. These were pool number three and pool number five to signal\ntaproot readiness. I'm not affiliated with them but was happy to help\nF2Pool out and return the favor[1] when they asked for help debugging\nwhy they don't include P2TR spends. I have the permission to share some\npool internals and hope this can help make future soft-forks even smoother.\n\nPlease take my comments with a grain of salt. On the one hand, it could\nbe that I'm naive in believing what the pools have told me in private\ncommunication. On the other hand, I'm not as marked from the blocksize\nwar as others might be.\n\n\nOn 11/15/21 3:42 PM, Michael Folkson via bitcoin-dev wrote:\n> [..]\n>\n> Block 709632: The first block where full nodes started enforcing\n> Taproot rules was mined by F2Pool. It seems [2] F2Pool wasn\u2019t\n> enforcing Taproot rules and did not include any Taproot spends (some\n> with high fee rates) in this block. [..] but it does lead to\n> discussions for a later time on whether Speedy Trial signaling was\n> effective if some mining pools signaled readiness months previous but\n> then did not enforce the new soft fork rules on activation.\n>\n> Block 709633: Mined by AntPool. Similar to F2Pool they didn\u2019t include\n> any Taproot spends in this block and one would assume that they also\n> weren\u2019t enforcing Taproot rules though this has not been confirmed.\n>\n> Block 709634: Also mined by AntPool.\n>\n> Block 709635: The first block which included (numerous) valid Taproot\n> spends (and no invalid Taproot spends) was mined by Foundry USA.\n>\n> [..]\n>\n> [1]: https://b10c.me/blog/007-spending-p2tr-pre-activation/\n> [2]: https://twitter.com/satofishi/status/1459868549674065926\n> <https://twitter.com/satofishi/status/1459868549674065926?s=20>\n>\n> [..]\n\nWhile F2Pool responded they \"Will upgrade soon\" [2] to my question if\nthey haven't upgraded their nodes yet, I think they were primarily\nbuying time as they them-self weren't sure what the issue is. \"We are\nlooking into it\" could have been a better and more fitting response.\n\nAn F2Pool engineer reached out on the 16th (two days after activation),\nmentioning they had been running Bitcoin Core v0.21.1 for a few weeks\nnow and upgraded to v22.0 today (on the 16th) in the hope that this\nwould fix their problem of not including P2TR spends. They asked if I\ncould create and _not_ broadcast a P2TR spending transaction for them to\ncheck with testmempoolaccept/sendrawtransaction/getblocktemplate.\n\nI constructed and sent a P2TR spend and asked them to check the versions\nof their nodes peers too. testmempoolaccept returned \"allowed\": true\n(expected as they were running >= v0.21.1). However, it turned out that\nthey weren't connected to any P2TR-spend-relaying peers. They didn't\nreceive any P2TR spending transaction and couldn't include them in their\nblock templates. It seems like this was caused by a) using addnode to\ndirectly connect to known, external peers that hadn't upgraded. But more\nimportantly, b) by applying a custom patch to their node's peer\nselection behavior based on a heuristic that worked for a few years but\nat some point broke without being noticed (I'm not publishing details on\npurpose). F2Pool has fixed this. With connections to >= v0.21.1 nodes,\nthey started receiving and constructing blocks with P2TR spends.\n\nI haven't been in contact with AntPool directly and don't know details\nabout their situation. Alejandro De La Torre (@bitentrepreneur)\ncommunicated with them and said I can include the following:\n\n\"I spoke with antpool after I noticed from b10c\u2019s tweet that they had\nnot included P2TR [spends]. They were quick to react when I inquired.\nThey had not updated to bitcoind 22.0, but had tested it and were\nplanning to update soon. The next day they indeed updated and were able\nto include a tx with a P2TR spend.\"\n\nand\n\n\u00a0\"[Anpool] said that the old peer issue that F2Pool faced 'may be the\nissue'.\"\n\nAntPool didn't provide more information to Alejandro. It's not clear to\nme what the actual issues and fixes were.\n\n\nI'll leave it to someone else to properly reflect on speedy trial. I,\nhowever, want to add: F2Pool mentioned that they \"upgraded to v0.21.1\nseveral weeks ago\". This indicates that they were signaling without\nbeing ready. I don't blame them and assume they were not the only pool\njust setting the version bit. IMO some of the motivations for fake\nsignaling: a) Immense community pressure (e.g., on Twitter) to just set\nthe bit and then get ready in the next six months. b) Running nodes with\ncustom patches. These require longer to upgrade, especially if you want\nto ensure there aren't any bugs in your patches...\n\n\nIt's out of the scope of the Bitcoin Core project to consider people's\ncustom patches, and it's impossible if they are unpublished. In this\nparticular case, upstreaming the patch does not make sense.\n\nHowever, with only just above 50% of taproot nodes[3] (more than a week\nafter activation), there might still be motivation for having a feature\n(sometime before the next soft-fork) to alert/warn a node operator if\nhis node does not have any peers relaying soft-fork X transaction.\nShould PROTOCOL_VERSION be bumped to show support for soft-fork\ntransaction relay? Or does a service flag for relay makes sense (it\nseems complicated to decommission service flags reliably)? Parsing the\nsubver (e.g. \"/Satoshi:0.21.1/\") doesn't make sense as there are not\nonly Bitcoin Core nodes on the network. Could there be a message that is\nexchanged between two nodes to indicate soft-fork readiness?\n\nHow a node operator can be alerted, besides logging, is an\nimplementation detail of Bitcoin Core. Maybe a getnodealerts RPC that a\nnode operator can hook up to his monitoring?\n\nAdditionally, for the next soft-fork where relay is affected,\ninstructions for mining pool signaling could state: \"Upgrade to version\nZ and make sure you have a few version Z peers before starting to signal\nreadiness\".\n\nThanks,\n0xB10C\n\n[3] http://azure.erisian.com.au/~aj/taproot/taproot.html"
            }
        ],
        "thread_summary": {
            "title": "Taproot activates - A time/block line",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "0xB10C",
                "Michael Folkson"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 11897
        }
    },
    {
        "title": "[bitcoin-dev] bitcoinj fork with Taproot support",
        "thread_messages": [
            {
                "author": "n1ms0s",
                "date": "2021-11-17T03:17:13",
                "message_text_only": "Hello all,\nI am currently working on a fork of bitcoinj with basic Taproot support. Currently it supports basic sending and receiving with Taproot addresses using a bitcoinj SPV wallet.\nSee here: https://github.com/n1ms0s/bitcoinj\n\nIt supports the above along with public/private key tweaking. Feel free to take a look, and leave feedback or even work on it yourself and submit a pull request.\n\nOne issue I am running into right now though is when broadcasting a Taproot transaction to older nodes (old as in ~0.18.0) I get an error response of \"Witness version reserved for soft-fork upgrades\". Anyone have any idea why this happens? I have a stackexchange question open here for it:\nhttps://bitcoin.stackexchange.com/questions/110787/issue-when-broadcasting-taproot-transaction-to-older-nodes\n\nn1ms0s\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211117/8e78d0c2/attachment.html>"
            },
            {
                "author": "Andrew Chow",
                "date": "2021-11-17T18:07:07",
                "message_text_only": "Prior to 0.19.0, creating outputs with an unknown witness version was considered non-standard. This was a violation of BIP 173 and was fixed for 0.19.0+ in PR #15846.\n\nOn 11/16/2021 10:17 PM, n1ms0s via bitcoin-dev wrote:\n\n> Hello all,\n> I am currently working on a fork of bitcoinj with basic Taproot support. Currently it supports basic sending and receiving with Taproot addresses using a bitcoinj SPV wallet.\n> See here: https://github.com/n1ms0s/bitcoinj\n>\n> It supports the above along with public/private key tweaking. Feel free to take a look, and leave feedback or even work on it yourself and submit a pull request.\n>\n> One issue I am running into right now though is when broadcasting a Taproot transaction to older nodes (old as in ~0.18.0) I get an error response of \"Witness version reserved for soft-fork upgrades\". Anyone have any idea why this happens? I have a stackexchange question open here for it:\n> https://bitcoin.stackexchange.com/questions/110787/issue-when-broadcasting-taproot-transaction-to-older-nodes\n>\n> n1ms0s\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211117/d99c1571/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2021-11-17T19:00:43",
                "message_text_only": "On Wednesday, November 17th, 2021 at 1:07 PM, Andrew Chow via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Prior to 0.19.0, creating outputs with an unknown witness version was considered non-standard. This was a violation of BIP 173 and was fixed for 0.19.0+ in PR #15846.\n\nThat's correct, but I think OP's problem is with getting P2TR _spends_ to relay. Those will be rejected by all post-segwit pre-taproot Bitcoin Core releases, as far as I know.\n\n--\nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211117/be85de1c/attachment.html>"
            },
            {
                "author": "n1ms0s",
                "date": "2021-11-17T20:05:55",
                "message_text_only": "This seems to be the case. I saw your reply on Bitcoin StackExchange as well. In bitcoinj I just made it so the client only connects to nodes with at least protocol version 70016. Seems to work well.\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\nOn Wednesday, November 17, 2021 7:00 PM, Pieter Wuille via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Wednesday, November 17th, 2021 at 1:07 PM, Andrew Chow via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Prior to 0.19.0, creating outputs with an unknown witness version was considered non-standard. This was a violation of BIP 173 and was fixed for 0.19.0+ in PR #15846.\n>\n> That's correct, but I think OP's problem is with getting P2TR _spends_ to relay. Those will be rejected by all post-segwit pre-taproot Bitcoin Core releases, as far as I know.\n>\n> --\n> Pieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211117/61d7eafc/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "bitcoinj fork with Taproot support",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Andrew Chow",
                "Pieter Wuille",
                "n1ms0s"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 3906
        }
    },
    {
        "title": "[bitcoin-dev] Finding peers that relay taproot spends, was Re: bitcoinj fork with Taproot support",
        "thread_messages": [
            {
                "author": "David A. Harding",
                "date": "2021-11-21T06:31:31",
                "message_text_only": "On Wed, Nov 17, 2021 at 08:05:55PM +0000, n1ms0s via bitcoin-dev wrote:\n> This seems to be the case. I saw your reply on Bitcoin StackExchange\n> as well. In bitcoinj I just made it so the client only connects to\n> nodes with at least protocol version 70016. Seems to work well.\n\nHi,\n\nThis is a clever solution, but when I looked into this I found that P2P\nprotocol version 70016 was introduced in Bitcoin Core version 0.21.0[1].\nThis release will not ever relay taproot spends because it doesn't\ncontain taproot activation parameters[2].  So this heuristic is\nimperfect: it only works when it happens to connect to the 0.21.1 and\n22.0 versions of Bitcoin Core (or compatible nodes) which were\nprogrammed to begin relaying taproot spends starting one block before\nactivation.\n\nCan anyone recommend a better heuristic lite wallets can use to ensure\nthey're connecting to a taproot-activated node?  (If not, maybe this is\nsomething we want nodes to advertise during activation of future\nprotocol extensions.)\n\nThanks,\n\n-Dave\n\n[1] https://github.com/bitcoin/bitcoin/commit/ccef10261efc235c8fcc8aad54556615b0cc23be\n    https://bitcoincore.org/en/releases/0.21.0/\n\n[2] https://github.com/bitcoin/bitcoin/pull/20165\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211121/d3dda5b4/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "Finding peers that relay taproot spends, was Re: bitcoinj fork with Taproot support",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "David A. Harding"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1481
        }
    },
    {
        "title": "[bitcoin-dev] Mock introducing vulnerability in important Bitcoin projects",
        "thread_messages": [
            {
                "author": "Prayank",
                "date": "2021-11-18T20:29:24",
                "message_text_only": "Good morning ZmnSCPxj,\n\n> Indeed, I believe we should take the position that \"review process is as much a part of the code as the code itself, and should be tested regularly\".\n\nAgree. Review process is an important part of open source Bitcoin projects. We should test and verify if everything is working as expected or there is any scope for improvement.\n\n> as they cannot opt out of \"the real thing\" other than to stop developing entirely\n\nTrue and it won't be as obvious as this. Nobody will announce it on dev mailing list and will use proxies (not networks but humans)\n\nAfter reading all the emails, personally experiencing review process especially on important issues like privacy and security, re-evaluating everything and considering the time I can spend on this, I have decided to do this exercise for 3 projects with just 1 account. I have created a salted hash for the username as you had mentioned in the first email:\n\nf40bcb13dbcbf7b6245becb757777586c22798ed7360cd9853572152ddf07a39\n\n3 Bitcoin projects are Bitcoin Core (full node implementation), LND (LN implementation) and Bisq (DEX).\n\nPull requests will be created in next 6 months. If vulnerability gets caught during review, will publicly announce here that the project caught the PR and reveal the de-commitment publicly. If not caught during review, will privately reveal both the inserted vulnerability and the review failure via the normal private vulnerability-reporting channels. A summary with all the details will be shared later.\n\nThis exercise cannot be same as one of the active developers trying to do the same thing because of few reasons mentioned by Ryan Grant in one of the emails: uneven reputation factor of various devs, and uneven review attention for new pull requests. However, I am expecting few interesting results which will help improve the review process hence make Bitcoin more secure.\n\nWill end the email by rephrasing one of the tweets from a respected cypherpunk recently: Independent thought is critical in aircraft crash investigations and in bitcoin development. Immunity from peer pressure can be very helpful during review process.\n\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n\n\n\nOct 4, 2021, 09:29 by ZmnSCPxj at protonmail.com:\n\n>\n> Good morning Luke,\n>\n>> All attempts are harmful, no matter the intent, in that they waste\n>> contributors' time that could be better spent on actual development.\n>>\n>> However, I do also see the value in studying and improving the review process\n>> to harden it against such inevitable attacks. The fact that we know the NSA\n>> engages in such things, and haven't caught one yet should be a red flag.\n>>\n>\n> Indeed, I believe we should take the position that \"review process is as much a part of the code as the code itself, and should be tested regularly\".\n>\n>> Therefore, I think any such a scheme needs to be at least opt-out, if not\n>> opt-in. Please ensure there's a simple way for developers with limited time\n>> (or other reasons) to be informed of which PRs to ignore to opt-out of this\n>> study. (Ideally it would also prevent maintainers from merging - maybe\n>> possible since we use a custom merging script, but what it really needs to\n>> limit is the push, not the dry-run.)\n>>\n>\n> Assuming developers are normal humans with typical human neurology (in particular a laziness circuit), perhaps this would work?\n>\n> Every commit message is required to have a pair of 256-bit hex words.\n>\n> Public attempts at attack / testing of the review process will use the first 256-bit as a salt, and when the salt is prepended to the string \"THIS IS AN ATTACK\" and then hashed with e.g. SHA256, should result in the second 256-bit word.\n>\n> Non-attacks / normal commits just use random 256-bit numbers.\n>\n> Those opting-out to this will run a script that checks commit messages for whether the first 256-bit hexword concatenated with \"THIS IS AN ATTACK\", then hashed, is the second 256-bit hexword.\n>\n> Those opting-in will not run that script and ignore the numbers.\n>\n> The script can be run as well at the maintainer.\n>\n> Hopefully, people who are not deliberately opting out will be too lazy to run the script (as is neurotypical for humans) and getting \"spoilered\" on this.\n>\n> ***HOWEVER***\n>\n> We should note that a putative NSA attack would of course not use the above protocol, and thus no developer can ever opt out of an NSA attempt at inserting vulnerabilities; thus, I think it is better if all developers are forced to opt in on the \"practice rounds\", as they cannot opt out of \"the real thing\" other than to stop developing entirely.\n>\n> Regards,\n> ZmnSCPxj\n>\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211118/0b378431/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Mock introducing vulnerability in important Bitcoin projects",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Prayank"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 4789
        }
    },
    {
        "title": "[bitcoin-dev] Taproot Fields for PSBT",
        "thread_messages": [
            {
                "author": "Sjors Provoost",
                "date": "2021-11-24T12:44:46",
                "message_text_only": "Hi Andrew,\n\nI'm confused why PSBT_IN_TAP_BIP32_DERIVATION and PSBT_OUT_TAP_BIP32_DERIVATION\ncontain not just the derivation path for the xonlypubkey, but also the tapleaf merkle path.\n\nFirst I thought it was perhaps necessary in order for a signer to guess which\nscript leaves it can sign with its own keys. But you can't really know that without\nactually seeing the script. When a signer looks at a script, it presumably already\nknows the leaf path.\n\n- Sjors\n\n> Op 22 jun. 2021, om 23:22 heeft Andrew Chow via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> het volgende geschreven:\n> \n> Hi All,\n> \n> I would like to propose a BIP which defines new fields for Taproot\n> support in PSBT.\n> \n> The full text is below, and the rendered file can be found at\n\nNow at: https://github.com/bitcoin/bips/blob/master/bip-0174.mediawiki <https://github.com/bitcoin/bips/blob/master/bip-0174.mediawiki>\n\n> -\n> | Taproot Key BIP 32 Derivation Path\n> | <tt>PSBT_IN_TAP_BIP32_DERIVATION = 0x16</tt>\n> | <tt><xonlypubkey></tt>\n> | The 32 byte X-only public key\n> | <tt><hashes len> <leaf hash>* <32-bit uint> <32-bit uint>*</tt>\n> | A compact size unsigned integer representing the number of leaf\n> hashes, followed by a list of leaf hashes, followed by the master key\n> fingerprint concatenated with the derivation path of the public key. The\n> derivation path is represented as 32-bit little endian unsigned integer\n> indexes concatenated with each other. Public keys are those needed to\n> spend this output. The leaf hashes are of the leaves which involve this\n> public key.\n\n> |-\n> | Taproot Key BIP 32 Derivation Path\n> | <tt>PSBT_OUT_TAP_BIP32_DERIVATION = 0x07</tt>\n> | <tt><xonlypubkey></tt>\n> | The 32 byte X-only public key\n> | <tt><hashes len> <leaf hash>* <32-bit uint> <32-bit uint>*</tt>\n> \n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211124/44e36110/attachment.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: Message signed with OpenPGP\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211124/44e36110/attachment.sig>"
            },
            {
                "author": "Greg Sanders",
                "date": "2021-11-24T16:00:42",
                "message_text_only": "I may be misunderstanding the question, but it seems essential data for the\nfinalizer role, which may not know such information on its own.\n\nOn Wed, Nov 24, 2021 at 11:15 PM Sjors Provoost via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi Andrew,\n>\n> I'm confused why PSBT_IN_TAP_BIP32_DERIVATION and\n> PSBT_OUT_TAP_BIP32_DERIVATION\n> contain not just the derivation path for the xonlypubkey, but also the\n> tapleaf merkle path.\n>\n> First I thought it was perhaps necessary in order for a signer to guess\n> which\n> script leaves it can sign with its own keys. But you can't really know\n> that without\n> actually seeing the script. When a signer looks at a script, it presumably\n> already\n> knows the leaf path.\n>\n> - Sjors\n>\n> Op 22 jun. 2021, om 23:22 heeft Andrew Chow via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> het volgende geschreven:\n>\n> Hi All,\n>\n> I would like to propose a BIP which defines new fields for Taproot\n> support in PSBT.\n>\n> The full text is below, and the rendered file can be found at\n>\n>\n> Now at: https://github.com/bitcoin/bips/blob/master/bip-0174.mediawiki\n>\n> -\n> | Taproot Key BIP 32 Derivation Path\n> | <tt>PSBT_IN_TAP_BIP32_DERIVATION = 0x16</tt>\n> | <tt><xonlypubkey></tt>\n> | The 32 byte X-only public key\n> | <tt><hashes len> <leaf hash>* <32-bit uint> <32-bit uint>*</tt>\n> | A compact size unsigned integer representing the number of leaf\n> hashes, followed by a list of leaf hashes, followed by the master key\n> fingerprint concatenated with the derivation path of the public key. The\n> derivation path is represented as 32-bit little endian unsigned integer\n> indexes concatenated with each other. Public keys are those needed to\n> spend this output. The leaf hashes are of the leaves which involve this\n> public key.\n>\n>\n> |-\n> | Taproot Key BIP 32 Derivation Path\n> | <tt>PSBT_OUT_TAP_BIP32_DERIVATION = 0x07</tt>\n> | <tt><xonlypubkey></tt>\n> | The 32 byte X-only public key\n> | <tt><hashes len> <leaf hash>* <32-bit uint> <32-bit uint>*</tt>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211125/1b84c9b6/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2021-11-24T16:08:18",
                "message_text_only": "On Wednesday, November 24th, 2021 at 7:44 AM, Sjors Provoost via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi Andrew,\n>\n> I'm confused why PSBT_IN_TAP_BIP32_DERIVATION and PSBT_OUT_TAP_BIP32_DERIVATION\n> contain not just the derivation path for the xonlypubkey, but also the tapleaf merkle path.\n>\n> First I thought it was perhaps necessary in order for a signer to guess which\n> script leaves it can sign with its own keys. But you can't really know that without\n> actually seeing the script. When a signer looks at a script, it presumably already\n> knows the leaf path.\n\nNo, that's exactly it. Signers aren't expected to know or understand scripts ahead of time. With a field telling them which keys are present in which leaves, and how those keys are derived, they can sign without fully understanding the script, or needing the ability to parse the relevant script at all. The actual script information is there too of course, for those that do want to analyze it, or factor that into the decision whether to sign or not.\n\nCheers,\n\n--\nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211124/30416f16/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Taproot Fields for PSBT",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Pieter Wuille",
                "Sjors Provoost",
                "Greg Sanders"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 5921
        }
    },
    {
        "title": "[bitcoin-dev] Trying to patch Core ZMQ \"rawtx\" topic to only publish unconfirmed transactions: How?",
        "thread_messages": [
            {
                "author": "Ali Sherief",
                "date": "2021-11-26T16:56:48",
                "message_text_only": "This has also been posted on Bitcointalk forum: https://bitcointalk.org/index.php?topic=5373341.msg58539261#msg58539261 I have republished it here hoping someone more knowledgeable can post some insight about this.\n----\nIt appears that the ZeroMQ topic I'm listening to, \"rawtx\", not only emits a raw transaction when it appears on the mempool, but once it's already confirmed too.\n\nThis messes with my software, causing it to add txids, addresses, etc. a second time inside arrays (this means that the same transaction is received twice in total).\n\nArray de-duping is not a viable solution long-term (because the array will quickly grow to be big eventually and then this has to happen every time a new element is added), so I'm trying to nip the problem from the source by instructing Core to only publish unconfirmed bitcoin transactions.\n\nAccording to https://bitcoin.stackexchange.com/questions/52848/is-it-possible-to-configure-the-bitcoin-daemon-to-only-broadcast-unconfirmed-tra , it is not possible to configure this from a configuration or command-line option. The source code must directly be edited. But since the codebase has changed greatly, the proposed solution no longer works.\n\n----\n\nSo basically, I know that something inside src/zmq/zmqnotificationinterface.cpp needs to be patched, but I'm not sure which function, or how to do it. Because I only need unconfirmed transactions to be published on ZeroMQ rawtx and not confirmed ones, it's one of the following functions that I need to patch for my own build:\n\nCZMQNotificationInterface::TransactionRemovedFromMempool\nvoid CZMQNotificationInterface::BlockDisconnected\n\nBoth of these call NotifyTransaction() method which I assume fires a message on \"rawtx\" channel.\n\nIn the Stack Exchange question I linked above, Jonas Schnelli suggested adding an `if (!pblock)` check, but that was several years ago and the function he was referencing no longer exists.\n\nBut I still wonder if the pblock check is still applicable in the present day (i.e. if it's indicating a block the transaction is inside).\n\n- Ali Sherief\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211126/f91da026/attachment.html>"
            },
            {
                "author": "Prayank",
                "date": "2021-11-27T13:42:38",
                "message_text_only": "Hi Ali,\n\nNot sure if this is exactly what you are looking for but maybe trying to solve this I might also learn few things:\n\nSave zmqpubsequence=tcp://127.0.0.1:28332 in bitcoin.conf\n\nRun bitcoind\n\nRun this python script: https://pastebin.com/raw/tNp2x5y3\n\nYou will see results like this: \nhttps://i.imgur.com/xKzFJbl.png\nhttps://i.imgur.com/gpsTTHZ.png\n\nA - Accepted, C- Connect (block) and R- Removal in the above screenshots\n\nIf you are looking for unconfirmed transactions printed in sequence I think this should help. Since transactions can be printed twice (accept,remove) in this case as well, python script can be modified to manage this IMO.\n\nOther alternatives can be debug=mempool and reading debug.log for changes without polling.\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211127/d9d4e66e/attachment.html>"
            },
            {
                "author": "0xB10C",
                "date": "2021-11-29T09:32:15",
                "message_text_only": "Hi Ali,\n\nI've run into this multiple times myself. I've opened a draft PR [0]\nadding a rawmempooltx publisher.\n\nYou're right. In zmq/zmqnotificationinterface.cpp the\nCZMQNotificationInterface is notified about TransactionAddedToMempool.\nCurrently, this calls NotifyTransaction() (the publisher with the rawtx\ntopic) and NotifyTransactionAcceptance() (the publisher with the\nsequence topic)[1]. I've added a call to a new\nNotifyMempoolTransaction() function (the publisher with the rawmempooltx\ntopic).\n\nI'd find a mempool transaction publisher with both the raw transaction\nand transaction fee useful too. However, this requires changes to the\nchain notifications in interfaces/chain.h.\u00a0\n\n[0]: https://github.com/bitcoin/bitcoin/pull/23624\n[1]:\nhttps://github.com/bitcoin/bitcoin/pull/23624/files#diff-ac4b2d3a8de2c4dd41ad9d75505ea6ce4dc87a476710a9ebee8acf9bebf5cca2L146-L148\u00a0\n\n\nBest,\n0xB10C\n\n\n\nOn 11/26/21 5:56 PM, Ali Sherief via bitcoin-dev wrote:\n>\n> This has also been posted on Bitcointalk\n> forum:\u00a0https://bitcointalk.org/index.php?topic=5373341.msg58539261#msg58539261\n> <https://bitcointalk.org/index.php?topic=5373341.msg58539261#msg58539261>\u00a0I\n> have republished it here hoping someone more knowledgeable can post\n> some insight about this.\n> ----\n> It appears that the ZeroMQ topic I'm listening to, \"rawtx\", not only\n> emits a raw transaction when it appears on the mempool, but once it's\n> already confirmed too.\n>\n> This messes with my software, causing it to add txids, addresses, etc.\n> a second time inside arrays (this means that the same transaction is\n> received twice in total).\n>\n> Array de-duping is not a viable solution long-term (because the array\n> will quickly grow to be big eventually and then this has to happen\n> every time a new element is added), so I'm trying to nip the problem\n> from the source by instructing Core to only publish unconfirmed\n> bitcoin transactions.\n>\n> According to\n> https://bitcoin.stackexchange.com/questions/52848/is-it-possible-to-configure-the-bitcoin-daemon-to-only-broadcast-unconfirmed-tra\n> <https://bitcoin.stackexchange.com/questions/52848/is-it-possible-to-configure-the-bitcoin-daemon-to-only-broadcast-unconfirmed-tra>\n> , it is not possible to configure this from a configuration or\n> command-line option. The source code must directly be edited. But\n> since the codebase has changed greatly, the proposed solution no\n> longer works.\n>\n> ----\n>\n> So basically, I know that something inside\n> src/zmq/zmqnotificationinterface.cpp needs to be patched, but I'm not\n> sure which function, or how to do it. Because I only need unconfirmed\n> transactions to be published on ZeroMQ rawtx and not confirmed ones,\n> it's one of the following functions that I need to patch for my own build:\n>\n> CZMQNotificationInterface::TransactionRemovedFromMempool\n> void CZMQNotificationInterface::BlockDisconnected\n>\n> Both of these call NotifyTransaction() method which I assume fires a\n> message on \"rawtx\" channel.\n>\n> In the Stack Exchange question I linked above,\u00a0Jonas Schnelli\n> suggested adding an `if (!pblock)` check, but that was several years\n> ago and the function he was referencing no longer exists.\n>\n> But I still wonder if the pblock check is still applicable in the\n> present day (i.e. if it's indicating a block the transaction is inside).\n>\n> - Ali Sherief\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "LORD HIS EXCELLENCY JAMES HRMH",
                "date": "2021-11-29T14:10:11",
                "message_text_only": "Wasn't this already not a problem because you can check if it was confirmed? The transaction is not finalised in the mempool it is just speculation of a transaction, so it makes sense to emit when the transaction is confirmed.  Just already check..\n\n> It appears that the ZeroMQ topic I'm listening to, \"rawtx\", not only\n> emits a raw transaction when it appears on the mempool, but once it's\n> already confirmed too.\n\nKING JAMES HRMH\nGreat British Empire\n\nRegards,\nThe Australian\nLORD HIS EXCELLENCY JAMES HRMH (& HMRH)\nof Hougun Manor & Glencoe & British Empire\nMR. Damian A. James Williamson\nWills\n\net al.\n\n\nWilltech\nwww.willtech.com.au\nwww.go-overt.com\nduigco.org DUIGCO API\nand other projects\n\n\nm. 0487135719\nf. +61261470192\n\n\nThis email does not constitute a general advice. Please disregard this email if misdelivered.\n________________________________\nFrom: bitcoin-dev <bitcoin-dev-bounces at lists.linuxfoundation.org> on behalf of 0xB10C via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>\nSent: Monday, 29 November 2021 8:32 PM\nTo: Ali Sherief <ali at notatether.com>; Bitcoin Protocol Discussion <bitcoin-dev at lists.linuxfoundation.org>\nSubject: Re: [bitcoin-dev] Trying to patch Core ZMQ \"rawtx\" topic to only publish unconfirmed transactions: How?\n\nHi Ali,\n\nI've run into this multiple times myself. I've opened a draft PR [0]\nadding a rawmempooltx publisher.\n\nYou're right. In zmq/zmqnotificationinterface.cpp the\nCZMQNotificationInterface is notified about TransactionAddedToMempool.\nCurrently, this calls NotifyTransaction() (the publisher with the rawtx\ntopic) and NotifyTransactionAcceptance() (the publisher with the\nsequence topic)[1]. I've added a call to a new\nNotifyMempoolTransaction() function (the publisher with the rawmempooltx\ntopic).\n\nI'd find a mempool transaction publisher with both the raw transaction\nand transaction fee useful too. However, this requires changes to the\nchain notifications in interfaces/chain.h.\n\n[0]: https://github.com/bitcoin/bitcoin/pull/23624\n[1]:\nhttps://github.com/bitcoin/bitcoin/pull/23624/files#diff-ac4b2d3a8de2c4dd41ad9d75505ea6ce4dc87a476710a9ebee8acf9bebf5cca2L146-L148\n\n\nBest,\n0xB10C\n\n\n\nOn 11/26/21 5:56 PM, Ali Sherief via bitcoin-dev wrote:\n>\n> This has also been posted on Bitcointalk\n> forum: https://bitcointalk.org/index.php?topic=5373341.msg58539261#msg58539261\n> <https://bitcointalk.org/index.php?topic=5373341.msg58539261#msg58539261> I\n> have republished it here hoping someone more knowledgeable can post\n> some insight about this.\n> ----\n> It appears that the ZeroMQ topic I'm listening to, \"rawtx\", not only\n> emits a raw transaction when it appears on the mempool, but once it's\n> already confirmed too.\n>\n> This messes with my software, causing it to add txids, addresses, etc.\n> a second time inside arrays (this means that the same transaction is\n> received twice in total).\n>\n> Array de-duping is not a viable solution long-term (because the array\n> will quickly grow to be big eventually and then this has to happen\n> every time a new element is added), so I'm trying to nip the problem\n> from the source by instructing Core to only publish unconfirmed\n> bitcoin transactions.\n>\n> According to\n> https://bitcoin.stackexchange.com/questions/52848/is-it-possible-to-configure-the-bitcoin-daemon-to-only-broadcast-unconfirmed-tra\n> <https://bitcoin.stackexchange.com/questions/52848/is-it-possible-to-configure-the-bitcoin-daemon-to-only-broadcast-unconfirmed-tra>\n> , it is not possible to configure this from a configuration or\n> command-line option. The source code must directly be edited. But\n> since the codebase has changed greatly, the proposed solution no\n> longer works.\n>\n> ----\n>\n> So basically, I know that something inside\n> src/zmq/zmqnotificationinterface.cpp needs to be patched, but I'm not\n> sure which function, or how to do it. Because I only need unconfirmed\n> transactions to be published on ZeroMQ rawtx and not confirmed ones,\n> it's one of the following functions that I need to patch for my own build:\n>\n> CZMQNotificationInterface::TransactionRemovedFromMempool\n> void CZMQNotificationInterface::BlockDisconnected\n>\n> Both of these call NotifyTransaction() method which I assume fires a\n> message on \"rawtx\" channel.\n>\n> In the Stack Exchange question I linked above, Jonas Schnelli\n> suggested adding an `if (!pblock)` check, but that was several years\n> ago and the function he was referencing no longer exists.\n>\n> But I still wonder if the pblock check is still applicable in the\n> present day (i.e. if it's indicating a block the transaction is inside).\n>\n> - Ali Sherief\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211129/aeb29320/attachment-0001.html>"
            },
            {
                "author": "Ali Sherief",
                "date": "2021-11-29T14:13:37",
                "message_text_only": "Theoretically that would be the desired outcome for me but this change is going to be implemented as part of a casino which must display the status of new deposits that are made, even when they are unconfirmed to now. Hence why I need to receive the unconfirmed messages.\n\n- Ali Sherief\n\nOn Mon, Nov 29, 2021 at 5:10 PM, LORD HIS EXCELLENCY JAMES HRMH <willtech at live.com.au> wrote:\n\n> Wasn't this already not a problem because you can check if it was confirmed? The transaction is not finalised in the mempool it is just speculation of a transaction, so it makes sense to emit when the transaction is confirmed. Just already check..\n>\n>> It appears that the ZeroMQ topic I'm listening to, \"rawtx\", not only\n>> emits a raw transaction when it appears on the mempool, but once it's\n>> already confirmed too.\n>\n> KING JAMES HRMH\n> Great British Empire\n>\n> Regards,\n> The Australian\n> LORD HIS EXCELLENCY JAMES HRMH (& HMRH)\n> of Hougun Manor & Glencoe & British Empire\n> MR. Damian A. James Williamson\n> Wills\n>\n> et al.\n>\n> Willtech\n> www.willtech.com.au\n> www.go-overt.com\n> duigco.org DUIGCO API\n> and other projects\n>\n> m. 0487135719\n> f. +61261470192\n>\n> This email does not constitute a general advice. Please disregard this email if misdelivered.\n> ---------------------------------------------------------------\n>\n> From: bitcoin-dev <bitcoin-dev-bounces at lists.linuxfoundation.org> on behalf of 0xB10C via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>\n> Sent: Monday, 29 November 2021 8:32 PM\n> To: Ali Sherief <ali at notatether.com>; Bitcoin Protocol Discussion <bitcoin-dev at lists.linuxfoundation.org>\n> Subject: Re: [bitcoin-dev] Trying to patch Core ZMQ \"rawtx\" topic to only publish unconfirmed transactions: How?\n>\n> Hi Ali,\n>\n> I've run into this multiple times myself. I've opened a draft PR [0]\n> adding a rawmempooltx publisher.\n>\n> You're right. In zmq/zmqnotificationinterface.cpp the\n> CZMQNotificationInterface is notified about TransactionAddedToMempool.\n> Currently, this calls NotifyTransaction() (the publisher with the rawtx\n> topic) and NotifyTransactionAcceptance() (the publisher with the\n> sequence topic)[1]. I've added a call to a new\n> NotifyMempoolTransaction() function (the publisher with the rawmempooltx\n> topic).\n>\n> I'd find a mempool transaction publisher with both the raw transaction\n> and transaction fee useful too. However, this requires changes to the\n> chain notifications in interfaces/chain.h.\n>\n> [0]: https://github.com/bitcoin/bitcoin/pull/23624\n> [1]:\n> https://github.com/bitcoin/bitcoin/pull/23624/files#diff-ac4b2d3a8de2c4dd41ad9d75505ea6ce4dc87a476710a9ebee8acf9bebf5cca2L146-L148\n>\n> Best,\n> 0xB10C\n>\n> On 11/26/21 5:56 PM, Ali Sherief via bitcoin-dev wrote:\n>>\n>> This has also been posted on Bitcointalk\n>> forum: https://bitcointalk.org/index.php?topic=5373341.msg58539261#msg58539261\n>> <https://bitcointalk.org/index.php?topic=5373341.msg58539261#msg58539261> I\n>> have republished it here hoping someone more knowledgeable can post\n>> some insight about this.\n>> ----\n>> It appears that the ZeroMQ topic I'm listening to, \"rawtx\", not only\n>> emits a raw transaction when it appears on the mempool, but once it's\n>> already confirmed too.\n>>\n>> This messes with my software, causing it to add txids, addresses, etc.\n>> a second time inside arrays (this means that the same transaction is\n>> received twice in total).\n>>\n>> Array de-duping is not a viable solution long-term (because the array\n>> will quickly grow to be big eventually and then this has to happen\n>> every time a new element is added), so I'm trying to nip the problem\n>> from the source by instructing Core to only publish unconfirmed\n>> bitcoin transactions.\n>>\n>> According to\n>> https://bitcoin.stackexchange.com/questions/52848/is-it-possible-to-configure-the-bitcoin-daemon-to-only-broadcast-unconfirmed-tra\n>> <https://bitcoin.stackexchange.com/questions/52848/is-it-possible-to-configure-the-bitcoin-daemon-to-only-broadcast-unconfirmed-tra>\n>> , it is not possible to configure this from a configuration or\n>> command-line option. The source code must directly be edited. But\n>> since the codebase has changed greatly, the proposed solution no\n>> longer works.\n>>\n>> ----\n>>\n>> So basically, I know that something inside\n>> src/zmq/zmqnotificationinterface.cpp needs to be patched, but I'm not\n>> sure which function, or how to do it. Because I only need unconfirmed\n>> transactions to be published on ZeroMQ rawtx and not confirmed ones,\n>> it's one of the following functions that I need to patch for my own build:\n>>\n>> CZMQNotificationInterface::TransactionRemovedFromMempool\n>> void CZMQNotificationInterface::BlockDisconnected\n>>\n>> Both of these call NotifyTransaction() method which I assume fires a\n>> message on \"rawtx\" channel.\n>>\n>> In the Stack Exchange question I linked above, Jonas Schnelli\n>> suggested adding an `if (!pblock)` check, but that was several years\n>> ago and the function he was referencing no longer exists.\n>>\n>> But I still wonder if the pblock check is still applicable in the\n>> present day (i.e. if it's indicating a block the transaction is inside).\n>>\n>> - Ali Sherief\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211129/af5366c7/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Trying to patch Core ZMQ \"rawtx\" topic to only publish unconfirmed transactions: How?",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Ali Sherief",
                "0xB10C",
                "Prayank",
                "LORD HIS EXCELLENCY JAMES HRMH"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 17620
        }
    },
    {
        "title": "[bitcoin-dev] A fee-bumping model",
        "thread_messages": [
            {
                "author": "darosior",
                "date": "2021-11-29T14:27:23",
                "message_text_only": "Hi everyone,\n\nFee-bumping is paramount to the security of many protocols building on Bitcoin, as they require the\nconfirmation of a transaction (which might be presigned) before the expiration of a timelock at any\npoint after the establishment of the contract.\n\nThe part of Revault using presigned transactions (the delegation from a large to a smaller multisig)\nis no exception. We have been working on how to approach this for a while now and i'd like to share\nwhat we have in order to open a discussion on this problem so central to what seem to be The Right\nWay [0] to build on Bitcoin but which has yet to be discussed in details (at least publicly).\n\nI'll discuss what we came up with for Revault (at least for what will be its first iteration) but my\nintent with posting to the mailing list is more to frame the questions to this problem we are all\ngoing to face rather than present the results of our study tailored to the Revault usecase.\nThe discussion is still pretty Revault-centric (as it's the case study) but hopefully this can help\nfuture protocol designers and/or start a discussion around what everyone's doing for existing ones.\n\n\n## 1. Reminder about Revault\n\nThe part of Revault we are interested in for this study is the delegation process, and more\nspecifically the application of spending policies by network monitors (watchtowers).\nCoins are received on a large multisig. Participants of this large multisig create 2 [1]\ntransactions. The Unvault, spending a deposit UTxO, creates an output paying either to the small\nmultisig after a timelock or to the large multisig immediately. The Cancel, spending the Unvault\noutput through the non-timelocked path, creates a new deposit UTxO.\nParticipants regularly exchange the Cancel transaction signatures for each deposit, sharing the\nsignatures with the watchtowers they operate. They then optionally [2] sign the Unvault transaction\nand share the signatures with the small multisig participants who can in turn use them to proceed\nwith a spending. Watchtowers can enforce spending policies (say, can't Unvault outside of business\nhours) by having the Cancel transaction be confirmed before the expiration of the timelock.\n\n\n## 2. Problem statement\n\nFor any delegated vault, ensure the confirmation of a Cancel transaction in a configured number of\nblocks at any point. In so doing, minimize the overpayments and the UTxO set footprint. Overpayments\nincrease the burden on the watchtower operator by increasing the required frequency of refills of the\nfee-bumping wallet, which is already the worst user experience. You are likely to manage a number of\nUTxOs with your number of vaults, which comes at a cost for you as well as everyone running a full\nnode.\n\nNote that this assumes miners are economically rationale, are incentivized by *public* fees and that\nyou have a way to propagate your fee-bumped transaction to them. We also don't consider the block\nspace bounds.\n\nIn the previous paragraph and the following text, \"vault\" can generally be replaced with \"offchain\ncontract\".\n\n\n## 3. With presigned transactions\n\nAs you all know, the first difficulty is to get to be able to unilaterally enforce your contract\nonchain. That is, any participant must be able to unilaterally bump the fees of a transaction even\nif it was co-signed by other participants.\n\nFor Revault we can afford to introduce malleability in the Cancel transaction since there is no\nsecond-stage transaction depending on its txid. Therefore it is pre-signed with ANYONECANPAY. We\ncan't use ANYONECANPAY|SINGLE since it would open a pinning vector [3]. Note how we can't leverage\nthe carve out rule, and neither can any other more-than-two-parties contract.\nThis has a significant implication for the rest, as we are entirely burning fee-bumping UTxOs.\n\nThis opens up a pinning vector, or at least a significant nuisance: any other party can largely\nincrease the absolute fee without increasing the feerate, leveraging the RBF rules to prevent you\nfrom replacing it without paying an insane fee. And you might not see it in your own mempool and\ncould only suppose it's happening by receiving non-full blocks or with transactions paying a lower\nfeerate.\nUnfortunately i know of no other primitive that can be used by multi-party (i mean, >2) presigned\ntransactions protocols for fee-bumping that aren't (more) vulnerable to pinning.\n\n\n## 4. We are still betting on future feerate\n\nThe problem is still missing one more constraint. \"Ensuring confirmation at any time\" involves ensuring\nconfirmation at *any* feerate, which you *cannot* do. So what's the limit? In theory you should be ready\nto burn as much in fees as the value of the funds you want to get out of the contract. So... For us\nit'd mean keeping for each vault an equivalent amount of funds sitting there on the watchtower's hot\nwallet. For Lightning, it'd mean keeping an equivalent amount of funds as the sum of all your\nchannels balances sitting there unallocated \"just in case\". This is not reasonable.\n\nSo you need to keep a maximum feerate, above which you won't be able to ensure the enforcement of\nall your contracts onchain at the same time. We call that the \"reserve feerate\" and you can have\ndifferent strategies for choosing it, for instance:\n- The 85th percentile over the last year of transactions feerates\n- The maximum historical feerate\n- The maximum historical feerate adjusted in dollars (makes more sense but introduces a (set of?)\n  trusted oracle(s) in a security-critical component)\n- Picking a random high feerate (why not? It's an arbitrary assumption anyways)\n\nTherefore, even if we don't have to bet on the broadcast-time feerate market at signing time anymore\n(since we can unilaterally bump), we still need some kind of prediction in preparation of making\nfunds available to bump the fees at broadcast time.\nApart from judging that 500sat/vb is probably more reasonable than 10sat/vbyte, this unfortunately\nsounds pretty much crystal-ball-driven.\n\nWe currently use the maximum of the 95th percentiles over 90-days windows over historical block chain\nfeerates. [4]\n\n\n## 5. How much funds does my watchtower need?\n\nThat's what we call the \"reserve\". Depending on your reserve feerate strategy it might vary over\ntime. This is easier to reason about with a per-contract reserve. For Revault it's pretty\nstraightforward since the Cancel transaction size is static: `reserve_feerate * cancel_size`. For\nother protocols with dynamic transaction sizes (or even packages of transactions) it's less so. For\nyour Lightning channel you would probably take the maximum size of your commitment transaction\naccording to your HTLC exposure settings + the size of as many `htlc_success` transaction?\n\nThen you either have your software or your user guesstimate how many offchain contracts the\nwatchtower will have to watch, time that by the per-contract reserve and refill this amount (plus\nsome slack in practice). Once again, a UX tradeoff (not even mentioning the guesstimation UX):\noverestimating leads to too many unallocated funds sitting on a hot wallet, underestimating means\n(at best) inability to participate in new contracts or being \"at risk\" (not being able to enforce\nall your contracts onchain at your reserve feerate) before a new refill.\n\nFor vaults you likely have large-value UTxOs and small transactions (the Cancel is one-in one-out in\nRevault). For some other applications with large transactions and lower-value UTxOs on average it's\nlikely that only part of the offchain contracts might be enforceable at a reasonable feerate. Is it\nreasonable?\n\n\n## 6. UTxO pool layout\n\nNow that you somehow managed to settle on a refill amount, how are you going to use these funds?\nAlso, you'll need to manage your pool across time (consolidating small coins, and probably fanning\nout large ones).\n\nYou could keep a single large UTxO and peel it as you need to sponsor transactions. But this means\nthat you need to create a coin of a specific value according to your need at the current feerate\nestimation, hope to have it confirmed in a few blocks (at least for now! [5]), and hope that the\nvalue won't be obsolete by the time it confirmed. Also, you'd have to do that for any number of\nCancel, chaining feebump coin creation transactions off the change of the previous ones or replacing\nthem with more outputs. Both seem to become really un-manageable (and expensive) in many edge-cases,\nshortening the time you have to confirm the actual Cancel transaction and creating uncertainty about\nthe reserve (how much is my just-in-time fanout going to cost me in fees that i need to refill in\nadvance on my watchtower wallet?).\nThis is less of a concern for protocols using CPFP to sponsor transactions, but they rely on a\npolicy rule specific to 2-parties contracts.\n\nTherefore for Revault we fan-out the coins per-vault in advance. We do so at refill time so the\nrefiller can give an excess to pay for the fees of the fanout transaction (which is reasonable since\nit will occur just after the refilling transaction confirms). When the watchtower is asked to watch\nfor a new delegated vault it will allocate coins from the pool of fanned-out UTxOs to it (failing\nthat, it would refuse the delegation).\nWhat is a good distribution of UTxOs amounts per vault? We want to minimize the number of coins,\nstill have coins small enough to not overpay (remember, we can't have change) and be able to bump a\nCancel up to the reserve feerate using these coins. The two latter constraints are directly in\ncontradiction as the minimal value of a coin usable at the reserve feerate (paying for its own input\nfee + bumping the feerate by, say, 5sat/vb) is already pretty high. Therefore we decided to go with\ntwo distributions per vault. The \"reserve distribution\" alone ensures that we can bump up to the\nreserve feerate and is usable for high feerates. The \"bonus distribution\" is not, but contains\nsmaller coins useful to prevent overpayments during low and medium fee periods (which is most of the\ntime).\nBoth distributions are based on a basic geometric suite [6]. Each value is half the previous one.\nThis exponentially decreases the value, limiting the number of coins. But this also allows for\npretty small coins to exist and each coin's value is equal to the sum of the smaller coins,\nor smaller by at most the value of the smallest coin. Therefore bounding the maximum overpayment to\nthe smallest coin's value [7].\n\nFor the management of the UTxO pool across time we merged the consolidation with the fanout. When\nfanning out a refilled UTxO, we scan the pool for coins that need to be consolidated according to a\nheuristic. An instance of a heuristic is \"the coin isn't allocated and would not have been able to\nincrease the fee at the median feerate over the past 90 days of blocks\".\nWe had this assumption that feerate would tend to go up with time and therefore discarded having to\nsplit some UTxOs from the pool. We however overlooked that a large increase in the exchange price of\nBTC as we've seen during the past year could invalidate this assumption and that should arguably be\nreconsidered.\n\n\n## 7. Bumping and re-bumping\n\nFirst of all, when to fee-bump? At fixed time intervals? At each block connection? It sounds like,\ngiven a large enough timelock, you could try to greed by \"trying your luck\" at a lower feerate and\nonly re-bumping every N blocks. You would then start aggressively bumping at every block after M\nblocks have passed. But that's actually a bet (in disguised?) that the next block feerate in M blocks\nwill be lower than the current one. In the absence of any predictive model it is more reasonable to\njust start being aggressive immediately.\nYou probably want to base your estimates on `estimatesmartfee` and as a consequence you would re-bump\n(if needed )after each block connection, when your estimates get updated and you notice your\ntransaction was not included in the block.\n\nIn the event that you notice a consequent portion of the block is filled with transactions paying\nless than your own, you might want to start panicking and bump your transaction fees by a certain\npercentage with no consideration for your fee estimator. You might skew miners incentives in doing\nso: if you increase the fees by a factor of N, any miner with a fraction larger than 1/N of the\nnetwork hashrate now has an incentive to censor your transaction at first to get you to panic. Also\nnote this can happen if you want to pay the absolute fees for the 'pinning' attack mentioned in\nsection #2, and that might actually incentivize miners to perform it themselves..\n\nThe gist is that the most effective way to bump and rebump (RBF the Cancel tx) seems to just be to\nconsider the `estimatesmartfee 2 CONSERVATIVE` feerate at every block your tx isn't included in, and\nto RBF it if the feerate is higher.\nIn addition, we fallback to a block chain based estimation when estimates aren't available (eg if\nthe user stopped their WT for say a hour and we come back up): we use the 85th percentile over the\nfeerates in the last 6 blocks. Sure, miners can try to have an influence on that by stuffing their\nblocks with large fee self-paying transactions, but they would need to:\n1. Be sure to catch a significant portion of the 6 blocks (at least 2, actually)\n2. Give up on 25% of the highest fee-paying transactions (assuming they got the 6 blocks, it's\n   proportionally larger and incertain as they get less of them)\n3. Hope that our estimator will fail and we need to fall back to the chain-based estimation\n\n\n## 8. Our study\n\nWe essentially replayed the historical data with different deployment configurations (number of\nparticipants and timelock) and probability of an event occurring (event being say an Unvault, an\ninvalid Unvault, a new delegation, ..). We then observed different metrics such as the time at risk\n(when we can't enforce all our contracts at the reserve feerate at the same time), or the\noperational cost.\nWe got the historical fee estimates data from Statoshi [9], Txstats [10] and the historical chain\ndata from Riccardo Casatta's `blocks_iterator` [11]. Thanks!\n\nThe (research-quality..) code can be found at https://github.com/revault/research under the section\n\"Fee bumping\". Again it's very Revault specific, but at least the data can probably be reused for\nstudying other protocols.\n\n\n## 9. Insurances\n\nOf course, given it's all hacks and workarounds and there is no good answer to \"what is a reasonable\nfeerate up to which we need to make contracts enforceable onchain?\", there is definitely room for an\ninsurance market. But this enters the realm of opinions. Although i do have some (having discussed\nthis topic for the past years with different people), i would like to keep this post focused on the\ntechnical aspects of this problem.\n\n\n\n[0] As far as i can tell, having offchain contracts be enforceable onchain by confirming a\ntransaction before the expiration of a timelock is a widely agreed-upon approach. And i don't think\nwe can opt for any other fundamentally different one, as you want to know you can claim back your\ncoins from a contract after a deadline before taking part in it.\n\n[1] The Real Revault (tm) involves more transactions, but for the sake of conciseness i only\ndetailed a minimum instance of the problem.\n\n[2] Only presigning part of the Unvault transactions allows to only delegate part of the coins,\nwhich can be abstracted as \"delegate x% of your stash\" in the user interface.\n\n[3] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-May/017835.html\n\n[4] https://github.com/revault/research/blob/1df953813708287c32a15e771ba74957ec44f354/feebumping/model/statemachine.py#L323-L329\n\n[5] https://github.com/bitcoin/bitcoin/pull/23121\n\n[6] https://github.com/revault/research/blob/1df953813708287c32a15e771ba74957ec44f354/feebumping/model/statemachine.py#L494-L507\n\n[7] Of course this assumes a combinatorial coin selection, but i believe it's ok given we limit the\nnumber of coins beforehand.\n\n[8] Although there is the argument to outbid a censorship, anyone censoring you isn't necessarily a\nminer.\n\n[9] https://www.statoshi.info/\n\n[10] https://www.statoshi.info/\n\n[11] https://github.com/RCasatta/blocks_iterator"
            },
            {
                "author": "Antoine Riard",
                "date": "2021-11-30T01:43:49",
                "message_text_only": "Hi Darosior,\n\nNice work, few thoughts binding further your model for Lightning.\n\n> For any delegated vault, ensure the confirmation of a Cancel transaction\nin a configured number of\n> blocks at any point. In so doing, minimize the overpayments and the UTxO\nset footprint. Overpayments\n> increase the burden on the watchtower operator by increasing the required\nfrequency of refills of the\n> fee-bumping wallet, which is already the worst user experience. You are\nlikely to manage a number of\n> UTxOs with your number of vaults, which comes at a cost for you as well\nas everyone running a full\n> node.\n\nFor any opened channel, ensure the confirmation of a Commitment transaction\nand the children HTLC-Success/HTLC-Timeout transactions. Note, in the\nLightning security game you have to consider (at least) 4 types of players\nmoves and incentives : your node, your channel counterparties, the miners,\nthe crowd of bitcoin users. The number of the last type of players is\nunknown from your node, however it should not be forgotten you're in\ncompetition for block space, therefore their block demands bids should be\nanticipated and reacted to in consequence. With that remark in mind,\nimplications for your LN fee-bumping strategy will be raised afterwards.\n\nFor a LN service provider, on-chain overpayments are bearing on your\noperational costs, thus downgrading your economic competitiveness. For the\naverage LN user, overpayment might price out outside a LN non-custodial\ndeployment, as you don't have the minimal security budget to be on your own.\n\n> This opens up a pinning vector, or at least a significant nuisance: any\nother party can largely\n> increase the absolute fee without increasing the feerate, leveraging the\nRBF rules to prevent you\n> from replacing it without paying an insane fee. And you might not see it\nin your own mempool and\n> could only suppose it's happening by receiving non-full blocks or with\ntransactions paying a lower\n> feerate.\n\nSame issue with Lightning, we can be pinned today on the basis of\nreplace-by-fee rule 3. We can be also blinded by network mempool\npartitions, a pinning counterparty can segregate all the full-nodes  in as\nmany subsets by broadcasting a revoked Commitment transaction different for\neach. For Revault, I think you can also do unlimited partitions by mutating\nthe ANYONECANPAY-input of the Cancel.\n\nThat said, if you have a distributed towers deployment, spread across the\np2p network topology, and they can't be clustered together through\ncross-layers or intra-layer heuristics, you should be able to reliably\nobserve such partitions. I think such distributed monitors are deployed by\nfew L1 merchants accepting 0-conf to detect naive double-spend.\n\n> Unfortunately i know of no other primitive that can be used by\nmulti-party (i mean, >2) presigned\n> transactions protocols for fee-bumping that aren't (more) vulnerable to\npinning.\n\nHave we already discussed a fee-bumping \"shared cache\", a CPFP variation ?\nStrawman idea: Alice and Bob commit collateral inputs to a separate UTXO\nfrom the main \"offchain contract\" one. This UTXO is locked by a multi-sig.\nFor any Commitment transaction pre-signed, also counter-sign a CPFP with\ntop mempool feerate included, spending a Commitment anchor output and the\nshared-cache UTXO. If the fees spike,  you can re-sign a high-feerate CPFP,\nassuming interactivity. As the CPFP is counter-signed by everyone, the\noutputs can be CSV-1 encumbered to prevent pinnings. If the share-cache is\nfeeded at parity, there shouldn't be an incentive to waste or maliciously\ninflate the feerate. I think this solution can be easily generalized to\nmore than 2 counterparties by using a multi-signature scheme. Big issue, if\nthe feerate is short due to fee spikes and you need to re-sign a\nhigher-feerate CPFP, you're trusting your counterparty to interact, though\narguably not worse than the current update fee mechanism.\n\n> For Lightning, it'd mean keeping an equivalent amount of funds as the sum\nof all your\nchannels balances sitting there unallocated \"just in case\". This is not\nreasonable.\n\nAgree, game-theory wise, you would like to keep a full fee-bumping reserve,\nready to burn as much in fees as the contested HTLC value, as it's the\nmaximum gain of your counterparty. Though perfect equilibrium is hard to\nachieve because your malicious counterparty might have an edge pushing you\nto broadcast your Commitment first by witholding HTLC resolution.\n\nFractional fee-bumping reserves are much more realistic to expect in the LN\nnetwork. Lower fee-bumping reserve, higher liquidity deployed, in theory\nhigher routing fees. By observing historical feerates, average offchain\nbalances at risk and routing fees expected gains, you should be able to\ndiscover an equilibrium where higher levels of reserve aren't worth the\nopportunity cost. I guess this  equilibrium could be your LN fee-bumping\nreserve max feerate.\n\nNote, I think the LN approach is a bit different from what suits a custody\nprotocol like Revault,  as you compute a direct return of the frozen\nfee-bumping liquidity. With Revault, if you have numerous bitcoins\nprotected, it's might be more interesting to adopt a \"buy the mempool,\nstupid\" strategy than risking fund safety for few percentages of interest\nreturns.\n\n> This is easier to reason about with a per-contract reserve.\n\nFor Lightning, this per-channel approach is safer too, as one Commitment\ntransaction pinned or jammed could affect the confirmation odds of your\nremaining LN Commitment transactions.\n\n> For your Lightning channel you would probably take the maximum size of\nyour commitment transaction\n> according to your HTLC exposure settings + the size of as many\n`htlc_success` transactions?\n\nYes, I guess it's your holder's `max_accepted_htcls` * `HTLC-Success\nweight` + counterparty's `max_accepted_htlcs` * `HTLC-Timeout weight`\nBetter to adopt this worst-case as the base transaction weight to fee-bump,\nas currently we can't dynamically update channel policies.\n\n> For some other applications with large transactions and lower-value UTxOs\non average it's\n> likely that only part of the offchain contracts might be enforceable at a\nreasonable feerate. Is it\n> reasonable?\n\nThis is where the \"anticipate the crowd of bitcoin users move\" point can be\nlaid out. As the crowd of bitcoin users' fee-bumping reserves are\nultimately unknown from your node knowledge, you should be ready to be a\nbit more conservative than the vanilla fee-bumping strategies shipped by\ndefault. In case of massive mempool congestion, your additional\nconservatism might get your time-sensitive transactions and game on the\ncrowd of bitcoin users. First Problem: if all offchain bitcoin software\nadopt that strategy we might inflate the worst-case feerate rate at the\nbenefit of the miners, without holistically improving block throughput.\nSecond problem : your class of offchain bitcoin softwares might have\nridiculous fee-bumping reserve compared\nto other classes of offchain bitcoin softwares (Revault > Lightning) and\njust be priced out bydesign in case of mempool congestion. Third problem :\nas the number of offchain bitcoin applications should go up with time, your\nfee-bumping reserve levels based from historical data might be always late\nby one \"bank-run\" scenario.\n\nFor Lightning, if you're short in fee-bumping reserves you might still do\npreemptive channel closures, either cooperatively or unilaterally and get\nback the off-chain liquidity to protect the more economically interesting\nchannels. Though again, that kind of automatic behavior might be compelling\nat the individual node-level, but make the mempol congestion worse\nholistically.\n\n> First of all, when to fee-bump? At fixed time intervals? At each block\nconnection?\n\nIn case of massive mempool congestion, you might try to front-run the crowd\nof bitcoin users relying on block connections for fee-bumping, and thus\nstart your fee-bumping as soon as you observe feerate groups fluctuations\nin your local mempool(s).\n\nAlso you might proceed your fee-bumping ticks on a local clock instead of\nblock connections in case of time-dilation or deeper eclipse attacks of\nyour local node. Your view of the chain might be compromised but not your\nability to broadcast transactions thanks to emergency channels (in the\nnon-LN sense...though in fact quid of txn wrapped in onions ?) of\ncommunication.\n\n> You might skew miners incentives in doing\n> so: if you increase the fees by a factor of N, any miner with a fraction\nlarger than 1/N of the\n> network hashrate now has an incentive to censor your transaction at first\nto get you to panic.\n\nYes I think miner-harvesting attacks should be weighed carefully in the\ndesign of offchain contracts fee-bumping strategies, at least in the future\nwhen the mining reward exhausts further. I wonder if a more refined formula\nshould encompass the miner loss for empty blocks and ensure this loss stays\nmore substantial than the fees increased. So something like computing \"for\nX censored blocks, the Y average loss should be superior to the Z\nfee-bumping increase\".\n\n> Of course, given it's all hacks and workarounds and there is no good\nanswer to \"what is a reasonable\n> feerate up to which we need to make contracts enforceable onchain?\",\nthere is definitely room for an\n> insurance market.\n\nYes, stay open the question on how you enforce this block insurance market.\nReputation, which might be to avoid due to the latent centralization\neffect, might be hard to stack and audit reliably for an emergency\nmechanism running, hopefully, once in a halvening period. Maybe maybe some\ncryptographic or economically based mechanism on slashing or swaps could be\nfound...\n\nAntoine\n\nLe lun. 29 nov. 2021 \u00e0 09:34, darosior via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Hi everyone,\n>\n> Fee-bumping is paramount to the security of many protocols building on\n> Bitcoin, as they require the\n> confirmation of a transaction (which might be presigned) before the\n> expiration of a timelock at any\n> point after the establishment of the contract.\n>\n> The part of Revault using presigned transactions (the delegation from a\n> large to a smaller multisig)\n> is no exception. We have been working on how to approach this for a while\n> now and i'd like to share\n> what we have in order to open a discussion on this problem so central to\n> what seem to be The Right\n> Way [0] to build on Bitcoin but which has yet to be discussed in details\n> (at least publicly).\n>\n> I'll discuss what we came up with for Revault (at least for what will be\n> its first iteration) but my\n> intent with posting to the mailing list is more to frame the questions to\n> this problem we are all\n> going to face rather than present the results of our study tailored to the\n> Revault usecase.\n> The discussion is still pretty Revault-centric (as it's the case study)\n> but hopefully this can help\n> future protocol designers and/or start a discussion around what everyone's\n> doing for existing ones.\n>\n>\n> ## 1. Reminder about Revault\n>\n> The part of Revault we are interested in for this study is the delegation\n> process, and more\n> specifically the application of spending policies by network monitors\n> (watchtowers).\n> Coins are received on a large multisig. Participants of this large\n> multisig create 2 [1]\n> transactions. The Unvault, spending a deposit UTxO, creates an output\n> paying either to the small\n> multisig after a timelock or to the large multisig immediately. The\n> Cancel, spending the Unvault\n> output through the non-timelocked path, creates a new deposit UTxO.\n> Participants regularly exchange the Cancel transaction signatures for each\n> deposit, sharing the\n> signatures with the watchtowers they operate. They then optionally [2]\n> sign the Unvault transaction\n> and share the signatures with the small multisig participants who can in\n> turn use them to proceed\n> with a spending. Watchtowers can enforce spending policies (say, can't\n> Unvault outside of business\n> hours) by having the Cancel transaction be confirmed before the expiration\n> of the timelock.\n>\n>\n> ## 2. Problem statement\n>\n> For any delegated vault, ensure the confirmation of a Cancel transaction\n> in a configured number of\n> blocks at any point. In so doing, minimize the overpayments and the UTxO\n> set footprint. Overpayments\n> increase the burden on the watchtower operator by increasing the required\n> frequency of refills of the\n> fee-bumping wallet, which is already the worst user experience. You are\n> likely to manage a number of\n> UTxOs with your number of vaults, which comes at a cost for you as well as\n> everyone running a full\n> node.\n>\n> Note that this assumes miners are economically rationale, are incentivized\n> by *public* fees and that\n> you have a way to propagate your fee-bumped transaction to them. We also\n> don't consider the block\n> space bounds.\n>\n> In the previous paragraph and the following text, \"vault\" can generally be\n> replaced with \"offchain\n> contract\".\n>\n>\n> ## 3. With presigned transactions\n>\n> As you all know, the first difficulty is to get to be able to unilaterally\n> enforce your contract\n> onchain. That is, any participant must be able to unilaterally bump the\n> fees of a transaction even\n> if it was co-signed by other participants.\n>\n> For Revault we can afford to introduce malleability in the Cancel\n> transaction since there is no\n> second-stage transaction depending on its txid. Therefore it is pre-signed\n> with ANYONECANPAY. We\n> can't use ANYONECANPAY|SINGLE since it would open a pinning vector [3].\n> Note how we can't leverage\n> the carve out rule, and neither can any other more-than-two-parties\n> contract.\n> This has a significant implication for the rest, as we are entirely\n> burning fee-bumping UTxOs.\n>\n> This opens up a pinning vector, or at least a significant nuisance: any\n> other party can largely\n> increase the absolute fee without increasing the feerate, leveraging the\n> RBF rules to prevent you\n> from replacing it without paying an insane fee. And you might not see it\n> in your own mempool and\n> could only suppose it's happening by receiving non-full blocks or with\n> transactions paying a lower\n> feerate.\n> Unfortunately i know of no other primitive that can be used by multi-party\n> (i mean, >2) presigned\n> transactions protocols for fee-bumping that aren't (more) vulnerable to\n> pinning.\n>\n>\n> ## 4. We are still betting on future feerate\n>\n> The problem is still missing one more constraint. \"Ensuring confirmation\n> at any time\" involves ensuring\n> confirmation at *any* feerate, which you *cannot* do. So what's the limit?\n> In theory you should be ready\n> to burn as much in fees as the value of the funds you want to get out of\n> the contract. So... For us\n> it'd mean keeping for each vault an equivalent amount of funds sitting\n> there on the watchtower's hot\n> wallet. For Lightning, it'd mean keeping an equivalent amount of funds as\n> the sum of all your\n> channels balances sitting there unallocated \"just in case\". This is not\n> reasonable.\n>\n> So you need to keep a maximum feerate, above which you won't be able to\n> ensure the enforcement of\n> all your contracts onchain at the same time. We call that the \"reserve\n> feerate\" and you can have\n> different strategies for choosing it, for instance:\n> - The 85th percentile over the last year of transactions feerates\n> - The maximum historical feerate\n> - The maximum historical feerate adjusted in dollars (makes more sense but\n> introduces a (set of?)\n>   trusted oracle(s) in a security-critical component)\n> - Picking a random high feerate (why not? It's an arbitrary assumption\n> anyways)\n>\n> Therefore, even if we don't have to bet on the broadcast-time feerate\n> market at signing time anymore\n> (since we can unilaterally bump), we still need some kind of prediction in\n> preparation of making\n> funds available to bump the fees at broadcast time.\n> Apart from judging that 500sat/vb is probably more reasonable than\n> 10sat/vbyte, this unfortunately\n> sounds pretty much crystal-ball-driven.\n>\n> We currently use the maximum of the 95th percentiles over 90-days windows\n> over historical block chain\n> feerates. [4]\n>\n>\n> ## 5. How much funds does my watchtower need?\n>\n> That's what we call the \"reserve\". Depending on your reserve feerate\n> strategy it might vary over\n> time. This is easier to reason about with a per-contract reserve. For\n> Revault it's pretty\n> straightforward since the Cancel transaction size is static:\n> `reserve_feerate * cancel_size`. For\n> other protocols with dynamic transaction sizes (or even packages of\n> transactions) it's less so. For\n> your Lightning channel you would probably take the maximum size of your\n> commitment transaction\n> according to your HTLC exposure settings + the size of as many\n> `htlc_success` transaction?\n>\n> Then you either have your software or your user guesstimate how many\n> offchain contracts the\n> watchtower will have to watch, time that by the per-contract reserve and\n> refill this amount (plus\n> some slack in practice). Once again, a UX tradeoff (not even mentioning\n> the guesstimation UX):\n> overestimating leads to too many unallocated funds sitting on a hot\n> wallet, underestimating means\n> (at best) inability to participate in new contracts or being \"at risk\"\n> (not being able to enforce\n> all your contracts onchain at your reserve feerate) before a new refill.\n>\n> For vaults you likely have large-value UTxOs and small transactions (the\n> Cancel is one-in one-out in\n> Revault). For some other applications with large transactions and\n> lower-value UTxOs on average it's\n> likely that only part of the offchain contracts might be enforceable at a\n> reasonable feerate. Is it\n> reasonable?\n>\n>\n> ## 6. UTxO pool layout\n>\n> Now that you somehow managed to settle on a refill amount, how are you\n> going to use these funds?\n> Also, you'll need to manage your pool across time (consolidating small\n> coins, and probably fanning\n> out large ones).\n>\n> You could keep a single large UTxO and peel it as you need to sponsor\n> transactions. But this means\n> that you need to create a coin of a specific value according to your need\n> at the current feerate\n> estimation, hope to have it confirmed in a few blocks (at least for now!\n> [5]), and hope that the\n> value won't be obsolete by the time it confirmed. Also, you'd have to do\n> that for any number of\n> Cancel, chaining feebump coin creation transactions off the change of the\n> previous ones or replacing\n> them with more outputs. Both seem to become really un-manageable (and\n> expensive) in many edge-cases,\n> shortening the time you have to confirm the actual Cancel transaction and\n> creating uncertainty about\n> the reserve (how much is my just-in-time fanout going to cost me in fees\n> that i need to refill in\n> advance on my watchtower wallet?).\n> This is less of a concern for protocols using CPFP to sponsor\n> transactions, but they rely on a\n> policy rule specific to 2-parties contracts.\n>\n> Therefore for Revault we fan-out the coins per-vault in advance. We do so\n> at refill time so the\n> refiller can give an excess to pay for the fees of the fanout transaction\n> (which is reasonable since\n> it will occur just after the refilling transaction confirms). When the\n> watchtower is asked to watch\n> for a new delegated vault it will allocate coins from the pool of\n> fanned-out UTxOs to it (failing\n> that, it would refuse the delegation).\n> What is a good distribution of UTxOs amounts per vault? We want to\n> minimize the number of coins,\n> still have coins small enough to not overpay (remember, we can't have\n> change) and be able to bump a\n> Cancel up to the reserve feerate using these coins. The two latter\n> constraints are directly in\n> contradiction as the minimal value of a coin usable at the reserve feerate\n> (paying for its own input\n> fee + bumping the feerate by, say, 5sat/vb) is already pretty high.\n> Therefore we decided to go with\n> two distributions per vault. The \"reserve distribution\" alone ensures that\n> we can bump up to the\n> reserve feerate and is usable for high feerates. The \"bonus distribution\"\n> is not, but contains\n> smaller coins useful to prevent overpayments during low and medium fee\n> periods (which is most of the\n> time).\n> Both distributions are based on a basic geometric suite [6]. Each value is\n> half the previous one.\n> This exponentially decreases the value, limiting the number of coins. But\n> this also allows for\n> pretty small coins to exist and each coin's value is equal to the sum of\n> the smaller coins,\n> or smaller by at most the value of the smallest coin. Therefore bounding\n> the maximum overpayment to\n> the smallest coin's value [7].\n>\n> For the management of the UTxO pool across time we merged the\n> consolidation with the fanout. When\n> fanning out a refilled UTxO, we scan the pool for coins that need to be\n> consolidated according to a\n> heuristic. An instance of a heuristic is \"the coin isn't allocated and\n> would not have been able to\n> increase the fee at the median feerate over the past 90 days of blocks\".\n> We had this assumption that feerate would tend to go up with time and\n> therefore discarded having to\n> split some UTxOs from the pool. We however overlooked that a large\n> increase in the exchange price of\n> BTC as we've seen during the past year could invalidate this assumption\n> and that should arguably be\n> reconsidered.\n>\n>\n> ## 7. Bumping and re-bumping\n>\n> First of all, when to fee-bump? At fixed time intervals? At each block\n> connection? It sounds like,\n> given a large enough timelock, you could try to greed by \"trying your\n> luck\" at a lower feerate and\n> only re-bumping every N blocks. You would then start aggressively bumping\n> at every block after M\n> blocks have passed. But that's actually a bet (in disguised?) that the\n> next block feerate in M blocks\n> will be lower than the current one. In the absence of any predictive model\n> it is more reasonable to\n> just start being aggressive immediately.\n> You probably want to base your estimates on `estimatesmartfee` and as a\n> consequence you would re-bump\n> (if needed )after each block connection, when your estimates get updated\n> and you notice your\n> transaction was not included in the block.\n>\n> In the event that you notice a consequent portion of the block is filled\n> with transactions paying\n> less than your own, you might want to start panicking and bump your\n> transaction fees by a certain\n> percentage with no consideration for your fee estimator. You might skew\n> miners incentives in doing\n> so: if you increase the fees by a factor of N, any miner with a fraction\n> larger than 1/N of the\n> network hashrate now has an incentive to censor your transaction at first\n> to get you to panic. Also\n> note this can happen if you want to pay the absolute fees for the\n> 'pinning' attack mentioned in\n> section #2, and that might actually incentivize miners to perform it\n> themselves..\n>\n> The gist is that the most effective way to bump and rebump (RBF the Cancel\n> tx) seems to just be to\n> consider the `estimatesmartfee 2 CONSERVATIVE` feerate at every block your\n> tx isn't included in, and\n> to RBF it if the feerate is higher.\n> In addition, we fallback to a block chain based estimation when estimates\n> aren't available (eg if\n> the user stopped their WT for say a hour and we come back up): we use the\n> 85th percentile over the\n> feerates in the last 6 blocks. Sure, miners can try to have an influence\n> on that by stuffing their\n> blocks with large fee self-paying transactions, but they would need to:\n> 1. Be sure to catch a significant portion of the 6 blocks (at least 2,\n> actually)\n> 2. Give up on 25% of the highest fee-paying transactions (assuming they\n> got the 6 blocks, it's\n>    proportionally larger and incertain as they get less of them)\n> 3. Hope that our estimator will fail and we need to fall back to the\n> chain-based estimation\n>\n>\n> ## 8. Our study\n>\n> We essentially replayed the historical data with different deployment\n> configurations (number of\n> participants and timelock) and probability of an event occurring (event\n> being say an Unvault, an\n> invalid Unvault, a new delegation, ..). We then observed different metrics\n> such as the time at risk\n> (when we can't enforce all our contracts at the reserve feerate at the\n> same time), or the\n> operational cost.\n> We got the historical fee estimates data from Statoshi [9], Txstats [10]\n> and the historical chain\n> data from Riccardo Casatta's `blocks_iterator` [11]. Thanks!\n>\n> The (research-quality..) code can be found at\n> https://github.com/revault/research under the section\n> \"Fee bumping\". Again it's very Revault specific, but at least the data can\n> probably be reused for\n> studying other protocols.\n>\n>\n> ## 9. Insurances\n>\n> Of course, given it's all hacks and workarounds and there is no good\n> answer to \"what is a reasonable\n> feerate up to which we need to make contracts enforceable onchain?\", there\n> is definitely room for an\n> insurance market. But this enters the realm of opinions. Although i do\n> have some (having discussed\n> this topic for the past years with different people), i would like to keep\n> this post focused on the\n> technical aspects of this problem.\n>\n>\n>\n> [0] As far as i can tell, having offchain contracts be enforceable onchain\n> by confirming a\n> transaction before the expiration of a timelock is a widely agreed-upon\n> approach. And i don't think\n> we can opt for any other fundamentally different one, as you want to know\n> you can claim back your\n> coins from a contract after a deadline before taking part in it.\n>\n> [1] The Real Revault (tm) involves more transactions, but for the sake of\n> conciseness i only\n> detailed a minimum instance of the problem.\n>\n> [2] Only presigning part of the Unvault transactions allows to only\n> delegate part of the coins,\n> which can be abstracted as \"delegate x% of your stash\" in the user\n> interface.\n>\n> [3]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-May/017835.html\n>\n> [4]\n> https://github.com/revault/research/blob/1df953813708287c32a15e771ba74957ec44f354/feebumping/model/statemachine.py#L323-L329\n>\n> [5] https://github.com/bitcoin/bitcoin/pull/23121\n>\n> [6]\n> https://github.com/revault/research/blob/1df953813708287c32a15e771ba74957ec44f354/feebumping/model/statemachine.py#L494-L507\n>\n> [7] Of course this assumes a combinatorial coin selection, but i believe\n> it's ok given we limit the\n> number of coins beforehand.\n>\n> [8] Although there is the argument to outbid a censorship, anyone\n> censoring you isn't necessarily a\n> miner.\n>\n> [9] https://www.statoshi.info/\n>\n> [10] https://www.statoshi.info/\n>\n> [11] https://github.com/RCasatta/blocks_iterator\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211129/883846a6/attachment-0001.html>"
            },
            {
                "author": "darosior",
                "date": "2021-11-30T15:19:42",
                "message_text_only": "Hi Antoine,\n\nThanks for your comment. I believe for Lightning it's simpler with regard to the management of the UTxO pool, but harder with regard to choosing\na threat model.\nResponses inline.\n\n> For any opened channel, ensure the confirmation of a Commitment transaction and the children HTLC-Success/HTLC-Timeout transactions. Note, in the Lightning security game you have to consider (at least) 4 types of players moves and incentives : your node, your channel counterparties, the miners, the crowd of bitcoin users. The number of the last type of players is unknown from your node, however it should not be forgotten you're in competition for block space, therefore their block demands bids should be anticipated and reacted to in consequence. With that remark in mind, implications for your LN fee-bumping strategy will be raised afterwards.\n>\n> For a LN service provider, on-chain overpayments are bearing on your operational costs, thus downgrading your economic competitiveness. For the average LN user, overpayment might price out outside a LN non-custodial deployment, as you don't have the minimal security budget to be on your own.\n\nI think this problem statement can be easily generalised to any offchain contract. And your points stand for all of them.\n\"For any opened contract, ensure at any point the confirmation of a (set of) transaction(s) in a given number of blocks\"\n\n> Same issue with Lightning, we can be pinned today on the basis of replace-by-fee rule 3. We can be also blinded by network mempool partitions, a pinning counterparty can segregate all the full-nodes in as many subsets by broadcasting a revoked Commitment transaction different for each. For Revault, I think you can also do unlimited partitions by mutating the ANYONECANPAY-input of the Cancel.\n\nWell you can already do unlimited partitions by adding different inputs to it. You could malleate the witness, but since we are using Miniscript i'm confident you would only be able in a marginal way.\n\n> That said, if you have a distributed towers deployment, spread across the p2p network topology, and they can't be clustered together through cross-layers or intra-layer heuristics, you should be able to reliably observe such partitions. I think such distributed monitors are deployed by few L1 merchants accepting 0-conf to detect naive double-spend.\n\nWe should aim to more than 0-conf (in)security level..\nIt seems to me the only policy-level mitigation for RBF pinning around the \"don't decrease the abolute fees of a less-than-a-block mempool\" would be to drop the requirement on increasing absolute fees if the mempool is \"full enough\" (and the feerate increases exponentially, of course).\nAnother approach could be by introducing new consensus rules as proposed by Jeremy last year [0]. If we go in the realm of new consensus rules, then i think that simply committing to a maximum tx size would fix pinning by RBF rule 3. Could be in the annex, or in the unused sequence bits (although they currently are by Lightning, meh). You could also check in the output script that the input commits to this.\n\n[0] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-September/018168.html\n\n> Have we already discussed a fee-bumping \"shared cache\", a CPFP variation ? Strawman idea: Alice and Bob commit collateral inputs to a separate UTXO from the main \"offchain contract\" one. This UTXO is locked by a multi-sig. For any Commitment transaction pre-signed, also counter-sign a CPFP with top mempool feerate included, spending a Commitment anchor output and the shared-cache UTXO. If the fees spike, you can re-sign a high-feerate CPFP, assuming interactivity. As the CPFP is counter-signed by everyone, the outputs can be CSV-1 encumbered to prevent pinnings. If the share-cache is feeded at parity, there shouldn't be an incentive to waste or maliciously inflate the feerate. I think this solution can be easily generalized to more than 2 counterparties by using a multi-signature scheme. Big issue, if the feerate is short due to fee spikes and you need to re-sign a higher-feerate CPFP, you're trusting your counterparty to interact, though arguably not worse than the current update fee mechanism.\n\nIt really looks just like `update_fee`. Except maybe with the property that you have the channel liquidity not depend on the onchain feerate.\nIn any case, for Lightning i think it's a bad idea to re-introduce trust on this side post anchor outputs. For Revault it's clearly out of the question to introduce trust in your counterparties (why would you bother having a fee-bumping mechanism in the first place then?). Probably the same holds for all offchain contracts.\n\n>> For Lightning, it'd mean keeping an equivalent amount of funds as the sum of all your\n> channels balances sitting there unallocated \"just in case\". This is not reasonable.\n>\n> Agree, game-theory wise, you would like to keep a full fee-bumping reserve, ready to burn as much in fees as the contested HTLC value, as it's the maximum gain of your counterparty. Though perfect equilibrium is hard to achieve because your malicious counterparty might have an edge pushing you to broadcast your Commitment first by witholding HTLC resolution.\n>\n> Fractional fee-bumping reserves are much more realistic to expect in the LN network. Lower fee-bumping reserve, higher liquidity deployed, in theory higher routing fees. By observing historical feerates, average offchain balances at risk and routing fees expected gains, you should be able to discover an equilibrium where higher levels of reserve aren't worth the opportunity cost. I guess this equilibrium could be your LN fee-bumping reserve max feerate.\n>\n> Note, I think the LN approach is a bit different from what suits a custody protocol like Revault, as you compute a direct return of the frozen fee-bumping liquidity. With Revault, if you have numerous bitcoins protected, it's might be more interesting to adopt a \"buy the mempool, stupid\" strategy than risking fund safety for few percentages of interest returns.\n\nTrue for routing nodes. For wallets (if receiving funds), it's not about an investment: just users expectations to being able to transact without risking to lose their funds (ie being able to enforce their contract onchain). Although wallets they are much less at risk.\n\n> This is where the \"anticipate the crowd of bitcoin users move\" point can be laid out. As the crowd of bitcoin users' fee-bumping reserves are ultimately unknown from your node knowledge, you should be ready to be a bit more conservative than the vanilla fee-bumping strategies shipped by default. In case of massive mempool congestion, your additional conservatism might get your time-sensitive transactions and game on the crowd of bitcoin users. First Problem: if all offchain bitcoin software adopt that strategy we might inflate the worst-case feerate rate at the benefit of the miners, without holistically improving block throughput. Second problem : your class of offchain bitcoin softwares might have ridiculous fee-bumping reserve compared\n> to other classes of offchain bitcoin softwares (Revault > Lightning) and just be priced out bydesign in case of mempool congestion. Third problem : as the number of offchain bitcoin applications should go up with time, your fee-bumping reserve levels based from historical data might be always late by one \"bank-run\" scenario.\n\nBlack swan event 2.0? Just rule n\u00b03 is inherent to any kind of fee estimation.\n\n> For Lightning, if you're short in fee-bumping reserves you might still do preemptive channel closures, either cooperatively or unilaterally and get back the off-chain liquidity to protect the more economically interesting channels. Though again, that kind of automatic behavior might be compelling at the individual node-level, but make the mempol congestion worse holistically.\n\nYeah so we are back to the \"fractional reserve\" model: you can only enforce X% of the offchain contracts your participate in.. Actually it's even an added assumption: that you still have operating contracts, with honest counterparties.\n\n> In case of massive mempool congestion, you might try to front-run the crowd of bitcoin users relying on block connections for fee-bumping, and thus start your fee-bumping as soon as you observe feerate groups fluctuations in your local mempool(s).\n\nI don't think any kind of mempool-based estimate generalizes well, since at any point the expected time before the next block is 10 minutes (and a lot can happen in 10min).\n\n> Also you might proceed your fee-bumping ticks on a local clock instead of block connections in case of time-dilation or deeper eclipse attacks of your local node. Your view of the chain might be compromised but not your ability to broadcast transactions thanks to emergency channels (in the non-LN sense...though in fact quid of txn wrapped in onions ?) of communication.\n\nOh, yeah, i didn't explicit \"not getting eclipsed\" (or more generally \"data availability\") as an assumption since it's generally one made by participants of any offchain contract. In this case you can't even have decent fee estimation, so you are screwed anyways.\n\n> Yes, stay open the question on how you enforce this block insurance market. Reputation, which might be to avoid due to the latent centralization effect, might be hard to stack and audit reliably for an emergency mechanism running, hopefully, once in a halvening period. Maybe maybe some cryptographic or economically based mechanism on slashing or swaps could be found...\n\nUnfortunately, given current mining centralisation, pools are in a very good position to offer pretty decent SLAs around that. With a block space insurance, you of course don't need all these convoluted fee-bumping hacks.\nI'm very concerned that large stakeholders of the \"offchain contracts ecosystem\" would just go this (easier) way and further increase mining centralisation pressure.\n\nI agree that a cryptography-based scheme around this type of insurance services would be the best way out.\n\n> Antoine\n>\n> Le lun. 29 nov. 2021 \u00e0 09:34, darosior via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>\n>> Hi everyone,\n>>\n>> Fee-bumping is paramount to the security of many protocols building on Bitcoin, as they require the\n>> confirmation of a transaction (which might be presigned) before the expiration of a timelock at any\n>> point after the establishment of the contract.\n>>\n>> The part of Revault using presigned transactions (the delegation from a large to a smaller multisig)\n>> is no exception. We have been working on how to approach this for a while now and i'd like to share\n>> what we have in order to open a discussion on this problem so central to what seem to be The Right\n>> Way [0] to build on Bitcoin but which has yet to be discussed in details (at least publicly).\n>>\n>> I'll discuss what we came up with for Revault (at least for what will be its first iteration) but my\n>> intent with posting to the mailing list is more to frame the questions to this problem we are all\n>> going to face rather than present the results of our study tailored to the Revault usecase.\n>> The discussion is still pretty Revault-centric (as it's the case study) but hopefully this can help\n>> future protocol designers and/or start a discussion around what everyone's doing for existing ones.\n>>\n>> ## 1. Reminder about Revault\n>>\n>> The part of Revault we are interested in for this study is the delegation process, and more\n>> specifically the application of spending policies by network monitors (watchtowers).\n>> Coins are received on a large multisig. Participants of this large multisig create 2 [1]\n>> transactions. The Unvault, spending a deposit UTxO, creates an output paying either to the small\n>> multisig after a timelock or to the large multisig immediately. The Cancel, spending the Unvault\n>> output through the non-timelocked path, creates a new deposit UTxO.\n>> Participants regularly exchange the Cancel transaction signatures for each deposit, sharing the\n>> signatures with the watchtowers they operate. They then optionally [2] sign the Unvault transaction\n>> and share the signatures with the small multisig participants who can in turn use them to proceed\n>> with a spending. Watchtowers can enforce spending policies (say, can't Unvault outside of business\n>> hours) by having the Cancel transaction be confirmed before the expiration of the timelock.\n>>\n>> ## 2. Problem statement\n>>\n>> For any delegated vault, ensure the confirmation of a Cancel transaction in a configured number of\n>> blocks at any point. In so doing, minimize the overpayments and the UTxO set footprint. Overpayments\n>> increase the burden on the watchtower operator by increasing the required frequency of refills of the\n>> fee-bumping wallet, which is already the worst user experience. You are likely to manage a number of\n>> UTxOs with your number of vaults, which comes at a cost for you as well as everyone running a full\n>> node.\n>>\n>> Note that this assumes miners are economically rationale, are incentivized by *public* fees and that\n>> you have a way to propagate your fee-bumped transaction to them. We also don't consider the block\n>> space bounds.\n>>\n>> In the previous paragraph and the following text, \"vault\" can generally be replaced with \"offchain\n>> contract\".\n>>\n>> ## 3. With presigned transactions\n>>\n>> As you all know, the first difficulty is to get to be able to unilaterally enforce your contract\n>> onchain. That is, any participant must be able to unilaterally bump the fees of a transaction even\n>> if it was co-signed by other participants.\n>>\n>> For Revault we can afford to introduce malleability in the Cancel transaction since there is no\n>> second-stage transaction depending on its txid. Therefore it is pre-signed with ANYONECANPAY. We\n>> can't use ANYONECANPAY|SINGLE since it would open a pinning vector [3]. Note how we can't leverage\n>> the carve out rule, and neither can any other more-than-two-parties contract.\n>> This has a significant implication for the rest, as we are entirely burning fee-bumping UTxOs.\n>>\n>> This opens up a pinning vector, or at least a significant nuisance: any other party can largely\n>> increase the absolute fee without increasing the feerate, leveraging the RBF rules to prevent you\n>> from replacing it without paying an insane fee. And you might not see it in your own mempool and\n>> could only suppose it's happening by receiving non-full blocks or with transactions paying a lower\n>> feerate.\n>> Unfortunately i know of no other primitive that can be used by multi-party (i mean, >2) presigned\n>> transactions protocols for fee-bumping that aren't (more) vulnerable to pinning.\n>>\n>> ## 4. We are still betting on future feerate\n>>\n>> The problem is still missing one more constraint. \"Ensuring confirmation at any time\" involves ensuring\n>> confirmation at *any* feerate, which you *cannot* do. So what's the limit? In theory you should be ready\n>> to burn as much in fees as the value of the funds you want to get out of the contract. So... For us\n>> it'd mean keeping for each vault an equivalent amount of funds sitting there on the watchtower's hot\n>> wallet. For Lightning, it'd mean keeping an equivalent amount of funds as the sum of all your\n>> channels balances sitting there unallocated \"just in case\". This is not reasonable.\n>>\n>> So you need to keep a maximum feerate, above which you won't be able to ensure the enforcement of\n>> all your contracts onchain at the same time. We call that the \"reserve feerate\" and you can have\n>> different strategies for choosing it, for instance:\n>> - The 85th percentile over the last year of transactions feerates\n>> - The maximum historical feerate\n>> - The maximum historical feerate adjusted in dollars (makes more sense but introduces a (set of?)\n>> trusted oracle(s) in a security-critical component)\n>> - Picking a random high feerate (why not? It's an arbitrary assumption anyways)\n>>\n>> Therefore, even if we don't have to bet on the broadcast-time feerate market at signing time anymore\n>> (since we can unilaterally bump), we still need some kind of prediction in preparation of making\n>> funds available to bump the fees at broadcast time.\n>> Apart from judging that 500sat/vb is probably more reasonable than 10sat/vbyte, this unfortunately\n>> sounds pretty much crystal-ball-driven.\n>>\n>> We currently use the maximum of the 95th percentiles over 90-days windows over historical block chain\n>> feerates. [4]\n>>\n>> ## 5. How much funds does my watchtower need?\n>>\n>> That's what we call the \"reserve\". Depending on your reserve feerate strategy it might vary over\n>> time. This is easier to reason about with a per-contract reserve. For Revault it's pretty\n>> straightforward since the Cancel transaction size is static: `reserve_feerate * cancel_size`. For\n>> other protocols with dynamic transaction sizes (or even packages of transactions) it's less so. For\n>> your Lightning channel you would probably take the maximum size of your commitment transaction\n>> according to your HTLC exposure settings + the size of as many `htlc_success` transaction?\n>>\n>> Then you either have your software or your user guesstimate how many offchain contracts the\n>> watchtower will have to watch, time that by the per-contract reserve and refill this amount (plus\n>> some slack in practice). Once again, a UX tradeoff (not even mentioning the guesstimation UX):\n>> overestimating leads to too many unallocated funds sitting on a hot wallet, underestimating means\n>> (at best) inability to participate in new contracts or being \"at risk\" (not being able to enforce\n>> all your contracts onchain at your reserve feerate) before a new refill.\n>>\n>> For vaults you likely have large-value UTxOs and small transactions (the Cancel is one-in one-out in\n>> Revault). For some other applications with large transactions and lower-value UTxOs on average it's\n>> likely that only part of the offchain contracts might be enforceable at a reasonable feerate. Is it\n>> reasonable?\n>>\n>> ## 6. UTxO pool layout\n>>\n>> Now that you somehow managed to settle on a refill amount, how are you going to use these funds?\n>> Also, you'll need to manage your pool across time (consolidating small coins, and probably fanning\n>> out large ones).\n>>\n>> You could keep a single large UTxO and peel it as you need to sponsor transactions. But this means\n>> that you need to create a coin of a specific value according to your need at the current feerate\n>> estimation, hope to have it confirmed in a few blocks (at least for now! [5]), and hope that the\n>> value won't be obsolete by the time it confirmed. Also, you'd have to do that for any number of\n>> Cancel, chaining feebump coin creation transactions off the change of the previous ones or replacing\n>> them with more outputs. Both seem to become really un-manageable (and expensive) in many edge-cases,\n>> shortening the time you have to confirm the actual Cancel transaction and creating uncertainty about\n>> the reserve (how much is my just-in-time fanout going to cost me in fees that i need to refill in\n>> advance on my watchtower wallet?).\n>> This is less of a concern for protocols using CPFP to sponsor transactions, but they rely on a\n>> policy rule specific to 2-parties contracts.\n>>\n>> Therefore for Revault we fan-out the coins per-vault in advance. We do so at refill time so the\n>> refiller can give an excess to pay for the fees of the fanout transaction (which is reasonable since\n>> it will occur just after the refilling transaction confirms). When the watchtower is asked to watch\n>> for a new delegated vault it will allocate coins from the pool of fanned-out UTxOs to it (failing\n>> that, it would refuse the delegation).\n>> What is a good distribution of UTxOs amounts per vault? We want to minimize the number of coins,\n>> still have coins small enough to not overpay (remember, we can't have change) and be able to bump a\n>> Cancel up to the reserve feerate using these coins. The two latter constraints are directly in\n>> contradiction as the minimal value of a coin usable at the reserve feerate (paying for its own input\n>> fee + bumping the feerate by, say, 5sat/vb) is already pretty high. Therefore we decided to go with\n>> two distributions per vault. The \"reserve distribution\" alone ensures that we can bump up to the\n>> reserve feerate and is usable for high feerates. The \"bonus distribution\" is not, but contains\n>> smaller coins useful to prevent overpayments during low and medium fee periods (which is most of the\n>> time).\n>> Both distributions are based on a basic geometric suite [6]. Each value is half the previous one.\n>> This exponentially decreases the value, limiting the number of coins. But this also allows for\n>> pretty small coins to exist and each coin's value is equal to the sum of the smaller coins,\n>> or smaller by at most the value of the smallest coin. Therefore bounding the maximum overpayment to\n>> the smallest coin's value [7].\n>>\n>> For the management of the UTxO pool across time we merged the consolidation with the fanout. When\n>> fanning out a refilled UTxO, we scan the pool for coins that need to be consolidated according to a\n>> heuristic. An instance of a heuristic is \"the coin isn't allocated and would not have been able to\n>> increase the fee at the median feerate over the past 90 days of blocks\".\n>> We had this assumption that feerate would tend to go up with time and therefore discarded having to\n>> split some UTxOs from the pool. We however overlooked that a large increase in the exchange price of\n>> BTC as we've seen during the past year could invalidate this assumption and that should arguably be\n>> reconsidered.\n>>\n>> ## 7. Bumping and re-bumping\n>>\n>> First of all, when to fee-bump? At fixed time intervals? At each block connection? It sounds like,\n>> given a large enough timelock, you could try to greed by \"trying your luck\" at a lower feerate and\n>> only re-bumping every N blocks. You would then start aggressively bumping at every block after M\n>> blocks have passed. But that's actually a bet (in disguised?) that the next block feerate in M blocks\n>> will be lower than the current one. In the absence of any predictive model it is more reasonable to\n>> just start being aggressive immediately.\n>> You probably want to base your estimates on `estimatesmartfee` and as a consequence you would re-bump\n>> (if needed )after each block connection, when your estimates get updated and you notice your\n>> transaction was not included in the block.\n>>\n>> In the event that you notice a consequent portion of the block is filled with transactions paying\n>> less than your own, you might want to start panicking and bump your transaction fees by a certain\n>> percentage with no consideration for your fee estimator. You might skew miners incentives in doing\n>> so: if you increase the fees by a factor of N, any miner with a fraction larger than 1/N of the\n>> network hashrate now has an incentive to censor your transaction at first to get you to panic. Also\n>> note this can happen if you want to pay the absolute fees for the 'pinning' attack mentioned in\n>> section #2, and that might actually incentivize miners to perform it themselves..\n>>\n>> The gist is that the most effective way to bump and rebump (RBF the Cancel tx) seems to just be to\n>> consider the `estimatesmartfee 2 CONSERVATIVE` feerate at every block your tx isn't included in, and\n>> to RBF it if the feerate is higher.\n>> In addition, we fallback to a block chain based estimation when estimates aren't available (eg if\n>> the user stopped their WT for say a hour and we come back up): we use the 85th percentile over the\n>> feerates in the last 6 blocks. Sure, miners can try to have an influence on that by stuffing their\n>> blocks with large fee self-paying transactions, but they would need to:\n>> 1. Be sure to catch a significant portion of the 6 blocks (at least 2, actually)\n>> 2. Give up on 25% of the highest fee-paying transactions (assuming they got the 6 blocks, it's\n>> proportionally larger and incertain as they get less of them)\n>> 3. Hope that our estimator will fail and we need to fall back to the chain-based estimation\n>>\n>> ## 8. Our study\n>>\n>> We essentially replayed the historical data with different deployment configurations (number of\n>> participants and timelock) and probability of an event occurring (event being say an Unvault, an\n>> invalid Unvault, a new delegation, ..). We then observed different metrics such as the time at risk\n>> (when we can't enforce all our contracts at the reserve feerate at the same time), or the\n>> operational cost.\n>> We got the historical fee estimates data from Statoshi [9], Txstats [10] and the historical chain\n>> data from Riccardo Casatta's `blocks_iterator` [11]. Thanks!\n>>\n>> The (research-quality..) code can be found at https://github.com/revault/research under the section\n>> \"Fee bumping\". Again it's very Revault specific, but at least the data can probably be reused for\n>> studying other protocols.\n>>\n>> ## 9. Insurances\n>>\n>> Of course, given it's all hacks and workarounds and there is no good answer to \"what is a reasonable\n>> feerate up to which we need to make contracts enforceable onchain?\", there is definitely room for an\n>> insurance market. But this enters the realm of opinions. Although i do have some (having discussed\n>> this topic for the past years with different people), i would like to keep this post focused on the\n>> technical aspects of this problem.\n>>\n>> [0] As far as i can tell, having offchain contracts be enforceable onchain by confirming a\n>> transaction before the expiration of a timelock is a widely agreed-upon approach. And i don't think\n>> we can opt for any other fundamentally different one, as you want to know you can claim back your\n>> coins from a contract after a deadline before taking part in it.\n>>\n>> [1] The Real Revault (tm) involves more transactions, but for the sake of conciseness i only\n>> detailed a minimum instance of the problem.\n>>\n>> [2] Only presigning part of the Unvault transactions allows to only delegate part of the coins,\n>> which can be abstracted as \"delegate x% of your stash\" in the user interface.\n>>\n>> [3] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-May/017835.html\n>>\n>> [4] https://github.com/revault/research/blob/1df953813708287c32a15e771ba74957ec44f354/feebumping/model/statemachine.py#L323-L329\n>>\n>> [5] https://github.com/bitcoin/bitcoin/pull/23121\n>>\n>> [6] https://github.com/revault/research/blob/1df953813708287c32a15e771ba74957ec44f354/feebumping/model/statemachine.py#L494-L507\n>>\n>> [7] Of course this assumes a combinatorial coin selection, but i believe it's ok given we limit the\n>> number of coins beforehand.\n>>\n>> [8] Although there is the argument to outbid a censorship, anyone censoring you isn't necessarily a\n>> miner.\n>>\n>> [9] https://www.statoshi.info/\n>>\n>> [10] https://www.statoshi.info/\n>>\n>> [11] https://github.com/RCasatta/blocks_iterator\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211130/79715fe8/attachment-0001.html>"
            },
            {
                "author": "Prayank",
                "date": "2021-11-30T01:47:31",
                "message_text_only": "Good morning darosior,\n\nSubject of the email looks interesting and I have few comments on the things shared:\n\n> The part of Revault we are interested in for this study is the delegation process, and more specifically the application of spending policies by network monitors (watchtowers). Participants regularly exchange the Cancel transaction signatures for each deposit, sharing the signatures with the watchtowers they operate.Watchtowers can enforce spending policies (say, can't Unvault outside of business hours) by having the Cancel transaction be confirmed before the expiration of the timelock.\n\nWhat are the privacy issues associated with such watchtowers?\n\n> ## 4. We are still betting on future feerate\nThe problem is still missing one more constraint. \"Ensuring confirmation at any time\" involves ensuring confirmation at *any* feerate, which you *cannot* do.\n\nAgree\n\n> historical feerate: We currently use the maximum of the 95th percentiles over 90-days windows over historical block chain\nfeerates.\n\nDisagree that fee rates used in past should matter.\n\n> Apart from judging that 500sat/vb is probably more reasonable than 10sat/vbyte, this unfortunately sounds pretty much crystal-ball-driven.\n\nAgree\n\n> ## 7. Bumping and re-bumping\nFirst of all, when to fee-bump? At fixed time intervals? At each block connection? It sounds like, given a large enough timelock, you could try to greed by \"trying your luck\" at a lower feerate and only re-bumping every N blocks. You would then start aggressively bumping at every block after M blocks have passed.\n\nAgree\n\n> You probably want to base your estimates on `estimatesmartfee`\n\nDisagree. `estimatesmartfee` RPC has few issues: https://github.com/bitcoin/bitcoin/pull/22722#issuecomment-901907447\n\n> ## 9. Insurances\nthere is definitely room for an insurance market.\n\nAgree. I think its possible using discreet log contracts with some trust assumptions and use of multi oracles.\n\nI had one idea about creating insurance project for LGBTQ community in India as they don't have enough options like others. Have shared the details here:\u00a0https://gist.github.com/prayank23/f30ab1ab68bffe6bcb2ceacec599cd36 \nAs final point, I guess you already know about this presentation by Jack Mallers in which he has described how we could create derivatives for users to hedge fees: https://youtu.be/rBCG0btUlTw\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211130/b6554626/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "A fee-bumping model",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "darosior",
                "Prayank",
                "Antoine Riard"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 73306
        }
    }
]