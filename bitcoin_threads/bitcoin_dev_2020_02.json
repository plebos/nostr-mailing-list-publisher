[
    {
        "title": "[bitcoin-dev] Onchain fee insurance mechanism",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2020-02-01T00:39:36",
                "message_text_only": "Good morning David,\n\n> On Fri, Jan 31, 2020 at 03:42:08AM +0000, ZmnSCPxj via bitcoin-dev wrote:\n>\n> > Let me then propose a specific mechanism for feerate insurance against onchain feerate spikes.\n> > [...]\n> > At current blockheight B, Alice and Ingrid then arrange a series of transactions:\n> >\n> >     nLockTime: B+1\n> >     nSequence: RBF enabled, no relative locktime.\n> >     inputs: Alice 5000000, Ingrid 800000\n> >     outputs:\n> >         Bob 400000\n> >         Alice 99400\n> >         Ingrid 800400\n> >     fee: 200\n> >\n> >\n> > [...]\n>\n> Ingrid is able to rescind this series of pre-signed transactions at any\n> time before one of the transactions is confirmed by double spending her\n> UTXO (e.g. via a RBF fee bump). If Alice needs to trust Ingrid to honor\n> the contract anyway, they might as well not include Ingrid's input or\n> output in the transaction and instead use an external accounting and\n> payment mechanism. For example, Alice and Ingrid agree to a fee\n> schedule:\n>\n> >     height: B+1\n> >     fee: 200\n> >\n> >     height: B+2\n> >     fee: 400\n> >\n> >     height: B+3\n> >     fee: 599\n> >\n> >     height: B+4\n> >     fee: 3600\n> >\n>\n> Then they wait for whichever version of the transaction to confirm and\n> one of them remits to the other the appropriate amount (either 400, 200,\n> or 1 base unit to Ingrid, or 3,000 base units to Alice). This\n> remittance can be done by whatever mechanism they both support (e.g. an\n> onchain transaction, an LN payment, or just credit on an exchange).\n>\n> Since it's possible to achieve equivilent security (or lack thereof)\n> without the locktime mechanism, I don't think the locktime mechanism\n> adds anything to the idea of hedging fees---and, as you note, it suffers\n> from incompatibility with some cases where users would be especially\n> eager to obtain feerate insurance.\n\nIndeed, the rescindability is a flaw.\nI will now do something really evil: I will attempt to patch this flaw without considering that the patch will of course have other detrimental side effects.\n\nRather than have the Ingrid-input (and output) be solely under the control of Ingrid, it is a 2-of-2 with Ingrid and Alice.\nLong before the Alice->Bob transaction, Alice has already commissioned the services of Ingrid.\nThey have already agreed on the specs of the insurance policy, and in particular, have agreed that this agreement terminates at some future data.\nAt setup, Alice and Ingrid create a claim transaction for Ingrid, with `nLockTime` set to the agreed-upon end-of-insurance-contract, which allows Ingrid to reclaim the original fund.\n\nThen, at height B when Alice wants to send to Bob, they create the series of timelocked transactions, with the Ingrid output similarly having an `nLockTime`d transaction that lets Ingrid reclaim the earned funds.\n\nAgainst this patched scheme, of course, new problems arise:\n\n* During times of low fees, Alice can just create a non-insured transaction directly on the blockchain, denying Ingrid its earnings.\n* During times of high fees, Ingrid can go offline and refuse to provide signatures needed for the insured transactions, denying Alice its service.\n  * This is significant if Alice prepaid for the insurance contract.\n\nThus, as we can see, patching a flawed protocol still leaves us with a flawed protocol.\n\n--\n\nOn the other hand, the above \"Spilmanizing\" of the protocol leads to a possible insurance policy for Lightning channel closures.\nAt the same time as channel establishment between Alice and Bob, Alice also starts an insurance contract with Ingrid.\nAlice prepays Ingrid, using a CoinJoined transaction that spends from Alice and Ingrid inputs, with the combined premium plus Ingrid inputs value put in an output locked to Alice && Ingrid, and a maximum contract lifetime (an `nLockTime`d transaction that claims the Alice&&Ingrid output and returns the fund, plus insurance premium, to Ingrid).\n\nThen, at each commitment transaction signing, there is an additional unencumbered but tiny output that Alice can claim immediately (obviously this requires a change in the BOLT spec).\nIngrid and Alice create an insurance transaction with high feerate, which spends the above tiny output, and spends the Alice&&Ingrid output, deducting the fees from the Alice&&Ingrid output and returning what is left to Ingrid.\n\nThen, if Alice decides to drop the unilateral close onchain:\n\n* If fees are low at the time that unilateral close, then Alice can just claim the tiny output itself.\n  * Alice is incentivized to do so because it means she will still control that tiny output.\n  * Ingrid can then reclaim its fund, plus the premium, at the end of the insurance contract lifetime.\n* If fees are high at the time that unilateral close, then Alice can sacrifice the value of the tiny output and attach the insurance transaction with high feerate.\n\nFurther:\n\n* If on a new commitment transaction, Ingrid does not cooperate, then Alice can drop onchain *and* punish Ingrid by dropping the previous commitment and also broadcasting the insurance transaction.\n  * Alice has to sacrifice its tiny output to do so, but it would be worth it to punish Ingrid and deter this non-cooperation.\n* When the insurance contract lifetime is near, Alice and Ingrid can renew the contract by cooperatively spending the Alice&&Ingrid output to a new Alice&&Ingrid output (possibly with some payment from Alice to renew the contract).\n* This gives an upper bound for what Alice will pay to ensure its channel is closeable at any time very quickly, which is the entire point.\n\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Onchain fee insurance mechanism",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 5567
        }
    },
    {
        "title": "[bitcoin-dev] CTV through SIGHASH flags",
        "thread_messages": [
            {
                "author": "Bob McElrath",
                "date": "2020-02-01T20:39:42",
                "message_text_only": "We propose that OP_CHECKTEMPLATEVERIFY should behave more like CHECKSIG,\nincluding a flags byte that specify what is hashed. This unifies the ways a\nSigHash is computed, differing only in the final checksig which is omitted in\nfavor of chacking the hash directly. Having two paths to create a signature hash\nintroduces extra complexity, especially as concerns potential future SIGHASH\nflag upgrades.\n\nCTV omits inputs as part of its semantics, so CTV-type functionality using\nCHECKSIG is also achievable if some form of NOINPUT flag is also deployed. With\nNOINPUT alone, a standard CHECKSIG can be used to implement a covenant -- though\nit uses an unnecessarily large number of bytes just to check a 32-byte hash.\nTherefore, any pitfalls CTV intends to evade can be evaded by using a CHECKSIG,\nif NOINPUT is deployed in some form, adding new flexibility.  Beyond what's\npossible with NOINPUT/ANYPREVOUT, CTV additionally commits to: \n\n\u00bb\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b71. Number of inputs\n\u00bb\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b72. Number of outputs\n\u00bb\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b73. Index of input\n\nThe justification given for committing to the number of inputs and outputs is\nthat \"it makes CTV hashes easier to compute with script\", however doing so would\nrequire OP_CAT. It's noted that both of these are actually redundant\ncommitments. Since the constexpr requirement was removed, if OP_CAT were\nenabled, this commitment to the input index could be evaded by computing the CTV\nhash within the script, modifying the input index using data taken from the\nwitness. Therefore committing to the input index is a sender-specified-policy\nchoice, and not an anti-footgun measure for the redeemer. As such, it's\nappropriate to consider committing to the input index using a flag instead.\n\nI believe the above may be possible *without* a new opcode and simply with a\nsighash flag. That is, consider a flag SIGHASH_NOSIG which behaves as follows:\nThe stack is expected to contain <hash> <flags>, where the hash to be checked is\n<hash> and is in the place where you'd normally put a pubkey. The byte <flags>\nis the second thing on the stack. This is intended to be an empty \"signature\"\nwith the flags byte appended (which must contain SIGHASH_NOSIG to succeed).\n\nThere are probably reasons this might not work as a flag that I haven't\ndiscovered yet. Alternatively CTV might be considered to be an alternative type\nof CHECKSIG operator and might be renamed to CHECKSIGHASH, appending flag bytes\nto the hash to be checked.\n\nThe flags discussed above, NOINPUT, NOSIG, INPUTINDEX are all really\nsender-policy choices, while SIGHASH flags are redeemer-choice as they usually\noccur in the witness. There's really no way currently for an output to specify\nthat the redeemer must use a particular set of flags. One way to achieve this is\nto put the CHECKSIG(HASH) including its flags into the redeemScript -- which is\nfunctionally what CTV does (or a CHECKSIG in a redeemScript using NOINPUT).\nThis is committed to in outputs and therefore specifies sender policies, however\nthe redeemScript is specified by the receiver.  Perhaps an anti-footgun measure\nwould be to require that certain SIGHASH flags like these MUST be committed to\nin the output, by the sender.\n\nCSV (CHECKSEQUENCEVERIFY) is an example that redemption policies are committed\nto in the output by the sender via the sequence_no field, and then checked with\nan opcode OP_CSV to enable relative timelocks. It's probably possible to add new\nflags to the sequence_no field, and check the new semantics with CSV instead of\nan entiely new opcode.\n\nAs user policy choices, NOINPUT might be considered \"MAY\" conditions. A user MAY\nuse NOINPUT on an output, but let's not require it.  Covenants on the other\nhand, are a MUST condition. The CTV proposal imposes \"MUST\" conditions on the\nexistence of the covenant itself, as well as the number of inputs and outputs,\nto prevent txid malleability so that such transactions can be used in offline\nprotocols. Txid non-malleability can be achieved by enforcing that the output\nmust be spent using SIGHASH_ALL instead of committing to the number of inputs\nseparately with a new opcode. The MUST condition also helps with sighash caching\nfor batch validation.\n\nINPUTINDEX is required in a CTV/CHECKSIGHASH world because of the half-spend\nproblem. Normally outputs are spent uniquely as long as different addresses are\nused on the outputs. A transaction with the same address appearing twice would\nalso have a half-spend problem. Anyone signing the first output and giving that\nPSBT to another person can allow them to spend the second input. Therefore one\nmight even want INPUTINDEX for non-covenant transactions, though making a tx\nwith the same address twice seems like a silly idea to me.\n\nTherefore, assuming a CSV-type mechanism can be devised using sequence_no, CTV\nis equivalent to a flag in sequence_no that is logically\nMUST|ALL|NOSIG|INPUTINDEX and a redeemScript of <hash> <flags>.\n\nLightning-like use cases might put sequence_no flags that are logically\nMAY|ALL|NOINPUT.\n\nThe other mechanism for sender policy is scriptPubKey, which is heavily\nrestricted by isStandard, but might be another place to specify flags like the\nabove.\n\nThoughts? \n\nDoes this idea address any of the NOINPUT footguns? (which I'm not up on) \nIs there a reason this cannot be done with sequence_no and OP_CSV?\nIs there a reason that a separate opcode (CTV) is different/better than this\napproach?\n\n--\nCheers, Bob McElrath\n\n\"For every complex problem, there is a solution that is simple, neat, and wrong.\"\n    -- H. L. Mencken"
            },
            {
                "author": "Jeremy",
                "date": "2020-02-03T08:20:52",
                "message_text_only": "I think these ideas shows healthy review of how OP_CTV is specified against\nalternatives, but I think most of the ideas presented are ill advised.\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n\n\nOn Sat, Feb 1, 2020 at 2:15 PM Bob McElrath via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> We propose that OP_CHECKTEMPLATEVERIFY should behave more like CHECKSIG,\n> including a flags byte that specify what is hashed. This unifies the ways a\n> SigHash is computed, differing only in the final checksig which is omitted\n> in\n> favor of chacking the hash directly. Having two paths to create a\n> signature hash\n> introduces extra complexity, especially as concerns potential future\n> SIGHASH\n> flag upgrades.\n\n<snip>\n\n> I believe the above may be possible *without* a new opcode and simply with\n> a\n> sighash flag. That is, consider a flag SIGHASH_NOSIG which behaves as\n> follows:\n> The stack is expected to contain <hash> <flags>, where the hash to be\n> checked is\n> <hash> and is in the place where you'd normally put a pubkey. The byte\n> <flags>\n> is the second thing on the stack. This is intended to be an empty\n> \"signature\"\n> with the flags byte appended (which must contain SIGHASH_NOSIG to succeed).\n>\n\n\nI've previously brought this up in IRC\nhttp://gnusha.org/bitcoin-wizards/2019-11-28.log\n\n\nAFAIK, using an actual CheckSig SIGHASH Flag as is is a bad idea because\nthen you need to include an encoding valid signature and pubkey that map\nonto the hash to check. This is not just extra 11 extra bytes of data (33\nbytes PubKey + 9 bytes Signature + 2 push -32 bytes - 1 byte push), it's\nalso a very awkward API. I don't think you can soft-fork around these\nencoding rules. But you're right that it's possible to add this as a\nSIGHASH flag. I don't think doing CTV as a sighash flag is worth\nconsidering further.\n\nI get your point that CTV is kind of a signature hash, and that we might\nwant to not have a separate path. This ignores, however, that the current\nSIGHASH code-path is kind of garbage and literally no one likes it and it\nhas been the source of nasty issues previously. Thus I posit that a\nseparate path creates less complexity, as we don't need to worry about\naccidentally introducing a weird interaction with other sighash flags.\n\n\n\n\n> CTV omits inputs as part of its semantics, so CTV-type functionality using\n> CHECKSIG is also achievable if some form of NOINPUT flag is also deployed.\n> With\n> NOINPUT alone, a standard CHECKSIG can be used to implement a covenant --\n> though\n> it uses an unnecessarily large number of bytes just to check a 32-byte\n> hash.\n> Therefore, any pitfalls CTV intends to evade can be evaded by using a\n> CHECKSIG,\n> if NOINPUT is deployed in some form, adding new flexibility.  Beyond what's\n> possible with NOINPUT/ANYPREVOUT, CTV additionally commits to:\n>\n> \u00bb\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b71. Number of inputs\n> \u00bb\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b72. Number of outputs\n> \u00bb\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b73. Index of input\n>\n\nNOINPUT as specified here\nhttps://github.com/ajtowns/bips/blob/bip-anyprevout/bip-anyprevout.mediawiki\n(is this the latest?) isn't a great surrogate for CTV because CTV commits\nto the input index which prevents half-spend. This also encumbers, as\nproposed, an additional chaperone signature to fix it to a specific output.\n\nThis adds a lot of complexity and space to using CTV. Maybe NOINPUT could\nmake changes to special-case CTV, but then we're back to CTV again.\n\n\n\n>\n> The justification given for committing to the number of inputs and outputs\n> is\n> that \"it makes CTV hashes easier to compute with script\", however doing so\n> would\n> require OP_CAT. It's noted that both of these are actually redundant\n> commitments. Since the constexpr requirement was removed, if OP_CAT were\n> enabled, this commitment to the input index could be evaded by computing\n> the CTV\n> hash within the script, modifying the input index using data taken from the\n> witness. Therefore committing to the input index is a\n> sender-specified-policy\n> choice, and not an anti-footgun measure for the redeemer. As such, it's\n> appropriate to consider committing to the input index using a flag instead.\n>\n> This is incorrect almost entirely.\n\n1. There is a semantic difference between the *commitment* being strictly\nredundant, which has more to do with malleation, and being redundant from a\nfeature perspective. I could maybe do a better job here of expanding what\n\"easier\" means here -- there are actually some scripts which are quite\ndifficult to write/impossible without this. I've described this a couple\nplaces outside of the BIP, but essentially it allows you to pin the number\nof inputs/outputs separately from the hashes themselves. So if you're\ntrying to build the template in script, you might want to allow the\nSequences to be set to any value, and pass them via a hash. But then from a\nhash you can't check the validity of the length. An external length\ncommitment lets you do this, but without it you would have to pass in the\nsequences directly.\n2. The constexpr requirement was implemented in a soft-fork re-moveable\nmanner specifically so that if we wanted OP_CAT, we could add it without\nalso introducing constructing CTVs on the stack. Similarly, it would be\npossible to specify with OP_CAT as a soft-fork removeable rule that if\nOP_CAT executes before an OP_CTV, the script is invalid. The constexpr rule\nwas removed on the sentiment that if we introduce OP_CAT, we almost surely\nintend to introduce it for OP_CTV (or related) use cases.\n3. Committing to the input-index is not a *sender* policy choice. It's a\nreceiver policy choice. I tell a Payer my invoice/address, and they emit a\ntransaction matching it. From an address containing a CTV, I as the\nreceiver set the input_index. I don't see how this is related to the\nanti-footgun-ness\n4. You write as if OP_CTV + OP_CAT allows the input index to stripped\n*unconditionally*. That's wrong. It's an opt in if you write a script\ntaking it as a parameter. You can't evade it in general use.\n5. The \"anti-footgun\" measure is that it precludes reused-keys being spent\nin the same transaction. Were you to opt out of the mechanism (via OP_CAT\ninput_index), then you opt out of the reuse protection. (This only matters\nif there is more than one input to begin with).\n6. Committing to it via a flag is strictly less flexible, because I can do\na lot more with OP_CAT than with a flag. For instance, I can do\n<input_index> <min> <max> OP_WITHIN OP_VERIFY to ensure that it falls\nwithin a certain range of inputs.\n7. A flag is also an extra byte somewhere or uses a sighash bit.\n8. Enabling a flag right away enables a big footgun right off the bat. I\nthink it's bad for use safety.\n9. Rather than add flags, if you wanted to add this, I would suggest\nreserving max input_index to specify a don't care value. Then you always\ncheck a given CTV against the don't care value and the specified value.\nHashing the don't care value can be done in the PreComputedTxData. But I\ndon't think it's worth special casing or making available today because of\n8.\n\n\n\n> There are probably reasons this might not work as a flag that I haven't\n> discovered yet. Alternatively CTV might be considered to be an alternative\n> type\n> of CHECKSIG operator and might be renamed to CHECKSIGHASH, appending flag\n> bytes\n> to the hash to be checked.\n>\n\nSure -- happy to go down the renaming path again. Keep in mind that CTV\ncurrently only applies rules when the argument is 32-bytes. Future\nsoft-forkers are welcome to define a rule for a 33byte 1st argument that\ntreats it as a pubkey and has CHECKSIG semantics, and looks for another\nargument.\n\n\n\n>\n> The flags discussed above, NOINPUT, NOSIG, INPUTINDEX are all really\n> sender-policy choices, while SIGHASH flags are redeemer-choice as they\n> usually\n> occur in the witness. There's really no way currently for an output to\n> specify\n> that the redeemer must use a particular set of flags. One way to achieve\n> this is\n> to put the CHECKSIG(HASH) including its flags into the redeemScript --\n> which is\n> functionally what CTV does (or a CHECKSIG in a redeemScript using NOINPUT).\n> This is committed to in outputs and therefore specifies sender policies,\n> however\n> the redeemScript is specified by the receiver.  Perhaps an anti-footgun\n> measure\n> would be to require that certain SIGHASH flags like these MUST be\n> committed to\n> in the output, by the sender.\n>\n>\nI think this \"sender/redeemer\" framework is a bit bunk. Ultimately all\nredeemers are senders, and you aren't forcing a choice on someone. You\ncould be on to something though, but I think in general Bitcoin has gone\nthe way of opaque addresses so that people can't encumber arbitrary\npolicies on your coins. Maybe it swings the other way...\n\n\n\n> CSV (CHECKSEQUENCEVERIFY) is an example that redemption policies are\n> committed\n> to in the output by the sender via the sequence_no field, and then checked\n> with\n> an opcode OP_CSV to enable relative timelocks. It's probably possible to\n> add new\n> flags to the sequence_no field, and check the new semantics with CSV\n> instead of\n> an entiely new opcode.\n>\n\n\nThe sender commits to them, but legally, if you add a contract that I\ndidn't agree to as receipt (e.g., in segwit address -- which the script is\nhashed) I won't even know I got paid. So the way Bitcoin works today, these\nare receiver set policies.\n\nOne way to think of CTV is it's precise the opcode that lets you \"wrap\"\nsomeone's known address in arbitrary new scripts. E.g., if you gave me an\naddress X, but I need to (for whatever reason) add an additional 1 month\nCSV.\n\nSo i just get the txn:\n\nA:\n    sequence 1mo\n    1 input\n    1 output: pay X 1 coin\n\nthen take the STH(A), and create B\n\n\nB:\n    ... inputs\n    1 output pay `STH(A) CTV` 1 coin\n\nI can also add other things, like secondary signers/alternatives\n\n`IF {some checksig program} STH(A) CTV ELSE {multisig program} ENDIF\n\n\n\n> As user policy choices, NOINPUT might be considered \"MAY\" conditions. A\n> user MAY\n> use NOINPUT on an output, but let's not require it.  Covenants on the other\n> hand, are a MUST condition. The CTV proposal imposes \"MUST\" conditions on\n> the\n> existence of the covenant itself, as well as the number of inputs and\n> outputs,\n> to prevent txid malleability so that such transactions can be used in\n> offline\n> protocols. Txid non-malleability can be achieved by enforcing that the\n> output\n> must be spent using SIGHASH_ALL instead of committing to the number of\n> inputs\n> separately with a new opcode. The MUST condition also helps with sighash\n> caching\n> for batch validation.\n>\n\nANYPREVOUT/ANYSCRIPT are actually weirder than that, because once it has\nbeen used that key is permanently \"burned\". Hence ANYPREVOUT has such\npubkeys be explicity tagged as ANYPREVOUT compatible. So a user kind of has\nto pre-decide if they want to have ANYPREVOUT semantics or not.\nAnd in this case, key-reuse is relatively unsafe (as you need to track what\nelse you've signed) so I think what you're suggesting is not robust.\n\nThese \"MUST\" conditions sound nice, but they don't actually help with\nvalidity caching because we want to be able to compute this information\nbefore we've fetched the outputs from the database so we can't know what to\ncache yet. Contextless things are things you can precompute not knowing\ninput scripts.\n\nAgain, I don't think this sender/redeemer framework is super useful but I\nadmire the attempt.\n\n\n\n>\n> INPUTINDEX is required in a CTV/CHECKSIGHASH world because of the\n> half-spend\n> problem. Normally outputs are spent uniquely as long as different\n> addresses are\n> used on the outputs. A transaction with the same address appearing twice\n> would\n> also have a half-spend problem. Anyone signing the first output and giving\n> that\n> PSBT to another person can allow them to spend the second input. Therefore\n> one\n> might even want INPUTINDEX for non-covenant transactions, though making a\n> tx\n> with the same address twice seems like a silly idea to me.\n>\n\nI'm confused. Transactions don't have addresses. What are you talking about?\n\nInput indexes accomplish two goals.\n\nOne, they eliminate third-party malleability of input order (which would\nmuck with sighash singles too).\nTwo, they prevent half-spend.\n\nSignatures today commit to this in the signature hash (the field is nIn,\nwhich is confusing because nIn might also look like vin.size()).\n\nSo the half spend problem doesn't exist today...\n\n>\n> Therefore, assuming a CSV-type mechanism can be devised using sequence_no,\n> CTV\n> is equivalent to a flag in sequence_no that is logically\n> MUST|ALL|NOSIG|INPUTINDEX and a redeemScript of <hash> <flags>.\n>\n> Lightning-like use cases might put sequence_no flags that are logically\n> MAY|ALL|NOINPUT.\n>\n> The other mechanism for sender policy is scriptPubKey, which is heavily\n> restricted by isStandard, but might be another place to specify flags like\n> the\n> above.\n>\n> Thoughts?\n>\n>\n\nSo OP_CAT already lets you do this kind of stuff with the SIGHASHes rather\nthan a new special-purpose verifier. Just pass the signature separately\nfrom the flags, and cat them togehther for the checker but just look at the\nflags for your new thing. Then check that the flags are exactly what you\nwanted. If you don't want OP_CAT, you can also add OP_SUBSTRVERIFY wherein\nyou verify that a provided substr was inside another string. Then you pass\nin the witness the full string you want, as well as sub-bits you want to\ncheck properties on (like the flags).\n\nIt's not clear to me that we want this kind of stuff though. OP_CAT\nrequires very careful review because it has very surprising functional\nconsequences across the board.\n\n\n\n> Does this idea address any of the NOINPUT footguns? (which I'm not up on)\n> Is there a reason this cannot be done with sequence_no and OP_CSV?\n>\n\nANYPREVOUT already precludes these by using a separate key type and\nchaperone signatures.  I think a flag for MUST NOT ANYPREVOUT would maybe\nhelp with making it safer. But this is a complete sidebar from CTV. This\nexists already by generating a non-anyprevout capable key though...\n\n\n\n> Is there a reason that a separate opcode (CTV) is different/better than\n> this\n> approach?\n>\n>\nI'll let the email above serve as the answer to your question.\n\nI don't think there's anything gained by expressing CTV as a sighash type\ntoday, especially since a future soft fork (when we've taking the time to\ndeeply rethink sighash flags, like bitmask sighash flags proposed for\nelements) can make CTV (as specified today) a valid hash in this new\nlanguage and use the OP_NOP4 with a non 32-byte argument as the new\nCheckSig operator anyways.\n\n\n\n\n\n\n\nBut now I'll pose a different question: why shouldn't we compute the\nsighash entirely as a type of Bitcoin Script? SIGHASH_FLAGS are essentially\na tiny, crappy, language for putting together a message digest. You can\nthink of SIGHASH_FLAGS as being like optimized \"jets\" for known programs.\nFor custom programs, you can construct the digest pattern you want in\nscript. This is essentially what the bitmask sighash flags proposal is. I\nthink you're going to waste a lot of mental-cycles trying to cram in all\nthis logic into flags. As this stuff gets more complicated, you should just\nwrite an actual language for dealing with sighashes and go from there.\n\nNow why don't we want this sighash language? Quadratic hashing. If every\noutput commits to some different complex thing, we end up doing a lot of\nrehashing. Flags are actually kind of bad because a few different flags can\ntrigger a lot of rehashing. But the way flags are *today* is relatively OK\nbecause we can cache the important parts so validation is cheap.\n\nThe more complicated you plan gets, the less context free validation we can\ndo.\n\nCTV is fully compatible with context free validation optimizations,\ntrivially. It's not clear if your other stuff is, I suspect not.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200203/6001e9ed/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "CTV through SIGHASH flags",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Bob McElrath",
                "Jeremy"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 21560
        }
    },
    {
        "title": "[bitcoin-dev] Characterizing orphan transaction in the Bitcoin network",
        "thread_messages": [
            {
                "author": "Matt Corallo",
                "date": "2020-02-03T03:41:21",
                "message_text_only": "The orphan pool has nontrivial denial of service properties around transaction validation. In general, I think the goal has been to reduce/remove it, not the other way around. In any case, this is likely the wrong forum for software-level discussion of Bitcoin Core. For that, you probably want to open an issue on github.com/bitcoin/bitcoin.\n\nMatt\n\n> On Feb 1, 2020, at 14:12, Anas via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> \ufeff\n> Hi all,\n> \n> This paper - https://arxiv.org/pdf/1912.11541.pdf - characterizes orphan transactions in the Bitcoin network and shows that increasing the size of the orphan pool reduces network overhead with almost no additional performance overhead. What are your thoughts?\n> \n> Abstract: \n>> Orphan transactions are those whose parental income-sources are missing at the time that they are processed. These transactions are not propagated to other nodes until all of their missing parents are received, and they thus end up languishing in a local buffer until evicted or their parents are found. Although there has been little work in the literature on characterizing the nature and impact of such orphans, it is intuitive that they may affect throughput on the Bitcoin network. This work thus seeks to methodically research such effects through a measurement campaign of orphan transactions on live Bitcoin nodes. Our data show that, surprisingly, orphan transactions tend to have fewer parents on average than non-orphan transactions. Moreover, the salient features of their missing parents are a lower fee and larger size than their non-orphan counterparts, resulting in a lower transaction fee per byte. Finally, we note that the network overhead incurred by these orphan transactions can be significant, exceeding 17% when using the default orphan memory pool size (100 transactions). However, this overhead can be made negligible, without significant computational or memory demands, if the pool size is merely increased to 1000 transactions.\n> \n> Regards,\n> Anas\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200202/96d7bc24/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Characterizing orphan transaction in the Bitcoin network",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Matt Corallo"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2395
        }
    },
    {
        "title": "[bitcoin-dev] Purge attacks (spin on sabotage attacks)",
        "thread_messages": [
            {
                "author": "Mike Kelly",
                "date": "2020-02-07T13:55:29",
                "message_text_only": "Since I raised this with Hasu in early Jan[0], I've been looking for ways\nto eliminate transaction replacement that are consensus compatible (since\nfirst safe seen is not). The best I could come up with is \"Uncontested\nSafe\", which I've tried to sketch out in a brief medium article[1].\n\nAm I retracing steps? Feedback would be appreciated.\n\n[0] https://twitter.com/mikekelly85/status/1217590668735983622\n[1] https://medium.com/@mikekelly85/uncontested-safe-protocol-e5af8c145f1\n\nCheers,\nM\n\nOn Sat, Feb 1, 2020 at 10:12 PM ha su via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi all,\n>\n> I think I discovered an interesting form of sabotage attack (possible for\n> miners) that tries to create coordination disincentives among Bitcoin users\n> - named after the dystopian movie The Purge, where all crime is legal for\n> one night every year.\n>\n> TLDR\n> * An attacker replaces the most recent blocks full of transactions with\n> empty blocks.\n> * Previously confirmed txns return into the mempool, where anyone with a\n> minimum of technical knowledge or access to public tools can\n> opportunistically double-spend their txns back to themselves. (the process\n> is the same as double-spending regular zero-conf txns)\n>\n> The attack seems useful to undermine trust in Bitcoin's assurances, e.g.\n> the future finality of transactions. It differs from other forms of\n> sabotage (e.g. DoS by mining only empty blocks) in that it specifically\n> disrupts the coordination process among users in response to the attack.\n>\n> By giving some users a chance to benefit from the attack, the attacker\n> gives them a vested interest in staying on the attack chain. If enough\n> users accept the invitation to double-spend, it might become harder to come\n> to consensus on how to deal with the attack.\n>\n> Purge attacks probably don\u2019t constitute a bigger risk than other known\n> forms of sabotage attacks, but seem like an interesting spin where the\n> attacker specifically targets the pre-coordination of defenders.\n>\n> You can find the full report, incl. some mitigations against sabotage\n> attacks, at\n> https://blog.deribit.com/insights/destabilizing-bitcoin-consensus-with-purge-attacks/\n>\n> Your feedback is highly appreciated.\n>\n> Regards,\n> Hasu\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n\n\n-- \nMike\n\nhttp://twitter.com/mikekelly85\nhttp://linkedin.com/in/mikekelly123\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200207/a4b88ed5/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2020-02-08T02:15:32",
                "message_text_only": "Good morning M,\n\nWhat do you mean by this?\n\n> Nodes reject announced blocks that:\n>\n> * include transactions that are in contest with any in their mempool\n> * include transactions that are in contest with any in the contest pool\n\nIs this intended to be a consensus rule, i.e. nodes will never accept such a block?\n\nBecause if so, this fails the principle of Blockchain Self-Containment, i.e. consensus rules can only check what is in the blockchain.\nThe mempool (and contest pool) is not in the blockchain as it is never attested to in the blockchain.\n\nIf this is not a consensus rule (i.e.e nodes can be convinced to accept an announced block that violates the above via some rule, such as sufficient confirmations) then this does not protect against purge attacks.\n\n--\n\nPurge attacks can still be defended against and does not require mass cooperation.\nIf there is a transaction that is economically beneficial to me, it does so by paying some Bitcoins to me.\nIf it pays Bitcoins to me, I can spend those Bitcoins in a transaction that just offers to pay mining fees and transfers it back to me (i.e. child pays for parent) to convince miners to mine the purged transaction.\nAs the Purge attack is \"just\" a censorship attack (i.e. a censorship of all transactions in the block under attack), the increased mining fees for the transactions being censored (i.e. offered via child-pays-for-parent in this case) is an economic counterattack on the censoring miner (i.e. it forgoes the mining fees).\n\nWith enough self-interested users, the fee offered to confirm the transactions can be substantial enough that non-censoring miners can be convinced to mine those transactions.\nNo coordination necessary, as is typical for all defenses against censorship (and the basis of the censorship-resistance of Bitcoin).\n\nRegards,\nZmnSCPxj\n\n\n> Since I raised this with Hasu in early Jan[0], I've been looking for ways to eliminate transaction replacement that are consensus compatible (since first safe seen is not). The best I could come up with is \"Uncontested Safe\", which I've tried to sketch out in a brief medium article[1].\n>\n> Am I retracing steps? Feedback would be appreciated.\n>\n> [0]\u00a0https://twitter.com/mikekelly85/status/1217590668735983622\n> [1]\u00a0https://medium.com/@mikekelly85/uncontested-safe-protocol-e5af8c145f1\n>\n> Cheers,\n> M\n>\n> On Sat, Feb 1, 2020 at 10:12 PM ha su via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> > Hi all,\n> >\n> > I think I discovered an interesting form of sabotage attack (possible for miners) that tries to create coordination disincentives among Bitcoin users - named after the dystopian movie The Purge, where all crime is legal for one night every year.\n> >\n> > TLDR\n> > * An attacker replaces the most recent blocks full of transactions with empty blocks.\n> > * Previously confirmed txns return into the mempool, where anyone with a minimum of technical knowledge or access to public tools can opportunistically double-spend their txns back to themselves. (the process is the same as double-spending regular zero-conf txns)\n> >\n> > The attack seems useful to undermine trust in Bitcoin's assurances, e.g. the future finality of transactions. It differs from other forms of sabotage (e.g. DoS by mining only empty blocks) in that it specifically disrupts the coordination\u00a0process among users in response to the attack.\u00a0\n> >\n> > By giving some users a chance to benefit from the attack, the attacker gives them a vested interest in staying on the attack chain. If enough users accept the invitation to double-spend, it might become harder to come to consensus on how to deal with the attack.\n> >\n> > Purge attacks probably don\u2019t constitute a bigger risk than other known forms of sabotage attacks, but seem like an interesting spin where the attacker specifically targets the pre-coordination of defenders.\n> >\n> > You can find the full report, incl. some mitigations against sabotage attacks, at\u00a0https://blog.deribit.com/insights/destabilizing-bitcoin-consensus-with-purge-attacks/\n> >\n> > Your feedback is highly appreciated.\n> >\n> > Regards,\n> > Hasu\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n> --\n> Mike\n>\n> http://twitter.com/mikekelly85\n> http://linkedin.com/in/mikekelly123"
            },
            {
                "author": "Mike Kelly",
                "date": "2020-02-08T08:11:17",
                "message_text_only": "Hi ZmnSCPxj, thanks for your reply. Comments in line.\n\nOn Sat, Feb 8, 2020 at 02:15, ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning M,\n>\n> What do you mean by this?\n>\n> > Nodes reject announced blocks that:\n> >\n> > * include transactions that are in contest with any in their mempool\n> > * include transactions that are in contest with any in the contest pool\n>\n> Is this intended to be a consensus rule, i.e. nodes will never accept such\n> a block?\n>\n> Because if so, this fails the principle of Blockchain Self-Containment,\n> i.e. consensus rules can only check what is in the blockchain.\n> The mempool (and contest pool) is not in the blockchain as it is never\n> attested to in the blockchain.\n\n\nYes, it intentionally violates that rule. It\u2019s unclear to me right now what\nthe consequence/cost of doing so in this specific way would be. Are you\nable to explain?\n\n\n>\n> Purge attacks can still be defended against and does not require mass\n> cooperation.\n> If there is a transaction that is economically beneficial to me, it does\n> so by paying some Bitcoins to me.\n> If it pays Bitcoins to me, I can spend those Bitcoins in a transaction\n> that just offers to pay mining fees and transfers it back to me (i.e. child\n> pays for parent) to convince miners to mine the purged transaction.\n> As the Purge attack is \"just\" a censorship attack (i.e. a censorship of\n> all transactions in the block under attack), the increased mining fees for\n> the transactions being censored (i.e. offered via child-pays-for-parent in\n> this case) is an economic counterattack on the censoring miner (i.e. it\n> forgoes the mining fees).\n\n\n>\n> With enough self-interested users, the fee offered to confirm the\n> transactions can be substantial enough that non-censoring miners can be\n> convinced to mine those transactions.\n> No coordination necessary, as is typical for all defenses against\n> censorship (and the basis of the censorship-resistance of Bitcoin).\n\n\nThe attack itself is better classified as a form of sabotage than\ncensorship. The goal is to demonstrate the ongoing mutability of\ntransactions beyond any inherent heuristic for \u201cfinality\u201d. iow it is a\ndemonstration that will damage the network\u2019s future ability to offer\nsettlement assurances.\n\nTrying to use Child Pays For Parent to defend in a bidding war against an\nopportunist attacker retrieving spent Bitcoin via RBF is a losing game for\nthe defender. There\u2019s no opportunity cost for the attacker, any amount\nretrieved is profit. The defender, on the other hand, is always losing\nvalue. This is exactly the kind of conflict and discoordination the attack\nis intended to induce.\n\nCheers,\nM\n\n\n>\n> Regards,\n> ZmnSCPxj\n>\n>\n> > Since I raised this with Hasu in early Jan[0], I've been looking for\n> ways to eliminate transaction replacement that are consensus compatible\n> (since first safe seen is not). The best I could come up with is\n> \"Uncontested Safe\", which I've tried to sketch out in a brief medium\n> article[1].\n> >\n> > Am I retracing steps? Feedback would be appreciated.\n> >\n> > [0] https://twitter.com/mikekelly85/status/1217590668735983622\n> > [1]\n> https://medium.com/@mikekelly85/uncontested-safe-protocol-e5af8c145f1\n> >\n> > Cheers,\n> > M\n> >\n> > On Sat, Feb 1, 2020 at 10:12 PM ha su via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >\n> > > Hi all,\n> > >\n> > > I think I discovered an interesting form of sabotage attack (possible\n> for miners) that tries to create coordination disincentives among Bitcoin\n> users - named after the dystopian movie The Purge, where all crime is legal\n> for one night every year.\n> > >\n> > > TLDR\n> > > * An attacker replaces the most recent blocks full of transactions\n> with empty blocks.\n> > > * Previously confirmed txns return into the mempool, where anyone with\n> a minimum of technical knowledge or access to public tools can\n> opportunistically double-spend their txns back to themselves. (the process\n> is the same as double-spending regular zero-conf txns)\n> > >\n> > > The attack seems useful to undermine trust in Bitcoin's assurances,\n> e.g. the future finality of transactions. It differs from other forms of\n> sabotage (e.g. DoS by mining only empty blocks) in that it specifically\n> disrupts the coordination process among users in response to the attack.\n> > >\n> > > By giving some users a chance to benefit from the attack, the attacker\n> gives them a vested interest in staying on the attack chain. If enough\n> users accept the invitation to double-spend, it might become harder to come\n> to consensus on how to deal with the attack.\n> > >\n> > > Purge attacks probably don\u2019t constitute a bigger risk than other known\n> forms of sabotage attacks, but seem like an interesting spin where the\n> attacker specifically targets the pre-coordination of defenders.\n> > >\n> > > You can find the full report, incl. some mitigations against sabotage\n> attacks, at\n> https://blog.deribit.com/insights/destabilizing-bitcoin-consensus-with-purge-attacks/\n> > >\n> > > Your feedback is highly appreciated.\n> > >\n> > > Regards,\n> > > Hasu\n> > > _______________________________________________\n> > > bitcoin-dev mailing list\n> > > bitcoin-dev at lists.linuxfoundation.org\n> > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> >\n> > --\n> > Mike\n> >\n> > http://twitter.com/mikekelly85\n> > http://linkedin.com/in/mikekelly123\n>\n-- \nMike\n\nhttp://twitter.com/mikekelly85\nhttp://linkedin.com/in/mikekelly123\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200208/249328fc/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2020-02-09T00:00:41",
                "message_text_only": "Good morning M,\n\n> > > Nodes reject announced blocks that:\n> > >\n> > > * include transactions that are in contest with any in their mempool\n> > > * include transactions that are in contest with any in the contest pool\n> >\n> > Is this intended to be a consensus rule, i.e. nodes will never accept such a block?\n> >\n> > Because if so, this fails the principle of Blockchain Self-Containment, i.e. consensus rules can only check what is in the blockchain.\n> > The mempool (and contest pool) is not in the blockchain as it is never attested to in the blockchain.\n>\n> Yes, it intentionally violates that rule. It\u2019s unclear to me right now what the consequence/cost of doing so in this specific way would be. Are you able to explain?\n\nViolation of this principle can cause persistent chainsplits where you induce one set of nodes to see one view of reality while another set of nodes see another view.\nFor instance, suppose two innocent miners happen to find blocks at nearly the same time.\nUnfortunately for them, one miner happened to be using \"SPV\" mining i.e. mining empty blocks.\n\n>From the point of view of arbitrary nodes, this is indistinguishable from a one-block purge attack as described.\nYet this happenstance occurrence now causes a chainsplit, as some number of nodes (those near to the SPV-mining miner) think that miner is innocent of wrongdoing and will support the \"purged\" chainsplit, whereas those near the other miner will consider that block bad and will support the other \"unpurged\" chainsplit.\nThis is an even worse consequence than any purge attack, and could happen completely by chance with no malice involved.\n\nAlways avoid violating that principle in any consensus code.\nIf it is not committed to in the block and is not provable using only data you provide with the block, you cannot use it safely without risking chainsplit.\n\n(and no, banning or even disincentivizing SPV mining will not work, different nodes have different views of the mempool and temporary chainsplits can occur by chance where one chainsplit has transactions that are not confirmed in the other chainsplit, which again is just another short-term inadvertent Purge attack on the network.)\n\n\n>\n> > Purge attacks can still be defended against and does not require mass cooperation.\n> > If there is a transaction that is economically beneficial to me, it does so by paying some Bitcoins to me.\n> > If it pays Bitcoins to me, I can spend those Bitcoins in a transaction that just offers to pay mining fees and transfers it back to me (i.e. child pays for parent) to convince miners to mine the purged transaction.\n> > As the Purge attack is \"just\" a censorship attack (i.e. a censorship of all transactions in the block under attack), the increased mining fees for the transactions being censored (i.e. offered via child-pays-for-parent in this case) is an economic counterattack on the censoring miner (i.e. it forgoes the mining fees).\n>\n> > With enough self-interested users, the fee offered to confirm the transactions can be substantial enough that non-censoring miners can be convinced to mine those transactions.\n> > No coordination necessary, as is typical for all defenses against censorship (and the basis of the censorship-resistance of Bitcoin).\n>\n> The attack itself is better classified as a form of sabotage than censorship. The goal is to demonstrate the ongoing mutability of transactions beyond any inherent heuristic for \u201cfinality\u201d. iow it is a demonstration that will damage the network\u2019s future ability to offer settlement assurances.\n>\n> Trying to use Child Pays For Parent to defend in a bidding war against an opportunist attacker retrieving spent Bitcoin via RBF is a losing game for the defender. There\u2019s no opportunity cost for the attacker, any amount retrieved is profit. The defender, on the other hand, is always losing value. This is exactly the kind of conflict and discoordination the attack is intended to induce.\n\nYour defender, in this attack, should avoid the Sunk Cost Fallacy here.\nIf the defender has been so foolish as to provide a product or service based on only a *few* confirmations, like 1 or 2, then that product or service has been Sunk, and it should ignore the Sunk Cost here.\n\n>From that point of view, the attacker and the defender are simply bidding up from the *same* value, i.e. the value of the UTXO that is being removed by the purge attack.\nAs the same value is under contest on both sides, they are equally matched and both censoring and non-censoring miners will get the same incentive, splitting up the network into two nearly equal halves, and then chance (lucky block discovery) decides between which is the winner or the loser.\n\nThe difference here is that the chainsplit in this case is in a metastable state, and once a string of lucky block discoveries occurs, it falls into a stable state and now everybody agrees again on who won and who lost.\nYour solution risks *persistent* *stable* chainsplits.\nWorse, this occurrence without your solution would only happen if some miners actually attack the blockchain.\nWith your solution, persistent chainsplits can occur without malice, simply chance.\n\nAnd as in many things in life, the only winning move is not to play.\nJust wait for more than a small number of confirmations (e.g. 6 is generally considered safe), and the chance that a Purge attack on your transactions succeeds is low enough that worse force majeur (a rogue asteroid hitting your datacenter, for example) is more likely.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Mike Kelly",
                "date": "2020-02-09T10:15:18",
                "message_text_only": "Hi ZmnSCPxj,\n\nOn Sun, Feb 9, 2020 at 12:00 AM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning M,\n>\n> > > > Nodes reject announced blocks that:\n> > > >\n> > > > * include transactions that are in contest with any in their mempool\n> > > > * include transactions that are in contest with any in the contest\n> pool\n> > >\n> > > Is this intended to be a consensus rule, i.e. nodes will never accept\n> such a block?\n> > >\n> > > Because if so, this fails the principle of Blockchain\n> Self-Containment, i.e. consensus rules can only check what is in the\n> blockchain.\n> > > The mempool (and contest pool) is not in the blockchain as it is never\n> attested to in the blockchain.\n> >\n> > Yes, it intentionally violates that rule. It\u2019s unclear to me right now\n> what the consequence/cost of doing so in this specific way would be. Are\n> you able to explain?\n>\n> Violation of this principle can cause persistent chainsplits where you\n> induce one set of nodes to see one view of reality while another set of\n> nodes see another view.\n> For instance, suppose two innocent miners happen to find blocks at nearly\n> the same time.\n> Unfortunately for them, one miner happened to be using \"SPV\" mining i.e.\n> mining empty blocks.\n>\n> From the point of view of arbitrary nodes, this is indistinguishable from\n> a one-block purge attack as described.\n> Yet this happenstance occurrence now causes a chainsplit, as some number\n> of nodes (those near to the SPV-mining miner) think that miner is innocent\n> of wrongdoing and will support the \"purged\" chainsplit, whereas those near\n> the other miner will consider that block bad and will support the other\n> \"unpurged\" chainsplit.\n> This is an even worse consequence than any purge attack, and could happen\n> completely by chance with no malice involved.\n>\n>\nI don't see how the scenario you outline here has anything to do with the\nmechanism I proposed. An empty block doesn't contain any transactions (by\ndefinition) so it wont contest any transactions in any given node's\nmempool. The aim isn't to prevent empty nodes, it's to discourage miners\nfrom including transactions in their block that conflict with the\neventually-consistent state of consensus in the mempool.\n\n\n> Always avoid violating that principle in any consensus code.\n> If it is not committed to in the block and is not provable using only data\n> you provide with the block, you cannot use it safely without risking\n> chainsplit.\n>\n> (and no, banning or even disincentivizing SPV mining will not work,\n> different nodes have different views of the mempool and temporary\n> chainsplits can occur by chance where one chainsplit has transactions that\n> are not confirmed in the other chainsplit, which again is just another\n> short-term inadvertent Purge attack on the network.)\n>\n>\n> >\n> > > Purge attacks can still be defended against and does not require mass\n> cooperation.\n> > > If there is a transaction that is economically beneficial to me, it\n> does so by paying some Bitcoins to me.\n> > > If it pays Bitcoins to me, I can spend those Bitcoins in a transaction\n> that just offers to pay mining fees and transfers it back to me (i.e. child\n> pays for parent) to convince miners to mine the purged transaction.\n> > > As the Purge attack is \"just\" a censorship attack (i.e. a censorship\n> of all transactions in the block under attack), the increased mining fees\n> for the transactions being censored (i.e. offered via child-pays-for-parent\n> in this case) is an economic counterattack on the censoring miner (i.e. it\n> forgoes the mining fees).\n> >\n> > > With enough self-interested users, the fee offered to confirm the\n> transactions can be substantial enough that non-censoring miners can be\n> convinced to mine those transactions.\n> > > No coordination necessary, as is typical for all defenses against\n> censorship (and the basis of the censorship-resistance of Bitcoin).\n> >\n> > The attack itself is better classified as a form of sabotage than\n> censorship. The goal is to demonstrate the ongoing mutability of\n> transactions beyond any inherent heuristic for \u201cfinality\u201d. iow it is a\n> demonstration that will damage the network\u2019s future ability to offer\n> settlement assurances.\n> >\n> > Trying to use Child Pays For Parent to defend in a bidding war against\n> an opportunist attacker retrieving spent Bitcoin via RBF is a losing game\n> for the defender. There\u2019s no opportunity cost for the attacker, any amount\n> retrieved is profit. The defender, on the other hand, is always losing\n> value. This is exactly the kind of conflict and discoordination the attack\n> is intended to induce.\n>\n> Your defender, in this attack, should avoid the Sunk Cost Fallacy here.\n> If the defender has been so foolish as to provide a product or service\n> based on only a *few* confirmations, like 1 or 2, then that product or\n> service has been Sunk, and it should ignore the Sunk Cost here.\n>\n> From that point of view, the attacker and the defender are simply bidding\n> up from the *same* value, i.e. the value of the UTXO that is being removed\n> by the purge attack.\n> As the same value is under contest on both sides, they are equally matched\n> and both censoring and non-censoring miners will get the same incentive,\n> splitting up the network into two nearly equal halves, and then chance\n> (lucky block discovery) decides between which is the winner or the loser.\n>\n> The difference here is that the chainsplit in this case is in a metastable\n> state, and once a string of lucky block discoveries occurs, it falls into a\n> stable state and now everybody agrees again on who won and who lost.\n> Your solution risks *persistent* *stable* chainsplits.\n> Worse, this occurrence without your solution would only happen if some\n> miners actually attack the blockchain.\n> With your solution, persistent chainsplits can occur without malice,\n> simply chance.\n>\n\nHow would this mechanism produce a chainsplit by chance?\n\n\n>\n> And as in many things in life, the only winning move is not to play.\n> Just wait for more than a small number of confirmations (e.g. 6 is\n> generally considered safe), and the chance that a Purge attack on your\n> transactions succeeds is low enough that worse force majeur (a rogue\n> asteroid hitting your datacenter, for example) is more likely.\n>\n>\nI got to thinking about \"purge attacks\" and mitigations because I was red\nteaming how G20 states that have seized the major mining operations could\nmost effectively destroy value and confidence in Bitcoin. This scenario is\n_a lot_ more likely than rogue asteroids.\n\nWhat happens if the G20 decide to reorg deeper 6 - say 10, or even 20?\n\nIf the Bitcoin continues to offer replace by fee I think this will be their\nfirst attack with seized majority hashrate;\n\n- mine offline\n- reach > 10 deep empty block reorg as heaviest chain\n- announce it\n- semi-honest mine with a preference for RBF'ed \"root\" txns, ignoring any\nprofitable child pays for parent.\n- repeat above, until some goal reached (eg. $ value of Bitcoin reaching x)\n- switch to \"DoS mode\" where you empty block reorg the chain tip\n\nIf we got rid of RBF, their only option would be DoS mode. Once it stops,\nhonest mining could resume and the blocks will fill back up again with\ntransactions out of the mempool preserved in the right order.#\n\nHope that makes sense.\n\nBest,\nMike\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200209/7995d198/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2020-02-09T23:59:56",
                "message_text_only": "Good morning M,\n\n\n> I don't see how the scenario you outline here has anything to do with the mechanism I proposed. An empty block doesn't contain any transactions (by definition) so it wont contest any transactions in any given node's mempool. The aim isn't to prevent empty nodes, it's to discourage miners from including transactions in their block that conflict with the eventually-consistent state of consensus in the mempool.\n> \u00a0\n\nWhat?\n\n>From the original post:\n\n> TLDR\n> * An attacker replaces the most recent blocks full of transactions with empty blocks.\n\nAre you sure you are solving the same problem?\n\nThe mempool **has no consensus**.\nIt is strictly an optimization, preventing a node from needlessly broadcasting transactions.\n\nMaking consensus dependent on the state of the mempool requires that you record the state of the mempool at the point at which the block snapshot was taken.\nOtherwise, newly-started nodes can be fooled into taking the \"wrong\" consensus branch leading to persistent chainsplits.\n\n>\n> > Always avoid violating that principle in any consensus code.\n> > If it is not committed to in the block and is not provable using only data you provide with the block, you cannot use it safely without risking chainsplit.\n> >\n> > (and no, banning or even disincentivizing SPV mining will not work, different nodes have different views of the mempool and temporary chainsplits can occur by chance where one chainsplit has transactions that are not confirmed in the other chainsplit, which again is just another short-term inadvertent Purge attack on the network.)\n> >\n> > >\n> > > > Purge attacks can still be defended against and does not require mass cooperation.\n> > > > If there is a transaction that is economically beneficial to me, it does so by paying some Bitcoins to me.\n> > > > If it pays Bitcoins to me, I can spend those Bitcoins in a transaction that just offers to pay mining fees and transfers it back to me (i.e. child pays for parent) to convince miners to mine the purged transaction.\n> > > > As the Purge attack is \"just\" a censorship attack (i.e. a censorship of all transactions in the block under attack), the increased mining fees for the transactions being censored (i.e. offered via child-pays-for-parent in this case) is an economic counterattack on the censoring miner (i.e. it forgoes the mining fees).\n> > >\n> > > > With enough self-interested users, the fee offered to confirm the transactions can be substantial enough that non-censoring miners can be convinced to mine those transactions.\n> > > > No coordination necessary, as is typical for all defenses against censorship (and the basis of the censorship-resistance of Bitcoin).\n> > >\n> > > The attack itself is better classified as a form of sabotage than censorship. The goal is to demonstrate the ongoing mutability of transactions beyond any inherent heuristic for \u201cfinality\u201d. iow it is a demonstration that will damage the network\u2019s future ability to offer settlement assurances.\n> > >\n> > > Trying to use Child Pays For Parent to defend in a bidding war against an opportunist attacker retrieving spent Bitcoin via RBF is a losing game for the defender. There\u2019s no opportunity cost for the attacker, any amount retrieved is profit. The defender, on the other hand, is always losing value. This is exactly the kind of conflict and discoordination the attack is intended to induce.\n> >\n> > Your defender, in this attack, should avoid the Sunk Cost Fallacy here.\n> > If the defender has been so foolish as to provide a product or service based on only a *few* confirmations, like 1 or 2, then that product or service has been Sunk, and it should ignore the Sunk Cost here.\n> >\n> > From that point of view, the attacker and the defender are simply bidding up from the *same* value, i.e. the value of the UTXO that is being removed by the purge attack.\n> > As the same value is under contest on both sides, they are equally matched and both censoring and non-censoring miners will get the same incentive, splitting up the network into two nearly equal halves, and then chance (lucky block discovery) decides between which is the winner or the loser.\n> >\n> > The difference here is that the chainsplit in this case is in a metastable state, and once a string of lucky block discoveries occurs, it falls into a stable state and now everybody agrees again on who won and who lost.\n> > Your solution risks *persistent* *stable* chainsplits.\n> > Worse, this occurrence without your solution would only happen if some miners actually attack the blockchain.\n> > With your solution, persistent chainsplits can occur without malice, simply chance.\n>\n> How would this mechanism produce a chainsplit by chance?\n\nI already described it in the previous post.\n\nPurge attacks happen all the time, when two miners mine blocks at nearly the same time, but with different sets of transactions in their blocks.\nAnd as I pointed out, any mechanism which uses non-block data (such as mempool data) *will* lead to persistent chainsplits.\n\n> \u00a0\n>\n> > And as in many things in life, the only winning move is not to play.\n> > Just wait for more than a small number of confirmations (e.g. 6 is generally considered safe), and the chance that a Purge attack on your transactions succeeds is low enough that worse force majeur (a rogue asteroid hitting your datacenter, for example) is more likely.\n>\n> I got to thinking about \"purge attacks\" and mitigations because I was red teaming how G20 states that have seized the major mining operations could most effectively destroy value and confidence in Bitcoin. This scenario is _a lot_ more likely than\u00a0rogue asteroids.\n>\n> What happens if the G20 decide to reorg deeper 6 - say 10, or even 20?\n>\n> If the Bitcoin continues to offer replace by fee I think this will be their first attack with seized majority hashrate;\n>\n> - mine offline\n> - reach > 10 deep empty block reorg as heaviest chain\u00a0\n> - announce it\n> - semi-honest mine with a preference for RBF'ed \"root\" txns, ignoring any profitable child pays for parent.\n> - repeat above, until some goal reached (eg. $ value of Bitcoin reaching x)\n> - switch to \"DoS mode\" where you empty block reorg the chain tip\n>\n> If we got rid of RBF, their only option would be DoS mode. Once it stops, honest mining could resume and the blocks will fill back up again with transactions out of the mempool preserved in the right order.#\n\nYou ***cannot*** get rid of RBF.\nThe incentives of miners mean they will actually want to implement RBF and ignore any \"convention\" of RBF-flagging.\nMy understanding is that there are claims that a minority of miners already do this (possibly Peter Todd has more information, but I am uncertain), and will accept \"full\" RBF i.e. ignore the RBF flag and always apply RBF to all transactions regardless.\nNothing in consensus prevents this, and this is why we always wait for confirmation.\n\n\nRegardless of however many blocks are attacked, always remember that in the end, this is still a *censorship* attack: it is attempting to censor Bitcoin completely.\nAs such, this page applies: https://github.com/libbitcoin/libbitcoin-system/wiki/Censorship-Resistance-Property\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Mike Kelly",
                "date": "2020-02-10T15:28:20",
                "message_text_only": "Hi ZmnSCPxj, thanks for sticking with me on this.\n\nOn Mon, Feb 10, 2020 at 12:00 AM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning M,\n>\n>\n> > I don't see how the scenario you outline here has anything to do with\n> the mechanism I proposed. An empty block doesn't contain any transactions\n> (by definition) so it wont contest any transactions in any given node's\n> mempool. The aim isn't to prevent empty nodes, it's to discourage miners\n> from including transactions in their block that conflict with the\n> eventually-consistent state of consensus in the mempool.\n> >\n>\n> What?\n>\n> From the original post:\n>\n> > TLDR\n> > * An attacker replaces the most recent blocks full of transactions with\n> empty blocks.\n>\n> Are you sure you are solving the same problem?\n>\n\nYes.\n\nThere is no way to prevent someone with the majority of hash rate doing\nempty block reorgs. This is not new and it's not the problem/point of a\npurge attack. The point of a purge attack is that _under the conditions of\nan empty block reorg_ if the network affords transaction replacement (ie.\nRBF) then users with who instructed transactions which are now un-confirmed\nand back in the mempool have the opportunity to double spend them by\nreplacing the transaction that was considered finalised. We can prevent\nthis scenario by eliminating transaction replacement in the network.\n\n\n>\n> The mempool **has no consensus**.\n> It is strictly an optimization, preventing a node from needlessly\n> broadcasting transactions.\n>\n> Making consensus dependent on the state of the mempool requires that you\n> record the state of the mempool at the point at which the block snapshot\n> was taken.\n> Otherwise, newly-started nodes can be fooled into taking the \"wrong\"\n> consensus branch leading to persistent chainsplits.\n>\n\nNo need to record the state of the mempool. Newly-started nodes should\nselect the heaviest chain as per usual.\n\n\n>\n> >\n> > > Always avoid violating that principle in any consensus code.\n> > > If it is not committed to in the block and is not provable using only\n> data you provide with the block, you cannot use it safely without risking\n> chainsplit.\n> > >\n> > > (and no, banning or even disincentivizing SPV mining will not work,\n> different nodes have different views of the mempool and temporary\n> chainsplits can occur by chance where one chainsplit has transactions that\n> are not confirmed in the other chainsplit, which again is just another\n> short-term inadvertent Purge attack on the network.)\n> > >\n> > > >\n> > > > > Purge attacks can still be defended against and does not require\n> mass cooperation.\n> > > > > If there is a transaction that is economically beneficial to me,\n> it does so by paying some Bitcoins to me.\n> > > > > If it pays Bitcoins to me, I can spend those Bitcoins in a\n> transaction that just offers to pay mining fees and transfers it back to me\n> (i.e. child pays for parent) to convince miners to mine the purged\n> transaction.\n> > > > > As the Purge attack is \"just\" a censorship attack (i.e. a\n> censorship of all transactions in the block under attack), the increased\n> mining fees for the transactions being censored (i.e. offered via\n> child-pays-for-parent in this case) is an economic counterattack on the\n> censoring miner (i.e. it forgoes the mining fees).\n> > > >\n> > > > > With enough self-interested users, the fee offered to confirm the\n> transactions can be substantial enough that non-censoring miners can be\n> convinced to mine those transactions.\n> > > > > No coordination necessary, as is typical for all defenses against\n> censorship (and the basis of the censorship-resistance of Bitcoin).\n> > > >\n> > > > The attack itself is better classified as a form of sabotage than\n> censorship. The goal is to demonstrate the ongoing mutability of\n> transactions beyond any inherent heuristic for \u201cfinality\u201d. iow it is a\n> demonstration that will damage the network\u2019s future ability to offer\n> settlement assurances.\n> > > >\n> > > > Trying to use Child Pays For Parent to defend in a bidding war\n> against an opportunist attacker retrieving spent Bitcoin via RBF is a\n> losing game for the defender. There\u2019s no opportunity cost for the attacker,\n> any amount retrieved is profit. The defender, on the other hand, is always\n> losing value. This is exactly the kind of conflict and discoordination the\n> attack is intended to induce.\n> > >\n> > > Your defender, in this attack, should avoid the Sunk Cost Fallacy here.\n> > > If the defender has been so foolish as to provide a product or service\n> based on only a *few* confirmations, like 1 or 2, then that product or\n> service has been Sunk, and it should ignore the Sunk Cost here.\n> > >\n> > > From that point of view, the attacker and the defender are simply\n> bidding up from the *same* value, i.e. the value of the UTXO that is being\n> removed by the purge attack.\n> > > As the same value is under contest on both sides, they are equally\n> matched and both censoring and non-censoring miners will get the same\n> incentive, splitting up the network into two nearly equal halves, and then\n> chance (lucky block discovery) decides between which is the winner or the\n> loser.\n> > >\n> > > The difference here is that the chainsplit in this case is in a\n> metastable state, and once a string of lucky block discoveries occurs, it\n> falls into a stable state and now everybody agrees again on who won and who\n> lost.\n> > > Your solution risks *persistent* *stable* chainsplits.\n> > > Worse, this occurrence without your solution would only happen if some\n> miners actually attack the blockchain.\n> > > With your solution, persistent chainsplits can occur without malice,\n> simply chance.\n> >\n> > How would this mechanism produce a chainsplit by chance?\n>\n> I already described it in the previous post.\n>\n> Purge attacks happen all the time, when two miners mine blocks at nearly\n> the same time, but with different sets of transactions in their blocks.\n> And as I pointed out, any mechanism which uses non-block data (such as\n> mempool data) *will* lead to persistent chainsplits.\n>\n> >\n> >\n> > > And as in many things in life, the only winning move is not to play.\n> > > Just wait for more than a small number of confirmations (e.g. 6 is\n> generally considered safe), and the chance that a Purge attack on your\n> transactions succeeds is low enough that worse force majeur (a rogue\n> asteroid hitting your datacenter, for example) is more likely.\n> >\n> > I got to thinking about \"purge attacks\" and mitigations because I was\n> red teaming how G20 states that have seized the major mining operations\n> could most effectively destroy value and confidence in Bitcoin. This\n> scenario is _a lot_ more likely than rogue asteroids.\n> >\n> > What happens if the G20 decide to reorg deeper 6 - say 10, or even 20?\n> >\n> > If the Bitcoin continues to offer replace by fee I think this will be\n> their first attack with seized majority hashrate;\n> >\n> > - mine offline\n> > - reach > 10 deep empty block reorg as heaviest chain\n> > - announce it\n> > - semi-honest mine with a preference for RBF'ed \"root\" txns, ignoring\n> any profitable child pays for parent.\n> > - repeat above, until some goal reached (eg. $ value of Bitcoin reaching\n> x)\n> > - switch to \"DoS mode\" where you empty block reorg the chain tip\n> >\n> > If we got rid of RBF, their only option would be DoS mode. Once it\n> stops, honest mining could resume and the blocks will fill back up again\n> with transactions out of the mempool preserved in the right order.#\n>\n> You ***cannot*** get rid of RBF.\n>\n\nWhat is the evidence for this claim? Is there a proof?\n\n\n> The incentives of miners mean they will actually want to implement RBF and\n> ignore any \"convention\" of RBF-flagging.\n>\n\nYes, under the current design. This is an attempt to change the incentives\nof the protocol so that this is not the case. To try and reduce the\nseverity of empty block reorg attacks.\n\n\n\n> My understanding is that there are claims that a minority of miners\n> already do this (possibly Peter Todd has more information, but I am\n> uncertain), and will accept \"full\" RBF i.e. ignore the RBF flag and always\n> apply RBF to all transactions regardless.\n> Nothing in consensus prevents this, and this is why we always wait for\n> confirmation.\n>\n>\nThe whole point of this sabotage attack is that it demonstrates that\nconfirmation is not a reasonable way of managing this risk. If the depth of\nthe empty block reorg was 20, even if everyone stuck to the arbitrary 6\nconfirmation rule, nearly every Bitcoin transaction from the 14 blocks\nbetween `chaintip-6` and `chaintip-20` is at risk of being double spent as\nit lands back in the mempool.\n\n\n>\n> Regardless of however many blocks are attacked, always remember that in\n> the end, this is still a *censorship* attack: it is attempting to censor\n> Bitcoin completely.\n> As such, this page applies:\n> https://github.com/libbitcoin/libbitcoin-system/wiki/Censorship-Resistance-Property\n\n\nCensorship of availability of the network? That's DoS ie. what a standard\nempty reorg attack.\n\nPurge attack is an extension of this that extends such an into the realm of\nsabotage, where the integrity of previously-adequately-confirmed\ntransactions is compromised by allowing users to double spend them.\n\nCheers,\nM\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200210/45a64d58/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Purge attacks (spin on sabotage attacks)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "Mike Kelly"
            ],
            "messages_count": 7,
            "total_messages_chars_count": 42373
        }
    },
    {
        "title": "[bitcoin-dev] Taproot (and graftroot) complexity",
        "thread_messages": [
            {
                "author": "Bryan Bishop",
                "date": "2020-02-09T20:19:55",
                "message_text_only": "The following is a message forwarded from an anonymous email that, for\nwhatever reason, couldn't be relayed through the mailing list without my\nassistance.\n\nThis email is the first of a collection of sentiments from a group of\ndevelopers\nwho in aggregate prefer to remain anonymous. These emails have been sent\nunder a\npseudonym so as to keep the focus of discussion on the merits of the\ntechnical\nissues, rather than miring the discussion in personal politics. Our goal\nisn't\nto cause a schism, but rather to help figure out what the path forward is\nwith\nTaproot. To that end, we:\n\n1) Discuss the merits of Taproot's design versus simpler alternatives (see\nthread subject, \"Taproot (and Graftroot) Complexity\").\n2) Propose an alternative path to deploying the technologies described in\nBIP-340, BIP-341, and BIP-342 (see thread subject, \"An Alternative\nDeployment\nPath for Taproot Technologies\").\n3) Suggest a modification to Taproot to reduce some of the overhead (see\nthread\nsubject, \"Taproot Public NUMS Optimization\").\n\nNow that the BIP has moved to draft we felt that now was the time to\nprioritize\nreview to make sure it was an acceptable change for our activities. As a\ngroup,\nwe're excited about the totality of what Taproot has to offer. However,\nafter\nour review, we're left perplexed about the development of Taproot (and\nGraftroot, to a lesser extent).\n\nWe also want to convey that we have nothing but respect for the developers\nand\ncommunity who have poured their heart and soul into preparing Taproot. Self\nevidently, it is an impressive synthesis of ideas. We believe that the\nhighest\nform of respect to pay such a synthesis of ideas is a detailed and critical\nreview, as it's pertinent to closely consider changes to Bitcoin.\n\n\nIn essence, Taproot is fundamentally the same as doing\nhttps://github.com/bitcoin/bips/blob/master/bip-0114.mediawiki and Schnorr\nsignatures separately.\n\nThe main reason for putting them together -- as mentioned in the BIP -- is a\ngain in efficiency. But this efficiency pre-supposes a specific use case and\nprobability distribution of use cases.\n\nCompare:\n\nSuppose a MAST for {a,b,c,d,e,f,g,h} spending conditions it looks something\nlike this:\n\n      /\\\n     /  \\\n    /    \\\n   /      \\\n  /\\      /\\\n /  \\    /  \\\n/\\  /\\  /\\  /\\\na b c d e f g h\n\nIf we want this to be functionally equivalent to Taproot, we add a new path:\n\n       /\\\n      /\\ {<pk> schnorr_checksig}\n     /  \\\n    /    \\\n   /      \\\n  /\\      /\\\n /  \\    /  \\\n/\\  /\\  /\\  /\\\na b c d e f g h\n\nNow, to spend from this MBV you have to reveal 32 bytes on the stack for\nthe not\ntaken branch, and 35 bytes for the <pk> schnorr_checksig (1 byte push, 33\nbytes\nPK, 1 byte checksig).\n\nThis is 67 bytes more than Taproot would require for the same spending\ncondition.\n\nHowever, suppose we wanted to use one of the script paths instead. We still\nneed\nto have one extra hash for the {<pk> schnorr_checksig} (depending on if we\nput\nthe key in this position or not--see below). But now we can spend with just\na\nlogarithmic control program path.\n\nHowever, if we do the same script via taproot, we now need to provide the\nbase\npublic key (33 bytes) as well as the root hash (32 bytes) and path and then\nthe\nactual scripts. With the need for 2 push bytes, this ends up being back at\n67\nbytes extra.\n\nIs Taproot just a probability assumption about the frequency and likelihood\nof\nthe signature case over the script case? Is this a good assumption?  The BIP\nonly goes as far as to claim that the advantage is apparent if the outputs\n*could be spent* as an N of N, but doesn't make representations about how\nlikely\nthat N of N case would be in practice compared to the script paths. Perhaps\namong use cases, more than half of the ones we expect people to be doing\ncould be\nspent as an N of N. But how frequently would that path get used? Further,\nwhile\nthe *use cases* might skew toward things with N of N opt-out, we might end\nup in\na power law case where it's the one case that doesn't use an N of N opt out\nat\nall (or at a de minimis level) that becomes very popular, thereby making\nTaproot\nmore costly then beneficial.\n\nFurther, if you don't want to use a Taproot top-level key (e.g., you need\nto be\nable to audit that no one can spend outside of one of the script\nconditions),\nthen you need to use a NUMS (nothing up my sleeve) point. This forces users\nwho\ndon't want Taproot to pay the expense, when if they just had a MAST based\nwitness type they would be cheaper. So if this use case is at all common,\nTaproot leaves them worse off in terms of fees. Given that script paths are\nusually done in the case where there is some contested close, it's actually\nin\nthe interest of protocol developers that the contested script path be as\nefficient as possible so that the fees paid maximally increase the feerate.\nWe\nthink this can be fixed simply in Taproot though, as noted below.\n\n\n\nOn privacy, we're also a bit confused as to the goal of Taproot over MAST\nand\nSchnorr. Earlier, we presented a design with MAST which is very close to\nTaproot.\nHowever, it'd also be possible to just add {<pk> schnorr_checksig} to the\nset\n{a,b,c,d,e,f,g,h}, shuffle them, and compute some MAST structure (perhaps\nprobability encoded) on them. This has the effect of not having much\nadditional\nfees for adding the extra Schnorr path at redeem time (only 1 extra branch\non\n2/8 script paths), e.g.\n\n      /\\\n     /  \\\n    /    \\\n   /      \\\n  /\\      /\\\n /  \\    /  \\\n/\\  /\\  /\\  /\\\na b c d e f/\\ {<pk> schnorr_checksig}\n          g  h\n\nWe could argue that this is more private than Taproot, because we don't\ndistinguish between the Schnorr key case and other cases by default, so\nchain\nanalyzers can't tell if the signature came from the Taproot case or from\none of\nthe Script paths. There's also no NUMS point required, which means chain\nanalyzers can't tell when you spend that there was no top level key if the\nNUMS\npoint is not per-output indistinguishable. By using a semi-randomized MAST\nstructure, chain analyzers also can't tell exactly how big your spend\ncondition\nMAST was. In particular, you care more about privacy when you are\ncontesting a\nclose of a channel or other script path because then the miners could be\nmore\nlikely to extract a rent from you as \"ransom\" for properly closing your\nchannel\n(or in other words, in a contested close the value of the closing\ntransaction is\nlarger than usual).\n\nIt would also be possible to do something really simple which is to allow\nthe\nwitness type to be either a MAST hash OR a schnorr key (but not a Taproot).\nThis\nallows you to not completely fracture the anonymity set between people who\nwant\nplain Schnorr and people who want MAST (at least until they go to spend).\nThis\nfix can also be used in Taproot in place of a NUMS point, to decrease extra\nfees. It's unclear if this plays negatively with any future batch validation\nmechanism though, but the contextual checks to exclude a witness program\nfrom\nthe batch are relatively simple. See thread subject, \"Taproot Public NUMS\nOptimization\".\n\nThe considerations around Graftroot, a proposed delegation mechanism, is a\nbit\nsimilar. Delegation is a mechanism by which a UTXO with script S can sign a\nscript R which can then be executed in addition to S without requiring a\ntransaction. This allows an output to monotonically and dynamically\nincrease the\nnumber of conditions under which it can be spent. As noted by Pieter Wiulle\nhere:\nhttps://github.com/kanzure/diyhpluswiki/commit/a03f6567d714f8733b578de263a4b149441cd058\ndelegation was originally possible in Bitcoin, but got broken during an\nemergency fork to split the scriptSig and scriptpubkey separation. Rather\nthan\nadding some fancy delegation mechanism in Bitcoin, why not just have a\nP2SH-like\nsemantic which allows a delegated script to be evaluated? See BIP-117\nhttps://github.com/bitcoin/bips/blob/master/bip-0117.mediawiki. This way we\naren't special casing where delegation can occur, and we can allow taproot\nnested spending conditions (i.e., with timelocks) to generate their own\ndelegations. As I've seen Graftroot discussed thus far, it is as a top-level\nwitness program version like Taproot and non-recursive. Similar to the above\ndiscussion, top-level is more efficient if you suspect that delegation will\nbe\nmost likely occurring at the top level, but it's not clear that's a good\nassumption as it may be common to want to allow different scripts to\ndelegate.\n\n\nOverall, we are left with concerns both about the merit of doing Taproot\nversus alternatives, as well as the process through which we got to be here.\n\n1) Is Taproot actually more private than bare MAST and Schnorr separately?\nWhat\nare the actual anonymity set benefits compared to doing the separately?\n2) Is Taproot actually cheaper than bare MAST and Schnorr separately? What\nevidence do we have that the assumption it will be more common to use\nTaproot\nwith a key will outweigh Script cases?\n3) Is Taproot riskier than bare MAST and Schnorr separately given the new\ncrypto? How well reviewed is the actual crypto parts? None of us personally\nfeel\ncomfortable reviewing the crypto in Schnorr -- what's the set of people who\nhave\nthoroughly reviewed the crypto and aren't just ACKing because they trust\nother\ndevelopers to have looked at it close enough?\n4) Design wise, couldn't we forego the NUMS point requirement and be able to\ncheck if it's a hash root directly? This would encumber users who don't\nneed the\nkey path a cheaper spend path. See thread subject, \"Taproot Public NUMS\nOptimization\".\n5) Is the development model of trying to jam a bunch of features into\nBitcoin\nall at once good for Bitcoin development? Would we be better off if we\nembraced\nincremental improvements that can work together (e.g., MAST and then\nSchnorr)?\nAlthough the BIP raises some points about anonymity sets being why to do\nthem\nall at once, it's not clear to me this argument holds water (same goes for\nbusinesses not upgrading). If we can take things as smaller steps, we are\nnot\nonly more secure, but we also have more time to dedicate review to each\nchange\nindependently. We also end up co-mingling changes that people end up\naccepting\nonly because they want one and they're bundled (e.g., MAST and Schnorr, MAST\nseems like a much less risky addition versus Schnorr). See thread subject,\n\"An\nAlternative Deployment Path for Taproot Technologies\".\n\n\n\n\nOur provocation with this email is primarily that we think we should more\ncarefully consider the benefits of Taproot over simpler primitives that are\nnot\nonly easier to review, but could have been made available much sooner rather\nthan waiting on putting everything all together for an unclear aggregate\nbenefit.\n\nWe do think that most of the developers have been honest about the benefits\nof\nTaproot, but that on closer look we feel the general ecosystem has oversold\nTaproot as being the key enabler for a collection of techniques that we\ncould do\nwith much simpler building blocks.\n\n\nAt the end of the day, we do not strongly advocate not deploying Taproot at\nthis\npoint in the review cycle. We think the Taproot Public NUMS Optimization\nmay be\na good idea, worth considering if it's not insecure, as it cuts through the\ncase\nwhere you would otherwise need a NUMS point. Things like TapScript and its\nMAST\nmechanisms are well designed and offer exciting new deployment paths, and\nwould\nbe something we would use even if we opted for MAST instead of Taproot.\nHowever,\nwe also believe it is our duty to raise these concerns and suggestions, and\nwe\nlook forward to listening to the responses of the community.\n\nGreat thanks,\n\nThe Group\n\nSUBJECT: An Alternative Deployment Path for Taproot Technologies\n\nThis email is the second of a collection of sentiments from a group of\ndevelopers\nwho in aggregate prefer to remain anonymous. These emails have been sent\nunder a\npseudonym so as to keep the focus of discussion on the merits of the\ntechnical\nissues, rather than miring the discussion in personal politics. Our goal\nisn't\nto cause a schism, but rather to help figure out what the path forward is\nwith\nTaproot. To that end, we:\n\n1) Discuss the merits of Taproot's design versus simpler alternatives (see\nthread subject, \"Taproot (and Graftroot) Complexity\").\n2) Propose an alternative path to deploying the technologies described in\nBIP-340, BIP-341, and BIP-342 (see thread subject, \"An Alternative\nDeployment\nPath for Taproot Technologies\").\n3) Suggest a modification to Taproot to reduce some of the overhead (see\nthread\nsubject, \"Taproot Public NUMS Optimization\").\n\nAs a follow up to our prior message, we propose a different path forward\nfor the\nTaproot family of changes:\n\n1) A separate soft-fork for Merkle Branch Witnesses based on Taproot;\n2) A separate soft-fork for Schnorr Signatures\n3) A separate follow up soft-fork which enables Taproot and Graftroot\n\nWe think that the first 2 forks can be offered at the same time or one at a\ntime.\n\nTaproot, as a follow up to changes 1 and 2, can be enabled as a soft-fork\non the\nexisting semantics, but requiring a new witness version. With the Public\nNUMS Optimization, wallets could upgrade by just changing one version byte\nto be\nin the same anonymity set as Taproot.\n\nIt's not clear to us that the time to prepare a BIP and implementation for\n1 and\n2 at this point would be any less than the time to do Taproot as currently\nproposed. However, we believe that such a deployment plan is a reasonable\noption\nas it is more conservative, as Merkle Branch witnesses are relatively\nsimple and\nusers only have to use Schnorr signing if they want to, and can otherwise\ncontinue to use ECDSA. A further benefit of waiting on 3 is that we get to\ncollect real world protocol engineering experience to see how frequently the\nTaproot frequency of use assumption holds, and if it is worth doing or not.\n\n\nGreat thanks,\n\nThe Group\n\n-- \n- Bryan\nhttp://heybryan.org/\n1 512 203 0507\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200209/94a71b6f/attachment-0001.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2020-02-09T20:40:27",
                "message_text_only": "Responding purely to one point as this may be sufficient to clear up\nlots of discussion:\n\nOn 2/9/20 8:19 PM, Bryan Bishop via bitcoin-dev wrote:\n> Is Taproot just a probability assumption about the frequency and\n> likelihood of\n> the signature case over the script case? Is this a good assumption?\u00a0 The BIP\n> only goes as far as to claim that the advantage is apparent if the outputs\n> *could be spent* as an N of N, but doesn't make representations about\n> how likely\n> that N of N case would be in practice compared to the script paths. Perhaps\n> among use cases, more than half of the ones we expect people to be doing\n> could be\n> spent as an N of N. But how frequently would that path get used?\n> Further, while\n> the *use cases* might skew toward things with N of N opt-out, we might\n> end up in\n> a power law case where it's the one case that doesn't use an N of N opt\n> out at\n> all (or at a de minimis level) that becomes very popular, thereby making\n> Taproot\n> more costly then beneficial.\nIts not just about the frequency and likelihood, no. If there is a\nclearly-provided optimization for this common case in the protocol, then\nit becomes further more likely that developers put in the additional\neffort required to make this possibility a reality. This has a very\nsignificant positive impact on user privacy, especially those who wish\nto utilize more advanced functionality in Bitcoin. Further, yes, it is\nanticipated that the N of N case is possible to take in the vast\nmajority of deployed use-cases for advanced scripting systems, ensuring\nthat it is maximally efficient to do so (and thereby encouraging\ndevelopers to do so) is a key goal in this work.\n\nMatt"
            },
            {
                "author": "Antoine Riard",
                "date": "2020-02-09T22:32:41",
                "message_text_only": "> In particular, you care more about privacy when you are contesting a\n> close of a channel or other script path because then the miners could be\nmore\n> likely to extract a rent from you as \"ransom\" for properly closing your\nchannel\n> (or in other words, in a contested close the value of the closing\ntransaction is\n> larger than usual).\n\nNot sure this point holds, independently of which Taproot/MASTmechanism\ndeployed,\nany time-sensitive transaction will likely leak its \"contestness\" by the\nsetting of its\nnSequence/nLocktime fields. E.g, for LN, justice tx are not encumbered by a\nCSV\ndelay which distinguish them from a non-revoked spend. And when you're\nrelaying\nhtlcs and need to close unilaterally channel to prevent different\nsettlement on\nincoming/outgoing links the HTLC-timeout tx broadcast have a nLocktime set.\n\nBeyond LN, timelocks are a privacy leak and miner-withholding vector for any\noffchain protocols but this problem is not tied to Taproot design.\nConfidential\nenforcement of them would be great but that's another debate..\n\nAntoine\n\n\n\n\n\n\n\n\nLe dim. 9 f\u00e9vr. 2020 \u00e0 15:40, Matt Corallo via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Responding purely to one point as this may be sufficient to clear up\n> lots of discussion:\n>\n> On 2/9/20 8:19 PM, Bryan Bishop via bitcoin-dev wrote:\n> > Is Taproot just a probability assumption about the frequency and\n> > likelihood of\n> > the signature case over the script case? Is this a good assumption?  The\n> BIP\n> > only goes as far as to claim that the advantage is apparent if the\n> outputs\n> > *could be spent* as an N of N, but doesn't make representations about\n> > how likely\n> > that N of N case would be in practice compared to the script paths.\n> Perhaps\n> > among use cases, more than half of the ones we expect people to be doing\n> > could be\n> > spent as an N of N. But how frequently would that path get used?\n> > Further, while\n> > the *use cases* might skew toward things with N of N opt-out, we might\n> > end up in\n> > a power law case where it's the one case that doesn't use an N of N opt\n> > out at\n> > all (or at a de minimis level) that becomes very popular, thereby making\n> > Taproot\n> > more costly then beneficial.\n> Its not just about the frequency and likelihood, no. If there is a\n> clearly-provided optimization for this common case in the protocol, then\n> it becomes further more likely that developers put in the additional\n> effort required to make this possibility a reality. This has a very\n> significant positive impact on user privacy, especially those who wish\n> to utilize more advanced functionality in Bitcoin. Further, yes, it is\n> anticipated that the N of N case is possible to take in the vast\n> majority of deployed use-cases for advanced scripting systems, ensuring\n> that it is maximally efficient to do so (and thereby encouraging\n> developers to do so) is a key goal in this work.\n>\n> Matt\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200209/9ce15c2f/attachment.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2020-02-10T00:20:11",
                "message_text_only": "On Sun, Feb 09, 2020 at 02:19:55PM -0600, Bryan Bishop via bitcoin-dev wrote:\n> However, after\n> our review, we're left perplexed about the development of Taproot (and\n> Graftroot, to a lesser extent).\n\nI think the main cause of the perplexity is not seeing the benefit of\ntaproot. \n\nFor me, the simplest benefit is that taproot lets everyone's wallet change\nfrom \"if you lose this key, your funds are gone\" to \"if you lose this key,\nyou'll have to recover 3 of your 5 backup keys that you sent to trusted\nfriends, and pay a little more, but you won't have lost your funds\". That\nwon't cost you *anything* beyond upgrading your wallet sotware/hardware;\nif you never lose your main key, it doesn't cost you any more, but if\nyou do, you now have a new recovery option (or many recovery options).\n\nNote that doing graftroot isn't proposed as it requires non-interactive\nhalf-signature aggregation to be comparably efficient, and the crypto\nhasn't been worked out for that -- or at least, the maths hasn't been\nproperly written up for criticism. (If you don't care about efficiency,\nyou can do a poor man's graftroot with pre-signed transactions and CPFP)\n\nMore detailed responses below. Kinda long.\n\n> In essence, Taproot is fundamentally the same as doing\n> https://github.com/bitcoin/bips/blob/master/bip-0114.mediawiki\u00a0and Schnorr\n> signatures separately.\n> \n> Suppose a MAST for {a,b,c,d,e,f,g,h} spending conditions it looks something\n> like this:\n> \n> \u00a0 \u00a0 \u00a0 /\\\n> \u00a0 \u00a0 \u00a0/\u00a0 \\\n> \u00a0 \u00a0 /\u00a0 \u00a0 \\\n> \u00a0 \u00a0/\u00a0 \u00a0 \u00a0 \\\n> \u00a0 /\\\u00a0 \u00a0 \u00a0 /\\\n> \u00a0/\u00a0 \\\u00a0 \u00a0 /\u00a0 \\\n> /\\\u00a0 /\\\u00a0 /\\\u00a0 /\\\n> a b c d e f g h\n> \n> If we want this to be functionally equivalent to Taproot, we add a new path:\n> \n> \u00a0 \u00a0 \u00a0 \u00a0/\\\n> \u00a0 \u00a0 \u00a0 /\\ {<pk> schnorr_checksig}\n> \u00a0 \u00a0 \u00a0/\u00a0 \\\n> \u00a0 \u00a0 /\u00a0 \u00a0 \\\n> \u00a0 \u00a0/\u00a0 \u00a0 \u00a0 \\\n> \u00a0 /\\\u00a0 \u00a0 \u00a0 /\\\n> \u00a0/\u00a0 \\\u00a0 \u00a0 /\u00a0 \\\n> /\\\u00a0 /\\\u00a0 /\\\u00a0 /\\\n> a b c d e f g h\n\nThere's a bit more subtlety to the difference between a merkle branch\nand a taproot alternative. In particular, imagine you've got three\nalternatives, one of which has 60% odds of being taken, and the other\ntwo have 20% odds each. You'd construct a merkle tree:\n\n    /\\\n   a /\\\n    b  c\n\nAnd would reveal:\n\n  60%: a [#(b,c)]\n  20%: b [#a, #c]\n  20%: c [#a, #b]\n\nSo your overhead would be 32B 60% of the time and 64B 40% of the time,\nor an expected overhead of 44.8 bytes.\n\nWith taproot, you construct a tree of much the same shape, but 60% of\nthe time you no longer have to reveal anything about the path not taken:\n\n  60%: a-tweaked\n  20%: b [a, #c]\n  20%: c [a, #b]\n\nSo your overhead is 0B 60% of the time, and 65B 40% of the time, for an\nexpected overhead of 26B.\n\nThat math only works out as an improvement if your common case really\nis (or can be made to be) a simple key path spend, though.\n\nYou can generalise taproot and combine it with a merkle tree arbitrarily,\nwith the end result being that using a merkle branch means you can\nchoose either the left or right sub-tree for a cost of 32B, while a\ntaproot branch lets you choose the left *leaf* for free, or a right\nsub-tree for (essentially) 64B. So for equally likely branches you'd\nwant to use the merkle split, while if there's some particular outcome\nthat's overwhelmingly likely, with others just there for emergencies,\nthen a taproot-style alternative will be better. See:\n\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-July/016249.html\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-October/016461.html\n\nfor slightly more detailed background.\n\nUltimately, I think we can do this better, so that you could choose\nwhether to make the free \"taproot\" path be a key or a script, or to use\nthe taproot method to make other likely leaves cheaper than unlikely\nones, rather than just having that benefit available for the most likely\nleaf.\n\nBut I also think that's a lot of work, much of which will overlap with\nthe work to get cross-input signature aggregation working, so fwiw,\nmy view that the current taproot feature set is a good midway point to\ndraw a line, and get stuff out and released. This overall approach was\ndiscussed quite a while ago:\n\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-May/015951.html\n\n> However, if we do the same script via taproot, we now need to provide the base\n> public key (33 bytes) as well as the root hash (32 bytes) and path and then the\n> actual scripts. \n\nYou need to provide the internal public key, the actual script and the\npath back; the root hash is easily calculated from the script and the\npath, and then verified by ECC math against the scriptPubKey and the\ninternal public key.\n\n> \u00a0 \u00a0 \u00a0 /\\\n> \u00a0 \u00a0 \u00a0/\u00a0 \\\n> \u00a0 \u00a0 /\u00a0 \u00a0 \\\n> \u00a0 \u00a0/\u00a0 \u00a0 \u00a0 \\\n> \u00a0 /\\\u00a0 \u00a0 \u00a0 /\\\n> \u00a0/\u00a0 \\\u00a0 \u00a0 /\u00a0 \\\n> /\\\u00a0 /\\\u00a0 /\\\u00a0 /\\\n> a b c d e f/\\ {<pk> schnorr_checksig}\n> \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 g\u00a0 h\n>\n> We could argue that this is more private than Taproot, because we don't\n> distinguish between the Schnorr key case and other cases by default, so chain\n> analyzers can't tell if the signature came from the Taproot case or from one of\n> the Script paths.\n\nIn that example there is no taproot case -- you reveal the existance of\nother paths no matter which leaf you make use of. In particular, the \"pk\nschnorr_checksig\" alternative now has 96B of additional overhead (#gh,\n#ef, #abcd).\n\n> This\n> allows you to not completely fracture the anonymity set between people who want\n> plain Schnorr and people who want MAST \n> (at least until they go to spend).\n\nThe benefit of taproot is that often you can preserve the anonymity set\neven after you spend.\n\n> Overall, we are left with concerns both about the merit of doing Taproot\n> versus alternatives, as well as the process through which we got to be here.\n> 1) Is Taproot actually more private than bare MAST and Schnorr separately? What\n> are the actual anonymity set benefits compared to doing the separately?\n\nYes, presuming single-pubkey-single-signature remains a common\nauthorisation pattern.\n\n> 2) Is Taproot actually cheaper than bare MAST and Schnorr separately? What\n> evidence do we have that the assumption it will be more common to use Taproot\n> with a key will outweigh Script cases?\n\nTaproot with a key is about as cheap as it gets -- you've got a 35 byte\nscriptPubKey and 66 bytes of witness data.\n\nIt's then 33 bytes of witness data more expensive to use a script, which\npresumably will make it more likely that people use the simple key path.\n\nAt the time you create a utxo, provided you don't reuse keys, all taproot\nspends are indistinguishable. At the time you spend a taproot utxo,\nyou can distinguish:\n\n - spent via key path\n - spent via script path, internal key not known\n - spent via script path, internal key known NUMS point\n\nbut there's no fee rate advantage between reusing a NUMS point and\ngenerating a fresh NUMS point (via NUMS + rand*G), so the third case is\navoidable.\n\nLooking at blocks 616650 to 616700, I see outputs of:\n\n     738  0.3% \"pubkey\"\n    2091  0.8% \"witness_v0_scripthash\"\n   42749 16.8% \"witness_v0_keyhash\"\n  102962 40.4% \"pubkeyhash\"\n  106441 41.7% \"scripthash\"\n\nSo for plain segwit, over 95% of outputs are plain key; and overall,\nover 57.5% of outputs are plain key/signature -- that's not counting\nhowever many p2sh-encoded p2wpkh there are, because they'll just look\nlike pubkeyhash until they're spent.\n\n> 3) Is Taproot riskier than bare MAST and Schnorr separately given the new\n> crypto? \n\nI don't think so; most of the risk for either of those is in getting\nthe details right.\n\n> How well reviewed is the actual crypto parts? \n\nThat's pretty hard to evaluate if you can't review the crypto parts\nyourself, but some resources are:\n\nhttps://github.com/bitcoin-core/secp256k1/pull/558\nhttps://github.com/apoelstra/taproot\nhttps://github.com/ajtowns/taproot-review\n\nMost of the complicated crypto parts are at the application layer: muSig,\nthreshold signatures, adaptor signatures, scriptless scripts, etc.\n\n> None of us personally feel\n> comfortable reviewing the crypto in Schnorr -- what's the set of people who\n> have\n> thoroughly reviewed the crypto and aren't just ACKing because they trust other\n> developers to have looked at it close enough?\n\nThat... sounds like it's asking for a group of other developers that\nhave looked at it close enough for you to trust?\n\n> 4) Design wise, couldn't we forego the NUMS point requirement and be able to\n> check if it's a hash root directly?\n\nThat would decrease the anonymity set by a lot, make the code a bit\nmore complicated, and only end up saving 8 vbytes.\n\n> 5) Is the development model of trying to jam a bunch of features into Bitcoin\n> all at once good for Bitcoin development? Would we be better off if we embraced\n> incremental improvements that can work together (e.g., MAST and then Schnorr)?\n\nIMO, the driving force for bundling these changes together is the\nadvantages of taproot -- that is:\n\n - you can have either a simple public-key and signature to authorise\n   a spend, or you can have a script, and decide which to use when\n   you spend\n - using the key path comes at no cost compared to not using taproot\n - adding a script path comes at no cost if you don't end up using it\n - if you can interactively verify the script conditions off-chain,\n   you can always use the key path\n\nThe latter of those means we want schnorr so that the key path can be\nmultisig, and using schnorr means that we can use scriptless scripts /\nadaptor signatures for things like lightning making the key path more\ncommon.\n\nYou can't do taproot cheaply with segwit v0 -- you'd have to use p2wsh\nand then reveal something like \"<point> OP_TAPROOT_VERIFY DROP DROP 1\"\nas the script, and then have either a signature or the script and its\nwitness data encoded as the arguments to that script, which is ugly,\nbut more importantly requires an extra 37 odd byte reveal of the point\nevery time.\n\nSo that leads to doing segwit v1 -- as otherwise you'd lose the\nmalleability protection segwit brought, or you'd have to reimplement\nsegwit to allow a top level \"OP_TAPROOTVERIFY\" to use witness data.\n\nIf you're doing segwit v1, you might as well make it so script is\nmore upgradable -- otherwise as soon as you want to upgrade script\nfurther you'll end up having to jump to segwit v2. That brings in the\ngeneralisation of \"p2sh\" that allows different scripts to satisfy a script\nhash via a merkle path, the leaf version, OP_SUCCESS and the CHECKSIG*\nchanges, and that pretty much covers everything that's in bips 340-342.\n\n> SUBJECT: An Alternative Deployment Path for Taproot Technologies\n> 1) A separate soft-fork for Merkle Branch Witnesses based on Taproot;\n\nIt's not clear to me what \"Merkle Branch Witnesses\" are. Google comes up\nwith:\n\n  https://notes.ethereum.org/@vbuterin/rkhCgQteN\n  https://www.btc-way.com/?p=8153\n\nwhich don't go into specifics. There's different \"MAST\" proposals in\nBitcoin, such as bip 116+117 vs bip 114 -- bip 114 and taproot's bip 341\nhave a similar approach; bip 116 on the other hand gives a merkle verify\nopcode, and 117 provides a tail-call semantic that combine allow a\nscript to produce MAST semantics; though in a more programmable way --\nif you had a CAT opcode you could have two MASTs in a single script,\ncombine their result, and then execute it, for instance.\n\n> 2) A separate soft-fork for Schnorr Signatures\n> 3) A separate follow up soft-fork which enables Taproot and Graftroot\n\nIn order to do something like bip 341's merkle script paths, you'd need\na new segwit version, where the scriptpubkey would be the merkle root\nof scripts. If not combined with Schnorr signatures, you'd need to\nprovide leaf versions or change the way CHECKSIG works from how it works\nnow in order to upgrade to Schnorr later.\n\nBut if we're designing soft-fork 1 in a particular way because we already\nknow we want to make particular changes from soft-fork 2, I don't think\nit makes much sense to split them up.\n\nHaving done both of those, in order to do taproot, you'd need another\nnew segwit version, so that the scriptpubkey could be a taproot point,\nbut could otherwise reuse the script path.\n\nObviously I think taproot's desirable, and (roughly) ready to go now,\nso I don't see any reason to split that up, particularly when doing so\nwould use up an additional segwit version.\n\n> users only have to use Schnorr signing if they want to, and can otherwise\n> continue to use ECDSA. \n\nUpdating to schnorr signing makes it easier to validate the blockchain\n(batch validation gives a modest speedup once there are many schnorr\nsignatures), and updating to the signature hashing algorithms described\nin bip 341/342 has benefits for making hardware wallets more secure.\nWhile it's obviously fine for people to not upgrade; upgrading sooner\nrather than later does have systemic benefits.\n\nCheers,\naj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2020-02-10T06:27:24",
                "message_text_only": "Good morning The Group,\n\nThere are already many excellent arguments presented for Taproot, let me present a related one.\n\nNotice your example MAST:\n\n>\n> \u00a0 \u00a0 \u00a0 /\\\n> \u00a0 \u00a0 \u00a0/\u00a0 \\\n> \u00a0 \u00a0 /\u00a0 \u00a0 \\\n> \u00a0 \u00a0/\u00a0 \u00a0 \u00a0 \\\n> \u00a0 /\\\u00a0 \u00a0 \u00a0 /\\\n> \u00a0/\u00a0 \\\u00a0 \u00a0 /\u00a0 \\\n> /\\\u00a0 /\\\u00a0 /\\\u00a0 /\\\n> a b c d e f g h\n\nOf particular note is that the MAST has a predetermined set of scripts, `a` to `h`.\n\nNow, practically speaking, each of these scripts `a`..`h` will be claimable by one or a number of known, pre-determined participants as well.\nScripts that do not have a pre-determined set of participants exist (e.g. a simple `OP_HASH160 <hash> OP_EQUAL` without any `OP_CHECKSIG` operations) but are generally not expected to actually be *useful* for a majority of use-cases (the above hash-only example could be double-spent by a majority miner, for example).\nWe expect a vast majority of scripts that will be in use will have a pre-determined fixed finitely-enumerable set of participants (so that miners cannot steal coins once the \"solution\" to the script puzzle is published in mempools), represented by pubkeys that are fed into `OP_CHECKSIG` operations in the script.\n\nSince each script has (with high probability approaching 1.0) a pre-determined fixed finitely-enumerable set of participants within that script, and the entire MAST itself has a pre-determined fixed finitely-enumerable set of scripts, we can take the union of all sets of participants of all the scripts in the MAST.\n\nThen we put the union of those sets as the signatories of a single Schnorr n-of-n multisignature, to be used as the Taproot keypath branch.\n\nThe advantage now is that with Taproot:\n\n* If you can induce all participants to sign a transaction using the keypath spend, then you gain privacy (no part of the MAST is ever published, not even its root or the presence of the MAST!) *and* reduced onchain fees (because the MAST is not published and does not take up space on the blockchain).\n  * You can incentivize cooperation (beyond just the incentive of improved privacy) by letting participants recover some of the saved onchain fees.\n    Lightning does this, for example: the funder of the channel is the one paying for the closing fees, and the closing fee of the mutual close is almost always lower than the unilateral close case (or else is equal: the closing ritual has the unilateral close fee as the upper bound on whatever fee can be proposed at the mutual close ritual).\n* Even if a participant does not cooperate (for example, it might have been hit by a rogue asteroid in the meantime) we still have the fallback of revealing the entire MAST.\n\n(Just to be clear: I do not *currently* own any datacenters at locations that are likely to be hit by rogue asteroids.)\n\n>From this, we can generally conclude that the Taproot assumption --- that there exists some finitely enumerable set of participants we can derive from the scripts needed to enforce a contract --- holds, at a probability near ~1.0, for almost all complicated contracts and protocols we would find useful.\nSuch contracts and protocols can then be Taproot-ized in order to gain some privacy and transaction size benefits.\n\nOther optimizations, such as selecting k of the n participants as \"key participants\" who are the most likely to be online and interested in the conclusion of the contract, can then be used to reduce the n-of-n to k-of-n, but the basic Taproot \"there exists some n-of-n\" assumption still holds and this is just an optimization on top of that.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Taproot (and graftroot) complexity",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Bryan Bishop",
                "Antoine Riard",
                "Anthony Towns",
                "ZmnSCPxj",
                "Matt Corallo"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 35200
        }
    },
    {
        "title": "[bitcoin-dev] An alternative deployment path for taproot technology (Re: Taproot (and graftroot) complexity)",
        "thread_messages": [
            {
                "author": "Bryan Bishop",
                "date": "2020-02-09T20:22:56",
                "message_text_only": "The following is a message forwarded from an anonymous email that, for\nwhatever reason, couldn't be relayed through the mailing list without my\nassistance. This is message (2/3).\n\nThis email is the second of a collection of sentiments from a group of\ndevelopers\nwho in aggregate prefer to remain anonymous. These emails have been sent\nunder a\npseudonym so as to keep the focus of discussion on the merits of the\ntechnical\nissues, rather than miring the discussion in personal politics. Our goal\nisn't\nto cause a schism, but rather to help figure out what the path forward is\nwith\nTaproot. To that end, we:\n\n1) Discuss the merits of Taproot's design versus simpler alternatives (see\nthread subject, \"Taproot (and Graftroot) Complexity\").\n2) Propose an alternative path to deploying the technologies described in\nBIP-340, BIP-341, and BIP-342 (see thread subject, \"An Alternative\nDeployment\nPath for Taproot Technologies\").\n3) Suggest a modification to Taproot to reduce some of the overhead (see\nthread\nsubject, \"Taproot Public NUMS Optimization\").\n\nAs a follow up to our prior message, we propose a different path forward\nfor the\nTaproot family of changes:\n\n1) A separate soft-fork for Merkle Branch Witnesses based on Taproot;\n2) A separate soft-fork for Schnorr Signatures\n3) A separate follow up soft-fork which enables Taproot and Graftroot\n\nWe think that the first 2 forks can be offered at the same time or one at a\ntime.\n\nTaproot, as a follow up to changes 1 and 2, can be enabled as a soft-fork\non the\nexisting semantics, but requiring a new witness version. With the Public\nNUMS Optimization, wallets could upgrade by just changing one version byte\nto be\nin the same anonymity set as Taproot.\n\nIt's not clear to us that the time to prepare a BIP and implementation for\n1 and\n2 at this point would be any less than the time to do Taproot as currently\nproposed. However, we believe that such a deployment plan is a reasonable\noption\nas it is more conservative, as Merkle Branch witnesses are relatively\nsimple and\nusers only have to use Schnorr signing if they want to, and can otherwise\ncontinue to use ECDSA. A further benefit of waiting on 3 is that we get to\ncollect real world protocol engineering experience to see how frequently the\nTaproot frequency of use assumption holds, and if it is worth doing or not.\n\n\nGreat thanks,\n\nThe Group\n\n\n-- \n- Bryan\nhttp://heybryan.org/\n1 512 203 0507\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200209/3b963b14/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "An alternative deployment path for taproot technology (Re: Taproot (and graftroot) complexity)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Bryan Bishop"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2582
        }
    },
    {
        "title": "[bitcoin-dev] Taproot public NUMS optimization (Re: Taproot (and graftroot) complexity)",
        "thread_messages": [
            {
                "author": "Bryan Bishop",
                "date": "2020-02-09T20:24:32",
                "message_text_only": "The following is a message forwarded from an anonymous email that, for\nwhatever reason, couldn't be relayed through the mailing list without my\nassistance. This is message (3/3).\n\nThis email is the third of a collection of sentiments from a group of\ndevelopers\nwho in aggregate prefer to remain anonymous. These emails have been sent\nunder a\npseudonym so as to keep the focus of discussion on the merits of the\ntechnical\nissues, rather than miring the discussion in personal politics. Our goal\nisn't\nto cause a schism, but rather to help figure out what the path forward is\nwith\nTaproot. To that end, we:\n\n1) Discuss the merits of Taproot's design versus simpler alternatives (see\nthread subject, \"Taproot (and Graftroot) Complexity\").\n2) Propose an alternative path to deploying the technologies described in\nBIP-340, BIP-341, and BIP-342 (see thread subject, \"An Alternative\nDeployment\nPath for Taproot Technologies\").\n3) Suggest a modification to Taproot to reduce some of the overhead (see\nthread\nsubject, \"Taproot Public NUMS Optimization\").\n\nWe propose to modify Taproot's specification in BIP-341 by adding the rule:\n\nIf there is one element on the witness stack:\n\n1) Attempt hashing it to see if it's equal to  the witness program. The\nfirst\nbyte is the control byte for leaf versioning.\n2) If it's not the witness program, and it's 65 bytes, try signature\nvalidation\n\nIf there is more than one element on the witness stack:\n\nIf the control block is even, treat it as a non-Taproot MAST and get the\nleaf\nversion as the last byte of the script (so you can pop it off before\nhashing).\n\n\nIf greater anonymity is required, a NUMS point can still be used in\nTaproot, at\nthe expense of the additional data. However, if NUMS points are just a\ncouple\nwell known constants this could actually decrease privacy as then the NUMS\npoints could differ from application to application fingerprinting wallets.\nInstead, the NUMS point should only be used when a single use nonce can be\nsent, so that NUMS cannot be distinguished from a normal Taproot to a third\nparty who doesn't know the setup (e.g., that the NUMS is H(X) for known X).\n\n\nGreat thanks,\n\nThe Group\n\n\n- Bryan\nhttp://heybryan.org/\n1 512 203 0507\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200209/0d8ecea6/attachment.html>"
            },
            {
                "author": "Jeremy",
                "date": "2020-02-14T21:21:15",
                "message_text_only": "I am working on CTV, which has cases where it's plausible you'd want a\ntaproot tree with a NUMS point.\n\nThe need for NUMS points is a little bit annoying. There are a few reasons\nyou would want to use them instead of multisig:\n\n1) Cheaper to verify/create.\nIf I have a protocol with 1000 people in it, if I add a multisig N of N to\nverify I need a key for all those people, and the probability of use seems\nlow.\nI then also need to prove to each person in the tree that their key is\npresent. My memory on MuSig is a bit rusty, but I think they key\naggregation requires sending all the public keys and re-computing. (Maybe\nyou can compress this to O(log n) using a Merkle tree for the tweak L?)\nFurther, these keys can't just be the addresses provided for those 1000\npeople, as if those addresses are themselves N of Ns or scripts it gets\ncomplicated, fast (and potentially broken). Instead we should ask that each\nparticipant give us a list of keys to include in the top-level. We'd also\nwant each participant to provide\ntwo signatures with that key of some piece of non-txn data (so as to prove\nit itself wasn't a NUMS point -- otherwise may as well skip this all and\njust use a top-level nums point).\n2) Auditable.\nIf I set up an inheritance scheme, like an annuity or something, and the\nIRS wants me to pay taxes on what I've received, adverse inference will\ntell them to assume that my parent gave me a secret get all the money path\nand this is a tax dodge. With a NUMS point, heirs can prove there was no\ntop-level N of N.\n3) I simply don't want to spend it without a script condition, e.g.,\ntimelock.\n\n\nNow, assuming you do want a NUMS, there is basically 4 ways to make one\n(that I could think of):\n\n1) Public NUMS -- this is a constant, HashToCurve(\"I am a NUMS Point\").\nAnyone scanning the chain can see spends are using this constant. Hopefully\neveryone uses the same constant (or everyone uses 2,3,4) so that \"what type\nof NUMS you are using\" isn't a new fingerprint.\n2) Moslty Public NUMS -- I take the hash of some public data (like maybe\nthe txid) on some well defined protocol, and use that. Anyone scanning the\nchain and doing an EC operation per-txid can see I'm using a constant --\nmaybe my HashToCurve takes 10 seconds (perhaps through a VDF to make it\nextra annoying for anyone who hasn't been sent the shortcut), but in\npractice it's no better than 1.\n3) Interactive NUMS -- I swap H(Rx), H(Ry) with the other participant and\nthen NUMS with H(Rx || Ry). This is essentially equivalent to using a MuSig\nkey setup where one person's key is a NUMS. Now no one passively scanning\ncan see that it's NUMS, but I can prove to an auditor later.\n4) 1/2 RTT Async-Interactive NUMS -- I take some public salt -- say the\ntxid T, and hash it with a piece of random data R and then HashToCurve(T ||\nR)... I think this is secure? Not clear the txid adds any security. Now I\ncan prove to you that the hash was based on the txid, but I've blinded it\nwith R to stop passive observers. But I also need ot send you data out of\nband for R (but I already had to do this for Taproot maybe?)\n\nThe downsides with 3/4 is that if you lose your setup, you lose your\nability to spend/prove it's private (maybe can generate R from a seed?). So\nbetter hold on to those tightly! Or use a public NUMS.\n\nOnly 3,4 provide any \"real\" privacy benefit and at a small hit to\nlikelihood of losing funds (more non-deterministic data to store). I guess\nthe question becomes how likely are we to have support for generating a\nbunch of NUMS points?\n\nComparing with this proposal which removes the NUMS requirement:\n\n1) NUMS/Taproot anonymity set *until* spend, MAST set after spend\n2) No complexity around NUMS generation/storage\n3) If people don't have ecosystem-wide consistent NUMS practices, leads to\nadditional privacy leak v.s. bare MAST which would be equivalent to case 1\n(Public NUMS)\n4) Slightly less chain overhead (32 bytes/8 vbytes).\n5) Slightly faster chain validation (EC Point tweak is what like 10,000 -\n100,000 times slower than a hash?)\n\n\nMatt raises a interesting point in the other thread, which is that if we\nput the option for a more private NUMS thing, someone will eventually write\nsoftware for it. But that seems to be irrespective of if we make no-NUMS an\noption for bare MAST spends.\n\nOverall I think this is a reasonable proposal. It effectively only\nintroduces bare MAST to prevent the case where people are using a few\ndifferent Public NUMS leaking metadata by putting incentive to use the same\none -- none. Using a private NUMS is unaffected incentive wise as it's\nessentially just paying a bit more to be in the larger anonymity set. I\nthink it makes some class of users better off, and no one else worse off,\nso this change seems Pareto.\n\nThus I'm in favor of adding a rule like this.\n\nI think reasonable alternative responses to accepting this proposed change\nwould be to:\n\n1) Add a BIP for a standard Public NUMS Point exported through secp256k1 to\nhead off people defining their own point.\n2) Add a discounting rule if the point P is the Public NUMS that discounts\nthe extra weight somehow.\n3) Take a Bit out of the leaf version portion of C[0] to denote Public NUMS\nand then elide having to include the point (as it's just standard). This\nhas the benefit of not needing as much code-change as The Group's proposed\nchange, but the downside of still requiring an extra EC Mul in validation.\n\nRejecting the proposal is also, IMO, reasonable. On my personal\npreferences, I'd rather get something like Taproot and MAST available\nsooner than later, even if there are small quirks on privacy and cost, and\nignore a small benefit rule change/exception that would hold it up by more\nthan a month or two. I don't see why a small tweak would add substantial\ndelay, but I think other BIP authors/reviewers would be able to better\ncomment.\n\nBest,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n\n\nOn Sun, Feb 9, 2020 at 12:25 PM Bryan Bishop via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> The following is a message forwarded from an anonymous email that, for\n> whatever reason, couldn't be relayed through the mailing list without my\n> assistance. This is message (3/3).\n>\n> This email is the third of a collection of sentiments from a group of\n> developers\n> who in aggregate prefer to remain anonymous. These emails have been sent\n> under a\n> pseudonym so as to keep the focus of discussion on the merits of the\n> technical\n> issues, rather than miring the discussion in personal politics. Our goal\n> isn't\n> to cause a schism, but rather to help figure out what the path forward is\n> with\n> Taproot. To that end, we:\n>\n> 1) Discuss the merits of Taproot's design versus simpler alternatives (see\n> thread subject, \"Taproot (and Graftroot) Complexity\").\n> 2) Propose an alternative path to deploying the technologies described in\n> BIP-340, BIP-341, and BIP-342 (see thread subject, \"An Alternative\n> Deployment\n> Path for Taproot Technologies\").\n> 3) Suggest a modification to Taproot to reduce some of the overhead (see\n> thread\n> subject, \"Taproot Public NUMS Optimization\").\n>\n> We propose to modify Taproot's specification in BIP-341 by adding the rule:\n>\n> If there is one element on the witness stack:\n>\n> 1) Attempt hashing it to see if it's equal to  the witness program. The\n> first\n> byte is the control byte for leaf versioning.\n> 2) If it's not the witness program, and it's 65 bytes, try signature\n> validation\n>\n> If there is more than one element on the witness stack:\n>\n> If the control block is even, treat it as a non-Taproot MAST and get the\n> leaf\n> version as the last byte of the script (so you can pop it off before\n> hashing).\n>\n>\n> If greater anonymity is required, a NUMS point can still be used in\n> Taproot, at\n> the expense of the additional data. However, if NUMS points are just a\n> couple\n> well known constants this could actually decrease privacy as then the NUMS\n> points could differ from application to application fingerprinting wallets.\n> Instead, the NUMS point should only be used when a single use nonce can be\n> sent, so that NUMS cannot be distinguished from a normal Taproot to a third\n> party who doesn't know the setup (e.g., that the NUMS is H(X) for known X).\n>\n>\n> Great thanks,\n>\n> The Group\n>\n>\n> - Bryan\n> http://heybryan.org/\n> 1 512 203 0507\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200214/e7c9e7e2/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Taproot public NUMS optimization (Re: Taproot (and graftroot) complexity)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Bryan Bishop",
                "Jeremy"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 11090
        }
    },
    {
        "title": "[bitcoin-dev] Taproot (and graftroot) complexity (reflowed)",
        "thread_messages": [
            {
                "author": "Bryan Bishop",
                "date": "2020-02-09T20:47:29",
                "message_text_only": "Apologies for my previous attempt at relaying the message- it looks like\nthe emails got mangled on the archive. I am re-sending them in this\ncombined email with what I hope will be better formatting. Again this is\nfrom some nym that had trouble posting to this mailing list; I didn't see\nany emails in the queue so I couldn't help to publish this sooner.\n\nSUBJECT: Taproot (and Graftroot) Complexity\n\nThis email is the first of a collection of sentiments from a group of\ndevelopers who in aggregate prefer to remain anonymous. These emails have\nbeen sent under a pseudonym so as to keep the focus of discussion on the\nmerits of the technical issues, rather than miring the discussion in\npersonal politics.  Our goal isn't to cause a schism, but rather to help\nfigure out what the path forward is with Taproot. To that end, we:\n\n1) Discuss the merits of Taproot's design versus simpler alternatives (see\nthread subject, \"Taproot (and Graftroot) Complexity\").\n\n2) Propose an alternative path to deploying the technologies described in\nBIP-340, BIP-341, and BIP-342 (see thread subject, \"An Alternative\nDeployment Path for Taproot Technologies\").\n\n3) Suggest a modification to Taproot to reduce some of the overhead (see\nthread subject, \"Taproot Public NUMS Optimization\").\n\nNow that the BIP has moved to draft we felt that now was the time to\nprioritize review to make sure it was an acceptable change for our\nactivities. As a group, we're excited about the totality of what Taproot\nhas to offer. However, after our review, we're left perplexed about the\ndevelopment of Taproot (and Graftroot, to a lesser extent).\n\nWe also want to convey that we have nothing but respect for the developers\nand community who have poured their heart and soul into preparing Taproot.\nSelf evidently, it is an impressive synthesis of ideas. We believe that the\nhighest form of respect to pay such a synthesis of ideas is a detailed and\ncritical review, as it's pertinent to closely consider changes to Bitcoin.\n\n\nIn essence, Taproot is fundamentally the same as doing\nhttps://github.com/bitcoin/bips/blob/master/bip-0114.mediawiki and Schnorr\nsignatures separately.\n\nThe main reason for putting them together -- as mentioned in the BIP -- is\na gain in efficiency. But this efficiency pre-supposes a specific use case\nand probability distribution of use cases.\n\nCompare:\n\nSuppose a MAST for {a,b,c,d,e,f,g,h} spending conditions it looks something\nlike this:\n\n\n      /\\\n     /  \\\n    /    \\\n   /      \\\n  /\\      /\\\n /  \\    /  \\\n/\\  /\\  /\\  /\\\na b c d e f g h\n\nIf we want this to be functionally equivalent to Taproot, we add a new path:\n\n       /\\\n      /\\ {<pk> schnorr_checksig}\n     /  \\\n    /    \\\n   /      \\\n  /\\      /\\\n /  \\    /  \\\n/\\  /\\  /\\  /\\\na b c d e f g h\n\nNow, to spend from this MBV you have to reveal 32 bytes on the stack for\nthe not taken branch, and 35 bytes for the <pk> schnorr_checksig (1 byte\npush, 33 bytes PK, 1 byte checksig).\n\nThis is 67 bytes more than Taproot would require for the same spending\ncondition.\n\nHowever, suppose we wanted to use one of the script paths instead. We still\nneed to have one extra hash for the {<pk> schnorr_checksig} (depending on\nif we put the key in this position or not--see below). But now we can spend\nwith just a logarithmic control program path.\n\nHowever, if we do the same script via taproot, we now need to provide the\nbase public key (33 bytes) as well as the root hash (32 bytes) and path and\nthen the actual scripts. With the need for 2 push bytes, this ends up being\nback at 67 bytes extra.\n\nIs Taproot just a probability assumption about the frequency and likelihood\nof the signature case over the script case? Is this a good assumption?  The\nBIP only goes as far as to claim that the advantage is apparent if the\noutputs *could be spent* as an N of N, but doesn't make representations\nabout how likely that N of N case would be in practice compared to the\nscript paths. Perhaps among use cases, more than half of the ones we expect\npeople to be doing could be spent as an N of N. But how frequently would\nthat path get used? Further, while the *use cases* might skew toward things\nwith N of N opt-out, we might end up in a power law case where it's the one\ncase that doesn't use an N of N opt out at all (or at a de minimis level)\nthat becomes very popular, thereby making Taproot more costly then\nbeneficial.\n\nFurther, if you don't want to use a Taproot top-level key (e.g., you need\nto be able to audit that no one can spend outside of one of the script\nconditions), then you need to use a NUMS (nothing up my sleeve) point. This\nforces users who don't want Taproot to pay the expense, when if they just\nhad a MAST based witness type they would be cheaper. So if this use case is\nat all common, Taproot leaves them worse off in terms of fees. Given that\nscript paths are usually done in the case where there is some contested\nclose, it's actually in the interest of protocol developers that the\ncontested script path be as efficient as possible so that the fees paid\nmaximally increase the feerate. We think this can be fixed simply in\nTaproot though, as noted below.\n\n\n\nOn privacy, we're also a bit confused as to the goal of Taproot over MAST\nand Schnorr. Earlier, we presented a design with MAST which is very close\nto Taproot.  However, it'd also be possible to just add {<pk>\nschnorr_checksig} to the set {a,b,c,d,e,f,g,h}, shuffle them, and compute\nsome MAST structure (perhaps probability encoded) on them. This has the\neffect of not having much additional fees for adding the extra Schnorr path\nat redeem time (only 1 extra branch on 2/8 script paths), e.g.\n\n\n      /\\\n     /  \\\n    /    \\\n   /      \\\n  /\\      /\\\n /  \\    /  \\\n/\\  /\\  /\\  /\\\na b c d e f/\\ {<pk> schnorr_checksig}\n          g  h\n\nWe could argue that this is more private than Taproot, because we don't\ndistinguish between the Schnorr key case and other cases by default, so\nchain analyzers can't tell if the signature came from the Taproot case or\nfrom one of the Script paths. There's also no NUMS point required, which\nmeans chain analyzers can't tell when you spend that there was no top level\nkey if the NUMS point is not per-output indistinguishable. By using a\nsemi-randomized MAST structure, chain analyzers also can't tell exactly how\nbig your spend condition MAST was. In particular, you care more about\nprivacy when you are contesting a close of a channel or other script path\nbecause then the miners could be more likely to extract a rent from you as\n\"ransom\" for properly closing your channel (or in other words, in a\ncontested close the value of the closing transaction is larger than usual).\n\nIt would also be possible to do something really simple which is to allow\nthe witness type to be either a MAST hash OR a schnorr key (but not a\nTaproot). This allows you to not completely fracture the anonymity set\nbetween people who want plain Schnorr and people who want MAST (at least\nuntil they go to spend). This fix can also be used in Taproot in place of a\nNUMS point, to decrease extra fees. It's unclear if this plays negatively\nwith any future batch validation mechanism though, but the contextual\nchecks to exclude a witness program from the batch are relatively simple.\nSee thread subject, \"Taproot Public NUMS Optimization\".\n\nThe considerations around Graftroot, a proposed delegation mechanism, is a\nbit similar. Delegation is a mechanism by which a UTXO with script S can\nsign a script R which can then be executed in addition to S without\nrequiring a transaction. This allows an output to monotonically and\ndynamically increase the number of conditions under which it can be spent.\nAs noted by Pieter Wiulle here:\nhttps://github.com/kanzure/diyhpluswiki/commit/a03f6567d714f8733b578de263a4b149441cd058\ndelegation was originally possible in Bitcoin, but got broken during an\nemergency fork to split the scriptSig and scriptpubkey separation. Rather\nthan adding some fancy delegation mechanism in Bitcoin, why not just have a\nP2SH-like semantic which allows a delegated script to be evaluated? See\nBIP-117 https://github.com/bitcoin/bips/blob/master/bip-0117.mediawiki.\nThis way we aren't special casing where delegation can occur, and we can\nallow taproot nested spending conditions (i.e., with timelocks) to generate\ntheir own delegations. As I've seen Graftroot discussed thus far, it is as\na top-level witness program version like Taproot and non-recursive. Similar\nto the above discussion, top-level is more efficient if you suspect that\ndelegation will be most likely occurring at the top level, but it's not\nclear that's a good assumption as it may be common to want to allow\ndifferent scripts to delegate.\n\n\nOverall, we are left with concerns both about the merit of doing Taproot\nversus alternatives, as well as the process through which we got to be here.\n\n1) Is Taproot actually more private than bare MAST and Schnorr separately?\nWhat are the actual anonymity set benefits compared to doing the separately?\n\n2) Is Taproot actually cheaper than bare MAST and Schnorr separately? What\nevidence do we have that the assumption it will be more common to use\nTaproot with a key will outweigh Script cases?\n\n3) Is Taproot riskier than bare MAST and Schnorr separately given the new\ncrypto? How well reviewed is the actual crypto parts? None of us personally\nfeel comfortable reviewing the crypto in Schnorr -- what's the set of\npeople who have thoroughly reviewed the crypto and aren't just ACKing\nbecause they trust other developers to have looked at it close enough?\n\n4) Design wise, couldn't we forego the NUMS point requirement and be able\nto check if it's a hash root directly? This would encumber users who don't\nneed the key path a cheaper spend path. See thread subject, \"Taproot Public\nNUMS Optimization\".\n\n5) Is the development model of trying to jam a bunch of features into\nBitcoin all at once good for Bitcoin development? Would we be better off if\nwe embraced incremental improvements that can work together (e.g., MAST and\nthen Schnorr)?  Although the BIP raises some points about anonymity sets\nbeing why to do them all at once, it's not clear to me this argument holds\nwater (same goes for businesses not upgrading). If we can take things as\nsmaller steps, we are not only more secure, but we also have more time to\ndedicate review to each change independently. We also end up co-mingling\nchanges that people end up accepting only because they want one and they're\nbundled (e.g., MAST and Schnorr, MAST seems like a much less risky addition\nversus Schnorr). See thread subject, \"An Alternative Deployment Path for\nTaproot Technologies\".\n\n\n\n\nOur provocation with this email is primarily that we think we should more\ncarefully consider the benefits of Taproot over simpler primitives that are\nnot only easier to review, but could have been made available much sooner\nrather than waiting on putting everything all together for an unclear\naggregate benefit.\n\nWe do think that most of the developers have been honest about the benefits\nof Taproot, but that on closer look we feel the general ecosystem has\noversold Taproot as being the key enabler for a collection of techniques\nthat we could do with much simpler building blocks.\n\n\nAt the end of the day, we do not strongly advocate not deploying Taproot at\nthis point in the review cycle. We think the Taproot Public NUMS\nOptimization may be a good idea, worth considering if it's not insecure, as\nit cuts through the case where you would otherwise need a NUMS point.\nThings like TapScript and its MAST mechanisms are well designed and offer\nexciting new deployment paths, and would be something we would use even if\nwe opted for MAST instead of Taproot. However, we also believe it is our\nduty to raise these concerns and suggestions, and we look forward to\nlistening to the responses of the community.\n\nGreat thanks,\n\nThe Group\n\n----\n\nSUBJECT: An Alternative Deployment Path for Taproot Technologies\n\nThis email is the second of a collection of sentiments from a group of\ndevelopers who in aggregate prefer to remain anonymous. These emails have\nbeen sent under a pseudonym so as to keep the focus of discussion on the\nmerits of the technical issues, rather than miring the discussion in\npersonal politics. Our goal isn't to cause a schism, but rather to help\nfigure out what the path forward is with Taproot. To that end, we: [clip\nrepeat]\n\nAs a follow up to our prior message, we propose a different path forward\nfor the Taproot family of changes:\n\n1) A separate soft-fork for Merkle Branch Witnesses based on Taproot;\n\n2) A separate soft-fork for Schnorr Signatures\n\n3) A separate follow up soft-fork which enables Taproot and Graftroot\n\nWe think that the first 2 forks can be offered at the same time or one at a\ntime.\n\nTaproot, as a follow up to changes 1 and 2, can be enabled as a soft-fork\non the existing semantics, but requiring a new witness version. With the\nPublic NUMS Optimization, wallets could upgrade by just changing one\nversion byte to be in the same anonymity set as Taproot.\n\nIt's not clear to us that the time to prepare a BIP and implementation for\n1 and 2 at this point would be any less than the time to do Taproot as\ncurrently proposed. However, we believe that such a deployment plan is a\nreasonable option as it is more conservative, as Merkle Branch witnesses\nare relatively simple and users only have to use Schnorr signing if they\nwant to, and can otherwise continue to use ECDSA. A further benefit of\nwaiting on 3 is that we get to collect real world protocol engineering\nexperience to see how frequently the Taproot frequency of use assumption\nholds, and if it is worth doing or not.\n\n\nGreat thanks,\n\nThe Group\n\n\n----\n\nSUBJECT: Taproot Public NUMS Optimization\n\nThis email is the third of a collection of sentiments from a group of\ndevelopers who in aggregate prefer to remain anonymous. These emails have\nbeen sent under a pseudonym so as to keep the focus of discussion on the\nmerits of the technical issues, rather than miring the discussion in\npersonal politics. Our goal isn't to cause a schism, but rather to help\nfigure out what the path forward is with Taproot. To that end, we: [clipped\nagain]\n\nWe propose to modify Taproot's specification in BIP-341 by adding the rule:\n\nIf there is one element on the witness stack:\n\n1) Attempt hashing it to see if it's equal to  the witness program. The\nfirst byte is the control byte for leaf versioning.\n\n2) If it's not the witness program, and it's 65 bytes, try signature\nvalidation\n\nIf there is more than one element on the witness stack:\n\nIf the control block is even, treat it as a non-Taproot MAST and get the\nleaf version as the last byte of the script (so you can pop it off before\nhashing).\n\n\nIf greater anonymity is required, a NUMS point can still be used in\nTaproot, at the expense of the additional data. However, if NUMS points are\njust a couple well known constants this could actually decrease privacy as\nthen the NUMS points could differ from application to application\nfingerprinting wallets.  Instead, the NUMS point should only be used when a\nsingle use nonce can be sent, so that NUMS cannot be distinguished from a\nnormal Taproot to a third party who doesn't know the setup (e.g., that the\nNUMS is H(X) for known X).\n\n\nGreat thanks,\n\nThe Group\n\n-- \n- Bryan\nhttp://heybryan.org/\n1 512 203 0507\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200209/fdfb2b51/attachment-0001.html>"
            },
            {
                "author": "David A. Harding",
                "date": "2020-02-10T00:15:54",
                "message_text_only": "On Sun, Feb 09, 2020 at 02:47:29PM -0600, Anon via Bryan Bishop via bitcoin-dev wrote:\n> 1) Is Taproot actually more private than bare MAST and Schnorr separately?\n\nYes.\n\n> What are the actual anonymity set benefits compared to doing the separately?\n\nWhen schnorr and taproot are done together, all of the following\ntransaction types can be part of the same set:\n\n    - single-sig spends (similar to current use of P2PKH and P2WPKH)\n\n    - n-of-n spends with musig or equivalent (similar to current use of\n      P2SH and P2WSH 2-of-2 multisig without special features as used by\n      Blockstream Green and LN mutual closes)\n\n    - k-of-n (for low values of n) using the most common k signers\n      (similar to BitGo-style 2-of-3 where the keys involved are\n      alice_hot, alice_cold, and bob_hot and almost all transactions are\n      expected to be signed by {alice_hot, bob_hot}; that common case\n      can be the key-path spend and the alternatives {alice_hot,\n      alice_cold} and {alice_cold, bob_hot} can be script-path spends)\n\n    - contract protocols that can sometimes result in all parties\n      agreeing on an outcome (similar to LN mutual closes, cross-chain\n      atomic swaps, and same-chain coinswaps)\n\nThe four cases above represent an overwhelming percentage of the spends\nseen on the block chain today and throughout Bitcoin's entire history to\ndate, so optimizing to include them in the anonymity set presents a huge\nbenefit.\n\n> 2) Is Taproot actually cheaper than bare MAST and Schnorr separately? \n\nEarlier in y'alls email, you claim that the difference between the two\napproaches for a particular example is 67 bytes.  I haven't checked that\ncalculation, but it seems you're talking entirely about bytes that could\nappear in the witness data and so would only represent 16.75 vbytes.\nCompare that to the size of the other elements which would need to be\npart of a typical input:\n\n- (36 vbytes) outpoint\n- (1) scriptSig compactSize uint\n- (4) nSequence \n- (16.25) schnorr signature (includes size byte)\n\nThat's 57.25 vbytes exclusive of your example data or 74.00 vbytes\ninclusive.  That means the overhead you're concerned about adds only\nabout 23% to the size of the input (or 30% on an exclusive basis).\nThat's definitely worth considering optimizations for, but I'm\npersonally ok with requiring users of advanced scripts (who can't manage\nto produce mutual closes) pay an extra 23% for their inputs in order to\nallow the creation of the large anonymity set described above for all\nthe other cases.\n\nIf, subsequent to deployment, large numbers of users do end up using\ntaproot script-path spends and we want to make things more fair, we can\neven out the weighting, perhaps by simply increasing the weight of\nkey-path spends by 16.75 vbytes (though that would, of course,\nproportionally lower the capacity of the block chain).  As mentioned in\na separate email by Matt Corallo, it seems worthwhile to optimize for\nthe case where script-path spenders are encouraged to look for\nmutually-agreed contract resolutions in order to both minimize block\nchain use and increase the size of the anonymity set.\n\n> What evidence do we have that the assumption it will be more common to\n> use Taproot with a key will outweigh Script cases?\n\nThe evidence that current users of single-sig, n-of-n, and k-of-n (for\nsmall n) with a default k-set, and mutual-agreed contract protocol\noutcomes vastly outweigh all other transaction inputs today and for all\nof Bitcoin's history to date.\n\n-Dave\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200209/c79b6dae/attachment.sig>"
            },
            {
                "author": "Jonas Nick",
                "date": "2020-02-10T16:28:32",
                "message_text_only": "I agree with most of the comments so far, but the group brings up an often\noverlooked point with respect to the privacy benefits of taproot. In the extreme\ncase, if there would be no policies that have both a key and a script spend\npath, then taproot does not improve anonymity sets compared to the \"Taproot\nPublic NUMS Optimization\" proposal (which saves 8 vbytes in a script-spend). (*)\n\nIn fact, the cases where scripts would have to be used given usage of Bitcoin\ntoday are be rare because threshold policies, their conjunctions and\ndisjunctions can be expressed with a single public key. Even if we disregard\nspeculation that timelocks, ANYPREVOUT/NOINPUT and other interesting scripts\nwill be used in the future (which can be added through the leaf or key versions\nwithout affecting key-spend anonymity sets), not all of today's applications are\nable to be represented single public keys because there are applications that\ncan not deal with interactive key setups or interactive signing. For\napplications where this is possible it will be a gradual change because of the\nengineering challenges involved. For example, k-of-n threshold policies could\nhave the most likely k-of-k in the taproot output key and other k-of-k in the\nleaves, instead of going for a k-of-n taproot output key immediately.\n\nGiven that anonymity sets in Bitcoin are permanent and software tends to be\ndeployed longer than anyone would expect at the time of deployment,\nrealistically Taproot is superior to the \"Public NUMS Optimization\" and \"An\nAlternative Deployment Path\".\n\n(*) One could argue that the little plausible deniability gained by a very small\nprobability of the change of a script-spend being a key-spend and vice versa is\nsignificantly better than no probability at all.\n\nOn 2/9/20 8:47 PM, Bryan Bishop via bitcoin-dev wrote:\n> Apologies for my previous attempt at relaying the message- it looks like\n> the emails got mangled on the archive. I am re-sending them in this\n> combined email with what I hope will be better formatting. Again this is\n> from some nym that had trouble posting to this mailing list; I didn't see\n> any emails in the queue so I couldn't help to publish this sooner.\n> \n> SUBJECT: Taproot (and Graftroot) Complexity\n> \n> This email is the first of a collection of sentiments from a group of\n> developers who in aggregate prefer to remain anonymous. These emails have\n> been sent under a pseudonym so as to keep the focus of discussion on the\n> merits of the technical issues, rather than miring the discussion in\n> personal politics.  Our goal isn't to cause a schism, but rather to help\n> figure out what the path forward is with Taproot. To that end, we:\n> \n> 1) Discuss the merits of Taproot's design versus simpler alternatives (see\n> thread subject, \"Taproot (and Graftroot) Complexity\").\n> \n> 2) Propose an alternative path to deploying the technologies described in\n> BIP-340, BIP-341, and BIP-342 (see thread subject, \"An Alternative\n> Deployment Path for Taproot Technologies\").\n> \n> 3) Suggest a modification to Taproot to reduce some of the overhead (see\n> thread subject, \"Taproot Public NUMS Optimization\").\n> \n> Now that the BIP has moved to draft we felt that now was the time to\n> prioritize review to make sure it was an acceptable change for our\n> activities. As a group, we're excited about the totality of what Taproot\n> has to offer. However, after our review, we're left perplexed about the\n> development of Taproot (and Graftroot, to a lesser extent).\n> \n> We also want to convey that we have nothing but respect for the developers\n> and community who have poured their heart and soul into preparing Taproot.\n> Self evidently, it is an impressive synthesis of ideas. We believe that the\n> highest form of respect to pay such a synthesis of ideas is a detailed and\n> critical review, as it's pertinent to closely consider changes to Bitcoin.\n> \n> \n> In essence, Taproot is fundamentally the same as doing\n> https://github.com/bitcoin/bips/blob/master/bip-0114.mediawiki and Schnorr\n> signatures separately.\n> \n> The main reason for putting them together -- as mentioned in the BIP -- is\n> a gain in efficiency. But this efficiency pre-supposes a specific use case\n> and probability distribution of use cases.\n> \n> Compare:\n> \n> Suppose a MAST for {a,b,c,d,e,f,g,h} spending conditions it looks something\n> like this:\n> \n> \n>       /\\\n>      /  \\\n>     /    \\\n>    /      \\\n>   /\\      /\\\n>  /  \\    /  \\\n> /\\  /\\  /\\  /\\\n> a b c d e f g h\n> \n> If we want this to be functionally equivalent to Taproot, we add a new path:\n> \n>        /\\\n>       /\\ {<pk> schnorr_checksig}\n>      /  \\\n>     /    \\\n>    /      \\\n>   /\\      /\\\n>  /  \\    /  \\\n> /\\  /\\  /\\  /\\\n> a b c d e f g h\n> \n> Now, to spend from this MBV you have to reveal 32 bytes on the stack for\n> the not taken branch, and 35 bytes for the <pk> schnorr_checksig (1 byte\n> push, 33 bytes PK, 1 byte checksig).\n> \n> This is 67 bytes more than Taproot would require for the same spending\n> condition.\n> \n> However, suppose we wanted to use one of the script paths instead. We still\n> need to have one extra hash for the {<pk> schnorr_checksig} (depending on\n> if we put the key in this position or not--see below). But now we can spend\n> with just a logarithmic control program path.\n> \n> However, if we do the same script via taproot, we now need to provide the\n> base public key (33 bytes) as well as the root hash (32 bytes) and path and\n> then the actual scripts. With the need for 2 push bytes, this ends up being\n> back at 67 bytes extra.\n> \n> Is Taproot just a probability assumption about the frequency and likelihood\n> of the signature case over the script case? Is this a good assumption?  The\n> BIP only goes as far as to claim that the advantage is apparent if the\n> outputs *could be spent* as an N of N, but doesn't make representations\n> about how likely that N of N case would be in practice compared to the\n> script paths. Perhaps among use cases, more than half of the ones we expect\n> people to be doing could be spent as an N of N. But how frequently would\n> that path get used? Further, while the *use cases* might skew toward things\n> with N of N opt-out, we might end up in a power law case where it's the one\n> case that doesn't use an N of N opt out at all (or at a de minimis level)\n> that becomes very popular, thereby making Taproot more costly then\n> beneficial.\n> \n> Further, if you don't want to use a Taproot top-level key (e.g., you need\n> to be able to audit that no one can spend outside of one of the script\n> conditions), then you need to use a NUMS (nothing up my sleeve) point. This\n> forces users who don't want Taproot to pay the expense, when if they just\n> had a MAST based witness type they would be cheaper. So if this use case is\n> at all common, Taproot leaves them worse off in terms of fees. Given that\n> script paths are usually done in the case where there is some contested\n> close, it's actually in the interest of protocol developers that the\n> contested script path be as efficient as possible so that the fees paid\n> maximally increase the feerate. We think this can be fixed simply in\n> Taproot though, as noted below.\n> \n> \n> \n> On privacy, we're also a bit confused as to the goal of Taproot over MAST\n> and Schnorr. Earlier, we presented a design with MAST which is very close\n> to Taproot.  However, it'd also be possible to just add {<pk>\n> schnorr_checksig} to the set {a,b,c,d,e,f,g,h}, shuffle them, and compute\n> some MAST structure (perhaps probability encoded) on them. This has the\n> effect of not having much additional fees for adding the extra Schnorr path\n> at redeem time (only 1 extra branch on 2/8 script paths), e.g.\n> \n> \n>       /\\\n>      /  \\\n>     /    \\\n>    /      \\\n>   /\\      /\\\n>  /  \\    /  \\\n> /\\  /\\  /\\  /\\\n> a b c d e f/\\ {<pk> schnorr_checksig}\n>           g  h\n> \n> We could argue that this is more private than Taproot, because we don't\n> distinguish between the Schnorr key case and other cases by default, so\n> chain analyzers can't tell if the signature came from the Taproot case or\n> from one of the Script paths. There's also no NUMS point required, which\n> means chain analyzers can't tell when you spend that there was no top level\n> key if the NUMS point is not per-output indistinguishable. By using a\n> semi-randomized MAST structure, chain analyzers also can't tell exactly how\n> big your spend condition MAST was. In particular, you care more about\n> privacy when you are contesting a close of a channel or other script path\n> because then the miners could be more likely to extract a rent from you as\n> \"ransom\" for properly closing your channel (or in other words, in a\n> contested close the value of the closing transaction is larger than usual).\n> \n> It would also be possible to do something really simple which is to allow\n> the witness type to be either a MAST hash OR a schnorr key (but not a\n> Taproot). This allows you to not completely fracture the anonymity set\n> between people who want plain Schnorr and people who want MAST (at least\n> until they go to spend). This fix can also be used in Taproot in place of a\n> NUMS point, to decrease extra fees. It's unclear if this plays negatively\n> with any future batch validation mechanism though, but the contextual\n> checks to exclude a witness program from the batch are relatively simple.\n> See thread subject, \"Taproot Public NUMS Optimization\".\n> \n> The considerations around Graftroot, a proposed delegation mechanism, is a\n> bit similar. Delegation is a mechanism by which a UTXO with script S can\n> sign a script R which can then be executed in addition to S without\n> requiring a transaction. This allows an output to monotonically and\n> dynamically increase the number of conditions under which it can be spent.\n> As noted by Pieter Wiulle here:\n> https://github.com/kanzure/diyhpluswiki/commit/a03f6567d714f8733b578de263a4b149441cd058\n> delegation was originally possible in Bitcoin, but got broken during an\n> emergency fork to split the scriptSig and scriptpubkey separation. Rather\n> than adding some fancy delegation mechanism in Bitcoin, why not just have a\n> P2SH-like semantic which allows a delegated script to be evaluated? See\n> BIP-117 https://github.com/bitcoin/bips/blob/master/bip-0117.mediawiki.\n> This way we aren't special casing where delegation can occur, and we can\n> allow taproot nested spending conditions (i.e., with timelocks) to generate\n> their own delegations. As I've seen Graftroot discussed thus far, it is as\n> a top-level witness program version like Taproot and non-recursive. Similar\n> to the above discussion, top-level is more efficient if you suspect that\n> delegation will be most likely occurring at the top level, but it's not\n> clear that's a good assumption as it may be common to want to allow\n> different scripts to delegate.\n> \n> \n> Overall, we are left with concerns both about the merit of doing Taproot\n> versus alternatives, as well as the process through which we got to be here.\n> \n> 1) Is Taproot actually more private than bare MAST and Schnorr separately?\n> What are the actual anonymity set benefits compared to doing the separately?\n> \n> 2) Is Taproot actually cheaper than bare MAST and Schnorr separately? What\n> evidence do we have that the assumption it will be more common to use\n> Taproot with a key will outweigh Script cases?\n> \n> 3) Is Taproot riskier than bare MAST and Schnorr separately given the new\n> crypto? How well reviewed is the actual crypto parts? None of us personally\n> feel comfortable reviewing the crypto in Schnorr -- what's the set of\n> people who have thoroughly reviewed the crypto and aren't just ACKing\n> because they trust other developers to have looked at it close enough?\n> \n> 4) Design wise, couldn't we forego the NUMS point requirement and be able\n> to check if it's a hash root directly? This would encumber users who don't\n> need the key path a cheaper spend path. See thread subject, \"Taproot Public\n> NUMS Optimization\".\n> \n> 5) Is the development model of trying to jam a bunch of features into\n> Bitcoin all at once good for Bitcoin development? Would we be better off if\n> we embraced incremental improvements that can work together (e.g., MAST and\n> then Schnorr)?  Although the BIP raises some points about anonymity sets\n> being why to do them all at once, it's not clear to me this argument holds\n> water (same goes for businesses not upgrading). If we can take things as\n> smaller steps, we are not only more secure, but we also have more time to\n> dedicate review to each change independently. We also end up co-mingling\n> changes that people end up accepting only because they want one and they're\n> bundled (e.g., MAST and Schnorr, MAST seems like a much less risky addition\n> versus Schnorr). See thread subject, \"An Alternative Deployment Path for\n> Taproot Technologies\".\n> \n> \n> \n> \n> Our provocation with this email is primarily that we think we should more\n> carefully consider the benefits of Taproot over simpler primitives that are\n> not only easier to review, but could have been made available much sooner\n> rather than waiting on putting everything all together for an unclear\n> aggregate benefit.\n> \n> We do think that most of the developers have been honest about the benefits\n> of Taproot, but that on closer look we feel the general ecosystem has\n> oversold Taproot as being the key enabler for a collection of techniques\n> that we could do with much simpler building blocks.\n> \n> \n> At the end of the day, we do not strongly advocate not deploying Taproot at\n> this point in the review cycle. We think the Taproot Public NUMS\n> Optimization may be a good idea, worth considering if it's not insecure, as\n> it cuts through the case where you would otherwise need a NUMS point.\n> Things like TapScript and its MAST mechanisms are well designed and offer\n> exciting new deployment paths, and would be something we would use even if\n> we opted for MAST instead of Taproot. However, we also believe it is our\n> duty to raise these concerns and suggestions, and we look forward to\n> listening to the responses of the community.\n> \n> Great thanks,\n> \n> The Group\n> \n> ----\n> \n> SUBJECT: An Alternative Deployment Path for Taproot Technologies\n> \n> This email is the second of a collection of sentiments from a group of\n> developers who in aggregate prefer to remain anonymous. These emails have\n> been sent under a pseudonym so as to keep the focus of discussion on the\n> merits of the technical issues, rather than miring the discussion in\n> personal politics. Our goal isn't to cause a schism, but rather to help\n> figure out what the path forward is with Taproot. To that end, we: [clip\n> repeat]\n> \n> As a follow up to our prior message, we propose a different path forward\n> for the Taproot family of changes:\n> \n> 1) A separate soft-fork for Merkle Branch Witnesses based on Taproot;\n> \n> 2) A separate soft-fork for Schnorr Signatures\n> \n> 3) A separate follow up soft-fork which enables Taproot and Graftroot\n> \n> We think that the first 2 forks can be offered at the same time or one at a\n> time.\n> \n> Taproot, as a follow up to changes 1 and 2, can be enabled as a soft-fork\n> on the existing semantics, but requiring a new witness version. With the\n> Public NUMS Optimization, wallets could upgrade by just changing one\n> version byte to be in the same anonymity set as Taproot.\n> \n> It's not clear to us that the time to prepare a BIP and implementation for\n> 1 and 2 at this point would be any less than the time to do Taproot as\n> currently proposed. However, we believe that such a deployment plan is a\n> reasonable option as it is more conservative, as Merkle Branch witnesses\n> are relatively simple and users only have to use Schnorr signing if they\n> want to, and can otherwise continue to use ECDSA. A further benefit of\n> waiting on 3 is that we get to collect real world protocol engineering\n> experience to see how frequently the Taproot frequency of use assumption\n> holds, and if it is worth doing or not.\n> \n> \n> Great thanks,\n> \n> The Group\n> \n> \n> ----\n> \n> SUBJECT: Taproot Public NUMS Optimization\n> \n> This email is the third of a collection of sentiments from a group of\n> developers who in aggregate prefer to remain anonymous. These emails have\n> been sent under a pseudonym so as to keep the focus of discussion on the\n> merits of the technical issues, rather than miring the discussion in\n> personal politics. Our goal isn't to cause a schism, but rather to help\n> figure out what the path forward is with Taproot. To that end, we: [clipped\n> again]\n> \n> We propose to modify Taproot's specification in BIP-341 by adding the rule:\n> \n> If there is one element on the witness stack:\n> \n> 1) Attempt hashing it to see if it's equal to  the witness program. The\n> first byte is the control byte for leaf versioning.\n> \n> 2) If it's not the witness program, and it's 65 bytes, try signature\n> validation\n> \n> If there is more than one element on the witness stack:\n> \n> If the control block is even, treat it as a non-Taproot MAST and get the\n> leaf version as the last byte of the script (so you can pop it off before\n> hashing).\n> \n> \n> If greater anonymity is required, a NUMS point can still be used in\n> Taproot, at the expense of the additional data. However, if NUMS points are\n> just a couple well known constants this could actually decrease privacy as\n> then the NUMS points could differ from application to application\n> fingerprinting wallets.  Instead, the NUMS point should only be used when a\n> single use nonce can be sent, so that NUMS cannot be distinguished from a\n> normal Taproot to a third party who doesn't know the setup (e.g., that the\n> NUMS is H(X) for known X).\n> \n> \n> Great thanks,\n> \n> The Group\n> \n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Jeremy",
                "date": "2020-02-14T20:07:15",
                "message_text_only": "Dave,\n\nI think your point:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n*When schnorr and taproot are done together, all of the following\ntransaction types can be part of the same set:     - single-sig spends\n(similar to current use of P2PKH and P2WPKH)     - n-of-n spends with musig\nor equivalent (similar to current use of       P2SH and P2WSH 2-of-2\nmultisig without special features as used by       Blockstream Green and LN\nmutual closes)     - k-of-n (for low values of n) using the most common k\nsigners       (similar to BitGo-style 2-of-3 where the keys involved are\n    alice_hot, alice_cold, and bob_hot and almost all transactions are\n  expected to be signed by {alice_hot, bob_hot}; that common case       can\nbe the key-path spend and the alternatives {alice_hot,       alice_cold}\nand {alice_cold, bob_hot} can be script-path spends)     - contract\nprotocols that can sometimes result in all parties       agreeing on an\noutcome (similar to LN mutual closes, cross-chain       atomic swaps, and\nsame-chain coinswaps) *\n\nIs the same if Schnorr + Merkle Branch without Taproot optimization, unless\nI'm missing something in one of the cases? I guess there's a distinction on\n\"can\" v.s. \"are likely\"?\n\n\nJonas,\n\nThat's a really interesting point about K-N systems making the most likely\nK-K the taproot key. (For the uninitiated, MuSig can do N-of-N aggregation\nnon-interactively, but K-of-N requires interaction). I think this works\nwith small (N choose K), but as (N choose K) increases it seems the\nprobability of picking the correct one goes down?\n\nI guess the critical question is if cases where there's not some timelock\nwill be mandatory across all signing paths.\n\n\ncheers,\n\njeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n\n\nOn Mon, Feb 10, 2020 at 9:16 AM Jonas Nick via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> I agree with most of the comments so far, but the group brings up an often\n> overlooked point with respect to the privacy benefits of taproot. In the\n> extreme\n> case, if there would be no policies that have both a key and a script spend\n> path, then taproot does not improve anonymity sets compared to the \"Taproot\n> Public NUMS Optimization\" proposal (which saves 8 vbytes in a\n> script-spend). (*)\n>\n> In fact, the cases where scripts would have to be used given usage of\n> Bitcoin\n> today are be rare because threshold policies, their conjunctions and\n> disjunctions can be expressed with a single public key. Even if we\n> disregard\n> speculation that timelocks, ANYPREVOUT/NOINPUT and other interesting\n> scripts\n> will be used in the future (which can be added through the leaf or key\n> versions\n> without affecting key-spend anonymity sets), not all of today's\n> applications are\n> able to be represented single public keys because there are applications\n> that\n> can not deal with interactive key setups or interactive signing. For\n> applications where this is possible it will be a gradual change because of\n> the\n> engineering challenges involved. For example, k-of-n threshold policies\n> could\n> have the most likely k-of-k in the taproot output key and other k-of-k in\n> the\n> leaves, instead of going for a k-of-n taproot output key immediately.\n>\n> Given that anonymity sets in Bitcoin are permanent and software tends to be\n> deployed longer than anyone would expect at the time of deployment,\n> realistically Taproot is superior to the \"Public NUMS Optimization\" and \"An\n> Alternative Deployment Path\".\n>\n> (*) One could argue that the little plausible deniability gained by a very\n> small\n> probability of the change of a script-spend being a key-spend and vice\n> versa is\n> significantly better than no probability at all.\n>\n> On 2/9/20 8:47 PM, Bryan Bishop via bitcoin-dev wrote:\n> > Apologies for my previous attempt at relaying the message- it looks like\n> > the emails got mangled on the archive. I am re-sending them in this\n> > combined email with what I hope will be better formatting. Again this is\n> > from some nym that had trouble posting to this mailing list; I didn't see\n> > any emails in the queue so I couldn't help to publish this sooner.\n> >\n> > SUBJECT: Taproot (and Graftroot) Complexity\n> >\n> > This email is the first of a collection of sentiments from a group of\n> > developers who in aggregate prefer to remain anonymous. These emails have\n> > been sent under a pseudonym so as to keep the focus of discussion on the\n> > merits of the technical issues, rather than miring the discussion in\n> > personal politics.  Our goal isn't to cause a schism, but rather to help\n> > figure out what the path forward is with Taproot. To that end, we:\n> >\n> > 1) Discuss the merits of Taproot's design versus simpler alternatives\n> (see\n> > thread subject, \"Taproot (and Graftroot) Complexity\").\n> >\n> > 2) Propose an alternative path to deploying the technologies described in\n> > BIP-340, BIP-341, and BIP-342 (see thread subject, \"An Alternative\n> > Deployment Path for Taproot Technologies\").\n> >\n> > 3) Suggest a modification to Taproot to reduce some of the overhead (see\n> > thread subject, \"Taproot Public NUMS Optimization\").\n> >\n> > Now that the BIP has moved to draft we felt that now was the time to\n> > prioritize review to make sure it was an acceptable change for our\n> > activities. As a group, we're excited about the totality of what Taproot\n> > has to offer. However, after our review, we're left perplexed about the\n> > development of Taproot (and Graftroot, to a lesser extent).\n> >\n> > We also want to convey that we have nothing but respect for the\n> developers\n> > and community who have poured their heart and soul into preparing\n> Taproot.\n> > Self evidently, it is an impressive synthesis of ideas. We believe that\n> the\n> > highest form of respect to pay such a synthesis of ideas is a detailed\n> and\n> > critical review, as it's pertinent to closely consider changes to\n> Bitcoin.\n> >\n> >\n> > In essence, Taproot is fundamentally the same as doing\n> > https://github.com/bitcoin/bips/blob/master/bip-0114.mediawiki and\n> Schnorr\n> > signatures separately.\n> >\n> > The main reason for putting them together -- as mentioned in the BIP --\n> is\n> > a gain in efficiency. But this efficiency pre-supposes a specific use\n> case\n> > and probability distribution of use cases.\n> >\n> > Compare:\n> >\n> > Suppose a MAST for {a,b,c,d,e,f,g,h} spending conditions it looks\n> something\n> > like this:\n> >\n> >\n> >       /\\\n> >      /  \\\n> >     /    \\\n> >    /      \\\n> >   /\\      /\\\n> >  /  \\    /  \\\n> > /\\  /\\  /\\  /\\\n> > a b c d e f g h\n> >\n> > If we want this to be functionally equivalent to Taproot, we add a new\n> path:\n> >\n> >        /\\\n> >       /\\ {<pk> schnorr_checksig}\n> >      /  \\\n> >     /    \\\n> >    /      \\\n> >   /\\      /\\\n> >  /  \\    /  \\\n> > /\\  /\\  /\\  /\\\n> > a b c d e f g h\n> >\n> > Now, to spend from this MBV you have to reveal 32 bytes on the stack for\n> > the not taken branch, and 35 bytes for the <pk> schnorr_checksig (1 byte\n> > push, 33 bytes PK, 1 byte checksig).\n> >\n> > This is 67 bytes more than Taproot would require for the same spending\n> > condition.\n> >\n> > However, suppose we wanted to use one of the script paths instead. We\n> still\n> > need to have one extra hash for the {<pk> schnorr_checksig} (depending on\n> > if we put the key in this position or not--see below). But now we can\n> spend\n> > with just a logarithmic control program path.\n> >\n> > However, if we do the same script via taproot, we now need to provide the\n> > base public key (33 bytes) as well as the root hash (32 bytes) and path\n> and\n> > then the actual scripts. With the need for 2 push bytes, this ends up\n> being\n> > back at 67 bytes extra.\n> >\n> > Is Taproot just a probability assumption about the frequency and\n> likelihood\n> > of the signature case over the script case? Is this a good assumption?\n> The\n> > BIP only goes as far as to claim that the advantage is apparent if the\n> > outputs *could be spent* as an N of N, but doesn't make representations\n> > about how likely that N of N case would be in practice compared to the\n> > script paths. Perhaps among use cases, more than half of the ones we\n> expect\n> > people to be doing could be spent as an N of N. But how frequently would\n> > that path get used? Further, while the *use cases* might skew toward\n> things\n> > with N of N opt-out, we might end up in a power law case where it's the\n> one\n> > case that doesn't use an N of N opt out at all (or at a de minimis level)\n> > that becomes very popular, thereby making Taproot more costly then\n> > beneficial.\n> >\n> > Further, if you don't want to use a Taproot top-level key (e.g., you need\n> > to be able to audit that no one can spend outside of one of the script\n> > conditions), then you need to use a NUMS (nothing up my sleeve) point.\n> This\n> > forces users who don't want Taproot to pay the expense, when if they just\n> > had a MAST based witness type they would be cheaper. So if this use case\n> is\n> > at all common, Taproot leaves them worse off in terms of fees. Given that\n> > script paths are usually done in the case where there is some contested\n> > close, it's actually in the interest of protocol developers that the\n> > contested script path be as efficient as possible so that the fees paid\n> > maximally increase the feerate. We think this can be fixed simply in\n> > Taproot though, as noted below.\n> >\n> >\n> >\n> > On privacy, we're also a bit confused as to the goal of Taproot over MAST\n> > and Schnorr. Earlier, we presented a design with MAST which is very close\n> > to Taproot.  However, it'd also be possible to just add {<pk>\n> > schnorr_checksig} to the set {a,b,c,d,e,f,g,h}, shuffle them, and compute\n> > some MAST structure (perhaps probability encoded) on them. This has the\n> > effect of not having much additional fees for adding the extra Schnorr\n> path\n> > at redeem time (only 1 extra branch on 2/8 script paths), e.g.\n> >\n> >\n> >       /\\\n> >      /  \\\n> >     /    \\\n> >    /      \\\n> >   /\\      /\\\n> >  /  \\    /  \\\n> > /\\  /\\  /\\  /\\\n> > a b c d e f/\\ {<pk> schnorr_checksig}\n> >           g  h\n> >\n> > We could argue that this is more private than Taproot, because we don't\n> > distinguish between the Schnorr key case and other cases by default, so\n> > chain analyzers can't tell if the signature came from the Taproot case or\n> > from one of the Script paths. There's also no NUMS point required, which\n> > means chain analyzers can't tell when you spend that there was no top\n> level\n> > key if the NUMS point is not per-output indistinguishable. By using a\n> > semi-randomized MAST structure, chain analyzers also can't tell exactly\n> how\n> > big your spend condition MAST was. In particular, you care more about\n> > privacy when you are contesting a close of a channel or other script path\n> > because then the miners could be more likely to extract a rent from you\n> as\n> > \"ransom\" for properly closing your channel (or in other words, in a\n> > contested close the value of the closing transaction is larger than\n> usual).\n> >\n> > It would also be possible to do something really simple which is to allow\n> > the witness type to be either a MAST hash OR a schnorr key (but not a\n> > Taproot). This allows you to not completely fracture the anonymity set\n> > between people who want plain Schnorr and people who want MAST (at least\n> > until they go to spend). This fix can also be used in Taproot in place\n> of a\n> > NUMS point, to decrease extra fees. It's unclear if this plays negatively\n> > with any future batch validation mechanism though, but the contextual\n> > checks to exclude a witness program from the batch are relatively simple.\n> > See thread subject, \"Taproot Public NUMS Optimization\".\n> >\n> > The considerations around Graftroot, a proposed delegation mechanism, is\n> a\n> > bit similar. Delegation is a mechanism by which a UTXO with script S can\n> > sign a script R which can then be executed in addition to S without\n> > requiring a transaction. This allows an output to monotonically and\n> > dynamically increase the number of conditions under which it can be\n> spent.\n> > As noted by Pieter Wiulle here:\n> >\n> https://github.com/kanzure/diyhpluswiki/commit/a03f6567d714f8733b578de263a4b149441cd058\n> > delegation was originally possible in Bitcoin, but got broken during an\n> > emergency fork to split the scriptSig and scriptpubkey separation. Rather\n> > than adding some fancy delegation mechanism in Bitcoin, why not just\n> have a\n> > P2SH-like semantic which allows a delegated script to be evaluated? See\n> > BIP-117 https://github.com/bitcoin/bips/blob/master/bip-0117.mediawiki.\n> > This way we aren't special casing where delegation can occur, and we can\n> > allow taproot nested spending conditions (i.e., with timelocks) to\n> generate\n> > their own delegations. As I've seen Graftroot discussed thus far, it is\n> as\n> > a top-level witness program version like Taproot and non-recursive.\n> Similar\n> > to the above discussion, top-level is more efficient if you suspect that\n> > delegation will be most likely occurring at the top level, but it's not\n> > clear that's a good assumption as it may be common to want to allow\n> > different scripts to delegate.\n> >\n> >\n> > Overall, we are left with concerns both about the merit of doing Taproot\n> > versus alternatives, as well as the process through which we got to be\n> here.\n> >\n> > 1) Is Taproot actually more private than bare MAST and Schnorr\n> separately?\n> > What are the actual anonymity set benefits compared to doing the\n> separately?\n> >\n> > 2) Is Taproot actually cheaper than bare MAST and Schnorr separately?\n> What\n> > evidence do we have that the assumption it will be more common to use\n> > Taproot with a key will outweigh Script cases?\n> >\n> > 3) Is Taproot riskier than bare MAST and Schnorr separately given the new\n> > crypto? How well reviewed is the actual crypto parts? None of us\n> personally\n> > feel comfortable reviewing the crypto in Schnorr -- what's the set of\n> > people who have thoroughly reviewed the crypto and aren't just ACKing\n> > because they trust other developers to have looked at it close enough?\n> >\n> > 4) Design wise, couldn't we forego the NUMS point requirement and be able\n> > to check if it's a hash root directly? This would encumber users who\n> don't\n> > need the key path a cheaper spend path. See thread subject, \"Taproot\n> Public\n> > NUMS Optimization\".\n> >\n> > 5) Is the development model of trying to jam a bunch of features into\n> > Bitcoin all at once good for Bitcoin development? Would we be better off\n> if\n> > we embraced incremental improvements that can work together (e.g., MAST\n> and\n> > then Schnorr)?  Although the BIP raises some points about anonymity sets\n> > being why to do them all at once, it's not clear to me this argument\n> holds\n> > water (same goes for businesses not upgrading). If we can take things as\n> > smaller steps, we are not only more secure, but we also have more time to\n> > dedicate review to each change independently. We also end up co-mingling\n> > changes that people end up accepting only because they want one and\n> they're\n> > bundled (e.g., MAST and Schnorr, MAST seems like a much less risky\n> addition\n> > versus Schnorr). See thread subject, \"An Alternative Deployment Path for\n> > Taproot Technologies\".\n> >\n> >\n> >\n> >\n> > Our provocation with this email is primarily that we think we should more\n> > carefully consider the benefits of Taproot over simpler primitives that\n> are\n> > not only easier to review, but could have been made available much sooner\n> > rather than waiting on putting everything all together for an unclear\n> > aggregate benefit.\n> >\n> > We do think that most of the developers have been honest about the\n> benefits\n> > of Taproot, but that on closer look we feel the general ecosystem has\n> > oversold Taproot as being the key enabler for a collection of techniques\n> > that we could do with much simpler building blocks.\n> >\n> >\n> > At the end of the day, we do not strongly advocate not deploying Taproot\n> at\n> > this point in the review cycle. We think the Taproot Public NUMS\n> > Optimization may be a good idea, worth considering if it's not insecure,\n> as\n> > it cuts through the case where you would otherwise need a NUMS point.\n> > Things like TapScript and its MAST mechanisms are well designed and offer\n> > exciting new deployment paths, and would be something we would use even\n> if\n> > we opted for MAST instead of Taproot. However, we also believe it is our\n> > duty to raise these concerns and suggestions, and we look forward to\n> > listening to the responses of the community.\n> >\n> > Great thanks,\n> >\n> > The Group\n> >\n> > ----\n> >\n> > SUBJECT: An Alternative Deployment Path for Taproot Technologies\n> >\n> > This email is the second of a collection of sentiments from a group of\n> > developers who in aggregate prefer to remain anonymous. These emails have\n> > been sent under a pseudonym so as to keep the focus of discussion on the\n> > merits of the technical issues, rather than miring the discussion in\n> > personal politics. Our goal isn't to cause a schism, but rather to help\n> > figure out what the path forward is with Taproot. To that end, we: [clip\n> > repeat]\n> >\n> > As a follow up to our prior message, we propose a different path forward\n> > for the Taproot family of changes:\n> >\n> > 1) A separate soft-fork for Merkle Branch Witnesses based on Taproot;\n> >\n> > 2) A separate soft-fork for Schnorr Signatures\n> >\n> > 3) A separate follow up soft-fork which enables Taproot and Graftroot\n> >\n> > We think that the first 2 forks can be offered at the same time or one\n> at a\n> > time.\n> >\n> > Taproot, as a follow up to changes 1 and 2, can be enabled as a soft-fork\n> > on the existing semantics, but requiring a new witness version. With the\n> > Public NUMS Optimization, wallets could upgrade by just changing one\n> > version byte to be in the same anonymity set as Taproot.\n> >\n> > It's not clear to us that the time to prepare a BIP and implementation\n> for\n> > 1 and 2 at this point would be any less than the time to do Taproot as\n> > currently proposed. However, we believe that such a deployment plan is a\n> > reasonable option as it is more conservative, as Merkle Branch witnesses\n> > are relatively simple and users only have to use Schnorr signing if they\n> > want to, and can otherwise continue to use ECDSA. A further benefit of\n> > waiting on 3 is that we get to collect real world protocol engineering\n> > experience to see how frequently the Taproot frequency of use assumption\n> > holds, and if it is worth doing or not.\n> >\n> >\n> > Great thanks,\n> >\n> > The Group\n> >\n> >\n> > ----\n> >\n> > SUBJECT: Taproot Public NUMS Optimization\n> >\n> > This email is the third of a collection of sentiments from a group of\n> > developers who in aggregate prefer to remain anonymous. These emails have\n> > been sent under a pseudonym so as to keep the focus of discussion on the\n> > merits of the technical issues, rather than miring the discussion in\n> > personal politics. Our goal isn't to cause a schism, but rather to help\n> > figure out what the path forward is with Taproot. To that end, we:\n> [clipped\n> > again]\n> >\n> > We propose to modify Taproot's specification in BIP-341 by adding the\n> rule:\n> >\n> > If there is one element on the witness stack:\n> >\n> > 1) Attempt hashing it to see if it's equal to  the witness program. The\n> > first byte is the control byte for leaf versioning.\n> >\n> > 2) If it's not the witness program, and it's 65 bytes, try signature\n> > validation\n> >\n> > If there is more than one element on the witness stack:\n> >\n> > If the control block is even, treat it as a non-Taproot MAST and get the\n> > leaf version as the last byte of the script (so you can pop it off before\n> > hashing).\n> >\n> >\n> > If greater anonymity is required, a NUMS point can still be used in\n> > Taproot, at the expense of the additional data. However, if NUMS points\n> are\n> > just a couple well known constants this could actually decrease privacy\n> as\n> > then the NUMS points could differ from application to application\n> > fingerprinting wallets.  Instead, the NUMS point should only be used\n> when a\n> > single use nonce can be sent, so that NUMS cannot be distinguished from a\n> > normal Taproot to a third party who doesn't know the setup (e.g., that\n> the\n> > NUMS is H(X) for known X).\n> >\n> >\n> > Great thanks,\n> >\n> > The Group\n> >\n> >\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> >\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200214/5b6e9b27/attachment-0001.html>"
            },
            {
                "author": "David A. Harding",
                "date": "2020-02-14T22:36:42",
                "message_text_only": "On Fri, Feb 14, 2020 at 12:07:15PM -0800, Jeremy via bitcoin-dev wrote:\n> Is the same if Schnorr + Merkle Branch without Taproot optimization, unless\n> I'm missing something in one of the cases? \n\nThat's fair.  However, it's only true if everyone constructs their\nmerkle tree in the same way, with a single `<schnorr_pk> OP_CHECKSIG` as\none of the top leaves.   Taproot effectively standardizes the position\nof the all-parties-agree condition and so its anonymity set may contain\nspends from scripts whose creators buried or excluded the the all-agree\noption because they didn't think it was likely to be used.\n\nMore importantly, there's no incentive for pure single-sig users to use a\nmerkle tree, since that would make both the scriptPubKey and the witness\ndata are larger for them than just continuing to use v0 segwit P2WPKH.\nGiven that single-sig users represent a majority of transactions at\npresent (see AJ Towns's previous email in this thread), I think we\nreally want to make it as convenient as possible for them to participate\nin the anonymity set.\n\n(To be fair, taproot scriptPubKeys are also larger than P2WPKH\nscriptPubKeys, but its witness data is considerably smaller, giving\nreceivers an incentive to demand P2TR payments even if spenders don't\nlike paying the extra 12 vbytes per output.)\n\nRough sums:\n\n- P2WPKH scriptpubkey (22.00 vbytes): `OP_0 PUSH20 <hash160>`\n- P2WPKH witness data (26.75): `size(72) <sig>, size(33) <pubkey>`\n- P2TR scriptpubkey (34.00): `OP_1 PUSH32 <pubkey>`\n- P2TR witness data (16.25): `size(64) <sig>`\n- BIP116 MBV P2WSH scriptpubkey (34.00): `OP_0 PUSH32 <sha256>`\n- BIP116 MBV P2WSH witness data (42.00): `size(64) <signature>, size(32)\n  <pubkey>, size(32) <inclusion_proof>, size(36) <PUSH1 <n> PUSH32\n  <merkle_root> OP_MBV>`\n\n-Dave\n\nP.S. I think this branch of the thread is just rehashing points that\n     were originally covered over two years ago and which haven't really\n     changed since then.  E.g.:\n\n    https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-January/015629.html\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200214/bc1102fd/attachment-0001.sig>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2020-02-18T23:29:21",
                "message_text_only": "On Fri, 14 Feb 2020 at 14:37, David A. Harding via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> On Fri, Feb 14, 2020 at 12:07:15PM -0800, Jeremy via bitcoin-dev wrote:\n> > Is the same if Schnorr + Merkle Branch without Taproot optimization, unless\n> > I'm missing something in one of the cases?\n>\n> That's fair.  However, it's only true if everyone constructs their\n> merkle tree in the same way, with a single `<schnorr_pk> OP_CHECKSIG` as\n> one of the top leaves.   Taproot effectively standardizes the position\n> of the all-parties-agree condition and so its anonymity set may contain\n> spends from scripts whose creators buried or excluded the the all-agree\n> option because they didn't think it was likely to be used.\n>\n> More importantly, there's no incentive for pure single-sig users to use a\n> merkle tree, since that would make both the scriptPubKey and the witness\n> data are larger for them than just continuing to use v0 segwit P2WPKH.\n> Given that single-sig users represent a majority of transactions at\n> present (see AJ Towns's previous email in this thread), I think we\n> really want to make it as convenient as possible for them to participate\n> in the anonymity set.\n\nRight, I think we shouldn't just see Taproot as adding a possibility\nof a cheap single-key branch to Merkle tree. It is actively choosing\nto incentivize protocols and implementations that can use the key\npath, making sure that the cheapest way spending of spending is also\nthe most private one - as we can assume that it indeed will be the\nmost frequent one. I don't believe having a separate MAST-no-Taproot\nspending type (through whatever mechanism) is beneficial to that.\nTaproot effectively gives everyone a \"key path spend is included in\nthe price\", making it maximally appealing even to those who don't care\nabout privacy.\n\nI don't think this is an unreasonable angle. There are plenty of other\noptions that exists if we just want to make verification constructions\ncheap but disregard incentives for privacy. For example, why don't we\nresearch account-based state/payments? Being able to avoid change\nwould make simple payments significantly cheaper (both in terms of\nblock space and computation). Of course, the reason (or at least one\nof them) is that it would result in a discount for those willing to\nreduce their privacy (use accounts = reuse address = don't pay for\nchange), and this hurts everyone (and indirectly the fungibility of\nthe system as a whole). This is true even when there are use cases\nthat would legitimately benefit from having this option.\n\nThis is of course a much weaker case, but I think it is similar.\nHaving no-Taproot available would reduce the incentives for those who\ndon't care about spending policy privacy to put the engineering effort\ninto building key-path-spendable protocols and implementations - and I\nthink such protocols, wherever possible, should be the goal. There\nprobably are some use cases where key path spending is truly not an\noption, but I suspect they're rare, or sufficiently high value that 8\nvbyte differences don't matter to them. If that turns out to be wrong,\nit remains possible to add a new output type/witness version that does\nsupport them. This does mean distinguishable outputs, but only for\nthings that would be distinguishable at spending time anyway, and\nthat's a cost we'll have to pay anyway for future structural script\nimprovements (like cross-input aggregation or graftroot).\n\nCheers,\n\n-- \nPieter"
            }
        ],
        "thread_summary": {
            "title": "Taproot (and graftroot) complexity (reflowed)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Bryan Bishop",
                "Jeremy",
                "David A. Harding",
                "Pieter Wuille",
                "Jonas Nick"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 64027
        }
    },
    {
        "title": "[bitcoin-dev] BIP OP_CHECKTEMPLATEVERIFY",
        "thread_messages": [
            {
                "author": "Dmitry Petukhov",
                "date": "2020-02-14T11:18:26",
                "message_text_only": "I decided to take this thread back on-list because I beleive that the\n'revocation utxo' feature enabled by OP_CTV commiting to scriptSig may\nhave wider implications that can slightly change the behavior of Bitcoin\nas a system, and some might not expect such changes or might not find\nthem desireable (although there is already a case for such behaviour\nwith RBF).\n\nThere is a principle that some find valuable: \"During reorgs of depth\nless than 100, it is always possible to eventually replay transactions\nfrom the old branch into the new branch as long as no double spends are\nattempted\" (quoted from Russel O'Connor from the discussion about\n'revocation utxo' on Elements Slack channel).\n\nAs far as I can tell, this principle can be violated with the use of\nRBF: \"(tx) that was included in branch A and then RBF-ed (tx') in branch\nB and then branch A wins -> children of (tx') can't be replayed\"\n\nSome may hold an opinion that introducing new rules that violate that\nprinciple should be done with caution.\n\nThe 'revocation utxo' feature enabled by OP_CTV essentially introduces\na manually triggered 'inverse timelock' -  normal timelocks make tx\ninvalid until certain point in time, and inverse timelock make tx\ninvalid _after_ certain point in time, in this case by spending an\nunrelated UTXO.\n\nIn a reorg, one branch can have that UTXO spent before the OP_CTV\ntransaction that depends on it is included in the block, and the OP_CTV\ntransaction and its children can't be replayed.\n\nThis is the same issue as an 'automatic inverse timelock' that could\nbe enforced by the structure of the transaction itself, if there was\nappropriate mechanism, with the difference that 'revocation utxo' is\nmanually triggered.\n\nThe absense of 'automatic inverse timelock' mechanism in Bitcoin hints\nthat it was not seen as desireable historically. I was not able to find\nthe relevant discussions, though.\n\nI would like to add that the behaviour enabled by inverse timelocks\ncould be useable in various schemes with covenants, like the vaults\nwith access revocable by spending the 'revocation utxo', or in the\ntrustless lending schemes where the covenant scripts can enforce\ndifferent amounts of interest paid to lender based on the point in time\nwhen the loan is returned - the obsolete script paths (with smaller\ninterest paid) can be disabled by inverse timelock.\n\n\u0412 Fri, 13 Dec 2019 23:37:19 -0800\nJeremy <jlrubin at mit.edu> wrote:\n\n> That's a cool use case. I've thought previously about an\n> OP_CHECKINPUT, as a separate extension. Will need to think about if\n> your construction introduces a hash cycle (unless\n> SIGHASH_ALL|SIGHASH_ANYONECANPAY is used it seems likely).\n> \n> Also re signatures I think it's definitely possible to pick a\n> (signature, message) pair and generate a pk from it, but in general\n> the Bitcoin message commits to the pk so forging isn't possible.\n> \n> On Fri, Dec 13, 2019, 11:25 PM Dmitry Petukhov <dp at simplexum.com>\n> wrote:\n> \n> > Another idea for smart vaults:\n> >\n> > The ability to commit to scriptSig of a non-segwit input could be\n> > used for on-chain control of spending authorization (revoking the\n> > spending authorization), where CTV ensures that certain input is\n> > present in the transaction.\n> >\n> > scriptSig of that input can contain a signature that commits to\n> > certain prevout. Unless it is possible to forge an identical\n> > signature (and I don't know how strong are guarantees of that),\n> > such an input can only be valid if that prevout was not spent.\n> >\n> > Thus spending such prevout makes it impossible to spend the input\n> > with CTV that commits to such scriptSig, in effect revoking an\n> > ability to spend this input via CTV path, and alternate spending\n> > paths should be used (like, another taproot branch)\n> >\n> >\n> > \u0412 Fri, 13 Dec 2019 15:06:59 -0800\n> > Jeremy via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>\n> > \u043f\u0438\u0448\u0435\u0442: \n> > > I've prepared a draft of the changes noted above (some small\n> > > additional modifications on the StandardTemplateHash described in\n> > > the BIP), but have not yet updated the main branches for the BIP\n> > > to leave time for any further feedback.\n> > >\n> > > See below:\n> > >\n> > > BIP:\n> > > https://github.com/JeremyRubin/bips/blob/ctv-v2/bip-ctv.mediawiki\n> > > Implementation:\n> > > https://github.com/JeremyRubin/bitcoin/tree/checktemplateverify-v2\n> > >\n> > > Thank you for your feedback,\n> > >\n> > > Jeremy\n> > > --\n> > > @JeremyRubin <https://twitter.com/JeremyRubin>\n> > > <https://twitter.com/JeremyRubin>  \n> >\n> >"
            },
            {
                "author": "Jeremy",
                "date": "2020-02-14T19:16:26",
                "message_text_only": "Hi Dmitry,\n\nI don't think that this is fundamentally introducing new behavior, but\nlet's take a closer look.\n\nWe can talk about the issue you bring up purely in terms of a hypothetical\n\"OP_CHECKINPUTOUTPOINTVERIFY\" and \"OP_CHECKINPUTSCRIPTVERIFY\" (CIOV, CISV)\nwith obvious implied by name semantics, as a separate construct from CTV\nitself. Such opcodes would be strictly more powerful/flexible than what CTV\nis enabling.\n\nUsing these opcodes I can make an output that can *only* be spent with\nanother output -- e.g.,\n\n<s> <n> OP_CISV OP_DROP <pk> OP_CHECKSIGVERIFY\n<h, i> <n> OP_CIOV OP_DROP <pk> OP_CHECKSIGVERIFY\n\nLet's look at CISV first:\n\n1) Assume that <s> is from the same owner as PK\n2) Assume that <s> is from a different owner than PK\n\nIn case 1, the wallet can create or recreate the appropriate output as\nneeded if it gets spent/stuck\n\nIn case 2, the wallet can get \"frozen\" in a reorg until a signer on <s>\nre-spends.\n\n\nFor CIOV:\n\n1) Assume that <h, i> exists in the chain somewhere\n2) Assume that <h, i> exists in the mempool somewhere\n3) Assume that <h, i> does not exist (or, is provably non-creatable -- h =\ntxid(x) | x.IsValid() == false)\n\nIn case 2, this is just a fancy op-return.\n\nCase 1 degrades into case 2 in the event of a reorg.\n\nIn Case 2, if the output <h, i> is spent in another transaction, our script\nbecomes provably unspendable (unless a second reorg).\n\nOtherwise, it is possible to mine a block with our transaction.\n\n\nCompare the above to normal transactions:\n\n1) If a reorg occurs, and someone double-spends, your transaction gets\ncancelled.\n2) You can re-sign your UTXO onto a different transaction\n\nHowever, if you have deleted your key (e.g. using a pre-signing HSM), or\nyour transaction was using a multi-sig with an uncooperating party, you\nwill have an output that may be effectively burned.\n\nThese issues are -- as with CTV -- not present in the single input use case.\n\nThus I argue that CTV -- whose semantics are less powerful/flexible than\nCISV/CIOV -- aren't introducing something that's not already present when\ndoing protocols involving more than one input.\n\nFurther, on CTV \"monotonic authorization\":\n\nGenerally we want Bitcoin Scripts to have the property that once a\ncondition is reached, it is 'permanently' a true case. E.g., showing a hash\npreimage to C x, H(x) == C. This can't change with the weather or anything\nelse. Even things like timelocks -- although not obvious at first glance --\nhave this property. They express logic that says \"given the chain is at\nthis height, ...\". This means that on any chain at such a height the txn is\nvalid. CISV/CIOV semantics also fall in line with this description. It\nsays, \"given such an input U, ...\". If that input is realizable one time,\nit is provably realizable across reorgs. However, that doesn't mean someone\ncouldn't interrupt U from being created. But generally, with Reorg + Double\nspend, or Reorg > 100 blocks (potentially destroying CB reward), all bets\nare off as to the replay-ability of transactions.\n\nI want to also point out that this \"revocation\" property -- to the extent\nit is something new that can't already be emulated with pre-signeds or RBF\n-- is entirely opt-in as far as CTV is concerned. You have to specify that\nan output can only be spent with another, most wallets shouldn't do that,\nand it can't \"infect\" other wallets to an extent more than spending from\nany recently confirmed output exposes you to more reorg risk.\n\n*In sum, we do not need to worry about this for CTV.*\n\n\nLastly, I want to note that revocation is part of what CTV is designed to\ndo (absent reorgs). It allows us to prune spending conditions by playing a\ntransaction forward.\n\nE.g., spending conditions {Alice & Bob, Preimage(H(X)) + Eve, CTV({Alice &\nBob}, 1 day)}\n\nExpresses that Eve has 1 day to reveal the preimage to H(X), otherwise\nAlice and Bob can take the coin back by removing Eve's HTLC path. What's\ncool about this revocation v.s. just {Alice & Bob, Preimage(H(X)) + Eve} is\nthat Alice and Bob don't need to coordinate a multisig to revoke Eve.\n\n\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n\n\nOn Fri, Feb 14, 2020 at 3:17 AM Dmitry Petukhov <dp at simplexum.com> wrote:\n\n> I decided to take this thread back on-list because I beleive that the\n> 'revocation utxo' feature enabled by OP_CTV commiting to scriptSig may\n> have wider implications that can slightly change the behavior of Bitcoin\n> as a system, and some might not expect such changes or might not find\n> them desireable (although there is already a case for such behaviour\n> with RBF).\n>\n> There is a principle that some find valuable: \"During reorgs of depth\n> less than 100, it is always possible to eventually replay transactions\n> from the old branch into the new branch as long as no double spends are\n> attempted\" (quoted from Russel O'Connor from the discussion about\n> 'revocation utxo' on Elements Slack channel).\n>\n> As far as I can tell, this principle can be violated with the use of\n> RBF: \"(tx) that was included in branch A and then RBF-ed (tx') in branch\n> B and then branch A wins -> children of (tx') can't be replayed\"\n>\n> Some may hold an opinion that introducing new rules that violate that\n> principle should be done with caution.\n>\n> The 'revocation utxo' feature enabled by OP_CTV essentially introduces\n> a manually triggered 'inverse timelock' -  normal timelocks make tx\n> invalid until certain point in time, and inverse timelock make tx\n> invalid _after_ certain point in time, in this case by spending an\n> unrelated UTXO.\n>\n> In a reorg, one branch can have that UTXO spent before the OP_CTV\n> transaction that depends on it is included in the block, and the OP_CTV\n> transaction and its children can't be replayed.\n>\n> This is the same issue as an 'automatic inverse timelock' that could\n> be enforced by the structure of the transaction itself, if there was\n> appropriate mechanism, with the difference that 'revocation utxo' is\n> manually triggered.\n>\n> The absense of 'automatic inverse timelock' mechanism in Bitcoin hints\n> that it was not seen as desireable historically. I was not able to find\n> the relevant discussions, though.\n>\n> I would like to add that the behaviour enabled by inverse timelocks\n> could be useable in various schemes with covenants, like the vaults\n> with access revocable by spending the 'revocation utxo', or in the\n> trustless lending schemes where the covenant scripts can enforce\n> different amounts of interest paid to lender based on the point in time\n> when the loan is returned - the obsolete script paths (with smaller\n> interest paid) can be disabled by inverse timelock.\n>\n> \u0412 Fri, 13 Dec 2019 23:37:19 -0800\n> Jeremy <jlrubin at mit.edu> wrote:\n>\n> > That's a cool use case. I've thought previously about an\n> > OP_CHECKINPUT, as a separate extension. Will need to think about if\n> > your construction introduces a hash cycle (unless\n> > SIGHASH_ALL|SIGHASH_ANYONECANPAY is used it seems likely).\n> >\n> > Also re signatures I think it's definitely possible to pick a\n> > (signature, message) pair and generate a pk from it, but in general\n> > the Bitcoin message commits to the pk so forging isn't possible.\n> >\n> > On Fri, Dec 13, 2019, 11:25 PM Dmitry Petukhov <dp at simplexum.com>\n> > wrote:\n> >\n> > > Another idea for smart vaults:\n> > >\n> > > The ability to commit to scriptSig of a non-segwit input could be\n> > > used for on-chain control of spending authorization (revoking the\n> > > spending authorization), where CTV ensures that certain input is\n> > > present in the transaction.\n> > >\n> > > scriptSig of that input can contain a signature that commits to\n> > > certain prevout. Unless it is possible to forge an identical\n> > > signature (and I don't know how strong are guarantees of that),\n> > > such an input can only be valid if that prevout was not spent.\n> > >\n> > > Thus spending such prevout makes it impossible to spend the input\n> > > with CTV that commits to such scriptSig, in effect revoking an\n> > > ability to spend this input via CTV path, and alternate spending\n> > > paths should be used (like, another taproot branch)\n> > >\n> > >\n> > > \u0412 Fri, 13 Dec 2019 15:06:59 -0800\n> > > Jeremy via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>\n> > > \u043f\u0438\u0448\u0435\u0442:\n> > > > I've prepared a draft of the changes noted above (some small\n> > > > additional modifications on the StandardTemplateHash described in\n> > > > the BIP), but have not yet updated the main branches for the BIP\n> > > > to leave time for any further feedback.\n> > > >\n> > > > See below:\n> > > >\n> > > > BIP:\n> > > > https://github.com/JeremyRubin/bips/blob/ctv-v2/bip-ctv.mediawiki\n> > > > Implementation:\n> > > > https://github.com/JeremyRubin/bitcoin/tree/checktemplateverify-v2\n> > > >\n> > > > Thank you for your feedback,\n> > > >\n> > > > Jeremy\n> > > > --\n> > > > @JeremyRubin <https://twitter.com/JeremyRubin>\n> > > > <https://twitter.com/JeremyRubin>\n> > >\n> > >\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200214/0329d6b6/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2020-02-15T00:24:37",
                "message_text_only": "Good morning Dmitry, and Jeremy,\n\n\n> There is a principle that some find valuable: \"During reorgs of depth\n> less than 100, it is always possible to eventually replay transactions\n> from the old branch into the new branch as long as no double spends are\n> attempted\" (quoted from Russel O'Connor from the discussion about\n> 'revocation utxo' on Elements Slack channel).\n>\n> As far as I can tell, this principle can be violated with the use of\n> RBF: \"(tx) that was included in branch A and then RBF-ed (tx') in branch\n> B and then branch A wins -> children of (tx') can't be replayed\"\n\nBut an RBF-ed transaction *is* a double-spend, and the principle makes an exception specifically for double-spends.\nThus RBF, and other double-spends, are exempt from this principle.\n\nMy vague understanding of the \"revocation UTXO\" feature is that it is implemented as a double-spend of a precommitted `OP_CTV`, so that also is exempted from the principle.\n\nAs Jeremy notes as well, the *actual* principle is that \"a script that is valid now remains valid in the future\" (as this is required by the script cache implementation of Bitcoin Core), and this principle does not mention UTXOs, only scripts; the existence or non-existence of a required UTXO is separate here.\nThus, an \"automatic inverse timelock\" is not possible to implement with **only** script (it implies that a script that is valid now will become invalid in the future), but requires some action on the blockchain (notice that HTLCs effectively implement an \"inverse timelock\" on the hash-branch participant, by threatening a spend (i.e. blockchain activity) by the counterparty if does not comply).\n\nRegards,\nZmnSCPxj\n\n\n\n>\n> Some may hold an opinion that introducing new rules that violate that\n> principle should be done with caution.\n>\n> The 'revocation utxo' feature enabled by OP_CTV essentially introduces\n> a manually triggered 'inverse timelock' - normal timelocks make tx\n> invalid until certain point in time, and inverse timelock make tx\n> invalid after certain point in time, in this case by spending an\n> unrelated UTXO.\n>\n> In a reorg, one branch can have that UTXO spent before the OP_CTV\n> transaction that depends on it is included in the block, and the OP_CTV\n> transaction and its children can't be replayed.\n>\n> This is the same issue as an 'automatic inverse timelock' that could\n> be enforced by the structure of the transaction itself, if there was\n> appropriate mechanism, with the difference that 'revocation utxo' is\n> manually triggered.\n>\n> The absense of 'automatic inverse timelock' mechanism in Bitcoin hints\n> that it was not seen as desireable historically. I was not able to find\n> the relevant discussions, though.\n>\n> I would like to add that the behaviour enabled by inverse timelocks\n> could be useable in various schemes with covenants, like the vaults\n> with access revocable by spending the 'revocation utxo', or in the\n> trustless lending schemes where the covenant scripts can enforce\n> different amounts of interest paid to lender based on the point in time\n> when the loan is returned - the obsolete script paths (with smaller\n> interest paid) can be disabled by inverse timelock.\n>\n> \u0412 Fri, 13 Dec 2019 23:37:19 -0800\n> Jeremy jlrubin at mit.edu wrote:\n>\n> > That's a cool use case. I've thought previously about an\n> > OP_CHECKINPUT, as a separate extension. Will need to think about if\n> > your construction introduces a hash cycle (unless\n> > SIGHASH_ALL|SIGHASH_ANYONECANPAY is used it seems likely).\n> > Also re signatures I think it's definitely possible to pick a\n> > (signature, message) pair and generate a pk from it, but in general\n> > the Bitcoin message commits to the pk so forging isn't possible.\n> > On Fri, Dec 13, 2019, 11:25 PM Dmitry Petukhov dp at simplexum.com\n> > wrote:\n> >\n> > > Another idea for smart vaults:\n> > > The ability to commit to scriptSig of a non-segwit input could be\n> > > used for on-chain control of spending authorization (revoking the\n> > > spending authorization), where CTV ensures that certain input is\n> > > present in the transaction.\n> > > scriptSig of that input can contain a signature that commits to\n> > > certain prevout. Unless it is possible to forge an identical\n> > > signature (and I don't know how strong are guarantees of that),\n> > > such an input can only be valid if that prevout was not spent.\n> > > Thus spending such prevout makes it impossible to spend the input\n> > > with CTV that commits to such scriptSig, in effect revoking an\n> > > ability to spend this input via CTV path, and alternate spending\n> > > paths should be used (like, another taproot branch)\n> > > \u0412 Fri, 13 Dec 2019 15:06:59 -0800\n> > > Jeremy via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org\n> > > \u043f\u0438\u0448\u0435\u0442:\n> > >\n> > > > I've prepared a draft of the changes noted above (some small\n> > > > additional modifications on the StandardTemplateHash described in\n> > > > the BIP), but have not yet updated the main branches for the BIP\n> > > > to leave time for any further feedback.\n> > > > See below:\n> > > > BIP:\n> > > > https://github.com/JeremyRubin/bips/blob/ctv-v2/bip-ctv.mediawiki\n> > > > Implementation:\n> > > > https://github.com/JeremyRubin/bitcoin/tree/checktemplateverify-v2\n> > > > Thank you for your feedback,\n> > > >\n> > > > Jeremy\n> > > >\n> > > > -------\n> > > >\n> > > > @JeremyRubin https://twitter.com/JeremyRubin\n> > > > https://twitter.com/JeremyRubin\n>\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "BIP OP_CHECKTEMPLATEVERIFY",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "Jeremy",
                "Dmitry Petukhov"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 19222
        }
    },
    {
        "title": "[bitcoin-dev] LN & Coinjoin, a Great Tx Format Wedding",
        "thread_messages": [
            {
                "author": "Antoine Riard",
                "date": "2020-02-21T22:17:54",
                "message_text_only": "Coinjoins interceptions seem to raise at an increasing pace. Their onchain\nfingerprint (high-number of inputs/outputs, lack of anti-fee snipping,\nscript\ntype, ...) makes their detection quite easy for a chain observer. A ban of\ncoinjoin'ed coins or any other coins linked through a common ownwer would\nundermine the long-term fungibility of the whole ecosystem.\n\nOf course, they do provide privacy for the participating coins but at the\ntradeoffs of creating two observable sets: coinjoin'ed vs non-coinjoin'ed.\nIdeally, all onchain transactions should conform to a common transaction\npattern that provides unobservability -- i.e a specific transaction would\nbe indistinguishable from any other transaction at all. For LN or Coinjoin\nit means an external observer, not-involved in the protocol, should be\nunable to tell which protocol is being used, or if _any_ specific protocol\nis being used.\n\nHow can a Bitcoin tranaction leak protocol usage ?\n* the output type (p2sh, p2wsh, ...)\n* the spending policy (2-of-3 multisig, timelock, hashlock,...)\n* outputs ordering (BIP69)\n* nLocktime/nSequence\n* RBF-signaling\n* Equal-value outputs\n* weird watermark (LN commitment tx obfuscated commitment number)\n* fees strategy like CPFP\n* in-protocol announcements [0]\n\nA solution could be to blur multiple protocol onchain transactions into\none common transaction format [1]. For example, if one of them uses\nnSequence\nfor some protocol semantic all the other ones should do it too. Any\ndeviation\nwould be enough to be leverage as a watermark and blow up all other tweaks.\nIf Schnorr-Taproot gets adopted and deployed by the community and LN\nspecifies\nan interactive tx construction protocol [2], the timing would be pretty good\nto adopt such format IMO.\n\nCoinjoin:\n* nSequence can be set, it's still secure if party don't resign [3]\n* nLocktime can be set for anti-fee snipping\n* Taproot spending\n\nLN (cooperative case):\n* splicing may blur funding/closing as the same thing, closing\naddress can be a funding output\n* splice-in would allow equal value outputs\n* nSequence likely to be set for multi-party tx construction\n* nLocktime can be set for anti-fee snipping\n\nAdopting a common transaction format isn't a cure-all solution\non the long-term privacy road but if it circumvent ban of some class\nof transactions that would be already a nice win and a worthy effort\nto do so.\n\nQuestions:\n* Are there any protocol-specific semantic wrt to onchain transactions\nincompatibility\nbetween Coinjoin and cooperative LN txn ?\n* What about RBF-by-default ?\n* Core wallet or any other protocol or even batching algorithms could adopt\nto this format ?\n* Is artificially increasing the number of outputs to mimic Coinjoins txn\nacceptable wrt to utxo bloat/fees ?\n\nCheers,\n\nAntoine\n\n[0] Like LN announcing public channels with signatures committing both\nto onchain utxos and nodes static pubkeys. And them being display on LN\nsearch engines with full owner info...\n\n[1] By format, I don't mean a *binary* format a la PSBT but mere something\nlike BOLT3, a *logical* format.\n\n[2]\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2020-February/002500.html\n\n[3] But \"blank\" RBF would be a privacy leak if Coinjoin are never bumped,\nbecause if you see both a low-fees and high-fees transaction you now know\nthey are a LN one, so Coinjoins implems should do some time spurious RBFs\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200221/671ebd47/attachment.html>"
            },
            {
                "author": "AdamISZ",
                "date": "2020-02-22T12:10:52",
                "message_text_only": "\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\nOn Friday, 21 February 2020 22:17, Antoine Riard via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> How can a Bitcoin tranaction leak protocol usage ?\n> * the output type (p2sh, p2wsh, ...)\n> * the spending policy (2-of-3 multisig, timelock, hashlock,...)\n> * outputs ordering (BIP69)\n> * nLocktime/nSequence\n> * RBF-signaling\n> * Equal-value outputs\n> * weird watermark (LN commitment tx obfuscated commitment number)\n> * fees strategy like CPFP\n> * in-protocol announcements [0]\n>\nGood list.\nAnother one, usually wouldn't be *protocol* as much as wallet leakage, but could be: utxo selection algorithm (which of course may be difficult to deduce, but often, far from impossible).\n(Also trivial and increasingly irrelevant, but nVersion).\n\nWith regards to coinjoin in this context (I know your points are much broader), my comment is:\nFor existing protocols (joinmarket's, wasabi's, samourai's), in the equal-outs paradigm, I don't see much that can be done in this area.\nBut I would ask people to consider CoinJoinXT[1] more seriously in a taproot/schnorr world, since it addresses this exact point. With a short (not cross-block like swaps or LN setup) interaction, participants can arrange the effect of coinjoin without the on-chain watermark of coinjoin (so, steganographic). The taproot/schnorr part is needed there because multisig is required from transaction to transaction in that protocol, so doing it today is less interesting (albeit still interesting).\n\nwaxwing\n\n[1] https://joinmarket.me/blog/blog/coinjoinxt/"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2020-02-23T00:59:46",
                "message_text_only": "Good morning waxwing,\n\n> \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n> On Friday, 21 February 2020 22:17, Antoine Riard via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:\n>\n> > How can a Bitcoin tranaction leak protocol usage ?\n> >\n> > -   the output type (p2sh, p2wsh, ...)\n> > -   the spending policy (2-of-3 multisig, timelock, hashlock,...)\n> > -   outputs ordering (BIP69)\n> > -   nLocktime/nSequence\n> > -   RBF-signaling\n> > -   Equal-value outputs\n> > -   weird watermark (LN commitment tx obfuscated commitment number)\n> > -   fees strategy like CPFP\n> > -   in-protocol announcements [0]\n>\n> Good list.\n> Another one, usually wouldn't be protocol as much as wallet leakage, but could be: utxo selection algorithm (which of course may be difficult to deduce, but often, far from impossible).\n> (Also trivial and increasingly irrelevant, but nVersion).\n>\n> With regards to coinjoin in this context (I know your points are much broader), my comment is:\n> For existing protocols (joinmarket's, wasabi's, samourai's), in the equal-outs paradigm, I don't see much that can be done in this area.\n> But I would ask people to consider CoinJoinXT[1] more seriously in a taproot/schnorr world, since it addresses this exact point. With a short (not cross-block like swaps or LN setup) interaction, participants can arrange the effect of coinjoin without the on-chain watermark of coinjoin (so, steganographic). The taproot/schnorr part is needed there because multisig is required from transaction to transaction in that protocol, so doing it today is less interesting (albeit still interesting).\n\nCoinJoinXT is indeed something I am interested in at some point: https://zmnscpxj.github.io/bitcoin/coinjoinxt.html\nThe above writeup is a client-server model, with multiple clients mixing.\nIf none of the participants reveal that a CoinJoinXT was done, then the graph is difficult to detect as such.\nHowever, if any participants reveal that a CoinJoinXT was done, it has a fallback such that it is almost as good as an equal-value CoinJoin (but takes up more block space).\nAt least it is not immediately obvious that it is in fact a CoinJoinXT from *just* a simple transaction analysis, which we hope is enough to deter simple policies like \"check N transactions back for a transaction with more than one equal-valued output\".\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Antoine Riard",
                "date": "2020-02-24T17:58:02",
                "message_text_only": "> Another one, usually wouldn't be *protocol* as much as wallet leakage,\nbut could be: utxo selection algorithm (which of course may be difficult to\ndeduce, but often, far from impossible).\n\nYes sure that's a good point, it may affect protocol too if your LN\nimplementation has its own onchain wallet. If not, and it reuses a non-LN\nwallet you just carry on its fingerprint.\nAn extension in the future could be for closing/splicing transaction, your\nliquidity algorithm may select in a really specific fashion which channels\nmust be closed or increased...\n\n> But I would ask people to consider CoinJoinXT[1] more seriously in a\ntaproot/schnorr world, since it addresses this exact point.\n\nThe equal value paradigm is such a watermark and I assume it leans to\nincrease the number of outputs so I don't see it followed by any other\nprotocol. But yes CoinjoinXT, if you can come up with a easy interactive\nmulti-tx construction protocol that would be interesting (and could be\nreused by any cut-through implementation I guess).\n\nOverall, my thinking was to start specifying this now because such thing\nwould take a fair amount of time/coordination to get adopted. This way if\nand when Taproot/Schnorr happen we don't\nhave to wait another period to start enjoying the privacy enhancement\n(worst-case we can fallback on 2p-ecdsa).\n\n\n\nLe sam. 22 f\u00e9vr. 2020 \u00e0 07:10, AdamISZ <AdamISZ at protonmail.com> a \u00e9crit :\n\n> \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n> On Friday, 21 February 2020 22:17, Antoine Riard via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> > How can a Bitcoin tranaction leak protocol usage ?\n> > * the output type (p2sh, p2wsh, ...)\n> > * the spending policy (2-of-3 multisig, timelock, hashlock,...)\n> > * outputs ordering (BIP69)\n> > * nLocktime/nSequence\n> > * RBF-signaling\n> > * Equal-value outputs\n> > * weird watermark (LN commitment tx obfuscated commitment number)\n> > * fees strategy like CPFP\n> > * in-protocol announcements [0]\n> >\n> Good list.\n> Another one, usually wouldn't be *protocol* as much as wallet leakage, but\n> could be: utxo selection algorithm (which of course may be difficult to\n> deduce, but often, far from impossible).\n> (Also trivial and increasingly irrelevant, but nVersion).\n>\n> With regards to coinjoin in this context (I know your points are much\n> broader), my comment is:\n> For existing protocols (joinmarket's, wasabi's, samourai's), in the\n> equal-outs paradigm, I don't see much that can be done in this area.\n> But I would ask people to consider CoinJoinXT[1] more seriously in a\n> taproot/schnorr world, since it addresses this exact point. With a short\n> (not cross-block like swaps or LN setup) interaction, participants can\n> arrange the effect of coinjoin without the on-chain watermark of coinjoin\n> (so, steganographic). The taproot/schnorr part is needed there because\n> multisig is required from transaction to transaction in that protocol, so\n> doing it today is less interesting (albeit still interesting).\n>\n> waxwing\n>\n> [1] https://joinmarket.me/blog/blog/coinjoinxt/\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200224/82833d80/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2020-02-23T01:29:09",
                "message_text_only": "Ggood morning Antoine, and list,\n\n\n> * nLocktime/nSequence\n> ...\n> * weird watermark (LN commitment tx obfuscated commitment number)\n> ...\n> LN (cooperative case):\n\nI notice your post puts little spotlight on unilateral cases.\nA thing to note, is that we only use `nSequence` and the weird watermark on unilateral closes.\nEven HTLCs only exist on unilateral closes --- on mutual closes we wait for HTLCs to settle one way or the other before doing the mutual close.\n\nIf we assume that unilateral closes are rare, then it might be an acceptable risk to lose privacy in that case.\nOf course, it takes two to tango, and it takes two to make a Lightning channel, so ---\nIn any case, I explored some of the difficulties with unilateral closes as well:\n\n* https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-January/002421.html\n* https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-January/002415.html\n\nOn mutual closes, we should probably set `nLockTime` to the current blockheight + 1 as well.\nThis has greater benefit later in a Taproot world.\n\n> Questions:\n> * Are there any protocol-specific semantic wrt to onchain transactions incompatibility\n> between Coinjoin and cooperative LN txn ?\n\nA kind of non-equal-value CoinJoin could emulate a Lightning open + close, but most Lightning channels will have a large number of blocks (thousands or tens of thousands) between the open and the close; it seems unlikely that a short-term channel will exist that matches the non-equal-value CoinJoin.\n\nIn particular, a LN cooperative close will, in general, have only one input.\nA new form of CoinJoin could, instead of using a single transaction, use two, with an entry transaction that spends into an n-of-n of the participants, and the n-of-n being spent to split the coin back to their owners.\nBut again: a Lightning network channel would have much time with the funds in a single UTXO before later splitting the funds,\nThis also starts edging closer to CoinJoinXT territory.\n\n> * What about RBF-by-default ?\n\nShould always be on, even if we do not (yet) have a facility to re-interact to bump fees higher.\nWhile it is true that a surveillor can determine that a transaction has in fact been replaced (by observing the mempool) and thus eliminate the set of transactions that arose from protocols that mark RBF but do not (yet) have a facility to bump fees higher, this information is not permanently recorded on all fullnodes and at least we force surveillors to record this information themselves.\n\n> * Core wallet or any other protocol or even batching algorithms could adopt\n> to this format ?\n\nIt seems likely.\nHowever, it seems to me that we need to as well nail down the details of this format.\n\n> * Is artificially increasing the number of outputs to mimic Coinjoins txn\n> acceptable wrt to utxo bloat/fees ?\n\nThat is indeed an issue.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Antoine Riard",
                "date": "2020-02-24T18:26:52",
                "message_text_only": "> I notice your post puts little spotlight on unilateral cases.\n> A thing to note, is that we only use `nSequence` and the weird watermark\non unilateral closes.\n> Even HTLCs only exist on unilateral closes --- on mutual closes we wait\nfor HTLCs to settle one way or the other before doing the mutual close.\n\nYes, I'm only aiming LN-cooperative cases, as your noticed HTLCs only exist\non commitment txn and masquerading them in some Taptree would come\nwith its own challenges. Cooperative closings should be the majority of\nchannels if network is reliable and so would be a set big enough to achieve\nthe goal\nof blurring Coinjoins among LN transactions.\n\nRight now we don't use `nSequence` but the current interactive tx\nconstruction proposal uses it for RBF (weird watermark was an example).\n\n> On mutual closes, we should probably set `nLockTime` to the current\nblockheight + 1 as well.\n> This has greater benefit later in a Taproot world.\n\nI assume mutual closes would fall under the aforementioned tx construction\nproposal, so a closing may be a batch to fund other channels or\nsplice existent ones.\n\n> A kind of non-equal-value CoinJoin could emulate a Lightning open +\nclose, but most Lightning channels will have a large number of blocks\n(thousands or tens of thousands) between the open and the close; it seems\nunlikely that a short-term channel will exist > that matches the\nnon-equal-value CoinJoin.\n\nThat's a really acute point, utxo age and spending frequency may be obvious\nprotocol leaks. Splicing may help there because a LN node would do multiple\nchain writes during channel lifecycle for liquidity reasons but it's\nnear-impossible to predict its frequency without deployment. Even with\nthis, I do fear an analysis gap between Coinjoin spending delta and LN\nones. A way to circumvent this would be for CoinjoinXT to timelock its PTG\ntransactions to mimick actively-spliced LN channels. That's where adoption\nof a common format by other onchain transactions than LN ones would help a\nlot.\n\n> Should always be on, even if we do not (yet) have a facility to\nre-interact to bump fees higher.\n> While it is true that a surveillor can determine that a transaction has\nin fact been replaced (by observing the mempool) and thus eliminate the set\nof transactions that arose from protocols that mark RBF but do not (yet)\nhave a facility to bump fees higher, this > information is not permanently\nrecorded on all fullnodes and at least we force surveillors to record this\ninformation themselves.\n\nYes but if you do this for Core and given some merchants are refusing RBF\ntransactions for onchain payments, people are going to complain...\nAlso see footnote on spurious-RBF about not-having facility to bump fees\nhigher (you can sign multiple RBF transactions in 1-RTT and agree to\nbroadcast them later to obfuscate mempool analysis).\n\n> However, it seems to me that we need to as well nail down the details of\nthis format.\n\nOf course, just curious of people opinions right now but if it's a good way\nto solve the described problem, will draft a spec.\n\nLe sam. 22 f\u00e9vr. 2020 \u00e0 20:29, ZmnSCPxj <ZmnSCPxj at protonmail.com> a \u00e9crit :\n\n> Ggood morning Antoine, and list,\n>\n>\n> > * nLocktime/nSequence\n> > ...\n> > * weird watermark (LN commitment tx obfuscated commitment number)\n> > ...\n> > LN (cooperative case):\n>\n> I notice your post puts little spotlight on unilateral cases.\n> A thing to note, is that we only use `nSequence` and the weird watermark\n> on unilateral closes.\n> Even HTLCs only exist on unilateral closes --- on mutual closes we wait\n> for HTLCs to settle one way or the other before doing the mutual close.\n>\n> If we assume that unilateral closes are rare, then it might be an\n> acceptable risk to lose privacy in that case.\n> Of course, it takes two to tango, and it takes two to make a Lightning\n> channel, so ---\n> In any case, I explored some of the difficulties with unilateral closes as\n> well:\n>\n> *\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-January/002421.html\n> *\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-January/002415.html\n>\n> On mutual closes, we should probably set `nLockTime` to the current\n> blockheight + 1 as well.\n> This has greater benefit later in a Taproot world.\n>\n> > Questions:\n> > * Are there any protocol-specific semantic wrt to onchain transactions\n> incompatibility\n> > between Coinjoin and cooperative LN txn ?\n>\n> A kind of non-equal-value CoinJoin could emulate a Lightning open + close,\n> but most Lightning channels will have a large number of blocks (thousands\n> or tens of thousands) between the open and the close; it seems unlikely\n> that a short-term channel will exist that matches the non-equal-value\n> CoinJoin.\n>\n> In particular, a LN cooperative close will, in general, have only one\n> input.\n> A new form of CoinJoin could, instead of using a single transaction, use\n> two, with an entry transaction that spends into an n-of-n of the\n> participants, and the n-of-n being spent to split the coin back to their\n> owners.\n> But again: a Lightning network channel would have much time with the funds\n> in a single UTXO before later splitting the funds,\n> This also starts edging closer to CoinJoinXT territory.\n>\n> > * What about RBF-by-default ?\n>\n> Should always be on, even if we do not (yet) have a facility to\n> re-interact to bump fees higher.\n> While it is true that a surveillor can determine that a transaction has in\n> fact been replaced (by observing the mempool) and thus eliminate the set of\n> transactions that arose from protocols that mark RBF but do not (yet) have\n> a facility to bump fees higher, this information is not permanently\n> recorded on all fullnodes and at least we force surveillors to record this\n> information themselves.\n>\n> > * Core wallet or any other protocol or even batching algorithms could\n> adopt\n> > to this format ?\n>\n> It seems likely.\n> However, it seems to me that we need to as well nail down the details of\n> this format.\n>\n> > * Is artificially increasing the number of outputs to mimic Coinjoins txn\n> > acceptable wrt to utxo bloat/fees ?\n>\n> That is indeed an issue.\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200224/9f4a88b6/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2020-02-24T23:35:56",
                "message_text_only": "Good morning Antoine,\n\n\n> > On mutual closes, we should probably set `nLockTime` to the current blockheight + 1 as well.\n> > This has greater benefit later in a Taproot world.\n>\n> I assume mutual closes would fall under the aforementioned tx construction proposal, so a closing may be a batch to fund other channels or\n> splice existent ones.\n\nAh, that is indeed of great interest.\nI proposed before to consider splicing as a form of merged closing plus funding, rather than a modification of channel state; in particular we might note that, for compatibility with our existing system, a spliced channel would have to change its short channel ID and channel ID, so it is arguably a different channel already.\n\n>\n> > A kind of non-equal-value CoinJoin could emulate a Lightning open + close, but most Lightning channels will have a large number of blocks (thousands or tens of thousands) between the open and the close; it seems unlikely that a short-term channel will exist > that matches the non-equal-value CoinJoin.\n>\n> That's a really acute point, utxo age and spending frequency may be obvious protocol leaks.\n\nYes; I am curious how JoinMarket reconciles how makers mix their coins vs. how takers do; presumably the tumbler.py emulates the behavior of a maker somehow.\n\n> Splicing may help there because a LN node would do multiple chain writes during channel lifecycle for liquidity reasons but it's\n> near-impossible to predict its frequency without deployment.\n\nLong ago, I proposed an alternative to splicing, which would today be recognizable as a \"submarine swap\" or \"lightning loop\". https://lists.linuxfoundation.org/pipermail/lightning-dev/2017-May/000692.html\nPerhaps the frequencies of those operations may hint as to how much splicing would occur in practice in the future.\n\n> Even with this, I do fear an analysis gap between Coinjoin spending delta and LN ones. A way to circumvent this would be for CoinjoinXT to timelock its PTG\n> transactions to mimick actively-spliced LN channels. That's where adoption of a common format by other onchain transactions than LN ones would help a lot.\n\nWell, one way to implement splice-in would be to have an output that is first dedicated to the splice-in, and *then* a separate transaction which actually does the splice-in.\nThis has a drawback of requiring an extra transaction, which wins us the facility to continue operation of the channel even while the splice-in transactions are being confirmed while retaining only one state.\n(the latest proposal, I believe, does *not* use this construction, and instead requires both sides to maintain two sets of states, with one state being a fallback in case the splice-in gets double spent; but in times of high blockchain load this can lead to the channel having a two sets of states until blockchain load reduces.)\n\nAs it happens, my alternate proposal for CoinJoinXT is similar in that there are \"entry transactions\" that introduce coins into the PTG, which are needed to prevent participants from double-spending while the mix is on-going. https://zmnscpxj.github.io/bitcoin/coinjoinxt.html\nNote the proposal differs from the original by waxwing, which requires backouts at each intermediate output, and the entry transactions are potential watermarks on the CoinJoinXT protocol as well.\nA Chaumian CoinJoinXT cannot use backouts at each intermediate output since no participant should have any knowledge of how much each participant has contributed to each intermediate output, they only know they put so many funds in and so should get so many funds out.\n\nEmulating LN splices mildly makes ConJoinXT less desirable, however, as the mix takes longer and is more costly.\n\n>\n> > Should always be on, even if we do not (yet) have a facility to re-interact to bump fees higher.\n> > While it is true that a surveillor can determine that a transaction has in fact been replaced (by observing the mempool) and thus eliminate the set of transactions that arose from protocols that mark RBF but do not (yet) have a facility to bump fees higher, this > information is not permanently recorded on all fullnodes and at least we force surveillors to record this information themselves.\n>\n> Yes but if you do this for Core and given some merchants are refusing RBF transactions for onchain payments, people are going to complain...\n\nGrumble grumble, all unconfirmed transaction are RBF because miners like money, grumble grumble, flagged RBF is just a node relay policy, grumble grumble, some humans sometimes, grumble grumble....\n\nDoes not Electrum do RBF by default?\nUnless I have a lower-level agent that always enables RBF option when I install new Electrums, hmmm, maybe I should check first.\n\n> Also see footnote on spurious-RBF about not-having facility to bump fees higher (you can sign multiple RBF transactions in 1-RTT and agree to broadcast them later to obfuscate mempool analysis).\n\n1.5RTT with MuSig.\n\nAn issue here is that if not all participants contribute to the fees equally, then a participant who is paying lower fee or no fee has a mild incentive to just broadcast the highest-fee version and be done with it: forget the other transactions and just aim for the highest fee immediately, ignore the mempool state so you do not have to do all those calculations or even maintain a mempool, and so on.\nThis can be mitigated if all participants contribute equal or nearly-equally to the fees, though that complicates single-funding, and may violate Initiator Pays Principle (the initiator of an action should pay all fees related to the action, as otherwise it may be possible to create a null operation that the acceptor of the action ends up paying fees for, which can be used as a financial attack to drain acceptors).\n\n\n> > However, it seems to me that we need to as well nail down the details of this format.\n>\n> Of course, just curious of people opinions right now but if it's a good way to solve the described problem, will draft a spec.\n\nThere may be other protocols interested in this as well --- for instance \"submarine swaps\" and \"lightning loops\", which are the same thing.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Antoine Riard",
                "date": "2020-02-25T19:16:03",
                "message_text_only": "Morning Zeeman,\n\n> I proposed before to consider splicing as a form of merged closing plus\nfunding, rather than a modification of channel state; in particular we\nmight note that, for compatibility with our existing system, a spliced\nchannel would have to change its short channel ID > and channel ID, so it\nis arguably a different channel already.\n\nYes but you may want alias to keep your channel routing-score across\nsplicing, though how to do this is more LN-dev specific.\n\n> Emulating LN splices mildly makes ConJoinXT less desirable, however, as\nthe mix takes longer and is more costly.\n\nIntuitively, a lot of Coinjoin traffic may be redirected in the future\nthrough LN when protocol matures, privacy properties may be better (though\nneed careful analysis).\nCoinjoins would be only for high-amounts for which security/liquidity isn't\noffered by LN, and in this case time for increasing privacy is IMO an\nacceptable tradeoff.\n\n> Does not Electrum do RBF by default?\n\nDunno, for more context on RBF and its controversies see\nhttps://bitcoincore.org/en/faq/optin_rbf/ (or Optech resources)\n\n> 1.5RTT with MuSig\n\nYes right I meaned you don't need to assume latter interactivity if it's a\nmulti-party tx construction you sign multiple RBF versions at same time.\nStill need to think about privacy-preserving fee bumping wrt to mempool\nobserver\n\n> This can be mitigated if all participants contribute equal or\nnearly-equally to the fees, though that complicates single-funding, and may\nviolate Initiator Pays Principle (the initiator of an action should pay all\nfees related to the action, as otherwise it may be  possible to create a\nnull operation that the acceptor of the action ends up paying fees for,\nwhich can be used as a financial attack to drain acceptors).\n\nYes, but also you want the acceptor to pay for its inputs announced to\navoid pouring the spending burden on the initiator only, or doing any\nfree-ride aggregation .\n\n> There may be other protocols interested in this as well --- for instance\n\"submarine swaps\" and \"lightning loops\", which are the same thing.\n\nYes good point, specially batched submarine swaps are good candidates, also\nDLCs (will enquiry on tx pattern of more bitcoin protocol)\n\n\nLe lun. 24 f\u00e9vr. 2020 \u00e0 18:36, ZmnSCPxj <ZmnSCPxj at protonmail.com> a \u00e9crit :\n\n> Good morning Antoine,\n>\n>\n> > > On mutual closes, we should probably set `nLockTime` to the current\n> blockheight + 1 as well.\n> > > This has greater benefit later in a Taproot world.\n> >\n> > I assume mutual closes would fall under the aforementioned tx\n> construction proposal, so a closing may be a batch to fund other channels or\n> > splice existent ones.\n>\n> Ah, that is indeed of great interest.\n> I proposed before to consider splicing as a form of merged closing plus\n> funding, rather than a modification of channel state; in particular we\n> might note that, for compatibility with our existing system, a spliced\n> channel would have to change its short channel ID and channel ID, so it is\n> arguably a different channel already.\n>\n> >\n> > > A kind of non-equal-value CoinJoin could emulate a Lightning open +\n> close, but most Lightning channels will have a large number of blocks\n> (thousands or tens of thousands) between the open and the close; it seems\n> unlikely that a short-term channel will exist > that matches the\n> non-equal-value CoinJoin.\n> >\n> > That's a really acute point, utxo age and spending frequency may be\n> obvious protocol leaks.\n>\n> Yes; I am curious how JoinMarket reconciles how makers mix their coins vs.\n> how takers do; presumably the tumbler.py emulates the behavior of a maker\n> somehow.\n>\n> > Splicing may help there because a LN node would do multiple chain writes\n> during channel lifecycle for liquidity reasons but it's\n> > near-impossible to predict its frequency without deployment.\n>\n> Long ago, I proposed an alternative to splicing, which would today be\n> recognizable as a \"submarine swap\" or \"lightning loop\".\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2017-May/000692.html\n> Perhaps the frequencies of those operations may hint as to how much\n> splicing would occur in practice in the future.\n>\n> > Even with this, I do fear an analysis gap between Coinjoin spending\n> delta and LN ones. A way to circumvent this would be for CoinjoinXT to\n> timelock its PTG\n> > transactions to mimick actively-spliced LN channels. That's where\n> adoption of a common format by other onchain transactions than LN ones\n> would help a lot.\n>\n> Well, one way to implement splice-in would be to have an output that is\n> first dedicated to the splice-in, and *then* a separate transaction which\n> actually does the splice-in.\n> This has a drawback of requiring an extra transaction, which wins us the\n> facility to continue operation of the channel even while the splice-in\n> transactions are being confirmed while retaining only one state.\n> (the latest proposal, I believe, does *not* use this construction, and\n> instead requires both sides to maintain two sets of states, with one state\n> being a fallback in case the splice-in gets double spent; but in times of\n> high blockchain load this can lead to the channel having a two sets of\n> states until blockchain load reduces.)\n>\n> As it happens, my alternate proposal for CoinJoinXT is similar in that\n> there are \"entry transactions\" that introduce coins into the PTG, which are\n> needed to prevent participants from double-spending while the mix is\n> on-going. https://zmnscpxj.github.io/bitcoin/coinjoinxt.html\n> Note the proposal differs from the original by waxwing, which requires\n> backouts at each intermediate output, and the entry transactions are\n> potential watermarks on the CoinJoinXT protocol as well.\n> A Chaumian CoinJoinXT cannot use backouts at each intermediate output\n> since no participant should have any knowledge of how much each participant\n> has contributed to each intermediate output, they only know they put so\n> many funds in and so should get so many funds out.\n>\n> Emulating LN splices mildly makes ConJoinXT less desirable, however, as\n> the mix takes longer and is more costly.\n>\n> >\n> > > Should always be on, even if we do not (yet) have a facility to\n> re-interact to bump fees higher.\n> > > While it is true that a surveillor can determine that a transaction\n> has in fact been replaced (by observing the mempool) and thus eliminate the\n> set of transactions that arose from protocols that mark RBF but do not\n> (yet) have a facility to bump fees higher, this > information is not\n> permanently recorded on all fullnodes and at least we force surveillors to\n> record this information themselves.\n> >\n> > Yes but if you do this for Core and given some merchants are refusing\n> RBF transactions for onchain payments, people are going to complain...\n>\n> Grumble grumble, all unconfirmed transaction are RBF because miners like\n> money, grumble grumble, flagged RBF is just a node relay policy, grumble\n> grumble, some humans sometimes, grumble grumble....\n>\n> Does not Electrum do RBF by default?\n> Unless I have a lower-level agent that always enables RBF option when I\n> install new Electrums, hmmm, maybe I should check first.\n>\n> > Also see footnote on spurious-RBF about not-having facility to bump fees\n> higher (you can sign multiple RBF transactions in 1-RTT and agree to\n> broadcast them later to obfuscate mempool analysis).\n>\n> 1.5RTT with MuSig.\n>\n> An issue here is that if not all participants contribute to the fees\n> equally, then a participant who is paying lower fee or no fee has a mild\n> incentive to just broadcast the highest-fee version and be done with it:\n> forget the other transactions and just aim for the highest fee immediately,\n> ignore the mempool state so you do not have to do all those calculations or\n> even maintain a mempool, and so on.\n> This can be mitigated if all participants contribute equal or\n> nearly-equally to the fees, though that complicates single-funding, and may\n> violate Initiator Pays Principle (the initiator of an action should pay all\n> fees related to the action, as otherwise it may be possible to create a\n> null operation that the acceptor of the action ends up paying fees for,\n> which can be used as a financial attack to drain acceptors).\n>\n>\n> > > However, it seems to me that we need to as well nail down the details\n> of this format.\n> >\n> > Of course, just curious of people opinions right now but if it's a good\n> way to solve the described problem, will draft a spec.\n>\n> There may be other protocols interested in this as well --- for instance\n> \"submarine swaps\" and \"lightning loops\", which are the same thing.\n>\n> Regards,\n> ZmnSCPxj\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200225/cf3c907a/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "LN & Coinjoin, a Great Tx Format Wedding",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "AdamISZ",
                "Antoine Riard"
            ],
            "messages_count": 8,
            "total_messages_chars_count": 34874
        }
    },
    {
        "title": "[bitcoin-dev] Non-equal value CoinJoins. Opinions.",
        "thread_messages": [
            {
                "author": "nopara73",
                "date": "2020-02-22T18:01:14",
                "message_text_only": ">  It seems to me that most users will not have nearly the same output of\n\"around 1 BTC\"\n\nWhile that would be true out of context, it depends on how you interpret it\nand they interpret it really broadly: \" One input might be 0.03771049 BCH;\nthe next might be 0.24881232 BCH, etc. \"\n\n> anyway if you deploy this on a real live mainnet, and if your math\nrequires that you have \"around 1 BTC\" outputs per user. you might as well\njust use equal-valued CoinJoins, where the equal-valued outputs at least\nare completely unlinked from the inputs.\n>  e.g. if you have a CashFusion transaction with outputs 1.0, 1.1, 0.99,\nyou could transform that to a CoinJoin with 0.99, 0.99, 0.99, 0.01, 0.11\noutputs.\n\nEqual valued coinjoins (1) waste more blockspace as your example\nillustrates and (2) prevent arbitrary amounts, so you cannot send in\ncoinjoins.\n\n> Indeed, the change outputs of an equal-valued CoinJoin would have similar\nanalyses to CashFusion, since the same analysis \"around 1 BTC\" can be\nperformed with the CoinJoin change outputs \"around 0 BTC\".\n\nI've been wondering about this too. I think it cannot be applied to\nexisting CoinJoin schemes, as coin selection heuristics are quite a help\nand that could be a reason why the changes can be deanonymized (I assume.)\nFor example if I want to analyze a Wasabi CJ, then I assume every input\nthat have > 0.1 BTC value to be THE valid input partition and I will only\nlook for the valid matching partition on the output side. I won't try to\nfind all the partitions and look at all the possible subset sums. (\nhttps://github.com/nopara73/Notes/blob/master/BellNumber.md,\nhttps://github.com/nopara73/Notes/blob/master/SubSetSum.md)\n\nAt the very least coin selection for equal value coinjoins can be relaxed\nto remove such assumptions and make the above math applicable for the\nchange. (If works.)\n\n\n\nOn Sun, Dec 29, 2019 at 12:25 AM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Adam,\n>\n> > The CashFusion research came out of the Bitcoin Cash camp, thus this\n> probably went under the radar of many of you. I would like to ask your\n> opinions on the research's claim that, if non-equal value coinjoins can be\n> really relied on for privacy or not.\n> >\n> > (Btw, there were also similar ideas in the Knapsack paper in 2017:\n> https://www.comsys.rwth-aachen.de/fileadmin/papers/2017/2017-maurer-trustcom-coinjoin.pdf\n>  )\n> >\n> >\n> https://github.com/cashshuffle/spec/blob/master/CASHFUSION.md#avoiding-amount-linkages-through-combinatorics\n>\n> >\n> > I copy the most relevant paragraphs here:\n> >\n> >   ---------BEGIN QUOTE ---------\n> >\n> >\n> > Consider a transaction where 10 people have each brought 10 inputs of\n> arbitary amounts in the neighborhood of ~0.1 BCH. One input might be\n> 0.03771049 BCH; the next might be 0.24881232 BCH, etc. All parties have\n> chosen to consolidate their coins, so the transaction has 10 outputs of\n> around 1 BCH. So the transaction has 100 inputs, and 10 outputs. The first\n> output might be 0.91128495, the next could be 1.79783710, etc.\n> >\n> > Now, there are 100!/(10!)^10 ~= 10^92 ways to partition the inputs into\n> a list of 10 sets of 10 inputs, but only a tiny fraction of these\n> partitions will produce the precise output list. So, how many ways produce\n> this exact output list? We can estimate with some napkin math. First,\n> recognize that for each partitioning, each output will typically land in a\n> range of ~10^8 discrete possibilities (around 1 BCH wide, with a 0.00000001\n> BCH resolution). The first 9 outputs all have this range of possibilities,\n> and the last will be constrained by the others. So, the 10^92 possibilies\n> will land somewhere within a 9-dimensional grid that cointains\n> (10^8)^9=10^72 possible distinct sites, one site which is our actual output\n> list. Since we are stuffing 10^92 possibilties into a grid that contains\n> only 10^72 sites, then this means on average, each site will have 10^20\n> possibilities.\n> >\n> > Based on the example above, we can see that not only are there a huge\n> number of partitions, but that even with a fast algorithm that could find\n> matching partitions, it would produce around 10^20 possible valid\n> configurations. With 10^20 possibilities, there is essentially no linkage.\n> The Cash Fusion scheme actually extends this obfuscation even further. Not\n> only can players bring many inputs, they can also have multiple outputs.\n> >\n> > ---------END QUOTE ---------\n> > --\n>\n>\n> It seems to me that most users will not have nearly the same output of\n> \"around 1 BTC\" anyway if you deploy this on a real live mainnet, and if\n> your math requires that you have \"around 1 BTC\" outputs per user. you might\n> as well just use equal-valued CoinJoins, where the equal-valued outputs at\n> least are completely unlinked from the inputs.\n>\n> Indeed, the change outputs of an equal-valued CoinJoin would have similar\n> analyses to CashFusion, since the same analysis \"around 1 BTC\" can be\n> performed with the CoinJoin change outputs \"around 0 BTC\".\n>\n> * You can always transform a CashFusion transaction whose outputs are\n> \"around 1 BTC\" to a CoinJoin transaction with equal-valued outputs and some\n> change outputs, with the equal-valued outputs having equal value to the\n> smallest CashFusion output.\n>  * e.g. if you have a CashFusion transaction with outputs 1.0, 1.1, 0.99,\n> you could transform that to a CoinJoin with 0.99, 0.99, 0.99, 0.01, 0.11\n> outputs.\n> * Conversely, you can transform an equal-valued CoinJoin transaction to a\n> CashFusion transaction using the same technique.\n> * That implies that the change outputs of an equal-valued CoinJoin have\n> the same linkability as the outputs of the equivalent CashFusion\n> transaction.\n> * At least with equal-valued CoinJoin, the equal-valued outputs have 0\n> linkability with inputs (at least with only that transaction in isolation).\n>   The same cannot be said of CashFusion, because the value involved is\n> just in a single UTXO.\n>\n> Regards,\n> ZmnSCPxj\n>\n\n\n-- \nBest,\n\u00c1d\u00e1m\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200222/2e98f28b/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Non-equal value CoinJoins. Opinions.",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "nopara73"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 6187
        }
    },
    {
        "title": "[bitcoin-dev] Composable MuSig",
        "thread_messages": [
            {
                "author": "Erik Aronesty",
                "date": "2020-02-23T07:27:39",
                "message_text_only": "> Thus, two-phase MuSig is potentially unsafe.\n> https://eprint.iacr.org/2018/417.pdf describes the argument.\n\nOne solution is to add a signature timeout to the message (say a block\nheight) .\n\nA participant refuses to sign if that time is too far in the future, or is\nat all in the past, or if a message M is the same as any previous message\nwithin that time window.\n\nSeems to resolve the attacks on 2 round musig.\n\n\n\n\n\n\n\n\n\n\n\n\nOn Mon, Nov 25, 2019, 6:00 AM ZmnSCPxj via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> So I heard you like MuSig.\n>\n>\n> Introduction\n> ============\n>\n> Previously on lightning-dev, I propose Lightning Nodelets, wherein one\n> signatory of a channel is in fact not a single entity, but instead an\n> aggregate:\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-October/002236.html\n>\n> Generalizing:\n>\n> * There exists some protocol that requires multiple participants agreeing.\n>   * This can be implemented by use of MuSig on the public keys of the\n> participants.\n> * One or more of the participants in the above protocol is in fact an\n> aggregate, not a single participant.\n>   * Ideally, no protocol modification should be needed to support such\n> aggregates, \"only\" software development without modifying the protocol\n> layer.\n>   * Obviously, any participant of such a protocol, whether a direct\n> participant, or a member of an aggregated participant of that protocol,\n> would want to retain control of its own money in that protocol, without\n> having to determine if it is being Sybilled (and all other participants are\n> in fact just one participant).\n>   * Motivating example: a Lightning Network channel is the aggregate of\n> two participants, the nodes creating that channel.\n>     However, with nodelets as proposed above, one of the participants is\n> actually itself an aggregate of multiple nodelets.\n>     * This requires that a Lightning Network channel with a MuSig address,\n> to have one or both participants, be potentially an aggregate of two or\n> more nodelet participants, e.g. `MuSig(MuSig(A, B), C)`\n>\n> This is the \"MuSig composition\" problem.\n> That is, given `MuSig(MuSig(A, B), C)`, and the *possibility* that in fact\n> `B == C`, what protocol can A use to ensure that it uses the three-phase\n> MuSig protocol (which has a proof of soundness) and not inadvertently use a\n> two-phase MuSig protocol?\n>\n> Schnorr Signatures\n> ==================\n>\n> The scheme is as follows.\n>\n> Suppose an entity A needs to show a signature.\n> At setup:\n>\n> * It generates a random scalar `a`.\n> * It computes `A` as `A = a * G`, where `G` is the standard generator\n> point.\n> * It publishes `A`.\n>\n> At signing a message `m`:\n>\n> * It generates a random scalar `r`.\n> * It computes `R` as `R = r * G`.\n> * It computes `e` as `h(R | m)`, where `h()` is a standard hash function\n> and `x | y` denotes the serialization of `x` concatenated by the\n> serialization of `y`.\n> * It computes `s` as `s = r + e * a`.\n> * It publishes as signature the tuple of `(R, s)`.\n>\n> An independent validator can then get `A`, `m`, and the signature `(R, s)`.\n> At validation:\n>\n> * It recovers `e[validator]` as so: `e[validator] = h(R | m)`\n> * It computes `S[validator]` as so: `S[validator] = R + e[validator] * A`.\n> * It checks if `s * G == S[validator]`.\n>   * If `R` and `s` were indeed generated as per signing algorithm above,\n> then:\n>     * `S[validator] = R + e[validator] * A`\n>     * `== r * G + e[validator] * A`; subbstitution of `R`\n>     * `== r * G + h(R | m) * A`; substitution of `e[validator]`\n>     * `== r * G + h(R | m) * a * G`; substitution of `A`.\n>     * `== (r + h(R | m) * a) * G`; factor out `G`\n>     * `== (r + e * a) * G`; substitution of `h(R | m)` with `e`\n>     * `== s * G`; substitution of `r + e * a`.\n>\n> MuSig\n> =====\n>\n> Under MuSig, validation must remain the same, and multiple participants\n> must provide a single aggregate key and signature.\n>\n> Suppose there exist two participants A and B.\n> At setup:\n>\n> * A generates a random scalar `a` and B generates a random scalar `b`.\n> * A computes `A` as `A = a * G` and B computes `B` as `B = b * G`.\n> * A and B exchange `A` and `B`.\n> * They generate the list `L`, by sorting their public keys and\n> concatenating their representations.\n> * They compute their aggregate public key `P` as `P = h(L) * A + h(L) * B`.\n> * They publish the aggregate public key `P`.\n>\n> Signing takes three phases.\n>\n> 1.  `R` commitment exchange.\n>   * A generates a random scalar `r[a]` and B generates a random scalar\n> `r[b]`.\n>   * A computes `R[a]` as `R[a] = r[a] * G` and B computes `R[b]` as `R[b]\n> = r[b] * G`.\n>   * A computes `h(R[a])` and B computes `h(R[b])`.\n>   * A and B exchange `h(R[a])` and `h(R[b])`.\n> 2.  `R` exchange.\n>   * A and B exchange `R[a]` and `R[b]`.\n>   * They validate that the previous given `h(R[a])` and `h(R[b])` matches.\n> 3.  `s` exchange.\n>   * They compute `R` as `R = R[a] + R[b]`.\n>   * They compute `e` as `h(R | m)`.\n>   * A computes `s[a]` as `s[a] = r[a] + e * h(L) * a` and B computes\n> `s[b]` as `s[b] = r[b] + e * h(L) * b`.\n>   * They exchange `s[a]` and `s[b]`.\n>   * They compute `s` as `s = s[a] + s[b]`.\n>   * They publish the signature as the tuple `(e, s)`.\n>\n> At validation, the validator knows `P`, `m`, and the signature `(R, s)`.\n>\n> * It recovers `e[validator]` as so: `e[validator] = h(R | m)`\n> * It computes `S[validator]` as so: `S[validator] = R + e[validator] * P`.\n> * It checks if `s * G == S[validator]`.\n>   * `S[validator] = R + e[validator] * P`\n>   * `== R[a] + R[b] + e[validator] * P`; substitution of `R`\n>   * `== r[a] * G + r[b] * G + e[validator] * P`; substitution of `R[a]`\n> and `R[b]`\n>   * `== r[a] * G + r[b] * G + e * P`; substitution of `e[validator]` with\n> `e`\n>   * `== r[a] * G + r[b] * G + e * (h(L) * A + h(L) * B)`; substitution of\n> `P`\n>   * `== r[a] * G + r[b] * G + e * h(L) * A + e * h(L) * B`; distribution\n> of `e` inside parentheses.\n>   * `== r[a] * G + r[b] * G + e * h(L) * a * G + e * h(L) * b * G`;\n> substitution of `A` and `B`.\n>   * `== (r[a] + r[b] + e * h(L) * a + e * h(L) * b) * G`; factoring out of\n> `G`\n>   * `== (r[a] + e * h(L) * a + r[b] + e * h(L) * b) * G`; rearrangement of\n> terms\n>   * `== (s[a] + s[b]) * G`; substitution of `r[a] + e * h(L) * a` and\n> `r[b] + e * h(L) * b`\n>   * `== s * G`;  substitution of `s[a] + s[b]`\n>\n>\n> Two-Phase MuSig Unsafe\n> ======================\n>\n> Original proposal of MuSig only had two phases, `R` exchange and `s`\n> exchange.\n> However, there was a flaw found in the security proof in this two-phase\n> MuSig.\n> In response, an earlier phase of exchanging commitments to `R` was added.\n>\n> Thus, two-phase MuSig is potentially unsafe.\n>\n> https://eprint.iacr.org/2018/417.pdf describes the argument.\n> Briefly, with two-phase MuSig, one of the participants can deliberately\n> delay its side of a `R` exchange and control the resulting sum `R` by\n> cancelling the `R` of the other participant.\n> Executed over many (aborted) signing sessions, one participant can induce\n> the other to create a signature for a message it might not agree to, by\n> using the Wagner Generalized Birthday Paradox attack.\n>\n> Briefly, a two-phase MuSig signing would go this way:\n>\n> 1.  `R` exchange.\n>   * A generates random scalar `r[a]` and B generates random scalar `r[b]`.\n>   * A computes `R[a]` as `r[a] * G` and B computes `R[b]` as `r[b] * G`.\n>   * They exchange `R[a]` and `R[b]`.\n> 2.  `s` exchange.\n>   * They compute `R` as `R = R[a] + R[b]`.\n>   * They compute `e` as `h(R | m)`.\n>   * A computes `s[a]` as `s[a] = r[a] + e * h(L) * a` and B computes\n> `s[b]` as `s[b] = r[b] + e * h(L) * b`.\n>   * They exchange `s[a]` and `s[b]`.\n>   * They compute `s` as `s = s[a] + s[b]`.\n>   * They publish the signature as the tuple `(R, s)`.\n>\n> The sticking point is \"exchange\" here.\n> Given that we live in a relativistic universe where there is no such thing\n> as simultaneity-in-time-without-simultaneity-in-space, it is impossible to\n> ensure that both A and B send their data \"at the same time\" in such a way\n> that it is impossible for, for example, the send of B to be outside the\n> future light cone of the send of A.\n> Or in human-level terms, it is not possible to ensure over the network\n> that B will not send `R[b]` *after* it receives `R[a]`.\n>\n> Suppose that instead of B generating a random `r[b]` and *then* computing\n> `R[b] = r[b] * G`, it instead selects an arbitrary `R[selected]` it wants\n> to target, then compute `R[b]` as `R[selected] - R[a]`.\n> Then at `s` exchange:\n>\n> * They compute `R` as `R[a] + R[b]`, which is in fact `R[a] + R[selected]\n> - R[a]`, or `R[selected]`, i.e. `R == R[selected]`.\n> * They compute `e` as `h(R[selected] | m)`.\n> * A computes `s[a]` as `s[a] = r[a] + e * h(L) * a`.\n> * B is unable to compute `s[b]` as it has no `r[b]` it can use in the\n> computation, and aborts the signing.\n>\n> The attack involved is that multiple such signing sessions, for the same\n> message or for separate distinct messages, might be done in parallel.\n> Suppose that there are `n` such sessions, such that A provides `n`\n> different `R[a][i]`, e.g. `R[a][1]`, `R[a][2]`, `R[a][3]` up to `R[a][n]`.\n> Then:\n>\n> * B delays each session, pretending to have Internet connectivity problems.\n> * B selects a message `m[target]` that it knows A will never sign (e.g.\n> \"I, A, give all my money to B\").\n> * B computes `R[target]` as `sum where i = 1 to n of R[a][i]`.\n> * B uses the Wagner Generalized Birthday Paradox technique to find\n> `R[selected][i]` with the following constraint:\n>   * `h(R[target] | m[target]) == sum where i = 1 to n of h(R[selected][i]\n> | m[i])`.\n>   * Given a large enough number of parallel sessions `n`, this can greatly\n> reduce the effort from 2^128 to find a match to within the realm of a large\n> computer to compute within a few seconds.\n> * B computes `R[b][i]` as `R[selected][i] - R[a][i]`, for each `i` from 1\n> to `n`.\n> * B provides `R[b][i]` for each session.\n> * A computes `R[i]` as `R[a][i] + R[b][i]` for each session.\n>   * However we know that `R[b][i] == R[selected][i] - R[a][i]` for each\n> session, cancelling out `R[a][i]` and leaving `R[i] == R[selected][i]`\n> * A computes `s[a][i]` as `r[a][i] + h(R[selected][i] | m[i]) * h(L) * a`\n> for each session.\n> * A gives `s[a][i]` for each session.\n> * B aborts each session.\n> * B sums up all the `s[a][i]`:\n>   * `(sum where i = 1 to n of r[a][i]) + (sum where i = 1 to n of\n> h(R[selected][i] | m[i]) * h(L) * a)`.\n>   * Remember, B has specifically selected `R[selected][i]` such that\n> `h(R[target] | m[target])` is equal to the sum of `h(R[selected][i] |\n> m[i])`.\n>   * `== (sum where i = 1 to n of r[a][i]) + h(R[target] | m[target]) *\n> h(L) * a)`.\n> * B adds `h(R[target] | m[target]) * h(L) * b` to the above sum.\n>   * This results in a signature for MuSig(A, B) to the message\n> `m[target]`, even though A would never have agreed to this message.\n>\n> Thus, 2-phase MuSig enables a Wagner attack on the participant, thus it is\n> unsafe.\n>\n> Now, any method of ensuring a \"simultaneous\" exchange of `R` points is\n> largely the same as adding a \"commit to `R`\" phase, i.e. the fix for this\n> is simply to add the \"`R` commitment exchange\" phase.\n>\n> References: https://eprint.iacr.org/2018/417.pdf\n>\n> MuSig Composition\n> =================\n>\n> Let us suppose that we have some protocol that requires a MuSig signing\n> session between signers with public keys `P` and `C`.\n> Let us further suppose that in fact, `P = MuSig(A, B)`, i.e. one of the\n> public keys in this protocol is, in reality, itself a MuSig of two\n> participants.\n>\n> At the point of view of signer C, P is a single participant and it acts as\n> such.\n> However, in reality, P is an aggregate.\n>\n> We want to have the following properties:\n>\n> * C should never need to know that P is in fact an aggregate.\n> * Even if B is secretly the same as C, the entire protocol as a whole\n> (including the aggregate signing of `MuSig(A, B)`) should remain\n> three-phase MuSig.\n>\n> Now, from the point of view of C, what it sees are:\n>\n> At setup:\n>\n> * It generates a random scalar `c` and the public key `C` as `C = c * G`.\n> * It exchanges keys with P and gets the public key `P`.\n> * It computes `L` by sorting `C` and `P` and concatenating them.\n> * It determines their aggregate key as `h(L) * C + h(L) * P`.\n>\n> At signing:\n>\n> 1.  `R` commitment exchange.\n>   * It generates a random scalar `r[c]` and computes `R[c]` as `R[c] =\n> r[c] * G`.\n>   * It computes `h(R[c])`.\n>   * It exchanges the hash `h(R[c])` with P and gets `h(R[p])`.\n> 2.  `R` exchange.\n>   * It exchanges `R[c]` with P and gets `R[p]`.\n>   * It validates that the hash `h(R[p])` matches the previously-committed\n> hash.\n> 3.  `s` exchange.\n>   * It computes `R` as `R = R[c] + R[p]`.\n>   * It computes `e` as `e = h(R | m)`.\n>   * It computes `s[c]` as `s[c] = r[c] + e * c`.\n>   * It exchanges `s[c]` with P and gets `s[p]`.\n>   * It computes `s` as `s = s[c] + s[p]`.\n>\n> However, from point of view of A and B, what actually happens is this:\n>\n> At setup:\n>\n> * A generates a random scalar `a` and computes `A = a * G`, B generates a\n> random scalar `b` and computes `B = b * G`.\n> * They exchange `A` and `B`.\n> * They generate their own `L[ab]`, by sorting `A` and `B` and\n> concatenating their representations.\n> * They compute the inner MuSig pubkey `P` as `P = h(L[ab]) * A + h(L[ab])\n> * B`.\n> * They exchange `P` with C, and get `C`.\n> * They compute the outer MuSig pubkey as `h(L) * P + h(L) * C`.\n>\n> At signing:\n>\n> 1.  `R` commitment exchange.\n>   * A generates a random scalar `r[a]` and computes `R[a] = r[a] * G`, B\n> generates a random scalar `r[b]` and computes `R[b] = r[b] * G`.\n>   * A computes `h(R[a])`, B computes `h(R[b])`.\n>   * They exchange `h(R[a])` and `h(R[b])`.\n>   * They need to compute `h(R[p])` for the protocol with C.\n>     * However, even if we know `R[p] == R[a] + R[b]`, we cannot generate\n> `h(R[p])`.\n>     * Thus, they have no choice but to exchange `R[a]` and `R[b]` at this\n> phase, even though this is supposed to be the `R` commitment exchange phase\n> (and they should not share `R[a]` and `R[b]` yet)!\n>\n> Unfortunately, this means that, between A and B, we are now reduced to a\n> two-phase MuSig.\n> This is relevant if B and C happen to be the same entity or are\n> cooperating.\n>\n> Basically, before C has to provide its `h(R[c])`, B now knows the\n> generated `R[a]` and `R[b]`.\n> If B and C are really the same entity, then C can compute `R[c]` as\n> `R[selected] - R[a] - R[b]` before providing `h(R[c])`.\n> Then this devolves to the same attack that brings down 2-phase MuSig.\n>\n> Thus, composition with the current MuSig proposal is insecure.\n>\n> Towards a Composable Multi-`R` MuSig\n> ====================================\n>\n> A key element is that the first phase simply requires that all\n> participants provide *commitments* to their individual `R`, and the second\n> phase reveals their `R`.\n>\n> I propose here the modification below:\n>\n> * In the first phase, any participant in the MuSig may submit one *or\n> more* `R` commitments.\n> * In the second phase, the participant in the MuSig submits each `R` that\n> decommits each of the `R` commitments it sent.\n>\n> I call this the Remote R Replacement Remanded: Redundant R Required\n> Realistically, or, in shorter terms, the Multi-`R` proposal.\n>\n> This is a simple and direct extension of the MuSig protocol, and expected\n> to not have any effect on the security proof of MuSig.\n>\n> In the case where all MuSig participants are singletons and each\n> participant just generates and sends a single `R` commitment, then this\n> proposal reduces to the original MuSig proposal.\n>\n> However, in the case where one participant is in reality itself an\n> aggregate, then we shall describe it below.\n> The below example is `MuSig(MuSig(A, B), C)`.\n>\n> 1.  `R` commitment exchange.\n>   * A generates a random number `r[a]`, B generates a random number\n> `r[b]`, C generates a random number `r[c]`.\n>   * A computes `R[a]` as `r[a] * G`, B computes `R[b]` as `r[b] * G`, C\n> computes `R[c]` as `r[c] * G`.\n>   * A computes `h(R[a])`, B computes `h(R[b])`, C computes `h(R[c])`.\n>   * A and B exchange their hashes with each other.\n>   * A and B jointly exchange their `h(R[a])` and `h(R[b])` with the\n> `h(R[c])` from C.\n> 2.  `R` exchange.\n>   * A and B reveal their `R[a]` and `R[b]` with each other.\n>   * A and B validate the given `R[a]` matches `h(R[a])` and the given\n> `R[b]` matches `h(R[b])`.\n>   * A and B jointly exchange their `R[a]` and `R[b]` with the `R[c]` from\n> C.\n>   * C validates `R[a]` and `R[b]`, A and B validate `R[c]`.\n>   * They compute `R` as the sum of all `R[a] + R[b] + R[c]`.\n> 3.  `s` exchange.\n>   * They compute `e` as `h(R | m)`.\n>   * A computes `s[a]` as `r[a] + e * h(L[abc]) * h(L[ab]) * a`, B computes\n> `s[b]` as `r[b] + e * h(L[abc]) * h(L[ab]) * b`.\n>   * C computes `s[c]` as `r[c] + e * h(L[abc]) * c`.\n>   * A and B exchange `s[a]` and `s[b]`.\n>   * A and B compute `s[ab]` as `s[a] + s[b]`.\n>   * A and B jointly exchange their `s[ab]` with `s[c]` from C.\n>   * They compute `s` as `s[ab] + s[c]`.\n>   * They publish the signature as the tuple `(R, s)`.\n>\n> Of note, is that the number of `R` a participant provides is a strong hint\n> as to whether it is actually an aggregate or not, and forms an upper bound\n> as to the size of the aggregate (i.e. an aggregate of `n` members can\n> pretend to be an aggregate of `m` members where `n < m`, but cannot pretend\n> with `n > m`).\n> Thus, C here learns that its counterparty is actually itself an aggregate\n> rather than a singleton.\n> This may be acceptable as a weak reduction in privacy (in principle, C\n> should never learn that it is talking to an aggregate rather than a single\n> party).\n>\n> Alternative Composable MuSig Schemes\n> ====================================\n>\n> The above proposal is not the only one.\n> Below are some additional proposals which have various flaws, ranging from\n> outright insecurity to practical implementation complexity issues.\n>\n> Pedersen Commitments in Phase 1\n> -------------------------------\n>\n> My initial proposal was to use Pedersen commitments in phase 1.\n> At phase 1, each party would generate a `r[x]` and `q[x]`, and exchange\n> the Pedersen commitments `r[x] * G + q[x] * H`, where `H` is a NUMS point\n> used as a second standard generator.\n> Then at phase 2, each party reveals its `q[x]`.\n> All the Pedersen commitments are summed, then all `q[x]` are summed,\n> multiplied by `H`, then subtracted from the sum of Pedersen commitments.\n>\n> Unfortunately, this leads to a Wagner attack.\n>\n> Suppose A and B have an aggregate MuSig(A, B).\n>\n> * B initiates multiple parallel signing sessions with A.\n> * B selects a message `m[target]` that it knows A will never sign (e.g.\n> \"I, A, give all my money to B\").\n> * In the first phase, B selects random points `R[b][i]` for each session\n> `i` and provides that as its Pedersen commitment, receiving `R[a][i] +\n> q[a][i] * H` in exchange.\n> * In the second phase, B delays each session, pretending to have Internet\n> connectivity problems.\n> * A sends B the `q[a][i]` for all `i`.\n> * B computes `R[a][i]` for all `i` by subtracting `q[a][i] * H` from the\n> Pedersen commitments given by A.\n> * B computes `R[target]` as `sum where i = 1 to n of R[a][i]`.\n> * B uses the Wagner Generalized Birthday Paradox technique to find\n> `q[b][i]` with the following constraint:\n>   * First compute `R[selected][i]` as `R[a][i] +  R[b][i] - q[b][i] * H`\n> for all `i`.\n>   * Then ensure this constraint: `h(R[target] | m[target]) == sum where i\n> = 1 to n of h(R[selected][i] | m[i])`.\n> * B sends the `q[b][i]` found above.\n> * A computes `R[i]` as `R[a][i] + q[a][i] * H + R[b][i] - q[a][i] * H -\n> q[b][i] * H` for all `i`.\n>   * This resolves down to `R[a][i] + R[b][i] - q[b][i] * H`, or\n> `R[selected][i]`.\n> * A computes `s[a][i]` as `r[a][i] + h(R[selected][i] | m[i]) * a` for all\n> `i`.\n> * B sums all `s[a][i]` for all `i` together, forming `(sum where i = 1 to\n> n of r[a][i]) + (sum where i = 1 to n of h(R[selected][i] | m[i])) * a`.\n>   * This is also a valid signature on `m[target]`, since `sum where i = 1\n> to n of h(R[selected][i] | m[i])` equals `h(R[target] | m[target])`.\n>\n> Thus, Pedersen commitments for phase 1 are insecure, as it allows\n> counterparties to control `R`.\n>\n> ElGamal Commitments in Phase 1\n> ------------------------------\n>\n> ElGamal commitments prevent B from just giving random `q[b][i]`, thus\n> preventing the above Wagner attack.\n> However, it is still possible for B to control the resulting `R`, but in\n> doing so this prevents the protocol from completing (thus, even with full\n> control of `R`, B is still unable to promote this to an `R`-reuse attack or\n> the above Wagner attack schema).\n> This is not quite as bad as the above case, but having just one\n> participant control the nonce `R` should make us worry that other attacks\n> may now become possible (unless we acquire some proof that this will be\n> equivalent in security to the hash-using MuSig).\n>\n> Briefly, with ElGamal commitments in Phase 1:\n>\n> 1. `R` commitment exchange.\n>   * A generates random numbers `r[a]` and `q[a]`, B generates random\n> numbers `r[b]` and `q[b]`.\n>   * A computes its commitment as two points, `q[a] * G` and `r[a] * G +\n> q[a] * H`, B computes its commitment as two points, `q[b] * G` and `r[b] *\n> G + q[b] * H`.\n>     * `H` is a NUMS point used as a second standard generator.\n>     * Note that one point uses `q[] * G` while the other adds `q[] * H` to\n> `r[] * G`.\n>   * They exchange their pairs of points.\n> 2. `R` exchange.\n>   * They exchange `q[a]` and `q[b]`, and the points `r[a] * G` (== `R[a]`)\n> and `r[b] * G` (== `R[b]`).\n>   * They validate the exchanged data from the previous `R` commitments.\n>   * They compute `R` as `R[a]` + `R[b]`.\n> 3. `s` exchange.\n>   * Same as before.\n>\n> B can attack this by delaying its phases as below:\n>\n> 1. `R` commitment exchange.\n>   * B generates random `q[selected]`.\n>   * B selects target `R[selected]`.\n>   * After receiving `q[a] * G` and `r[a] * G + q[a] * H`, B computes\n> `q[selected] * G - q[a] * G` and `R[selected] + q[selected] * H - r[a] * G\n> - q[a] * H` and sends those points as its own commitment.\n> 2. `R` exchange.\n>   * After receiving `q[a]` and `R[a]`, B computes `q[b]` as `q[selected] -\n> q[a]` and computes `R[b]` as `R[selected] - R[a]` and sends both as its\n> decommitment.\n>   * The resulting `R` will now be `R[selected]` chosen by B.\n>\n> `s` exchange cannot complete, as that would require that B know\n> `r[selected] - r[a]` where `R[selected] = r[selected] * G`.\n> Even if B generates `R[selected]` from `r[selected]`, it does not know\n> `r[a]`.\n> A would provide `r[a] + h(R[selected] | m) * h(L[ab]) * a`, but B would be\n> unable to complete this signature.\n>\n> The difference here is that B has to select `R[selected]` before it learns\n> `R[a]`, and thus is unable to mount the above Wagner attack schema.\n> In particular, B first has to compute an `R[target]` equal to `sum where i\n> = 1 to n of R[a][i]` across `n` sessions numbered `i`, in addition to\n> selecting a message `m[i]`.\n> Then B has to perform a Wagner attack with the constraint `h(R[target] |\n> m[target]) == sum where i = 1 to n of h(R[selected][i] | m[i])`\n> Fortunately for this scheme, B cannot determine such an `R[target]` before\n> it has to select `R[selected]`, thus preventing this attack.\n>\n> It may be possible that this scheme is safe, however I am not capable of\n> proving it safe.\n>\n> Acknowledgments\n> ===============\n>\n> I contacted Yannick Seurin, Andrew Poelstra, Pieter Wuille, and Greg\n> Maxwell, the authors of MuSig, regarding this issue, and proposing to use\n> Pedersen commitments for the first phase.\n> They informed me that Tim Ruffing had actually been thinking of similar\n> issue before I did, and also pointed out that Pedersen commitments do not\n> commit to `r * G`, only to `r` (i.e. would have to reveal `r` to serve as a\n> verifiable commitment).\n> It seemed to me that the general agreement was that ElGamal commitments\n> should work for committing to `r * G`.\n> However as I show above, this still allows a delaying participant to\n> cancel the `R` contributions of the other parties, allowing it to control\n> the aggregate `R` (though not quite to launch a Wagner attack).\n>\n> `nickler` and `waxwing` on IRC confirmed my understanding of the attack on\n> 2-phase MuSig.\n> `waxwing` also mentioned that the paper attacking 2-phase MuSig really\n> attacks CoSi, and that the paper itself admits that given a\n> knowledge-of-secret-keys, CoSi (and presumably 2-phase MuSig) would be safe.\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200223/ae7bd72a/attachment-0001.html>"
            },
            {
                "author": "Tim Ruffing",
                "date": "2020-02-24T11:16:38",
                "message_text_only": "On Sun, 2020-02-23 at 02:27 -0500, Erik Aronesty via bitcoin-dev wrote:\n> > Thus, two-phase MuSig is potentially unsafe.\n> > https://eprint.iacr.org/2018/417.pdf describes the argument.\n> \n> One solution is to add a signature timeout to the message (say a\n> block height) .  \n> \n> A participant refuses to sign if that time is too far in the future,\n> or is at all in the past, or if a message M is the same as any\n> previous message within that time window.\n> \n> Seems to resolve the attacks on 2 round musig.\n\nI don't understand this. Can you elaborate?\n\nBest,\nTim"
            },
            {
                "author": "Erik Aronesty",
                "date": "2020-02-24T15:30:54",
                "message_text_only": "Basically just some mechanism for preventing repeated signings of the\nsame message, and using a \"validity\" time window so that the amount of\nstate you need to enquire about isn't unbounded.\n\nThe Drijvers, et al paper is specifically concerned with parallel and\naborted signings, where ksums can be used.  In general, the more\nvariables that an attacker can control ,the more \"k\" lists they can\nform, and the more likely they can find collisions.\n\nIf signers refused to sign \"stale\" messages, refused to sign in\nparallel beyond a certain limit, and refused to sign the same message\ntwice, it should help reduce the attack surface.\n\nOn Mon, Feb 24, 2020 at 6:41 AM Tim Ruffing via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> On Sun, 2020-02-23 at 02:27 -0500, Erik Aronesty via bitcoin-dev wrote:\n> > > Thus, two-phase MuSig is potentially unsafe.\n> > > https://eprint.iacr.org/2018/417.pdf describes the argument.\n> >\n> > One solution is to add a signature timeout to the message (say a\n> > block height) .\n> >\n> > A participant refuses to sign if that time is too far in the future,\n> > or is at all in the past, or if a message M is the same as any\n> > previous message within that time window.\n> >\n> > Seems to resolve the attacks on 2 round musig.\n>\n> I don't understand this. Can you elaborate?\n>\n> Best,\n> Tim\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Tim Ruffing",
                "date": "2020-02-24T16:56:06",
                "message_text_only": "The only thing that matters is the number of parallel sessions. If you\nbound this to something like 2 or 3, then the resulting scheme may be\nsecure. But you need to the actual math of Wagner's attack, and who\nknows how efficient it can be implemented in practice. \n\nTimeouts on top of this won't help. And who needs 2 or 3 parallel\nsessions? If you need parallel sessions (or not), use 3-round MuSig and\nthe entire issue is simply eliminated.\n\nTim  \n\nOn Mon, 2020-02-24 at 10:30 -0500, Erik Aronesty wrote:\n> Basically just some mechanism for preventing repeated signings of the\n> same message, and using a \"validity\" time window so that the amount\n> of\n> state you need to enquire about isn't unbounded.\n> \n> The Drijvers, et al paper is specifically concerned with parallel and\n> aborted signings, where ksums can be used.  In general, the more\n> variables that an attacker can control ,the more \"k\" lists they can\n> form, and the more likely they can find collisions.\n> \n> If signers refused to sign \"stale\" messages, refused to sign in\n> parallel beyond a certain limit, and refused to sign the same message\n> twice, it should help reduce the attack surface.\n> \n> On Mon, Feb 24, 2020 at 6:41 AM Tim Ruffing via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > On Sun, 2020-02-23 at 02:27 -0500, Erik Aronesty via bitcoin-dev\n> > wrote:\n> > > > Thus, two-phase MuSig is potentially unsafe.\n> > > > https://eprint.iacr.org/2018/417.pdf describes the argument.\n> > > \n> > > One solution is to add a signature timeout to the message (say a\n> > > block height) .\n> > > \n> > > A participant refuses to sign if that time is too far in the\n> > > future,\n> > > or is at all in the past, or if a message M is the same as any\n> > > previous message within that time window.\n> > > \n> > > Seems to resolve the attacks on 2 round musig.\n> > \n> > I don't understand this. Can you elaborate?\n> > \n> > Best,\n> > Tim\n> > \n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "Composable MuSig",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Tim Ruffing",
                "Erik Aronesty"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 29330
        }
    },
    {
        "title": "[bitcoin-dev] BIP 340 updates: even pubkeys, more secure nonce generation",
        "thread_messages": [
            {
                "author": "Pieter Wuille",
                "date": "2020-02-24T04:26:17",
                "message_text_only": "Hello list,\n\nDespite saying earlier that I expected no further semantical changes\nto BIP 340-342, I've just opened\nhttps://github.com/bitcoin/bips/pull/893 to make a number of small\nchanges that I believe are still worth making.\n\n1. Even public keys\n\nOnly one change affects the validation rules: the Y coordinate of\n32-byte public keys is changed from implicitly square to implicitly\neven. This makes signing slightly faster (in the microsecond range),\nthough also verification negligibly slower (in the nanosecond range).\nIt also simplifies integration with existing key generation\ninfrastructure. For example BIP32 produces public keys with known\neven/oddness, but squaredness would need to be computed separately.\nSimilar arguments hold for PSBT and probably many other things.\n\nNote that the Y coordinate of the internal R point in the signature\nremains implicitly square: for R the squaredness gives an actual\nperformance gain at validation time, but this is not true for public\nkeys. Conversely, for public keys integration with existing\ninfrastructure matters, but R points are purely internal.\n\nThis affects BIP 340 and 341.\n\n2. Nonce generation\n\nAll other semantical changes are around more secure nonce generation\nin BIP 340, dealing with various failure cases:\n\n* Since the public key signed for is included in the signature\nchallenge hash, implementers will likely be eager to use precomputed\nvalues for these (otherwise an additional EC multiplication is\nnecessary at signing time). If that public key data happens to be\ngathered from untrusted sources, it can lead to trivial leakage of the\nprivate key - something that Greg Maxwell started a discussion about\non the moderncrypto curves list:\nhttps://moderncrypto.org/mail-archive/curves/2020/001012.html. We\nbelieve it should therefore be best practice to include the public key\nalso in the nonce generation, which largely mitigates this problem.\n\n* To protect against fault injection attacks it is recommended to\ninclude actual signing-time randomness into the nonce generation\nprocess. This was mentioned already, but the update elaborates much\nmore about this, and integrates this randomness into the standard\nsigning process.\n\n* To protect against differential power analysis, a different way of\nmixing in this randomness is used (masking the private key completely\nwith randomness before continuing, rather than hashing them together,\nwhich is known in the literature to be vulnerable to DPA in some\nscenarios).\n\n3. New tagged hash tags\n\nTo make sure that any code written for the earlier BIP text fails\nconsistently, the tags used in the tagged hashes in BIP 340 are\nchanged as well.\n\nWhat do people think?\n\n-- \nPieter"
            },
            {
                "author": "Lloyd Fournier",
                "date": "2020-02-26T04:20:58",
                "message_text_only": "Hi Pieter,\n\nLet me put change (1) into my own words. We are already computing affine\ncoordinates since we store public keys as the affine x-coordinate. It is\nfaster to compute is_even(y) than is_quadratic_residue(y) so we get a speed\nup here during keypair generation. In the verification algorithm, we do the\nfollowing for the public key  x_only => affine + negate if not is_even(y)\n=> jacobian. The minor slowdown in verification comes from the extra\nevenness check and possible negation which we didn't have to be done in the\nprevious version. This seems like a reasonable change if it makes things\neasier for existing code bases and infrastructure.\n\nWith change (2), I feel like including this auxiliary random data is\noverkill for the spec. For me, the main point of the spec is the\nverification algorithm which actually affects consensus. Providing a note\nthat non-deterministic signatures are preferable in many cases and here's\nexactly how you should do that (hash then xor with private key) is\nvaluable. In the end, people will want several variations of the signing\nalgorithm anyway (e.g. pass in public key with secret key) so I think\nspecifying the most minimal way to produce a signature securely is the most\nuseful thing for this document.\n\nI feel similarly about hashing the public key to get the nonce. A note in\nthe alternative signing section that \"if you pass the public key into\n`sign` along with the secret key then you should do hash(bytes(d) ||\nbytes(P) || m)\" would suffice for me.\n\nDespite only being included in the alternative signing section, I it would\nbe nice to have a few of test vectors for these alternative methods anyway.\nPerhaps they even deserve their own BIP?\n\nCheers,\n\nLL\n\n\nOn Mon, Feb 24, 2020 at 3:26 PM Pieter Wuille via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hello list,\n>\n> Despite saying earlier that I expected no further semantical changes\n> to BIP 340-342, I've just opened\n> https://github.com/bitcoin/bips/pull/893 to make a number of small\n> changes that I believe are still worth making.\n>\n> 1. Even public keys\n>\n> Only one change affects the validation rules: the Y coordinate of\n> 32-byte public keys is changed from implicitly square to implicitly\n> even. This makes signing slightly faster (in the microsecond range),\n> though also verification negligibly slower (in the nanosecond range).\n> It also simplifies integration with existing key generation\n> infrastructure. For example BIP32 produces public keys with known\n> even/oddness, but squaredness would need to be computed separately.\n> Similar arguments hold for PSBT and probably many other things.\n>\n> Note that the Y coordinate of the internal R point in the signature\n> remains implicitly square: for R the squaredness gives an actual\n> performance gain at validation time, but this is not true for public\n> keys. Conversely, for public keys integration with existing\n> infrastructure matters, but R points are purely internal.\n>\n> This affects BIP 340 and 341.\n>\n> 2. Nonce generation\n>\n> All other semantical changes are around more secure nonce generation\n> in BIP 340, dealing with various failure cases:\n>\n> * Since the public key signed for is included in the signature\n> challenge hash, implementers will likely be eager to use precomputed\n> values for these (otherwise an additional EC multiplication is\n> necessary at signing time). If that public key data happens to be\n> gathered from untrusted sources, it can lead to trivial leakage of the\n> private key - something that Greg Maxwell started a discussion about\n> on the moderncrypto curves list:\n> https://moderncrypto.org/mail-archive/curves/2020/001012.html. We\n> believe it should therefore be best practice to include the public key\n> also in the nonce generation, which largely mitigates this problem.\n>\n> * To protect against fault injection attacks it is recommended to\n> include actual signing-time randomness into the nonce generation\n> process. This was mentioned already, but the update elaborates much\n> more about this, and integrates this randomness into the standard\n> signing process.\n>\n> * To protect against differential power analysis, a different way of\n> mixing in this randomness is used (masking the private key completely\n> with randomness before continuing, rather than hashing them together,\n> which is known in the literature to be vulnerable to DPA in some\n> scenarios).\n>\n> 3. New tagged hash tags\n>\n> To make sure that any code written for the earlier BIP text fails\n> consistently, the tags used in the tagged hashes in BIP 340 are\n> changed as well.\n>\n> What do people think?\n>\n> --\n> Pieter\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200226/187389b1/attachment-0001.html>"
            },
            {
                "author": "Jonas Nick",
                "date": "2020-02-26T15:34:04",
                "message_text_only": "> Let me put change (1) into my own words.\n\nCorrect, except that the speedup from is_even(y) over is_quadratic_residue(y)\naffects signing and not keypair generation.\n\n> With change (2), I feel like including this auxiliary random data is overkill\n> for the spec. [...] I feel similarly about hashing the public key to get the\n> nonce.\n\nIt's not clear why removing these features from the spec would be an improvement.\nThe BIP follows a more reasonable approach: it specifies a reasonably secure\nsigning algorithm and provides the rationale behind the design choices. This\nallows anyone to optimize for their use case if they choose to do so.\nImportantly, \"reasonably secure\" includes misuse resistance which would be\nviolated if the pubkey was not input to the nonce generation function.\n\n> Perhaps they even deserve their own BIP?\n\nYes, a standard for nonce exfiltration protection and MuSig would be important\nfor compatibility across wallets.\n\n\nOn 2/26/20 4:20 AM, Lloyd Fournier via bitcoin-dev wrote:\n> Hi Pieter,\n> \n> Let me put change (1) into my own words. We are already computing affine\n> coordinates since we store public keys as the affine x-coordinate. It is\n> faster to compute is_even(y) than is_quadratic_residue(y) so we get a speed\n> up here during keypair generation. In the verification algorithm, we do the\n> following for the public key  x_only => affine + negate if not is_even(y)\n> => jacobian. The minor slowdown in verification comes from the extra\n> evenness check and possible negation which we didn't have to be done in the\n> previous version. This seems like a reasonable change if it makes things\n> easier for existing code bases and infrastructure.\n> \n> With change (2), I feel like including this auxiliary random data is\n> overkill for the spec. For me, the main point of the spec is the\n> verification algorithm which actually affects consensus. Providing a note\n> that non-deterministic signatures are preferable in many cases and here's\n> exactly how you should do that (hash then xor with private key) is\n> valuable. In the end, people will want several variations of the signing\n> algorithm anyway (e.g. pass in public key with secret key) so I think\n> specifying the most minimal way to produce a signature securely is the most\n> useful thing for this document.\n> \n> I feel similarly about hashing the public key to get the nonce. A note in\n> the alternative signing section that \"if you pass the public key into\n> `sign` along with the secret key then you should do hash(bytes(d) ||\n> bytes(P) || m)\" would suffice for me.\n> \n> Despite only being included in the alternative signing section, I it would\n> be nice to have a few of test vectors for these alternative methods anyway.\n> Perhaps they even deserve their own BIP?\n> \n> Cheers,\n> \n> LL\n> \n> \n> On Mon, Feb 24, 2020 at 3:26 PM Pieter Wuille via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n>> Hello list,\n>>\n>> Despite saying earlier that I expected no further semantical changes\n>> to BIP 340-342, I've just opened\n>> https://github.com/bitcoin/bips/pull/893 to make a number of small\n>> changes that I believe are still worth making.\n>>\n>> 1. Even public keys\n>>\n>> Only one change affects the validation rules: the Y coordinate of\n>> 32-byte public keys is changed from implicitly square to implicitly\n>> even. This makes signing slightly faster (in the microsecond range),\n>> though also verification negligibly slower (in the nanosecond range).\n>> It also simplifies integration with existing key generation\n>> infrastructure. For example BIP32 produces public keys with known\n>> even/oddness, but squaredness would need to be computed separately.\n>> Similar arguments hold for PSBT and probably many other things.\n>>\n>> Note that the Y coordinate of the internal R point in the signature\n>> remains implicitly square: for R the squaredness gives an actual\n>> performance gain at validation time, but this is not true for public\n>> keys. Conversely, for public keys integration with existing\n>> infrastructure matters, but R points are purely internal.\n>>\n>> This affects BIP 340 and 341.\n>>\n>> 2. Nonce generation\n>>\n>> All other semantical changes are around more secure nonce generation\n>> in BIP 340, dealing with various failure cases:\n>>\n>> * Since the public key signed for is included in the signature\n>> challenge hash, implementers will likely be eager to use precomputed\n>> values for these (otherwise an additional EC multiplication is\n>> necessary at signing time). If that public key data happens to be\n>> gathered from untrusted sources, it can lead to trivial leakage of the\n>> private key - something that Greg Maxwell started a discussion about\n>> on the moderncrypto curves list:\n>> https://moderncrypto.org/mail-archive/curves/2020/001012.html. We\n>> believe it should therefore be best practice to include the public key\n>> also in the nonce generation, which largely mitigates this problem.\n>>\n>> * To protect against fault injection attacks it is recommended to\n>> include actual signing-time randomness into the nonce generation\n>> process. This was mentioned already, but the update elaborates much\n>> more about this, and integrates this randomness into the standard\n>> signing process.\n>>\n>> * To protect against differential power analysis, a different way of\n>> mixing in this randomness is used (masking the private key completely\n>> with randomness before continuing, rather than hashing them together,\n>> which is known in the literature to be vulnerable to DPA in some\n>> scenarios).\n>>\n>> 3. New tagged hash tags\n>>\n>> To make sure that any code written for the earlier BIP text fails\n>> consistently, the tags used in the tagged hashes in BIP 340 are\n>> changed as well.\n>>\n>> What do people think?\n>>\n>> --\n>> Pieter\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n> \n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Lloyd Fournier",
                "date": "2020-02-27T04:55:21",
                "message_text_only": "> Correct, except that the speedup from is_even(y) over\nis_quadratic_residue(y) affects signing and not keypair generation.\n\nIsn't this the same thing since in the spec it generates the public key in\nthe signing algorithm? If you pre-generate public key and pass it in there\nwould be no speedup to signing that I can see.\n\n> It's not clear why removing these features from the spec would be an\nimprovement.\n\nIt could just be me but \"here's the most minimal signing algorithm, you can\nadd things in these ways to make it more robust  in some settings\" is more\nintuitive than \"here's the most robust signing algorithm, you can remove\nthese things in these ways if they don't apply to your setting\". I see your\npoint that if it is likely to be misused then maybe the latter is\npreferable.\n\nLL\n\nOn Thu, Feb 27, 2020 at 2:33 AM Jonas Nick <jonasdnick at gmail.com> wrote:\n\n> > Let me put change (1) into my own words.\n>\n> Correct, except that the speedup from is_even(y) over\n> is_quadratic_residue(y)\n> affects signing and not keypair generation.\n>\n> > With change (2), I feel like including this auxiliary random data is\n> overkill\n> > for the spec. [...] I feel similarly about hashing the public key to get\n> the\n> > nonce.\n>\n> It's not clear why removing these features from the spec would be an\n> improvement.\n> The BIP follows a more reasonable approach: it specifies a reasonably\n> secure\n> signing algorithm and provides the rationale behind the design choices.\n> This\n> allows anyone to optimize for their use case if they choose to do so.\n> Importantly, \"reasonably secure\" includes misuse resistance which would be\n> violated if the pubkey was not input to the nonce generation function.\n>\n> > Perhaps they even deserve their own BIP?\n>\n> Yes, a standard for nonce exfiltration protection and MuSig would be\n> important\n> for compatibility across wallets.\n>\n>\n> On 2/26/20 4:20 AM, Lloyd Fournier via bitcoin-dev wrote:\n> > Hi Pieter,\n> >\n> > Let me put change (1) into my own words. We are already computing affine\n> > coordinates since we store public keys as the affine x-coordinate. It is\n> > faster to compute is_even(y) than is_quadratic_residue(y) so we get a\n> speed\n> > up here during keypair generation. In the verification algorithm, we do\n> the\n> > following for the public key  x_only => affine + negate if not is_even(y)\n> > => jacobian. The minor slowdown in verification comes from the extra\n> > evenness check and possible negation which we didn't have to be done in\n> the\n> > previous version. This seems like a reasonable change if it makes things\n> > easier for existing code bases and infrastructure.\n> >\n> > With change (2), I feel like including this auxiliary random data is\n> > overkill for the spec. For me, the main point of the spec is the\n> > verification algorithm which actually affects consensus. Providing a note\n> > that non-deterministic signatures are preferable in many cases and here's\n> > exactly how you should do that (hash then xor with private key) is\n> > valuable. In the end, people will want several variations of the signing\n> > algorithm anyway (e.g. pass in public key with secret key) so I think\n> > specifying the most minimal way to produce a signature securely is the\n> most\n> > useful thing for this document.\n> >\n> > I feel similarly about hashing the public key to get the nonce. A note in\n> > the alternative signing section that \"if you pass the public key into\n> > `sign` along with the secret key then you should do hash(bytes(d) ||\n> > bytes(P) || m)\" would suffice for me.\n> >\n> > Despite only being included in the alternative signing section, I it\n> would\n> > be nice to have a few of test vectors for these alternative methods\n> anyway.\n> > Perhaps they even deserve their own BIP?\n> >\n> > Cheers,\n> >\n> > LL\n> >\n> >\n> > On Mon, Feb 24, 2020 at 3:26 PM Pieter Wuille via bitcoin-dev <\n> > bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >\n> >> Hello list,\n> >>\n> >> Despite saying earlier that I expected no further semantical changes\n> >> to BIP 340-342, I've just opened\n> >> https://github.com/bitcoin/bips/pull/893 to make a number of small\n> >> changes that I believe are still worth making.\n> >>\n> >> 1. Even public keys\n> >>\n> >> Only one change affects the validation rules: the Y coordinate of\n> >> 32-byte public keys is changed from implicitly square to implicitly\n> >> even. This makes signing slightly faster (in the microsecond range),\n> >> though also verification negligibly slower (in the nanosecond range).\n> >> It also simplifies integration with existing key generation\n> >> infrastructure. For example BIP32 produces public keys with known\n> >> even/oddness, but squaredness would need to be computed separately.\n> >> Similar arguments hold for PSBT and probably many other things.\n> >>\n> >> Note that the Y coordinate of the internal R point in the signature\n> >> remains implicitly square: for R the squaredness gives an actual\n> >> performance gain at validation time, but this is not true for public\n> >> keys. Conversely, for public keys integration with existing\n> >> infrastructure matters, but R points are purely internal.\n> >>\n> >> This affects BIP 340 and 341.\n> >>\n> >> 2. Nonce generation\n> >>\n> >> All other semantical changes are around more secure nonce generation\n> >> in BIP 340, dealing with various failure cases:\n> >>\n> >> * Since the public key signed for is included in the signature\n> >> challenge hash, implementers will likely be eager to use precomputed\n> >> values for these (otherwise an additional EC multiplication is\n> >> necessary at signing time). If that public key data happens to be\n> >> gathered from untrusted sources, it can lead to trivial leakage of the\n> >> private key - something that Greg Maxwell started a discussion about\n> >> on the moderncrypto curves list:\n> >> https://moderncrypto.org/mail-archive/curves/2020/001012.html. We\n> >> believe it should therefore be best practice to include the public key\n> >> also in the nonce generation, which largely mitigates this problem.\n> >>\n> >> * To protect against fault injection attacks it is recommended to\n> >> include actual signing-time randomness into the nonce generation\n> >> process. This was mentioned already, but the update elaborates much\n> >> more about this, and integrates this randomness into the standard\n> >> signing process.\n> >>\n> >> * To protect against differential power analysis, a different way of\n> >> mixing in this randomness is used (masking the private key completely\n> >> with randomness before continuing, rather than hashing them together,\n> >> which is known in the literature to be vulnerable to DPA in some\n> >> scenarios).\n> >>\n> >> 3. New tagged hash tags\n> >>\n> >> To make sure that any code written for the earlier BIP text fails\n> >> consistently, the tags used in the tagged hashes in BIP 340 are\n> >> changed as well.\n> >>\n> >> What do people think?\n> >>\n> >> --\n> >> Pieter\n> >> _______________________________________________\n> >> bitcoin-dev mailing list\n> >> bitcoin-dev at lists.linuxfoundation.org\n> >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> >>\n> >\n> >\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> >\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200227/594baa84/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "BIP 340 updates: even pubkeys, more secure nonce generation",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jonas Nick",
                "Pieter Wuille",
                "Lloyd Fournier"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 21366
        }
    },
    {
        "title": "[bitcoin-dev] Fwd:  BIP 340 updates: even pubkeys, more secure nonce generation",
        "thread_messages": [
            {
                "author": "Russell O'Connor",
                "date": "2020-02-26T03:26:42",
                "message_text_only": "On Sun, Feb 23, 2020 at 11:26 PM Pieter Wuille via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n> 2. Nonce generation\n>\n> All other semantical changes are around more secure nonce generation\n> in BIP 340, dealing with various failure cases:\n>\n> * To protect against fault injection attacks it is recommended to\n> include actual signing-time randomness into the nonce generation\n> process. This was mentioned already, but the update elaborates much\n> more about this, and integrates this randomness into the standard\n> signing process.\n>\n\nI do worry that standardizing on a non-deterministic nonce generation\nscheme makes the problem of private key exfiltration a much bigger concern\nin the application of hardware signing devices.\nWhile sorely imperfect, with a deterministic nonce scheme, we at least have\nthe option of spot checking hardware devices to see if they are producing\nsignatures in accordance with their specified nonce scheme.  But short of\nproviding some kind of certificate, we won't be able to do such checks\nagainst hardware devices that use the proposed synthetic nonce. (Question:\ncan a hardware device safely output the random value 'a' it used its\n\"certificate\"?  AFAIU 'a' is not considered secret data; it just needs to\nbe not under attacker control.  Should hardware wallets be encouraged to\nreturn this value?)\n\nThe best way to mitigate this is to use the Nonce exfiltration protection\nmentioned; however there are no references on how to do this.  Ideally we'd\nstandardize this Nonce exfiltration protection scheme within this synthetic\nnonce scheme.  However, I don't think it is worth holding this BIP up on\nthat; it seems reasonable to introduce a new section to this BIP addressing\nthat problem in the future.  Maybe instead we can get references to more\ninformation about this Nonce exfiltration protection that is mentioned?\n\nReally I just want to do whatever we reasonably can do to avoid a world\nwhere we end up providing hardware signing devices with a hard to detect\nunderhanded communications channel.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200225/1f8350fd/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Fwd:  BIP 340 updates: even pubkeys, more secure nonce generation",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Russell O'Connor"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2243
        }
    },
    {
        "title": "[bitcoin-dev] node-Tor - phases 4 and 5",
        "thread_messages": [
            {
                "author": "Aymeric Vitte",
                "date": "2020-02-25T15:36:32",
                "message_text_only": "Please see the current status here:\nhttps://github.com/Ayms/node-Tor#phases-and-funding\n\nQuick reminder: this is a javascript implementation of the Tor protocol\ninside nodes and browsers\n\nPhase 4 (evented pipes) has been developped (self funded) but is not\nfully tested/released, however the doc is here:\nhttps://github.com/Ayms/node-Tor/blob/master/docs/README.md, it allows\nto simply anonymize any protocol piping it to the Tor protocol\n\nWe were about to implement phase 5 (elliptic crypto, Tor v3 features and\nWebRTC) but are running out of funding, while we have self funded the\nvast majority of this project since 2012 we can't continue (thanks to\nNLnet for supporting phases 1 to 3) and are looking for funding to\ncomplete this work (cf above link)\n\nThe timing is supposed to be now because restarting such a project in\nmonths or years is not trivial and despite of the huge efforts for the\nrefactoring/update/cleaning of the initial code and split into modules\nit's probably still difficult to use/integrate/modify (see\nhttps://github.com/Ayms/node-Tor/issues/14), it will become quite easy\nif the project goes to its targeted phase\n\nThe code is subtle and minimal, it represents only 1MB browserified not\nminified, so 500kB minified, which is quite small for what it does with\nzero external dependencies, redevelopping everything from scratch would\nbe long and difficult\n\nSome examples of what node-Tor does (as nodes or inside browsers using\nWebSockets/WebRTC/XHR):\n\nhttp.pipe(parser).pipe(gzip).pipe(tls).pipe(node-Tor)\n\nipfs.pipe(node-Tor)\n\nwebtorrent.pipe(node-Tor)\n\nbitcoin | node-Tor | bitcoin (via stdin/stdout or using IPC)\n\nOf course the Tor protocol itself might not be enough and each project\nmight have to design the full anonymization system (peer discovery,\nintroduction, etc) but they can rely on node-Tor to implement the Tor\nprotocol (not to be misunderstood again with the Tor network)\n\nIt does implement direct p2p via the Tor protocol or via RendezVous\n(RDV) points using also Tor protocol hops to connect to them, the peers\nadvertise what they have or do using a hash to the RDV points they are\nconnected to, please see\nhttps://github.com/Ayms/node-Tor#phase-4-and-phase-5\n\nExample: by convention a bitcoin node could advertise a hash of \"Satoshi\nNakamoto\" to tell it is a bitcoin node, then bitcoin nodes will connect\nto each others via RDV points or several Tor protocol hops requesting\nthis hash, they can also connect directly via several hops for well\nknown bitcoin nodes that don't need to hide themselves but want to hide\nto whom they are connected to, which can be wallets too, for example to\nhide who originated a transaction\n\nSince peers are implementing both direct p2p and RDV functions (both via\nTor protocol hops), and can extend to other peers as peers or RDV points\nagain, it becomes difficult to understand who is doing what and how many\nhops finally are used between the peers (suggested setting for p2p is 2\nhops instead of 3 for a Tor protocol circuit, knowing that the number of\nhops can extend via RDV points)\n\nThis is the current design and can of course be adapted\n\nIt would look logical that this techno is integrated natively one day\ninside browsers, again it must not be misunderstood with what the Tor\nBrowser is doing (with many specific features inside the browser itself)\nand is not a replacement for it, this is different but could be used\nalso by the Tor network with browsers acting as real Tor nodes (a bit \u00e0\nla Snowflake but not only relaying messages via WebRTC, implementing the\nTor protocol inside browsers), or uproxy-like for those that remember it\n\n-- \nMove your coins by yourself (browser version): https://peersm.com/wallet\nBitcoin transactions made simple: https://github.com/Ayms/bitcoin-transactions\nZcash wallets made simple: https://github.com/Ayms/zcash-wallets\nBitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets\nGet the torrent dynamic blocklist: http://peersm.com/getblocklist\nCheck the 10 M passwords list: http://peersm.com/findmyass\nAnti-spies and private torrents, dynamic blocklist: http://torrent-live.org\nPeersm : http://www.peersm.com\ntorrent-live: https://github.com/Ayms/torrent-live\nnode-Tor : https://www.github.com/Ayms/node-Tor\nGitHub : https://www.github.com/Ayms"
            }
        ],
        "thread_summary": {
            "title": "node-Tor - phases 4 and 5",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Aymeric Vitte"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 4277
        }
    },
    {
        "title": "[bitcoin-dev] A proposal for WTXID-based transaction relay",
        "thread_messages": [
            {
                "author": "Suhas Daftuar",
                "date": "2020-02-25T19:48:23",
                "message_text_only": "Hi all,\n\nI've been working on a proposal to add support for relaying transactions\nbased on their wtxid, rather than just their txid.  The current draft is at\nhttps://github.com/sdaftuar/bips/blob/2020-02-wtxid-relay/bip-wtxid-relay.mediawiki,\nand for some background I'll paste the motivation section here:\n\nHistorically, the INV messages sent on the Bitcoin peer-to-peer network to\n> announce transactions refer to transactions by their txid, which is a hash\n> of the transaction that does not include the witness (see BIP 141). This\n> has been the case even since Segregated Witness (BIP 141/143/144) has been\n> adopted by the network.\n>\n\n\n> Not committing to the witness in transaction announcements creates\n> inefficiencies: because a transaction's witness can be malleated without\n> altering the txid, a node in receipt of a witness transaction that the node\n> does not accept will generally still download that same transaction when\n> announced by other peers. This is because the alternative -- of not\n> downloading a given txid after rejecting a transaction with that txid --\n> would allow a third party to interfere with transaction relay by malleating\n> a transaction's witness and announcing the resulting invalid transaction to\n> nodes, preventing relay of the valid version of the transaction as well.\n>\n\n\n> We can eliminate this concern by using the wtxid in place of the txid when\n> announcing and fetching transactions.\n>\n\nOne point specifically that I'm seeking feedback on is feature negotiation:\nfor efficiency, I think it makes sense for peers to negotiate at the\nbeginning of a connection whether they are going to use wtxid- or\ntxid-based, prior to announcing any transactions.  To achieve this, I\npropose in the BIP to send a message between receiving a VERSION message\nand prior to sending VERACK (to nodes advertising version at least 70016)\nto announce support for this new feature; if both sides send it then they\neach know to enable it on the link.  My thinking is that in general, it'd\nbe great to use messages sent between VERSION and VERACK to negotiate\nfeatures prior to fully initializing a peer connection (it's sort of a\nnatural way to extend what we might want to send in a VERSION message,\nwithout breaking existing VERSION-message parsers).  However, I don't know\nwhether inserting a message before VERACK would break any assumptions of\nother software on the network, or if this is a problematic paradigm for\nsome reason, so I'd welcome feedback here.\n\nThanks,\nSuhas\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200225/c8652b1f/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "A proposal for WTXID-based transaction relay",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Suhas Daftuar"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2687
        }
    },
    {
        "title": "[bitcoin-dev] Removing Single Point of Failure with Seed Phrase Storage",
        "thread_messages": [
            {
                "author": "Contact Team",
                "date": "2020-02-26T13:02:20",
                "message_text_only": "Hi Everyone,\nSeed phrase security has been a subject of discussion for a long time now.\nThough there are varying opinions on the subject but the conflict usually\narises due to different security models used by different individuals. The\ngeneral practice in the space has been to use paper or metal engraving\noptions to secure seed phrase but those too act as a single point of\nfailure when secure storage is concerned. The hardware wallets, no matter\nwhether use a secure element or not can be hacked either through basic\nglitching or through bigger schemes state enforced backdoors in the closed\nsoured SE used.\n\nThe option that Cypherock (Cypherock X1 Wallet)  is working on removes a\nsingle point of failure when it comes to storage of seed phrases. It uses 2\nof 4 (with the option of setting up custom threshold limit) Shamir Secret\nSharing to  split the seed phrase into 4 different shares. Each share gets\nstored in a PIN ( hardware enforced ) Card with an EAL 6+ secure element.\nThe user would need any 2 of these 4 cyCards to recover the seed or make a\ntransaction. Ideally they should all be stored at different locations and\nthis added security through distribution makes losing seed phrase highly\nimprobable. We have decoupled storage and computation aspect of a hardware\nwallet. More information can be obtained from cypherock.com. The purpose of\nthis mail is to get feedback from the community. Let us know if there is\nany feedback, we would love it.\n\nThanks\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200226/dfd153da/attachment.html>"
            },
            {
                "author": "Jeremy",
                "date": "2020-02-26T19:56:09",
                "message_text_only": "As a replacement for paper, something like this makes sense v.s. what you\ndo with a ledger presently.\n\nHowever, shamir's shares notoriously have the issue that the key does exist\nplaintext on a device at some point.\n\nNon-interactive multisig has the benefit of being able to sign transactions\nwithout having keys in the same room/place/device ever.\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n\n\nOn Wed, Feb 26, 2020 at 9:14 AM Contact Team via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi Everyone,\n> Seed phrase security has been a subject of discussion for a long time now.\n> Though there are varying opinions on the subject but the conflict usually\n> arises due to different security models used by different individuals. The\n> general practice in the space has been to use paper or metal engraving\n> options to secure seed phrase but those too act as a single point of\n> failure when secure storage is concerned. The hardware wallets, no matter\n> whether use a secure element or not can be hacked either through basic\n> glitching or through bigger schemes state enforced backdoors in the closed\n> soured SE used.\n>\n> The option that Cypherock (Cypherock X1 Wallet)  is working on removes a\n> single point of failure when it comes to storage of seed phrases. It uses 2\n> of 4 (with the option of setting up custom threshold limit) Shamir Secret\n> Sharing to  split the seed phrase into 4 different shares. Each share gets\n> stored in a PIN ( hardware enforced ) Card with an EAL 6+ secure element.\n> The user would need any 2 of these 4 cyCards to recover the seed or make a\n> transaction. Ideally they should all be stored at different locations and\n> this added security through distribution makes losing seed phrase highly\n> improbable. We have decoupled storage and computation aspect of a hardware\n> wallet. More information can be obtained from cypherock.com. The purpose\n> of this mail is to get feedback from the community. Let us know if there is\n> any feedback, we would love it.\n>\n> Thanks\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200226/5c9f93c7/attachment-0001.html>"
            },
            {
                "author": "Christopher Allen",
                "date": "2020-02-26T20:26:44",
                "message_text_only": "On Wed, Feb 26, 2020 at 11:56 AM Jeremy via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> As a replacement for paper, something like this makes sense v.s. what you\n> do with a ledger presently.\n>\n> However, shamir's shares notoriously have the issue that the key does\n> exist plaintext on a device at some point.\n>\n> Non-interactive multisig has the benefit of being able to sign\n> transactions without having keys in the same room/place/device ever.\n>\n\nI agree that that interactive multisig is best for transactional recovery,\nbut there is still a place in our tool chest for Shamir split backups\nespecially in operational security scenarios, but as you state, you must be\naware of the limitations of Shamir, not only from the fact that there is\nvulnerability in that you must restore keys in one place, but also there\nare denial edge cases where when you only have k-1 of n, someone can deny\nyou knowledge of who gave you the corrupted share.\n\nRight now the best C-library for Shamir sharding of recovery seeds is at\nthe Blockchain Commons Github\nhttps://github.com/BlockchainCommons/sss/blob/master/README_slip39.md however,\nthis code base needs refactoring to be a good standalone library. This\nrequires us separating out the Shamir portions from the SLIP-39 mnemonic\nelements and command-line elements. We also want to separate out the\nrandomness portions of the code so you can test different implementations\nagainst deterministic randomness to ensure they give the same value (but\ndon't use this for production!) Once this is complete, we will be\nsubmitting this library for formal review.\n\nWe are also working on air-gapped open-source open hardware for seed\ncreations and Shamir restoration. For instance, this device has no wifi,\nBluetooth, or persistent memory, and the serial port is disabled. It is\nsized to fit in a bank safe deposit box. See\nhttps://twitter.com/ChristopherA/status/1175465994644574208?s=20\n\n[image: image.png]\n\nLonger-term for seed sharding, we want to leverage the VSS (Verifiable\nSecret Sharing) that future Bitcoin musig uses, which we believe has\nadvantages over Shamir Secret Sharing. It can be used for both traditional\nsharding, but also for musig transactional recovery without restoring of\nmaster seeds on a single device, and it is easier to prove that shares are\nlive and avoid denial use cases.\n\nAll this being said, we recommend Bitcoin multisig transactions as the best\nsolution for self-sovereign recovery of funds vs using Shamir (or SLIP39).\nLately, we've been working on PSBT and bitcoind descriptor support in our\nalpha bitcoin wallet for iOS, FullyNoded 2\nhttps://github.com/BlockchainCommons/FullyNoded-2 \u2014 in theory, this iOS\nwallet can facilitate any scenario that bitcoind supports as a descriptor\nas it communicates with your own private full-node over Tor v3.\n\nP.S. If you've not seen it yet, our free #SmartCustody book\nhttp://bit.ly/SmartCustodyBookV101 offers what we think are current best\npractices for single seed recovery. We did not include multisig scenarios\nin v1 of this book as at the time they were too difficult and error-prone\nfor most people. We are now working on v2 of the book which will cover\nmultisign and fiduciary scenarios now that PSBT is maturing and more\nwallets are improving multisig support.\n\n\u2014 Christopher Allen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200226/55bd31cd/attachment-0001.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: image.png\nType: image/png\nSize: 4330187 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200226/55bd31cd/attachment-0001.png>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2020-02-28T13:10:27",
                "message_text_only": "Good morning Christopher,\n\n>\n> > As a replacement for paper, something like this makes sense v.s. what you do with a ledger presently.\n> >\n> > However, shamir's shares notoriously have the issue that the key does exist plaintext on a device at some point.\n> >\n> > Non-interactive multisig has the benefit of being able to sign transactions without having keys in the same room/place/device ever.\n>\n> I agree that that interactive multisig is best for transactional recovery, but there is still a place in our tool chest for Shamir split backups especially in operational security scenarios, but as you state, you must be aware of the limitations of Shamir, not only from the fact that there is vulnerability in that you must restore keys in one place, but also there are denial edge cases where when you only have k-1 of n, someone can deny you knowledge of who gave you the corrupted share.\u00a0\n>\n> Right now the best C-library for Shamir sharding of recovery seeds is at the Blockchain Commons Github\u00a0https://github.com/BlockchainCommons/sss/blob/master/README_slip39.md\u00a0however, this code base needs refactoring to be a good standalone library. This requires us separating out the Shamir portions from the SLIP-39 mnemonic elements and command-line elements. We also want to separate out the randomness portions of the code so you can test different implementations against deterministic randomness to ensure they give the same value (but don't use this for production!) Once this is complete, we will be submitting this library for formal review.\n>\n> We are also working on air-gapped open-source open hardware for seed creations and Shamir restoration. For instance, this device has no wifi, Bluetooth, or persistent memory, and the serial port is disabled. It is sized to fit in a bank safe deposit box. See\u00a0https://twitter.com/ChristopherA/status/1175465994644574208?s=20\n>\n> Longer-term for seed sharding, we want to leverage the VSS (Verifiable Secret Sharing) that future Bitcoin musig uses, which we believe has advantages over Shamir Secret Sharing. It can be used for both traditional sharding, but also for musig transactional recovery without restoring of master seeds on a single device, and it is easier to prove that shares are live and avoid denial use cases.\n>\n> All this being said, we recommend Bitcoin multisig transactions as the best solution for self-sovereign recovery of funds vs using Shamir (or SLIP39). Lately, we've been working on PSBT and bitcoind descriptor support in our alpha bitcoin wallet for iOS, FullyNoded 2\u00a0https://github.com/BlockchainCommons/FullyNoded-2\u00a0\u2014 in theory, this iOS wallet can facilitate any scenario that bitcoind supports as a descriptor as it communicates with your own private full-node over Tor v3.\n>\n> P.S. If you've not seen it yet, our free #SmartCustody book http://bit.ly/SmartCustodyBookV101 offers what we think are current best practices for single seed recovery. We did not include multisig scenarios in v1 of this book as at the time they were too difficult and error-prone for most people. We are now working on v2 of the book which will cover multisign and fiduciary scenarios now that PSBT is maturing and more wallets are improving multisig support.\n\nI believe you missed a point that Jeremy was making:\n\n* In a Shamir Secret Sharing Scheme, at some point in the past, some device had to contain the actual original pre-shared secret, and that device might have had backdoors you were unaware of at the time that you were doing the secret sharing.\n  * Thus the entire secret might already be compromised long before you recover the secret again.\n\nVerifiable Secret Sharing seems to be better in this regard, as each signer device can generate its own share independently of every other and effectively do an interactive setup to determine their public key (as I understand it --- I am not a mathist, and while I do have some notes regarding this from gmax, I confess I still have only a vague grasp of this math).\nIt may be better to outright forget Shamir Secret Sharing even exists, and prefer to use Verifiable Secret Sharing instead.\n\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Removing Single Point of Failure with Seed Phrase Storage",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "Jeremy",
                "Christopher Allen",
                "Contact Team"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 11997
        }
    },
    {
        "title": "[bitcoin-dev] Nonce blinding protocol for hardware wallets and airgapped signers",
        "thread_messages": [
            {
                "author": "Stepan Snigirev",
                "date": "2020-02-27T02:59:46",
                "message_text_only": "This topic appeared in the list a few times so I would like to discuss it\nin more detail and maybe push forward to standardization.\n\nWe have to accept that any hardware wallet or an air-gapped computer we use\nto sign transactions can be compromised. It may happen via a supply chain\nattack or malicious firmware update.\n\nIf the signer is isolated (faraday cage, airgap and so on), it still can\nleak private keys to the outside world by choosing nonces for signatures in\na funny way such that the attacker can calculate our private keys. Back in\nthe days, I wrote a small post [1] and a proof-of-concept demo [2] of this\nattack.\n\nDeterministic nonce generation can be verified only if we have private keys\nsomewhere else. It doubles the attack surface - now we need to maintain two\nindependent signers from different vendors that use the same private key\nand the same deterministic algorithm for a nonce generation. In addition to\nthat, as Pieter mentioned in the Schnorr-BIP, deterministic nonces are\nvulnerable to glitch attacks [3].\n\nA simple way to fix it is by forcing the signer to use additional entropy\nfrom the host. This protocol takes away the privilege of picking nonce from\nthe signer and doesn't require any secret material outside the signer.\n\nI suggest the following implementation of the protocol for signing a\nmessage `m`:\n\n1. Host picks a random number `n` and sends its hash together with the\nmessage `m` to the signer.\n2. Signer computes a nonce `k` it wants to use for signing. It can be\neither a deterministic scheme or using RNG. Signer commits to the chosen\nnonce by sending the corresponding point `R=kG` to the host.\n3. Host sends the preimage `n` to the signer\n4. Signer tweaks the nonce by this number `k'=k+n`, signs the message and\nsends back the signature (R',s)\n5. Host verifies that the public point in the signature is tweaked by n:\n`R'==R+nG`\n\nASCII-art:\n\n   Host                                Untrusted signer\n1. Pick random n   --- sha256(n),m -->  calculate nonce k\n2.                 <------ R=kG ------  commit to k\n3. Send preimage   -------- n ------->  sign with nonce k'=k+n\n4. Verify R'==R+nG <------- sig ------\n\nI believe this protocol solves the problem. A drawback of this scheme is\nthat the number of communication rounds doubles, so it might be pretty\ninconvenient for air-gapped remotely located signers.\n\nI also suggest the following extensions that might be helpful for certain\nuse-cases\n\n# Extensions\n\n## Multiple hosts\n\nThere are some use-cases where multiple hosts are involved in the setup and\nall hosts don't trust each other and the signer. So all of them want to\ngive extra entropy to the signer and verify that it was included. At the\nmoment I have exactly this scenario - our main MCU doesn't trust the\nproprietary closed-source secure element, and the computer doesn't trust\nthe whole hardware wallet. We need a way to convince both of them that\ntheir entropy was used in the nonce.\n\nIt can be solved by concatenating hashes and preimages:\n\nHost1 ------- h(n1) --> Host 2 -- h(n1) h(n2) --> Signer\n      <--- R+n2 G -----        <------- R -------\n      ------- n1 ----->        ------ n1 n2 ----> sign with k''=k+n1+n2\nVer: R''==R'+n1 G       Ver: R''==R+n2 G + n1 G\n\nIn this case, the first host doesn't even notice that the second host was\nalso using this protocol and mixing in the entropy. And the signer only\nneeds to add one extra number to the nonce.\n\n## Stateless random signer\n\nIf the signer wants to generate a nonce non-deterministically but doesn't\nhave an ability to store a generated nonce it may send back to the host\nsome meta-information that would help it to re-generate the same nonce\nlater. It can be for example additional random data used in a deterministic\nscheme, either encrypted and authenticated or just as a plain text (I am\nmore a fan of encrypted though).\n\nGenerally, the host shouldn't care what this data is about - he just stores\nthe data between rounds and sends it back to the signer with the next round.\n\n# Implementation for PSBT\n\nWe can either use proprietary fields [4] or define key-value pairs and add\nthem to the BIP-174. Depends if anyone else is interested in using this\nprotocol or not.\n\nI would suggest the following key-value per-input pairs assuming multiple\nhosts want to mix in external entropy:\n\n1. Key: {PSBT_IN_EXT_NONCE_HASH}|{pubkey}, Value:\n{sha256(n1)}|{sha256(n2)}|...\n2. Key: {PSBT_IN_NONCE_COMMITMENT}|{pubkey}, Value: {33-byte R point}\n3. Key: {PSBT_IN_NONCE_SIGNER_METADATA}|{pubkey}, Value: {anything}\n4. Key: {PSBT_IN_EXT_NONCE_PREIMAGE}|{pubkey}, Value: {n1}|{n2}|...\n\nThen the signature from the signer is placed into existing\nPSBT_IN_PARTIAL_SIG. Combiner and Finaliser should verify that nonce in the\nsignature includes external entropy and may remove their own entropy from\nthe set. They should also verify that the values of the fields did not\nchange between rounds.\n\nSo, list, what do you think? Am I missing something? Would it be\ninteresting to have this protocol standardized and deployed?\n\n# References\n\n[1]\nhttps://medium.com/cryptoadvance/hardware-wallets-can-be-hacked-but-this-is-fine-a6156bbd199\n[2]\nhttps://github.com/stepansnigirev/chosen_nonce_demo/blob/master/HD_key.ipynb\n[3]\nhttps://github.com/bitcoin/bips/blob/master/bip-0340.mediawiki#alternative-signing\n[4]\nhttps://github.com/bitcoin/bips/blob/master/bip-0174.mediawiki#proprietary-use-type\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200227/ec3bb6ad/attachment.html>"
            },
            {
                "author": "freedom at reardencode.com",
                "date": "2020-02-27T03:26:07",
                "message_text_only": "I've been working on developing exactly this for a hardware wallet in working on as well. So far I only had a rough sketch of how the comments would flow. I'd be thrilled to see this standardized.\n\nThanks much,\n\n--Brandon, sent by an android\n\n-----Original Message-----\nFrom: Stepan Snigirev via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>\nTo: bitcoin-dev at lists.linuxfoundation.org\nSent: Wed, 26 Feb 2020 19:13\nSubject: [bitcoin-dev] Nonce blinding protocol for hardware wallets and airgapped signers\n\nThis topic appeared in the list a few times so I would like to discuss it\nin more detail and maybe push forward to standardization.\n\nWe have to accept that any hardware wallet or an air-gapped computer we use\nto sign transactions can be compromised. It may happen via a supply chain\nattack or malicious firmware update.\n\nIf the signer is isolated (faraday cage, airgap and so on), it still can\nleak private keys to the outside world by choosing nonces for signatures in\na funny way such that the attacker can calculate our private keys. Back in\nthe days, I wrote a small post [1] and a proof-of-concept demo [2] of this\nattack.\n\nDeterministic nonce generation can be verified only if we have private keys\nsomewhere else. It doubles the attack surface - now we need to maintain two\nindependent signers from different vendors that use the same private key\nand the same deterministic algorithm for a nonce generation. In addition to\nthat, as Pieter mentioned in the Schnorr-BIP, deterministic nonces are\nvulnerable to glitch attacks [3].\n\nA simple way to fix it is by forcing the signer to use additional entropy\nfrom the host. This protocol takes away the privilege of picking nonce from\nthe signer and doesn't require any secret material outside the signer.\n\nI suggest the following implementation of the protocol for signing a\nmessage `m`:\n\n1. Host picks a random number `n` and sends its hash together with the\nmessage `m` to the signer.\n2. Signer computes a nonce `k` it wants to use for signing. It can be\neither a deterministic scheme or using RNG. Signer commits to the chosen\nnonce by sending the corresponding point `R=kG` to the host.\n3. Host sends the preimage `n` to the signer\n4. Signer tweaks the nonce by this number `k'=k+n`, signs the message and\nsends back the signature (R',s)\n5. Host verifies that the public point in the signature is tweaked by n:\n`R'==R+nG`\n\nASCII-art:\n\n   Host                                Untrusted signer\n1. Pick random n   --- sha256(n),m -->  calculate nonce k\n2.                 <------ R=kG ------  commit to k\n3. Send preimage   -------- n ------->  sign with nonce k'=k+n\n4. Verify R'==R+nG <------- sig ------\n\nI believe this protocol solves the problem. A drawback of this scheme is\nthat the number of communication rounds doubles, so it might be pretty\ninconvenient for air-gapped remotely located signers.\n\nI also suggest the following extensions that might be helpful for certain\nuse-cases\n\n# Extensions\n\n## Multiple hosts\n\nThere are some use-cases where multiple hosts are involved in the setup and\nall hosts don't trust each other and the signer. So all of them want to\ngive extra entropy to the signer and verify that it was included. At the\nmoment I have exactly this scenario - our main MCU doesn't trust the\nproprietary closed-source secure element, and the computer doesn't trust\nthe whole hardware wallet. We need a way to convince both of them that\ntheir entropy was used in the nonce.\n\nIt can be solved by concatenating hashes and preimages:\n\nHost1 ------- h(n1) --> Host 2 -- h(n1) h(n2) --> Signer\n      <--- R+n2 G -----        <------- R -------\n      ------- n1 ----->        ------ n1 n2 ----> sign with k''=k+n1+n2\nVer: R''==R'+n1 G       Ver: R''==R+n2 G + n1 G\n\nIn this case, the first host doesn't even notice that the second host was\nalso using this protocol and mixing in the entropy. And the signer only\nneeds to add one extra number to the nonce.\n\n## Stateless random signer\n\nIf the signer wants to generate a nonce non-deterministically but doesn't\nhave an ability to store a generated nonce it may send back to the host\nsome meta-information that would help it to re-generate the same nonce\nlater. It can be for example additional random data used in a deterministic\nscheme, either encrypted and authenticated or just as a plain text (I am\nmore a fan of encrypted though).\n\nGenerally, the host shouldn't care what this data is about - he just stores\nthe data between rounds and sends it back to the signer with the next round.\n\n# Implementation for PSBT\n\nWe can either use proprietary fields [4] or define key-value pairs and add\nthem to the BIP-174. Depends if anyone else is interested in using this\nprotocol or not.\n\nI would suggest the following key-value per-input pairs assuming multiple\nhosts want to mix in external entropy:\n\n1. Key: {PSBT_IN_EXT_NONCE_HASH}|{pubkey}, Value:\n{sha256(n1)}|{sha256(n2)}|...\n2. Key: {PSBT_IN_NONCE_COMMITMENT}|{pubkey}, Value: {33-byte R point}\n3. Key: {PSBT_IN_NONCE_SIGNER_METADATA}|{pubkey}, Value: {anything}\n4. Key: {PSBT_IN_EXT_NONCE_PREIMAGE}|{pubkey}, Value: {n1}|{n2}|...\n\nThen the signature from the signer is placed into existing\nPSBT_IN_PARTIAL_SIG. Combiner and Finaliser should verify that nonce in the\nsignature includes external entropy and may remove their own entropy from\nthe set. They should also verify that the values of the fields did not\nchange between rounds.\n\nSo, list, what do you think? Am I missing something? Would it be\ninteresting to have this protocol standardized and deployed?\n\n# References\n\n[1]\nhttps://medium.com/cryptoadvance/hardware-wallets-can-be-hacked-but-this-is-fine-a6156bbd199\n[2]\nhttps://github.com/stepansnigirev/chosen_nonce_demo/blob/master/HD_key.ipynb\n[3]\nhttps://github.com/bitcoin/bips/blob/master/bip-0340.mediawiki#alternative-signing\n[4]\nhttps://github.com/bitcoin/bips/blob/master/bip-0174.mediawiki#proprietary-use-type"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2020-02-28T13:31:29",
                "message_text_only": "Good morning Stepan,\n\n> This topic appeared in the list a few times so I would like to discuss it in more detail and maybe push forward to standardization.\n>\n> We have to accept that any hardware wallet or an air-gapped computer we use to sign transactions can be compromised. It may happen via a supply chain attack or malicious firmware update.\n>\n> If the signer is isolated (faraday cage, airgap and so on), it still can leak private keys to the outside world by choosing nonces for signatures in a funny way such that the attacker can calculate our private keys. Back in the days, I wrote a small post [1] and a proof-of-concept demo [2] of this attack.\n>\n> Deterministic nonce generation can be verified only if we have private keys somewhere else. It doubles the attack surface - now we need to maintain two independent signers from different vendors that use the same private key and the same deterministic algorithm for a nonce generation. In addition to that, as Pieter mentioned in the Schnorr-BIP, deterministic nonces are vulnerable to glitch attacks [3].\n>\n> A simple way to fix it is by forcing the signer to use additional entropy from the host. This protocol takes away the privilege of picking nonce from the signer and doesn't require any secret material outside the signer.\n>\n> I suggest the following implementation of the protocol for signing a message `m`:\n>\n> 1. Host picks a random number `n` and sends its hash together with the message `m` to the signer.\n> 2. Signer computes a nonce `k` it wants to use for signing. It can be either a deterministic scheme or using RNG. Signer commits to the chosen nonce by sending the corresponding point `R=kG` to the host.\n\nI think it would be unsafe to use a deterministic scheme, that takes as input the message m and the privkey only.\n\nLet us consider the case where the hardware signer has its power supply coming from USB and the communication channel is over USB as well.\nThus, the host can selectively turn on/off the hardware signer (e.g. a hacker with physical access can just unplug it).\n\nWith R determined from m and the privkey, then the host knows the R that the signer will use, and can arrange an n that cancels that R and adds a specific R it wants to target.\nIt could, for example, arrange to have two different `m` signed with the same `R'`.\n\nWhat would have to be done would be derive `k` from the message `m` plus the `sha256(n)` and the privkey.\nPerhaps you considered this already, but it may be useful to have it explicitly stated that this has to be mixed as well, i.e. if `k` is generated deterministically it has to be `k = f(sha256(n), m, privkey)` where `f()` is some suitable hashing function.\n\nOtherwise a completely-random `k` would be much better, but the signer might not have enough resources to gather sufficient entropy.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Stepan Snigirev",
                "date": "2020-02-28T14:40:21",
                "message_text_only": "Dear ZmnSCPxj,\n\n> I think it would be unsafe to use a deterministic scheme, that takes as\ninput the message m and the privkey only.\n\nYes, using only the message and the private key is unsafe. Signer should\nuse all the data coming from the host, so f(sha256(n), m, privkey) is a\ngood candidate. If more than one blinding factor is sent - all of them\nshould be used as well.\n\n> Otherwise a completely-random `k` would be much better, but the signer\nmight not have enough resources to gather sufficient entropy.\n\nI am not a big fan of pure RNG-generated nonces, so I would suggest to use\nthis entropy only as additional data for a deterministic scheme.\nFor example, Yubikey had a problem with RNG initialization that caused\nleakage of the private key [1].\nIf the signer has any source of entropy, even if it is not a very good one,\nthe entropy from this source can be mixed into the nonce generation\nfunction:\nf(sha256(n),m,privkey,entropy).\n\nAnother issue is that deterministic nonce generation is vulnerable to\nglitch attacks - if I ask the wallet to sign the same message twice but\nafter nonce generation I glitch and flip a bit in the message, I will get\ntwo signatures with the same nonce but with different messages - from these\nsignatures I can calculate the private key.\nSo I would recommend to include a monotonic counter into the nonce\ngeneration function as well: f(sha256(n), m, privkey, entropy, counter)\nAs usual, counter should be increased _before_ signing.\n\nRef: [1]\nhttps://www.yubico.com/support/security-advisories/ysa-2019-02/#technical-details\n\nBest,\nStepan\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200228/4e3805b4/attachment-0001.html>"
            },
            {
                "author": "Marko",
                "date": "2020-02-28T17:42:15",
                "message_text_only": "Thanks for starting this initiative; it has been a long standing goal of\nmine to implement and release this protocol. Your blog post on the topic\nactually inspired me to pick up this work again a few months ago.\n\nJonas Nick has implemented the protocol in the secp256k1 library for\nSchnorr sigs here: https://github.com/bitcoin-core/secp256k1/pull/590\n\nI have backported the same scheme to ECDSA in the secp256k1 library\nhere, so it can be used also for current transactions:\n\nhttps://github.com/bitcoin-core/secp256k1/pull/669\n\nI also made proof of concepts for the BitBox02 hw wallet firmware and\nBitBoxApp wallet to verify that the protocol also works well in practice.\n\nThe actual scheme used in those implementations is a generalized\nsign-to-contract scheme, where the final nonce is computed as `k' = k +\nH(k*G, n)` instead of `k'=k+n`, but otherwise it works mostly the same\nfor the anti nonce covert channel protocol. I suggest to use this scheme\nin PSBT as well.\n\n> We can either use proprietary fields [4] or define key-value pairs and add\n> them to the BIP-174. Depends if anyone else is interested in using this\n> protocol or not.\n\nI'd definitely be interested in seeing widespread support for this, and\nstandardizing it would help with that.\n\nWith PSBT used with an air-gapped signer, there is increased danger in\nimplementing the protocol wrongly by relying on the contents of the PSBT\nalone in the final verification step of a signature. The PSBT must be\nverified carefully against state stored by the host for the PSBT.\nOtherwise the signer can for example change or pre-fill the relevant\nNONCE fields and leak the private keys anyway. Is there a current best\npractice for how a PSBT can be identified by the host to store/retrieve\nthe state?\n\nAre there other examples in PSBT where the host can't trust the contents\nof the PSBT the signer returns (except of course for the parts the user\ncan verify themselves, like recipients, amounts, etc.)? In any case,\nguidelines or conventions on how to avoid the pitfalls would be good.\n\nBest, Marko\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: 0x67A2B160F74DB275.asc\nType: application/pgp-keys\nSize: 8704 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200228/b87d6f4d/attachment.bin>"
            }
        ],
        "thread_summary": {
            "title": "Nonce blinding protocol for hardware wallets and airgapped signers",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "freedom at reardencode.com",
                "Stepan Snigirev",
                "ZmnSCPxj",
                "Marko"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 18431
        }
    },
    {
        "title": "[bitcoin-dev] [Annoucement] Discreet Log Contract Protocol Specification",
        "thread_messages": [
            {
                "author": "Nadav Kohen",
                "date": "2020-02-27T16:17:47",
                "message_text_only": "Hi List,\n\nWe now have have some working code for executing 2-outcome Discreet Log\nContracts as well as documentation\n<https://bitcoin-s.org/docs/next/applications/dlc> on how to use it, and\nwould love if anyone wanted to try it out!\n\nHere's a short blog post\n<https://suredbits.com/discreet-log-contract-demonstration/> with a video\ndemo of an execution.\n\nAnd as always, we would also be thrilled if anyone interested in reviewing\nor contributing took a look at our work-in-progress specification\n<https://github.com/discreetlogcontracts/dlcspecs/>.\n\nBest,\nNadav\n\nOn Tue, Jan 28, 2020 at 3:38 AM Lloyd Fournier via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi Chris,\n>\n> This is a really exciting effort. I hope I will be able to contribute to\n> it. I was wondering if you had seen the idea that DLCs can be done in only\n> two transaction using Schnorr[1]. I also think this can be done in Bitcoin\n> as it is today using ECDSA adaptor signatures [2]. In my mind, the adaptor\n> signature protocol is both easier to specify and implement on top of being\n> cheaper and more private.\n>\n> LL\n>\n> [1] https://lists.launchpad.net/mimblewimble/msg00485.html\n> [2]\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-November/002316.html\n>\n> On Tue, Jan 14, 2020 at 2:12 AM Chris Stewart via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Hi all,\n>>\n>> Suredbits and Crypto Garage have begun to work on a specification for\n>> using discreet log contracts <https://adiabat.github.io/dlc.pdf> in a\n>> safe, private and interoperable way. We are writing to the mailing list to\n>> inform and solicit feedback for the protocol specification so that we can\n>> -- as a community -- agree on a common standard to use Bitcoin oracles.\n>>\n>> Our goal is to end up with a set of documents like the BIPs (Bitcoin\n>> Improvement Proposals) and BOLTs (Basis of Lightning Technology) so that\n>> others that wish to use the technology can easily write software to\n>> integrate into the protocol.\n>>\n>> A secondary goal of ours is to remain compatible with standards used by\n>> other bitcoin related protocols (like Lightning) so that every future\n>> bitcoin related protocol can reach for a \u201ctoolbox\u201d of agreed standards for\n>> things like funding transactions and closing transactions. We want to avoid\n>> reinventing the wheel where possible and allow for library developers to\n>> re-use software to hook into many bitcoin related protocols.\n>>\n>> You can find the specification repository here:\n>>\n>> https://github.com/discreetlogcontracts/dlcspecs/\n>>\n>> For more information on DLCs:\n>>\n>> [1] - https://adiabat.github.io/dlc.pdf\n>>\n>> [2] - https://cryptogarage.co.jp/p2pd/\n>>\n>> [3] -\n>> https://suredbits.com/discreet-log-contracts-part-1-what-is-a-discreet-log-contract/\n>>\n>> [4] -\n>> https://blockstream.com/2019/04/19/en-transacting-bitcoin-based-p2p-derivatives/\n>>\n>> [5] - https://dci.mit.edu/smart-contracts\n>>\n>> -Chris\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200227/1b595a2f/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Discreet Log Contract Protocol Specification",
            "categories": [
                "bitcoin-dev",
                "Annoucement"
            ],
            "authors": [
                "Nadav Kohen"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 3534
        }
    },
    {
        "title": "[bitcoin-dev] [bitcin-dev] BIP157 Filter Type Extensibility Proposal for Combinations",
        "thread_messages": [
            {
                "author": "nopara73",
                "date": "2020-02-28T14:37:14",
                "message_text_only": "BIP157 defines a section called \"Filter Types\" (\nhttps://github.com/bitcoin/bips/blob/master/bip-0157.mediawiki#filter-types\n )\n\n>  For the sake of future extensibility and reducing filter sizes, there\nare multiple *filter types* that determine which data is included in a\nblock filter as well as the method of filter construction/querying. In this\nmodel, full nodes generate one filter per block per filter type supported.\n\n>  Each type is identified by a one byte code, and specifies the contents\nand serialization format of the filter. A full node MAY signal support for\nparticular filter types using service bits. The initial filter types are\ndefined separately in BIP 158\n<https://github.com/bitcoin/bips/blob/master/bip-0158.mediawiki>, and one\nservice bit is allocated to signal support for them.\n\nWhile it provides a way to extend to multiple filter types, it does not\nprovide a way to extend to filter type combinations. Such combinations are\npossible if the filter types would have to be defined with the power of\ntwo: 1, 2, 4, 8, 16..., so every octet of a byte array could denote a\nspecific filter type, this way we could be able to signal for any number of\ncombinations of those filter types.\n\nOriginally this idea is described in more details and with code here:\nhttps://github.com/bitcoin/bitcoin/issues/18221\n\nMarcoFalke suggested for me to submit it to the mailing list instead of a\nGitHub issue then propose an update to the BIP if consensus is reached.\n\n-- \nBest,\n\u00c1d\u00e1m\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20200228/33b62fd8/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "BIP157 Filter Type Extensibility Proposal for Combinations",
            "categories": [
                "bitcoin-dev",
                "bitcin-dev"
            ],
            "authors": [
                "nopara73"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1675
        }
    }
]